\documentclass[10pt]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{bbm}
\usepackage{comment}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\PassOptionsToPackage{table}{xcolor}
\usepackage[table]{xcolor}

% Use the following line for the initial blind version submitted for review:
\usepackage{natbib}
\bibliographystyle{apalike}

\bibpunct{(}{)}{;}{a}{,}{,}

\usepackage{url}            % simple URL typesetting
\usepackage[backref=page]{hyperref}       % hyperlinks

\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in AcrobatÕs bookmarks
    pdftoolbar=true,        % show AcrobatÕs toolbar?
    pdfmenubar=true,        % show AcrobatÕs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={Sample-efficient Learning of Concepts},    % title
    pdfauthor={Hidde Fokkema},
    pdfsubject={Machine Learning},   % subject of the document
    pdfcreator={pdflatex},   % creator of the document
    pdfproducer={Producer}, % producer of the document
    pdfkeywords={Interpretability} {Concepts} {Causal Representation Learning}
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,       % color of internal links
}



\usepackage{thm-restate}


%Macros
\usepackage{macros}

\usepackage{arxiv}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% Tikz settings
\usepackage{tikz-cd}
\usetikzlibrary{fit}
\usetikzlibrary{calc}
\newcommand{\xshift}{-3.5em}
\newcommand{\yshift}{-3em}
\usetikzlibrary{patterns.meta}

\tikzset{
    noise/.style={
        circle,
        draw=black, 
        fill=black!30, 
        very thick, 
        inner sep=0pt,
        minimum size=8mm},
    feat/.style={
        circle,
        draw=black, 
        fill=orange, 
        very thick, 
        inner sep=0pt,
        minimum size=8mm},
    rep/.style={
        circle,
        draw=black, 
        fill=blue!50, 
        very thick, 
        inner sep=0pt,
        minimum size=8mm},
    bicolor/.style args={#1 and #2}{
        path picture={
            \tikzset{rounded corners=0} 
            \path [fill=#1,
                postaction={
                    pattern={
                        Lines[angle=45,distance={4pt},line width=2pt]
                    },
                    pattern color=blue!40
                },
                postaction={draw, semithick}
                ] (path picture bounding box.west)
                rectangle (path picture bounding box.north east);
            \draw [black, very thick] (path picture bounding box.west)
                rectangle (path picture bounding box.north east);
            %
            \fill [#2] (path picture bounding box.west)
                rectangle (path picture bounding box.south east);
            \draw [black, very thick] (path picture bounding box.west)
                rectangle (path picture bounding box.south east);}}}

% Plate node
\tikzstyle{plate} = [draw, rectangle, rounded corners, fit=#1, fill=black!5]
% Invisible wrapper node
\tikzstyle{wrap} = [inner sep=0pt, fit=#1]
% Gate
\tikzstyle{gate} = [draw, rectangle, dashed, fit=#1]

% Caption node
\tikzstyle{caption} = [font=\footnotesize, node distance=0] %
\tikzstyle{plate caption} = [caption, node distance=0, inner sep=0pt,
below left=5pt and 0pt of #1.south east] %
\tikzstyle{factor caption} = [caption] %
\tikzstyle{every label} += [caption] %


\newcommand{\plate}[4][]{ %
  \node[wrap=#3] (#2-wrap) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
  \node[plate=(#2-wrap)(#2-caption), #1] (#2) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
}

\newcommand*\circled[1]{%
    \tikz[baseline=(char.base)]{%
        \node[%
            shape=circle,
            draw=black, 
            solid, 
            inner sep=2pt, 
            line width=0.5mm
        ] (char) {#1};
    }
}

% Comment this out when pushing
% \pagecolor{darkgray}
% \color{white}

% Trying out a nicer fraction setup
\newcommand{\nfrac}[2]{\nicefrac{#1}{#2}}

% Independent and dependent symbols
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1\:\:\!\!#2}}}

\newcommand\dependent{\protect\mathpalette{\protect\dependenT}{\perp}}
\def\dependenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{\not#1\:\:\!\!#2}}}

% KL
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\newcommand{\infdivKL}{\text{KL}\infdivx}

% Calculus notation
\newcommand{\Jac}[2]{\mathbf{J#1}(#2)}

% SCM related macros
\newcommand{\Do}[1]{\mathrm{do}(#1)}
\newcommand{\PA}{\mathrm{Pa}}
\newcommand{\Val}{\mathrm{Val}}

% Abtraction related macros
\newcommand{\low}[1]{#1^{\mathcal{L}}}
\newcommand{\high}[1]{#1^{\mathcal{H}}}

\newcommand{\Mlow}{\mathcal{M}^{\mathcal{L}}}
\newcommand{\Mhigh}{\mathcal{M}^{\mathcal{H}}}
\newcommand{\Ilow}{\mathcal{I}^{\mathcal{L}}}
\newcommand{\Ihigh}{\mathcal{I}^{\mathcal{H}}}

\newcommand{\Lsim}{\stackrel{L}{\sim}}
\newcommand{\Psim}{\stackrel{P}{\sim}}

\newcommand{\diag}{\textrm{diag}}
\newcommand{\longvec}[1]{\overrightarrow{#1}}

\newcommand{\pmin}{p_{\textup{\textrm{min}}}}
\newcommand{\pmax}{p_{\textup{\textrm{max}}}}

\newcommand{\fstar}{\textbf{\textup{\textrm{f}}}^*}
\newcommand{\Mvec}{\textbf{\textup{\textrm{M}}}}
\newcommand{\Hvec}{\textbf{\textup{\textrm{C}}}}

\newcommand{\smallsum}{\mathop{\textstyle\sum}}


\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\Gaussian}{\mathcal{N}}
\newcommand{\ip}[1]{\langle #1 \rangle}   % inner product

\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\sara}[1]{\textcolor{red}{[Sara: #1]}}
\newcommand{\tim}[1]{\textcolor{magenta}{[Tim: #1]}}


\title{Sample-efficient Learning of Concepts with Theoretical Guarantees:\linebreak
from Data to Concepts without Interventions}
\shorttitle{Sample-efficient Learning of Concepts}

\author{%
    \name{Hidde Fokkema} \email{h.j.fokkema@uva.nl}\\
    \and
    \name{Tim van Erven\thanks{Equal contribution}} \email{tim@timvanerven.nl}\\
    \addr{Korteweg-de Vries Institute for Mathematics, University of Amsterdam}
    \AND
    \name{Sara Magliacane\footnotemark[1]} \email{s.magliacane@uva.nl} \\
    \addr{Informatics Institute, University of Amsterdam}
}


\begin{document}

\maketitle

\begin{abstract}
Machine learning is a vital part of many real-world systems, but several
concerns remain about the lack of interpretability, explainability and
robustness of black-box AI systems. Concept-based models (CBM) address some of
these challenges by learning interpretable \emph{concepts} from
high-dimensional data, e.g.\ images, which are used to predict labels. An
important issue in CBMs is \emph{concept leakage}, i.e., spurious information
in the learned concepts, which effectively leads to learning ``wrong''
concepts. Current mitigating strategies are heuristic, have strong assumptions,
e.g., they assume that the concepts are statistically independent of each
other, or require substantial human interaction in terms of both interventions
and labels provided by annotators. In this paper, we describe a framework that
provides theoretical guarantees on the correctness of the learned concepts and
on the number of required labels, without requiring any interventions. Our
framework leverages causal representation learning (CRL) to learn high-level
causal variables from low-level data, and learns to align these variables with
interpretable concepts. We propose a linear and a non-parametric estimator for
this mapping, providing a finite-sample high probability result in the linear
case and an asymptotic consistency result for the non-parametric estimator. We
implement our framework with state-of-the-art CRL methods, and show its
efficacy in learning the correct concepts in synthetic and image benchmarks.
\end{abstract}
\begin{keywords}
Interpretability; Concepts; Causal Representation Learning.
\end{keywords}


\section{Introduction}
Machine learning is a vital part of many real-world systems, but concerns
remain about the lack of interpretability, robustness and adherence to
regulations of current systems \citep{BengioEtAl2025}. These issues might be
exacerbated by the lack of guarantees in explaining the behavior of AI systems
in terms of human interpretable, high-level \emph{concepts}. 

The field of interpretable machine learning and explainable AI have developed
many techniques to interpret models and explain their predictions
\citep{molnar2022}, either by extracting known concepts from the internals of
black-box models 
\citep{kim2018interpretability, goyal2019explaining, 
graziani2023concept, lovering2022unit},
or by building the explicit use of  
concepts into the internals of these systems, e.g.\ as in \emph{concept-based models} (CBM) 
\citep{koh2020conceptbottleneck, ismail2023concept, 
marconato2022glance, zarlenga2022concept}. 

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        neuron/.style={
            circle, 
            draw=black!60, 
            very thick, 
            fill=white, 
            inner sep=0pt,
            minimum size=4pt
        },
        output/.style={
            circle, 
            draw=black, 
            very thick, 
            fill=white, 
            inner sep=0pt,
            minimum size=20pt
        },
        concept/.style={
            rectangle,
            draw=black,
            minimum width=20pt, 
            minimum height=20pt,
            line width=1.5pt
        },
        c_text/.style={
            rectangle, 
            rounded corners, 
            draw=black, 
            fill=pearTwo!30,
            thick, 
            anchor=west
        },
        >=stealth
    ]

    \draw[rounded corners, dashed, very thick] (-5.8, 2) rectangle (-0.2, -2);
    \draw[rounded corners, dashed, very thick] (2.8, 2) rectangle (6.3, -2);

    \node[
        rectangle, 
        rounded corners, 
        draw=black, 
        very thick, 
        fill=orange]
        (pic) at (-4.6, 0) 
        {\includegraphics[scale=1]{./figs/causal_ident/teapot.png}};

    % Input layer (3 neurons)
    \foreach \i in {1,2,3} {
        \node[neuron] (I\i) at (-3,0.6-0.3*\i) {};
        \draw[->] (pic) -- (I\i);
    }

    % Hidden layer (6 neurons)
    \foreach \i in {1,2,3,4,5,6} {
        \node[neuron] (H\i) at (-2.5,1.05-0.3*\i) {};
    }

    % Hidden layer (6 neurons)
    \foreach \i in {1,2,3,4,5,6} {
        \node[neuron] (H_2\i) at (-2,1.05-0.3*\i) {};
    }

    % Output layer (4 neurons)
    \foreach \i in {1,2,3,4} {
        \node[output, fill=blue!50] (O\i) at (-1,2.5 - 1*\i) {$M_\i$};
    }

    \foreach \i in {1,2,3,4} {
        \node[concept, fill=red!50] (C\i) at (3.5,2.5-1*\i) {$C_\i$};
    }

    \node[c_text]
        (concept_1) at (4.1, 1.5) {\small Green};
    \node[c_text]
        (concept_2) at (4.1, 0.5) {\small Rot.\ object};   
    \node[c_text]
        (concept_3) at (4.1,-0.5) {\small Rot.\ spotlight};
    \node[c_text]
        (concept_4) at (4.1,-1.5) {\small Teapot};

    % Connections between input and hidden layer
    \foreach \i in {1,2,3} {
        \foreach \j in {1,2,3,4,5,6} {
            \draw[->, opacity=0.4] (I\i) -- (H\j);
        }
    }

    % Connections between input and hidden layer
    \foreach \i in {1,2,3,4,5,6} {
        \foreach \j in {1,2,3,4,5,6} {
            \draw[->, opacity=0.4] (H\i) -- (H_2\j);
        }
    }

    % Connections between hidden and output layer
    \foreach \i in {1,2,3,4,5,6} {
        \foreach \j in {1,2,3,4} {
            \draw[->, opacity=0.4] (H_2\i) -- (O\j);
        }
    }

    \draw[->, dashed, color=red, very thick] (O1) -- (C3);
    \draw[->, dashed, color=red, very thick] (O2) -- (C4);
    \draw[->, dashed, color=red, very thick] (O3) -- (C2);
    \draw[->, dashed, color=red, very thick] (O4) -- (C1);

    % Layer labels
    \node[
        align=center, 
        rectangle, 
        draw=black!100, 
        very thick,
        fill=pearTwo!30,
        minimum size=10pt
        ] at (1.25,1.5) {\Large \phantom{$\rightarrow\rightarrow$} = $\alpha$};

    \draw[->, dashed, color=red, ultra thick] (0.35, 1.5) -- (1.35, 1.5);
    \end{tikzpicture}
    \caption{An overview of our framework: we aim to provide theoretical
    guarantees for concepts learned from data in terms of correctness and
    required numbers of labelled data by leveraging the disentangled
    representations $M_j$ provided by causal representation learning methods (left
    box) and learning an alignment map $\alpha$ to the concepts $C_i$ (right box).}
\label{fig:concept-pic}
\end{figure}
An advantage of CBMs is that they can provide similar accuracy
in terms of prediction, while also ensuring interpretability by
construction, as opposed to  post-hoc methods
\citep{belinkov2022probing}. On the other hand, CBMs are susceptible to
\emph{concept leakage}
\cite{margeloiu2021conceptbottleneckmodelslearn,mahinpei2021promisespitfallsblackboxconcept},
i.e.\ a learned concept can potentially encode information that is unrelated to
its intended meaning, even with concept-level supervision. This is particularly
problematic if there exist spurious correlations between concepts in the
training dataset. The CBM model might rely on this correlation when learning
two concepts (e.g.\ learning to associate the ``Cow'' concept with mountains,
if we train on a dataset of images from Switzerland), which would then fail in
a different setting where this correlation does not appear. 

Most current mitigating strategies for concept leakage are heuristic
\cite{10.5555/3327757.3327875, orthonormal-concepts}. An exception are
\citet{marconato2022glance}, who leverage disentangled representation learning
to ensure leak-proof CBMs, but assume independent concepts.
\citet{marconato2023humanintepretable} allow for some type of dependence
between concepts, but they require interventions on the data generating
process, for which they require annotations. None of these works provide
guarantees in terms of correctness of the learned concepts or the number of
labels. 

In this paper, we describe a general framework that provides theoretical
guarantees on the correctness of the learned concepts and on the number of
required labels, without requiring any interventions. 
Our framework leverages state-of-the-art CRL methods
\citep{khemakhem2020vaeica, lachapelle2022dms, LippeMLACG22}, to learn
high-level causal variables from low-level unlabelled data, e.g.\
images, with identifiability guarantees. We assume that the ground truth
high-level causal variables correspond to the concepts that we would
like to learn. Typically, CRL methods only provide \emph{identifiability
up to permutation and element-wise transformations}, so we need to learn
an alignment map from the learned embeddings $M_i$ to the concepts
$C_j$, which consists of a permutation and a set of simple functions, as
shown in Figure~\ref{fig:concept-pic}. We leverage techniques from
high-dimensional statistics \citep{buhlmannVG11,bach2008consistency} to
provide theoretical guarantees for this alignment map.

In particular, we propose both a linear and a non-parametric estimator for this
mapping based on a convex optimization problem with a Group Lasso
regularization \citep{yuan2006model}. 
The first method is based on group regularized linear regression, with the
possibility to incorporate a feature map, and comes with a high probability
result on its correctness. This result comes with an explicit dependence on all
the relevant parameters. In particular, the dependence on the number of data
points allows a practitioner to tune the regularization parameter in a
principled way. This allows for good results even with a small number of data
points. The second method is based on a kernelized procedure, allowing for more
flexibility, but comes with asymptotic guarantees instead of finite sample
guarantees. We implement our framework with state-of-the-art CRL methods, and
show its efficacy in learning the correct concepts in synthetic and image
benchmarks from the CRL literature with few labels.
 
 
\section{Related Work}

Extracting high-level concepts from the inner workings of machine learning
models has gained traction over the last years \citep{alain2017understanding,
ghorbani2019towards, mcgrath2021acquisition}. These concepts can be used to
create interpretable explanations. In Concept Based Models (CBMs), the concepts
are hard coded in the structure of the model \citep{koh2020conceptbottleneck,
marconato2022glance, ismail2023concept, zarlenga2022concept}. The explanations
can be constructed by looking at the activations for a prediction of the
components where the concepts are hard coded. The benefit of this approach is
that the concept is faithfully represented in the model and explanation. One of
the caveats is that in the case of concept leakage
\cite{margeloiu2021conceptbottleneckmodelslearn,mahinpei2021promisespitfallsblackboxconcept},
which can happen even with concept-level supervision, the concepts might be
learned incorrectly. 

Most current mitigating strategies for concept leakage are heuristic, e.g.\
encouraging sparsity \cite{10.5555/3327757.3327875} or orthonormality of the
concepts \cite{orthonormal-concepts}. Our work takes inspiration from
\cite{marconato2022glance} who leverage disentangled representation learning to
learn an embedding of ground truth concepts that can be then aligned with the
concept labels to ensure no concept leakage in CBMs. This work assumes that the
concepts are  independent in the dataset, but
\citet{marconato2023humanintepretable} consider a simplified version of causal
representation learning (CRL) methods \cite{scholkopf2021crl}, which allows
dependent concepts. They require interventions on the data generating process
to align the learned embeddings and concepts, for which they require
annotations. In contrast to our frameworl, these works do not provide
guarantees about correctness of the learned concepts or the number of labels. 

We draw heavily on the field of Causal Representation Learning (CRL)
\citep{scholkopf2021crl}. This area assumes that there is an underlying
unobserved causal system with causal variables and for each state of this
system, we can only observe an entangled measurement or observation of the
variables, which we say is produced by a \emph{mixing function}. The task is
then to recover the latent causal variables from the observation by learning an
\emph{unmixing} function. It can be shown that recovering these latent
variables is only possible up to a permutation and transformation, which is
typically element-wise for each variable. Many CRL methods exist with different
assumptions on the available data, e.g.\ the availability of interventional,
counterfactual or temporal data, or parametric assumptions on the underlying
system and mixing function, e.g.\ (non) Gaussianity of the causal variables or
(piecewise) linearity of the mixing function, and different types of
identifiability guarantees, e.g.\
\citep{hyvarinen2019nonlinear,khemakhem2020vaeica,
KugelgenSGBSBL21,lachapelle2022dms, LippeMLACG22, ahujaMWB2023,
lachapelle2024nonparametric, YaoXLMTMKL24} and many others. Our work is
agnostic to which CRL method one is using, and instead focuses on the
downstream task of aligning concepts efficiently to the causal representations
based on general identifiability guarantees.

\section{Framework and Main Definitions}
\label{sec:framework}
% 
\begin{figure}
    \centering
    \begin{tikzpicture}
        \centering
        \node [noise, xshift=\xshift] (G_1) {$G_1$};       
        \node (G_2) {$\ldots$};       
        \node [noise, xshift=-\xshift] (G_d) {$G_d$};       
        \node [feat, yshift=\yshift] (X) {$X$};

        \node [rep, xshift=2.25*\xshift, yshift=2.5*\yshift] (M_1) {$M_1$};
        \node [xshift=1.5*\xshift, yshift=2.5*\yshift] (M_2) {$\ldots$};
        \node [rep, xshift=0.75*\xshift, yshift=2.5*\yshift] (M_d) {$M_d$};

        \node [rep, xshift=-0.75*\xshift, yshift=2.5*\yshift, fill=red!50] (H_1) {$C_1$};
        \node [xshift=1.5*-\xshift, yshift=2.5*\yshift] (H_2) {$\ldots$};
        \node [rep, xshift=-2.25*\xshift, yshift=2.5*\yshift, fill=red!50] (H_d) {$C_d$};

        \draw [-stealth, thick] (G_1) -- (X);
        \draw [-stealth, thick] (G_d) -- (X);

        \draw [-stealth, thick] (X) --
            node [midway, above=0.25em, black] {$g_{\psi}$}
            ($(M_2.north) + (0, 0.5)$);
        \draw [-stealth, thick] (X) -- 
            node [midway, above=0.25em, black] {$g_{\psi'}$}
            ($(H_2.north) + (0, 0.5)$);
     
        \draw [dashed, thick, red, ->] ($(M_2.south) + (0, -0.5)$)
            to[out=330, in=210] 
            node [midway, below=0.25em, black] {$\alpha$}
            ($(H_2.south) + (0, -0.5)$);        
        


        \draw[dashed] ($(M_1.north west) + (-0.3, 0.3)$)
            rectangle ($(M_d.south east) + (0.3, -0.3)$); 
        \draw[dashed] ($(H_1.north west) + (-0.3, 0.3)$)
            rectangle ($(H_d.south east) + (0.3, -0.3)$);

    \end{tikzpicture}
    \vspace{-0.5em}
    \caption{Data generating process, where $G_i$ are the ground truth causal
    variables, $X$ is an observation, $M_i$ represent the representations
    learned through the encoder $g_\psi$, $C_j$ represent the concepts and $\alpha$
    is the alignment map.}
    \vspace{-0.5em}
\label{fig:dgp}
\end{figure}
%
Our setting takes inspiration from causal representation learning (CRL)
\cite{scholkopf2021crl} and the connections between CRL and concept-based
models described by \citet{marconato2022glance,marconato2023humanintepretable}.
As illustrated in Figure~\ref{fig:dgp}, we assume that the underlying causal
system consists of several unobserved random variables: the \textit{causal
variables} $G = (G_1, \ldots, G_d) \in \mathcal{G} \subseteq \mathbb{R}^{d}$,
which can potentially have causal relations between them. The
\textit{observation} is denoted by $X \in \mathcal{X} \subseteq \mathbb{R}^{D}$
and is generated by an unobserved mixing function 
$f\colon \mathcal{G} \to\mathcal{X}$, 
possibly with additive noise $\epsilon$:

\begin{align*}
    X = f(G) + \epsilon, \quad (f \,\text{invertible onto its image})
.\end{align*}
%
The goal of CRL is to identify the causal variables by learning an unmixing function
$g_{\psi} \colon \mathcal{X} \to  \mathbb{R}^{d}$ that approximates
$f^{-1}$, but usually only up to a permutation and element-wise
transformations. We denote the learned causal variables as
$M=(M_1,\ldots, M_d) \in \mathbb{R}^{d}$. For simplicity of exposition,
in the main paper we will assume that the human interpretable concepts
$C=(C_1,\ldots, C_d)^{\top} \in \mathbb{R}^{d}$ that we are interested
in, correspond to the ground truth causal variables $G_1, \dots, G_d$ up
to permutation and element-wise transformations, or in other words
$g_{\psi'}$ shown in Figure~\ref{fig:dgp} also identifies ground truth
causal variables. In Appendix~\ref{app:proofs_params} and
Section~\ref{sec:non_param_learning} this is extended to allow each of the
concepts to be a transformation of a group of causal variables. Our goal
is to learn the alignment map $\alpha$ that transforms the learned
representations $M$ to the concepts $C$ efficiently and accurately.

In order to formalize our setting, we will need to introduce some concepts from CRL.
A parametric model class 
$\mathcal{P} = \{P_\theta \colon \theta \in \Theta\} $ is called 
\emph{identifiable} if the map from the parameter $\theta$ to the model $P_\theta$ is injective.
In CRL, identifiability of the unmixing function
is often too much to ask for, as there are often many
unmixing functions that result in the same observational distribution.
What is possible, and can still be useful in
practical settings, is \textit{identifiability up to an equivalence class}. 
Let $\sim$ denote an equivalence relation on the space $\Theta$. Then, 
$\theta$ is \textit{identifiable up to}  $\sim$ if
%
\begin{align*}
    P_\theta(X) = P_{\theta'}(X) \implies \theta \sim \theta'
.\end{align*}
%
The equivalence class $\Theta /\mathord{\sim}$ is also called an
\textit{identifiability class}. Intuitively this identifiability class
describes up to which transformations we can recover the ground truth variables
under the appropriate assumptions. 
%
\begin{defn}
    \label{def:lin_perm_class}
    Two models $(f, P)$ and  $(g, Q)$ are equivalent according to
    $\sim$, if $P = Q$ and there exists an invertible matrix $A \in
    \mathbb{R}^{d \times d}$ and a element-wise transformation 
    $T(z) = (T_1(z_1), \ldots, T_d(z_d))^{\top}$ such that
    %
    \begin{align*}
        f^{-1}(x) = A T(g^{-1}(x)) \text{ for all }
        x \in \mathcal{X}\subseteq \mathbb{R}^{D}
    .\end{align*}
    %
    If $A$ is a (block)-permutation matrix then we denote the relation
    by $\Psim$ and $A$ is often replaced by $P$. 
    %
\end{defn}
A common version of Definition~\ref{def:lin_perm_class} is that the model is
identifiable up to a scaling and permutation, which is described by $T$ being a
diagonal scalar matrix \citep{identifiability2023ica}. In other settings, the
transformation is often a diffeomorphism and in some cases the causal variables
can be either multidimensional \cite{LippeMLACG22} or they can only be
identified up to a \emph{block}, i.e.\ a group of causal variables
\citep{ahujaMWB2023, KugelgenSGBSBL21, YaoXLMTMKL24,
lachapelle2024nonparametric}.
For our theoretical analysis, we assume that we are given an unmixing function
$g_{\psi}$ from a CRL method that perfectly identifies the causal variables up
to Definition~\ref{def:lin_perm_class} and that the human-interpretable concepts are
also a result of another unknown unmixing function $g_{\psi}'$.
%
\begin{asump}\label{asump:central}
    We are given a model $g_{\psi}\colon \mathcal{X} \to \mathbb{R}^{d}$, that
    is a diffeomorphism onto its image, such that $g_{\psi} \Psim f^{-1}$,
    where $f$ is the true mixing function. The unknown function $g_\psi'$ that
    recovers the concepts $C$ is also $g_{\psi'} \Psim f^{-1}$.
\end{asump}
%
This assumption means that both the representations learned by a CRL method $M$
and the human interpretable concepts $C$ identify the ground truth causal
variables $G$ up to $\Psim$, and the relation between $M$ and $C$ is also up to
the same equivalence. More formally, let $\pi\colon \{1,\ldots, d\} \to \{1,
\ldots, d\} $ be a permutation of the variable indices. Let $P \in
\mathbb{R}^{d \times d}$ be the permutation matrix associated with $\pi$,
meaning that $P_{ip(i)} = 1$ and $0$ otherwise, and $T$ a map as in
Definition~\ref{def:lin_perm_class}, then  $M$ and $C$ are related by
%
\begin{align}\label{eq:human_machine_relation}
    PT(M) 
    &=
    \begin{bmatrix} 
        T_{\pi(1)}(M_{\pi(1)}) \\ 
        T_{\pi(2)}(M_{\pi(2)}) \\ 
        \vdots \\ 
        T_{\pi(d)}(M_{\pi(d)})\end{bmatrix} 
    =
    \begin{bmatrix} 
        C_1 \\ 
        C_2 \\ 
        \vdots \\ 
        C_d\end{bmatrix} 
    =
    C
.\end{align}
%
Finding the $\alpha$ in Figure~\ref{fig:dgp} reduces to learning the permutation
$\pi$ and a separate regression per concept $C_i$ to learn the transformation
from machine variable $M_{\pi(i)}$ to $C_i$. If we had access to the
permutation, this would be a standard regression problem. What is less well
studied is identifying $\pi$ from observational data, i.e.\ without performing
interventions. In the following we introduce two estimators for this setting,
one assuming the element-wise transformation is linear, e.g.\ as is the case in
some CRL methods like \cite{hyvarinen2019nonlinear, khemakhem2020vaeica}, for
which we will be able to provide finite sample results based on a tunable
parameters, and a second, non-parametric method based on kernel methods that
allows for arbitrary invertible element-wise transformations, and can hence be
applied to most CRL methods. For the second approach we will only be able to
provide asymptotic results, which tell the practitioner how the regularization
parameter has to scale given the number of data points. We show in the
experiments that both methods work well in the low data regime and when the
data is not even fully disentangled, so violating Assumption\ref{asump:central}. 

\section{Linear Regression Permutation Learning with the Group Lasso}
\label{sec:param_learning}

%
\begin{algorithm}[t]
    \caption{Estimating the permutation 
        using linear regression with Group Lasso regularization}
    \label{alg:param}
    \begin{algorithmic}[1]
        \STATE Input: regularization parameter $\lambda > 0$
        \STATE Data: $\{(C^{(\ell)}, M^{(\ell)})\}_{\ell=1}^{n}$
        \FOR{$i=1, \ldots, d$}
            \STATE 
            $\displaystyle\hbeta_i \gets \argmin_{\beta \in \mathbb{R}^{dp}}
            \|\Hvec_{i} - \Phi \beta\|^2 + \lambda \sqrt{p} \|\beta\|_{2,1}$
        \ENDFOR
        \STATE $\displaystyle\hpi \gets \argmax_{\pi \in \Pi} 
            \sum_{i=1}^{d}\|\hbeta^{\pi(i)}_{i}\|$
    \end{algorithmic} 
\end{algorithm}

%
In this section, we describe a linear regression approach based on the
Group Lasso to learn the permutation $\pi$ and transformation $T$
in~\eqref{eq:human_machine_relation}. The approach is summarized in
Algorithm~\ref{alg:param}. We will prove that this method simultaneously
provides accurate regression estimates for $T$ and identifies $\pi$
correctly with high probability. To simplify the exposition, we focus
here on the case of scalar variables. Proofs are in
Appendix~\ref{app:proofs_params}, which also contains discussion of the
assumptions, and a generalization to block variables. The proof combines
techniques from high-dimensional statistics
\citep{buhlmannVG11,lounici2011oracle}.

\subsection{Method}

Linear regression can describe non-linear relations by transforming
covariates using a feature map $\varphi : \reals \to \reals^p$. In this
section, we assume that $T_i$ can be expressed as a linear function
of $\varphi(M_{\pi(i)})$. The choice of $\varphi$ therefore gives
precise control to trade off interpretability with expressive power for
$T_i$. For instance, in the simplest and most easily interpretable case,
$\varphi$ can be the identity function, so that $p=1$ and $C_i$ and
$M_{\pi(i)}$ are related by scaling. In more challenging settings,
richer functional relations may be needed, e.g.\ splines or random Fourier features.
We apply the same feature map to all machine variables in $M$, for
which we write $\varphi(M) = [\varphi(M_1)^{\top}, \ldots,
\varphi(M_d)^{\top}]^{\top}$. Then each $C_i$ is modeled as a linear
function of the transformed variables:
%
\begin{align}\label{eq:single_model}
    C_i = \varphi(M)\bstar_i 
    + \epsilon_i, 
\end{align}
%
where $\bstar_i \in \mathbb{R}^{pd}$ is an unknown parameter vector, and
$\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ is Gaussian noise. By assumption,
$C_i$ only depends on $M_{\pi(i)}$ and not on any of the other variables, so
$\bstar_i$ is sparse: only the coefficients for $\varphi(M_{\pi(i)})$ are
non-zero. To express this formally, let $G_j = \{(j-1)p, \ldots, jp\} $ be the
indices that belong to variable $M_j$ and, for any $\beta \in \reals^{pd}$,
define $\beta^{j}= (\beta_k \mid  k \in G_j)$ to be the corresponding
coefficients. Then $(\bstar_i)^j$ is non-zero only for $j = \pi(i)$.

We assume we are given a data set $\mathcal{D}=
\{(C^{(\ell)},M^{(\ell)})\}_{\ell=1}^{n}$ that contains $n$ independent
samples of corresponding pairs $C^{(\ell)} =
(C_1^{(\ell)},\ldots,C_d^{(\ell)})$ and $M^{(\ell)} = (M_1^{(\ell)},
\ldots, M_d^{(\ell)})$. We stack the $C^{(\ell)}$ into a matrix
$\Hvec\in\mathbb{R}^{n\times d}$ and the feature vectors
$\varphi(M^{(\ell)})$ into $\Phi \in
\mathbb{R}^{n \times pd}$. This leads to the relation
%
\begin{align*}
    \Hvec_{i} 
    &= \Phi \bstar_i + \bm{\epsilon}_i, 
\end{align*}
%
where $\Hvec_i$ is the $i$-th column of $\Hvec$ and the noise vector
$\bm{\epsilon}_i$ consists of $n$ independently drawn $\cN(0, \sigma^2)$
variables.
To estimate $\bstar_i$, we use the Group Lasso with parameter $\lambda > 0$:
%
\begin{equation}\label{eq:group_est}
    \hbeta_i
        = \argmin_{\beta \in \mathbb{R}^{dp}} \tfrac{1}{n}\|\Hvec_i - \Phi\beta \|^2 + 
        \lambda  \sqrt{p} \|\beta\|_{2, 1}.
\end{equation}
%
The $(2, 1)$-mix norm $\|\beta\|_{2, 1}$ in \eqref{eq:group_obj}
encourages group-wise sparsity. It
applies the Euclidean norm $\|\beta^{j}\|$ to each group~$j$ separately,
and sums the results over groups, as defined below. We also define the $(2, \infty)$-mix norm:
%
\begin{align}
    \label{eq:group_obj}
    \|\beta\|_{2, 1} 
    &=  \sum_{j=1}^{d} \|\beta^{j}\|,
    & & 
    \|\beta\|_{2, \infty} = \max_{j=1, \ldots, d}\|\beta^j\|
.\end{align}

\subsection{Theoretical Analysis}

We denote the full covariance matrix by $\hS =
\tfrac{1}{n}\Phi^{\top}\Phi$. For the group of $p$ columns of $\Phi$
that correspond to $\varphi(M_j)$ we write $\Phi_j = \Phi_{G_j}$. Also
let $\hS_{jj'} = \tfrac{1}{n}\Phi_j^{\top} \Phi_j'$ denote the
covariance matrix between groups $j$ and $j'$, and abbreviate $\hS_j =
\hS_{jj}$. Then, w.l.o.g., we can assume that the data within each group
have been centered and decorrelated: 
%
\begin{align}\label{eqn:standardized}
     \tfrac{1}{n}\mathbbm{1}^{\top}\Phi_j  &= 0
    \quad \text{and} \quad
    &
    \hS_j = I 
    & & 
    \text{for all } j=1,\ldots, d
.\end{align}
%
This can be achieved by pre-processing: subtract the empirical mean of
$\Phi_j$ and multiply it from the right by the inverse square root of
the empirical covariance matrix. Preprocessing is allowed in our
theoretical results, because they apply to the fixed design setting,
so probabilities refer to the randomness in $\Hvec$
conditional on already having observed $\Phi$.
If $\varphi(M_j)$ and $\varphi(M_{j'})$ are completely correlated, then
$\bstar_i$ is not uniquely identifiable, no matter how much data we
have. To rule out this possibility, we make the following assumption,
which limits the amount of correlation:
%
\begin{asump}\label{asump:structure}
    There exists $a > 1$ s.t.
    for all $j\neq j'$,
    \begin{align*}
        \max_{t\in \{1,\ldots,p\}}|(\hS_{jj'})_{t t}| 
            \le \frac{1}{14a} ,
        \max_{t, t' \in \{1,\ldots,p\}}|(\hS_{jj'})_{t t'}| 
            \le \frac{1}{14a p } 
    .\end{align*}
    %
\end{asump}
\begin{restatable}{theorem}{GroupStruct}\label{thm:group_structure}
    Suppose the data have been pre-processed to satisfy
    \eqref{eqn:standardized} and 
    let Assumption~\eqref{asump:structure} hold. Take
    $\lambda \ge 4\lambda_0$, where
    \begin{align*}
        \lambda_0 = \frac{2\sigma}{\sqrt{n} }
        \sqrt{1 + \sqrt{\frac{8\log(d/\delta))}{p}} +
        \frac{8\log(d/\delta) }{p}},
    \end{align*}
    %
    and set $c = \left( 1 + \tfrac{24}{7(a - 1)} \right) $. 
    Then, for any $\delta \in (0, 1)$, any solution 
    $\hbeta_i$ of the Group Lasso objective \eqref{eq:group_est}
    satisfies
    %
    \begin{align}
        \|\hbeta_i - \bstar_i\|_{2, \infty} \le c\lambda \sqrt{p} 
        \label{eq:param_max_bound_main}
    \end{align}
    %
    with probability at least $1 - \frac{\delta}{d}$.

    If, in addition, $\|(\bstar_i)^{\pi(i)}\| > 2c\lambda\sqrt{p} $, then
    \eqref{eq:param_max_bound_main}
    implies that
    %
    \begin{align*}
        \hJ_i = \argmax_{j=1, \ldots, d} \|\hbeta_i^{j}\| 
    \end{align*}
    %
    estimates $\pi(i)$ correctly.
\end{restatable}
Theorem~\ref{thm:group_structure} gives us an explicit relation between
the parameters $n, p,d,\delta$ of the learning task, the tuning of the
hyperparameter $\lambda$, and the estimation errors for $\bstar_i$ and
$\pi(i)$. For example, if we set $\delta = \tfrac{1}{n}$, $\lambda =
4 \lambda_0$ and let $n\to
\infty$, then $\lambda \to 0$ and $\hJ_i$ estimates the correct
index $\pi(i)$ with probability tending to $1$. So, regardless of the true
parameter magnitude $\|(\bstar_i)^{\pi(i)}\|$, the estimator is
consistent given a sufficient amount of data. Another way to express
this is to ask about \emph{sample complexity}: which sample size $n$ do
we need to reach accuracy $E > 0$? Setting $\lambda = 4 \lambda_0$ and
solving for $n$ large enough that $c\lambda \sqrt{p} \leq E$, we see
that
%
\[
  n \geq \frac{64 c^2\sigma^2
        \big(p + \sqrt{8p\log(d/\delta))} +
      8\log(d/\delta)\big)}{E^2}
\]
%
is sufficient. For estimating the permutation $\pi(i)$ correctly, 
the required accuracy is $E \leq \|(\bstar_i)^{\pi(i)}\|/2$, so the
larger the true parameters, the easier this task becomes.

The estimation is performed separately for each concept $C_i$, and, if
$\hJ_i$ is correct for all $i$ simultaneously, we can construct a
valid estimate of the permutation by $\tpi(i) = \hJ_i$. However, this estimate is
not robust to estimation errors and may even produce functions
$\tpi$ that are not permutations if some $\hJ_i$ are incorrect. The
actual estimator of the permutation, $\hpi$, therefore optimizes a weighted
matching problem, which leads to the same estimate as $\tpi$ if the $\hJ_i$
together produce a valid permutation, but forces $\hpi$ to be a valid
permutation even if they do not:
%
\begin{align}\label{eq:perm-match}
    \hpi = \argmax_{\pi \in \Pi} \sum_{i=1}^d 
    \|\hbeta_i^{\pi(i)}\|
.\end{align}
%
Here, $\Pi$ is the set of all permutations. This assignment can be
solved without cycling through all permutations, with has cubic runtime
in the dimension $d$. By a union bound over $i$, it follows from
Theorem~\ref{thm:group_structure} that $\hpi$ estimates the true
permutation $\pi$ with high probability:
%
\begin{restatable}{cor}{TotalProb}\label{thm:total_prob}
    Assume the same setting as Theorem~\ref{thm:group_structure}
    such that for each $i=1,\ldots, d, \|(\bstar_i)^{\pi(i)}\| >2c\lambda \sqrt{p}$ 
    and consider the estimator $\hpi$ as defined in \eqref{eq:perm-match}.
    Then $\hpi = \pi$ with probability 
    at least $1 - \delta$.
\end{restatable}
%
\begin{figure}[!t]
    \def\trimLeft{1cm}
    \def\trimRight{0.5cm}
    \def\scaleValue{3cm}

    \def\incFigure#1#2#3{%
        \node{\scalebox{0.8}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
    ]{./figs/spline/#3.pdf}}};
        }
    \centerfloat
    \begin{tikzpicture}
%    \node[above=-0.5cm of plotmatrix.north]
        \node (legend) {\includegraphics[scale=1]{./figs/spline/legend.pdf}};
 
        \matrix[below=-0.5cm of legend.south,
            row sep=-0.3cm, column sep=0.5cm] (plotmatrix) {
            \incFigure{0.25cm}{0cm}{figure_0}&[-0.97cm]
            \incFigure{1.38cm}{0cm}{figure_1}&

            \incFigure{0.65cm}{0cm}{figure_2}&[-0.98cm]
            \incFigure{1.38cm}{0cm}{figure_3}&\\

            \incFigure{0.25cm}{0cm}{figure_4}&
            \incFigure{1.38cm}{0cm}{figure_5}&

            \incFigure{0.65cm}{0cm}{figure_6}&
            \incFigure{1.38cm}{0cm}{figure_7}&\\
        };

            \end{tikzpicture}
    \vspace{-1em}
    \caption{Permutation error rate for spline features.
        From top left to bottom right we vary: (i) the
        regularization parameter, (ii) the number of dimensions, (iii) the correlation of
        the variables and (iv) the number of labels. The first plot of each
        pair shows the wellspecified and the second the
        misspecified case. We average over $10$ seeds and shade the
    25-75th percentile.}
    \label{fig:synth_data}
\end{figure}
%
\section{Kernelized Permutation Learning}
\label{sec:non_param_learning}
\begin{algorithm}[t]
    \caption{Estimating the permutation 
        using kernels}
    \label{alg:non_param}
    \begin{algorithmic}[1]
        \STATE Input: reg.\ parameter $\lambda > 0$, kernels
        $\kappa_1,\ldots,\kappa_d$
        \STATE Data: $\{(C^{(\ell)}, M^{(\ell)})\}_{\ell=1}^{n}$
        \FOR{$j=1,\ldots,d$}
            \STATE $(K_j)_{\ell k} \gets \kappa_j(M^{(\ell)}, M^{(k)})$
            \STATE $L_j \gets \text{CholeskyDecomposition}(K_j)$
        \ENDFOR
        \FOR{$i=1, \ldots d$}
            \STATE 
            $\displaystyle\widehat{\gamma}_i \gets 
            \argmin_{
                \gamma \in \reals^{np}
            }
            \tfrac{1}{n}  
            \|\Hvec_i -  \sum_{j=1}^{d} L_j \gamma^{j}\|^2
            + 
            \lambda \|\gamma\|_{2, 1}$
        \ENDFOR
        \STATE $\displaystyle\hpi \gets \argmax_{\pi \in \Pi} 
        \sum_{i=1}^{d}
            \|\widehat{\gamma}^{\pi(i)}_{i}\|$
    \end{algorithmic}
\end{algorithm}

The previous section describes how to learn functions with finite-dimensional
representations. We now extend the estimator to use general functions from a
\emph{Reproducing Kernel Hilbert Space} (RKHS)
\citep{HofmannScholkopfSmola2008}. This may be interpreted as a (typically
infinite-dimensional) feature map $\varphi$ that maps to the RKHS. However,
using a representer theorem, all operations can be performed on
finite-dimensional representations. We summarize the method in
Algorithm~\ref{alg:non_param}. 

\subsection{Method}

Define again $M = (M_1, \ldots, M_d)$, where we now allow each machine
variable $M_j$ to take values in an abstract space $\mathcal{Z}_j$. Let
$\mathcal{Z} = \mathcal{Z}_1 \times \ldots \mathcal{Z}_d$. We then
generalize \eqref{eq:single_model} to
%
\[
    C_i = \bstar_i(M) + \epsilon_i, 
\]
%
where $\bstar_i \in \mathcal{H}$ is a function from $\mathcal{Z}$ to
$\reals$, and $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$. We take the space of
possible functions $\mathcal{H}$ to be an RKHS containing functions of the
form $\beta(M) = \sum_{j=1}^d \beta^j(M_j)$, where each $\beta^j$ is an
element of an RKHS $\mathcal{H}_j$ that captures the effect of variable
$M_j$ on $C_i$. The assumption that $C_i$
depends only on $M_{\pi(i)}$ means that $\bstar_i(M) =
(\bstar_i)^{\pi(i)}(M_{\pi(i)})$. Each $\mathcal{H}_j$ can be freely
chosen, and is typically specified indirectly by the choice of a positive definite \emph{kernel} $\kappa_j : \mathcal{Z}_j
\times \mathcal{Z}_j \to \reals$ \citep{HofmannScholkopfSmola2008}. This
kernel defines a measure of similarity between inputs:
$\kappa_j(M_j,M_j') = \left \langle \varphi_j(M_j), \varphi_j(M_j')
\right\rangle_{\mathcal{H}_j}$, where $\varphi_j : \mathcal{Z}_j \to
\mathcal{H}_j$ is the corresponding feature map. See
p.\,\pageref{p:kernel_examples} for examples.

Given data $\mathcal{D}= \{(C^{(\ell)},M^{(\ell)})\}_{\ell=1}^{n}$, let
$\Mvec \in \mathcal{Z}^n$ denote the matrix with the machine variables
$(M^{(\ell)})^\top$ stacked as rows. If we further define $\beta(\Mvec)
= [\beta(M^{(1)}), \ldots, \beta(M^{(\ell)})]^{ \top}$, then the Group
Lasso objective \eqref{eq:group_est} generalizes to
%
\begin{equation}\label{eq:feature_optim}
  \hbeta_i = \argmin_{\beta \in \mathcal{H}} 
        \tfrac{1}{n}
        \|\Hvec_i - \beta( \Mvec ) \|^2
        + 
        \lambda \sum_{j=1}^{d} \|\beta^{j}\|_{\mathcal{H}_j}
,\end{equation}
%
where $\|\beta^{j}\|_{\mathcal{H}_j}$ is the norm associated with $\mathcal{H}_j$. 

To optimize the objective in \eqref{eq:feature_optim}, we 
need a finite dimensional objective to give to a Group Lasso solver. We
provide a version of the Representer Theorem showing that
the solution of $\eqref{eq:feature_optim}$ lives in a subspace of
$\mathcal{H}$ that can be described by finite-dimensional parameters
$\hat c_i^{1}, \ldots, \hat c_i^{j} \in \mathbb{R}^{n}$:
%

\begin{restatable}{theorem}{FeatKernEquiv}
    \label{thm:feature_kernel_equivalence}   Let $\varphi_1, \ldots,
    \varphi_d$ be the feature maps associated with $\mathcal{H}_1,
    \ldots, \mathcal{H}_d$. Then there exist $\hat c_i^{1}, \ldots, \hat
    c_i^{d} \in \mathbb{R}^{n}$ such that the optimization problem in
    \eqref{eq:feature_optim} has solution $\hbeta_i$ with each
    $\hbeta_i^j$ of the form
    \begin{align*}%\label{eqn:representer}
        \hbeta_i^{j} = \sum_{\ell=1}^{n} 
            \varphi_j(M_j^{(\ell)})(\hat{c}_{i}^{j})_\ell
    .\end{align*}
\end{restatable}
%
Substitution of this form into \eqref{eq:feature_optim} gives that $\hat
c_i^{1}, \ldots, \hat c_i^{d}$ will be the minimizers of the following
finite-dimensional optimization problem:
%
\begin{align}\label{eq:kernel_optim_unsquared}
      \min_{c^{1}, \ldots, c^{d} \in \mathbb{R}^{n}}
    \tfrac{1}{n}\|\Hvec_i- \sum_{j=1}^{d} K_jc^{j}\|^2
    + 
    \lambda\sum_{j=1}^{d} \|c^{j}\|_{K_j}
,\end{align}
where $K_j \in \mathbb{R}^{n \times n}$ denotes the Gramm matrix 
$K_j$, with $(K_j)_{\ell k} 
= \kappa_j(M_j^{(\ell)}, M_j^{(k)})$, and
$\|c^{j}\|_{K_j} = \sqrt{{c^j}^{\top}K_jc^{j}} $.

This procedure is then performed for  $i=1,\ldots d$.
The permutation is estimated similarly to the linear case:
\begin{align*}
    \hpi
    =\argmax_{\pi \in \Pi}\sum_{i=1}^{d} 
    \|\hat{c}^{\pi(i)}_i\|_{K_{\pi(i)}}
.\end{align*}

\subsection{Theoretical Analysis}
%
Using a result by \citet{bach2008consistency}, we can prove that our estimator
for $\pi$ is consistent under suitable conditions, which are discussed
in Appendix~\ref{app:proofs_non_params}. This result holds for random
design, so for the joint randomness of $\mathcal{D}$.
%
\begin{restatable}{theorem}{KernelProb}\label{thm:kernel-prob}
    Assume ($A$--$D$) in Appendix~\ref{app:kernel_consistency}. Then, for any 
    sequence of regularization parameters $\lambda_n$ such that $\lambda_n \to 0$
    and $\sqrt{n} \lambda_n \to +\infty$ when $n \to \infty$,
    the estimated permutation $\hpi$ converges in probability to $\pi$. 
\end{restatable}
%

\subsection{Implementation}

To use a standard Group Lasso solver we need to reparametrize the
optimization problem in \eqref{eq:kernel_optim_unsquared}, because of
the scaled norms $\|\cdot\|_{K_j}$. We can do this with a Cholesky
decomposition:
\begin{restatable}{lemma}{KernelImp}\label{lem:kernel-imp}
    For each $j=1,\ldots, d$ let $L_j$ be the Cholesky 
    decomposition of the Gramm matrix $K_j$ and let $\hat \gamma_i^1,
    \ldots, \hat \gamma_i^d$ be minimizers of
    %
    \begin{align}\label{eq:kernel_optim_lstsq}
          \min_{\gamma^{1}, \ldots, \gamma^{d}\in \mathbb{R}^{n}}
            \tfrac{1}{n}  
            \|\Hvec_i - \sum_{j=1}^{d} L_j\gamma^{j}\|^2 
            + 
            \lambda \|\gamma^{j}\|_{2, 1}
    .\end{align}
    %
    Then $\hat c_i^{j}= (L_j^{\top})^{-1}\hat \gamma_i^{j}$.
\end{restatable}
%

\section{Experiments}
\label{sec:experiments}
We perform three sets of experiments to evaluate our estimators. First, we
focus on a completely synthetic setting, called ``Toy dataset". Here the
concepts are generated using either a linear combination of features (which we
call the wellspecified case) or diffeomorphisms (which we call the misspecified
case) of the representations. The concepts are then permuted. We then evaluate
the alignment of the representations learned by several state-of-the-art CRL
methods to the ground truth causal variables. We look at two synthetic datasets
from \citet{lachapelle2022dms} and  compare the alignment of representations
learned by DMS-VAE \citep{lachapelle2022dms}, iVAE \citep{khemakhem2020vaeica}
and TCVAE \citep{chen2018isolating}. Finally, we evaluate on an image
benchmark, Temporal Causal3Dident \citep{LippeMLACG22}, comparing the alignment
of representations learned by iVAE and CITRIS-VAE \citep{LippeMLACG22}. All
experimental details are in Appendix~\ref{app:experiments} and our code is 
publicly available\footnote{\url{https://github.com/HiddeFok/sample-efficient-learning-of-concepts}}

%
\begin{table}[ht!]
    \caption{Permutation Errors and $R^2$-scores. The permutation error is
        bounded between $0$ and  $1$, where  $0$ indicates a perfect score. For
        the $R^2$-score, on the diagonal refers to the average score between
        the learned causal variable and the ground truth causal variable. The
        optimal score is $1$. We average over $50$ seeds and we write the best
        method in \textbf{bold}.}
    \label{tbl:perm-error-r2}
    \centerfloat
    \setlength\tabcolsep{2pt}
    \notsotiny
    \begin{tabular}{m{1.5cm}m{1.2cm}|ccccc@{\hskip 1em}|@{\hskip 1em}ccccc}
        \toprule
        & &  
        \multicolumn{5}{c}{{\small Permutation Error $\downarrow$ $(n)$}} & 
        \multicolumn{5}{c}{{\small $R^2$-score on the diagonal $\uparrow$ $(n)$}}\\
        \hline
        \input{./tables/sample_table.tex}
    \end{tabular}
    \vspace{-1.2em}
\end{table}

\paragraph{Performance Metrics}
To asses our estimator we report 
the mean error in the learned permutation of the variables, 
$\text{MPE} = \frac{1}{d}\sum_{i=1}^{d} \ind\{\tpi(i) \neq \pi(i)\}$,
the average $R^2$-score of the prediction
in each dimension, and the runtime. 


\paragraph{Toy Dataset}
The synthetic data experiments consist of $4$ settings, each using a different
set of features to perform the regression. The settings are linear, splines,
random Fourier features and kernels. The $M$ variables are distributed
according to $\cN(0, (1 - \rho)I_{d\times d} + \rho \ind)$, where $\ind$
denotes a matrix filled with only $1$'s. The $\rho \in (0, 1)$ parameter
controls the amount of correlation between the marginal variables. We sample
$n$ data points, on which we perform a $80/20$ train/test data split. In the
wellspecified case we generate the $M$ variables by using the features from
that setting. For each dimension $j=1,\ldots, d$, we draw a random weight
vector $\bstar_j \in \mathbb{R}^{p}$, such that $\|\bstar_j\| \in [16\lambda_0,
32\lambda_0]$ uniformly. A permutation $\pi\colon \{1, \ldots, d\} \to \{1,
\ldots, d\}$ is sampled uniformly from all possible permutations. Finally, with
independent $\epsilon_i \sim N(0, \sigma^2)$ noise variables we get 
%
\begin{align*}
    C_{i} = \varphi(M_{\pi(i)})^{\top}\bstar_{\pi(i)} +  \epsilon_i
.\end{align*}

The misspecified case is inspired by the most general identifiability classes.
In this setting the $M$ variables are still sampled the same as in the
wellspecified case, but the $C$ variables are generated by sampling a
diffeomorphism for each dimension. These outcomes are then permuted using a
random permutation again. In the experiments we cover a large range of the
possible values of $\rho$,$d$, $n$ and  $\lambda$ as shown in
Appendix~\ref{app:synth_experiments}. Each experiment is repeated $10$ times to
estimate confidence bounds around the mean metric. As baseline, we compare to
permutations that one can learn with Pearson or Spearman correlation. Using
these correlations, we can construct a weight matrix like in our estimator and
use the linear sum assignment approach to estimate the permutation. This is a
fast approach, but one still needs to perform a regression afterwards, while
our estimator does  the permutation and regression jointly, and provides
correctness guarantees.

We show a set of representative results in Figure~\ref{fig:synth_data}. Our
estimator is able to reconstruct the correct permutation perfectly with only a
small number of features. It performs well for a broad range of regularization
parameters. In the wellspecified case we calculate the $\lambda_0$ parameter
and see that the estimator performs well around this value. The dimension
dependence is almost negligible, which was predicted by the dimension appearing
only in the log factor in Theorem~\ref{thm:group_structure}. The estimator works
well with relatively few data points. Finally, the estimator works even better
than theory predicts, as it has a low error even when the correlation between
the dimensions is quite high. More figures using random Fourier features and
the kernelized approach are in Appendix~\ref{app:synth_experiments}.

\paragraph{Action/Temporal Sparsity Datasets.} We use the two synthetic
datasets from \citep{lachapelle2022dms} that represent the action and temporal
sparsity settings in a time series. These settings have 10 causal
variables $z_1, \ldots, z_{10} \in [-2, 2]$ with a causal structure. The mixing function is 
an invertible neural network with Gaussian random 
weights, after which the columns in the linear layers are orthogonalized 
to ensure injectivity. 
To recover the ground truth permutation, we
follow \citet{lachapelle2022dms} and use the test set to
calculate a permutation based on  Pearson correlations.


\paragraph{Temporal Causal3DIdent.} We evaluate our methods on an image
benchmark, TemporalCausal3DIdent \citep{LippeMLACG22}. The dataset consists of
images of 3D objects, rendered in different positions, rotation and lighting.
The causal variables are  the position $(x, y, z) \in [-2, 2]^2$, the object
rotation with two dimensions $[\alpha, \beta] \in [0, 2\pi)^2$, the hue, the
background and spotlight also in $[0,2\pi)$. The object shape is a categorical
variable. We use a pretrained CITRIS-VAE encoder, which outputs a $32$
dimensional latent space and a grouping of which dimensions relate to which
causal variables. Although CITRIS-VAE provides the correct permutation of the
groups, we ignore it and perform a random permutation on the variables. Similar
to \citet{LippeMLACG22}, we train an MLP for each of the $32$ dimensions that
predicts all causal variables. Based on the $R^2$ scores of these regressions,
we learn the group assignments. Some CRL methods provide identifiability up to
blocks of variables, but do not specify how these blocks correspond to the
ground truth. Our estimator offers an efficient solution in these cases.

\begin{figure}[t]
    \def\incFigure#1{%
        \node{\includegraphics[
            trim={0.22cm 0.25cm 0 0},
            clip
        ]{./tables/#1/times_no_two_stage.pdf}};
        }
    \centering
    \begin{tikzpicture}
        \matrix[row sep=-0.5cm, column sep=-0.3cm] (plotmatrix) {
            \incFigure{dms-vae}&\\
        };

        \node[above right=-0.55cm and 0.75cm of plotmatrix.north, anchor=south]
            {\includegraphics[%
                scale=0.75, 
                trim={0.2cm 3.6cm 0 0},
                clip
            ]{./tables/dms-vae/legend_no_two_stage.pdf}};
    \end{tikzpicture}
    \vspace{-1em}
    \caption{Execution times of the baseline and multiple versions of our
    estimator of learned causal representations and concepts based on the
    Action/Temporal Sparsity Dataset. }
    \label{fig:times}
\end{figure}


\paragraph{Results.}
A selection of results for the action sparsity dataset and temporal
Causal3DIdent dataset are reported in Table~\ref{tbl:perm-error-r2}, while the
complete set of experiments is reported in
Appendix~\ref{app:at_experimets}--\ref{app:temp_causal_experimets}. We report two
baselines, NN and Spearman. The method NN refers to using a trained MLP to
predict all causal variables with each encoding. The $R^2$-scores are then used
to determine the matching. We also added the kernelized version of our
estimator with the Laplacian kernel. Our estimator consistently ranks amongst
the lowest MPE scores, and it still works empirically even if there is a
dependence between the ground truth variables that is potentially larger than
Assumption~\ref{asump:structure}. 
The Spearman correlation performs well with regards to the MPE, but a separate
regression still needs to be performed, while our estimator does everything in
one procedure. In terms of $R^2$-score we slightly underperform against the
neural network approach, but this approach requires $100$x more computing
time (see Figure~\ref{fig:times}). Our estimator is able to handle blocks or
vectors of inputs in the results of the temporal Causal3DIdent dataset, as
predicted by Theorem~\ref{thm:group_structure} in Appendix~\ref{app:proofs_params}.


\section{Conclusion}
\label{sec:conclusion}

We propose a framework that provides theoretical guarantees on learning of
concepts in deep learning models by leveraging causal representation learning
(CRL) and techniques from high-dimensional statistics. We provide two
estimators that are able to recover the permutation and mapping between the
learned representations and the true concepts: a linear estimator with finite
sample guarantees and a non-parametric kernelized estimator with asymptotic
guarantees. We test our methods on CRL benchmarks and show they perform even
better than the theory predicted. Our work can be incorporated into current
concept-based models, ensuring that the concepts are learned faithfully even
with few labels. For future work, it would be interesting to incorporate ideas
from the \textit{causal abstraction} literature
\citep{rubenstein2017abstraction, geiger2021causal, beckers2020approximate} and
learn hierarchies of concepts. 

\acks{We thank SURFsara for the support in using the Snellius Compute
Cluster. T. Van Erven was supported by the Netherlands Organization for
Scientific Research (NWO) under grant number VI.Vidi.192.095.}

\clearpage
\section*{Impact Statement} This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

\IfFileExists{certifiedxai.bib}{
    \bibliography{certifiedxai}
}{
    \bibliography{../certifiedxai}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proofs Linear Regression Approach}
\label{app:proofs_params}
As discussed in Section~\ref{sec:param_learning}, we will extend
Theorem~\ref{thm:group_structure} to a more general result
Theorem~\ref{thm:group_structure_general} that allows for blocks of causal
variables corresponding to a single concept. To state the general result we
will redefine our model and introduce additional notation. The variables 
$C_i \in \reals$ and $M_j \in \reals^{k_j}$ now live in potentially different
spaces. Let $C = (C_1,\ldots,C_d)^\top \in \reals^d$ and 
$M = (M_1,\ldots,M_d)^\top \in \reals^k$, where $k = \sum_{j=1}^d k_j \geq d$. Let
$K_j = \{\sum_{a = 1}^{j-1} k_a + b | b \in \{1,\ldots,k_j\}\}$ denote the
subset of indices in $M$ that correspond to the block $M_j$. The permutation
that we want to recover is~$\pi$.
Each dimension in the $M$ variable can be transformed through a
separate feature map $\varphi_t\colon \mathbb{R} \to \mathbb{R}^{p_t}$ that can
be different for each $t=1,\ldots,k$. We will denote the total feature
vector 
by
%
\begin{align*}
    \varphi(M) = \begin{bmatrix} 
        \varphi_1(M_1) \\
        \vdots \\ 
        \varphi_k(M_k) 
    \end{bmatrix} 
.\end{align*}
%

The grouped features will be denoted by 
$\varphi(M)^{j} = (\varphi_t(M_t)  \mid  t \in K_j)$.
Define the average feature set size 
as $\overline{p} = \frac{1}{k}\sum_{t=1}^{k} p_t$. The model is described by
\begin{align*}
    C_{i} 
    &= \varphi(M) \bstar_i + \epsilon_i, 
    \quad
    \bstar_i \in \mathbb{R}^{k \overline{p}}, 
    \epsilon_i \sim \mathcal{N}(0, \sigma^2)
.\end{align*}
%
For the actual regression task, we can define data matrices again.
The matrix $\Hvec$ will be defined as in the main text and 
$\Phi$ now becomes an $n \times k\overline{p}$ matrix, in which
all feature vectors $\varphi(M^{(\ell)})$ are stacked. 
With $\bm{\epsilon}_i$ denoting $n$ independently 
draw $\cN(0, \sigma^2)$ variables, this
results in the relation
%
\begin{align*}
    \Hvec_i = \Phi \bstar_i + \bm{\epsilon_i}    
.\end{align*}
%
The $\bstar_i$ again has a sparse structure, because only the
parameters corresponding to $\varphi(M)^{j}$ should be non-zero.  Let
the indices of these parameters be denoted by $G_{j}$. Alternatively, 
this $G_j$ is defined through $\varphi(M)_{G_j} = \varphi(M)^{j}$. 
Thus, in this setting
we again have $d$ groups all denoted by $G_j$. 
To ease notation we set again $\beta^{j}_i = ((\beta_i)_t  \mid t \in G_j)$. 
The definitions of the norm $\|\cdot\|_{2, \infty}$ 
and covariance matrices, 
$\hS_{jj'} = \tfrac{1}{n}\varphi(M)_{G_j}^{\top}\varphi(M)_{G_{j'}}
= \tfrac{1}{n}\Phi_j^{\top}\Phi_{j'}$,
are altered in accordance with these groups. As the groups 
can now be of different size, we have to change the  
definition of the $\|\cdot\|_{2, 1}$-norm to take the different group sizes
into account,
%
\begin{align*}
    \|\beta\|_{2, 1} = \sum_{j=1}^{d} \|\beta^{j}\|\sqrt{p^{j}}  
,\end{align*}
%
where $p^{j} = \sum_{t \in K_j}p_t$. 
The loss function that we want to optimize to
estimate $\bstar_i$ has the same form as before
%
\begin{align}\label{eq:general_group_obj}
    \hbeta_{i}
        &= \argmin_{\beta \in \mathbb{R}^{k \overline{p}}}
        \tfrac{1}{n}\|\Hvec_i - \Phi\beta \|^2 + 
        \lambda \|\beta\|_{2, 1}
        =\argmin_{\beta \in \mathbb{R}^{k \overline{p}}}\tfrac{1}{n}\|\Hvec_i - \Phi\beta \|^2 + 
        \lambda \sum_{j=1}^{d} \|\beta^{j}\|\sqrt{p^{j}}  
.\end{align}
%
The optimality conditions for any solution, $\hbeta_i$, for this convex 
optimization problem are given by
%
\begin{align}
    \tfrac{1}{n}(\Phi^{\top}(\Hvec_i - \Phi\hbeta_i)^{j} 
    &= \frac{\lambda\sqrt{p^{j}}}{2} \frac{\hbeta^{j}}{\|\hbeta^{j}\|} 
    & \text{ if } \hbeta_i^{j} \neq 0,\label{eq:opt_cond_1}\\
    \tfrac{1}{n} \|\Phi^{\top}(\Hvec_i - \Phi\hbeta_i)\|^2 
    &\le  \frac{\lambda \sqrt{p^{j}}}{2}
    & \text{ if } \hbeta_i^{j} = 0\label{eq:opt_cond_2}
.\end{align}
%

To ensure that our results hold even in the case where $n < pd$, we
introduce a standard assumption on the data from the high-dimensional
statistics literature. Intuitively, this assumption ensures that the
data is ``variable enough'' in the directions that matter. 
%
\begin{asump}\label{asump:re_1}
    The Restricted Eigen Value (RE($1$)) 
    is satisfied by the data matrix $\Phi \in \mathbb{R}^{n \times  k \overline{p}}$
    if there exists a $\kappa > 0$
    such that for all $\Delta \in \mathbb{R}^{k \overline{p}} \setminus \{0\} $  
    and $j=1, \ldots, d$
    with $\sum_{i\neq j}\|\Delta^{i}\| \sqrt{p^i}\le 3\|\Delta^j\|\sqrt{p^j}$it holds that
    %
    \begin{align*}
        \frac{\|\Phi \Delta\|}{\sqrt{n}\|\Delta^{j}\| }  \mid 
         \ge \kappa
    .\end{align*}
    %
\end{asump}
%
This property is satisfied for any $\Delta \in \mathbb{R}^{k \overline{p}} \setminus \{0\} $
if $\hS = \tfrac{1}{n} \Phi^{\top} \Phi$ has a positive minimal eigenvalue. Let
$\lambda_{\text{min}} >0$ be the minimal eigenvalue of $\hS$, then
%
\begin{align*}
    \|\Phi\Delta\|^2 
    = \Delta^{\top} \Phi^{T}\Phi \Delta
    = n\Delta^{\top} \hS \Delta 
    \ge n\lambda_{\text{min}} \Delta^{\top}\Delta
    = n\lambda_{\text{min}}\|\Delta\|^2
.\end{align*}
%
Now divide by $n$ and take the square root on both sides. 
This gives us
%
\begin{align*}
    \frac{\|\Phi\Delta\|}{\sqrt{n}} 
    &\ge \sqrt{\lambda_{\text{min}}}\|\Delta\|
    = \sqrt{\lambda_{\text{min}}} \sqrt{
    \sum_{i=1}^{d} \|\Delta^{i}\|^2} 
    \ge \sqrt{\frac{\lambda_{\text{min}}}{d}}  
    \sum_{i=1}^{d} \|\Delta^{i}\|
    \ge \sqrt{\frac{\lambda_{\text{min}}}{d}}  \|\Delta^{j}\|
.\end{align*}
%
The second inequality follows from 
an application of Jensen's inequality. 
Dividing both sides by $\|\Delta^{j}\|$ gives the desired
result.

The matrix $\hS$ is the empirical covariance matrix and
will be positive definite almost surely whenever $n \ge k\overline{p}$ and hence RE(1) will be
satisfied if  $n \ge k\overline{p}$. 

Finally, define 
$\pmin = \min_{j=1, \ldots d} p^{j}$ and $\pmax = \max_{j=1, \ldots, d}p^{j}$.
The following theorems and proofs are adapted from Chapter~$8$ in 
\citet{buhlmannVG11} and sections $3$ and $5$ in \citet{lounici2011oracle}.
%
\begin{theorem}\label{thm:group_error_1}
    Assume that for all $\ell=1, \ldots, n$, $\epsilon^{(\ell)}_i \sim \mathcal{N}(0, \sigma^2)$ independently,  $\sigma^2 > 0$, 
    the RE(1) condition is satisfied with $\kappa > 0$
    and consider the Group Lasso estimator
    %
    \begin{align*}
        \hbeta_i = \argmin_{\beta \in \mathbb{R}^{k \overline{p}}} 
            \tfrac{1}{n} \|\Hvec_i - \Phi\beta\|^2 + 
            \lambda \|\beta\|_{2, 1}
    ,\end{align*}
    where $\lambda \ge 4 \lambda_0$
    with
    \begin{align*}
        \lambda_0 = \frac{2\sigma}{\sqrt{n} }
        \sqrt{1 + 
            \sqrt{\frac{8\log(d / \delta) }{\pmin}} 
            +
            \frac{8\log(d / \delta)}{\pmin}
        }.
    \end{align*}
    %
    Then, for any $\delta \in (0,1)$, with probability at least $1 - \frac{\delta}{d}$,
    %
    \begin{align}
        \label{eq:group_error_1}
        \tfrac{1}{n}\|\Hvec_i - \Phi \hbeta_i\|^2
            + 
            \lambda \|\hbeta_i - \bstar_i\|_{2, 1}
        &\le 
        \frac{24 \lambda^2 p^{\pi(i)}}{\kappa^2} \\
        \|(\hS(\hbeta - \bstar))^{j}\|
        &\le \lambda \sqrt{p^{j}} \qquad \text{ for all } j=1,\ldots,d 
            \label{eq:tech_result_2}\\
        \|\hbeta - \bstar\|_{2, 1}
        &\le
        \frac{24\lambda p^{\pi(i)}}{\kappa^2} \label{eq:group_error_3}
    .\end{align}
    %
\end{theorem}
%
This theorem offers us a several things. Equation~\ref{eq:group_error_1} gives us a 
bound on the true prediction error. The last two equations, 
(\ref{eq:tech_result_2},~ \ref{eq:group_error_3}), are needed to prove
that we find accurate
parameter values using the Group Lasso approach. The fact that the last equation gives 
a bound in the $(2, 1)$-norm, allows us to use a duality argument later on to provide 
a bound on the $(2, \infty)$-norm of the difference between the learned and true parameter. Knowing that only
one of the groups has to be non-zero combined with this uniform bound enables
us to conclude that the correct group has been identified in the proof of
Theorem~\ref{thm:group_structure_general}.
%
\begin{proof}
    First let us define for every $j=1,\ldots,d$ the random events
    $\mathcal{A}_j = \{\tfrac{1}{n}\|(\Phi^{\top}\bm{\epsilon}_i)^{j}\| 
    \le \tfrac{\lambda\sqrt{p^{j}}}{2}\}$
    and their intersection $\mathcal{A} = \bigcap_{j=1}^{d}\mathcal{A}_j$.
    Most importantly, we see from Lemma~\ref{lem:feature_concentration}
    that this event has probability at least $1 - \frac{\delta}{d}$.
    We get the $1 / d$ factor by using  $\widetilde{\delta} = \frac{\delta}{d}$ 
    in Lemma~\ref{lem:feature_concentration} and noticing that 
    this only adds a factor of $2$ in the log terms.
    The first assertion~\eqref{eq:group_error_1} is true on 
    the event $\mathcal{A}$ and follows from 
    the proof of Theorem~8.1 in \citep{buhlmannVG11} and noting
    that in our setting their oracle parameter is given by our $\bstar_i$
    and that $\textup{\textbf{f}}_0 = \Phi \bstar_i$. 
    
    Moving on towards \eqref{eq:tech_result_2}, by the optimality
    condition \eqref{eq:opt_cond_1} and  \eqref{eq:opt_cond_2} we have
    for each $j=1,\ldots, d$
    %
    \begin{align*}
        \tfrac{1}{n} \|(\Phi( \Hvec_i - \Phi\hbeta)^{j}\| 
        \le  
        \frac{\lambda \sqrt{p^{j}}}{2}
    .\end{align*}
    %
    Let us rewrite the expression in \eqref{eq:tech_result_2} into
    %
    \begin{align*}
        \|(\hS(\hbeta_i - \bstar_i))^{j}\| 
        = 
        \tfrac{1}{n}\|(\Phi^{\top}(\Phi\hbeta - \Phi\bstar))^{j}\|
    .\end{align*}
    %
    Substituting $\Phi\bstar_i = \Hvec_i - \bm{\epsilon}_i$ into this expression
    gives 
    %
    \begin{align*}
        \|(\hS(\hbeta_i - \bstar_i))^{j}\|
        &\le 
            \tfrac{1}{n} \|(\Phi^{\top}(\Phi\hbeta_i - \Hvec_i))^{j}\|
            +
            \tfrac{1}{n}\|(\Phi^{\top}\bm{\epsilon}_i)^{j}\|\\
        &\le \frac{\lambda \sqrt{p^{j}}}{2}   + \frac{\lambda \sqrt{p^{j}} }{2}
        = \lambda\sqrt{p^{j}} 
    .\end{align*}
    Note that this inequality only holds on $\mathcal{A}$.
    
    The final assertion is a direct consequence of the first, 
    %
    \begin{align*}
        \lambda \|\hbeta_i - \bstar_i\|_{2, 1} 
        &\le
          \tfrac{1}{n}\|\Phi(\hbeta_i - \bstar_i)\|^2
          + 
          \lambda \|\hbeta - \bstar\|_{2, 1} 
        \le 
        \frac{24 \lambda^2p^{\pi(i)}}{\kappa^2}\\
        \|\hbeta_i - \bstar_i\|_{2, 1} 
        &\le 
        \frac{24 \lambda p^{\pi(i)}}{\kappa^2} 
    .\end{align*}
    %
\end{proof}
%
To state and prove the general 
version of Theorem~\ref{thm:group_structure} we also need to generalize
Assumption~\ref{asump:structure}. 
\begin{asump}\label{asump:structure_general}
    There exists some constant $a > 1$ such that for any $j\neq j'$, 
    it holds that 
    \begin{align}
        \max_{1 \le t\le \min(p^{j}, p^{j'})}|(\hS_{jj'})_{t t}| 
        \le \frac{1}{14a}\sqrt{\frac{\pmin}{\pmax}}
    \end{align}
    and
    \begin{align}
        \max_{1 \le t\le p^{j}, 1 \le t' \le p^{j'}, t\neq t'}|(\hS_{jj'})_{t t'}| 
        \le \frac{1}{14a}\sqrt{\frac{\pmin}{\pmax}}
        \frac{1}{\sqrt{p^{j} p^{j'}} }.
    \end{align}
\end{asump}
The previous assumption is stronger than the RE($1$ ) property, as shown
by the
following lemma:
%
\begin{lemma}\label{lem:kappa_alpha}
    Let Assumption~\ref{asump:structure_general} be satisfied. Then RE($1$) is satisfied
    with $\kappa = \sqrt{1 - 1 / a} $.
\end{lemma}
\begin{proof}
    This is Lemma~B.3 in \citep{lounici2011oracle}.
\end{proof}
%
The following theorem is a modification of Theorem~$5.1$ by \citet{lounici2011oracle}, 
where some adaptations are made to adjust the result to our setting.
\begin{theorem}\label{thm:group_structure_general}
    Let Assumption~\eqref{asump:structure_general} hold, for $\ell=1,\ldots,d$,
    $\epsilon^{\ell}_i \sim
    \mathcal{N}(0,\sigma^2)$ independently, $\sigma^2 > 0$, and 
    with $\delta \in (0, 1)$  set $\lambda \ge 4\lambda_0$, where
    \begin{align*}
        \lambda_0 = \frac{2\sigma}{\sqrt{n} }
        \sqrt{1 + 
            \sqrt{\frac{8\log(d / \delta)}{\pmin}} 
            +
            \frac{8\log(d / \delta)}{\pmin}
        }.
    \end{align*}
    %
    Furthermore, set $c = \left( 1 + \tfrac{24}{7(a - 1)} \right) $. 
    Then, for any $\delta \in (0, 1)$, with probability at least $1 - \frac{\delta}{d}$, any solution 
    $\hbeta_i$ of \eqref{eq:general_group_obj} satisfies
    %
    \begin{align}
        \|\hbeta_i - \bstar_i\|_{2, \infty} \le c\lambda \sqrt{\pmax} 
        \label{eq:param_max_bound}
    .\end{align}
    %
    If, in addition, $\|(\bstar_i)^{\pi(i)}\| > 2c \lambda\sqrt{\pmax}$, 
    then \eqref{eq:param_max_bound} implies that
    %
    \begin{align*}
        \hJ_i = \argmax_{j=1, \ldots, d} \|\hbeta_i^{j}\|
    \end{align*}
    %
    estimates $\pi(i)$ correctly.
\end{theorem}

\begin{proof}
    Most of the proof is similar to the proof of Theorem~$5.1$ in \citet{lounici2011oracle}. 
    We supply a full proof for completeness and because our setting is slightly different. 
    We will need more notation to prove this statement. 
    Set $p_{\infty} = \max_{1\le d} p^{j}$ and define the extended covariance matrices
    $\widetilde{\Sigma}_{jj'}$ of size $p_{\infty}\times p_{\infty}$ as
    %
    \begin{align*}
        \widetilde{\Sigma}_{jj'} 
        = 
        \begin{bmatrix} 
        \begin{array}{c|c}
            \hS_{jj'} & 0 \\
            \hline
            0 & 0
        \end{array}
        \end{bmatrix} & \text{ if } j\neq j'
        \text{ and }
        \widetilde{\Sigma}_{jj} 
        = 
        \begin{bmatrix} 
        \begin{array}{c|c}
            \hS_{jj} - I_{p^{j}\times p^{j}} & 0 \\
            \hline
            0 & 0
        \end{array}
        \end{bmatrix} \text{ if } j=j'
    .\end{align*}
    %
    We also define for any $j=1,\ldots, d$ and $\Delta \in \mathbb{R}^{k\overline{p}}$
    the vector $\widetilde{\Delta}^{j}\in \mathbb{R}^{p_{\infty}}$ 
    such that 
    %
    \begin{align*}
        \widetilde{\Delta}^{j} = 
        \begin{bmatrix} \Delta^{j} \\ 0 \end{bmatrix} 
    .\end{align*}
    %
    Now set $\Delta = \hbeta_i - \bstar_i$ and bound
    %
    \begin{align*}
        \|\Delta\|_{2, \infty}
        = 
        \|\hS\Delta - (\hS - I_{k\overline{p} \times k \overline{p}})\Delta\|_{2, \infty}
        \le 
        \|\hS \Delta\|_{2, \infty}
        + \|(\hS - I_{k\overline{p} \times k \overline{p}}) \Delta\|_{2, \infty}
    .\end{align*}
    %
    The first term is controlled by \eqref{eq:tech_result_2} from
    Lemma~\ref{thm:group_error_1}.
    The latter term can be bounded by noticing that only
    the off-diagonal elements will contribute to the norm. We 
    can bound it using Cauchy-Schwarz:
    %
    \begin{align*}
        \|(\hS - I_{k\overline{p} \times k \overline{p}}) \Delta\|_{2, \infty}
        &=\max_{j=1, \ldots, d}\|((\hS - I_{k\overline{p} \times k \overline{p}}) \Delta)^{j}\|\\
        &=\max_{j=1, \ldots, d}\left[ 
            \sum_{t=1}^{p^{j}} \left( 
                \sum_{j'=1}^{d}\sum_{t'=1}^{p^{j'}} 
                (\widetilde{\Sigma}_{j j'})_{t t'}\widetilde{\Delta}_{t'}^{j'}
            \right)^2
        \right]^{1 / 2}\\
        &\le \max_{j=1, \ldots, d}\left[ 
            \sum_{t=1}^{p^{j}} \left( 
                \sum_{j'=1}^{d}
                (\widetilde{\Sigma}_{j j'})_{tt}\widetilde{\Delta}_{t}^{j'}
            \right)^2
        \right]^{1 / 2}\\
        &\phantom{=} +
        \max_{j=1, \ldots, d}\left[ 
            \sum_{t=1}^{p^{j}} \left( 
                \sum_{j'=1}^{d}\sum_{t'=1, t' \neq t}^{p^{j'}} 
                (\widetilde{\Sigma}_{j j'})_{t t'}\widetilde{\Delta}_{t'}^{j'}
            \right)^2
        \right]^{1 / 2}
    .\end{align*}
    We now bound both terms separately. The first term can be bounded using 
    an application of 
    Assumption~\ref{asump:structure_general} and then Minkowski's
    inequality.
    The Minkowski's inequality is true for $L^{p}$ norms and tells us
    %
    \begin{align*}
        \|x + y\|_p \le \|x\|_p + \|y\|_p
    .\end{align*}
    %
    In our case this generalises to 
    %
    \begin{align*}
        \left[ \sum_{t=1}^{p_{\infty}} \left( \sum_{j'=1}^{d} |\widetilde{\Delta}^{j'}_t| \right)^2  \right]^{1 / 2}
        =
        \|\sum_{j=1}^{d} \widetilde{\Delta}^{j}\|  
        \le 
        \sum_{j=1}^{d} \|\widetilde{\Delta}^{j}\|
        \le  
        \frac{1}{\sqrt{\pmin} }\sum_{j=1}^{d} \sqrt{p^{j}} \|\widetilde{\Delta}^{j}\|
        =
        \frac{1}{\sqrt{\pmin}}\|\widetilde{\Delta}\|_{2, 1}
    .\end{align*}
    %
    Combining Assumption~\ref{asump:structure_general} with the above inequality
    gives us
    %
    \begin{align*}
        \max_{j=1, \ldots, d}\left[ 
            \sum_{t=1}^{p_{j}} \left( 
                \sum_{j'=1}^{d}
                (\widetilde{\Sigma}_{j j'})_{tt}\widetilde{\Delta}_{t}^{j'}
            \right)^2
        \right]^{1 / 2}
        &\le 
        \frac{1}{14a }\sqrt{\frac{\pmin}{\pmax}}  \left[ 
            \sum_{t=1}^{p_{\infty}} \left( \sum_{j'=1}^{d} |\widetilde{\Delta}_t^{j'}| \right)^2 
        \right]^{ 1/2}\\
        &\le \frac{1}{14a }\sqrt{\frac{\pmin}{\pmax}} \frac{1}{\sqrt{\pmin} }
            \|\widetilde{\Delta}\|_{2, 1}\\
        &\le \frac{1}{14a }\sqrt{\frac{1}{\pmax}} \|\Delta\|_{2, 1}
    .\end{align*}
    %
    The second term can now be bounded by another application of Cauchy-Schwarz: 
    %
    \begin{align*}
         \max_{j=1, \ldots, d}\left[ 
        \sum_{t=1}^{p^{j}} \left( 
            \sum_{j'=1}^{d}\sum_{t'=1, t' \neq t}^{p^{j'}} 
            (\widetilde{\Sigma}_{j j'})_{t t'}\widetilde{\Delta}_{t'}^{j'}
            \right)^2
        \right]^{1 / 2}       
        &\le \frac{1}{14a }\sqrt{\frac{\pmin}{\pmax}}
        \max_{j=1,\ldots, d} \left[ \frac{1}{p^{j}}
            \sum_{t=1}^{p^{j}} \left( 
                \sum_{j'=1}^{d} \sum_{t'=1}^{p^{j'}} 
                \frac{|\widetilde{\Delta}_{t'}^{j'}|}{\sqrt{p^{j'}} } 
            \right)^2
        \right] ^{ 1/2}\\
        &\le \frac{1}{14a }\sqrt{\frac{\pmin}{\pmax}}
        \sum_{j'=1}^{d} \sum_{t'=1}^{p^{j'}} 
            \frac{|\widetilde{\Delta}_{t'}^{j'}|}{\sqrt{p^{j'}} }\\
        &\le \frac{1}{14a }\sqrt{\frac{\pmin}{\pmax}} 
            \frac{1}{\sqrt{\pmin}}\|\widetilde{\Delta}\|_{2,1} \\
        &\le \frac{1}{14a}\sqrt{\frac{1}{\pmax}} \|\Delta\|_{2,1}
    .\end{align*}
    %
    The $(2, 1)$-norm term is now bounded using \eqref{eq:group_error_3}. 
    Putting everything together we get 
    %
    \begin{align*}
        \|\hbeta_i - \bstar_i\|_{2, \infty} 
        &\le 
        \|\hS (\hbeta_i - \bstar_i)\|_{2, \infty}
        + \|(\hS - I_{pd \times pd}) (\hbeta_i - \bstar_i)\|_{2, \infty}\\
        &\le \lambda\sqrt{\pmax} 
            + \frac{2}{14a}\sqrt{\frac{1}{\pmax}}
            \left(
                \frac{24 \lambda p^{\pi(i)}}{\kappa^2} 
            \right)\\
        &\le \left(
            1
            +
            \frac{24}{7\kappa^2 a}
        \right)\lambda \sqrt{\pmax} 
    .\end{align*}
    %
    To satisfy both assumptions~(\ref{asump:re_1}, \ref{asump:structure}), we
    need to set $a \kappa^2 = (a - 1)$ as per Lemma~\ref{lem:kappa_alpha}.

    Finally, to prove the final claim, note that \eqref{eq:param_max_bound} combined
    with our sparsity assumption on the true parameters implies
    that for all $j' \neq \pi(i)$ 
    it must be that 
    $\|\hbeta_i^{j'}\| = \|\hbeta_i^{j'} - (\bstar_i)^{j'}\| < c\lambda \sqrt{\pmax} $. 
    We will show that for $\pi(i)$ it must be that $\|\hbeta_i^{\pi(i)} \| > c\lambda \sqrt{\pmax} $. 
    Hence, the estimator gets the correct index with high probability. Indeed, if
    $\|(\bstar_i)^{\pi(i)}\| > 2c \lambda \sqrt{\pmax} $ we get
    %
    \begin{align*}
        \|\hbeta_i^{\pi(i)}\| 
        &= 
        \|(\bstar_i)^{\pi(i)} - ((\bstar_i)^{\pi(i)} - \hbeta_i^{\pi(i)}) \|\\
        &\ge \left|\|(\bstar_i)^{\pi(i)}\| - \|((\bstar_i)^{\pi(i)} - \hbeta_i^{\pi(i)}) \|\right|\\
        &\ge 2c\lambda \sqrt{\pmax} - c \lambda  \sqrt{\pmax}\\
        &= c\lambda \sqrt{\pmax}
    .\end{align*}
    %
\end{proof}

Let us restate the specific version of Theorem~\ref{thm:group_structure} 
again for clarity. This theorem is now a corollary of 
Theorem~\ref{thm:group_structure_general}.
\GroupStruct*
\begin{proof}
    The result follows from Theorem~\ref{thm:group_structure_general}, where in this case $k=d$, 
    and $p^j = p$ for all $j=1,\ldots, d$. 
\end{proof}
\TotalProb*
\begin{proof}
Consider the following estimators
\begin{align*}
    \hbeta_i
    &= \argmin_{\beta \in \mathbb{R}^{dp}}
        \|\Hvec_{i} - \Phi \beta\|^2 + \lambda  \sqrt{p} \|\beta\|_{2,1},\\
    \hJ_{i} 
    &= \argmax_{j=1,\ldots d} \|\hbeta^{j}_i\|,\\
    \tpi
    &: [d] \to [d], i \mapsto \hJ_i
.\end{align*}
%
We will first show that $\tpi$ estimates $\pi$ with probability at
least $1 - \delta$.  Afterwards, we will show that the event on which $\tpi$ is correct, is
contained in the event that $\hpi$ estimates $\pi$ correctly, implying a lower
bound on the requested probability. 

We apply a union bound
%
\begin{align*}
    \mathbb{P}(\tpi = \pi) 
    &= 
    \mathbb{P}(\forall i=1,\ldots, d  \mid \hJ_i = \pi(i)) \\
    &= 
    1 - \mathbb{P}(\exists  i=1,\ldots, d  \mid \hJ_i \neq  \pi(i))\\
    &> 1 - \sum_{i=1}^{d} \frac{\delta}{d} \\
    &= 1 - \delta
.\end{align*}

We proceed to the second step. If $\tpi$ estimates $\pi$ correctly, then $\tpi$
is already a valid permutation and $\tpi=\hpi$. 
Indeed, if $\tpi$ is correct then that means that
$\|\hbeta_{i}^{\tpi(i)}\|= \|\hbeta_{i}^{\pi(i)}\|$ is the maximum norm 
for each $i$. Coincidentally, by $\pi$ being a correct permutation, $\tpi$ 
describes a correct matching with largest values, which means that 
$\tpi(i) = \hpi(i)$  for each $i=1,\ldots, d$ and 
%
\begin{align*}
    \mathbb{P}(\hpi = \pi) 
    \ge \mathbb{P}(\tpi = \pi)
    \ge 1 -\delta
.\end{align*}   
\end{proof}

\section{Proofs of Kernelized Permutation Estimator}
\label{app:proofs_non_params}

We will make one adjustment to the Group Lasso
regularization in the optimization problem in \eqref{eq:feature_optim},
which is that we square the regularization term. 
This form is theoretically more appealing, but is still
equivalent to the standard formulation. As \citet{bach2008consistency} argues,
the two versions of the optimization problem will have the same set of
solutions when varying the regularization parameters. For $\mu > 0$, the
objective is given by
\begin{align}\label{eq:feature_optim_squared}
    \inf_{\beta^{1}, \ldots, \beta^d} 
        \tfrac{1}{n}
        \|\Hvec_i - \beta( \Mvec ) \|^2
        + 
        \mu \left( \sum_{j=1}^{d} \|\beta^{j}\|_{\mathcal{H}_j} \right)^2
.\end{align}
Let $\hbeta^{1}_i, \ldots, \hbeta^{d}_i$ be the solutions of the above
optimization problem. The translation between regularization parameters that
give the same solutions for \eqref{eq:feature_optim} and
\eqref{eq:feature_optim_squared} is given by 
$\lambda = \mu\left(\sum_{j=1}^{d}\|\hbeta_i^{j}\|_{\mathcal{H}_j}\right)$.

\subsection{Representer Theorem}
The squared version of the optimization problem allows us to prove the Representer theorem 
from the main text:
\FeatKernEquiv*
\begin{proof}
    First we state the following result about a variational
    equality for positive numbers
    %
    \begin{align}\label{eq:var_norm}
        \left( \sum_{j=1}^{d} \|\beta^{j}\| \right)^2 
        =
        \inf_{\eta \in \Delta_d}\sum_{j=1}^{d} \frac{\|\beta^{j}\|^2}{\eta^{j}}
    .\end{align}
    %
    A proof of this statement can be found in section 1.5 of \citep{BachJMO12}. 
    Using~\eqref{eq:var_norm} and switching to the squared version of \eqref{eq:feature_optim}
    we rewrite \eqref{eq:feature_optim_squared} as 
    %
    \begin{align*}
        &\inf_{\beta_1, \ldots, \beta_d} 
        \frac{1}{n}
        \|\Hvec_i - \beta(\Mvec)\|^2
        + 
        \mu \left( \sum_{j=1}^{d} \|\beta^{j}\|_{\mathcal{H}_j} \right)^2\\
        &= 
        \inf_{\eta \in \Delta_d}
        \inf_{\beta_1, \ldots, \beta_d} 
        \frac{1}{n}
        \|\Hvec_i - \beta(\Mvec)\|^2
        + 
        \mu \sum_{j=1}^{d} \frac{\|\beta^{j}\|^2_{\mathcal{H}_j}}{\eta_j}\\
        &= (\text{OPT}_1).
    \end{align*}
    We can rewrite this expression further, using the reproducing property of
    the RKHSs $\mathcal{H}_j$, which gives $\beta^{j}(M_j)=\left<\beta^{j}, \varphi_j(M_j) \right>$. 
    Furthermore, defining $\widetilde{\beta^{j}} = \frac{\beta^{j}}{\sqrt{\eta_j} }$ 
    and $\widetilde{\varphi_j} = \sqrt{\eta_j} \varphi_j$
    we rewrite
    \begin{align*}
        (\text{OPT}_1)
        &= 
        \inf_{\eta \in \Delta_d}
        \inf_{\beta_1, \ldots, \beta_d} 
        \frac{1}{n}\sum_{\ell=1}^{n} 
        \left(C^{\ell}_i - \sum_{j=1}^{d} \left< \beta^{j}, \varphi_j(M^{(\ell)}_i)\right>\right)^2+ 
        \mu \sum_{j=1}^{d} \frac{\|\beta^{j}\|_{\mathcal{H}_j}^2}{\eta_j}\\
        &= 
        \inf_{\eta \in \Delta_d}
        \inf_{\widetilde{\beta_1}, \ldots, \widetilde{\beta_d}} 
        \frac{1}{n}\sum_{\ell=1}^{n} 
        \left(C^{(\ell)}_i - \sum_{j=1}^{d} 
            \left< \sqrt{\eta_j} \widetilde{\beta^{j}}, \varphi_j(M^{(\ell)}_j)\right>
            \right)^2
            +
        \mu \sum_{j=1}^{d} \|\widetilde{\beta^{j}}\|_{\mathcal{H}_j}^2\\
        &= 
        \inf_{\eta \in \Delta_d}
        \inf_{\widetilde{\beta_1}, \ldots, \widetilde{\beta_d}} 
        \frac{1}{n}\sum_{i=1}^{n} 
        \left(C^{(\ell)}_i - \sum_{j=1}^{d} 
            \left< \widetilde{\beta^{j}},  \widetilde{\varphi_j}(M^{(\ell)}_j)\right>
            \right)^2+ 
        \mu \sum_{j=1}^{d} \|\widetilde{\beta^{j}}\|_{\mathcal{H}_j}^2\\
        &=(\text{OPT}_2)
    .\end{align*}
    %
    This final expression should be recognized as the feature representation of
    the Representer theorem \citep{SHS2001rep} applied to the kernel described
    by
    %
    \begin{align*}
        \kappa(\eta)(M, M')
        &= \sum_{j=1}^{d} \left<\widetilde{\varphi_j}(M_j), \widetilde{\varphi_j}(M_j') \right>\\
        &= \sum_{j=1}^{d} \eta_j\left<\varphi_j(M_j), \varphi_j(M_j') \right>\\
        &= \sum_{j=1}^{d} \eta_j\kappa_j(M_j, M_j')
    .\end{align*}
    %
    The Representer theorem then gives us that the solution of the inner
    optimization problem in $(\text{OPT}_2)$ can be described by 
    \begin{align*}
        \widetilde{\beta^{j}} 
        = \sum_{\ell=1}^{n} \widetilde{\varphi_j}(M_j^{\ell})(c)_{\ell}
        \iff
        \frac{\beta^{j}}{\sqrt{\eta_j}}  
        = \sqrt{\eta_j}\sum_{\ell=1}^{n} \varphi_j(M^{(\ell)}_j)(c)_{\ell}
        \iff
        \beta^{j}
        = \sum_{\ell=1}^{n} \varphi_j(M^{(\ell)}_j)\eta_j(c)_{\ell}
    \end{align*}
    with $c \in \mathbb{R}^{n}$ and $\eta \in \Delta_{d}$. Alternatively
    interpreted this says that there exist $c^{1}, \ldots, c^{d} \in \mathbb{R}^{n}$ 
    such that $\beta^{j} = \sum_{\ell=1}^{n} \varphi_j(M_j^{(\ell)})(c^{j})_{\ell}$.
    We invoke again the equivalence between the squared and un-squared 
    versions of the optimization problem using the translation of regularization 
    parameters $\lambda = \mu\left( \sum_{j=1}^{d} \|\widehat{\beta}_i^{j}\|_{K_j} \right) $
    and conclude that the solutions 
    of \eqref{eq:feature_optim} are of the same form.
\end{proof}

To get the finite-dimensional optimization problem as stated in
\eqref{eq:kernel_optim_unsquared} we substitute the correct forms of
$\hbeta^{j}_i$ back into the original optimization problem. Define the Gramm
matrices $(K_j)_{\ell k} = \kappa_j(M^{(\ell)}_j, M^{k}_j) =
\left<\varphi_j(M_j^{(\ell)}, \varphi_j(M_j^{k}) \right>$ and observe
\begin{align*}
    &\inf_{\beta_1, \ldots, \beta_d} 
    \frac{1}{n}\sum_{\ell=1}^{n} 
    \left(C^{(\ell)}_i - \sum_{j=1}^{d} \left< \beta^{j}, \varphi_j(M^{(\ell)}_j\right>\right)^2
    + 
    \lambda \sum_{j=1}^{d} \|\beta^{j}\|_{\mathcal{H}_j} \\
    &=\inf_{c^{1}, \ldots, c^{d}\in \mathbb{R}^{n}}
    \frac{1}{n}\sum_{\ell=1}^{n} 
    \left( C^{(\ell)}_i - 
        \sum_{j=1}^{d} (K_jc^{j})_i \right)^2
        + \lambda \sum_{j=1}^{d} \sqrt{(c^{j})^{\top} K_j c^{j}}\\
    &=\inf_{c^{1}, \ldots, c^{d}\in \mathbb{R}^{n}}
    \tfrac{1}{n} 
    \| \Hvec_i - \sum_{j=1}^{d} K_jc^{j} \|^2
        + \lambda\sum_{j=1}^{d} \|c^{j}\|_{K_j}.
\end{align*}
%
\subsection{Estimator consistency}
\label{app:kernel_consistency}

As stated in the main text, the assumptions in Theorem~\ref{thm:kernel-prob}
are explained in this section. The assumptions stated in (A-D) ensure that the
RKHSs that we work with are nice enough and that the function we want to
estimate is not too miss specified. For a more complete discussion on the
assumptions, we refer to \citet{bach2008consistency}. To remind ourselves, we
are given $d$ random variables $M = (M_1,\ldots, M_d)$, where each random
variable lives in $\mathcal{Z}_j$, and $d$ RKHSs $\mathcal{H}_1, \ldots,
\mathcal{H}_d$ associated with $d$ kernels $\kappa_1,\ldots, \kappa_j$. The
cross-covariance operator, $\Sigma_{ij}$ for $\mathcal{H}_j$ to $\mathcal{H}_i$
is defined such that for all $(\beta^i, \beta^{j}) \in \mathcal{H}_i \times
\mathcal{H}_j$, 
%
\begin{align}
    \left<\beta^{j}, \Sigma_{ij}\beta^{j} \right> = 
    \mathbb{E}[\beta^{i}(M_i)\beta^{j}(M_j)] - \mathbb{E}[\beta^{i}(M_i)]\mathbb{E}[\beta^j(M_j)]
.\end{align}
%
The bounded correlation operators $\rho_{ij}$ are defined 
through the decomposition 
$\Sigma_{ij} = \Sigma_{ii}^{1 / 2} \rho_{ij} \Sigma_{jj}^{1 / 2}$ \citep{baker1973joint}.
%
\begin{enumerate}[label=(\Alph*)]
    \item For each $j=1,\ldots, d$, the Hilbert space $\mathcal{H}_j$ 
        is a separable reproducing kernel Hilbert space associated 
        with kernel $\kappa_j$ and the random variables $\kappa_j(\cdot, M_j)$ 
        are not constant and have finite fourth-order moments. 
    \item For all $i,j=1,\ldots, d$, the cross correlation operators are
        compact $\rho_{ij}$ and the joint correlation operator is invertible.
    \item For each $i=1,\ldots, d$, there exist functions 
        ${\bstar_i}^{1}, \ldots, {\bstar_i}^{d}\in \mathcal{H}_1,\ldots,\mathcal{H}_d$, 
        $b_i \in \mathbb{R}$ and a function $f_i$ of $M$ such that
        %
        \begin{align*}
            C_i = \sum_{j=1}^{d} {\bstar_i}^{j}(M_j) + b_i + f_i(M) + \epsilon_i
        ,\end{align*}
        %
        where $\mathbb{E}[\epsilon_i  \mid M] = 0$ and 
        $\sigma_{\text{min}}^2<\mathbb{E}[\epsilon_i^2  \mid M] < \sigma_{\text{max}}^2$
        with $\mathbb{E}[f_i(M)^2] < \infty$, $\mathbb{E}[f_i(M)] = 0$ 
        and $\mathbb{E}[f_i(M){\bstar_i}^j(M_j) = 0] $ for all $j=1\ldots, d$. 
        We define $\pi(i)$ to be the one index for which 
        ${\bstar_i}^{\pi(i)} \neq 0$.
    \item For all $i, j=1, \ldots, d$, there exists 
        $g_i^j \in \mathcal{H}_j$ such 
        that ${\bstar_i}^{j} = \Sigma^{1 / 2}_{jj} g_i^{j}$. 
\end{enumerate}
%
For each function, ${\bstar_i}^{\pi(i)}$, hat is non-zero we will require the following condition
%
\begin{align}\label{eq:kernel_condition}
    \max_{j \neq \pi(i)}
    \left\|
        \Sigma^{1 / 2}_{jj} \rho_{i\pi(i)} \rho^{-1}_{\pi(i)\pi(i)}
        Dg_{\pi(i)}
    \right\|_{\mathcal{H}_i} 
    < 1
,\end{align}
%
Where $D$ is a block diagonal operator where each block consists of
the operators $\tfrac{1}{\|{\bstar_i}^{j}\|_{\mathcal{H}_j}} I_{\mathcal{H}_j}$.
Condition (B) can be seen as an analogue to the correlation assumption
in Assumptions~\ref{asump:structure} and \ref{asump:structure_general}, as
it ensures that the variables are not too dependent. 

Before we prove Theorem~\ref{thm:kernel-prob}, we will first prove that each
individual index $\pi(i)$ can be estimated consistently. This follows from an
asymptotic result by \citet{bach2008consistency}.
\KernelProb*
\begin{proof}
    To prove this result we first define the estimator of each individual 
    index $\pi(i)$ as  
    \begin{align}
        \hJ_i = \argmax_{j=1,\ldots, d}\|\hbeta_i^{j}\|_{\mathcal{H}_j}
    .\end{align}
    Theorem~11 in \citep{bach2008consistency} gives consistency 
    for the estimated parameters $\hbeta^{j}_i$ and estimated index $\hJ_i$. 
    However, his result is stated for the squared version of the Group Lasso and 
    has as assumption that the $\mu_n$ regularization parameters have the
    property that $\mu_n \to \infty$ and $\sqrt{n}\mu_n \to + \infty$ 
    as the number of data points $n \to \infty$. 
    The translation factor between regularization parameters 
    $\lambda_n=\mu_n(\sum_{j=1}^{d} \|\hbeta^{j}_i\|_{K_j})$
    convergence to a constant in probability, by the consistancy
    of the estimated parameters. 
    This shows that the scalings for $\lambda_n$ and $\mu_n$ are the
    same asymptotically. We conclude that $\hJ_i$ estimates 
    $\pi(i)$ with probability tending to $1$. 

    Remember that the norms of $\hbeta^{j}_i$ and $\hat{c}^{j}_i$ are the
    same through $\|\hbeta^{j}_i\|_{\mathcal{H}_j} =
    \|\hat{c}_i^{j}\|_{K_j}$.  This means that we have consistency for
    estimators of $\pi(i)$ that are based on $\|\hat{c}_i^{j}\|_{K_j}$ as
    well. For each $i=1,\ldots, d$ we can repeat the above argumentation
    to get consistency for each $\hJ_i$ separately.
    To combine the conclusions, we apply the same argument as in the finite dimensional case
    and a a union bound that finishes our proof, 
    %
    \begin{align*}
        \mathbb{P}(\hpi= \pi) 
        &\ge  1 - \mathbb{P}(\exists i=1,\ldots,d  \mid \hJ_i \neq \pi(i))\\
        &\ge 1 - \sum_{i=1}^{d}\mathbb{P}(\hJ_i \neq \pi(i)) 
        \to 1
    .\end{align*}
    %
\end{proof}


\subsection{Implementation}

\KernelImp*
%
\begin{proof}
Applying the Cholesky decomposition $K_j = L_jL_j^{\top}$ 
for each $j=1,\ldots,d$  and substituting this into \eqref{eq:kernel_optim_lstsq}
gives
%
\begin{align*}
    \inf_{c^{1}, \ldots, c^{d}\in \mathbb{R}^{n}}
        \tfrac{1}{n} 
        \| \Hvec_i - \sum_{j=1}^{d} K_jc^{j} \|^2
            + \lambda \sum_{j=1}^{d} \|c^{j}\|_{K_j} 
    &= \inf_{c^{1}, \ldots, c^{d}\in \mathbb{R}^{n}}
        \tfrac{1}{n} 
        \| \Hvec_i - \sum_{j=1}^{d} L_jL_j^{\top}c^{j} \|
            + \lambda \sum_{j=1}^{d} \|L_j^{\top}c^{j}\|\\
    &= \inf_{\gamma^{1}, \ldots, \gamma^{d}\in \mathbb{R}^{n}}
        \tfrac{1}{n} 
        \| \Hvec_i - \sum_{j=1}^{d} L_j\gamma^{j} \|^2
            + \lambda \sum_{j=1}^{d} \|\gamma^{j}\|
.\end{align*}
%
\end{proof}

The number of parameters now scales with the number of data samples.
Computationally, this quickly becomes unwieldy.  We apply a Nystr\"om style
approximation by sub-sampling $m \ll n$ columns of each $K_j$ Gramm
matrix and using those to approximate the full Gramm matrix \citep{WS2000nystrom}. 

\section{Probability Results}
\label{app:probability results}
\begin{lemma}\label{lem:chi_concentration_bound}
For $j=1,\ldots, d$ and $\sigma^2> 0$ let $\frac{\chi_j^2}{\sigma^2}$ be 
independent chi-square distributed random variables with $p^{j}$ 
degrees of freedom for $j=1,\ldots, d$. Then, with $\delta \in (0, 1)$  and for 
%
\begin{align*}
    \lambda_0 = \frac{2\sigma}{\sqrt{n} }
        \sqrt{1 + 
            \sqrt{\frac{4\log(d / \delta)}{\pmin}} 
            +
            \frac{4\log(d /\delta)}{\pmin}
        },
\end{align*}
%
we have 
%
\begin{align*}
    \mathbb{P}\left(\max_{1\le j \le d} 
        \frac{\chi_j}{\sqrt{np^{j}}} \le \frac{\lambda_0 }{2}
    \right)
    \ge 1 - \delta
.\end{align*}
%
\end{lemma}
%
\begin{proof}
    This is Lemma 8.1 in \citep{buhlmannVG11} and substituting $x = \log(1 / \delta)$. 
\end{proof}
%
The previous lemma is the general version of a concentration inequality that
is need in the proof of Theorem~\ref{thm:group_structure_general}. 
The concentration inequality that we want to use is the following.

\begin{lemma}\label{lem:feature_concentration}
    Let $\sigma^2 > 0$ and assume that $\epsilon^{(1)}, \ldots \epsilon^{(n)}$ are independently 
    $\cN(0, \sigma^2)$ distributed, 
    $\Phi$ as in Appendix~\ref{app:proofs_params}, 
    and with $\delta \in (0,1)$ set $\lambda \ge 4\lambda_0$ for
     \begin{align*}
        \lambda_0 = \frac{2\sigma}{\sqrt{n} }
            \sqrt{1 + 
                \sqrt{\frac{4\log(d / \delta)}{\pmin}} 
                +
                \frac{4 \log(d / \delta)}{\pmin}
            } 
    \end{align*}   
    Then, $\mathbb{P}(\mathcal{A}) \ge 1-\delta$, where 
    $\mathcal{A} = \bigcap_{j=1}^{d}\mathcal{A}_j$ with the events
    $\mathcal{A}_j = \{\frac{1}{n}\|(\Phi^{\top}\bm{\epsilon})^{j}\| 
    \le \frac{\lambda \sqrt{p^{j}} }{2}\} $ for all $j=1,\ldots d$
    and $\bm{\epsilon} = [\epsilon^{(1)}, \ldots\epsilon^{(n)}]^{\top}$.
\end{lemma}
%
\begin{proof}
    By assumption, 
    $I_{p^{j}\times p^{j}}=\hS_{j} =\tfrac{1}{n}(\Phi_{j})^{\top}\Phi_{j}$ 
    and the fact that 
    $\bm{\epsilon} \sim \cN(0, \sigma^2I_{n \times n})$, 
    we first see
    %
    \begin{align*}
        \tfrac{1}{\sigma\sqrt{n} }(\Phi\bm{\epsilon})^{j}
       &= \tfrac{1}{\sigma\sqrt{n} }\Phi_j^{\top}\bm{\epsilon}\\
       &\sim \cN(0, \tfrac{1}{n}\Phi_j^{\top}I_{n\times n}\Phi_j)\\
       &\sim \cN(0, I_{p^{j}\times p^{j}})
    .\end{align*}
    %
    This shows us that $\tfrac{1}{\sigma^2n}\|(\Phi^{\top}\bm{\epsilon})^{j}\|^2$
    has a  chi-squared distribution. We can now apply 
    Lemma~\ref{lem:chi_concentration_bound} by noticing that it holds 
    also  holds for $\lambda \ge 4\lambda_0 \ge \lambda_0$ and that 
    %
    \begin{align*}
        \bigcap_{j=1}^{d}\mathcal{A}_j = 
        \left\{\max_{1\le j\le d} \frac{1}{\sqrt{np^{j}} } 
        \frac{1}{\sqrt{n} }\|(\Phi\bm{\epsilon})^{j}\| \le \frac{\lambda}{2}\right\} 
    .\end{align*}
    %
\end{proof}
%

\section{Experiment Details}
\label{app:experiments}
All the code to reproduce the experiments and figures in this paper
is provided as a GitHub repository at \url{https://github.com/HiddeFok/sample-efficient-learning-of-concepts}.
The synthetic experiments were performed on a single $32$-core CPU node 
(AMD Rome 7H12) with $56$GB of RAM. The DL experiments were performed
on single $18$-core GPU node (NVIDIA A100 GPU and Intel XEON CPU)
with $120$GB of RAM.

\subsection{Toy Dataset Experiments}
\label{app:synth_experiments}

\begin{figure}[t]
    \centering
    \includegraphics{./figs/diffeomorphisms.pdf}
    \caption{The diffeomorphisms used in the misspecified case}
    \label{fig:diffeomorphisms}
\end{figure}

The synthetic experiments can be subdivided into $4$ sets, based 
on which features mapping is used. These features mappings are 
linear features, spline features , random Fourier features and kernels. 
Here, we will describe how the data is generated and what 
the hyperparameters of the features and kernels were. 

\subsubsection{Data Generation}
For the synthetic experiment we sample the $C \in \mathbb{R}^{d}$ variables
from a $\cN(0, (1 - \rho)I_{d\times d} + \rho\ind)$ distribution, 
where $\ind$ denotes a matrix filled with only $1$'s. The  $\rho \in (0,1)$ 
parameter controls the amount of correlation between the variables. 
We sample $n$ data points, on which we perform a $80/20$ train/test
data split. The test data is not needed to measure the performance of the
permutation estimator, as we have access to the ground truth. We do
use it to measure the risk or $R^2$ metric. There are $2$ settings
in which we generate the $M \in \mathbb{R}^{d}$ variables. 

\paragraph{Wellspecified} In the wellspecified setting we generate
the $M$ variables by applying a map consisting of the features and kernels
used to estimate the permutation. This setting is a sanity check to see
if our estimator works in a setting that satisfy all the required
assumptions. For each dimension $j=1,\ldots, d$ a random weight
vector $\bstar_j\in \mathbb{R}^{p}$, such that 
$\|\bstar_j\| \in [16\lambda_0, 32\lambda_0]$ uniformly. A permutation 
$\pi\colon \{1, \ldots, d\} \to \{1, \ldots, d\}$ is uniformly sampled
from all possible permutations. Finally, with independent
$\epsilon_i \sim \cN(0, \sigma^2)$ noise variables we get 
%
\begin{align*}
    C_i = \varphi(M_{\pi(i)})^{\top}\bstar_{\pi(i)} + \epsilon_i
.\end{align*}
%
\paragraph{Misspecified} In the misspecified setting we
generate the $M$ variables by first sampling $d$ 
diffeomorphisms, $\{f_i \colon \mathbb{R} \to \mathbb{R}\}_{i=1}^{d}$ 
from a set of pre specified diffeomorphisms. These functions are plotted
in Figure~\ref{fig:diffeomorphisms}. Each function gets a random scaling 
$w_i$ uniformly in $[-2,  2]$. Finally, we get
 %
\begin{align*}
    C_{i} = w_{\pi(i)} f_{\pi(i)}(M_{\pi(i)}) + \epsilon_i
.\end{align*}
%

For each of the experiments we save the MPE, $R^2$ score and execution
time. To compare different settings of our estimator, 
we also save the MPE, $R^2$ and execution time of using only
the purely linear version of our estimator on that particular
data setting. We also save the MPE and execution times of using
Pearson or Spearman correlations. 

We vary the following parameters, the regularization parameter $\lambda$, 
the dimension $d$, the correlation  $\rho$ and the number of data points
$n$. As stated before, in each experiment we look at the wellspecified
and misspecified case. 
%
\begin{itemize}
    \item The regularization parameter varies in 
        $\lambda \in \{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1\} $.
        The other settings are set to $d\in \{20, 60, 100\} $, 
        $\rho=0$ and $n=1250$.
    \item The dimension is varied in $d \in \{5, 30, 60, 80, 100\} $.
        The other settings are set to $\lambda \in \{0.001, 0.01, 0.1\} $,
        $\rho=0$ and $n=1250$.
    \item The correlation parameter varies in 
        $\rho\in \{0, 0.2, 0.4, 0.6, 0.8, 0.95, 0.99\} $.
        The other settings are set to $\lambda \in\{0.001, 0.01, 0.1\} $,
        $d=60$ and $n=1250$.
    \item The dimension is varied in $n \in \{65, 125, 1250, 2500, 5000\} $.
        The other settings are set to $\lambda \in \{0.001, 0.01, 0.1\} $,
        $d=60$ and $\rho=0$.
\end{itemize}
%


\subsubsection{Feature and Kernel setting}
\paragraph{Linear features} In the linear case no transformation is applied
to the $M$ variables. 

\paragraph{Spline features} In the spline features case we perform the 
regression using a spline basis transformation, either piecewise linear or cubic splines. 
We expect this method to work especially well, because the cubic splines 
form a dense subset in the space of twice differentiable functions, of which
the diffeomorphisms are a subset. To calculate these features we use the
\verb|SplineTransformer| class of the \verb|scikit-learn| package. The total 
number of feature parameters is calculated as $p=n_k + n_d - 1$, where $n_k$ 
is the number of knots and $n_d$ is the degree of each spline. In each of 
the toy dataset experiments the number of knots was $n_l \in \{4, 8\} $ 
and the degrees are $n_d \in \{1, 3\} $. 

\paragraph{Random Fourier features} For the random Fourier features we use a 
varying amount of random features. We sample random features that approximate
the RBF kernel. To sample these features we use the \verb|RBFSampler| class
of the \verb|scikit-learn| package. The total number of feature parameters 
in this case is number of random Fourier features. The number
of features in the toy dataset experiments were $p \in \{2, 4, 6, 8\} $.

\paragraph{Kernels} For the kernel experiments we perform the experiments for 
several kernels, the polynomial kernel, the RBF kernel, the Brownian kernel
and a Sobolev kernel. These kernels are given by
\label{p:kernel_examples}
%
\begin{align*}
    \kappa_{\text{pol}}(x, y) &= (1 + \left<x, y \right>)^3\\
    \kappa_{\text{RBF}}(x, y) &= \e^{-(x - y)^2}\\
    \kappa_{\text{Lap}}(x, y) &= \e^{-|x - y|} \\
    \kappa_{\text{cos}}(x, y) &= \cos(\left<x, y \right>)
.\end{align*}

\subsection{Action/Temporal Dataset Experiments}
\label{app:at_experimets}
The data generation settings, model architectures and training hyperparameters
were taken from the original paper \citep{lachapelle2022dms}. Their implementation
can be found at 
\url{https://github.com/slachapelle/disentanglement_via_mechanism_sparsity/tree/main}.

\subsubsection{Dataset Details}
The dataset consists of temporal data sequences, $\{(X^{t}, z^{t},
a^{t})\}_{t=1}^{T}$, where $X^{t}\in \mathbb{R}^{20}$ is the observed data,
$a^{t} \in \mathbb{R}^{10}$ is an action, which is seen as an auxiliary
variable in the ICA framework developed in \citep{khemakhem2020vaeica}, and
$z^{t}\in \mathbb{R}^{10}$ the latent causal variable. 
The ground truth mixing function $f$ is a random neural network with three
hidden layers of $20$ units with Leaky-ReLU activations with negative slope of
$0.2$. The weight matrices are sampled independently 
according to $\cN(0, 1)$ and the weight matrices are the orthogonalized to
ensure inactivity of the mixing function. The observational noise $\epsilon$
in each dimension is sampled according $\cN(0, 10^{-4})$ and is added
to $f(z^{t})$. The transitions
from $(z^{t-1}, a^{t-1})$ to $z^{t}$ is sampled according
to $\cN(\mu(z^{t-1}, a^{t-1}), 10^{-4}I_{10 \times 10})$. The mean function
$\mu$ will be different between the Action Sparsity dataset
and the Temporal Sparsity dataset.

\paragraph{Action Sparsity} The sequences have length $T=1$ and the mean
function is given by
%
\begin{align*}
    \mu(z^{t-1}, a^{t-1})_i \coloneqq 
        \sin(\tfrac{2 + i}{\pi}a_i^{t-1} +(i-1))
        +
        \sin(\tfrac{2 + i}{\pi}a_{i-1}^{t-1} +(i-1) )
,\end{align*}
where the index $i=-1$ is periodically identified with $i=10$. 
%

\paragraph{Temporal Sparsity} The sequences have length $T=2$ and the mean
function is given by
%
\begin{align*}
    \mu(z^{t-1}, a^{t-1})_i \coloneqq 
    z_i^{t-1} + 0.5 \sum_{j=1}^{i} \sin(\tfrac{2 + i}{\pi}z_j^{t-1} + (i-1))
.\end{align*}
%

In both datasets we sample $10^{6}$ points and split the dat $80/20$ for the
train/test split.
\subsubsection{Model Architectures} 
\begin{table}[t]
    \caption{Architecture details for the encoder and decoders used in the 
    temporal and action sparsity dataset experiments.}
    \label{tbl:dms_architecture}
    \centerfloat
    \small
    \begin{tabular}{m{1cm}m{1cm}m{2.5cm}m{2.5cm}}
        \toprule
        & Layer & Hidden Size & Activation Function\\
        \bottomrule
        \multirow{7}*{Encoder} 
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & $2\cdot 10$ & -  \\
        \hline
        \multirow{7}*{Decoder}
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\
        & Linear & 512 & LeakyReLU(0.2)\\   
        & Linear & 512 & LeakyReLU(0.2)\\
        & Linear & 20 & -  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!hb]
    \caption{The hyperparameters used for the training of the DMS-VAE, TCVAE and 
    iVAE for the action and temporal sparsity datasets}
    \label{tbl:dms-training}
    \centering 
    \small
    \begin{tabular}{m{4cm}m{8cm}}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value}\\
        \bottomrule
        Batch Size & 1024 \\
        \multirow{2}*{Optimizer} & Adam \citep{KingmaB14} and \\
                  & Cooper \citep{gallego2021flexible}\\
        Learning rate & 5e-4 (DMS-VAE), 1e-4 (iVAE), 1e-3 (TCVAE)\\
        KL divergence factor $\beta$ & 1.0\\
        Number of latents & 20\\
        Number of epochs & 500\\
        Gumbel Softmax temperature & 1.0\\ 
        \bottomrule
    \end{tabular}
\end{table}

In all experiments a minibatch of $1024$ is used for the training. The same encoder
and decoder is used for all models: A MLP with $6$ layers of $512$ units
with LeakyReLU activations with negative slope $0.2$. See Table~\ref{tbl:dms_architecture}
for a detailed description. The encoder $f_{\text{enc}}(x; \theta)$ outputs the
mean and standard deviation of $q_{\theta}(z^{t} \mid x^{t})$, which are 
the densities of normal distribution. The latent 
transition distribution $\hat{p}_{\lambda}(z_i^{t} \mid z^{<t}, a^{<t})$, 
where $z^{<t}=(z^{t'})_{t'=1}^{t-1}$ and $a^{<t}=(a^{t'})_{t'=1}^{t-1}$, 
is also also learned by a fully connected neural network.
It's parameters are $\lambda$ and it outputs 
the variance of a $\cN\left( 0, \sigma^2 \right) $ distribution.
The decoder $f_{\text{dec}}(z; \psi)$ tries to reconstruct the original
data from the learned encodings. 
The differences between the $3$ methods comes from 
the loss function that is optimized. The common term in each of the optimizations
is the Evidence Lower Bound (ELBO) objective. This is given by 
%
\begin{align}
    \label{eq:elbo_obj_dms}
    \text{ELBO}(\theta, \psi, \lambda)
    =
    \sum_{t=1}^{T} \bE_{z^{t}\sim q_{\theta}(\cdot  \mid x^{t})}
    \left[ \log p_{\psi}(x^{t}  \mid z^{t}) \right] 
    - 
    \bE_{z^{<t} \sim q_{\theta}(\cdot  \mid x^{<t})}\left[
        \infdivKL{q_{\theta}(z^{t} \mid x^{t})}{\hat{p}_{\lambda}(z^{t}\mid z^{<t}, a^{<t})}
    \right]
,\end{align}
where $\infdivKL{\cdot}{\cdot}$ is the KL-divergence.
%
\paragraph{DMS-VAE} A regularization term is added to the ELBO objective
in \eqref{eq:elbo_obj_dms}. The regularization enforces sparsity to 
the learned graph describing the causal relations between the learned
encodings and actions. The new objective is given by
%
\begin{align*}
    \text{ELBO}(\theta, \psi, \lambda) 
    + \alpha_z \|\hat{G}_z \|_{0}
    + \alpha_a \|\hat{G}_a \|_{0}
.\end{align*}
%
The variable $\hat{G}_z$ is a learned matrix representing the relations between 
the latent variables between $2$ time steps. The variable $\hat{G}_a$ 
is a learned matrix representing the relations between 
the actions and the latent variables. The norm $\|\cdot\|_{0}$ counts
the number of non-zero terms. This is a discrete objective and 
can transformed into a continuous objective using the Gumbel-Softmax
trick \citep{MaddisonMT17, JangGP17}.

Alternatively, The authors prepose a constrained based
optimization procedure on the ELBO, where the constrained is determined by
the number of edges in the learned graph. For the constrained
optimization method the authors provide a optimization schedule that performs
this optimization procedure, which we use by setting the \verb|--constraint_schedule| 
parameter.
The other hyper parameters for the training 
procedure can be found in Table~\ref{tbl:dms-training}.


\paragraph{TCVAE} The implementation of the original paper
\citep{chen2018isolating} was adapted by the authors of
\citep{lachapelle2022dms}, which we also use. The loss function consists
of the same components as the ELBO in \eqref{eq:elbo_obj_dms}, but they decompose
it into $3$ terms and add a weight parameter to each of the terms.
The hyper parameters for the training 
procedure can be found in Table~\ref{tbl:dms-training}.

\paragraph{iVAE} The implementation of the original paper
\citep{khemakhem2020vaeica} was adapted by the authors of
\citep{lachapelle2022dms}, which we use. The loss function here is very
similar to the ELBO objective, but it adds one parameter $\beta$ to the KL-term
in the objective. The hyper parameters for the training 
procedure can be found in Table~\ref{tbl:dms-training}.

\subsection{Temporal Causal3DIdent}
\label{app:temp_causal_experimets}

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \centering
        \node (a) {\includegraphics{./figs/causal_ident/teapot.png}};       
        \node[right = 0.3cm of a ] (b) {\includegraphics{./figs/causal_ident/armadillo.png}};       
        \node[right = 0.3cm of b ] (c) {\includegraphics{./figs/causal_ident/bunny.png}};       
        \node[right = 0.3cm of c ] (d) {\includegraphics{./figs/causal_ident/cow.png}};       
        \node[right = 0.3cm of d ] (e) {\includegraphics{./figs/causal_ident/dragon.png}};       
        \node[right = 0.3cm of e ] (f) {\includegraphics{./figs/causal_ident/head.png}};       
        \node[right = 0.3cm of f ] (g) {\includegraphics{./figs/causal_ident/horse.png}};       

        \node[below = 0.1cm of a] {(a)};
        \node[below = 0.1cm of b] {(b)};
        \node[below = 0.1cm of c] {(c)};
        \node[below = 0.1cm of d] {(d)};
        \node[below = 0.1cm of e] {(e)};
        \node[below = 0.1cm of f] {(f)};
        \node[below = 0.1cm of g] {(g)};
        
    \end{tikzpicture}
    \caption{Examples of the $7$ shapes in the Temporal Causal3DIdent dataset.
        From left to right: teapot, armadillow, bunny, cow, dragon, head and
        horse.}
\label{fig:causal-3d-ident}
\end{figure}

The dataset, model architectures and training hyper parameters were taken 
from \citep{LippeMLACG22}. Their implementation 
can be found at 
\url{https://github.com/phlippe/CITRIS/tree/main}.

\subsubsection{Dataset Details}
The data comes from a setting
which is referred to as Temporal Intervened Sequences. The assumption is that 
there are $d$ causal variables $(G_1, \ldots, _d)$ and a corresponding
causal graph $\mathcal{G}=(V, E)$ where each node $i \in V$ represents a causal variable $G_i$.
The variables can be real-valued or vector-valued, and each edge $(i, j) \in E$ 
represents a relation between $G_i$ and $G_j$. Now, we assume there 
are $T$ time steps and for every $t=1,\ldots,T$ the causal variables
are generated through some process.
So, we have a sequence $\{(G_1^{t}, \ldots, G_d^{t})\}_{t=1}^{T}$, 
where only the causal variables in $t-1$ are the parents of the causal variables
in time step $t$. Identifiability is achieved in this setting by assuming 
that we have access to $d$-dimensional binary vector at each time step $I^{t}\in \{0,1\}^{d}$ 
that tells us which causal variables have been intervened on, but not with which value. 
The observations $X^t$ at each time step are created through a mixing function 
and some noise $X^{t}=f(G_1^{t}, \ldots G^{t}_d, \epsilon^{t})$. The authors 
in \citet{LippeMLACG22} propose a framework, CITRIS, that is able to
disentangle these causal variables into $k$ encodings. The number of encodings
is allowed to be bigger than $d$ , but the model subdivides the encodings into
$d$ possibly uneven blocks that get mapped to the causal variables. They also
altered the implementation of other methods, such as iVAE, to be able to work
in their setting and show that CITRIS works the best. 

The causal variables that are used in the data generating process are the following:
%
\begin{itemize}
    \item The \textbf{object position} (\verb|pos_o|) is modelled in $3$ dimensions
        $(x, y, z) \in [-2, 2]^{3}$. The values are forced to be in this interval to ensure
        that the object does not disappear from the image, becomes too small or
        covers the whole image.

    \item The \textbf{object rotation} (\verb|rot_o|) is modelled in $2$ dimensions
        $(\alpha, \beta) \in [0, 2\pi)^2$. Distances for angles are calculated in 
        a periodic fashion, ensuring that angles close to $0$ and $2\pi$ are 
        close together.

    \item The \textbf{spotlight rotation} (\verb|rot_s|) is the positioning of the spotlight
        that shines on the object. The value range is $[0, 2\pi)$, where distances
        are again calculated in a periodic fashion.

    \item The \textbf{spotlight hue} (\verb|hue_s|) is the color of the spotlight. The 
        range of the valeu is $[0, 2\pi)$, where $0$ corresponds to red. 

    \item The \textbf{background hue} (\verb|hue_b|) is the color of the background.
        The value range is $[0, 2\pi)$ with the $0$ corresponding to red again.

    \item The \textbf{object hue} (\verb|hue_o|) is the color of the object, 
        with value range is $[0,2\pi)$ and with again $0$ representing red.
\end{itemize}
%

The authors in \citet{LippeMLACG22} generated the data using Blender, a setup inspired
by \citet{KugelgenSGBSBL21} and using code provided by \citet{ZimmermannSSBB21}.
They generate the dataset by starting with an initial random set of causal variables. 
They then sample the causal variables in each subsequent time step by following a 
specific conditional distribution, which is given by the set of equations 
in \eqref{eq:set_equations}.
\begin{align}
    \begin{split}\label{eq:set_equations}
    f(a,b,c) 
    &= \frac{a - b}{2} + c\\
    \mathrm{pos\_x}^{t+1} 
    &= f(1.5 \cdot \sin(\mathrm{rot\_}\beta^{t}), \mathrm{pos\_x}^{t}, \epsilon_x^{t})\\
    \mathrm{pos\_y}^{t+1} 
    &= f(1.5 \cdot \sin(\mathrm{rot\_}\alpha^{t}), \mathrm{pos\_y}^{t}, \epsilon_y^{t})\\
    \mathrm{pos\_z}^{t+1} 
    &= f(1.5 \cdot \sin(\mathrm{rot\_}\alpha^{t}), \mathrm{pos\_z}^{t}, \epsilon_z^{t})\\
    \mathrm{rot\_}\alpha^{t+1} 
    &= f(\mathrm{hue\_}b^{t}, \mathrm{rot\_}\alpha^{t}, \epsilon_{\alpha}^{t})\\
    \mathrm{rot\_}\beta^{t+1} 
    &= f(\mathrm{hue\_}o^{t}, \mathrm{rot\_}\beta^{t}, \epsilon_{\beta}^{t})\\
    \mathrm{rot\_}s^{t+1} 
    &= f(\mathrm{atan}2(\mathrm{pos\_x}^{t}, \mathrm{pos\_y}^{t}, \mathrm{rot\_}s^{t}, \epsilon_{rs}^{t})\\
    \mathrm{hue\_s}^{t+1} 
    &= f(2\pi - \mathrm{hue\_b}^{t}, \mathrm{hue\_s}^{t}, \epsilon_{hs}^{t})\\
    \mathrm{hue\_b}^{t+1} 
    &= \mathrm{hue\_b}^{t}+ \epsilon_{b}^{t}\\ 
    \mathrm{hue\_b}^{t+1} 
    &= f(g(i), \mathrm{hue\_o}^{t}, \epsilon_{ho}^{t})
    \end{split}
\end{align}

\begin{table}[!hb]
    \parbox{0.35\linewidth}{
    \caption{Output of the $g$ function for each object shape}
    \label{tbl:shape-values}
    \begingroup
    \centering 
    \def\arraystretch{1.2}
    \small
    \begin{tabular}{m{2cm}m{3.5cm}}
        \toprule
        \textbf{Object shape} & \textbf{Object hue goal}\\
        \bottomrule
        Teapot Size & $0$ \\
        Armadillo & $\frac{2\pi}{5}$\\
        Hare & $\text{avg}(\mathrm{hue\_s}^{t}, \mathrm{hue\_b}^{t})$\\
        Cow & $\frac{4 \pi}{5}$\\
        Dragon & $ \pi + \text{avg}(\mathrm{hue\_s}^{t}, \mathrm{hue\_b}^{t})$\\
        Head & $\frac{6\pi}{5}$\\
        Horse & $\frac{8\pi}{5}$\\
        \bottomrule
    \end{tabular}
    \endgroup
    }
    \hfill
    \parbox{0.4\linewidth}{
        The $\text{avg}$ function is defined as
    \begin{align*}
        \text{avg}(\alpha, \beta)
        = 
        \text{atan}2\left( 
            \frac{\sin(\alpha) + \sin(\beta)}{2},  
        \frac{\cos(\alpha) + \cos(\beta)}{2}
    \right) 
    .\end{align*}
    %
}
\end{table}

%
All the noise $\epsilon$-variables are independently $\cN(0, 10^{-2})$ distributed for the position
and $\cN(0, (0.15)^2)$ distributed for the angels. The $g$ function in the final
line maps the object shapes to specific values detailed in Table~\ref{tbl:shape-values}.

The object shape is changed in each time step with a probability of $0.05$. If it is
changed, a new shape is sampled uniformly over the $7$ shapes.

They then sample for each time step the intervention targets $I^{t+1}_i \sim \text{Bernoulli}(0.1)$.
If a causal variables is intervened on it is replaced with a random sample
from $U(-2, 2)$ for continuous values or $U(0, 2\pi)$ for the angles. For the object
shape a uniform distribution over the $7$ shapes is used. They run this
generation for $250,000$ steps, which is the full dataset.

We use the already generated dataset downloaded from 
\url{https://zenodo.org/records/6637749#.YqcWCnVBxCA}.

\subsubsection{Model Architectures}
\begin{table}[!ht]
    \caption{Architecture details for the encoder and decoders used in the Temporal Causal3DIdent
    experiments.}
    \label{tbl:citris_architecture}
    \centerfloat
    \small
    \begin{tabular}{m{1cm}m{2cm}m{4cm}m{1cm}m{1cm}m{3cm}}
        \toprule
        & Layer & $\begin{array}{c}\text{Feature Dimension} \\ (\text{H} \times \text{W} \times \text{C})\end{array}$ & Kernel & Stride & Activation Function\\
        \bottomrule
        \multirow{11}*{Encoder} 
        & Conv & 32 $\times$ 32 $\times$ 64 & 3 & 2 & BatchNorm+SiLU\\
        & Conv & 32 $\times$ 32 $\times$ 64 & 3 & 1 & BatchNorm+SiLU\\
        & Conv & 16 $\times$ 16 $\times$ 64 & 3 & 2 & BatchNorm+SiLU\\
        & Conv & 16 $\times$ 16 $\times$ 64 & 3 & 1 & BatchNorm+SiLU\\
        & Conv & 8 $\times$ 8 $\times$ 64 & 3 & 2 & BatchNorm+SiLU\\
        & Conv & 8 $\times$ 8 $\times$ 64 & 3 & 1 & BatchNorm+SiLU\\
        & Conv & 4 $\times$ 4 $\times$ 64 & 3 & 2 & BatchNorm+SiLU\\
        & Conv & 4 $\times$ 4 $\times$ 64 & 3 & 1 & BatchNorm+SiLU\\
        & Reshape & 1 $\times$ 1 $\times$ 1024 & - & - & -\\
        & Linear & 1 $\times$ 1 $\times$ 256 & - & - & LayerNorm+SiLU\\
        & Linear & 1 $\times$ 1 $\times$ 2 $\cdot$\verb|num_latents| & - & - & -\\
        \hline
        \multirow{14}*{Decoder}
        & Linear & 1 $\times$ 1 $\times$ 256 & - & - & LayerNorm+SiLU\\
        & Linear & 1 $\times$ 1 $\times$ 1024 & - & - & - \\
        & Reshape & 4 $\times$ 4 $\times$ 1024 & - & - & - \\
        & Upsample & 8 $\times$ 8 $\times$ 64 & - & - & - \\
        & ResidualBlock & 8 $\times$ 8 $\times$ 64 & 3 & 1 & - \\
        & Upsample & 16 $\times$ 16 $\times$ 64 & - & - & - \\
        & ResidualBlock & 16 $\times$ 16 $\times$ 64 & 3 & 1 & - \\
        & Upsample & 32 $\times$ 32 $\times$ 64 & - & - & - \\
        & ResidualBlock & 32 $\times$ 32 $\times$ 64 & 3 & 1 & - \\
        & Upsample & 64 $\times$ 64 $\times$ 64 & - & - & - \\
        & ResidualBlock & 64 $\times$ 64 $\times$ 64 & 3 & 1 & - \\
        & Pre-Activations & 64 $\times$ 64 $\times$ 64 & - & - & BatchNorm+SiLU \\
        & Conv & 64 $\times$ 64 $\times$ 64 & 1 & 1 & BatchNorm+SiLU \\
        & Conv & 64 $\times$ 64 $\times$ 3 & 1 & 1 & Tanh \\
        \bottomrule
    \end{tabular}
\end{table}

In both the CITRIS-VAE and iVAE models, the encoder and decoder architecture
are set to be the same. The encoder is a convolutional neural network, which
outputs two parameters per latent variable. These will be the mean and the log
of the standard deviation for the normal distribution that models the latent
variable. The decoder uses bilinear upsampling and residual blocks to
reconstruct the image. The 
full architecture is described in Table~\ref{tbl:citris_architecture}.
For these experiments, the ELBO is defined to be
%
\begin{align*}
    \text{ELBO}(\theta, \phi, \gamma)
    =
    &-\bE_{z^{t+1}\sim q_{\theta}(\cdot  \mid x^{t+1})}\left[ \log p_{\theta}(x^{t+1}  \mid z^{t+1}) \right] \\
    &+
    \bE_{
        \substack{
            z^{t} \sim q_{\theta}(\cdot  \mid x^{t})\\
            \pi \sim \text{GS}(\gamma)
        }}\left[ 
            \sum_{i=1}^{d}
            \infdivKL{q_{\theta}(z_{\pi(i)}^{t+1}  \mid x^{t+1})}{p_{\phi}(z_{\pi(i)}^{t+1} \mid z^{t}, I^{t+1}_i)}
        \right] 
.\end{align*}
%
Here $p_{\theta}$ models the encoder, $q_{\theta}$ the decoder,
$p_{\varphi}(z^{t+1} \mid z^{t}, I^{t+1})$ the transition prior and GS
is the Gumbel-Softmax distribution
of the causal variables between time steps given the intervention targets.
Finally, $\pi$ is the target assignment between learned encoding variables
and the causal variables. 

To train the CITRIS-VAE and the iVAE method, an autoencoder is pre-trained to
map the high-dimensional images to lower-dimensional feature vectors, but
without enforcing disentanglement. This is done separately from the main training
procedure as \citet{LippeMLACG22} mention that this improves performance. 
During training a small bit of Gaussian
noise is added to the encodings to prevent collaps of the encoding
distribution. No prior is enforced for this encoder. This autoencoder will be
have  $2$ ResidualBlocks instead of $1$ per resolution in the decoder part. The
training hyperparameters are described in Table~\ref{tbl:ae-training}. The
autoencoder is trained using the MSE reconstruction loss. 


\paragraph{CITRIS-VAE} In the CITRIS model, an assignment $\pi\colon \{1, \ldots, k\} \to \{1, \ldots,d\} $ is learned between
the learned encodings, and the true causal variables. This done by assuming that
each $\pi(i)$ follows a Gumbel-Softmax distribution and we learn the continuous parameter
that governs this distributionm. During
training an encoding-to-causal variable  assignment is sampled, while during
inference the argmax is used. 

The transition prior $p_{\phi}$ is learned by an autoregressive model, 
which for each $z_{\pi(i)}^{t+1}$ takes $z^{t}, I^{t+1}_{i} $and $z^{t+1}$ 
as inputs and outputs a Gaussian random variable. The autoregressive 
model follows a MADE architecture \citep{KhajenezhadMB21}, 
with $16$ neurons per layer for each encoding, and the input
to these neurons are the features of all previous encodings. 
The prior is $2$ layers deep, and uses the SiLU activation function.

Finally, a small network is trained to predict the intervention 
targets, given $z^{t}$ and $z^{t+1}_{\pi(i)}$ for each $i=1,\ldots k$. 


\paragraph{iVAE} To adept the iVAE model for this setting, the auxiliary 
variable $u$ will be given by the previous observation $x^{t}$ and intervention
targets $I^{t+1}$. Another alteration that is made is that the prior
with the iVAE model only conditions on $(x^{t}, I^{t+1})$. The main 
difference between iVAE and the CITRIS-VAE is the structre of the
prior $p(z^{t+1} \mid z^t, I^{t+1})$. Another difference is that
no target assignment is learned during the training, but only after. 

For the iVAE a $2$-layer MLP with hidden dimensionality of $128$ is used
for the transition prior.

\begin{table}[!ht]
    \parbox{0.5\linewidth}{
    \caption{The hyperparameters used for the training of both the CITRIS-VAE and iVAE 
    models.}
    \label{tbl:citris-training}
    \centering 
    \footnotesize
    \begin{tabular}{m{3.25cm}m{3.5cm}}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value}\\
        \bottomrule
        Batch Size & 512 \\
        Optimizer & Adam \citep{KingmaB14}\\
        Learning rate & 1e-3\\
        Learning rate scheduler & Cosine Warmup (100 steps)\\
        KL divergence factor $\beta$ & 1.0\\
        KL divergence factor $\psi_{0}(\lambda)$ & 0.01\\
        Number of latents & 32\\
        Number of epochs & 600\\
        Target classifier weight & 2.0\\
        Gumbe Softmax temperature & 2.0\\ 
        \bottomrule
    \end{tabular}
    }
    \hfill
    \parbox{0.47\linewidth}{
    \caption{The hyperparameters used for the training of autoencoder used by both
    the CITRIS-VAE and iVAE.}
    \label{tbl:ae-training}
    \centering 
    \footnotesize
    \begin{tabular}{m{2.6cm}m{3.5cm}}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value}\\
        \bottomrule
        Batch Size & 512 \\
        Optimizer & Adam \citep{KingmaB14}\\
        Learning rate & 1e-3\\
        Learning rate scheduler & Cosine Warmup (100 steps)\\
        Number of latents & 32\\
        Gaussian noise $\sigma$ & 0.05\\
        NUmber of epochs & 1000\\
        \bottomrule
    \end{tabular}
}
\end{table}

\subsection{Performance metrics}
To assess the performance of our estimator we record the MPE of the
estimated permutation, together with the execution time. On 
top of that we record the $R^2$-score. Some of the baselines are
created by regression every input variable onto every output
variable and using the individual $R^2$-scores to extract a 
permutation. In those cases the $R^2$-scores that is documented
here in the paper is the average of the $R^2$-scores that are
chosen to be matched. This is also referred to as the $R^2$-score
on the diagonal. This gives 
%
\begin{align*}
    R^2 = \frac{1}{d}\sum_{i=1}^{d} 
    \left(1 - \frac{
            \|\Hvec_i - \widehat{h}_{\hpi(i)}\left(\Mvec_{\hpi(i)}\right)\|^2
        }{
            \|\Hvec_i - \overline{\Hvec_i}\|^2
        }\right),
    \qquad
    \overline{\Hvec_i} = \frac{1}{n}\sum_{\ell=1}^{d} C^{(\ell)}_i
.\end{align*}
%
Here, $\widehat{h}_j$ is the estimated mapping and it is applied
to the whole vector  $\Mvec_{j}$.

\paragraph{Estimator settings} We use various settings of our estimator, 
to asses if there are particular advantages for certain versions. The
versions that we used were
%
\begin{itemize}
    \item \textbf{Linear}, no feature map is applied. 
    \item \textbf{random Fourier Features}, we sample $8$ random Fourier features
        from the RBF kernel. 
    \item \textbf{Spline}, we calculate cubic spline features with  $6$ knots. 
    \item \textbf{Laplacian}, we use the Laplacian kernel with 
        $\min\{n, 20\}$ components. 
    \item \textbf{Two stage} we apply a two stage approach, where 
        we use $20\%$ of the data to estimate the permutation
        using no additional features and then use the rest of the 
        data to perform ridge regression with cubic spline features 
        using $ 6$ knots.
\end{itemize}
%
We define an array of regularization parameters and report the results 
of the best choice for each $n$. The parameters that were considered are
$\lambda \in \{0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2\} $.

\paragraph{Baselines} We again calculate the permutations using the
Pearson and Spearman correlations. For these experiments, we added another baseline, which
is given by trained neural networks. Each neural 
network takes an individual encoding as input and tries to predict
all the causal variables. The individual $R^2$-scores are used to 
construct a matching again. This neural network is a $2$-layer MLP
with $128$ hidden nodes in each layer and using the $\tanh$ as 
as an activation function. The network is trained for a $100$ epochs
with Adam and a learning rate of 4e-3. We considered using $32$ 
and $64$ hidden nodes and saw that $128$ performed the best. 

In the Temporal Causal3DIdent dataset experiments, both VAE models
learn groups of encoded variables that are matched with a
causal variable. To use the Pearson and Spearman correlations 
in this case, we first sum the encodings in the groups and then 
calculate the correlation coefficients. 

\section{Additional Results}
Here, we show all the results obtained in the experiments with the 
toy dataset, action/temporal sparsity dataset and temporal Causal3DIdent
dataset. 

\subsection{Toy Dataset}
We provide plots for experiments performed with linear features, spline
features, random Fourier features and kernels.

In each of the version of our estimator we see that the MPE scores
are good. Especially in the misspecified setting. It is interesting
to note that the estimator does not perform well in the well-specified
case, when the regularization parameter is not tuned correctly. 
This can be explained by the non-invertability of the functions
in this setting. This makes the identification of each matching more 
noisy and difficult, which can also be noted by the fact that the 
Pearson and Spearman correlation approaches are note able to find
the correct permutation, while our is able to. 

This also explains why the $R^2$ scores in the well-specified case are 
worse than the $R^2$-scores. Another reason for that observation is that
norms of the true parameters have to be quite large, to strictly 
adhere to the assumptions of our theoretical results. This increases
the variability of the output data by a large margin and makes
regression more difficult. 

Finally, we can also see that our estimator does work well, even when the
correlation is high, but the regularization parameter hast to be tuned
correctly. This does come at a cost of an increasing computation time. 

\subsection{Action/Temporal Sparsity Datasets and Temporal Causal3DIdent Dataset}
Here, we report all results obtained in the action/temporal sparsity datasets
and temporal Causal3DIdent dataset. The MPEs are reported in Table~\ref{tbl:perm-error-dms},
the $R^2$-scores in Table~\ref{tbl:r2-dms} and the execution times
are plotted in Figure~\ref{fig:times-both}. 

In terms of MPE, we see that a version of our estimator performs the best
the best or as good as the best in each of the datasets in terms of the
number of samples need to get the permutation correct. For the $R^2$scores
we see that we perform well in some cases in the low data regime, but when
using all the data available, the neural network often performs the best. 
This does come at a computational cost, where the neural network approach
requires two to three orders of magnitude more computation time. 

It is interesting to note that the estimator typically works better 
for the more advanced models developed. This can be explained by the fact
that these models achieve a better disentanglement, which should make it
easier to find the correct matching between the encodings and the 
causal variables. 
\newpage

\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/spline/perm_error/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/spline/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.1cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.38cm}{0cm}{alpha_1}&
            \incFigure{0.55cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_3}&
            \incFigure{0.55cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_5}&\\

            \incFigure{0.1cm}{0cm}{dim_0}&
            \incFigure{1.38cm}{0cm}{dim_1}&
            \incFigure{0.55cm}{0cm}{dim_2}&
            \incFigure{1.38cm}{0cm}{dim_3}&
            \incFigure{0.55cm}{0cm}{dim_4}&
            \incFigure{1.38cm}{0cm}{dim_5}&\\

            \incFigure{0.1cm}{0cm}{entanglement_0}&
            \incFigure{1.38cm}{0cm}{entanglement_1}&
            \incFigure{0.55cm}{0cm}{entanglement_2}&
            \incFigure{1.38cm}{0cm}{entanglement_3}&
            \incFigure{0.55cm}{0cm}{entanglement_4}&
            \incFigure{1.38cm}{0cm}{entanglement_5}&\\

            \incFigure{0.1cm}{0cm}{n_total_0}&
            \incFigure{1.38cm}{0cm}{n_total_1}&
            \incFigure{0.55cm}{0cm}{n_total_2}&
            \incFigure{1.38cm}{0cm}{n_total_3}&
            \incFigure{0.55cm}{0cm}{n_total_4}&
            \incFigure{1.38cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{Permutation Error using Spline Features for all parameters}
    \label{fig:spline-perm_error}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/spline/r2/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/spline/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.1cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.38cm}{0cm}{alpha_1}&
            \incFigure{0.55cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_3}&
            \incFigure{0.55cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_5}&\\

            \incFigure{0.1cm}{0cm}{dim_0}&
            \incFigure{1.38cm}{0cm}{dim_1}&
            \incFigure{0.55cm}{0cm}{dim_2}&
            \incFigure{1.38cm}{0cm}{dim_3}&
            \incFigure{0.55cm}{0cm}{dim_4}&
            \incFigure{1.38cm}{0cm}{dim_5}&\\

            \incFigure{0.1cm}{0cm}{entanglement_0}&
            \incFigure{1.38cm}{0cm}{entanglement_1}&
            \incFigure{0.55cm}{0cm}{entanglement_2}&
            \incFigure{1.38cm}{0cm}{entanglement_3}&
            \incFigure{0.55cm}{0cm}{entanglement_4}&
            \incFigure{1.38cm}{0cm}{entanglement_5}&\\

            \incFigure{0.1cm}{0cm}{n_total_0}&
            \incFigure{1.38cm}{0cm}{n_total_1}&
            \incFigure{0.55cm}{0cm}{n_total_2}&
            \incFigure{1.38cm}{0cm}{n_total_3}&
            \incFigure{0.55cm}{0cm}{n_total_4}&
            \incFigure{1.38cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{$R^2$-score on the diagonal using Spline Features for all parameters}
    \label{fig:spline-r2}
\end{figure}
\begin{figure}
\def\incFigure#1#2#3{%
    \node{\scalebox{0.6}{\includegraphics[
        % [trim={left bottom right top},clip]
        trim={{#1} 0 {#2} 0.2cm},
        clip
    ]{./figs/spline/time/#3.pdf}}};
}
\centerfloat
\begin{tikzpicture} 
    \node (legend) {\includegraphics[scale=1]{./figs/spline/legend.pdf}};
    \matrix[below=-0.2cm of legend.south, 
        row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.25cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.68cm}{0cm}{alpha_1}&
            \incFigure{0.70cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.68cm}{0cm}{alpha_3}&
            \incFigure{0.70cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.68cm}{0cm}{alpha_5}&\\

            \incFigure{0.25cm}{0cm}{dim_0}&
            \incFigure{1.68cm}{0cm}{dim_1}&
            \incFigure{0.70cm}{0cm}{dim_2}&
            \incFigure{1.68cm}{0cm}{dim_3}&
            \incFigure{0.70cm}{0cm}{dim_4}&
            \incFigure{1.68cm}{0cm}{dim_5}&\\

            \incFigure{0.25cm}{0cm}{entanglement_0}&
            \incFigure{1.68cm}{0cm}{entanglement_1}&
            \incFigure{0.70cm}{0cm}{entanglement_2}&
            \incFigure{1.68cm}{0cm}{entanglement_3}&
            \incFigure{0.70cm}{0cm}{entanglement_4}&
            \incFigure{1.68cm}{0cm}{entanglement_5}&\\

            \incFigure{0.25cm}{0cm}{n_total_0}&
            \incFigure{1.68cm}{0cm}{n_total_1}&
            \incFigure{0.70cm}{0cm}{n_total_2}&
            \incFigure{1.68cm}{0cm}{n_total_3}&
            \incFigure{0.70cm}{0cm}{n_total_4}&
            \incFigure{1.68cm}{0cm}{n_total_5}&\\    
        };
\end{tikzpicture}
\caption{Execution times using Spline Features for all parameters}
\label{fig:spline-time}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/rff/perm_error/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/rff/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.1cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.38cm}{0cm}{alpha_1}&
            \incFigure{0.55cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_3}&
            \incFigure{0.55cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_5}&\\

            \incFigure{0.1cm}{0cm}{dim_0}&
            \incFigure{1.38cm}{0cm}{dim_1}&
            \incFigure{0.55cm}{0cm}{dim_2}&
            \incFigure{1.38cm}{0cm}{dim_3}&
            \incFigure{0.55cm}{0cm}{dim_4}&
            \incFigure{1.38cm}{0cm}{dim_5}&\\

            \incFigure{0.1cm}{0cm}{entanglement_0}&
            \incFigure{1.38cm}{0cm}{entanglement_1}&
            \incFigure{0.55cm}{0cm}{entanglement_2}&
            \incFigure{1.38cm}{0cm}{entanglement_3}&
            \incFigure{0.55cm}{0cm}{entanglement_4}&
            \incFigure{1.38cm}{0cm}{entanglement_5}&\\

            \incFigure{0.1cm}{0cm}{n_total_0}&
            \incFigure{1.38cm}{0cm}{n_total_1}&
            \incFigure{0.55cm}{0cm}{n_total_2}&
            \incFigure{1.38cm}{0cm}{n_total_3}&
            \incFigure{0.55cm}{0cm}{n_total_4}&
            \incFigure{1.38cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{Permutation Errors using Random Fourier Features}
    \label{fig:rff-perm-errors}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/rff/r2/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/rff/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.1cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.38cm}{0cm}{alpha_1}&
            \incFigure{0.55cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_3}&
            \incFigure{0.55cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_5}&\\

            \incFigure{0.1cm}{0cm}{dim_0}&
            \incFigure{1.38cm}{0cm}{dim_1}&
            \incFigure{0.55cm}{0cm}{dim_2}&
            \incFigure{1.38cm}{0cm}{dim_3}&
            \incFigure{0.55cm}{0cm}{dim_4}&
            \incFigure{1.38cm}{0cm}{dim_5}&\\

            \incFigure{0.1cm}{0cm}{entanglement_0}&
            \incFigure{1.38cm}{0cm}{entanglement_1}&
            \incFigure{0.55cm}{0cm}{entanglement_2}&
            \incFigure{1.38cm}{0cm}{entanglement_3}&
            \incFigure{0.55cm}{0cm}{entanglement_4}&
            \incFigure{1.38cm}{0cm}{entanglement_5}&\\

            \incFigure{0.1cm}{0cm}{n_total_0}&
            \incFigure{1.38cm}{0cm}{n_total_1}&
            \incFigure{0.55cm}{0cm}{n_total_2}&
            \incFigure{1.38cm}{0cm}{n_total_3}&
            \incFigure{0.55cm}{0cm}{n_total_4}&
            \incFigure{1.38cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{$R^2$-score on the diagonal using Random Fourier Features}
    \label{fig:rff-r2}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/rff/time/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/rff/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.25cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.68cm}{0cm}{alpha_1}&
            \incFigure{0.70cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.68cm}{0cm}{alpha_3}&
            \incFigure{0.70cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.68cm}{0cm}{alpha_5}&\\

            \incFigure{0.25cm}{0cm}{dim_0}&
            \incFigure{1.68cm}{0cm}{dim_1}&
            \incFigure{0.70cm}{0cm}{dim_2}&
            \incFigure{1.68cm}{0cm}{dim_3}&
            \incFigure{0.70cm}{0cm}{dim_4}&
            \incFigure{1.68cm}{0cm}{dim_5}&\\

            \incFigure{0.25cm}{0cm}{entanglement_0}&
            \incFigure{1.68cm}{0cm}{entanglement_1}&
            \incFigure{0.70cm}{0cm}{entanglement_2}&
            \incFigure{1.68cm}{0cm}{entanglement_3}&
            \incFigure{0.70cm}{0cm}{entanglement_4}&
            \incFigure{1.68cm}{0cm}{entanglement_5}&\\

            \incFigure{0.25cm}{0cm}{n_total_0}&
            \incFigure{1.68cm}{0cm}{n_total_1}&
            \incFigure{0.70cm}{0cm}{n_total_2}&
            \incFigure{1.68cm}{0cm}{n_total_3}&
            \incFigure{0.70cm}{0cm}{n_total_4}&
            \incFigure{1.68cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{Execution times using Random Fourier Features}
    \label{fig:rff-time}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/kernel/perm_error/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/kernel/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.1cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.38cm}{0cm}{alpha_1}&
            \incFigure{0.55cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_3}&
            \incFigure{0.55cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_5}&\\

            \incFigure{0.1cm}{0cm}{dim_0}&
            \incFigure{1.38cm}{0cm}{dim_1}&
            \incFigure{0.55cm}{0cm}{dim_2}&
            \incFigure{1.38cm}{0cm}{dim_3}&
            \incFigure{0.55cm}{0cm}{dim_4}&
            \incFigure{1.38cm}{0cm}{dim_5}&\\

            \incFigure{0.1cm}{0cm}{entanglement_0}&
            \incFigure{1.38cm}{0cm}{entanglement_1}&
            \incFigure{0.55cm}{0cm}{entanglement_2}&
            \incFigure{1.38cm}{0cm}{entanglement_3}&
            \incFigure{0.55cm}{0cm}{entanglement_4}&
            \incFigure{1.38cm}{0cm}{entanglement_5}&\\

            \incFigure{0.1cm}{0cm}{n_total_0}&
            \incFigure{1.38cm}{0cm}{n_total_1}&
            \incFigure{0.55cm}{0cm}{n_total_2}&
            \incFigure{1.38cm}{0cm}{n_total_3}&
            \incFigure{0.55cm}{0cm}{n_total_4}&
            \incFigure{1.38cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{Permutation Errors using Kernels}
    \label{fig:kernel-perm-errors}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/kernel/r2/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/kernel/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.1cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.38cm}{0cm}{alpha_1}&
            \incFigure{0.55cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_3}&
            \incFigure{0.55cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.38cm}{0cm}{alpha_5}&\\

            \incFigure{0.1cm}{0cm}{dim_0}&
            \incFigure{1.38cm}{0cm}{dim_1}&
            \incFigure{0.55cm}{0cm}{dim_2}&
            \incFigure{1.38cm}{0cm}{dim_3}&
            \incFigure{0.55cm}{0cm}{dim_4}&
            \incFigure{1.38cm}{0cm}{dim_5}&\\

            \incFigure{0.1cm}{0cm}{entanglement_0}&
            \incFigure{1.38cm}{0cm}{entanglement_1}&
            \incFigure{0.55cm}{0cm}{entanglement_2}&
            \incFigure{1.38cm}{0cm}{entanglement_3}&
            \incFigure{0.55cm}{0cm}{entanglement_4}&
            \incFigure{1.38cm}{0cm}{entanglement_5}&\\

            \incFigure{0.1cm}{0cm}{n_total_0}&
            \incFigure{1.38cm}{0cm}{n_total_1}&
            \incFigure{0.55cm}{0cm}{n_total_2}&
            \incFigure{1.38cm}{0cm}{n_total_3}&
            \incFigure{0.55cm}{0cm}{n_total_4}&
            \incFigure{1.38cm}{0cm}{n_total_5}&\\
        };
    \end{tikzpicture}
    \caption{$R^2$-score on the diagonal using Kernels}
    \label{fig:kernel-r2}
\end{figure}
\begin{figure}
    \def\incFigure#1#2#3{%
        \node{\scalebox{0.6}{\includegraphics[
            % [trim={left bottom right top},clip]
            trim={{#1} 0 {#2} 0.2cm},
            clip
        ]{./figs/kernel/time/#3.pdf}}};
    }
    \centerfloat
    \begin{tikzpicture} 
        \node (legend) {\includegraphics[scale=1]{./figs/kernel/legend.pdf}};
        \matrix[below=-0.2cm of legend.south, 
            row sep=-0.1cm, column sep=-0.2cm] (plotmatrix) {
            \incFigure{0.25cm}{0cm}{alpha_0}&[-0.22cm]
            \incFigure{1.68cm}{0cm}{alpha_1}&
            \incFigure{0.70cm}{0cm}{alpha_2}&[-0.215cm]
            \incFigure{1.68cm}{0cm}{alpha_3}&
            \incFigure{0.70cm}{0cm}{alpha_4}&[-0.215cm]
            \incFigure{1.68cm}{0cm}{alpha_5}&\\

            \incFigure{0.25cm}{0cm}{dim_0}&
            \incFigure{1.68cm}{0cm}{dim_1}&
            \incFigure{0.70cm}{0cm}{dim_2}&
            \incFigure{1.68cm}{0cm}{dim_3}&
            \incFigure{0.70cm}{0cm}{dim_4}&
            \incFigure{1.68cm}{0cm}{dim_5}&\\

            \incFigure{0.25cm}{0cm}{entanglement_0}&
            \incFigure{1.68cm}{0cm}{entanglement_1}&
            \incFigure{0.70cm}{0cm}{entanglement_2}&
            \incFigure{1.68cm}{0cm}{entanglement_3}&
            \incFigure{0.70cm}{0cm}{entanglement_4}&
            \incFigure{1.68cm}{0cm}{entanglement_5}&\\

            \incFigure{0.25cm}{0cm}{n_total_0}&
            \incFigure{1.68cm}{0cm}{n_total_1}&
            \incFigure{0.70cm}{0cm}{n_total_2}&
            \incFigure{1.68cm}{0cm}{n_total_3}&
            \incFigure{0.70cm}{0cm}{n_total_4}&
            \incFigure{1.68cm}{0cm}{n_total_5}&\\        
        };
    \end{tikzpicture}
    \caption{Execution times using kernels}
    \label{fig:kernel-time}
\end{figure}

\newpage

\begin{table}
    \caption{Permutation Errors for the encodings learned in the
    Action/Temporal sparsity datasets and the Temporal Causal3DIdent datasets.
    We report the mean and standard deviation over $50$ random seeds and in each
    column we write the best method in \textbf{bold}.}
    \label{tbl:perm-error-dms}
    \centering 
    \setlength\tabcolsep{4pt}
    \small
    \begin{tabular}{m{2cm}m{1.5cm}|cccccc}
        \toprule
        & &  
        \multicolumn{6}{c}{\small Permutation Error $\downarrow$ $(n)$}\\
        \hline
        \input{./tables/appendix_table_perm_error.tex}
    \end{tabular}
\end{table}
\begin{table}
    \caption{$R^2$ scores  for the encodings learned in the Action/Temporal
    sparsity datasets and the Temporal Causal3DIdent datasets. We report the
    mean and standard deviation over $50$ random seeds and in each column 
    we write the best method in \textbf{bold}. If a score was below 
    $-100$, we indicate this with $\dagger$.}
    \label{tbl:r2-dms}
    \centering 
    \setlength\tabcolsep{2pt}
    \small
    \begin{tabular}{m{2cm}m{1.5cm}|cccccc}
        \toprule
        & &  
        \multicolumn{6}{c}{\small $R^2$-score on the diagonal $\uparrow$ $(n)$}\\
        \hline
        \input{./tables/appendix_table_r2.tex}
    \end{tabular}
\end{table}

\begin{figure}[!ht]
    \def\incFigure#1#2{%
        \node{\includegraphics[
            scale=1,
            trim={#2cm 0 0 0},
            clip
        ]{./tables/#1/times.pdf}};
        }
    \centerfloat
    \begin{tikzpicture}
        \matrix[row sep=0cm, column sep=0.2cm] (plotmatrix) {
            \incFigure{dms-vae}{0}&
            \incFigure{citris}{0.2}&\\
        };

        \node[above right=0cm and 0.2cm of plotmatrix.north, anchor=south]
            {\includegraphics[%
                scale=1, 
                trim={0.2cm 3.5cm 0 0},
                clip
            ]{./tables/citris/legend.pdf}};
    \end{tikzpicture}
    \caption{Execution times of the baseline and multiple versions of our
        estimator on the causal variables and encodings learned based on the
        Action/Temporal Sparsity Dataset and the Temporal Causal3DIdent dataset}
    \label{fig:times-both}
\end{figure}
\phantom{hey}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
