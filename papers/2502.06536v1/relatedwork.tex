\section{Related Work}
Extracting high-level concepts from the inner workings of machine learning
models has gained traction over the last years \citep{alain2017understanding,
ghorbani2019towards, mcgrath2021acquisition}. These concepts can be used to
create interpretable explanations. In Concept Based Models (CBMs), the concepts
are hard coded in the structure of the model \citep{koh2020conceptbottleneck,
marconato2022glance, ismail2023concept, zarlenga2022concept}. The explanations
can be constructed by looking at the activations for a prediction of the
components where the concepts are hard coded. The benefit of this approach is
that the concept is faithfully represented in the model and explanation. One of
the caveats is that in the case of concept leakage
\cite{margeloiu2021conceptbottleneckmodelslearn,mahinpei2021promisespitfallsblackboxconcept},
which can happen even with concept-level supervision, the concepts might be
learned incorrectly. 

Most current mitigating strategies for concept leakage are heuristic, e.g.\
encouraging sparsity \cite{10.5555/3327757.3327875} or orthonormality of the
concepts \cite{orthonormal-concepts}. Our work takes inspiration from
\cite{marconato2022glance} who leverage disentangled representation learning to
learn an embedding of ground truth concepts that can be then aligned with the
concept labels to ensure no concept leakage in CBMs. This work assumes that the
concepts are  independent in the dataset, but
\citet{marconato2023humanintepretable} consider a simplified version of causal
representation learning (CRL) methods \cite{scholkopf2021crl}, which allows
dependent concepts. They require interventions on the data generating process
to align the learned embeddings and concepts, for which they require
annotations. In contrast to our frameworl, these works do not provide
guarantees about correctness of the learned concepts or the number of labels. 

We draw heavily on the field of Causal Representation Learning (CRL)
\citep{scholkopf2021crl}. This area assumes that there is an underlying
unobserved causal system with causal variables and for each state of this
system, we can only observe an entangled measurement or observation of the
variables, which we say is produced by a \emph{mixing function}. The task is
then to recover the latent causal variables from the observation by learning an
\emph{unmixing} function. It can be shown that recovering these latent
variables is only possible up to a permutation and transformation, which is
typically element-wise for each variable. Many CRL methods exist with different
assumptions on the available data, e.g.\ the availability of interventional,
counterfactual or temporal data, or parametric assumptions on the underlying
system and mixing function, e.g.\ (non) Gaussianity of the causal variables or
(piecewise) linearity of the mixing function, and different types of
identifiability guarantees, e.g.\
\citep{hyvarinen2019nonlinear,khemakhem2020vaeica,
KugelgenSGBSBL21,lachapelle2022dms, LippeMLACG22, ahujaMWB2023,
lachapelle2024nonparametric, YaoXLMTMKL24} and many others. Our work is
agnostic to which CRL method one is using, and instead focuses on the
downstream task of aligning concepts efficiently to the causal representations
based on general identifiability guarantees.