@inproceedings{10.5555/3327757.3327875,
author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
title = {Towards robust interpretability with self-explaining neural networks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7786–7795},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{KugelgenSGBSBL21,
  author       = {Julius von K{\"{u}}gelgen and
                  Yash Sharma and
                  Luigi Gresele and
                  Wieland Brendel and
                  Bernhard Sch{\"{o}}lkopf and
                  Michel Besserve and
                  Francesco Locatello},
  title        = {Self-Supervised Learning with Data Augmentations Provably Isolates
                  Content from Style},
  booktitle    = {Advances in Neural Information Processing Systems, {NeurIPS}},
  year         = {2021},
}

@inproceedings{LippeMLACG22,
  author       = {Phillip Lippe and
                  Sara Magliacane and
                  Sindy L{\"{o}}we and
                  Yuki M. Asano and
                  Taco Cohen and
                  Stratis Gavves},
  title        = {{CITRIS:} Causal Identifiability from Temporal Intervened Sequences},
  booktitle    = {International Conference on Machine Learning, 
                  {ICML}},
  series       = {Proceedings of Machine Learning Research},
  publisher    = {{PMLR}},
  year         = {2022},
}

@inproceedings{YaoXLMTMKL24,
  author       = {Dingling Yao and
                  Danru Xu and
                  S{\'{e}}bastien Lachapelle and
                  Sara Magliacane and
                  Perouz Taslakian and
                  Georg Martius and
                  Julius von K{\"{u}}gelgen and
                  Francesco Locatello},
  title        = {Multi-View Causal Representation Learning with Partial Observability},
  booktitle    = {International Conference on Learning Representations,
                  {ICLR}},
  year         = {2024},
}

@inproceedings{ahujaMWB2023,
  title     = {Interventional causal representation learning},
  author    = {Ahuja, Kartik and 
               Mahajan, Divyat and 
               Wang, Yixin and 
               Bengio, Yoshua},
  booktitle = {International Conference on Machine Learning, {ICML}},
  year      = {2023},
  series    = {Proceedings of Machine Learning Research},
  organization={PMLR}
}

@inproceedings{alain2017understanding,
	title        = {Understanding intermediate layers using linear classifier probes},
	author       = {Guillaume Alain and
				    Yoshua Bengio},
	year         = {2017},
	booktitle    = {International Conference on Learning Representations, {ICLR}},
}

@article{ghorbani2019towards,
	title        = {Towards automatic concept-based explanations},
	author       = {Ghorbani, Amirata and
				    Wexler, James and
				    Zou, James Y and
				    Kim, Been},
	year         = {2019},
	journal      = {Advances in Neural Information Processing Systems, {NeurIPS}},
}

@inproceedings{hyvarinen2019nonlinear,
  author       = {Aapo Hyv{\"{a}}rinen and
                  Hiroaki Sasaki and
                  Richard E. Turner},
  title        = {Nonlinear {ICA} Using Auxiliary Variables and Generalized
                  Contrastive Learning},
  booktitle    = {International Conference on Artificial Intelligence and 
                  Statistics, {AISTATS}},
  series       = {Proceedings of Machine Learning Research},
  publisher    = {{PMLR}},
  year         = {2019}
}

@inproceedings{ismail2023concept,
	title        = {Concept Bottleneck Generative Models},
	author       = {Ismail, Aya Abdelsalam and
				    Adebayo, Julius and
				    Bravo, Hector Corrada and
				    Ra, Stephen and
				    Cho, Kyunghyun},
	year         = {2023},
	booktitle    = {International Conference on Learning Representations, {ICLR}}
}

@inproceedings{khemakhem2020vaeica,
  author       = {Ilyes Khemakhem and
                  Diederik P. Kingma and
                  Ricardo Pio Monti and
                  Aapo Hyv{\"{a}}rinen},
  title        = {Variational Autoencoders and Nonlinear {ICA:} {A}
                  Unifying Framework},
  booktitle    = {International Conference on Artificial Intelligence and
                  Statistics, {AISTATS} },
  series       = {Proceedings of Machine Learning Research},
  publisher    = {{PMLR}},
  year         = {2020},
}

@inproceedings{koh2020conceptbottleneck,
	title        = {Concept Bottleneck Models},
	author       = {Koh, Pang Wei and
				    Nguyen, Thao and
				    Tang, Yew Siang and
				    Mussmann, Stephen and
				    Pierson, Emma and
				    Kim, Been and
				    Liang, Percy},
	year         = {2020},
	booktitle    = {International Conference on Machine Learning, {ICML}},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
}

@article{lachapelle2024nonparametric,
  title     = {Nonparametric partial disentanglement via mechanism sparsity:
               Sparse actions, interventions and sparse temporal dependencies},
  author    = {Lachapelle, S{\'e}bastien and 
               L{\'o}pez, Pau Rodr{\'\i}guez and 
               Sharma, Yash and 
               Everett, Katie and 
               Priol, R{\'e}mi Le and 
               Lacoste, Alexandre and 
               Lacoste-Julien, Simon},
  journal   = {arXiv preprint arXiv:2401.04890},
  year      = {2024}
}

@misc{mahinpei2021promisespitfallsblackboxconcept,
      title={Promises and Pitfalls of Black-Box Concept Learning Models}, 
      author={Anita Mahinpei and Justin Clark and Isaac Lage and Finale Doshi-Velez and Weiwei Pan},
      year={2021},
      eprint={2106.13314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.13314}, 
}

@inproceedings{marconato2022glance,
	title        = {GlanceNets: Interpretable, Leak-proof Concept-based Models},
	author       = {Emanuele Marconato and 
                    Andrea Passerini and 
                    Stefano Teso},
	year         = {2022},
	booktitle    = {Neural Information Processing Systems, {NeurIPS}}
}

@article{marconato2023humanintepretable,
    title        = {Interpretability is in the Mind of the Beholder: {A} Causal
                    Framework for Human-interpretable Representation Learning},
	author       = {Emanuele Marconato and
				    Andrea Passerini and
				    Stefano Teso},
	year         = {2023},
	journal      = {Entropy},
    publisher    = {MDPI}
}

@misc{margeloiu2021conceptbottleneckmodelslearn,
      title={Do Concept Bottleneck Models Learn as Intended?}, 
      author={Andrei Margeloiu and Matthew Ashman and Umang Bhatt and Yanzhi Chen and Mateja Jamnik and Adrian Weller},
      year={2021},
      eprint={2105.04289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.04289}, 
}

@article{mcgrath2021acquisition,
	title        = {Acquisition of Chess Knowledge in AlphaZero},
	author       = {McGrath, Thomas and
				    Kapishnikov, Andrei and
				    Toma{\v{s}}ev, Nenad and
				    Pearce, Adam and
				    Hassabis, Demis and
				    Kim, Been and
				    Paquet, Ulrich and
				    Kramnik, Vladimir},
	year         = {2022},
	journal      = {Proceedings of the National Academy of Sciences, {PNAS}},
    publisher    = {National Academy of Sciences}
}

@article{orthonormal-concepts,
	abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can be misleading, unusable or rely on the latent space to possess properties that it may not have. Here, rather than attempting to analyse a neural network post hoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a convolutional neural network, the latent space is whitened (that is, decorrelated and normalized) and the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us with a much clearer understanding of how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens), the latent space. CW can be used in any layer of the network without hurting predictive performance.},
	author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
	date = {2020/12/01},
	date-added = {2025-01-30 14:55:28 +0100},
	date-modified = {2025-01-30 14:55:28 +0100},
	doi = {10.1038/s42256-020-00265-z},
	id = {Chen2020},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {12},
	pages = {772--782},
	title = {Concept whitening for interpretable image recognition},
	url = {https://doi.org/10.1038/s42256-020-00265-z},
	volume = {2},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-020-00265-z}}

@article{scholkopf2021crl,
	title        = {Toward Causal Representation Learning},
	author       = {Sch{\"o}lkopf, Bernhard and
				    Locatello, Francesco and
				    Bauer, Stefan and
				    Ke, Nan Rosemary and
				    Kalchbrenner, Nal and
				    Goyal, Anirudh and
				    Bengio, Yoshua},
	year         = {2021},
	journal      = {Proceedings of the IEEE}
}

@inproceedings{zarlenga2022concept,
  title     = {Concept embedding models: beyond the accuracy-explainability
               trade-off},
  author    = {Zarlenga, Mateo Espinosa and 
               Barbiero, Pietro and 
               Ciravegna, Gabriele and 
               Marra, Giuseppe and 
               Giannini, Francesco and
               Diligenti, Michelangelo and 
               Shams, Zohreh and 
               Precioso, Frederic and 
               Melacci, Stefano and 
               Weller, Adrian and 
               others},
  booktitle = {Advances in Neural Information Processing Systems, {NeurIPS}},
  year={2022}
}

