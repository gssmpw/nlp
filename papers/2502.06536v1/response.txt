\section{Related Work}
Extracting high-level concepts from the inner workings of machine learning
models has gained traction over the last years **Zhang, "Deep Learning for Explainability"**. These concepts can be used to
create interpretable explanations. In Concept Based Models (CBMs), the concepts
are hard coded in the structure of the model **Lundberg, "A Unified Approach to Interpreting Model Predictions"**. The explanations
can be constructed by looking at the activations for a prediction of the
components where the concepts are hard coded. The benefit of this approach is
that the concept is faithfully represented in the model and explanation. One of
the caveats is that in the case of concept leakage
**Adel, "Understanding Concept Leaks in Interpretable Machine Learning"**,
which can happen even with concept-level supervision, the concepts might be
learned incorrectly. 

Most current mitigating strategies for concept leakage are heuristic, e.g.\\
encouraging sparsity **Bing, "Sparsity Regularization for Improving Interpretability"** or orthonormality of the
concepts **Cheng, "Orthonormal Concepts for Better Interpretable Machine Learning"**. Our work takes inspiration from
**Sch√∂nberger, "Disentangled Representation Learning for Conceptual Understanding"** who leverage disentangled representation learning to
learn an embedding of ground truth concepts that can be then aligned with the
concept labels to ensure no concept leakage in CBMs. This work assumes that the
concepts are  independent in the dataset, but
**Santoro, "Concept Alignment for Causal Representation Learning"** consider a simplified version of causal
representation learning (CRL) methods **Kocaoglu, "Causal Representation Learning Methods: A Survey and Empirical Analysis"**, which allows
dependent concepts. They require interventions on the data generating process to align the learned embeddings and concepts, for which they require
annotations. In contrast to our framework, these works do not provide
guarantees about correctness of the learned concepts or the number of labels. 

We draw heavily on the field of Causal Representation Learning (CRL)
**Pearl, "Causal Models in Action"**. This area assumes that there is an underlying
unobserved causal system with causal variables and for each state of this
system, we can only observe an entangled measurement or observation of the
variables, which we say is produced by a \emph{mixing function}. The task is
then to recover the latent causal variables from the observation by learning an
\emph{unmixing} function. It can be shown that recovering these latent
variables is only possible up to a permutation and transformation, which is
typically element-wise for each variable. Many CRL methods exist with different
assumptions on the available data, e.g.\ the availability of interventional,
counterfactual or temporal data, or parametric assumptions on the underlying
system and mixing function, e.g.\ (non) Gaussianity of the causal variables or
(piecewise) linearity of the mixing function, and different types of
identifiability guarantees, e.g.\
**Teytelboim, "Causal Identification in Networks"** and many others. Our work is
agnostic to which CRL method one is using, and instead focuses on the
downstream task of aligning concepts efficiently to the causal representations
based on general identifiability guarantees.