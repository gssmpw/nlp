\section{Introduction}
In the classical secretary problem, $n$ numbers are presented to an online decision-maker (``player'') in random order. Upon seeing each number, the player has to make an irrevocable decision on whether to accept it---if the player accepts, the game terminates immediately; otherwise, the player moves on to the next element in the sequence. The goal of the player is to maximize the chosen number. It is well-known that there is a strategy for the player that chooses the largest element with probability at least $1/e$ for any value of $n$, which is essentially optimal.

The $k$-secretary problem is a natural extension of the above---instead of choosing a single element, the player is now allowed to accept up to $k$ elements. For this problem, Kleinberg~\cite{Kleinberg05} gave a $(1 - O(1/\sqrt{k}))$-\emph{competitive} algorithm. Here, an algorithm is $\alpha$-competitive if the expected sum of the accepted elements is at least an $\alpha$-fraction of the maximum possible result---the sum of the $k$ largest numbers. It was also shown that the competitive ratio of any algorithm is at best $1 - \Omega(1/\sqrt{k})$.

In this work, we revisit the $k$-secretary problem under an additional memory constraint on the player's algorithm. One potential use case of the $k$-secretary model is the routing setting~\cite{nakano2011message}, in which $n$ data packets arrive at a router in a sequential order, while the goal is to filter out certain low-quality packets and only forward $k$ out of the $n$ packets for downstream processing. In this setup, as the values of both $n$ and $k$ can be huge compared to the storage of the router, we need the algorithm to be both optimal (in terms of the competitive ratio) and memory-efficient (using a memory sublinear, or even logarithmic, in $k$).

Unfortunately, Kleinberg's algorithm, while being optimal, is not memory-efficient. A straightforward implementation of the algorithm requires $\Omega(k)$ memory. The bottleneck of the memory usage is for finding the $k$-th largest element in the sequence---a problem known as \emph{quantile estimation}---for which the straightforward method requires $\Omega(k)$ space.

One might hope to improve the memory bound using quantile estimation algorithms that are more space-efficient. We first note that there exist strong $\poly(k)$ memory lower bounds if we restrict ourselves to \emph{exact} algorithms that find the $k$-th largest element exactly (with high probability): a lower bound of Munro and Paterson~\cite{MP80} suggests that this requires $\Omega(\sqrt{k})$ space. Naturally, one might hope to use algorithms that finds the $k$-th largest element approximately (e.g., returning an element with rank $k \pm o(k)$). Guha and McGregor~\cite{guha2009stream} gave an algorithm that uses $O(1)$ words of memory\footnote{We assume a word size of $\Theta(\log n)$.} and finds an element of rank $k \pm O(\sqrt{k}\cdot \log^2n)$. Combining this result with Kleinberg's algorithm, however, would lead to extra $\polylog(n)$ factors in the competitive ratio.

In this work, we aim to answer the following two questions:
\begin{itemize}
    \item \textbf{Question 1.} In general, does a quantile estimator lead to a memory-efficient $k$-secretary algorithm? How would the quantile estimation error translate into the competitive ratio?
    \item \textbf{Question 2.} Concretely, to match the optimal competitive ratio for $k$-secretary, how much memory is needed? 
\end{itemize}

\subsection{Problem Setup}
\paragraph{Notations.} Throughout the paper, $s = (s_1, s_2, \ldots, s_n)$ denotes a random-order sequence, and we shorthand $s_{i_1:i_2}$ for the subsequence $(s_{i_1}, s_{i_1 + 1}, \ldots, s_{i_2})$. When it is clear from the context, we sometimes abuse the notation and let $s_{i_1:i_2}$ denote the set formed by the elements in the subsequence. For instance, $s_{i_1:i_2} \cap (-\infty, a)$ is used as a shorthand for $\{x \in \{s_{i_1}, s_{i_1 + 1}, \ldots, s_{i_2}\}: x < a\}$.

We will frequently use the \emph{rank} of an element $x$ within a contiguous subsequence, when only the elements that are strictly smaller than a threshold $x'$ are counted.
\begin{definition}[Rank]\label{def:rank}
For sequence $(s_1, s_2, \ldots, s_n)$ and $1 \le i_1 \le i_2 \le n$, we define
\[
    \rank_{i_1:i_2}(x; x')\coloneqq  \left|\left\{t\in \{s_{i_1}, \ldots, s_{i_2}\}| x' > t \ge x\right\}\right|.
\]
We also write $\rank_{i_1:i_2}(x)$ as a shorthand for $\rank_{i_1:i_2}(x; +\infty)$.
\end{definition}

\paragraph{The $k$-Secretary problem.}
We formally define the $k$-secretary problem as follows.
\begin{problem}[$k$-Secretary]\label{def.ksecretary}
    Let $x_1 > x_2 > \cdots > x_n \ge 0$ be $n$ numbers that are non-negative, distinct, and unknown to the algorithm. The algorithm reads a uniformly random permutation of the $n$ elements one by one. Upon seeing each element, the algorithm decides whether to accept it. The algorithm may accept at most $k$ elements, and aims to maximize their sum.
\end{problem}

The assumption that all the elements are distinct is for the simplicity of the exposition. It comes without loss of generality by a standard tie-breaking argument.

We say that a $k$-secretary algorithm is competitive, if it is guaranteed to secure a certain fraction of the maximum possible outcome, namely, the sum of the $k$ largest elements.

\begin{definition}[Competitive ratio]\label{def.cr}
    A $k$-secretary algorithm is $\alpha$-competitive if, on any $k$-secretary instance $x_1 > x_2 > \cdots > x_n$, the expected sum of the accepted elements is at least $\alpha \cdot \sum_{i=1}^{k}x_k$. Here, the expectation is over the randomness both in the permutation and in the algorithm.
\end{definition}

\paragraph{Quantile estimation.} Next, we formally define quantile estimation in a random-order stream.

\begin{problem}[Quantile estimation]\label{def.quantile_estimation}
Let $x_1 > x_2 > \cdots > x_n$ be $n$ distinct numbers that are unknown to the algorithm. The algorithm takes $n$ and $k$ as inputs, and reads a uniformly random permutation of the $n$ numbers one by one. The goal is to output the (approximately) $k$-th largest element $x_k$. When the output is $x^* \in \{x_1, x_2, \ldots, x_n\}$, the algorithm incurs an error of  $|\rank_{1:n}(x^*) - k|$.
\end{problem}

Again, the assumption that the $n$ elements are distinct is for simplifying the notations. The results for distinct elements can be extended to the general case via tie-breaking and a more careful definition of the error.

\paragraph{Memory model.} We informally introduce the model of computation and the memory usage of an algorithm. All our algorithms can be implemented in the word RAM model, assuming that each word can store either an integer of magnitude $\poly(n)$ or an element in the stream (in either $k$-secretary or quantile estimation). In particular, when all stream elements are bounded by $\poly(n)$, a word size of $\Theta(\log n)$ is sufficient. Moreover, all our algorithms are comparison-based: they only access the stream elements via pairwise comparisons, and do not perform any arithmetic operations on the elements. Therefore, a word size of $\Theta(\log n)$ would suffice if: (1) every stream element is presented as a unique identifier (which takes $O(\log n)$ bits); (2) the algorithm has access to an oracle that compares two elements specified by their identifiers.

\subsection{Our Results}
\paragraph{$k$-secretary problem.}
Our main result addresses Question~2: a logarithmic memory is sufficient for being optimally-competitive in the $k$-secretary problem.
\begin{theorem}\label{thm:k-secretary}
    There is a $k$-secretary algorithm that uses $O(\log k)$ memory and achieves a competitive ratio of $1 - O(1/\sqrt{k})$.
\end{theorem}

\paragraph{Reduction from $k$-secretary to quantile estimation.} Towards proving \Cref{thm:k-secretary}, we address Question~1: A quantile estimation algorithm leads to a competitive $k$-secretary algorithm with almost the same memory usage. Formally, the reduction applies to all \emph{comparison-based} quantile estimators. Roughly speaking, an algorithm is comparison-based if it only access the elements in the stream via pairwise comparisons. As a result, the output of a comparison-based algorithm is invariant (up to the renaming of elements) under any order-preserving (i.e., monotone) transformations. See \Cref{def:comparison-based} for a more formal definition.

\begin{restatable}{proposition}{reduction}\label{prop:reduction}
    Suppose that, for some $\alpha \in [1/2, 1]$, there is a comparison-based quantile estimation algorithm with memory usage $m$ and an error of $O(k^{\alpha})$ in expectation. Then, there is a $k$-secretary algorithm that uses $m + O(1)$ memory and achieves a competitive ratio of $1 -O(1/k^{1 - \alpha})$.
\end{restatable}

The proposition follows from a reduction that is essentially implicit in \cite{Kleinberg05}---Kleinberg's algorithm can be viewed as a special case in which the quantile estimator is exactly correct (and thus, we can take $\alpha = 1/2$). Our proof of \Cref{prop:reduction} shows that this reduction is actually robust to the inaccuracy in the quantile estimator, so the error bound in quantile estimation can be smoothly translated to the sub-optimality in $k$-secretary.

As we explain in \Cref{sec:overview-reduction}, directly following the reduction of~\cite{Kleinberg05} would increase the memory usage by a factor of $\log k$. We avoid this multiplicative increase by carefully modifying the reduction, so that the $k$-secretary algorithm would run $\log k$ copies of the quantile estimator \emph{sequentially}, rather than in parallel.

\paragraph{New results for quantile estimation.} In light of \Cref{prop:reduction}, the only missing piece towards proving \Cref{thm:k-secretary} is a memory-efficient quantile estimator with $O(\sqrt{k})$ error.

\begin{restatable}{theorem}{quantileapprox}\label{thm:quantile-approx}
    There is a quantile estimation algorithm that uses $O(\log k)$ memory and incurs an $O(\sqrt{k})$ error in expectation, over the randomness in both the algorithm and the order of the stream.
\end{restatable}

We note that both the memory usage and the error bound are independent of $n$. This is because we address the \emph{expected} error directly, rather than conditioning on a good event involving the entire length-$n$ stream, thus avoiding a potential $\polylog(n)$ dependence.

The key idea behind the algorithm for \Cref{thm:quantile-approx} is to reduce the problem of finding the $k$-th largest element to finding the $k'$-th largest in a subsequence, for some $k' \ll k$. By shrinking the value of $k$ fast enough, we ensure that the error incurred in the subproblems can be controlled. In \Cref{sec:overview-approx,sec:overview-details}, we sketch the algorithm and its analysis in more detail.

Our second algorithm finds the $k$-th largest element exactly with high probability, albeit with a looser memory bound of $O(\sqrt{k})$.

\begin{restatable}{theorem}{quantileexact}\label{thm:quantile-exact}
    For any $m \ge 1$, there is a quantile estimation algorithm that uses $O(m)$ words of memory and incurs zero error (i.e., returns the exactly $k$-th largest element) with probability at least
    \[
        1 - 12\lfloor\log_2k\rfloor\cdot\exp\left(-\frac{m}{12}\right) - 2\sum_{i=0}^{\lfloor\log_2k\rfloor-1}\exp\left(- \frac{m^2}{32(k/2^i)}\right).
    \]
\end{restatable}

In particular, setting $m = O(\sqrt{k\log(1/\delta)} + \log(1/\delta))$ is sufficient for succeeding with probability $\ge 1 - \delta$.

\Cref{thm:quantile-exact} follows from the technique of \cite{MP80}: the algorithm maintains $m$ consecutive elements among the stream that has been observed so far, in the hope that the $k$-th largest element is among these $m$ elements at the end. \cite{MP80} showed that this strategy finds the median of a length-$n$ random-order stream (i.e., the $k = n/2$ case) using $O(\sqrt{n})$ memory, and proved a matching $\Omega(\sqrt{n})$ lower bound. Our result extends the result of~\cite{MP80} to general $k$. In \Cref{sec:overview-exact}, we introduce the technique of~\cite{MP80} in more detail, and sketch our strategy for handling biased quantiles (i.e., the $k \ll n$ case).

\subsection{Related Work}
\paragraph{The secretary problem and its variants.} 
The classical secretary problem is often attributed to~\cite{dynkin1963optimum}, though the exact origin of the problem is obscure~\cite{freeman1983secretary,Ferguson1989}; different versions of the problem were studied in the contemporary work of~\cite{Lindley61,CMRS64,GM66}. 

The natural extension of selecting up to $k$ elements (i.e., the $k$-secretary problem) were studied by~\cite{GM66,AMW01,Kleinberg05,BIKK07} under various objectives (e.g., the probability of selecting the $k$ largest elements exactly, or the largest element being among the chosen ones). We followed the formulation of~\cite{Kleinberg05} in terms of the competitive ratio. \cite{Kleinberg05} showed that the optimal competitive ratio is $1 - \Theta(1/\sqrt{k})$ as $k \to \infty$. \cite{BIKK07} subsequently gave $1/e$-competitive algorithms for all $k \ge 1$, improving the result of~\cite{Kleinberg05} in the small-$k$ regime. A more recent work of~\cite{AL21} also focused on the non-asymptotic regime, and gave a deterministic algorithm with a competitive ratio $> 1/e$ for all $k \ge 2$.

A further extension of the $k$-secretary problem considers selecting multiple elements subject to a more general combinatorial constraint. \cite{BIKK07} studied the knapsack secretary problem. \cite{BIK07,BIKK18} introduced the \emph{matroid secretary problem}, in which the player is asked to select elements that form an independent set of a given matroid. This problem has been extensively studied~\cite{chakraborty2012improved,soto2013matroid,jaillet2013advances,dinitz2014matroid,lachish2014log, feldman2014simple,huynh2020matroid,soto2021strong,AKKO23}, and it remains a long-standing open problem whether an $O(1)$-competitive algorithm exists in general. Other variants of the secretary problem consider alternative models for how the player accesses the elements and makes decisions, including models of interview costs~\cite{bartoszynski1978secretary}, shortlists~\cite{ASS19}, reservation costs~\cite{burjons2021secretary}, and a ``pen testing'' variant~\cite{QV23,ganesh2023combinatorial}.

\paragraph{Quantile estimation.} For the quantile estimation problem in random-order streams, \cite{MP80} gave an exact selection algorithm for the $k = \lfloor n/2\rfloor$ case (i.e., finding the median) with $O(\sqrt{n})$ memory, and proved a matching $\Omega(\sqrt{n})$ memory lower bound. Our \Cref{thm:quantile-exact} generalizes their result to general values of $k$, and their lower bound implies that the $O(\sqrt{k})$ memory usage cannot be improved in general (specifically, in the $n = 2k$ case).

For approximate selection, \cite{guha2009stream}  proposed an algorithm that uses $O(1)$ words of memory and finds the $k$-th largest element up to an error of $O(\sqrt{k}\log^2 n\cdot\log(1/\delta))$ with probability $1 - \delta$. In comparison, \Cref{thm:quantile-approx} bounds the expected error and avoids the extra $\polylog(n)$ factor, both of which are crucial for the optimality of the resulting $k$-secretary algorithm. A subsequent work of~\cite{MV12} solved median estimation with an $n^{1/3 + o(1)}$ error using $O(1)$ words of memory. In comparison, \Cref{thm:quantile-approx} applies to all values of $k$ and makes the error dependent only on $k$ and not $n$, at the cost of a larger exponent of $1/2$ and a logarithmic memory usage. We note that, even if we could improve the error to $O(k^{1/3})$, the reduction from $k$-secretary still introduces an $\Omega(\sqrt{k})$ error, which would dominate the quantile estimation error.

While we focus on the random-order setup of quantile estimation, many prior work also addressed the more challenging setup in which elements arrive in an arbitrary order~\cite{MP80,manku1998approximate,KLL16,masson2019ddsketch,gupta2024optimal}. The multi-pass setting, in which the algorithm may scan the stream multiple times, were also considered~\cite{MP80,guha2009stream}.

\paragraph{Learning and decision making under memory constraints.} More broadly, our work is part of the endeavor to understand the role of memory in learning, prediction, and decision-making. Prior work along this line studied the memory bounds for parity learning in a streaming setting~\cite{valiant2016information,steinhardt2016memory,kol2017time,raz2018fast, garg2018extractor}, for the experts problem in online learning~\cite{srinivas2022memory,PZ23,peng2023near}, as well as the fundamental problem of linear regression~\cite{sharan2019memory, marsden2022efficient, blanchard2023memory, blanchard2024gradient}.