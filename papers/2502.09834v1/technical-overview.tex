\section{Proof Overview}
In this section, we give high-level sketches of our proofs and highlight some of the technical challenges. We recommend the readers to read this section before delving into the formal proofs in the appendix, as the technicalities might obscure some of the simple intuitions behind our algorithms.

We start by giving a simple quantile estimation algorithm in \Cref{sec:overview-warmup}. This algorithm uses $O(m)$ memory and has an $O(k / \sqrt{m})$ expected error. While this error bound is insufficient for our purposes, the simple algorithm serves as a starting point for our actual algorithms.

We introduce our algorithm for \Cref{thm:quantile-approx} in \Cref{sec:overview-approx}. The key idea is to consider a sub-problem obtained by filtering out the elements that are above a certain threshold, which reduces the scale of the problem rapidly enough. In \Cref{sec:overview-details}, we discuss a few technical challenges in turning an idealized analysis of the algorithm into a formal proof.

In \Cref{sec:overview-exact}, we sketch the proof of \Cref{thm:quantile-exact}, which is based on a different idea of~\cite{MP80}. The algorithm maintains a block of $m$ consecutive elements in the stream that has been observed so far. Whenever a new element comes, the algorithm tries to ``drift'' the length-$m$ block, in the hope that the $k$-th largest element is always within the block. \cite{MP80} framed this as a random walk problem, for which we give a new solution for general $k$ and in the $m = \Omega(\sqrt{k})$ regime.

We end the section by sketching the proof of \Cref{prop:reduction} (in \Cref{sec:overview-reduction}), which reduces the $k$-secretary problem to quantile estimation using a variant of the algorithm of~\cite{Kleinberg05}.

\subsection{Warm-up: Quantile Estimation via Subsampling}\label{sec:overview-warmup}
For quantile estimation, what can we do if we only have one word of memory? A simple idea is to output the largest element among a small subset subsampled at random: We start with $x^* = -\infty$. For each $i \in [n]$, we update $x^* \gets \max\{x^*, s_i\}$ with probability $p \coloneqq 1/k$. What can we say about the rank of the final outcome $x^*$? Let $x_i$ denote the $i$-th largest element in the stream. Then, $x^*$ is given by $x_{i^*}$, where $i^*$ is the smallest index such that we update $x^*$ when reading $x_{i^*}$. Note that $i^*$ is almost distributed as the geometric distribution $\Geo(p)$,\footnote{Except that a sample $> n$ corresponds to an ill-defined $i^*$.} with a mean of $1/p = k$ and a variance of $(1 - p)/p^2 \le k^2$. Thus, we have
\[
    \Ex{}{|\rank_{1:n}(x^*) - k|}
\approx \Ex{X \sim \Geo(p)}{|X - k|}
\le k.
\]

This approach can be easily extended to larger values of $m$. Instead of sampling at rate $1/k$, we subsample a $p \coloneqq m/k$ fraction of the elements,\footnote{If $m > k$, the straightforward algorithm solves the problem using $O(k) = O(m)$ memory.} and let $x^*$ be the $m$-th largest element among the subsampled elements. Then, the rank of $x^*$ can be written as $i^*_m$, where $i^*_1, i^*_2, \ldots, i^*_m$ are the $m$ smallest indices among $\{i \in [n]: x_i\text{ is in the subsample}\}$. Again, we note that
\[
    i^*_1, i^*_2 - i^*_1, i^*_3 - i^*_2, \ldots, i^*_m - i^*_{m-1}
\]
are almost independently distributed as $\Geo(p) = \Geo(m/k)$, with mean $k/m$ and variance $\le k^2/m^2$. It follows that $i^*_m$ concentrates around $m\cdot (k/m) = k$ up to an expected error of $\sqrt{m\cdot k^2/m^2} = k/\sqrt{m}$. This $k/\sqrt{m}$ error bound is insufficient for our purposes: To match the optimal competitive ratio of $1 - O(1/\sqrt{k})$, assuming \Cref{prop:reduction}, we need an $O(\sqrt{k})$ error for quantile estimation. This in turn requires a linear memory usage of $m = \Theta(k)$.

What could we have done better? We note that the analysis above never uses the assumption that the elements arrive in a random order! Indeed, instead of subsampling a $p$-fraction of the $n$ elements, we could have sampled $B \sim \Binomial(n, p)$ and output the $m$-th largest element among $s_1, s_2, \ldots, s_B$. This is because, over the randomness in both $B$ and the random ordering, $\{s_1, s_2, \ldots, s_B\}$ is identically distributed as a subsample of $\{s_1, s_2, \ldots, s_n\}$ at rate $p$ thanks to the random-order assumption. In other words, the algorithm above could have been implemented such that it only reads a short prefix of length $\approx n\cdot(m/k)$. If we could make better use of the remaining sequence, we can hopefully bring the error smaller.

\subsection{An Approximate Algorithm via Iterative Conditioning}\label{sec:overview-approx}
Now, we introduce our algorithm for \Cref{thm:quantile-approx}. The algorithm is a recursive one: we repeatedly reduce the problem of finding the $k$-th largest element to finding the $k'$-th largest (for some $k' \ll k$) in a subsequence of the stream. Before presenting the actual algorithm, we start by explaining why a na\"ive attempt fails, after which the key idea of our algorithm becomes more transparent.

\paragraph{A na\"ive reduction.} How should we solve quantile estimation for a specific value of $k$, if we already have an algorithm (denoted by $\A_{k/2}$) that solves the $k' = k/2$ case with an $O(\sqrt{k'}) = O(\sqrt{k})$ error? The answer is simple: we run $\A_{k/2}$ on the first $n/2$ elements of the sequence and return its answer. Assuming that the output of $\A_{k/2}$, denoted by $x^*$, is exactly the $(k/2)$-th largest element among $s_{1:(n/2)}$, a simple concentration argument would show that the rank of $x^*$ among $s_{1:n}$ is roughly $k \pm O(\sqrt{k})$. Now, let $l \coloneqq \rank_{1:(n/2)}(x^*)$ denote the actual rank of $x^*$ among the first half of the sequence. Our assumption on $\A_{k/2}$ implies that $l \approx k/2 \pm O(\sqrt{k})$, which further gives
\[
    \rank_{1:n}(x^*)
\approx 2l \pm O(\sqrt{l})
\approx k \pm O(\sqrt{k}).
\]

At first glance, the above seems to suggest an extremely simple solution to quantile estimation: we repeated apply the reduction above, and reduce the value of ``$k$'' to $k/2, k/4, \ldots, 1$. When $k = 1$, we can find the largest element exactly using $O(1)$ memory. The argument above would then imply that we incur an error of $O(\sqrt{k})$ in expectation.

However, the resulting algorithm would be essentially the same as the one in \Cref{sec:overview-warmup} for $m = 1$, since it simply finds the largest one among the first $\approx n/k$ elements. The resulting error is then shown to be $\Omega(k)$. What goes wrong here? The issue is that we ignored the blow-up in the error during the reduction. Define random variables $X_1, X_2, X_4, \ldots, X_{k/2}, X_k$ such that $X_{k'}$ denotes the error in the rank when we solve the instance with $k = k'$. Our discussion from the last paragraph shows that, for some universal constant $C > 0$, $(X_k)$ satisfies the dynamics
\begin{equation}\label{eq:dynamics-naive}
    X_1 = 0,
\quad
    X_k \approx 2X_{k/2} + C\sqrt{k}.
\end{equation}
Expanding the above gives
\[
    X_k
\approx C\sqrt{k} + 2C\sqrt{k/2} + 4C\sqrt{k/4} + \cdots + (k/2)\cdot C\sqrt{2}
=   \Theta(k),
\]
since the error incurred at $k' = O(1)$ would be doubled roughly $\log_2 k$ times through the iteration $X_k \approx 2X_{k/2} \pm C\sqrt{k}$, resulting in the dominant term of $\Omega(k)$ in the error.

\paragraph{The actual reduction.} We would get an $O(\sqrt{k})$ error bound if we could shrink the value of $k$ faster. Imagine that, instead of reducing to an instance with $k' = k/2$, we can reduce to an instance with $k' = k / 100$, while the error still propagates in the same way. Then, the recursion in \Cref{eq:dynamics-naive} would be replaced with $X_k \approx 2X_{k/100} + C\sqrt{k}$, which leads to
\[
    X_k \approx C\sqrt{k} + 2C\sqrt{k/100} + 2^2 C\sqrt{k/100^2} + \cdots + 2^{\log_{100} k}C\sqrt{1}
=   O(\sqrt{k}).
\]

Therefore, the key is to reduce the scale of the problem (measured by parameter $k$) at a slightly faster pace. Our algorithm does this in a fairly simple way. First, we divide the length-$n$ sequence $s$ into two halves $s_{1:B}$ and $s_{(B+1):n}$ for some $B \approx n / 2$. Then, we subsample a random $p$-fraction of the first half, where $p = \Theta(m/k)$. As discussed in \Cref{sec:overview-warmup}, this can be accomplished by drawing $B_1 \sim \Binomial(B, p)$ and taking the first $B_1$ elements. We find the top $m$ elements among $s_{1:B_1}$, denoted by $M[1], M[2], \ldots, M[m]$, using $O(m)$ memory (in the straightforward way). After reading the remaining $B - B_1$ elements in $s_{1:B}$, we can compute the rank $\rank_{1:B}(M[i])$ for every $i \in [m]$. Then, we find an index $i^*$ such that
\[
    \rank_{1:B}(M[i^*])
<   k
<   \rank_{1:B}(M[i^* + 1]).
\]
Let $a' \coloneqq M[i^*]$ and $k' \coloneqq k/2 - \rank_{1:B}(a')$. We know that the $(k/2)$-th largest element among $s_{1:B}$ is simply the $k'$-th largest element among $s_{1:B} \cap (-\infty, a')$. Therefore, we recursively find the $k'$-th largest element among $s_{(B+1):n} \cap (-\infty, a')$, in the hope that it will be a good approximation for the $k$-th largest element overall.

To make this work, we need the parameter $k'$ for the recursive call to be smaller than $k$ be a sufficiently large factor. This is indeed the case, since the gaps in the ranks of $M[1], M[2], \ldots, M[m]$ among $s_{1:B}$ roughly follow the geometric distribution $\Geo(p)$, so each of them has an expectation of $\le 1/p = O(k/m)$. By setting $m = \Omega(\log k)$, we can ensure that every gap is smaller than $C_0 \cdot k$ (for any small constant $C_0$), with probability $1 - 1/\poly(k)$.

The reduction sketched above actually reduces quantile estimation to a slightly different problem: instead of finding the $k$-th largest element among $s_{1:n}$, we find the $k$-th largest among $s_{1:n} \cap (-\infty, a')$. This does not pose a problem as the reduction works for this generalized version as well.

\subsection{A Few Technical Details}\label{sec:overview-details}

It should be noted that, while the intuition behind the algorithm is simple, it turns out to be highly non-trivial to state and analyze the algorithm rigorously.

\paragraph{Handling edge-cases.} Formally, the recursive algorithm takes parameters $n$, $k$, and $a$ as inputs, and aims to find the $k$-th largest element among $s_{1:n} \cap (-\infty, a)$, where $s_{1:n}$ is the upcoming random-order sequence of length $n$. The quantile estimation instance is invalid if $k > n' \coloneqq |s_{1:n} \cap (-\infty, a)|$. While the outermost call to the algorithm (where $a = +\infty$) is always valid, the algorithm might eventually lead to a recursive call in which the parameter $k$ becomes invalid. In this case, our algorithm always returns $-\infty$ as the answer. Then, in the previous level of recursion---which makes this invalid call---the algorithm translates this $-\infty$ to the smallest element in the stream.

The second edge case is that we fail to find a good choice of $a'$ that significantly reduces the value of $k$. This can, in turn, happen for three different reasons: (1) $|s_{1:B} \cap (-\infty, a)| < k/2$, in which the first half of the sequence does not have a $(k/2)$-th largest element; (2) the $(k/2)$-th largest element exists, but none of the elements with ranks in $[k/2 - C_0\cdot k, k/2]$ get subsampled into $s_{1:B_1}$, so none of them is present in the array $M$; (3) Some element with rank in $[k/2 - C_0\cdot k, k/2]$ gets subsampled into $s_{1:B_1}$, but more than $m$ elements with ranks in $[1, k/2]$ get subsampled, so that the one with a rank closest to $k/2$ is not kept in $M$.

For the latter two cases, we show that for some appropriate choice of $m = \Omega(\log k)$, their total probability is at most $1/\poly(k)$. Thus, if we detect that either of them happens, we can choose to return the largest element, which leads to a rank error of $k-1$ and becomes negligible after multiplying with the $1/\poly(k)$ probability. The first case is trickier since it \emph{can} happen with a constant probability.\footnote{Consider the case that $k \approx n' \coloneqq |s_{1:n} \cap (-\infty, a)|$, in which case $|s_{1:B} \cap (-\infty, a)|$ roughly follows $\Binomial(n', 1/2)$, and can be below $k/2$ with probability $\approx 1/2$.} In that case, we return the smallest element (with a rank error of $n' - k$), and it turns out that its contribution to the overall error is still under control.

\paragraph{Towards a rigorous analysis.} In addition to the need of handling the edge cases outlined above, the analysis of our algorithm is necessarily complicated due to the complex dependence between different parts of the algorithm.

For simplicity, we assume that the algorithm proceeds in a ``typical'' way, i.e., none of the edge cases happen. Furthermore, we analyze the outermost level of recursion, where the threshold parameter is $a = +\infty$ (i.e., no element gets ignored). Then, roughly speaking, our algorithm has the following three steps:
\begin{enumerate}
    \item[\textbf{Step 1.}] Compute a threshold $a' \in s_{1:B}$ and let $k' \coloneqq k/2 - \rank_{1:B}(a')$.
    \item[\textbf{Step 2.}] Run the algorithm recursively on $s_{(B+1):n}$ with parameters $n' = n - B$, $k'$ and $a'$.
    \item[\textbf{Step 3.}] Return the output $x^*$ of the recursive call as the answer.
\end{enumerate}

Then, the straightforward approach to analyzing the above would be an inductive one: Let $l \coloneqq \rank_{(B+1):n}(x^*; a')$ denote the actual rank of $x^*$ among $s_{(B+1):n} \cap (-\infty, a')$. By the inductive hypothesis, $|l - k'| = O(\sqrt{k'})$ in expectation (denoted by $l \approx k' \pm O(\sqrt{k'})$ informally). If $B$ is sampled from $\Binomial(n, 1/2)$, $\{s_1, s_2, \ldots, s_B\}$ would be uniformly distributed among all subsets of $s_{1:n}$. Then, a concentration argument suggests
\begin{equation}\label{eq:overview-concentration}
    \rank_{1:B}(x^*) \approx (k/2 - k') + l \pm O(\sqrt{l})
\quad\text{and}\quad
    \rank_{(B+1):n}(x^*) \approx (k/2 - k') + l \pm O(\sqrt{k}).
\end{equation}
In total, we would obtain $\rank_{1:n}(x^*) = \rank_{1:B}(x^*) + \rank_{(B+1):n}(x^*) \approx k \pm O(\sqrt{k})$.

Unfortunately, the analysis above is technically incorrect. We were analyzing the \emph{conditional} concentration of $\rank_{1:B}(x^*)$ and $\rank_{(B+1):n}(x^*)$ given the values of $a'$, $k'$ and $l$. Since $(a', k', l)$ is determined by the value of $B$ as well as ordering of $s$, after the conditioning, $s_{1:B}$ is no longer uniformly distributed, which invalidates the concentration argument.

Towards a rigorous analysis, we need to carefully untangle the randomness in the three steps above. Crucially, we note that our algorithm is comparison-based, i.e., the behavior of the algorithm is unchanged if we replace $s$ with a different sequence $s'$ with the same ordering as $s$. This motivates the following order in which we ``realize'' the randomness:
\begin{itemize}
    \item First, we sample $B \sim \Binomial(n, 1/2)$.
    \item Then, towards analyzing Step~1, we realize the \emph{relative ordering} of the elements $s_1, s_2, \ldots, s_B$ (out of the $B!$ possibilities). This ordering is sufficient for determining $i \coloneqq \rank_{1:B}(a')$, the rank of $a'$ among $s_{1:B}$ as well as the value of $k'$.
    \item We also realize the value of $i_1 \coloneqq \rank_{1:n}(a')$. Note that conditioning on $(B, i)$, $i_1$ is identically distributed as the $i$-th smallest element in a size-$B$ subset of $[n]$ chosen uniformly at random.
    \item After that, we realize the \emph{relative ordering} of the last $(n-B)$ elements $s_{(B+1):n}$ (out of the $(n-B)!$ possibilities). This, in turn, determines the rank of $x^*$ (the output of the recursive call) among $s_{(B+1):n} \cap (-\infty, a')$, denoted by $l \coloneqq \rank_{(B+1):n}(x^*; a')$.
    \item At this point, even after conditioning on the values of $(B, i, i_1, l)$, the subset $\{s_1, s_2, \ldots, s_B\}$ is still uniformly distributed among all size-$B$ subsets $S \subseteq \{s_1, s_2, \ldots, s_n\}$, subject to the constraint that the $i_1$-th largest element among $s_{1:n}$ (namely, $a'$) is the $i$-th largest element among $s_{1:B}$. 
\end{itemize}
The uniformity that we retain in the last step above allows us to rigorously prove bounds that are qualitatively similar to \Cref{eq:overview-concentration}.

\subsection{An Exact Algorithm via Maintaining Consecutive Elements}\label{sec:overview-exact}
We sketch our proof of~\Cref{thm:quantile-exact}, which is based on a very different algorithm. The key idea of the algorithm is to maintain \emph{consecutive} elements among the elements that have been observed so far, which was used by~\cite{MP80} to solve the median selection problem (i.e., the special case that $k = n / 2$).

\paragraph{The algorithm as a random walk (with limited control).} At any time $t$, after reading the first $t$ elements $s_1, s_2, \ldots, s_t$ in the random-order stream, the algorithm maintains $m$ consecutive elements out of them. Here, ``consecutive'' is with respect to the sorted order of the elements, rather than the order in which they arrive. Formally, suppose that the first $t$ elements of $s$, when sorted in decreasing order, are given by $x_1 > x_2 > \cdots > x_t$. The algorithm stores the elements $x_l, x_{l+1}, \ldots, x_r$ as well as the values of $l$ and $r$, where $r - l + 1 = m$.

Then, what happens when the next element $x' \coloneqq s_{t+1}$ arrives? By the random-order assumption, $x'$ is equally likely to fall into the $t+1$ intervals below:
\[
    (-\infty, x_t), (x_t, x_{t-1}), \ldots, (x_2, x_1), (x_1, +\infty).
\]
In particular, we have the three cases below:
\begin{enumerate}
    \item[\textbf{Case 1.}] $x' > x_l$. This happens with probability $l / (t + 1)$. In this case, we cannot add $x'$ to the array without breaking the invariant that the elements are consecutive. Therefore, we have to keep the $m$ stored elements unchanged, and the parameters $(l, r)$ become $(l', r') = (l + 1, r + 1)$ in the next step.
    \item[\textbf{Case 2.}] $x' < x_r$. Similarly, with probability $(t - r + 1) / (t + 1)$, the condition $x' < x_r$ holds. Again, we cannot update the $m$ elements, and the parameters in the next step remain unchanged, i.e., $(l', r') = (l, r)$.
    \item[\textbf{Case 3.}] $x' \in (x_r, x_l)$. The most interesting case is that $x'$ falls into one of the $r - l = m-1$ ``gaps'' among $x_l > x_{l+1} > \cdots > x_r$, which happens with probability $(m - 1) / (t + 1)$. In this case, we would obtain $m + 1$ consecutive elements among $s_{1:(t+1)}$. Then, we need to kick out one of the two elements at the ends. This gives us some freedom in deciding whether $(l', r') = (l, r)$ (by kicking out the smallest element) or $(l', r') = (l + 1, r + 1)$ (by kicking out the largest element).
\end{enumerate} 
At the end of the game, we successfully find the $k$-th largest element if $l \le k \le r$.

Following this strategy, the quantile estimation problem becomes a control problem: We start at time $t = m$ with $(l, r) = (1, m)$. At each of the following steps $t = m + 1, m + 2, \ldots, n$, the value of $(l, r)$ transitions according to the three cases above---either it transitions deterministically (in Cases 1~and~2), or the algorithm may specify whether $l$ and $r$ get incremented by $1$ in Case~3. The goal is to design our strategy in Case~3 (which may vary for different time step $t$ and the values of $(l, r)$ at that step), so that the probability of $k \in [l, r]$ is maximized at time $n$.

The above was exactly the idea in the work of~\cite{MP80}, who studied the special case of $k = n/2$. For that special case, their strategy for Case~3 is to choose $(l', r')$ such that $\left|\frac{l' + r'}{2} - \frac{t + 2}{2}\right|$ is minimized. Intuitively, this makes sure that the median of the stream that has been observed so far is as close to the center of the length-$m$ array as possible. They analyzed the resulting random walk, and showed that  a memory of $m = \Theta(\sqrt{n})$ is sufficient. For the general $k$ case, analyzing the random walk associated with the analogue of the algorithm of~\cite{MP80} becomes more difficult.\footnote{As~\cite{MP80} remarked in their work, this random walk is ``difficult to analyze exactly since the transition probabilities vary with [the value of $(l, r)$] and with time''.}

\paragraph{The actual algorithm.} Our algorithm behind \Cref{thm:quantile-exact} can be viewed as a \emph{staged} strategy for the control problem of~\cite{MP80}, which admits a slightly simpler analysis.

The algorithm is divided into $\approx \log_2 k$ stages. Roughly speaking, the goal of each stage~$i \in \{0, 1, \ldots, \log_2 k\}$ is to make sure that, after reading the first $(n/k)\cdot 2^i$ elements, we have $l \le 2^i \le r$. What happens when we go from stage~$i$ to stage~$i+1$? Suppose that, at the end of stage~$i$, the $2^i$-th largest element so far (denoted by $x^*$) is exactly in the middle of the array, i.e., $2^i = \frac{l + r}{2}$. As we read the additional elements in stage~$(i+1)$, we maintain the $m$ consecutive elements, such that $x^*$ is kept in the middle of the array.

By a concentration argument, after stage~$i+1$, the rank of $x^*$ is bounded between $2^{i+1} \pm O(\sqrt{2^i}) = 2^{i+1} \pm O(\sqrt{k})$. Then, as long as $m \gg \sqrt{k}$, the actual element with rank $2^{i+1}$, denoted by $\tilde x$, must be among the $m$ consecutive elements maintained by the algorithm. Then, our algorithm shortens the length-$m$ array that contains consecutive elements, so that $\tilde x$ becomes the center of the array. Here, we deviate from the plan outlined earlier, as the value of $r - l + 1$ can drop below $m$ from time to time. Fortunately, by another concentration argument, as we go from stage~$i+1$ to stage~$i+2$, we are going to ``absorb'' additional elements in to the array, so that the length of the array returns to $m$ before we shorten the array again.

The proof sketch above can be formalized into a high-probability guarantee, which states that, as long as $m = \Omega(\sqrt{k})$, the algorithm proceeds in the hoped-for manner except with a tiny probability.

\paragraph{Another perspective.} We remark that our algorithm can alternatively be viewed as a remedy of the ``na\"ive reduction'' from \Cref{sec:overview-approx}. In \Cref{eq:dynamics-naive}, by reducing to the $k' = k/2$ case, the error doubles and increases by an $O(\sqrt{k})$ amount, resulting in a trivial error bound of $O(k)$. In the algorithm outlined above, however, by utilizing the strategy of~\cite{MP80}, we can offset the error by $O(m)$ at each iteration. In particular, as long as $m \gg \sqrt{k}$, the errors accumulated in all stages can be canceled out. This leads to our exact selection guarantee.

\subsection{From Quantile Estimation to Secretary Problem}\label{sec:overview-reduction}
We sketch how an accurate quantile estimator leads to a competitive $k$-secretary algorithm via a reduction similar to the algorithm of~\cite{Kleinberg05}. Kleinberg's algorithm is a recursive one: to solve the $k$-secretary problem on a length-$n$ stream, we divide the stream into two halves, each of length $\approx n/2$. We run the algorithm recursively for the $(k/2)$-secretary instance formed by the first half. In parallel, we find the $(k/2)$-th largest element, denoted by $x^*$, among the first half. Then, we read the second half of the sequence, and accept every element that is larger than $x^*$, until at least $k/2$ elements among the second half have been accepted.

To gain some intuition about the $1 - O(1/\sqrt{k})$ competitive ratio, consider a $k$-secretary instance that consists of $k$ copies of $1$ and $n - k$ copies of $0$. To be exactly optimal, the algorithm needs to accept every single element of value $1$. When running Kleinberg's algorithm, however, it holds with probability $\Omega(1)$ that the second half of the stream only contains $k/2 - \Omega(\sqrt{k})$ copies of $1$, in which case we lose an $\Omega(1/\sqrt{k})$ term in the competitive ratio.

Towards proving \Cref{prop:reduction}, we apply Kleinberg's algorithm with the quantile estimation subroutine replaced by the (hypothetical) memory-efficient algorithm (denoted by $\A$) with an expected error of $O(k^{\alpha})$. Then, we revisit the instance considered before. For clarity, we break ties among the $k$ copies of $1$ by replacing them with $1 - \eps, 1 - 2\eps, \ldots, 1 - k\eps$ for some tiny value $\eps \ll 1/k$. Then, the first half of the sequence contains a random subset of $\{1 - i\eps: i \in [k]\}$. When we call the quantile estimator $\A$ to find the $k/2$-th largest element among the first half, it might hold with probability $\Omega(1)$ that $\A$ returns an element $x^* = 1 - i^*\eps$ for some $i^* = k - \Omega(k^{\alpha})$. In this case, the algorithm might reject $\Omega(k^{\alpha})$ non-zero elements among the second half by mistake, resulting in a competitive ratio of at best $1 - \Omega(1/k^{1-\alpha})$.

While the argument above suggests why \Cref{prop:reduction} gives the ``right'' dependence of the competitive ratio on $\alpha$, the actual proof is slightly more complicated, since we need to argue that the impact of the error in quantile estimator on the competitive ratio is smooth, so that an upper bound on the \emph{expected error} also translates into a competitive ratio (after considering the randomness in quantile estimator).

The reduction outlined above would not give the desired memory bound of $m + O(1)$ in \Cref{prop:reduction}. Kleinberg's algorithm involves running $\approx \log_2 k$ copies of the quantile estimator: roughly speaking, the $i$-th copy ($i \le \log_2 k$) is for finding the $k/2^i$-th largest element among $s_{1:(n/2^i)}$. Since these copies run on overlapping prefixes of the stream, they must run in parallel, and thus increasing the memory usage by a factor of $\log k$. The actual memory bound $m + O(1)$ is obtained from a slight modification to Kleinberg's algorithm: instead of finding the $(k/2)$-th largest element among $s_{1:(n/2)}$ and using it as the threshold $x^*$ for the second half, we choose $x^*$ as the $(k/4)$-th largest element among $s_{(n/4+1):(n/2)}$. This change only perturbs the rank of $x^*$ by $O(\sqrt{k})$ in expectation, and would be dominated by the $k^{\alpha}$ error in the quantile estimator $\A$. As a result, we would run $\log k$ copies of $\A$ on \emph{disjoint} intervals in $s$: $s_{(n/4+1):(n/2)}, s_{(n/8+1):(n/4)}, \ldots$, so it suffices to allocate $m$ words of memory for all the $\log k$ calls to $\A$, and we only use an $O(1)$ extra memory.