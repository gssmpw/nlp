\begin{table*}[ht]
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c}
Function                    & $(\vec{\sigma}, v)$   & Obs            & Architecture               & Batch      & $\eta$    & L.N. & Drop \% & $\lambda$     \\ \hline
Ex Ante                     & 10k                   & 100            & sr64x6;hlin                & 256        & 1e-3      & F      & 0.2        & 1e-3              \\
                            & 20k                   & 50             & sr256x6;hlin               & 512        & 1e-4      & F      & 0.5        & 1e-3              \\
                            & 40k                   & 25             & sr128x6;hlin               & 32         & 1e-4      & F      & 0.4        & 1e-3              \\
                            & 50k                   & 20             & sr128x6;hlin               & 128        & 1e-4      & F      & 0.4        & 1e-4             \\
                            & 100k                  & 10             & r128;r64;r32;hsr16;hlin    & 128        & 1e-4      & T       & 0.2        & 1e-3              \\
                            & 200k                  & 5              & r64;sr32;sr16;hlin         & 32         & 1e-4      & T       & 0          & 1e-2               \\ \hline
Interim                  & 10k                   & 100            & r256;sr128;sr64;hsr32;hlin & 128        & 1e-4      & F      & 0.3        & 0                  \\
                            & 20k                   & 50             & r256;sr128;sr64;hlin       & 128        & 1e-4      & T       & 0.3        & 0                  \\
                            & 40k                   & 25             & r128;sr64;sr32;hlin        & 256        & 1e-4      & T       & 0.1        & 1e-4              \\
                            & 50k                   & 20             & r64;r32;r16;hlin           & 512        & 1e-4      & T       & 0.1        & 1e-3               \\
                            & 100k                  & 10             & r128;sr64;sr32;hsr16;hlin  & 512        & 1e-4      & T       & 0.1        & 1e-4              \\
                            & 200k                  & 5              & r256;sr128;sr64;hlin       & 512        & 1e-4      & F      & 0.2        & 0                  
\end{tabular}
\caption{Summary of hyperparameters identified as optimal during hyperparameter tuning validation, for each function and training set.
Note that "r" denotes a ReLU layer, the number indicates the nodes in that layer, and "{r128}x6" represents six 128-node ReLU layers.
"h" denotes a strategy head, "s" indicates a skip connection from the input layer, and "lin" represents a linear layer.
Additionally, "Batch" refers to batch size, $\eta$ to learning rate, "L.N." to whether layer normalization was applied after each dense layer, "Drop \%" to the dropout percent after each dense layer, and $\lambda$ to the regularization strength.}
\label{table:game1_hyperparameters}
\end{table*}


