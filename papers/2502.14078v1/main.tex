\documentclass[format=acmsmall,nonacm]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
\usepackage{xcolor} 
\usepackage{soul}
\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{multirow}         % if you need multi-row cells

\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Choose a citation style by commenting/uncommenting the appropriate line:
%\setcitestyle{acmnumeric}
\setcitestyle{authoryear}

\author{Madelyn Gatchel}
\affiliation{
  \institution{University of Michigan}
  \city{Ann Arbor}
  \state{MI}
  \country{USA}
}
\email{gatchel@umich.edu}

\author{Michael P. Wellman}
\affiliation{
  \institution{University of Michigan}
  \city{Ann Arbor}
  \state{MI}
  \country{USA}
}
\email{wellman@umich.edu}

% Title. Note the optional short title for running heads. In the interest of anonymization, please do not include any acknowledgements.
\title{Learning Bayesian Game Families, with Application to Mechanism Design}

% Abstract. Note that this must come before \maketitle.
\begin{abstract}
Learning or estimating game models from data typically entails inducing separate models for each setting, even if the games are parametrically related.
In empirical mechanism design, for example, this approach requires learning a new game model for each candidate setting of the mechanism parameter. 
Recent work has shown the data efficiency benefits of learning a single parameterized model for families of related games. 
In \textit{Bayesian games}---a typical model for mechanism design---payoffs depend on both the actions and types of the players.
We show how to exploit this structure by learning an \textit{interim} game-family model that conditions on a single player’s type. 
We compare this approach to the baseline approach of directly learning the \textit{ex ante} payoff function, which gives payoffs in expectation of all player types. 
By marginalizing over player type, the interim model can also provide ex ante payoff predictions. 
This dual capability not only facilitates Bayes-Nash equilibrium approximation, but also enables new types of analysis using the conditional model. 
We validate our method through a case study of a dynamic sponsored search auction. 
In our experiments, the interim model more reliably approximates equilibria than the ex ante model and exhibits effective parameter extrapolation.
With local search over the parameter space, the learned game-family model can be used for mechanism design.
Finally, without any additional sample data, we leverage the interim model to compute piecewise best-response strategies and refine our model to incorporate these strategies, enabling an iterative approach to empirical mechanism design. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\dotproduct}{\kern-.06em \boldsymbol{\cdot} \kern-.06em}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\idx}[2]{#1^{(#2)}}
\newcommand{\bidx}[2]{\boldsymbol{#1}^{\textbf{(#2)}}}
\newcommand{\bvidx}[2]{\boldsymbol{\vec{#1}_*}^{\textbf{(#2)}}} % currently only used with \sigma_*
\newcommand{\vidx}[2]{\vec{#1}^{(#2)}}
\newcommand{\textitbf}[1]{\textit{\textbf{#1}}}
\newcommand{\vecsp}[1]{\vec{#1}\mkern2mu}
\newcommand{\vecspsp}[1]{\vec{#1}\mkern5mu}
\newcommand{\term}[1]{\textbf{\textit{#1}}}
\newcommand{\nmix}{m}
\newcommand{\nobs}{o}
\newcommand{\nsimq}{Q}
\newcommand{\val}{\theta}
\newcommand{\reserve}{r}
\newcommand{\reserveVar}{R}
\newcommand{\partition}{\mathcal{C}}
\newcommand{\interval}{C}
\newcommand{\typesamp}{\mathcal{N}}
\newcommand{\BNE}{\mathit{BNE}}
\newcommand{\pbr}{\phi}
\newcommand{\oppidx}{x}
\newcommand{\test}{\mathit{test}}
\DeclareMathOperator*{\argmax}{\arg\,max}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}



% Title page for title and abstract only.
\begin{titlepage}
\maketitle
\makeatletter \gdef\@ACM@checkaffil{} \makeatother
\end{titlepage}

% \makeatletter
% \fancyfoot[RO,LE]{} % Remove reference footer on both right and left
% \makeatother

% % % % % % % % %
% INTRODUCTION  %
% % % % % % % % %

\section{Introduction}

Many real-world strategic interactions can be modeled as \term{Bayesian games}, where payoffs depend on players’ actions as well as their private information, or \term{types}. 
Outcomes may also depend on parameters of the environment, such that each parameter setting induces a different Bayesian game. 
Given limited modeling resources, analysts must decide in advance the range of parameter settings to consider and the granularity of that range. 
While domain knowledge of the interaction may guide this selection, there is no guarantee that the most salient parameter settings are covered. 
What the analyst would ideally have is a model of the entire \term{Bayesian game family}, from which they could reason about any relevant parameter setting.

A motivating application for Bayesian game families is \term{mechanism design}, where a 
designer sets or influences an environment parameter that affects strategic incentives in the multi-agent interaction. 
Each value of this parameter results in a different \term{game instance}.
The mechanism designer's goal is to find the parameter setting that optimizes a relevant objective function, such as social welfare or revenue. 
In \term{empirical mechanism design} (EMD), game model instances are induced from simulation data. 
In past EMD studies \cite{vorobeychik2006empirical, jordan2010strategy, brinkman2017empirical}, researchers selected a limited set of mechanism settings, separately modeling and analyzing each game instance.

\citet{gatchel2023learning} demonstrated that learning a single parameterized payoff model for families of related normal-form games is more data-efficient than training separate models for each game instance. 
We extend this approach to Bayesian game families, exploiting the type-conditional form of strategies for these games.
Specifically, we investigate the learning of \term{interim payoff functions}, which explicitly condition on a single player's type.
By marginalizing out this type, we obtain the \term{ex ante payoff functions}, which are essentially the payoffs learned in a normal-form model. 
We also explore learning ex ante payoff functions directly, and compare this approach with that of learning interim models.

We validate our method through an EMD case study in the domain of sponsored search, where the publisher sets an auction reserve requirement in order to maximize revenue in equilibrium.
Our search auction model is designed to capture the dynamic nature of bidding, where advertisers can revise their bids based on provisional results of earlier bidding rounds. 
We do so in a two-stage scenario, in which the bidders can \emph{attempt} to modify their bids given the state of bidding after the first round.
These attempts succeed probabilistically, thus providing an incentive for the players to submit meaningful first-round bids.
The scenario is simple to describe and design heuristic strategies, yet too complex for straightforward analytic solution.
We implement an agent-based simulation model of the scenario, and from the simulation-generated data learn Bayesian game family models to support empirical game-theoretic analysis and mechanism design.

In our experiments, both ex ante and interim models achieve low payoff error across the trained parameter range. 
The interim model is able to maintain low error well beyond the trained range.
Moreover, we find that equilibria approximated using the interim model consistently have equal or lower regret absolute error compared to ex ante, both within and beyond the trained parameter range.
These improvements in extrapolation and equilibrium identification provide compelling evidence that exploiting type structure by learning an interim model is advantageous for Bayesian games.

In our application to mechanism design, we demonstrate that the learned models support effective EMD procedures. 
Analysis of a fine-grained grid over the game family reveals the benefit of learning multiple models from separate datasets, and further demonstrates the relative robustness of interim over ex ante.
Local search methods reliably produce approximately optimal reserve settings, requiring only a modest number of restarts.

A final feature of the interim model is that it enables us to generate new strategies that outperform those in the original set.
Specifically, we introduce \textit{piecewise-conditional} strategies that select from the original strategies based on the bidder's own type, and we show how to construct piecewise strategies that beneficially respond to equilibria over the original strategy set.
We can further integrate these strategies into an expanded model trained without requiring any additional simulation samples.
These operations enable an iterative procedure that expands the game family model from an initial set of atomic strategies through a double oracle \citep{mcmahan03} approach: repeated generation of new (piecewise) strategies that best-respond to an equilibrium of the previous configuration.
By choosing an equilibrium from the game instance that optimizes the design parameter, this becomes an iterative method for EMD.
In applying this method to the dynamic search auction, we demonstrate that even a couple of iterations refines the model to produce decisions that improve revenue.

Our investigation produces new insights about alternative model forms for Bayesian game families, with compelling experimental evidence in favor of learning interim payoff functions.
The interim model enables generation and integration of new strategies, based on optimal piecewise constructions.
Overall, the methods developed here support an automated approach to empirical mechanism design, given an agent-based simulator and a seed set of strategies. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % %
% Related Work  %
% % % % % % % % %
\section{Related Work}

% % % % % % % % % % % % % % % % % %
% Learning Game Models from Data  %
% % % % % % % % % % % % % % % % % %
\subsection{Learning Game Models from Data}

\term{Empirical game-theoretic analysis} (EGTA) \cite{wellman2024empirical} is a framework which includes modeling complex, multi-agent interactions using data from an agent-based simulator and using game-theoretic analysis to approximate equilibria in the resulting \term{empirical game} (also called a simulation-based game \cite{wellman2020economic} or a black-box game \cite{zhang2021finding, li2021evolution}).
Given limited sampling resources, it is infeasible to estimate payoffs for every strategy profile individually from simulation data.
An alternative is to learn game models that generalize from the available data, for example using regression to learn pure-strategy payoff functions  \cite{vorobeychik2007learning, wiedenbeck2018regression} or inducing compact game structures \cite{ficici2008learning, duong2009learning, li2020structure, liu2023nfgtransformer}.
\citet{sokota2019learning} train a neural network to learn the \term{deviation payoff function}---the expected utility for a unilateral deviator---for symmetric normal-form game instances.
This technique leverages the compactness of payoff functions in games with symmetric players.
The learned deviation payoff function supports equilibrium computation without the combinatorial mixture summations required for direct payoff functions.
\citet{li2021evolution} learn deviation payoffs as part of a method to approximate mixed-strategy Bayes-Nash equilibria through iterative evolutionary search. 
\citet{gatchel2023learning} learn the deviation payoff function for families of related symmetric normal-form game instances, showing that this approach achieves higher payoff accuracy with less data than learning separate models for each game instance. 

% % % % % % % % % % % %
% Applications of EMD %
% % % % % % % % % % % %
\subsection{Applications of EMD}

\citet{vorobeychik2006empirical} study the effectiveness of storage costs in deterring large initial procurement in a supply chain management scenario.
They analyze five game instances with different storage costs, and find that none effectively deter large procurement without sacrificing profitability. 
\citet{jordan2010strategy} examine how reserve scores impact publisher revenue in online advertising auctions, analyzing fourteen game instances and identifying the revenue-maximizing reserve score. 
Finally, \citet{brinkman2017empirical} investigate the optimal clearing interval in a frequent call market to maximize allocative efficiency as other environment parameters, such as the number of agents and trade opportunities, are varied. 

% % % % % % % % % % 
% Methods for EMD %
% % % % % % % % % %
\subsection{Methods for EMD}

\citet{vorobeychik2012constrained} present an automated mechanism design procedure, framing EMD as a black-box optimization problem compatible with any evaluation method.
Evaluating the objective function for a candidate mechanism setting is a subproblem that involves finding an approximate equilibrium in the induced game and computing the objective value in equilibrium. 
\citet{viqueira2020empirical} introduce a PAC-learning framework for EMD that includes approximate equilibrium estimation and parameter search with Bayesian optimization.
\citet{gemp2022designing} directly learn the equilibrium objective function in empirical all-pay auctions or crowdsourcing contests, bypassing player utility learning.
Their approach depends on direct equilibrium approximation in multiple game instances to generate the training set, which may pose challenges in a large empirical game family with a limited sampling budget. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % %
% Background  %
% % % % % % % %
\section{Background}

% % % % % % % % % %
% Bayesian Games  %
% % % % % % % % % %
\subsection{Bayesian Games}

A \term{symmetric Bayesian game} $\Gamma$ has $p$ indistinguishable players who may play actions $a$ from common  action set $A$ and whose types $t\in T$ are independent and identically distributed according to probability distribution $\mu\in\Delta(T)$.
In this work, we assume that $A$ is finite and that type space $T$ is infinite and ordered.
We define the utility function as $U: A^p\times T^p \to \mathbb{R}$, where the payoff for a player playing action $a$ with type $t$ while opponents with types $\vec{t}$ play actions $\vec{a}$, is denoted as $U\big((a, \vecsp{a}), \; (t, \vecspsp{t})\big).$ 
Vectors $\vec{t}$ and $\vec{a}$ have length $p-1$, have a consistent order,  and specify the corresponding types and actions of the $p-1$ opponents.
To improve readability, we omit the inner parentheses and write $U(a, \vec{a}, t, \vecspsp{t})$. 

Let $S$ be the set of pure strategies, each strategy $s\in S$ a function $s: T \to A$.
We assume $S$ is finite, thus a strict (typically highly restricted) subset of all mappings from types to actions.
An \term{opponent profile} is a length-$(p-1)$ vector $\vec{s}$ that specifies each opponent's strategy, where the order of entries matches the order of opponents in the type vector $\vec{t}$.
We assume access to an oracle (e.g., a simulator) that gives a noisy payoff estimate for a player with type $t$ who plays strategy~$s_j$, given $(\vec{s}, \vecspsp{t})$: 
$\tilde{U}\big(s_j(t), \vecsp{s}(\vecspsp{t}), t, \vecspsp{t}\big).$
% As players are indistinguishable in a symmetric game, the only information needed to compute the pure payoff for a player playing strategy $s_j$ is an \term{opponent profile}, a length-$\abs{S}$ vector $\vec{s}$ where entry $\vec{s}_k$ denotes the number of players playing strategy $s_k$.
% The set of all opponent profiles is characterized by:
% \[ \vec{S} = \left\{ \vec{s} \in  \mathbb{Z}^{\abs{S}} \;:\; \vec{s}_j \geq 0,\, \textstyle\sum_j \vec{s}_j = p-1 \right\}. \]

Given player symmetry, we can define a utility function $u_j$ \textit{for each strategy} $s_j$ that maps opponent profiles to payoffs.
We first define the \term{ex ante expected payoff}, 
% $u_j: \vec{S} \to \mathbb{R}$, where 
\begin{equation}u_j(\vecsp{s}) = \mathbb{E}_{(t, \vecspsp{t})} \Big[U\big(s_j(t), \vecsp{s}(\vecspsp{t}), \; t, \vecspsp{t} \big) \Big],\label{eq:ex_ante_exp_util}
\end{equation} %\quad \text{where } (t, \vec{t})\sim \mu^p\]
where $(t, \vecspsp{t})\sim \mu^p$. 
The \term{interim expected payoff} is given by
% defined as $u_j: \vec{S} \times T \to \mathbb{R}$, where
\begin{equation}u_j(\vecsp{s} \mid t) = \mathbb{E}_{\vec{t}} \Big[U\big(s_j(t), \vecsp{s}(\vecspsp{t}), t, \vecspsp{t} \big) \Big],\label{eq:ex_interim_exp_util}
\end{equation} %\qquad \text{where } \vec{t} \sim \mu^{p-1}\]
where $\vec{t} \sim \mu^{p-1}$.
Note that we can express ex ante as a marginalized version of interim: $u_j(\vecsp{s}) = \mathbb{E}_{t}u_j(\vecsp{s} \mid t)$.

We learn the deviation payoff function, a mapping from mixed-strategy profiles to vectors of expected utilities for unilateral strategy deviations.
Let $\sigma \in \Delta^{\abs{S}}$ denote a mixed strategy, where $\Delta^{\abs{S}}$ is the probability simplex over the strategy set $S$.
Further, let $\vec{\sigma}$ refer to a symmetric mixed-strategy profile where all $p$ (or $p-1$) players are playing according to~$\sigma$.
We define the \term{ex ante} and \term{interim deviation payoffs} for deviating to strategy $s_j$: 
\begin{align} u_j(\vec{\sigma}) &= \sum_{\vec{s} \in \vec{S}} \Pr(\vecsp{s} \mid \vec{\sigma}) u_j(\vecsp{s}), \label{eq:devpay_exante}\\
u_j(\vec{\sigma}\mid t) &= \sum_{\vec{s} \in \vec{S}} \Pr(\vecsp{s} \mid \vec{\sigma}) u_j(\vecsp{s} \mid t), \label{eq:devpay_exinterim}
\end{align}
where $\Pr(\vecsp{s}\mid\vec{\sigma})$ describes the probability that the opponent profile $\vec{s}$ would be drawn according to~$\vec{\sigma}$.
We can write the ex ante deviation payoff for strategy $s_j$ using the interim deviation payoff for strategy $s_j$:
\[u_j(\vec{\sigma}) = \mathbb{E}_{t\sim \mu} u_j(\vec{\sigma} \mid t) = \int_t u_j(\vec{\sigma}\mid t)d\mu(t).\]

The \term{ex ante deviation payoff function} is given by
% $u: \Delta^{\abs{S}}\to \mathbb{R}^{\abs{S}}$, i.e., 
the $\abs{S}$-dimensional vector $u(\vec{\sigma}) = [u_j(\vec{\sigma})]$, over $s_j \in S$.
Similarly, the \term{interim deviation payoff} function is 
% defined as $u: \Delta^{\abs{S}} \times T \to \mathbb{R}^{\abs{S}}$.%, i.e., the $\abs{S}$-dimensional vector 
$u(\vec{\sigma}\mid t)$.
% $ = [u_j(\vec{\sigma}\mid t)]$ for all $s_j \in S.$
We use this unsubscripted lowercase $u$ throughout to denote a vector of deviation payoffs.
% regardless of inputs or additional notational elements.}

We define \term{regret} using deviation payoffs: 
\begin{equation}\epsilon(\vec{\sigma}) = \max_j u_j(\vec{\sigma}) - \sigma \cdot u(\vec{\sigma}).\label{eq:regret}\end{equation} 
A \term{symmetric Bayes-Nash Equilibrium} (BNE) is a symmetric profile $\vec{\sigma}$ such that no player has an incentive to deviate; equivalently, $\epsilon(\vec{\sigma}) = 0.$ 
As simulation-based games are themselves approximations, we focus on finding $\varepsilon$-BNE, which are symmetric profiles $\vec{\sigma}$ such that $\epsilon(\vec{\sigma}) \leq \varepsilon$, where $\varepsilon$ is small.

% % % % % % % % % % % % % % % %
% Parameterized Game Families %
% % % % % % % % % % % % % % % %
\subsection{Parameterized Game Families}

Let $\Gamma(v)$ denote the symmetric Bayesian game instance where the environment parameter $V$ takes value $v$. 
A \term{parameterized game family}, $\mathcal{G}(V)$, is the set of game instances $\{\Gamma(v)\mid v\in V\}$.%
\footnote{For simplicity we focus on single-parameter game families; the generalization to multi-parameter is conceptually straightforward.}
In the game family, all payoff functions and functions that depend on payoffs (e.g., Equations~\ref{eq:ex_ante_exp_util}-\ref{eq:regret}) are parameterized by $V$.
For example, $u(\vec{\sigma}, V)$ represents the ex ante deviation payoff function in the game family, and $u(\vec{\sigma}, v)$ represents the ex ante deviation payoff function in game instance $\Gamma(v) \in \mathcal{G}(V).$
Because the regret of a mixed-strategy profile $\vec{\sigma}$ also depends on $V$, a mixed-strategy profile that is an $\varepsilon$-BNE in one game instance may have regret larger than $\varepsilon$ in another game instance.   
For a game family, payoff samples take the form $\tilde{U}\big(s_j(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % 
% Learning Bayesian Game Families %
% % % % % % % % % % % % % % % % % %
\section{Learning Bayesian Game Families}
\label{sec:learn_game_families}

Simulator queries are costly, and the results are noisy due to randomness in strategies or in the game environment. 
We assume a fixed budget of simulator queries for learning and validation, so we must allocate queries across the parameter space, strategy space, and type space. 
This simulation data is used to train a model representing the deviation payoff function for a symmetric Bayesian game family.
We experimentally compare ex ante and interim game-family learning methods. 
The ex ante method becomes equivalent to the normal-form approach developed by \citet{gatchel2023learning} once types are abstracted away, and thus serves as our baseline in the Bayesian setting.

% % % % % % % % % % % % % % %
% Ex Ante Deviation Payoffs %
% % % % % % % % % % % % % % % 
\subsection{Ex Ante Deviation Payoffs}
\label{sec:ex-ante}

We first train a neural network representing the ex ante deviation payoff function $\hat{u}: \Delta^{\abs{S}} \times V \to \mathbb{R}^{\abs{S}}$, 
where $\hat{u}_j(\vec{\sigma}, v)$ is the predicted payoff a symmetric player would receive by deviating to strategy $s_j$ when all other players play according to $\vec{\sigma}$ in game instance $\Gamma(v)$. 
Let $\nmix$ denote the number of $(\vec{\sigma}, v)$ pairs in our training set, let $\nobs$ denote the number of observations per pair, and let $\nsimq = \nmix \cdot \nobs$.
An \term{observation} for a given $(\vec{\sigma}, v)$ and deviation strategy $s_j$ involves sampling $\vec{s} \sim \vec{\sigma}$ and $(t, \vecspsp{t})\sim \mu^p$, and querying the simulator for
$\tilde{U}\big(s_j(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v).$
A training example is of the form $(\vec{\sigma}, v) \mapsto [\tilde{u}_j(\vec{\sigma}, v)]$, where $\tilde{u}_j(\vec{\sigma}, v)$ is the sample average of deviation payoffs across all observations for a given strategy $s_j$.
Specifically, the \term{ground truth deviation payoff} is given by $\tilde{u}_j(\vec{\sigma}, v) = \frac{1}{\nobs}\sum_{i=1}^\nobs \tilde{U}(s_j(\idx{t}{i}), \idx{\vecsp{s}}{i}(\idx{\vecspsp{t}}{i}), \idx{t}{i}, \idx{\vecspsp{t}}{i}, v),$ where $(\idx{t}{i}, \idx{\vecspsp{t}}{i}) \sim \mu^p$ and $\idx{\vecsp{s}}{i} \sim \vec{\sigma}$. 
We seek to minimize the training loss: 
$$\frac{1}{\nmix\cdot \abs{S}} \sum_{k=1}^m \sum_{s_j \in S} \Big(\tilde{u}_j(\vidx{\sigma}{k}, \idx{v}{k}) - \hat{u}_j(\vidx{\sigma}{k}, \idx{v}{k})  \Big)^2.$$

% % % % % % % % % % % % % % %
% Interim Deviation Payoffs %
% % % % % % % % % % % % % % %
\subsection{Interim Deviation Payoffs}

In Bayesian games, sampling over types represents a distinct source of noise in payoff estimates.
By conditioning estimates on the deviator's type, we can leverage type-specific information in each sample.  
We therefore propose learning the interim deviation payoff function, which is explicitly conditional on the symmetric deviator's type: $\hat{u}: \Delta^{\abs{S}} \times V \times T \to \mathbb{R}^{\abs{S}}$, where $\hat{u}(\vec{\sigma}, v \mid t)$ gives the predicted vector of conditional deviation payoffs, [$\hat{u}_j(\vec{\sigma}, v \mid t)$] for each $s_j \in S$ in game instance~$\Gamma(v)$.
An interim training example is a mapping $(\vec{\sigma}, v, \idx{t}{i}) \mapsto [\tilde{u}_j(\vec{\sigma}, v \mid \idx{t}{i})]$, where $\tilde{u}_j(\vec{\sigma}, v \mid \idx{t}{i})$ corresponds to the deviation payoff from a single observation.%
\footnote{In principle, we could define $\tilde{u}_j(\vec{\sigma}, v \mid \idx{t}{i})$ as the sample average of deviation payoffs over many $\vec{t}$. 
Since our goal is to compare ex ante and interim learning approaches, for fair comparison we use the same training data (i.e., underlying $\tilde{U}$ payoff samples) with a single $t$ and $\vec{t}$ sampled for each observation, which is the natural approach for ex ante learning. 
It is possible that the alternative interim ground truth approach might benefit interim learning specifically, though it is not obvious how many $\vec{t}$ samples per $t$ to employ, given the tradeoffs resulting from a fixed budget of simulator queries.}
We aim to minimize the training loss:
\[\frac{1}{\nsimq\abs{S}} \sum_{k=1}^\nmix \sum_{i=1}^\nobs \sum_{s_j \in S} \Big(\tilde{u}_j(\vidx{\sigma}{k}, \idx{v}{k} \mid \idx{t}{i}) - \hat{u}_j(\vidx{\sigma}{k}, \idx{v}{k} \mid \idx{t}{i})  \Big)^2.\]
We can compute ex ante deviation payoffs from the interim by marginalization: $\int_t \hat{u}_j(\vec{\sigma}, v \mid t) d\mu(t).$
In practice, this can be done via Monte Carlo integration by averaging deviation payoffs from the learned interim model over many sampled deviator types.
%In practice, this can be done via Monte Carlo integration by sampling deviation payoffs from the learned interim function across deviator types for each strategy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % % % %
% Using Learned Bayesian Game Families  %
% % % % % % % % % % % % % % % % % % % % %
\section{Using Learned Bayesian Game Families}

\subsection{Deriving Equilibria}
We use the learned game-family model to derive approximate equilibria in game instances corresponding to various settings of the parameter~$V$.
Our method adapts existing techniques for deviation payoffs \citep{gatchel2023learning,sokota2019learning} to the Bayesian case.
For game instance $\Gamma(v)$, we run an approximate-Nash-finding algorithm using deviation payoffs predicted by the model: $[\hat{u}_j(\vec{\sigma}, v)]$ (ex ante) or $[\int_t \hat{u}_j(\vec{\sigma}, v \mid t) d\mu(t)]$ (marginalized interim).
The output mixed-strategy profile is a \term{candidate $\epsilon$-equilibrium}, $\vec{\sigma}_*$, if the predicted regret is at most $\varepsilon$; otherwise, the algorithm did not converge. 
We validate the candidate with a modest number of additional simulator queries to compute the \term{true regret} of $\vec{\sigma}_*$ in $\Gamma(v)$. 
If the true regret is at most~$\varepsilon$, the approximate equilibrium is \term{confirmed}.
Running the Nash-finding algorithm from multiple starting points gives a set of solutions to which any equilibrium-selection method can be applied.  

% % % % % % % % % % % % % % % %
% Empirical Mechanism Design  %
% % % % % % % % % % % % % % % %
\subsection{Empirical Mechanism Design}
A motivating application of the learned Bayesian game-family model is EMD. 
Once trained, the game-family model can evaluate any game instance within the trained range (and plausibly beyond), supporting a more granular parameter search than previous EMD approaches. 
When used in an optimization algorithm, it eliminates the need to train separate models at each iteration, reducing the algorithm's dependence on the sampling budget. 
See App.~\ref{app:emd} for pseudocode and further explanation.

% % % % % % % % % % % % % % % % % % %
% Piecewise-Conditional Strategies  %
% % % % % % % % % % % % % % % % % % % 
\subsection{Piecewise-Conditional Strategies}
\label{sec:piecewise}

Analysis of Bayesian games typically focuses on identifying ex ante equilibria. 
In a BNE no player can gain by deviating to any other strategy in $S$, \textit{in expectation over player types}.
If $S$ includes all mappings from type to action, then in an ex ante equilibrium the player would also not wish to deviate conditional on its own type; that is, the ex ante BNE would also be an interim equilibrium.
Given a restricted set $S$, however---the norm for empirical game models---a player can often benefit by deviating to an alternative strategy in~$S$ once its own type is revealed.

We consider a particular form of higher-order strategy that exploits such opportunities by selecting a base-level  \term{atomic strategy} $s\in S$ conditional on revealed type~$t$.
Let $\partition$ be a set of contiguous type intervals, which collectively partition~$T$.
Specifically, a \term{piecewise strategy} is defined by a mapping $\partition\to S$, which selects an atomic strategy from the base set corresponding to the interval containing~$t$. 

\subsubsection{Computing Piecewise Best Responses}

% Suppose we could select a strategy $s\in S$ conditional 
% % define a new strategy that maps player type $t$ to its best-response strategy to the BNE, conditioned
% on having type~$t$.
From an ex ante perspective, how much additional payoff could a player gain through use of a piecewise strategy, with partition $\partition$, in a given strategic context (e.g., where other players play the BNE)? 
The interim deviation payoff function allows us to answer such questions directly.

We can compute an approximate version of the optimal deviation by deriving a piecewise best response to $(\vec{\sigma}, v)$.
% Let $\partition$ be a set of equiprobable, contiguous type intervals, which collectively partition~$T$.
First, we collect a large set of samples $\typesamp$ from the type distribution~$\mu(T)$.
Next, we compute the deviation payoff vector conditioned on the deviator having a type in interval $\interval\in\partition$:
\begin{equation*}
    \hat{u}(\vec{\sigma}, v \mid \interval) =  \frac{1}{\abs{\typesamp_C}}\sum_{\idx{t}{i}\in\typesamp_C} \hat{u}(\vec{\sigma}, v \mid \idx{t}{i}) 
    % \cdot \mathbb{I}(\idx{t}{i} \in \interval)
    ,
\end{equation*}
where 
% $\mathcal{N}$ is a pre-specified, large number of interim evaluations,  
$\typesamp_\interval = \typesamp\cap\interval$.
% \sum_{i=1}^{\mathcal{N}} \mathbb{I}(\idx{t}{i} \in \interval)$, and $\idx{t}{i} \sim \mu(T)$. 
The \term{piecewise best-response strategy}, denoted $\phi(t)$, assigns each type $t$ the best-response strategy for the interval $\interval$ containing $t$.
Given $\hat{u}(\vec{\sigma}, v \mid \interval)$ and $\typesamp_\interval$ for each interval $\interval$, the \term{predicted deviation payoff for playing the piecewise best response} against $\vec{\sigma}$ is:
\begin{equation}
    \hat{u}_{\pbr}(\vec{\sigma}, v) = \frac{1}{\abs{\typesamp}}\sum_{\interval\in\partition} \abs{\typesamp_\interval}\cdot \max_j \hat{u}_j(\vec{\sigma}, v \mid \interval).
    \label{eq:pred_pbr_payoff}
\end{equation}
For validation, the \term{true deviation payoff for playing the piecewise best response} against $\vec{\sigma}$ is: 
\begin{equation}
    \tilde{u}_{\pbr}(\vec{\sigma}, v) = \frac{1}{N}\sum_{\vec{s}\in\vec{S}}\Pr(\vec{s}\mid\vec{\sigma})\sum_{i=1}^N \tilde{U}(\pbr(\idx{t}{i}), \vec{s}(\idx{\vecsp{t}}{i}), \idx{t}{i}, \idx{\vecsp{t}}{i}, v), 
    \label{eq:true_pbr_payoff}
\end{equation}
where $N$ is the number of simulator queries for each $\vec{s}.$

\subsubsection{Iterative EMD with Piecewise Strategies}
\label{sec:iter_emd_w_pbr}

Let $S' = S \cup \{\pbr\}$, and let $\mathcal{G}'(V)$ be the game family with strategy set $S'$.
We wish to identify the equilibria of $\mathcal{G}'(V)$.
We can use the interim model, which was trained on $\mathcal{G}(V)$, to compute the deviation payoff vector in any $\Gamma'(v)$ for any mixed strategy whose support is a subset of $S$:

\begin{enumerate}
    \item Compute the marginalized interim deviation payoff vector, $[\int_t \hat{u}_j(\vec{\sigma}, v \mid t) d\mu(t)]$, using the learned model from $\mathcal{G}(V).$
    \item Compute the deviation payoff for playing the piecewise strategy, $\hat{u}_{\pbr}(\vec{\sigma}, v)$ (Equation~\ref{eq:pred_pbr_payoff}).
    \item Append $\hat{u}_{\pbr}(\vec{\sigma}, v)$ to form the length-$\abs{S'}$ deviation payoff vector. 
    \item Compute the predicted regret with respect to $\Gamma'(v).$
\end{enumerate}
% We can use the interim model, trained on $\mathcal{G}(V)$, to compute the length-$\abs{S'}$ deviation payoff vector in $\Gamma'(v)$ for any mixed strategy with support over the base set $S$: first compute the marginalized interim deviation payoff vector, $\hat{u}(\vec{\sigma}, v)$, and append the deviation payoff for playing the piecewise strategy, $\hat{u}_{\pbr}(\vec{\sigma}, v)$ (Equation~\ref{eq:pred_pbr_payoff}). 
% Using this deviation payoff vector, we can compute regret with respect to $\Gamma'(v)$.
If the predicted regret is less than $\varepsilon$, then the mixed strategy is a candidate equilibrium in $\Gamma'(v).$ 
However, since $\pbr$ is a piecewise best response to an equilibrium in $\Gamma(v)$, it is plausible that no mixed strategy with support in~$S$ is an equilibrium in $\Gamma'(v)$. 

%Let $\vec{\sigma}'$ be a mixed strategy where piecewise strategy $\pbr$ is played with positive probability. 
Now consider a mixed strategy $\vec{\sigma}'$ that assigns positive probability to $\pbr$. 
The learned model for $\mathcal{G}(V)$ cannot predict $\hat{u}_{j}(\vec{\sigma}', v)$ for $s_j \in S'$ when $\vec{\sigma}' \in \Delta^{\abs{S'}}$ includes $\pbr$ in the support because it does not preserve type-conditionality of the opponent players.
Whereas the interim model allows us to translate a piecewise strategy for the subject player into a combination of type-conditioned atomic strategies, we cannot perform such a translation for the opponents.
Thus, the current model does not support analysis of mixed strategies (or derivation of equilibria) that include piecewise strategies in the support. 

\paragraph{Extending the game model to incorporate $\pbr$}
%To fully incorporate $\pbr$, therefore, we need to extend the game model to cover~$S'$. 
We define a method for learning an interim model over $\mathcal{G}'(V)$, without running any additional simulator queries.
We do so by adapting the dataset used for training the original interim model over $\mathcal{G}(V)$.
% We employ the dataset used to train the interim model over $\mathcal{G}(V)$.
Recall that this dataset comprises $\nmix$ different sampled $(\vec{\sigma}, v)$ pairs and $\nobs$ observations per pair, where an observation includes:
\begin{itemize}
    \item An opponent profile $\vec{s} \sim \vec{\sigma}$ and types $(t, \vecsp{t}) \sim \mu^p$.
    \item A deviation payoff estimate $\tilde{u}_j(\vec{\sigma}, v \mid t) = \tilde{U}\big(s_j(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v)$ for each $s_j \in S$.
\end{itemize}
% Each observation of a given $(\vec{\sigma}, v)$ has a sampled $\vec{s} \sim \vec{\sigma}$ and $(t, \vec{t}) \sim \mu^p$, and associated deviation payoff estimate $\tilde{u}_j(\vec{\sigma}, v \mid \vecsp{t}) = \tilde{U}\big(s_j(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v)$ for each $s_j \in S$.
We adapt each such example to $\Gamma'(v)$ by: 
\begin{enumerate}
    \item Appending a 0 to $\vec{\sigma}$, denoting that $\pbr$ is played with probability 0. 
    \item Appending $\tilde{U}\big(\pbr(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v)$ to the existing vector of deviation payoffs.
\end{enumerate}
% We adapt each original training example from $\Gamma(v)$ to $\Gamma'(v)$ with two additions: appending a 0 to $\vec{\sigma}$, which says that the piecewise strategy is played with probability 0, and appending $\tilde{U}\big(\pbr(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v)$ to the existing vector of deviation payoffs. 
Next, we add synthetic training examples where $\pbr$ appears in the mixed-strategy support.
% We next add synthetic training examples for new mixed strategies $\vec{\sigma}'$, with $\pbr$ in the support, constructed based on the original training examples.
% Specifically, for each of the $m$ original $(\vec{\sigma},v)$, we find such $\vec{\sigma}'$ that---given the $o$ observations---can explain the distribution of atomic strategies $\vec{\sigma}$.
Specifically, for each of the $m$ original $(\vec{\sigma},v)$ pairs, we construct a new mixed strategy $\vec{\sigma}'$ that maximizes the piecewise strategy probability given the $\nobs$ observations.\footnote{An alternative approach might compute a different mixed strategy for each observation. We chose to compute a single mixed strategy over all observations as the resulting mixture seems to be a better statistical representation of the data.}
For each observation of $(\vec{\sigma},v)$, we identify all opponents who could be attributed to playing $\pbr$. 
Concretely, if an opponent $\oppidx$ has type~$\vec{t}_{\oppidx}$ and plays atomic strategy $\vec{s}_{\oppidx} = \pbr(\vec{t}_{\oppidx})$, that opponent can be considered to be playing $\pbr$.
We form a new opponent profile $\vec{s}'$ that replaces all such opponents' atomic strategies with $\pbr$, while other opponents retain their original strategies. 
Because $\vec{s}(\vecsp{t}) = \vec{s}'(\vecsp{t}) = \vec{a}$, the original observed deviation payoff vector remains valid. 
As before, the payoff for deviating to the piecewise strategy is $\tilde{U}\big(\pbr(t), \vecsp{s}(\vecspsp{t}), t, \vecsp{t}, v)$.
Because the game is symmetric, a mixed strategy $\vec{\sigma}'$ that can explain the $\nobs$ observations is the empirical distribution of strategies played over the $\nobs$ opponent profiles $\vec{s}'$.
So, for each observation $i=1,\dots,\nobs$, we add to our training set a mapping from $(\vec{\sigma}', v, \idx{t}{i})$ to the corresponding expanded deviation payoff vector described above.
We repeat this process for all $(\vec{\sigma},v)$ pairs in the original training set. 
Note that if the piecewise strategy is not played in any $\vec{s}'$, then we move on to the next $(\vec{\sigma}, v)$ pair.
% Consider opponent $\oppidx$, who has type~$\vec{t}_{\oppidx}$ and plays atomic strategy $\vec{s}_{\oppidx}$.
% We can attribute opponent $\oppidx$ to playing the piecewise strategy~$\pbr$ if $\vec{s}_{\oppidx} = \pbr(\vec{t}_{\oppidx})$. 
% In other words, we can say opponent $\oppidx$ plays the piecewise strategy if $\vec{t}_{\oppidx} \in \interval$ and the piecewise strategy maps interval $\interval$ to strategy $s = \vec{s}_{\oppidx}.$
% We can identify all such opponents who can be attributed to playing the piecewise strategy, and define a new opponent profile $\vec{s}'$ where those opponents play $\pbr$ and all remaining opponents play their original strategies from $\vec{s}$.
% The associated deviation payoff vector is still valid since $\vec{s}(\vecsp{t}) = \vec{s}'(\vecsp{t}) = \vec{a}$.
% Repeating this process---constructing a new $\vec{s}'$ maximizing play of the piecewise strategy---for all $\nobs$ observations gives us $\nobs \cdot (p - 1)$ overall sampled strategies. 
% Because the game is symmetric, a mixed strategy $\vec{\sigma}$ that can explain the $\nobs$ observations is the empirical distribution of the $\nobs \cdot (p - 1)$ strategies. 
% So, we can add to our training set $(\vec{\sigma}', v, \idx{t}{i}) \mapsto [\tilde{u}_j(\vec{\sigma}', v \mid \idx{t}{i})]$, where $\tilde{u}_j(\vec{\sigma}', v \mid \idx{t}{i}) = \tilde{U}(s_j(\idx{t}{i}), \idx{\vecsp{s}}{i}(\idx{\vecspsp{t}}{i}), \idx{t}{i}, \idx{\vecspsp{t}}{i}, v)$ for $s_j \in S$ and $\tilde{u}_{\pbr}(\vec{\sigma}', v \mid \idx{t}{i}) = \tilde{U}(\pbr(\idx{t}{i}), \idx{\vecsp{s}}{i}(\idx{\vecspsp{t}}{i}), \idx{t}{i}, \idx{\vecspsp{t}}{i}, v)$. 
% Note that if the piecewise strategy is not played in any $\vec{s}'$ for a given $\Gamma(v)$, then we move onto the next set of observations in the original training dataset.

In summary, our new training set includes all original training examples, where the piecewise strategy is not played, and many new training examples where opponents play the piecewise strategy with positive probability. 
For both original and new mixed strategies, the deviation payoff vector includes the deviation payoff for playing $\pbr$, which is simply the deviation payoff for playing the atomic strategy based on the deviator's type and the piecewise mapping. 
With this augmented training set, we can learn a new interim deviation payoff model for $\mathcal{G}'(V)$.

\paragraph{Iterative EMD procedure}
Incorporating $\pbr$ into the game model as described above fits into empirical mechanism design based on the following iterative approach:
\begin{enumerate}
    \item Learn an interim game-family model for the current strategy set.  
    \item Run a local search algorithm to identify the optimal parameter in equilibrium. 
    \item Compute a piecewise best response $\pbr$ to the equilibrium. 
    \item Construct a new training set which includes the new piecewise best response strategy. 
\end{enumerate}
In principle, we repeat these steps until the newest $\pbr$ is not a beneficial deviation from the current equilibrium. 
In practice, the number of iterations may be limited by how effectively existing data can be reused or how many new simulator queries can be afforded.

% THINGS NOT CURRENTLY INCLUDED IN THIS SECTION 
% ---------------------------------------------
% Two options for getting new learned model: (a) train a new network from scratch (what I ended up doing, since models are relatively small and I didn't want to risk having inconsistent errors between old vs new strategies) and (b) extending existing model by adding new input + output dimensions + relevant edges and doing NN surgery/other selective retraining 
% Contexualizing these ideas with PSRO
% Explanation about why it might be relevant to do this for a single game instance instead of a game family 
% How we choose which equilibrium to best respond to if there are multiple 
% The best way to use small number of additional samples is left to future work

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % %
% Dynamic Sponsored Search Auctions % 
% % % % % % % % % % % % % % % % % % %
\section{Dynamic Sponsored Search Auctions}

Our case study evaluates game-family learning approaches for EMD in a dynamic sponsored search auction scenario.
A \term{sponsored search auction} allocates ad slots on a search page to advertisers based on their bids.
The publisher ranks bids based on the offered price, typically adjusted by factors such as click-through rates and advertiser quality.
Slots are allocated in order of effective bids, and typically priced through a \term{generalized second-price auction}, where winning bidders pay an amount based on the next highest effective bid.
Many studies have modeled sponsored search as a one-shot simultaneous-move game \cite{lahaie2006analysis, edelman2007internet, varian2007position, thompson2009computational}, typically assuming perfect information on the basis that advertisers gain insights about competitors through repeated interactions.
When a bidder submits a bid, they may glean information based on the resulting state which may be used to adjust their bid. 
% Through such repeated interactions, advertisers may infer strategically relevant information about competitors, but will always have imperfect information about opponents' valuations.
This dynamic adaptation process, however, is abstracted away by one-shot models, which thereby do not capture imperfect adaptation or transient effects.

Our formulation aims to capture a simple form of the missing dynamics, by modeling a two-step bidding process. 
In the first stage, players simultaneously submit their initial bid based only on their valuation (type).
Each player is then informed about their tentative slot and its price, as well as the prices for other slots assuming competitor bids remained fixed. 
In the second stage, they may update their bid based on this observation.
% The second strategy component specifies whether the bidder updates their bid to best respond to this information. 
Updates are submitted simultaneously, and each is received ``on time'' by the auction with a specified probability. 
The final bids determine the slot allocations and prices.
For example, a strategy may set the initial bid to valuation minus two and the final bid as its optimal response to the provisional state.
Because all players have the option to change their bids, a player may sometimes be worse off by changing rather than keeping their initial bid. 
Because the updates are only probabilistically successful, players have an incentive to submit meaningful bids in the first stage.
This setup captures agents' imperfect information about opponents and their ability to gain information and adjust heuristically through interaction in a single-shot game.%
\footnote{We are not aware of previous work using a two-stage model of this kind.
\citet{vorobeychik2008equilibrium} identify pure equilibria in a symmetric sequential auction game with four greedy bidding strategies and \citet{nisan2011bestresponse} study best-response auctions where agents sequentially best respond until no further changes, then execute the auction. }

We study a symmetric version of this two-stage sponsored search auction with 5 players, 4 ad slots, and 10 available strategies.
The game family is parameterized by a \term{reserve requirement}~$\reserve$, typically set by the search publisher, which dictates the minimum effective bid needed to win an ad slot.
The auction mechanism is a weighted generalized second-price auction with quality-weighted reserves and Varian preference model~\cite{varian2007position}, taking agents' final bids as input. 
Each bidder has an advertiser quality score, $q \sim U(0, 1)$, and a valuation, $\val \sim U(0, 25)$.
An \term{auction setting} is a particular sample of $q$ and $\val$ for all players.
To participate, each bidder's \term{effective bid}, the product of their quality score and bid, must meet the reserve requirement. 
Slots are allocated by descending effective bids.
See App.~\ref{app:dynamic_game_details} for more details, including strategy specification. 
We use action-graph games \cite{thompson2009computational, thompson2013revenue, thompson2017computational} for efficient computation of payoffs and revenue for a particular game instance, auction setting, and final bid vector.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % %
% Ex Ante vs Interim Learning % 
% % % % % % % % % % % % % % % %
\section{Ex Ante vs Interim Learning}
\label{sec:ea_vs_i_learning}

We evaluate the performance of learned ex ante and marginalized interim deviation payoff functions for Bayesian game families. 
The ex ante model takes a symmetric mixed strategy and reserve price as input; the interim model additionally inputs a deviator quality score $q$ and valuation $\val$.
We employ a training set with $\nmix$ $(\vec{\sigma}, \reserve)$ pairs and $\nobs$ observations per pair.
The interim model trains on $\nmix \cdot \nobs$ training examples, while the ex ante model trains on $\nmix$ training examples, using the average deviation payoff vector across $\nobs$ observations as the target. 
Assuming a budget of $\nsimq$ simulator queries for training, we construct six training sets, each with different values of $\nmix$ and $\nobs$ such that $\nmix \cdot \nobs = \nsimq$. 
With each set, we train six ex ante and six interim neural networks, where the reserve price ranges from 0.01 to 8 in the training data.
App.~\ref{app:training_info} Table~\ref{table:game1_train_payoff_stats} lists $\nmix$,  $\nobs$, and payoff statistics for each training set. 

The initial performance comparison uses a test set of a realistic size for a large empirical game family.
This test set is noisy due to sampling limitations.
App.~\ref{app:training_info} Table~\ref{table:game1_train_val_test_split} details train-validation-test splits for all training sets and functions.
For method validation, we also evaluate on a larger, less noisy dataset, which would be infeasible for a large empirical game family. 
Our results show that with sufficient marginalization samples, the interim model achieves similar error to the ex ante model, and exhibits better extrapolation. 
Further, performance trends on the noisy test set align with those on the larger test set, suggesting the noisy test set provides reliable performance insights. 

% % % % % % % % % % % % % % % %
% Model Learning Test Results % 
% % % % % % % % % % % % % % % %
\subsection{Model Learning Test Results}

Fig.~\ref{fig:ml_test_results} compares deviation payoff errors among learned ex ante and interim game-family deviation payoff models. 
To marginalize interim deviation payoffs, we use 1k, 5k, and 10k Monte Carlo type samples (i.e., randomly generated deviator quality scores and valuations).
By saving the specific $q$ and $\val$ values for each $(\vec{\sigma}, \reserve)$ pair in the model learning (ML) test set, we can also compute marginalized interim deviation payoffs using these exact values. 
While this information would not in practice be available for model inference, for our experimental purpose it helps contextualize the interim test-set results. 
For each method, we compute mean squared error (MSE) over the test set using the same formulation as for training loss in~\S\ref{sec:ex-ante}.

\begin{figure*}[t]
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.2]{plots/ea_v_ei_binned_r_test_300ob.png} && 
\includegraphics[scale=0.2]{plots/ea_v_ei_per_NN_300ob.png} \\
(a) && (b) \\ 
\end{tabular}
\caption{With enough marginalization samples, interim model accuracy matches ex ante. 
Deviation payoff errors are consistent (a) across the reserve price range and (b) across training datasets.
Error bars represent 95\% confidence intervals.}
\label{fig:ml_test_results}
\end{figure*}
% In both plots, we compute 95\% confidence intervals for MSE.

Plot~\ref{fig:ml_test_results}(a) shows errors across the reserve price range, with test-set reserve prices binned in intervals of width 0.5.
Performance is aggregated across the six training datasets for each method. 
The ex ante and marginalized interim models with 5k and 10k Monte Carlo type samples perform similarly, with only a slight performance decrease for the marginalized interim model with 1k samples. 
It also shows that having the exact deviator types (purple) would make the interim model noticeably more accurate.
For all models, deviation payoff errors remain consistent across the reserve range. 

Plot~\ref{fig:ml_test_results}(b) shows model performance across training datasets, aggregated across the reserve price range. 
Relative performance trends across the 5 methods are consistent across the six training datasets, suggesting these are robust to different $\nmix$ and $\nobs$ combinations.
With fixed $\nsimq$ and more extreme $\nobs$ values (e.g., 1-2 or 1000), deviation payoff errors increase significantly such that we did not pursue further hyperparameter tuning or formal test set evaluation for these cases.
Our test set, with 300 observations per $(\vec{\sigma}, \reserve)$ pair, is realistic for large empirical game families but has sample noise in the ground truth deviation payoff vectors. 
We confirm that increasing test observations reduces error magnitude; see App.~\ref{app:ml_test}. 

% % % % % % % % % % % % % % % % % % %
% Fine-Grained Grid Mixture Results % 
% % % % % % % % % % % % % % % % % % %
\subsection{Fine-Grained Grid Mixture Results}
\label{sec:extra_test_set}

To further validate our methods, we create a less noisy dataset that includes a grid of 300 reserve prices from 0.05 to 15 in increments of 0.05. 
For each game instance, we also have a grid of symmetric mixed strategies with support size up to 3, with probabilities a multiple of 0.1. 
For each $(\vec{\sigma}, \reserve)$ we compute payoffs for \textit{all opponent profiles}, $\vec{s}$, that can be sampled from $\vec{\sigma}$, eliminating one source of noise entirely. 
Specifically, for each $(\vec{s}, \reserve)$ we run 10,000 simulations of the auction game, and compute the average deviation payoff for each strategy. 
We define the \term{true deviation payoff vector} for a given $(\vec{\sigma}, \reserve)$ pair as the average deviation payoff across all $\vec{s}$, weighted by their probabilities in~$\vec{\sigma}$.

Fig.~\ref{fig:extra_test_results} shows ex ante and marginalized interim deviation payoff MSEs on this larger dataset, with errors computed as before and 95\% confidence intervals shaded. % used line plot instead of point plot since not doing binned r like in Fig 1 
Plot~\ref{fig:extra_test_results}(a) shows aggregate performance across six models for each method. 
All error magnitudes are lower, indicating that the learned models interpolate and compute expectations more accurately than our ML test set captured. 
The relative performance trends remain consistent with those of the ML test set.
% suggesting that the ML test set results are reliable despite the noise. 
Marginalized interim models with 5k and 10k samples have errors comparable to (slightly higher than) ex ante.
Nonetheless, all errors are small relative to the payoff scale (see App.~\ref{app:training_info} Table~\ref{table:game1_test_payoff_stats}).

\begin{figure}[ht]
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.2]{plots/ex_ante_vs_cond_dp_extra_test_rleq8.png} &&
\includegraphics[scale=0.2]{plots/ex_ante_vs_cond_dp_extra_test_rleq15.png} \\
(a) && (b) \\  
\end{tabular}
\caption{Results for fine-grained data set.
(a)~All models perform better than shown by the noisy ML test set (Fig.~\ref{fig:ml_test_results}), but with consistent trends.
(b)~Interim but not ex ante models extrapolate well beyond the trained range, $\reserve\in[0,8]$.}
\label{fig:extra_test_results}
\end{figure}

As the parameter range covered by the training data must be set prior to any game-theoretic analysis, there is a risk that it may be too narrow.
Thus, a model with extrapolation capability is beneficial for applications like EMD. 
Plot~\ref{fig:extra_test_results}b shows method performance as the reserve price ranges from 0.05 to 15, with $\reserve>8.0$ representing extrapolation results.
See App.~\ref{app:extra_test} for individual NN extrapolation results.
Notably, all marginalized interim models extrapolate well, while ex ante models exhibit a noticeable increase in error beyond the trained parameter range.
Intuitively, the higher the reserve price, the greater the role of the deviator's type in determining deviation payoffs. 
Bidders never bid above their valuation, so do not substantively participate when $q \cdot \val < \reserve$. 
At higher reserve prices, many bidders are effectively eliminated (or highly constrained), while a few can shade their bid significantly to receive a large deviation payoff. 
These results suggest that the interim models implicitly learn the relationship between $q$, $\val$, $\reserve$, and $\Pr(q\cdot \val \geq \reserve)$. 
This highlights a key advantage of the interim learning approach.  

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % % % % % % % % % % %
% Deriving Equilibria % 
% % % % % % % % % % % %
\section{Deriving Equilibria}
\label{sec:find_appx_ne} 

To derive equilibria for a given game instance $\Gamma(\reserve)$ and learned deviation payoff model $\hat{u}$, we run replicator dynamics (RD) \cite{taylor1978evolutionary} from a variety of starting points, using $\hat{u}$ to approximate deviation payoffs.
Specifically, we initialize from 11 full-support mixed-strategy profiles: the uniform random mixed strategy and, for each strategy $s_j$ of the 10 strategies, a mixed strategy with probability $\frac{2}{11}$ on strategy $s_j$ and $\frac{1}{11}$ on the others. 
In our experiments, we separately apply these RD runs with each of the twelve learned models on the same 300 game instances used to evaluate learning performance (reserve prices from 0.05 to 15).
For interim models, we use 1000 Monte Carlo type samples for marginalization. 
For each returned $\vec{\sigma}$, we use $\hat{u}$ to predict regret.
If predicted regret is at most $\varepsilon$, it is a candidate $\varepsilon$-BNE; otherwise RD did not converge on that run and it is discarded.
% Note to self -- for practical purupses, two ways an RD mixture is said to be nonconvergent: (a) predicted regret on RD mixture less than epsilon; (b) predicted regret on truncated mixture less than epsilon 

The \term{support of a candidate $\varepsilon$-BNE} is the set of strategies played with at least 0.01 probability. 
Probabilities below 0.01 are set to 0 and the mixture is renormalized. 
We compute the true deviation payoff vector as in \S\ref{sec:extra_test_set}, simulating 10,000 auction games for each opponent profile $\vec{s}$ that could be sampled from the (possibly renormalized) candidate mixture.
Using this true deviation payoff vector we can calculate \term{true regret}, and regard a candidate $\varepsilon$-BNE as confirmed if true regret is at most $\varepsilon=0.01$. 
% rather than the effectiveness of RD

Fig.~\ref{fig:cand_ne_regr_abs_err} shows mean absolute error between predicted and true regret for ex ante and interim methods, with 95\% confidence intervals shaded.
Each point represents the average regret absolute error across all candidate $\varepsilon$-BNE returned by RD using each of the 6 trained models.%
\footnote{For consistent comparison, both predicted and true regret are computed on the truncated and renormalized candidates.} 
See App.~\ref{app:nash_approximation} Fig.~\ref{fig:ea_vs_ei_cand_ne_regr_abs_err} for a comparison of ex ante vs interim for each trained model. 
The interim method consistently exhibits equal or lower regret absolute error for candidate equilibria compared to ex ante, both within the trained range ($\reserve \leq 8$) and in extrapolation ($\reserve > 8$), despite its slightly higher deviation payoff learning errors.
The ex ante method is particularly unreliable in extrapolation, with regret absolute error around 0.01 or higher from $\reserve$ values of about 8 to 13, matching or exceeding $\varepsilon = 0.01$.
This suggests that overall learning error does not tell the whole story, as we generally care more about accuracy near equilibrium than elsewhere. 
% and the interim method may be more advantageous if the goal is equilibrium approximation.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.25]{plots/eac_cand_ne_regr_abs_err_eps=0.01_short.png}
\caption{On the trained range ($\reserve \leq 8$) and in extrapolation ($\reserve > 8$), the interim method has equal or lower regret error than ex ante for candidate equilibria, indicating it can better distinguish candidate equilibria from non-convergent RD mixtures.}
\label{fig:cand_ne_regr_abs_err}
\end{figure}

One challenge with learned ex ante models in extrapolation is that, for certain mixed strategies, the predicted deviation payoff vector is effectively all zeros. 
This disrupts RD, as multiplying by a zero vector ``kills'' the iteratively improving mixed strategy, creating what we call a \term{dead mixture}.
Thus, for each game instance and learned model $\hat{u}$, the 11 returned RD mixtures can be classified as dead mixtures, non-convergent starts, rejected candidates, or confirmed $\varepsilon$-BNE. 
Notably, within the trained range ($\reserve \leq 8$), 15.8\% of ex ante RD mixtures were rejected candidates, compared to 9.32\% with the interim method, consistent with the higher ex ante regret error seen in Fig.~\ref{fig:cand_ne_regr_abs_err}. 
Beyond the trained range ($\reserve > 8$), 46.96\% of ex ante RD mixtures were rejected candidates, compared to only 14.53\% for interim. 
Additionally, 2.58\% of ex ante RD results were dead mixtures.
App.~\ref{app:nash_approximation} Tables \ref{table:game1_mix_class_rleq8} and~\ref{table:game1_mix_class_rgt8} detail mixture classifications for $\reserve \leq 8$ and $\reserve > 8$. 
These results further evidence the relative robustness of the interim method in approximating equilibria and distinguishing equilibria from non-equilibria.

% OLD INFORMATION!
% r <= 8
% Rejected NE: 8.14% vs 1.89%
% Did not converge: 0% vs 0.01% EI

% r >= 8
% Dead Mixtures: 3.15% vs 0% 
% Did Not Converge: 0% 
% Rejected NE: 24.81% vs 4.22% 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % %
% Application to EMD  %
% % % % % % % % % % % %
\section{Application to Empirical Mechanism Design}

In this section, we demonstrate the effectiveness of conducting EMD with a learned game family via grid optimization and local search. 
Our objective is to identify the reserve price(s) that maximize expected revenue in equilibrium.
Game-family learning facilitates the approximation of multiple, distinct BNE, improving the robustness of statistics computed based on the $\varepsilon$-BNE set (e.g., worst case, average, max entropy). 
While the appropriate equilibrium selection method depends on the domain, we found that relying on the first-returned equilibrium was unreliable, as neighboring game instances sometimes had different equilibria, affecting revenue smoothness.
In our experiments, the reported revenue for a game instance is the average revenue across all confirmed equilibria.  

% % % % % % % % % % %
% Grid Optimization %
% % % % % % % % % % %
\subsection{Grid Optimization}
\label{sec:grid_optimization}

We first conduct a grid search on the expected revenue curve using the $\varepsilon$-BNE sets from~\S\ref{sec:find_appx_ne}.
The \term{true revenue for a confirmed $\varepsilon$-BNE} is the probability-weighted average of pure-strategy revenues, each calculated as the sample average revenue over 10,000 auction simulations.
Fig.~\ref{fig:emd_grid_optimization} plots (a) and~(b) show true expected revenue in equilibrium across all confirmed equilibria found by each (a)~ex ante and (b)~interim model. 
Observe the discontinuities present in the expected revenue curves for both ex ante and interim models (See App.~\ref{app:nash_approximation} Table~\ref{table:game1_holes} for full breakdown).  
These discontinuities result from 11 rejected candidate equilibria, yielding zero confirmed equilibria.
Four ex ante models exhibit discontinuities within the trained range, with an average of 14.8 holes in the expected revenue curve across all ex ante models (out of 160 game instances). 
In contrast, only two of the interim models have holes, with an average of 10.7 holes across all interim models. 
In extrapolation, all ex ante models have holes, averaging 62.8 holes of 140 game instances across all ex ante models, while only two interim models have holes, averaging 19.7 holes across all interim models.
These results illustrate the impact of differences in Nash approximation performance between ex ante and interim methods (Fig.~\ref{fig:cand_ne_regr_abs_err}) on EMD, and support the argument for training multiple models to facilitate more complete game-theoretic analysis and parameter optimization. 

\begin{figure*}[ht] 
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.14]{plots/rd_ex_ante_6NN_max15_eps=0.01_no_ci_no_na.png} & 
\includegraphics[scale=0.14]{plots/rd_cond_6NN_max15_eps=0.01_no_ci_no_na.png} & 
\includegraphics[scale=0.14]{plots/rd_ex_ante_vs_cond_max15_w_gt_eps=0.01.png} \\
(a) & (b) & (c) \\ 
\end{tabular}
\caption{Expected revenue for confirmed equilibria. 
Revenue curves from ex ante models exhibit more discontinuities on the trained range and in extrapolation (Plot a) compared to revenue curves from interim models (Plot b). Four methods to aggregate expected revenue in equilibrium produce similar curves (Plot c).}
\label{fig:emd_grid_optimization}
\end{figure*}

For several game instances, expected revenue varies drastically across $\varepsilon$-BNE sets found by running RD with different models, especially for $6 \leq \reserve \leq 8.$
What is the expected revenue for a larger $\varepsilon$-BNE set?  
Using a grid of all 3-support\footnote{Most confirmed $\varepsilon$-BNE found by RD have a support size of at most 3.} mixed-strategy distributions across 10 strategies with increments of 0.01 and for all 300 game instances, we calculate true regret, identify $\varepsilon$-BNE, and compute true expected revenue.
For each game instance, the expected revenue is the uniform average revenue across all grid-mixture $\varepsilon$-BNE. 
However, this uniform average may overemphasize higher-support equilibria, so we also compute weighted revenue: for each possible 3-strategy subset $Y \subset S$, we identify all equilibria whose support is contained within $Y$, and compute the average revenue for these equilibria. 
The weighted average revenue is the average revenue across all~$Y$.
Plot~\ref{fig:emd_grid_optimization}(c) shows ex ante and interim expected revenue curves, aggregated over six models, as well as expected revenue for grid equilibria.
% On average, the ex ante method finds equilibria with higher expected revenue; for this auction game family, higher-revenue equilibria often have higher regret, and equilibria found with the ex ante method exhibit higher regret (e.g., App.~\ref{app:emd} Figure \hl{\#}).
Ex ante and interim expected revenue curves resemble those based on grid equilibria, supporting the practical effectiveness of the learning approaches for parameter optimization.

In summary, ex ante models reject a higher proportion of candidate NE, causing several discontinuities in the expected revenue curve which may hinder parameter optimization with a single model. 
Interim extrapolation was crucial to confirm that the optimal reserve price was less than 8. 
The presence of multiple equilibria and the possible need for extrapolation suggest that learning the equilibrium objective function directly would be insufficient. 
Although no single optimal reserve price emerges for this game family, identifying the optimal revenue plateau (e.g., $6\leq \reserve \leq 8$) remains valuable as optimal plateaus have been present in past EMD studies \cite{brinkman2017empirical}. 


% % % % % % % % % % % % % % %
% Local Search Optimization % 
% % % % % % % % % % % % % % %
\subsection{Local Search Optimization}
\label{sec:local_search}

In this experiment, we compare using ex ante and interim models in stochastic hill climbing and simulated annealing local search algorithms, as described in App.~\ref{app:emd}.
Stochastic hill climbing terminates when all uphill neighbors have been explored ($\reserve \pm 0.05, \reserve \pm 0.1, \reserve \pm 0.25)$ or after 50 iterations. 
Simulated annealing terminates after 50 iterations, but typically evaluates a comparable number of game instances due to initial high temperature causing repeated selections.
To evaluate a given game instance with learned model $\hat{u}$, we run RD with $\hat{u}$ (using 1000 Monte Carlo type samples for interim) starting from the 11 full-support initial mixtures described in~\S\ref{sec:find_appx_ne}.  
We use model~$\hat{u}$ to compute predicted regret for each masked and renormalized mixture, and identify candidate equilibria. 
% Consider adding sentence about what happens if no Nash found for game instance: revenue is assigned to be -1 so hill climbing would never choose this game instance and simulated annealing just keeps on going; also keep running RD with increased iterations 5 times before we get to this point  
Expected revenue for a candidate equilibrium $\vec{\sigma}$ is approximated by sampling 100 pure profiles from $\vec{\sigma}$, and computing average revenue across 10,000 auction settings for each pure profile.
For realistic local search, the expected revenue in equilibrium is the average revenue across all \textit{candidate} equilibria.  
After search termination, we identify the game instance with the highest revenue in equilibrium, and use true deviation payoff samples to determine the set of confirmed equilibria. 
The final reported expected revenue in equilibrium is the average revenue over all \textit{confirmed} equilibria. 

Fig.~\ref{fig:local_search_results} shows 95\% bootstrap confidence intervals for optimal expected revenue in equilibrium from hill climbing (HC) and simulated annealing (SA) using ex ante (EA) or interim (I) models. 
For each combination, we perform 50 random $\reserve$ restarts for robust evaluation.
As 5 restarts is more practically realistic, we sample 5 (of~50) restart experiments with replacement, and determine the game instance with the max revenue for confirmed equilibria. 
We repeat this process 100,000 times, and plot 95\% confidence intervals around the mean optimal expected revenue for confirmed equilibria.  
For each model, we also plot the maximum expected revenue based on grid optimization. 
In nearly all cases, the upper confidence bound aligns with the optimal value from grid optimization. 
Note that Nash approximation with ex ante models is deterministic whereas Nash approximation with interim is not, which is why the interim confidence interval may extend beyond the max revenue from grid optimization. 
The decrease in max revenue from grid optimization to average max revenue in local search is no more than 5\% (App.~\ref{app:emd} Table~\ref{table:game1_pct_decr_results}). 
Further, interim models consistently exhibit a smaller decrease than ex ante under the same conditions.
This is notable considering the granularity ($\delta = 0.05$, $0.05 \leq \reserve \leq 15$) and that local search evaluates only 30-40\% of the game instances on average compared to grid optimization (App.~\ref{app:emd} Fig.~\ref{fig:num_game_instances_evaluated}). 
This experiment highlights the effectiveness of local search with learned game families and a modest number of restarts.

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{plots/max_rev_conf_ne_5restart_100k_bootstrap.png}
    \caption{Local search with learned game families and limited restarts identifies high-revenue game instances comparable on average to grid optimization.}
    \label{fig:local_search_results}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % % % % % % % % % % % % % % % % % %
% Piecewise-Conditional Strategies  % 
% % % % % % % % % % % % % % % % % % %
\section{Piecewise-Conditional Strategies}

\subsection{Computing Piecewise Best Responses}
\label{sec:computing_pwbr_results}

As described in~\S\ref{sec:piecewise}, one of the advantages of the interim model is that it allows us to derive new piecewise strategies as best responses to an opponent profile, without any additional sampling or training.
To assess the value of this capability, we use the learned interim model to (1)~compute a piecewise best-response strategy to an $\varepsilon$-BNE, and (2)~quantify the payoff gain by playing this strategy while opponents play this equilibrium. 
Our auction game family has two type parameters, $q$ and $\val$, which we reduce to one dimension by taking the product $q\cdot \val$, representing a player's maximum effective bid. 
The effective bid space ranges from 0 to 25 and is partitioned into five equiprobable intervals. %: [0, 1.25), [1.25, 3.31), [3.31, 6.31), [6.31, 10.96), and [10.96, 25).
For a given $\reserve$ and confirmed $\vec{\sigma}_\BNE$, we sample $\abs{\typesamp}= 100,000$ values for $q \sim U(0, 1)$ and $\val \sim U(0, 25)$ and use the learned interim model to get $\hat{u}(\vec{\sigma}_{\BNE}, \reserve \mid q, \val)$ for each sampled $q$ and $\val$.
The predicted percentage increase from playing the atomic best response to playing the computed piecewise best response is: 
$$\frac{\hat{u}_{\pbr}(\vec{\sigma}_{\BNE}, \reserve)-\max(\hat{u}(\vec{\sigma}_{\BNE}, \reserve))}{\max(\hat{u}(\vec{\sigma}_{\BNE}, \reserve))}\times 100,$$
where $\hat{u}_{\pbr}(\vec{\sigma}_{\BNE}, \reserve)$ is given by 
Equation~\ref{eq:pred_pbr_payoff} and where $\hat{u}(\vec{\sigma}_{\BNE}, \reserve)$ is the marginalized interim deviation payoff vector.
The true percentage increase in payoff is defined as:
$$\frac{\tilde{u}_{\pbr}(\vec{\sigma}_{\BNE}, \reserve)-\max(\tilde{u}(\vec{\sigma}_{\BNE}, \reserve))}{\max(\tilde{u}(\vec{\sigma}_{\BNE}, \reserve))}\times 100,$$
where $\tilde{u}_{\pbr}(\vec{\sigma}_{\BNE}, \reserve)$ is given by Equation~\ref{eq:true_pbr_payoff} and where $\tilde{u}(\vec{\sigma}_{\BNE}, \reserve)$ is the true deviation payoff vector based on 10,000 auction simulations sampled for each possible opponent profile. 

Fig.~\ref{fig:pct_incr_pred_and_gt} compares the predicted and true percentage increases to using the piecewise best response.
The x-axis reserve values correspond to optimal values from the grid optimization experiments. 
For each $\reserve$, interim neural network, and confirmed $\varepsilon$-BNE, we compute the piecewise best response to the $\varepsilon$-BNE and then compute the predicted and true percentage payoff increases from the best atomic strategy to this piecewise best response. 
Each point represents the average (predicted, true) payoff percentage increase across all 6 interim neural networks and up to 11 confirmed equilibria for each neural network. %and the up-to-11 confirmed equilibria approximated using the given interim neural network in RD. 
The plot indicates a $3-4\%$ payoff gain (larger than $\varepsilon$) by playing the piecewise best-response strategy, and shows that lower reserves admit higher percentage increases.
These results suggest that consideration of these additional composite strategies can make a meaningful difference, and is worth pursuing in studies of empirical Bayesian games.
% further analysis of piecewise best-response strategies may be needed to better capture equilibrium behavior in Bayesian empirical game instances with heuristic strategies. 

\begin{figure}
\centering 
\includegraphics[scale=0.25]{plots/pct_incr_pred_and_gt.png}
\caption{A player can gain 3-4\% payoff by playing a piecewise best response to an equilibrium compared to playing the atomic best response.}
\label{fig:pct_incr_pred_and_gt}
\end{figure}

\subsection{Iterative EMD with Piecewise Strategies}
\label{sec:iter_emd_w_pwbr_results}

We now focus on the dynamic auction environment with 6 initial strategies, specified in App.~\ref{app:dynamic_game_details}, and iteratively expand the game family to add piecewise best-response strategies.
We label the game family, strategy set, equilibria, and optimal $\reserve$ based on the number of strategies in the associated game environment. 
For example, the game family $\idx{\mathcal{G}}{k}(\reserveVar)$ has a strategy set $\idx{S}{k}$ consisting of $k$ total strategies, including $k-6$ piecewise strategies.
The optimal reserve price, denoted by $\idx{\reserve_*}{k}$, is the reserve price that maximizes the expected revenue for candidate equilibria $\vidx{\sigma_*}{k}$ in game instance $\idx{\Gamma}{k}(\idx{\reserve_*}{k})$ within the game family $\idx{\mathcal{G}}{k}(\reserveVar)$.
% For example, the initial game family is denoted $\vidx{\mathcal{G}}{6}(\reserveVar)$ with strategy set $\idx{S}{6}$, and the game family with one piecewise strategy is denoted $\vidx{\mathcal{G}}{7}(\reserveVar)$, with strategy set $\idx{S}{7} = \idx{S}{6} \cup \{\pbr_1\}$.

We follow the procedure for iterative EMD with piecewise strategies as specified in~\S\ref{sec:iter_emd_w_pbr}.
The general local search procedure matches the one described in~\S\ref{sec:local_search}, with the following modifications. 
The number of full-support initial RD mixtures is set to one more than the current number of strategies (just as we used 11 for the 10-strategy game), with the relative structure of the mixtures the same. 
For $\idx{\mathcal{G}}{6}(\reserveVar)$, we sample the initial reserve values $\reserve$ uniformly from $(0, 8]$, whereas for $\idx{\mathcal{G}}{7}(\reserveVar)$ and $\idx{\mathcal{G}}{8}(\reserveVar)$, we sample from a bell-shaped distribution over (0, 8] centered at 4. 
As before, we use the interim model---using 1000 marginalization samples---to identify candidate equilibria based on predicted regret. 
We compute the expected revenue across all candidate equilibria based on 1000 revenue samples for each candidate. 
To determine the optimal reserve price, we run both simulated annealing and stochastic hill climbing with five random restarts each, and select the reserve $\idx{\reserve_*}{k}$ that yields the highest expected revenue among the equilibria found in the ten local searches. 
We then use the learned interim model to compute a piecewise best response to the candidate equilibrium with the largest basin of attraction (i.e., the most frequent equilibrium identified by RD across all initial mixtures). 
Finally, we augment the training and validation sets to cover the new strategy set following the approach in \S\ref{sec:iter_emd_w_pbr}, retrain the interim model to include the newest piecewise strategy, and tune the model on a small grid of hyperparameters (including previous model settings and related settings). 

To evaluate this approach, we perform two iterations of EMD with piecewise strategies using four distinct initial models, each trained on and identified by a different initial training set. 
For example, the evaluation run where the initial neural network was trained on a dataset with $\nmix=100,000$ and $\nobs=10$ will be referred to by (100k, 10), even though subsequent neural networks in this run will train on more data. 
For every model at each iteration, we run ten local search trials and compute a new piecewise best response to the equilibrium derived by that model.  
% We separately conduct a local search 10 times with each neural network model, and use the model to compute a piecewise best response to the equilibrium derived by the same model.
After the two iterations, we evaluate the candidate equilibria at each iteration based on regret calculated with \emph{true} deviation payoffs (see \S\ref{sec:extra_test_set}), and compare with the predicted regret using 100,000 marginalization samples.
We also compute expected revenue for the candidate equilibria as described in \S\ref{sec:grid_optimization}.

For $\idx{\mathcal{G}}{6}(\reserveVar)$, all four NN models correctly identified the exact pure-strategy BNE, $s_4$, with predicted regret 0 in the respective optimal game instance. 
Table~\ref{iter0_iter1_results} shows regret for candidate equilibria evaluated on game instances in $\idx{\mathcal{G}}{7}(\reserveVar)$.
%, where $\idx{\vec{\sigma}_*}{7}$ refers to candidate equilibria from the optimal 7-strategy game instance with reserve price $\idx{\reserve_*}{7}$ and $\idx{\vec{\sigma}_*}{6}$ refers to candidate equilibria from the 6-strategy game instance with reserve price $\idx{\reserve_*}{6}$. 
Each row corresponds to a different evaluation run, where the initial neural network model was trained on the specified $\nmix$ $(\vec{\sigma}, \reserve)$ pairs with $\nobs$ observations per pair; refer to App.~\ref{app:game2_training_info} Table~\ref{table:game2_nmix_per_iter} for later iteration training set sizes. 
In each pair of columns we report the true regret, averaged over all candidate equilibria, and the mean absolute error between predicted and true regret for candidate equilibria.
The first pair of columns demonstrates that atomic strategy $s_4$ is not an equilibrium in the corresponding 7-strategy game instance, and all four NN models correctly predict this. 
% This supports the relevance of analyzing game families with piecewise strategies. 
The second and third pairs of columns show that $\idx{\vec{\sigma}_*}{7}$, which corresponds to playing the pure piecewise strategy, is an exact equilibrium in the 7-strategy game instance with reserve $\idx{\reserve_*}{7}$ and also with reserve $\idx{\reserve_*}{6}$, and all four NN models correctly determine this.
This result is notable because the original learned model could not predict deviation payoffs for this profile at all, and for the new model there are no training data points where the piecewise strategy is played with more than roughly 0.7 probability (see App.~\ref{app:game2_training_info} Table~\ref{table:game2_max_pbr_prob}).

Table~\ref{iter0_iter1_results} also shows the expected revenue results for the candidate equilibria. 
The expected revenue values in the first column are gray because $\idx{\vec{\sigma}_*}{6}$ is not an equilibrium in the 7-strategy game instance. 
For the first evaluation run, (100k, 10), the expected revenue from playing the piecewise equilibrium strategy in $\idx{\Gamma}{7}\bigl(\idx{\reserve_*}{7}\bigr)$ is similar to the expected revenue from playing the same equilibrium in $\idx{\Gamma}{7}\bigl(\idx{\reserve_*}{6}\bigr)$.
This result is sensible because the $\reserve$ values are close, and during local search we used revenue samples from only 1000 simulations of the auction game compared to 10,000+ for evaluation.
For the remaining three evaluation runs, expected revenue from playing the piecewise equilibrium in $\idx{\Gamma}{7}\bigl(\idx{\reserve_*}{7}\bigr)$ is larger than in $\idx{\Gamma}{7}\bigl(\idx{\reserve_*}{6}\bigr)$.
This provides evidence that the local search procedure worked effectively in $\idx{\mathcal{G}}{7}(\reserveVar)$.

\input{tables/iter0_iter1_results}

Table~\ref{iter1_iter2_results} shows regret for candidate equilibria evaluated on game instances in $\idx{\mathcal{G}}{8}(\reserveVar)$. 
The first column says that playing $\idx{\vec{\sigma}_*}{7}$, the first piecewise strategy, in the 8-strategy game instance is still an equilibrium.
However, none of the four models predicted this, as they predicted payoff gains by deviating to the second piecewise strategy. 
We believe this may be due to not having any training examples where the first piecewise strategy is played with more than 0.7 probability, so the deviation payoff for the 2nd piecewise strategy, trained on limited available data, is not a reliable estimate (see App.~\ref{app:game2_training_info} Table~\ref{table:game2_max_pbr_prob}).
Also, the (100k, 10) model struggled to find 0.01-equilibria throughout the local search process. 
For this  model, $\idx{\vec{\sigma}_*}{8}$ refers to a single mixed strategy, whereas for the other three models it refers to up to 9 different candidate mixed-strategy equilibria (with support including the two piecewise strategies). 
This model has a larger number of $(\vec{\sigma}$, $\reserve)$ pairs in the initial training set compared to the (100k, 5) and (50k, 10) models, and has fewer observations per pair compared to the (50k, 20) model. 
This means that the number of new training examples grows faster than for the (100k, 5) and (50k, 10) models, but with new mixed strategies computed based on fewer observations compared to the (50k, 20) model. 
We hypothesize that this model trained on a large number of new mixed strategies with noisy deviation payoff targets, and that this noise hindered equilibrium approximation for $\varepsilon=0.01$. 
Fortunately, for the other three models the candidate equilibria $\idx{\vec{\sigma}_*}{7}$ are confirmed in both $\idx{\Gamma}{8}\bigl(\idx{\reserve_*}{8}\bigr)$ and $\idx{\Gamma}{8}\bigl(\idx{\reserve_*}{7}\bigr)$.
This suggests that these three models are capable of correctly identifying higher entropy equilibria, but may need additional training examples to adequately predict deviation payoffs when opponents play a piecewise strategy. 

Table~\ref{iter1_iter2_results} also shows the expected revenue results in $\idx{\mathcal{G}}{8}(\reserveVar)$.
For the (100k, 10) model, only the first column corresponds to a confirmed equilibrium.
For the remaining three models, all three columns correspond to confirmed equilibria, with no single equilibrium combination consistently achieving the highest expected revenue. 
This may be because all $\reserve$ values fall within an optimal plateau and local search has converged, or 
it may be an indication that we need more simulation data---deviation payoff training samples to improve predicted regret or revenue samples to reduce noise in the expected revenue curve---to refine the search. 

\input{tables/iter1_iter2_results}

Overall, these results show that the inclusion of piecewise strategies can change the equilibria in the game family as well as the optimal reserve price and associated expected revenue. 
They also show that we can train a new game model to include piecewise strategies using data only from the original training set, and this model can be used in local search to approximate equilibria in the game family with an expanded strategy set. 
Determining which plausible examples to include in the new training set as well as how to strategically use a small number of additional data samples for the new training set remain open problems. 



% ITER 1 -- ITER 2 REMARKS
% ------------------------
% revenue analysis: only 1k revenue samples, noisier with 2 piecewise strategies 
% Since all 3 combinations are confirmed BNE (for bottom 3 NNs), we can look at revenue for all 3. Don't think we can conclude anything here -- it's possible that we need more revenue samples with 2 piecewise strategies and that would help the search. Also possible we need to be more selective about which new data points are added or gather new data 
% In the 10-strategy game and the 6-strategy game, the expected revenue curve did have an optimal plateau. It is likely that the 7-strategy games and 8-strategy games also have optimal plateaus, so possibly most/all returned r are from that plateau
% Overall, results show that the addition of a piecewise strategy can change the equilibria in the game as well as the optimal r value and associated expected revenue
% Also showed that we can do alright without using any additionanl deviation payoff simulator queries, but it is probable that we could do even better by strategically using a small number of additional simulator queries 

% Something that we might need to address in a rebuttal: 
% r=3.6 case: revenues are different. yes, the pwbr mappings are different but for r=3.6 the effective mappings are the same. so why is revenue that different? noise from piecewise strategy, probably. 





% Things I may (need to) include in the future
% --------------------------------------------
% There's a validation set for each NN at each iteration (since piecewise best response strategies are different)
% Piecewise strategies can technically include a mapping to another piecewise strategy. Since partition is fixed, this effectively just means there's an extra layer of unrolling to get to the atomic strategy but it doesn't change which ground truth payoffs are applicable. Reason I allowed this is because piecewise strategy is included in more mixed strategies than the mapped atomic strategy (even though overall probabilty placed on the previous piecewise strategy is lower)
% Note: The piecewise best responses are different meaning that after the 1st piecewise strategy is added, the 4 models correspond to 4 different games; this also might happen due to different returned optimal r values (even if NE is the same or similar)
% Random: For strategies where we expect the NN to give zero dps for not meeting the reserve requirement, the NN gives slightly different outputs which means that the piecewise best response for that interval is arbitrary (well whichever one has the most positive error) among the strategies with effectively dp of 0. We are assuming we don't/might not know this payoff property and therefore wouldn't want to clip the payoffs to 0. 
% Related note: The above situation is interesting when types can begin meeting the reserve requirement in the middle of the type interval. Ex: when $r$ is 3.15 for players with types in the interval [1.25, 3.3] the best response for that interval depends on how large (or not) the payoffs are for $r=3.15$ to 3.3 relative to the positive dp errors of [1.25, 3.3]. From quick my manual inspection it seems like this isn't an issue but I could see a scenario where it could be 
% future work: it would be interesting to think about how to better carve up the type space when there are relevant r values where 1-2 type intervals can't do anything. 
% potentially relevant to the discussion: how much does whichever initial r we land on with local search bias which strategies (after the first) get added and what the final r value is?

% Add to appendix: piecewise strategy 1 -- for the 3 type intervals (of 5) where the deviator can play at least one strategy and meet the reserve requirement, the actual piecewise best response mappings are nearly the same: {s1 or s3}, s4, s5, where s1 and s3 correspond to bidding valuation and differ only based on whether to try to best respond


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % %
% Conclusion  %
% % % % % % % %
\section{Conclusion} 

Through an in-depth study of EMD for a dynamic sponsored search auction scenario, we demonstrate the advantages of learning an interim model for Bayesian game families.
The interim model achieved low payoff error within the trained parameter range and in extrapolation, and was more effective overall at approximating Bayes-Nash equilibria than the ex ante model. 
Both of these benefits proved advantageous for parameter optimization---the relative smoothness of the expected revenue curve and the ability to extrapolate enabled us to identify the optimal reserve range via grid optimization.
Using the interim model in a local search algorithm produced reserve settings with, on average, revenue comparable to that obtained through grid optimization. 
The interim model also supports computation of piecewise best-response strategies without additional sampling, effectively guiding expansion of the strategy set for iterative EMD.

% The model was more effective at finding distinguishing $\varepsilon$-BNE from non-convergent mixed strategies returned by the equilibrium approximation algorithm. 
% Both these proved to be important for parameter optimization -- few to no discontinuities in the objective curve, and able to confirm optimal range via extrapolation. 




% In the interest of anonymization, please do not include acknowledgements in your submission.
%
%\begin{acks}
%
%	The authors would like to thank Dr. Maura Turolla of Telecom
%	Italia for providing specifications about the application scenario.
%
%	The work is supported by the \grantsponsor{GS501100001809}{National
%		Natural Science Foundation of
%		China}{http://dx.doi.org/10.13039/501100001809} under Grant
%	No.:~\grantnum{GS501100001809}{61273304\_a}
%	and~\grantnum[http://www.nnsf.cn/youngscientsts]{GS501100001809}{Young
%		Scientsts' Support Program}.
%
%
%\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\newpage
% Appendix
\appendix

% % % % % % % % % % % % % 
% Dynamic Game Details  % 
% % % % % % % % % % % % %
\section{Dynamic Game Details}
\label{app:dynamic_game_details}

In the auction game family, valuations, $\val$, are real numbers and bids are integers.
We assume that no bidder will bid above their valuation.
For the initial bid, a strategy specifies bidding $\max(\left\lfloor \val \right\rfloor - \mathit{offset}, 0)$, for some integer offset. 
For the final bid, a strategy specifies whether to submit a \term{best response bid}, an updated bid representing the optimal value based on the tentative slot allocations and prices.
We conducted significance tests to confirm that there were statistically significant differences in expected payoffs for each pair of strategies across the reserve price range. 

\subsection{10-Strategy Game Family}
For \S\ref{sec:ea_vs_i_learning}-\ref{sec:computing_pwbr_results}, we consider a game family with five offset increments, \{0, 2, 4, 6, 8\}, and consider strategies that involve submitting a best response bid update or not.  
The strategy set is characterized by: 

\begin{table}[ht]
\centering 
\begin{tabular}{c|c|c}
$S$ & Initial Bid & Final Bid \\ 
\hline 
$s_0$ & $\max(\left\lfloor \val \right\rfloor, 0)$ & No change \\
$s_1$ & $\max(\left\lfloor \val \right\rfloor -2, 0)$ & No change \\
$s_2$ & $\max(\left\lfloor \val \right\rfloor -4, 0)$ & No change \\
$s_3$ & $\max(\left\lfloor \val \right\rfloor -6, 0)$ & No change \\
$s_4$ & $\max(\left\lfloor \val \right\rfloor -8, 0)$ & No change \\
$s_5$ & $\max(\left\lfloor \val \right\rfloor, 0)$ & Best response bid \\
$s_6$ & $\max(\left\lfloor \val \right\rfloor - 2, 0)$ & Best response bid \\
$s_7$ & $\max(\left\lfloor \val \right\rfloor -4, 0)$ & Best response bid \\
$s_8$ & $\max(\left\lfloor \val \right\rfloor -6, 0)$ & Best response bid \\
$s_{9}$ & $\max(\left\lfloor \val \right\rfloor -8, 0)$ & Best response bid \\
\end{tabular}
\end{table}

\subsection{6-Strategy Game Family}
For \S\ref{sec:iter_emd_w_pwbr_results}, the initial strategy set is characterized by: 

\begin{table}[ht]
\centering 
\begin{tabular}{c|c|c}
$S$ & Initial Bid & Final Bid \\ 
\hline 
$s_0$ & $\max(\left\lfloor \val \right\rfloor, 0)$ & No change \\
$s_1$ & $\max(\left\lfloor \val \right\rfloor -4, 0)$ & No change \\
$s_2$ & $\max(\left\lfloor \val \right\rfloor -8, 0)$ & No change \\
$s_3$ & $\max(\left\lfloor \val \right\rfloor, 0)$ & Best response bid \\
$s_4$ & $\max(\left\lfloor \val \right\rfloor -4, 0)$ & Best response bid \\
$s_{5}$ & $\max(\left\lfloor \val \right\rfloor -8, 0)$ & Best response bid \\
\end{tabular}
\end{table}

For computing piecewise best responses, we reduce the two type parameters, $q$ and $\val$, to one dimension by taking the product $q\cdot \val$, representing a player's maximum effective bid. 
We partition this single-dimensional type space into 5 roughly equiprobable intervals, shown in Table~\ref{tab:type_intervals}.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c}
        & \textbf{Start} & \textbf{End} \\
        \hline
        $\interval_0$ & 0.        & 1.251829  \\
        $\interval_1$ & 1.251829  & 3.308699  \\
        $\interval_2$ & 3.308699  & 6.312033  \\
        $\interval_3$ & 6.312033  & 10.963049 \\
        $\interval_4$ & 10.963049 & 25.       \\
    \end{tabular}
    \caption{Type partition for used for computing piecewise best responses, where type is $q \cdot \val$.  The first column shows the starting cutoff, and the second column shows the ending cutoff.}
    \label{tab:type_intervals}
\end{table}

Table~\ref{table:game2_pwbr_strategy_spec} specifies the piecewise best-response strategy mappings computed and used during the iterative EMD procedure in \S\ref{sec:iter_emd_w_pwbr_results}.
We identify each evaluation run (row) based on the amount of training data used to train the initial NN model, and specify the two piecewise mappings computed for that evaluation run. 
For example, the piecewise best response strategy, $s_6$, computed using the neural network trained on 100k ($\vec{\sigma}, \reserve)$ pairs with 10 observations per pair maps types in interval $\interval_0$ to atomic strategy $s_2$ and types in interval $\interval_4$ to atomic strategy $s_5$.
Corresponding piecewise strategies differ slightly from row to row as a result of different identified optimal $\reserve$ values and differences in deviation payoff errors across the strategy space. 
Of note, the true deviation payoffs conditioned on having a type in interval $\interval_0$ are all zero for the four optimal $\reserve$ values returned by local search, as the reserve prices (all $\reserve \geq 3.15$) are larger than the maximum effective bid for this interval (1.25), so no player with type in this interval could meet the reserve requirement, regardless of which strategy they select. 
As the predicted payoffs are never exactly zero, the model selects the optimal strategy for the interval based on the strategy with the largest relative positive error (which is still quite small compared to the payoff scale).
We opted against manually specifying the associated atomic strategy (e.g., to $s_0$) to not bias the process based on domain-specific payoff information which may or may not be available in other Bayesian settings. 

\input{tables/game2_pwbr_strategies}


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % % %
% Game-Family Model Learning Details  % 
% % % % % % % % % % % % % % % % % % % %
\section{Game-Family Model Learning Details}
\label{app:game_learning_details}

% % % % % % % % % % % % % % % % % % %
% Training & Evaluation Information %
% % % % % % % % % % % % % % % % % % %
\subsection{Training \& Evaluation Information: 10-Strategy Game Family}
\label{app:training_info}

We include train-validation-test split information in Table~\ref{table:game1_train_val_test_split}, payoff statistics for all training datasets in Table~\ref{table:game1_train_payoff_stats} and all test datasets in Table~\ref{table:game1_test_payoff_stats}, and a summary of hyperparameters for all twelve neural networks in Table~\ref{table:game1_hyperparameters}.
For the interim models, the training loss was interim mean-squared error and the validation loss was marginalized interim mean-squared error using the 300 specific types associated with target test payoff samples for each $(\vec{\sigma}, v)$ pair. 

Note that due to a bug in the training data, the results had to be statistically adjusted to represent correct payoffs. 
The statistical adjustment gives unbiased results, but introduces some noise.
Since the ex ante and interim models were trained on the same underlying data, this adjustment does not impact the fairness of the comparison. 

\input{tables/game1_train_val_test_split}

\input{tables/game1_train_payoff_statistics}

\input{tables/game1_test_payoff_statistics}

\input{tables/game1_hyperparameters}

\newpage
\subsection{Training Information: 6-Strategy Game Family}
\label{app:game2_training_info}
Table~\ref{table:game2_nmix_per_iter} shows the number of distinct $(\vec{\sigma}, v)$ pairs in the training sets used for iterative EMD, where the superscript denotes the number of strategies in the game family and each row corresponds to a different evaluation run. 
The number of total training examples for a given model with $k$ strategies is the product between $\idx{m}{k}$ and the number of observations in the initial training set. 
We also report the raw number of new training examples for that iteration, all containing the newest piecewise strategy, as well as percent increase from $\idx{m}{k-1}$ to $\idx{m}{k}$.


\begin{table}[ht]
\begin{tabular}{c|c|c|c|c|c|c|c}
Initial Dataset & $\idx{\nmix}{6}$ & $\idx{\nmix}{7}$ & $\idx{\nmix}{7}$-$\idx{\nmix}{6}$ & \% incr & $\idx{\nmix}{8}$ & $\idx{\nmix}{8}$-$\idx{\nmix}{7}$ & \% incr \\
\hline
(100k, 10)      & 100000                               & 188308                               & +88308                                                 & 88.31\%                     & 353004                               & +164696                                                & 87.46\%                     \\
(50k, 20)       & 50000                                & 94594                                & +44594                                                 & 89.19\%                     & 166324                               & +71730                                                 & 75.83\%                     \\
(100k, 5)       & 100000                               & 186314                               & +86314                                                 & 86.31\%                     & 298003                               & +111689                                                & 59.95\%                     \\
(50k, 10)       & 50000                                & 89122                                & +39122                                                 & 78.24\%                     & 147293                               & +58171                                                 & 65.27\%               
\end{tabular}
\caption{The number of $(\vec{\sigma}, \reserve)$ pairs in each training set for each iteration of EMD. The total number of training examples is equal to the product of $\nmix$ and the number of initial observations.}
\label{table:game2_nmix_per_iter}
\end{table}

Table~\ref{table:game2_max_pbr_prob} shows the maximum probability that each piecewise strategy is played in any mixed strategy in the particular training set. 
When the piecewise strategy is in the support, the probability it is played is based on the proportion of opponents who can be attributed to playing the piecewise strategy across the $\nobs$ observations. 
So, the maximum probability is determined by the particular data available in the original training set. 

\begin{table}[ht]
\begin{tabular}{c|c|c}
Initial Dataset & \multicolumn{1}{l|}{$\pbr_0 = s_6$} & \multicolumn{1}{l}{$\pbr_1 = s_7$} \\
\hline 
(100k, 10)      & 0.613                              & 0.413                              \\
(50k, 20)       & 0.544                              & 0.556                              \\
(100k, 5)       & 0.675                              & 0.875                              \\
(50k, 10)       & 0.750                              & 0.775                             
\end{tabular}
\caption{The maximum probability that each piecewise strategy is played in any mixed strategy in the training set for each evaluation run.}
\label{table:game2_max_pbr_prob}
\end{table}

% % % % % % % %
% ML Test Set %
% % % % % % % %
\subsection{ML Test Set}
\label{app:ml_test}

We evaluate the method of using marginalized interim deviation payoffs with $n$ Monte Carlo type samples for marginalization, for each $n \in \{1000, 5000, 10000\}$.
For a given learned interim model $\hat{u}$ with $n$ marginalization samples and a particular $(\vec{\sigma}, v)$ pair, the marginalized interim deviation payoff vector is computed by: $\frac{1}{n}\sum_{i=1}^{n}\hat{u}(\vec{\sigma}, v \mid \idx{t}{i})$, where $\idx{t}{i}\sim \mu$.
The MSE for a given interim model $\hat{u}$ with $n$ marginalization samples is equal to the squared error between the ground truth deviation payoff vector and the marginalized interim deviation payoff vector, averaged across all strategies and the 1000 $(\vec{\sigma}, v)$ pairs in the test set. 
The MSE for the marginalized interim method with $n$ samples is the average over each learned interim model's MSE when using $n$ marginalization samples.

ML test set \#1 (used for Fig.~\ref{fig:ml_test_results}) is known to be noisy, so for further method validation, we provide results using two related test sets, each containing the same set of $(\vec{\sigma}, v)$ pairs as ML test set \#1. 
ML test set \#1 has 300 observations for each $(\vec{\sigma}, v)$; ML test set \#2 has 500 observations for each $(\vec{\sigma}, v)$; ML test set \#3 has 700 observations per pair. 
We follow the same process used to generate the plots in Fig.~\ref{fig:ml_test_results} with ML test set \#1 to generate corresponding plots for ML test set \#2 (Plots (a) and (c)) and ML test set \#3 (Plots (b) and (d)).
Observe that the relative performance trends among the 5 methods are the same for Fig.~\ref{fig:ml_test_results}(a), Fig.~\ref{fig:ml_test_results_more_obs}(a) and Fig.~\ref{fig:ml_test_results_more_obs}(b); the only noteworthy difference is that as the number of observations per pair increases, the magnitude of the deviation payoff error decrease. 
The same is true for Fig.~\ref{fig:ml_test_results}(b), Fig.~\ref{fig:ml_test_results_more_obs}(c), and Fig.~\ref{fig:ml_test_results_more_obs}(d). 

\begin{figure*}[ht]
\centering
\begin{tabular}{ccc}

\includegraphics[scale=0.2]{plots/ea_v_ei_binned_r_test_500ob.png} && 
\includegraphics[scale=0.2]{plots/ea_v_ei_binned_r_test_700ob.png} \\
(a) && (b) \\ 

\includegraphics[scale=0.2]{plots/ea_v_ei_per_NN_500ob.png} && 
\includegraphics[scale=0.2]{plots/ea_v_ei_per_NN_700ob.png} \\
(c) && (d) \\
\end{tabular}
\caption{Top: Deviation payoff errors across the reserve price range using (a) ML Test Set \#2 (500 observations per pair) and (b) ML Test Set \#3 (700 observations per pair) show a similar trend across methods compared to Fig.~\ref{fig:ml_test_results}(a) which uses ML Test Set \#1 (300 observations per pair). 
Bottom: Deviation payoff errors across training datasets using (c) ML Test Set \#2 and (d) ML Test set \#3 show a similar trend across methods compared to Fig.~\ref{fig:ml_test_results}(b).}
\label{fig:ml_test_results_more_obs}
\end{figure*}

% % % % % % % % % % % % % % % % % % %
% Fine-Grained Grid Mixture Results % 
% % % % % % % % % % % % % % % % % % %
\subsection{Fine-Grained Grid Mixture Results}

The results in Fig.~\ref{fig:extra_test_results} aggregate performance for each (ex ante, interim) method across the associated six learned models.
In this section we present the same results as shown in Fig.~\ref{fig:extra_test_results}, but instead show the performance for each of the twelve learned models. 
Fig.~\ref{fig:extra_test_results_per_NN} aggregates over reserve prices, and Fig.~\ref{fig:extra_test_results_per_NN_1k_fill} shows performance for each learned model across the reserve space.

\label{app:extra_test}
\begin{figure*}[ht]
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.2]{plots/ex_ante_vs_cond_dp_extra_test_per_NN_rleq8.png} && 
\includegraphics[scale=0.2]{plots/ex_ante_vs_cond_dp_extra_test_per_NN_rleq15.png} \\
(a) && (b) \\ 
\end{tabular}
\caption{(a) The same results as Fig.~\ref{fig:extra_test_results}(a) except performance is now shown for each learned model (one for each training set and method), aggregated over the reserve range $0.05 \leq r \leq 8$.
(b) The same results as Fig.~\ref{fig:extra_test_results}(b) except performance is now shown for each learned model, aggregated over the reserve range $0.05 \leq r \leq 15$.}
\label{fig:extra_test_results_per_NN}
\end{figure*}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.2]{plots/ex_ante_vs_cond_dp_1k_fill_per_NN_extra_test_rleq15.png}
\caption{The same results as Fig.~\ref{fig:extra_test_results}(b) except performance is now shown for each learned model for the reserve range $0.05 \leq r \leq 15$.}
\label{fig:extra_test_results_per_NN_1k_fill}
\end{figure}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % % %
% Using the Learned Game-Family Model % 
% % % % % % % % % % % % % % % % % % % %
\section{Using the Learned 10-Strategy Game-Family Model}
\label{app:use_learned_model}

% % % % % % % % % % % %
% Deriving Equilibria % 
% % % % % % % % % % % %
\subsection{Deriving Equilibria}
\label{app:nash_approximation}

Fig.~\ref{fig:ea_vs_ei_cand_ne_regr_abs_err} shows the same results as in Fig.~\ref{fig:cand_ne_regr_abs_err} except performance is now shown for (a) each ex ante model and (b) each interim model.
We also include tables of mixture classification results for each method and model for $r \leq 8$ (Table~\ref{table:game1_mix_class_rleq8}) and for $8 < r \leq 15$ (Table~\ref{table:game1_mix_class_rgt8}).

\begin{figure*}[ht]
\centering
\begin{tabular}{ccc}

\includegraphics[scale=0.2]{plots/ea_cand_ne_regr_abs_err_eps=0.01.png} && 
\includegraphics[scale=0.2]{plots/cd_cand_ne_regr_abs_err_eps=0.01.png} \\
(a) && (b) \\ 
\end{tabular}
\caption{Regret absolute error for candidate equilibria found using each (a) ex ante model and (b) interim model. For ex ante and interim models trained on the same training set, most often the interim model  has lower regret error compared to the ex ante model.}
\label{fig:ea_vs_ei_cand_ne_regr_abs_err}
\end{figure*}

\input{tables/game1_mix_classification_rleq8}
\input{tables/game1_mix_classification_rgt8}



% % % % 
% EMD %
% % % %
\subsection{EMD}
\label{app:emd}

When we plot the expected revenue in equilibrium in Figure~\ref{fig:emd_grid_optimization}, we plot the expected revenue for \textit{confirmed equilibria}. 
So, if all candidate equilibria identified by a given model have true regret larger than $\varepsilon = 0.01$ for a given game instance, there is a hole in the expected revenue curve located at the associated reserve value. 
Table~\ref{table:game1_holes} shows the number of holes in the expected revenue curve for each model, separated by the game instances covered by the trained range (160 game instances with $\reserve \leq 8$) and beyond (140 game instances with $\reserve > 8)$.
We also report the average number of holes per learning method for the trained range and beyond, and we note these values in the main body. 

\input{tables/game1_grid_opt_holes}

We describe how to use the learned Bayesian game-family model for local search; see Alg.~\ref{alg:local_search_emd} for pseudocode. %shows how the learned Bayesian game-family model may be used in a local search algorithm.  
For some initial parameter $v$, we use the learned model to find at least one $\varepsilon$-BNE, denoted by $\vec{\sigma}_*$, in game instance $\Gamma(v)$.
Given a set of $\varepsilon$-BNE in $\Gamma(v)$, we apply the pre-specified equilibrium selection criteria and compute the equilibrium objective value, $z_*$.
Then we select some a neighbor $v'$ and get equilibrium and objective function information, $(\vec{\sigma}'_*, z'_*)$, for game instance $\Gamma(v')$.
According to the local search algorithm and the values $z_*$ and $z'_*$, we determine which game instance to explore next.
In general, if $z'_*$ is better than $z_*$, then we accept neighbor $v'$ as our next parameter and relabel as $v$. 
In the case of simulated annealing, for example, the probability of accepting a game instance with a worse objective value in equilibrium decreases as the number of iterations increases, as determined by the temperature cooling schedule. 
This algorithm can easily be extended to look at multiple neighbors at each iteration. 
In stochastic hill climbing, the algorithm may select a neighbor $v'$ with probability proportional to the relative improvement in $z'_*$ compared to the other neighboring game instances. 
This process is repeated for a pre-specified number of iterations, and the optimal game instance has the best equilibrium objective value out of the set of game instances evaluated.  


\begin{algorithm}[ht]
\DontPrintSemicolon
\caption{EMD with a general local search algorithm using a learned Bayesian game family.}
\label{alg:local_search_emd}
\KwIn{\textit{gameFamModel}, \textit{numIters}}
\Begin{
    $v \gets \text{getInitParam()}$\;
    $(\vec{\sigma}_*, v) \gets \text{findNash}(\textit{gameFamModel}, \text{param}=v)$\;
    $z_* \gets \text{ComputeNECriteria}(\vec{\sigma}_*, v)$\;
    $\text{history} \gets \{(v, z_*)\}$\;
    
    \Repeat{\textit{numIters}}{
        $v' \gets \text{getNeighborParam}(v)$\;
        $(\vec{\sigma}'_*, v') \gets \text{findNash}(\textit{gameFamModel}, \text{param}=v')$\;
        $z_*' \gets \text{ComputeNECriteria}(\vec{\sigma}'_*, v')$\;
        $(v, z_*) \gets \text{getNextState}((v, z_*), (v', z_*'))$\;
        $\text{history}[v] \gets z_*$\;
    }
    
    \Return $\displaystyle \arg\max_{v} \,\text{history}[v]$\;
}
\end{algorithm}

\newpage

Using the results from Fig.~\ref{fig:local_search_results}, we compute the percent decrease in revenue from the optimal grid revenue to the average revenue for each method and search algorithm, shown in Table~\ref{table:game1_pct_decr_results}:

\input{tables/game1_ls_pct_decr_rev}


Concurrent with computing bootstrap confidence intervals for optimal expected revenue in equilibrium, we also compute bootstrap confidence intervals for the average number of game instances explored using local search. 
Specifically, for each of the 100,000 5-restart experiment samples (described in \S\ref{sec:local_search}), we determine the number of distinct game instances evaluated across the 5 restarts.
In Fig.~\ref{fig:num_game_instances_evaluated}, we plot the resulting 95\% confidence intervals around the average total number of game instances explored. 


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{plots/avg_game_inst_eval_conf_ne_5restart_100k_bootstrap.png}
    \caption{The average number of distinct game instances evaluated in local search experiments with 5 restarts. 
    Bootstrap confidence intervals are computed concurrently with the intervals in Fig.~\ref{fig:local_search_results}. 
    On average, local search with 5 restarts evaluates roughly 100-125 distinct game instances compared to 300 game instances for grid optimization (30-40\%).}
    \label{fig:num_game_instances_evaluated}
\end{figure}

% \newpage
% \subsection{Computing Piecewise Best Responses}
% Fig.~\ref{fig:pred_vs_true_pct_incr_per_NN} shows the same results from Fig.~\ref{fig:pct_incr_pred_and_gt} separating performance for each trained interim model model. 

% \label{app:pbr}
% \begin{figure*}[ht]
% \centering
% \begin{tabular}{ccc}
% \includegraphics[scale=0.25]{plots/pct_incr_pred_dp.png}
% &&
% \includegraphics[scale=0.25]{plots/pct_incr_gt_dp.png} \\
% (a) && (b) \\
% \end{tabular}
% \caption{(a) Predicted vs (b) true percent increase in payoff from playing a piecewise best response to equilibrium computed using each interim model.}
% \label{fig:pred_vs_true_pct_incr_per_NN}
% \end{figure*}



\end{document}
