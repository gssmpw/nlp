\section{Related Work}
\label{related}
The foundational theories of option pricing and hedging, such as the Black-Scholes and Merton models, have established essential frameworks for valuing financial derivatives and guiding risk management strategies. However, these models encounter significant challenges in adapting to the dynamic and rapidly evolving conditions of modern financial markets________. Advanced computational methods were developed to better address the uncertainties and volatilities in financial trading systems________.

Dynamic Vega hedging, crucial for managing volatility risks in financial derivatives, has evolved with the increasing application of machine learning techniques, particularly RL____. This transition highlights a significant shift from traditional methods to strategies capable of adapting to complex market dynamics____.

Recent advancements in reinforcement learning have expanded its utility in financial markets, allowing for the development of sophisticated models that learn and adapt through iterative interactions with market data. The integration of deep learning with RL has further enhanced the capability of these models to process large volumes of data and extract actionable insights for effective risk management________.

A key innovation in this area is the application of distributional perspectives to RL, which models the entire distribution of possible outcomes rather than just the expected values________. This approach provides a comprehensive understanding of potential risks and returns, facilitating more robust financial decision-making____. Implicit Quantile Networks, which utilize quantile regression within distributional RL frameworks, exemplify this advancement by enabling the estimation of potential financial outcomes without predefined distributional assumptions________.

Furthermore, the field has seen significant enhancements in algorithm efficiency and effectiveness, particularly through the development of algorithms like Distributional Policy Optimization, which optimize policy decisions in complex, continuous control tasks common in financial applications____. These innovations underscore the growing importance of adaptive and sophisticated computational techniques in the effective management of financial risks.

Despite recent advancements in applying reinforcement learning to hedging, the integration of cutting-edge RL methods with sophisticated optimization techniques, especially for Vega hedging, remains relatively unexplored. Existing work often lacks the adaptability required to handle the rapid fluctuations and tail risks characteristic of modern markets, where traditional RL methods may struggle to achieve stable, efficient performance. Key open challenges include the need for faster convergence, heightened sensitivity to volatility shifts, and improved accuracy in risk management across a range of possible outcomes. Addressing these gaps, this paper introduces an innovative approach that combines distributional RL with adaptive Nesterov acceleration, aiming to enhance stability, precision, and responsiveness in Vega hedging applications.

\begin{table}[t]
	\caption{Description of Notations in Training Procedure}
	\label{sample-table}
	\begin{center}
		\begin{tabular}{ll}
			\multicolumn{1}{c}{\bf NOTATION}  &\multicolumn{1}{c}{\bf DESCRIPTION}
			\\ \hline \\
			$\mathcal{S}$ & Set of states\\
			$\mathcal{A}$ & Set of actions\\
			$\kappa$      &  The transaction cost associated with the option used for hedging\\
			$p(s' | s, a)$ & Transition probability from state $s$ to state $s'$ under action $a$.\\
			$\mathcal{D}$ & Replay buffer containing experiences $(s, a, r, s')$\\
			$H_{\tau}(\delta)$& Huber loss function with parameter $\tau$\\
			$\theta$ & Parameters of the distributional RL model.\\
			$\mathcal{L}(\theta)$ & Loss function parameterized by $\theta$.\\
			$\tau$ & Index representing the quantile level.\\
			$\gamma$ & Discount factor for future rewards.\\
			$Z_{\tau}$& Target distribution for the $\tau$-th quantile\\
			$\delta_{\tau}$ & Temporal difference error for the $\tau$-th quantile \\
			$\pi(s)$ & Softmax policy for state $s$ parameterized by $\theta$.
			\\ \hline
		\end{tabular}
	\end{center}
\end{table}