%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[toc,page]{appendix}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algpseudocode}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{xcolor}
\newcommand\prince[1]{\textcolor{blue}{Prince: #1}}
\newcommand\ryan[1]{\textcolor{orange}{Ryan: #1}}
\newcommand\woody[1]{\textcolor{red}{Woody: #1}}
\newcommand\nf[1]{\textcolor{purple}{$^\textbf{NF}$: [#1]}}
\newcommand\jinhao[1]{\textcolor{brown}{Jinhao: #1}}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Generative Decision-Focused-Learning for Robust Optimization (just a placeholder)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Prince Zizhuang Wang}{yyy}
% % \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Shuyi Chen}{yyy}
\icmlauthor{Jinhao Liang}{uva}
\icmlauthor{Ferdinando Fioretto}{uva}
\icmlauthor{Shixiang Zhu}{yyy}
% % \icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Heinz College, Carnegie Mellon University, USA}
\icmlaffiliation{uva}{Computer Science, University of Virginia, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Prince Wang}{princewang@cmu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

\end{abstract}

\section{Theorems}

\newpage
\onecolumn
\appendix

% \begin{theorem}[Dimension-Dependent Regret Bounds]
% Let $p(c|x)$ denote the true conditional distribution of $c$, and let $\hat{p}(c|x)$ be its empirical estimate from $n$ samples. Define:
% \[
% \hat{w}^\star(\theta) = \arg\min_{w\in \mathcal{W}} \ \mathbb{E}_{\hat{c}\sim p_\theta(c|x)}[\hat{c}^T w]
% \]
% and
% \[
% w^\star = \arg\min_{w\in \mathcal{W}} \ \mathbb{E}_{c\sim p(c|x)}[c^T w].
% \]
% For the Pred-DFL, define the regret as:
% \[
% R_{\text{pred}}(x) = |c^T(\hat{w}^\star - w^\star)|,
% \]
% For the Gen-DFL with empirical distribution, define:
% \[
% \hat{R}_{\text{gen}}(x) = \mathbb{E}_{c \sim \hat{p}(c|x)} [c^T(\hat{w}^\star - w^\star)].
% \]

% Assume:
% \begin{enumerate}
%     \item $w^\star(c)$ is Lipschitz continuous with constant $L_w$:
%     \[
%     \|w^\star(c_1) - w^\star(c_2)\| \leq L_w \|c_1 - c_2\|
%     \]
%     \item The conditional distribution $p(c|x)$ has the structure:
%     \[
%     c = \bar{r} + Lf + \sigma\xi
%     \]
%     where $L \in \mathbb{R}^{d\times k}$ is a low-rank factor loading matrix, $f \sim N(0, I_k)$ represents systematic factors, and $\xi \sim N(0, I_d)$ represents idiosyncratic noise.
%     \item The empirical distribution $\hat{p}(c|x)$ is estimated from $n$ independent samples.
% \end{enumerate}

% Then, the regret gap between the generative and predictive DFL is upper bounded as:
% \[
% |R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)| \leq L_w\left(\frac{C_1k}{\sqrt{d}} + \sigma^2\right)(1 + \|\hat{c}\|) + C_2\mathcal{W}_2(\hat{p}(c|x), p(c|x)),
% \]
% where $C_1, C_2$ are constants, and:
% \begin{itemize}
%     \item The term $\frac{C_1k}{\sqrt{d}}$ captures systematic risk that decreases with dimension
%     \item The term $\sigma^2$ represents constant idiosyncratic risk
%     \item The term $\mathcal{W}_2(\hat{p}(c|x), p(c|x))$ is the 2-Wasserstein distance between empirical and true distributions, which is bounded by $O(\sqrt{d/n})$ with high probability
% \end{itemize}
% \end{theorem}

% \begin{proof}
% Step 1: Triangle Inequality Decomposition
% \[
% |R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)| \leq |R_{\text{pred}}(x) - R_{\text{gen}}(x)| + |R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)|
% \]

% Step 2: First Term Bound (from original proof)
% \[
% |R_{\text{pred}}(x) - R_{\text{gen}}(x)| \leq L_w \|\text{Var}[c|x]\|_F \big(1 + \|\hat{c}\|\big)
% \]

% Step 3: Second Term Bound
% The term $|R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)|$ represents the difference between expectations under true and empirical distributions:
% \[
% |\mathbb{E}_{c\sim p(c|x)}[c^T(\hat{w}^\star - w^\star)] - \mathbb{E}_{c\sim \hat{p}(c|x)}[c^T(\hat{w}^\star - w^\star)]|
% \]

% This can be bounded using the Wasserstein distance:
% \[
% |R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)| \leq C\mathcal{W}_2(\hat{p}(c|x), p(c|x))
% \]

% Step 4: By known concentration results for empirical measures, with high probability:
% \[
% \mathcal{W}_2(\hat{p}(c|x), p(c|x)) \leq O(\sqrt{\frac{d}{n}})
% \]

% Therefore:
% \[
% |R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)| \leq L_w \|\text{Var}[c|x]\|_F \big(1 + \|\hat{c}\|\big) + C\sqrt{\frac{d}{n}}
% \]
% \end{proof}

% \begin{theorem}[Expected Dimension-Dependent Regret Bounds]
% Let $p(c|x)$ denote the true conditional distribution of $c$, and let $\hat{p}(c|x)$ be its empirical estimate from $n$ samples. Let $f(x)$ be a predictor that estimates $\mathbb{E}_{c\sim p(c|x)}[c|x]$. Define:

% For the Pred-DFL:
% \[
% \hat{w}^\star_{\text{pred}}(x) = \arg\min_{w\in \mathcal{W}} \ [f(x)^T w]
% \]

% For the Gen-DFL:
% \[
% \hat{w}^\star_{\text{gen}}(x) = \arg\min_{w\in \mathcal{W}} \ \mathbb{E}_{c\sim \hat{p}(c|x)}[c^T w]
% \]

% Let the true optimal solution be:
% \[
% w^\star = \arg\min_{w\in \mathcal{W}} \ \mathbb{E}_{c\sim p(c|x)}[c^T w]
% \]

% Assume:
% \begin{enumerate}
%     \item $w^\star(c)$ is Lipschitz continuous with constant $L_w$:
%     \[
%     \|w^\star(c_1) - w^\star(c_2)\| \leq L_w \|c_1 - c_2\|
%     \]
%     \item The conditional distribution $p(c|x)$ has the structure:
%     \[
%     c = \mathbb{E}_{c\sim p(c|x)}[c|x] + \epsilon
%     \]
%     where $\epsilon = Lf + \sigma\xi$, with $L \in \mathbb{R}^{d\times k}$ being a low-rank factor loading matrix, $f \sim N(0, I_k)$ representing systematic factors, and $\xi \sim N(0, I_d)$ representing idiosyncratic noise.
%     \item The predictor bias is defined as:
%     \[
%     \text{Bias}[f] = f(x) - \mathbb{E}_{c\sim p(c|x)}[c|x]
%     \]
% \end{enumerate}

% Then:
% \[
% \mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq L_w\left(\frac{C_1k}{\sqrt{d}} + \sigma^2\right)(1 + \|\mathbb{E}_{c\sim p(c|x)}[c|x]\|) + L_w\mathbb{E}_x\|\text{Bias}[f]\| + C_2\sqrt{\frac{d}{n}}
% \]

% where:
% \begin{itemize}
%     \item The first term captures the variance effect which decreases with dimension for systematic risk but remains constant for idiosyncratic risk
%     \item The second term captures how predictor bias affects the regret gap
%     \item The third term reflects finite-sample effects in estimating the empirical distribution
% \end{itemize}
% \end{theorem}

% \begin{proof}
% Step 1: Decompose the expected regret gap.
% \[
% \mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq \mathbb{E}_x[|R_{\text{pred}}(x) - R_{\text{gen}}(x)|] + \mathbb{E}_x[|R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)|]
% \]

% Step 2: For the first term, consider $R_{\text{pred}}(x)$:
% \[
% R_{\text{pred}}(x) = |c^T(\hat{w}^\star_{\text{pred}} - w^\star)|
% \]
% \[
% = |(\mathbb{E}_{c\sim p(c|x)}[c|x] + \epsilon)^T(\hat{w}^\star_{\text{pred}} - w^\star)|
% \]
% \[
% = |(\mathbb{E}_{c\sim p(c|x)}[c|x] + \epsilon)^T(\hat{w}^\star_{\text{pred}} - \hat{w}^\star_{\text{gen}} + \hat{w}^\star_{\text{gen}} - w^\star)|
% \]

% Step 3: By Lipschitz continuity:
% \[
% \|\hat{w}^\star_{\text{pred}} - \hat{w}^\star_{\text{gen}}\| \leq L_w\|f(x) - \mathbb{E}_{c\sim p(c|x)}[c|x]\| = L_w\|\text{Bias}[f]\|
% \]

% Step 4: Using triangle inequality and Cauchy-Schwarz:
% \[
% |R_{\text{pred}}(x) - R_{\text{gen}}(x)| \leq L_w\|\text{Bias}[f]\|\|\mathbb{E}_{c\sim p(c|x)}[c|x]\| + L_w\|\epsilon\|^2
% \]

% Step 5: For the variance term $\|\epsilon\|^2$:
% \[
% \mathbb{E}[\|\epsilon\|^2] = \|LL^T\|_F + d\sigma^2 \leq \frac{C_1k}{\sqrt{d}} + \sigma^2
% \]

% Step 6: For the empirical estimation term:
% \[
% |R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)| \leq C_2\mathcal{W}_2(\hat{p}(c|x), p(c|x)) \leq C_2\sqrt{\frac{d}{n}}
% \]

% Step 7: Combining all terms and taking expectation over x:
% \[
% \mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq L_w\left(\frac{C_1k}{\sqrt{d}} + \sigma^2\right)(1 + \|\mathbb{E}_{c\sim p(c|x)}[c|x]\|) + L_w\mathbb{E}_x\|\text{Bias}[f]\| + C_2\sqrt{\frac{d}{n}}
% \]
% \end{proof}

% \newpage
\begin{theorem}
\label{theorem:ultimate}
Let $p(c|x)$ denote the true conditional distribution of $c$, and let $\hat{p}(c|x)$ be its empirical estimate from $n$ samples. $p_\theta(c|x)$ denotes the learned generative model. Let $g(x)$ be a predictor that estimates $\mathbb{E}_{c\sim p(c|x)}[c|x]$. Define:

For the Pred-DFL, define the regret as:
\[
R_{\text{pred}}(x) = |c^T(\hat{w}_{\text{pred}}^\star - w^\star)|,
\]
For the Gen-DFL with empirical distribution, define:
\[
\hat{R}_{\text{gen}}(x) = \mathbb{E}_{c \sim \hat{p}(c|x)} [c^T(\hat{w}_{\text{gen}}^\star - w^\star)].
\]

where, $\hat{w}^\star_{\text{pred}}(x) = \arg\min_{w\in \mathcal{W}} \ [g(x)^T w]$, $\hat{w}^\star_{\text{gen}}(x) = \arg\min_{w\in \mathcal{W}} \ \mathbb{E}_{c\sim p_\theta(c|x)}[c^T w]$.

And, let the true optimal solution be:
\[
w^\star = \arg\min_{w\in \mathcal{W}} \ \mathbb{E}_{c\sim p(c|x)}[c^T w]
\]

Assume:
\begin{enumerate}
    \item $w^\star(c)$ is Lipschitz continuous with constant $L_w$:
    \[
    \|w^\star(c_1) - w^\star(c_2)\| \leq L_w \|c_1 - c_2\|
    \]
    \item The conditional distribution $p(c|x)$ has the structure:
    \[
    c = \bar{c} + Lf + \sigma\xi
    \]
    where $\bar{c} = \mathbb{E}_{c\sim p(c|x)}[c|x]$,  $L \in \mathbb{R}^{d\times k}$ is a low-rank factor loading matrix, $f \sim N(0, I_k)$ represents systematic factors, and $\xi \sim N(0, I_d)$ represents idiosyncratic noise.

    The benefit of constructing $c$ in this way is that we can break down the variance of $p(c|x)$ into two parts, that is,
    \[
    \text{Var}[c|x] = LL^T + \sigma^2 I = \text{systematic noise} + \text{idiosyncratic noise}
    \]
    \item The predictor bias is defined as:
    \[
    \text{Bias}[g] = g(x) - \mathbb{E}_{c\sim p(c|x)}[c|x]
    \]
\end{enumerate}

Then, the regret difference that demonstrates the performance gap between Pred-DFL and Gen-DFL is upper bounded by:
\[
\mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq L_w\mathbb{E}_x||Var[c|x]||(1 + \mathbb{E}_x\|\mathbb{E}_{c\sim p(c|x)}[c|x]\|) + L_w\mathbb{E}_x \mathbb{E}_{c\sim p(c|x)}||c|| \|\text{Bias}[g]\| + C_2\sqrt{\frac{d}{n}}
\]

where $C_1, C_2$ are some constants, $||Var[c|x]|| = \mathcal{O}(\sigma^2 \sqrt{d})$, and:
\begin{itemize}
    \item The term $\frac{C_1k}{\sqrt{d}}$ captures systematic risk that decreases with dimension
    \item The term $\sigma^2$ represents constant idiosyncratic risk
    \item The term $\mathcal{W}_2(\hat{p}(c|x), p(c|x))$ is the 2-Wasserstein distance between empirical and true distributions, which is bounded by $O(\sqrt{d/n})$ with high probability
\end{itemize}
\end{theorem}

To prove the above theorem, we will need the following lemmas:
\begin{lemma}
For the given data generation process:
\[
c = \bar{c} + Lf + \sigma\xi,
\]
the conditional variance has the decomposition:
\[
\text{Var}[c|x] = LL^T + \sigma^2I_d
\]
where $\|LL^T\|_F = O(k)$ for fixed factor dimension $k$.
\end{lemma}

\begin{lemma}[High-Dimensional Scaling]
For fixed factor dimension $k$ and increasing data dimension $d$:
\[
\frac{\|LL^T\|_F}{\sqrt{d}} \to 0 \text{ as } d \to \infty
\]
This follows because $\|LL^T\|_F^2 = \text{tr}(LL^TLL^T) = O(k)$ for fixed $k$.
\end{lemma}

\begin{lemma}
For the systematic and idiosyncratic decomposition:
\[
\epsilon = L(f - \mathbb{E}[f]) + \sigma\xi
\]

This gives:
\[
\|\text{Var}[c|x]\|_F = \|LL^T + \sigma^2I_d\|_F \leq\sqrt{C_1k + 2\sigma^2 k + \sigma^4 d}
\]    
\end{lemma}

\begin{proof}[proof of Theorem \ref{theorem:ultimate}]

Step 1: Decomposing the Regret Gap.

From the definitions of \( \hat{R}_{\text{gen}}(x) \) and \( R_{\text{pred}}(x) \), the regret gap is:
\[
\Delta R(x) = \hat{R}_{\text{gen}}(x) - R_{\text{pred}}(x),
\]

By triangular inequality, we have,
\[
|\Delta R(x)| = |R_{\text{gen}}(x) - R_{\text{pred}}(x) + \hat{R}_{\text{gen}}(x) - R_{\text{gen}}(x)| \le |R_{\text{gen}}(x) - R_{\text{pred}}(x)| + |\hat{R}_{\text{gen}}(x) - R_{\text{gen}}(x)|
\]

And, since we wrote $c$ as 
\[
c = \bar{c} + \epsilon
\]
Note that to differentiate this $c$ from the samples of $p(c|x)$, we use $c_\text{true}$ interchanglely.

we can further expand $R_\text{gen}$ as
\[
R_{\text{gen}}(x) = \mathbb{E}_{c \sim p(c|x)} \big| \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) + \epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big|,
\]

and
\[
R_{\text{pred}}(x) = \big| c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big|.
\]

where $\hat{c} = \mathbb{E}_{c \sim p_\theta(c|x)}[c|x]$ and $c_\text{pred} = f_\theta(x)$

Expanding the gap:
\[
\Big|\Delta R(x)\Big| \le \Big|\Big|\mathbb{E}_{c \sim p(c|x)} \big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) - c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big]\Big|\Big| + \Big|\Big|\mathbb{E}_{c\sim p(c|x)}\big[\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big]\Big|\Big|  + \Big|\hat{R}_{\text{gen}}(x) - R_{\text{gen}}(x)\Big|.
\]

Step 2. Bounding the first term involving $\bar{c}$

For the first term, we observe that:
\[
\mathbb{E}_{c \sim p(c|x)} \Big| \big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) - c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big] \Big| \le \mathbb{E}_{c \sim p(c|x)} \Big|\big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big]\Big| + \mathbb{E}_{c\sim p(c|x)} \Big| \big[ c^\top \big(w^\star(c_\text{pred}) - w^\star\big)\big] \Big|
\]

Since, \( \|w^\star_{\text{gen}}(\hat{c}) - w^\star(c_\text{true})\| \leq L_w \|c_\text{true} - \hat{c}\| = L_w \|\epsilon\| \), we can bound:
\[
\big\|\mathbb{E}_{c \sim p(c|x)}[w^\star_\text{gen}(\hat{c}) - w^\star]\big\| \leq L_w \|\text{Var}[c|x]\|_F.
\]

Thus, the difference is bounded by:
\[
\Big|\Big|\mathbb{E}_{c \sim p(c|x)} \big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) - c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big]\Big|\Big| \leq \|\bar{c}\| \cdot L_w \|\text{Var}[c|x]\|_F + \mathbb{E}_{c\sim p(c|x)}\big[||c||\big] L_w || \text{Bias}[f] ||.
\]

Step 3: Bounding the \(\epsilon\)-Term.

For the second term:
\[
\mathbb{E}_{c\sim p(c|x)}\big[\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big],
\]
use the Cauchy-Schwarz inequality:
\[
\big|\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big)\big| \leq \|\epsilon\| \cdot \|w_{\text{gen}}^\star(\hat{c}) - w^\star\|.
\]

Since \( \|w_{\text{gen}}^\star(\hat{c}) - w^\star(c_\text{true})\| \leq L_w \|\epsilon\| \), we have:
\[
\big|\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star(c_\text{true})\big)\big| \leq L_w \|\epsilon\|^2.
\]

Taking the expectation over \( c \sim p(c|x) \):
\[
\Big|\Big|\mathbb{E}_{c\sim p(c|x)}\big[\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big]\Big|\Big \le \mathbb{E}_{c \sim p(c|x)} \big|\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star(c_\text{true})\big)\big| \leq L_w \mathbb{E}_{c \sim p(c|x)}[\|\epsilon\|^2] = L_w \|\text{Var}[c|x]\|_F.
\]

Step 4: For the empirical estimation term:
\[
|R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)| \leq C_2\mathcal{W}_2(\hat{p}(c|x), p(c|x)) \leq C_2\sqrt{\frac{d}{n}}
\]

Finally, combining all the above steps and taking expectation over x, we get:
\[
\mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq L_w\mathbb{E}_x||Var[c|x]||(1 + \mathbb{E}_x\|\mathbb{E}_{c\sim p(c|x)}[c|x]\|) + L_w\mathbb{E}_x \mathbb{E}_{c\sim p(c|x)}||c|| \|\text{Bias}[f]\| + C_2\sqrt{\frac{d}{n}}
\]
\end{proof}




\section{CVaR bounds}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREM: CVaR-based Regret Difference
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}[CVaR-based Regret Difference]
\label{theorem:cvar_extension}
Let $p(c \mid x)$ denote the true conditional distribution of $c$, 
and let $\hat{p}(c \mid x)$ be its empirical estimate from $n$ i.i.d.\ samples.
Suppose $p_\theta(c \mid x)$ is a learned generative model.
Define $Q_c$ to be the “worst $\alpha\%$ tail” representative for $c$ under $p(c \mid x)$, 
i.e.\ $Q_c[\alpha] := \mathbb{E}[\,c \mid c^\top w^\star \ge \mathrm{VaR}_\alpha(c^\top w^\star)\,]$, 
where 
\[
  w^\star[\alpha] \;=\; \arg\min_{w \in \mathcal{W}} \,\mathrm{CVaR}_{c \sim p(c \mid x)}\bigl[c^\top w;\alpha\bigr].
\]

When $\alpha \rightarrow 0$, this degenerates to
\[
  w^\star[\alpha] \;=\; \arg\min_{w \in \mathcal{W}} \,\max_{c \sim p(c \mid x)}\bigl[c^\top w\bigr].
\]

Let $g(x)$ be a predictor approximating $Q_c$, 
with bias defined as $\mathrm{Bias}[g] = g(x) - Q_c$.
Define the solutions and regrets as follows:

\begin{itemize}
\item 
\textbf{Predictive-DFL (Pred-DFL):}
\[
  \hat{w}_{\mathrm{pred}}^\star(x)
  \;=\;
  \arg\min_{w \in \mathcal{W}}\; g(x)^\top w,
  \qquad
  R_{\mathrm{pred}}(x;\alpha)
  \;=\;
  \bigl|\,Q_c[\alpha]^\top\!\bigl(\hat{w}_{\mathrm{pred}}^\star - w^\star[\alpha]\bigr)\bigr|.
\]

\item
\textbf{Generative-DFL (Gen-DFL):}
\[
  \hat{w}_{\mathrm{gen}}^\star(x;\alpha)
  \;=\;
  \arg\min_{w \in \mathcal{W}}\;
  \mathrm{CVaR}_{c \sim p_\theta(c \mid x)}\bigl[c^\top w;\,\alpha\bigr],
  \quad
  \hat{R}_{\mathrm{gen}}(x;\alpha)
  \;=\;
  \mathrm{CVaR}_{c \sim \hat{p}(c \mid x)}
  \Bigl[
    c^\top \bigl(\hat{w}_{\mathrm{gen}}^\star - w^\star\bigr)
    ;\,\alpha
  \Bigr].
\]

\end{itemize}

Assume:
\begin{enumerate}
    \item \emph{(Lipschitzness)} $w^\star(\cdot)$ is $L_w$-Lipschitz: 
    \[
      \bigl\|w^\star(c_1) - w^\star(c_2)\bigr\| \;\le\; L_w\,\|c_1 - c_2\|.
    \]
    \item \emph{(Variance Decomposition)} We may write $c = \bar{c} + \epsilon$, 
    and $\|\mathrm{Var}[c \mid x]\| = \mathcal{O}(\sigma^2 \,\sqrt{d_c})$.
    \item \emph{(Predictor Bias)} 
    \(
      \|\mathrm{Bias}[g]\|
      \;=\;
      \bigl\|g(x) - Q_c[\alpha]\bigr\|
      \;\approx\;
      \widetilde{\mathcal{O}}
      \!\Bigl(\tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_c}{n}}\Bigr)
    \)
    under sub-Gaussian assumption.
\end{enumerate}

Then, there exist constants $C_0, C_1, C_2$ such that with high probability over the sample draw,
% \[
% \mathbb{E}_x
% \Bigl[
%   \bigl|\,
%     R_{\mathrm{pred}}(x;\alpha)
%     ~-~
%     \hat{R}_{\mathrm{gen}}(x;\alpha)
%   \bigr|
% \Bigr]
% ~\;\le\;
% L_w \,\mathbb{E}_x\bigl[\|\mathrm{Var}[c \mid x]\|\bigr]
% \bigl(1 + \mathbb{E}_x[\|\bar{c}\|]\bigr)
% \;+\;
% L_w \,\mathbb{E}_x
% \bigl[\mathbb{E}_{c \sim p(c \mid x)}[\|c\|]\bigr]\,
% \|\mathrm{Bias}[g]\|
% \;+\;
% C_2\,\frac{1}{\alpha}\,\sqrt{\frac{d_x + d_c}{n}},
% \]
\[
\mathbb{E}_x
\Bigl[
  |\Delta R(x;\alpha)|
\Bigr]
\;\le\;
L_w\,C_0\Bigl(\tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_c}{n}}\Bigr)\,\|Q_c\|
\;+\;
C_1\,\mathbb{E}_x[\|\mathrm{Var}[c|x]\|] + C_2\,\frac{1}{\alpha}\,\sqrt{\frac{d_c}{n}}.
\]
% which the bias term $||\text{Bias}[g]||$ scale with 
% \emph{in the CVaR-based objective}.

\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REVISED PROOF of Theorem cvar_extension
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Theorem \ref{theorem:cvar_extension}]
We proceed in structured steps, incorporating both bias and statistical estimation errors.

\paragraph{Step 1: Decomposing the Regret Gap.}

Define the regret gap:
\[
\Delta R(x) 
\;:=\;
R_{\mathrm{pred}}(x) 
\;-\;
\hat{R}_{\mathrm{gen}}(x).
\]
Recall the regret definitions:
\[
R_{\mathrm{pred}}(x)
\;=\;
\bigl|
  Q_c^\top(\hat{w}_{\mathrm{pred}}^\star - w^\star)
\bigr|,
\qquad
\hat{R}_{\mathrm{gen}}(x)
\;=\;
\mathrm{CVaR}_{c \sim \hat{p}(c \mid x)}
\Bigl[
  c^\top(\hat{w}_{\mathrm{gen}}^\star - w^\star)
  ;\,\alpha
\Bigr].
\]

Introduce the auxiliary solutions:
\begin{itemize}
    % \item \textbf{Predictive-based solution:}
    % \[
    %   \hat{w}_{\mathrm{pred}}^\star(x)
    %   \;=\;
    %   \arg\min_{w \in \mathcal{W}}\; g(x)^\top w.
    % \]

    % \item \textbf{Generative-based solution:}
    % \[
    %   \hat{w}_{\mathrm{gen}}^\star(x)
    %   \;=\;
    %   \arg\min_{w \in \mathcal{W}}\;
    %   \mathrm{CVaR}_{c \sim p_\theta(c \mid x)}\bigl[c^\top w;\,\alpha\bigr].
    % \]

    % \item \textbf{True CVaR-optimal solution:}
    % \[
    %   w^\star
    %   \;=\;
    %   \arg\min_{w \in \mathcal{W}}
    %   \mathrm{CVaR}_{c \sim p(c \mid x)}\bigl[c^\top w;\,\alpha\bigr].
    % \]

    \item \textbf{Linear tail-mean solution:}
    \[
      \tilde{w}^\dagger
      \;=\;
      \arg\min_{w \in \mathcal{W}}
      Q_c^\top w,
      \quad
      \text{where }
      Q_c
      :=
      \mathbb{E}\bigl[
        c \;\bigm|\; c^\top w^\star \ge \mathrm{VaR}_\alpha(c^\top w^\star)
      \bigr].
    \]
\end{itemize}

To bound \(\Delta R(x)\), we add and subtract terms involving \(\tilde{w}^\dagger\), giving:
\[
\Delta R(x) 
\;=\;
\bigl[
  Q_c^\top(\hat{w}_{\mathrm{pred}}^\star - \tilde{w}^\dagger)
\bigr]
~+\!
\bigl[
  Q_c^\top(\tilde{w}^\dagger - w^\star)
\bigr]
~-\!
\Bigl[
  \mathrm{CVaR}_{c \sim p}\bigl[c^\top(\hat{w}_{\mathrm{gen}}^\star - w^\star);\,\alpha\bigr]
  - Q_c^\top(\tilde{w}^\dagger - w^\star)
\Bigr].
\]

Define:
\[
\begin{aligned}
\Delta_1 
&:= 
Q_c^\top(\hat{w}_{\mathrm{pred}}^\star - \tilde{w}^\dagger), \\
\Delta_2
&:= 
Q_c^\top(\tilde{w}^\dagger - w^\star), \\
\Delta_3 
&:= 
\mathrm{CVaR}_{c \sim p}\bigl[
  c^\top(\hat{w}_{\mathrm{gen}}^\star - w^\star);\,\alpha
\bigr]
- Q_c^\top(\tilde{w}^\dagger - w^\star).
\end{aligned}
\]
Then:
\[
|\Delta R(x)|
~\;\le\;\;
|\Delta_1|
\;+\;
|\Delta_2|
\;+\;
|\Delta_3|.
\]

\paragraph{Step 2: Bounding $\Delta_1$: (Predictor vs. Tail-Mean Solutions).}

We have:
\[
\Delta_1
= Q_c^\top(\hat{w}_{\mathrm{pred}}^\star - \tilde{w}^\dagger),
\]
where:
\[
\hat{w}_{\mathrm{pred}}^\star
= \arg\min_{w \in \mathcal{W}} g(x)^\top w,
\quad
\tilde{w}^\dagger
= \arg\min_{w \in \mathcal{W}} Q_c^\top w,
\quad
g(x) = Q_c + \mathrm{Bias}[g].
\]

Using linear-program sensitivity results, the solutions of \(\arg\min_w v^\top w\) over \(\mathcal{W}\) are Lipschitz in \(v\), so:
\[
\bigl\|\hat{w}_{\mathrm{pred}}^\star - \tilde{w}^\dagger\bigr\|
\;\le\;
L_w \,\|\mathrm{Bias}[g]\|.
\]
Thus:
\[
|\Delta_1|
~=~
\bigl|
  Q_c^\top (\hat{w}_{\mathrm{pred}}^\star - \tilde{w}^\dagger)
\bigr|
\;\le\;
\|Q_c\|\,\bigl\|\hat{w}_{\mathrm{pred}}^\star - \tilde{w}^\dagger\bigr\|
\;\le\;
L_w\,\|Q_c\|\,\|\mathrm{Bias}[g]\|.
\]

\paragraph{Step 3: Bounding $\Delta_2$: (Tail-Mean vs. CVaR Solutions).}

By definition, \(\tilde{w}^\dagger\) minimizes \(Q_c^\top w\), so:
\[
Q_c^\top(\tilde{w}^\dagger - w^\star)
~\le~
0.
\]
Thus:
\[
|\Delta_2|
~=\;
-Q_c^\top(\tilde{w}^\dagger - w^\star)
~\;\le\;
\|Q_c\|\,\|\tilde{w}^\dagger - w^\star\|.
\]
From Lipschitzness of \(w^\star\), \(\|\tilde{w}^\dagger - w^\star\|\) can be bounded in terms of \(\|\mathrm{Var}[c]\|\).

\paragraph{Step 4: Bounding $\Delta_3$: (CVaR vs. Linear Tail-Mean Objective).}

Split $\Delta_3$ into:
\[
\Delta_3
\;=\;
\underbrace{
  \mathrm{CVaR}_{c \sim p}\bigl[c^\top(\hat{w}_{\mathrm{gen}}^\star - w^\star)\bigr]
  - 
  Q_c^\top(\hat{w}_{\mathrm{gen}}^\star - w^\star)
}_{\Delta_{3a}}
~+\;
\underbrace{
  Q_c^\top(\hat{w}_{\mathrm{gen}}^\star - \tilde{w}^\dagger)
}_{\Delta_{3b}}.
\]

- For $\Delta_{3a}$, use CVaR sensitivity results (e.g., Rockafellar--Uryasev):
\[
|\Delta_{3a}|
\;\le\;
\kappa_1\,\|\mathrm{Var}[c]\|.
\]

- For $\Delta_{3b}$, statistical estimation error introduces:
\[
|\Delta_{3b}|
~\le~
C_2\,\frac{1}{\alpha}\,\sqrt{\frac{d_c}{n}}.
\]

Thus:
\[
|\Delta_3|
~\;\le\;
(\kappa_1\,\|\mathrm{Var}[c]\| + C_2\,\frac{1}{\alpha}\,\sqrt{\frac{d_c}{n}}).
\]

\paragraph{Step 5: Combine Bounds.}

Summing:
\[
|\Delta R(x;\alpha)|
\;\le\;
L_w\,\|\mathrm{Bias}[g]\|\,\|Q_c\|
\;+\;
C_1\,\mathbb{E}_x[\|\mathrm{Var}[c]\|]
\;+\;
C_2\,\frac{1}{\alpha}\,\sqrt{\frac{d_c}{n}}.
\]

Taking expectation over $x$, we get

\[
\mathbb{E}_x
\Bigl[
  |\Delta R(x;\alpha)|
\Bigr]
\;\le\;
L_w\,\mathbb{E}_x[\|\mathrm{Bias}[g]\|\,\|Q_c\|]
\;+\;
C_1\,\mathbb{E}_x[\|\mathrm{Var}[c]\|] + C_2\,\frac{1}{\alpha}\,\sqrt{\frac{d_c}{n}}.
\]

\end{proof}


\section{Non-linear Case}

\begin{theorem}[Nonlinear version of Theorem \ref{theorem:ultimate}]
\label{theorem:ultimate_nonlinear}
Let $p(c|x)$ be the true conditional distribution of $c$, and let $\hat{p}(c|x)$ be its empirical estimate from $n$ samples. Denote by $p_\theta(c|x)$ the learned generative model. Let $g(x)$ be a predictor that estimates $\mathbb{E}_{c\sim p(c|x)}[c|x]$.

Define:
\begin{itemize}
    \item \emph{Pred-DFL Regret}:
    \[
    R_{\text{pred}}(x) 
    \;=\; 
    \mathbb{E}_{c \sim \hat{p}(c|x)}
    \Bigl[\,f\bigl(c, \hat{w}_{\text{pred}}^\star\bigr) 
    \;-\; 
    f\bigl(c, w^\star\bigr)\Bigr],
    \]
    where 
    \[
    \hat{w}^\star_{\text{pred}}(x) 
    \;=\; 
    \arg\min_{w\in \mathcal{W}} \ f\bigl(g(x), w\bigr).
    \]
    \item \emph{Gen-DFL Regret (empirical)}:
    \[
    \hat{R}_{\text{gen}}(x) 
    \;=\; 
    \mathbb{E}_{c \sim \hat{p}(c|x)} 
    \Bigl[
      f\bigl(c, \hat{w}_{\text{gen}}^\star\bigr) 
      \;-\; 
      f\bigl(c, w^\star\bigr)
    \Bigr],
    \]
    where 
    \[
    \hat{w}^\star_{\text{gen}}(x) 
    \;=\; 
    \arg\min_{w\in \mathcal{W}} 
    \ \mathbb{E}_{c\sim p_\theta(c|x)}\!\bigl[f(c, w)\bigr].
    \]
\end{itemize}

\noindent
Let the \emph{true optimal solution} be:
\[
w^\star 
\;=\; 
\arg\min_{w\in \mathcal{W}} 
\ \mathbb{E}_{c\sim p(c|x)}\!\bigl[f(c, w)\bigr].
\]

Assume:
\begin{enumerate}
    \item The map $w^\star(\cdot)$ is Lipschitz continuous in $c$ with constant $L_w$, that is,
    \[
    \bigl\|w^\star(c_1) - w^\star(c_2)\bigr\| \;\le\; L_w \,\|c_1 - c_2\|.
    \]
    \item The conditional distribution $p(c|x)$ is given by a factor model
    \[
    c \;=\; \bar{c} \;+\; L f \;+\; \sigma\,\xi,
    \]
    where $\bar{c} = \mathbb{E}_{c\sim p(c|x)}[c \mid x]$, $L\in\mathbb{R}^{d\times k}$ is a low-rank loading matrix, $f \sim \mathcal{N}(0,I_k)$ (systematic factor), and $\xi \sim \mathcal{N}(0,I_d)$ (idiosyncratic noise). Hence,
    \[
    \text{Var}[c \mid x] \;=\; L\,L^\top \;+\; \sigma^2\,I.
    \]
    \item The \emph{bias} of $g(x)$ is 
    \[
    \mathrm{Bias}[g] 
    \;=\; 
    g(x) \;-\; \bar{c}.
    \]
    \item The objective $f(c,w)$ is \emph{$L_f$-smooth} in $c$, i.e., its Hessian in $c$ is bounded: 
    \[
    \bigl\|\nabla_c^2 f(c,w)\bigr\| 
    \;\le\; 
    L_f,
    \]
    for all $(c,w)$ in the relevant domains.
\end{enumerate}

Then, for
\[
\Delta R(x) 
\;=\; 
R_{\text{pred}}(x) 
\;-\;
\hat{R}_{\text{gen}}(x),
\]
we have an upper bound of the form:
\[
\mathbb{E}_x\bigl[\bigl|\Delta R(x)\bigr|\bigr] 
\;\le\; 
L_w\,\mathbb{E}_x\bigl[\|\mathrm{Var}[c \mid x]\|\bigr]\,
\Bigl(1 + \mathbb{E}_x\bigl[\|\bar{c}\|\bigr]\Bigr)
\;+\;
L_w\,\mathbb{E}_x \!\bigl[\mathbb{E}_{c\sim p(c|x)}\|c\|\bigr]\;\|\mathrm{Bias}[g]\|
\;+\;
\underbrace{C_f\,\mathbb{E}_x\|\mathrm{Var}[c \mid x]\|}_{\text{2nd-order term}}
\;+\;
\underbrace{C_f\,\mathbb{E}_x\|\mathrm{Var}[c \mid x]^{3/2}\|}_{\text{3rd-order term}}
\;+\;
C_2 \,\sqrt{\frac{d}{n}},
\]
where $C_f$ is a constant depending on $L_f$ (the second-order Lipschitz constant of $f$), and $C_2\,\sqrt{\tfrac{d}{n}}$ controls the sampling error between $\hat{p}(c|x)$ and $p(c|x)$.
\end{theorem}



\newpage
\section{Results on Quantile/CVaR regression}

%=========================================================
\begin{theorem}[Finite-Sample Bound for CVaR Estimation]
\label{thm:unconditional_CVaR_bound}
Suppose $Y$ takes values in the interval $[m, M]$.  Let $\widehat{\mathrm{CVaR}}_{\alpha}$ be the empirical estimator derived from 
\[
  \hat{\phi}_n(\eta)
  \;=\;
  \eta \;+\;\frac{1}{\alpha}\,\frac{1}{n}\sum_{i=1}^n (Y_i - \eta)_+,
  \quad
  \widehat{\mathrm{CVaR}}_{\alpha}
  \;=\;
  \inf_{\eta \in \mathbb{R}} \,\hat{\phi}_n(\eta),
\]
where $(y - \eta)_+ := \max\{y - \eta,0\}$ and $Y_1,\dots,Y_n$ are i.i.d.\ samples of $Y$. 
Then there is a universal constant $C>0$ such that for all $\delta>0$, with probability at least $1-\delta$,
\[
  \bigl|\,\widehat{\mathrm{CVaR}}_\alpha \;-\; \mathrm{CVaR}_\alpha(Y)\bigr|
  \;\le\;
  C\,\frac{(M-m)}{\alpha}\,\sqrt{\frac{\ln(1/\delta)}{n}}.
\]
In other words, the estimation error for $\mathrm{CVaR}_{\alpha}$ converges on the order of 
$\sqrt{\ln(1/\delta) / n}$ as $n$ grows.
\end{theorem}
%=========================================================

\begin{remark}
Here, $\mathrm{CVaR}_{\alpha}(Y) = \mathbb{E}[\,Y \mid Y \le \mathrm{VaR}_{\alpha}(Y)\,]$, and
\[
  \mathrm{VaR}_{\alpha}(Y) 
  \;=\; 
  \inf\{\,t : \Pr(Y \le t)\;\ge\;\alpha\}.
\]
The key step in the proof is the Rockafellar--Uryasev identity,
\[
  \mathrm{CVaR}_\alpha(Y) 
  \;=\;
  \inf_{\eta \in \mathbb{R}}
  \Bigl\{
    \eta \;+\;\tfrac{1}{\alpha}\,\mathbb{E}\bigl[(\,Y-\eta\,)_+\bigr]
  \Bigr\},
\]
combined with uniform convergence arguments (e.g.\ Hoeffding or Rademacher complexity bounds).
\end{remark}

\begin{proof}
\textbf{Step 1: Rockafellar--Uryasev Representation.}

Recall the identity (Rockafellar--Uryasev):
\[
  \mathrm{CVaR}_\alpha(Y) 
  \;=\;
  \min_{\eta \in \mathbb{R}}
  \Bigl(
    \eta \;+\; \frac{1}{\alpha}\,\mathbb{E}\bigl[(\,Y-\eta\,)_+\bigr]
  \Bigr).
\]
Set 
\[
  \phi(\eta) 
  \;=\;
  \eta \;+\;\frac{1}{\alpha}\,\mathbb{E}[(\,Y-\eta\,)_+].
\]
Then $\mathrm{CVaR}_\alpha(Y) = \min_{\eta \in \mathbb{R}}\, \phi(\eta)$.

\noindent
\textbf{Step 2: Empirical Estimator.}

Given i.i.d.\ samples $Y_1,\dots,Y_n$, define the empirical counterpart
\[
  \hat{\phi}_n(\eta)
  \;=\;
  \eta 
  \;+\;
  \frac{1}{\alpha}\,\frac{1}{n}\,\sum_{i=1}^n (\,Y_i - \eta\,)_+,
\]
and let
\[
  \widehat{\mathrm{CVaR}}_{\alpha} 
  \;=\;
  \min_{\eta\in\mathbb{R}}\;\hat{\phi}_n(\eta).
\]
Similarly, let $\eta^* \in \arg\min_{\eta}\phi(\eta)$ and 
$\hat{\eta}_n \in \arg\min_{\eta}\hat{\phi}_n(\eta)$.

\noindent
\textbf{Step 3: Uniform Convergence.}

Observe that
\[
  |\hat{\phi}_n(\eta) - \phi(\eta)|
  \;=\;
  \Bigl|\,
    \frac{1}{\alpha} \bigl(\tfrac{1}{n}\sum_{i=1}^n (\,Y_i - \eta\,)_+ - 
    \mathbb{E}[(\,Y-\eta\,)_+]\bigr)
  \Bigr|
  \;\le\;
  \frac{1}{\alpha}
  \sup_{\eta\in\mathbb{R}}
  \Bigl|
    \tfrac{1}{n}\sum_{i=1}^n f_\eta(Y_i)
    \;-\;
    \mathbb{E}[\,f_\eta(Y)\bigr]
  \Bigr|,
\]
where $f_\eta(y) := (y-\eta)_+$ is bounded by $(M-m)$ if $y\in[m,M]$.  
By standard Hoeffding (or VC / Rademacher) arguments, with probability $\ge1-\delta$,
\[
  \sup_{\eta\in\mathbb{R}}
  \Bigl|
    \tfrac{1}{n}\sum_{i=1}^n (Y_i - \eta)_+ 
    \;-\;
    \mathbb{E}[(Y-\eta)_+]
  \Bigr|
  \;\le\;
  C_1\,(M-m)\,\sqrt{\frac{\ln(1/\delta)}{n}}
\]
for some universal constant $C_1>0$.  
Hence,
\[
  \sup_{\eta\in\mathbb{R}}
  \bigl|\hat{\phi}_n(\eta) - \phi(\eta)\bigr|
  \;\le\;
  \frac{C_1\,(M-m)}{\alpha}\,\sqrt{\frac{\ln(1/\delta)}{n}}
  \;=\;: \varepsilon_n.
\]

\noindent
\textbf{Step 4: Error Between Minimizers.}

By definition of $\hat{\eta}_n$ and $\eta^*$,
\[
  \hat{\phi}_n(\hat{\eta}_n)
  \;\le\;
  \hat{\phi}_n(\eta^*).
\]
Also,
\[
  \phi(\hat{\eta}_n) - \phi(\eta^*)
  \;\le\;
  \bigl[\,\hat{\phi}_n(\hat{\eta}_n) - \phi(\hat{\eta}_n)\bigr]
  \;+\;
  \bigl[\,\hat{\phi}_n(\eta^*) - \phi(\eta^*)\bigr]
  \;\le\; 2\,\varepsilon_n.
\]
Thus
\[
  \phi(\hat{\eta}_n)
  \;\le\;
  \phi(\eta^*) + 2\,\varepsilon_n
  \;\Longrightarrow\;
  \hat{\phi}_n(\hat{\eta}_n)
  \;=\;
  \phi(\hat{\eta}_n)
  +\bigl[\hat{\phi}_n(\hat{\eta}_n)- \phi(\hat{\eta}_n)\bigr]
  \;\le\;
  \phi(\eta^*) + 3\,\varepsilon_n.
\]
Similarly, by symmetry, we get $\phi(\eta^*) \le \hat{\phi}_n(\hat{\eta}_n) + 3\,\varepsilon_n$,
so
\[
  \bigl|\hat{\phi}_n(\hat{\eta}_n) - \phi(\eta^*)\bigr|
  \;\le\; 3\,\varepsilon_n.
\]
Since $\mathrm{CVaR}_\alpha(Y)=\phi(\eta^*)$ and 
$\widehat{\mathrm{CVaR}}_{\alpha}=\hat{\phi}_n(\hat{\eta}_n)$, we conclude
\[
  \bigl|\widehat{\mathrm{CVaR}}_{\alpha} - \mathrm{CVaR}_\alpha(Y)\bigr|
  \;\le\; 3\,\varepsilon_n
  \;\;=\;
  \mathcal{O}\!\Bigl(\tfrac{M-m}{\alpha}\,\sqrt{\tfrac{\ln(1/\delta)}{n}}\Bigr).
\]
Finally, we absorb constant factors into a single $C$, yielding the stated bound.
\end{proof}

%=========================================================
\begin{theorem}[Generalization Bound for Conditional CVaR Estimation]
\label{thm:conditional_CVaR_bound}
Let $(X,Y)$ be distributed on $\mathcal{X}\times\mathbb{R}$, and let
$\mathcal{G}$ be a class of measurable functions $g:\mathcal{X}\to \mathbb{R}$.
Define the population Rockafellar--Uryasev (RU) risk of any predictor $g$ by
\[
  R(g) 
  \;:=\;
  \mathbb{E}\!\Bigl[
    g(X) 
    \;+\;
    \frac{1}{\alpha}\,\bigl(Y - g(X)\bigr)_{+}
  \Bigr],
\]
and let
\[
  R^* 
  \;=\;
  \inf_{g \in \mathcal{G}}\,R(g), 
  \quad
  g^*\in \arg\min_{g\in \mathcal{G}}\;R(g).
\]
Given i.i.d.\ samples $\{(x_i,y_i)\}_{i=1}^n$, define the empirical RU risk
\[
  \widehat{R}_n(g)
  \;:=\;
  \frac{1}{n}\,\sum_{i=1}^n\Bigl[
    g(x_i) + \tfrac{1}{\alpha}\,\bigl(y_i - g(x_i)\bigr)_{+}
  \Bigr],
\]
and let
\[
  \hat{g}_n 
  \;\;\in\;
  \arg\min_{g\in\mathcal{G}} 
  \;\widehat{R}_n(g).
\]
Suppose that, with probability at least $1-\delta$, 
\[
   \sup_{g\in \mathcal{G}}
   \Bigl|
      \widehat{R}_n(g) - R(g)
   \Bigr|
   \;\;\le\;\;
   \varepsilon_n,
\]
where $\varepsilon_n$ is a term that typically behaves like 
$\mathcal{O}\!\Bigl(\tfrac{1}{\alpha}\,\sqrt{\tfrac{\ln(1/\delta)}{n}}\Bigr)$ 
under standard assumptions (boundedness, sub-Gaussian tails, etc.).  
Then on that event,
\[
  R(\hat{g}_n) 
  \;-\; 
  R^*
  \;\;\le\;\;
  2\,\varepsilon_n.
\]
Hence the learned predictor $\hat{g}_n$ achieves a CVaR-type risk 
within $2\,\varepsilon_n$ of the best $g^*\in \mathcal{G}$, with high probability.
\end{theorem}
%=========================================================

% \begin{remark}
% If $\mathcal{G}$ is sufficiently rich (e.g., large neural networks or universal approximators),
% then $R^*$ is close to the true \emph{conditional} CVaR function 
% $x\mapsto \mathrm{CVaR}_\alpha(Y \mid X=x)$. 
% In simpler function classes (e.g.\ linear or parametric), there may be an additional
% \emph{approximation gap} if the true conditional CVaR is not well-represented by $\mathcal{G}$.
% \end{remark}

\begin{proof}
\textbf{Step 1: Setup \& Definitions.}

For each $g\in \mathcal{G}$, define the population RU risk
\[
  R(g) 
  \;=\;
  \mathbb{E}\Bigl[
    g(X) + \tfrac{1}{\alpha}\,\bigl(Y - g(X)\bigr)_{+}
  \Bigr].
\]
The empirical counterpart based on samples $(x_i,y_i)_{i=1}^n$ is
\[
  \widehat{R}_n(g)
  \;=\;
  \frac{1}{n}\,\sum_{i=1}^n 
  \bigl[
    g(x_i) 
    \;+\;
    \tfrac{1}{\alpha}\,(y_i - g(x_i))_+
  \bigr].
\]
Let 
\[
  \hat{g}_n \;\in\; \arg\min_{g \in \mathcal{G}}\;\widehat{R}_n(g),
  \quad
  g^* \;\in\; \arg\min_{g \in \mathcal{G}}\;R(g).
\]

\noindent
\textbf{Step 2: Decompose the Excess Risk.}

We want $R(\hat{g}_n) - R(g^*)$.  
Note that
\[
   R(\hat{g}_n) \;-\; R(g^*)
   \;=\;
   \underbrace{\bigl[R(\hat{g}_n) - \widehat{R}_n(\hat{g}_n)\bigr]}_{(A)}
   \;+\;
   \underbrace{\bigl[\widehat{R}_n(\hat{g}_n) - \widehat{R}_n(g^*)\bigr]}_{(B)}
   \;+\;
   \underbrace{\bigl[\widehat{R}_n(g^*) - R(g^*)\bigr]}_{(C)}.
\]
Since $\hat{g}_n$ minimizes $\widehat{R}_n$, the middle term $(B)\le 0$.  Hence
\[
   R(\hat{g}_n) - R(g^*)
   \;\le\;
   (A) + (C).
\]
But
\[
   (A) 
   \;=\;
   R(\hat{g}_n) - \widehat{R}_n(\hat{g}_n)
   \;\le\;
   \sup_{g\in\mathcal{G}}\bigl|\,R(g) - \widehat{R}_n(g)\bigr|,
\]
and similarly 
\[
   (C)
   \;=\;
   \widehat{R}_n(g^*) - R(g^*)
   \;\le\;
   \sup_{g\in\mathcal{G}}\bigl|\,\widehat{R}_n(g) - R(g)\bigr|.
\]
Therefore,
\[
   R(\hat{g}_n) - R(g^*)
   \;\le\;
   2\,\sup_{g\in \mathcal{G}}
   \Bigl|
      \widehat{R}_n(g) - R(g)
   \Bigr|.
\]

\noindent
\textbf{Step 3: Uniform Convergence Bound.}

By hypothesis (or by a standard Rademacher / VC argument), we have
\[
  \sup_{g\in \mathcal{G}}
  \bigl|\widehat{R}_n(g) - R(g)\bigr|
  \;\le\;
  \varepsilon_n
\]
with probability $\ge 1-\delta$, where $\varepsilon_n$ scale like 
$\mathcal{O}\bigl(\tfrac{1}{\alpha}\sqrt{\tfrac{\ln(1/\delta)}{n}}\bigr)$.  
Hence on that event:
\[
   R(\hat{g}_n) - R(g^*)
   \;\le\; 2\,\varepsilon_n.
\]

\noindent
\textbf{Step 4: Why $\varepsilon_n$ Includes a Factor of $1/\alpha$.}

Observe that
\[
  \phi_\alpha(x,y;g)
  \;=\;
  g(x) 
  \;+\;
  \frac{1}{\alpha}(y-g(x))_+.
\]
Because it is scaled by $\frac{1}{\alpha}$, any standard concentration bound (e.g.\ Hoeffding or Rademacher) for $\phi_\alpha$ incurs an extra factor of $1/\alpha$.  Specifically:

\begin{itemize}
\item \emph{Boundedness:} 
If $|g(x)|\le G_{\max}$ and $|y|\le Y_{\max}$, then $(y-g(x))_+\le |\,y-g(x)\,|\le Y_{\max}+G_{\max}$.  Hence 
\(
  \phi_\alpha(x,y;g) 
  \le 
  G_{\max} + \tfrac{1}{\alpha}(Y_{\max}+G_{\max}).
\)
\item \emph{Rademacher complexity or Hoeffding:} 
A uniform‐convergence or covering‐number argument yields a $\sqrt{\frac{\ln(1/\delta)}{n}}$ factor multiplied by the supremum of $|\phi_\alpha|$, which is $\le \frac{C}{\alpha}$ for some constant $C$.
\end{itemize}
Thus $\varepsilon_n$ \emph{necessarily} scales like $\frac{1}{\alpha}\sqrt{\frac{\ln(1/\delta)}{n}}$ (up to constants and possibly adding a $\mathfrak{R}_n(\mathcal{G})$ term if $\mathcal{G}$ is large).

\end{proof}



%=============================================

\begin{theorem}[High-Dimensional Conditional CVaR Generalization Bound]
\label{thm:highdim_conditional_CVaR}
Let $(X,Y)$ be a random pair taking values in $\mathbb{R}^{d_x}\times \mathbb{R}^{d_y}$,
and let $\alpha\in(0,1)$ be fixed.
Suppose we have:
\begin{itemize}
\item A \emph{scalar loss} $\ell: \mathbb{R}\times \mathbb{R}^{d_y}\to \mathbb{R}$,
\item A hypothesis class $\mathcal{G}$ of measurable functions $g:\mathbb{R}^{d_x}\to\mathbb{R}$,
\end{itemize}
and define the \emph{Rockafellar--Uryasev (RU) risk} of any predictor $g\in \mathcal{G}$ by
\[
  R(g) 
  \;:=\;
  \mathbb{E}\!\Bigl[
    g(X) 
    \;+\;
    \frac{1}{\alpha}\,\Bigl(\,\ell\bigl(g(X),Y\bigr) - g(X)\Bigr)_{+}
  \Bigr].
\]
Let $R^*=\inf_{g\in \mathcal{G}} R(g)$, and choose $g^*$ such that $R(g^*)=R^*$.
Given $n$ i.i.d.\ samples $\{(x_i,y_i)\}_{i=1}^n\subset \mathbb{R}^{d_x}\times \mathbb{R}^{d_y}$,
define the \emph{empirical} RU risk
\[
  \widehat{R}_n(g)
  \;:=\;
  \frac{1}{n}\,\sum_{i=1}^n
    \Bigl[
      g(x_i) 
      \;+\;
      \frac{1}{\alpha}\,\bigl(\ell(g(x_i),\,y_i) - g(x_i)\bigr)_{+}
    \Bigr],
\]
and let $\hat{g}_n \in \arg\min_{g\in \mathcal{G}}\,\widehat{R}_n(g)$.
Assume that with probability at least $1-\delta$, we have a uniform-convergence bound
\[
   \sup_{g\in \mathcal{G}}
   \Bigl|\widehat{R}_n(g) - R(g)\Bigr|
   \;\;\le\;\;
   \varepsilon_n,
\]
where $\varepsilon_n$ scales as
\[
   \varepsilon_n 
   \;=\; 
   \widetilde{\mathcal{O}}
   \!\Bigl(
     \tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_y}{n}}
   \Bigr),
\]
under suitable boundedness/sub-Gaussian assumptions on $(X,Y)$ and $\ell$. Then on that event,
\[
  R(\hat{g}_n) 
  \;-\; 
  R^*
  \;\;\le\;\;
  2\,\varepsilon_n.
\]
Hence the learned predictor $\hat{g}_n$ achieves a CVaR-type risk 
within $2\,\varepsilon_n$ of the best $g^*\in \mathcal{G}$, with high probability.
\end{theorem}

\begin{proof}[Proof Sketch]
\textbf{Step 1: Excess Risk Decomposition.}

By definition,
\[
  R(\hat{g}_n) - R(g^*)
  \;=\;
  \bigl[R(\hat{g}_n) - \widehat{R}_n(\hat{g}_n)\bigr]
  \;+\;
  \bigl[\widehat{R}_n(\hat{g}_n) - \widehat{R}_n(g^*)\bigr]
  \;+\;
  \bigl[\widehat{R}_n(g^*) - R(g^*)\bigr].
\]
Because $\hat{g}_n$ is the empirical risk minimizer,
the middle term is $\le 0$. Thus
\[
  R(\hat{g}_n) - R(g^*)
  \;\le\;
  \bigl|R(\hat{g}_n) - \widehat{R}_n(\hat{g}_n)\bigr|
  \;+\;
  \bigl|\widehat{R}_n(g^*) - R(g^*)\bigr|
  \;\le\;
  2\,\sup_{g\in \mathcal{G}}
  \Bigl|
    \widehat{R}_n(g) - R(g)
  \Bigr|.
\]
\smallskip

\noindent
\textbf{Step 2: Uniform Convergence.}

By hypothesis, with probability at least $1-\delta$,
\[
  \sup_{g\in \mathcal{G}}
  \Bigl|\widehat{R}_n(g) - R(g)\Bigr|
  \;\;\le\;\;
  \varepsilon_n.
\]
Hence on that event,
\[
  R(\hat{g}_n) - R(g^*)
  \;\;\le\;\;
  2\,\varepsilon_n.
\]

\noindent
\textbf{Step 3: Why $\varepsilon_n$ Scales with $1/\alpha$ and $\sqrt{\tfrac{d_x + d_y}{n}}$.}

Consider the RU loss:
\[
  \phi_\alpha\bigl(\theta,y\bigr)
  \;=\;
  \theta + \frac{1}{\alpha}\bigl(\ell(\theta,y) - \theta\bigr)_+.
\]
Any bound (Hoeffding, Bernstein, Rademacher) on 
\(\bigl|\widehat{R}_n(g) - R(g)\bigr|\)
inevitably picks up a factor $\frac{1}{\alpha}$ because $\phi_\alpha$ is scaled by $\frac{1}{\alpha}$.

Moreover, if $x \in \mathbb{R}^{d_x}$ and $y \in \mathbb{R}^{d_y}$ are bounded or sub-Gaussian,
the complexity measure (VC dimension, covering numbers, or Rademacher complexity) 
for $\mathcal{G}$ typically introduces a $\sqrt{\tfrac{d_x}{n}}$ factor.
Simultaneously, bounding or controlling $\ell(\theta,y)$ often involves $\|y\|$,
leading to a dimension effect $\sqrt{d_y}$. 
Hence we get 
\[
  \varepsilon_n
  \;=\;
  \widetilde{\mathcal{O}}
  \!\Bigl(
    \tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_y}{n}}
  \Bigr),
\]
up to logarithmic factors. 
Putting this into the \emph{excess risk} yields 
$R(\hat{g}_n)-R(g^*) \le 2\,\varepsilon_n.$
\end{proof}

\begin{remark}
The exact constant and logarithmic factors (e.g.\ $\ln(1/\delta)$, $\ln(n)$) can be made explicit 
by choosing the appropriate concentration or empirical-process bound 
(Hoeffding/Bernstein, Rademacher complexity, etc.). 
The main point is that 
\[
  \varepsilon_n
  \;=\;
  \mathcal{O}\!\bigl(\tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_y}{n}}\bigr)
\]
captures the correct dependence on $\alpha$, the sample size $n$, 
and the dimensions $d_x$, $d_y$.
\end{remark}

\newpage
\section{Proof sketch}

\textbf{Step 1: Decomposition of the Regret}
\begin{proof}
\begin{align*}
|\Delta R(x)| &= \big|\mathbb{E}_{c\sim p(c|x)}[f(c, w_{gen}^\star) - f(c, w^\star)] - \mathbb{E}_{c\sim p(c|x)}[f(c, w_{pred}^\star) - f(c, w^\star)] \big|\\
&= \big| \mathbb{E}_{c\sim p(c|x)}[f(c, w_{gen}^\star) - f(c, w_{pred}^\star)] \big|\\
&= \mathbb{E}_{c\sim p(c|x)}\left[ \left[f(\bar{c}, w_{gen}^\star) - f(\bar{c}, w_{pred}^\star)\right] + \left[ f(c, w_{gen}^\star) - f(\bar{c}, w_{gen}^\star) \right] - \left[ f(c, w_{pred}^\star) - f(\bar{c}, w_{pred}^\star) \right] \right]\\
&= \mathbb{E}_{c\sim p(c|x)}\left[ \left[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)\right] + \left[ f(c, w_{gen}^\star) - f(g(x), w_{gen}^\star) \right] - \left[ f(c, w_{pred}^\star) - f(g(x), w_{pred}^\star) \right] \right]\\
&\le \big|\mathbb{E}_{c\sim p(c|x)}[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)]\big| + 2 L_c \mathbb{E}_{c\sim p(c|x)}[||c - g(x)||]\\
&= \big|\mathbb{E}_{c\sim p(c|x)}[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)]\big| + 2 L_c \mathbb{E}_{c\sim p(c|x)}[\|c - \bar{c} + \bar{c} - g(x)\|]\\
& \le \big|\mathbb{E}_{c\sim p(c|x)}[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)]\big| + 2L_c \bigl| \mathbb{E}_{p(c|x)}[\|c - \bar{c}\|] + \mathbb{E}_{p(c|x)}[\|\text{Bias}[g]\|] \bigr|\\
& \le \big|\mathbb{E}_{c\sim p(c|x)}[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)]\big| + 2L_c \bigl| \sqrt{d}\|Var[c\mid x] \|^{1/2} + \mathbb{E}_{p(c|x)}[\|\text{Bias}[g]\|] \bigr|
\end{align*}

where we used the fact $f(c, w_{gen}^\star) = f(\bar{c}, w_{gen}^\star) + [f(c, w_{gen}^\star) - f(\bar{c}, w_{gen}^\star)]$ and $f(c, w_{pred}^\star) = f(\bar{c}, w_{pred}^\star) + [f(c, w_{pred}^\star) - f(\bar{c}, w_{pred}^\star)]$, and $\bar{c} = \mathbb{E}_{p(c|x)}[c]$

\textbf{Step 2: Bounding $\Delta_{\mathrm{Term}} = \big|\mathbb{E}_{c\sim p(c|x)}[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)]\big|$}

Now, we need to bound the $\Delta_{\mathrm{Term}}= \big|\mathbb{E}_{c\sim p(c|x)}[f(g(x), w_{gen}^\star) - f(g(x), w_{pred}^\star)]\big|$ term.

By assumption, for any fixed $c_0$, the map $w \mapsto f(c_0, w)$ is $L_w$-Lipschitz in $w$. Equivalently,
\[
\bigl|\,f(c_0, w_1) - f(c_0, w_2)\bigr| 
\;\le\;
L_w\,\|\,w_1 - w_2\|.
\]
Applying this specifically at $c_0 = g(x)$, we get:
\[
\bigl|\,f(g(x), w_{\mathrm{gen}}^\star) 
      - f(g(x), w_{\mathrm{pred}}^\star)\bigr|
\;\le\;
L_w\,\bigl\|\,w_{\mathrm{gen}}^\star - w_{\mathrm{pred}}^\star\bigr\|.
\]
Since $\mathbb{E}_{c\sim p}[\,\cdot\,]$ is merely an expectation that does not affect the integrand here (it does not depend on $c$ anymore), we have
\[
\Delta_{\mathrm{Term}}
\;=\;
\bigl|\mathbb{E}_{c \sim p(c\mid x)}[
  f(g(x), w_{\mathrm{gen}}^\star) - f(g(x), w_{\mathrm{pred}}^\star)
]\bigr|
\;\le\;
L_w\,\bigl\|\,
w_{\mathrm{gen}}^\star - w_{\mathrm{pred}}^\star
\bigr\|.
\]
This step is straightforward (but not yet informative). The crux is to bound 
\(\|w_{\mathrm{gen}}^\star - w_{\mathrm{pred}}^\star\|\) in terms of model parameters.

\textbf{Step 3: Bounding $\|w_{\mathrm{gen}}^\star - w_{\mathrm{pred}}^\star\|$ }

First, we define the following auxiliary (aggregate objectives) functions for both Gen-DFL and Pred-DFL,

\[
\text{Gen-DFL: } 
J_{\mathrm{gen}}(w) 
= \mathbb{E}_{c\sim p_\theta(c\mid x)}[\,f(c,w)\bigr],
\quad
\text{Pred-DFL: }
J_{\mathrm{pred}}(w) 
= f\bigl(g(x), w\bigr).
\]
So
\[
w_{\mathrm{gen}}^\star 
= \arg\min_{w\in \mathcal{W}} J_{\mathrm{gen}}(w),
\quad
w_{\mathrm{pred}}^\star 
= \arg\min_{w\in \mathcal{W}} J_{\mathrm{pred}}(w).
\]

Next, let's define 
\[
\Delta(w) 
\;=\;
J_{\mathrm{gen}}(w) - J_{\mathrm{pred}}(w)
\;=\;
\mathbb{E}_{c\sim p_\theta}\bigl[f(c,w)\bigr]
\;-\;
f\bigl(g(x), w\bigr).
\]
We take a uniform bound over $w$:
\[
T 
\;=\;
\sup_{w \in \mathcal{W}}
\;\bigl|\Delta(w)\bigr|.
\]
Typically, $\Delta(w)$ is broken down into terms reflecting
\begin{itemize}
    \item \(\|p_\theta - p\|\) (model mismatch),
    \item Lipschitz in $c$ plus variance to compare $\mathbb{E}_p[f(c,w)]$ and $f(\bar{c},w)$,
    \item Predictor bias $\|g(x)-\bar{c}\|$.
\end{itemize}
We will then show that,
\[
T \;\le\; \kappa_1 \|p_\theta - p\| + \kappa_2 \sqrt{\|\mathrm{Var}[c\mid x]\|} + \kappa_3 \|\mathrm{Bias}[g]\|.
\]

%===================bounding T====================
\textbf{Step 4: Bounding T}

\textbf{Defining \(T\) and Splitting into Three Terms}

Let
\[
T 
:= 
\sup_{w \in \mathcal{W}}
\;\Bigl|\,
  \mathbb{E}_{c\sim p_\theta}[\,f(c,w)\bigr]
  -
  f\bigl(g(x), w\bigr)
\Bigr|.
\]
To relate this to the \emph{true} distribution \(p\) and the \emph{true} mean \(\bar{c} = \mathbb{E}_{c\sim p}[\,c]\), we do an \emph{add-and-subtract} decomposition:
\[
\mathbb{E}_{p_\theta}[\,f(c,w)\bigr]
\;-\;
f(g(x), w)
\;=\;
\underbrace{
  \Bigl(
    \mathbb{E}_{p_\theta}[f(c,w)]
    -
    \mathbb{E}_{p}[f(c,w)]
  \Bigr)}_{\text{(A) model mismatch}}
\;+\;
\underbrace{
  \Bigl(
    \mathbb{E}_{p}[f(c,w)]
    -
    f(\bar{c}, w)
  \Bigr)}_{\text{(B) variance/mean diff}}
\;+\;
\underbrace{
  \Bigl(
    f(\bar{c}, w)
    -
    f(g(x), w)
  \Bigr)}_{\text{(C) predictor bias}}.
\]
Hence, if we set
\[
T 
= 
\sup_{w \in \mathcal{W}} 
\bigl|\text{(A)} + \text{(B)} + \text{(C)}\bigr|,
\]
then by triangle inequality:
\[
T \;\le\; 
\underbrace{
  \sup_{w \in \mathcal{W}} \bigl|\text{(A)}\bigr|
}_{T_1}
\;+\;
\underbrace{
  \sup_{w \in \mathcal{W}} \bigl|\text{(B)}\bigr|
}_{T_2}
\;+\;
\underbrace{
  \sup_{w \in \mathcal{W}} \bigl|\text{(C)}\bigr|
}_{T_3}.
\]
We now bound each piece \(T_1, T_2, T_3\) separately.

\textbf{Bounding Term (A): $\sup_{w} \bigl|\mathbb{E}_{p_\theta}[f(c,w)] - \mathbb{E}_{p}[f(c,w)]\bigr|$}

This \emph{model mismatch} part depends on how well $p_\theta$ approximates the true $p$ in a certain metric, \emph{and} how $f(c,w)$ depends on $c$:
\[
T_1 
:= 
\sup_{w \in \mathcal{W}}
\;\bigl|\mathbb{E}_{p_\theta}[f(c,w)]
      -
      \mathbb{E}_{p}[f(c,w)]\bigr|.
\]
% A typical approach:

% \begin{itemize}
% \item \textbf{Assume} $f(\cdot,w)$ is $L_c$-Lipschitz in $c$ for each fixed $w$,
% \item Use a distributional metric (e.g.\ Wasserstein, TV, or KL) to bound $\bigl|\mathbb{E}_{p_\theta}[f(c,w)] - \mathbb{E}_{p}[f(c,w)]\bigr|$ in terms of $\|p_\theta - p\|$ plus a constant factor from Lipschitz or integrability.
% \end{itemize}

Hence we set
\[
T_1
\;\le\;
% \kappa_1\;\|p_\theta - p\|,
\kappa_1\;\mathcal{W}(p_\theta, p),
\]
where $\kappa_1$ depends on the Lipschitz constant of $f$ in $c$ and possibly $\sup_{w\in\mathcal{W}}\|w\|$ or other geometry.

\textbf{Bounding Term (B): $\sup_{w} \bigl|\mathbb{E}_p[f(c,w)] - f(\bar{c},w)\bigr|$}

This part captures the difference between using the full distribution $p(c)$ vs.\ plugging in the mean $\bar{c}$. A typical approach is either:

\begin{itemize}
\item \emph{First-order expansion} around $\bar{c}$, ignoring the linear term since $\mathbb{E}[c-\bar{c}]=0$, and bounding second-order remainders via $\mathrm{Var}[c\mid x]$,
\item or \emph{Lipschitz in $c$} so that $\bigl|f(c,w) - f(\bar{c},w)\bigr|\le L_c\|\,c-\bar{c}\|$ and then bounding $\mathbb{E}\|c-\bar{c}\|\lesssim \sqrt{d\|\mathrm{Var}[c\mid x]\|}$.
\end{itemize}

Thus
\[
T_2 
:= 
\sup_{w \in \mathcal{W}}
\bigl|\mathbb{E}_{p}[f(c,w)] - f(\bar{c}, w)\bigr|
\;\le\;
\kappa_2 \,\sqrt{d}\sqrt{\|\mathrm{Var}[c\mid x]\|}.
\]
Here $\kappa_2$ incorporates Lipschitz constants.

\textbf{Bounding Term (C): $\sup_{w} \bigl|f(\bar{c}, w) - f(g(x), w)\bigr|$}

Finally, the \emph{predictor bias} part:

\[
T_3
= 
\sup_{w \in \mathcal{W}}
\bigl|f(\bar{c}, w) - f(g(x), w)\bigr|.
\]
Again, if $f(\cdot,w)$ is $L_c$-Lipschitz in $c$, then
\[
\bigl|f(\bar{c}, w) - f(g(x), w)\bigr|
\;\le\;
L_c \,\|\bar{c} - g(x)\|
\;=\;
L_c\;\|\mathrm{Bias}[g]\|.
\]
Hence,
\[
T_3
\;\le\;
\kappa_3\;\|\mathrm{Bias}[g]\|.
\]

\subsection*{5. Combine and Conclude}

Collecting $T_1, T_2, T_3$:

\[
\begin{aligned}
T
&=\;
\sup_{w \in \mathcal{W}}
\Bigl|
  \mathbb{E}_{p_\theta}[f(c,w)]
  -
  f(g(x), w)
\Bigr|
\;\;\le\;\;
T_1 + T_2 + T_3
\\[6pt]
&\;\le\;
\kappa_1\;\mathcal{W}(p_\theta, p)
\;+\;
\kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
\;+\;
\kappa_3\,\|\mathrm{Bias}[g]\|.
\end{aligned}
\]
Thus,
\[
\boxed{%
T
\;\;\le\;\;
\kappa_1\;\mathcal{W}(p_\theta, p)
\;+\;
\kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
\;+\;
\kappa_3\,\|\mathrm{Bias}[g]\|.
}
\]
%===================bounding T====================

\textbf{Strong Convexity in $w$ Yields Solution Stability.}

Assume each $J_{\mathrm{gen}}(\cdot)$ and $J_{\mathrm{pred}}(\cdot)$ is $\alpha$-strongly convex in $w$. Then a standard perturbation/sensitivity argument gives
\[
\bigl\|
  w_{\mathrm{gen}}^\star - w_{\mathrm{pred}}^\star
\bigr\|
\;\;\le\;\;
\frac{2}{\alpha}\;
\sup_{w \in \mathcal{W}} \bigl|\Delta(w)\bigr|
\;=\;
\frac{2}{\alpha}\;T.
\]
Substituting a bound on $T$ in terms of $\|p_\theta - p\|$, $\sqrt{\|\mathrm{Var}[c\mid x]\|}$, and $\|\mathrm{Bias}[g]\|$ completes this step.

\textbf{Combining all the steps}

Combining the Lipschitz step with the solution-stability bound:

\[
\Delta_{\mathrm{Term}}
=\;
\bigl|\mathbb{E}_{c\sim p}[f(g(x), w_{\mathrm{gen}}^\star)
    -
    f(g(x), w_{\mathrm{pred}}^\star)]\bigr|
\;\le\;
L_w\;\bigl\|\,w_{\mathrm{gen}}^\star - w_{\mathrm{pred}}^\star\bigr\|
\;\le\;
L_w \,\frac{2}{\alpha}\;T.
\]
If, for example,
\[
T 
\;\le\; 
\kappa_1\;\mathcal{W}(p_\theta, p)
+ 
\kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
+ 
\kappa_3\,\|\mathrm{Bias}[g]\|,
\]
we get
\[
\Delta_{\mathrm{Term}}
\;\;\le\;\;
\underbrace{L_w \cdot \frac{2}{\alpha}}_{\text{geometry factor}}
\;\bigl[
  \kappa_1\;\mathcal{W}(p_\theta, p)
  \;+\;
  \kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
  \;+\;
  \kappa_3\,\|\mathrm{Bias}[g]\|
\bigr].
\]
% This statement is \emph{non-trivial} because it explicitly shows how (1)~the difference in solutions depends on \(\|p_\theta-p\|\), (2)~predictor bias $\|g(x)-\bar{c}\|$, and (3)~the variance/tail of $p(c\mid x)$, \emph{combined} with strong-convex geometry (through factors $\alpha$ and $L_w$).

% \paragraph{Interpretation.}
% When
% \begin{itemize}
%     \item $p_\theta$ is \emph{accurate} ($\|p_\theta - p\|$ is small),
%     \item $g(x)$ is \emph{unbiased or low-bias},
%     \item $f$ is strongly convex and Lipschitz,
%     \item $\mathrm{Var}[c\mid x]$ is moderate (sub-Gaussian or bounded tails),
% \end{itemize}
% then the bound indicates $w_{\mathrm{gen}}^\star \approx w_{\mathrm{pred}}^\star$ in a \emph{quantitative} way, and hence 
% $\bigl|f(g(x), w_{\mathrm{gen}}^\star) - f(g(x), w_{\mathrm{pred}}^\star)\bigr|$ is small.

Finally, we get,
\begin{align*}
\mathbb{E}_x|\Delta R(x)| &\le \mathbb{E}_x \Bigl[ L_w \cdot \frac{2}{\alpha}
\;\bigl[
  \kappa_1\;\mathcal{W}(p_\theta, p)
  \;+\;
  \kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
  \;+\;
  \kappa_3\,\|\mathrm{Bias}[g]\|
\bigr]\\
&+ 2L_c \bigl| \sqrt{d}\|Var[c\mid x] \|^{1/2} + \mathbb{E}_{p(c|x)}[\|\text{Bias}[g(x)]\|] \bigr|\Bigr].
\end{align*}

\end{proof}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
