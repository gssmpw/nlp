%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{enumitem}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage[toc,page]{appendix}
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\usepackage{afterpage}
\usepackage{placeins}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

% \usepackage{algorithm}
% \usepackage{algpseudocode}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{xcolor}
\newcommand\prince[1]{\textcolor{blue}{Prince: #1}}
\newcommand\ryan[1]{\textcolor{orange}{Ryan: #1}}
\newcommand\woody[1]{\textcolor{red}{Woody: #1}}
\newcommand\nando[1]{\textcolor{purple}{$^\texttt{NF}$: [#1]}}
\newcommand\jinhao[1]{\textcolor{brown}{Jinhao: #1}}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\texttt{Gen-DFL}: Decision-Focused Generative Learning for Robust Decision Making}

\begin{document}

\twocolumn[
\icmltitle{\texttt{Gen-DFL}: Decision-Focused Generative Learning for Robust Decision Making}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Prince Zizhuang Wang}{yyy}
% % \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Jinhao Liang}{uva}
\icmlauthor{Shuyi Chen}{yyy}
\icmlauthor{Ferdinando Fioretto}{uva}
\icmlauthor{Shixiang Zhu}{yyy}
% % \icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Heinz College, Carnegie Mellon University, USA}
\icmlaffiliation{uva}{Computer Science, University of Virginia, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Prince Wang}{princewang@cmu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Decision Focused Learning, Generative Models}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Decision-focused learning (DFL) integrates predictive models with downstream optimization, directly training machine learning models to minimize decision errors. While DFL has been shown to provide substantial advantages when compared to a counterpart that treats the predictive and prescriptive models separately, it has also been shown to struggle in high-dimensional and risk-sensitive settings, limiting its applicability in real-world settings. 
To address this limitation, this paper introduces decision-focused generative learning (\texttt{Gen-DFL}), a novel framework that leverages generative models to adaptively model uncertainty and improve decision quality. Instead of relying on fixed uncertainty sets, \texttt{Gen-DFL} learns a structured representation of the optimization parameters and samples from the tail regions of the learned distribution to enhance robustness against worst-case scenarios. This approach mitigates over-conservatism while capturing complex dependencies in the parameter space.
The paper shows, theoretically, that \texttt{Gen-DFL} achieves improved worst-case performance bounds compared to traditional DFL. Empirically, it evaluates \texttt{Gen-DFL} on various scheduling and logistics problems, demonstrating its strong performance against existing DFL methods.
\end{abstract}
% \woody{Avoid using ChatGPT when it comes to math (at least, you need proofread these content and double check the details). I found a lot factual errors in the GPT generated content. Also, please use ``\$\$'', instead of ``\ (  \ )''}

\section{Introduction}

Decision-making under uncertainty is central to many real-world applications, including supply chain management, energy grid optimization, portfolio management, and transportation planning~\cite{SAHINIDIS2004971, liu2009theory, hhl003, delage2010distributionally, hu2016toward, kim2005optimal}. In these domains, decision makers must act based on incomplete information, relying on predictions from machine learning models to estimate key parameters such as future demand, asset returns, or power grid failures.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.\linewidth]{figures/gendfl-figure1.pdf}
    \caption{Comparison of the proposed decision-focused generative learning (\texttt{Gen-DFL}) framework with conventional predict-then-optimize (PTO) and decision-focused learning (DFL).
    }
    \label{fig:pipeline}
    \vspace{-0.1in}
\end{figure}

Standard methods, commonly referred to as predict-then-optimize (PTO)~\cite{elmachtoub2017smart}, tackle this problem by first training a predictive model to estimate the parameters of an optimization problem (e.g., expected demand or cost coefficients) and then using these estimates as inputs to an optimization model. While the separation between prediction and optimization enhances efficiency, it also introduces a fundamental drawback. Predictive models are typically trained to minimize standard loss functions (e.g., mean squared error), which may not align with the true objective of minimizing decision costs. As a result, small prediction errors can propagate through the optimization process, leading to costly, suboptimal decisions. For instance, in power outage management \cite{zhu2021quantifying}, overestimating energy demand may lead to unnecessary resource allocation, whereas underestimation could result in supply shortages and prolonged downtime.

To address this issue, decision-focused learning (DFL) integrates prediction and optimization into a single end-to-end framework~\cite{donti2017task, Mandi_2024}. Instead of optimizing purely for predictive accuracy, DFL trains machine learning models with the explicit goal of minimizing the final decision cost. This key idea is enabled by differentiating the optimization process within the learning loop, and results in an alignment of the model’s predictions with their downstream impact. This approach has shown clear improvements in structured decision-making tasks where the optimization landscape is well-behaved and relatively low-dimensional.

Despite these advantages, DFL suffers from several critical limitations: 
($i$) \emph{Scalability}: In high-dimensional settings, the curse of dimensionality~\cite{koppen2000curse} degrades the predictive model’s ability to capture complex dependencies in the parameter space. Since DFL typically relies on single-point predictions, it struggles to encode the full distributional uncertainty of the decision variables \cite{Fioretto:jair24}. This leads to overconfident estimates that degrade decision quality when uncertainty is high.
($ii$) \emph{Risk Sensitivity}: In many applications, decision-makers prioritize robustness over worst-case outcomes rather than optimizing for expected performance. Traditional DFL models, however, are primarily trained to improve average-case decisions and do not explicitly model tail risks \cite{ben2009robust, beyer2007robust}. 

To overcome these challenges, this paper proposes decision-focused generative learning (\texttt{Gen-DFL}), a novel end-to-end framework that leverages generative models to enhance decision quality in high-dimensional and risk-sensitive settings. Unlike traditional approaches that rely on fixed uncertainty sets, \texttt{Gen-DFL} learns a distributional representation of uncertain parameters using deep generative models. 
Recent advances in generative modeling enable efficient learning of complex, high-dimensional distributions~\cite{dong2023conditional, wu2024counterfactual}, allowing for adaptive sampling from tail regions to support risk-aware decision-making without excessive conservatism.
By dynamically balancing robustness and efficiency, \texttt{Gen-DFL} provides a more flexible and principled approach to decision optimization. A schematic comparison of the predict-then-optimize (PTO) model, standard DFL, and \texttt{Gen-DFL} is shown in Figure \ref{fig:pipeline}.

\textbf{Contributions.} The paper makes three key contributions: 
\begin{itemize}[left=0pt,topsep=0pt,parsep=0pt,itemsep=0pt]
\item It introduces \texttt{Gen-DFL}, the first DFL framework that leverages generative models to capture uncertainty in high-dimensional stochastic optimization and enable task-specific risk management for controllable robustness.
\item It provides a theoretical analysis elucidating the conditions under which \texttt{Gen-DFL} outperforms traditional DFL, with a particular emphasis on high-dimensional and risk-sensitive decision problems.
\item Through comprehensive experiments on both synthetic and real-world decision-making tasks, the paper shows that \texttt{Gen-DFL} significantly improves decision quality compared to existing DFL baselines.
\end{itemize}

\section{Related Works}
Decision-making under uncertainty has driven research in decision-focused learning, robust optimization, and risk-aware optimization. We review these approaches and their limitations for high-dimensional uncertainty and risk-sensitive decisions, motivating our proposed framework.

Decision-focused learning (DFL) enhances decision-making under uncertainty by integrating prediction and optimization into a single framework. \citet{bengio1997using} showed that optimizing predictive models for decision outcomes improves financial performance. 
Differentiable optimization layers have further expanded DFL applications \cite{agrawal2019differentiable}. For example, \citet{pmlr-v70-amos17a} introduced differentiable quadratic programs, enabling backpropagation through constrained optimization, while \citet{agrawal2019differentiable} extended this to all convex programs. Parallel work has explored integrating integer programming into neural networks~\cite{NEURIPS2020_51311013,wilder2019melding}.

However, existing DFL methods rely on single-point predictions, failing to capture uncertainty and leading to suboptimal decisions~\cite{koppen2000curse,ben2009robust}. Additionally, they typically optimize for average-case performance, making them unsuitable for risk-sensitive applications~\cite{Mandi_2024}. Approaches like Conformal-Predict-Then-Optimize (CPO)~\cite{patel2024conformal} attempt to address this by constructing fixed uncertainty sets but can be overly conservative, especially in high-dimensional settings.

Robust Optimization (RO) provides a principled approach to decision-making under uncertainty by ensuring solutions remain feasible under the worst-case scenario~\cite{ben2002robust,bertsimas2004robust,ben2006extending}. Instead of relying on probabilistic assumptions about uncertain parameters, RO constructs uncertainty sets that define the range of possible parameter values \cite{bertsimas2011theory} and aims to find the decision that is robust against the worst-case in the uncertainty sets. This approach has found applications in domains such as supply chains \cite{bertsimas2004robust}, currency portfolio management \cite{fonseca2011robust}, and power system optimization \cite{10384836}. 

% \woody{There are two limitations in RO: 1. difficult to specify the uncertainty set. most of the literature relies on the heuristic choice of uncertainty set; 2. being overly conservative due to relying on the single worst case scenarios.}
Despite its guarantees, the solutions suggested by RO suffer from two major limitations: 
($i$) Uncertainty set construction usually relies on heuristic choices, making it difficult to capture the real dynamics in the real-world applications~\cite{10384836}.
($ii$) Such pre-specified uncertainty sets tend to be overly conservative~\cite{Roos2020ReducingConservatism} as it focuses solely on the worst-case outcome, whereas many high-stakes applications require accounting for multiple adverse scenarios.
% To alleviate this limitation, conditional value-at-risk (CVaR) has been widely adopted as a measure for quantifying high-loss scenarios beyond a given risk level~\cite{gabrel2014recent,lotfi2018robust}. CVaR optimizes over the worst $\alpha$-quantile of possible outcomes, providing a principled approach to minimizing expected costs in tail regions.

% \woody{Consider adding a discussion on generative model somewhere in the related work: The proposed framework also relates to generative modeling ...}

The proposed framework also relates to generative modeling. Generative modeling has shown promise for a number of fields such as image generation~\cite{ho2020denoising}, chemical species design~\cite{anstine2023generative}, and trajectory planning~\cite{liang2024multi}. Recently, flow-based generative modeling approaches outperform others by establishing a mapping between complex distributions and a simple prior directly~\cite{lipman2022flow,zheng2023guided}. In this study, we also adopt the flow-based method, conditional normalizing flows (CNFs)~\cite{winkler2019learning}, to capture the target distribution in high-risk regions.

\section{Preliminaries}

This section revisits the background of decision-focused learning and robust optimization.

\subsection{Decision-Focused Learning}

Consider a general stochastic optimization problem:  
\begin{equation}
w^\star \coloneqq \arg \min_{w} \mathbb{E}_{c\sim p(c)}[f(c, w)],
\label{eq:general_risk_min}
\end{equation}
where $c$ is a random vector characterizing the problem parameters, and $f(c,w)$ is the objective function. The goal is to find the optimal decision $w^*$ that minimizes the expected decision cost under the conditional distribution $p(c)$.

A common approach, predict-then-optimize (PTO), assumes a linear objective, which simplifies the problem to
\begin{equation}
    w^*(\hat{c}) \coloneqq \arg \min_{w} \hat{c}^T w.
    \label{eq:pto}
\end{equation}
where $\hat{c}$ is the estimate of $\mathbb{E}[c|x]$ conditioning on covariate $x$. 
This framework consists of two components: ($i$) A predictor $\hat{c} \coloneqq g_\theta(x)$, trained to minimize the standard mean squared error (MSE) $\mathbb{E}||\hat{c} - c||^2$; ($ii$) An optimization model that finds the best decision $w$ given $\hat{c}$. As noted by \cite{elmachtoub2017smart}, this approach often leads to suboptimal decisions, as minimizing prediction error does not necessarily translate to improved decision quality.

To mitigate this issue, decision-focused learning (DFL) \cite{Mandi_2024} integrates prediction with decision-making by training $g_\theta(x)$ using decision regret as the loss function. The loss function is defined as follows:
\begin{align*}
    \ell_{\text{DFL}}(\theta) = & ~\mathbb{E}_x \left[\text{Regret}(g_\theta(x), c)\right],~\text{where}\\ 
    \text{Regret}(g_\theta(x), c) = & ~f(c, w^\star(g_\theta(x))) - f(c, w^\star(c)).
\end{align*}
For notational simplicity, we use $c$ to denote the true mean of the optimization parameters given $x$. 
By optimizing $g_\theta(x)$ directly with respect to decision performance, DFL ensures that the predicted parameters yield decisions that are robust to downstream cost objectives. We will refer to this conventional DFL approach, which relies on explicit prediction models, as Pred-DFL. 

\subsection{Robust Optimization}

In some real-world applications, the expectation-based optimization in \eqref{eq:pto} may fail to provide reliable decisions under adverse conditions, potentially leading to severe consequences \cite{ben2009robust, beyer2007robust}. To mitigate this risk, robust optimization (RO) \cite{Kouvelis1997RobustDiscreteOptimization,ben2009robust,shalev2016minimizing} seeks decisions that perform well in the worst-case scenario within an uncertainty set $\mathcal{U}(x)$, by solving the min-max formulation below:
% \begin{equation}
%     w^\star(x) \coloneqq \arg \min_{w} \ \max_{c \in \mathcal{U}(x)} f(c, w).
%     \label{eq:ro}
% \end{equation}
\begin{equation}
    w^\star(x) \coloneqq \arg \min_{w} \ \max_{c \in \mathcal{U}(x)} f(c, w).
    \label{eq:ro}
\end{equation}
% \jinhao{I usually use uncertainty parameters (e.g., prediction errors $\epsilon$) to represent the uncertainty set $\mathcal{U}(\epsilon)$. I don't know if it's feasible  to represent the uncertainty set using observation $x$.}
This formulation ensures robustness against the most adverse realization of $c$, providing worst-case protection. However, it can be overly conservative, potentially leading to suboptimal decisions in typical scenarios. In many risk-sensitive applications, a more nuanced approach is required -- one that balances robustness and flexibility by considering a broader range of adverse outcomes beyond just the extreme worst case \cite{sarykalin2008value}. This has led to the development of alternative robust and risk-aware optimization frameworks, such as distributionally robust optimization (DRO) {\cite{NEURIPS2018_a08e32d2, zhu2022distributionally} and conditional value-at-risk (CVaR) optimization \cite{duffie1997overview, rockafellar2000optimization, rockafellar2002conditional}, which offer a more refined trade-off between robustness and performance.

\section{Proposed Framework: \texttt{Gen-DFL}}

This section presents the proposed decision-focused generative learning (\texttt{Gen-DFL}) framework.
Specifically, we develop a novel decision-making paradigm, generate-then-optimize (GTO), designed for risk-sensitive decision problems. Our approach frames the problem as a conditional value-at-risk (CVaR) optimization, leveraging a generative model to produce plausible samples that capture the dynamics of high-risk regions. 
To effectively learn the generative model, we propose a new loss function that integrates both decision-focused learning and generative modeling objectives, ensuring that the generated samples not only reflect the underlying data distribution but also lead to robust, high-quality decisions. Figure~\ref{fig:gendfl} provides an overview of the proposed framework.

\subsection{Problem Setup}
\label{sec:setup}

We seek robust decisions that effectively manage risk by minimizing the percentiles of loss distributions. This approach has been widely adopted in risk-sensitive domains such as financial portfolio optimization, where regulatory frameworks often define risk management requirements in terms of loss percentiles \cite{sarykalin2008value}.

A widely used measure for quantifying high-loss scenarios is conditional value-at-risk (CVaR) \cite{duffie1997overview, rockafellar2000optimization, rockafellar2002conditional}, which provides a characterization of tail risk by capturing the expected loss beyond a given percentile threshold. 
Formally, given a confidence level $\alpha$, CVaR is defined as:
\begin{equation}
\text{\text{CVaR}}[f(c, w);\alpha] = \mathbb{E}\left[f(c, w) \mid f(c, w) \geq \text{VaR}_\alpha \right],
\end{equation}
where $\text{VaR}_\alpha$ represents the value-at-risk threshold, meaning the probability of exceeding this threshold is at most $1 - \alpha$.

Our objective is to find the optimal decision $w^\star$ that minimizes the expected costs in the worst-$\alpha\%$ of outcomes. This leads to the following risk-sensitive optimization formulation \cite{krokhmal2002portfolio}:
% \begin{equation}
% \label{eg:cvar_ro}
%     w_\alpha^\star(x) \coloneqq \arg \min_{w} \ \text{CVaR}_{c \sim p(c|x)} [f(c, w); \alpha].
% \end{equation}
\begin{equation}
\label{eg:cvar_ro}
    w^\star(x; \alpha) \coloneqq \arg \min_{w} \ \text{CVaR}_{c \sim p(c|x)} [f(c, w); \alpha].
\end{equation}
We note that the $c$ is defined over the high-risk region of the distribution $p(c|x)$, allowing for a more flexible and probabilistic characterization of uncertainty compared to the ``hard'' uncertainty set used in \eqref{eq:ro}.
This formulation bridges robust and expectation-based optimization:
($i$) As $\alpha \to 0$, the problem reduces to robust optimization, focusing exclusively on the worst-case scenario in \eqref{eq:ro}. ($ii$) As $\alpha \to 1$, it converges to standard expectation-based optimization in \eqref{eq:general_risk_min}, minimizing the expected cost across all possible outcomes. Thus, our approach generalizes robust optimization by ensuring resilience against adverse outcomes beyond a single worst-case scenario, balancing conservatism and probabilistic risk awareness in decision-making.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/GenDFL_figure2.pdf}
    \caption{Overview of the proposed \texttt{Gen-DFL} framework. 
    The right panel compares \texttt{Gen-DFL} with the traditional DFL approaches which either relies a point predictor (Pred-DFL) or assume that the conditional distribution $p(c|x)$ follows a simpler form (an isotropic Gaussian) (Pred-DFL+). 
    In contrast, \texttt{Gen-DFL} leverages a generative model to capture $p(c|x)$ while incorporating the decision-making objective which emphasizes the high-risk region. 
    }
    \label{fig:gendfl}
    \vspace{-.15in}
\end{figure}

\subsection{Generate-Then-Optimize}

To solve \eqref{eg:cvar_ro}, we introduce a novel generate-then-optimize (GTO) paradigm, which leverages generative modeling to approximate the risk-sensitive optimization problem.

Conventional decision-focused learning (Pred-DFL) relies on a point estimate $\hat{c}$ of the optimization parameters. While effective in some cases, this approach fails to capture the full distribution $p(c|x)$, particularly in high-dimensional settings, making it inadequate for risk-sensitive applications where adverse outcomes must be explicitly considered. 
Moreover, point estimates are only appropriate when the objective function is linear, as the optimization problem in such cases depends solely on the expected value of $c$, making variance and higher-order moments irrelevant. 
% Additionally, point estimates are only appropriate when the objective function is linear, because in such cases, the optimization problem depends solely on the expected value of $c$, making variance and higher-order moments irrelevant. 
% \ryan{i think this is a run-on sentence that is too long}

To overcome these limitations, we replace deterministic predictions with a generative model, capturing the full risk distribution. This allows us to account for uncertainty in a data-driven manner, ensuring that risk-sensitive scenarios are explicitly considered. The optimization problem is then solved using sample-average approximation (SAA)
% \ryan{SAA acronym mentioned here for the first time. Should we change to sample average apprixmation (SAA) here?} 
~\cite{pagnoncelli2009sample, kim2015guide, emelogu2016enhanced}. 
Formally, we aim to optimize:
% \begin{equation}
% \label{eq:gto}
% w_\alpha^\star(x;\theta) \coloneqq \arg \min_{w} \ \text{CVaR}_{c \sim p_\theta(c|x)} \left [f(c, w); \alpha \right ].   
% \end{equation}
\begin{equation}
\label{eq:gto}
w_\theta^\star(x;\alpha) \coloneqq \arg \min_{w} \ \text{CVaR}_{c \sim p_\theta(c|x)} \left [f(c, w); \alpha \right ].   
\end{equation}
% \begin{equation}
% \label{eq:gto}
% \theta^\star \coloneqq \arg \min_{w} \ \text{CVaR}_{c \sim p_\theta(c|x)} \left [f(c, w_\theta^\star(x;\alpha)) \right ].   
% \end{equation}
Unlike traditional RO, which requires a pre-defined uncertainty set $\mathcal{U}(x)$ -- often leading to overly conservative or restrictive formulations -- our approach treats uncertainty as a learnable distribution. Specifically, we model $p_\theta(c|x)$ using a generative model parameterized by $\theta$, 
allowing it to adaptively capture risk-sensitive regions based on empirical data. 
This approach provides a more nuanced and adaptive approach to uncertainty modeling, ensuring that decisions are informed by the full distribution of possible outcomes rather than rigid, pre-specified constraints.
% allowing us to adaptively capture risk-sensitive regions based on both the empirical data as well as the structure of the downstream optimization.

We emphasize that the proposed \texttt{Gen-DFL} framework is \emph{model-agnostic} and does not rely on a specific generative modeling choice. 
In this work, we adopt conditional normalizing flows (CNFs)~\cite{winkler2019learning} to model the conditional distribution $p(c|x)$ due to their flexibility. 
CNFs transform a simple base distribution $p_Z(z)$ (e.g., Gaussian) into a complex target distribution via an invertible mapping $g_\theta : \mathcal{C} \rightarrow \mathcal{Z}$, where $\mathcal{C}, \mathcal{Z}$ are the supports of the resulting distribution and the base distribution. This enables the representation of arbitrarily complex distributions. This transformation follows the change-of-variables formula \cite{tabak2013family, papamakarios2021normalizing}:
\[
p_\theta(c|x) = p_Z(g_\theta(c; x)) \left|\frac{\det \partial g_\theta(c; x)}{ \partial c}\right|.
\]
% \ryan{what is base distribution in our case? What is Z? What is the difference of $f_\theta$ and $f$? I think this section needs more discussion for audience not familiar with this model}
This expressiveness enables our model to generate samples that accurately capture both typical and high-risk scenarios, improving robustness in decision-making under CVaR.

% Future work could explore more advanced generative models, such as diffusion models~\cite{ho2020denoising}, to further enhance performance in risk-sensitive decision-making.


\subsection{Decision-Focused Generative Learning}

We now present the \texttt{Gen-DFL} framework, which provides a decision-focused solution to the GTO problems. 
For simplicity, we denote the optimal decision obtained from our model $w_\theta^\star(x; \alpha)$ in \eqref{eq:gto} as $w_\theta^\star$, omitting $x$ and $\alpha$.
Similar to other DFL frameworks, \texttt{Gen-DFL} consists of two alternating steps: 
\begin{enumerate}[left=0pt,topsep=0pt,parsep=0pt,itemsep=0pt]
    \item \emph{Generate-Then-Optimize}: Generate samples $\{c_k\}_{k=1}^K$ using conditional generative model (CGM) $p_\theta(c|x)$ and solve \eqref{eq:gto} for the optimal decision via SAA. 
    \item \emph{Model Learning}: Given the resulting decision $w_\theta^\star$, update the generative model parameters by jointly minimizing the generative loss and the decision cost under $w_\theta^\star$.
\end{enumerate}
A detailed description of the learning procedure is provided in Algorithm~\ref{alg:gen-dfl}. Below, we elaborate on key components of our framework.

% \begin{equation}
% \theta^\star \coloneqq \min_\theta \text{CVaR}_{c\sim p_\theta(c|x)}\left [f(c,w_\theta^\star);\alpha \right].
% \end{equation}
    
\paragraph{Regret in CVaR.}

Unlike Pred-DFL, where the decision cost is computed as the regret for a single pair $(\hat{c}, c)$,
in our stochastic optimization problem, the parameter $c$ follows a distribution, requiring regret to be evaluated over all possible realizations of $c$.
Moreover, in robust decision-making, we seek to minimize decision costs based on the worst-$\alpha\%$ outcomes, rather than the full distribution. To capture this, we define regret using CVaR:
\begin{align*}
    & \text{Regret}_{\theta, p}(x;\alpha) \coloneqq \text{CVaR}_{p(c|x)}\Big[f(c, w_\theta^\star) - f(c, w^\star);\alpha\Big],    
\end{align*}
where $w^\star \coloneqq \arg\min_{w} \text{CVaR}_{c\sim p(c|x)}[f(c, w);\alpha]$ is the optimal decision under the true distribution.
% and $w_\alpha^\star \coloneqq \arg\min_{w} \text{CVaR}_{c\sim p_\theta(c|x)}[f(c, w);\alpha]$ is the decision found by the model-based distribution.
The parameter $\alpha$ controls the level of risk sensitivity: 
The lower values of $\alpha$ emphasize the worst-case outcomes, making decisions more conservative. 
When $\alpha=1$, it recovers the expected regret across all realizations:
$\mathbb{E}_{c\sim p(c|x)} [f\left( c, w_\theta^\star \right)  -   f\left( c, w^\star \right)]$.

\paragraph{Sample-Based Regret Estimation.}

In practice, the true data distribution $p(c|x)$ is typically inaccessible, making direct regret evaluation infeasible. To address this challenge, we introduce an auxiliary model $q(c|x)$, trained on available data to approximate $p(c|x)$. 
Once learned, $q(c|x)$ remains fixed and serves as a proxy distribution to compute the estimated $\text{Regret}_{\theta, q}(x, \alpha)$ and the corresponding surrogate loss function $\ell(\theta;\alpha, q)$. This enables practical regret evaluation even when the true distribution is not directly observable.

\paragraph{\texttt{Gen-DFL} Loss.}
The training objective for \texttt{Gen-DFL} is formulated as the aggregated regret across all inputs $x$, with an additional regularization term to ensure stability in generative modeling:
\begin{equation}
\label{eq:gendfl-loss}
    \ell_\texttt{Gen-DFL}(\theta;q, \alpha) \coloneqq 
    \mathbb{E}_x[\text{Regret}_{\theta, q}(x;\alpha)] + \gamma \cdot \ell_\text{gen}(\theta),
\end{equation}
where $\ell_\text{gen}(\theta)$ is the generative model loss (e.g., negative log-likelihood, evidence lower bound (ELBO) for variational autoencoders~\cite{kingma2013auto}, or score-matching loss for diffusion models~\cite{ho2020denoising}). Here, $\gamma$ is a hyper-parameter that balances the decision-focused regret loss and the generative model loss. 
% \ryan{should we mention choice of $\alpha$ here or somewhere? Doesn't it act like a hyperparameter, like $\gamma$?}
The generative loss term $\ell_\text{gen}(\theta)$ acts as a regularization, preventing the learned generative model from deviating excessively from the true data distribution, ensuring reliable sample generation for decision-making.


\begin{algorithm}[tb]
   \caption{Learning Algorithm for \texttt{Gen-DFL}}
   \label{alg:gen-dfl}
\begin{algorithmic}
   \STATE {\bfseries Input:} Dataset $\mathcal{D} = \{(x_i, c_i)\}_{i=1}^N$, CGM $p_\theta(c | x)$, learning rate $\eta$, regularization ratio $\gamma$, sampling size $K$, risk-level $\alpha$, a proxy model $q(c|x)$ trained on $\mathcal{D}$.
   % , proxy model $q(c|x)$, \woody{what is CGM? define it in the paper. $\beta$ is the step size? $\gamma$ is the ratio of the panelty? is the proxy model supposed to be $q$? $q$ should appear in the algorithm?}
   % Samples $K$, $\widehat{\mathcal{U}}_\theta$ where $P(\widehat{C} \in \widehat{\mathcal{U}}_\theta) \geq 1 - \alpha$
   \WHILE{not converged}
   % \STATE $\{\tilde{c}_k\}_{k=1}^K \sim p_\alpha(C | x)$ \text{s.t.} $\tilde{c}_k \in \widehat{\mathcal{U}}_\theta$
   % \STATE $\{w_k^\star\}_{k=1}^K \gets \{\arg \min_w f(w, \tilde{c}_k)\}_{k=1}^K$
   % \STATE $k' \gets \arg \max_k f(w_k^\star, \tilde{c}_k)$
   % \STATE $c_{\text{worst}} = c_{k'}$
   \STATE $\{c_k\}_{k=1}^K \sim p_\theta(c | x)$; $K_\alpha \leftarrow (1-\alpha)^K$;
   % \STATE $\hat{w}_\theta^\star(x) \leftarrow \arg \min_{w}\text{CVaR}_{c\sim p_\theta(c|x)}[f(c, w)]$; %\woody{CVaR by Quantile?}
   \STATE $w_\theta^\star \leftarrow \arg \min\limits_{w}  \sum\limits_{k=1}^{K} \frac{f(c_k, w)}{K_\alpha} \mathbbm{1}\{f(c_k, w) \geq \text{VaR}_\alpha\}$;
   % \STATE $\text{Regret}_{\alpha, p}(x;\theta) = \text{CVaR}_{p}\Big[ f(c, \hat{w}_\theta^\star(x)) - f(c, w^\star);\alpha \Big]$
   % \STATE $\text{Regret}_\alpha(x;p) =\text{CVaR}_{p(c|x)}\Big[f(c, \hat{w}^\star) - f(c, w^\star);\alpha\Big]$
   \STATE $\ell(\theta;q, \alpha) \leftarrow \frac{1}{n} \sum\limits_{i=1}^n \text{Regret}_{\theta, q}(x_i;\alpha) + \gamma \cdot \ell_\text{gen}(\theta)$;
   \STATE $\theta \leftarrow \theta - \eta \cdot \partial \ell / \partial \theta$;
   \ENDWHILE
\end{algorithmic}
\end{algorithm}

% \subsection{Approximating the Regret}
% \paragraph{Approximation of Regret when $p(c|x)$ is not given}


% \subsection{Connection to Predict-Then-Optimize}

% % \woody{Dive further into the discussion on how to generalize the proposed framework to other classical setups and what are their key differences, mathematically. Discuss pros and cons and the tradeoff from a high-level. You can reveal more detailed theoretical insights in the section 4. } 
% \prince{Addressed but may need further refinement}

% In Pred-DFL and \textit{Predict-Then-Optimize}, the optimization problem is often reformulated as $\min_{w}f(g_\theta(x), w)$ where $g_\theta(x)$ is a learned predictor that outputs a point estimate $\hat{c}$ of the optimization parameter $c$. This formulation implicitly assumes that optimizing over the expected costs $\text{CVaR}_{p(c|x)}[f(c,x)]$ is equivalent to optimizing over a single point estimate $f(\hat{c}, w)$, which holds only when $f(c,w)$ is linear in $c$ and is only an approximation of the true expected costs otherwise, especially in the high-dimensional case where the point estimate of the mean would be even less informative. When $f(c,w)$ is non-linear or when the decision is sensitive to the risk under the tail region which requires us to model the full contextual $p(c|x)$, the above simplification is inadequate and introduces a bias that leads to suboptimal decisions. 

% Our Gen-DFL framework replaces the point estimate $\hat{c} = g_\theta(x)$ with a generative model $p_\theta(c|x)$, allowing decisions to be optimized over samples drawn from the full uncertainty set of the parameter space rather than a single predicted value. By directly minimizing the CVaR-based regret $\text{Regret}_{\alpha, p}(x;\theta)$, Gen-DFL ensures that the resulting decision is robust against a set of adverse outcomes beyond some risk-sensitive level under the support of $p(c|x)$. While this introduces computational complexity due to sampling, it provides a principled way to handle risk-sensitive objectives and high-dimensional uncertainties that Pred-DFL. Moreover, Gen-DFL avoids the limitations of naive quantile regression approaches, which struggle in high-dimensional settings due to increasing estimation variance. By leveraging a generative model, we ensure that the decision-maker accounts for the full uncertainty structure, leading to more robust and adaptive decisions. We will discuss more details about the difference between Gen-DFL and Pred-DFL in the following analysis section.

% ============== the following is deprecated but might be useful ==================
% When operating on stochastic optimization problems, many DFL frameworks [\prince{fix citations}] often assume that the objective function $f(c,w) = c^T w$ is linear, so that the problem can be written as $\min_{w}\mathbb{E}_c[c]^Tw$, where $\mathbb{E}_c[c]$ can be replaced by the point estimate $\hat{c}=g_\theta(x)$ from the predictor. Under this assumption, optimizing over the point estimate $\hat{c}^Tw$ is equivalent to optimizing over the entire expectation $\mathbb{E}_c[c^T w]$.
% % Hence, many DFL frameworks are limited to linear settings and are not designed to handle more complex stochastic optimization problems where the objective is non-linear. 

% The above approach, however, is not suitable for our robust optimization setting where one needs to model the full conditional distribution $p(c|x)$ in order to find the worst-$\alpha\%$ of the outcomes in the tailed regions. Moreover, the above assumption does not work for the cases where the objective function $f(c,x)$ is non-linear.
% To deal with non-linear cases, some methods such as ~\cite{donti2017task} assume that the conditional distribution $p(c|x)$ follows a simpler form, such as Gaussian, and sort to predict the parameters of the given distribution instead. This, however, also limits the expressiveness of the model.
% ============== the following is deprecated but might be useful =================

% ==================In progress: skipped the discussion on NCE FOR NOW =========================
% \subsection{Surrogate Loss Function}

% When training DFL models w.r.t the decision loss, backpropagating the error through the decision-making process is necessary. However, this requires calculating the partial derivatives of the loss $ \ell $ with respect to the model parameters $ \theta $. Since the loss is defined as a function of the optimal decision $ w^\star(\hat{c}) $ with respect to an estimated cost $ \hat{c} $, differentiating it involves complex dependency chains.

% Specifically, as shown in Equation (7), the gradient of the real loss $ \ell $ with respect to $ \theta $ can be decomposed using the chain rule:
% $
% \frac{d \ell(w^\star(\hat{c};\theta), c)}{d \theta} = \frac{d \ell(w^\star(\hat{c};\theta), c)}{d w^\star(\hat{c};\theta)} \cdot \frac{d w^\star(\hat{c};\theta)}{d \hat{c}} \cdot \frac{d \hat{c}}{d \theta}.
% $
% The first term and the third term on the right-hand side are easy to compute. In the case of generative model, we can compute the gradient $ \frac{d\hat{c}}{d\theta}$ via reparameterization trick. However, calculating $ \frac{d x^\star(\hat{c})}{d \hat{c}} $ poses a significant challenge, as it involves differentiating through a combinatorial optimization mapping. Such mappings are typically non-smooth and computationally expensive to differentiate, making gradient-based learning difficult and unstable. Consequently, optimizing directly with respect to the real loss $ \ell $ is often infeasible in practice, especially for large-scale problems or complex models.

% Inspired by \cite{mulamba2020contrastive}, we propose a surrogate contrastive loss in our Gen-DFL setting for stochastic optimization to address the above challenge of differentiating the combinatorial optimization mapping: 
% % \[  
% % \mathbb{E}_x \Big[ \sum_{w^s \in S} \left( \mathbb{E}_{p_\theta(c|x)} \left[ f\left( c, w^s) \right) \\ -  f\left( c, w^\star \right) \right] \right) \Big],
% % \]
% % Similarly, in the context of risk-sensitive or robust optimization, we can extend the above surrogate loss to the case of CVaR setting:
% % \begin{align*} 
% % \ell_{NCE}(\theta;\alpha) &= \mathbb{E}_x \Big[ \sum_{w^s \in S} \left( \text{CVaR}_{p_\theta(c|x)} \left[ f\left( c, w^s \right) \\ & -  f\left( c, w^\star \right) ; \alpha \right] \right) \Big] + \ell_{gen}(\theta),
% % \end{align*}
% \begin{align*}
%     \ell_{N}(\theta;\alpha) &= \mathbb{E}_x \Big[ \sum_{w^s \in S} \left( \text{CVaR}_{p_\theta(c|x)} \left[ f\left( c, w^s) \right) -  f\left( c, w^\star \right);\alpha \right] \right) \Big] \\
%     &+ \ell_{gen}(\theta)
% \end{align*}
% where $w^\star = \arg\min_{w} \ \mathbb{E}_{c\sim p_\theta(c|x)}[f(c, w)]$ is the target solution, and negative samples $w^s \in S \subset \mathcal{W}  \setminus w^\star$ is a subset of solutions that are different from the target solution.
% ==================In progress: skipped the discussion on NCE FOR NOW =========================

% ============= Diffusion Model Intro: if needed=================
% \paragraph{Conditional Diffusion Model}

% ============= Diffusion Model Intro: if needed=================


\section{Theoretical Analysis}
\label{analysis}

This section provides an analysis of the validity of our sample-based regret estimation method and compares \texttt{Gen-DFL} and traditional Pred-DFL across different problem settings by examining their regret bounds.
Our analysis reveals that as the complexity of the optimization problem increases -- whether due to higher dimensionality, greater variance in the data, or more nonlinear objective function -- \texttt{Gen-DFL}'s advantage over Pred-DFL becomes more pronounced, leading to improved decision quality in challenging settings.

We first derive the bound for the loss difference $|\ell(\theta;p, \alpha) - \ell(\theta;q,\alpha)|$, comparing the loss function $\ell(\theta;p, \alpha)$ under the ground-truth distribution $p(c|x)$ with the surrogate loss $\ell(\theta;q, \alpha)$ computed using the proxy model $q(c|x)$.
\begin{theorem}
% Let $ f(c, w) $ be an objective function that is $ L_f $-Lipschitz continuous with respect to $ c $ for a fixed decision variable $ w $, i.e.,
% \[
% |f(c, w) - f(c', w)| \leq L_f \| c - c' \| \quad \text{for all } c, c' .
% \]
% Let $ p(c|x) $ be the ground-truth distribution and $ q(c|x) $ be a surrogate distribution that approximates $ p(c|x) $. 

Under the assumption that the objective function $f(c,w)$ is $L_f$-Lipschitz continuous with respect to $c$ for a fixed decision variable $w$, the gap between $ \ell(\theta;p, \alpha)$ and $\ell(\theta;q, \alpha)$
% \woody{Any typo here? one of them should be coming with a star?} (\prince{$\ell(\theta;\alpha, p)$ is the loss assuming we have the access to $p(c|x)$, $\ell(\theta;\alpha, q)$ is the loss when we only the proxy $q(c|x)$})
is bounded by
% \woody{replace $K_q$ by $C_q$? because $K$ has been used as the number of generated samples.}
\[
|\ell(\theta;p, \alpha) - \ell(\theta;q,\alpha)| \leq K_q \cdot \mathbb{E}_x \left[ \mathcal{W}(p(c|x), q(c|x)) \right],
\]
where $ \mathcal{W}(p(c|x), q(c|x)) $ is the Wasserstein-1 distance between $ p(c|x) $ and $ q(c|x) $ and $K_q$ is some constant.
% This can be extended to the constrastive loss functions as well. That is,
% \[
% |\ell_{NCE}(\theta) - \ell_{NCE}_q(\theta)| \leq K_q \cdot \mathbb{E}_x \left[ W_1(p(c|x), q(c|x)) \right],
% \]
\end{theorem}

\begin{proof}
See Appendix \ref{appendix:theorem}.
\end{proof}
% Similarly, we can also bound the gap between the real surrogate contrastive loss and the proxy contrastive loss,

% \begin{theorem}
% Let $ f(c, w) $ be an objective function that is $ L_f $-Lipschitz continuous with respect to $ c $ for a fixed decision variable $ w $, i.e.,
% \[
% |f(c, w) - f(c', w)| \leq L_f \| c - c' \| \quad \text{for all } c, c' .
% \]
% Let $ p(c|x) $ be the ground-truth distribution and $ q(c|x) $ be a surrogate distribution that approximates $ p(c|x) $. Define the contrastive loss function $ \ell_{NCE}(\theta) $ as

% \[  
% \ell_{NCE}(\theta) = \mathbb{E}_x \Big[ \sum_{w^s \in S} \left( \text{CVaR}_{p(c|x)} \left[ f\left( c, w^s) \right) \\ -  f\left( c, w^\star \right) \right] \right) \Big],
% \]
% where $w^\star = \arg\min_{w} \ \mathbb{E}_{c\sim p(c|x)}[f(c, w)]$ and $w^s \in S \subset \mathcal{W } \setminus w^\star$ are treated as the negative samples consisting of solutions that are different from the target solution $w^\star$. 

% And the surrogate contrastive loss $ \ell_{NCE}_q(\theta) $ with the proxy model $ q(c|x) $ is defined as
% \[  
% \ell_{NCE}_q(\theta) = \mathbb{E}_x \Big[ \sum_{w^s \in S} \left( \mathbb{E}_{q(c|x)} \left[ f\left( c, w^s) \right) \\ -  f\left( c, w^\star \right) \right] \right) \Big],
% \]
% % where $w^\star' = \arg\min_{w} \ \mathbb{E}_{c\sim q(c|x)}[f(c, w)]$

% Then, the gap between $ \ell_{NCE}(\theta) $ and $ \ell_{NCE}_q(\theta) $ is bounded by
% \[
% |\ell_{NCE}(\theta) - \ell_{NCE}_q(\theta)| \leq K_q \cdot \mathbb{E}_x \left[ W_1(p(c|x), q(c|x)) \right],
% \]
% where $ W_1(p(c|x), q(c|x)) $ is the Wasserstein-1 distance between $ p(c|x) $ and $ q(c|x) $ and $K_q$ is some constant.
% \end{theorem}

% \begin{proof}
% See Appendix \ref{appendix:theorem}.
% \end{proof}
The theorem above implies that the surrogate loss provides a valid approximation to the original loss function, provided the proxy model $ q(c|x) $ can estimate the ground-truth $ p(c|x) $ well. The bound is directly proportional to the $\mathcal{W}(p(c|x), q(c|x))$, which quantifies the discrepancy between these distributions. 
% This implies that improving the fidelity of $ q(c|x) $ in modeling $ p(c|x) $ reduces the gap between the original and surrogate losses.

% ================== Truncated to save space================
% For practical purposes, this bound validates the use of surrogate models in scenarios where the true distribution $ p(c|x) $ is unknown or intractable. By ensuring that the proxy $ q(c|x) $ is well-calibrated and learned effectively (e.g., through powerful generative models like CNFs), we can achieve robust performance in decision-focused learning frameworks.
% ================== Truncated to save space================
% \subsection{Gen-DFL vs Pred-DFL}

% We are now ready to characterize when \texttt{Gen-DFL} will be advantageous over Pred-DFL in various scenarios. We first introduce two definitions which will be used later in our analysis.
We now establish the conditions under which \texttt{Gen-DFL} outperforms Pred-DFL. To facilitate our analysis, we first introduce the following two definitions.
% ===================== we moved the following theorem to the appendix========================
% \begin{theorem}[CVaR Estimation Bound]
% \label{thm:highdim_conditional_CVaR}
% Let $(X,Y)$ be a random pair taking values in $\mathbb{R}^{d_x}\times \mathbb{R}^{d_y}$,
% and let $\alpha\in(0,1)$ be fixed.
% Suppose we have:
% \begin{itemize}
% \item A \emph{scalar loss} $\ell: \mathbb{R}\times \mathbb{R}^{d_y}\to \mathbb{R}$,
% \item A hypothesis class $\mathcal{G}$ of measurable functions $g:\mathbb{R}^{d_x}\to\mathbb{R}$,
% \end{itemize}
% and define the \emph{Rockafellar--Uryasev (RU) risk} of any predictor $g\in \mathcal{G}$ by
% \[
%   R(g) 
%   \;\coloneqq\;
%   \mathbb{E}\!\Bigl[
%     g(X) 
%     \;+\;
%     \frac{1}{\alpha}\,\Bigl(\,\ell\bigl(g(X),Y\bigr) - g(X)\Bigr)_{+}
%   \Bigr].
% \]
% Let $R^*=\inf_{g\in \mathcal{G}} R(g)$, and choose $g^*$ such that $R(g^*)=R^*$.
% Given $n$ i.i.d.\ samples $\{(x_i,y_i)\}_{i=1}^n\subset \mathbb{R}^{d_x}\times \mathbb{R}^{d_y}$,
% define the \emph{empirical} RU risk
% \[
%   \widehat{R}_n(g)
%   \;\coloneqq\;
%   \frac{1}{n}\,\sum_{i=1}^n
%     \Bigl[
%       g(x_i) 
%       \;+\;
%       \frac{1}{\alpha}\,\bigl(\ell(g(x_i),\,y_i) - g(x_i)\bigr)_{+}
%     \Bigr],
% \]
% and let $\hat{g}_n \in \arg\min_{g\in \mathcal{G}}\,\widehat{R}_n(g)$.
% Assume that with probability at least $1-\delta$, we have a uniform-convergence bound
% \[
%    \sup_{g\in \mathcal{G}}
%    \Bigl|\widehat{R}_n(g) - R(g)\Bigr|
%    \;\;\le\;\;
%    \varepsilon_n,
% \]
% where $\varepsilon_n$ scales as
% \[
%    \varepsilon_n 
%    \;=\; 
%    \widetilde{\mathcal{O}}
%    \!\Bigl(
%      \tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_y}{n}}
%    \Bigr),
% \]
% under suitable boundedness/sub-Gaussian assumptions on $(X,Y)$ and $\ell$. Then on that event,
% \[
%   R(\hat{g}_n) 
%   \;-\; 
%   R^*
%   \;\;\le\;\;
%   2\,\varepsilon_n.
% \]
% Hence the learned predictor $\hat{g}_n$ achieves a CVaR-type risk 
% within $2\,\varepsilon_n$ of the best $g^*\in \mathcal{G}$, with high probability.
% \end{theorem}


% The above theorem shows that CVaR estimation error scales as $\varepsilon_n \propto \frac{1}{\alpha} \sqrt{\frac{d_x + d_y}{n}}$, increasing with dimensionality ($d_x, d_y$) and decreasing with sample size ($n$). The inverse dependence on $\alpha$ highlights the challenge of estimating CVaR for small $\alpha$, as it focuses on tail regions with sparser data. This underscores the importance of generative models for capturing tail distributions effectively in high-dimensional and low-$\alpha$ settings.

% ===================== we moved the following theorem to the appendix========================

\begin{definition}
Let $p(c|x)$ denote the true conditional distribution of $c$, 
and let $p_\theta(c|x)$ be the generative model.
We define $Q_c$ to be the “worst $\alpha\%$ tail” representative for $c$ under $p(c|x)$ based on the target decision $w^\star$. Formally, 
% \[Q_c[\alpha] \coloneqq \mathbb{E}[\,c \mid c^\top w^\star \ge \mathrm{VaR}_\alpha(f(c, w^\star))\,].\]
\[
Q_c[\alpha] \coloneqq \mathbb{E}[c \mid f(c, w^\star) \ge \mathrm{VaR}_\alpha].
\]
\end{definition}

% \woody{if $w^\star_\text{gen}$ same as $w^\star_\theta$? If it is, please use $w^\star_\theta$ to ensure the consistency. I have changed some of them and please fix the rest of them. Also, I suggest to change $R_\text{gen}$ to $R_\theta$ to make it consistent with the previous definition.}

\begin{definition}
Given the target decision $w^\star$ and the decisions found by Pred-DFL ($w^\star_\text{pred}$) and \texttt{Gen-DFL} ($w^\star_\theta$), we can define the regret of Pred-DFL as:
\[
  R_{\mathrm{pred}}(x;\alpha)
  =
  f(Q_c[\alpha], w_{\mathrm{pred}}^\star) - f(Q_c[\alpha], w^\star),
\]
and the regret of \texttt{Gen-DFL} is the same as before:
\[
  R_{\theta}(x;\alpha)
  \;=\;
  \mathrm{CVaR}_{p(c \mid x)}
  \Bigl[
    f\bigl(c, w^\star_\theta\bigr) - f\bigl(c, w^\star\bigr)
    ;\alpha
  \Bigr].
\]
\end{definition}

Next, we develop a regret bound that quantifies the performance gap between \texttt{Gen-DFL} and Pred-DFL, incorporating data variance and the complexity of the optimization problem, such as the dimensionality of the parameter space and the risk-sensitive level.
% capturing the performance gap between \texttt{Gen-DFL} and Pred-DFL w.r.t.~the variance of the data and the complexity of the optimization problem (the dimensionality of the parameter space, risk-sensitive level, etc).

\begin{theorem}
\label{theorem:cvar_extension}
% Let
% % $c, x\in \mathcal{X}$, where 
% % $\mathcal{C} \subset\mathcal{R}^{d_c}, \mathcal{X}\subset \mathcal{R}^{d_x}$ be the feasible regions of the parameter space and the input space respectively.
% $d_c$ and $d_x$ denote the dimensions of $\mathcal{C}$ and $\mathcal{X}$, respectively.
Let $g:\mathcal{X} \rightarrow \mathcal{C}$ be the predictor in Pred-DFL. Assume the objective function $f(c,w)$ is Lipschitz continuous for any $c , w$. 
There exists some constants $L_w, L_c, \kappa_1, \kappa_2, \kappa_3$ such that the following upper-bound holds for the aggregated regret gap $\mathbb{E}_x|\Delta R(x)|$:
\begin{align*}
\mathbb{E}_x|\Delta R(x)| &\le \mathbb{E}_x \Bigl[ \frac{2L_w}{\alpha}
\;\bigl[
  \kappa_1\;\mathcal{W}(p_\theta, p)
  +
  \kappa_2\,\|\mathrm{Bias}[g]\|\bigr]
\\ 
&+ (\frac{2L_w}{\alpha}\kappa_3 +2L_c) \sqrt{\|\mathrm{Var}[c\mid x]\|}\\
&+ \mathrm{CVaR}_{p(c|x)}[\|\mathrm{Bias}[g(x)]\|;\alpha] \bigr|\Bigr].
\end{align*}
% \woody{$\mathcal{W}$ is undefined.} 
\begin{remark}
Let
% $c, x\in \mathcal{X}$, where 
% $\mathcal{C} \subset\mathcal{R}^{d_c}, \mathcal{X}\subset \mathcal{R}^{d_x}$ be the feasible regions of the parameter space and the input space respectively.
$d_c$ and $d_x$ denote the dimension of $\mathcal{C}$ and $\mathcal{X}$, respectively.
The bias term $||\text{Bias}[g]||$ of the predictor grows at a rate of $\mathcal{O}( \sqrt{(d_x+d_c)/n} / \alpha)$.
% , where $d_x+d_c$ is the dimensionality of the problem. 
This suggests that the smaller the $\alpha$ is, the harder for the predictor in the Pred-DFL to get an accurate estimation of $Q_c[\alpha]$.
\end{remark}

\begin{remark}
We may write $c = \bar{c} + \sigma\epsilon$, where $\bar{c} = \mathbb{E}_{p(c|x)}[c]$. Under some mild assumptions such as $\epsilon$ being Gaussian, the variance term is of the order $\mathcal{O}(\sigma^2 \,\sqrt{d_c})$.
% \item Generally, the Wasserstein distance scales as $\mathcal{O}(\sqrt{\frac{d_c}{n}})$
\end{remark}
\end{theorem}
\begin{proof}
    See Appendix~\ref{theorem:ultimate_nonlinear}.
\end{proof}

% particularly when the objective function $f(c, w)$ is nonlinear, as reflected in the additional $\|\mathrm{Var}[c \mid x]^{3/2}\|$ term. 
The above results reveal how the following three factors affect the performance gap between \texttt{Gen-DFL} and Pred-DFL: 
($i$) Variance of the parameter space $\|\mathrm{Var}[c|x]\|$: Higher variance in $c$ conditioned on $x$ increases uncertainty and amplifies the difficulty of accurately approximating the objective. Pred-DFL, which relies on point estimates from $g(x)$, struggles in high-variance settings.
In contrast, \texttt{Gen-DFL} benefits from modeling the full distribution $p(c | x)$, capturing the variability and structure needed for robust decision-making under uncertainty;
($ii$) Dimensionality of the parameter space, including $d_c$ and $d_x$: As the dimensionality increases, the estimation error of the predictor in Pred-DFL grows at a rate of $\mathcal{O}(\sqrt{(d_x + d_c)/n} / \alpha)$, making it increasingly difficult to obtain reliable point estimates; 
($iii$) Risk level $\alpha$: The inverse dependence of the estimation error on $\alpha$ implies that smaller values of $\alpha$ make quantile regression more challenging for Pred-DFL, as data in the tail regions of the worst $\alpha\%$ outcomes become increasingly sparse. This leads to a larger bias in $g(x)$ for smaller $\alpha$. In contrast, \texttt{Gen-DFL} leverages a generative model to capture the full conditional distribution $p(c|x)$.
Together, these insights demonstrate that \texttt{Gen-DFL} offers significant advantages over Pred-DFL in complex, high-dimensional, and risk-sensitive scenarios.

% \FloatBarrier
% \afterpage{
% \begin{table*}[ht]
% \caption{Comparison of regret on three problem sets. We present the average percentage regret (\textcolor{red}{$\downarrow\%$}) of Gen-DFL and different Pred-DFL models. \woody{Add standard error (SE). With SE, let the table occupy the full width of the two-column layout. Add four merged cells in a separate column for Portforlio, Knapsack, Shortest Path, and Energy. They are wasting space vertically but you have extra space to place them horizontally.}}
% \label{table:1}
% \centering
% \scalebox{0.85}{
% \begin{tabular}{l|cccccc|c}
% \hline
% & Pairwise & Listwise & NCE & MAP & SPO+ & MSE (PTO) & \textbf{Gen-DFL} \\
% \hline
% \multicolumn{1}{c}{Portfolio}\\
% \hline
% Deg-2 & 11.48 & 22.87 & 8.57 & 8.88  & 6.92  & 16.90 & \textbf{3.71} \\
% Deg-4 & 11.16 & 20.70 & 7.81 & 8.43 & 7.23 & 14.89 & \textbf{3.81} \\
% Deg-6 & 11.54 & 18.57 & 8.69 & 8.51 & 7.01 & 16.02 & \textbf{4.31} \\
% Deg-8 & 10.44 & 21.92  & 7.93  & 8.9  & 6.98 & 16.17 & \textbf{3.59} \\
% \hline
% \multicolumn{1}{c}{Knapsack} \\
% \hline
% Deg-2 & 34.93$\pm$(9.37) & 27.03$\pm$(8.43) & 24.75$\pm$(7.87) & 35.54$\pm$(4.70) & 21.90$\pm$(7.46) & 20.27$\pm$(9.46) & \textbf{17.60$\pm$(3.38)} \\
% Deg-4 & 38.32$\pm$(4.44) & 26.37$\pm$(3.03) & 23.43$\pm$(4.94) & 46.87$\pm$(14.43) &  20.37$\pm$(5.18) & 16.58$\pm$(3.68) & \textbf{15.21$\pm$(3.75)} \\
% Deg-6 & 33.85$\pm$(8.24) & 24.50$\pm$(1.19) & 20.07$\pm$(10.76) & 40.33$\pm$(5.63) & \textbf{17.45$\pm$(7.2)} & 21.66$\pm$(6.46) & 17.91$\pm$(2.44) \\
% Deg-8 & 33.25$\pm$(6.48) & 20.38$\pm$(6.70) & 22.36$\pm$(7.89) & 34.07$\pm$(6.66) & 22.90$\pm$(11.48) & 21.13$\pm$(7.40) & \textbf{19.29$\pm$(3.75)} \\
% \hline
% \multicolumn{1}{c}{Shortest Path} \\
% \hline
% Deg-2 & 10.74  & 10.41  & 30.84  & 14.15  & 10.06  & 10.07  & \textbf{1.87} \\
% Deg-4 & 8.06  & 7.71  & 65.10  & 9.44  & 7.98   & 10.44  & \textbf{3.64} \\
% Deg-6 & 9.27  & 8.29  & 129.42  & 9.45  & 16.19   & 15.96  & \textbf{6.52} \\
% Deg-8 & 12.75  & \textbf{12.38}  & 242.30  & 13.68  & 33.75   & 28.50  & 13.36 \\
% \hline
% \multicolumn{1}{c}{Energy} \\
% \hline
% Energy & 1.65 & 1.67 & 1.69 & 1.59 & 1.56 & 1.91 & \textbf{1.21 } \\
% % \hline
% % \multicolumn{1}{c}{Pandemic} \\
% % \hline
% %  &  &  &  &  &  &  & \textbf{} \\
% \hline
% \end{tabular}
% }
% \end{table*}
% }

\FloatBarrier
\begin{table*}[ht]
\caption{Comparison of Decision Quality Across Tasks in High-Variance Settings ($\sigma=20$). We report the average percentage regret (\textcolor{red}{$\downarrow$}, lower is better) for \texttt{Gen-DFL} and various Pred-DFL models across different optimization tasks. Results are averaged over 10 repeated experiments, with standard error (SE) provided for all tested tasks.}
\label{table:1}
\centering
\resizebox{\textwidth}{!}{ % This ensures the table fits within the 
\begin{tabular}{ll|ccccccc|c}
\toprule
\textbf{Task}  & \textbf{} & {Pairwise} & {Listwise} & {NCE} & {MAP} & {SPO+} & Diff-DRO & {2Stage (PTO)} & \textbf{Gen-DFL} \\
\midrule
\multirow{4}{*}{\textbf{Portfolio}} 
& Deg-2 & 11.48$\pm$(0.50) & 22.87$\pm$(1.11) & 8.57$\pm$(0.48) & 8.88$\pm$(0.34)  & 6.92$\pm$(0.26)  & 8.30$\pm$(0.36)  & 16.90$\pm$(0.55) & \textbf{3.71$\pm$(0.18)} \\
& Deg-4 & 11.16$\pm$(0.32) & 20.70$\pm$(1.19) & 7.81$\pm$(0.52) & 8.43$\pm$(0.65) & 7.23$\pm$(0.60) & 7.41$\pm$(0.67) & 14.89$\pm$(0.63) & \textbf{3.81$\pm$(0.22)} \\
& Deg-6 & 11.54$\pm$(0.78) & 18.57$\pm$(0.87) & 8.69$\pm$(0.61) & 8.51$\pm$(0.38) & 7.01$\pm$(0.26) & 8.56$\pm$(0.71) & 16.02$\pm$(0.78) & \textbf{4.31$\pm$(0.32)} \\
& Deg-8 & 10.44$\pm$(0.36) & 21.92$\pm$(0.95)  & 7.93$\pm$(0.40)  & 8.90$\pm$(0.48)  & 6.98$\pm$(0.98) & 8.65$\pm$(0.52) & 16.17$\pm$(0.60) & \textbf{3.59$\pm$(0.31)} \\
\midrule
\multirow{4}{*}{\textbf{Knapsack}} 
& Deg-2 & 34.93$\pm$(9.37) & 27.03$\pm$(8.43) & 24.75$\pm$(7.87) & 35.54$\pm$(4.70) & 21.90$\pm$(7.46) & 19.63$\pm$(4.5) & 20.27$\pm$(9.46) & \textbf{17.60$\pm$(3.38)} \\
& Deg-4 & 38.32$\pm$(4.44) & 26.37$\pm$(3.03) & 23.43$\pm$(4.94) & 46.87$\pm$(14.43) & 20.37$\pm$(5.18) & 18.45$\pm$(3.81) & 16.58$\pm$(3.68) & \textbf{15.21$\pm$(3.75)} \\
& Deg-6 & 33.85$\pm$(8.24) & 24.50$\pm$(1.19) & 20.07$\pm$(10.76) & 40.33$\pm$(5.63) & \textbf{17.45$\pm$(7.2)} & 17.51$\pm$(5.20) & 21.66$\pm$(6.46) & 17.91$\pm$(2.44) \\
& Deg-8 & 33.25$\pm$(6.48) & 20.38$\pm$(6.70) & 22.36$\pm$(7.89) & 34.07$\pm$(6.66) & 22.90$\pm$(11.48) & 21.48$\pm$(6.28) & 21.13$\pm$(7.40) & \textbf{19.29$\pm$(3.75)} \\
\midrule
\multirow{4}{*}{\textbf{Shortest Path}} 
& Deg-2 & 8.30$\pm$(2.35)  & 2.65$\pm$(0.25)  & 9.59$\pm$(0.75)  & 12.92$\pm$(3.63)  & 3.23$\pm$(0.72)  & 2.91$\pm$(0.93)  & 10.07$\pm$(1.2)  & \textbf{1.87$\pm$(0.20)} \\
& Deg-4 & 18.91$\pm$(5.30)  & 12.19$\pm$(1.04)  & 42.87$\pm$(2.57)  & 52.47$\pm$(6.49)  & 28.73$\pm$(11.23)   & 11.78$\pm$(2.89)  & 22.44$\pm$(2.84)  & \textbf{3.64$\pm$(0.43)} \\
& Deg-6 & 29.63$\pm$(7.20)  & 33.15$\pm$(4.60)  & 68.94$\pm$(6.79)  & 94.46$\pm$(10.91)  & 26.46$\pm$(9.31)   & 23.76$\pm$(4.21)  & 38.64$\pm$(2.3)  & \textbf{6.52$\pm$(0.71)} \\
& Deg-8 & 63.61$\pm$(18.82)  & 51.65$\pm$(13.77)  & 139.09$\pm$(22.08)  & 173.17$\pm$(36.28)  & 81.78$\pm$(21.82)   & 39.81$\pm$(5.46)  & 45.75$\pm$(5.10)  & \textbf{13.36$\pm$(2.59)} \\
\midrule
\textbf{Energy} 
& & 1.65$\pm$(0.23) & 1.67$\pm$(0.17) & 1.69$\pm$(0.13) & 1.59$\pm$(0.11) & 1.56$\pm$(0.11) & 1.49$\pm$(0.12) & 1.91$\pm$(0.22) & \textbf{1.09$\pm$(0.09) } \\
\bottomrule
\end{tabular}
 }
% \vspace{-.15in}
\end{table*}


% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{0.33\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/regret_noise.pdf}
%         \caption{Regrets vs Noise.}
%         \label{fig:portfolio_noise}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/regret_d_violin_portfolio.png}
%         \caption{Regrets vs Dimension.}
%         \label{fig:portfolio_dimension}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/regret_n_violin_portfolio.png}
%         \caption{Regrets vs Sample size.}
%         \label{fig:portfolio_sample}
%     \end{subfigure}
%     \caption{Gen-DFL vs Pred-DFL vs a Standard ML model (two-stage) under various settings. We used the average of relative regrets as the metric ($\textcolor{red}{\downarrow}$). \woody{Use box-plot, make it shorter (or thinning to fit in one column) so that they will take less space}}
%     \label{fig:gendfl-preddfl}
% \end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/regret_comparison.pdf}
    \vspace{-.15in}
    \caption{
    % Gen-DFL vs Pred-DFL vs traditional predi (two-stage) model under various settings. We used the average of relative regrets as the metric ($\textcolor{red}{\downarrow}$). 
    Comparison of decision quality in the portfolio task under different settings.
We present box plots of the percentage regret (\textcolor{red}{$\downarrow$} , lower is better), generated from 10 repeated experiments.
    % Box plots are generated using 10 repeated experiments.
    % \woody{Use a more striking color (e.g., red) to highlight our method and other colors (e,g. blue-ish, orange-ish, or green-ish) to represent other methods.}
    % \woody{Remove dots (No information). Consider adding vertical dash lines between groups for better visual comparison. Move legend inside the plot. Algin the y axis across three plots and remove the ticks in the second and third plot, which will save a lot of space and make all three plots a bit wider. } 
   }
    \label{fig:gendfl-preddfl}
    \vspace{-.15in}
\end{figure}



% \section{Evaluation}

% \[
% \text{average objective} = \frac{1}{N}\sum_i^N \min_{w} \mathbb{E}_{\hat{c}\sim p_\theta(c|x_i)}[f(w, \hat{c})]
% \]

% When 

% \subsection{Infeasibility Metric}

% Apart from the average regret, we also need to evaluate whether the worst-case scenarios generated by our model are realistic or over-conservative. In this section, we introduce the \textit{Infeasibility metric}.

% For each sample $\hat{c}$ generated by $p_\theta(c|x)$, we can define the level of Infeasibility as,

% \[
% I(\hat{c}, x) = \frac{f(w^\star(\hat{c}), c)}{p(\hat{c}|x)}
% \]

% where the numerator quantifies the objective value led by the solution $w^\star(\hat{c})$ given the current candidate $\hat{c}$, and the denominator quantifies the likelihood of the given candidate $\hat{c}$. Intuitively, if $\hat{c}$ is one of the worst cases generated by $p_\theta(c|x)$ which has a small likelihood of occurring, then $I(\hat{c}, x)$ will be large, indicating that the given worst-case scenario $\hat{c}$ is too-conservative and therefore infeasible. 

% To ensure that our metric has the same magnitude across different set of problems whose objective functions are very different, we propose to use the normalized or relative objective function value,

% \[
% f_{\textit{norm}}(w^\star(\hat{c}), c) = \frac{f(w^\star(\hat{c}), c) - \mu_f}{\sigma_f}
% \]

% Then, given a learned model $m_\theta = p_\theta(c|x)$, we can define the Infeasibility of the model as,

% \[
% I_\alpha(M_\theta) = \mathbb{E}_{(x, c) \sim D}\mathbb{E}_{\hat{c} \sim \widehat{\mathcal{U}}_\alpha} [\frac{f(w^\star(\hat{c}), c)}{p(\hat{c}|x)}]
% \]

% where $\widehat{\mathcal{U}}_\alpha$ is an uncertainty set defined by the $1-\alpha$ quantile of the generated outcomes of the model $M_\theta$.

% Instead of averaging over all the generated scenarios, we can also define the Infeasibility based on the worst-case or the set of worst-cases given a certain confidence level (CVaR), that is,

% \[
% I_\alpha(M_\theta)_w = \mathbb{E}_{(x, c) \sim D}\max_{\hat{c} \sim \widehat{\mathcal{U}}_\alpha} [\frac{f(w^\star(\hat{c}), c)}{p(\hat{c}|x)}]
% \]

% \[
% I_\alpha(M_\theta)_{CVaR} = \mathbb{E}_{(x, c) \sim D} [\frac{\text{CVaR}_\beta(f(C, w))}{p(\hat{c}|x)}]
% \]

% where 

% \[
% \text{CVaR}_\beta(f(C, w)) = \mathbb{E}_{\hat{c}\sim M_\theta}[f(w^\star(\hat{c}), c) | f \ge \text{VaR}_\beta]
% \]




% \section{Experiments}

% Next, we demonstrate the use cases and the performance of our model on synthetic and real-world examples. 

% \begin{table*}[t]
%     \centering
%     \caption{Summary of Average Objectives for Different Models and Problems, $m=20$}
%     \begin{tabular}{lcccc}
%         \toprule
%         \textbf{Model} & \textbf{Knapsack} \uparrow & \textbf{Portfolio} \uparrow & \textbf{Shortest Path} \downarrow & \textbf{Traveling Salesman} \downarrow  \\
%         \midrule
%         Gen-DFL & 1.10 \pm 0.07 & - & - & - \\
%         Pred-DFL        & 0.18 \pm 0.05  & 3.0 \pm 1.05 & - & - \\
%         Pred-ML &  & - & - & - \\
%         Gen-ML & - & - & - & - \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:summary_results}
% \end{table*}

\section{Experiments}

% \paragraph{Synthetic Experiments}
% \woody{A short paragraph summarizing the experiment section}
In this section, we assess the performance of \texttt{Gen-DFL} and compare it with seven other baseline methods under four settings. The results show that the proposed \texttt{Gen-DFL} outperforms its competitors in most scenarios.

\subsection{Experimental Setup}

We evaluate the proposed framework using three synthetic optimization problems: Portfolio Management, Fractional Knapsack, and Shortest-Path, as well as a real data~\cite{ifrim2012properties} set in Energy Management Problem~\cite{simonis1999csplib}.

For the synthetic experiments, we adopt some of the settings from \cite{elmachtoub2022smart}. For example, in the Portfolio experiment, the feature vector $ x_i \in \mathbb{R}^{d_x} $ follows a standard multivariate Gaussian distribution $ \mathcal{N}(0, I) $, and the optimization parameters (price vector) $ c_i \in \mathbb{R}^{d_c} $ are generated from the following polynomial function 
\[
\bar{c}_{ij} = \left( \frac{0.05}{\sqrt{p}} \mathbf{B} x_i + 0.1 \right)^{deg} + Lf + 0.01\sigma\epsilon,
\]
where $ \epsilon \sim \mathcal{N}(0, I) $, $\mathbf{B}, L$ are random matrices, $f\sim\mathcal{N}(0, I)$, and the polynomial degree reflects the level of non-linearity between the feature and the price vector. In Portfolio, $c$ represents the asset prices and the dimension of $c_i$ is the number of assets. The non-linear, risk-sensitive optimization problem in Portfolio Management is then formulated as,
\begin{equation}
\begin{aligned}
w^\star(x;\alpha) &\coloneqq \arg \min_{w} \text{CVaR}_{p(c|x)} [-c^T w + w^T \Sigma w;\alpha] \\ 
\text{s.t.} \ & w \in [0,1]^n, \ \mathbf{1}^T w \leq 1,
\end{aligned}    
\end{equation}
where $\Sigma=LL^T + (0.01\sigma)^2 I$ is the covariance among the asset prices $c$, and the quadratic term $w^T \Sigma w$ reflects the amount of risk.
The configurations of our synthetic experiments include the training size, feature dimension $ d_x $, polynomial degree, and the noise scale $ \sigma $ that reflects the amount of variance in the parameter space and the non-linearity of the above stochastic optimization, since, by our construction, $\sigma$ would affect the magnitude of the quadratic term $w^T \Sigma w$. The problem setup and model configurations for the Fractional Knapsack and Shortest-Path problem are similar to that of Portfolio. Full details of this data synthesis process and the problem setups are provided in Appendix \ref{app:experiments}.

% Given feature vectors that are sampled from a multivariate Gaussian distribution, the optimization parameters are generated as,
% \[
% \bar{c}_{ij} = \left( \frac{0.05}{\sqrt{p}} \mathbf{B} x_i + 0.1 \right)^{deg}\epsilon_i^j,
% \]
% where $ \mathbf{B} $ is a random matrix. $\bar{c}_{ij}$ is the $j$-th component of the vector $c_i$, and the covariance matrix is expressed as:

% \[
% \text{Covariance} = \mathbf{L} \mathbf{L}^T + (0.01 \tau)^2 \mathbf{I},
% \]
% with $ \mathbf{L} $, $ \mathbf{B} $, and $ \tau $ being random variables.

% \paragraph{Portfolio Management} 
% The portfolio optimization problem maximizes expected returns subject to uncertainty in asset returns. The objective is formulated as:
% \[
% w^\star(x) \coloneqq \min_{w} \mathbb{E}_c [-c^T w],
% \]
% subject to $ w \in [0,1]^n $, $ \mathbf{1}^T w \leq 1 $, and $ P_{X,C}(C \in U(X)) \geq 1 - \alpha $. Here, $ \bar{r}_{ij} $, the return vector, depends on the features $ x_i $ sampled from a Gaussian distribution and is influenced by noise and polynomial degree. The covariance matrix includes random components, adding complexity to the optimization process.

% \paragraph{Fractional Knapsack} 
% In the fractional knapsack problem, the goal is to minimize costs while satisfying capacity constraints under uncertain costs. The problem is expressed as:
% \[
% w^\star(x) \coloneqq \min_{w} \mathbb{E}_c [-c^T w],
% \]
% subject to $ w \in [0,1]^n $, $ p^T w \leq B $, and $ P_{X,C}(C \in U(X)) \geq 1 - \alpha $. Feature vectors $ x_i $ are Gaussian, and cost coefficients $ c_i $ are generated using polynomial functions with additive noise. Parameters include data size $ n $, feature dimension $ p $, polynomial degree, and noise width $ \overline{\epsilon} $.

% \paragraph{Shortest-Path} 
% The shortest-path problem minimizes path costs in a graph with uncertain edge weights. The objective is:
% \[
% w^\star(x) \coloneqq \min_{w} \mathbb{E}_c [c^T w],
% \]
% subject to $ w \in [0,1]^n $ and $ P_{X,C}(C \in U(X)) \geq 1 - \alpha $. Costs $ c_i^j $ depend on Gaussian features $ x_i $, random matrices $ \mathbf{B} $, and noise terms $ \epsilon_i^j $. The uncertainty affects only the objective function, while weights remain fixed. Key parameters include resource dimensions, item counts, and noise levels.


% \paragraph{Real Datasets: Energy-Cost Aware Scheduling}

% \prince{Consider making it more compact and leave the mathematical formulation to Appendix.}
For the real Energy-cost Aware Scheduling experiment, we consider a demand response program in which an operator schedules electricity consumption $ p_t $ over a time horizon $ t \in \Omega_t $. The objective is to minimize the total cost of electricity while adhering to operational constraints. The electricity price for each time step is denoted by $ \pi_t $, which is not known in advance. However, the operator can schedule the electricity consumption $ p_t $ within a specified lower bound $ P_t $ and upper bound $ \overline{P}_t $. Additionally, the total consumption for the day, denoted as $ P_t^{\text{sch}} $, must remain constant. This assumes flexibility in shifting electricity demand across time steps, provided the total demand is met. 
% The optimization problem is then formulated as:
% \begin{equation}
% \begin{aligned}
% \min_{p_t} \text{CVaR}_{\pi} \Big[\sum_{t \in \Omega_t} \pi_t p_t;\alpha \Big], \\
% \end{aligned}    
% \end{equation}
The details of this experiment can be found in Appendix \ref{app:energy}.

% The optimization problem, assuming perfect information about prices $ \pi_t $, can be formulated as:
% \[
% \min_{p_t} \mathbb{E}_{\pi} \sum_{t \in \Omega_t} \pi_t p_t,
% \]
% subject to the constraints:
% \[
% P_t \leq p_t \leq \overline{P}_t, \quad \forall t,
% \]
% \[
% \sum_{t \in \Omega_t} p_t = \sum_{t \in \Omega_t} P_t^{\text{sch}}.
% \]

% Here, $ P_t \leq p_t \leq \overline{P}_t $ ensures the consumption at each time step is within the allowed bounds, while the equality constraint guarantees that the total electricity consumption remains fixed across the time horizon.

% This setup reflects the practical challenges of demand-side electricity management, where prices are uncertain, and demand shifting across time steps provides opportunities for cost reduction while maintaining overall consumption levels. The problem serves as a testbed for evaluating optimization approaches under uncertain electricity prices and operational constraints.

% \paragraph{Real Datasets: Power Outage}
% \ryan{...}

% % GENDFL === Evaluating on Test Set ===
% % Evaluation metrics:
% % alpha=1.0 = {'obj': 0.0, 'reg': 22255.961588541668}
% % alpha=0.5 = {'obj': 14704.1015625, 'reg': 7551.860677083333}
% % alpha=0.1 = {'obj': 7551.860026041667, 'reg': 14704.102213541666}
% % Done.


\paragraph{Model Configuration.}

The hyperparameters in our learning algorithm include the decision cost weight $\beta$ and the negative log-likelihood weight $\gamma$ in \ref{eq:gendfl-loss}, which serves as regularization. We introduce an additional hyperparameter $\beta$ in our experiment to study how different magnitude of DFL loss will affect the model's performance. We set $\gamma = 1$ across all experiments and study the effect of different $\beta$ values on \texttt{Gen-DFL}’s performance (Figure~\ref{fig:cvar_beta}). When $\beta=0$, the loss reduces to that of a standard generative model, only fitting data without considering decision costs, which results in the worst regret in all risk-sensitive settings. Increasing $\beta$ improves downstream decision quality across all risk levels. Full hyperparameter details are provided in Appendix \ref{app:hyper}.

\paragraph{Baseline Methods.}
% \woody{discuss pred-dfl+?}
% \ryan{possibaly in end of 3.a?}
% \ryan{refer to figure 2}
We evaluate the performance of \texttt{Gen-DFL} against various state-of-the-art Pred-DFL baselines across all tasks. Specifically, we compare against Smart-Predict-Then-Optimize (SPO+)~\cite{elmachtoub2022smart}, contrastive loss-based Pred-DFL models (NCE, MAP)~\cite{mulamba2020contrastive}, ranking-based Pred-DFL models~\cite{mandi2022decision}, and the recently proposed Pred-DFL approach with differentiable Distributionally Robust Optimization layers, which we refer to as Diff-DRO~\cite{ma2024differentiable}. These baselines represent a range of decision-focused learning strategies, differing in their loss formulations and optimization objectives. The main results of our comparison are summarized in Table~\ref{table:1}.

\paragraph{Evaluation Metric}
We evaluate the decision quality of different models on various tasks in terms of the average relative regret, 
\begin{equation}
\mathbb{E}_x\Big[\frac{\text{CVaR}_{p(c|x)}[f(c,\hat{w}^\star) - f(c, w^\star);\alpha]}{\mathbb{E}_{p(c|x)}[f(c, w^\star)]}\Big]\times100\%.
\label{eq:eval_risk}
\end{equation}
where lower $\alpha$ indicates greater risk sensitivity.
% \subsection{Approximating the Regret}
% \woody{This is part of the evaluation}
% \woody{This can be introduced later in the experiment section. Not here. }
% \prince{Moved from the method section to here in the experimental section.}
For our real data experiment, we will first train a proxy model $q(c|x)$ given the data, which will then be used to evaluate the average relative regret during evaluation.
% In our synthetic experiments, we assume that the ground-truth $p(c|x)$ is given, which can then be used to evaluate the regret. However, in many real-world scenarios, the ground-truth $p(c|x)$ is often not known. In these cases, we utilize an additional model $q(c|x)$ to first estimate the ground-truth $p(c|x)$ given training data. Then, $q(c|x)$ is fixed and will be used as a proxy model to evaluate the approximated regret $\text{Regret}_{\alpha, q}(x, \theta)$ and the corresponding surrogate loss function $\ell(\theta;\alpha, q)$. 
% We employ two separate CNF models. The first model $\hat{p}(c|x)$ is trained on the $ \{(x_i, c_i)\}_i^N $ dataset to approximate the conditional distribution $ p(c | x) $. Once the model is well-trained to approximate $p(c|x)$, it can be used to approximate the original regret as the following,
% \[
% \widehat{\text{Regret}} =\mathbb{E}_{c\sim \hat{p}(c|x)} \Big[ \left|f\left( c, \hat{w}^\star \right)  -   f\left( c, w^\star \right) \right| \Big],
% \]
% We can therefore use the following approximation of the original loss function,
% \begin{align*}
% \mathcal{\hat{L}}(\theta;\alpha) & = \mathbb{E}_x\Big[  \text{CVaR}_{c\sim \hat{p}(c|x)}\left[\left|f(c, \hat{w}^\star) - f(c, w^\star) \right|;\alpha\right] \Big]\\
% &+ \beta \cdot \ell_{gen}(\theta)
% \end{align*}
% In section \ref{analysis}, we will present a theoretical bound on the gap between the original loss $\ell(\theta;\alpha, p)$ and the surrogate loss $\ell(\theta;\alpha, q)$ with the proxy model, which demonstrates that the above approximation is sound and reliable provided that $q(c|x)$ can approximate $p(c|x)$ well.

\subsection{Results}
% \ryan{I think we should shorten this paragraph. Some interpretation are intuitive and obvious}
% \ryan{And discussion of table 1 is missing}
% \prince{In progress...}

% \paragraph{Regret vs Variance, Data Dimension, and Training Size}



% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/combined_deg4_deg6_portfolio_low_alpha.pdf}
%         \caption{Regret in more risk-sensitive region.}
%         \label{fig:portfolio-cvar-low}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/combined_deg4_deg6_portfolio_gen_samples.pdf}
%         \caption{Regret in more risk-sensitive region.}
%         \label{fig:cvar_gen_samples}
%     \end{subfigure}
%     \caption{
%     \prince{Need to make these taller}
%     % \woody{Fontsize is too small.} \woody{Make them taller so that the figure content and their comparison are more visible.} \woody{Remove yaxis, tick numbers, and legend labels in the second panel to make more space for the figure contents. } \prince{-Addressed but may need further feedback (consider making the figures smaller and condensed)}
%     \woody{what are the differences between figure 4 and 5?}
%     \prince{The variables in these three rows of panels are: the weight $\beta$ of the DFL loss, the risk level $\alpha$ in the CVaR loss function, and the number of samples we generate for SAA. Perhaps we can make them into three consecutive rows.}}
%     \label{fig:portfolio-cvar}
% \end{figure}



% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figures/combined_deg4_deg6_portfolio.pdf}
%     \caption{\prince{Pending: deciding whether to include these}\woody{Fontsize is too small.} \woody{Make them taller so that the figure content and their comparison are more visible.} \woody{Remove yaxis, tick numbers, and legend labels in the second panel to make more space for the figure contents. }}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tradeoffs/tradeoff-10.png}
%         \caption{CVaR $10\%$ Regret}
%         \label{fig:portfolio_deg4}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tradeoffs/tradeoff-20.png}
%         \caption{CVaR $20\%$ Regret}
%         \label{fig:portfolio_deg4}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tradeoffs/tradeoff-30.png}
%         \caption{CVaR $30\%$ Regret}
%         \label{fig:portfolio_deg4}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tradeoffs/tradeoff-40.png}
%         \caption{CVaR $40\%$ Regret}
%         \label{fig:portfolio_deg4}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tradeoffs/tradeoff-50.png}
%         \caption{CVaR $50\%$ Regret}
%         \label{fig:portfolio_deg4}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tradeoffs/tradeoff-100.png}
%         \caption{Average Regret ($\alpha=100\%$)}
%         \label{fig:portfolio_deg4}
%     \end{subfigure}
%     \hfill
%     \caption{\text{NLL} vs Regret tradeoff. \prince{In progress... plotting more fine-grained figures}}
%     \label{fig:tradeoff}
% \end{figure*}
% \ryan{will revise}
% Table~\ref{table:1} summarizes the performance of Gen-DFL compared to Pred-DFL baselines across different problem settings. Gen-DFL consistently achieves the lowest regret, particularly excelling in portfolio and shortest-path problems, where it outperforms most baselines by significant margins. These results highlight the adaptability of Gen-DFL to handle complex non-linear relationships between the covariates and the parameter space.

% Table~\ref{table:1} summarizes the performance of Gen-DFL compared to Pred-DFL and two-stage method across various problem settings. Notice how Gen-DFL consistently achieves lower regret, particularly excelling in portfolio and shortest-path problems. This is particularly relevant as these results highlight \texttt{Gen-DFL}'s ability to model complex non-linear relationships between covariates and the parameter space.
% \woody{Need a sentence to summarize this section}
This section presents a comprehensive evaluation of Gen-DFL, demonstrating its advantages over baseline methods across various decision-making tasks, particularly in high-dimensional and risk-sensitive settings.

Table~\ref{table:1} presents the comparative performance of \texttt{Gen-DFL}, Pred-DFL, and the two-stage method across different problem settings. \texttt{Gen-DFL} consistently outperforms baseline methods, reducing regret by up to 58.5\% compared to Diff-DRO and up to 48.5\% compared to SPO+ in Portfolio tasks.
\texttt{Gen-DFL}'s advantage is particularly pronounced in high-dimensional tasks like Shortest-Path (Deg-8), where it achieves a remarkable 83.7\% reduction in regret over SPO+ (13.36 vs.~81.78). This demonstrates \texttt{Gen-DFL}'s ability to overcome the curse of dimensionality by effectively capturing the distributional structure of $p(c|x)$ rather than relying on point estimates.
Conversely, in Knapsack (Deg-2), \texttt{Gen-DFL}'s improvements over SPO+ and Diff-DRO are more moderate (19.6\% and 10.3\% respectively), suggesting that the benefits of generative modeling are especially significant in problems where uncertainty is highly non-linear or where high-dimensional interactions dominate the optimization landscape.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/cvar_beta_deg4deg6.pdf}
    \vspace{-.15in}
    \caption{Decision quality against different risk-sensitive regions vs various hyperparameters $\beta$. }
    \label{fig:cvar_beta}
    \vspace{-.15in}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/combined_deg4_deg6_portfolio_low_alpha.pdf}
    \vspace{-.15in}
    \caption{Decision quality evaluated w.r.t the risk levels for models trained by different $\alpha$. }
    \label{fig:portfolio-cvar-low}
    \vspace{-.15in}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/combined_deg4_deg6_portfolio_gen_samples.pdf}
    \vspace{-.15in}
    \caption{The impact of the number of generated samples in the optimization step on the decision quality evaluated w.r.t different risk levels.}
    \label{fig:cvar_gen_samples}
    \vspace{-.15in}
\end{figure}

Figure~\ref{fig:gendfl-preddfl} illustrates the impact of variance $\text{Var}[c| x]$, problem dimensionality, and training size on model performance. \texttt{Gen-DFL} demonstrates robustness across all variance levels ($\sigma \in [40, 60, 80, 100]$), effectively capturing the full conditional distribution $p(c|x)$, unlike Pred-DFL models, which rely on less expressive predictors and are more sensitive to variance. 
% By the construction of our data synthesis process, $\sigma$ also reflects the non-linearity of the given optimization problems. Hence, the results also demonstrate the advantages of Gen-DFL under non-linear settings.
% The impact of variance $Var[c| x]$, dimensionality of the parameter space, and training size on model performance is shown in Figure~\ref{fig:gendfl-preddfl}. Gen-DFL demonstrates robustness to the variance of parameter space, achieving the lowest regret across all variance levels ($\sigma \in[40, 60, 80, 100$]), as it effectively models the full conditional distribution $p(c|x)$, unlike Pred-DFL models, which rely on less expressive predictors and are more sensitive to variance.

As dimensionality increases, baseline methods suffer from the curse of dimensionality, leading to higher regret. In contrast, \texttt{Gen-DFL} maintains superior performance by learning the structural complexity of $p(c|x)$, as predicted in Theorem~\ref{theorem:cvar_extension}. Additionally, while Pred-DFL performance deteriorates with smaller training sizes due to increased predictor bias, \texttt{Gen-DFL} remains stable by effectively modeling the underlying distribution. The quadratic term in the objective further amplifies the non-linearity in high-variance settings, demonstrating \texttt{Gen-DFL}'s adaptability to complex optimization problems.
% As data dimensionality increases, baseline methods suffer significantly from the curse of dimensionality, resulting in higher regret. In contrast, Gen-DFL maintains superior performance by capturing the structural complexity of $p(c|x)$, as predicted in Theorem~\ref{theorem:cvar_extension}.

We also evaluate \texttt{Gen-DFL} under various risk-sensitive settings (indicated by the "eval $\alpha$" on the x-axis, where smaller "eval $\alpha$" indicates that we are evaluating under the higher-risk regions) using CVaR, which measures the decision quality (in terms of regret) over the worst-$\alpha\%$ of outcomes (Equation~\eqref{eq:eval_risk}). Figure~\ref{fig:portfolio-cvar-low} shows that models trained with smaller $\alpha$ (e.g., $\alpha=0.5$) outperform those trained with larger $\alpha$ (e.g., $\alpha=1.0$), demonstrating better adaptation to adverse outcomes. The performance gap widens as risk sensitivity increases, confirming that smaller $\alpha$ enhances robustness while larger $\alpha$ prioritizes average-case performance.
% We also observe that the performance of Pred-DFL models deteriorates substantially with smaller training sample sizes due to increased predictor bias. Gen-DFL, however, achieves consistent performance across all sample sizes by leveraging its ability to model the underlying distribution more expressively. Moreover, by the construction of the quadratic term, higher variance scale also increases the non-linearity of the objective function in the optimization. Hence, the results also demonstrate the adaptability of Gen-DFL for more complex optimization problems. 

% ================== Condensed=================
% As demonstrated in Theorem~\ref{theorem:cvar_extension}, the advantages of Gen-DFL over other baselines can be dependent on the variance of the dataset, the dimensionality of the optimization problem, and the sample size available during training. We evaluate the impact of these problem configurations and compare them with the baseline Pred-DFL models, as shown in Figure~\ref{fig:gendfl-preddfl}. Each subplot represents a distinct experimental setting: (a) varying noise levels, (b) increasing data dimensions, and (c) different sample sizes.

% In Figure~\ref{fig:gendfl-preddfl}, we observe the impact of increasing the variance levels $||Var[c\mid x]||$ on the model performance. Gen-DFL consistently achieves the lowest regret across all noise settings, demonstrating its robustness to noise perturbations. Other methods, such as NCE and the two-stage ML model, exhibit significantly higher variance in the regret as the noise increases. This observation aligns with what we demonstrated in Theorem~\ref{theorem:cvar_extension}, which highlights the impact of variance on the performance gap. As the variance increases, the conditional distribution $p(c|x)$ becomes more complex, and the less informative the predictor in Pred-DFL would become as it only regresses to the mean. Gen-DFL, on the other hand, by modeling the full distribution $p(c|x)$, effectively captures the variability and structure of the data more easily even under increasing level of uncertainty, resulting in more robust decisions that achieve consistent decision performance among various settings

% As shown in Figure~\ref{fig:gendfl-preddfl}, we analyze the impact of increasing data dimension. As the dimensionality grows, most baseline methods experience a substantial increase in regret due to the curse of dimensionality. In contrast, Gen-DFL maintains superior performance with lower regret, showcasing its capability to handle high-dimensional settings effectively. This observation is consistent with Theorem\ref{theorem:cvar_extension} as well. As the dimensionality of the parameter space increases, the less informative the mean is~\cite{koppen2000curse}. In contrast, Gen-DFL achieves consistent results by effectively capturing more information of the conditional distribution $p(c|x)$.

% We also evaluate the impact of the amount of training data on the performance of each model. As shown in Figure~\ref{fig:gendfl-preddfl}, when we have less training data, we observe that the performance of all baseline Pred-DFL significantly gets worse. This observation aligns with what we saw in Theorem~\ref{theorem:cvar_extension}, where we demonstrate that the performance of Pred-DFL is largely dependent on the bias of the predictor $c=g(x)$, which would increase when less amount of training samples are given. On the other hand, Gen-DFL achieves consistent performance among all settings, as the generative model is able to model the conditional distribution $p(c|x)$ of the parameter space behind the given stochastic optimization more expressively, unlike the predictor in Pred-DFL which only regresses to the mean or the $\alpha\%$ quantiles.


% \subsection{Decision-Making under Risk}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/cvars_deg4_portfolio.pdf}
%     \caption{\prince{Pending: deciding whether to include these}}
%     \label{fig:portfolio-cvar}
% \end{figure}




% \paragraph{Conditional Value at Risk}
% \paragraph{Decision-Making under Risk}
% We evaluate our framework’s performance in risk-sensitive settings using CVaR, which measures expected regret of the worst-case $\alpha$ of outcomes, as shown in \eqref{eq:eval_risk}. During the evaluation, we vary risk levels ($\alpha \in [0, 0.75]$) to test the framework under different risk-sensitive scenarios, where lower $\alpha$ indicates greater risk sensitivity. Models are trained with $\alpha = \{0.5, 0.75, 1.0\}$ to examine the impact of different risk-aware training strategies.
% \ryan{we should either define $\text{Eval } \alpha$, or call it evaluation at alpha, to be consistent}
% \ryan{Explain choice of upper bound of Eval $\alpha$ being 0.5?} \ryan{Need to add evaluation metric equation reference or new equation where the eval risk and regret under eval alpha is defined}.

To further assess stability, we examine the impact of sample size in the sample-average-approximation step (Figure~\ref{fig:cvar_gen_samples}). Increasing generated samples consistently improves decision quality across all risk levels, reinforcing the importance of uncertainty modeling in \texttt{Gen-DFL}. These results highlight \texttt{Gen-DFL}'s flexibility, making it particularly effective in high-stakes, risk-sensitive environments.
% Figure~\ref{fig:portfolio-cvar-low} shows that models trained with smaller $\alpha$ (e.g. $\alpha=0.5$) consistently outperform those trained with larger $\alpha$ (e.g., $\alpha=1.0$) in these settings, demonstrating better adaptation to adverse outcomes.
% When the models are evaluated under high risk level, the gap between training configurations increases. This suggests that training with larger $\alpha$ prioritizes average-case performance, while smaller $\alpha$ enhances robustness against the adverse outcomes. These results highlight the flexibility and interpretability of our framework, allowing practitioners to tune risk sensitivity for different decision-making tasks, making it particularly effective in high-stakes, risk-sensitive environments. We set the number of generated samples for the sample-average-approximation step in these experiments to be 200. In Figure~\ref{fig:cvar_gen_samples}, we observe that having more generated samples for optimization steps would enhance the decision quality evaluated under all risk-sensitive levels.

% To evaluate the robustness of our framework in risk-sensitive decision-making scenarios, we analyze the performance of models trained with different CVaR objectives (indicated by training $\alpha$) under the most risk-sensitive evaluation settings ($\text{eval } \alpha < 0.5$). Figure~\ref{fig:portfolio-cvar-low} presents shows how average regret varies as evaluation $\alpha$ decreases.

% As shown in the plots, models trained with smaller training $\alpha$ (e.g., $\alpha=0.5$) consistently outperform those trained with larger $\alpha$ (e.g., $\alpha=1.0$) in the most risk-sensitive scenarios. This behavior reflects the ability of our models to better adapt to adverse outcomes. The trend highlights the importance of aligning the risk level $\alpha$ in the loss function with the desired level of risk sensitivity during evaluation.

% The results also confirm that as evaluation $\alpha$ increases, the gap between models trained with different $\alpha$ values diminishes. When $\text{eval } \alpha=0.5$, regret differences become minimal across all training configurations. This indicates that models trained with larger $\alpha$ (e.g., $\alpha=1.0$) are more suited for average-case performance, while smaller $\alpha$ models prioritize robustness in worst-case scenarios. These observations validate the flexibility of our framework, which can adapt to varying levels of risk sensitivity by appropriately selecting training $\alpha$. This adaptability makes the framework particularly effective in decision-making tasks where robustness under adverse conditions is critical.

% \paragraph{Controllable Risk-sensitive Decision-Making with Human-in-the-loop (Regrets vs Likelihood Tradeoff)}
% \prince{In progress}~\ref{fig:tradeoff}



% % ================= This Experiment is a Placeholder=================
% \subsection{Real-world Example: Power Outage}
% \ryan{WIP}
% In this section, we introduce the application of Gen-DFL in a Power Hardening application. The ultimate goal of this problem is to select the optimal subset of locations for intervention under budgetary constraints. This optimization problem we aim to solve, known as the power hardening problem, is defined as follows:
% \begin{equation} 
% \begin{aligned}
% \boldsymbol{x}^* = & \arg \min_{\boldsymbol{x}} \;\;  g(\boldsymbol{x}, {Y})\\
% \text{\text{s.t.}} \quad &\sum_{i = 1}^{I}{\boldsymbol{x}_i} - C \le 0  \\
% & \boldsymbol{x}_i \in \{0, 1\} & \forall i, \\
% \end{aligned}
% \label{eq:opt_power}
% \end{equation}

% where $C$ is the resource constraint limiting the number of cities that can be hardened, $\hat{Y}$ is the prediction of the number of outage customers based on states prediction $\hat{\mathbf{M}}$, and $g$ is a decision loss function. In our study, we adopt System Average Interruption Duration Index (SAIDI) \cite{6209381, 9955492}:

% \begin{align}
%     g(\boldsymbol{x}, {Y}) = \frac{1}{I} \sum_{i=1}^I \frac{1}{n_i} \int_{t=0}^\infty &\left(  \mathbbm{1}({x}_i = 0) {Y}_i(t) \right. \nonumber \\
%     & \left. + \mathbbm{1}({x}_i = 1) \bar{Y}_i(t)\right)~dt,
%     \label{eq:saidi}
% \end{align}

% where $\hat{Y}_i(t)$ denote the predicted number of outages at location $i$ and time $t$ and $\bar{Y}_i(t)$ represents the predicted number of outages under the intervention. We further assume all hardening measure are effective, setting $\bar{Y}_i(t)$ to 0 in \eqref{eq:saidi}. In this case, the optimal solution to \eqref{eq:opt_power}, $\boldsymbol{x}^*$, is the optimal set of cities that should receive hardening to minimize the total impact of the predicted outages. 
% The optimal decision $\boldsymbol{x}^*$ given by \eqref{eq:opt_power} can be evaluated with SAIDI using the actual outage data, $g(\boldsymbol{x}^*, {Y})$. 
% The starting time of these processes is chosen when the outage ratio reaches a predefined threshold $\eta$, which is 2\% in our experiments. 
% They are introduced to describe the speed at which a blackout occurs or is resolved at location $i$. 
% In our setting, we model each location $i$  $k$ covariates—such as weather conditions and census data—that may impact outages, represented as  $\boldsymbol{w}_i = (w_{1}, \dots, w_{I})$ (e.g., metrics like wind speed, temperature, and population).
% % ================= This Experiment is a Placeholder=================

% \subsection{Evaluation}


\section{Conclusion}
% \prince{will work on this at the end after we finish everything}

% \prince{Emphasize that our main contribution is that we proposed the Gen-DFL framework rather than the details of generative model itself.} We leave the studies of using more advanced generaitve models such as Diffusion Model to future exploration.
% \woody{We typically use past tense in conclusion.}
We presented \texttt{Gen-DFL}, a novel decision-focused learning framework that leverages generative modeling to solve robust decision-making problems under various risk-sensitive settings.
% overcoming the limitations of traditional DFL under complex and risk-sensitive optimization problems. 
We also presented a thorough theoretical analysis that demonstrates the performance gain brought by \texttt{Gen-DFL} under various high-risk decision-making problems, which was verified by a set of comprehensive experiments. Our main contribution is the development and the theories of \texttt{Gen-DFL} framework, and we will leave the exploration of using more advanced generative models or optimization schemes under our framework for future studies.

% Our key contribution lies in integrating generative models within DFL, enabling more expressive representations of uncertainty and supporting nonlinear task losses. Additionally, our theoretical analysis establishes formal bounds on regret, demonstrating that Gen-DFL provides provable improvements over traditional DFL, particularly in high-variance and high-dimensional settings. While we employ normalizing flows, future work could easily extend it to alternative generative models such as diffusion models.

% Beyond the evaluated optimization tasks, Gen-DFL has strong potential in reinforcement learning and policy optimization under uncertainty. Further exploration of these applications could solidify the role of generative models as a fundamental component of decision-focused learning.

% \newpage


% This paper advances the field of Machine Learning by introducing the Gen-DFL framework, a decision-focused learning approach that enhances optimization under uncertainty through generative modeling. By modeling the full conditional distribution $p(c|x)$, Gen-DFL has the potential to advance fairness, robustness, and risk-aware decision-making in applications such as disaster response, energy resilience, and financial optimization.

% The broader societal implications of this work lie in its ability to improve decision-making in high-stakes, uncertain environments, mitigating risks in domains where robustness are critical. While we do not foresee any immediate ethical concerns, future work should explore ways to ensure the responsible deployment of such models, particularly in applications affecting public safety and resource allocation.

% \[
% \text{Regret} = \min_{w} \mathbb{E}_{\hat c}[f(w^\star(\hat{c}), c)]
% \]

% For example, in a Predict-Then-Optimize problem, we typically first predict $ \hat{C}(x) $ based on the covariates $x$, and then solve the optimization problem:

% \[
% w^\star(x) = \min_{w } \hat{C}(x)^T w
% \]

% However, the focus of decision-focused learning is not merely improving the accuracy of $ \hat{C}(x) $, but rather ensuring that the decision $w^\star(x)$ minimizes the regret:

% \[
% \text{Regret}(w^\star(x), C) = C^T w^\star(x) - \min_{w } C^T w
% \]

% where $ \text{Regret}(w^\star(x), C) $ quantifies the suboptimality of the decision $w^\star(x)$ relative to the optimal decision based on the true cost $C$.

% This joint framework of predicting and optimizing has been studied under various settings, including stochastic optimization and robust optimization, where the goal is to account for uncertainty in the predicted parameters and ensure that the decisions are robust against these uncertainties.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theorems and Proofs}

\subsection{Surrogate Loss Function}
\label{appendix:theorem}
In this subsection, we present a theoretical bound on the gap between the loss function $ \ell(\theta;p,\alpha) $ w.r.t the ground-truth distribution $ p(c|x) $, and the surrogate loss $ \ell(\theta;q,\alpha) $ w.r.t the proxy distribution $ q(c|x) $ that approximates $ p(c|x) $. 

\begin{theorem}
% Let $ f(c, w) $ be an objective function that is $ L_f $-Lipschitz continuous with respect to $ c $ for a fixed decision variable $ w $, i.e.,
% \[
% |f(c, w) - f(c', w)| \leq L_f \| c - c' \| \quad \text{for all } c, c' .
% \]
Let $ p(c|x) $ be the ground-truth distribution and $ q(c|x) $ be a surrogate distribution that approximates $ p(c|x) $. 

Under the assumption that the objective function $f(c,w)$ is $L_f$-Lipschitz continuous with respect to $c$ for a fixed decision variable $w$, the gap between the loss function $ \ell(\theta;p, \alpha) $ and the surrogate loss $ \ell(\theta;q, \alpha) $ is bounded by
\[
|\ell(\theta;p, \alpha) - \ell(\theta;q, \alpha)| \leq K_q \cdot \mathbb{E}_x \left[ \mathcal{W}(p(c|x), q(c|x)) \right],
\]
where $ \mathcal{W}(p(c|x), q(c|x)) $ is the Wasserstein distance between $ p(c|x) $ and $ q(c|x) $ and $K_q$ is some constant.
% This can be extended to the constrastive loss functions as well. That is,
% \[
% |\ell_{NCE}(\theta) - \ell_{NCE}_q(\theta)| \leq K_q \cdot \mathbb{E}_x \left[ W_1(p(c|x), q(c|x)) \right],
% \]
\end{theorem}

\begin{proof}

First, by linearity of expectation and the triangle inequality, we see that
\begin{equation}    
\left|\ell(\theta;p, \alpha) - \ell(\theta;q, \alpha) \right| \le \mathbb{E}_x\left| A - B\right|, 
\end{equation}
where
\[
A = |\text{CVaR}_{p(c|x)}[f(c, w_\theta^\star)] - \text{CVaR}_{p(c|x)}[f(c, w^\star)]|, \qquad B =  |\text{CVaR}_{q(c|x)}[f(c, w_\theta^\star)] - \text{CVaR}_{q(c|x)}[f(c, w^\star)]|.
\]
For simplicity, we omit $\alpha$ inside $\text{CVaR}$ for now.

Then, we can begin by examining the gap between the expectations under $ p(c|x) $ and $ q(c|x) $ for a fixed context $ x $.

By the reverse triangle inequality ($\left| |x| - |y| \right| \le \left| x-y \right|$), we have
\begin{align*}
\left| A - B\right| \le 
\left| \text{CVaR}_{p(c|x)}[f(c, w^\star)] -  \text{CVaR}_{q(c|x)}[f(c, w^\star)]\right|+ \left| \text{CVaR}_{p(c|x)}[f(c, w_\theta^\star)] -  \text{CVaR}_{q(c|x)}[f(c, w_\theta^\star)]\right|.
\end{align*} 
Let's define $g(c) = f(c, w^\star)$ and $h(c) = f(c, w_\theta^\star)$. By assumption, $ f(c, w) $ is $ L_f $-Lipschitz continuous with respect to $ c $, which implies that that $g(c)$ and $h(c)$ are also $ L_f $-Lipschitz. Hence, by the Kantorvorich-Rubinstein duality for the Wasserstein distance, we have,
\[
\mathcal{W}(p(c|x), q(c|x)) = \sup_{\|g\|_{\text{Lip}} \leq 1} \left| \mathbb{E}_{c \sim p(c|x)}[g(c)] - \mathbb{E}_{c \sim q(c|x)}[g(c)] \right| = \sup_{\|h\|_{\text{Lip}} \leq 1} \left| \mathbb{E}_{c \sim p(c|x)}[h(c)] - \mathbb{E}_{c \sim q(c|x)}[h(c)] \right|,
\]
where the supremum is over all functions $ g, h $ that are 1-Lipschitz.

By definition of CVaR, we can see that,
\[
\left| \text{CVaR}_{p(c|x)}[f(c, w^\star)] -  \text{CVaR}_{q(c|x)}[f(c, w^\star)]\right| \le \sup_{\|h\|_{\text{Lip}} \leq 1} \left| \mathbb{E}_{c \sim p(c|x)}[h(c)] - \mathbb{E}_{c \sim q(c|x)}[h(c)] \right|.
\]
% \begin{align*}
% \left| \text{CVaR}_{p(c|x)}[f(c, w^\star)] -  \text{CVaR}_{q(c|x)}[f(c, w^\star)]\right| &\le \left| \mathbb{E}_{p(c|x)}[f(c, w^\star)] -  \mathbb{E}_{q(c|x)}[f(c, w^\star)]\right|  \\
% &\le \sup_{\|h\|_{\text{Lip}} \leq 1} \left| \mathbb{E}_{c \sim p(c|x)}[h(c)] - \mathbb{E}_{c \sim q(c|x)}[h(c)] \right|
% \end{align*}

Again, using the assumption that $g(c)$ and $h(c)$ are also $ L_f $-Lipschitz, we can bound the gap in (2) by
\[
\left| \text{CVaR}_{p(c|x)}[f(c, w^\star)] -  \text{CVaR}_{q(c|x)}[f(c, w^\star)]\right|
+ \left| \text{CVaR}_{p(c|x)}[f(c, w_\theta^\star)] -  \text{CVaR}_{q(c|x)}[f(c, w_\theta^\star)]\right| \le 2L_f \mathcal{W}(p(c|x), q(c|x)).
\]

Finally, taking the expectation over $x$ on both sides and using equation (1) and set the constant $K_q = 2L_f$, we get:
\[
|\ell(\theta;p, \alpha) - \ell(\theta;q, \alpha)| \leq K_q \cdot \mathbb{E}_x \left[ \mathcal{W}(p(c|x), q(c|x)) \right].
\]

This completes the proof.
\end{proof}

% \begin{theorem}
% Let $ f(c, w) $ be an objective function that is $ L_f $-Lipschitz continuous with respect to $ c $ for a fixed decision variable $ w $, i.e.,
% \[
% |f(c, w) - f(c', w)| \leq L_f \| c - c' \| \quad \text{for all } c, c' .
% \]
% Let $ p(c|x) $ be the ground-truth distribution, $ \hat{p}(c|x) $ the empirical distribution based on $ N $ samples, and $ q(c|x) $ the surrogate distribution. Define the loss function $ L(\theta) $ as
% \[
% L(\theta) = \mathbb{E}_x \left| \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, \hat{w}^\star(\theta) \right) \right] - \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^\star \right) \right] \right|,
% \]
% where $ \hat{w}^\star(\theta) = \arg\min_{w } \mathbb{E}_{c \sim q(c|x)}[f(c, w)] $ and $ w^\star = \arg\min_{w } \mathbb{E}_{c \sim p(c|x)}[f(c, w)] $.

% The gap between $ L(\theta) $ and the surrogate loss $ L_q(\theta) $, defined as
% \[
% L_q(\theta) = \mathbb{E}_x \left| \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, \hat{w}^\star(\theta) \right) \right] - \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^\star \right) \right] \right|,
% \]
% is bounded by
% \[
% |L(\theta) - L_q(\theta)| \leq K \cdot \mathbb{E}_x \left[ W_1(\hat{p}(c|x), q(c|x)) \right] + \mathcal{O}(N^{-1/d}),
% \]
% where $ W_1(\hat{p}(c|x), q(c|x)) $ is the Wasserstein-1 distance between $ \hat{p}(c|x) $ and $ q(c|x) $, and $ K $ is a constant dependent on $ L_f $.
% \end{theorem}

% \begin{proof}
% Using the triangle inequality, we decompose $ |L(\theta) - L_q(\theta)| $ as
% \[
% |L(\theta) - L_q(\theta)| \leq |L(\theta) - \hat{L}(\theta)| + |\hat{L}(\theta) - L_q(\theta)|,
% \]
% where $ \hat{L}(\theta) $ is the empirical loss defined using $ \hat{p}(c|x) $:
% \[
% \hat{L}(\theta) = \mathbb{E}_x \left| \mathbb{E}_{c \sim \hat{p}(c|x)} \left[ f\left( c, \hat{w}^\star(\theta) \right) \right] - \mathbb{E}_{c \sim \hat{p}(c|x)} \left[ f\left( c, w^\star \right) \right] \right|.
% \]

% First, we will bound $ |L(\theta) - \hat{L}(\theta)| $

% For the true loss $ L(\theta) $ and the empirical loss $ \hat{L}(\theta) $, the gap arises from approximating $ p(c|x) $ with $ \hat{p}(c|x) $. Using the Lipschitz continuity of $ f(c, w) $ with respect to $ c $, we have
% \[
% |L(\theta) - \hat{L}(\theta)| \leq \mathbb{E}_x \left| \mathbb{E}_{c \sim p(c|x)}[f(c, \hat{w}^\star)] - \mathbb{E}_{c \sim \hat{p}(c|x)}[f(c, \hat{w}^\star)] \right|
% + \mathbb{E}_x \left| \mathbb{E}_{c \sim p(c|x)}[f(c, w^\star)] - \mathbb{E}_{c \sim \hat{p}(c|x)}[f(c, w^\star)] \right|.
% \]
% Since $ f(c, w) $ is $ L_f $-Lipschitz in $ c $, the Kantorovich-Rubinstein duality gives
% \[
% \left| \mathbb{E}_{c \sim p(c|x)}[f(c, w)] - \mathbb{E}_{c \sim \hat{p}(c|x)}[f(c, w)] \right| \leq L_f \cdot W_1(p(c|x), \hat{p}(c|x)).
% \]
% Thus,
% \[
% |L(\theta) - \hat{L}(\theta)| \leq 2L_f \cdot \mathbb{E}_x \left[ W_1(p(c|x), \hat{p}(c|x)) \right].
% \]

% Next, we will try to bound $ |\hat{L}(\theta) - L_q(\theta)| $

% Using the triangle inequality again, we bound $ |\hat{L}(\theta) - L_q(\theta)| $ in the same way as in the original theorem, with the approximation error arising from $ W_1(\hat{p}(c|x), q(c|x)) $:
% \[
% |\hat{L}(\theta) - L_q(\theta)| \leq K \cdot \mathbb{E}_x \left[ W_1(\hat{p}(c|x), q(c|x)) \right].
% \]

% Hence, we get
% \[
% |L(\theta) - L_q(\theta)| \leq 2L_f \cdot \mathbb{E}_x \left[ W_1(p(c|x), \hat{p}(c|x)) \right] + K \cdot \mathbb{E}_x \left[ W_1(\hat{p}(c|x), q(c|x)) \right].
% \]

% The Wasserstein-1 distance between $ p(c|x) $ and $ \hat{p}(c|x) $ decays as $ \mathcal{O}(N^{-1/d}) $, where $ d $ is the dimensionality of $ c $. Thus,
% \[
% \mathbb{E}_x \left[ W_1(p(c|x), \hat{p}(c|x)) \right] = \mathcal{O}(N^{-1/d}).
% \]

% Substituting this into the bound, we obtain:
% \[
% |L(\theta) - L_q(\theta)| \leq K \cdot \mathbb{E}_x \left[ W_1(\hat{p}(c|x), q(c|x)) \right] + \mathcal{O}(N^{-1/d}),
% \]
% where $ K = 2L_f $.
% \end{proof}

% Similarly, we can also the bound gap between the real surrogate contrastive loss and the proxy contrastive loss,

% \begin{theorem}
% Let $ f(c, w) $ be an objective function that is $ L_f $-Lipschitz continuous with respect to $ c $ for a fixed decision variable $ w $, i.e.,
% \[
% |f(c, w) - f(c', w)| \leq L_f \| c - c' \| \quad \text{for all } c, c' .
% \]
% Let $ p(c|x) $ be the ground-truth distribution and $ q(c|x) $ be a surrogate distribution that approximates $ p(c|x) $. Define the contrastive loss function $ \ell_{NCE}(\theta) $ as
% \[
% \ell_{NCE}(\theta) = \mathbb{E}_x \left[ \sum_{w^s \in S} \left( \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^s) \right) \right] - \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^\star \right) \right] \right) \right],
% \]
% where $w^\star = \arg\min_{w} \ \mathbb{E}_{c\sim p(c|x)}[f(c, w)]$ and $w^s \in S \subset \mathcal{W } \setminus w^\star$ are treated as the negative samples consisting of solutions that are different from the target solution $w^\star$. 

% And the surrogate contrastive loss $ \ell_{NCE_q}(\theta) $ with the proxy model $ q(c|x) $ is defined as
% \[
% \ell_{NCE_q}(\theta) = \mathbb{E}_x \left[ \sum_{w^s \in S} \left( \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^s) \right) \right] - \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^\star \right) \right] \right) \right],
% \]
% % where $w^\star' = \arg\min_{w} \ \mathbb{E}_{c\sim q(c|x)}[f(c, w)]$

% Then, the gap between $ \ell_{NCE}(\theta) $ and $ \ell_{NCE_q}(\theta) $ is bounded by
% \[
% |\ell_{NCE}(\theta) - \ell_{NCE_q}(\theta)| \leq K_q \cdot \mathbb{E}_x \left[ W_1(p(c|x), q(c|x)) \right],
% \]
% where $ W_1(p(c|x), q(c|x)) $ is the Wasserstein-1 distance between $ p(c|x) $ and $ q(c|x) $ and $K_q$ is some constant.
% \end{theorem}

% \begin{proof}
% First, let
% \[
% A = \sum_{w^s \in S} \left( \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^s) \right) \right] - \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^\star \right) \right] \right), \quad B = \sum_{w^s \in S} \left( \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^s) \right) \right] - \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^\star \right) \right] \right)
% \]
% we can see that 
% \[
% |A - B| \le \sum_{w^s \in S} \left| \left( \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^s) \right) \right] - \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^s) \right) \right] \right)\right| + N_S \left| \left( \mathbb{E}_{c \sim p(c|x)} \left[ f\left( c, w^\star) \right) \right] - \mathbb{E}_{c \sim q(c|x)} \left[ f\left( c, w^\star) \right) \right] \right)\right|
% \]
% \end{proof}

% Given the Lipschitz assumption and following the steps of the previous proof, we get:
% \[
% |A - B| \le 2N_S L_f W_1(p(c|x), q(c|x))
% \]

% Taking the expectation over $x$ on both sides and set $K_q = 2N_S L_f$, we have
% \[
% |\ell_{NCE}(\theta) - \ell_{NCE_q}(\theta)| \leq K_q \cdot \mathbb{E}_x \left[ W_1(p(c|x), q(c|x)) \right],
% \]

% \begin{theorem}
% Let $ f(c, w) $ be an objective function that is $ L_f $-Lipschitz continuous with respect to $ c $ for a fixed decision variable $ w $, i.e.,
% \[
% |f(c, w) - f(c', w)| \leq L_f \|c - c'\|, \quad \forall c, c' .
% \]

% Assume that $ p(c|x) $ and $ \hat{p}(c|x) $ are \textit{long-tailed distributions}, with their tail probabilities characterized by the following inequality for some threshold $ \tau $:
% \[
% P(c \notin \mathcal{C}_\tau) \leq T(\tau),
% \]
% where $ \mathcal{C}_\tau = \{ c  : \|c\| \leq \tau \} $, and $ T(\tau) $ is a monotonically decreasing function of $ \tau $ that describes the tail decay (e.g., $ T(\tau) \sim \exp(-\tau) $ for exponential tails or $ T(\tau) \sim \tau^{-\beta} $ for power-law tails).

% Define the Conditional Value at Risk (CVaR) loss $ L_{\text{CVaR}}(\theta) $ as:
% \[
% L_{\text{CVaR}}(\theta) = \mathbb{E}_x \Big| \text{CVaR}_\alpha \big( f(c, \hat{w}^\star(\theta)) \big) - \text{CVaR}_\alpha \big( f(c, w^\star) \big) \Big|,
% \]
% where:
% \[
% \hat{w}^\star(\theta) = \arg\min_{w } \mathbb{E}_{\hat{c} \sim \hat{p}(c|x)}[f(\hat{c}, w)], \quad
% w^\star = \arg\min_{w } \mathbb{E}_{c \sim p(c|x)}[f(c, w)].
% \]

% Similarly, define the surrogate CVaR loss $ L_{\text{CVaR}, q}(\theta) $ as:
% \[
% L_{\text{CVaR}, q}(\theta) = \mathbb{E}_x \Big| \text{CVaR}_\alpha \big( f(c, \hat{w}^\star(\theta)) \big) - \text{CVaR}_\alpha \big( f(c, w^\star) \big) \Big|_{c \sim q(c|x)}.
% \]

% Assume that the tail sensitivity of $ f(c, w) $ to the decision variable $ w $ is characterized by:
% \[
% R_w(\tau) = \sup_{c \notin \mathcal{C}_\tau} \big| f(c, w^\star) - f(c, \hat{w}^\star) \big|,
% \]
% which depends on the behavior of $ f(c, w) $ in the tail region $ c \notin \mathcal{C}_\tau $.

% Then, the gap between $ L_{\text{CVaR}}(\theta) $ and $ L_{\text{CVaR}, q}(\theta) $ is bounded as:
% \[
% |L_{\text{CVaR}}(\theta) - L_{\text{CVaR}, q}(\theta)| \leq K_\text{CVaR} \cdot \mathbb{E}_x \big[ W_1(p(c|x), q(c|x)) \big] + T(\tau) \cdot R_w(\tau),
% \]
% where:
% \begin{itemize}
%     \item $ W_1(p(c|x), q(c|x)) $ is the Wasserstein-1 distance between $ p(c|x) $ and $ q(c|x) $,
%     \item $ K_\text{CVaR} $ is a constant depending on $ L_f $ and $ \alpha $,
%     \item $ T(\tau) \cdot R_w(\tau) $ reflects the combined effect of the tail behavior and the sensitivity of $ f(c, w) $ to the decision variable $ w $.
% \end{itemize}
% \end{theorem}

% \begin{theorem}
% \label{theorem:ultimate}
% Let $p(c|x)$ denote the true conditional distribution of $c$, and let $\hat{p}(c|x)$ be its empirical estimate from $n$ samples. $p_\theta(c|x)$ denotes the learned generative model. Let $f(x)$ be a predictor that estimates $\mathbb{E}_{c\sim p(c|x)}[c|x]$. Define:

% For the Pred-DFL, define the regret as:
% \[
% R_{\text{pred}}(x) = |c^T(\hat{w}_{\text{pred}}^\star - w^\star)|,
% \]
% For the Gen-DFL with empirical distribution, define:
% \[
% \hat{R}_{\text{gen}}(x) = \mathbb{E}_{c \sim \hat{p}(c|x)} [c^T(\hat{w}_{\text{gen}}^\star - w^\star)].
% \]

% where, $\hat{w}^\star_{\text{pred}}(x) = \arg\min_{w} \ [f(x)^T w]$, $\hat{w}^\star_{\text{gen}}(x) = \arg\min_{w} \ \mathbb{E}_{c\sim p_\theta(c|x)}[c^T w]$.

% And, let the true optimal solution be:
% \[
% w^\star = \arg\min_{w} \ \mathbb{E}_{c\sim p(c|x)}[c^T w]
% \]

% Assume:
% \begin{enumerate}
%     \item $w^\star(c)$ is Lipschitz continuous with constant $L_w$:
%     \[
%     \|w^\star(c_1) - w^\star(c_2)\| \leq L_w \|c_1 - c_2\|
%     \]
%     \item The conditional distribution $p(c|x)$ has the structure:
%     \[
%     c = \bar{c} + Lf + \sigma\xi
%     \]
%     where $\bar{c} = \mathbb{E}_{c\sim p(c|x)}[c|x]$,  $L \in \mathbb{R}^{d\times k}$ is a low-rank factor loading matrix, $f \sim N(0, I_k)$ represents systematic factors, and $\xi \sim N(0, I_d)$ represents idiosyncratic noise.

%     The benefit of constructing $c$ in this way is that we can break down the variance of $p(c|x)$ into two parts, that is,
%     \[
%     \text{Var}[c|x] = LL^T + \sigma^2 I = \text{systematic noise} + \text{idiosyncratic noise}
%     \]
%     \item The predictor bias is defined as:
%     \[
%     \text{Bias}[f] = f(x) - \mathbb{E}_{c\sim p(c|x)}[c|x]
%     \]
% \end{enumerate}

% Then, the regret difference that demonstrates the performance gap between Pred-DFL and Gen-DFL is upper bounded by:
% \[
% \mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq L_w\left(\frac{C_1k}{\sqrt{d}} + \sigma^2\right)(1 + \mathbb{E}_x\|\mathbb{E}_{c\sim p(c|x)}[c|x]\|) + L_w\mathbb{E}_x \mathbb{E}_{c\sim p(c|x)}||c|| \|\text{Bias}[f]\| + C_2\sqrt{\frac{d}{n}}
% \]

% where $C_1, C_2$ are some constants, and:
% \begin{itemize}
%     \item The term $\frac{C_1k}{\sqrt{d}}$ captures systematic risk that decreases with dimension
%     \item The term $\sigma^2$ represents constant idiosyncratic risk
%     \item The term $\mathcal{W}_2(\hat{p}(c|x), p(c|x))$ is the 2-Wasserstein distance between empirical and true distributions, which is bounded by $O(\sqrt{d/n})$ with high probability
% \end{itemize}
% \end{theorem}

% To prove the above theorem, we will need the following lemmas:
% \begin{lemma}
% For the given data generation process:
% \[
% c = \bar{c} + Lf + \sigma\xi,
% \]
% the conditional variance has the decomposition:
% \[
% \text{Var}[c|x] = LL^T + \sigma^2I_d
% \]
% where $\|LL^T\|_F = O(k)$ for fixed factor dimension $k$.
% \end{lemma}

% \begin{lemma}[High-Dimensional Scaling]
% For fixed factor dimension $k$ and increasing data dimension $d$:
% \[
% \frac{\|LL^T\|_F}{\sqrt{d}} \to 0 \text{ as } d \to \infty
% \]
% This follows because $\|LL^T\|_F^2 = \text{tr}(LL^TLL^T) = O(k)$ for fixed $k$.
% \end{lemma}

% \begin{lemma}

% For the systematic and idiosyncratic decomposition:
% \[
% \epsilon = L(f - \mathbb{E}[f]) + \sigma\xi
% \]

% This gives:
% \[
% \|\text{Var}[c|x]\|_F = \|LL^T + \sigma^2I_d\|_F \leq \frac{C_1k}{\sqrt{d}} + \sigma^2
% \]    
% \end{lemma}

% \begin{proof}[proof of Theorem \ref{theorem:ultimate}]

% Step 1: Decomposing the Regret Gap.

% From the definitions of $ \hat{R}_{\text{gen}}(x) $ and $ R_{\text{pred}}(x) $, the regret gap is:
% \[
% \Delta R(x) = \hat{R}_{\text{gen}}(x) - R_{\text{pred}}(x),
% \]

% By triangular inequality, we have,
% \[
% |\Delta R(x)| = |R_{\text{gen}}(x) - R_{\text{pred}}(x) + \hat{R}_{\text{gen}}(x) - R_{\text{gen}}(x)| \le |R_{\text{gen}}(x) - R_{\text{pred}}(x)| + |\hat{R}_{\text{gen}}(x) - R_{\text{gen}}(x)|
% \]

% And, since we wrote $c$ as 
% \[
% c = \bar{c} + \epsilon
% \]
% Note that to differentiate this $c$ from the samples of $p(c|x)$, we use $c_\text{true}$ interchanglely.

% we can further expand $R_\text{gen}$ as
% \[
% R_{\text{gen}}(x) = \mathbb{E}_{c \sim p(c|x)} \big| \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) + \epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big|,
% \]

% and
% \[
% R_{\text{pred}}(x) = \big| c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big|.
% \]

% where $\hat{c} = \mathbb{E}_{c \sim p_\theta(c|x)}[c|x]$ and $c_\text{pred} = f_\theta(x)$

% Expanding the gap:
% \[
% \Big|\Delta R(x)\Big| \le \Big|\Big|\mathbb{E}_{c \sim p(c|x)} \big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) - c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big]\Big|\Big| + \Big|\Big|\mathbb{E}_{c\sim p(c|x)}\big[\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big]\Big|\Big|  + \Big|\hat{R}_{\text{gen}}(x) - R_{\text{gen}}(x)\Big|.
% \]

% Step 2. Bounding the first term involving $\bar{c}$

% For the first term, we observe that:
% \[
% \mathbb{E}_{c \sim p(c|x)} \Big| \big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) - c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big] \Big| \le \mathbb{E}_{c \sim p(c|x)} \Big|\big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big]\Big| + \mathbb{E}_{c\sim p(c|x)} \Big| \big[ c^\top \big(w^\star(c_\text{pred}) - w^\star\big)\big] \Big|
% \]

% Since, $ \|w^\star_{\text{gen}}(\hat{c}) - w^\star(c_\text{true})\| \leq L_w \|c_\text{true} - \hat{c}\| = L_w \|\epsilon\| $, we can bound:
% \[
% \big\|\mathbb{E}_{c \sim p(c|x)}[w^\star_\theta(\hat{c}) - w^\star]\big\| \leq L_w \|\text{Var}[c|x]\|_F.
% \]

% Thus, the difference is bounded by:
% \[
% \Big|\Big|\mathbb{E}_{c \sim p(c|x)} \big[ \bar{c}^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) - c^\top \big(w^\star(c_\text{pred}) - w^\star\big) \big]\Big|\Big| \leq \|\bar{c}\| \cdot L_w \|\text{Var}[c|x]\|_F + \mathbb{E}_{c\sim p(c|x)}\big[||c||\big] L_w || \text{Bias}[f] ||.
% \]

% Step 3: Bounding the $\epsilon$-Term.

% For the second term:
% \[
% \mathbb{E}_{c\sim p(c|x)}\big[\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big],
% \]
% use the Cauchy-Schwarz inequality:
% \[
% \big|\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big)\big| \leq \|\epsilon\| \cdot \|w_{\text{gen}}^\star(\hat{c}) - w^\star\|.
% \]

% Since $ \|w_{\text{gen}}^\star(\hat{c}) - w^\star(c_\text{true})\| \leq L_w \|\epsilon\| $, we have:
% \[
% \big|\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star(c_\text{true})\big)\big| \leq L_w \|\epsilon\|^2.
% \]

% Taking the expectation over $ c \sim p(c|x) $:
% \[
% \Big|\Big|\mathbb{E}_{c\sim p(c|x)}\big[\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star\big) \big]\Big|\Big| \le \mathbb{E}_{c \sim p(c|x)} \big|\epsilon^\top \big(w_{\text{gen}}^\star(\hat{c}) - w^\star(c_\text{true})\big)\big| \leq L_w \mathbb{E}_{c \sim p(c|x)}[\|\epsilon\|^2] = L_w \|\text{Var}[c|x]\|_F.
% \]

% Step 4: For the empirical estimation term:
% \[
% |R_{\text{gen}}(x) - \hat{R}_{\text{gen}}(x)| \leq C_2\mathcal{W}_2(\hat{p}(c|x), p(c|x)) \leq C_2\sqrt{\frac{d}{n}}
% \]

% Finally, combining all the above steps and taking expectation over x, we get:
% \[
% \mathbb{E}_x[|R_{\text{pred}}(x) - \hat{R}_{\text{gen}}(x)|] \leq L_w\left(\frac{C_1k}{\sqrt{d}} + \sigma^2\right)(1 + \mathbb{E}_x\|\mathbb{E}_{c\sim p(c|x)}[c|x]\|) + L_w\mathbb{E}_x \mathbb{E}_{c\sim p(c|x)}||c|| \|\text{Bias}[f]\| + C_2\sqrt{\frac{d}{n}}
% \]
% \end{proof}

% \begin{theorem}
% Let $ p(c|x) $ denote the true conditional distribution of $ c $, and let $ \hat{c} = \mathbb{E}_c[c|x] $. Define the regret for the Pred-DFL as:
% \[
% R_{\text{pred}}(x) = \big| c^\top \big(w^\star(\hat{c}) - w^\star(c) \big) \big|,
% \]
% where $ w^\star(c) = \arg\min_{w } f(c, w) $. For the Gen-DFL, the regret is:
% \[
% R_{\text{gen}}(x) = \mathbb{E}_{c \sim p(c|x)} \big| c^\top \big(w^\star(\hat{c}) - w^\star(c) \big) \big|.
% \]

% Assume:
% \begin{enumerate}
%     \item $ w^\star(c) $ is Lipschitz continuous with respect to $ c $, with Lipschitz constant $ L_w $:
%     \[
%     \|w^\star(c) - w^\star(\hat{c})\| \leq L_w \|c - \hat{c}\|.
%     \]
%     \item $ p(c|x) $ has a covariance matrix $\text{Var}[c|x]$, and its Frobenius norm measures the dispersion of $ c $ around $ \hat{c} = \mathbb{E}_c[c|x] $.
% \end{enumerate}

% Then, the regret gap between the generative-model DFL and the predicted-model DFL is upper bounded as:
% \[
% \Delta R(x) = R_{\text{gen}}(x) - R_{\text{pred}}(x) \leq L_w \|\text{Var}[c|x]\|_F \big(1 + \|\hat{c}\|\big),
% \]
% where:
% \begin{itemize}
%     \item $ \|\text{Var}[c|x]\|_F $ is the Frobenius norm of the covariance matrix of $ p(c|x) $,
%     \item $ \|\hat{c}\| $ is the norm of the conditional mean of $ c $.
% \end{itemize}
% \end{theorem}

% \begin{proof}

% Step 1: Decomposing the Regret Gap.

% From the definitions of $ R_{\text{gen}}(x) $ and $ R_{\text{pred}}(x) $, the regret gap is:
% \[
% \Delta R(x) = R_{\text{gen}}(x) - R_{\text{pred}}(x),
% \]
% where:
% \[
% R_{\text{gen}}(x) = \mathbb{E}_{c \sim p(c|x)} \big| \hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big) + \epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big) \big|,
% \]
% and
% \[
% R_{\text{pred}}(x) = \big| \hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big) \big|.
% \]

% Expanding the gap:
% \[
% \Delta R(x) = \mathbb{E}_{c \sim p(c|x)} \big[\hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big] - \hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big) + \mathbb{E}_{c \sim p(c|x)} \big[\epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big].
% \]

% Step 2: Bounding the $\hat{c}^\top$-Term.

% For the first term:
% \[
% \mathbb{E}_{c \sim p(c|x)} \big[\hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big] - \hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big),
% \]
% we observe:
% \[
% \hat{c}^\top \big(\mathbb{E}_{c \sim p(c|x)}[w^\star(\hat{c}) - w^\star(c)] - (w^\star(\hat{c}) - w^\star(c))\big).
% \]

% Since $ \|w^\star(\hat{c}) - w^\star(c)\| \leq L_w \|c - \hat{c}\| = L_w \|\epsilon\| $, we can bound:
% \[
% \big\|\mathbb{E}_{c \sim p(c|x)}[w^\star(\hat{c}) - w^\star(c)]\big\| \leq L_w \|\text{Var}[c|x]\|_F.
% \]

% Thus, the difference is bounded by:
% \[
% \big|\mathbb{E}_{c \sim p(c|x)}[\hat{c}^\top (w^\star(\hat{c}) - w^\star(c))] - \hat{c}^\top (w^\star(\hat{c}) - w^\star(c))\big| \leq \|\hat{c}\| \cdot L_w \|\text{Var}[c|x]\|_F.
% \]

% Step 3: Bounding the $\epsilon$-Term.

% For the second term:
% \[
% \mathbb{E}_{c \sim p(c|x)} \big[\epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big],
% \]
% use the Cauchy-Schwarz inequality:
% \[
% \big|\epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big| \leq \|\epsilon\| \cdot \|w^\star(\hat{c}) - w^\star(c)\|.
% \]

% Since $ \|w^\star(\hat{c}) - w^\star(c)\| \leq L_w \|\epsilon\| $, we have:
% \[
% \big|\epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big| \leq L_w \|\epsilon\|^2.
% \]

% Taking the expectation over $ c \sim p(c|x) $:
% \[
% \mathbb{E}_{c \sim p(c|x)} \big|\epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big| \leq L_w \mathbb{E}_{c \sim p(c|x)}[\|\epsilon\|^2] = L_w \|\text{Var}[c|x]\|_F.
% \]

% Step 4: Combining Results.

% Combining the bounds from Steps 2 and 3, the regret gap is:
% \[
% \Delta R(x) \leq L_w \|\text{Var}[c|x]\|_F + \|\hat{c}\| \cdot L_w \|\text{Var}[c|x]\|_F.
% \]

% Factoring out $ L_w \|\text{Var}[c|x]\|_F $:
% \[
% \Delta R(x) \leq L_w \|\text{Var}[c|x]\|_F \big(1 + \|\hat{c}\|\big).
% \]
% \end{proof}

% \begin{theorem}
% Let $ p(c|x) $ denote the true conditional distribution, and let $ \hat{c} = \mathbb{E}[c|x] $ denote the predictor in Predicted-DFL. Assume:
% \begin{itemize}
%     \item Variance per dimension is fixed: $ \sigma^2 = \text{Var}[c_j|x], \, j = 1, \dots, m $,
%     \item Bias in Predicted-DFL scales with $ m $: $ \text{Bias}(\hat{c}) \propto \sqrt{m} \cdot \text{Bias}(\sigma) $,
%     \item Sampling noise scales with $ m $: $ W_1(p, \hat{p}) \propto \sqrt{\frac{m}{N}} $.
% \end{itemize}

% The regret gap between Gen-DFL and Predicted-DFL is upper-bounded as:
% \[
% \Delta R(x) \leq L_w \bigg( \underbrace{\sqrt{\frac{m}{N}}}_{\text{Sampling noise}} + \underbrace{\sqrt{m} \cdot \sigma^2 \cdot (1 + \|\hat{c}\|)}_{\text{Variance contribution}} + \underbrace{\sqrt{m} \cdot \text{Bias}(\sigma)}_{\text{Bias contribution}} \bigg),
% \]
% where:
% \begin{itemize}
%     \item The total variance grows with $ \sqrt{m} $, but the marginal contribution per dimension diminishes,
%     \item Sampling noise dominates for small $ N $,
%     \item Bias grows with $ \sqrt{m} $, amplifying the regret for Predicted-DFL.
% \end{itemize}
% \end{theorem}

% \begin{theorem}[Dimension-Dependent Regret Bounds]
% Let $p(c|x)$ denote the true conditional distribution of $c$, and let $\hat{p}(c|x)$ be its empirical estimate from $n$ samples. Let $\hat{c} = \mathbb{E}_c[c|x]$ and $\hat{c}_n$ be its empirical estimate. For the Pred-DFL, define the regret as:
% \[
% R_{\text{pred}}(x) = \big| c^\top \big(w^\star(\hat{c}_n) - w^\star(c) \big) \big|,
% \]
% where $w^\star(c) = \arg\min_{w } f(c, w)$. For the Gen-DFL, the regret is:
% \[
% R_{\text{gen}}(x) = \mathbb{E}_{c \sim \hat{p}(c|x)} \big| c^\top \big(w^\star(\hat{c}_n) - w^\star(c) \big) \big|.
% \]
% Assume:
% \begin{enumerate}
%     \item $w^\star(c)$ is Lipschitz continuous with constant $L_w$:
%     \[
%     \|w^\star(c_1) - w^\star(c_2)\| \leq L_w \|c_1 - c_2\|
%     \]
%     \item The conditional distribution $p(c|x)$ has the structure:
%     \[
%     c = \bar{r} + Lf + \sigma\xi
%     \]
%     where $L \in \mathbb{R}^{d\times k}$ is a low-rank factor loading matrix, $f \sim N(0, I_k)$ represents systematic factors, and $\xi \sim N(0, I_d)$ represents idiosyncratic noise.
%     \item The empirical distribution $\hat{p}(c|x)$ is estimated from $n$ independent samples.
% \end{enumerate}
% Then, the regret gap between the generative and predictive DFL is upper bounded as:
% \[
% \Delta R(x) \leq L_w\left(\frac{C_1k}{\sqrt{d}} + \sigma^2\right)(1 + \|\hat{c}\|) + L_w\|\text{Bias}[\hat{c}|x]\| + C_2\sqrt{\frac{d}{n}},
% \]
% where $C_1, C_2$ are constants, and:
% \begin{itemize}
%     \item The term $\frac{C_1k}{\sqrt{d}}$ captures systematic risk that decreases with dimension
%     \item The term $\sigma^2$ represents constant idiosyncratic risk
%     \item The term $\sqrt{\frac{d}{n}}$ reflects finite-sample effects
% \end{itemize}
% \end{theorem}

% \begin{proof}
% We proceed in several steps:

% \textbf{Step 1:} First, we establish key properties of the variance decomposition.
% \begin{lemma}[Variance Decomposition]
% For the given data generation process:
% \[
% c = \bar{r} + Lf + \sigma\xi,
% \]
% the conditional variance has the decomposition:
% \[
% \text{Var}[c|x] = LL^T + \sigma^2I_d
% \]
% where $\|LL^T\|_F = O(k)$ for fixed factor dimension $k$.
% \end{lemma}

% \textbf{Step 2:} We analyze the high-dimensional scaling.
% \begin{lemma}[High-Dimensional Scaling]
% For fixed factor dimension $k$ and increasing data dimension $d$:
% \[
% \frac{\|LL^T\|_F}{\sqrt{d}} \to 0 \text{ as } d \to \infty
% \]
% This follows because $\|LL^T\|_F^2 = \text{tr}(LL^TLL^T) = O(k)$ for fixed $k$.
% \end{lemma}

% \textbf{Step 3:} We bound the empirical estimation error.
% \begin{lemma}[Empirical Estimation]
% Given $n$ independent samples, with probability at least $1-\delta$:
% \[
% \|\hat{\text{Var}}[c|x] - \text{Var}[c|x]\|_F \leq O\left(\sqrt{\frac{d\log(1/\delta)}{n}}\right)
% \]
% \end{lemma}

% \textbf{Step 4:} We decompose $c - \hat{c}$ into systematic and idiosyncratic components:
% \begin{enumerate}
%     \item Systematic component (from $Lf$):
%     \[
%     \|L(f - \mathbb{E}[f])\|^2 = O(k)
%     \]
%     \item Idiosyncratic component (from $\sigma\xi$):
%     \[
%     \|\sigma\xi\|^2 = O(d\sigma^2)
%     \]
% \end{enumerate}

% \textbf{Step 5:} Following the original decomposition:
% \[
% \Delta R(x) = \mathbb{E}_{c \sim p(c|x)} \big[\hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big] - \hat{c}^\top \big(w^\star(\hat{c}) - w^\star(c)\big) + \mathbb{E}_{c \sim p(c|x)} \big[\epsilon^\top \big(w^\star(\hat{c}) - w^\star(c)\big)\big]
% \]

% For the systematic and idiosyncratic decomposition:
% \[
% \epsilon = L(f - \mathbb{E}[f]) + \sigma\xi
% \]

% This gives:
% \[
% \|\text{Var}[c|x]\|_F = \|LL^T + \sigma^2I_d\|_F \leq \frac{C_1k}{\sqrt{d}} + \sigma^2
% \]

% For the empirical estimation with n samples:
% \[
% \|\hat{\text{Var}}[c|x] - \text{Var}[c|x]\|_F \leq C_2\sqrt{\frac{d}{n}}
% \]

% Combining these with your original bound:
% \[
% \Delta R(x) \leq L_w \|\text{Var}[c|x]\|_F \big(1 + \|\hat{c}\|\big) + L_w\|\text{Bias}[\hat{c}|x]\| + C_2\sqrt{\frac{d}{n}}
% \]
% This bound reveals three distinct regimes:
% \begin{enumerate}
%     \item Low-dimensional regime $(d \ll k)$: $\Delta R(x) \approx O(k/\sqrt{d})$
%     \item High-dimensional regime $(d \gg k)$: $\Delta R(x) \approx O(\sigma^2)$
%     \item Sample-limited regime $(d \approx n)$: $\Delta R(x) \approx O(\sqrt{d/n})$
% \end{enumerate}
% \end{proof}

% \begin{proof}
% Step 1: Original decomposition as in your proof:
% \[
% \Delta R(x) = \mathbb{E}_{c \sim p(c|x)} \big[\hat{c}^\top \big(w^\star(\hat{c}_n) - w^\star(c)\big)\big] - \hat{c}^\top \big(w^\star(\hat{c}_n) - w^\star(c)\big) + \mathbb{E}_{c \sim p(c|x)} \big[\epsilon^\top \big(w^\star(\hat{c}_n) - w^\star(c)\big)\big]
% \]

% Step 2: Decompose estimation error:
% \[
% \hat{c}_n = \hat{c} + (\hat{c}_n - \hat{c})
% \]
% where $\hat{c}_n - \hat{c}$ is the estimation error.

% Step 3: By Hoeffding's inequality, with probability at least $1-\delta$:
% \[
% \|\hat{c}_n - \hat{c}\| \leq C\sqrt{\frac{d\log(1/\delta)}{n}}
% \]

% Step 4: Using triangle inequality:
% \[
% \|w^\star(\hat{c}_n) - w^\star(c)\| \leq L_w(\|\hat{c}_n - \hat{c}\| + \|\hat{c} - c\|)
% \]

% Step 5: The term $\|\hat{c} - c\|$ can be decomposed into:
% \[
% \|\hat{c} - c\| \leq \|\text{Bias}[\hat{c}|x]\| + \|\text{Var}[c|x]\|_F
% \]

% Step 6: Combining these bounds:
% \[
% \Delta R(x) \leq L_w\|\text{Var}[c|x]\|_F(1 + \|\hat{c}\|) + L_w\|\text{Bias}[\hat{c}|x]\| + L_wC\sqrt{\frac{d}{n}}
% \]

% where:
% - $\|\text{Var}[c|x]\|_F = \|LL^T + \sigma^2I_d\|_F \leq \frac{C_1k}{\sqrt{d}} + \sigma^2$
% - The bias term captures model misspecification
% - The $\sqrt{\frac{d}{n}}$ term comes from finite sample estimation
% \end{proof}

\subsection{CVaR/Quantile Regression}

%=========================================================
\begin{theorem}[Finite-Sample Bound for CVaR Estimation]
\label{thm:unconditional_CVaR_bound}
Suppose $Y$ takes values in the interval $[m, M]$.  Let $\widehat{\mathrm{CVaR}}_{\alpha}$ be the empirical estimator derived from 
\[
  \hat{\phi}_n(\eta)
  \;=\;
  \eta \;+\;\frac{1}{\alpha}\,\frac{1}{n}\sum_{i=1}^n (Y_i - \eta)_+,
  \quad
  \widehat{\mathrm{CVaR}}_{\alpha}
  \;=\;
  \inf_{\eta \in \mathbb{R}} \,\hat{\phi}_n(\eta),
\]
where $(y - \eta)_+ \coloneqq \max\{y - \eta,0\}$ and $Y_1,\dots,Y_n$ are i.i.d.\ samples of $Y$. 
Then there is a universal constant $C>0$ such that for all $\delta>0$, with probability at least $1-\delta$,
\[
  \bigl|\,\widehat{\mathrm{CVaR}}_\alpha \;-\; \mathrm{CVaR}_\alpha(Y)\bigr|
  \;\le\;
  C\,\frac{(M-m)}{\alpha}\,\sqrt{\frac{\ln(1/\delta)}{n}}.
\]
In other words, the estimation error for $\mathrm{CVaR}_{\alpha}$ converges on the order of 
$\sqrt{\ln(1/\delta) / n}$ as $n$ grows.
\end{theorem}
%=========================================================

\begin{remark}
Here, $\mathrm{CVaR}_{\alpha}(Y) = \mathbb{E}[\,Y \mid Y \le \mathrm{VaR}_{\alpha}(Y)\,]$, and
\[
  \mathrm{VaR}_{\alpha}(Y) 
  \;=\; 
  \inf\{\,t : \Pr(Y \le t)\;\ge\;\alpha\}.
\]
The key step in the proof is the Rockafellar--Uryasev identity,
\[
  \mathrm{CVaR}_\alpha(Y) 
  \;=\;
  \inf_{\eta \in \mathbb{R}}
  \Bigl\{
    \eta \;+\;\tfrac{1}{\alpha}\,\mathbb{E}\bigl[(\,Y-\eta\,)_+\bigr]
  \Bigr\},
\]
combined with uniform convergence arguments (e.g.\ Hoeffding or Rademacher complexity bounds).
\end{remark}

\begin{proof}
\textbf{Step 1: Rockafellar--Uryasev Representation.}

Recall the identity (Rockafellar--Uryasev):
\[
  \mathrm{CVaR}_\alpha(Y) 
  \;=\;
  \min_{\eta \in \mathbb{R}}
  \Bigl(
    \eta \;+\; \frac{1}{\alpha}\,\mathbb{E}\bigl[(\,Y-\eta\,)_+\bigr]
  \Bigr).
\]
Set 
\[
  \phi(\eta) 
  \;=\;
  \eta \;+\;\frac{1}{\alpha}\,\mathbb{E}[(\,Y-\eta\,)_+].
\]
Then $\mathrm{CVaR}_\alpha(Y) = \min_{\eta \in \mathbb{R}}\, \phi(\eta)$.

\noindent
\textbf{Step 2: Empirical Estimator.}

Given i.i.d.\ samples $Y_1,\dots,Y_n$, define the empirical counterpart
\[
  \hat{\phi}_n(\eta)
  \;=\;
  \eta 
  \;+\;
  \frac{1}{\alpha}\,\frac{1}{n}\,\sum_{i=1}^n (\,Y_i - \eta\,)_+,
\]
and let
\[
  \widehat{\mathrm{CVaR}}_{\alpha} 
  \;=\;
  \min_{\eta\in\mathbb{R}}\;\hat{\phi}_n(\eta).
\]
Similarly, let $\eta^* \in \arg\min_{\eta}\phi(\eta)$ and 
$\hat{\eta}_n \in \arg\min_{\eta}\hat{\phi}_n(\eta)$.

\noindent
\textbf{Step 3: Uniform Convergence.}

Observe that
\[
  |\hat{\phi}_n(\eta) - \phi(\eta)|
  \;=\;
  \Bigl|\,
    \frac{1}{\alpha} \bigl(\tfrac{1}{n}\sum_{i=1}^n (\,Y_i - \eta\,)_+ - 
    \mathbb{E}[(\,Y-\eta\,)_+]\bigr)
  \Bigr|
  \;\le\;
  \frac{1}{\alpha}
  \sup_{\eta\in\mathbb{R}}
  \Bigl|
    \tfrac{1}{n}\sum_{i=1}^n f_\eta(Y_i)
    \;-\;
    \mathbb{E}[\,f_\eta(Y)\bigr]
  \Bigr|,
\]
where $f_\eta(y) \coloneqq (y-\eta)_+$ is bounded by $(M-m)$ if $y\in[m,M]$.  
By standard Hoeffding (or VC / Rademacher) arguments, with probability $\ge1-\delta$,
\[
  \sup_{\eta\in\mathbb{R}}
  \Bigl|
    \tfrac{1}{n}\sum_{i=1}^n (Y_i - \eta)_+ 
    \;-\;
    \mathbb{E}[(Y-\eta)_+]
  \Bigr|
  \;\le\;
  C_1\,(M-m)\,\sqrt{\frac{\ln(1/\delta)}{n}}
\]
for some universal constant $C_1>0$.  
Hence,
\[
  \sup_{\eta\in\mathbb{R}}
  \bigl|\hat{\phi}_n(\eta) - \phi(\eta)\bigr|
  \;\le\;
  \frac{C_1\,(M-m)}{\alpha}\,\sqrt{\frac{\ln(1/\delta)}{n}}
  \;=\;: \varepsilon_n.
\]

\noindent
\textbf{Step 4: Error Between Minimizers.}

By definition of $\hat{\eta}_n$ and $\eta^*$,
\[
  \hat{\phi}_n(\hat{\eta}_n)
  \;\le\;
  \hat{\phi}_n(\eta^*).
\]
Also,
\[
  \phi(\hat{\eta}_n) - \phi(\eta^*)
  \;\le\;
  \bigl[\,\hat{\phi}_n(\hat{\eta}_n) - \phi(\hat{\eta}_n)\bigr]
  \;+\;
  \bigl[\,\hat{\phi}_n(\eta^*) - \phi(\eta^*)\bigr]
  \;\le\; 2\,\varepsilon_n.
\]
Thus
\[
  \phi(\hat{\eta}_n)
  \;\le\;
  \phi(\eta^*) + 2\,\varepsilon_n
  \;\Longrightarrow\;
  \hat{\phi}_n(\hat{\eta}_n)
  \;=\;
  \phi(\hat{\eta}_n)
  +\bigl[\hat{\phi}_n(\hat{\eta}_n)- \phi(\hat{\eta}_n)\bigr]
  \;\le\;
  \phi(\eta^*) + 3\,\varepsilon_n.
\]
Similarly, by symmetry, we get $\phi(\eta^*) \le \hat{\phi}_n(\hat{\eta}_n) + 3\,\varepsilon_n$,
so
\[
  \bigl|\hat{\phi}_n(\hat{\eta}_n) - \phi(\eta^*)\bigr|
  \;\le\; 3\,\varepsilon_n.
\]
Since $\mathrm{CVaR}_\alpha(Y)=\phi(\eta^*)$ and 
$\widehat{\mathrm{CVaR}}_{\alpha}=\hat{\phi}_n(\hat{\eta}_n)$, we conclude
\[
  \bigl|\widehat{\mathrm{CVaR}}_{\alpha} - \mathrm{CVaR}_\alpha(Y)\bigr|
  \;\le\; 3\,\varepsilon_n
  \;\;=\;
  \mathcal{O}\!\Bigl(\tfrac{M-m}{\alpha}\,\sqrt{\tfrac{\ln(1/\delta)}{n}}\Bigr).
\]
Finally, we absorb constant factors into a single $C$, yielding the stated bound.
\end{proof}

%=========================================================
\begin{theorem}[Generalization Bound for Conditional CVaR Estimation]
\label{thm:conditional_CVaR_bound}
Let $(X,Y)$ be distributed on $\mathcal{X}\times\mathbb{R}$, and let
$\mathcal{G}$ be a class of measurable functions $g:\mathcal{X}\to \mathbb{R}$.
Define the population Rockafellar--Uryasev (RU) risk of any predictor $g$ by
\[
  R(g) 
  \;\coloneqq\;
  \mathbb{E}\!\Bigl[
    g(X) 
    \;+\;
    \frac{1}{\alpha}\,\bigl(Y - g(X)\bigr)_{+}
  \Bigr],
\]
and let
\[
  R^* 
  \;=\;
  \inf_{g \in \mathcal{G}}\,R(g), 
  \quad
  g^*\in \arg\min_{g\in \mathcal{G}}\;R(g).
\]
Given i.i.d.\ samples $\{(x_i,y_i)\}_{i=1}^n$, define the empirical RU risk
\[
  \widehat{R}_n(g)
  \;\coloneqq\;
  \frac{1}{n}\,\sum_{i=1}^n\Bigl[
    g(x_i) + \tfrac{1}{\alpha}\,\bigl(y_i - g(x_i)\bigr)_{+}
  \Bigr],
\]
and let
\[
  \hat{g}_n 
  \;\;\in\;
  \arg\min_{g\in\mathcal{G}} 
  \;\widehat{R}_n(g).
\]
Suppose that, with probability at least $1-\delta$, 
\[
   \sup_{g\in \mathcal{G}}
   \Bigl|
      \widehat{R}_n(g) - R(g)
   \Bigr|
   \;\;\le\;\;
   \varepsilon_n,
\]
where $\varepsilon_n$ is a term that typically of the order $\mathcal{O}\!\Bigl(\tfrac{1}{\alpha}\,\sqrt{\tfrac{\ln(1/\delta)}{n}}\Bigr)$ 
under standard assumptions (boundedness, sub-Gaussian tails, etc.).  
Then on that event,
\[
  R(\hat{g}_n) 
  \;-\; 
  R^*
  \;\;\le\;\;
  2\,\varepsilon_n.
\]
Hence the learned predictor $\hat{g}_n$ achieves a CVaR-type risk 
within $2\,\varepsilon_n$ of the best $g^*\in \mathcal{G}$, with high probability.
\end{theorem}
%=========================================================

% \begin{remark}
% If $\mathcal{G}$ is sufficiently rich (e.g., large neural networks or universal approximators),
% then $R^*$ is close to the true \emph{conditional} CVaR function 
% $x\mapsto \mathrm{CVaR}_\alpha(Y \mid X=x)$. 
% In simpler function classes (e.g.\ linear or parametric), there may be an additional
% \emph{approximation gap} if the true conditional CVaR is not well-represented by $\mathcal{G}$.
% \end{remark}

\begin{proof}
\textbf{Step 1: Setup \& Definitions.}

For each $g\in \mathcal{G}$, define the population RU risk
\[
  R(g) 
  \;=\;
  \mathbb{E}\Bigl[
    g(X) + \tfrac{1}{\alpha}\,\bigl(Y - g(X)\bigr)_{+}
  \Bigr].
\]
The empirical counterpart based on samples $(x_i,y_i)_{i=1}^n$ is
\[
  \widehat{R}_n(g)
  \;=\;
  \frac{1}{n}\,\sum_{i=1}^n 
  \bigl[
    g(x_i) 
    \;+\;
    \tfrac{1}{\alpha}\,(y_i - g(x_i))_+
  \bigr].
\]
Let 
\[
  \hat{g}_n \;\in\; \arg\min_{g \in \mathcal{G}}\;\widehat{R}_n(g),
  \quad
  g^* \;\in\; \arg\min_{g \in \mathcal{G}}\;R(g).
\]

\noindent
\textbf{Step 2: Decompose the Excess Risk.}

We want $R(\hat{g}_n) - R(g^*)$.  
Note that
\[
   R(\hat{g}_n) \;-\; R(g^*)
   \;=\;
   \underbrace{\bigl[R(\hat{g}_n) - \widehat{R}_n(\hat{g}_n)\bigr]}_{(A)}
   \;+\;
   \underbrace{\bigl[\widehat{R}_n(\hat{g}_n) - \widehat{R}_n(g^*)\bigr]}_{(B)}
   \;+\;
   \underbrace{\bigl[\widehat{R}_n(g^*) - R(g^*)\bigr]}_{(C)}.
\]
Since $\hat{g}_n$ minimizes $\widehat{R}_n$, the middle term $(B)\le 0$.  Hence
\[
   R(\hat{g}_n) - R(g^*)
   \;\le\;
   (A) + (C).
\]
But
\[
   (A) 
   \;=\;
   R(\hat{g}_n) - \widehat{R}_n(\hat{g}_n)
   \;\le\;
   \sup_{g\in\mathcal{G}}\bigl|\,R(g) - \widehat{R}_n(g)\bigr|,
\]
and similarly 
\[
   (C)
   \;=\;
   \widehat{R}_n(g^*) - R(g^*)
   \;\le\;
   \sup_{g\in\mathcal{G}}\bigl|\,\widehat{R}_n(g) - R(g)\bigr|.
\]
Therefore,
\[
   R(\hat{g}_n) - R(g^*)
   \;\le\;
   2\,\sup_{g\in \mathcal{G}}
   \Bigl|
      \widehat{R}_n(g) - R(g)
   \Bigr|.
\]

\noindent
\textbf{Step 3: Uniform Convergence Bound.}

By hypothesis (or by a standard Rademacher / VC argument), we have
\[
  \sup_{g\in \mathcal{G}}
  \bigl|\widehat{R}_n(g) - R(g)\bigr|
  \;\le\;
  \varepsilon_n,
\]
with probability $\ge 1-\delta$, where $\varepsilon_n$ grows at a rate of $\mathcal{O}\bigl(\tfrac{1}{\alpha}\sqrt{\tfrac{\ln(1/\delta)}{n}}\bigr)$.  
Hence on that event:
\[
   R(\hat{g}_n) - R(g^*)
   \;\le\; 2\,\varepsilon_n.
\]

\noindent
\textbf{Step 4: Why $\varepsilon_n$ Includes a Factor of $1/\alpha$.}

Observe that
\[
  \phi_\alpha(x,y;g)
  \;=\;
  g(x) 
  \;+\;
  \frac{1}{\alpha}(y-g(x))_+.
\]
Because it is scaled by $\frac{1}{\alpha}$, any standard concentration bound (e.g.\ Hoeffding or Rademacher) for $\phi_\alpha$ incurs an extra factor of $1/\alpha$.  Specifically:

\begin{itemize}
\item \emph{Boundedness:} 
If $|g(x)|\le G_{\max}$ and $|y|\le Y_{\max}$, then $(y-g(x))_+\le |\,y-g(x)\,|\le Y_{\max}+G_{\max}$.  Hence 
$
  \phi_\alpha(x,y;g) 
  \le 
  G_{\max} + \tfrac{1}{\alpha}(Y_{\max}+G_{\max}).
$
\item \emph{Rademacher complexity or Hoeffding:} 
A uniform‐convergence or covering‐number argument yields a $\sqrt{\frac{\ln(1/\delta)}{n}}$ factor multiplied by the supremum of $|\phi_\alpha|$, which is $\le \frac{C}{\alpha}$ for some constant $C$.
\end{itemize}
Thus $\varepsilon_n$ \emph{necessarily} scales like $\frac{1}{\alpha}\sqrt{\frac{\ln(1/\delta)}{n}}$ (up to constants and possibly adding a $\mathfrak{R}_n(\mathcal{G})$ term if $\mathcal{G}$ is large).

\end{proof}



%=============================================

\begin{theorem}[High-Dimensional Conditional CVaR Generalization Bound]
\label{thm:highdim_conditional_CVaR}
Let $(X,Y)$ be a random pair taking values in $\mathbb{R}^{d_x}\times \mathbb{R}^{d_y}$,
and let $\alpha\in(0,1)$ be fixed.
Suppose we have:
\begin{itemize}
\item A \emph{scalar loss} $\ell: \mathbb{R}\times \mathbb{R}^{d_y}\to \mathbb{R}$,
\item A hypothesis class $\mathcal{G}$ of measurable functions $g:\mathbb{R}^{d_x}\to\mathbb{R}$,
\end{itemize}
and define the \emph{Rockafellar--Uryasev (RU) risk} of any predictor $g\in \mathcal{G}$ by
\[
  R(g) 
  \;\coloneqq\;
  \mathbb{E}\!\Bigl[
    g(X) 
    \;+\;
    \frac{1}{\alpha}\,\Bigl(\,\ell\bigl(g(X),Y\bigr) - g(X)\Bigr)_{+}
  \Bigr].
\]
Let $R^*=\inf_{g\in \mathcal{G}} R(g)$, and choose $g^*$ such that $R(g^*)=R^*$.
Given $n$ i.i.d.\ samples $\{(x_i,y_i)\}_{i=1}^n\subset \mathbb{R}^{d_x}\times \mathbb{R}^{d_y}$,
define the \emph{empirical} RU risk
\[
  \widehat{R}_n(g)
  \;\coloneqq\;
  \frac{1}{n}\,\sum_{i=1}^n
    \Bigl[
      g(x_i) 
      \;+\;
      \frac{1}{\alpha}\,\bigl(\ell(g(x_i),\,y_i) - g(x_i)\bigr)_{+}
    \Bigr],
\]
and let $\hat{g}_n \in \arg\min_{g\in \mathcal{G}}\,\widehat{R}_n(g)$.
Assume that with probability at least $1-\delta$, we have a uniform-convergence bound
\[
   \sup_{g\in \mathcal{G}}
   \Bigl|\widehat{R}_n(g) - R(g)\Bigr|
   \;\;\le\;\;
   \varepsilon_n,
\]
where $\varepsilon_n$ scales as
\[
   \varepsilon_n 
   \;=\; 
   \widetilde{\mathcal{O}}
   \!\Bigl(
     \tfrac{1}{\alpha}\,\sqrt{\tfrac{d_x + d_y}{n}}
   \Bigr),
\]
under suitable boundedness/sub-Gaussian assumptions on $(X,Y)$ and $\ell$. Then on that event,
\[
  R(\hat{g}_n) 
  \;-\; 
  R^*
  \;\;\le\;\;
  2\,\varepsilon_n.
\]
Hence the learned predictor $\hat{g}_n$ achieves a CVaR-type risk 
within $2\,\varepsilon_n$ of the best $g^*\in \mathcal{G}$, with high probability.
\end{theorem}

\subsection{\texttt{Gen-DFL} vs Pred-DFL}

\begin{definition}
Let $p(c|x)$ denote the true conditional distribution of $c$, 
and let $p_\theta(c|x)$ be the generative model.
We define $Q_c$ to be the “worst $\alpha\%$ tail” representative for $c$ under $p(c|x)$ based on the target decision $w^\star$. Formally, 
% \[Q_c[\alpha] \coloneqq \mathbb{E}[\,c \mid c^\top w^\star \ge \mathrm{VaR}_\alpha(f(c, w^\star))\,].\]
\[
Q_c[\alpha] \coloneqq \mathbb{E}[c \mid f(c, w^\star) \ge \mathrm{VaR}_\alpha].
\]
\end{definition}

\begin{definition}
Given the target decision $w^\star$, the decision $w^\star_{pred}$ found by Pred-DFL and the decision $w^\star_{\theta}$ found by \texttt{Gen-DFL}, we can define the regret of Pred-DFL formally as:
% \[
%   R_{\mathrm{pred}}(x;\alpha)
%   =
%   \,f\bigl(Q_c[\alpha], \bigl(w_{\mathrm{pred}}^\star - w^\star[\alpha])\bigr).
% \]
\[
  R_{\mathrm{pred}}(x;\alpha)
  =
  f(Q_c[\alpha], w_{\mathrm{pred}}^\star) - f(Q_c[\alpha], w^\star).
\]
and the regret of \texttt{Gen-DFL} as:
\[
  R_\theta(x;\alpha)
  \;=\;
  \mathrm{CVaR}_{p(c \mid x)}
  \Bigl[
    f\bigl(c, w^\star_\theta\bigr) - f\bigl(c, w^\star\bigr)
    ;\,\alpha
  \Bigr].
\]
\end{definition}

% \begin{theorem}
% \label{theorem:ultimate_nonlinear}
% Let $p(c|x)$ be the true conditional distribution of $c$, and denote by $p_\theta(c|x)$ the learned generative model. Let $g(x)$ be a predictor that estimates $\mathbb{E}_{c\sim p(c|x)}[c|x]$.


% Assume:
% \begin{enumerate}
%     \item The objective function $f(c, w^\star)$ is Lipschitz continuous in both $c$ and $w$ with constant $L_c$ and $L_w$, that is,
%     \[
%     \bigl\|f(c_0, w^\star) - f(c_1, w^\star)\bigr\| \;\le\; L_c \,\|c_1 - c_2\|, \qquad 
%     \bigl\|f(c, w_0^\star) - f(c, w_1^\star)\bigr\| \;\le\; L_w \,\|w_0^\star - w_1^\star\|.
%     \]
%     \item The conditional distribution $p(c|x)$ is given by a factor model
%     \[
%     c \;=\; \bar{c} \;+\; L f \;+\; \sigma\,\xi,
%     \]
%     where $\bar{c} = \mathbb{E}_{c\sim p(c|x)}[c \mid x]$, $L\in\mathbb{R}^{d\times k}$ is a low-rank loading matrix, $f \sim \mathcal{N}(0,I_k)$ (systematic factor), and $\xi \sim \mathcal{N}(0,I_d)$ (idiosyncratic noise). Hence,
%     \[
%     \text{Var}[c \mid x] \;=\; L\,L^\top \;+\; \sigma^2\,I.
%     \]
%     \item The \emph{bias} of $g(x)$ is 
%     \[
%     \mathrm{Bias}[g] 
%     \;=\; 
%     g(x) \;-\; \bar{c}.
%     \]
%     \item The objective $f(c,w)$ is \emph{$L_f$-smooth} in $c$, i.e., its Hessian in $c$ is bounded: 
%     \[
%     \bigl\|\nabla_c^2 f(c,w)\bigr\| 
%     \;\le\; 
%     L_f,
%     \]
%     for all $(c,w)$ in the relevant domains.
%     \item 
%     We define, \[
%     \text{Gen-DFL: } 
%     J_{\mathrm{gen}}(w) 
%     = \text{CVaR}_{p(c|x)}[\,f(c,w)\bigr],
%     \quad
%     \text{Pred-DFL: }
%     J_{\mathrm{pred}}(w) 
%     = f\bigl(g(x), w\bigr).
%     \]
%     Then, we assume each $J_{\mathrm{gen}}(\cdot)$ and $J_{\mathrm{pred}}(\cdot)$ is $\alpha$-strongly convex in $w$.
% \end{enumerate}

% Then, for
% \[
% \Delta R(x) 
% \;=\; 
% R_{\text{pred}}(x) 
% \;-\;
% R_{\text{gen}}(x),
% \]
% we have an upper bound of the form:
% \begin{align*}
% \mathbb{E}_x|\Delta R(x)| &\le \mathbb{E}_x \Bigl[ L_w \cdot \frac{2}{\alpha}
% \;\bigl[
%   \kappa_1\;\mathcal{W}(p_\theta, p)
%   \;+\;
%   \kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
%   \;+\;
%   \kappa_3\,\|\mathrm{Bias}[g]\|
% \bigr]\\
% &+ 2L_c \bigl| \sqrt{d}\|Var[c\mid x] \|^{1/2} + \text{CVaR}_{p(c|x)}[\|\text{Bias}[g(x)]\|] \bigr|\Bigr].
% \end{align*}
% % where $C_f$ is a constant depending on $L_f$ (the second-order Lipschitz constant of $f$), and $C_2\,\sqrt{\tfrac{d}{n}}$ controls the sampling error between $\hat{p}(c|x)$ and $p(c|x)$.
% \end{theorem}

\begin{theorem}
\label{theorem:ultimate_nonlinear}
% Let
% % $c, x\in \mathcal{X}$, where 
% % $\mathcal{C} \subset\mathcal{R}^{d_c}, \mathcal{X}\subset \mathcal{R}^{d_x}$ be the feasible regions of the parameter space and the input space respectively.
% $d_c$ and $d_x$ denote the dimensions of $\mathcal{C}$ and $\mathcal{X}$, respectively.
Let $g:\mathcal{X} \rightarrow \mathcal{C}$ be the predictor in Pred-DFL. Assume the objective function $f(c,w)$ is Lipschitz continuous for any $c , w$. 
Then, there exists some constants $L_w, L_c, \kappa_1, \kappa_2, \kappa_3$ such that the following upper-bound holds for the aggregated regret gap $\mathbb{E}_x|\Delta R(x)|$:
\begin{align*}
\mathbb{E}_x|\Delta R(x)| &\le \mathbb{E}_x \Bigl[ L_w \cdot \frac{2}{\alpha}
\;\bigl[
  \kappa_1\;\mathcal{W}(p_\theta, p)
  \;+\;
  \kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
  \;+\;
  \kappa_3\,\|\mathrm{Bias}[g]\|
\bigr]\\
&+ 2L_c \bigl| \sqrt{d\|Var[c\mid x] \|} + \text{CVaR}_{p(c|x)}[\|\text{Bias}[g(x)]\|] \bigr|\Bigr].
\end{align*}
% \woody{I don't see a need of using bullet points here.} \prince{Addressed.}
\begin{remark}
Let
% $c, x\in \mathcal{X}$, where 
% $\mathcal{C} \subset\mathcal{R}^{d_c}, \mathcal{X}\subset \mathcal{R}^{d_x}$ be the feasible regions of the parameter space and the input space respectively.
$d_c$ and $d_x$ denote the dimension of $\mathcal{C}$ and $\mathcal{X}$, respectively.
The bias term $||\text{Bias}[g]||$ of the predictor grows at a rate of $\mathcal{O}(\frac{1}{\alpha} \sqrt{(d_x+d_c)/n})$.
% , where $d_x+d_c$ is the dimensionality of the problem. 
This suggests that the smaller the $\alpha$ is, the harder for the predictor in the Pred-DFL to get an accurate estimation of $Q_c[\alpha]$.
\end{remark}

\begin{remark}
We may write $c = \bar{c} + \sigma\epsilon$, where $\bar{c} = \mathbb{E}_{p(c|x)}[c]$. Under some mild assumptions, such as $\epsilon$ being Gaussian, the variance term is of the order $\mathcal{O}(\sigma^2 \,\sqrt{d_c})$.
% \item Generally, the Wasserstein distance scales as $\mathcal{O}(\sqrt{\frac{d_c}{n}})$
\end{remark}
\end{theorem}

\begin{proof}

\textbf{Step 1: Decomposition of the Regret}
\begin{align*}
|\Delta R(x)| &= \big|\text{CVaR}_{p(c|x)}[f(c, w_\theta^\star) - f(c, w^\star);\alpha] - \text{CVaR}_{p(c|x)}[f(c, w_{pred}^\star) - f(c, w^\star);\alpha] \big|\\
&= \big| \text{CVaR}_{p(c|x)}[f(c, w_\theta^\star) - f(c, w_{pred}^\star);\alpha] \big|\\
% &= \text{CVaR}_{p(c|x)}\left[ \left[f(\bar{c}, w_\theta^\star) - f(\bar{c}, w_{pred}^\star)\right] + \left[ f(c, w_\theta^\star) - f(\bar{c}, w_\theta^\star) \right] - \left[ f(c, w_{pred}^\star) - f(\bar{c}, w_{pred}^\star) \right] \right]\\
&= \text{CVaR}_{p(c|x)}[\left[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star)\right] + \left[ f(c, w_\theta^\star) - f(g(x), w_\theta^\star) \right] \\&- [ f(c, w_{pred}^\star) - f(g(x), w_{pred}^\star) ] ;\alpha]\\
&\le \big|\text{CVaR}_{p(c|x)}[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star);\alpha]\big| + 2 L_c \text{CVaR}_{p(c|x)}[||c - g(x)||;\alpha]\\
&= \big|\text{CVaR}_{p(c|x)}[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star);\alpha]\big| + 2 L_c \text{CVaR}_{p(c|x)}[\|c - Q_c[\alpha] + Q_c[\alpha] - g(x)\|;\alpha]\\
& \le \big|\text{CVaR}_{p(c|x)}[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star);\alpha]\big| + 2L_c \bigl| \text{CVaR}_{p(c|x)}[\|c - Q_c[\alpha]\|;\alpha] + \text{CVaR}_{p(c|x)}[\|\text{Bias}[g]\|;\alpha] \bigr|\\
& \le \big|\text{CVaR}_{p(c|x)}[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star);\alpha]\big| + 2L_c \bigl| \sqrt{d\|Var[c\mid x] \|} + \text{CVaR}_{p(c|x)}[\|\text{Bias}[g]\|;\alpha] \bigr|
\end{align*}

where we used the fact $f(c, w_\theta^\star) = f(g(x), w_\theta^\star) + [f(c, w_\theta^\star) - f(g(x), w_\theta^\star)]$ and $f(c, w_{pred}^\star) = f(g(x), w_{pred}^\star) + [f(c, w_{pred}^\star) - f(g(x), w_{pred}^\star)]$.

\textbf{Step 2: Bounding $\Delta_{\mathrm{Term}} = \big|\text{CVaR}_{p(c|x)}[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star);\alpha]\big|$}

Now, we need to bound the $\Delta_{\mathrm{Term}}= \big|\text{CVaR}_{p(c|x)}[f(g(x), w_\theta^\star) - f(g(x), w_{pred}^\star);\alpha]\big|$ term.

By assumption, for any fixed $c_0$, the map $w \mapsto f(c_0, w)$ is $L_w$-Lipschitz in $w$. Equivalently,
\[
\bigl|\,f(c_0, w_1) - f(c_0, w_2)\bigr| 
\;\le\;
L_w\,\|\,w_1 - w_2\|.
\]
Applying this specifically at $c_0 = g(x)$, we get:
\[
\bigl|\,f(g(x), w^\star_\theta) 
      - f(g(x), w_{\mathrm{pred}}^\star)\bigr|
\;\le\;
L_w\,\bigl\|\,w^\star_\theta - w_{\mathrm{pred}}^\star\bigr\|.
\]
Since $\text{CVaR}_{p}[\cdot]$ is merely an expectation that does not affect the integrand here (it does not depend on $c$ anymore), we have
\[
\Delta_{\mathrm{Term}}
% \;=\;
% \bigl|\text{CVaR}_{p(c|x)}[
%   f(g(x), w^\star_\theta) - f(g(x), w_{\mathrm{pred}}^\star)
% ;\alpha]\bigr|
\;\le\;
L_w\,\bigl\|\,
w^\star_\theta - w_{\mathrm{pred}}^\star
\bigr\|.
\]

\textbf{Step 3: Bounding $\|w^\star_\theta - w_{\mathrm{pred}}^\star\|$ }

First, we define the following auxiliary (aggregate objectives) functions for both \texttt{Gen-DFL} and Pred-DFL,

\[
\text{Gen-DFL: } 
J_{\mathrm{gen}}(w) 
= \text{CVaR}_{p_\theta}[\,f(c,w);\alpha \bigr],
\quad
\text{Pred-DFL: }
J_{\mathrm{pred}}(w) 
= f\bigl(g(x), w\bigr).
\]
So
\[
w^\star_\theta 
= \arg\min_{w} J_{\mathrm{gen}}(w),
\quad
w_{\mathrm{pred}}^\star 
= \arg\min_{w} J_{\mathrm{pred}}(w).
\]

Next, let's define 
\[
\Delta(w) 
\;=\;
J_{\mathrm{gen}}(w) - J_{\mathrm{pred}}(w)
\;=\;
\text{CVaR}_{p_\theta}\bigl[f(c,w);\alpha \bigr]
\;-\;
f\bigl(g(x), w\bigr).
\]
We take a uniform bound over $w$:
\[
T 
\;=\;
\sup_{w}
\;\bigl|\Delta(w)\bigr|.
\]
% Typically, $\Delta(w)$ is broken down into terms reflecting
% \begin{itemize}
%     \item $\|p_\theta - p\|$ (model mismatch),
%     \item Lipschitz in $c$ plus variance to compare $\text{CVaR}_{p}[f(c,w);\alpha]$ and $f(Q_c[\alpha],w)$,
%     \item Predictor bias $\|g(x)-Q_c[\alpha]\|$.
% \end{itemize}
We will then show that,
\[
T \;\le\; \kappa_1 \|p_\theta - p\| + \kappa_2 \sqrt{\|\mathrm{Var}[c\mid x]\|} + \kappa_3 \|\mathrm{Bias}[g]\|.
\]

%===================bounding T====================
\textbf{Step 4: Bounding $T$}

By definition,
\[
T 
\coloneqq 
\sup_{w}
\;\Bigl|\,
  \text{CVaR}_{p_\theta}[\,f(c,w);\alpha \bigr]
  -
  f\bigl(g(x), w\bigr)
\Bigr|.
\]
To relate this to the \emph{true} distribution $p$ and $Q_c[\alpha] = \mathbb{E}_{c\sim p}[\,c]$, we can do the following decomposition:
\begin{align*}
\text{CVaR}_{p_\theta}[\,f(c,w);\alpha \bigr]
\;-\;
f(g(x), w)
&\;=\;
  \Bigl(
    \text{CVaR}_{p_\theta}[f(c,w);\alpha ]
    -
    \text{CVaR}_{p}[f(c,w)]
  \Bigr)
\;+\;
  \Bigl(
    \text{CVaR}_{p}[f(c,w);\alpha]
    -
    f(Q_c[\alpha], w)
  \Bigr)\\
&\;+\;
  \Bigl(
    f(Q_c[\alpha], w)
    -
    f(g(x), w)
  \Bigr).
\end{align*}
Hence, if we set
\[
T 
= 
\sup_{w} 
\bigl|\text{(A)} + \text{(B)} + \text{(C)}\bigr|,
\]
then by triangle inequality:
\[
T \;\le\; 
\underbrace{
  \sup_{w} \bigl|\text{(A)}\bigr|
}_{T_1}
\;+\;
\underbrace{
  \sup_{w} \bigl|\text{(B)}\bigr|
}_{T_2}
\;+\;
\underbrace{
  \sup_{w} \bigl|\text{(C)}\bigr|
}_{T_3}.
\]
We now bound each piece $T_1, T_2, T_3$ separately.

First, we can see that
\[
T_1 = \sup_{w}\bigl|\text{CVaR}_{p_\theta}[f(c,w);\alpha ]
    -
    \text{CVaR}_{p}[f(c,w)]\bigr|
\;\le\;
% \kappa_1\;\|p_\theta - p\|,
\kappa_1\;\mathcal{W}(p_\theta, p),
\]
where $\kappa_1$ depends on the Lipschitz constant of $f$ in $c$.



% \begin{itemize}
% \item \emph{First-order expansion} around $Q_c[\alpha]$, ignoring the linear term since $\mathbb{E}[c-Q_c[\alpha]]=0$, and bounding second-order remainders via $\mathrm{Var}[c\mid x]$,
% \item or \emph{Lipschitz in $c$} so that $\bigl|f(c,w) - f(Q_c[\alpha],w)\bigr|\le L_c\|\,c-Q_c[\alpha]\|$ and then bounding $\mathbb{E}\|c-Q_c[\alpha]\|\lesssim \sqrt{d\|\mathrm{Var}[c\mid x]\|}$.
% \end{itemize}

% Assume \emph{Lipschitz in $c$} so that $\bigl|f(c,w) - f(Q_c[\alpha],w)\bigr|\le L_c\|\,c-Q_c[\alpha]\|$ and then bounding $\text{CVaR}[\|c-Q_c[\alpha]\|;\alpha]\lesssim \sqrt{d\|\mathrm{Var}[c\mid x]\|}$
Next, by taking the Taylor expansion, we have
\[
f(c,w) = f(g(x), w) + \nabla_c f(g(x), w)^T (c - g(x)) + \frac{1}{2}(c - g(x))^T \nabla_c^2 f(g(x), w)(c - g(x)) + \mathcal{O}(||c - g(x)||^2)
\]
After taking the CVaR expectation, we see that
\[
T_2 =
\sup_{w}
\bigl|\text{CVaR}_{p}[f(c,w);\alpha] - f(g(x), w)\bigr|
\;\le\;
\kappa_2 \sqrt{\|\mathrm{Var}[c\mid x]\|},
\]
where $\kappa_2$ incorporates the Lipschitz constant.

Finally, for $T_3$, assuming that $f(\cdot, w)$ is Lipschitz in $c$, then
% \[
% T_3
% = 
% \sup_{w}
% \bigl|f(Q_c[\alpha], w) - f(g(x), w)\bigr|.
% \]
% Again, if $f(\cdot,w)$ is $L_c$-Lipschitz in $c$, then
\[
T_3 = \sup_w\bigl|f(Q_c[\alpha], w) - f(g(x), w)\bigr|
\;\le\;
L_c \,\|Q_c[\alpha] - g(x)\|
\;\le\;
L_c\;\|\mathrm{Bias}[g]\|.
\]
Hence,
\[
T_3
\;\le\;
\kappa_3\;\|\mathrm{Bias}[g]\|.
\]

\textbf{Combining all the steps.}

Collecting $T_1, T_2, T_3$:

\[
\begin{aligned}
T
&=\;
\sup_{w}
\Bigl|
  \text{CVaR}_{p_\theta}[f(c,w);\alpha]
  -
  f(g(x), w)
\Bigr|
\;\;\le\;\;
T_1 + T_2 + T_3
\\[6pt]
&\;\le\;
\kappa_1\;\mathcal{W}(p_\theta, p)
\;+\;
\kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
\;+\;
\kappa_3\,\|\mathrm{Bias}[g]\|.
\end{aligned}
\]
Thus,
\[
\boxed{%
T
\;\;\le\;\;
\kappa_1\;\mathcal{W}(p_\theta, p)
\;+\;
\kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
\;+\;
\kappa_3\,\|\mathrm{Bias}[g]\|.
}
\]
%===================bounding T====================

\textbf{Strong Convexity in $w$ Yields Solution Stability.}

Assume $J_{\mathrm{gen}}(\cdot)$ and $J_{\mathrm{pred}}(\cdot)$ are $\alpha$-strongly convex in $w$. Then, 
% a standard perturbation/sensitivity analysis gives
\[
\bigl\|
  w^\star_\theta - w_{\mathrm{pred}}^\star
\bigr\|
\;\;\le\;\;
\frac{2}{\alpha}\;
\sup_{w} \bigl|\Delta(w)\bigr|
\;=\;
\frac{2}{\alpha}\;T.
\]
Therefore,
\[
\bigl\|
  w^\star_\theta - w_{\mathrm{pred}}^\star
\bigr\| \le \frac{2}{\alpha}(\kappa_1\;\mathcal{W}(p_\theta, p)
\;+\;
\kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
\;+\;
\kappa_3\,\|\mathrm{Bias}[g]\|).
\]

\textbf{Combining all the steps}
\[
\Delta_{\mathrm{Term}}
=\;
\bigl|\text{CVaR}_{p}[f(g(x), w^\star_\theta)
    -
    f(g(x), w_{\mathrm{pred}}^\star);\alpha]\bigr|
\;\le\;
L_w\;\bigl\|\,w^\star_\theta - w_{\mathrm{pred}}^\star\bigr\|
\;\le\;
L_w \,\frac{2}{\alpha}\;T,
\]
Therefore,
\[
\Delta_{\mathrm{Term}}
\;\;\le\;\;
L_w \cdot \frac{2}{\alpha}
\;\bigl[
  \kappa_1\;\mathcal{W}(p_\theta, p)
  \;+\;
  \kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
  \;+\;
  \kappa_3\,\|\mathrm{Bias}[g]\|
\bigr].
\]
% This statement is \emph{non-trivial} because it explicitly shows how (1)~the difference in solutions depends on $\|p_\theta-p\|$, (2)~predictor bias $\|g(x)-Q_c[\alpha]\|$, and (3)~the variance/tail of $p(c\mid x)$, \emph{combined} with strong-convex geometry (through factors $\alpha$ and $L_w$).

% \paragraph{Interpretation.}
% When
% \begin{itemize}
%     \item $p_\theta$ is \emph{accurate} ($\|p_\theta - p\|$ is small),
%     \item $g(x)$ is \emph{unbiased or low-bias},
%     \item $f$ is strongly convex and Lipschitz,
%     \item $\mathrm{Var}[c\mid x]$ is moderate (sub-Gaussian or bounded tails),
% \end{itemize}
% then the bound indicates $w^\star_\theta \approx w_{\mathrm{pred}}^\star$ in a \emph{quantitative} way, and hence 
% $\bigl|f(g(x), w^\star_\theta) - f(g(x), w_{\mathrm{pred}}^\star)\bigr|$ is small.

Finally, we get,
\begin{align*}
\mathbb{E}_x|\Delta R(x)| &\le \mathbb{E}_x \Bigl[ L_w \cdot \frac{2}{\alpha}
\;\bigl[
  \kappa_1\;\mathcal{W}(p_\theta, p)
  \;+\;
  \kappa_2\,\sqrt{\|\mathrm{Var}[c\mid x]\|}
  \;+\;
  \kappa_3\,\|\mathrm{Bias}[g]\|
\bigr]\\
&+ 2L_c \bigl| \sqrt{d\|Var[c\mid x] \|} + \text{CVaR}_{p(c|x)}[\|\text{Bias}[g(x)]\|] ; \alpha\bigr|\Bigr].
\end{align*}

Moreover, by Theorem~\ref{thm:highdim_conditional_CVaR} that we developed earlier, we can see the bias term $||\text{Bias[g]}||$ grows at a rate of $\mathcal{O}(\frac{1}{\alpha}\sqrt{(d_x + d_c)/n})$

\end{proof}



% ----------------algorithm box 1----------------

\section{Experimental Setups}
\label{app:experiments}

\subsection{Synthetic: Portfolio Optimization}

In the Portfolio experiment, we generate the synthetic data as follows:
\[
x_i \sim \mathcal{N}(0, I^{d_x}),
\]
\[
\mathbf{B}_{ij} \sim \text{Bernoulli}(0.5), 
\]
\[
L_{ij} \sim \text{Uniform}[-0.0025\sigma, 0.0025\sigma]
\]
\[
\epsilon \sim \mathcal{N}(0, I^{d_c})
\]
\[
\bar{c}_{ij} = \left( \frac{0.05}{\sqrt{p}} \mathbf{B} x_i + 0.1 \right)^{deg} + Lf + 0.01\sigma\epsilon,
\]
where $d_x, d_c$ are the dimensionality of the input features $x$ and the cost vector $c$.
The polynomial degree reflects the level of non-linearity between the feature and the price vector. In Portfolio, $c$ represents the asset prices and the dimension of $c_i$ is the number of assets.

The non-linear, risk-sensitive optimization problem in Portfolio Management is then formulated as,
\begin{equation}
\begin{aligned}
w^\star(x) &\coloneqq \min_{w} \text{CVaR}_{p(c|x)} [-c^T w + w^T \Sigma w;\alpha] \\ 
\text{s.t.} \ & w \in [0,1]^n, \ \mathbf{1}^T w \leq 1,
\end{aligned}    
\end{equation}
where $\Sigma=LL^T + (0.01\sigma)^2 I$ is the covariance over the asset prices $c$, and the quadratic term $w^T \Sigma w$ reflects the amount of risk.

\subsection{Synthetic: Fractional Knapsack}

In the Knapsack experiment, we generate the synthetic data as follows:
\[
x_i \sim \mathcal{N}(0, I^{d_x}),
\]
\[
\mathbf{B}_{ij} \sim \text{Bernoulli}(0.5), 
\]
\[
L_{ij} \sim \text{Uniform}[-0.0025\sigma, 0.0025\sigma]
\]
\[
\epsilon \sim \mathcal{N}(0, I^{d_c})
\]
\[
\bar{c}_{ij} = \left( \frac{0.05}{\sqrt{p}} \mathbf{B} x_i + 0.1 \right)^{deg} + Lf + 0.01\sigma\epsilon,
\]
where $d_x, d_c$ are the dimensionality of the input features $x$ and the cost vector $c$.

The optimization problem in Knapsack is formulated as:

\begin{equation}
\begin{aligned}
w^\star(x) & \coloneqq \min_{w} \text{CVaR}_{p(c|x)} [-c^T w;\alpha] \\
\text{s.t.} \ & w \in [0,1]^n, p^T w \leq \mathbf{B}, 
\end{aligned}    
\end{equation}
where $ p \in \mathbb{R}^n $ and $ \mathbf{B} > 0 $ represent the capacity and weight vector, respectively.

% For this experiment, the synthetic datasets consist of feature vectors $ x_i $ and cost coefficients $ c_i $. The feature vector $ x_i \in \mathbb{R}^p $ follows a standard multivariate Gaussian distribution $ \mathcal{N}(0, I) $, and the corresponding costs $ c_i \in \mathbb{R}^d $ are generated from a polynomial function $ f(x_i) $ multiplied by a random noise $ \epsilon_i \sim U(1 - \overline{\epsilon}, 1 + \overline{\epsilon}) $, where $ \overline{\epsilon} $ is the noise half-width. The general parameters of the synthetic dataset include the data size $ n $, feature dimension $ p $, polynomial degree of $ f(x_i) $, and the noise width $ \overline{\epsilon} $. Full details of this data synthesis process are provided in the Appendix.

\subsection{Synthetic: Shortest-Path}

In the Shortest-Path experiment, we generate the synthetic data as follows:
\[
x_i \sim \mathcal{N}(0, I^{d_x}),
\]
\[
\mathbf{B}_{ij} \sim \text{Bernoulli}(0.5), 
\]
% \[
% L_{ij} \sim \text{Uniform}[-0.0025\sigma, 0.0025\sigma]
% \]
\[
\epsilon_{ij} \sim \text{Uniform}[0.5, 1.5]
\]
\[
\bar{c}_{ij} = \left[ \frac{1}{3.5^{deg}} \left( \frac{1}{\sqrt{p}} \mathbf{B} x_i + 3 \right)^{deg} + 1 \right] \cdot \epsilon_i^j,
\]
where $d_x, d_c$ are the dimensionality of the input features $x$ and the cost vector $c$.
The polynomial degree reflects the level of non-linearity between the feature and the price vector.

The optimization problem in Shortest-Path is formulated as:
\begin{equation}
\begin{aligned}
w^\star(x) & \coloneqq \min_{w} \text{CVaR}_{p(c|x)} [c^T w;\alpha] \\
\text{s.t.}\ & w \in [0,1]^n, 
\end{aligned}    
\end{equation}
where $ c^T w $ represents the cost of the selected path, and the cost vector $ c_i^j $ is defined as follows:

\[
c_i^j = \left[ \frac{1}{3.5^{deg}} \left( \frac{1}{\sqrt{p}} \mathbf{B} x_i + 3 \right)^{deg} + 1 \right] \cdot \epsilon_i^j,
\]
where $ \mathbf{B} $ is a random matrix, and $ \epsilon_i^j $ is the noise component. 

The features $ x_i \in \mathbb{R}^{d_x} $ follow a standard multivariate Gaussian distribution, and the uncertain coefficients $ c_i^j $ exist only on the objective function, meaning that the weights of the items remain fixed throughout the dataset. The parameters include the dimension of resources $ k $, the number of items $ m $, and the noise width.



% \subsection{Synthetic: Traveling Salesperson}

% Lastly, we study the Traveling Salesperson Problem (TSP), where the goal is to minimize the total distance traveled. The optimization problem is formulated as:

% \[
% w^\star(x) \coloneqq \min_{w, \mu} \max_{\mu \in \widehat{\mathcal{U}}(x)} d^T w
% \]
% subject to:
% \[
% w \in [0,1]^n, \ P_{X,C}(C \in U(X)) \geq 1 - \alpha,
% \]
% where $ d^T w $ represents the total distance traveled, and the distance $ d_i^j $ is defined as:

% \[
% d_i^j = \left[ \frac{1}{3^{deg-1}} \left( \frac{1}{\sqrt{p}} \mathbf{B} x_i + 3 \right)^{deg} \right] \cdot \epsilon_i^j,
% \]
% where $ \mathbf{B} $ is a random matrix, and $ \epsilon_i^j $ is the noise component.

% For TSP, the number of nodes $ m $ is an additional parameter that influences the dataset generation. The distance consists of two parts: one derived from random Euclidean coordinates, and the other from feature encoding. The Euclidean distance is calculated based on random coordinates, while the feature encoding follows the same formulation as above. The parameters for the dataset include the number of nodes, the feature dimension $ p $, and the degree of the encoding function.

\subsection{Real Dataset: Energy-Cost Aware Scheduling Problem}
\label{app:energy}
In this task, we consider a demand response program in which an operator schedules electricity consumption $ p_t $ over a time horizon $ t \in \Omega_t $. The objective is to minimize the total cost of electricity while adhering to operational constraints. The electricity price for each time step is denoted by $ \pi_t $, which is not known in advance. However, the operator can schedule the electricity consumption $ p_t $ within a specified lower bound $ P_t $ and upper bound $ \overline{P}_t $. Additionally, the total consumption for the day, denoted as $ P_t^{\text{sch}} $, must remain constant. This assumes flexibility in shifting electricity demand across time steps, provided the total demand is met.

% \begin{table}[!t]
%     \centering
%     \caption{Hyperparameters and Problem Configurations}
%     \label{tab:hyperparameters_condensed}
%     \begin{tabular}{l l c c c}
%         \toprule
%         & & \textbf{Portfolio} & \textbf{Knapsack} & \textbf{Shortest-Path} \\
%         \midrule
%         \multirow{4}{*}{\textbf{Pairwise}} & learning rate & 0.001 & 0.001 & 0.1 \\
%         & noise scale $\sigma$ & 20 & 20 & 5 \\
%         & dimension $d$ & 50 & 50 & 25 \\
%         & training samples & 320 & 320 & 320 \\
%         \midrule
%         \multirow{4}{*}{\textbf{Listwise}} & learning rate & 0.001 & 0.001 & 0.1 \\
%         & noise scale $\sigma$ & 20 & 20 & 5 \\
%         & dimension $d$ & 50 & 50 & 25 \\
%         & training samples & 320 & 320 & 320 \\
%         \midrule
%         \multirow{4}{*}{\textbf{NCE}} & learning rate & 0.001 & 0.001 & 0.1 \\
%         & noise scale $\sigma$ & 20 & 20 & 5 \\
%         & dimension $d$ & 50 & 50 & 25 \\
%         & training samples & 320 & 320 & 320 \\
%         \midrule
%         \multirow{4}{*}{\textbf{MAP}} & learning rate & 0.001 & 0.001 & 0.1 \\
%         & noise scale $\sigma$ & 20 & 20 & 5 \\
%         & dimension $d$ & 50 & 50 & 25 \\
%         & training samples & 320 & 320 & 320 \\
%         \midrule
%         \multirow{4}{*}{\textbf{SPO}} & learning rate & 0.001 & 0.001 & 0.1 \\
%         & noise scale $\sigma$ & 20 & 20 & 5 \\
%         & dimension $d$ & 50 & 50 & 25 \\
%         & training samples & 320 & 320 & 320 \\
%         \midrule
%         \multirow{4}{*}{\textbf{MSE (PTO)}} & learning rate & 0.001 & 0.001 & 0.1 \\
%         & noise scale $\sigma$ & 20 & 20 & 5 \\
%         & dimension $d$ & 50 & 50 & 25 \\
%         & training samples & 320 & 320 & 320 \\
%         \midrule
%         \multirow{5}{*}{\textbf{Gen-DFL}} & learning rate & 0.001 & 0.001 & 0.001 \\
%         & noise scale $\sigma$ & 20 & 20 & 20 \\
%         & dimension $d$ & 50 & 50 & 50 \\
%         & training samples & 320 & 320 & 320 \\
%         & DFL loss weight $\beta$ & 10.0 & 10.0 & 10.0 \\
%         \bottomrule
%     \end{tabular}
%     \vspace{-.15in}
% \end{table}

The optimization problem, assuming perfect information about prices $ \pi_t $, can be formulated as:
\[
\min_{p_t} \text{CVaR}_{\pi} \Big[\sum_{t \in \Omega_t} \pi_t p_t;\alpha \Big],
\]
subject to the constraints:
\[
P_t \leq p_t \leq \overline{P}_t, \quad \forall t,
\]
\[
\sum_{t \in \Omega_t} p_t = \sum_{t \in \Omega_t} P_t^{\text{sch}}.
\]

Here, $ P_t \leq p_t \leq \overline{P}_t $ ensures the consumption at each time step is within the allowed bounds, while the equality constraint guarantees that the total electricity consumption remains fixed across the time horizon.

This setup reflects the practical challenges of demand-side electricity management, where prices are uncertain, and demand shifting across time steps provides opportunities for cost reduction while maintaining overall consumption levels. The problem serves as a testbed for evaluating optimization approaches under uncertain electricity prices and operational constraints.

% \subsection{Pandemic Resource Allocation}

% The COVID-19 pandemic has highlighted the challenges policymakers and epidemiologists face in planning for surges in medical resource demand, such as hospital beds. As the number of infected patients increases, accurate forecasts of hospitalizations become critical for effective resource allocation. To achieve this, epidemiological models based on Ordinary Differential Equations (ODEs) are often employed to capture and forecast the dynamics of infectious disease outbreaks. These forecasts are then used as guidance for planning future resource allocation.

% In this task, we focus on the optimization problem of hospital bed preparation during a pandemic, a critical task for ensuring adequate medical infrastructure. Specifically, the goal is to decide how many hospital beds $ a \in \mathbb{R}^7 $ to allocate for the next seven days based on the forecasted number of hospitalized patients $ y \in \mathbb{R}^7 $. The optimization objective combines linear and quadratic costs to account for both over-preparation ($[a_i - y_i]_+$) and under-preparation ($[y_i - a_i]_+$) of hospital beds, ensuring both efficiency and safety in resource allocation.

% The optimization problem is formulated as:
% \[
% \min_{a \in \mathbb{R}^7} \sum_{i=1}^7 c_b [a_i - y_i]_+ + c_h [y_i - a_i]_+ + q_b ([y_i - a_i]_+)^2 + q_h ([a_i - y_i]_+)^2,
% \]
% where $ c_b $ and $ c_h $ represent the linear cost coefficients for over- and under-preparation, while $ q_b $ and $ q_h $ represent the quadratic penalty coefficients for the same. These terms reflect the trade-offs between allocating too many beds, which leads to wasted resources, and too few beds, which risks inadequate care for patients.

% This formulation integrates ODE-based forecasts to guide decision-making, enabling a data-driven approach to resource planning under uncertainty. The problem is designed to balance competing objectives effectively, ensuring sufficient resources while minimizing waste during critical periods of high demand.


\section{Hyperparameter Configurations}
\label{app:hyper}

% \woody{Need a paragraph to discuss the tables and figures in the section. }

% \woody{Condense the tables so that they can take less space and fit into the appendix. }

Table~\ref{tab:hyperparameters_condensed} summarizes the hyperparameter settings and problem configurations across different tasks and baselines. For all methods, we maintain a consistent number of training samples ($n = 320$) and input dimensionality ($d = 50$ for Portfolio and Knapsack, $d = 25$ for Shortest-Path) to ensure a fair comparison. The learning rates vary across tasks, with a higher value ($0.1$) used for the Shortest-Path problem, reflecting its different optimization landscape. The noise scale $\sigma$ remains fixed at $20$ for Portfolio and Knapsack, while a lower value ($\sigma = 5$) is used for Shortest-Path to account for its different problem structure.

For Gen-DFL, we introduce an additional DFL loss weight $\beta$ which controls the balance between the decision-focused objective and the negative log-likelihood (NLL) regularization, so that
\[
\ell_\texttt{Gen-DFL}(\theta;q, \alpha) \coloneqq 
    \beta \cdot\mathbb{E}_x[\text{Regret}_{\theta, q}(x;\alpha)] + \gamma \cdot \ell_\text{gen}(\theta).
\]
Unlike baseline Pred-DFL models, which optimize directly over point estimates, Gen-DFL leverages generative modeling and requires careful tuning of $\beta, \gamma$ to ensure stable training. The uniformity in hyperparameter selection across methods helps isolate the impact of different learning paradigms.


\FloatBarrier
\begin{table*}[!t]
    \centering
    % \resizebox{\textwidth}{!}{ % This ensures the table fits within the 
    \caption{Hyperparameters and Problem Configurations}
    \label{tab:hyperparameters_condensed}
    \begin{tabular}{l c c c c c c c c}
        \toprule
        \textbf{Task} & \textbf{Method} & \textbf{Learning Rate} & \textbf{Variance} $\sigma$ & \textbf{Dimension} $d$ & \textbf{Training Size} & $\beta$ \\
        \midrule
        \multirow{7}{*}{\textbf{Portfolio}} 
        & Pairwise & $10^{-3}$ & 20 & 50 & 320 & - \\
        & Listwise & $10^{-3}$  & 20 & 50 & 320 & - \\
        & NCE & $10^{-3}$  & 20 & 50 & 320 & - \\
        & MAP & $10^{-3}$  & 20 & 50 & 320 & - \\
        & SPO & $10^{-3}$  & 20 & 50 & 320 & - \\
        & MSE (PTO) & $10^{-3}$  & 20 & 50 & 320 & - \\
        & Gen-DFL & $10^{-3}$  & 20 & 50 & 320 & 10.0 \\
        \midrule
        \multirow{7}{*}{\textbf{Knapsack}} 
        & Pairwise & $10^{-3}$  & 20 & 50 & 320 & - \\
        & Listwise & $10^{-3}$  & 20 & 50 & 320 & - \\
        & NCE & $10^{-3}$ & 20 & 50 & 320 & - \\
        & MAP & $10^{-3}$  & 20 & 50 & 320 & - \\
        & SPO & $10^{-3}$  & 20 & 50 & 320 & - \\
        & MSE (PTO) & $10^{-3}$  & 20 & 50 & 320 & - \\
        & Gen-DFL & $10^{-3}$  & 20 & 50 & 320 & 10.0 \\
        \midrule
        \multirow{7}{*}{\textbf{Shortest-Path}} 
        & Pairwise & $10^{-1}$ & 5 & 25 & 320 & - \\
        & Listwise & $10^{-1}$ & 5 & 25 & 320 & - \\
        & NCE & $10^{-1}$ & 5 & 25 & 320 & - \\
        & MAP & $10^{-1}$ & 5 & 25 & 320 & - \\
        & SPO & $10^{-1}$ & 5 & 25 & 320 & - \\
        & MSE (PTO) & $10^{-1}$ & 5 & 25 & 320 & - \\
        & Gen-DFL & $10^{-3}$ & 20 & 50 & 320 & 10.0 \\
        \bottomrule
    \end{tabular}
 % }
\vspace{-.15in}
\end{table*}


% \section{Additional Experimental Results}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
