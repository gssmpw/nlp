\section{Analysis on Alignment Tax}
\label{sec:alignment_tax}

Despite all the other role-playing and instruction-following abilities of the LLMs are enhanced during the DPO alignment, we observe a slight yet common deterioration in multi-turn instruction-following performance (Appendix\,\ref{sec:app_judge}). We refer to this phenomenon as an ``alignment tax'', which is characterized by a gradual forgetting of knowledge acquired during pre-training\,\cite{ouyang2022training}.

\paragraph{Neuron-Level Localization.}
To identify the underlying cause of this alignment tax, we examine the neuron activation patterns of our \textsc{RoleMRC} models (LLaMA3.1-8B SFT vs.\ DPO). Following \citet{tang-etal-2024-language}, we probe and collect activations from each  attention layer, focusing on highly activated neurons by selecting the top 20\% of activations. Specifically, for each input instruction, we measure activations when first forwarding the instruction. We then group the activation maps by the evaluation dimension of the test instruction, generating layer-specific differences in neuron usage.

Next, we count the activation frequency of each neuron and normalize it by the total number of test cases. Figure~\ref{fig:neuron_visual} visualizes the resulting discrepancy between the SFT and DPO models. Layers 3–11 exhibit minimal changes, whereas layers beyond the 13th show substantial activation differences, with layers 12–31 ({\color{salmon}{highlighted in red}}) differing the most. Notably, layer 19 is \textbf{significantly more active in multi-turn instruction}.

This observation aligns with \citet{tang-etal-2024-language}, who found that \emph{only the top and bottom layers of a language model are primarily used for language processing}. These shifts in neuron activations suggest that \emph{certain neurons are activated very differently between the SFT and DPO models}. Further details and results are provided in Appendix\,\ref{sec:further_interpet}.

\paragraph{Neuron-Level Restraint.}
After identifying these critical neuron subsets, we apply a minor scaling restraint (multiplicative factor $1 - 10^{-6}$) to modulate their impact. As shown in Table~\ref{tab:neuron_restrain}, constraining the most changed neurons \textbf{provides consistent improvements across \underline{both} reference-based metrics and the LLM-as-a-judge approach}. 
In particular, multi-turn instruction accuracy increases by 1.6\%, mitigating the alignment tax \textbf{\underline{without} requiring further model retraining}. We also observe gains in dimensions of knowledge boundary and nested instruction-following, highlighting that targeted neuron-level adjustments can manipulate LLMs' capabilities under alignment constraints.

% In summary, our findings reveal that while DPO alignment generally enhances instruction adherence, it may inadvertently degrade certain capabilities, such as multi-turn understanding. Nonetheless, fine-grained neuron-level analysis opens a promising avenue to counteract or minimize this alignment tax.