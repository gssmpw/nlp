\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/intro.png}
    \vspace{-6mm}
    \caption{Example of  instructional requests from human user, answered by role-playing LLMs in different ways.}
    \label{fig:intro}
    \vspace{-6mm}
\end{figure}

Role-playing is one of the key capabilities of LLMs. % Without loss of generality, 
Modern LLMs are designed to interact with human users under certain role-playing settings\,\cite{chen2024persona,tseng-etal-2024-two}. In this context, LLMs respond to various instructions, serving as AI assistants\,\cite{openai2023gpt4, ji-etal-2022-achieving}, personalized agents\,\cite{zhong2022less,lu2023memochat,lei-etal-2022-assistsr}, leisure partners\,\cite{li2023chatharuhi,agrawal-etal-2023-multimodal}, content creators\,\cite{narrativeplay_aaai,chen2024hollmwood,narrativeplay_eacl}, social experimental simulator\,\cite{park2023generative,xu2024character} among other roles\,\cite{tian2023chatplug}. 

Figure\,\ref{fig:intro} demonstrates an example of LLM role-playing. In the first turn of dialogue, when asked to \emph{give advice on paper writing}, the LLM should respond based on the pre-defined role profile (shown at the top of Figure\,\ref{fig:intro}). Among the responses, the reply ``[a]'' completely ignored the role setting, ``[b]'' misinterpreted the role and thus did not respond well, only ``[c]'' correctly \emph{gave suggestions in a \emph{cat girl} style}. In the second turn of dialogue (continuing with ``[c]''), the user not only asked a new question, but also modified the role setting (\emph{adding a heart emoji at the beginning of the answer}). While both replies ``[d]'' and ``[e]'' maintained the initial \emph{cat girl} style, only ``[e]'' correctly incorporated the additional role-playing instruction.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\quad & \quad & \quad & \quad & \quad & \multicolumn{3}{c}{\textbf{Scenarios}}  \\
\cmidrule(lr){6-8} \textbf{Dataset} & \textbf{Data Scale} & \textbf{Role Num.} & \textbf{\#Turns} & \textbf{\#Words per Reply} & \textbf{Free Chat} & \textbf{On Scene} & \textbf{Ruled Chat} \\
\midrule
\textsc{CharacterLLM\,\cite{shao2023character}} & 14.2k & 9 & 13.2 & 36 & \ding{52} & \ding{56} & \ding{56} \\
\textsc{ChatHaruhi\,\cite{li2023chatharuhi}}* & 11.6k & 35 & 5.5 & 7 & \ding{52} & \ding{56} & \ding{56} \\
\textsc{RoleLLM\,\cite{wang2023rolellm}} & 168.1k & 100 & 1 & 30.5 & \ding{52} & \ding{56} & \ding{56} \\
\textsc{CharacterGLM\,\cite{zhou2023characterglm}} & 1k & 250 & 15.8 & 24.3 & \ding{52} & \ding{56} & \ding{56} \\
\textsc{CharacterEval\,\cite{tu2024charactereval}} & 1.8k & 77 & 9.3 & 27.4 & \ding{56} & \ding{52} & \ding{56} \\
\textsc{DITTO\,\cite{lu-etal-2024-large}} & 7.2k & 4k & 5.1 & -* & \ding{52} & \ding{56} & \ding{56} \\
\textsc{Character100\,\cite{wang2024characteristic}} & 10.6k & 106 & 1 & 74.1 & \ding{56} & \ding{52} & \ding{56} \\
\textsc{MMRole\,\cite{dai2024mmrole}} & 14.3k & 85 & 4.15 & 66.8 & \ding{56} & \ding{52} & \ding{56} \\
\midrule
\textsc{RoleMRC (ours)} & 37.9k & 10.2k & 3.5 (9.5) & 40.6 & \ding{52} & \ding{52} & \ding{52} \\
\textsc{RoleMRC-mix (ours)} & 107.7k & 10.2k & 2 (9.5) & 67.1 & \ding{52} & \ding{52} & \ding{52} \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{Comparison of different role-playing datasets. For ChatHaruhi\,\cite{li2023chatharuhi}, we list the statistics of its 1.0 version. For DITTO\,\cite{lu-etal-2024-large}, we can not find its public version for computing utterance statistics. In RoleMRC, free chats have significantly more conversational turns than on-scene dialogues and ruled chats, so we mark them separately in the middle brackets of the last two lines. The RoleMRC-mix is a robust version mixed with subsets of RoleLLM, RLHFlow, and UltraFeedback\,\cite{wang2023rolellm,dong2024rlhf,cui2023ultrafeedback}.}
\label{tab:dataset_comparison}
\vspace{-5mm}
\end{table*}

Existing role-playing datasets generally focus on controlling the role-playing styles and knowledge boundaries, 
encouraging responses similar to replies ``[b]'', ``[c]'', or ``[d]'' in Figure\,\ref{fig:intro}. However, they lack focus on role-playing over fine-grained, multi-layered instructions, such as nested or prioritized requests exemplified %, which is similar to reply 
by ``[e]''. To address this gap, we propose a fine-grained role-playing instruction-following dataset, named RoleMRC, aiming to enhance and evaluate the diverse role-playing and instruction-following capabilities of LLMs. In Table\,\ref{tab:dataset_comparison}, we compare RoleMRC with existing datasets. In general, other datasets focus on a single aspect of role-playing, while RoleMRC supports: (1) \textbf{Free Chats}, where roles and users interact freely without a fixed topic or specific constraints; (2) \textbf{On-scene Dialogues}, where roles share thoughts or answer questions relevant to the given passages; (3) \textbf{Ruled Chats}, where the role's response needs to adhere to particular requirements from the system or the user, such as specific formatting, constraints or refusal guidelines. With 10.2k structured role profiles, RoleMRC offers the most comprehensive role-playing dataset to date. Our contributions are briefly summarized as follows:
\begin{enumerate}
[leftmargin=*,noitemsep,topsep=0pt]
    \item We introduce RoleMRC, the first large-scale, diverse role-playing dataset covering fine-grained instruction-following scenarios (\hyperref[sec:method]{\textsection \ref{sec:method}}).
    \item By using RoleMRC, we create an evaluation pipeline to assess the fine-grained role-playing and instruction-following capabilities of leading LLMs and  fine-tuned models (\hyperref[sec:results]{\textsection \ref{sec:results}}).
    \item Probing of neurons in post-tuned LLMs reveals activation patterns linked to different instruction-following and role-playing abilities (\hyperref[sec:alignment_tax]{\textsection \ref{sec:alignment_tax}}).
\end{enumerate}
