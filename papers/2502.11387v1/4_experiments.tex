\section{Experimental Setup}
\label{sec:setup}

\subsection{Foundation Models and Post-tuning}
\label{sec:baselines}
We evaluate leading LLMs and fine-tuned models:
\begin{itemize}
[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{Proprietary LLMs.} gpt-3.5-turbo and gpt-4o.
    \item \textbf{SOTA Open-source LLMs.} Qwen2.5-7B/72B-Instruct\,\cite{yang2024qwen2} and LlaMA3.1-8B/70B-Instruct\,\cite{dubey2024llama}.
    \item \textbf{Role-playing LLMs.} CharacterGLM-6B\,\cite{zhou2023characterglm}, Humanish-Llama-3.1-8B\,\cite{huminish}, and Peach-9B-Roleplay\,\cite{peach-rp}.
    \item \textbf{Local Post-tuned LLMs.} We start with \textbf{pure base models Llama-3.1-8B and Qwen2.5-7B}. We first use single-label in RoleMRC-mix for SFT, then apply the pair-label set for Direct Preference Optimization (DPO,\citealt{rafailov2023direct}).
\end{itemize}

\subsection{Reference-based Metrics} 

We evaluate model-generated outputs using standard heuristic metrics commonly used in NLG:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{BLEU} \citep{papineni2002bleu} computes the precision of n-gram overlaps between generated text and a ground truth reference.
\item \textbf{ROUGE} \citep{lin2004rouge} measures the overlap of n-grams and longest common subsequences between the hypothesis and references. We include ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum to capture various granularities of overlap.
\item \textbf{METEOR} \citep{meteor} aligns generated and reference tokens using stemming and synonym matching, aiming to provide a more linguistically grounded evaluation%than simple n-gram overlap
.
\item \textbf{BERTScore F1} \citep{zhang2019bertscore} computes the similarity between generated and reference sentences using contextual embeddings% from a large pre-trained language model
. %This embedding-based approach provides a semantic overlap measure, potentially offering higher sensitivity to meaning than surface-based metrics.
\end{itemize}
For each metric, higher scores indicate better alignment with the reference lexically or semantically.

\subsection{Reference-free LLM-as-a-judge}
Apart from reference-based metrics, LLM-as-a-judge\,\cite{zheng2024judging} is another evaluation approach by instructional prompting advanced LLMs. In reference to Table\,\ref{tab:roleMRC}, we curate a 1.4k test set similar to the On-scene MRC Dialogues and Ruled Chats, then evaluate model performance across five dimensions: (1) \textbf{Knowledge Boundary} focuses on distinguishing between answerable queries (``Answer'') and refusal scenarios (``Refusal'') in On-scene MRC Dialogues; (2) \textbf{Role Style} examines whether the model accurately produces role-specific responses (``Answer'', ``No Answer'', ``Refusal'', and ``Attempt'') in On-scene MRC Dialogues without drifting into narration; while (3) \textbf{Multi-turn Instruction-following}, (4) \textbf{Nested Instruction-following}, and (5) \textbf{Prioritized Instruction-following} assess a model's adherence to higher-level constraints in Ruled Chats.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Models} & \textbf{BLEU} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{ROUGE-Lsum} & \textbf{METEOR} & \textbf{BERTScore F1} \\
\midrule
\texttt{gpt-3.5-turbo} & 0.0234 & 0.2141 & 0.0606 & 0.1548 & 0.1579 & 0.1992 & 0.8552 \\
\texttt{gpt-4o} & 0.0288 & 0.2487 & 0.0742 & 0.1689 & 0.1835 & 0.2697 & 0.8516 \\
\midrule
\texttt{CharacterGLM-6B} & 0.0058 & 0.1225 & 0.0253 & 0.0901 & 0.0967 & 0.1188 & 0.7944 \\
\texttt{Humanish-Llama-3.1-8B} & 0.0153 & 0.2062 & 0.0518 & 0.1309 & 0.3207 & 0.2389 & 0.8376 \\
\texttt{Peach-9B-Roleplay} & 0.0207 & 0.2297 & 0.0562 & 0.1544 & 0.1571 & 0.2299 & 0.8418 \\
\midrule
LLaMA3.1-8B-Instruct & 0.0226 & 0.2277 & 0.0615 & 0.1509 & 0.1650 & 0.2594 & 0.8478 \\
LLaMA3.1-70B-Instruct & 0.0232 & 0.2258 & 0.0646 & 0.1500 & 0.1661 & 0.2632 & 0.8480 \\
\rowcolor{verylightgrey} LLaMA3.1-8B-RoleMRC-SFT & \textbf{0.1782} & \textbf{0.4628} & \textbf{0.2676} & \textbf{0.3843} & \textbf{0.3853} & 0.3975 & \textbf{0.8831} \\
\rowcolor{verylightgrey} LLaMA3.1-8B-RoleMRC-DPO & 0.1056 & 0.3989 & 0.1785 & 0.2988 & 0.3001 & \textbf{0.4051} & 0.8805 \\
\midrule
Qwen2.5-7B-Instruct & 0.0224 & 0.2283 & 0.0621 & 0.1518 & 0.1599 & 0.2490 & 0.8471 \\
Qwen2.5-72B-Instruct & 0.0245 & 0.2350 & 0.0656 & 0.1554 & 0.1660 & 0.2579 & 0.8485 \\
\rowcolor{verylightgrey} Qwen2.5-7B-RoleMRC-SFT & \textbf{0.1963} & \textbf{0.4764} & \textbf{0.2744} & \textbf{0.3959} & \textbf{0.3968} & \textbf{0.4337} & \textbf{0.9063} \\
\rowcolor{verylightgrey} Qwen2.5-7B-RoleMRC-DPO & 0.1244 & 0.4178 & 0.1916 & 0.3164 & 0.3177 & 0.4205 & 0.8931 \\
\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{Comparison of reference-based evaluation results on the RoleMRC test data. Our evaluation includes zero-shot query results for baselines (\hyperref[sec:baselines]{\textsection \ref{sec:baselines}}), and \colorbox{verylightgrey}{our SFT and DPO models} fine-tuned on the RoleMRC-mix.}
\label{tab:main_table}
\vspace{-1mm}
\end{table*}

\begin{figure*}[t]
    \centering
    % Third subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llm_judge/merged.png}
        \caption{Instruct \& Role-play models.}
        \label{fig:subfig1}
    \end{subfigure}
    % Fourth subfigure
    % \begin{subfigure}[b]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figures/llm_judge/role_play_models.png}
    %     \caption{Role-play models.}
    %     \label{fig:subfig2}
    % \end{subfigure}
    % First subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llm_judge/llama_models.png}
        \caption{LLaMA3.1 models.}
        \label{fig:subfig3}
    \end{subfigure}
    % Second subfigure
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llm_judge/qwen_models.png}
        \caption{Qwen2.5 models.}
        \label{fig:subfig4}
    \end{subfigure}
    \vspace{-2mm}
    \caption{Visualization of reference-free LLM-as-a-judge results. We provide numerical result in Table \ref{tab:llm_as_judge}.}
    \vspace{-2mm}
    \label{fig:mainfig}
\end{figure*}

We adopt a well-designed \underline{reference-free} evaluation prompt (Figure\,\ref{box:llm_judge_prompt}), requiring the evaluator to verify whether the model's role-playing performance comply with the corresponding rules, which avoids the risk of potential bias or error in any ground truth answer. Since we use a binary evaluation criterion, we directly extract 0 or 1 judgments from the feedback, enabling score comparison and accuracy computation. We chose gpt-4-turbo\,\cite{gpt4turbo} as the evaluator, reducing the possible judging bias\,\cite{wataoka2024self}.

\section{Evaluation on Inner RoleMRC Test Set}
\label{sec:results}

% 1. generally larger parameter size Qwen2.5-7B-Instruct Vs Qwen2.5-72B-Instruct and LLaMA3.1-8B-Instruct Vs. LLaMA3.1-70B-Instruct achieve better result on reference-based evaluation.

% 2. open-source role-play llms perform not well on both reference-based evaluation, could becuase (1) very specificalized training data, (2) those models are based on former version base models.

% 3. we don't know the size of gpt-3.5-turbo and gpt-4o. But our performance aligns with exisiting general capability test results, gpt-4o is better than gpt-3.5-turbo. This could also because we used gpt-4o to generated the synthetic dataset, so it could generate outputs that has higher reference-based scores.

% 4. lastly our SFT and DPO methods achieved the best reference-based results, becasuse we tuned our models on the training set. Interestingly, we see that DPO result achieved lower in reference-based evaluation. This could related to alignment tax, that initially changed the predictive distribution for tokens.

By leveraging the above \textbf{reference-based metrics} and \textbf{reference-free LLM-as-a-judge} approaches, we report evaluation on RoleMRC in what follows.

\paragraph{Performance of Proprietary LLMs.}
As shown in Table~\ref{tab:main_table}, gpt-4o achieves slightly higher BLEU, ROUGE, and METEOR scores than gpt-3.5-turbo. This observation is consistent with existing evaluations on general benchmarks~\cite{openai2023gpt4}, and may also be influenced by the fact that our RoleMRC training data was synthesized by gpt-4o. The LLM-as-a-judge results (Figure~\ref{fig:subfig1}) similarly highlight gpt-4o's strengths in Knowledge Boundary, Role Style, and Nested Instruction-following, whereas gpt-3.5-turbo outperforms gpt-4o on Prioritized and Multi-turn Instruction-following.

\paragraph{Evaluation on Commonly Used LLMs.}
For the LLaMA3.1 and Qwen2.5 families, larger models generally yield higher reference-based scores. For instance, LLaMA3.1-70B-Instruct slightly leads its 8B sibling (BLEU from $0.0226$ to $0.0232$), and Qwen2.5-72B-Instruct outperforms its 7B version (BLEU from $0.0224$ to $0.0245$). Although these improvements are modest, the results align with the broader observation that increasing model scale typically benefits language modeling and generalization. Likewise, LLM-as-a-judge results (Figures~\ref{fig:subfig3} and~\ref{fig:subfig4}) show larger models are consistently better, particularly in Knowledge Boundary, Role Style, Nested and Prioritized Instruction-following.

\paragraph{Results of Role-playing LLMs.} Three open-source role models obtain generally lower heuristic metrics than those general-purpose instruct models with similar size (Table\,\ref{tab:main_table}). This discrepancy may stem from their training data, which emphasizes limited role styles and persona consistency rather than factual correctness and coverage. On LLM-as-a-judge (Figure\,\ref{fig:subfig1}), \texttt{CharacterGLM-6B} again performs poorly, while \texttt{Humanish-Llama-3.1-8B} and \texttt{Peach-9B-Roleplay} show decent performance in Knowledge Boundary, Role Style, and Multi-turn Instruction-following, but struggle with Nested and Prioritized Instruction-following.

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\quad & \multicolumn{4}{c}{\textbf{RoleBenchInstEng (32.8k)}} & \multicolumn{4}{c}{\textbf{RoleBenchRoleEng (7.5k)}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{ROUGE-Sum} 
& \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{ROUGE-Sum} \\
\midrule
\texttt{CharacterGLM-6B} & 0.1761 & 0.0546 & 0.1441 & 0.1530 & 0.1841 & 0.0628 & 0.1473 & 0.1552 \\
\texttt{Humanish-Llama-3.1-8B} & 0.2069 & 0.0639 & 0.1341 & 0.1645 & 0.1851 & 0.0468 & 0.1193 & 0.1432 \\
\texttt{Peach-9B-Roleplay} & 0.3216 & 0.1293 & 0.2573 & 0.2646 & 0.3454 & 0.1450 & 0.2705 & 0.2732 \\
\midrule
LLaMA3.1-8B-Instruct & 0.2528 & 0.0864 & 0.1755 & 0.1931 & 0.2395 & 0.0754 & 0.1691 & 0.1844 \\
LLaMA3.1-70B-Instruct & 0.2846 & 0.1064 & 0.2062 & 0.2258 & 0.2756 & 0.1036 & 0.2036 & 0.2204 \\
LLaMA3.1-8B-RoleMRC-SFT & 0.3329 & 0.1601 & 0.2755 & 0.2770 & \textbf{0.3980} & \textbf{0.2022} & 0.3270 & \textbf{0.3278} \\
LLaMA3.1-8B-RoleMRC-DPO & \textbf{0.3605} & \textbf{0.1696} & \textbf{0.2812} & \textbf{0.2846} & 0.3970 & 0.1952 & \textbf{0.3149} & 0.3163 \\
\midrule
Qwen2.5-7B-Instruct & 0.3216 & 0.1376 & 0.2437 & 0.2599 & 0.3337 & 0.1463 & 0.2582 & 0.2692 \\
Qwen2.5-72B-Instruct & 0.3225 & 0.1354 & 0.2364 & 0.2524 & 0.3370 & 0.1460 & 0.2577 & 0.2672 \\
Qwen2.5-7B-RoleMRC-SFT & 0.3963 & 0.1922 & \textbf{0.3294} & \textbf{0.3312} & \textbf{0.4442} & \textbf{0.2298} & \textbf{0.3680} & \textbf{0.3692} \\
Qwen2.5-7B-RoleMRC-DPO & \textbf{0.3969} & \textbf{0.1958} & 0.3143 & 0.3180 & 0.4298 & 0.2187 & 0.3452 & 0.3470 \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{Evaluations on external RoleBench\,\cite{wang2023rolellm} test set. The best results for each metric are \textbf{bold}.}
\label{tab:RoleBench}
\vspace{-2mm}
\end{table*}

\paragraph{Impact on Task-Specific Fine-tuning.} Our locally post-tuned \textbf{RoleMRC-SFT} models dramatically outperform all above baselines on reference-based metrics, improving BLEU by around $8\times$ over their respective base models. Although the \textbf{SFT} models excel at matching ground-truth references, \textbf{DPO}-aligned models win in reference-free LLM-as-a-judge, in terms of \emph{Knowledge Boundary} and \emph{Role Style}. For instance, \textbf{LLaMA3.1-8B-RoleMRC-DPO} reaches a \emph{Role Style} accuracy of 97.00\%, while its SFT counterpart score is only around 70.00\% (Figure\,\ref{fig:subfig3}, detailed numbers in Appendix\,\ref{sec:app_judge}). However, DPO models typically score lower on reference-based metrics (Table\,\ref{tab:main_table}), reflecting a trade-off: shifting the model's distribution toward instruction compliance and human preference can reduce exact lexical matches.

Overall, our curated evaluation framework realizes robust effectiveness for assessing LLM's role-playing instruction-following capabilities.

\section{Evaluation on External Benchmarks}
We present cross-evaluation on external datasets.

\paragraph{[1] Fine-tuning on RoleMRC would not interfere the learning of other role-playing data.} In Table\,\ref{tab:RoleBench}, we follow \citet{wang2023rolellm} and evaluate on two of their test sets: (1) RoleBenchInstEng (32.8k), an \emph{instruction-based} split that tests how well models handle various instructions, and (2) RoleBenchRoleEng (7.5k), a \emph{role-based} split that tests model performance across different roles. %The ROUGE scores in Table~\ref{tab:RoleBench} follows the original work's evaluation setup.
On RoleBenchInstEng, all RoleMRC-aligned models consistently outperform instruct and role-playing baselines. Notably, \textsc{Qwen2.5-7B-RoleMRC-SFT} achieves significant gains, pushing ROUGE-1 and ROUGE-2 to $0.3963$ and $0.1922$, respectively. %This indicates that targeted fine-tuning with RoleMRC data enhances a model's ability to handle diverse instruction formulations, surpassing even larger-scale models like Qwen2.5-72B-Instruct on this task. Meanwhile, the DPO variants (\textsc{RoleMRC-Llama3.1-8B (DPO)}, \textsc{RoleMRC-Qwen2.5-7B (DPO)}) yield further improvements on some metrics (e.g., ROUGE-1 and ROUGE-2), suggesting that instruction-focused alignment techniques can boost instruction generalization.
In the right panel of Table\,\ref{tab:RoleBench}, results on RoleBenchRoleEng reveal similar trends. Our models outperform standard instruct models by sizeable margins. \textsc{Qwen2.5-7B-RoleMRC-SFT} obtains the highest ROUGE-1 ($0.4442$) and ROUGE-L ($0.3680$). %Notably, the DPO variant achieves comparable performance on key metrics, reflecting improved instruction alignment without major trade-offs in content coverage. Together, these results demonstrate the robustness of RoleMRC-aligned models in adapting to new and diverse role-based instructions, further affirming their ability to preserve factual correctness and style consistency across role-specific queries.
We thus conclude that RoleMRC did not counter the learning of RoleBench.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\quad & \multicolumn{2}{c}{\textbf{OOD CharacterLLM}} & \quad \\
\cmidrule(lr){2-3} \textbf{Model} & \textbf{Single} & \textbf{Turns} & \textbf{General $\Delta$} \\
\midrule
\texttt{CharacterGLM-6B} & 5.9495 & 5.8676 & 1.00 \\
\texttt{Humanish-Llama-3.1-8B} & 5.3781 & 6.0444 & 0.68 \\
\texttt{Peach-9B-Roleplay} & 6.3074 & 6.0120 & -2.46 \\
\midrule
LLaMA3.1-8B-Instruct & 6.5244 & \textbf{6.0533} & 11.82 \\
%LLaMA3.1-70B-Instruct & 6.5825 & \textbf{6.0569} & - \\
LlaMA3.1-8B-RoleMRC-SFT & 6.4320 & 6.0196 & 4.08 \\
LlaMA3.1-8B-RoleMRC-DPO & 6.5179 & 5.9884 & 1.16\\
\midrule
Qwen2.5-7B-Instruct & 6.2485 & 5.9996 & 3.64 \\
%Qwen2.5-72B-Instruct & \textbf{6.6173} & 6.0151 & - \\
Qwen2.5-7B-RoleMRC-SFT & 6.4520 & 6.0200 & -0.33 \\
Qwen2.5-7B-RoleMRC-DPO & \textbf{6.5295} & 6.0311 & 1.14 \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{Out-of-distribution (OOD) evaluation on CharacterLLM\,\cite{shao2023character}, where models are evaluated on ``Single'' and ``Turns'' settings. ``General $\Delta$'' denotes the average gain for each model, compared with its fine-tuning starting point, across nine non-role-playing general-purpose benchmarks. Check details of OOD testing in Appendix\,\ref{sec:app_character_llm} and \,\ref{sec:app_benchmark}.}
\vspace{-1.5em}
\label{tab:ood_brief}
\end{table}

%\paragraph{Comparison with Role-Play Models.}
%Existing open-source role-play models (e.g., \textsc{CharacterGLM-6B}, \textsc{Humanish-Llama-3.1-8B}, \textsc{Peach-9B-Roleplay}) generally perform worse than RoleMRC-fine-tuned models on both instruction and role-based splits. This gap further suggests that training solely for persona or style consistency does not translate directly into handling more diverse, role-oriented instructions or complex role-based scenarios.

%Overall, our experiments on RoleBenchInstEng and RoleBenchRoleEng confirm that RoleMRC-tuned models generalize effectively to broader role-playing instruction tasks, outperforming both generic and role-play-specific baselines by substantial margins.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/MULTI_TURN_INSTRUCTION_layer_active_heatmap.png}
    \vspace{-3mm}
    \caption{\small Discrepancies between SFT and DPO neuron activations (top-20\% active neurons) in LLaMA3.1-8B for multi-turn instructions. Layers 3-11 show minimal changes ({\color{lightgreen}{green}}), while layers 12--31 exhibit larger shifts ({\color{salmon}{red}}).}
    \label{fig:neuron_visual}
    \vspace{-6mm}
\end{figure}

\begin{table*}[!ht]
\resizebox{\textwidth}{!}{
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lccccccccc}
        \toprule
        % DIMENSIONS & & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-L & ROUGE-Lsum & METEOR & BERTScore F1 & LLM as judge \\
        \textbf{Dimensions} & & \textbf{BLEU} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} & \textbf{ROUGE-Lsum} & \textbf{METEOR} & \textbf{BERTScore F1} & \textbf{LLM as judge} \\
        \midrule
        \multirow{2}{*}{Knowledge Boundary} & (B) & 0.0950 & 0.3909 & 0.1631 & 0.2860 & 0.2860 & 0.3876 & 0.8798& 74.67\% \\
        & (A) & 0.1000$\uparrow$ & 0.3946$\uparrow$ & 0.1677$\uparrow$ & 0.2924$\uparrow$ & 0.2924$\uparrow$ & 0.3883$\uparrow$ & 0.8798 & 77.33\%$\uparrow$\\
        \midrule
        \multirow{2}{*}{Role Style} & (B) & 0.1007 & 0.3948 & 0.1696 & 0.2886 & 0.2887 & 0.3883 & 0.8782 & 97.00\%\\
        & (A) & 0.1283$\uparrow$ & 0.3985$\uparrow$ & 0.1889$\uparrow$ & 0.3138$\uparrow$ & 0.3228$\uparrow$ & 0.3910$\uparrow$ & 0.8790$\uparrow$ & 94.50\%\\
        \midrule
        \multirow{2}{*}{Multi-turn Instruction-following} & (B) & 0.1183 & 0.4196 & 0.2078 & 0.3232 & 0.3232 & 0.4506 & 0.8851 & 90.50\%\\
        & (A) & 0.1185$\uparrow$ & 0.4215$\uparrow$ & 0.2110$\uparrow$ & 0.3240$\uparrow$ & 0.3240$\uparrow$ & 0.4544$\uparrow$ & 0.8852$\uparrow$ & 92.00\%$\uparrow$\\
        \midrule
        \multirow{2}{*}{Nested Instruction-following} & (B) & 0.1274 & 0.4010 & 0.1895 & 0.3138 & 0.3242 & 0.3944 & 0.8793 & 79.11\%\\
        & (A) & 0.1283$\uparrow$ & 0.3985 & 0.1889 & 0.3138 & 0.3228 & 0.3910 & 0.8790 & 79.75\%$\uparrow$\\
        \midrule
        \multirow{2}{*}{Prioritized Instruction-following} & (B) & 0.0952 & 0.3639 & 0.1537 & 0.2700 & 0.2700 & 0.3840 & 0.8796 & 83.33\%\\
         & (A) & 0.0965$\uparrow$ & 0.3776$\uparrow$ & 0.1531 & 0.2753$\uparrow$ & 0.2753$\uparrow$ & 0.3934$\uparrow$ & 0.8807$\uparrow$ &  73.81\% \\
        \bottomrule
    \end{tabular}}
    \vspace{-2mm}
    \caption{Performance comparison category by each dimensions (B)efore and (A)fter neuron-level restrain.}
    % \caption{Performance evaluation by dimensions (B)efore and (A)fter neuron-level restrain on LLaMA3.1-8B-RoleMRC-DPO.}
    \label{tab:neuron_restrain}
    \vspace{-2mm}
\end{table*}

\paragraph{[2] RoleMRC helps naive LLMs gain high-quality generalized role-playing abilities.} %We introduce  and 
We performed OOD tests of the RoleMRC-aligned models on an external role-playing dataset, Character-LLM, following its \emph{Single} and \emph{Turns} settings. The OOD results, in the middel columns of Table\,\ref{tab:ood_brief}, show that among all role-playing models, our RoleMRC-aligned model (\textsc{Qwen2.5-7B-RoleMRC-DPO}) reach a best score of 6.5295 in ``single'' evaluation and leads the ``turns'' evaluation. %Therefore, RoleMRC is generally beneficial.

\paragraph{[3] The local fine-tuned models did not overfit RoleMRC.} In the last column of Table\,\ref{tab:ood_brief}, we summarize the fine-tuning gains of different role models and general models across nine general-purpose benchmarks (e.g., GSM8K\,\cite{cobbe2021training}). The ``General $\Delta$'' is obtained by calculating the performance gap between the fine-tuning endpoint model and the starting point, such as the improvement of LlaMA3.1-8B-Instruct relative to LlaMA3.1-8B. Except for \texttt{Peach-9B-Roleplay}, all role-playing LLMs have not lost general abilities when gaining role-playing abilities.
