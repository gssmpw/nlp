\begin{abstract}

Tensor decomposition is a fundamental tool for analyzing multi-dimensional data by learning low-rank factors to represent high-order interactions. While recent works on temporal tensor decomposition have made significant progress by incorporating continuous timestamps in latent factors, they still struggle with general tensor data with continuous indexes not only in the temporal mode but also in other modes, such as spatial coordinates in climate data. Additionally, the problem of determining the tensor rank remains largely unexplored in temporal tensor models. To address these limitations, we propose \underline{G}eneralized temporal tensor decomposition with \underline{R}ank-r\underline{E}vealing laten\underline{T}-ODE (\textsc{GreT}). 
 Our approach encodes continuous spatial indexes as learnable Fourier features and employs neural ODEs in latent space to learn the temporal trajectories of factors. To automatically reveal the rank of temporal tensors, we introduce a rank-revealing Gaussian-Gamma prior over the factor trajectories. We develop an efficient variational inference scheme with an analytical evidence lower bound, enabling sampling-free optimization. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that \MODEL not only reveals the underlying ranks of temporal tensors but also significantly outperforms existing methods in prediction performance and robustness against noise.


    
    % Temporal tensor decomposition, which often incorporates continuous timestamps, is a powerful tool for handling multiway temporal data. Existing methods primarily focus on enc\MODEL  g these timestamps into the low-rank representations of tensors, which are associated with finite objects in each mode,  corresponding to discrete indexes. While effective for certain cases, these approaches overlook the intrinsic property of real-world dynamic tensor data like spatiotemporal tensors, where entries are characterized by both continuous mode indexes and timestamps.
    % %Although several methods based on functional tensors have been proposed recently to model continuous-indexed data, these approaches do not explicitly model temporal dynamics. 
    % To address the limitation, we propose  Ode-aided Deep varIational decompositioN for Continuous-indexed Temporal Tensor (\MODEL  ).
    % We treat the continuous-indexed temporal data as the interaction between  a group of latent functions,  under a sparse Bayesian framework with automatic rank (i.e., model complexity) determination.
    % We  model the posterior mean of the latent functions  with  the continuous-indexed latent ODE, which is efficient and scalable for large tensor data. Furthermore, we derive an analytical variational lower bound for the marginal data likelihood, allowing for efficient optimization via gradient descent without relying on reparameterization tricks.
    % Our method facilitates the learning of off-grid relationships and complex temporal dynamics while allowing for self-adaptation of model complexity.
    %  Extensive experiments on synthetic and real-world datasets demonstrate the superior performance of the proposed approach. 
    \end{abstract}
    