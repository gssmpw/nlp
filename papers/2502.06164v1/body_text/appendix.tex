
\appendix
\onecolumn
\section{Details of derivations.}
\label{ap:derivation}
\subsection{Log-marginal likelihood}
\label{ap:lml}
\begin{equation}
\begin{split}
    \ln p(\mathcal{D}) &= \int q(\mathcal{U}, \boldsymbol{\lambda}, \tau)\ln p(\mathcal{D}) d\mathcal{U}d \boldsymbol{\lambda}d\tau = \int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{p( \mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})}d\mathcal{U}d \boldsymbol{\lambda}d\tau \\
    &=\int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}{p( \mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}d\mathcal{U}d \boldsymbol{\lambda}d\tau \\
    &=\int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}d\mathcal{U}d \boldsymbol{\lambda}d\tau - \int q(\mathcal{U}, \boldsymbol{\lambda}, \tau) \ln \frac{p( \mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}d\mathcal{U}d \boldsymbol{\lambda}d\tau \\
    & = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}] - \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau)|\mathcal{D}}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}]\\
    &=\mathcal{L}(q) + \text{KL}(q(\mathcal{U}, \boldsymbol{\lambda}, \tau)\|p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})).
    \end{split}
\end{equation}
\subsection{Lower bound of log-marginal likelihood}
\label{ap:A.2}
\begin{equation}
\begin{split}
    &\mathcal{L}(q) = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln \frac{p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})}{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}] = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{U}, \boldsymbol{\lambda}, \tau, \mathcal{D})] - \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln q(\mathcal{U}, \boldsymbol{\lambda}, \tau)]\\
    &=\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] + \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{U}, \boldsymbol{\lambda}, \tau)] - \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln q(\mathcal{U}, \boldsymbol{\lambda}, \tau)]\\
    &=\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)]+  \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})} + \ln 
    \frac{p(\boldsymbol{\lambda})}{q(\boldsymbol{\lambda})} + \ln 
    \frac{p(\tau)}{q(\tau)}]\\
    &=\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)]+\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}]  - \text{KL}(q(\boldsymbol{\lambda})\|p(\boldsymbol{\lambda})) - \text{KL}(q(\tau)\|p(\tau)).
\end{split}
\end{equation}
The first term of evidence lower bound (posterior expectation of log-likelihood) can be written as:
\begin{equation}
    \begin{split}
        \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}&[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] = -\frac{N}{2}\ln(2\pi) + \frac{N}{2}\mb{E}_q[\ln\tau]-\frac{1}{2}\sum_{n=1}^{N}\mb{E}_q[\tau]\mb{E}_q[(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2]\\
        &=-\frac{N}{2}\ln(2\pi) + \frac{N}{2}(\psi(\rho)-\ln\iota)-\frac{1}{2}\sum_{n=1}^{N}\frac{\rho}{\iota}\mb{E}_q[(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2],
    \end{split}
\end{equation}
where $\psi(\cdot)$ is the digamma function.
The posterior expectation of model error is:
\begin{equation}
    \begin{split}
&\mb{E}_q[(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2]=y_n^2 -2y_n\boldsymbol{1}^{\T}\underset{k}{\circledast} \mathbf{g}^k(i_k^n,t_n) + \boldsymbol{1}^{\T}\underset{k}{\circledast} \text{vec}(\mathbf{g}^{k}(i_k^{n},t_n)\mathbf{g}^{k}(i_k^{n},t_n)^{\T}+\sigma^2\mf{I}).
    \end{split}
    \label{eq:model_err}
\end{equation}
If $\sigma=0$, then posterior expectation of model error becomes  $(y_n-\boldsymbol{1}^{\T}\underset{k}{\circledast} \mf{u}^k(i_k^n,t_n))^2$.

The second term of evidence lower bound can be written as\footnote{The KL divergence between two Gaussian distributions $p(x)\sim \mathcal{N}(x|\mu_1, \sigma_1^2)$ and $q(x)\sim \mathcal{N}(x|\mu_2, \sigma_2^2)$ can be computed using $\text{KL}(p\|q)=\frac{1}{2}[\ln(\frac{\sigma_2^2}{\sigma_1^2})+ \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2}-1]$. Detailed derivation can be found in Appendix \ref{GaussianKL}. }:
\begin{equation}
    \begin{split}
    \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}]&= \int\int q(\mathcal{U}, \boldsymbol{\lambda})\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})} d\mathcal{U}d\boldsymbol{\lambda} = \int\int q(\mathcal{U}, \boldsymbol{\lambda})\ln p(\mathcal{U}|\boldsymbol{\lambda})d\mathcal{U}d\boldsymbol{\lambda} - \int q(\mathcal{U})\ln q(\mathcal{U})d\mathcal{U}\\
    &=\int q(\mathcal{U})\ln p(\mathcal{U}|\boldsymbol{\lambda}=\mb{E}_q(\boldsymbol{\lambda}))d\mathcal{U} - \int q(\mathcal{U})\ln q(\mathcal{U})d\mathcal{U} \\
    &= -\text{KL}(q(\mathcal{U}) \| p(\mathcal{U}|\boldsymbol{\lambda}=\mb{E}_q(\boldsymbol{\lambda}))) = -\sum_{n=1}^N \sum_{k=1}^K\sum_{r=1}^R \frac{1}{2}[\ln(\frac{\beta_r}{\alpha_r\sigma^2})+ \frac{\alpha_r}{\beta_r}(\sigma^2+(\mathbf{g}^k_r(i_k^n, t_n))^2)-1],
    \end{split}
\end{equation}
where $\mathbf{g}^k_r(i_k^n, t_n)$ is the $r$-$th$ element of $\mathbf{g}^k(i_k^n, t_n)$.
%\frac{a_r^0}{\beta_r}

The third term of evidence lower bound can be written as\footnote{The KL divergence between two Gamma distributions $p(x) \sim \text{Gamma}(x|a_1, b_1)$ and $q(x)\sim \text{Gamma}(x|a_2, b_2)$ can be computed using $\text{KL}(p \parallel q) = a_2 \ln \frac{b_1}{b_2} - \ln\frac{\Gamma(a_2)}{\Gamma(a_1)}+(a_1 - a_2)\psi(a_1)-(b_2 - b_1)  \frac{a_1}{b_1}$. Detailed derivation can be found in \ref{GammKL}.}:
\begin{equation}
    \begin{split}
        \text{KL}(q(\boldsymbol{\lambda})\|p(\boldsymbol{\lambda}))= \sum_{r=1}^{R}
        a_0 \ln \frac{\beta_r}{b_0} - \ln\frac{\Gamma(a_0)}{\Gamma(\alpha_r)}+(\alpha_r - a_0)\psi(\alpha_r)-(b_0 - b_1)  \frac{\alpha_r}{\beta_r}.
        \label{eq:kl1}
    \end{split}
\end{equation}

The fourth term of evidence lower bound can be written as:
\begin{equation}
    \begin{split}
        \text{KL}(q(\tau)\|p(\tau))= c_0 \ln \frac{\iota}{d_0} - \ln\frac{\Gamma(c_0)}{\Gamma(\rho)}+(\rho - c_0)\psi(\rho)-(d_0 - \iota)  \frac{\rho}{\iota}.
        \label{eq:kl2}
    \end{split}
\end{equation}

\subsection{KL divergence of two Gaussian distribution}
\label{GaussianKL}
The Kullback-Leibler (KL) Divergence between two probability distributions \(p\) and \(q\) is defined as:
\[
\text{KL}(p \| q) = \int_{-\infty}^{\infty} p(x) \ln \left( \frac{p(x)}{q(x)} \right) dx.
\]
 Let \( p \sim \mathcal{N}(\mu_1, \sigma_1^2) \) and \( q \sim \mathcal{N}(\mu_2, \sigma_2^2) \), where the probability density functions (PDFs) are given by:
\[
p(x) = \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right),
\]
\[
q(x) = \frac{1}{\sqrt{2\pi \sigma_2^2}} \exp \left( -\frac{(x - \mu_2)^2}{2 \sigma_2^2} \right).
\]
Substitute the Gaussian PDFs into the definition of KL divergence:
\[
\text{KL}(p \| q) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \ln \left( \frac{\frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right)}{\frac{1}{\sqrt{2\pi \sigma_2^2}} \exp \left( -\frac{(x - \mu_2)^2}{2 \sigma_2^2} \right)} \right) dx.
\]
Simplify the logarithmic term:
\[
\ln \left( \frac{p(x)}{q(x)} \right) = \ln \left( \frac{\frac{1}{\sqrt{2\pi \sigma_1^2}}}{\frac{1}{\sqrt{2\pi \sigma_2^2}}} \right) + \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right)
\]
\[
= \ln \left( \frac{\sigma_2}{\sigma_1} \right) + \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right).
\]
Thus, the integral for KL divergence becomes:
\[
\text{KL}(p \| q) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \left( \ln \left( \frac{\sigma_2}{\sigma_1} \right) + \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right) \right) dx.
\]
Simplifying the Integral: We can now break the integral into two parts:
1. The constant term:
\[
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \ln \left( \frac{\sigma_2}{\sigma_1} \right) dx.
\]
Since \( \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \) is the PDF of a Gaussian distribution, its integral is 1, so this term evaluates to:
\[
\ln \left( \frac{\sigma_2}{\sigma_1} \right).
\]
2. The difference of squared terms:
\[
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_1^2}} \exp \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} \right) \left( -\frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_2)^2}{2 \sigma_2^2} \right) dx.
\]
This term can be split into two parts:
- The first part involves \( \mu_1 \), and after calculation, it simplifies to:
\[
\frac{\sigma_1^2}{2 \sigma_2^2} - \frac{1}{2}.
\]
The second part involves the difference between \( \mu_1 \) and \( \mu_2 \), and after calculation, it simplifies to:
\[
\frac{(\mu_1 - \mu_2)^2}{2 \sigma_2^2}.
\]
Combining all parts, the KL divergence between two Gaussian distributions is:
\[
\text{KL}(p \| q) = \ln \left( \frac{\sigma_2}{\sigma_1} \right) + \frac{\sigma_1^2}{2 \sigma_2^2} - \frac{1}{2} + \frac{(\mu_1 - \mu_2)^2}{2 \sigma_2^2}.
\]


\subsection{KL divergence of two Gamma distribution}
\label{GammKL}
The Kullback-Leibler (KL) Divergence between two probability distributions \( p \) and \( q \) is defined as:
\[
\text{KL}(p \parallel q) = \int_0^\infty p(x) \ln \left( \frac{p(x)}{q(x)} \right) dx.
\]
Let \( p \sim \text{Gamma}(a_1, b_1) \) and \( q \sim \text{Gamma}(a_2, b_2) \), where the probability density functions (PDFs) with rate parameters are given by:
\[
p(x) = \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)}, \quad x \geq 0,
\]
\[
q(x) = \frac{b_2^{a_2} x^{a_2 - 1} \exp\left( -b_2 x \right)}{\Gamma(a_2)}, \quad x \geq 0.
\]
Substitute the PDFs of the Gamma distributions into the definition of KL divergence:
\[
\text{KL}(p \parallel q) = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} \ln \left( \frac{\frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)}}{\frac{b_2^{a_2} x^{a_2 - 1} \exp\left( -b_2 x \right)}{\Gamma(a_2)}} \right) dx.
\]
Simplify the logarithmic term:
\[
\ln \left( \frac{p(x)}{q(x)} \right) = \ln \left( \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{b_2^{a_2} x^{a_2 - 1} \exp\left( -b_2 x \right)} \cdot \frac{\Gamma(a_2)}{\Gamma(a_1)} \right)
\]
\[
= (a_1 - a_2) \ln(x) + \left( -b_1 x + b_2 x \right) + \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) + (a_1 \ln(b_1) - a_2 \ln(b_2)).
\]
Thus, the integral becomes:
\[
\text{KL}(p \parallel q) = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} \left[ (a_1 - a_2) \ln(x) + (b_2 - b_1) x + \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) + (a_1 \ln(b_1) - a_2 \ln(b_2)) \right] dx.
\]
We now break this into four separate integrals.
\[
I_1 = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} (a_1 - a_2) \ln(x) dx.
\]
This integral can be solved using the properties of the Gamma distribution and the digamma function \( \psi(a) \):
\[
I_1 = (a_2 -  a_1) \left( \ln(b_1) - \psi(a_1) \right),
\]
where \( \psi(a) \) is the digamma function, the derivative of the logarithm of the Gamma function.
\[
I_2 = \int_0^\infty \frac{b_1^{a_1} x^{a_1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} (b_2 - b_1) dx.
\]
After performing the integration, we obtain:
\[
I_2 = (b_2 - b_1) \frac{\Gamma(a_1 + 1)}{\Gamma(a_1)} \cdot \frac{1}{b_1} = (b_2 - b_1) a_1 \frac{1}{b_1},
\]
\[
I_3 = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) dx.
\]
Since this is a constant term, we can immediately evaluate it:
\[
I_3 = \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right),
\]
\[
I_4 = \int_0^\infty \frac{b_1^{a_1} x^{a_1 - 1} \exp\left( -b_1 x \right)}{\Gamma(a_1)} (a_1 \ln(b_1) - a_2 \ln(b_2)) dx.
\]
This integral simplifies to:
\[
I_4 = a_1 \ln(b_1) - a_2 \ln(b_2).
\]
Combining all the parts, the KL divergence between two Gamma distributions with rate parameters is:
\[
\text{KL}(p \parallel q) = (a_2 - a_1) \left( \ln(b_1) - \psi(a_1) \right) + (b_2 - b_1) a_1 \frac{1}{b_1} + \ln \left( \frac{\Gamma(a_2)}{\Gamma(a_1)} \right) + a_1 \ln(b_1) - a_2 \ln(b_2).
\]

\subsection{Algorithm}
\label{ap:algorithm}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figure/algo1.png}
    %\caption{Enter Caption}
    \label{alg:1}
\end{figure}

% \begin{algorithm}[H]
%   \SetAlgoLined
% \KwIn{Training data $\mathcal{D}=\{y_n, \mf{i}_n, t_n\}_{n=1}^{N}$ }
% Collect all possible $I_k$ indexes for $K$ modes and $T$ possible timestamps.
%  Initialize  $\{\boldsymbol{\omega}_k\}_{k=1}^K, \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota$.
 
%   \While{not convergence}{
% Construct a set of initial ODE state tables $\mathcal{Z}_0$ using  Fourier features and Encoder, encompassing all possible indexes.

% \For{$i = 1, 2, \cdots, T$}{
%     $\mathcal{Z}(t_i)$ = \text{ODESolve}($\mathcal{Z}(t_{i-1})$, 
%     $\{h_{\boldsymbol{\theta}_k}\}_{k=1}^{K}$,$(t_{i-1},t_i))$
    
%     Compute necessary $\mathbf{g}^k(i_k,t_i)$ from $\mathcal{Z}(t)$ using \eqref{eq:latent-ode3}.  
%     }
% Take gradient step on \eqref{eq:loss}.
%     }
%   \caption{Training process of \MODEL}
%   \label{alg:1}
% \end{algorithm}



\subsection{Predictive distribution}
\label{pred_distr}
Through minimizing the negative log-marginal likelihood with observed training data, we can infer the distributions of the latent variables $q$, with which a predictive distribution  can be derived. Given index set $\{i^p_1,\cdots, i^p_k, t_p\}$, we are to predict the corresponding value, we have:
\begin{equation}
    \begin{split}
        p(y_p|\mathcal{D}) &\simeq \int p(y_p|\{\mf{u}^{k}_{i_k^p, t_p}\}_{k=1}^K, \tau)q(\{\mf{u}^{k}_{i_k^p, t_p}\}_{k=1}^K)q(\tau)d(\{\mf{u}^{k}_{i_k^p, t_p}\}_{k=1}^K) d\tau\\
        &=\int\int \mathcal{N}(y_p|\boldsymbol{1}^{\T}(\mf{u}^{1}_{i_1^p, t_p} \circledast \cdots \circledast \mf{u}^{k}_{i_k^p, t_p}),\tau^{-1}) \prod_{k=1}^K
        q(\mf{u}^{k}_{i_k^p, t_p})d(\mf{u}^{K}_{i_K^p, t_p})q(\tau)d\tau\\
        &=\int\int \mathcal{N}(y_p|\boldsymbol{1}^{\T}(\mf{u}^{1}_{i_1^p, t_p} \circledast \cdots \circledast \mf{u}^{K}_{i_K^p, t_p}),\tau^{-1}) \prod_{k=1}^K \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau\\
        &=\int\int \mathcal{N}(y_p|\boldsymbol{1}^{\T}(\mf{u}^{1}_{i_1^p, t_p} \circledast \cdots \circledast \mf{u}^{K}_{i_K^p, t_p}),\tau^{-1})\mathcal{N}(\mf{u}^{1}_{i_1^p, t_p}|\mathbf{g}^1(i_1^p, t_p),\sigma^2\mf{I})d(\mf{u}^{1}_{i_1^p, t_p}) \\& \qquad \qquad \prod_{k\ne 1} \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau \\
        &= \int\int \mathcal{N}(y_p|(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p})^{\T}\mf{u}^{1}_{i_1^p, t_p} ,\tau^{-1})\mathcal{N}(\mf{u}^{1}_{i_1^p, t_p}|\mathbf{g}^1(i_1^p, t_p),\sigma^2\mf{I})d(\mf{u}^{1}_{i_1^p, t_p}) \\& \qquad \qquad\prod_{k\ne 1} \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau\\
 & = \int\int \mathcal{N}(y_p|(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p})^{\T}\mathbf{g}^1(i_1^p, t_p) ,\tau^{-1}+\sigma^2(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p})^{\T}(\underset{k\ne 1}{\circledast} 
 \mf{u}^{k}_{i_k^p, t_p}) )\\& \qquad \qquad\prod_{k\ne 1} \mathcal{N}(\mf{u}^{k}_{i_k^p, t_p}|\mathbf{g}^k(i_k^p, t_p),\sigma^2\mf{I})d(\mf{u}^{k}_{i_k^p, t_p}) \text{Gamma}(\tau|\rho, \iota)d\tau\\
 &\qquad \vdots \\
 &=\int \mathcal{N}\big(y_p|\boldsymbol{1}^{\T}\underset{k}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p), \tau^{-1}+ \sigma^2\sum_{j=1}^K(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p))^{\T}(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p)\big) \text{Gamma}(\tau|\rho, \iota)d\tau \\
 &= \mathcal{T}\big(y_p|\boldsymbol{1}^{\T}\underset{k}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p),  \{\frac{\iota}{\rho}+\sigma^2\sum_{j=1}^K(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p))^{\T}(\underset{k\ne j}{\circledast} 
 \mathbf{g}^k(i_k^p, t_p))\}^{-1}, 2\rho\big).
    \end{split}
\end{equation}
We found the prediction distribution follows the student's-t distribution.




\newpage
\section{Additional experiment results.}
\subsection{Experiment settings: synthetic data}
\label{ap:setting1}
The \MODEL  was implemented with PyTorch \cite{paszke2019pytorch} and $\texttt{torchdiffeq}$ library (\url{https://github.com/rtqichen/torchdiffeq}). We employed a single hidden-layer neural network (NN) to parameterize the encoder. Additionally, we used two  NNs, each with two hidden layers, for derivative learning and for parameterizing the decoder, respectively. Each layer in all networks contains 100 neurons.
  We set the dimension of Fourier feature $M=32$, the  ODE state $J=5$ and the initial number of components  of the latent factor trajectories $R=5$.  The \MODEL  was trained  using Adam \cite{kingma2014adam} optimizer with the learning rate set as $5e^{-3}$. The hyperparamters $\{a_r^0, b_r^0\}_{r=1}^{R},c^0, d^0$ and initial values of learnable parameters $\{\alpha_r, \beta_r\}_{r=1}^{R},\rho, \sigma^2,  \iota$ are  set to $1e^{-6}$ (so that all the initial posterior means of $\{\lambda_r\}_{r=1}^{R}$ equal $1$). We ran 2000 epochs, which is sufficient for convergence.
  
\subsection{Experiment settings: real-world data}
\label{ap:setting2}
For THIS-ODE, we used a two-layer network with the layer width  chosen from $\{50, 100\}$. For DEMOTE, we used two hidden layers for both the reaction process and entry value prediction, with the layer width chosen from $\{50, 100\}$. For LRTFR, we  used two hidden layers with $100$ neurons to  parameterize the latent function of each mode.
We varied $R$  from $\{3,5,7\}$ for all baselines.
 For deep-learning based methods, we all used  $\texttt{tanh}$ activations. For FunBaT, we varied \text{Matérn Kernel} $\{1/2, 3/2\}$ along the kernel parameters for optimal performance for different datasets.
We use ADAM optimizer with the learning rate tuned from  $\{5e^{-4}, 1e^{-3}, 5e^{-3}, 1e^{-2}\}$. 


\subsection{Rank learning curves of different datasets}
\label{ap:learning_curve}
\begin{figure*}[h]
    \centering
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/syn_uv_sum1.png}
        \caption*{(a) Power of learned factor trajectories.}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/syn_uv_sum2.png}
        \caption*{(b) Posterior means of $\boldsymbol{\lambda}$.}
    \end{minipage}
    \caption{Rank learning curves of the synthetic data.}
    \label{fig:learning_curve_syn}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/traffic_uv_sum1.png}
        \caption*{(a) Power  of learned factor trajectories (CA traffic).}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/traffic_uv_sum2.png}
        \caption*{(b) Posterior means of $\boldsymbol{\lambda}$ (CA traffic).}
    \end{minipage}
    
    \vspace{2mm}
        \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/server_uv_sum1.png}
        \caption*{(c) Power  of learned factor trajectories (Server).}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/server_uv_sum2.png}
        \caption*{(d) Posterior means of $\boldsymbol{\lambda}$ (Server).}
    \end{minipage}
    
        \vspace{2mm}
        \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_uv_sum1.png}
        \caption*{(e) Power  of learned factor trajectories (SSF).}
    \end{minipage}
    \begin{minipage}{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_uv_sum2.png}
        \caption*{(f) Posterior means of $\boldsymbol{\lambda}$ (SSF).}
    \end{minipage}
    \caption{Rank learning curves of three datasets.}
    \label{fig:learning_curve_three}
\end{figure*}

Fig.~\ref{fig:learning_curve_syn}  plots the rank-learning curves during the gradient descent iterations of the synthetic data. That is, the evolutions of (a) the power of $R$ components of the estimated posterior mean of the factor trajectories \footnote{We define the power  of $r$-th component of factor trajectories as: $\sum_{k=1}^{K}\int_{i_k}\int_{t}(\mathbf{g}^k_r(i_k,t))^2di_kdt$, which represents  the contribution of the $r$-th component to the final output. The power can be approximated using $\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{g}^k_r(i_k^n,t_n))^2$.}
and (b) the values of estimated posterior mean of $\{\lambda_r\}_{r=1}^{R}$. Note that the power of $r$-$th$ component of factor trajectories is conditioned by $\lambda_r$ (as shown \eqref{term2} and \textit{Remark 1}), and we plot the pair with the same color. One can see that as the epoch increases, the power of 4 components of the factor trajectories are forced to 0, which  aligns with the increments of 4 corresponding $\lambda_r$s. And we can manually exclude the four components, which will not affect the final prediction results\footnote{Our criterion for excluding the $r$-$th$ rank is when $\mathbb{E}(\lambda_r)$ is large and power $\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{g}^k_r(i_k^n,t_n))^2$ is relatively small.}. Only the third component ($r=3$) is activated after convergence and $\lambda_3$ settles at a small value correspondingly. This indicates that our method successfully identifies the true underlying rank (i.e., 1) of the synthetic data while effectively pruning the other four components. Fig.~(\ref{fig:learning_curve_three}) plots the rank-learning curves of the CA traffic, Server and SSF datasets respectively. In the same sense, we can infer that the revealed ranks of these three datasets are 5,7,7 respectively.
\label{ap:b}





\subsection{Noise robustness}
\label{ap:noise_rob}
\begin{table*}[h]
\small
\centering
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{|c|c|c|c|c|}
\hline 
\textbf{} & \MODEL  \quad & \MODEL w.o. ARD & 
 LRTFR (R=5)   & DEMOTE (R=5) \\ \hline
Noise Variance & \multicolumn{4}{c|}{\textbf{MAE}}  \\ \hline
% 0.3 &\textbf{0.299 $\pm$ 0.026} &0.339 $\pm$ 0.017 &0.542 $\pm$ 0.008 &0.463 $\pm$ 0.004 & 0.399 $\pm$ 0.001 \\ \hline
0.5 & \textbf{0.134 $\pm$ 0.001}&0.169 $\pm$ 0.022 & 0.213 $\pm$ 0.007
   &0.148 $\pm$ 0.003
 \\ \hline
1 &\textbf{0.182 $\pm$ 0.006} & 0.219 $\pm$ 0.006 &0.305 $\pm$ 0.009
 &0.219 $\pm$ 0.034
 \\ \hline
\end{tabular}
\caption{Extra experimental results  on the robustness of automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
\label{Table:as1}
\vspace{-15pt}
\end{table*}
\begin{table*}[h]
\small
\centering
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\textbf{} & LRTFR (R=7)   & DEMOTE (R=7) & 
 LRTFR (R=10)   & DEMOTE (R=10)& 
 LRTFR (R=15)   & DEMOTE (R=15) \\ \hline
 Noise Variance & \multicolumn{6}{c|}{\textbf{RMSE}}  \\ \hline
0.5 & 0.484 $\pm$ 0.023&0,423 $\pm$ 0.019 &0.469 $\pm$ 0.053 &0.4375 $\pm$ 0.005 &0.475 $\pm$ 0.018 &0.455 $\pm$ 0.005\\ \hline
1   & 0.5475 $\pm$ 0.011 &0.517 $\pm$ 0.020 &0.621 $\pm$ 0.038&0.547 $\pm$ 0.024&0.708 $\pm$ 0.013&0.552 $\pm$ 0.003\\ \hline
Noise Variance & \multicolumn{6}{c|}{\textbf{MAE}}  \\ \hline
0.5& 0.224 $\pm$ 0.019& 0.157 $\pm$ 0.006 &0.232 $\pm$ 0.012&0.148 $\pm$ 0.001&0.245 $\pm$ 0.011&0.140 $\pm$ 0.002\\ \hline
1   & 03335 $\pm$ 0.018&0.209 $\pm$ 0.020 &0.402 $\pm$ 0.032 &0.312 $\pm$ 0.013&0.469 $\pm$ 0.0135&0.319 $\pm$ 0.0025\\ \hline
\end{tabular}
\caption{Extra experimental results  on the robustness of automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
\label{Table:as2}
\vspace{-15pt}
\end{table*}

\subsection{Running time}
\label{ap:running}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c|c|c|c}
\hline
 \textbf{} & \textbf{CA traffic} & \textbf{Server Room} & \textbf{SSF}  \\
\hline

\textbf{THIS-ODE} & 283.9 & 144.8 & 158.9  \\

\textbf{NONFAT} & 0.331 & 0.81 & 0.67  \\

\textbf{DEMOTE} & 0.84 & 7.25 & 1.08  \\
\textbf{FunBaT-CP} & 0.059 & 0.027 & 0.075  \\
\textbf{FunBaT-Tucker} & 2.13 & 3.20 & 2.72  \\
\textbf{LRTFR} & 0.098 & 0.178 & 0.120  \\
\textbf{GRET} & 0.227 & 0.83 & 0.247  \\
\hline
\end{tabular}
\caption{Per-epoch/iteration running time of different methods (in seconds).}
\label{Tab:runtime}
\end{table}








