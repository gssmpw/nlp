
\section{Experiment}
\subsection{Synthetic Data}
\label{syt_data}
\begin{figure}[t]
    \centering
    \begin{minipage}{0.242\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Syn1_1.png}
        \caption*{(a) $\bc{Y}(0.152, 0.823,  t)$}
    \end{minipage}
    \hspace{-0.22cm} % 调整两张图片之间的间距
    % \begin{minipage}{0.242\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figure/Syn1_2.png}
    %     \caption*{(b) $\bc{Y}(0.406, 0.133,  t)$}
    % \end{minipage}
    % \vspace{-0.02cm}
    % \begin{minipage}{0.242\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figure/Syn1_3.png}
    %     \caption*{(c) $\bc{Y}(0.679, 0.553,  t)$}
    % \end{minipage}
    % \hspace{-0.22cm} % 调整两张图片之间的间距
    \begin{minipage}{0.242\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Syn1_4.png}
        \caption*{(b) $\bc{Y}(0.992, 0.982,  t)$}
    \end{minipage}
    
    \caption{Prediction results on different coordinates.}
    \label{fig:syn1}
    \vspace{-0.05in}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{figure/Syn2.png}
    \caption{Visualizations of learned factor trajectories at timestamp $t=0.235$. Only the $3_{\text{rd}}$ component is revealed to be informative and others are pruned to be zero.}
    \label{fig:s2}
    \vspace{-0.05in}
\end{figure}





\begin{table*}[t]
\small
\centering
\renewcommand{\arraystretch}{1.015}
\begin{tabular}{c|c c c| c c c}
\hline & \multicolumn{3}{c|}{\textbf{RMSE}} & \multicolumn{3}{c}{\textbf{MAE}} \\ \quad 
\text{Datasets} & \textit{CA Traffic} & \textit{Server Room} & \textit{SSF} & \textit{CA Traffic} & \textit{Server Room} & \textit{SSF}\\ \hline
\multicolumn{7}{c}{$R=3$} \\ \hline
%P-Tucker  & 0.358 $\pm$ 0.012 & 0.677 $\pm$ 0.129 & 1.446 $\pm$ 0.007 & 0.207 $\pm$ 0.035 & 0.323 $\pm$ 0.053 & 1.089 $\pm$ 0.007 \\ 


THIS-ODE & 0.672 $\pm$ 0.002  & 0.132 $\pm$ 0.002 & 2.097 $\pm$ 0.003 & 0.587 $\pm$ 0.002 & 0.083 $\pm$ 0.002 & 2.084 $\pm$ 0.003\\ 
NONFAT & 0.504 $\pm$ 0.010 &  0.129 $\pm$ 0.002 & 9.796 $\pm$ 0.010 & 0.167 $\pm$ 0.009 & 0.078 $\pm$ 0.001  & 8.771 $\pm$ 0.043 \\
DEMOTE & 0.447 $\pm$ 0.001 &  0.131 $\pm$ 0.001 & 9.789 $\pm$ 0.001 & 0.118 $\pm$ 0.002 & 0.090 $\pm$ 0.0015  & 8.757 $\pm$ 0.001 \\ 

FunBaT-CP & 0.563 $\pm$ 0.025 & 0.425 $\pm$ 0.003 & 0.696 $\pm$ 0.047 & 0.244 $\pm$ 0.025 & 0.308 $\pm$ 0.001 & 0.549 $\pm$ 0.038 \\ 
FunBaT-Tucker & 0.584 $\pm$ 0.009 & 0.498 $\pm$ 0.058 & 0.730 $\pm$ 0.201 & 0.189 $\pm$ 0.014 & 0.381 $\pm$ 0.053 & 0.614 $\pm$ 0.128 \\ 
LRTFR & 0.379 $\pm$ 0.042 & 0.151 $\pm$ 0.004 & 0.595 $\pm$ 0.018 & 0.187 $\pm$ 0.022 & 0.110 $\pm$ 0.002 & 0.464 $\pm$ 0.0165 \\ \hline


\multicolumn{7}{c}{$R=5$} \\ \hline
%P-Tucker  & 0.582 $\pm$ 0.001 & 0.458 $\pm$ 0.039 & 1.728 $\pm$ 0.003 & 0.307 $\pm$ 0.001 & 0.259 $\pm$ 0.007 & 1.378 $\pm$ 0.004 \\ 


THIS-ODE & 0.632 $\pm$ 0.002 & 0.132 $\pm$ 0.003 & 1.039 $\pm$ 0.015 & 0.552 $\pm$ 0.001 &   0.083 $\pm$ 0.002 &1.032 $\pm$ 0.002\\ 
NONFAT & 0.501 $\pm$ 0.002 &  0.117 $\pm$ 0.006 & 9.801 $\pm$ 0.014 & 0.152 $\pm$ 0.001 & 0.071 $\pm$ 0.004  & 8.744 $\pm$ 0.035 \\ 
DEMOTE & 0.421 $\pm$ 0.002 &  0.105 $\pm$ 0.003 & 9.788 $\pm$ 0.001 & 0.103 $\pm$ 0.001 & 0.068 $\pm$ 0.003  & 8.757 $\pm$ 0.001 \\ 


FunBaT-CP & 0.547 $\pm$ 0.025& 0.422 $\pm$ 0.001 & 0.675 $\pm$ 0.061 & 0.204 $\pm$ 0.052 & 0.307 $\pm$ 0.002 &  0.531 $\pm$ 0.051 \\ 
FunBaT-Tucker & 0.578 $\pm$ 0.005 & 0.521$\pm$ 0.114 & 0.702 $\pm$ 0.054& 0.181 $\pm$ 0.005 & 0.391 $\pm$ 0.097& 0.557 $\pm$ 0.041 \\

LRTFR & 0.376 $\pm$ 0.016 & 0.167 $\pm$ 0.006 & 0.532 $\pm$ 0.036 & 0.182 $\pm$ 0.012 & 0.121 $\pm$ 0.005 & 0.418 $\pm$ 0.003 \\ \hline

\multicolumn{7}{c}{$R=7$} \\ \hline
%P-Tucker  & 0.594 $\pm$ 0.001 & 0.665 $\pm$ 0.103 & 1.468 $\pm$ 0.001 & 0.283 $\pm$ 0.001 & 0.305 $\pm$ 0.042 & 1.139 $\pm$ 0.002 \\ 



THIS-ODE & 0.628 $\pm$ 0.007 & 0.154 $\pm$ 0.016 & 1.685 $\pm$ 0.009 & 0.548 $\pm$ 0.006 & 0.089 $\pm$ 0.002 & 1.674 $\pm$ 0.008\\ 
NONFAT & 0.421 $\pm$ 0.016 &  0.128 $\pm$ 0.002 & 9.773 $\pm$ 0.015 & 0.137 $\pm$ 0.006 & 0.077 $\pm$ 0.002  & 8.718 $\pm$ 0.035 \\ 
DEMOTE & 0.389 $\pm$ 0.005 &  0.094 $\pm$ 0.006 & 9.790 $\pm$ 0.002 & 0.091 $\pm$ 0.001 & 0.062 $\pm$ 0.006  & 8.753 $\pm$ 0.006 \\ 


FunBaT-CP & 0.545 $\pm$ 0.009& 0.426 $\pm$ 0.001 & 0.685 $\pm$ 0.049 & 0.204 $\pm$ 0.037 & 0.307 $\pm$ 0.001 & 0.541 $\pm$ 0.039 \\ 
FunBaT-Tucker  &0.587 $\pm$ 0.011 & 0.450 $\pm$ 0.041 & 0.642 $\pm$ 0.037& 0.195 $\pm$ 0.022 & 0.330 $\pm$ 0.026 & 0.507 $\pm$ 0.029 \\ 
LRTFR & 0.365 $\pm$ 0.042 &  0.156 $\pm$ 0.012 & 0.502 $\pm$ 0.033 & 0.161 $\pm$ 0.014 & 0.118 $\pm$ 0.009  & 0.392 $\pm$ 0.028 \\ \hline



\multicolumn{7}{c}{Automatic Rank Determination} \\ \hline
\MODEL(Ours)  &\textbf{ 0.284 $\pm$ 0.016} & \textbf{0.078 $\pm$ 0.001} & \textbf{0.373 $\pm$ 0.003} & \textbf{0.085 $\pm$ 0.004} & \textbf{0.047 $\pm$ 0.003} & \textbf{0.288 $\pm$ 0.003} \\ \hline
\end{tabular}
\caption{Predictive errors and standard deviation. The results were averaged over five runs.}
\label{Tab:results}
\vspace{-3pt}
\end{table*}







\begin{figure*}[t]
    \centering
    \begin{minipage}{0.242\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_variance_power.png}
        \caption*{(a) Posterior mean of $\frac{1}{\boldsymbol{\lambda}}$}
    \end{minipage}
     \hspace{-0.22cm} % 调整两张图片之间的间距
    \begin{minipage}{0.242\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_lf1.png}
        \caption*{(b) Factor trajectories}
    \end{minipage}    
     \hspace{-0.22cm} % 调整两张图片之间的间距
    \begin{minipage}{0.242\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/ssf_visual1.png}
        \caption*{(c) Entry value predictions}
    \end{minipage}
    \hspace{-0.22cm} % 调整两张图片之间的间距
    \begin{minipage}{0.242\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/syn_scability.png}
        \caption*{(d) Scalability}
    \end{minipage}
    \caption{Illustrations on (a) the 
    posterior mean of the variance 
    %(i.e.,  $1/\mathbb{E}_{q}(\boldsymbol{\lambda})$)
    from the SSF dataset ($R=10$),  (b) the dominant components of learned depth-mode factor trajectories  from the SSF dataset,  (c) the entry value predictions indexed in ($17^{\circ}$N,  $114.7^{\circ}$E,  30m) of the SSF dataset,  (d) the scalability over the length of time series on  synthetic dataset.}
    \label{fig:last}
    \vspace{-12pt}
\end{figure*}




% \begin{table*}[h]
% \small
% \centering
% \renewcommand{\arraystretch}{1}
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline 
% \textbf{} & \MODEL  \quad & \MODEL w.o. ARD & 
%  FunBat-CP (R=7) & 
%  NONFAT (R=7)  & DEMOTE (R=7) \\ \hline
% Noise Variance & \multicolumn{5}{c|}{\textbf{RMSE}}  \\ \hline
% % 0.3 &\textbf{0.299 $\pm$ 0.026} &0.339 $\pm$ 0.017 &0.542 $\pm$ 0.008 &0.463 $\pm$ 0.004 & 0.399 $\pm$ 0.001 \\ \hline
% 0.5 & \textbf{0.305 $\pm$ 0.005}&0.356 $\pm$ 0.010 & 0.554 $\pm$ 0.007 &0.495 $\pm$ 0.018 &0.423 $\pm$ 0.019\\ \hline
% 1 &\textbf{0.406 $\pm$ 0.007} & 0.461 $\pm$ 0.015 &0.563 $\pm$ 0.006 & 0.536 $\pm$ 0.006 & 0.547 $\pm$ 0.001\\ \hline
% % \textbf{} & \multicolumn{5}{c|}{\textbf{MAE}}  \\ \hline
% % % 0.3 &\textbf{0.115 $\pm$ 0.009} & 0.117 $\pm$ 0.005& 0.217 $\pm$ 0.036& 0.143 $\pm$ 0.002 &0.118 $\pm$ 0.001 \\ \hline
% % 0.5 &\textbf{0.134 $\pm$ 0.001} &0.169 $\pm$ 0.022& 0.216 $\pm$ 0.034&0.148 $\pm$ 0.004 & 0.157 $\pm$ 0.006 \\ \hline
% % 1 &\textbf{0.182 $\pm$ 0.006} &0.219 $\pm$ 0.006& 0.239 $\pm$ 0.025&0.193 $\pm$ 0.004&0.268 $\pm$ 0.003\\ \hline
% \end{tabular}
% \caption{Experiments  on the robustness of automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
% \label{Table:as}
% \end{table*}
% \vspace{-2mm}
 



\begin{table*}[h]
\small
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|c|c|}
\hline 
\textbf{} & \MODEL  \quad & \MODEL w.o. ARD & 
 LRTFR (R=5)   & DEMOTE (R=5) \\ \hline
Noise Variance & \multicolumn{4}{c|}{\textbf{RMSE}}  \\ \hline
% 0.3 &\textbf{0.299 $\pm$ 0.026} &0.339 $\pm$ 0.017 &0.542 $\pm$ 0.008 &0.463 $\pm$ 0.004 & 0.399 $\pm$ 0.001 \\ \hline
0.5 & \textbf{0.305 $\pm$ 0.005}&0.356 $\pm$ 0.010 & 0.434 $\pm$ 0.047 &0.430 $\pm$ 0.010\\ \hline
1 &\textbf{0.406 $\pm$ 0.007} & 0.461 $\pm$ 0.015 &0.516 $\pm$ 0.034 &0.552 $\pm$ 0.009\\ \hline
% \textbf{} & \multicolumn{5}{c|}{\textbf{MAE}}  \\ \hline
% % 0.3 &\textbf{0.115 $\pm$ 0.009} & 0.117 $\pm$ 0.005& 0.217 $\pm$ 0.036& 0.143 $\pm$ 0.002 &0.118 $\pm$ 0.001 \\ \hline
% 0.5 &\textbf{0.134 $\pm$ 0.001} &0.169 $\pm$ 0.022& 0.216 $\pm$ 0.034&0.148 $\pm$ 0.004 & 0.157 $\pm$ 0.006 \\ \hline
% 1 &\textbf{0.182 $\pm$ 0.006} &0.219 $\pm$ 0.006& 0.239 $\pm$ 0.025&0.193 $\pm$ 0.004&0.268 $\pm$ 0.003\\ \hline
\end{tabular}
\caption{Experiments  on the robustness of automatic rank determination mechanism against the noise on the CA traffic dataset. The results were averaged over five runs.}
\label{Table:as}
\vspace{-12pt}
\end{table*}

\begin{table}[ht]
\small
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|c|c|c|c|c|}
\hline 
$J$ & 6  & 8 &
 10 & 
 16  \\ \hline
RMSE &\textbf{0.279}&0.294 & 0.284  &0.290 \\ \hline
MAE &0.090 & 0.088 & \textbf{0.085} & 0.088 \\ \hline
\end{tabular}
\caption{Performance of \MODEL under different $J$ on the CA traffic dataset. The results were averaged over five runs.}
\label{Table:J}
\vspace{-16pt}
\end{table}



We first evaluated \MODEL  on a synthetic  task.
We generated a two-mode temporal tensor,  and each entry is defined as: 
\begin{equation}
\begin{split}
    	&\bc{Y}(i_1,  i_2,  t) = \boldsymbol{1}^{\T} [\mf{u}^{1}(i_1, t)\circledast \mf{u}^{2}(i_2, t)], \\
     & \mf{u}^{1}(i_1, t)= -\cos^{3}(2\pi t + 2.5\pi i_1), \\
     & \mf{u}^{2}(i_2, t)= \sin(3\pi t+3.5\pi i_2).
\end{split}
\label{eq:syn}
\end{equation}
We randomly sampled $25\times 25 \times 50$ off-grid indexes entries from interval $ [0, 1] \times [0, 1] \times [0, 1]$.
We added  Gaussian noise $\boldsymbol{\epsilon} \sim \mathcal{N}(0,  0.05) $ to the  generated data.
We  randomly selected $20\%$ of the data (6250 points in total) as the training data. Detailed model settings can be found in Appendix~\ref{ap:setting1}.


In Figure~\ref{fig:syn1},  we showed  the predictive trajectories of entry value  indexed in different coordinates. 
The dotted line represents the ground truth and the full line represents the  the predictive mean  learned by our model. The cross symbols represent the training points. The shaded region represents  the predictive   uncertainty region.
One can see that although the training points are sparse and noisy,   \MODEL  accurately recovered the ground truth,  demonstrating that it has effectively captured the  temporal dynamics. 
Figure~\ref{fig:s2}  depicts $R$ components of the learned  factor trajectories   at timestamp $t=0.235$.
 One can see that \MODEL  identifies the underlying rank (i.e.,  1) through uniquely  recovering the real mode functions and other four components are learned to be zero. More detailed interpretations on the rank revealing   process  were   provided  in Appendix \ref{ap:learning_curve}.











\subsection{Real-world Data}
 \textbf{Datasets:} We examined \MODEL  on three real-world benchmark datasets. (1) CA traffic,   lane-blocked records  in California from January 2018 to December 2020. We extracted a three-mode temporal tensor between 5 severity levels,  20 latitudes and 16 longitudes. We collected 10K entry values and their timestamps.\url{(https://smoosavi.org/dataset s/lstw}; (2) Server Room,  temperature logs of Poznan Supercomputing and Networking Center. We extracted a three-mode temporal tensor between 3 air conditioning modes ($24^{\circ}$,  $27^{\circ}$ and $30^{\circ}$),  3 power usage levels $(50\%,  75\%,  100\%)$ and 34 locations. We collected 10K entry values and their timestamps.(\url{https://zenodo.org/record/3610078#%23.Y8SYt3bMJGi}); (3) SSF,  sound speed field measurements in the pacific ocean covering the region between latitudes $17^{\circ}$N $\sim 20^{\circ}$N,  longitude $114.7^{\circ}$E $\sim 117.7^{\circ}$E and depth $0$m $\sim200$m. 
We extracted a three-mode continuous-indexed temporal tensor data contains 10K observations across 10 latitudes,  20 longitudes,  10 depths  and 34 timestamps over 4 days. (\url{https://ncss.hycom.org/thredds/ncss/grid/GLBy0.08/expt_93.0/ts3z/dataset.html}).

\textbf{Baselines and Settings:} We compared \MODEL with state-of-the-art temporal and functional tensor methods: (1) THIS-ODE~\citep{thisode},  a continuous-time decomposition using a neural ODE to estimate tensor entries from static factors and time; (2) NONFAT~\citep{NONFAT},  a bi-level latent GP model that estimates dynamic factors  with  Fourier bases; (3) DEMOTE~\citep{wang2023dynamicdemote},  a neural diffusion-reaction process model for learning dynamic factors in tensor decomposition; (4) FunBaT~\citep{fang2023functional},  a Bayesian method using GPs as functional priors for continuous-indexed tensor data;  (5) LRTFR~\citep{luo2023lowrank},  a low-rank functional Tucker model that uses factorized neural representations for decomposition. 
We followed~\citep{wang2023dynamicdemote, fang2023functional} to randomly draw $80\%$ of observed entries for training  and the rest for testing. 
The performance metrics include the root-mean-square error (RMSE) and the  mean average error (MAE).
Each experiment was conducted five times and we reported the average test errors along with their standard deviations. For \textsc{Gret},  we set  the ODE state dimension $J=10$ and  the initial number
of components of the factor trajectories $R = 10$, 
while keeping the other configurations the same as in Sec.~\ref{syt_data}. We provided more detailed baseline settings  in Appendix~\ref{ap:setting2}.



\textbf{Prediction Performance: }
Table~\ref{Tab:results} shows that \MODEL  is superior to the other baselines by a large margin in all cases,  without  needing to tune the   rank. The learned ranks of the CA traffic,  Server Room and SSF datasets are $5,  7,  7$ respectively. 
We illustrated  their rank-learning curves  of three datasets in Figure~\ref{fig:learning_curve_three} in  Appendix \ref{ap:b}.
We observed that methods which do not consider the continuously indexed mode (e.g.,  NONFAT,  DEMOTE,  THIS-ODE) perform poorly on the SSF dataset. In contrast,  approaches that leverage this continuity achieve significantly better results. This is because the SSF dataset exhibits strong continuity across three modes,  and methods that fail to incorporate this information struggle to deliver satisfactory reconstructions.



\textbf{Revealed Rank Analysis and Interpretability:}
We analyzed the revealed rank and  the learned factor  trajectories of the SSF dataset. 
  Figure~\ref{fig:last}(a) shows the posterior mean of  the variance of the learned  factor trajectories (i.e.,  $\mathbb{E}_{q}(\frac{1}{\boldsymbol{\lambda}})$),  which governs the fluctuations of their corresponding $R=10$ components of factor  trajectories.
One can see that  $\mathbb{E}_q(\frac{1}{\lambda_3}), \mathbb{E}_q(\frac{1}{\lambda_{4}})$ and $\mathbb{E}_q(\frac{1}{\lambda_{8}})$ are small, indicating that these components concentrate around zero and  can be pruned without affecting the final predictions. Thus,  our method revealed the rank of the SSF dataset to be  7.
Additionally,   
$\mathbb{E}_q(\frac{1}{\lambda_1})$ and $\mathbb{E}_q(\frac{1}{\lambda_{10}})$  dominate,  indicating that the corresponding $1_{\text{st}}$ and $10_{\text{th}}$ components of the  factor trajectories form the primary structure of the data. 
To illustrate this,  we plotted these two components of the depth-mode factor trajectories  at depth 30m in Figure~\ref{fig:last}(b).
 As we can see,  the trajectories show periodic patterns,  which are  influenced by the the day-night cycle of ocean temperature.
We also compared the predicted curves of \MODEL and LRTFR for an entry  
 located at  $17^{\circ}$N,  $114.7^{\circ}$E and a depth of 30m.  \MODEL outperforms LRTFR,  providing more accurate predictions with uncertainty quantification,  even outside the training region (right to the dashed vertical line).
  This suggests that our model holds promise for extrapolation tasks.
The results collectively highlighted the advantage of \MODEL  in capturing continuous-indexed multidimensional dynamics,  which is crucial for analyzing real-world temporal data and performing predictive tasks.



\textbf{Robustness against Noise:}
The incorporated automatic rank determination (ARD) mechanism can reveal the underlying rank of the  temporal data and prune the unnecessary components of the  factor trajectories,  helping to improve model's robustness against noise. To evaluate its performance,  we  added Gaussian noise with varying  variance levels to the training set of CA traffic dataset and compared the results of different methods,  as  summarized in Table~\ref{Table:as}.
We disabled ARD by using a simple mean square error criterion as the objective function to constitute an ablation study.   \MODEL  achieves lower prediction errors than \MODEL w.o. ARD,  demonstrating that the introduced ARD offers superior noise robustness. Our proposed method also shows better noise impedance than the other baselines. Due to the limited space, we provided more detailed results of the baselines with varying ranks in Table~\ref{Table:as1} and Table~\ref{Table:as2} in Appendix~\ref{ap:noise_rob}.






\textbf{Sensitivity and Scalability:}
We  examined the sensitivity of \MODEL with respect to the dimensionality of ODE state $J$ on the CA traffic dataset. The results were given in Table~\ref{Table:J}. Empirically, \MODEL performs consistently well across different $J$.
We further evaluated the scalability of \MODEL  with respect to the  length of  the time series $T$ and   $J$. For $T$,    we randomly sampled $25 \times 25 \times T$ off-grid index entries from the interval $ [0, 1] \times [0, 1] \times [0, 1]$ using
Eq.~\eqref{eq:syn} for training. We varied $T$ across $\{50,  200,  500,  1000\}$ and $J$ across $\{5,  25,  30\}$.   To better quantify the scalability,  we  employed the simple Euler scheme with fixed step size~\citep{platen2010numerical} for network training.  The results  shown in Figure~\ref{fig:last}(d)  indicate that the running time of \MODEL  grows linearly with $T$  and  is insensitive to   $J$,  demonstrating its suitability for large-scale applications. 

\textbf{Computational Efficiency:} We  compared the per-epoch/iteration running time of  \MODEL  with the other methods. We tested all the methods in a computer with one NVIDIA GeForce RTX 4070 Graphics Card,  13th Generation Intel Core i9-13900H Processor,  32 GB RAM,  and 1 TB SSD. The results are shown in Table~\ref{Tab:runtime} in Appendix~\ref{ap:running}. One can see that \MODEL  runs slightly faster than NONFAT and DEMOTE, and significantly faster than THIS-ODE.




