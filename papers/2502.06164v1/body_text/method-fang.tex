\section{Methodology}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/flowchart.pdf}
    \caption{Graphical illustration of the proposed \textsc{Gret} (the case of $K=3$).}
    \label{fig:flowchart}
    \vspace{-0.05in}
\end{figure*}
Despite recent advances in modeling temporal tensors, most of these methods are still unsuitable for generalized tensor data with continuous indexes across all domains. 
While functional tensor methods offer greater flexibility, they simply treat temporal dynamics as an independent mode, which tends to underfit the inherent complexity of the temporal dynamics.  Furthermore, rank determination remains a less explored issue in temporal tensor models.
To address these issues, we propose \textsc{GreT}, a novel temporal tensor model that integrates the continuous-indexed  features into a latent ODE  model with rank-revealing prior. 

Without loss of generality, we consider a $K$-mode generalized temporal tensor  with continuous indexes over all domains, and it actually corresponds to a function $\mf{F}(i_1, \cdots, i_K, t):\mb{R}_{+}^{K+1}\rightarrow \mb{R}^{1}$ to map the continuous indexes and timestamp to the tensor entry, denoted as $y_{\mf{i}}(t) = \mf{F}(i_1, \cdots, i_K, t)$. We assume the function $\mf{F}(i_1, \cdots, i_K, t)$ can be factorized into $K$  factor trajectories following the CP format with rank $R$, i.e., 
\vspace{-2mm}
\begin{equation}    
    y_{\mf{i}}(t) = \mf{F}(i_1, \cdots, i_K, t) \approx \boldsymbol{1}^{\T}[\underset{k}{\circledast}  \mf{u}^k(i_k, t)], \label{eq:gen_CP}
\end{equation}
where $\mf{u}^k(i_k, t):\mb{R}_{+}^{2}\rightarrow \mb{R}^{R}$  is the trajectory of latent factor, which is a $R$-size vector-valued function mapping the continuous index $i_k$ of mode-$k$ and timestamp $t$ to a $R$-dimensional latent factor. We claim that the proposed model \eqref{eq:gen_CP} is a generalization of existing temporal tensor methods \eqref{eq:temporal_CP} via modeling continuous-indexed patterns not only in the temporal mode but in all modes. If we restrict $i_k$ to finite and discrete, \eqref{eq:gen_CP} will degrade to \eqref{eq:temporal_CP}. Compared to the fully-factorized functional tensor \eqref{eq:function_CP}, the proposed method \eqref{eq:gen_CP} explicitly models the time-varying factor trajectories of all modes, known as dynamic factor learning~\citep{SFTL,NONFAT}. Given the fact that temporal mode always dominates and interacts with other modes, the proposed method is expected to improve the model's capability by learning time-varying representations in dynamical data.

\subsection{Continuous-indexed Latent-ODE}\label{sec:Latent-ODE-Flow}
To allow flexible modeling of the factor trajectory and continuous-indexed information, 
 we propose a temporal function $\mf{g}^k(i_k,t):\mb{R}^{2}_{+}\to \mb{R}^{R}$ based on encoder-decoder structure and neural ODE~\citep{chen2018neural} to approximate the factor trajectory $\mf{u}^k(i_k,t)$ of mode-$k$.  Specifically, we have:
 \vspace{-2mm}
\begin{align}
        \mf{z}^{k}(i_k,0) = & \text{Encoder}\big([\cos(2\pi\mf{b}_ki_k); \sin(2\pi\mf{b}_ki_k)]\big), \label{eq:latent-ode1}\\
        \mf{z}^{k}(i_k,t) = & \mf{z}^{k}(i_k,0) + \int_0^{t} h_{\boldsymbol{\theta}_k}(\mf{z}^{k}(i_k,s), s)ds,\label{eq:latent-ode2}\\
        \mf{g}^k(i_k,t) = & \text{Decoder}\big(\mf{z}^{k}(i_k,t)).\label{eq:latent-ode3}
\end{align}
 Eq.~\eqref{eq:latent-ode1} shows that how we obtain $\mf{z}(i,0) \in \mb{R}^{J}$, the initial state of the latent dynamics by encoding the continuous index $i_k$. In particular, the input coordinate $i_k$ is firstly expanded into a set of Fourier features $[\cos(2\pi\mf{b}_ki_k); \sin(2\pi\mf{b}_ki_k)] \in \mb{R}^{2M}$, where $\mf{b}_k\in \mb{R}^{M}$ is a learnable vector that scales  $i_k$  by $M$ different frequencies. This effectively expands the input space with high-frequency components~\citep{tancik2020fourier}, helping to capture fine-grained index information. The Fourier features are then fed into an encoder
$\text{Encoder}(\cdot):\mb{R}^{2M}\to \mb{R}^{J}$ to get $\mf{z}^{k}(i_k,0)$. Give the initial state, we then apply a neural network $ h_{\boldsymbol{\theta}_k}(\mf{z}^{k}(i_k,s), s):\mb{R}^{J}\to \mb{R}^{J}$ to model the state transition of the dynamics at each timestamp, which is parameterized by $\boldsymbol{\theta}_k$, and the state value can be calculated through integrations as shown in \eqref{eq:latent-ode2}. Finally, we will pass the output of the latent dynamics through a decoder $\text{Decoder}(\cdot):\mb{R}^{J}\to \mb{R}^{R}$ to obtain $\mf{g}^k(i_k,t)$ as the approximation  of the factor trajectory, as described in \eqref{eq:latent-ode3}. We  simply use the multilayer perceptrons (MLPs) to parameterize the encoder and the decoder.

 Note that \eqref{eq:latent-ode2} actually represents the neural ODE model~\citep{Tenenbaum_Pollard_ode, chen2018neural}, and we  follow \citet{chen2018neural} to track the gradient of $\boldsymbol{\theta}_k$ efficiently, when we handle the integration to obtain $\mf{z}^k(i_k,t)$ in \eqref{eq:latent-ode2} at arbitrary $t$ by using numerical ODE solvers:
 \vspace{-2mm}
 \begin{equation}
    \mf{z}^k(i_k,t) =  \text{ODESolve}(\mf{z}^k(i_k,0),h_{\boldsymbol{\theta}_k}).
\end{equation}
For computing efficiency, we  concatenate the initial states of multiple indexes together, and construct a matrix-valued trajectory, where each row corresponds to the initial state of an unique index. Then, we only need to call the ODE solver once to obtain the factor trajectory of observed indexes. For simplicity, we denote the all learnable parameters in \eqref{eq:latent-ode1}-\eqref{eq:latent-ode3} as $\boldsymbol{\omega}_k$ for mode-$k$, which includes the frequency-scale vectors $\mf{b}_k$ as well as the parameters of neural ODE $\boldsymbol{\theta}_k$ and the encoder-decoder.


\subsection{Rank-revealing Prior over Factor Trajectories}
To automatically determine the underlying rank in the dynamical scenario, we apply the Bayesian sparsity-promoting priors. Specifically, we extend the classical framework on automatic rank determination~\citep{zhao2015bayesianCP}, stated in \eqref{eq:u}\eqref{eq:lambda}, and assign  a dimension-wise Gaussian-Gamma prior to the factor trajectory,
\vspace{-2mm}
\begin{equation}
    p(\mf{u}^{k}(i_k,t)|\boldsymbol{\lambda}) = \mathcal{N}(\mf{u}^{k}(i_k,t)|\boldsymbol{0}, \boldsymbol{\Lambda}^{-1}), \forall k,
\end{equation}
where $\boldsymbol{\Lambda} = \text{diag}(\boldsymbol{\lambda})$ and $\boldsymbol{\lambda}=[\lambda_1, \cdots, \lambda_r, \cdots, \lambda_R]^{\T} \in \mb{R}^{R}$. We assign Gamma priors to $\boldsymbol{\lambda}$: $p(\boldsymbol{\lambda}) = \prod_{r=1}^{R} \text{Gamma}(\lambda_r|a_r^0, b_r^0)$, identical to \eqref{eq:lambda}. Then, the rank-revealing prior over all factor trajectories is:
\vspace{-2mm}
\begin{equation}
    p(\mathcal{U}, \boldsymbol{\lambda}) = p(\boldsymbol{\lambda}) \prod_{k=1}^K p(\mf{u}^{k}(i_k,t)|\boldsymbol{\lambda}), \label{eq:ODE_prior} 
\end{equation}
where $\mathcal{U}$ denotes the set of  factor trajectories $\{\mf{u}^{k}(\cdot,\cdot)\}_{k=1}^{K}$.
It is worth noting that the proposed prior is assigned over a group of latent functions, but not a set of static factors~\citep{zhao2015bayesianCP}. With proper inference, the informative components of each factor trajectory, i.e., the rank of the generalized temporal tensor, can be automatically revealed, and redundant components can be pruned.  

With finite observed entries $\mathcal{D}=\{y_n, \mf{i}_n, t_n\}_{n=1}^{N}$, where $y_{n}$ denotes the $n$-$th$ entry observed at continuous index tuple $\mf{i}_n = (i_1^n, \cdots, i_K^n)$ and timestamp $t_n$. Our goal is to learn a factorized function as described in \eqref{eq:gen_CP}  to construct a direct mapping from $(\mf{i}_n, t_n)$ to $y_{n}$. 
Therefore, for each observed entry $\{y_n, \mf{i}_n, t_n\}$, we model the Gaussian likelihood  as:
\vspace{-1mm}
\begin{equation}
\begin{split}
        p(y_{n}&|\mathcal{U},\tau)=\mathcal{N}(y_{n}|\boldsymbol{1}^{\T}[\underset{k}{\circledast} 
 \mf{u}^k(i_k^n,t_n)], \tau^{-1}),
        \label{eq:likelihood}
\end{split}
\end{equation}
where  $\tau$ is the inverse of the observation noise. We further assign a Gamma prior, $p(\tau) = \text{Gamma}(\tau|c^0, d^0)$, and the joint probability can be written as:
\vspace{-2mm}
\begin{equation}
\begin{split}
        &p(\mathcal{U}, \tau, \mathcal{D}) =p(\mathcal{U}, \boldsymbol{\lambda})p(\tau) \prod_{n=1}^{N}p(y_{n}|\mathcal{U}, \tau).
\end{split}
    \label{eq:joint}
\end{equation}
We illustrate \textsc{GreT} with the case of $K=3$ in Figure~\ref{fig:flowchart}.