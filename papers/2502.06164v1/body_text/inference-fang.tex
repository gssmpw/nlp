\section{Model Inference}
\subsection{Factorized Posterior and Analytical Evidence Lower Bound}
It is intractable to compute the full posterior of latent variables in \eqref{eq:joint} due to the high-dimensional integral and complex form of likelihood. We take a workaround to construct a variational distribution $q(\mathcal{U}, \boldsymbol{\lambda}, \tau)$ to approximate the exact posterior $p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})$. 
Similar to the widely-used mean-field assumption, we  design the approximate posterior in a fully factorized form: $q(\mathcal{U}, \boldsymbol{\lambda}, \tau) = q(\mathcal{U})q(\boldsymbol{\lambda})q(\tau)$.

Specifically, the conditional conjugate property of Gaussian-Gamma distribution motivates us to formulate the corresponding variational posteriors as follows:
\vspace{-2mm}
\begin{equation}
    \begin{split}
        &q(\mathcal{U}) = \prod_{n=1}^{N}\prod_{k=1}^{K} \mathcal{N}(\mf{u}^k(i_k^n,t_n)|\mf{g}^k(i_k^n,t_n), \sigma^2\mf{I}),
        \label{eq:q_u} 
    \end{split}
\end{equation}
where $\mf{g}^k(\cdot, \cdot)$  is the mode-wise latent temporal representations parameterized by $\boldsymbol{\omega}_k$, as we mentioned in Section \ref{sec:Latent-ODE-Flow}, and $\sigma$ is the variational variance shared by all $\mf{u}^k$.

Similarly, we formulate $q(\boldsymbol{\lambda}), q(\tau)$ as:
\vspace{-2mm}
\begin{align}
        &q(\boldsymbol{\lambda}) =  \prod_{r=1}^{R} \text{Gamma}(\lambda_r|\alpha_r, \beta_r),\label{eq:q_lambda}\\
        &q(\tau) = \text{Gamma}(\tau|\rho, \iota),\label{eq:q_tau}
\end{align}
where $\{\alpha_r, \beta_r\}_{r=1}^{R}, \rho, \iota$ are the variational parameters to characterize the approximated posteriors. 

Our goal is to estimate the latent ODE parameters $\boldsymbol{\omega}_k$ and variational parameters $\{ \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota \}$ in \eqref{eq:q_u} \eqref{eq:q_lambda} \eqref{eq:q_tau} to make the approximated posterior $q(\mathcal{U}, \boldsymbol{\lambda}, \tau)$ as close as possible to the true posterior $p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D})$. To do so, we follow the variational inference framework~\citep{variational_inference} and construct the following objective function by minimizing the Kullback-Leibler (KL) divergence between the approximated posterior and the true posterior $\text{KL}(q(\mathcal{U}, \boldsymbol{\lambda}, \tau)\|p(\mathcal{U}, \boldsymbol{\lambda}, \tau|\mathcal{D}))$, which  leads to the maximization of the evidence lower bound (ELBO): 
\vspace{-2mm}
\begin{align}
        &\text{ELBO} = \mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] +\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
        \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}] \nonumber \\
        &\qquad - \text{KL}(q(\boldsymbol{\lambda})\|p(\boldsymbol{\lambda})) - \text{KL}(q(\tau)\|p(\tau)) \label{eq:elbo}.
\end{align}
The ELBO is consist of four terms. The first term is posterior expectation of log-likelihood  while the last three are KL terms. Usually, the first term is intractable if the likelihood model is complicated and requires the costly  sampling-based approximation to handle the integrals in the expectation ~\citep{doersch2016tutorialVAE, NONFAT}. Fortunately, by leveraging the well-designed conjugate priors and factorized structure of the posterior, we make an endeavor to derive its analytical expression:
\vspace{-2mm}
\begin{align}
        &\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda}, \tau)}[\ln p(\mathcal{D}|\mathcal{U}, \boldsymbol{\lambda}, \tau)] =-\frac{N}{2}\ln(2\pi) + \frac{N}{2}(\psi(\rho)-\ln\iota) \nonumber\\
        &-\frac{1}{2}\sum_{n=1}^{N}\frac{\rho}{\iota}\big\{ y_n^2 -2y_n\{\boldsymbol{1}^{\T}[\underset{k}{\circledast} \mf{g}^k(i_k^n,t_n)]\}\nonumber \\
&  +\boldsymbol{1}^{\T}[\underset{k}{\circledast} \text{vec}(\mf{g}^{k}(i_k^{n},t_n)\mf{g}^{k}(i_k^{n},t_n)^{\T}+\sigma^2\mf{I})]\big\},\label{term1}
\end{align}
where $\mf{g}^k_r(i_k^n, t_n)$ is the $r$-th element of the $k$-th mode's latent temporal representation $\mf{g}^k(i_k^n, t_n)$. We refer to Appendix \ref{ap:A.2} for the detailed derivation. 
%We note that the first term computes the posterior expectation of the log-likelihood of the noisy observation.
%, which approximates  the MAE criterion.
The second term of \eqref{eq:elbo} computes the KL divergence between prior and posterior of factor trajectories, which is also with a closed from:
\vspace{-2mm}
\begin{align}
    &\mb{E}_{q(\mathcal{U}, \boldsymbol{\lambda})}[\ln 
    \frac{p(\mathcal{U}|\boldsymbol{\lambda})}{q(\mathcal{U})}]= -\text{KL}(q(\mathcal{U}) \| p(\mathcal{U}|\boldsymbol{\lambda}=\mb{E}_q(\boldsymbol{\lambda})))=\nonumber \\
    &-\sum_{n=1}^N \sum_{k=1}^K\sum_{r=1}^R \frac{1}{2}\big\{\ln(\frac{\beta_r}{\alpha_r\sigma^2})+ \frac{\alpha_r}{\beta_r}\{\sigma^2+[\mf{g}^k_r(i_k^n, t_n)]^2\}-1\big\}, \label{term2}
\end{align}
where $\mb{E}_q(\boldsymbol{\lambda})=[\mb{E}_q({\lambda_1}),\ldots,\mb{E}_q({\lambda_R})]^{\T}=[\frac{\alpha_1}{\beta_1},\ldots,\frac{\alpha_R}{\beta_R}]^{\T}$.
This term encourages rank reduction,  as it drives the 
posterior mean of $\lambda_r$  to be large, thereby forcing the corresponding $r$-th component of $K$ factor trajectories $\{\mf{g}^{k}_r(\cdot, \cdot)\}_{k=1}^K$ to be zero. The combination of the above two terms enables an automatic rank determination mechanism by striking a balance between capacity of representation and model complexity. 
As the prior and posterior of $\boldsymbol{\lambda}$ and $\tau$ are both Gamma distribution, the last two KL terms in the ELBO are analytically computable, as shown in \eqref{eq:kl1}\eqref{eq:kl2} in Appendix \ref{ap:A.2}.




We highlight that all terms in \eqref{eq:elbo} are with analytical forms, so we don't need sampling-based approximation to compute the ELBO. 
This offers a significant advantage during training, as we can directly compute the gradient of the ELBO with respect to the variational parameters, enabling the use of standard gradient-based optimization methods to optimize both the variational and latent ODE parameters:
\begin{equation} 
    \text{argmax}_{\{\boldsymbol{\omega}_k\}_{k=1}^K, \{\alpha_r, \beta_r\}_{r=1}^{R}, \sigma, \rho, \iota} \text{ELBO}.
    \label{eq:loss}
\end{equation}
We summarize the inference algorithm in Algorithm 1 in Appendix~\ref{ap:algorithm}. 
When $N$ is large, we can use the mini-batch gradient descent method to accelerate the optimization process.

\subsection{Closed Form of Predictive Distribution}
After obtaining the variational posteriors of the latent variables, we can further derive the predictive distribution of the new data with arbitrary indexes. Given the index set $\{i^p_1,\cdots, i^p_k, t_p\}$ for prediction, we can obtain the variational predictive posterior distribution, which follows a Student's t-distribution (See Appendix \ref{pred_distr} for more details):
\begin{equation}
    \begin{split}
        &p(y_p|\mathcal{D}) \sim \mathcal{T}(y_p|\mu_p, s_p, \nu_p),\\
       &\mu_p =  \boldsymbol{1}^{\T}[\underset{k}{\circledast} 
 \mf{g}^k(i_k^p, t_p)], \quad \nu_p = 2\rho,\\
        &s_p =\big\{\frac{\iota}{\rho}+\sigma^2\sum_{j=1}^K[\underset{k\ne j}{\circledast} 
 \mf{g}^k(i_k^p, t_p)]^{\T}[\underset{k\ne j}{\circledast} 
 \mf{g}^k(i_k^p, t_p)]\big\}^{-1},
    \end{split}
\end{equation}
where $\mu_p$, $s_p$, $\mu_p$ is the mean, scale parameter and degree of freedom of the Student's t-distribution, respectively. The closed-form  predictive distribution is a great advantage for the prediction process, as it allows us to do the probabilistic reconstruction and prediction with uncertainty quantification over the arbitrary continuous indexes.


% % Algorithm
% \begin{algorithm}[H]
% \SetAlgoLined
% \caption{Training process of \MODEL}
% \KwIn{Training data $\mathcal{D}=\{y_n, \mf{i}_n, t_n\}_{n=1}^{N}$ }
% Collect all possible $I_k$ indexes for $K$ modes and construct $K$ initial time embedding tables $\{\mf{Z}_0^k\}_{k=1}^{K}$ with Encoder.
% \BlankLine
% % Steps
% \REPEAT{
% \For{$i = 1, 2, \cdots$}{
%     $\{\mf{Z}^k(t)\}_{k=1}^{K}$ = \text{ODESolve}($\{\mf{Z}^k_0\}_{k=1}^{K}$, 
%     $\{h_{\boldsymbol{\theta}}\}_{k=1}^{K}$,$t_{i-1},t_i)$
%     }
%     }
% Return $Y$\;
% \end{algorithm}




%\textit{Interpretation of Evidence Lower Bound:}

