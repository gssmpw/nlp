\section{Preliminary}
\subsection{Tensor Decomposition}
Tensor decomposition   represents multi-dimensional arrays  by decomposing them into  lower-dimensional components, thus revealing underlying patterns in high-dimensional data. We denote a $K$-mode tensor as $\bc{Y} \in \mb{R}^{I_1\times \cdots \times I_k  \times \cdots \times I_K}$, where the $k$-$th$ mode  consists of $I_k$ dimensions. Each entry of $\bc{Y}$, termed $y_{\mf{i}}$, is indexed by a $K$-tuple $\mf{i}=(i_1,\cdots,i_k, \cdots, i_K)$, where $i_k$ denotes the index of the node along the mode $k$ ($1\le k \le K$). 
For tensor decomposition,  a set of  factor matrices $\{\mf{U}^{k}\}_{k=1}^{K}$ are introduced to represent the nodes in each mode. Specifically, the $k$-$th$ factor matrix $\mf{U}^{k}$ is composed of $I_k$ latent factors, i.e., $\mf{U}^{k}=[\mf{u}^{k}_{1}, \cdots, \mf{u}^{k}_{i_k}, \cdots,\mf{u}^{k}_{I_k}]^{\T} \in \mb{R}^{I_k \times R_k}$ and $\mf{u}^{k}_{i_k} = [u_{i_k,1}^{k}, \cdots, u_{i_k,r_k}^{k}, \cdots, u_{i_k,R_k}^{k}]^{\T} \in \mb{R}^{R_k}$, where $R_k$ denotes  the  rank of mode-$k$.
The classic CANDECOMP/PARAFAC (CP) decomposition ~\citep{HarshmanCP} aims to decompose a tensor into a sum of rank-one tensors. It sets $R_1 = \cdots = R_k =\cdots = R_K = R$ and represents each entry using 
\vspace{-2mm}
\begin{equation}
    y_{\mf{i}} \approx \boldsymbol{1}^{\T}[\underset{k}{\circledast} 
\mf{u}_{i_k}^k]=\sum_{r=1}^{R}\prod_{k=1}^{K}u^{k}_{i_k, r},
 \label{eq:CP}
\end{equation}
 where $\boldsymbol{1} \in \mb{R}^{R}$ is the all-one vector and 
 $\underset{k}{\circledast} $ is   Hadamard product of a set of vectors  defined as  $\underset{k}{\circledast}\mf{u}_{i_k}^k = (\mf{u}^{1}_{i_1} \circledast \cdots \circledast \mf{u}^{k}_{i_k} \circledast  \cdots \circledast \mf{u}^{K}_{i_K})$. Here,  $\circledast$ is  Hadamard product.
Another popular model is  Tucker decomposition~\citep{sidiropoulos2017tensor}, which approximates each entry with the interactions between a core tensor and $K$ latent factors. Tucker model will degenerate into  CP model when all modes' ranks are set to the same and the core tensor is diagonal.

% with
% \begin{equation}
% \begin{split}
%         y_{\mf{i}} &\approx \text{vec}(\bc{W})\underset{k}{\otimes} 
%  \mf{u}_{i_k}^k=\sum_{r=1}^{R_1}\cdots \sum_{r=K}^{R_K}[w_{r_1,\cdots,r_K}\prod_{k=1}^{K}u_{i_k, r_k}^{k} ], 
% \end{split}
% \end{equation}

% where $\bc{W}\in \mathbb{R}^{R_1 \times \cdots \times R_k \times \cdots \times R_K}$ denotes the core tensor, 
% $\text{vec}(\cdot)$ is the vectorization operator. Similarly,   $\underset{k}{\otimes} 
%  \mf{u}_{i_k}^k = (\mf{u}^{1}_{i_1} \otimes \cdots \otimes \mf{u}^{k}_{i_k} \otimes \cdots  \otimes \mf{u}^{K}_{i_K})$ where $\otimes$ is the Kronecker product. Tucker model is able to capture more interactions in data than the CP model and it degenerate into CP model when all modes' ranks are set to the same and $\bc{W}$ is diagonal.

\subsection{ Automatic Tensor Rank Determination}
% \fang{
% 1.pls add content on the NP-hardness of the tensor model's rank determination.}
The tensor rank $R$ 
determines the complexity of the tensor model. An improper choice of the rank can lead to overfitting  or underfitting to the signal sources, potentially compromising the model interpretability. However, the optimal determination of the tensor rank is known to be NP-hard ~\citep{cheng2022towards, kolda2009tensor(NPhard), haastad1989tensor_np}.
Rather than exhaustively searching for the optimal tensor rank via trial and error experiments,  Bayesian methods have been introduced to facilitate Tucker/CP decomposition with automatic tensor rank learning~\citep{morup2009automatic_ARD, zhao2015bayesianCP, cheng2022towards, pmlr-v32-rai14}. These methods impose sparsity-promoting priors (e.g., the Gaussian-Gamma prior and Laplacian prior)  on the latent factors.

For example, Bayesian CP decomposition with  Gaussian-Gamma priors models the mean and precision of all latent factors with  zero elements and a set of latent variables $\boldsymbol{\lambda}=[\lambda_1, \cdots, \lambda_r, \cdots, \lambda_R]^{\T} \in \mb{R}^{R}$, respectively:  
\begin{align}
        p(\mf{u}_{i_k}^{k}|\boldsymbol{\lambda}) = \mathcal{N}(\mf{u}_{i_k}^{k}|\boldsymbol{0}, \boldsymbol{\Lambda}^{-1}), \forall k,\label{eq:u}\\
    p(\boldsymbol{\lambda}) = \prod_{r=1}^{R} \text{Gamma}(\lambda_r|a_r^0, b_r^0),
    \label{eq:lambda}
\end{align}
where $\boldsymbol{\Lambda} = \text{diag}(\boldsymbol{\lambda})$ is the inverse covariance matrix  shared by all latent factors  over  $K$ modes.   Note that $R$ components of $\mf{u}_{i_k}^{k}$ are assumed to be statistically independent and the distribution of the $r$-$th$ component is controlled by $\lambda_r$. For example, if $\lambda_r$ is large, then the density function peaks at mean zero, so the $r$-$th$ component is concentrated at zero. Otherwise, if $\lambda_r$ is small (which leads to heavy tails), it allows the component to spread out to wider range of values. The conjugated Gamma priors are assigned to $\boldsymbol{\lambda}$. Here $\text{Gamma}(x|a,b)=\frac{b^ax^{a-1}e^{-bx}}{\Gamma(a)}$ for $x \ge 0$, which represents the Gamma distribution for $\boldsymbol{\lambda}$. In this context, $a$ and $b$ denote the shape and rate parameters respectively, and $\Gamma(\cdot)$ denotes the Gamma function. $\{a_r^0, b_r^0\}_{r=1}^R$ are pre-determined hyperparameters. 
The tensor rank will be automatically determined by the inferred posteriors of $\boldsymbol{\lambda}$.

\subsection{Generalized Tensor with Continuous Modes}
Real-world tensor data often contains continuous modes, prompting increased studies on generalized tensor data with continuous indexes. Existing approaches can be broadly classified into two categories:



\textit{1.Temporal tensor model with continuous timestamps:} Recent studies encode the representations of continuous timestamps into the latent factor of  CP model~\citep{SFTL, NONFAT} or the tensor core of  Tucker model~\citep{bctt}. 
Taking CP as an example, the temporal tensor model with continuous timestamps can be written as:
\vspace{-2mm}
\begin{align}
    y_{\mf{i}}(t) \approx \boldsymbol{1}^{\T}[\underset{k}{\circledast} 
    \mf{u}_{i_k}^k(t)], \label{eq:temporal_CP}
\end{align}
where $\mf{i}$ is the tensor index, $t$ is the continuous timestamp, $\mf{u}_{i_k}^k(t)$ is the factor trajectory of the $i_k$-th node in the $k$-th mode. Although this modeling approach effectively captures complex temporal dynamics, it is inadequate for generalizing to data with continuous indexes over the entire domain, such as spatiotemporal data which has continuous coordinates on both spatial and temporal modes.~\citep{hamdi2022spatiotemporal}. 

\textit{2.Functional tensor model:} 
Another popular model to handle  continuous-indexed modes is the functional tensor~\citep{schmidt2009function_tensor, luo2023lowrank, Ballester-Ripoll_Paredes_Pajarola_2019_tt}, which assumes that the continuous-indexed tensor can be factorized as a set of mode-wise functions and the continuous timestamp is simply modeled as an extra mode. Still taking CP as an example, the functional tensor model can be written as:
\begin{align}
    y_{\mf{i}}(t) \approx  \boldsymbol{1}^{\T}[\underset{k}{\circledast} 
    \mf{u}^k(i_k){\circledast} \mf{u}^{\text{Temporal}}(t) ],\label{eq:function_CP}
\end{align} 
where  $\mf{u}^k(i_k): \mathbb{R}_{+} \to \mathbb{R}^{R}$ is the latent vector-valued function of the $k$-th mode, which takes the continuous index $i_k$ as input and outputs the  latent factor. $\mf{u}^{\text{Temporal}}(t) : \mathbb{R}_{+} \to \mathbb{R}^{R}$ is the latent function of the temporal mode. 
The fully-factorized form of \eqref{eq:function_CP} models each mode equally and independently. It often overlooks the complex dynamics of the temporal mode, which requires special treatment~\citep{hamdi2022spatiotemporal}.



