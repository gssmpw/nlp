
\documentclass[10pt]{article} % For LaTeX2e
\input{Ancillary/packages.tex}
\input{Ancillary/commands.tex}
\usepackage[accepted]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Rudi Coppola \email r.coppola@tudelft.nl \\
      \addr Department of Mechanical Engineering\\
      Delft University of Technology
      \AND
      \name Manuel Mazo Jr. \email m.mazo@tudelft.nl \\
      \addr Department of Mechanical Engineering\\
      Delft University of Technology
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{02}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=pSk5qyt1ob}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Estimating the expectation of a Bernoulli random variable based on $N$ independent trials is a classical problem in statistics, typically addressed using Binomial Proportion Confidence Intervals (BPCI). In the control systems community, many critical tasks—such as certifying the statistical safety of dynamical systems—can be formulated as BPCI problems.

Conformal Prediction (CP), a distribution-free technique for uncertainty quantification, has gained significant attention in recent years and has been applied to various control systems problems, particularly to address uncertainties in learned dynamics or controllers. A variant known as \emph{training-conditional CP} was recently employed to tackle the problem of safety certification.

In this note, we highlight that the use of training-conditional CP in this context does not provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and argue that traditional BPCI methods are better suited for statistical safety certification.
\end{abstract}

\section{Introduction}
Uncertainty quantification is a critical aspect in fields where predictions influence safety and performance guarantees, such as in control systems. Probabilistic guarantees, including those derived from the theory of Probably Approximately Correct (PAC) learning, play an important role in providing bounds on the accuracy of predictions under limited training data.
%

Conformal Prediction (CP) is one of the approaches that has gained visibility due to its ability to provide valid prediction sets without requiring strong distributional assumptions. A distinctive characteristic of CP is that, rather than providing a point prediction of the variable of interest, it provides set predictions with a valid bound on the probability that the predicted set contains the true variable \citep{vovk2005algorithmic}. This technical note focuses on a specific formulation of CP known as training-conditional CP \citep{vovk2012conditional}. However, existing applications in areas such as safety verification for dynamical systems have shown limitations in the interpretation of these guarantees.
%
In particular, recent works  have applied training-conditional CP to safety verification problems in control systems \citep{chilakamarri2024reachability, lin2024verification, vincent2024guarantees}. While promising, these applications have misinterpreted the implications of CP's set prediction framework, especially in cases where the underlying data can be modeled as Bernoulli random variables. This paper aims to rigorously analyze these limitations and provide an alternative framework for interpreting PAC-based guarantees in such contexts.
%
In \secref{sec:bin-prop} we recall existing methods for estimating the expectation of a Bernoulli random variable.
In \secref{sec:CP-preliminaries} we introduce the formalism of training-conditional CP, followed by a detailed analysis of its PAC guarantees. In \secref{sec:special-case} we present a special case of interest, where the nonconformity measure corresponds to an indicator function, leading to Bernoulli-distributed conformity scores. We demonstrate that the PAC guarantees derived from this setting are unsuitable for estimating the expectation of a Bernoulli random variable. 

\section{Binomial Proportion Confidence Intervals}\label{sec:bin-prop}
Consider a setting where there are $N + 1$ independent and identically distributed (i.i.d.) Bernoulli random variables (r.v.) $\Score_1,\Score_2,...,\Score_N,\Score_{N+1}$ with parameter $b$, i.e. $\Score_i\sim\text{Bern}_b$ and $\text{Pr}_{\text{Bern}_b}(\Score_i=1)\doteq b$. Given a realization of the first $N$ r.v., the problem is to estimate an interval of values for the probability that the $N+1$-th variable will be equal to 1, or in other words we want to estimate the parameter $b$. This is a very well studied problem and it is known in the literature under the name of Binomial Proportion Confidence Intervals (BPCI), see \citet{dean2015evaluating} for a survey. We give below a quick overview of the setting. 
%

Define the new r.v. $Y\doteq\sum_{i=1}^N\Score_i$. It is well known that $Y$ has a binomial distribution $Y \sim \text{Bin}_{N,b}$ with $N$ trials and probability of success $b$, defined by $\text{Pr}_{\text{Bin}_{N,b}}(Y=y)\doteq \binom{N}{y}b^y(1-b)^{N-y}$ for $y\in \integers_{[0,N]}$, where $\integers_{[0,N]}$ denotes the integers $0,1,...,N$. Let $\check{b}:\integers_{[0,N]}\rightarrow[0,1]$ and $\hat{b}:\integers_{[0,N]}\rightarrow[0,1]$ be two random variables serving as interval estimators. The coverage probability of the interval estimator $[\check{b} ,\hat{b}]$ for $Y \sim \text{Bin}_{N,b}$ is defined as
\begin{equation}\label{eq:cov-prob}
    \rho(b, \check{b} ,\hat{b}) \doteq \text{Pr}_{\text{Bin}_{N,b}} (\check{b}(Y)\leq b \leq \hat{b}(Y)).
\end{equation}
In the expression above $b$ is fixed and it's the true parameter of the binomial distribution describing $Y$. Note that $\check{b}$ and $\hat{b}$ are a transformation of the same random variable $Y$. This expression can also be rewritten equivalently as 
\begin{equation*}
    \rho(b, \check{b} ,\hat{b}) = \sum_{y\in I} \text{Pr}_{\text{Bin}_{N,b}}(Y = y),
\end{equation*}
where $I\doteq\{y\in\integers_{[0,N]} : \check{b}(y)\leq b \leq \hat{b}(y)\}$. For $\alpha\in(0,1)$ an interval estimator $[\check{b} ,\hat{b}]$ is a \emph{conservatively valid} (sometimes also called 'exact' or 'secure') $1-\alpha$ confidence interval if the coverage probability $\rho(b, \check{b} ,\hat{b})$ is greater or equal to $1-\alpha$ for all the values of $b$. An example of a conservatively valid interval estimator is given by the Clopper-Pearson method \citep{clopper1934use}, see also \citet{dean2015evaluating} for more estimators.
%

Before concluding this section, we rewrite \eqref{eq:cov-prob} in an equivalent form that is more commonly found in the literature on Probably Approximately Correct (PAC) bounds. First, note that $\text{Pr}_{\text{Bern}_b}(\Score_{N+1}=1) = b$. Second, since $Y$ is a r.v. obtained as a transformation of the i.i.d. Bernoulli random variables $R_1,\ldots,R_N$, the probability of any event $M\subseteq \integers_{[0,N]}$, $\text{Pr}_{\text{Bin}_{N,b}}(Y\in M)$ can be equivalently described by $\text{Pr}_{\text{Bern}_b}^N(\{(r_1,...,r_N): \sum_{i\leq N} r_i \in M\})$, where $\text{Pr}_{\text{Bern}_b}^N$ is the product probability measure induced by the $N$ i.i.d. Bernoulli random variables. Hence, we rewrite the definition of a conservatively valid $1-\alpha$ confidence interval by revisiting \eqref{eq:cov-prob}:
\begin{equation}\label{eq:PAC-Bern-guarantees}
    \text{Pr}_{\text{Bin}_{N,b}} (\check{b}(Y)\leq b \leq \hat{b}(Y))
    = \text{Pr}_{\text{Bern}_b}^N \left(\check{b}\left(\sum_{i\leq N}\Score_i\right)\leq \text{Pr}_{\text{Bern}_b}(\Score_{N+1}=1) \leq \hat{b}\left(\sum_{i\leq N}\Score_i\right)\right) \geq 1-\alpha,
\end{equation}
for all $b\in[0,1]$.
We will use this form of the coverage probability to draw a comparison with the guarantees given by training-conditional CP.

\section{Training-conditional Conformal Prediction}\label{sec:CP-preliminaries}
Conformal Prediction is a statistical tool that uses the available data sampled from identically and independently from an underlying distribution to output predictions for which an error probability can be computed. The original formulation of CP can be informally explained as follows. Suppose that we want to solve a classification problem and we have method that given a feature $x$ outputs a label $\hat{y}$. Given a desired error probability $\epsilon$, conformal prediction uses the available data to generate a \emph{set of labels}, typically containing $\hat{y}$, containing the true label $y$ corresponding to the feature $x$ with a probability not smaller than $1-\epsilon$ \citep{shafer2008tutorial}. It is a method capable of augmenting a (usually unreliable) point prediction to a set prediction with probabilistic guarantees of correctness, i.e. it construct a \emph{set predictor}. The original formulation of CP has been successfully applied to both classification and regression problems, see \citet{angelopoulos2023conformal, fontana2023conformal} for a recent survey.

In this section we introduce instead the basic concepts of \emph{training-conditional} CP \citep{vovk2012conditional}. Training-conditional CP is a variant of the original formulation of CP. While the quality of the guarantees differs from the original, the core idea remains the same, that is, constructing set predictions with some form of guarantees: training-conditional CP produces PAC-style guarantees. In the following, we give a self-contained overview of the theoretical details of training-conditional CP.

Let $\ProbSpace$ be a probability space where $\UncertaintySpace$, $\SigmaAlgebra$ and $\ProbMeas$ denote a sample set, a $\sigma$-algebra, and a probability measure respectively, and consider $\NumSamples+1$  i.i.d. random variables (r.v.) $\RV'_1,...,\RV'_{\NumTrainSamples},\RV_1,...,\RV_\NumCalSamples$ and $\RV_{\NumCalSamples+1}$ with $\NumSamples=\NumCalSamples+\NumTrainSamples$. Let $\RV_i'$ for $i=1,...,\NumTrainSamples$ be the \emph{training set} and $\RV_i$ for $i=1,...,\NumCalSamples$ be the \emph{calibration set}. Note that $\RV_{\NumCalSamples+1}$ is not part of either set. We use the lower case of a r.v. to denote a realization\footnote{Our considerations hold also for the case where $\UncertaintySpace=\mathbf{X}\times\mathbf{Y}$ where $\mathbf{X}$ and $\mathbf{Y}$ represent a measurable feature space and label space respectively and each $z\in\UncertaintySpace$ may be written as $z=(x,y)$ where $x\in\mathbf{X}$ is some feature and $y\in\mathbf{Y}$ a label. For clarity we omit the exact structure of $\UncertaintySpace$.}. An \emph{Inductive Nonconformity $\NumTrainSamples$-measure} (INM) is a measurable function $\NonConfMeas:\UncertaintySpace^\NumTrainSamples\times\UncertaintySpace\rightarrow\reals$. While no additional requirements are needed for $\NonConfMeas$, intuitively an effective INM will assign a high real number to any element in $\UncertaintySpace$ that does not conform to a training set (in $\UncertaintySpace^\NumTrainSamples$). An \emph{Inductive Nonconformal Predictor} (INP) is a set predictor defined as 
\begin{equation}\label{eq:INP}
\INP{\epsilon}(z_1,...,z_\NumCalSamples,z_1',...,z_\NumTrainSamples') \doteq \{z \in \UncertaintySpace \ : \ p^z>\epsilon\},
\end{equation}
where $\epsilon\in[0,1]$ is the \emph{significance level}, the $p$-values are defined as
\begin{equation}\label{eq:p-scores}
    \PVal^z \doteq \frac{|\{i : \Score_i\geq\Score^z\}|+1}{\NumCalSamples+1},
\end{equation}
and
\begin{equation}\label{eq:nonconformity-scores}
\Score_i\doteq\NonConfMeas((z_1',...,z_\NumTrainSamples'),z_i) \text{ for } i=1,...N, \qquad \Score^z\doteq \NonConfMeas((z_1',...,z_\NumTrainSamples'),z),
\end{equation}
are the \emph{nonconformity scores}.

In the following, when it is clear from the context we omit the arguments of the INP and write $\INP{\epsilon}$ instead of  $\INP{\epsilon}(z_1,...,z_\NumCalSamples,z_1',...,z_\NumTrainSamples')$. Intuitively, $z$ belongs to the INP $\INP{\epsilon}$ if there are strictly more than $\lfloor\epsilon(\NumCalSamples+1)-1\rfloor$ elements $\Score_i$ in the calibration set with a higher (worse) or equal nonconformity score than $\Score^z$. It is easy to see that $\epsilon' < \epsilon''$ implies that $\INP{\epsilon''}\subseteq\INP{\epsilon'}$. The INP is the set predictor mentioned in the discussion at the beginning of this section: similarly to the original formulation of CP, given some prediction method depending on the training set, the INP uses the available calibration set to produce a set prediction guaranteed to contain the correct prediction. The elements included in the set prediction are all the $z\in\UncertaintySpace$ that conform well enough with the calibration set, according to the chosen INM. The following theorem specifies the PAC-style guarantees for training-conditional CP.


\begin{theorem}[\citet{vovk2012conditional}]\label{theo:vovk}
Choose $\epsilon,E\in[0,1]$\footnote{A brief note on the notation. In the original formulation of CP $\epsilon$ has a double role: it is the significance level (appearing as the index to the INP $\INP{\epsilon}$) and it describes the coverage probability as $1-\epsilon$, see \citet{shafer2008tutorial} for details. In the training-conditional formulation the latter role is covered by $E$, that is $1-E$ is the coverage probability and $\epsilon$ remains the significance level,  see \citet{vovk2012conditional}.}, fix the training set $\RV'_1=z_1',...,\RV'_\NumTrainSamples=z_\NumTrainSamples'$, let $N$ be the size of the calibration set, and consider the event
\begin{equation}\label{eq:inner-event-PAC}
    S_E\doteq\{(z_1,...,z_\NumCalSamples) \in \UncertaintySpace^\NumCalSamples: \ProbMeas(\RV_{\NumCalSamples+1}\in\INP{\epsilon}(z_1,...,z_\NumCalSamples,z_1',...,z_\NumTrainSamples')) \geq 1-E \}
\end{equation} in the $\sigma$-algebra $\SigmaAlgebra^\NumCalSamples$ of the product probability space $\CalProbSpace$, where $\INP{\epsilon}$ is defined according to equations \ref{eq:INP}-\ref{eq:nonconformity-scores}. It holds that
\begin{equation}\label{eq:PAC-vovk}
    \ProbMeas^\NumCalSamples(S_E) \geq 1 - \delta,
\end{equation}
where $\delta\doteq\text{Bin}_{\NumCalSamples, E}(J) = \sum_{j=0}^J\binom{N}{j}E^j(1-E)^{N-j}$ is the cumulative binomial distribution with $\NumCalSamples$ trials and probability of success $E$, with $J \doteq \lfloor \epsilon(\NumCalSamples+1) - 1 \rfloor $.
\end{theorem}
The quantities $1 - \delta$ and $1-E$ are sometimes referred to as the \emph{confidence} and \emph{coverage probability} (which is not the coverage probability mentioned in \secref{sec:bin-prop}). Theorem \ref{theo:vovk} is to be understood in the following way. Given two values $\epsilon$ and $E$, for the given training set, the event $S_E$ is the subset of $\UncertaintySpace^\NumCalSamples$ containing all the tuples $(z_1,...,z_\NumCalSamples)$ such that the INP $\Gamma^\epsilon$ contains a realization of $\RV_{\NumCalSamples+1}$ with probability at least $1-E$, or, in other words, $\INP{\epsilon}$ returns a subset of $\UncertaintySpace$ of measure at least $1-E$. By $\eqref{eq:PAC-vovk}$ the measure of this set of tuples $S_E$ is at least $1-\delta$, where $\delta$ depends on $\epsilon$, $\beta$ and $N$.
This form of guarantees where a double layer of nested probabilities is present is called Probably Approximately Correct (PAC). Moreover, this is a distribution-free result, that is, it holds for every $\ProbMeas$ as long as the samples used to construct the INP are i.i.d. and $\ProbMeas$-distributed. In particular, in this work we focus on Bernoulli-distributed r.v.'s, and Theorem \ref{theo:vovk} holds for any value of the parameter $b$ of a Bernoulli distribution. Observe that the confidence $1-\delta$ and the quantity $1-\alpha$ mentioned in \secref{sec:bin-prop} play a similar role in that they described the outmost layer of probability, compare for instance equations \eqref{eq:PAC-vovk} and \eqref{eq:PAC-Bern-guarantees}. Finally, we note that $\epsilon$ and $E$ are chosen \emph{a priori}; in other words, they cannot be defined as random variables depending on a realization of the calibration set, as is erroneously done in \citet{lin2024verification}.

\section{A Special Case of Interest}\label{sec:special-case}
In this section we draw a parallel between the BPCI and training-conditional CP and show the fundamental difference between the two approaches. 

Let the INM be an indicator function for the set $Q\subset\UncertaintySpace$, that is
\begin{equation}
    \NonConfMeas((z_1',...,z_\NumTrainSamples'),z)\doteq
    \begin{cases}
        1 \text{ if } z\in Q, \\
        0 \text{ if } z\in\overline{Q}.
    \end{cases}
\end{equation}
Typically $Q$ depends on $z_1',...,z_\NumTrainSamples'$.  For example, in binary classification problems, the training set may be used to train a parameterized function that assigns one of two labels to all $z\in Q$, as in Support Vector Machines. However, since Theorem \ref{theo:vovk} assumes a given training set, we omit this dependency here. A point $z$ with a high nonconformity score is interpreted as poorly conforming to the training set. For this reason, in \secref{sec:safety} the set $Q$ will represent the unsafe region of a dynamical system. 
\newline
Given a fixed training set, the nonconformity scores of the calibration set follow an i.i.d. Bernoulli distribution with parameter $b$, i.e. $R_i\sim\Bern{b}$, where $b \doteq \ProbMeas(Q)$. Using a BPCI method it is directly possible to derive a conservatively valid confidence interval for the parameter $b$ describing the probability of drawing a sample in $Q$, as shown in \secref{sec:bin-prop}. Can a training-conditional CP approach also provide a conservatively valid confidence interval for $b$ based on the calibration set? The answer is no. We illustrate this with an example.
%

\begin{example}
\emph{- Part 1.} 

Suppose that  the calibration set has size 2, i.e. $\NumCalSamples=2$. Up to reindexing, there are three distinct outcomes.
\newline
\emph{Case 1:} With probability $(1-b)^2$ we have $z_1,z_2\notin Q$, resulting in nonconformity scores $\Score_1=\Score_2=0$. We construct the prediction set $\INP{\epsilon}$ following its definition \eqref{eq:INP}. 
\begin{itemize}
    \item For all $z\in Q$, we have that $\Score^z=1$, meaning $z$ has the highest (worst) nonconformity score. Since $|\{i\leq2 : \Score_i\geq\Score^z\}| = 0$ the corresponding $p$-value is $p^z = \frac{1}{3}$.
    \item For all $z\in\overline{Q}$ we have that $\Score^z=0$ resulting in and $\PVal^z=1$.
\end{itemize}

The inclusion of $z$ in the predicted set $\INP{\epsilon}$ depends on the significance level $\epsilon$. 
\begin{itemize}
    \item If $\epsilon\in[\frac{1}{3},1)$ then any $z\in Q$ is excluded from $\INP{\epsilon}$ since $\PVal^z=\frac{1}{3}\leq\epsilon$, while all $z\in \overline{Q}$ are included since $\PVal^z=1>\epsilon$. Thus, $\INP{\epsilon}=\overline{Q}$.
    \item If $\epsilon\in[0,\frac{1}{3})$ then any $z\in Q\cup\overline{Q}=\UncertaintySpace$ has a sufficiently high $\PVal$-value, meaning $\INP{\epsilon} = \UncertaintySpace$.
\end{itemize}
  
\emph{Case 2:} With probability $2b(1-b)$ we have $(z_1 \in Q \wedge z_2 \in \overline{Q})$ or $(z_2 \in Q \wedge z_1 \in \overline{Q})$ hence $\Score_1\cup\Score_2 = \{0,1\}$. 
\begin{itemize}
    \item If $\Score^z=1$ then $\PVal^z = \frac{2}{3}$.
    \item If $\Score^z=0$ then $\PVal^z=1$.
\end{itemize}
Thus:
\begin{itemize}
    \item If $\epsilon\in[0,\frac{2}{3})$ then $\INP{\epsilon} = \UncertaintySpace$.
    \item If $\epsilon\in[\frac{2}{3},1)$ then $\INP{\epsilon} = \overline{Q}$.
\end{itemize}
\emph{Case 3:} With probability $b^2$ we have $z_1,z_2\in Q$ and $\Score_1=\Score_2=1$. 
\begin{itemize}
    \item If $\Score^z=1$ then $p^z = 1$.
    \item If $\Score^z=0$ then $p^z=1$ as well.
\end{itemize} 
Then for any significance level $\epsilon\in[0,1)$ it holds $\INP{\epsilon}=\UncertaintySpace$.

\emph{In summary, for any fixed $\epsilon$ the INP is fully determined by the calibration set through equations \ref{eq:INP}-\ref{eq:nonconformity-scores}; as a result, $\INP{\epsilon}$ can be thought equivalently as a discrete random variable with support $Q$, $\overline{Q}$ and $\UncertaintySpace$, see Figure \ref{fig:INP-visual}.}
\begin{figure}[h]
\begin{minipage}[t]{0.40\columnwidth} % First image in its TikZ environment
    \begin{tikzpicture}
        % Include the first image
        \node[anchor=south west, inner sep=0] (img1) at (0,0)
            {\includegraphics[width=\textwidth]{Figures/CalSpace.png}};
        
        % Overlay math symbols
        \node at (5, 4) {$(z_1,z_2)$};
        \node at (1.3, -0.2) {$Q$};
        \node at (5.9, 0.2) {$\UncertaintySpace$};
        \node at (-0.1, 6.2) {$\UncertaintySpace$};
        \node at (1.3,1.5) {$Q\times Q$};
        \node at (4.1,1.5) {$\overline{Q}\times Q$};
        \node at (4.1,4.8) {$\overline{Q}\times \overline{Q}$};
        \node at (1.3,4.8) {$Q\times \overline{Q}$};
        
    \end{tikzpicture}
\end{minipage}
\hfill % Horizontal space between images
\begin{minipage}[t]{0.45\textwidth} % Second image in its TikZ environment
    \begin{tikzpicture}
        % Include the second image
        \node[anchor=south west, inner sep=0] (img2) at (0,0)
            {\includegraphics[width=\textwidth]{Figures/INP_distribution.png}};
        
        % Overlay math symbols
        \node at (0.9, 0.7) {$\epsilon$};
        \fill[white!50] (4.8,0.2) rectangle (5.1,0.6);
        \node at (5.1, 0.7) {$Q$};
        \fill[white!50] (5.7,1.2) rectangle (6,1.6);
        \node at (6.0, 1.6) {$\overline{Q}$};
        \fill[white!50] (6.5,2.1) rectangle (6.8,2.5);
        \node at (6.8, 2.5) {$\UncertaintySpace$};
        \node at (6.3, 1) {$\INP{\epsilon}$};

        \node[rotate=270] at (7.8,4.3) {Mass};
    \end{tikzpicture}
    
\end{minipage}
\caption{On the left, a representation of the product space $\UncertaintySpace^2=\UncertaintySpace\times\UncertaintySpace$, partitioned accordingly to the sets $Q$ and $\overline{Q}$, and a hypothetical calibration set $(z_1,z_2)$ as in Case 1. On the right, a summary of Case 1, 2 and 3. On the $x$-,$y$-,$z$-axes are represented the values of $\epsilon$, the prediction (or support) of the INP, and the probability mass function respectively. For any given $\epsilon$, the INP $\INP{\epsilon}$ can be viewed as a discrete random variable with support $Q$, $\overline{Q}$ and $\UncertaintySpace$. In the figure, for $\epsilon=0.8$ and $b=0.3$, the INP predicts $Q$ with probability 0, $\UncertaintySpace$ with probability $b^2$, and $\overline{Q}$ with probability $1-b^2$.}\label{fig:INP-visual}
\end{figure}
%

\emph{\textbf{Example 1} - Part 2.}

Now, fix $E\in[0,1]$ and consider any $\epsilon\in[\frac{2}{3},1)$. Theorem \ref{theo:vovk} implies that
\begin{equation}\label{eq:pac-example}
    \ProbMeas^2(S_E) \geq E^2,
\end{equation}
where
\begin{equation*}
    S_E\doteq\{(z_1,z_2) \in \UncertaintySpace^2: \ProbMeas(\RV_{3}\in\INP{\epsilon}(z_1,z_2,z_1',...,z_\NumTrainSamples')) \geq 1-E \}.
\end{equation*}
However, \eqref{eq:pac-example} does not provide a confidence interval for the probability of drawing a new sample in $Q$, or conversely in $\overline{Q}$. For $\epsilon\in[\frac{2}{3},1)$, $\INP{\epsilon} = \UncertaintySpace$ with probability $b^2$ (from Case 3) and $\INP{\epsilon}=\overline{Q}$ with probability $1-b^2$ (from Case 1 and 2)\footnote{If $\INP{\epsilon}$ predicts $\overline{Q}$ it implies that the nonconformity score of $\RV_{\NumCalSamples+1}$ is predicted to be $0$, whereas if it predicts $\UncertaintySpace$ then all we know is that the nonconformity score of $\RV_{\NumCalSamples+1}$ is predicted to be in $\{0,1\}$ which is uninformative.}, see Figure \ref{fig:INP-visual}. Theorem \ref{theo:vovk} is a distribution-free result and as such it holds for all values of $b$, leading to two cases $b\leq E$ and $b > E$: 

\begin{enumerate}
    \item If $b\leq E$ (i.e. $1-b\geq1-E$):
    \begin{itemize}
        \item If $z_1,z_2\in Q$ (Case 3) we have that $\ProbMeas(\RV_{\NumCalSamples+1}\in\INP{\epsilon}) = \ProbMeas(\UncertaintySpace) = 1 \geq 1-E$, hence $Q\times Q\subseteq S_E$.
        \item If at least one of $z_1$ and $z_2$ belongs to $\overline{Q}$ (Case 1 and 2) we have that $\ProbMeas(\RV_{\NumCalSamples+1}\in\INP{\epsilon}) = \ProbMeas(\overline{Q}) = 1 - b \geq 1-E$, hence $\overline{Q\times Q}\subseteq S_E$.
        \item Thus, $S_E=\UncertaintySpace^2$. Trivially, $\ProbMeas^2(S_E)=\ProbMeas^2(\UncertaintySpace^2) = 1 \geq E^2$.
    \end{itemize}

\item If $b>E$ (i.e. $1-b<1-E$)
\begin{itemize}
    \item If $z_1,z_2\in Q$ (Case 3), as before, $\ProbMeas(\RV_{\NumCalSamples+1}\in\INP{\epsilon}) = \ProbMeas(\UncertaintySpace) = 1 \geq 1-E$, and once again $Q\times Q\subseteq S_E$.
    \item If at least one of $z_1$ and $z_2$ belongs to $\overline{Q}$ (Case 1 and 2) then $\ProbMeas(\RV_{\NumCalSamples+1}\in\INP{\epsilon}) = \ProbMeas(\overline{Q})=1-b<1-E$, hence such $z_1$ and $z_2$ do not belong to $S_E$ by definition.
    \item Thus, $\ProbMeas^2(S_E)=\ProbMeas^2(Q\times Q) = b^2 \geq E^2$.
\end{itemize}
\end{enumerate}
Theorem \ref{theo:vovk} holds for both cases, since we have either $\ProbMeas^2(\UncertaintySpace^2) = 1 \geq E^2$ or $\ProbMeas^2(Q\times Q) = b^2 \geq E^2$. 
Now, assume $b>E$ and that the calibration set gives $R_1=0$ and $R_2=1$. What can we say about $b$? 

For the given calibration set and significance level the INP predicts $\INP{\epsilon}=\overline{Q}$, hence it is tempting to say that $\ProbMeas^2(\ProbMeas(\overline{Q})\geq 1-E)=\ProbMeas^2(1-b\geq 1-E)\geq E^2$, or equivalently $\ProbMeas^2(b\leq E)\geq E^2$: recalling \eqref{eq:PAC-Bern-guarantees}, we may conclude that  $[0,E]$ is a $E^2$ confidence interval for $b$. But this is clearly not true: since we assumed that $b>E$ the interval $[0,E]$ will never contain the parameter $b$ (note that none of the arguments of $\ProbMeas^2()$ depends on $(z_1,z_2)$ in the preceding statement, unlike \eqref{eq:PAC-Bern-guarantees}). We conclude from this example that this is not a viable path to obtain a PAC bound for $b$ comparable to \eqref{eq:PAC-Bern-guarantees}. 
\end{example}

The example above leads us to the following remark and main message of this technical note.
\begin{remark}\label{rem:main}
Theorem \ref{theo:vovk} guarantees the correctness of the \emph{set} predictor $\INP{\epsilon}$. Adopting the frequentist perspective, it is a statement on how often the set predictor $\INP{\epsilon}$ constructed from $\NumCalSamples$ samples attains the desired coverage level $1-E$ for a new realization of $Z_{\NumCalSamples+1}$. In other words, since $b$ is unknown, if $b> E$ the INP attains the desired coverage level only when $\INP{\epsilon} = \UncertaintySpace$ (which is a trivial prediction), and it does not attain the desired coverage level when $\INP{\epsilon} = \overline{Q}$. Essentially, the confidence level of $E^2$ is attained by making trivial predictions sufficiently often. If instead $b\leq E$, the INP is always correct.
Thus, Theorem \ref{theo:vovk} does not estimate $b$ or provide information on the probability of a specific score or class, which is the goal of BPCI methods. See the appendix for a graphical representation.
\end{remark}

To further clarify, consider the equivalent set predictor mapping the elements $z$ predicted by $\INP{\epsilon}$ to their respective nonconformity score
\begin{equation*}
\overline{\INP{\epsilon}}(z_1,...,z_\NumCalSamples,z_1',...,z_\NumTrainSamples')\doteq \bigcup_{z\in\INP{\epsilon}(z_1,...,z_\NumCalSamples,z_1',...,z_\NumTrainSamples')} \NonConfMeas((z_1',...,z_\NumTrainSamples'),z),
\end{equation*}
which amounts to $\overline{\INP{\epsilon}} = \{0,1\}$ when $\INP{\epsilon}=\UncertaintySpace$ and $\overline{\INP{\epsilon}} = \{0\}$ when $\INP{\epsilon}=\overline{Q}$. Let $\Score_{N+1}\doteq \NonConfMeas((z_1',...,z_\NumTrainSamples'),Z_{N+1})$ be the score of the $N+1$-th sample. Then, we can replace the event $\Score_{N+1}\in\overline{\INP{\epsilon}}$ with $\RV_{N+1}\in\INP{\epsilon}$ in \eqref{eq:inner-event-PAC}.
In essence, both BPCI methods and training-conditional CP provide PAC guarantees but differ in scope: while BPCI methods compute an interval containing the true value $b$ describing the probability of \emph{the event that the $\NumCalSamples+1$-th score equals $1$, i.e. $\Score_{N+1}=1$}   (with probability not less than $1-\alpha$), training-conditional CP computes a lower bound for the probability of \emph{the event that the $\NumCalSamples+1$-th score is contained in the predicted set of scores, i.e. $\Score_{N+1}\in\overline{\INP{\epsilon}}$} (with probability not less than $1-\delta$).

Remark \ref{rem:main} extends to any scenario where the nonconformity score takes values from a finite set, effectively defining a classification problem. Training-conditional CP provides a framework for constructing a set predictor that guarantees the desired coverage level with a minimum confidence. The predictor adapts to the calibration data: for ‘good’ calibration data, it produces tight sets (few classes), while for ‘poor’ calibration data, it outputs loose sets (many classes). On average, the probability that the calibration data yields a predictor attaining the coverage level of $1-E$ is at least $1-\delta$. 

Depending on the choice of $\epsilon$ and $E$, we have shown that the $1-\delta$ confidence level may be achieved simply by predicting the entire sample space (i.e., all classes) sufficiently often (see Figure \ref{fig:cp-plot}). However, this approach does not provide meaningful information about the probability of a specific class, which is the focus of \eqref{eq:PAC-Bern-guarantees} and, more generally, BPCI methods.


\subsection{A Note on Safety Verification for Dynamical Systems}\label{sec:safety}
Recent studies have applied training-conditional CP, particularly Theorem \ref{theo:vovk}, to provide PAC guarantees on the safety of control systems with neural network-based controllers \citep{chilakamarri2024reachability, lin2024verification}, and more broadly, on the safety of autonomous systems \citep{vincent2024guarantees}. In this section we show that these works follow the reasoning outlined in \secref{sec:special-case}, and are therefore incorrect. Below, we follow the notation used in \citet{lin2024verification}, but the same applies to the other works.

Consider a dynamical system defined by $\dot{x}=f(x)$ where $x\in X\subseteq \reals^n$, a fixed time horizon $T\in\reals_{>0}$. Denote by $\xi_x(\tau)$ for $\tau\in[0,T]$ the state trajectory of the system at time $\tau$ when initialized at $x$ (for simplicity we assume that the solution to the differential equation exists and is unique)\footnote{In the original paper the trajectory $\xi$ depends on a learned controller and depends on a training set $\RV'_1,...,\RV'_{\NumTrainSamples}$. For clarity we omit this dependence here, since the training set is given and is fixed.}. Let $X_A\subset X$ represent a set of undesirable states, and consider the cost function defined as
\begin{equation*}
J(x)\doteq\min_{\tau\in[0,T]}d(\xi_x(\tau)),
\end{equation*} 
where  $d:X\rightarrow\reals$ is a function satisfying
\begin{equation*}
    d(x)\leq \gamma \iff x\in X_A, \> d(x)> \gamma \iff x\in X\setminus X_A,
\end{equation*} 
for some threshold $\gamma\in\reals$. The function $d$ measures  the distance between a point in the domain and the unsafe set $X_A$. An instructive example for the discussion is below is to choose $\gamma=0$ and $d:X\rightarrow\{0,1\}$, with $d=0\iff x\in X_A$ and $d=1 \iff x\in X\setminus X_A$, but the same applies for any different choice. In this case, $J$ assigns a positive real number to a point $ x\in X$ if and only if the state trajectory from $x$ never intersects with $X_A$. Let $(X,\SigmaAlgebra,\ProbMeas)$ be a probability space. To quantify system safety probabilistically, we seek to estimate $\ProbMeas(\{x : J(x) > 0\})$, i.e. the probability of sampling an initial state that leads to a safe trajectory. In \citet{lin2024verification} the authors define the nonconformity score as $R_i\doteq J(x_i)$ for $i=1,...,\NumCalSamples$, and are therefore interested in estimating $\ProbMeas(\{x : R^x > 0\})$. However, this is equivalent to defining a nonconformity measure as
\begin{equation}
    \NonConfMeas(x)\doteq
    \begin{cases}
        1 \text{ if } x\in X_A, \\
        0 \text{ if } x\in X\setminus X_A,
    \end{cases}
\end{equation}
and we have shown that this line of reasoning is not suitable for estimating the parameter $b$ of a Bernoulli r.v. given $N$ i.i.d. realizations $\Score_i\sim\text{Bern}_b$ of it. 

Since \citet{chilakamarri2024reachability} relies on the framework of \citet{lin2024verification}, it suffers from the same issue. Additionally, in \citet[Theorem~1]{vincent2024guarantees}, the authors re-derive Theorem \ref{theo:vovk}, originally from \citet{vovk2012conditional}. They claim that training-conditional CP reduces to the Clopper-Pearson confidence interval when the underlying i.i.d. random variables are Bernoulli-distributed (see their Sec. Proofs-D). However, we have disproved this claim.


\section{Conclusion}
In this note we examined existing methodologies to use training-conditional CP for statistical safety verification, a problem that can be reduced to estimating the expectation of a Bernoulli random variable. While training-conditional CP remains a powerful tool for uncertainty quantification we have shown that it is not appropriate for BPCI problems.
Specifically, we clarified the correct interpretation of confidence intervals and PAC-style guarantees for training-conditional CP.
We do not rule out the possibility that a different formulation of CP could be applied to BPCI problems. This is left for future work.

%\subsubsection*{Author Contributions}
%If you'd like to, you may include a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors. Only add
%this information once your submission is accepted and deanonymized. 

%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.
%Only add this information once your submission is accepted and deanonymized. 

\bibliography{library}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}
\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/cp_above_new.png}
    
    \label{fig:subfig1}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/cp_below_new.png}
  \end{minipage}
  \caption{On the left the curves resulting from $b_{2,q} > E_q$, on the right the curves resulting from $b_{1,q}\leq E_q$, for $q=0,...,98$.}
  \label{fig:cp-plot}
  \label{fig:figures}
\end{figure}
We validate empirically \eqref{eq:pac-example} as follows and represent the results graphically in Figure \ref{fig:cp-plot}. 

We define a list of values for $E$ by $E_q= 0.01 + 0.01*q$ for $q=0,...,98$. 
For every value of $E_q$ we consider an underlying Bernoulli distribution with parameter $b_{1,q}=E_q - \alpha E_q < E_q$ (right figure) and an underlying Bernoulli distribution with parameter $b_{2,q}=E + \alpha E_q\% > E_q$ (left figure) with $\alpha=0.005$. For every value of $q=0,...,98$ we examine the two situations $b_{1,q} \leq E_q$ and $b_{2,q}>E_q$, as mentioned in Example 1 - Part 2. The significance level $\epsilon$ is set to $2/3$.  We draw $n_{\text{cal}}=5\cdot10^{4}$ pairs of calibration points $\{z^{(i)}_1,z^{(i)}_2\}_{i=1}^{n_{\text{cal}}}$. For every pair of calibration points $z^{(i)}_1,z^{(i)}_2$ we construct the resulting INP as $\INP{\epsilon}_{(i)}\doteq\INP{\epsilon}(z^{(i)}_1,z^{(i)}_2,...)$, draw $n_{\text{test}}=5\cdot10^{4}$ test points $\{z_{N+1}^{(j)}\}_{i=j}^{n_{\text{test}}}$ and compute the empirical frequency $\hat{g}_i = \frac{|\{j=1,\ldots n_{\text{test}} : z_{N+1}^{(j)} \in \INP{\epsilon}_{(i)}\}|}{n_{\text{test}}}$ as an approximation for $\mathbb{P}(\RV_{\NumCalSamples+1}\in\INP{\epsilon}_{(i)})$; finally we compute $\hat{h}=\frac{|\{i=j,\ldots n_{\text{cal}} : \hat{g}_i \geq 1-E\}|}{n_{\text{cal}}}$ as an approximation to $\ProbMeas^2(S_E)$ shown in the plots as the solid red line. The solid black line represents the curve given by $E^2$, which remains always below the red line in both plots, as expected. The area shaded in blue represents the fraction of the $\hat{g}_i$'s for which the INP $\INP{\epsilon}_{(i)}$ is equal to $\UncertaintySpace$, whereas the area shaded in red represents the fraction of the $\hat{g}_i$'s for which the INP $\INP{\epsilon}_{(i)}$ is equal to $\overline{Q}$ \emph{and} $\hat{g}_i$ is greater or equal than $1-E$. It is visible in the left plot that the only reason why the solid red line (approximating $\ProbMeas^2(S_{E_q}) = b_{2,q}^2$) is above $E_q^2$ is that the INP is allowed to predict the entire set $\UncertaintySpace$. In contrast, on the right the solid red line approximates $\ProbMeas^2(S_{E_q}) = 1$ since any pair of $z^{(i)}_1,z^{(i)}_2$ results in a prediction $\INP{\epsilon}_{(i)}$ satisfying $\mathbb{P}(\RV_{\NumCalSamples+1}\in\INP{\epsilon}_{(i)}) \geq 1-E_q$; accordingly, for a fixed $q$, the area shaded in red covers approximately $1-b_{1,q}^2$ of the 'Probability' axis and the area shaded in blue approximately $b_{1,q}^2$.

In summary, in both situations the theorem is confirmed empirically, since the red line is always above the black line. In the first case, where $b_{2,q} > E_q$, the minimum confidence level of $E_q^2$ is attained by predicting sufficiently often the entire sample space $\UncertaintySpace$, precisely with a frequency of $b_{2,q}^2$, as this is the only set prediction attaining the required coverage probability of $1-E_q$. Unfortunately, a prediction of the entire sample space is uninformative. In the second case, where $b_{1,q} \leq E_q$, any predicted set between $\overline{Q}$ and $\UncertaintySpace$ attains the required coverage probability of $1-E_q$.
%You may include other additional sections here.

\end{document}
