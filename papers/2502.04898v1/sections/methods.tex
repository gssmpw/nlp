\section{Methods}
\label{sec:methods}
In this section, details about the dataset and the general implementation of the ARTInp framework are presented, describing the dual architecture, which includes the completion and the translation networks, the dataset and the metrics used for the evaluation. 

\subsection{Dataset}
\label{ssec:dataset}
The dataset used in this work is from the SynthRad 2023 challenge \cite{Thummerer2023}. 
It includes paired CBCT and CT images of the brain, acquired from three Dutch medical centers between 2018 and 2022, with a total of 180 patients (60 patients per center). 
The patients were included regardless of tumor etiology, with age ranging from 3 to 93 years (mean is 65); the images were acquired using different scanner models and manufacturer; these aspects helped in promoting a substantial degree of diversity in the data.
All images were resampled with a voxel spacing of $1 \times 1 \times 1 mm^3$; rigid image registration between the CBCT and the CT was performed using Elastix\footnote{\url{https://elastix.lumc.nl/index.php}}.
Finally, the images were anonymized by defacing the patients and converted in the Nifti format, with each axial slice having a resolution of $256 \times 256$ pixels.
For this work we generated 80\%-10\%-10\% random splits on the patients (144-10-10 patients) for, respectively, training, validation, and test sets; then, we extracted each patient's axial slices and stored them as individual images, encoded in a single channel TIFF format to preserve the entire range of Hounsfield units (HUs).


\subsection{ARTInp Framework}
\label{ssec:ARTInp}
\begin{figure*}[tb]
	\centering
		\includegraphics[scale=0.22]{methods/FrameworkInpainting.pdf}
	\caption{The overview of the ARTInp framework that includes a completion network and a translation network.}
	\label{fig:framwork_inpainting_translation}
\end{figure*}

We propose a novel framework, Adaptive Radiation Therapy Inpainting, dubbed ARTInp, that combines deep learning-based inpainting and CBCT-to-CT translation to enhance CBCT-based workflows in radiotherapy. 
Fig.~\ref{fig:framwork_inpainting_translation} shows the overview of the framework, consisting of two main components: a completion network, that fills the anatomical gaps in the CBCT volumes, and an image translation network, that generates synthetic CT (sCT) images from the inpainted CBCT images.
Both these networks are based on a GAN architecture, which involves a generator for image synthesis a discriminator for evaluating the quality of the generated images.
The completion network has been trained to receive in input a sagittal CBCT slice of the patient with a gap, the corresponding CT slice, and a binary mask indicating the location of the gap in the CBCT slice; the network outputs a \emph{synthetic} CBCT slice with the gap filled (sCBCT in Fig.~\ref{fig:framwork_inpainting_translation}); then, the \emph{inpainted} CBCT is generated by combining the original CBCT with the gap region of 
the generated sCBCT, employing a Poisson image blending~\cite{10.1145/882262.882269} to ensure a smooth transition between the inpainted part and the original ones.
The resulting \emph{inpainted} CBCT slices generated by the completion network are then joined to form the inpainted CBCT volume of the patient, which is used to extract the axial slices that are fed to the translation network.
In fact, the translation network is trained to convert an input \emph{axial} CBCT slice into a corresponding sCT axial slice, allowing to generate a full sCT volume from the \emph{inpainted} CBCT volume. 

%ARTInp employs a dual-network approach: a completion network that fills anatomical gaps in CBCT volumes and a custom Generative Adversarial Network (GAN) to generate high-quality synthetic CT (sCT) images. The inpainting network is trained to fill the gaps in the CBCT images, while the translation network is trained to convert the inpainted CBCT images into sCT images. The inpainting network is trained using sagittal slices of the patient, while the translation network is trained using axial slices. The inpainting network is trained using the CBCT, the corresponding CT, and a binary mask indicating the location of the gap in the CBCT. The translation network is trained using the inpainted CBCT and the corresponding CT. The inpainted CBCT images are converted into axial slices and used as input to the translation network to generate sCT images. The sCT images are then reconstructed into NIfTI files and evaluated using the Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM) metrics.

\subsection{Completion Network}
\label{ssec:completion}
\begin{figure*}[tb]
	\centering
		\includegraphics[scale=0.36]{images/methods/CompletionNetwork.pdf}
	\caption{The architecture of the completion network.}
	\label{fig:completion_network}
\end{figure*}
Fig.~\ref{fig:completion_network} shows the architecture of the completion network, which is based on GLCIC~\cite{glcic} and consists of an encoder-decoder structure with convolutional, dilated convolutional, and transposed convolutional layers. 
The encoder-decoder architecture combines convolutional, dilated convolutional, and transposed convolutional layers.
The generator network takes three paired sagittal images of equal size in input: a CT slice, the corresponding CBCT with a gap, and the binary mask of the gap region in the CBCT; the images are 16-bit encoded and have a resolution of $160 \times 160$ pixels.
%There are three images used in the three-channel input of the network, all in the 16-bit range as well and with a 160x160 resolution each one. The sagittal slices of the patient are extracted, both from the CBCT and the CT paired volumes. A gap is generated randomly in the CBCT. and the network takes as input, the sagittal slice of the CT, the sagittal slice of the CBCT with the gap, and, a binary mask with the location of the gap. 
Convolutional layers in the encoding portion of the network have the purpose of extracting features at increasing scale (relatively to the original image) while while dilated convolutions expand the receptive field without reducing resolution; the decoding layers upsample the features, refining details through transposed and standard convolutions in sequence; the final layer produces the grayscale image in output using a $3x3$ convolution and sigmoid activation function. 
To finalise the output, the 16-bit grayscale sCBCT yielded by the network is combined to the input gaped one applying the Poisson image blending to smooth the transition between the inpainted area and the area where the original image was available; the result is a 16-bt grayscale inpainted CBCT; repeating this process for all the slices in the volume and stacking the results, yields the final inpainted CBCT volume. 

The discriminator is a dual-network architecture (a Global Context discriminator and a Local Context discriminator) that evaluates the inpainted CBCT slices at both global and local levels.
The Global Context Discriminator processes the entire inpainted CBCT through $6$ convolutional layers (stride 2, padding 2) with $5\times 5$ filters, each halving spatial dimensions and increasing the number of feature maps; the terminal fully connected layer outputs a 1024-dimensional feature vector capturing the global context.
The Local Context Discriminator focuses on $96\times 96$ patches centered on the inpainted regions; its architecture mimics the Global Discriminator but uses $5$ convolutional layers; it returns a 1024-dimensional vector representing the local context.
The two output vectors are concatenated into a 2048-dimensional vector, processed by a fully connected layer, and passed through a sigmoid activation that yields an estimate of the probability of the image being real.

The training of the completion network consists of three phases: 
\begin{enumerate}
    \item the generator is pre-trained by itself in a supervised fashion;
    \item the discriminators are pre-trained with an semi-adversarial approach where the generator's weights frozen; 
    \item the whole network is trained together in a proper adversarial setting.
\end{enumerate}
The loss function used to train the completion network combines two components: a weighted Mean Squared Error (MSE) loss and an adversarial loss, ensuring both stability and image fidelity: let \( C(x, M_c) \) represent the completion network, where \( x \) is the input image and \( M_c \) is the binary mask indicating the gap region (it has 1's in correspondence of the gap and 0's elsewhere); the MSE loss is defined as follows:
\begin{align}
    \loss_{\text{MSE}}(x, M_c) &= \left\| M_c \odot (C(x, M_c) - x) \right\|_2
\label{eq:mse}
\end{align}
It measures the difference between the CBCT input and the sCBCT output focusing on the regions to be completed.
The adversarial loss combines the discriminators' output in a single term,  \( D(\cdot) \), formulated as a min-max optimization problem: 

\begin{align}
    D(\cdot) = \min_C \max_D \mathbb{E} \Big[ & \log D(x, M_d) + \nonumber \\
    & + \log(1 - D(C(x, M_c), M_c)) \Big]
\label{eq:minmax_gan}
\end{align}

where \( M_d \) is a random mask that determines which parts of the image are considered during the discrimination process, \( D(x, M_d) \) represents the discriminator's output when fed a real CBCT image \( x \) along with the mask \( M_d \), and \( D(C(x, M_c), M_c) \) represents the discriminator's output when fed an sCBCT from the generator.
The discriminator \( D \) aims to maximize its ability to distinguish real and generated images, while the completion network \( C \) attempts to generate images that fool the discriminator.
The final objective function combines the MSE and the adversarial term, balancing them with the parameter $\alpha$, that can be tuned to increase the effect of one term: 

\begin{align}
    \min_C \max_D \mathbb{E} \Big[ &\loss_{\text{MSE}}(x, M_c) + \alpha \big( \log D(x, M_d) \nonumber \\
    & + \log(1 - D(C(x, M_c), M_c)) \big) \Big]
\label{eq:final_loss_completion}
\end{align}

%After the training, the model is used for inference in the test set but this time, the creating of the gaps in the test set is different. In training, the gap was created randomly in any given sagittal slice.
%These holes have the width and height between 48 and 96 pixels and they were positioned randomly within the slice.
%In the testing part, the gap is created patient-wise. It has the width of the sagittal slice and a a height of $2.5cm$, this means that the gap is equivalent to removing a set of entire axial slices from the patient's 3D volume. The gap now represents a missing part of the entire volume ($2.5cm$) and this type of gap is more likely to occur in clinical practice, for example, if you need to combine several CBCT scans over a greater area of the patient and the combination of these scans leaves a gap in between.
%Dividing the total height of the patient's volume based on its slice thickness by the total height of the patient in pixels results in the ratio of millimeters per pixel and thus the $2.5cm$ gap can be calculated individually for each patient.

\subsection{Translation Network}
\begin{figure*}[tb]
	\centering
		\includegraphics[scale=0.36]{images/methods/TranslationNetwork.pdf}
	\caption{The architecture of the translation network that uses the axial CBCT slices in the 16-bit change and outputs sCT slices.}
	\label{fig:image_translation_network}
\end{figure*}

Fig.~\ref{fig:image_translation_network} shows the architecture of the translation network, based on pix2pix~\cite{isola2017image}; it employs a UNet256 generator and a PatchGAN discriminator.
The generator takes as input a 16-bit $256 \times 256$ axial CBCT slice in input and outputs a 16-bit sCT slice of the same size.
The architecture of the generator consists of an encoder-decoder structure with symmetric layer and skip connections; it features $8$ downsampling blocks comprising a convolutional layer with $4\times4$ filters (stride 2, padding 1), batch normalization, and leaky-ReLU activation; after the bottleneck, the symmetric decoder starts; each decoder block is composed by $4\times4$ transposed convolutions (stride 2, padding 1) to increase the spatial dimensionality, batch normalization, ReLU activation, and skip connections with corresponding encoder layers, that combine high-resolution details from the encoder with the upsampled feature maps, preserving fine-grained details and spatial structure; finally, the outermost block generates the output image through a $4\times4$ transpose convolution (stride 2, padding 1) and a $\tanh$ activation function.
The discriminator is designed to classify $70\times70$ pixel patches of input images as real or fake; by focusing on local patches rather than entire images, it captures fine-grained details efficiently, making it adaptable to various image sizes through its fully convolutional structure.
The architecture of the discriminator consists of a $4\times4$ convolutional layer (stride 2, padding 1) featuring 64 filters, followed by a Leaky ReLU activation; subsequent layers progressively double the number of filters (128, 256, and 512), using the same convolutional structure with batch normalization and leaky-ReLU activation; the final layer generates a single-channel output using a $4\times4$ convolution (stride 1, padding 1), predicting the authenticity for the patches.

The translation network is trained using a loss function composed of two main components: adversarial loss and $\loss_1$ loss.
The adversarial loss ensures that the generator \( G \) produces images that are indistinguishable from real target images \( Y \). To achieve this, a the discriminator \( D \) learns to differentiate between real images \( Y \) and generated images \( G(X) \). The discriminator assigns a probability \( D(y) \) close to \(1\) for real images and \( D(G(x)) \) close to \(0\) for fake images.
The adversarial loss function is formulated as follows:

\begin{align}
    \loss_{\text{GAN}}(G,D) &= \mathbb{E}_{y \sim p_{\text{data}}(y)} 
    \left[ \log D(y) \right]  \nonumber \\ 
    &+ \mathbb{E}_{x \sim p_{\text{data}}(x)} 
    \left[ \log (1 - D(G(x))) \right]
\end{align}

This results in a \emph{min-max} optimization problem:

\begin{equation}
    \min_G \max_D \loss_{\text{GAN}}(G,D)
\end{equation}

While the adversarial loss encourages image fidelity, it does not guarantee that the generated image \( G(x) \) closely matches the corresponding ground truth image \( y \). 
To address this, $\loss_1$ loss enforces pixel-wise similarity:

\begin{align}
    \loss_{1}(G) &= \mathbb{E}_{x,y \sim p_{\text{data}}(x,y)} 
    \left[ \| y - G(x) \|_1 \right]
\end{align}

where \( \| \cdot \|_1 \) denotes the $\loss_1$ norm, which minimizes the absolute differences between the generated image \( G(x) \) and the real image \( y \).

The final objective function is a weighted combination of the adversarial loss and the $\loss_1$ loss:

\begin{align}
    \loss(G,D) &= \loss_{\text{GAN}}(G,D) + \lambda \cdot \loss_1(G)
\end{align}

where \( \lambda \) is a hyperparameter that controls the trade-off between adversarial realism and structural similarity. The full optimization objective remains:

\begin{align}
    \arg \min_G \max_D & \loss_{\text{GAN}}(G,D) + \lambda \cdot \loss_1(G)
\end{align}

This formulation ensures that the generated images are both visually realistic and structurally aligned with the corresponding ground truth images.
%The image translating training ran for 60 epochs, using a batch size of 1 in the Kaggle platform.

\subsection{Evaluation Metrics}
To evaluate the quality of sCT images generated by our models, we used three metrics: 
\begin{itemize}
    \item Mean Percentage Absolute Error (MAE\%): measures the average absolute difference between the pixel values of the generated images and the ground truth images as a percentage of the overall pixel value range;
    \begin{equation*}
        \text{MAE\%} = \frac{100}{N \cdot R} \sum_{i=1}^{N} \lvert I_i - \hat{I}_i \rvert \\[10pt]
    \end{equation*}
    
    \item Peak Signal-to-Noise Ratio (PSNR): quantifies the ratio between the maximum possible power of the image and the power of the noise that affects the image;
    \begin{equation*}
        \text{PSNR} = 10 \cdot \log_{10} \left( \frac{I_\text{MAX}^2}{\frac{1}{N} \sum_{i=1}^{N} (I_i - \hat{I}_i)^2} \right) \\[10pt]
    \end{equation*}
    \item Structural Similarity Index (SSIM).
    \begin{equation*}
        \text{SSIM}(I, \hat{I}) = \frac{(2 \mu_I \mu_{\hat{I}} + C_1)(2 \sigma_{I\hat{I}} + C_2)}{(\mu_I^2 + \mu_{\hat{I}}^2 + C_1)(\sigma_I^2 + \sigma_{\hat{I}}^2 + C_2)}
    \end{equation*}
\end{itemize}

In the formulas above, $N$ is the number of pixels in the image, $I_i$ $\hat{I}_i$ are the pixel values of the original and generated image respectively, $R$ is the range of pixel values in the dataset, $I_\text{MAX}$ represents the maximum possible pixel value, $\mu_I$ and $\mu_{\hat{I}}$ are the mean values of the original and generated images, $\sigma_I^2$ and $\sigma_{\hat{I}}^2$ are their variances,$\sigma_{I\hat{I}}$ is the covariance between the two images, $C_1$ and $C_2$ are small constants to stabilize the division.

All these metrics are computed slice by slice considering only the region of the image that contains the patient, such that the background of the image does not affect the calculation.

%A statistical test between the patients in the test of both models is made and the difference is considered statistical significant if the p-value is bellow $0.05$.



%The original CT volumes are 16-bit encoded and by compressing their information to 8-bit, some of the details of the image might be lost and they could have some clinical significance.
%To preserve this information the pix2pix architecture was changed to use 16-bit encoded images as input.

%To test the applicability of translating an inpainted image, the first experiment uses only the translation network to transform CBCT into sCT and evaluates its performance. 
%The second experiment started with the CBCT with added gaps and aimed to fill the generated gaps along the z-dimension of the patient in its CBCT image. Sagittal slices of the patient were used to train the inpainting model instead of the axial slices because of the gaps completely removed some axial slices.
%The CBCT gaped slice was concatenated with the binary mask indicating the region to be inpainted and the paired CT slice to provide additional information to the inpainting network and allow it to see patient specific information.
%Therefore, the inpainting model model training included three images: the gapped CBCT, the corresponding complete CT, and the binary mask indicating the location of the gap of the CBCT.

%The output CBCT images with the gaps filled were used as inputs to the image translation model. The sagittal slices were converted to axial slices and used for inference to obtain sCT and finally reconstruct the patients' NIfTI files and perform the evaluation between using only the translation network and the whole ARTInp framework.
