\section{Related Works}
\subsection{Post-Training Quantization}
Post-training quantization is an efficient and expeditious quantization approach which merely necessitates a limited amount of calibration data to statistically determine the quantization parameters that help to scale float values to low-bit. AdaRound ____ analyzes the quantization errors and employs a layer-wise optimization approach to learn the optimal rounding mechanism. BrecQ ____ divides the model weights into multiple blocks and independently quantizes each block, allowing for finer control over quantization errors.

In recent years, there has been a growing research interest in PTQ methods for LLMs. GPTQ ____ quantizes weights column-wise based on the Hessian matrix and dynamically updates the remaining weights to compensate for quantization errors. AWQ ____ retains 1\% of salient weights and calculates quantization parameters based on output activations. ZeroQuant ____ performs group-wise quantization for weights and finer-grained per-channel quantization for activations. SmoothQuant ____ considers that the cause of quantization errors in activations lies in the presence of channel-wise outliers and proposes smoothing parameters to reduce their magnitude. OmniQuant ____ combines the advantages of previous works and learns the optimal smoothing and quantization parameters through back propagation, making it the current state-of-the-art PTQ method for LLMs.

Unfortunately, for extremely low-bit (sub 2-bit) quantization, which offers the highest compression ratio, the performance of such methods generally suffers significantly. 



\subsection{Extremely Low-Bit Quantization}

Extremely low-bit quantization refers to approaches where the effective bit-width for weights is sub 2-bit. It has been widely welcomed due to significant compression ratio but suffers from severe performance degradation. BNN ____ is the first model binarization method and %utilizes STE ____ for backpropagation. 
XNOR-Net ____ presents scaling factors which reduce binarization errors with acceptable additional memory cost. RBNN ____ indicates that except for magnitude gaps, angular biases ought to be considered so that extra rotation matrices are introduced to overcome the drawback.

Several extremely low-bit QAT methods for LLMs ____ have been proposed recently. Regrettably, the immense computational resource consumption and the lack of open-source availability have hindered their widespread application so that there is a growing demand for more economical PTQ methods. PB-LLM ____ investigate the importance of salient weights and design extra 1-bit unstructured masks to retain them into 8-bit while binarizing the others. BiLLM ____ further presents finer-grained masks to divide into multi-groups for binarization using different scaling factors. However, the fine-grained masks that cannot be compressed in both methods results in the equivalent bitwidths exceeding 2 bits. To make contributions for truly extremely low-bit PTQ research, we propose \textbf{PTQ\textit{1.61}} which addresses the issues above and obtains promising performance.