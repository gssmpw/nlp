\section{Related Works}
\subsection{Post-Training Quantization}
Post-training quantization is an efficient and expeditious quantization approach which merely necessitates a limited amount of calibration data to statistically determine the quantization parameters that help to scale float values to low-bit. AdaRound \citep{nagel2020up} analyzes the quantization errors and employs a layer-wise optimization approach to learn the optimal rounding mechanism. BrecQ \citep{li2021brecq} divides the model weights into multiple blocks and independently quantizes each block, allowing for finer control over quantization errors.

In recent years, there has been a growing research interest in PTQ methods for LLMs. GPTQ \citep{frantar2022gptq} quantizes weights column-wise based on the Hessian matrix and dynamically updates the remaining weights to compensate for quantization errors. AWQ \citep{lin2023awq} retains 1\% of salient weights and calculates quantization parameters based on output activations. ZeroQuant \citep{yao2022zeroquant} performs group-wise quantization for weights and finer-grained per-channel quantization for activations. SmoothQuant \citep{xiao2023smoothquant} considers that the cause of quantization errors in activations lies in the presence of channel-wise outliers and proposes smoothing parameters to reduce their magnitude. OmniQuant \citep{OmniQuant} combines the advantages of previous works and learns the optimal smoothing and quantization parameters through back propagation, making it the current state-of-the-art PTQ method for LLMs.

Unfortunately, for extremely low-bit (sub 2-bit) quantization, which offers the highest compression ratio, the performance of such methods generally suffers significantly. 



\subsection{Extremely Low-Bit Quantization}

Extremely low-bit quantization refers to approaches where the effective bit-width for weights is sub 2-bit. It has been widely welcomed due to significant compression ratio but suffers from severe performance degradation. BNN \citep{courbariaux2016binarized} is the first model binarization method and %utilizes STE \citep{bengio2013estimating} for backpropagation. 
XNOR-Net \citep{rastegari2016xnor} presents scaling factors which reduce binarization errors with acceptable additional memory cost. RBNN \citep{lin2020rotated} indicates that except for magnitude gaps, angular biases ought to be considered so that extra rotation matrices are introduced to overcome the drawback.

Several extremely low-bit QAT methods for LLMs \citep{xu2024onebit,wang2023bitnet,ma2024era} have been proposed recently. Regrettably, the immense computational resource consumption and the lack of open-source availability have hindered their widespread application so that there is a growing demand for more economical PTQ methods. PB-LLM \citep{shang2023pb} investigate the importance of salient weights and design extra 1-bit unstructured masks to retain them into 8-bit while binarizing the others. BiLLM \citep{huang2024billm} further presents finer-grained masks to divide into multi-groups for binarization using different scaling factors. However, the fine-grained masks that cannot be compressed in both methods results in the equivalent bitwidths exceeding 2 bits. To make contributions for truly extremely low-bit PTQ research, we propose \textbf{PTQ\textit{1.61}} which addresses the issues above and obtains promising performance.