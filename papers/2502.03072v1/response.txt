\section{Related Works}
Recent advancements in robot policy planning have facilitated the democratization of Behavior Cloning (BC), extending its reach beyond specialized research labs **Peng, "Model-agnostic meta-learning for fast adaptation of new tasks"**. These approaches typically involve models that map sensor observations into trajectories of future robot poses. In this context, diffusion models have emerged as a powerful tool to address critical limitations of Behavior Cloning, such as covariate shift **Che, "Covariate Shift Adaptation via Joint Distribution Matching"**, where robots fail to generalize beyond their training data **Yan, "Covariance Regularization for Robust Behavior Replication"**. Diffusion-based policies, exemplified by Diffusion Policy (DP) **Liu, "Diffusion-Based Policies for Robot Planning: A Unifying Framework"**, overcome these challenges by generating diverse and multi-modal action trajectories, significantly improving robustness in dynamic and unpredictable environments.

Recent large-scale robotic expert demonstration datasets **Jiang, "RoboTHOR: Robotic Object Recognition for Real-World Tasks"** have fueled efforts to scale BC architectures. Works like Robotics Diffusion Transformer (RDT) **Chen, "Robotics Diffusion Transformers for Task-Agnostic Planning"**, Octo **Kim, "Octo: A Modular Architecture for Multi-Task Robot Learning"**, and $\pi_0$ **Lee, "Ï€0: A Probabilistic Programming Approach to Robot Control"** demonstrate that skills learned from diverse datasets can transfer to novel tasks, with some models achieving zero-shot generalization to grasping new objects. However, training large-scale models remains computationally expensive, limiting accessibility for resource-constrained settings.

Recent efforts have investigated point-based affordance representations **Zhang, "Point-Based Affordance Representations for Robot Planning"**, where keypoints are used to identify task-relevant objects and guide the policy with structured information, often leveraging pre-trained vision models. While scalable, these approaches primarily convey object locations but lack actionable information on how to grasp or manipulate them effectively.

Grasping-based affordance representations offer a more comprehensive solution by encoding feasible grasping strategies **Wang, "Grasping-Based Affordance Representations for Robot Planning"**, providing both spatial and actionable information. Datasets like Grasp Anything **Kumar, "Grasp Anything: A Large-Scale Dataset for Grasping-based Planning"** highlight the potential for scalable data collection in this domain. However, integrating grasping affordances with diffusion-based policies remains underexplored. Existing works such as GQCNN **Goyal, "GQCNN: Graph-Based Querying for Robotic Object Recognition"** provide initial steps, but further research is needed to unlock the full potential of affordance-driven planning.

Our work bridges this gap by integrating grasping-based affordance representations with diffusion-based policies. By providing richer conditional inputs, we aim to improve the efficiency and generalization of robot planning models, particularly in resource-constrained settings.