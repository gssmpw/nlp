\section{Related Works}
Recent advancements in robot policy planning have facilitated the democratization of Behavior Cloning (BC), extending its reach beyond specialized research labs \cite{aloha, aloha2, umi}. These approaches typically involve models that map sensor observations into trajectories of future robot poses. In this context, diffusion models have emerged as a powerful tool to address critical limitations of Behavior Cloning, such as covariate shift \cite{covariate-shift}, where robots fail to generalize beyond their training data \cite{domain-generalisation}. Diffusion-based policies, exemplified by Diffusion Policy (DP) \cite{diffusion-policy}, overcome these challenges by generating diverse and multi-modal action trajectories, significantly improving robustness in dynamic and unpredictable environments.

Recent large-scale robotic expert demonstration datasets \cite{x-embodiment} have fueled efforts to scale BC architectures. Works like Robotics Diffusion Transformer (RDT) \cite{rdt}, Octo \cite{octo}, and $\pi_0$ \cite{pi0} demonstrate that skills learned from diverse datasets can transfer to novel tasks, with some models achieving zero-shot generalization to grasping new objects. However, training large-scale models remains computationally expensive, limiting accessibility for resource-constrained settings.

Recent efforts have investigated point-based affordance representations \cite{moka, kalie, rekep}, where keypoints are used to identify task-relevant objects and guide the policy with structured information, often leveraging pre-trained vision models. While scalable, these approaches primarily convey object locations but lack actionable information on how to grasp or manipulate them effectively.

Grasping-based affordance representations offer a more comprehensive solution by encoding feasible grasping strategies \cite{grasping-survey}, providing both spatial and actionable information. Datasets like Grasp Anything \cite{grasp-anything} highlight the potential for scalable data collection in this domain. However, integrating grasping affordances with diffusion-based policies remains underexplored. Existing works such as GQCNN \cite{gqcnn} provide initial steps, but further research is needed to unlock the full potential of affordance-driven planning.

Our work bridges this gap by integrating grasping-based affordance representations with diffusion-based policies. By providing richer conditional inputs, we aim to improve the efficiency and generalization of robot planning models, particularly in resource-constrained settings.