\section{Related Work}
\textbf{Trajectory Segmentation:} A significant body of work has focused on learning reusable skills from a set of trajectories in an unsupervised manner **Kwiatkowski, "Learning Reusable Skills"**. The underlying approach shared by these methods is to model the distribution of trajectories using a latent variable model, where the latent variables represents the active skill at each timestep. After training the probabilistic model via maximum likelihood **Kingma and Welling, "Auto-encoding Variational Bayes"**, inferring the latents for a trajectory results in a segmentation of the trajectory **Higgins et al., "Beta-Variational Autoencoders"**. However, there is no guarantee that the segments will correspond to specific instructions. To ensure that the extracted subsegments align with natural language instructions, a set of annotated examples can be employed **Sukhbaatar et al., "Weakly-supervised Learning of Instructional Policies"**. Given the high cost of annotations, it is desirable for this set to be only a small fraction of the available demonstrations, which naturally leads to a semi-supervised setup **Ravi and Larochelle, "Zero-Shot Task Generalization with Multi-Task Deep Learning"**. Prior work looked at data setups in which trajectories are paired with plans describing the sequence of skills performed by the agent **Chen et al., "Deep Reinforcement Learning for Instructional Policies"**. Here, the annotation are segments of individual instructions within longer trajectories. Therefore we do not need to model the segmentation as a latent variable but can learn the parameters of a conditional distribution over segmentations given a trajectory.

\textbf{Semi-Supervised Instruction Following}: The standard approach to train instructable agents is to perform imitation learning on a dataset of instruction-trajectory pairs **Pomerleau, "Algorithms for Learning to Perceive"**. Due to the difficulty of generating natural language annotated trajectories, the problem is often studied in a semi-supervised learning setup **Rosenfeld et al., "Meta-Learning with Latent Embedding Optimization"**. For example, **Guo and Liu, "Deep Unsupervised Segmentation via Self-Supervised Learning"** train a labelling  model using CLIP embeddings **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** on a small annotated dataset to then label a large unannotated dataset. Training a policy on the joint dataset improves the task accuracy. However, the unlabelled and labelled trajectories come from the same distribution, making it unnecessary to identify segments corresponding to instructions in the unlabelled trajectories. The model that has been trained in a setting closest to ours is **Mnih et al., "SL3: Pre-training for Policy Transfer"**. Given a small set of trajectories labelled with an overall goal and the sequence of instructions contained in it, **Mnih et al., "SL3: Pre-training for Policy Transfer"** applies an iterative procedure of segmenting trajectories, labelling segments and learning an instruction conditioned policy. The labelled dataset here consists of individual instructions and not plans, but unlike the case of SL3 does contain information about the start and end of these instructions.

\textbf{Video Segmentation}: The vision community has studied the task of identifying actions in uncropped videos under related but distinct setups. While Action Segmentation (AS) methods **Huang et al., "Deep Learning for Action Segmentation"** try to predict for each frame the correct action class, Temporal Action Localization (TAL) methods try to predict the boundary timesteps of an action segment and a corresponding action label **Lea et al., "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition"**. Some frameworks are developed to handle multiple of these tasks **Carreira et al., "Quo Vadis, Action Recognition? A Grand Challenge Dataset and State-of-the-Art Analysis"**. Datasets for training contain uncropped videos containing multiple, not necessarily contiguous, action segments that are annotated on a frame level or with segment boundary information **Kovashka et al., "ActionBank: An Action-Driven Framework for Video Understanding"**. Both classes of methods have been studied under different levels of supervision **Liu and Shah, "Learning to Recognize Temporal Segments of Actions in Videos"**.
Our setting provides a unique challenge where the length of the training videos and the number of activities in the training videos differ from the videos at test time. To the best of our knowledge we are the first to investigate the usage of video segmentation models for data augmentation in sequential decision making settings.