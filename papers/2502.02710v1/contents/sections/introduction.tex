% \nico{I write [mrw] to flash where I moved or merged related work into main text}

The success of machine learning methods widely depends on the assumption that the data follow the same distribution during training and test time. However, this assumption is often violated in practice, that is, the test distribution is shifted. For instance, this can happen if the test data are collected at a later time or using a different measuring device (e.g., \cite{koh2021wilds}).
%- . This setting is generally known as \emph{distribution shift} \cite{quinonero2022dataset}. \fy{this is a century old problem - if we cite, need to cite carefully - here} 
%The task of creating models which perform well under distribution shifts, known as \emph{domain generalization}, remains one of the biggest challenges in modern machine learning. 
Without further assumptions on the test distribution, 
generalization under distribution shift is impossible.
%the domain generalization problem is, \fy{do we need to use this jargon here? } in general, ill-posed. 
However, practitioners often have partial information about the 
set of possible shifts that may occur during test time, giving rise to a set of \emph{feasible test distributions} that one would like to generalize to, typically referred to as the \emph{robustness set}.
%strength or structure of possible distribution shifts during test time (e.g., the covariate shift assumption \cite{shimodaira2000improving} or bounded $f$-divergence \citep{ben2010theory, hu2018does}) \fy{here, I expected practical shifts rather than theoretical, probably would hold back with this in first paragraph}, which gives rise to a set of feasible test distributions called the \emph{robustness set} \fy{called by whom?}. 
In safety-critical applications,
%such as medicine or autonomous driving \fy{this is a little bold} \julia{remove examples then?}, 
the goal is often
%that the model stays reliable on  \emph{all} feasible distribution shifts: thus, one aims 
to find a \emph{robust prediction model} â€“ a model which shows the best performance on the \emph{worst-case} distribution out of the robustness set. 
%This estimator is commonly referred to as the \emph{robust predictor}. 
\par
% \fy{content-wise suggestion:}

When no prior information is available to describe the set of expected %"realistic" 
distribution shifts, a natural approach is to be robust in 
%When the robustness set is explicitly known \fy{such 
a neighborhood of the training distribution, % \fy{plural here?},
%with respect to some discrepancy measure, 
typically referred to as distributionally robust optimization (DRO), e.g., \citep{bental2013robust, duchi2021learning, sinha2017certifying, mohajerin2018data}. 
% \julia{add more work on DRO}
%DRO methods based on Wasserstein distance \citep{sinha2017certifying, mohajerin2018data} allow for test distributions outside of the training support, unlike the ones based on $f$-divergences \citep{bental2013robust, duchi2021learning}. Group DRO \citep{sagawa2019distributionally} 
%This is sth for which one can compute "optimally" robust estimator explicitly e.g. DRO \fy{maybe also covariate shift here?} 
% However, these robustness sets often need to be overly conservative to cover all possible shifts, resulting in large performance drawbacks "on average" \fy{maybe citation?}
%Considering all test distributions in a discrepancy ball can lead to overly 
However, enforcing good prediction performance in an entire neighborhood can be too conservative and lead to worse generalization on the actual test distribution \citep{hu2018does,frogner2019incorporating}. 
%\fy{clearer that prediction acc. suffers on test} 
%conservative predictions 
%Instead %realistic \fy{?} distribution shifts often exhibit strong structure or lie in a low-dimensional manifold \citep{belkin2003laplacian, block2022intrinsic}. 
%Alternatively, one can assume that some parts of the joint distribution of the data  do not change during test time (e.g., covariate shift \citep{shimodaira2000covariate, sugiyama2008direct} or label shift assumptions \citep{lipton2018detecting,garg2020unified}). Although covariate- and label shift based methods have been successfully applied in some scenarios, in many practical applications, their assumptions are violated \fy{need ref}. 
This paper considers an alternative scenario where partial information about future test distributions
%perturbations at test time 
is available at training time. 
In some settings, one might have access to the marginal test distribution of the covariates, reducing the problem to the domain adaptation setting \citep{pan2009survey, redko2020survey}. In other scenarios, one might have heterogeneous training data that share invariant properties that stay unchanged in all environments, including the test data. For example, suppose that we are conducting a long-term medical study, where data is collected from the same group of patients over the years to predict a health parameter $Y$ from a set $(X_1, ..., X_5)$ of covariates. During training time, we are given data from multiple past studies, and during test time, we want to predict $Y$ from newly collected data $(X_1,...,X_5) \sim \probtest$. Suppose that we have observed shifts of $X_1$ (e.g., age) across the training environments, and the distribution of the rest of the covariates has remained stable. In future data, that distribution might shift, however, since the data is collected in the same hospital, we believe that covariates $X_4, X_5$ will remain unshifted, and $X_2, X_3$ are particularly prone to distribution shift.
%For example, certain covariate distributions and dependencies of patients could be stable across different hospitals and regions due to invariant biological processes. \julia{here: real-world example 6 lines}
%Other times .... for different hospitals/regions some covariates stay unchanged while some may vary \fy{by some strength}

%Often some partial/structural information about the distribution shifts during test time is given. 
%This is the scenario we consider in this paper. 

Such prior structural information in the latter case can be particularly well-formalized through the framework of structural causal models (SCMs), via the approach of  \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}.
In this framework, the parameters  $\thetastar$ of the SCM (or parts of them) stay invariant, while distribution shifts can be modeled as (bounded or unbounded) shift interventions on the covariates. 
%where test shifts are induced by \emph{interventions} on the model, as typically studied in \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}. 
%\fy{merge} Some stuff invariant (thetastar) and  distribution shifts can be modeled as interventions on the covariates \citep{meinshausen2018causality, buhlmann2020invariance}. 
%In the end, both infinite and finite robustness methods aim to generalize to a set of distributions induced by the (invariant) causal data-generating process, characterized by some model parameters $\thetastar$, and a set of admissible (bounded or unbounded) test shifts. 
The goal is then to leverage (heterogeneous) training data to find a model that generalizes to a set of distributions induced by $\thetastar$ and a set of interventions. 
%the causal parameters  (invariant) causal data-generating process
This is informally captured by the
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
%This results in the 
following optimization problem:
\begin{equation}\label{eq:conventional-robust-risk}
   \betarob \coloneqq \argmin_{\beta} \sup_{\substack{\text{shift} \in \\ \text{shift set}}} \Loss (\beta; \prob^{\thetastar}_{\text{shift}}),
\end{equation}
where $\Loss(\beta; \prob^{\thetastar}_{\text{shift}})$ is the risk of an estimator $\beta$ on the distribution $\prob^{\thetastar}_{\text{shift}}$
%generated by the 
induced by the SCM with parameters $\thetastar$ and shifts from a given set of shifts/interventions.
%perturbed by a specific shift from the shift set. 
The minimizer of \cref{eq:conventional-robust-risk} ensures robustness with respect to the \emph{robustness set} $\{\prob^{\thetastar}_{\text{shift}}:\text{shift} \in  \text{shift set}\}$. However, in general this set is not fully known during training. %from training distribution. 
In prior works, the robustness set is computable 
% under the assumption
%by assuming 
% that 
if
the training distributions 
are heterogeneous enough
% contain enough information \fy{have enough heterogeneity} 
to identify the relevant parts of the causal model --
%for robust prediction 
this could involve finding a 
% which could either be a 
transformation of the covariates $X$ (also called representation) so that the conditional distribution of the labels given the transformed covariates is invariant \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant,arjovsky2020invariant, ahuja2020invariant,
shi2021gradient, 
xie2020risk, krueger2021out, ahuja2021invariance},
% \fy{cite also bottleneck bla}, 
or identifying the causal parameters themselves   \citep{angrist1996identification, hartford2017deep, singh2019kernel, bennett2019deep, muandet2020dual}. 
Most settings in both lines of work can achieve
% consider
generalization to test interventions of arbitrary strength 
% \citep{peters2016causal,arjovsky2020invariant} \fy{add IV citations}
% that 
and so we refer to them as \emph{infinite robustness methods}. 
Since it is more realistic to encounter bounded interventions in the real world, this approach would again be too conservative and pessimistic. This observation motivated \emph{finite robustness methods} \citep{rothenhausler2021anchor, jakobsen2022distributional, kook2022distributional, shen2023causality, christiansen2021causal} that trade off robustness strength against predictive power depending on the maximum expected strength of the test shifts. We refer to \cref{sec:related_work} for a more detailed discussion and comparison of these methods. 
% \fy{pointer to appendix for more detailed discussion}%, achieving \emph{finite robustness}. 
%invariance-based with infinite robustness methods \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, achiam2017constrained}) or by restricting the shift set to directions observed during training time (e.g., finite robustness methods \citep{rothenhausler2021anchor,shen2023causality, jakobsen2022distributional, kook2022distributional,christiansen2021causal}).

%In general, the robust predictor \prettyref{eq:conventional-robust-risk} can be only computed if the corresponding robustness set $\{\prob^{\thetastar}_{\text{shift}}:\text{shift} \in  \text{shift set}\}$ is identified from training data. However, if the model parameter $\thetastar$ of the data-generating process is unknown, computability of the robustness set from training data cannot be ensured. 


% A recent line of \emph{invariance-based methods} \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, arjovsky2020invariant} assumes the existence of an invertible mapping of the covariates which remains invariant across distribution shifts. In most of these methods, the invariances are formalized through the framework of structural causal models (SCMs) \citep{pearl2009causality}, where distribution shifts can be modeled as interventions on the covariates \citep{meinshausen2018causality, buhlmann2020invariance}. Given a sufficiently rich collection of shifted training environments, invariance-based methods can achieve \emph{infinite robustness}, i.e. identify an invariant prediction model which generalizes well to test interventions of arbitrary strength \citep{peters2016causal,arjovsky2020invariant}. In practice, considering shifts of arbitrary strength on all covariates can lead to overly conservative estimators. A related line of causality-oriented robustness methods \citep{rothenhausler2021anchor, jakobsen2022distributional, kook2022distributional, shen2023causality, christiansen2021causal} trades off robustness against predictive power depending on the maximum expected strength of the test shifts, achieving \emph{finite robustness}. 
%, and therefore, alternatives have been proposed in, e.g., the Group DRO literature \citep{sagawa2019distributionally, frogner2019incorporating, liu2022distributionally}. However, these methods cannot protect against perturbations larger than those seen during training time and do not provide a clear interpretation of the perturbation class.
% 
% \nico{This paper considers an alternative, more realistic scenario where partial information about the perturbations at test time is available at training time. }
%though the robustness set itself may not be fully given. 
% For example, for the same study conducted in different hospitals, one might anticipate/know which covariates will stay invariant and which ones will experience shifts.

% \julia{causality sentence}
% \nico{[mrw]: One way to model such prior information is to assume that the data are}
% % such prior information can be conveniently formalized if the data are assumed to be 
% generated by a structural causal model (SCM) \citep{pearl2009causality} and the test shifts are induced by \emph{interventions} on the model, as typically studied in \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}. 
% In this case, one can formulate the robustness problem and characterize the robustness set by leveraging the interventional structure of the training and test distributions, and the availability of multiple environments.
% % \julia{IRM sentence} 
% \nico{[mrw]:
% Depending on the strength and direction of interventions, we distinguish between \emph{infinite} and \emph{finite robustness} methods.
% }
% For example, t
% The line of work based on invariance \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, arjovsky2020invariant} uses multiple training environments to identify the underlying "stable representation" of the data which does not change across environments. If the data heterogeneity is sufficient to fully identify the invariant representation, such methods are robust against arbitrary shifts in the covariates, \nico{and achieve \emph{infinite robustness}.}
% \nico{[mrw]:
% In real data, however, shifts of arbitrary direction and strength in the covariates are unrealistic. Thus, a different line of work \citep{rothenhausler2021anchor, jakobsen2022distributional, christiansen2021causal, kook2022distributional, shen2023causality} trades
% off robustness against predictive power to achieve what is known as \emph{finite robustness}.
% The main idea of finite robustness methods is to learn a function that is as predictive as possible while protecting against shifts up to some strength in the directions observed during training time.
% }
% \julia{anchor sentence}
% Similarly, a branch of methods around instrumental variable and anchor regression \citep{rothenhausler2021anchor, jakobsen2022distributional, kook2022distributional, shen2023causality, christiansen2021causal} exploits heterogeneity of training data to find a predictor which is robust against training-time shifts of bounded (but potentially larger) strength.  

% making the robustness a function of the training environments, making it identifiable by construction .     
% Prior work has focused on the case when the robust predictor is identifiable, i.e. the robustness set is known , and optimal robustness can be achieved by directly minimizing the robust risk $\sup_{\substack{\text{shift} \in \\ \text{shift set}} }\Loss (\prob^{\theta}_{\text{shift}} , \betahat)$. Or in a dual view, for a given estimator, we can find the robustness set for which it minimizes this loss \fy{cite anchor bla}.
% In \emph{causality-oriented robustness}, such prior information can be  well-formalized via the causality assumption and the notion of interventions. 
% \fy{For example, for different hospitals/regions some covariates stay unchanged but some may}
%This is the scenario we consider in this paper \julia{caution, makes it sound like we perturb single covariates}
%one may still be able to identify the robust predictor.

% In this case, the robustness set may still be identifiable from data by leveraging
% structural assumptions on the training and test shifts and availability of multiple heterogeneous environments.
% %when heterogeneous data from similar environments is available \fy{e.g. data from different hospitals (if you want to continue with self-driving cars - different regions)}. 
% \fy{IRM sentence} For example, in the setting of invariance-based methods, identifying
%often, more than one data distribution is available during training. For instance, in the medical setting, data for a particular study may be collected from different hospitals, 
%multiple environments (e.g., the same study conducted in different hospitals.
%In this case, the robust predictor can often be identified by exploiting \emph{invariances} across training distributions to identify which  \fy{do we want to introduce the term "invariance" here? there is tradeoff since many of these terms of overloaded}
% components of the data that result in a stable prediction across different training environments \citep{muandet2013domain, arjovsky2020invariant} can lead to robust solution against all arbitrary shifts.
% %\fy{not so clear or hard to guess what stable prediction means here}. 
% \fy{IV/anchor sentence} Or in the presence of latent confounding, IV/anchor setting, given shifts during training time, one can be robust against shifts 
% In all prior work so far, the provable robustness of the methods heavily rely on identifiability of the robustness set from the training distribution - which is "the same" as assuming similar shifts during training and test.  In particular, it is possible using training data alone to minimize $\sup_{\prob \in \text{rob. set} }\Loss (\prob, \beta)$
% \begin{wraptable}{r}{.5\columnwidth}
% \resizebox{.5\columnwidth}{!}{%
% \setlength{\tabcolsep}{0.5pt}
% \begin{tabular}{@{}c|c|c|c@{}}
% \toprule
% Framework accounts for~ &
%   \begin{tabular}[c]{@{}c@{}}~bounded~~\\ shifts\end{tabular} & 
%   \begin{tabular}[c]{@{}c@{}}partial id. of\\  ~~causal param.~~\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}partial id. of\\  ~~robustness set\end{tabular} \\ 
%   \toprule
% \begin{tabular}[c]{@{}c@{}}DRO\\ \citep{ben2013robust, duchi2021learning, sinha2017certifying, mohajerin2018data, sagawa2019distributionally} \end{tabular} & \cmark & $-$  & \xmark  \\ \midrule
% \begin{tabular}[c]{@{}c@{}}Inf. robustness methods\\
% \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, arjovsky2020invariant}
% \end{tabular}
% & \xmark  & \xmark & \xmark \\ \midrule
% \begin{tabular}[c]
% {@{}c@{}}Finite robustness \\
% \citep{rothenhausler2021anchor, jakobsen2022distributional, christiansen2021causal, kook2022distributional, shen2023causality}
% \end{tabular} &\cmark  & \cmark  & \xmark  \\ \midrule
% \begin{tabular}[c]
% {@{}c@{}}\\
% \textbf{partially id. robustness}~~\\
% \phantom{~}
% \end{tabular} & \cmark & \cmark & \cmark \\ \bottomrule
% \end{tabular}%
% }
% \caption{Distributional robustness frameworks.}\label{tab:rw}
% \end{wraptable}
%However, in the real world
In practical applications, the training distributions may lack sufficient heterogeneity to ensure the identifiability of the robustness set, and thus, the robustness properties of the methods mentioned before would not be guaranteed.
% required by the aforementioned prior work under their respective assumptions.
%In practical applications, the heterogeneity of the training distributions might not be enough to ensure identifiability of the robustness set that all of the above prior work requires. \fy{not great sentence}
%test shifts are not guaranteed to be very similar to the training shifts, or other necessary assumptions do not hold, rendering the robustness set non-identifiable. 
%Equally, the assumptions necessary for the identification of the robustness set are not likely to hold exactly (for instance, if one only has access to few training environments).  -- this is kind of the same assumption as the one above?
Even minor violations of the identifiability assumptions can cause invariance- and causality-based methods to fail (Examples 4 and 5 in \cite{kamath2021does}, Theorem 3.1 in 
\cite{rosenfeld2020risks}, Theorems 3 and 4) and perform equally or worse than empirical risk minimization (ERM) \citep{ahuja2020empirical, gulrajani2020search, rosenfeld2022online}, which is oblivious to heterogeneity. 
Thus, the analysis of distributional robustness in the setting of \Cref{eq:conventional-robust-risk} is so far limited to either finding the robust solution under the assumptions that the robustness set is identified, or impossibility results when the robustness set is not known. \julia{Here: cite reviewer's impossibility results}


% %%%%%% this was ERM relation specific atttempt 2
% \fy{fanny attempt (content - to polish): (switch in order)} Empirically, invariance-based methods are often critiqued for not working well or better than ERM. 
% %of these methods when their motivating distributional assumptions do not hold, e.g. in the presence of latent confounding.
% \fy{shorten}
% In particular, such \emph{invariance-based} methods for the multi-environment setting are only proven to be beneficial (compared to ERM)
% %beneficial by proving robustness of the method output \fy{of certain causal parameters/latent variables} 
% for specific combinations of "underlying DAG" assumptions, the number of training environments and their "degree of variability" \citep{peters2016causal, arjovsky2020invariant} \fy{also anchor here}.
% %\fy{try 2:} On the constructive side, usually people started with assumptions on DAG or distribution heterogeneity and derived a method that recovers some causal parameters/latent variables. Alternatively, they derived robustness set for which some methods outputs the robust model \fy{discrete anchor}. 
% Failure is then explained by finding a set of similar assumptions/settings under which those methods fail to output these parameters/variables \fy{ICP ones}.
% %%%%%%%%%%%%%%%%%%%




%\fy{try 1:}
%In many real-world scenarios, however, these assumptions often do not hold and the theoretical analysis so far has been "content" with explaining failure by showing  assumptions/settings under which those methods do not identify these parameters/variables. Then they either 1) come up with a new method which can output the desired parameters or 2) derive robustness set/perturbation set for which the method 
%missing identifiability under some DAG assumptions (for icp style things) and "move on" or come up with another algorithm that can solve this assumption.

%\fy{well, they usually end when e.g. causal parameters can't be identified? which is even a stop beyond?}
%and at the same time the possible robustness set is bounded.  

%%%%%%%%%%%%%%%%
% \fy{previous ERM flow}
% However, \fy{changed:} such \emph{invariance-based} methods for the multi-environment setting are only provably beneficial when the number of training environments and their "degree of variability"  is large\citep{peters2016causal, arjovsky2020invariant}.
% When these assumptions are violated, invariance-based methods can fail to identify the robust predictor and
% %do not offer benefits compared to vanilla
% thus perform comparably to 
% empirical risk minimization (ERM) \citep{kamath2021does, rosenfeld2020risks}.\fy{these are icp ones - could be dnagerous?}
% This fact is also reflected in the empirical "belief" that ERM cannot be improved upon in generic settings \citep{gulrajani2021in,vedantam2021an}.
% \fy{the problem is that we still say that some sophisticated method could be better - i think one reason in practice is that they don't adversarially test (but just some random shift)} 
%%%%%%%%%%%%%%%%

%theoretical guarantees for identification of the robust predictor in a multi-environment setting often pose strict assumptions on the number of training environments and their "degree of variability" \citep{peters2016causal, arjovsky2020invariant}. 

%, and 
%In line with these observations, \fy{this is an empirical statement?} it has been argued that ERM might be 
%is the state-of-the-art domain generalization algorithm 
%\fy{can we add these to previous citation block?} 
%In most settings in prior work, even though some methods are proven to fail, others can still perfectly identify the robust solution from the training environments. For scenarios where this is not possible, the statement usually "stops at the impossibility". 

%So far in the literature, the focus has been on assumptions such that robustness set is identifiable. We want to also discuss robustness of any when this is not possible.
In this paper, we would like to extend the discussion beyond an
%we argue against the 
"all-or-nothing" view on robustness and instead suggest a framework to \emph{quantify} the algorithmic-independent limits of robustness for 
arbitrary 
%a more general 
combinations of training and test distributions in the SCM paradigm.
In particular, we aim to answer the following question more broadly:
%\begin{tcolorbox}[colframe=white!, top=2pt,left=2pt,right=2pt,bottom=2pt]
\begin{center}
\emph{Given a set of training distributions and expected shifts during test time, what is the optimal worst-case performance a model can have? How "optimal" are existing methods?}

 %   \emph{What is the optimal performance any model can have if it was trained on a given set of training distributions and deployed on the "worst-case" distribution in a set of distributions?}
\end{center}

%What's the best possible worst-case/robust performance of a model that is trained on multiple distributions
%can a model be on a set of possible test distributions that is trained on multiple observed 
%training distributions
%\end{tcolorbox}


% \fy{given above flow, this is now a bit redundant}
% \fy{maybe here we can place very informal juxtaposition of robust risk vs. PI robust risk, even more informally}
% Prior work has focused on the case when the robust predictor is identifiable, i.e. the robustness set is known, and optimal robustness can be achieved by directly minimizing the robust risk $\sup_{\substack{\text{shift} \in \\ \text{shift set}} }\Loss (\prob^{\theta}_{\text{shift}} , \betahat)$. Or in a dual view, for a given estimator, we can find the robustness set for which it minimizes this loss \fy{cite anchor bla}.
% In practice, the robustness set is often not explicitly known because some ground truth parameters describing the model are not uniquely identified merely by observing the training distributions, typically due to unobserved variables confounding certain covariates and outcomes \fy{examples?}. 
To the best of our knowledge, this is the first work
%we are the first 
to propose a framework that allows a discussion of distributional robustness under arbitrary shifts when the robustness set
% may only be
is only partially identifiable. We contrast our framework compared to other analytic frameworks on distributional robustness in~\Cref{tab:rw}. 

\begin{table}[t]
    \centering
    \caption{Comparison of various distributional robustness frameworks and what kind of assumptions their analysis can account for (with an incomplete list of examples for each framework). % on identifiability.
    }\label{tab:rw}
\resizebox{.8\columnwidth}{!}{%
% \setlength{\tabcolsep}{0.5pt}
\begin{tabular}{@{}cccc@{}}
\toprule
Framework accounts for~ &
  \begin{tabular}[c]{@{}c@{}}~bounded~~\\ shifts\end{tabular} & 
  \begin{tabular}[c]{@{}c@{}}partial identifiability of\\  ~~causal parameters ~~\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}partial identifiability of\\  ~~robustness set\end{tabular} \\ 
  \toprule
\begin{tabular}[c]{@{}c@{}}DRO\\ \citep{bental2013robust, duchi2021learning, sinha2017certifying, mohajerin2018data, sagawa2019distributionally} \end{tabular} & \cmark & $-$  & \xmark  \\ \midrule
\begin{tabular}[c]{@{}c@{}}Infinite robustness methods\\
\citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant,arjovsky2020invariant, ahuja2020invariant,
shi2021gradient, 
xie2020risk, krueger2021out, ahuja2021invariance}\\

\end{tabular}
& \xmark  & \xmark & \xmark \\ \midrule
\begin{tabular}[c]
{@{}c@{}}Finite robustness \\
\citep{rothenhausler2021anchor, jakobsen2022distributional, christiansen2021causal, kook2022distributional, shen2023causality}
\end{tabular} &\cmark  & \cmark  & \xmark  \\ \midrule
\begin{tabular}[c]
{@{}c@{}}\\
\textbf{partially id. robustness}~~\\
\phantom{~}
\end{tabular} & \cmark & \cmark & \cmark \\ \bottomrule
\end{tabular}%
}
\end{table}

%For linear structural causal models, we first derive information-theoretic population lower bounds that provide a fine-grained characterization of robustness limitations for bounded additive test shifts and then provide a general method, called the \emph{identifiable robust prediction model}, that can achieve this lower bound.

\paragraph{Our contributions}
In \Cref{sec:prop-invariant-set}, we introduce our framework of partially identified robustness in the context of linear structural causal models with latent confounding and additive distribution shifts. In particular, for any estimator $\beta$, we introduce a new risk measure called the \emph{identifiable robust risk}\footnote{Note that partial identifiability is commonly studied in the context of set-identifying the causal parameter \citep{tamer2010partial} (see \cref{sec:related_work} for a further discussion). Here, additionally, we use the term in the context of identifiability of the robustness set.} that is informally defined as
\begin{align*}
    \Lossrobpi(\beta; \text{shift bound} \fy{bound or set?}) = \sup_{\substack{\text{possible}\\ \text{true model $\theta$ }}} \sup_{\substack{\text{shift} \in \\ \text{shift set}} }\Loss (\beta; \prob^{\theta}_{\text{shift}} ).
\end{align*}
The minimal value of this quantity over all estimators $\beta$ then characterizes the hardness of the problem in the given partially identifiable setting. For example, it can offer guidance to the practitioner, such as whether to collect more data (connecting the result to active causal learning) or use a good algorithm. Since $\Lossrobpi$ is identifiable from data, one can also derive an approximate finite-sample estimator for better robustness in the partially identified setting.
%In line with the majority of works in this area, 
%\julia{ all the sections changed, redo}
%In \Cref{sec:PI-lower-bound}, we then apply the framework on linear structural causal models with latent confounding and additive distribution shifts. 
%The set of training and test distributions may differ in additive interventions on the covariates, while the causal mechanism and confounding stay invariant. 
In~\Cref{sec:identifiability-linear-SCM},  we discuss conditions for risk identifiability in our linear SCM setting and derive a lower bound 
%In a setup similar to anchor regression, we derive a lower bound 
for the identifiable robust risk. This lower bound is tight and achievable for some regimes of shift strength. 
In \cref{sec:comp-with-finite-robustness-methods},
we finally
evaluate the identifiable robust risk of prior robustness algorithms and show theoretically and empirically that the minimizer of the identifiable robust risk outperforms existing methods in partially identified settings.
% \nico{cannot understand last sentence}
% \fy{sounds a little negative - could also write sth like "outperform"}

% that shows how existing methods can be far from optimal and close to ERM in robust performance in the partially identifiable setting. 
% Finally, in \Cref{sec:optimal-estimator}, based on 
% results in \Cref{sec:PI-lower-bound}, 
% we propose a new estimator that is optimal for bounded distribution shifts under partial identifiability.
%%%%%%%%%%%%%%%%%%% fanny try 1 %%%%%%%%%
% \fy{this somehow didn't fit in flow -maybe fits better to related work and when we actually introduce stuff} The case of unbounded interventions is studied in causality/IV literature, and unless the estimator is equal to the causal parameter, this risk can be infinity. Instead, in our fine-grained analysis, we focus on bounded shifts during test time for the worst-case risk to be well-defined and meaningful. 
% %In case the model may not be fully identified but no "unseen" test shifts are observed, the PI-robust risk reduces to the conventional robust risk and there exists an estimator that can actually reach zero robust risk (in population). We say that in this setting, the robust predictor is identifiable. Such settings have been studied before, like in anchor regression \cite{rothenhausler2021anchor} and distributionally robust gradients (DRIG) \cite{shen2023causality}.
% In contrast to previous works, (to the best of our knowledge,) we are the first to study distributional robustness under arbitrary bounded additive shifts 
% when the robust predictor may only be partially identifiable.  We first derive information-theoretic lower bounds that provide a fine-grained characterization of robustness limitations for different combinations of training environments and robustness sets during test time. For this purpose, we introduce a new minimax risk measure called the \emph{partially identified (PI) robust risk} ...

% ion to considering a broad class of distribution shifts, we do not require full identifiability of the model or a minimum number of training environments. Instead, we quantify the best possible robustness achievable for a given structure of training and test environments, even when few training environments are present and the causal structural model is not identifiable. To the best of our knowledge, we are the first to study distributional robustness under arbitrary bounded additive shifts in a partially identifiable setting. 
% In \prettyref{sec:prop-invariant-set}, we explicitly describe non-identifiability of the data-generating model in a multi-environment setting and connect it to the notion of \emph{observational equivalence} from the partial identifiability literature [cite]. As a consequence, we show that the robust predictor with respect to a specified robustness set can be computed from training data if and only if the robustness set does not contain "unseen" directions on which the model has not been identified \fy{that does not sound like the major contribution}. In \prettyref{sec:PI-lower-bound}, we introduce a new risk measure, which we call partially identified (PI)-robust loss. This minimax notion quantifies the worst-case robust risk under partial identifiability of the data-generating model. We derive a lower bound on this measure and show that no infinite robustness is possible for distribution shifts in \emph{any} direction not covered by the training data. In 
% \prettyref{sec:optimal-estimator}, we discuss the PI-robust loss and propose a new estimator which is optimal with respect to a bounded set distribution shifts under partial identifiability. Finally, in \prettyref{sec:experiments}, we conduct synthetic and semi-synthetic experiments that confirm our findings. 
%%%%%%%%%%%%%


%%%%%%%%%%%% example %%%%%%%%%%%
% \fy{hm my hunch for ml paper is that the contributions should come basically here}
% To illustrate our question, we use the following toy example: 
% \begin{example}\label{ex:multiple-studies}
%     Suppose that we are conducting a long-term medical study, where data is collected from the same group of patients over the years to predict a health parameter $Y$, e.g. the life expectancy \fy{but do you measure this label?}. We are given data $\{(X^e, Y^e)\}_{e \in \Ecaltrain}$ from multiple past studies, where $\Ecaltrain = \{2010, 2015, 2020 \}$. We assume that the data $(X, Y)$ are generated by an underlying causal model, which is unobserved \fy{here enough to say latent confounding? causality/causal lingo suddenly pops up without a warning ;)}. By observing multiple studies, we are able to partially identify the causal mechanism. We now want to train a model which generalizes best on the future study $(X^{2025}, Y^{2025})$. We expect this study to have a distribution shift compared to past studies, including both previously observed shifts (e.g. for the age variable, which shifts by 5 years with every study and retains the same distribution otherwise), and new, previously unobserved shifts (e.g. changes in blood parameters caused by the Covid-19 pandemic). One could discard all covariates with unpredictable shifts \fy{if its unpredictable how do you know which ones these are?}, however, this would severely diminish the predictive power of the model. The practitioner is left with two questions: 1) Do I have enough data to reliably generalize to the new study? 2) What is the optimal model to train on existing data which still has predictive power, but does not fail too severely even on unobserved test shifts? 
%     \fy{as we are not epxerts in clinical study this can easily look unrealistic and need to be very carefully constructed. at least it didn't read very convincingly to me (both the unpredictable shift and measuring/predicting which Y part)}
% \end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%

% \fy{this seems too much detail at this point}
% In the following sections, we aim to provide a fine-grained analysis of robustness limitations in settings like \prettyref{ex:multiple-studies}. We consider data generated by a linear structural causal model, in which the response variable $Y$ is caused by covariates $X$. However, further correlations can be present through unobserved variables $H$, rendering the true causal relationship unidentifiable. As an input, we consider multiple training distributions $\{(X^e, Y^e)\}_{e \in \Ecaltrain}$, which differ by (bounded) additive shifts. Given this data, we are want to be robust to test distribution shifts of a specified (but arbirtary) strength and direction. It is known that \emph{infinite robustness} in arbitrary shift directions is generally impossible unless one can identify the true causal parameter, which, however, requires as many (sufficiently different) environments as there are covariates \cite{peters2016causal}. We find that existing \emph{finite robustness} methods only achieve robustness in directions on which the causal predictor is identified, leaving open the question what happens if the test shift occurs in a previously unobserved direction. To answer this question, we introduce a new minimax risk measure for an estimator $\betahat$, called the \emph{partially identified (PI) robust risk}:
% \begin{align*}
%     \Lossrobpi(\text{shift bound}, \betahat) = \sup_{\substack{\text{ground-truth}\\ \text{model $\theta$ }}} \sup_{\substack{\text{shift} \leq \\ \text{shift bound}} }\Loss (\prob^{\theta}_{\text{shift}} , \betahat).
% \end{align*}
% In case no "unseen" test shifts are observed or the model is fully identified, the PI-robust risk coincides with the conventional robust risk. Thus, our framework includes settings of anchor regression \cite{rothenhausler2021anchor} and distributionally robust gradients (DRIG) \cite{shen2023causality}. If these identifiability assumptions are not fullfilled, we find that the minimizer of the PI-robust risk, called the \emph{PI-robust estimator}, achieves better performance on unseen test shifts in partially identified settings than the existing finite robustness methods, which in turn behave similarly to ERM. This might potentially explain some of the previous findings of ERM outperforming domain generalization methods. 

% Depending on the assumptions on the test shift, our results could be interpreted in both domain generalization and domain adaptation settings. Moreover, in our derivation of the lower bound, we identify the most adversarial test shift directions under non-identifiability. This connects our findings to the field of \emph{active learning}: by collecting data corresponding to the most adversarial test shift, one maximally reduces the PI-robust risk and thus the non-identifiability of the robust predictor. 

% \fy{note to self: connection to abstention as per alex's question - to some extent we are doing abstention on the nonidentified directions to some extent (unless the shifts are unbounded where we want to utilize the spurious stuff)}
% We find that ex
% \begin{itemize}
%     \item In the following sections, we [our contribution]
%     \item In causality-oriented robustness methods, a method is frequently developed and then a robustness set for this method is derived.
%     \item We show by following this recipe, one only looks at cases where the robust loss is identifiable.
%     \item we will show that as soon as new directions appear, the methods fail.
%     \item We introduce the minimax quantity and the lower bound [informal]
%     \item we demonstrate how common methods are suboptimal w.r.t. lower bound theoretically and empirically
%     \item we empirically show that the minimizer of the PI-robust loss shows significantly improved robustness compared to methods that don't account for partial identifiability. 
%     \item Our results both useful for DG and DA
%     \item Our results could potentially be applied to active learning settings. 
% \end{itemize}
% In the following sections, we [...]. We show that for 








% If multiple varying data distributions are observed during training (e.g., studies from different hospitals), it is sometimes possible to exploit the heterogeneity to identify which components of the data result in a stable prediction across different environments. \cite{}

% In the recent years, a number of works have studied the setting of \emph{multi-environment}, or \emph{heterogeneous training data}, to identify components of the data, prediction on which remains stable across different distributions. 
% Computing the robust predictor is a challenging task and implicitly requires identifying components of the data, prediction on which remains stable across different distributions. 


% \begin{itemize}
%     \item X In safety-related fields, need for robust predictors 
%     \item X Current attempts: obtain these robust predictors from multi-environment training data (exploit heterogeneity)
%     \item However, in general not enough information in training data to compute the robust predictor. 
%     \item In the past, this has been seen in a binary way: "if not enough information, everything fails, if enough training information --> method to compute robust predictor"
%     \item In this paper, we challenge this binary view and instead aim to answer the question 
%     \item Fundamental question: what is the best possible robustness
%     \item Illustrative example
%     \item 
% \end{itemize}
% In the past years, various types structural assumptions have been explored, such as covariate shift [CITE], label shift [CITE] or similarity of distributions with respect to a probability discrepancy measure.




% \julia{include a simple, illustrative example along the lines of the following}:
% \begin{example}
%     Suppose that we are conducting a long-term medical study, where the same blood parameters are collected from the same group of patients over the years to predict a health parameter $Y$. We train our model on a number of past studies $\{S_{2005}, S_{2010}, S_{2015}, S_{2020}\}$. What determines how well this model can predict the results of the future study $S_{2025}$? Assuming a causal data-generating process, we can extract, for instance, how the variable $X_{age}$ affects the prediction, since in every new study, its mean shifts by 5 years, and the variance remains invariant. The same shift is expected for $S_{2025}$, and thus, we can reliably use  $X_{age}$ for prediction. However, some other covariates, previously stable, might shift distributions, for instance due to the Covid-19 pandemic. How can we decide whether to use those covariates for prediction, and if so, to which extent? 
% \end{example}