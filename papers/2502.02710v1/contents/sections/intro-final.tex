% \nico{I write [mrw] to flash where I moved or merged related work into main text}

The success of machine learning methods typically relies on the assumption that the training and test data follow the same distribution. However, this assumption is often violated in practice. For instance, this can happen if the test data are collected at a different point in time or space, or using a different measuring device.
%- . This setting is generally known as \emph{distribution shift} \cite{quinonero2022dataset}. \fy{this is a century old problem - if we cite, need to cite carefully - here} 
%The task of creating models which perform well under distribution shifts, known as \emph{domain generalization}, remains one of the biggest challenges in modern machine learning. 
Without further assumptions on the test distribution, 
generalization under distribution shift is impossible.
%the domain generalization problem is, \fy{do we need to use this jargon here? } in general, ill-posed. 
However, practitioners often have partial information about the 
set of possible "shifts" 
% \julia{what are shifts outside of causality?} 
that may occur during test time, inducing a set of \emph{feasible test distributions} that the model should generalize to. We refer to the resulting set as the \emph{robustness set}. 
%strength or structure of possible distribution shifts during test time (e.g., the covariate shift assumption \cite{shimodaira2000improving} or bounded $f$-divergence \citep{ben2010theory, hu2018does}) \fy{here, I expected practical shifts rather than theoretical, probably would hold back with this in first paragraph}, which gives rise to a set of feasible test distributions called the \emph{robustness set} \fy{called by whom?}. 
When a probabilistic model for these possible test distributions is available or estimable, one may aim 
%This goal differs from the common objective of \emph{domain generalization}, which aims 
for good performance on a "typical" held-out distribution using a probabilistic framework. %\fy{add citation?}
When no extra information is given, one possibility is to find a model $\beta$ that has a small risk $\Loss(\beta;\prob)$ on the \emph{hardest} feasible test distribution. More formally, we aim to achieve a small
%With $\Loss(\beta;\prob)$ denoting the population risk of a model $\beta$ for a data distribution $\prob$, 
%the 
robust risk defined by
%can be written as 
\begin{equation}\label{eq:conventional-robust-risk}
   \Lossrob(\beta) \coloneqq \sup_{\prob \in \robset(\thetastar)} \Loss (\beta; \prob),
\end{equation}
where $\robset(\thetastar)$ corresponds to the robustness set which depends on 
%is partially characterized by \fy{dependent on}
%that we assume to be fully characterized by
some true parameter $\thetastar$.
% all possible test sets, and we assume that the robustness set is fully characterized by some true parameter $\thetastar$.
% In security applications this robustness set may, e.g., correspond to a maximal set for a malicious attack on $\Atest$ to remain unnoticed or require expert intervention. \fy{alternative sentence in comments}
In fact, this worst-case robustness aligns with security and safety-critical applications, where a small robust risk is necessary to confidently guard against possible malicious attacks.
%the goal is often to find a model %minimizer 
%that has good performance
%of the robust risk, i.e. a \emph{robust prediction model} that shows the best performance 
%on the \emph{worst-case} distribution out of the robustness set. 
% \fy{maybe we can cut this next sentence for space}
% Note that as robustness prioritizes 
% %safety, ensuring good performance on 
% worst-case distributions which might almost never occur in practice, evaluation often requires evaluation on partially synthetic adversarial benchmarks. 
% \fy{somewhere we should stress the following:} Note that this worst-case robustness notion asks for a different methodology and evaluation than domain generalization - \fy{in a nutshell, domain generalization would test on a random "real" held-out environment, whereas robustness prioritizes safety in environments that specifically do not occur naturally and hence requires artificially adversarially perturbed datasets}


%This estimator is commonly referred to as the \emph{robust predictor}. 
\par
% \fy{content-wise suggestion:}

To find a robust prediction model that minimizes \eqref{eq:conventional-robust-risk}, existing lines of work in distributional robustness assume a known robustness set, i.e., full knowledge of the robust risk objective. They then focus on how to minimize the resulting objective. For instance, in distributionally robust optimization \citep{bental2013robust, duchi2021learning}, or relatedly, adversarial robustness  \citep{goodfellow2014explaining, madry2018towards}, the robustness set is chosen to be some neighborhood (w.r.t. to a distributional distance notion) of the training distribution $\prob$. When multiple training distributions are available, related works aim to achieve robustness against the set of all convex combinations of the training distributions \citep{mansour2008, meinshausen2014, sagawa2019distributionally}.  
%\fy{we should add Mansour's mixture distribution here - they don't belong to either} 
%and $\thetastar$ corresponds to $\prob$ itself. 
In causality-oriented robustness  (see, e.g. \citep{buhlmann2020invariance,meinshausen2018causality,shen2023causalityoriented}) on the other hand, the robustness set is not explicitly given, but implicitly defined. Specifically, it is assumed that certain structural parameters (e.g. specific parts of the structural causal model) %of the model that 
remain invariant across distributions, whereas other parameters shift. 
%some structural parameters (like a graphical structure of the model) remain invariant across distributions, while other distributional parameters may vary. 
The robustness set may or may be not be fully known during training time, depending on the relationship of the varying parameters during training and shift time. 
%if the data , 
%For a given set of training distributions, there exist certain sets of varying parameters (``test shifts'') which render the robust risk identifiable. Similarly, for a given set of test shifts, a heterogeneous enough set of training distributions may identify the robust risk. 

%may then be identified and minimized if enough heterogeneous training environments are available. 

%(either to identify the invariant parameter or when test shifts are similar to training shifts)
%In practice, 
If the robust risk objective is known, methods can be derived to estimate its minimizer. However, in many scenarios such procedures suffer from ineffectiveness.
%compared to vanilla methods such as empirical risk minimization. 
For adversarial robustness for example, it is known that when the perturbations during training and test time differ, the robustness resulting from adversarial and standard training is comparable %\fy{could easily find more from later papers} 
(see, e.g. \cite{Tramer19,kang19}). Similarly, invariance-based methods such as \citep{peters2016causal,rojas2018invariant,arjovsky2020invariant,krueger2021out} often exhibit no advantage over vanilla empirical risk minimization \citep{ahuja2020empirical,ahuja2020invariant}.
In both cases, one of the main failure reasons is that the robust objective, which the final model is evaluated on, is not known during training.  For instance, invariance-based methods often fail on new environments if the true invariant predictor is not identifiable (e.g. \citep{kamath2021does,rosenfeld2020risks}). As a possible solution, in some recent works \citep{rothenhausler2021anchor,shen2023causalityoriented}, the set of feasible test distributions is chosen in a specific way which renders the robust risk objective computable despite the non-identifiability of the invariant parameters. 
%One of the reasons for the failure of invariance-based methods is the non-identifiability of the true invariant predictor (e.g. \citep{kamath2021does,rosenfeld2020risks}).  
%Empirically, this is observed when evaluated on real unseen environments, \citep{ahuja2020empirical,gulrajani2021in},
%Besides being effective only for very specific data-generating models \cite{ahuja2021invariance}, invariance-based methods generally are bound to fail when the heterogeneity of the training data is not enough for a given set of possible test shifts \citep{kamath2021does,rosenfeld2020risks}.
%While prior work has pointed out such non-identifiable scenarios as failure cases, 
%there have been no efforts to quantify algorithm-independent limits. Even though this issue of non-identifiability has been pointed out previously,
%\fy{currently this reads like we mean identifiable in terms of invariant predictor? instead of identifiability of the robust risk?}
In total, the theoretical analysis in prior work remains rather of ``binary'' nature: it either analyzes the fully identifiable case in which invariance-based methods are successful, or simply discards non-identifiable scenarios as failure cases without further quantification of the limits of robustness.
%that have not been previously quantified. 
%prior work so far was primarily satisfied with such a binary statement  - whether identifiability is given or not. \fy{failure when it's not identifiable and else success}
%\fy{well, at least gulrajani seems to say that when you take an average test shift ERM is not worse, does anybody actually evaluate on worst-case perturbed sets and claim it?}. 
%Many different reasons could underlie the latter observation, such as model misspecification or optimization issues [cite] \fy{rosenfeld2022online} \fy{what have people hypothesized so far?, in this sentence i would put the ones that are theoretical/provide analysis}. 
%One possible reason that has not yet been considered is the case when the robust risk simply cannot be identified. In this work we show that in that case, invariance-based methods are not effective. We also formalize how to quantify the best possible robustness for this partially identifiable setting
%In practice, causality- and invariance-based methods often result in wrong representations of the data  \citep{kamath2021does,rosenfeld2020risks} and end up
%Empirically, %it is observed that these methods 
%performing similarly to empirical risk minimization (ERM) that ignores the multi-environment information
%\citep{ahuja2020empirical,gulrajani2021in,rosenfeld2022online}. Many possible explanations for this observation have been proposed in the literature.
%\fy{still need to merge} In our work, we focus on examining/quantifying the impact of \emph{non-identifiability} failure scenario.
%In this work we show that in this case, invariance-based methods are not effective. 
%In particular, we aim to formalize how to quantify the best possible robustness for this partially identifiable setting.
% the scenario where the training data is not heterogeneous enough.
%We argue that the non-identifiable scenario warrants a more detailed discussion. 
%In particular, we extend the discussion of invariance-based methods to include the partially identifiable setting, where not only the causal parameter, but the robust risk \eqref{eq:conventional-robust-risk} is not determinable using training data either. 
In this paper, we aim to include the partially identifiable setting\footnote{Here, we mean partial identifiability of the robust risk, which is reminiscent of outputting uncertainty sets for a quantity of interest in the field of partial identification \cite{tamer2010partial, frake2023perfect}.} in our analysis and more specifically discuss the following question:

% A number of subfields in machine learning and optimization have addressed this problem. For example, in distributionally robust optimization (DRO) \citep{bental2013robust, duchi2021learning}, the parameter $\thetastar$ may be the training distribution $\prob$ and the robustness set the \emph{neighborhood} of $\prob$ in some probability distance metric \citep{kuhn2019wasserstein, mohajerin2018data, gao2022wasserstein, duchi2016variance}. Relatedly, adversarial robustness \citep{goodfellow2014explaining, madry2018towards} studies the risk on worst-case transformations of examples drawn from some distribution $\prob$ and can be seen as equivalent to distribution shift robustness \cite{sinha2017certifying}.
% DRO-type methods minimize the worst-case robustness against arbitrary distribution shifts in the neighborhood without structural assumptions. Although being assumption-agnostic can be viewed as a strength, it also has its caveat: even when available, prior knowledge about the structure of expected test shifts cannot be incorporated.
% %Both a caveat and strength of these methods is that they do not require nor allow incorporating structural assumptions on the distribution shifts, even when some prior knowledge about expected test shifts may be available. 
% In such cases, the robust model's prediction might be overly conservative, resulting in suboptimal performance when the test shifts are in fact more benign. \cite{sagawa2019distributionally}.
% %methods like group DRO \cite{sagawa2019distributionally}, in general, just enlarge their robustness set as the number of training environments increases. \fy{not so sure what you mean here by enlarge? aren't there a lot that talk about convex hull of training in different contexts?}

% In many practical scenarios, data from \emph{heterogeneous sources} is available at training time â€“  for example, data from different geographic locations or time ranges. 
% Due to the lack of modeling assumptions, multiple environments in the DRO setting cannot, in general, be leveraged to achieve better robustness in a given robustness set -- in those contexts, the 
% presence of multiple environments is usually argued to enable 
% robustness against a larger robustness set. Instead, domain experts can anticipate which aspects of the joint probability distribution of $(X,Y)$ are more likely to shift.
% %, thus allowing for incorporating structural assumptions. 
% Such prior structural information can, for example, be incorporated 
% %particularly well-formalized 
% through the framework of structural causal models (SCMs), via the approach of  \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}. Importantly, the literature in this area has so far focused on settings when the desired robust objective $\Lossrob$ is identifiable, i.e. computable from training data. Traditional causal learning and invariance-based methods aim to fully identify some underlying causal parameter of the SCM for robustness against \emph{all} (potentially infinite) interventions \citep{peters2016causal,rojas2018invariant,arjovsky2020invariant,krueger2021out}. However, the training data is often not heterogeneous enough to fully identify the causal parameter. %At the same time, the interventions during test time may neither happen in all \fy{directions} nor have infinite strength. 
% Thus, another line of work \citep{rothenhausler2021anchor,shi2022nonlinear,kook2022distributional} focuses on the scenario when
% %In such cases, it might not be necessary to identify the full causal parameter, but instead only 
% the causal parameter is not necessarily identifiable, but the test shifts only occur in training directions, rendering the robust risk \eqref{eq:conventional-robust-risk} identifiable. We provide an overview in \Cref{tab:rw} and an additional discussion of related work in \Cref{sec:apx-related_work}.
%previous works on robust generalization under heterogeneous training data sources all
%develop and evaluate algorithms for settings where 
% \fy{cite impossibility paper} \julia{don't get which impossibility paper} 



%It has been widely observed that when the training environments do not
%, in some sense, express 
%have sufficient variability or the setting is slightly misspecified, causality- and invariance-based methods can pick wrong representations of the data \citep{kamath2021does,rosenfeld2020risks}, and their robust performance degrades significantly, often even becoming worse than ERM \citep{ahuja2020empirical,gulrajani2021in,rosenfeld2022online}.  

% For most  causality- or invariance-based algorithms, there are also numerous papers pointing out that this \fy{maybe need to divide reference into causal parameter identification and r.r.id.?} identifiability breaks under slightly modified settings \fy{and/in} that they fail to improve on empirical risk minimization 
%(see \cite{kamath2021does}, \cite{rosenfeld2020risks}, Theorems 3 and 4 \fy{what is this referring to?}) and perform equally or worse than empirical risk minimization (ERM) 
% \citep{ahuja2020empirical, gulrajani2020search, kamath2021does, rosenfeld2020risks, rosenfeld2022online}.
% \fy{i compiled two lists of references together that were separate, check}
%Even minor violations of the identifiability assumptions can cause invariance- and causality-based methods to fail (Examples 4 and 5 in \cite{kamath2021does}, Theorem 3.1 in \cite{rosenfeld2020risks}, Theorems 3 and 4) and perform equally or worse than empirical risk minimization (ERM) \citep{ahuja2020empirical, gulrajani2020search, rosenfeld2022online}, which is oblivious to heterogeneity. 



%\fy{are there actually works that prove in some case there exist no algorithm that can identify? or they just show one particular method can't identify?}
%either finding the robust solution under the assumptions that the robustness set is identifiable from training data, or impossibility results when this is not the case. We summarize our classification of causality- and invariance-based robustness methods in \Cref{tab:rw}. 
% \fy{see~\Cref{tab:rw}- how important is bounded shifts actually?}

% \begin{table}[tbp]
%     \centering
%     \caption{Comparison of various distributional robustness frameworks and what kind of assumptions their analysis can account for (with an incomplete list of examples for each framework). % on identifiability.
%     }\label{tab:rw}
%     \vspace{1pt}
% \resizebox{.8\columnwidth}{!}{%
% % \setlength{\tabcolsep}{0.5pt}
% \begin{tabular}{@{}cccc@{}}
% \toprule
% Framework accounts for~ &
%   \begin{tabular}[c]{@{}c@{}}~bounded~~\\ shifts\end{tabular} & 
%   \begin{tabular}[c]{@{}c@{}}partial identifiability of\\  ~~model parameters ~~\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}partial identifiability of\\  ~~robustness set\end{tabular} \\ 
%   \toprule
% \begin{tabular}[c]{@{}c@{}}DRO\\ \citep{bental2013robust, duchi2021learning, sinha2017certifying, mohajerin2018data, sagawa2019distributionally} \end{tabular} & \cmark & $-$  & \xmark  \\ \midrule
% \begin{tabular}[c]{@{}c@{}}Infinite robustness \\
% \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant,arjovsky2020invariant, ahuja2020invariant,
% shi2021gradient, 
% xie2020risk, krueger2021out, ahuja2021invariance}\\

% \end{tabular}
% & \xmark  & \xmark & \xmark \\ \midrule
% \begin{tabular}[c]
% {@{}c@{}}Finite robustness \\
% \citep{rothenhausler2021anchor, jakobsen2022distributional, christiansen2021causal, kook2022distributional, shen2023causalityoriented}
% \end{tabular} &\cmark  & \cmark  & \xmark  \\ \midrule
% \begin{tabular}[c]
% {@{}c@{}}
% Partially id. robustness\\
% (this work)~~
% \phantom{~}
% \end{tabular} & \cmark & \cmark & \cmark \\ \bottomrule
% \end{tabular}}
% % \vspace{-15pt}
% \end{table} 
\begin{comment}


%\fy{bounded interventions does not have to do with identifying causal parameter?} 
Now a paragraph on fully identifiable causal -> fully identifiable robust risk
%how heterogeneity could be leveraged to find invariant mechanisms in all distributions, ultimately leading to predictive models with better robustness. 

When no prior information is available to describe the set of expected %"realistic" 
distribution shifts, a natural approach is to be robust in 
%When the robustness set is explicitly known \fy{such 
a neighborhood of the training distribution, % \fy{plural here?},
%with respect to some discrepancy measure, 
typically referred to as distributionally robust optimization (DRO), e.g., \citep{bental2013robust, duchi2021learning, sinha2017certifying, mohajerin2018data}. 
% \julia{add more work on DRO}
%DRO methods based on Wasserstein distance \citep{sinha2017certifying, mohajerin2018data} allow for test distributions outside of the training support, unlike the ones based on $f$-divergences \citep{bental2013robust, duchi2021learning}. Group DRO \citep{sagawa2019distributionally} 
%This is sth for which one can compute "optimally" robust estimator explicitly e.g. DRO \fy{maybe also covariate shift here?} 
% However, these robustness sets often need to be overly conservative to cover all possible shifts, resulting in large performance drawbacks "on average" \fy{maybe citation?}
%Considering all test distributions in a discrepancy ball can lead to overly 
However, enforcing good prediction performance in an entire neighborhood can be too conservative and lead to worse generalization on the actual test distribution \citep{hu2018does,frogner2019incorporating}. 
%\fy{clearer that prediction acc. suffers on test} 
%conservative predictions 
%Instead %realistic \fy{?} distribution shifts often exhibit strong structure or lie in a low-dimensional manifold \citep{belkin2003laplacian, block2022intrinsic}. 
%Alternatively, one can assume that some parts of the joint distribution of the data  do not change during test time (e.g., covariate shift \citep{shimodaira2000covariate, sugiyama2008direct} or label shift assumptions \citep{lipton2018detecting,garg2020unified}). Although covariate- and label shift based methods have been successfully applied in some scenarios, in many practical applications, their assumptions are violated \fy{need ref}. 
This paper considers an alternative scenario where partial information about future test distributions
%perturbations at test time 
is available at training time. 
In some settings, one might have access to the marginal test distribution of the covariates, reducing the problem to the domain adaptation setting \citep{pan2009survey, redko2020survey}. In other scenarios, one might have heterogeneous training data that share invariant properties that stay unchanged in all environments, including the test data. For example, suppose that we are conducting a long-term medical study, where data is collected from the same group of patients over the years to predict a health parameter $Y$ from a set $(X_1, ..., X_5)$ of covariates. During training time, we are given data from multiple past studies, and during test time, we want to predict $Y$ from newly collected data $(X_1,...,X_5) \sim \probtest$. Suppose that we have observed shifts of $X_1$ (e.g., age) across the training environments, and the distribution of the rest of the covariates has remained stable. In future data, that distribution might shift, however, since the data is collected in the same hospital, we believe that covariates $X_4, X_5$ will remain unshifted, and $X_2, X_3$ are particularly prone to distribution shift.
%For example, certain covariate distributions and dependencies of patients could be stable across different hospitals and regions due to invariant biological processes. \julia{here: real-world example 6 lines}
%Other times .... for different hospitals/regions some covariates stay unchanged while some may vary \fy{by some strength}

%Often some partial/structural information about the distribution shifts during test time is given. 
%This is the scenario we consider in this paper. 

Such prior structural information in the latter case can be particularly well-formalized through the framework of structural causal models (SCMs), via the approach of  \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}.
In this framework, the parameters  $\thetastar$ of the SCM (or parts of them) stay invariant, while distribution shifts can be modeled as (bounded or unbounded) shift interventions on the covariates. 
%where test shifts are induced by \emph{interventions} on the model, as typically studied in \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}. 
%\fy{merge} Some stuff invariant (thetastar) and  distribution shifts can be modeled as interventions on the covariates \citep{meinshausen2018causality, buhlmann2020invariance}. 
%In the end, both infinite and finite robustness methods aim to generalize to a set of distributions induced by the (invariant) causal data-generating process, characterized by some model parameters $\thetastar$, and a set of admissible (bounded or unbounded) test shifts. 
The goal is then to leverage (heterogeneous) training data to find a model that generalizes to a set of distributions induced by $\thetastar$ and a set of interventions. 
%the causal parameters  (invariant) causal data-generating process
This is informally captured by the
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
%This results in the 
following optimization problem:
\begin{equation}\label{eq:conventional-robust-risk}
   \betarob \coloneqq \argmin_{\beta} \sup_{\substack{\text{shift} \in \\ \text{shift set}}} \Loss (\beta; \prob^{\thetastar}_{\text{shift}}),
\end{equation}
where $\Loss(\beta; \prob^{\thetastar}_{\text{shift}})$ is the risk of an estimator $\beta$ on the distribution $\prob^{\thetastar}_{\text{shift}}$
%generated by the 
induced by the SCM with parameters $\thetastar$ and shifts from a given set of shifts/interventions.
%perturbed by a specific shift from the shift set. 
The minimizer of \cref{eq:conventional-robust-risk} ensures robustness with respect to the \emph{robustness set} $\{\prob^{\thetastar}_{\text{shift}}:\text{shift} \in  \text{shift set}\}$. However, in general this set is not fully known during training. %from training distribution. 
In prior works, the robustness set is computable 
% under the assumption
%by assuming 
% that 
if
the training distributions 
are heterogeneous enough
% contain enough information \fy{have enough heterogeneity} 
to identify the relevant parts of the causal model --
%for robust prediction 
this could involve finding a 
% which could either be a 
transformation of the covariates $X$ (also called representation) so that the conditional distribution of the labels given the transformed covariates is invariant \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant,arjovsky2020invariant, ahuja2020invariant,
shi2021gradient, 
xie2020risk, krueger2021out, ahuja2021invariance},
% \fy{cite also bottleneck bla}, 
or identifying the causal parameters themselves   \citep{angrist1996identification, hartford2017deep, singh2019kernel, bennett2019deep, muandet2020dual}. 
Most settings in both lines of work can achieve
% consider
generalization to test interventions of arbitrary strength 
% \citep{peters2016causal,arjovsky2020invariant} \fy{add IV citations}
% that 
and so we refer to them as \emph{infinite robustness methods}. 
Since it is more realistic to encounter bounded interventions in the real world, this approach would again be too conservative and pessimistic. This observation motivated \emph{finite robustness methods} \citep{rothenhausler2021anchor, jakobsen2022distributional, kook2022distributional, shen2023causalityoriented, christiansen2021causal} that trade off robustness strength against predictive power depending on the maximum expected strength of the test shifts. We refer to \cref{sec:related_work} for a more detailed discussion and comparison of these methods. 
% \fy{pointer to appendix for more detailed discussion}%, achieving \emph{finite robustness}. 
%invariance-based with infinite robustness methods \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, achiam2017constrained}) or by restricting the shift set to directions observed during training time (e.g., finite robustness methods \citep{rothenhausler2021anchor,shen2023causalityoriented, jakobsen2022distributional, kook2022distributional,christiansen2021causal}).

%In general, the robust predictor \prettyref{eq:conventional-robust-risk} can be only computed if the corresponding robustness set $\{\prob^{\thetastar}_{\text{shift}}:\text{shift} \in  \text{shift set}\}$ is identified from training data. However, if the model parameter $\thetastar$ of the data-generating process is unknown, computability of the robustness set from training data cannot be ensured. 


% A recent line of \emph{invariance-based methods} \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, arjovsky2020invariant} assumes the existence of an invertible mapping of the covariates which remains invariant across distribution shifts. In most of these methods, the invariances are formalized through the framework of structural causal models (SCMs) \citep{pearl2009causality}, where distribution shifts can be modeled as interventions on the covariates \citep{meinshausen2018causality, buhlmann2020invariance}. Given a sufficiently rich collection of shifted training environments, invariance-based methods can achieve \emph{infinite robustness}, i.e. identify an invariant prediction model which generalizes well to test interventions of arbitrary strength \citep{peters2016causal,arjovsky2020invariant}. In practice, considering shifts of arbitrary strength on all covariates can lead to overly conservative estimators. A related line of causality-oriented robustness methods \citep{rothenhausler2021anchor, jakobsen2022distributional, kook2022distributional, shen2023causalityoriented, christiansen2021causal} trades off robustness against predictive power depending on the maximum expected strength of the test shifts, achieving \emph{finite robustness}. 
%, and therefore, alternatives have been proposed in, e.g., the Group DRO literature \citep{sagawa2019distributionally, frogner2019incorporating, liu2022distributionally}. However, these methods cannot protect against perturbations larger than those seen during training time and do not provide a clear interpretation of the perturbation class.
% 
% \nico{This paper considers an alternative, more realistic scenario where partial information about the perturbations at test time is available at training time. }
%though the robustness set itself may not be fully given. 
% For example, for the same study conducted in different hospitals, one might anticipate/know which covariates will stay invariant and which ones will experience shifts.

% \julia{causality sentence}
% \nico{[mrw]: One way to model such prior information is to assume that the data are}
% % such prior information can be conveniently formalized if the data are assumed to be 
% generated by a structural causal model (SCM) \citep{pearl2009causality} and the test shifts are induced by \emph{interventions} on the model, as typically studied in \emph{causality-oriented robustness} \citep{meinshausen2018causality, buhlmann2020invariance}. 
% In this case, one can formulate the robustness problem and characterize the robustness set by leveraging the interventional structure of the training and test distributions, and the availability of multiple environments.
% % \julia{IRM sentence} 
% \nico{[mrw]:
% Depending on the strength and direction of interventions, we distinguish between \emph{infinite} and \emph{finite robustness} methods.
% }
% For example, t
% The line of work based on invariance \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, arjovsky2020invariant} uses multiple training environments to identify the underlying "stable representation" of the data which does not change across environments. If the data heterogeneity is sufficient to fully identify the invariant representation, such methods are robust against arbitrary shifts in the covariates, \nico{and achieve \emph{infinite robustness}.}
% \nico{[mrw]:
% In real data, however, shifts of arbitrary direction and strength in the covariates are unrealistic. Thus, a different line of work \citep{rothenhausler2021anchor, jakobsen2022distributional, christiansen2021causal, kook2022distributional, shen2023causalityoriented} trades
% off robustness against predictive power to achieve what is known as \emph{finite robustness}.
% The main idea of finite robustness methods is to learn a function that is as predictive as possible while protecting against shifts up to some strength in the directions observed during training time.
% }
% \julia{anchor sentence}
% Similarly, a branch of methods around instrumental variable and anchor regression \citep{rothenhausler2021anchor, jakobsen2022distributional, kook2022distributional, shen2023causalityoriented, christiansen2021causal} exploits heterogeneity of training data to find a predictor which is robust against training-time shifts of bounded (but potentially larger) strength.  

% making the robustness a function of the training environments, making it identifiable by construction .     
% Prior work has focused on the case when the robust predictor is identifiable, i.e. the robustness set is known , and optimal robustness can be achieved by directly minimizing the robust risk $\sup_{\substack{\text{shift} \in \\ \text{shift set}} }\Loss (\prob^{\theta}_{\text{shift}} , \betahat)$. Or in a dual view, for a given estimator, we can find the robustness set for which it minimizes this loss \fy{cite anchor bla}.
% In \emph{causality-oriented robustness}, such prior information can be  well-formalized via the causality assumption and the notion of interventions. 
% \fy{For example, for different hospitals/regions some covariates stay unchanged but some may}
%This is the scenario we consider in this paper \julia{caution, makes it sound like we perturb single covariates}
%one may still be able to identify the robust predictor.

% In this case, the robustness set may still be identifiable from data by leveraging
% structural assumptions on the training and test shifts and availability of multiple heterogeneous environments.
% %when heterogeneous data from similar environments is available \fy{e.g. data from different hospitals (if you want to continue with self-driving cars - different regions)}. 
% \fy{IRM sentence} For example, in the setting of invariance-based methods, identifying
%often, more than one data distribution is available during training. For instance, in the medical setting, data for a particular study may be collected from different hospitals, 
%multiple environments (e.g., the same study conducted in different hospitals.
%In this case, the robust predictor can often be identified by exploiting \emph{invariances} across training distributions to identify which  \fy{do we want to introduce the term "invariance" here? there is tradeoff since many of these terms of overloaded}
% components of the data that result in a stable prediction across different training environments \citep{muandet2013domain, arjovsky2020invariant} can lead to robust solution against all arbitrary shifts.
% %\fy{not so clear or hard to guess what stable prediction means here}. 
% \fy{IV/anchor sentence} Or in the presence of latent confounding, IV/anchor setting, given shifts during training time, one can be robust against shifts 
% In all prior work so far, the provable robustness of the methods heavily rely on identifiability of the robustness set from the training distribution - which is "the same" as assuming similar shifts during training and test.  In particular, it is possible using training data alone to minimize $\sup_{\prob \in \text{rob. set} }\Loss (\prob, \beta)$
% \begin{wraptable}{r}{.5\columnwidth}
% \resizebox{.5\columnwidth}{!}{%
% \setlength{\tabcolsep}{0.5pt}
% \begin{tabular}{@{}c|c|c|c@{}}
% \toprule
% Framework accounts for~ &
%   \begin{tabular}[c]{@{}c@{}}~bounded~~\\ shifts\end{tabular} & 
%   \begin{tabular}[c]{@{}c@{}}partial id. of\\  ~~causal param.~~\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}partial id. of\\  ~~robustness set\end{tabular} \\ 
%   \toprule
% \begin{tabular}[c]{@{}c@{}}DRO\\ \citep{ben2013robust, duchi2021learning, sinha2017certifying, mohajerin2018data, sagawa2019distributionally} \end{tabular} & \cmark & $-$  & \xmark  \\ \midrule
% \begin{tabular}[c]{@{}c@{}}Inf. robustness methods\\
% \citep{peters2016causal, fan2023environment, magliacane2018domain, rojas2018invariant, arjovsky2020invariant}
% \end{tabular}
% & \xmark  & \xmark & \xmark \\ \midrule
% \begin{tabular}[c]
% {@{}c@{}}Finite robustness \\
% \citep{rothenhausler2021anchor, jakobsen2022distributional, christiansen2021causal, kook2022distributional, shen2023causalityoriented}
% \end{tabular} &\cmark  & \cmark  & \xmark  \\ \midrule
% \begin{tabular}[c]
% {@{}c@{}}\\
% \textbf{partially id. robustness}~~\\
% \phantom{~}
% \end{tabular} & \cmark & \cmark & \cmark \\ \bottomrule
% \end{tabular}%
% }
% \caption{Distributional robustness frameworks.}\label{tab:rw}
% \end{wraptable}
%However, in the real world
In practical applications, the training distributions may lack sufficient heterogeneity to ensure the identifiability of the robustness set, and thus, the robustness properties of the methods mentioned before would not be guaranteed.
% required by the aforementioned prior work under their respective assumptions.
%In practical applications, the heterogeneity of the training distributions might not be enough to ensure identifiability of the robustness set that all of the above prior work requires. \fy{not great sentence}
%test shifts are not guaranteed to be very similar to the training shifts, or other necessary assumptions do not hold, rendering the robustness set non-identifiable. 
%Equally, the assumptions necessary for the identification of the robustness set are not likely to hold exactly (for instance, if one only has access to few training environments).  -- this is kind of the same assumption as the one above?
Even minor violations of the identifiability assumptions can cause invariance- and causality-based methods to fail (Examples 4 and 5 in \cite{kamath2021does}, Theorem 3.1 in 
\cite{rosenfeld2020risks}, Theorems 3 and 4) and perform equally or worse than empirical risk minimization (ERM) \citep{ahuja2020empirical, gulrajani2020search, rosenfeld2022online}, which is oblivious to heterogeneity. 
Thus, the analysis of distributional robustness in the setting of \Cref{eq:conventional-robust-risk} is so far limited to either finding the robust solution under the assumptions that the robustness set is identified, or impossibility results when the robustness set is not known. \julia{Here: cite reviewer's impossibility results}


% %%%%%% this was ERM relation specific atttempt 2
% \fy{fanny attempt (content - to polish): (switch in order)} Empirically, invariance-based methods are often critiqued for not working well or better than ERM. 
% %of these methods when their motivating distributional assumptions do not hold, e.g. in the presence of latent confounding.
% \fy{shorten}
% In particular, such \emph{invariance-based} methods for the multi-environment setting are only proven to be beneficial (compared to ERM)
% %beneficial by proving robustness of the method output \fy{of certain causal parameters/latent variables} 
% for specific combinations of "underlying DAG" assumptions, the number of training environments and their "degree of variability" \citep{peters2016causal, arjovsky2020invariant} \fy{also anchor here}.
% %\fy{try 2:} On the constructive side, usually people started with assumptions on DAG or distribution heterogeneity and derived a method that recovers some causal parameters/latent variables. Alternatively, they derived robustness set for which some methods outputs the robust model \fy{discrete anchor}. 
% Failure is then explained by finding a set of similar assumptions/settings under which those methods fail to output these parameters/variables \fy{ICP ones}.
% %%%%%%%%%%%%%%%%%%%




%\fy{try 1:}
%In many real-world scenarios, however, these assumptions often do not hold and the theoretical analysis so far has been "content" with explaining failure by showing  assumptions/settings under which those methods do not identify these parameters/variables. Then they either 1) come up with a new method which can output the desired parameters or 2) derive robustness set/perturbation set for which the method 
%missing identifiability under some DAG assumptions (for icp style things) and "move on" or come up with another algorithm that can solve this assumption.

%\fy{well, they usually end when e.g. causal parameters can't be identified? which is even a stop beyond?}
%and at the same time the possible robustness set is bounded.  

%%%%%%%%%%%%%%%%
% \fy{previous ERM flow}
% However, \fy{changed:} such \emph{invariance-based} methods for the multi-environment setting are only provably beneficial when the number of training environments and their "degree of variability"  is large\citep{peters2016causal, arjovsky2020invariant}.
% When these assumptions are violated, invariance-based methods can fail to identify the robust predictor and
% %do not offer benefits compared to vanilla
% thus perform comparably to 
% empirical risk minimization (ERM) \citep{kamath2021does, rosenfeld2020risks}.\fy{these are icp ones - could be dnagerous?}
% This fact is also reflected in the empirical "belief" that ERM cannot be improved upon in generic settings \citep{gulrajani2021in,vedantam2021an}.
% \fy{the problem is that we still say that some sophisticated method could be better - i think one reason in practice is that they don't adversarially test (but just some random shift)} 
%%%%%%%%%%%%%%%%

%theoretical guarantees for identification of the robust predictor in a multi-environment setting often pose strict assumptions on the number of training environments and their "degree of variability" \citep{peters2016causal, arjovsky2020invariant}. 

%, and 
%In line with these observations, \fy{this is an empirical statement?} it has been argued that ERM might be 
%is the state-of-the-art domain generalization algorithm 
%\fy{can we add these to previous citation block?} 
%In most settings in prior work, even though some methods are proven to fail, others can still perfectly identify the robust solution from the training environments. For scenarios where this is not possible, the statement usually "stops at the impossibility". 

%So far in the literature, the focus has been on assumptions such that robustness set is identifiable. We want to also discuss robustness of any when this is not possible.

\end{comment}

%In practice however, not only the causal parameter is not identifiable, but we cannot even assume that the 


\begin{comment}
In practice, causality- and invariance-based methods often result in wrong representations of the data  \citep{kamath2021does,rosenfeld2020risks}, 
because the setting is slightly misspecified or the data is not heterogeneous enough. 
%In the latter case, the objective cannot be computed from data nor minimized by any algorithm.
%no algorithm would be able to learn the perfectly robust predictor. 
Empirically, %it is observed that 
these methods often perform at least as poorly as empirical risk minimization (ERM) that ignores the multi-environment information
\citep{ahuja2020empirical,gulrajani2021in,rosenfeld2022online}. Although many possible explanations have been proposed in the literature for this breakdown of invariance-based methods, in our work, we concentrate on a sptype of misspecification previously underexplored in the theoretical line of research -- \fy{when the robust risk is not identifiable?}
%lack of identifiability of the invariant representation. 
%In this paper, we propose to extend the discussion of invariance-based methods to include the partially identifiable setting, 
where not only the causal parameter, but the robust risk \eqref{eq:conventional-robust-risk} is not determinable using training data either.
\end{comment}

%In this paper, we argue that it is important to also consider a scenario that is likely to happen in practice -- where we cannot fully identify the robustness set but can still partially leverage invariances.

%crucial for practice to 
%consider the setting when not only the causal parameter is not identifiable but the robust risk \fy{and its minimizer?} in \Cref{eq:conventional-robust-risk} is not determinable using training data either.
% \fy{somewhere here we could consider putting the 5 covariate example??}\julia{probably not} 
%Specifically, we would like to 
%\begin{tcolorbox}[colframe=white!, top=2pt,left=2pt,right=2pt,bottom=2pt]
% \vspace{-5pt}
\begin{center}
\emph{
%\fy{only one of those} What is the inherent, algorithm-independent difficulty of a robust generalization problem given multiple training environments?
%how difficult is the robust generalization task? \fy{what does it depend on?} 
What is the optimal worst-case performance any model can have for given structural relationships between test and training distributions, and how do existing methods comparatively perform?}
%the partially identifiable setting?}  
\end{center}
% \vspace{-5pt}
% \julia{i feel like the questions became too general somehow}

When the robust risk is not identifiable from the collection of training distributions, 
%it means that instead of computing a single objective, 
we obtain a whole \emph{set} of possible objectives -- all compatible with the training distributions -- that includes 
%, containing, i.a., 
the true robust risk. 
% \Nicola{[here it sounds like a single true risk, later we say all true models. I understand what we mean, but perhaps more clear smth like:] ... of possible objectives that are compatible with the training distributions.} 
%We now want to evaluate
In this case, we are interested in the best achievable robustness for \emph{any algorithm}
that we capture in a quantity
%with the knowledge of this set of robust risks.
% \fy{How would be evaluate the best-achievable robustness in this case (for algorithms with this knowledge)? }
%In order to evaluate performance in the partially identifiable setting, 
%For this purpose, we introduce the concept of
called the \emph{\idRR}:
%As a metric 
%, we propose the notion of a \emph{identifiable robust risk} defined by
\begin{align}
\label{eq:identifiable-robust-risk}
    \Lossrobpi(\beta) := \sup_{\substack{\text{possible}\\ \text{true model $\thetastar$ }}}  \sup_{\prob \in \robset(\thetastar)} \Loss (\beta; \prob).
    %\text{shift bound} \fy{bound or set?}) = \sup_{\substack{\text{possible}\\ \text{true model $\theta$ }}} \sup_{\substack{\text{shift} \in \\ \text{shift set}} }\Loss (\beta; \prob^{\theta}_{\text{shift}} ).
\end{align}
Note that $\Lossrobpi(\beta)$ is well-defined %given the training distributions 
even when the standard robust risk is not identifiable -- 
it takes the supremum over the robust risks induced by 
%where the supremum is taken over all 
all model parameters $\thetastar$
that are consistent with the given set of training distributions. 
% \fy{not super happy with notation, here you don't see how different parts of the puzzle come in} \julia{we can change the inner sup to the robust risk, but i don't know what to change about the outer sup}
Furthermore, the minimal value of the worst-case robust risk corresponds to the optimal worst-case performance in the partially identifiable setting.
Spiritually, this \emph{minimax population quantity} is reminiscent of the 
%in settings where the robust risk cannot be identified.
algorithm-independent limits in classical statistical learning theory \cite{yu1997assouad}.\footnote{In particular, extending \eqref{eq:identifiable-robust-risk} to its finite-sample counterpart would introduce a more natural extension of the classical minimax risk statistical learning theory.  In this work, we focus on identifiability aspects instead of statistical rates.}
Even though our partial identifiability framework 
%is
%The identifiable robust risk~\eqref{eq:identifiable-robust-risk} is 
%defined generally and 
can be evaluated for arbitrary %restrictive or general 
modeling assumptions on the distribution shift (such as covariate/label shift, DRO, etc.), %However,
we present it
in \Cref{sec:setting} for a concrete linear setting for clarity of the exposition.
%the identifiable robust risk in a concrete instantiation of our partial identifiability framework~\eqref{eq:identifiable-robust-risk}.
Specifically, the setting is motivated by structural causal models (SCMs) with unobserved confounding (cf. \cref{sec:setting}), similar to the setting of instrumental variables (IV) and anchor regression \citep{rothenhausler2021anchor,saengkyongam2022exploiting}. Concurrent to our work, \cite{bellot2022partial} proposed a similar framework derived explicitly from structural causal models, with quantities that are closely related to our worst-case robust risk and its minimizer. 
%\fy{if we put this footnote or your original sentence in the main text this hinders the flow, but putting it in the footnote is also odd though? - leaving out more details would also be ok for me} \footnote{While we derive closed-form formulas on a continuous regression example, they analytically compute these values for specific discrete examples and empirically evaluate them on colored MNIST.}.
%framework for partial transportability which is conceptually related to our notion of \idRR. However, their approach leverages graphical assumptions, i.e., a priori knowledge about the structure of causal models during training and test time, whereas our focus is a more agnostic multi-environment setting. 
%In particular, 
%his instantiation allows us to explicitly compute the  identifiable robust risk and the optimal worst-case predictor and compare it with existing baselines. 


%For linear structural causal models, we first derive information-theoretic population lower bounds that provide a fine-grained characterization of robustness limitations for bounded additive test shifts and then provide a general method, called the \emph{identifiable robust prediction model}, that can achieve this lower bound.
%The \idRR~\eqref{eq:identifiable-robust-risk} quantifies the robust generalization performance of any predictor in the partially identifiable setting.
%represents a notion of algorithm-independent optimality for any combination of training and test shifts. 
In \Cref{sec:main-results}, we derive the \idRR~\eqref{eq:identifiable-robust-risk} and its minimum for the linear setting, and show theoretically and empirically that the ranking and optimality of different robustness methods change drastically in identifiable vs. partially identifiable settings.  Further, although the worst-case robust predictor derived in the paper is only provably optimal for the linear setting, experiments on real-world data in \Cref{sec:experiments} suggest that our estimator may significantly improve upon other invariance-based methods in more realistic scenarios. 
%Our experimental results 
Our experimental results provide evidence that evaluation and benchmarking on partially identifiable settings are important for determining the effectiveness of robustness methods.
%The same can be observed in experiments on real-world data. 
%in the presence of previously unobserved test shifts. 

%\fy{The framework instantiated for the particular linear SCM in this paper showcases the relevance of considering partial identifiability. The experimental results strongly suggest that benchmarking in general partial-identifiable settings is important to evaluate the robustness of a invariance-based method -- possibly beyond SCMs and for any distribution shift model. might also relevant for other multi-environment robustness models facing identifiability problems...}
%\fy{moved: Our analysis of partial identifiability in the SCM setting opens up avenues for extensions beyond this setting in future work.}

\begin{comment}
Our framework allows us to 
%We first 
theoretically benchmark robustness methods on this particular setting (cf. \Cref{sec:comp-with-finite-robustness-methods}). We find that in the partially identified setting, the OOD generalization of existing robustness methods is suboptimal, and the gap increases with the magnitude and "non-identifiability" of the test shifts. We validate our findings empirically, first on synthetic Gaussian data replicating our theoretical setting. Our experiments on real-world gene expression data suggest effectiveness of the identifiable robust predictor for more general additive shift scenarios. However, a more systematic evaluation on a diverse collection of datasets is needed. 


Based on our theoretical and empirical findings, we argue that our framework may be useful in practical OOD scenarios when some limited prior knowledge is available about the relationship between the distribution shifts during training and test time. Our findings highlight the importance of the partially identified setting for evaluating robustness methods in the future and open up avenues for extensions beyond the linear setting. \fy{moved: Our analysis of partial identifiability in the SCM setting opens up avenues for extensions beyond this setting in future work.}
\end{comment}
% we do  Benchmarking theoretically on particular setting  + empirically validated (on that setting) with (estimating (training shifts)) lower bound. as expected we see differences and hence we should pay attention to this setting.
% This id.rob. risk also naturally gives rise to a new finite-sample estimator. We study the irr minimizer of a specific setting (linear SCM assumption, specific M, Y unshifted) - 
% even though its optimal only under assumptions on the shift (M unknown, Y shifted) - trivially also confirmed by experiments - real-world experiments suggest effectiveness for more general linear shift scenarios (that would have to be evaluated more systematically).
% In \Cref{sec:setting}, we introduce the notion of an observationally equivalent set of parameters and the minimizer of the identifiable robust risk, as well as the formal definition 
% of the information-theoretic minimax quantity that is the minimum of \Cref{eq:identifiable-robust-risk}
% %induced by the risk \fy{refer to eq above?}.  
% \fy{following sentence does not fit flow:} We argue that our framework may be useful in practical scenarios when some limited prior knowledge is available about the relationship between the shifts during training and test time.
%In such cases, identifiable robust risk 
%In such cases, the notion of the identifiable robust risk offers the possibility to i) quantify/evaluate the difficulty of the robust generalization problem in terms of certain problem parameters and ii) can be directly used to obtain an optimal worst-case estimate \fy{if optimization is possible}.
% In \Cref{sec:main-results}, we then %initialize  \fy{what does initialize mean here? why not compute?}
% \fy{derive expressions for/compute} these quantities 
% %on the example of a linear SCM with unobserved confounding, similar to the setting of IV/anchor regression \cite{rothenhausler2021anchor}, 
% and compare the \emph{optimal} identifiable robust risk with the identifiable robust risk achieved by existing algorithms. 
%"We also compare the minimizer of the identifiable robust risk  (achieving optimality) to give a ballpark how far from optimal existing algorithms are in partial identifiable linear SCM setting"... 
% Finally, %in \Cref{sec:experiments}, 
% we also provide the corresponding empirical evaluation on synthetic Gaussian data as well as real-world gene expression data, and show that there is a significant gap in performance between the minimizer of the empirical identifiable risk and existing robust estimators in cases when the test data exhibits previously unobserved shifts. Thus, we provide a more refined explanation for failure of existing robustness algorithms in a partially identified setting, in particular when test data cannot be "interpolated" from training environments. 
% "Finally, the derived estimator also seems to work better on real data (where we don't do any adversarial evaluation)" \fy{here its important to admit that its trivial that an estimator thats made to minimize the id.rob.risk will be better than other methods, but question is more like how different - but I'm still not super happy with this current comparison cause  ideally: we'd also be comparing other algorithms on settings where only one algo is optimal for and the rest has some identifiability issue??}
\begin{comment}
In this paper, we introduce concepts that 
\paragraph{Our contributions}
In \Cref{sec:prop-invariant-set}, we introduce our framework of partially identified robustness in the context of linear structural causal models with latent confounding and additive distribution shifts. In particular, for any estimator $\beta$, we introduce a new risk measure called the \emph{identifiable robust risk}\footnote{Note that partial identifiability is commonly studied in the context of set-identifying the causal parameter \citep{tamer2010partial} (see \cref{sec:related_work} for a further discussion). Here, additionally, we use the term in the context of identifiability of the robustness set.} that is informally defined as
\begin{align*}
    \Lossrobpi(\beta; \text{shift bound} \fy{bound or set?}) = \sup_{\substack{\text{possible}\\ \text{true model $\theta$ }}} \sup_{\substack{\text{shift} \in \\ \text{shift set}} }\Loss (\beta; \prob^{\theta}_{\text{shift}} ).
\end{align*}
The minimal value of this quantity over all estimators $\beta$ then characterizes the hardness of the problem in the given partially identifiable setting. For example, it can offer guidance to the practitioner, such as whether to collect more data (connecting the result to active causal learning) or use a good algorithm. Since $\Lossrobpi$ is identifiable from data, one can also derive an approximate finite-sample estimator for better robustness in the partially identified setting.
%In line with the majority of works in this area, 
%\julia{ all the sections changed, redo}
%In \Cref{sec:PI-lower-bound}, we then apply the framework on linear structural causal models with latent confounding and additive distribution shifts. 
%The set of training and test distributions may differ in additive interventions on the covariates, while the causal mechanism and confounding stay invariant. 
In~\Cref{sec:identifiability-linear-SCM},  we discuss conditions for risk identifiability in our linear SCM setting and derive a lower bound 
%In a setup similar to anchor regression, we derive a lower bound 
for the identifiable robust risk. This lower bound is tight and achievable for some regimes of shift strength. 
In \cref{sec:comp-with-finite-robustness-methods},
we finally
evaluate the identifiable robust risk of prior robustness algorithms and show theoretically and empirically that the minimizer of the identifiable robust risk outperforms existing methods in partially identified settings.
\end{comment}
% \nico{cannot understand last sentence}
% \fy{sounds a little negative - could also write sth like "outperform"}
% that shows how existing methods can be far from optimal and close to ERM in robust performance in the partially identifiable setting. 
% Finally, in \Cref{sec:optimal-estimator}, based on 
% results in \Cref{sec:PI-lower-bound}, 
% we propose a new estimator that is optimal for bounded distribution shifts under partial identifiability.
%%%%%%%%%%%%%%%%%%% fanny try 1 %%%%%%%%%
% \fy{this somehow didn't fit in flow -maybe fits better to related work and when we actually introduce stuff} The case of unbounded interventions is studied in causality/IV literature, and unless the estimator is equal to the causal parameter, this risk can be infinity. Instead, in our fine-grained analysis, we focus on bounded shifts during test time for the worst-case risk to be well-defined and meaningful. 
% %In case the model may not be fully identified but no "unseen" test shifts are observed, the PI-robust risk reduces to the conventional robust risk and there exists an estimator that can actually reach zero robust risk (in population). We say that in this setting, the robust predictor is identifiable. Such settings have been studied before, like in anchor regression \cite{rothenhausler2021anchor} and distributionally robust gradients (DRIG) \cite{shen2023causalityoriented}.
% In contrast to previous works, (to the best of our knowledge,) we are the first to study distributional robustness under arbitrary bounded additive shifts 
% when the robust predictor may only be partially identifiable.  We first derive information-theoretic lower bounds that provide a fine-grained characterization of robustness limitations for different combinations of training environments and robustness sets during test time. For this purpose, we introduce a new minimax risk measure called the \emph{partially identified (PI) robust risk} ...
% ion to considering a broad class of distribution shifts, we do not require full identifiability of the model or a minimum number of training environments. Instead, we quantify the best possible robustness achievable for a given structure of training and test environments, even when few training environments are present and the causal structural model is not identifiable. To the best of our knowledge, we are the first to study distributional robustness under arbitrary bounded additive shifts in a partially identifiable setting. 
% In \prettyref{sec:prop-invariant-set}, we explicitly describe non-identifiability of the data-generating model in a multi-environment setting and connect it to the notion of \emph{observational equivalence} from the partial identifiability literature [cite]. As a consequence, we show that the robust predictor with respect to a specified robustness set can be computed from training data if and only if the robustness set does not contain "unseen" directions on which the model has not been identified \fy{that does not sound like the major contribution}. In \prettyref{sec:PI-lower-bound}, we introduce a new risk measure, which we call partially identified (PI)-robust loss. This minimax notion quantifies the worst-case robust risk under partial identifiability of the data-generating model. We derive a lower bound on this measure and show that no infinite robustness is possible for distribution shifts in \emph{any} direction not covered by the training data. In 
% \prettyref{sec:optimal-estimator}, we discuss the PI-robust loss and propose a new estimator which is optimal with respect to a bounded set distribution shifts under partial identifiability. Finally, in \prettyref{sec:experiments}, we conduct synthetic and semi-synthetic experiments that confirm our findings. 
%%%%%%%%%%%%%
%%%%%%%%%%%% example %%%%%%%%%%%
% \fy{hm my hunch for ml paper is that the contributions should come basically here}
% To illustrate our question, we use the following toy example: 
% \begin{example}\label{ex:multiple-studies}
%     Suppose that we are conducting a long-term medical study, where data is collected from the same group of patients over the years to predict a health parameter $Y$, e.g. the life expectancy \fy{but do you measure this label?}. We are given data $\{(X^e, Y^e)\}_{e \in \Ecaltrain}$ from multiple past studies, where $\Ecaltrain = \{2010, 2015, 2020 \}$. We assume that the data $(X, Y)$ are generated by an underlying causal model, which is unobserved \fy{here enough to say latent confounding? causality/causal lingo suddenly pops up without a warning ;)}. By observing multiple studies, we are able to partially identify the causal mechanism. We now want to train a model which generalizes best on the future study $(X^{2025}, Y^{2025})$. We expect this study to have a distribution shift compared to past studies, including both previously observed shifts (e.g. for the age variable, which shifts by 5 years with every study and retains the same distribution otherwise), and new, previously unobserved shifts (e.g. changes in blood parameters caused by the Covid-19 pandemic). One could discard all covariates with unpredictable shifts \fy{if its unpredictable how do you know which ones these are?}, however, this would severely diminish the predictive power of the model. The practitioner is left with two questions: 1) Do I have enough data to reliably generalize to the new study? 2) What is the optimal model to train on existing data which still has predictive power, but does not fail too severely even on unobserved test shifts? 
%     \fy{as we are not epxerts in clinical study this can easily look unrealistic and need to be very carefully constructed. at least it didn't read very convincingly to me (both the unpredictable shift and measuring/predicting which Y part)}
% \end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%
% \fy{this seems too much detail at this point}
% In the following sections, we aim to provide a fine-grained analysis of robustness limitations in settings like \prettyref{ex:multiple-studies}. We consider data generated by a linear structural causal model, in which the response variable $Y$ is caused by covariates $X$. However, further correlations can be present through unobserved variables $H$, rendering the true causal relationship unidentifiable. As an input, we consider multiple training distributions $\{(X^e, Y^e)\}_{e \in \Ecaltrain}$, which differ by (bounded) additive shifts. Given this data, we are want to be robust to test distribution shifts of a specified (but arbirtary) strength and direction. It is known that \emph{infinite robustness} in arbitrary shift directions is generally impossible unless one can identify the true causal parameter, which, however, requires as many (sufficiently different) environments as there are covariates \cite{peters2016causal}. We find that existing \emph{finite robustness} methods only achieve robustness in directions on which the causal predictor is identified, leaving open the question what happens if the test shift occurs in a previously unobserved direction. To answer this question, we introduce a new minimax risk measure for an estimator $\betahat$, called the \emph{partially identified (PI) robust risk}:
% \begin{align*}
%     \Lossrobpi(\text{shift bound}, \betahat) = \sup_{\substack{\text{ground-truth}\\ \text{model $\theta$ }}} \sup_{\substack{\text{shift} \leq \\ \text{shift bound}} }\Loss (\prob^{\theta}_{\text{shift}} , \betahat).
% \end{align*}
% In case no "unseen" test shifts are observed or the model is fully identified, the PI-robust risk coincides with the conventional robust risk. Thus, our framework includes settings of anchor regression \cite{rothenhausler2021anchor} and distributionally robust gradients (DRIG) \cite{shen2023causalityoriented}. If these identifiability assumptions are not fullfilled, we find that the minimizer of the PI-robust risk, called the \emph{PI-robust estimator}, achieves better performance on unseen test shifts in partially identified settings than the existing finite robustness methods, which in turn behave similarly to ERM. This might potentially explain some of the previous findings of ERM outperforming domain generalization methods. 
% Depending on the assumptions on the test shift, our results could be interpreted in both domain generalization and domain adaptation settings. Moreover, in our derivation of the lower bound, we identify the most adversarial test shift directions under non-identifiability. This connects our findings to the field of \emph{active learning}: by collecting data corresponding to the most adversarial test shift, one maximally reduces the PI-robust risk and thus the non-identifiability of the robust predictor. 
% \fy{note to self: connection to abstention as per alex's question - to some extent we are doing abstention on the nonidentified directions to some extent (unless the shifts are unbounded where we want to utilize the spurious stuff)}
% We find that ex
% \begin{itemize}
%     \item In the following sections, we [our contribution]
%     \item In causality-oriented robustness methods, a method is frequently developed and then a robustness set for this method is derived.
%     \item We show by following this recipe, one only looks at cases where the robust loss is identifiable.
%     \item we will show that as soon as new directions appear, the methods fail.
%     \item We introduce the minimax quantity and the lower bound [informal]
%     \item we demonstrate how common methods are suboptimal w.r.t. lower bound theoretically and empirically
%     \item we empirically show that the minimizer of the PI-robust loss shows significantly improved robustness compared to methods that don't account for partial identifiability. 
%     \item Our results both useful for DG and DA
%     \item Our results could potentially be applied to active learning settings. 
% \end{itemize}
% In the following sections, we [...]. We show that for 
% If multiple varying data distributions are observed during training (e.g., studies from different hospitals), it is sometimes possible to exploit the heterogeneity to identify which components of the data result in a stable prediction across different environments. \cite{}
% In the recent years, a number of works have studied the setting of \emph{multi-environment}, or \emph{heterogeneous training data}, to identify components of the data, prediction on which remains stable across different distributions. 
% Computing the robust predictor is a challenging task and implicitly requires identifying components of the data, prediction on which remains stable across different distributions. 
% \begin{itemize}
%     \item X In safety-related fields, need for robust predictors 
%     \item X Current attempts: obtain these robust predictors from multi-environment training data (exploit heterogeneity)
%     \item However, in general not enough information in training data to compute the robust predictor. 
%     \item In the past, this has been seen in a binary way: "if not enough information, everything fails, if enough training information --> method to compute robust predictor"
%     \item In this paper, we challenge this binary view and instead aim to answer the question 
%     \item Fundamental question: what is the best possible robustness
%     \item Illustrative example
%     \item 
% \end{itemize}
% In the past years, various types structural assumptions have been explored, such as covariate shift [CITE], label shift [CITE] or similarity of distributions with respect to a probability discrepancy measure.

% \julia{include a simple, illustrative example along the lines of the following}:
% \begin{example}
%     Suppose that we are conducting a long-term medical study, where the same blood parameters are collected from the same group of patients over the years to predict a health parameter $Y$. We train our model on a number of past studies $\{S_{2005}, S_{2010}, S_{2015}, S_{2020}\}$. What determines how well this model can predict the results of the future study $S_{2025}$? Assuming a causal data-generating process, we can extract, for instance, how the variable $X_{age}$ affects the prediction, since in every new study, its mean shifts by 5 years, and the variance remains invariant. The same shift is expected for $S_{2025}$, and thus, we can reliably use  $X_{age}$ for prediction. However, some other covariates, previously stable, might shift distributions, for instance due to the Covid-19 pandemic. How can we decide whether to use those covariates for prediction, and if so, to which extent? 
% \end{example}
