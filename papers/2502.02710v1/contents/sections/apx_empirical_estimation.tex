
In this section, we discuss how to compute the worst-case robust loss and its minimizer from finite-sample multi-environment training data. We first describe the finite-sample setting and provide a high-level algorithm. We then discuss some parts of the algorithm in more detail. Finally, we show that the empirical worst-case robust loss is consistent under certain assumptions.  
Recall that we assume that $\Mtest = \gamma \PSMpop + \gammaprime R R^\top$, where $\gamma, \gammaprime \geq 0$, $\PSMpop $ is a PSD matrix satisfying $\range (\PSMpop) \subset \cS$ and $R$ is a semi-orthogonal matrix satisfying $\range (R) \subset \cSperp$. 

\subsection{Computing the worst-case robust loss}

\begin{algorithm}[h!]
    \caption{Computation of the worst-case robust loss} \label{alg:id-rob-loss}
    \begin{algorithmic}[1]
    \State \textbf{Input:} Multi-environment data $\cD \coloneqq \cup_{e \in \Ecaltrain} \cD_e$, test shift strengths $\gamma, \gammaprime > 0$, test shift directions $\Mtest \in \R^{d \times d}$, upper bound $C > 0$ on the norm of $\betastar$.
    % nuisances $S_0$, $R_0$, $\Cker$, $\betastarS$.
    
    \State \textbf{Step 1:} Estimate the training shift directions $\cShat(\cD)$, its  orthogonal complement $\cSperphat(\cD)$, and the identified linear parameter $\betaShat$.
      % \begin{align*}
    %     S(\cD) = \sum_{e = 1}^m ( \Cov(X^e)- \Cov(X^0) + \mu_e \mu_e^\top - \mu_0 \mu_0^\top)
    % \end{align*}
    \State \textbf{Step 2:} Estimate the identified and non-identified test shift upper bounds $\Mseenemp$, $\Rhat \Rhat^\top$, respectively, from $\Mtest$, $\cShat(\cD)$ and $\cSperphat(\cD)$.
 % \vphantom{$\betaShat$}
 %    \begin{align*}
 %        S_M, \PSM &\gets f(M, S); \quad S^{\perp}_M, \PSperpM \gets f(M, S^{\perp}). 
 %    \end{align*}
    \State \textbf{Step 3:} Estimate the maximum norm $\Ckerhat$ of the non-identified linear parameter.
    % \begin{align*}
    %     \Ckerhat \gets \sqrt{C^2 - \| \betaShat \|_2^2}.
    % \end{align*}
    % \State Compute the loss term on the reference environment:
    % \begin{align*}
    %    \LLref(\beta; \cD_0) &\gets \sum_{i = 1}^{n_0} (Y_{0,i} - \beta^\top X_{0,i})^2. 
    % \end{align*}
    % \State Compute the invariance penalty term:
    % \begin{align*}
    %    \LLinv(\beta; \betaShat, \PSM, \gamma) &\gets \gamma \| \PSM (\beta - \betastarS) \|^2_2. 
    % \end{align*}

    % \State Compute the non-identifiability penalty term \fy{probably you mean C-hat-ker}
    % \begin{align*}
    %    \LLid(\beta; \Cker, \PSperpM, \gamma) &\gets \gamma (\Cker + \| \PSperpM \beta \|_2)^2. 
    % \end{align*}
    \State \vphantom{$\hat{R}$}\textbf{Step 4:} Compute the worst-case robust loss function 
    \begin{align*}
        \LL_n(\beta; \betaShat, \PSM, \PSperpM) &\gets \underbrace{\LLref(\beta; \cD_0)}_{\text{reference loss}} + \underbrace{\LLinv(\beta; \betaShat, \PSM, \gamma)}_{\text{invariance penalty term}} + \underbrace{\LLid(\beta; \Ckerhat, \RRhat, \gammaprime)}_{\text{non-identifiability penalty term}}.
    \end{align*}
    \State \textbf{Return:}  worst-case robust predictor and the estimated minimax "hardness" of the problem: 
    \begin{align*}
        \betarobpihat &\gets \argmin_{\beta \in \R^d} \LL_n(\beta; \betaShat, \PSM, \PSperpM); \\ 
       \hat{\mathfrak{M}}(\cD,\gamma, \gammaprime, \Mtest) &\gets \min_{\beta \in \R^d} \LL_n(\beta; \betaShat, \PSM, \PSperpM).
    \end{align*}
    
        
    \end{algorithmic}
\end{algorithm}


\paragraph{Training data.} We observe data from $m + 1$ training environments indexed by $E \in \Ecaltrain = \{0,..., m\}$, where $E = 0$ represents the reference environment. We impose a discrete probability distribution $\prob^{E}$ on the training environment $E \in \Ecaltrain$, resulting in the joint distribution $(X, Y, E) \sim \prob^{X, Y \mid E} \times \prob^{E}$.  For each environment $E = e$, we observe the samples $\cD_e \coloneqq \{(X_{e,i}, Y_{e,i})\}_{i = 1}^{n_e}$, where $(X_{e, i}, Y_{e, i})$ are independent copies of $(X_e, Y_e) \sim \prob^{X, Y \mid E = e}$. Then, the resulting dataset is $\cD \coloneqq \cup_{e \in \Ecaltrain} \cD_e$ with $n \coloneqq n_0 + \cdots + n_m$. Furthermore, for each environment $E = e$, we define the weights $w_e \coloneqq n_e / n$. 

\paragraph{Computation of the worst-case robust loss.} In \cref{alg:id-rob-loss}, we present a high-level scheme for computing the worst-case robust loss from multi-environment data, which consists of multiple steps. First, nuisance parameters related to the training and test shift directions are estimated, which we describe in more detail below. Afterwards, the three terms of the loss are computed: the (squared) loss $\LLref(\beta; \cD_0)$ on the reference environment is computed as 
\begin{align*}
       \LLref(\beta; \cD_0) = \sum_{i = 1}^{n_0} (Y_{0,i} - \beta^\top X_{0,i})^2. 
    \end{align*}
The invariance penalty term $\LLinv(\beta; \betaShat, \PSM, \gamma)$ (which increasingly aligns any estimator $\beta$ in the direction of the estimated invariant predictor $\betaShat$ as $\gamma \to \infty$) can be computed as following in the linear setting:
    \begin{align*}
       \LLinv(\beta; \betaShat, \PSM, \gamma) = \gamma  (\beta - \betaShat)^\top \PSM (\beta - \betaShat) . 
    \end{align*}
Finally, the non-identifiability penalty term $\LLid(\beta; \Ckerhat, \PSperpM, \gamma)$ can be computed as follows:
    \begin{align*}
       \LLid(\beta; \Ckerhat, \PSperpM, \gammaprime) &= \gammaprime (\Cker + \| \PSperpM \beta \|_2)^2. 
    \end{align*}
The non-identifiability term, with increasing $\gammaprime$, penalizes any predictor $\beta$ towards zero on the subspace $\Rhat$ of non-identified test shift directions. In total, the worst-case robust loss (in the linear setting) equals
\begin{align*}
    \LL_n(\beta; \betaShat, \PSM, \PSperpM) = \sum_{i = 1}^{n_0} (Y_{0,i} - \beta^\top X_{0,i})^2 + \gamma (\beta - \betaShat)^\top \PSM (\beta - \betaShat) + \gammaprime (\Cker + \| \PSperpM \beta \|_2)^2, 
\end{align*}
where we suppress dependence on $C$, $\gamma$ and $\gammaprime$ and only leave the dependence on the nuisance parameters.
\paragraph{Choice/Estimation of nuisance parameters.} We now provide more details on the empirical estimation of the nuisance parameters $\cShat, \cSperphat, \Rhat$, $\PSM$, and $\betaShat$. 
\begin{itemize}
    \item The \textbf{constant} $C$ corresponds to the upper bound on the norm of the true causal parameter $\betastar$. Thus, the practitioner chooses $C$ in advance to ensure that (with high probability) $\| \betastar \|_2 \leq C$. 
    \item The \textbf{training shift directions} $\cShat$ can be computed via 
     \begin{align}\label{eq:S-estimation}
        \cShat(\cD) = \mathrm{range} \left[\sum_{e = 1}^m ( \Cov(X^e)- \Cov(X^0) + \mu_e \mu_e^\top - \mu_0 \mu_0^\top)\right],
    \end{align}
where for $e \in \Ecaltrain$, the matrix $\Cov(X^e)$ is the empirical covariance matrix estimated within the training environment $E = e$, and $\mu_e \in \R^d$ is the empirical mean of the covariates within the training environment $E = e$. Additionally, we compute the orthogonal complement $\cSperphat(\cD)$ of the space $\cShat(\cD)$\footnote{In general, $S(\cD)$ is a proper subspace of $\R^d$ and the RHS of \eqref{eq:S-estimation} corresponds to a sum of low-rank second moments. This can be consistently estimated if, for instance, the rank of each shift is known (e.g. in the mean shift setting), or the covariances have a spiked structure, allowing to cut off small eigenvalues.}. 
\item  Computation of the \textbf{seen and unseen test shift directions.} Multiple options are possible for the practitioner to compute the empirical test shift directions $\Mseenemp$ and $\Rhat \Rhat^\top$. One option is to choose $\Mseenemp = \sum_{e} w_e ( \Cov(X^e)- \Cov(X^0) + \mu_e \mu_e^\top - \mu_0 \mu_0^\top)$, where $w_e$ is the proportions of observations in environment $e \in \Ecaltrain$, akin to anchor regression \cite{rothenhausler2021anchor} and DRIG \cite{shen2023causalityoriented} with an appropriate shift magnitude $\gamma$. Afterwards, $\Rhat \Rhat^\top$ is chosen to be a projection onto an appropriate subspace of $\cSperphat$ (if additional information about test shift directions is available). If no information is given, we can choose $\Rhat \Rhat^\top = \Pi_{\cSperphat}$. Alternatively, if the information about potential test shift directions is given in form of a PSD matrix $\Mtest$, for instance, $\Mtest$ being a projection onto some subspace, we can decompose $\Mtest$ 
into identified and non-identified shift directions (and their corresponding projection matrices). 
Let $\PicShat$ and $\PicSperphat$ denote the projection matrices on $\cShat(\cD)$ and $\cSperphat(\cD)$, respectively. Consider the singular value decompositions $\PicShat \Mtest = U_{\cShat} \Sigma_{\cShat} V_{\cShat}^\top$ and $\PicSperphat \Mtest = U_{\cSperphat} \Sigma_{\cSperphat} V_{\cSperphat}^\top$ Then,  define
\begin{align*}
\Shat &= U_{\cShat}, \quad \Rhat = U_{\cSperphat}.
\end{align*}
The subspaces $\range (\PicShat \Mtest)$ and $\range( \PicSperphat \Mtest)$ are minimal subspaces contained in $\cShat$ and $\cSperphat$, respectively, such that $\range(\Mtest) \subset \range( \PicShat \Mtest) \oplus \range( \PicSperphat \Mtest)$. We can then take as $\PSM$ and $\PSperpM$ their corresponding projection matrices. 
\item The \textbf{identified parameter} $\betaShat$ (approximately) equals the true invariant parameter $\betastar$ on the space of training shift directions $\cShat$. 
% As known from the anchor regression/IV literature \citep{rothenhausler2021anchor,shen2023causalityoriented}, 
As conjectured in the anchor regression literature \citep{rothenhausler2021anchor,shen2023causalityoriented, jakobsen2022distributional} (see, for example, the discussion right after Theorem~3.4 in \citep{jakobsen2022distributional} and Appendix~H.3 therein)
for $\gamma \to \infty$, the estimators $\beta^{\gamma}_{\mathrm{anchor}}$ and $\beta^{\gamma}_{\mathrm{DRIG}}$ converge to the invariant parameter $\betastar$ on $\cS$. Thus, the identified parameter can be estimated as
\begin{align*}
    \betaShat \coloneqq \PicShat \beta^{\infty}_{\mathrm{anchor}} \quad \text{or} \quad \betaShat \coloneqq \PicShat \beta^{\infty}_{\mathrm{DRIG}}
\end{align*}
for the setting of mean or mean+variance shifts, respectively. 
% \nico{cite PULSE Theorem~3.4 and Figure H.6 in Appendix~H.3}
\end{itemize}

% \begin{algorithm}[t]
%     \caption{Computation of the worst-case robust loss} \label{alg:id-rob-loss}
%     \begin{algorithmic}[1]
%     \State \textbf{Input:} Multi-environment data $\cD \coloneqq \cup_{e \in \Ecaltrain} \cD_e$, test shift strength $\gamma > 0$, test shift directions $M \in \R^{d \times d}$, causal parameter upper bound $C > 0$.
%     % nuisances $S_0$, $R_0$, $\Cker$, $\betastarS$.
%     \State Estimate the training shift directions $S(\cD)$ and compute the orthogonal complement $S^{\perp}(\cD)$.
%     \State Estimate the identified causal parameter $\betaShat$ on $S$.
%     % \begin{align*}
%     %     S(\cD) = \sum_{e = 1}^m ( \Cov(X^e)- \Cov(X^0) + \mu_e \mu_e^\top - \mu_0 \mu_0^\top)
%     % \end{align*}
%     \State Estimate the identified and non-identified test shift directions and their projections:\vphantom{$\betaShat$}
%     \begin{align*}
%         S_M, \PSM &\gets f(M, S); \quad S^{\perp}_M, \PSperpM \gets f(M, S^{\perp}). 
%     \end{align*}
%     \State Estimate the norm of the non-identified causal parameter part:
%     \begin{align*}
%         \Ckerhat \gets \sqrt{C^2 - \| \betaShat \|_2^2}.
%     \end{align*}
%     \State Compute the loss term on the reference environment:
%     \begin{align*}
%        \LLref(\beta; \cD_0) &\gets \sum_{i = 1}^{n_0} (Y_{0,i} - \beta^\top X_{0,i})^2. 
%     \end{align*}
%     \State Compute the invariance penalty term:
%     \begin{align*}
%        \LLinv(\beta; \betaShat, \PSM, \gamma) &\gets \gamma \| \PSM (\beta - \betastarS) \|^2_2. 
%     \end{align*}

%     \State Compute the non-identifiability penalty term \fy{probably you mean C-hat-ker}
%     \begin{align*}
%        \LLid(\beta; \Cker, \PSperpM, \gamma) &\gets \gamma (\Cker + \| \PSperpM \beta \|_2)^2. 
%     \end{align*}
%     \State Compute the resulting loss function \fy{typo, should be perp in the id loss}
%     \begin{align*}
%         \LL_n(\beta; \betaShat, \PSM, \PSperpM, \Ckerhat, \gamma) &\gets \LLref(\beta; \cD_0) + \LLinv(\beta; \betaShat, \PSM, \gamma) + \LLid(\beta; \Ckerhat, \PSM, \gamma).
%     \end{align*}
%     \State \textbf{Return:}  worst-case robust predictor and the estimated minimax "hardness" of the problem: 
%     \begin{align*}
%         \betarobpihat &\gets \argmin_{\beta \in \R^d} \LL_n(\beta; \betaShat, \PSM, \PSperpM, \Ckerhat, \gamma); \\ 
%        \hat{\mathfrak{M}}(\cD,\gamma, M) &\gets \min_{\beta \in \R^d} \LL_n(\beta; \betaShat, \PSM, \PSperpM, \Ckerhat, \gamma).
%     \end{align*}
    
        
%     \end{algorithmic}
% \end{algorithm}




% \begin{enumerate}
%     \item \julia{put in the algorithm (very high level), then plug in all parts, then go over to the empirical estimation of our loss. High-level comments in the alg (as comments in the algorithm). cf Piers paper }

%     \item \nico{Fix argument for bound $MM^\top \preceq P_{S_{0}, M} + P_{R_{0}, M}$. Quick fix: no prior info on shift directions, i.e., $M = I$, so $MM^\top \preceq S_0S_0^\top + R_0R_0^\top$ trivially.}

%     \item \nico{Single $\gamma$ or $\gamma$, $\gammaprime$?}

%     \item \nico{No results in the literature for the consistency of the anchor estimator where $\gamma \to \infty$. High-level difficulty: need to define estimator where $\gamma_n \to \infty$ at the right speed.
%     Quick fix: Consider anchor setting like $X = MA + \eta$, where $M$ has distinct singular values [to discuss with Julia]}
% \end{enumerate}

% \paragraph{Nuisance parameters.}
% Let us fix the reference environment $0 \in \Ecaltrain$ and define the symmetric matrix
% \begin{align}\label{eqn:sigma-sim}
%     \Sigma \coloneqq
%     \sum_{e \in \Ecaltrain} w_e \left\{\Sigma_e - \Sigma_0 + (\mu_e - \mu_0)(\mu_e - \mu_0)^\top\right\} \in \R^{d \times d},
% \end{align}
% \julia{Sigma0 and mu0 are zero} where $w_e \coloneqq \prob^E(\{e\})$, $\mu_e \coloneqq \EE[X_e]$, and $\Sigma_e \coloneqq \EE[(X_e - \mu_e)(X_e - \mu_e)^\top]$.
% The symmetric matrix $\Sigma$ uniquely defines the space of training shift directions $\cS \coloneqq \range\(\Sigma)$.
% Let $S_0 \in \R^{d \times q}$ and $R_0 \in \R^{d \times (d - q)}$ denote orthonormal matrices whose columns form a basis for $\cS$ and $\cSperp$, respectively.
% Recall that we have access to some information about the anticipated test shift directions consisting of  a subspace $\cM \coloneqq \range\(M)$\footnote{If no information on the potential test shift directions is given, we choose $M = I_d$.}, where $M \in \R^{d \times r}$ is an orthonormal matrix, and test shift strength $\gamma > 0$. Then, by \cref{lm:upper-bound}, it holds that $MM^\top \preceq P_{S_{0}, M} + P_{R_{0}, M}$, where \julia{fix argument, remove Lemma F2}
% \begin{align}\label{eq:proj-to-check}
%     P_{S_{0}, M} \coloneqq S_0S_0^\top M M^\top S_0S_0^\top \in \R^{d \times d},\quad P_{R_{0},M} \coloneqq R_0R_0^\top M M^\top R_0R_0^\top \in \R^{d \times d}, 
% \end{align} 
% with $\range\(P_{S_{0, M}}) \subseteq \cS$ and $\range\(P_{R_{0, M}}) \subseteq \cSperp$.
% Moreover, define the projection of the causal parameter onto the training directions as
% \begin{align}
%     \betastarS \coloneqq S_0S_0^\top \betastar \in \R^d.
% \end{align}
% We call $\varphi_0 \coloneqq (P_{S_0, M}, \PSperpMpop, \betastarS)$ the true nuisance parameters. 


% \paragraph{Estimation of the nuisance parameters.}
% We now describe how to estimate the nuisance parameters $\varphi_0 = (P_{S_0, M}, \PSperpMpop, \betastarS)$ from the dataset $\cD$.
% First, let $\hat{\Sigma} \in \R^{d\times d}$ be the sample version of~\eqref{eqn:sigma-sim}, where the moments are replaced with sample averages and the environment probabilities $w_e$ with their corresponding sample frequencies.
% Suppose now that the dimension of $\cS$ is known\footnote{This assumption holds, for example, in the anchor regression setting where the anchor variable $A$ is observed (see \cref{subsec:anchor}).}.
% Then, the matrix $S_0 \in \R^{d \times q}$ can be estimated by computing the eigenvectors $\hat{S}$ of $\hat{\Sigma}$ corresponding to the $q$ largest eigenvalues. Similarly, the matrix $R_0 \in \R^{d \times (d - q)}$ can be estimated by computing the eigenvectors $\hat{R}$ of $\hat{\Sigma}$ corresponding to $d - q$ smallest eigenvalues.
% Given knowledge of test shift directions $\cM \coloneqq \range\(M)$, we can then estimate \nico{update this depending on~\eqref{eq:proj-to-check}}
% \begin{align}
%     \hat{P}_{S, M} \coloneqq \hat{S}\hat{S}^\top M M^\top \hat{S}\hat{S}^\top \in \R^{d \times d},\quad \hat{P}_{R,M} \coloneqq \hat{R}\hat{R}^\top M M^\top \hat{R}\hat{R}^\top \in \R^{d \times d}. 
% \end{align} 
% Moreover, the vector $\betastarS$ can be estimated as $\hat{\beta}^\cS\coloneqq $ \nico{...to clarify how}


% \paragraph{Robust identifiable risk and worst-case robust predictor.}
% For fixed constants $C > 0$ and $\gamma, \gammaprime > 0$, 
% the robust identifiable risk is defined for all parameters of interest $\beta \in \R^d$ and nuisance parameters $\varphi \coloneqq (P_S, P_R, b)$ as
% \begin{align}\label{eqn:rob-loss-ell}
%     \LL(\beta, \varphi)  \coloneqq 
%     \EE\left[\left(Y_0 - \beta^\top X_0 \right)^2 \right]
%     + \gamma  \norm{P_S(b - \beta)}_2^2 
%     +   \gammaprime \left(\sqrt{C - \norm{b}_2^2} 
% + \norm{P_R\beta}_2\right)^2,
% \end{align}
% \julia{add discussion on $C$} where $(X_0, Y_0) \sim \prob^{X, Y \mid E = 0}$ denote the predictor-response pair from the reference environment.
% For fixed $\varphi$, the worst-case robust loss is a strongly convex objective function in $\beta$, even though it is not differentiable at $\beta = 0$. The strong convexity implies the existence of a unique minimizer, which we call the worst-case robust predictor and is defined as
% \begin{align}
%     \betarobpi \coloneqq \argmin_{\beta \in \R^d} \LL(\beta, \varphi_0).
% \end{align}
% If $\gamma = 0$, the worst-case robust loss $\LL(\beta, \varphi_0)$ evaluated at the true nuisance parameters $\varphi_0 = (P_{S_0, M}, P_{R_0, M}, \betastarS)$ coincides with the squared loss on the reference environment. If no new shift directions are expected, i.e. $R_0 = 0$ (and correspondingly $\| b \|_2^2 = C$), the loss coincides with the anchor regression/DRIG loss \citep{rothenhausler2021anchor,shen2023causalityoriented}, with robustness set given by $C^\gamma = \{ \Atest \in \R^d: \EE[\Atest {\Atest}^\top] \preceq \gamma P_{S_{0, M}} \}$.
% \nico{$C^\gamma$ confusing with $C$}

% The second penalization term of the worst-case robust loss can be seen as a regularizer along the non-identifiable directions.
% If the training environments are rich enough to identify the causal parameter, i.e., $\cS = \R^d$, $\betastarS = \betastar$, then the loss corresponds to the squared loss on the reference environment with a causal regularization term $\gamma \| P_{S_{0, M}}(\betastarS - \beta) \|_2^2$ which penalizes the prediction models on test shift directions. 
% If the test shift strength $\gamma$ is large enough, the optimal estimator corresponds to the robust estimator constrained to the direct sum $\cS \oplus (\cSperp - \range\ (P_{R_{0, M}}))$ of the "identified" directions $\cS$ and "stable" unidentified directions $\cSperp - \range\ (P_{R_{0, M}})$.

% Thus, the worst-case robust estimator interpolates on two levels: on $\cS$,  it interpolates between the OLS and the causal predictor $\betastarS$ for  $\gamma \in [1, \infty]$\Nicola{$\gamma \in [1, \infty)$} – similarly to anchor regression, which can be seen as the 
% "finite robustness" axis. However, it also interpolates between the anchor estimator and the "abstaining" estimator~\eqref{eqn:abstaining} as the strength of non-identified shifts increases (the "non-identifiability" axis).

\subsection{Consistency of the worst-case robust predictor} 
For any estimator $\beta \in \R^d$ and given the estimated nuisance parameters $\hat\varphi \coloneqq (\PSM, \PSperpM, \betaShat)$, we define the sample worst-case robust risk as
\begin{align}\label{eqn:rob-loss-ell-sample}
\begin{split}
    \LL_n(\beta, \hat{\varphi})  \coloneqq &
    \frac{1}{n_0}
    \sum_{i \in \cD_0}\left(Y_{0,i} - \beta^\top X_{0,i} \right)^2
    + \gamma (\betaShat - \beta)^\top \PSM (\betaShat - \beta) 
    +   \gammaprime \left(\sqrt{C - \norm{\betaShat}_2^2} 
+ \norm{\PSperpM\beta}_2\right)^2.
\end{split}
\end{align}
% \nico{remove any gammaprime}
Correspondingly, we define the estimator of the worst-case robust predictor by
\begin{align}\label{eqn:betarobpi-consistent}
    \betarobpihat \coloneqq \argmin_{\beta \in \mathcal{B}} \LL_n(\beta, \hat{\varphi}),
\end{align}
where $\mathcal{B} \subseteq \R^d$ is some compact set whose interior contains the true parameter $\betarobpi$.


To show the consistency of~\eqref{eqn:betarobpi-consistent}, we first require consistency of the nuisance parameter estimators, which we state as an assumption.
\begin{assumption}\label{ass:consistency-nuisance}
    The estimated nuisance parameters $\hat\varphi \coloneqq (\PSM, \PSperpM, \betaShat)$ are consistent, that is, for $n \to \infty$,
    \begin{align*}
       \norm{\PSM - \PSMpop}_F \stackrel{\prob}{\to}0,
        \quad
        \norm{\PSperpM - \PSperpMpop}_F \stackrel{\prob}{\to}0,
        \quad
        \hat\beta^\cS \stackrel{\prob}{\to} \betastarS \coloneqq \Pi_{\cS} \betastar,
    \end{align*}
    where for any matrix $A \in \R^{m \times q}$, $\norm{A}_F = \sqrt{\trace(A^\top A)}$ denotes the Frobenius norm,  $\PSMpop$ is a PSD matrix with bounded eigenvalues with $\range(\PSMpop) \subset \cS$, and $\PSperpMpop$ is the corresponding population projection matrix onto $\cSperp$. 
\end{assumption}
 Depending on the assumptions of the data-generating process, Assumption~\ref{ass:consistency-nuisance} can be shown to hold. For example, in the anchor regression setting \cite{rothenhausler2021anchor}, the consistency of $\Mseen = \Manchor$, the projection
matrix $\PSperpM$, and $\PicShat$ holds if the dimension of $\cS$ is known (due to the mean shift structure).
The proof relies on the Davis--Kahan theorem (see, for example, \citep{yu2015useful}) and the consistency of the covariance matrix estimator.
Moreover, in the anchor regression setting, it is conjectured that the estimator $\beta^{\infty}_{\mathrm{anchor}}$ converges to its population counterpart (as discussed right after Theorem~3.4 in \citep{jakobsen2022distributional} and Appendix~H.3 therein) which implies that $\betaShat \coloneqq \PicShat \beta^{\infty}_{\mathrm{anchor}} $ consistently estimates $\betastarS = \Pi_{\cS} \betastar$.
% \nico{cite PULSE Theorem~3.4 and Figure H.6 in Appendix~H.3}

Under the assumption of the consistency of the nuisance parameter estimators, we can now show that~\eqref{eqn:betarobpi-consistent} is a consistent estimator of the worst-case robust predictor.

\begin{proposition}\label{prop:consistency-predictor}
  Consider the estimator  $\betarobpihat$ of the worst-case robust predictor defined in~\eqref{eqn:betarobpi-consistent}. Suppose the optimization problem is over a compact set $\cB \subseteq \R^d$ whose interior contains the true minimizer $\betarobpi$. 
  Assume that the covariance matrix $\EE[X_0X_0^\top] \succ 0$ with bounded eigenvalues and $\EE[Y_0^2] < \infty$.
  % \Nicola{Moreover, suppose that $\PSMpop \succeq 0$ with bounded eigenvalues.}
  Then, 
  under Assumption~\ref{ass:consistency-nuisance},
  $\betarobpihat$ is a consistent estimator of~$\betarobpi$.
\end{proposition}

\subsection{Proof of \Cref{prop:consistency-predictor}}

    For ease of notation define $\beta_0 \coloneqq \betarobpi$ and $\hat{\beta} \coloneqq \betarobpihat$.
    For any parameter of interest $\beta \in \cB$ and nuisance parameters $\varphi = (P_S, P_R, b)$,
   define the function 
  \begin{align}\label{eq:g-func}
      (x, y) \mapsto g_{\beta,\varphi}(x, y) 
      \coloneqq (y - \beta^\top x)^2 
      + \gamma \norm{P_S^{1/2}(b - \beta)}_2^2
      + \gamma \left(\sqrt{C - \norm{b}_2^2}  + \norm{P_R \beta}_2\right)^2.
  \end{align}
  Using~\eqref{eq:g-func}, the robust identifiable risk and its sample version defined in~\eqref{eqn:rob-loss-ell-sample} can be written, respectively as
  \begin{align*}
      \LL(\beta, \varphi) = \EE[g_{\beta, \varphi}(X_0, Y_0)],
      \quad
      \LL_n(\beta, \varphi) = \frac{1}{n_0}\sum_{i \in \cD_0} g_{\beta, \varphi}(X_{0,i}, Y_{0,i}).
  \end{align*}
  % can be written as 
  % Similarly, the sample robust identifiable risk defined  can be written as 
    % For any $\beta_1, \beta_2 \in \cB$, define $d(\beta_1, \beta_2) \coloneqq \norm{\beta_1 - \beta_2}_2$.
    % By slight abuse of notation, for any nuisance vector $\nu_1, \nu_2 \in \R^d \times \R^{d \times k}$ also define $d(\nu_1, \nu_2) = \norm{b_1 - b_2}_2 + \norm{S_1 - S_2}_2$.
    Our goal is to show that $\hat{\beta} \stackrel{\prob}{\to}\beta_0$. First, we show that the minimum of the loss is well-separated.

    \begin{lemma}\label{lm:well-separation}
    Suppose that $\EE[X_0X_0^\top] \succ 0$.
    Then, for all $\delta > 0$, it holds that
\begin{align}\label{eqn:well-sep}
    \inf\left\{\LL(\beta, \varphi_0) \colon \norm{\beta - \beta_0}_2 > \delta \right\} > \LL(\beta_0, \varphi_0).
\end{align}
\end{lemma}
    
   Fix $\delta > 0$.  From the well-separation of the minimum from Lemma~\ref{lm:well-separation}, there exists $\varepsilon > 0$ such that 
    \begin{align*}
        \left\{\norm{\hat\beta - \beta_0}_2 > \delta\right\} \subseteq
        \left\{\LL(\hat\beta, \varphi_0)- \LL(\beta_0, \varphi_0) > \varepsilon\right\}.
    \end{align*}
    Therefore,
    \begin{align}
        \prob&\left(\norm{\hat\beta - \beta_0}_2 > \delta \right) \leq 
        \prob\left(\LL(\hat\beta, \varphi_0)- \LL(\beta_0, \varphi_0) > \varepsilon\right) \nonumber\\
        &\quad= 
        \prob\left( 
        \LL(\hat\beta, \varphi_0)- \LL_n(\hat\beta, \varphi_0) + \LL_n(\hat\beta, \varphi_0)-
        \LL_n(\hat\beta, \hat\varphi)
        \right. \nonumber\\
        &\quad\quad\quad\quad
        \left. 
        + \LL_n(\hat\beta, \hat\varphi)
        - \LL_n(\beta_0, \hat\varphi)
        + \LL_n(\beta_0, \hat\varphi)
        -
        \LL(\beta_0, \varphi_0) > \varepsilon
        \right) \nonumber\\
        &\quad \leq
        \prob\left(
        \LL(\hat\beta, \varphi_0)- \LL_n(\hat\beta, \varphi_0) > \varepsilon/4
        \right)
        + \prob\left(
        \LL_n(\hat\beta, \varphi_0)-
        \LL_n(\hat\beta, \hat\varphi) > \varepsilon/4
        \right) \label{eq:consistency-1}\\
        &\quad\quad +
        \prob\left(
        \LL_n(\hat\beta, \hat\varphi)
        - \LL_n(\beta_0, \hat\varphi) > \varepsilon / 4
        \right)
        + \prob\left(
        \LL_n(\beta_0, \hat\varphi)
        -
        \LL(\beta_0, \varphi_0)>\varepsilon/4
        \right) \label{eq:consistency-2}.
    \end{align}
    We now want to prove convergence the four terms in ~\eqref{eq:consistency-1} and~\eqref{eq:consistency-2}. For this, we use the following statements proved in \Cref{sec:auxlemmaproofs}.

\begin{lemma}\label{lm:ulln-2}
    Suppose $\cB \subseteq \R^d$ is a compact set.
    Moreover, assume that the covariance matrix $\EE[X_0X_0^\top] \succ 0$ with bounded eigenvalues and $\EE[Y_0^2] < \infty$.
    Then, as $n, n_0 \to \infty$ it holds that 
    \begin{align}\label{eqn:ulln-2}
            \sup_{\beta \in \cB} |\LL_n(\beta, \varphi_0) - \LL(\beta, \varphi_0)| \stackrel{\prob}{\to} 0.
        \end{align}
\end{lemma}
\begin{lemma}\label{lm:lipschitz}
    As $n \to \infty$, it holds that
    \begin{align}\label{eqn:lipschitz}
            \sup_{\beta\in\mathcal{B}}|\LL_n(\beta, \hat\varphi) - \LL_n(\beta, \varphi_0)| \stackrel{\prob}{\to} 0.
        \end{align}
\end{lemma}
The two terms in~\eqref{eq:consistency-1} converge to 0 by \cref{lm:ulln-2} and \cref{lm:lipschitz}, respectively. The first term in~\eqref{eq:consistency-2} equals 0 since $\hat\beta$ minimizes $\beta \mapsto \LL_n(\beta, \hat\varphi)$. 
Finally, we observe that \begin{align}\label{eqn:ulln-1}
        \sup_{\beta \in \cB} |\LL_n(\beta, \hat\varphi) - \LL(\beta, \varphi_0)| \stackrel{\prob}{\to} 0,
        \end{align}
since we have that
    \begin{align*}
        \sup_{\beta \in \cB} |\LL_n(\beta, \hat\varphi) - \LL(\beta, \varphi_0)|
        \leq &\
        \sup_{\beta \in \cB} |\LL_n(\beta, \hat\varphi) - \LL_n(\beta, \varphi_0)|
        +
        \sup_{\beta \in \cB} |\LL_n(\beta, \varphi_0) - \LL(\beta, \varphi_0)|,
    \end{align*}
    where the first term converges in probability by Lemma~\ref{lm:lipschitz}, and the second term converges in probability by Lemma~\ref{lm:ulln-2}. This implies that the second term in~\eqref{eq:consistency-2} converges to zero. Since $\delta > 0$ was arbitrary, it follows that $\hat{\beta} \stackrel{\prob}{\to}\beta_0$.
    

\subsection{Proof of auxiliary lemmas}
\label{sec:auxlemmaproofs}
\subsubsection{Proof of \Cref{lm:well-separation}}
By definition,
\begin{align*}
    \LL(\beta, \varphi_0) = \EE[(Y_0 - \beta^\top X_0)^2]
    + \gamma \norm{\PSMpop^{1/2}(\betastarS - \beta)}_2^2
      + \gamma \left(\sqrt{C - \norm{\betastarS}_2^2}  + \norm{\PSperpMpop \beta}_2\right)^2.
\end{align*}
Since $\EE[X_0X_0^\top] \succ 0$, the first term is strongly convex in $\beta$.
Moreover, the second and third terms are convex in $\beta$. Therefore, $\LL(\beta, \varphi_0)$ is strongly convex in $\beta$. Since $\LL(\beta, \varphi_0)$ is also continuous in $\beta$, it follows that there exists a unique global minimum. Let $\beta_0$ denote the global minimizer of $\LL(\beta, \varphi_0)$.
% Fix $\delta > 0$ and define $A_\delta \coloneqq \{\LL(\beta, \varphi_0) \colon \norm{\beta - \beta_0}_2 > \delta\}$. We want to show that $\inf A_\delta > \LL(\beta_0, \varphi_0)$.
By the fact that $\LL(\beta_0, \varphi_0)$ is a global  minimum, and by definition of strong convexity, there exists a positive constant $m > 0$ such that, for all $\beta \in \cB$,
\begin{align}\label{eq:strong-conv}
    \LL(\beta, \varphi_0) \geq \LL(\beta_0, \varphi_0) + \frac{m}{2} \norm{\beta - \beta_0}_2^2.
\end{align}
Fix $\delta > 0$.  
Then, by~\eqref{eq:strong-conv},  for all $\beta \in \cB$ such that $\norm{\beta-\beta_0}_2 > \delta$ it holds that
\begin{align*}
    \LL(\beta, \varphi_0) \geq \LL(\beta_0, \varphi_0) + \frac{m\delta^2}{2} > \LL(\beta_0, \varphi_0). 
\end{align*}
Since the inequality holds for all $\beta \in \cB$ such that $\norm{\beta-\beta_0}_2 > \delta$, we conclude that
\begin{align*}
    \inf \{\LL(\beta, \varphi_0)  \colon \norm{\beta-\beta_0}_2 > \delta\} > \LL(\beta_0, \varphi_0).
\end{align*}
Since $\delta > 0$ was arbitrary, the claim follows.

\subsubsection{Proof of \Cref{lm:ulln-2}}
    Recall that for any $\beta \in \cB$
  \begin{align*}
      \LL(\beta, \varphi_0) = \EE[g_{\beta, \varphi_0}(X_0, Y_0)],
      \quad
      \LL_n(\beta, \varphi_0) = \frac{1}{n_0}\sum_{i \in \cD_0} g_{\beta, \varphi_0}(X_{0,i}, Y_{0,i}).
  \end{align*}
    To show the result, we must establish that the class of functions $\{g_{\beta, \varphi_0} \colon \beta \in \cB\}$ is Glivenko--Cantelli. From~\cite{van2000asymptotic}, a set of sufficient conditions for being a Glivenko--Cantelli class is that (i) $\cB$ is compact, (ii) $\beta \mapsto g_{\beta, \varphi_0}(x, y)$ is continuous for every $(x, y)$, and (iii) $\beta \mapsto g_{\beta, \varphi_0}$ is dominated by an integrable function.
    By assumption, (i) holds.
    Moreover, by~\eqref{eq:g-func}, it follows that $\beta \mapsto g_{\beta, \varphi_0}$ is continuous for all $(x, y)$ and thus (ii) holds. We now show that (iii) holds. 
    Since $\cB$ is compact we have that $\sup_{\beta\in\cB} \norm{\beta}_2 = C_1 < \infty$.
    For fixed $\gamma > 0$, and all $(x, y)$, we have that
    \begin{align}
    \label{eq:dominates-i}
    \begin{split}
        g_{\beta, \varphi_0}(x, y) 
        \leq &\ \sup_{\beta \in \cB} |g_{\beta, \varphi_0}(x, y)|\\
        \leq &\ \sup_{\beta \in \cB} (y - \beta^\top x)^2
        +
        2\gamma  \norm{\PSMpop^{1/2}}_F^2 \left(\norm{\betastarS}_2^2 + \sup_{\beta \in \cB} \norm{\beta}_2^2\right)\\
        &+   \gamma \left(\sqrt{C - \norm{\betastarS}_2^2} 
        + \norm{\PSperpMpop}_F \sup_{\beta\in\cB}\norm{\beta}_2\right)^2\\
        \leq &\
        2y^2 + 2C_1^2 \norm{x}_2^2 + K \eqqcolon G(x, y),
    \end{split}
    \end{align} 
    where $K < \infty$ is a finite constant not depending on $(x, y)$.
    Furthermore, we have that
    \begin{align}\label{eq:dominates-ii}
        \EE[G(X_0, Y_0)] = 2 \EE[Y_0^2] + 2C_1^2\ \trace (\EE[X_0X_0^\top]) + K < \infty,
    \end{align}
    since $\EE[Y^2] < \infty$ and $\EE[X_0X_0^\top]$has bounded eigenvalues by assumption. From~\eqref{eq:dominates-i} and~\eqref{eq:dominates-ii}, it follows that (iii) holds.

\subsubsection{Proof of \Cref{lm:lipschitz}}
For fixed $\gamma > 0$, we have that
\begin{align}
    \frac{1}{\gamma} & \sup_{\beta \in \cB}|\LL_n(\beta, \hat\varphi) - \LL_n(\beta, \varphi_0)|
    \leq
    \sup_{\beta \in \cB}\left|
    \norm{\PSM^{1/2}(\hat\beta^{\cS} - \beta)}_2^2
    - \norm{\PSMpop^{1/2}(\betastarS - \beta)}_2^2
    \right| \label{eq:lip-step-1}
    \\
    & + 
    \sup_{\beta \in \cB}\left |\left(\sqrt{C - \norm{\hat{\beta}^{\cS}}_2^2}  + \norm{\PSperpM \beta}_2\right)^2 
    - \left(\sqrt{C - \norm{\betastarS}_2^2}  + \norm{\PSperpMpop \beta}_2\right)^2\right|.
    \label{eq:lip-step-2}
\end{align}
We first show that ~\eqref{eq:lip-step-1} converges in probability to 0. 
\begin{align}
    &\sup_{\beta \in \cB}\left|
    \norm{\PSM^{1/2}(\hat{\beta}^{\cS} - \beta)}_2^2
    - \norm{\PSMpop^{1/2}(\betastarS - \beta)}_2^2
    \right| \notag\\
    = &\ 
    \sup_{\beta \in \cB}
    \left|(\hat{\beta}^{\cS} - \beta)^\top \PSM(\hat{\beta}^{\cS} - \beta) 
    - (\betastarS - \beta)^\top \PSMpop (\betastarS - \beta) \right| \notag\\
    = &\
    \sup_{\beta \in \cB}\left|(\hat{\beta}^{\cS} - \beta)^\top \PSM (\hat{\beta}^{\cS} - \betastarS)
    + (\hat{\beta}^{\cS} - \betastarS)^\top \PSM(\betastarS - \beta) \right. \notag \\
    & \left. \quad\quad + (\betastarS - \beta)^\top (\PSM - \PSMpop) (\betastarS - \beta) \right| \notag\\
    \stackrel{\clubsuit}{\leq}&\ 
     \sup_{\beta \in \cB} \norm{\hat{\beta}^{\cS} - \beta}_2\ \norm{\PSM}_F\ \norm{\hat{\beta}^{\cS} - \betastarS}_2
     + \sup_{\beta \in \cB} \norm{\betastarS - \beta}_2\ \norm{\PSM}_F\ \norm{\hat{\beta}^{\cS} - \betastarS}_2 \notag \\
     & \left. \quad\quad +
     \sup_{\beta \in \cB} \norm{\betastarS - \beta}_2^2\ \norm{\PSM - \PSMpop}_F\right. \label{eq:bound-1-1},
\end{align}
where $\clubsuit$ follows from the Cauchy--Schwarz inequality and from the fact that $\norm{A}_2 \leq \norm{A}_F$.
For any $\delta_1, \delta_2 > 0$, define the event
\begin{align*}
    A_n \coloneqq \left\{\norm{\betaShat - \betastarS} \leq \delta_1, \norm{\PSM - \PSMpop}_F \leq \delta_2\right\},
\end{align*}
and note that $\prob(A_n) \to 1$ as $n \to \infty$ from Assumption~\ref{ass:consistency-nuisance}.
On the event $A_n$, it holds
\begin{align*}
    \sup_{\beta \in \cB} \norm{\hat{\beta}^{\cS} - \beta}_2 \leq \norm{\hat{\beta}^{\cS} - \betastarS}_2 + \sup_{\beta \in \cB}  \norm{\betastarS - \beta}_2 \leq \delta_1 + C_1,\\
    \norm{\PSM}_F \leq \norm{\PSM - \PSMpop}_F + \norm{\PSMpop}_F \leq \delta_2 +  C_2,   
\end{align*}
where $C_1 < \infty$ follows from the compactness of~$\cB$ and $C_2$ follows from the fact that  $\PSMpop$ has bounded eigenvalues.
Therefore, on the event $A_n$, we can upper bound~\eqref{eq:bound-1-1} by
\begin{align}
    (\delta_1 + C_1) (\delta_2 + C_2)\ \norm{\hat{\beta}^{\cS} - \betastarS}_2 + (\delta_1 + C_1)\ \norm{\PSM - \PSMpop}_F.\label{eq:bound-1-2} 
\end{align}
From Assumption~\ref{ass:consistency-nuisance}, \eqref{eq:bound-1-2} converges to 0 in probability, and therefore,~\eqref{eq:lip-step-1} converges to 0 in probability as well.

Now, we can upper bound~\eqref{eq:lip-step-2} as follows,
\begin{align}
    \sup_{\beta \in \cB} &\left |\left(\sqrt{C - \norm{\hat{\beta}^{\cS}}_2^2}  + \norm{\PSperpM \beta}_2\right)^2 
    - \left(\sqrt{C - \norm{\betastarS}_2^2}  + \norm{\PSperpMpop \beta}_2\right)^2\right| \notag\\
    = &\ \sup_{\beta \in \cB} \left|
    C - \norm{\hat{\beta}^{\cS}}_2^2
    + \norm{\PSperpM \beta}_2^2 
    + 2 \sqrt{C - \norm{\hat{\beta}^{\cS}}_2^2}\ \norm{\PSperpM \beta}_2\right.\notag\\
    &\phantom{\sup_{\beta \in \cB}\ }\left.
    - C + \norm{\betastarS}_2^2 
    - \norm{\PSperpMpop \beta}_2^2
    - 2 \sqrt{C - \norm{\betastarS}_2^2}\  \norm{\PSperpMpop \beta}_2
    \right| \notag\\
    \leq &\ \left|\norm{\hat{\beta}^{\cS}}_2^2 - \norm{\betastarS}_2^2\right|
    + \sup_{\beta \in \cB} \left|\beta^\top (\PSperpM - \PSperpMpop) \beta\right|\notag\\ 
    &+2 \sup_{\beta \in \cB}\left|\sqrt{C - \norm{\hat{\beta}^{\cS}}_2^2}\ \norm{\PSperpM \beta}_2-\sqrt{C - \norm{\betastarS}_2^2}\ \norm{\PSperpMpop \beta}_2\right|\notag\\
    = &\ (I) + (II) + (III). \notag 
\end{align}
By Assumption~\ref{ass:consistency-nuisance}, $(I)$ converges in probability to zero. 
Regarding $(II)$, we have
\begin{align*}
    \sup_{\beta \in \cB} \left|\beta^\top (\PSperpM - \PSperpMpop) \beta\right| 
    \leq 
     \sup_{\beta \in \cB} 
     \norm{\beta}_2^2\
     \norm{\PSperpM - \PSperpMpop}_F
     \stackrel{\prob}{\to}0,
\end{align*}
where the inequality follows from Cauchy--Schwarz and that $\norm{A}_2 \leq \norm{A}_F$, and the convergence in probability follows from Assumption~\ref{ass:consistency-nuisance} along with the compactness of~$\cB$.
It remains to upper bound $(III)$. We have that
\begin{align}
% \begin{split}
    \frac{(III)}{2} 
 \leq 
 &\
 \sup_{\beta \in \cB}
 \left|
 \sqrt{C - \norm{\hat{\beta}^{\cS}}_2^2}\ \norm{\PSperpM \beta}_2
  - \sqrt{C - \norm{\betastarS}_2^2}\ \norm{\PSperpM \beta}_2
 \right| \notag\\
 &+
  \sup_{\beta \in \cB}
 \left|
 \sqrt{C - \norm{\betastarS}_2^2}\ \norm{\PSperpM \beta}_2
  - \sqrt{C - \norm{\betastarS}_2^2}\ \norm{\PSperpMpop \beta}_2
 \right| \notag\\
 \leq &\
 \left(\sup_{\beta \in \cB} \norm{\beta}_2\ \norm{\PSperpM}_F\right)\
 \left|
 \sqrt{C - \norm{\hat{\beta}^{\cS}}_2^2} 
 - 
 \sqrt{C - \norm{\betastarS}_2^2}
 \right|
 \notag\\
 &+\sup_{\beta \in \cB}
 \left|
 \sqrt{\beta^\top \PSperpM  \beta}
 -
 \sqrt{\beta^\top \PSperpMpop \beta}
 \right|
 \left(
 \sqrt{C - \norm{\betastarS}_2^2}\right)\notag\\
 \leq &\
 C_3 
 \left| \norm{\betastarS}_2^2 - \norm{\hat{\beta}^{\cS}}_2^2
 \right|^{1/2}
 + 
 \sqrt{C} 
 \sup_{\beta \in \cB} 
  \left|
 \beta^\top ( \PSperpM- \PSperpMpop) \beta
  \right|^{1/2}
  \label{eq:bound-sqrt-1}\\
  \leq &\
   C_3 
 \left| \norm{\betastarS}_2^2 - \norm{\hat{\beta}^{\cS}}_2^2
 \right|^{1/2}
 + 
 \sqrt{C} 
 \left(
    \sup_{\beta \in \cB} \norm{\beta}_2^2\ \norm{ \PSperpM- \PSperpMpop}_F
 \right)^{1/2} \stackrel{\prob}{\to} 0.\label{eq:bound-sqrt-2}
 % \end{split}
\end{align}
The inequality in~\eqref{eq:bound-sqrt-1} follows from the compactness of $\cB$, the fact that $\PSperpM$ has bounded eigenvalues, and that $|\sqrt{x} - \sqrt{y}| \leq |x - y|^{1/2}$ for all $x, y \geq 0$.
The inequality in~\eqref{eq:bound-sqrt-2} follows from Cauchy--Schwarz and that $\norm{A}_2 \leq \norm{A}_F$.
The convergence in probability follows form Assumption~\ref{ass:consistency-nuisance} and the compactness of~$\cB$.



% \begin{align}\label{eqn:well-sep}
%     \inf\left\{\LL(\beta, \varphi_0) \colon \norm{\beta - \beta_0}_2 > \delta \right\} > \LL(\beta_0, \varphi_0).
% \end{align}
% Moreover, by compactness of $\cB$, continuity of $\beta \mapsto g_{\beta, \varphi_0}(x, y)$ and the fact that $\EE[\sup_{\beta \in \cB} g_{\beta, \varphi_0}(\Xref, \Yref)] < \infty$, it follows from, e.g., \cite{van2000asymptotic} that
% \begin{align}\label{eqn:GC}
%     \sup_{\beta \in \cB} |\LL_n(\beta, \varphi_0) - \LL(\beta, \varphi_0)| \stackrel{\prob}{\to} 0.
% \end{align}
% Finally, by Lemma~\ref{lm:lipschitz-type}, there exists a positive $K < \infty$ such that for all $\varphi_1, \varphi_2$ it holds    \begin{align}\label{eqn:lipschitz}
%     \sup_{\beta \in \cB} |\LL(\beta, \varphi_1) - \LL(\beta, \varphi_2)| \leq K d(\varphi_1, \varphi_2).
% \end{align}
% From~\eqref{eqn:well-sep} and~\eqref{eqn:lipschitz} it follows that for any $\delta > 0$ there exist $\varepsilon_1 > 0$ and $\varepsilon_2 > 0$ such that
% \begin{align*}
%     \prob\left(d(\hat\beta, \beta_0) > \delta\right) 
%     \leq 
%     \prob \left(\left|\LL(\hat{\beta}, \hat{\varphi}) - \LL(\beta_0, \hat{\varphi})\right| > \varepsilon_1 \right) + \prob\left(d(\hat{\varphi}, \varphi_0) > \varepsilon_2\right).
% \end{align*}
% The second term on the right-hand side vanishes by the consistency of $\hat{\varphi}$ from Proposition~\ref{prop:consistency-nuisance}. It remains to show that $\LL(\hat\beta, \hat{\varphi}) - \LL(\beta_0, \hat{\varphi})$ converges in probability to zero.
% To do so, notice that
% \begin{align}
%     \left|\LL(\hat\beta, \hat\varphi) - \LL(\beta_0, \hat\varphi)\right| 
%     \leq &\ \left|\LL(\hat\beta, \varphi_0) - \LL(\beta_0, \varphi_0)\right|
%     \label{eqn:bound-1}\\
%     &\ + \left|\LL(\beta_0, \varphi_0) - \LL(\beta_0, \hat\varphi)\right| + \left|\LL(\hat\beta, \hat\varphi) - \LL(\hat\beta, \varphi_0)\right|.
%     \label{eqn:bound-2}
% \end{align}
% The terms in~\eqref{eqn:bound-2} vanish by~\eqref{eqn:lipschitz} and the consistency of $\hat\varphi$ by Proposition~\ref{prop:consistency-nuisance}.
% It remains to show that the term on the right-hand side of~\eqref{eqn:bound-1} vanishes. We have that
% \begin{align}
%   0 \leq &\ \LL(\hat\beta, \varphi_0) - \LL(\beta_0, \varphi_0) \nonumber \\
%   = &\ 
%   [\LL(\hat\beta, \varphi_0) - \LL(\beta_0, \varphi_0)] - [\LL_n(\hat\beta, \varphi_0) - \LL_n(\beta_0, \varphi_0)] + [\LL_n(\hat\beta, \varphi_0) - \LL_n(\beta_0, \varphi_0)] \label{eqn:bound-3}\\
%   \leq &\ [\LL(\hat\beta, \varphi_0) - \LL_n(\hat\beta, \varphi_0)] - [\LL(\beta_0, \varphi_0) - \LL_n(\beta_0, \varphi_0)] \stackrel{\prob}{\to} 0, \label{eqn:bound-4}
% \end{align}
% where the last inequality holds since the last term in~\eqref{eqn:bound-3} is at most zero, and the convergence in probability in~\eqref{eqn:bound-4} follows from~\eqref{eqn:GC}.
% }

% \Nicola{
% \begin{lemma}\label{lm:lipschitz-type}
% There exists a positive $K < \infty$ such that for all $\varphi_1, \varphi_2$ it holds    \begin{align}\label{eqn:lipschitz}
%     \sup_{\beta \in \cB} |\LL(\beta, \varphi_1) - \LL(\beta, \varphi_2)| \leq K d(\varphi_1, \varphi_2).
% \end{align}
% \Nicola{define metric $d$}
% \end{lemma}
% \begin{proof}
% We have that
% \begin{align}
%     \frac{1}{\gamma} & \sup_{\beta \in \cB}|\LL(\beta, \varphi_1) - \LL(\beta, \varphi_2)|
%     \leq
%     \sup_{\beta \in \cB}\left|
%     \norm{S_1^\top(b_1 - \beta)}_2^2
%     - \norm{P_{S_0, M}(b_2 - \beta)}_2^2
%     \right| \label{eq:lip-step-1}
%     \\
%     & + 
%     \sup_{\beta \in \cB}\left |\left(\sqrt{C - \norm{b_1}_2^2}  + \norm{R_1^\top \beta}_2\right)^2 
%     - \left(\sqrt{C - \norm{b_2}_2^2}  + \norm{R_2^\top \beta}_2\right)^2\right|
%     \label{eq:lip-step-2}
% \end{align}
% We can upper bound~\eqref{eq:lip-step-1} as follows,
% \begin{align}
%     &\sup_{\beta \in \cB}\left|
%     \norm{S_1^\top(b_1 - \beta)}_2^2
%     - \norm{P_{S_0, M}(b_2 - \beta)}_2^2
%     \right| \notag\\
%     = &\ 
%     \sup_{\beta \in \cB}
%     \left|(b_1 - \beta)^\top S_1S_1^\top (b_1 - \beta) 
%     - (b_2 - \beta)^\top P_{S_0, M}^2 (b_2 - \beta) \right| \notag\\
%     = &\
%     \sup_{\beta \in \cB}\left|(b_1 - \beta)^\top S_1S_1^\top (b_1 - b_2)
%     + (b_1 - b_2)^\top S_1S_1^\top (b_2 - \beta) \right. \notag \\
%     & \left. \quad\quad + (b_2 - \beta)^\top (S_1S_1^\top - P_{S_0, M}^2) (b_2 - \beta) \right| \notag\\
%     \leq&\ 
%     2 \sup_{\beta \in \cB} \norm{b_1 - \beta}_2\ \norm{S_1S_1^\top}_F\ \norm{b_1 - b_2}_2
%     + \sup_{\beta \in \cB} \norm{b_2 - \beta}_2^2\ \norm{S_1S_1^\top - P_{S_0, M}^2}_F \label{eq:bound-1-1}\\
%     \leq &\ C_1 \norm{b_1 - b_2}_2 + C_2 \norm{S_1S_1^\top - P_{S_0, M}^2}_F, \label{eq:bound-1-2}
% \end{align}
% where~\eqref{eq:bound-1-1} follows from the Cauchy--Schwarz inequality and that $\norm{A}_2 \leq \norm{A}_F$, and~\eqref{eq:bound-1-2} follows from compactness of $\cB$. Furthermore, we can upper bound~\eqref{eq:lip-step-2} as follows,
% \begin{align}
%     ...
% \end{align}
% \Nicola{
% Use that $|\sqrt{x} - \sqrt{y}| \leq |x - y|^{1/2}$.
% }
% \end{proof}

% % \Nicola{found an issue in the proof above. A quick fix: assume the nuisance parameters are known -- then the proof shortens quite a lot.
% % Alternative: try briefly to fix the issue.
% % }
% % \begin{proposition}
% % % Let $\Ecaltrain = \{0, \dots, K\}$ and denote by $e = 0$ the reference environment. For each environment $e \in \Ecaltrain$, define the samples $\cD_e \coloneqq \{(X_i, Y_i)\}_{i = 1}^{n_e}$ and $\cD \coloneqq \cup_{e \in \Ecaltrain} \cD_e$. Partition the sample as $\cD = \cD_\beta \cup \cD_\varphi$, where $\cD_\beta \subseteq \cD_0$ is a random subset of the reference sample used to estimate $\beta$ and $\cD_\varphi \coloneqq \cD \setminus \cD_\beta$.
% % For fixed constants $C > 0$ and $\gamma > 0$, 
% % define the robust identifiable risk for all parameters of interest $\beta \in \R^d$ and nuisance parameters $\varphi \coloneqq (P_S, P_R, b)$ as
% % \begin{align}\label{eqn:rob-loss-ell}
% %     \LL(\beta, \varphi)  \coloneqq 
% %     \EE_{\prob_0}\left[g_{\beta, \varphi}(X, Y)\right],
% % \end{align}
% % where $\prob_0$ denotes the joint distribution of $(X, Y)$ in the reference environment and the function $g_{\beta, \varphi}$ is defined as
% % \begin{align}
% %     g_{\beta,\varphi}(x, y) 
% %   \coloneqq (y - \beta^\top x)^2 
% %   + \gamma \norm{P_S^\top(b - \beta)}_2^2
% %   + \gamma \left(\sqrt{C - \norm{b}_2^2}  + \norm{P_R^\top \beta}_2\right)^2.
% % \end{align}
% % Define the true nuisance parameter $\varphi_0 \coloneqq (P_{S_0, M}, \PSperpMpop, \betastarS)$ and the worst-case robust predictor as
% % \begin{align}
% %     \betarobpi \coloneqq \argmin_{\beta \in \cB} \LL(\beta, \varphi_0),
% % \end{align}
% % where $\cB \subseteq \R^d$ is a compact set.
% % Denote by $\hat\varphi = (\hat{P}_{S,M}, \hat{P}_{R,M}, \hat{\beta}^{\cS})$ the estimated nuisance parameter obtained from the estimation sample $\cD\coloneqq \{(X_i, Y_i)\}_{i = 1}^{n}$ and define the empirical worst-case robust risk as
% % \begin{align}
% %     \LL_n(\beta, \hat\varphi) \coloneqq \frac{1}{|\cD_0 |} \sum_{i \in \cD_0}  g_{\beta, \hat\varphi}(X_i, Y_i),
% % \end{align}
% % where $\cD_0 \coloneqq \{(X_i, Y_i)\}_{i = 1}^{n_0} \subseteq \cD$ denotes the sample from the reference environment.
% % Moreover, define the estimator of the worst-case robust predictor as
% % \begin{align}
% %     \betarobpihat \coloneqq \argmin_{\beta \in \cB} \LL_n(\beta, \hat\varphi).
% % \end{align}