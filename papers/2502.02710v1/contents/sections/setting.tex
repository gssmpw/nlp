%\julia{check}
In this section, we state the concrete distributional setting on which we introduce our partial identifiability framework. In particular, we consider a data generating process, motivated by  structural causal models (SCMs), that allows for hidden confounding, i.e., spurious correlations between the covariates $X$ and the target $Y$. We describe the structure of the distribution shifts occurring in the training and test environments, which is reminiscent of  interventions in causal models.
%our structural assumptions on the training and test distributions. 
Finally, we introduce our framework for distributional robustness that allows for partial identifiability and define the \emph{\idRR} â€“ for any given model, it corresponds to the maximum robust risk among all possible robust risks compatible with the training distributions. 
\subsection{Data distribution and a model of additive  environmental shifts}\label{sec:training-data}

\textbf{Data generating process (DGP).} We first describe the data-generating mechanism that underlies the distributions of all environments $e \in \Ecal$ that may occur during train or test time.
%Let $\Ecal = \Ecaltrain \cup \{\text{test}\}$ denote a finite set of environments from which the data are collected.
For each environment $e \in \Ecal$, we observe the random vector  $(\Xe, \Ye) \sim \PeXY$ consisting of input covariates $\Xe \in \R^d$ and  a target variable $\Ye \in \R$ which satisfy the following data generating process: 
\begin{align}
\label{eqn:SCM}
\begin{split}
    \Xe &= \Ae + \eta, \\ 
    \Ye &= {\betastar}^\top \Xe + \xi,
\end{split}
\end{align}
where $\Ae \in \R^d$, $(\eta,\xi) \in \R^{d+1}$ are random vectors such that $\Ae \sim \PeA$, $(\eta,\xi) \sim \Petaxi$ with finite first and second moments, and $\Ae \independent (\eta, \xi)$ for all $e \in \Ecal$. 
%\fy{why don't we actually have a set of possible test environments (and hence something like Etest here?},\fy{and then add sth about test here?}\julia{i am very unsure how to add this here without confusing everyone -- we only have Mtest so far, an arbitrary Etest would complicate matters}\fy{at this point we don't even have Mtest but its just weird that we talk about training distributions but not test distributions at this point. Maybe we can get rid of that part as well and move it to later}

%The vectors $\Ae$ represent (varying) additive shifts across different environments, whereas 
Note how in this setting, the parameter $\betastar$ and the distribution $\Petaxi$ of the noise vector $(\eta,\xi)$ remain invariant across environments. Without loss of generality, we assume that the noise $(\eta,\xi)$ has mean zero. Our linear setting
%This additive shift setting 
is, in general, more challenging than the standard linear regression setting where $\eta \independent \xi$: due to possible dependencies between $\eta$ and $\xi$ (induced by, e.g., \emph{hidden confounding/spurious features}), classical estimators, such as the ordinary least squares, are biased away from the true parameter~$\betastar$. 
Denote by $\Sigmastar \coloneqq \Cov((\eta,\xi))$ the joint covariance of the noise vector $(\eta,\xi)$, which can be written in block form as  $\Sigmastar = \begin{pmatrix}\noisecovxxstar& \noisecovxystar\\{\noisecovxystar}^\top&\noisecovyystar\end{pmatrix}$ and which we assume to be full-rank. In the following, we denote the concatenation of these two invariant parameters by $\thetastar := (\betastar, \Sigmastar) \in \Theta \subset  \R^{d}\times \R^{(d+1)\times(d+1)}$, where $\Theta$ is an appropriate parameter space. 
%of our semi-parametric
%\Nicola{I would be careful with the semi-parametric term since it's quite precise in statistics and the people working on this have very strong opinions.} \fy{i also think we might not need to use this}
%DGP, which are invariant across all environments. 

\textbf{Structure of the distribution shifts.}
Note that in the DGP, the distribution shifts across $\PeXY$ are induced solely by changes in the distribution of the variable $\Ae$, whose mean and covariance matrix we denote by $\mu_e \coloneqq \EE[\Ae]$ and $\covAe \coloneqq\Cov [\Ae]$, respectively. 
In general, we allow for degenerate and deterministic shifts, i.e. the covariance $\covAe$ can be singular or zero.  We remark that although the additive shift structure in \cref{eqn:SCM} allows the joint distribution $\PeXYA= \PeA \times \PXYgivenA$ to change solely via $\PeA$, our distribution shift setting is more general than covariate shift: due to the noise variables $\eta$ and $\xi$ being potentially dependent, both the marginal $\PeX$ and the conditional distribution $\PeYgivenX$ can change across environments. %\fy{We assume that during test time we have we have this Mtest }

\textbf{Training and test-time environments.} 
Throughout the paper, we assume that we are given the collection of training distributions $\probtrainarg{\thetastar} = \{ \PeXYarg{\thetastar} \}_{e \in \Ecaltrain}$, where $\Ecaltrain$ denotes the index set of training environments. We omit $\thetastar$ when it is clear from the context. Further, for ease of exposition, we  assume that $\Ecaltrain$ contains a reference (unshifted) environment $e = 0$ with $A_0 = 0$ a.s. In \cref{sec:apx-extension}, we discuss how our results apply if this condition is not met.
During test time, we expect to observe a new, previously unseen distribution $\probtestXY$ which is induced by the DGP \eqref{eqn:SCM} and a shift random variable $\Atest \sim \probtestA$, with corresponding finite mean $\mutest$  and covariance $\Sigmatest$.

% \begin{align}\label{eqn:testSCM}
% \begin{split}
%     \Atest &\sim \probtest_A; \\
%     (\eta, \xi) &\sim \cN(0, \Sigma); \\
%     \Xtest &= \Atest + \eta; \\
%     \Ytest &= \betastar^\top \Xtest + \xi.
% \end{split}
% \end{align}
%We denote the joint distribution of $(\Atest, \Xtest, \Ytest)$ by $\probtest$. 
%Again, by setting $\Sigmatest = 0$, one obtains deterministic mean shifts following the point distribution ($\probtest_A = \delta_{\Atest}$). 
% \nico{[needs clarification.
% We always observe $E$. I guess you refer to $A_e$?]}
%\fy{not sure how other works deal with weights / proportions of environments?}
% \subsection{Test data}\label{sec:test-data}
% \fy{is it possible not to talk about training vs. test time SCM (to save space and headspace) it seems like we don't need it anyway}
% During test time, we observe data $(\Xtest, \Ytest) \in \R^{d+1}$ which follow a new, previously unseen distribution $\probtest_{X,Y}$. The test data are generated by an SCM analogous to \cref{eqn:SCM}:
% \begin{align}\label{eqn:testSCM}
% \begin{split}
%     \Atest &\sim \probtest_A; \\
%     (\eta, \xi) &\sim \cN(0, \Sigma); \\
%     \Xtest &= \Atest + \eta; \\
%     \Ytest &= \betastar^\top \Xtest + \xi.
% \end{split}
% \end{align}
% We denote the joint distribution of $(\Atest, \Xtest, \Ytest)$ by $\probtest$. As during training, we require $\probtest_A$ to have finite mean and covariance $\mutest$ and $\Sigmatest$. Again, by setting $\Sigmatest = 0$, one obtains deterministic mean shifts following the point distribution ($\probtest_A = \delta_{\Atest}$). 
% Note that the data-generating mechanism in both \cref{eqn:SCM} and \cref{eqn:testSCM} is \emph{uniquely determined } by the model parameters $ \theta := (\beta, \Sigma) $, which stay invariant across all training and test environments, as well as the probability distribution of the shift $A$, which varies across environments. \\
% \paragraph{Structural assumptions on the test-time distribution shift.} 
Even though we do not have access to $\probtestXY$ during training,
the practitioner might have some information about the possible
%we do assume partial knowledge about the 
%directions and sizes of  
shift distributions $\probtestA$ that may occur during test time. 
As an example, we may 
%for instance, in case of deterministic mean shifts (corresponding to $\Sigmatest = 0$), one 
only have information about the maximum possible magnitude and approximate direction of the test-time mean shift $\EE[\Atest]$. 
%\fy{this point was highly contended in this talk, hence one more sentence} 
%the shift variable $\probtestA$,
%test shifts \fy{should we generally say shifts or interventions?} which induce $\probtestA$,
%We do not know the test distribution of the data in advance. However, we can express partial knowledge about the test distribution by imposing a boundedness condition on the test shift as follows:
% in that \julia{why "in that"}
%We model this knowledge via a \emph{shift upper bound}, 
%Mathematically, we could represent more general 
%\julia{check} 
In this work, we assume that we are given 
an upper bound on the second moment of the shift variable, represented by a positive semidefinite (PSD) matrix $\Mtest\succeq 0$ such that 
% \nico{
% in many scenarios, however, the practitioner would like to be able to specify the directions and strengths of perturbations to protect from, which is equivalent to requiring   of perturbations might want to specify an upper bound on the perturbations that might be encountered at test time, that is, 
% }
% \fy{shorten - we just need the Pmtest definition, can be shortened to about three lines?}
% However, in many scenarios, the practitioner has some background knowledge on the expected strength and direction of the shift (for instance, for a study done on the same patients years after, the mean shift of the age variable is known, and the variance shift is equal to zero (assuming no one died)). This corresponds to the so-called \emph{finite robustness} setting. 
% In general, considering finite-strength shifts allows us to conduct a more fine-grained analysis of distributional robustness than resorting to an \emph{infinite robustness} setting where covariates are either stable or infinitely perturbed. We formulate our boundedness condition on the test distribution shift as follows:
% \vspace{-0.1in}
\begin{align}\label{eqn:testAbound}
    % \EE_{\mathrm{test}}
    \EE[ \Atest {\Atest}^\top] = \Sigmatest + \mutest {\mutest}^\top \preceq \Mtest.
\end{align}
In practice, there may be different degrees of knowledge of the feasible set of shifts -- when no knowledge is available, one can always choose the most "conservative" bound $\Mtest$ with the range equal to $\R^{d}$ and large eigenvalues. The more information is available, the smaller the feasible set of test distributions becomes. For instance, when 
%\fy{to do after dinner: not sure I'd write it this way} If 
the test distribution 
$\prob_{\text{test}}^X$
of $X$ is available during training (as in the \emph{domain adaptation} setting \cite{shimodaira2000improving}), one can directly compute the optimal shift upper bound via $\Mtest =  \EE[\Xtest {\Xtest}^\top] $.
%which can be then plugged into our evaluation and optimization framework (cf. \cref{sec:prop-invariant-set}). \fy{wasn't sure we needed this}
In existing literature, $\Mtest$ is often proportional to the pooled first or second moment of the training shifts, for instance $\Mtest = \gamma \sum_{e \in \Ecaltrain} w_e \mu_e \mu_e^\top$ in discrete anchor regression \cite{rothenhausler2021anchor} or $\Mtest = \gamma \sum_{e \in \Ecaltrain} w_e (\mu_e \mu_e^\top + \Sigma_e)$ in causality-oriented robustness with invariant gradients \cite{shen2023causalityoriented}.
Here, $w_e$ are the weights representing the probability of a datapoint being sampled from the environment $e$. As will become apparent in the next sections, our population-level results are not impacted by the weights $w_e$, which we thus omit in the following.
% JULIA: PUT SOMEWHERE BELOW
% In the following, we consider the distributional robustness setting in which \emph{partial knowledge} about test shifts is given in form of their \emph{maximum strength} $\gamma$ and \emph{general direction} $\cM \subseteq \R^d$.  We can then formalize this partial knowledge by setting $
%     \Mtest = \gamma \projM,$
% where $\gamma > 0$ and $\projM$ is an orthogonal projection onto the subspace $\cM$.
%\footnote{When more refined information on the test shifts is given by a general PSD matrix $\Mtest \in \R^{d \times d}$, we can replace it by the upper bound $\Mtest \preceq \lambda_{\max}(\Mtest) \Pi_{\range (\Mtest)}$ and apply our results on the upper bound.}
% \fy{moved footnote outside}
% JULIA: PUT SOMEWHERE BELOW
% In what follows, we in fact only consider $\Mtest$ of the form $\Mtest = \gamma \projM$, where $\projM$ is the projection matrix for some subspace $\mathcal{M}$. Note that for any given matrix $\Mtest\succeq 0$ not of this form, we can use the upper bound $\lambda_{\max}(\Mtest) \Pi_{\range (\Mtest)}\succeq \Mtest$.  \fy{whats a bit weird is that later in the anchor setting I think we don't do this projection thing? but instead go with M-anchor?}

%\fy{i'm not certain this business with mtest and gamma Pi is necessary}
%  For instance, in a problem with 5 covariates, our domain knowledge could be that only covariates $X_1, X_2$ can be shifted during testing by strength at most $\gamma$. 
% %at most $3$ and $5$  correspondingly.
% \fy{why different? if simplistic then maybe both the same, also a bit more detail?} 
% (More refined information on the test shift given by a general upper bound $\Mtest \in \R^{d \times d}$ can be replaced by $\Mtest \preceq \lambda_{\max}(\Mtest) \Pi_{\spn (\Mtest)}$ to use our results.)


% $\thetastar := (\betastar, \Sigmastar) \in \Theta \subset  \R^{d}\times \R^{(d+1)\times(d+1)}$ $\Sigmastar$ the covariance 
% The DGP's unobserved parameters $\thetastar := (\betastar, \Sigmastar) \in \Theta \subset  \R^{d}\times \R^{(d+1)\times(d+1)}$ are invariant across environments. The joint covariance $\Sigmastar$ of $(\eta, \xi)$ can be written in block form as $\Sigmastar = \begin{pmatrix}\noisecovxxstar& \noisecovxystar\\{\noisecovxystar}^\top&\noisecovyystar\end{pmatrix}$ and is not restricted to be diagonal, allowing for hidden confounding between $X_e$ and $Y_e$.

We now provide an example based on structural causal models (SCM) that falls under the aforementioned distrubtion shift setting. 
%In the following example, we demonstrate that our distribution shift model includes linear structural causal models (SCMs) with hidden confounding and intervened covariates.
\begin{example}
    Consider the structural causal model \eqref{eqn:SCM-new} and its induced graph in \cref{fig:ex-scm}. In this model, interventions on the variable $Z$ correspond to soft interventions on the covariates $X$. Additionally, the exogenous noise vector $(\varepsilon_X, \varepsilon_Y, \varepsilon_H)$ and the intervention variable $Z$ are mutually independent. This model is the basis of multiple causality-oriented robustness works, e.g. \citep{rothenhausler2021anchor,shen2023causalityoriented}.
    Let $\betastar \coloneqq B_{YX}^\top$ and $\xi \coloneqq B_{YH}H + \varepsilon_Y$. Then, from~\eqref{eqn:SCM-new}, we obtain $Y = B_{YX} X + (B_{YH} H + \varepsilon_Y)=X^\top\betastar + \xi$.
    Suppose that $\mathbf{I} - \mathbf{B}$ is invertible and let $\mathbf{C}\coloneqq (\mathbf{I} - \mathbf{B})^{-1}$with entries $C_{XX}, C_{XY}$, etc. Define $A \coloneqq C_{XX} Z$ and $\eta \coloneqq C_{XX} \varepsilon_X + C_{XY} \varepsilon_Y + C_{XH} \varepsilon_H$. Then, we can write $X = A + \eta$.
    Since shifts in the distribution of $Z$ induce shifts in the distribution of $A$, a collection of interventions $\{Z_e\}_{e \in \Ecaltrain}$ translates into a collection of additive shifts $\{A_e\}_{e\in\Ecaltrain}$ and gives rise to training distributions varying with the environment $e$. In summary, our DGP \eqref{eqn:SCM} includes the classical setting of causality-oriented robustness as depicted in \cref{fig:ex-scm}. 
    % \vspace{-0.1in}
    \begin{figure}[h!]\label{fig:fig1setting}
\begin{subfigure}{.55\textwidth}
\begin{align}
\label{eqn:SCM-new}
\begin{split}
    \begin{pmatrix}
        X\\
        Y\\
        H
    \end{pmatrix}
    =
    \underbrace{\begin{pmatrix}
        B_{XX} & B_{XY} & B_{XH}\\
        B_{YX} & 0 & B_{YH}\\
        0 & 0 & 0
    \end{pmatrix}
    }_{
    \mathbf{B}
    }
    \begin{pmatrix}
        X\\
        Y\\
        H
    \end{pmatrix}
    +
    \begin{pmatrix}
        Z + \varepsilon_X\\
        \varepsilon_Y\\
        \varepsilon_H    
    \end{pmatrix}
\end{split}
\end{align}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \begin{tikzpicture}
  % Define styles
  \tikzset{
    observed/.style={circle, draw=black, fill=white, thick, minimum size=7mm},
    latent/.style={circle, draw=gray, fill=white, dashed, thick, minimum size=7mm},
    arrow/.style={->, thick, >=stealth},
    bidirectional/.style={<->, thick, black}
  }

  % Nodes
  \node[observed] (X)                        {$X$};
  \node[latent, above left=.6cm of X] (A) {$Z$};
  \node[latent, above right=.6cm of X] (H) {$H$};
  \node[observed, below right=.6cm of H] (Y) {$Y$};  

  % Paths
  \draw[arrow] (A) -- (X);
  \draw[arrow] (X) to[bend left=20] (Y);
  \draw[arrow] (Y) to[bend left=20] (X);
  \draw[arrow] (H) -- (X);
  \draw[arrow] (H) -- (Y);
\end{tikzpicture}
  % \caption{A subfigure}
\end{subfigure}%
\caption{\small{(Left) SCM with hidden confounding and (right) induced graph. The model allows for an arbitrary causal structure of the observed variables $(X,Y)$, as long as $\mathbf{I} - \mathbf{B}$ is invertible, e.g. when the underlying graph is acyclic. The shifts across different distributions are captured via shift interventions on $X$. However, the model does not allow for interventions on the target variable $Y$ or hidden confounders $H$.}}
\label{fig:ex-scm}
% \vspace{-10pt}
\end{figure}
\end{example}
% \paragraph{Assumptions on distribution shifts.}\julia{include info from this par before the example and in the example -> delete paragraph}
% % We assume that the distribution shifts in $(\Xe, \Ye)$ are induced by changes in the mean vector $\EE[\Ae] = \mu_e$ and covariance matrix $\Cov [\Ae] = \covAe$ of the exogenous random vector $\Ae$. 
% % In general, we allow for degenerate shifts, i.e. the covariance $\covAe$ can be singular.  
% % For ease of exposition, we  assume that $\Ecaltrain$ contains a reference environment $e = 0$ satisfying $\mu_0 = 0$ and $\Sigma_0 = 0$.  In \cref{sec:apx-extension}, we discuss how our results apply if this condition is not met.
% Our additive shift structure implies that the joint distribution $\PeXYA= \PeA \times \PXYgivenA$ may change across environments only via $\PeA$. 
% Moreover, shifts in $\PeA$ together with the hidden confounding between $\Xe$ and $\Ye$ induce changes in both $\PeX$ and $\PeYgivenX$, allowing for both covariate shifts and concept shifts.

% Our DGP models distributional shifts that are considered in the related literature. For instance, letting $| \Ecaltrain | = 1$ and $A \sim \cN(0, M \Sigma_A M^\top)$, where $M \in \R^{d \times q}, \Sigma_A \in \R^{q \times q}$, yields the setup of continuous anchor regression \cite{rothenhausler2021anchor}\footnote{Note that in the anchor regression setup, the environment shift $A$, called \emph{anchor}, is also observed, thus the training data consist of $(A, X, Y)$.}. 
% Discrete anchor regression \cite{rothenhausler2021anchor} corresponds to a finite environment set 
% $\Ecaltrain = [m], m \in \bN$, and deterministic mean shifts $\Ae = \mue \in \R^d$. 
% The more general additive shift setting in \cite{shen2023causalityoriented} corresponds to $\Ecaltrain = [m]$ and 
% $\Ae \sim \cN(\mue, \Sigmae)$. \fy{here it would be nice to say when we also need Gaussian and when we don't?} Note that in the above works, the environment index is modeled as a random variable  $E \sim \prob^E$ through which one assigns weights to different training environments. 
% The results in this paper only depend on the support of $\prob^E$, that is, whether an environment was seen or not. Thus, the population-level guarantees -- the focus of this paper -- are the same for any distributions with the same support on $\prob^E$. 

% \textbf{Data distribution.} We are given multiple training environments indexed by $e \in \Ecaltrain$, where $\Ecaltrain$ is a countable
% environment index set. For each training environment $e$, we observe data  $(\Xe, \Ye) \sim \PeXY$ consisting of input covariates $\Xe \in \R^d$ and  the target variable $\Ye \in \R$, which follow the linear model in \eqref{eqn:SCM}. \fy{could put the equations into the text, outside figure, move DAGs to appendix with concrete examples?}
% This linear model can easily arise in settings where the causal graph is acyclic and \fy{when exactly?}, see \fy{Appendix bla, where we give a few examples?}. 
% %are generated by the linear structural causal model (SCM) \eqref{eqn:SCM} 
% \fy{discuss and revise. In my head its clearer to think of any SCM (with explicit B notation?) that can be written as (3) where eta,xi independent of A, such as certain acyclic graphs/SCM (corresponding to B notation, trivially includes anchor examples with only x to y, but also y to some x), or certain linear dist shift situations (as we discussed with Julia) - do we have a concrete example there?} 
% %and its corresponding causal graph, depicted in \Cref{fig:fig1setting}.
% %In this paper, we focus our discussion on the population setting where
% %In the following, we consider the population setting, in which 
% Throughout the paper, we assume that we observe the collection of training distributions $\probtrainarg{\thetastar} = \{ \PeXYarg{\thetastar} \}_{e \in \Ecaltrain}$,
% omitting $\thetastar$ when it is clear from the context. We discuss the corresponding finite sample setting in \Cref{sec:apx-empirical-estimation}.

% % We consider a setting in which we are given data  $(A_i,X_i,Y_i)$ from multiple training environments from a (continuous or discrete) set $\Ecaltrain$.
% % \fy{i.e. environment variable is not observed?}

% %$\{ (X^e, Y^e): e \in \Ecaltrain  \}$. \fy{this looks like there are as many samples as there are environments? } 
% %are distributed according to a (continuous or discrete) random variable $\Etrain \in \Ecaltrain$ that follows the distribution $\bP_{\Etrain}$. 
% % We denote by $(X^e, Y^e)\in \R^{d+1}$ the data generated from environment $e$
% %$(X, Y) \in \R^{d+1}$ conditioned on $\Etrain = e$.  For each environment, the data are generated 
% % \begin{minipage}[t]{.6\textwidth}
% % \vspace{0pt} 
% % \centering
% % \begin{tikzpicture}
% %   % Define styles
% %   \tikzset{
% %     observed/.style={circle, draw=black, fill=white, thick, minimum size=8mm},
% %     latent/.style={circle, draw=gray, fill=white, dashed, thick, minimum size=8mm},
% %     arrow/.style={->, thick},
% %     bidirectional/.style={<->, thick, black}
% %   }

% %   % Nodes
% %   \node[observed] (X) {$X^e$};
% %   \node[latent, above left=of X, xshift=.5cm] (A) {$A^e$};
% %   \node[latent, above right=of X, xshift=-.5cm] (H) {$H$};
% %   \node[observed, below right=of H, xshift=-.5cm] (Y) {$Y^e$};  

% %   % Paths
% %   \draw[arrow] (A) -- (X);
% %   \draw[arrow] (X) -- (Y);
% %   \draw[bidirectional] (H) -- (X);
% %   \draw[bidirectional] (H) -- (Y);
% % \end{tikzpicture}
% % \captionof{figure}{Causal graph corresponding to the SCM in \cref{eqn:SCM}. The observed data ($X$ and $Y$) are shown in solid lines, while unobserved variables (additive shift $A^e$ and hidden confounders $H$) are shown in dashed lines.}
% % \label{fig:causal-model}
% % \end{minipage}
% % \begin{minipage}[t]{.3\textwidth}
% % \vspace{0pt} 
% % \begin{align}\label{eqn:SCM}
% % \begin{split}
% %     \Ae &\sim \PeA; \\
% %     (\eta, \xi) &\sim \cN(0, \Sigmastar); \\
% %     \Xe &= \Ae + \eta; \\ 
% %     \Ye &= \betastar^\top \Xe + \xi.
% % \end{split}
% % \end{align}
% % \end{minipage}
% \begin{figure}[ht]\label{fig:fig1setting}
% \centering
% \begin{subfigure}{.5\textwidth}
%   \centering
%   \begin{tikzpicture}
%   % Define styles
%   \tikzset{
%     observed/.style={circle, draw=black, fill=white, thick, minimum size=8mm},
%     latent/.style={circle, draw=gray, fill=white, dashed, thick, minimum size=8mm},
%     arrow/.style={->, thick},
%     bidirectional/.style={<->, thick, black}
%   }

%   % Nodes
%   \node[observed] (X) {$X_e$};
%   \node[latent, above left=of X, xshift=.5cm] (A) {$A_e$};
%   \node[latent, above right=of X, xshift=-.5cm] (H) {$H$};
%   \node[observed, below right=of H, xshift=-.5cm] (Y) {$Y_e$};  

%   % Paths
%   \draw[arrow] (A) -- (X);
%   \draw[arrow] (X) -- (Y);
%   \draw[bidirectional] (H) -- (X);
%   \draw[bidirectional] (H) -- (Y);
% \end{tikzpicture}
%   % \caption{A subfigure}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
% \begin{align}
% \label{eqn:SCM}
% \begin{split}
%     \Ae &\sim \PeA; \\
%     (\eta, \xi) &\sim \cN(0, \Sigmastar); \\
%     \Xe &= \Ae + \eta; \\ 
%     \Ye &= \betastar^\top \Xe + \xi.
% \end{split}
% \end{align}
% \end{subfigure}
% \caption{\fy{do we want to actually write causal graph in terms of usual B notation and then say, that can be written as eq3. also a bit weird that H appears on the left but not on the right} (Left) Causal graph corresponding to the SCM in \cref{eqn:SCM}. Observed variables $(X_e, Y_e)$ are indicated by solid circles while unobserved variables, namely the additive shift $A_e$ and confounders $H$, are shown in dashed circles. Note that here, bidirectional edges indicate that the relationship between two nodes can be in either direction.}
% \label{fig:fig1setting}
% % \vspace{-10pt}
% \end{figure}
% We assume that the true unobserved parameters  $\thetastar := (\betastar, \Sigmastar) \in \Theta$, where $\Theta \subset  \R^{d}\times \R^{(d+1)\times(d+1)}$ \fy{notation right?}, are invariant across environments. The joint covariance $\Sigmastar$ of the noise $(\eta, \xi)$ can be written in block form as $\Sigmastar = \begin{pmatrix}\noisecovxxstar& \noisecovxystar\\{\noisecovxystar}^\top&\noisecovyystar\end{pmatrix}$. We allow the presence of latent confounders between $X_e$ and $Y_e$, and hence the case where $\noisecovxystar \neq 0$.
% % The general structure of $\Sigmastar$ can capture 
% Note that the \emph{confounded noise setting} is, in general, more challenging than the independent noise setting, since, given any number of environments, common estimators such as the linear regression estimator are biased away from the causal parameter $\betastar$. 

% \paragraph{Additive distribution shift.}
% The distribution shift between environments is modeled by the (random) additive shift $\Ae \in \R^d$ with mean $\EE[\Ae] = \mu_e$ and covariance matrix $\Cov [\Ae] = \covAe$. 
% % \fy{should this also be betastar?}
% % where $\Ae \in \R^d$ is a (random) additive environment shift distributed with mean $\mu_e$ and variance $\covAe$, 
% % %$\beta \in \R^d$ is the causal parameter \fy{do we even needs this naming here?}, and $(\eta, \xi) \sim \cN(0, \Sigma) \in \R^{d+1}$ is the noise vector with variance 
% % and $\Sigma = \begin{pmatrix}\noisecovxx& \noisecovxy\\\noisecovxy^\top&\noisecovyy\end{pmatrix}$ 
% % is the noise covariance, 
% In general, the environment shifts $\Ae$ can be degenerate, i.e. the covariances $\Sigma_e$ are not assumed to be full rank.  For simplicity of presentation, we further assume that $\Ecaltrain$ contains a reference environment $e = 0$ satisfying $\mu_0 = 0$ and $\Sigma_0 = 0$.  In \cref{sec:apx-extension}, we discuss how our results apply if this condition is not met.
% Our additive shift structure implies that the joint distribution $\PeXYA= \PeA \times \PXYgivenA$ \fy{may change} changes in each environment. 
% \fy{this could go in the part where we discuss various causal graphs. I also don't quite get what we mean by intervention here (as A isn't the intervention anymore)}
% \fy{in the context of graph / SCM: Y and "H" can be intervened on via A} However, we do not allow for direct interventions %shifts 
% on $Y$ or the latent confounders, that is $\PXYgivenA$ remains invariant.
% %In other words, the unobserved joint distribution $\PeXYA = \PeA \times \PXYgivenA$ shifts across environments, with only $\PeA$ changing, and $\PXYgivenA$ remaining invariant. 
% Note that our distribution shift setting is \emph{more general than covariate shift}: due to unobserved confounding, the conditional distribution $\PeYgivenX$also varies across environments.
% % (unobserved variables)
% % that may account for additional correlation between $X$ and $Y$ beyond their causal relationship. 
% %An illustration of the causal model is shown in Figure~\ref{fig:causal-model}.
% % \cref{fig:causal_model}. 

% % is a set of plausible causal model parameters. 
% % \fy{structure was a bit scattered - what i tried to do (not yet finished): first discussion of confounding, then discussion of different environments}

% %Note that the data-generating mechanism in \cref{eqn:SCM} is uniquely determined by 

% %If we want to highlight the model parameters that generate $\PeXY$ and $\probtrain$, we write $\PeXYarg{\theta}$ and  $\probtrainarg{\theta}$, respectively. 


% % \fy{we have to say whats observed - i found it super confusing in other papers anyway. maybe one can write the above like this:} The distribution $\probtrain(\cdot| \Etrain =e)$ of a / conditioned on a particular environment from which we observe i.i.d. samples is defined by the following SCM
% % \begin{align}\label{eqn:SCM2}
% % \begin{split}
% %     A \given \{\Etrain=e\} &\sim P_e \\
% %     X \given \{\Etrain=e\} &= A + \eta, \\
% %     Y \given \{\Etrain=e\} &= \betastar^\top X + \xi, 
% % \end{split}
% % \end{align}
% % where $P_e$ is a distribution with mean $\mu_e$ and variance $\covAe$, and blabla.
% % \fy{i think it would be good to put a graphical model closeby, maybe also explain a bit more how this captures hidden confounding in the next paragraph}

% %assume that the joint noise covariance $\Sigma$ is full-rank \fy{this assumption should probably be put somewhere you can find again (in theorem or sth}, but make no further assumptions on its structure. In particular, it can capture the presence of 

% %that may account for additional correlation between $X$ and $Y$ \fy{addition to what?}. 


% % in detail
% % \nico{
% % [Note that the joint distributions $\prob_{A^e, X^e, Y^e} = \prob_{A^e} \times \prob_{(X^e, Y^e) \mid A^e}$ change across environments. However, distribution of $A^e$ $\prob_{A^e} $ can change ``freely'' between environments, While changes in $\prob_{(X^e, Y^e) \mid A^e}$ are constrainted by \cref{eqn:SCM}.
% % ]
% % }
% % \rmnico{allows additive mean and variance shifts}\Nicola{describes heterogeneous distributions} between different environments by  \Nicola{shifting the distribution of} the random vector $\Ae$.
% %represented by the shift variable $\Ae = A \given \Etrain = e$. 

% %, as well as the  , which varies across environments. 
% % \fy{In suggested new notation above that would be $P(Y|X=x, \Etrain=e)$ is not the same for different e but same x...}

% %As a consequence \fy{of what? you discussed many things}, 
% %\fy{merge the two sentences below}
% In summary, our model \eqref{eqn:SCM} describes a multi-environment setting where different training distributions vary by changing the distribution $\PeA$ of the random additive shifts $\Ae$, but
% the model parameters $ \thetastar$ 
% remain invariant across all training and test environments. 
% %First note that the SCM~\eqref{eqn:SCM} 
% % \fy{you don't have all the specifications in that equation atm, so "the" SCM doesn't really model anything concretely?} 
% It can model a variety of multi-environment settings in the related literature. For instance, choosing $| \Ecaltrain | = 1$ and $A \sim \cN(0, M \Sigma_A M^\top)$, where $M \in \R^{d \times q}, \Sigma_A \in \R^{q \times q}$, yields the setup of continuous anchor regression \cite{rothenhausler2021anchor}\footnote{Note that in the anchor regression setup, the environment shift $A$, called \emph{anchor}, is also observed, thus the training data consist of $(A, X, Y)$.}. 
% %is included via continuous environments $\Etrain \sim \N(0, \Sigma_E)$ and deterministic mean shifts $\Ae = M e$.  \fy{didn't get why "deterministic"}
% Discrete anchor regression \cite{rothenhausler2021anchor} corresponds to a discrete environment index set 
% $\Ecaltrain = [m], m \in \bN$, and deterministic mean shifts $\Ae = \mue \in \R^d$. The more general additive shift setting in \cite{shen2023causalityoriented} corresponds to $\Ecaltrain = [m]$ and %additive random shifts 
% $\Ae \sim \cN(\mue, \Sigmae)$. \fy{here it would be nice to say when we also need Gaussian and when we don't?} Note that in the above works, the environment index is modeled as a random variable  $E \sim \prob^E$ through which one assigns weights to different training environments. 
% % \nico{Previous works often modeled the environment variable $E \in \Ecaltrain$ as random, which induces a distribution $\prob_E$ over the environments. For example, in the categorical environment setting, the distribution $\prob_E$ specifies the proportion of the observed environments at training time. 
% % }
% % \rmnico{
% % Note that in the related work, a probability distribution is often imposed on the environment index $e \in \Ecaltrain$. This is supposed to model the proportion of an observed environment in pooled training data. 
% % }
% The results in this paper only depend on the support of $\prob^E$, that is, whether an environment was seen or not. Thus, the population-level guarantees -- the focus of this paper -- are the same for any distributions with the same support on $\prob^E$. 
% %However, for the results in this paper, \fy{it only matters whether ... (because of population - why did weights matter in the others again?} we are solely interested in whether an environment has been observed or not, and hence we do not specify a distribution on $\Ecaltrain$ for simplicity of exposition. Still, our results can be applied in random environment settings by considering the domain $\Ecaltrain$ of the random variable $E$. \par

\subsection{The robust risk}\label{sec:formulation-distributional-robustness}
Our goal is to find a model $\beta \in \R$ using the training distributions
% and $\gamma, \projM$,
that has a small risk
%denotes the risk w.r.t. to the squared loss., 
over the robustness set.
In this paper, we consider as risk function the expected square loss  $\Loss(\beta; \prob) \coloneqq \EE_{\prob} [(Y - \beta^\top X)^2]$.
%(the entire set of shifted test distributions). 
In our setting, given a test shift upper bound $\Mtest$ defined in \Cref{eqn:testAbound}, the robustness set corresponds to
% $\Robsetarg{\thetastar}$, 
%called the %, that we define as
%,with bounded test shifts $\Atest$ (the \emph{robustness set}), defined as 
\begin{align}\label{eqn:robustness-set}
    \Robsetarg{\thetastar} := \{ \probtestXYarg{\thetastar}: \:  \EE [ \Atest {\Atest}^\top ] \preceq \Mtest \},
\end{align}
yielding the corresponding robust risk that reads

% \nico{[a bit unclear what the set ranges over (I guess test distribution?)]}
% \nico{[What about phrasing it from the other way round, i.e. we want to protect from some shifts $\Mtest$, and construct an upper bound from it using $S$ and $R$ (see also below \cref{prop:upper-bound})?]}
% Note how the data distribution in \cref{eqn:SCM} or \cref{eqn:testSCM} 
% is fully determined 
% %can be parameterized 
% by the model parameters $(\beta, \Sigma)$, which stay invariant across all training and test environments, as well as the distribution of the shift $A$, which varies across environments. 
% In general, we do not know $\Atest$ or $\probtest_A$ in advance and would like to find an estimate $\betahat$ such that some loss is small for the entire set of test distributions $\probsettest$ with small shifts A, defined as follows
% \begin{align}
%     \probsettest := \{ \probtest: \:  \EE [ \Atest \Atest^\top ] \preceq \Mtest \}.
% \end{align}


% We define a subspace consisting of all shift directions seen during training:
% \begin{align}
%     \cS := \mybigplus_{e \in \Ecaltrain} \spn (\Sigma_e + \mu_e \mu_e^\top). 
% \end{align}



% \fy{I suggest the above instead of the following, without talking about invariant parameters at this point?}
% \fy{As I understand it, shouldn't it be sth like: P as before with A from a set of distributions which have bounded covariance? (the covariance can also be 0..., i.e. we have deterministic A)}
%  Let us denote by $\probtrain$ the mixture distribution $\bP_{\betastar, \Sigma^*_U, A}$  \fy{why mixture distribution?}
% \fy{not sure I understand the choice of subscripts - A is a random variable, but the others are deterministic quantities?} described by  \cref{eqn:SCM},  and by $\probtest$, the set of all data distributions defined  by \cref{eqn:testSCM} and \cref{eqn:testAbound}:
% \begin{align}
%     \probsettest := \{ \bP_{\betastar, \Sigma^*_U, \Atest}: \: \Atest \sim  \cN(\mutest, \Sigmatest), \: \EE [ \Atest \Atest ] \preceq \Mtest \}.
% \end{align}
% \fy{this is quite confusing to me - this is a ... random set? of distributions? Then what is $P\in\probsettest$? shouldn't sup then be expectation over random draws over P?}
%    \fy{also here you make assumption that sigma xixi is diagonal? seems to be a typo but also no explicit description/assertion of that type?}\julia{i don't understand this comment. }
% \fy{make clear this is for "identified"/known robustness set}
% Assume that the training data were generated according to \cref{eqn:SCM} with (generally unknown) \emph{ground-truth model parameters} $\thetastar := (\betastar, \noisecovxxstar, \noisecovxystar, \noisecovxystar)$. \fy{here we can just use Sigmastar instead of breaking it up?}
% As mentioned above, we would like to find a predictor which performs well on the whole set $\probsettestarg{\thetastar}$ of (unseen) test distributions, and thus minimizes the \emph{worst-case} expected prediction loss over the set $\probsettestarg{\thetastar}$.
% For a loss function $\ell: \R \times \R \to \R$, its corresponding risk is given by $\Loss(\prob,\beta) = \EE_\prob[\ell(Y, \beta^\top X)]$.
% as  %\fy{of ground truth - theta is to some extent a parameter for rob. risk? that we then max over later? hmmm. maybe its easier to say, for a given theta, we define robust risk as Rrob(theta, beta; gamma, M) = ...?}
%\fy{we can always define a robust risk for a set of distributions (its just not necessarily computable} 
% for a risk function $\Loss: \prob \times \R^d \to \R$ \fy{the first space should be mathcal P or even robustness set? - space of distributions, not a fixed distribution}  \fy{you mean loss function? i'm missing the loss function here shouldn't notation here be $\ell(\prob, \beta)$?} 
% as
%For a given robustness set $\Robsetarg{\thetastar}$, we then define the \emph{robust risk} 
\begin{align}\label{eqn:robust-risk}
    %\Lossrob(\Robsetarg{\thetastar}, \beta) := \sup_{\prob \in \Robsetarg{\thetastar}} \Loss( \prob, \beta),
    \Lossrob(\beta;\thetastar, \Mtest) \coloneqq \sup_{\prob \in \Robsetarg{\thetastar}} \Loss( \beta; \prob). 
\end{align}
%The goal is thus to find a linear predictor $\beta$ with small robust risk, where 

We call the minimizer of the robust risk 
 $\betarob_{\thetastar} \coloneqq \argmin_{\beta \in \R^d} \Lossrob(\beta; \thetastar,\Mtest) $ the \emph{robust predictor}.
% In this manuscript, we focus on the risk of the \emph{squared loss}, i.e. $\Loss(\prob, \beta) = \EE_{\prob} [(Y - \beta^\top X)^2]$. 
% \julia{here: define thetastar}
% \fy{along those lines: As mentioned above, we care about expected prediction loss / risk over a test distribution from $\probsettest$}
%
% For the squared loss, the
 For the squared loss and linear model, the robust risk and the robust predictor can be computed in closed form and \emph{solely} depend on $\Mtest$ and the invariant parameters $\thetastar = (\betastar,\Sigmastar)$, %of the DGP, 
and not on other properties of the distributions:
%The \emph{robust risk} and 
% Thus, when the true model parameters $\thetastar = (\betastar, \Sigmastar)$ are known, one can explicitly compute the robust risk against shifts bounded by $\Mtest$ and its corresponding \emph{robust predictor}  as
%\fy{you redefined thetatsar many times ;) - equal sign is ok} 
%and the test shift bound $\gamma \projM$:
\begin{align}\label{eqn:formula-robust-predictor}
\begin{split}
        \Lossrob(\beta, \thetastar, \Mtest) &=  (\betastar - \beta)^\top (\noisecovxxstar + \Mtest)(\betastar - \beta) + 2(\betastar-\beta)^\top\noisecovxystar + {\noisecovyystar}. 
        \\
        \betarobarg{\thetastar}  &= \argmin_{\beta \in \R^d} \Lossrob(\beta, \thetastar, \Mtest) = \betastar + (\Mtest + \noisecovxxstar)^{-1} \noisecovxystar.
\end{split}
\end{align}
\textbf{Induced equivalence of data generating processes.} This observation motivates us to define an equivalence relation between two data-generating processes that holds whenever they induce the same robust risk for any model $\beta$ and shift upper bound $\Mtest$. Specifically, observe that $\mathrm{DGP}_1$ and $\mathrm{DGP}_2$ induce the same robust risks for all $\Mtest$ and $\beta$ iff $\betastar_1 = \betastar_2$ and $\Petaxi_1 \cong \Petaxi_2$, where $\cong$ denotes the equivalence of distributions based on equality of their first and second moments. 
% Thus, in the following, we refer to our data generating process up to this equivalence relation, making it uniquely defined by $\thetastar$.
Thus, in the following, we treat our data-generating process as uniquely defined by $\thetastar =(\betastar, \Sigmastar)$ up to this equivalence relation.

In practice, the model parameters $\thetastar$ typically cannot be identified from the training distributions,
and the robust risk $\Lossrob$ can only be computed for specific combinations of training and test shifts, studied, e.g., in \citep{rothenhausler2021anchor,shen2023causalityoriented}. In the next section, we describe concepts that allow us to 
reason about robustness even when the robust risk is only partially identifiable. 

%cannot always be identified from the observed training distributions, and hence, it is unclear whether the robustness set, the robust risk, and the robust predictor can be computed from observed data. 

%Instead, in the next sections we show that %in general, 
%the robust prediction model only be \emph{set-identified}, except for specific combinations of the training and test shifts. 
%\fy{the content of this paragraph appears multiple times - can maybe consolidate?}
%it is unclear whether the robust predictor \cref{eqn:formula-robust-predictor} can be computed from observed data. 
%As we explain in the next section, this \fy{here, this is unclear} turns out to be only possible for specific combinations of the training and test distributions.

%%%%%%%%%%%%%%% CHECK IF THIS CAN BE CUT 
% By computing the robust predictor from training data \fy{didn't really get this sentence}, one would achieve perfect robustness with respect to distribution shifts $\probsettestarg{\thetastar}$. However, this turns out to only be possible under specific conditions. 
% With the starting point in \cref{eqn:formula-robust-predictor}, we now aim to answer the following questions: 
% \begin{tcolorbox}[colframe=white!, top=2pt,left=2pt,right=2pt,bottom=2pt]
% \center
% 1. When is the robust predictor identifiable from training data given by $\probtrain$? \\ 2. What is the smallest achievable robust loss 
% %"robust" training data-computable \fy{you mean finite sample or you mean identifiable?} predictor on $\probsettest$ 
% when the robust predictor $\betarob$ is not identifiable?
% \end{tcolorbox}
% We tackle the first question in \cref{sec:prop-invariant-set} and the second question in \cref{sec:PI-lower-bound}. We introduce the best-possible robust predictor in \cref{sec:optimal-estimator} and discuss its properties. 

\subsection{Partially identifiable robustness framework}\label{sec:prop-invariant-set}
% \julia{Proposition on the set of possible robust predictors -- proof in supplementary. Remarks that for anchor + DRIG set collapses to one point (supplementary). Write a bit of text what it means to "not know the model" during testing. } \\
%\fy{fix redundant transition}
We start by formally introducing the general notion of partial identifiability for the robust risk.
%In this section, we formally introduce the identifiable robust risk and related notions that allow us to 
%characterize model robustness in the case when the robust predictor cannot be identified. 
%Several various important notions that will help us describe model robustness in the case when the robust predictor cannot be identified. 
%To formalize this finding, 
The following notion of \emph{observational equivalence} of parameters is reminiscent of the
corresponding notion in the econometrics literature \cite{Dufour2010Econometrics}:
\begin{definition}[Observational equivalence]
 Two model parameter vectors $\theta_1 = (\beta_1, \Sigma_1)$ and $\theta_2 = (\beta_2, \Sigma_2)$ are \textbf{observationally equivalent} with respect to a set of shift distributions $\{ \PeA: e \in \Ecaltrain \}$\footnote{The distributions $\PeA$ are to be understood up to the equivalence relation $\cong$. In general, the distributions $\PeA$ are unknown, since the shift variables $\
 \Ae$ are unobserved. However, in our setting, $\PeA$ can be identified up to the second moment because of the reference environment.}  if they induce the same set $\probtrainarg{\theta}$ of training distributions over the observed variables $(X_e,Y_e)$ as 
described in \Cref{sec:training-data},
 %defined by \cref{eqn:SCM}, 
 i.e.
 \begin{align*}
    \PeXYarg{\theta_1} \cong \PeXYarg{\theta_2} \: \text{for all } e \in \Ecaltrain.
 \end{align*}
%\end{definition}
% \julia{the shifts $\Ae$ are unknown. However, in our setting, the $\mue$ and $\Sigmae$ suffice to make the same definition. but only because the noise is Gaussian. should we write this somewhere?} \fy{this could fit in a footnote}
%\fy{maybe we can merge these two? }
%Thus, by observing the set of training distributions $\probtrainarg{\thetastar}$, we can only identify the model parameters $\theta = (\beta, \Sigma)$ up to a \emph{set} of observationally equivalent parameters:
%\begin{definition}[\Idset]
    By observing $\probtrainarg{\thetastar}$, we can identify the model parameters $\thetastar$ up to the \textbf{\idset} defined as 
    \begin{align*}
       \Invset := \{ \theta = (\beta, \Sigma) \in \Theta: \probtrainarg{\theta} \cong \probtrainarg{\thetastar} \}. 
    \end{align*}
\end{definition}
% In general, the \idset can be unbounded. However, in practice, causal parameters cannot be arbitrarily large. Thus, we impose the following assumption:
% \begin{assumption}[Boundedness of the causal parameter]\label{as:bounded-betastar}
%     There exists a constant $C > 0$ such that any causal parameter $\beta$ generating the SCM  \cref{eqn:SCM} is norm-bounded by $C$, i.e.  $\norm{\beta}_2 \leq C$. 
% \end{assumption}

% Under \cref{as:bounded-betastar}, we define the set of \emph{feasible observationally equivalent model parameters} as
% \begin{align*}
%     \InvsetC := \Invset \cap \{ (\beta, \Sigma) \in \R^{d + (d+1) \times (d+1)}: \| \beta \|_2 \leq C \}. 
% \end{align*}
In general, the \idset is not a singleton, 
that is, $\thetastar$ is not identifiable from the collection of training environments $\probtrainarg{\thetastar}$.
However, prior work has exclusively considered test shifts $\Mtest$ that still allow identifiability of the robust risk nonetheless, depicted in \cref{fig:sub1-identified} and discussed again in \cref{sec:comp-with-finite-robustness-methods}. 
%Note that when the \idset is not a singleton, 
%the robust risk  and the robust prediction model \eqref{eqn:formula-robust-predictor} w.r.t. certain test shifts may still be identifiable. \fy{a bit misleading, because in prior work they don't always consider non-singleton thetaeq...} This is the scenario that prior work 
%on invariant methods focuses on exclusively, depicted in \cref{fig:sub1-identified} and discussed again in \cref{sec:comp-with-finite-robustness-methods}. 
%we are in the partially identified setting\julia{leftover sentence?} 
%The closed form of the robust predictor in \cref{eqn:formula-robust-predictor} 
%suggests that the identifiability of $\betarob$ is connected, albeit not necessarily equivalent, to the identifiability of the model parameters $\thetastar$. In \cref{sec:apx-anchor-connections}, we show how in the settings of prior work, 
%that in some cases, like the setting of anchor regression \fy{ppl dunno what it is yet}, 
%the robust estimator is computable from training data \fy{distributions?} \fy{i.e. , even though the ground-truth parameters $\thetastar$ of the model are not identifiable}. 
%In general however, it cannot be guaranteed that %the observation of 
%the robust risk is identifiable from the collection of training environments $\probtrainarg{\thetastar}$.
In this work, we argue for analyzing the more general partially identifiable setting, where set-identifiability of the invariant parameter $\thetastar$
%identifies 
%other cases, described in \cref{sec:identifiability-linear-SCM}, 
%the model parameters nor the robustness set or robust risk, which is the partially identified setting that we focus on. 
%\fy{in general we need to decide if we say we observe data or distributions. i tend to say distributions cause data is for me by default data points...}
%We now describe the crucial difference to identified distributional robustness in \cref{sec:formulation-distributional-robustness}, where the robustness set $\Robsetarg{\thetastar}$ was assumed to be known. In general, the observation of 
%multiple environments  $\probtrainarg{\thetastar}$ does not identify the robust risk \cref{eqn:robust-risk},
%observing the multi-environment data $\probtrainarg{\thetastar}$, the robust risk \cref{eqn:robust-risk} is in general \emph{ill-defined}
%since the robustness set is not identifiable. 
%Instead, 
only allows us to compute a superset
of the robustness set
% computing the set 
%\fy{and robust risks? then it links with the figure}
\begin{align*}
    \Robsetarg{\Invset} := \bigcup_{\theta \in \Invset} \Robsetarg{\theta} \supset\Robsetarg{\thetastar} 
\end{align*}
and correspondingly, a set of robust risks $\{\Lossrob(\beta; \theta,\Mtest): \, \theta \in \Invset \}$.
%and robust predictors  $ \{\betarobarg{\theta}: \, \theta \in \Invset \}$ (illustrated in \cref{fig:sub2-nonidentified}).
In this case, we would still like to achieve the 
``best-possible'' robustness, that is %intuitively
the test shift robustness
%the robustness \fy{good robust accuracy?/small error } against test shifts  - its not robustness against true parameters, but against shifts ...?
for the ``hardest-possible'' %model-generating 
parameters that could have induced the observed training distributions.
%\fy{do we need worst-case robustness set somewhere? i deleted the sentence with its def. - i think worst-case robustness set is actually confusing cause it depends on beta?}
%In order to still achieve good worst-case prediction error in the partially identified setting, we instead propose to choose an parameter $\beta$ that 
%minimize the robust risk with respect to the \emph{worst-case robustness set} \fy{this formulation might actually be misleading?} 
%minimizes the identifiable robust risk: the maximum of the robust risk for parameters in the \emph{worst-case robustness set}. 
%\fy{sth like: effectively, this is kind of suping over sets of sets}
%We now define these two quantities formally.
%\fy{here i would have found it natural to define the excess robust risk - i.e. compare the rob. risk when you dunno with the robust risk when you know. the worst case excess risk given training data - maybe worth a comment later} 
%Formally, 
%\fy{following paragraph maybe unnecessary}
%For this, we now define a real-valued risk measure, called the \idRR, and a minimax quantity which describes the \emph{best robustness achievable from multi-environment training data}:
\begin{definition}[\IdRR and the minimax quantity]\label{def:PI-robust-loss}
    For the data model in \Cref{eqn:SCM}, the \idRRs is defined as
    \begin{align}
    \label{eqn:PI-robust-loss} 
        \Lossrobpi(\beta; \Invset, \Mtest) :=   \sup_{\theta \in \Invset} \Lossrob  (\beta;\theta, \Mtest). 
    \end{align}
 %and refer to it as the .
The optimal robustness on test shifts bounded by $\Mtest$ given training data $\probtrainarg{\thetastar}$ is described by the minimax quantity 
%\fy{if we only consider cases where the minimizer exists, we might as well plug in betastar - do we want to? (rules out classification losses?)}
\begin{align}
\label{eqn:minimax-quantity}
    \minimaxPIloss  = \inf_{\beta \in \R^d}\Lossrobpi( \beta; \Invset, \Mtest ).
    %=  \inf_{\beta \in \R^d}\sup_{\theta \in \Invset} \Lossrob (\Robsetarg{\theta} , \beta).
\end{align}
When a minimizer of \Cref{eqn:minimax-quantity} exists, we call it the \idpred\, defined by
%\fy{somehow you used to put the actual formula later after Thm 3.1., but if you mention the concept here, then should also put equation here? other alternative is not to mention it here at all}
    \begin{equation}
        \betarobpi = \argmin_\beta \Lossrobpi(\beta; \Invset, \Mtest)
    \end{equation}
\end{definition}
%     \begin{figure}
%     \centering
%     \includegraphics[width = 0.4\textwidth]{paper/contents/images/synthetic_plot_paper.pdf}
%     \caption{KERNEL FIGURE PLACEHOLDER}
%     \label{fig:synthetic-experiments}    
% \end{figure}
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{contents/images/ill-anchor}
  \caption{Identifiable robust risk}
  \label{fig:sub1-identified}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{contents/images/ill-general}
  \caption{Partially identifiable robust risk}
  \label{fig:sub2-nonidentified}
\end{subfigure}
\caption{\small{Relationship between identifiability of the model parameters and identifiability of the robust risk. (a) The classical scenario where the test shift upper bound $\Mtest = M_{\text{seen}}$ is contained in the span of training shifts so that 
%reflected by . 
the robust risk is point-identified. (b) The more general scenario of this paper, where $\Mtest=M_{\text{unseen}}$ contains new shift directions and where only a set can be identified in which the true robust risk lies. }}
\label{fig:test}
% \vspace{-15pt}
\end{figure}
%\fy{merged with paragraph above def 3...}
% \vspace{-10pt} 
%\julia{here cut} \fy{i cut everything since we  now already motivate it above?}
%In words, the optimal parameter choice
%we propose to choose \fy{this sounds too method like?} the parameter 
%$\betarobpi$ is the one  that minimizes the worst-case robust risk for all parameters in the \idset.
%The definition of the \idRRs reflects the absence of knowledge of the model parameters in test shift directions that were not observed during training.
%\emph{worst-case robustness set} 
%\fy{maybe easier with sth like?: In particular, for a given model $\beta$, $\Lossrobpi(\beta, \Invset; \gamma,\cM)$ is the robust risk parameterized by/at $\thetastar\in \Invset$ with the largest robust risk}
% This minimizer is computable using training distributions, due to identifiability of \idset for any combination of the training distributions $\probtrainarg{\thetastar}$ \fy{Etrain?} and test shift assumptions $\gamma \projM$.

In the next sections, we 
%Our goal in the next sections is to 
explicitly compute these quantities for the linear setting of \cref{sec:training-data}.
This will allow us to compare
%interpret them to understand 
the best achievable robustness in the partially identified case with the guarantees of prior methods in this setting.
% \vspace{-5pt}
%and compare it to use them to analyse (sub)-optimality of prior methods in a partially identified setting. 
%Since the \idset is always computable from training data, the \idRRs is by definition computable as well. Hence, we can compute \fy{3x "compute"} the minimizer of the \idRRs for any combination of the training distributions $\probtrainarg{\thetastar}$ and test shift assumptions $\gamma \projM$. 
%\fy{this doesn't seem to be the main goal of the next sections for me?: In the next sections we compute these quantities explicitly for the setting in sec2, with the ultimate goal to understand compare the (achievable) minimax robust errors with upper bounds prior work }\julia{check if it's better now?}\fy{you can look at how i did it in the intro}
%In the next sections, we characterize under which conditions the \idRRs coincides with the conventional robust loss $\Lossrob(\beta,\thetastar; \gamma, \cM)$, and provide a lower bound on the minimax quantity that depends on the structure of the training and test distributions. 
% THIS DISCUSSION SHOULD HAPPEN LATER 
% For instance, it becomes evident from \cref{lm:invariant-set} and \cref{def:PI-robust-loss} that if the test shift $\Mtest$ consists only of identified directions, the PI-robust loss coincides with the conventional robust loss.