\subsection{Proof of \cref{prop:invariant-set}}\label{sec:apx-proof-invariant-set}
% \julia{update the proof to correspond to new notation}
For every environment $e \in \Ecaltrain$, we observe the first moments $\EE(X_e)$ and $\EE(Y_e)$,
and second moments $\EE(X_eX_e^\top)$, $\EE(Y_e^2)$ and $\EE(X_eY_e)$.
% $\Cov(X_e)$ of the covariates in all training environments, as well as the covariances $\Cov(X_e, Y_e)$. 
Since it holds by assumption that $\mu_0 = 0$ and $\Sigma_0 = 0$, we have that  $\EE(X_0X_0^\top) = \noisecovxxstar$, and so we can identify $\noisecovxxstar$ uniquely. Furthermore, it holds that
\begin{align}
    \EE(X_0Y_0) &= \noisecovxxstar \betastar + \noisecovxystar, \label{eqn:ident-1}\\
    \EE(X_eY_e) &= ( \Sigma_e + \mu_e \mu_e^\top + \noisecovxxstar) \betastar + \noisecovxystar.
    \label{eqn:ident-2}
\end{align}
By taking the difference between \cref{eqn:ident-2} and \cref{eqn:ident-1},
we can identify $(\Sigma_e + \mu_e \mu_e^\top) \betastar$.
Thus, 
% In other words,
the parameter $\betastar$ is identifiable on the subspace $\cS$ defined in \cref{eqn:def-S} and is not identifiable on its orthogonal complement $\cSperp$.
% the unions of the spans of $\Sigma_e$. Since it holds that $\range\ M = \cup_e \range\ \Sigma_e$, the causal parameter is identified on $\range\ M$ and non-identified on $\ker M = \Mperp$. 
Thus, for any vector $\alpha \in \cSperp$ , the vector $\beta = \betastar + \alpha$ is consistent with the data-generating process. It remains to compute the covariance parameters induced by an arbitrary $\tilde\beta \coloneqq \betastar + \alpha$, for $\alpha \in \cSperp$. For every environment $e \in \Ecaltrain$,  the second mixed moment between $X_e$ and $Y_e$ has to satisfy the following equality
\begin{align*}
    \EE(X_eY_e) = (\Sigma_e + \mu_e\mu_e^\top + \noisecovxxstar)\betastar + \noisecovxystar = (\Sigma_e + \mu_e\mu_e^\top + \noisecovxxstar) \tilde{\beta}+ \tilde{\Sigma}_{\eta, \xi},
\end{align*}
from which it follows that $\tilde{\Sigma}_{\eta, \xi}
 \coloneqq \noisecovxystar - \noisecovxxstar \alpha$. By computing $\EE(Y_e^2)$ and inserting $\tilde{\beta} = \betastar + \alpha$ and $\tilde{\Sigma}_{\eta, \xi}$, we similarly obtain 
\begin{align*}
    \tilde{\sigma}_{\xi}^{2} \coloneqq \noisecovyystar - 2 \alpha^\top \noisecovxystar + \alpha^\top \noisecovxxstar \alpha. 
\end{align*}
Thus, we obtain the following set of observationally equivalent model parameters consistent with $\probtrainarg{\thetastar}$:
\begin{align*}
    \Invset = \{ \betastar + \alpha, \noisecovxxstar, \noisecovxystar - \noisecovxxstar \alpha, \noisecovyystar - 2 \alpha^\top \noisecovxystar + \alpha^\top \noisecovxxstar \alpha \colon \alpha \in \cSperp \}. 
\end{align*}
Since the \idset is identifiable from the training distribution, but model parameters $\betastar$, $\noisecovxystar$, $\noisecovyystar$ are not, it is helpful to re-express the \idset through identifiable quantities. For this, we note that the "identifiable linear predictor" $\betastarS = \betastar - \betastarperp$ induces an observationally equivalent model given by 
% $(\betastarperp, \noisecovxxstar, \noisecovxystar + \noisecovxxstar \betastarker, \noisecovyystar + 2 \betastarker^\top \noisecovxystar + \betastarker^\top \noisecovxxstar \betastarker)$. 
\begin{align*}
    \thetastarS := (\betaS, \noisecovxxS, \noisecovxyS, \noisecovyyS) = (\betastarS, \noisecovxxstar, \noisecovxystar + \noisecovxxstar \betastarperp, \noisecovyystar + 2 \langle \noisecovxystar, \betastarperp\rangle + \langle \betastarperp, \noisecovxxstar \betastarperp\rangle).
\end{align*}
From this reparameterization, we infer the final form of the \idset:
\begin{align*}
   \Invset = \{ \betastarS + \alpha, \noisecovxx', \noisecovxyS - \noisecovxx' \alpha, \noisecovyyS - 2 \alpha^\top \noisecovxyS + \alpha^\top \noisecovxx' \alpha \colon \alpha \in \cSperp \}  \ni \thetastar 
\end{align*}
Therefore, \cref{eqn:def-invariant-set} follows.
To find the robust predictor $\betarob$, we write down the robust loss with respect to $\Mtest$ and any $\theta_\alpha$ from the \idset:
\begin{align*}
    \Lossrob(\beta;\theta_\alpha, \Mtest) &= (\betastarS + \alpha - \beta)^\top (\Mtest + \noisecovxxstar) (\betastarS + \alpha - \beta) \\ &+ 2 (\betastarS + \alpha - \beta)^\top (\noisecovxystar - \noisecovxxstar \alpha) + \noisecovyyS - 2 \alpha^\top \noisecovxyS + \alpha^\top \noisecovxxstar \alpha.
\end{align*}
inserting $\alpha \in \cSperp$ and rearranging, \cref{eqn:def-rob-pred-identif} follows.
% Denoting the latter two quantities by $\noisecovxyS$, $\noisecovyyS$ and reparameterizing we obtain the claim. 

\subsection{Proof of \cref{thm:pi-loss-lower-bound}}\label{sec:apx-proof-of-main-prop}

We structure the proof as follows: first, we quantify the non-identifiability of the robust risk by explicitly computing its supremum over the \idset of the model parameters (referred to as the \idRR). Second, we derive a lower bound for the \idRRs by considering two cases depending on how a predictor $\betabar$ interacts with the possible test shifts $\Mtest$. 
% In this proof, we use more general notation, with the test shifts bounded by a PSD matrix $\Mtest \preceq \gamma M + \gammaprime R R^\top$, which $\range M \subset \cS$ and $\range R \subset \cSperp$. The statement of the theorem follows by setting $\gamma = \gammaprime$. However, we believe that the more refined statement is useful, e.g., when one expects strong shifts in training directions and only weak "new" shifts.
\paragraph{Computation of the \idRR.} For any model-generating parameter $\theta = (\beta, \Sigma)$ it holds that the robust risk of the model \cref{eqn:SCM} under test shifts $\Mtest \succeq 0$ is given by 
\begin{align*}
%\label{eqn:apx-def-robust-loss}
    \Lossrob(\betabar;\theta, \Mtest) =  (\beta - \betabar)^\top(\Mtest + \noisecovxxstar)(\beta - \betabar) + 2(\beta - \betabar)^\top \noisecovxy + \noisecovyy. 
\end{align*}
We recall that the \idset of model parameters after observing the multi-environment training data \cref{eqn:SCM} is given by 
\begin{align}\label{eqn:apx-def-invariant-set}
     \Invset = \{ \betastarS + \alpha, \noisecovxxstar, \noisecovxyS - \noisecovxxstar \alpha, \noisecovyyS - 2 \alpha^\top \noisecovxyS + \alpha^\top \noisecovxx \alpha: \alpha \in \cSperp \},
\end{align}
where $\cS$ is the span of identified directions defined in \cref{eqn:def-S}. 
Moreover, we recall that by Assumption~\ref{as:bounded-betastar}, for any causal parameter $\beta$ it should hold that $\| \beta \|_2 = \| \betastarS + \alpha \|_2 \leq C$, which translates into the following constraint for the parameter $\alpha$:
\begin{align*}
    \| \alpha \|_2 \leq \sqrt{C^2 - \| \betastarS \|_2^2} =: \Cker. 
\end{align*}
Inserting \cref{eqn:apx-def-invariant-set} in \cref{eqn:PI-robust-loss}, we obtain
\begin{align*}
    \Lossrobpi(\betabar; \Invset, \Mtest) = \supalpha \Lossrob(\betabar; \theta_{\alpha}, \Mtest),
\end{align*}
where $\theta_\alpha$ is a short notation for $(\betastarS + \alpha, \noisecovxxstar, \noisecovxyS - \noisecovxxstar \alpha, \noisecovyyS - 2 \alpha^\top \noisecovxyS + \alpha^\top \noisecovxxstar \alpha)$. We now compute the supremum explicitly in case $\Mtest$ has the form $\Mtest = \gamma \Mseen + \gammaprime R R^\top$, where $\Mseen$ is a PSD matrix with $\range(M) \subseteq \cS$ and $R$ is a semi-orthogonal matrix with $\range(R) \subseteq \cSperp$. For any $\alpha \in \cSperp$, we write down the robust loss as
\begin{align*}
    \Lossrob(\betabar; \theta_\alpha, \Mtest) &= (\betastarS - \betabar)^\top (\Mtest + \noisecovxxstar) (\betastarS - \betabar) + 2 (\betastarS - \betabar)^\top \noisecovxyS + \noisecovyyS \\
    &+ \alpha^\top \Mtest \alpha + 2 \alpha^\top \Mtest(\betastarS -  \betabar ) \\
    &= \Lossrob(\betabar; \thetastarS, \Mtest) + \alpha^\top \Mtest \alpha + 2 \alpha^\top \Mtest(\betastarS -  \betabar ). 
\end{align*}
The first term is the robust risk of $\betabar$ under test shift $\Mtest$ and the identified model-generating parameter $\thetastarS$, thus it does not depend on $\alpha$. 
%We recall that $\Mtest = \gamma \cP_\cM = \gamma (S S^\top + R R^\top)$, where $\range\ S \subset \cS$ and $\range\ R \subset \cSperp$. 
By the structure of $\Mtest$, we obtain that 
\begin{align*}
    f(\alpha) := \alpha^\top \Mtest \alpha + 2 \alpha^\top \Mtest(\betastarS -  \betabar )  = \gammaprime \alpha^\top R R^\top \alpha - \gammaprime \alpha^\top R R^\top \betabar. 
\end{align*}
If $\gammaprime = 0$, i.e., the test shifts consist only of the identified directions, we have $f(\alpha) = 0$, independently of $\alpha$, and thus 
\begin{align*}
     \Lossrobpi(\betabar; \Invset, \Mtest) = \Lossrob(\betabar; \thetastarS, \Mtest).
\end{align*}
This implies the first statement of the theorem. 
\par
We now consider the case where $R \neq 0$, i.e., $R R^\top$ is a non-degenerate projection.
Our goal is to maximize $f(\alpha)$ subject to constraints $\alpha \in \cSperp$, $\| \alpha \|_2 \leq \Cker$. Let $\Rtilde$ be an orthonormal extension of $R$ such that $\range\ (R | \Rtilde) = \cSperp$. Then, we can parameterize $\alpha \in \cSperp$ as $\alpha = (R | \Rtilde) (\frac{w} {\wtilde})$ and the corresponding Lagrangian reads
\begin{align*}
    \mathcal{L}(\alpha, \lambda) &= \gammaprime \alpha^\top R R^\top \alpha - \gammaprime \alpha^\top R R^\top \betabar + \lambda(\Cker^2 - \| \alpha \|_2^2) \\ &= \gammaprime \| w \|_2^2 -\gammaprime w^\top R^\top \betabar + \lambda(\Cker^2 - \| (w, \wtilde) \|_2^2). 
\end{align*}
Differentiating with respect to $w, \wtilde$ yields
\begin{align*}
    w &= \frac{\gammaprime}{\gammaprime - \lambda} R^\top \betabar; \\
    \wtilde &= 0. 
\end{align*}
After differentiating w.r.t. $\lambda$, we obtain
$\frac{\gammaprime}{\gammaprime - \lambda} = \pm \frac{\Cker}{\| R^\top \betabar \|_2}$. By inserting in the objective function and comparing, we obtain the \textbf{value of the \idRR}: 
\begin{align}\label{eqn:proofs-id-robust-risk}
    \Lossrobpi(\betabar; \Invset, \Mtest) &= \gammaprime \Cker^2 + 2 \gammaprime \| R^\top \betabar \|_2 + \Lossrob(\betabar; \thetastarS, \Mtest) \\
    &= \gammaprime \Cker^2 + 2 \gammaprime \| R^\top \betabar \|_2 + \betabar^\top R R^\top \betabar + \gamma (\betastarS - \beta)^\top \Mseen (\betastarS - \beta) + \Loss_0 (\betabar,\thetastarS).
\end{align}
Putting together the two cases and simplifying, we obtain
\begin{align}\label{eqn:detailed-id-robust-risk}
\begin{split}
    \Lossrobpi(\betabar; \Invset, \Mtest) &= \gammaprime(\Cker + \| R^\top \betabar \|_2)^2 + \Lossrob(\betabar; \thetastarS, \gamma \Mseen) \\  &= \gammaprime  (\Cker + \| R^\top \betabar \|_2)^2 + \gamma (\betastarS - \betabar)^\top \Mseen (\betastarS-\betabar) + \Loss_0 (\betabar,\thetastarS), 
\end{split}
\end{align}
where $\Lossrob(\betabar; \thetastarS, \gamma \Mseen)$ is the robust risk of the estimator $\betabar$ w.r.t. the "identified" test shift $\gamma M$ and the identified model parameter $\thetastarS$, whereas $\Loss_0 (\betabar,\thetastarS)$ is the risk of $\betabar$ on the reference environment $e = 0$. 
\paragraph{Derivation of the lower bound for the \idRR.} Now that we have explicitly computed the \idRR, we devote ourselves to the computation of the lower bound for its best possible value
\begin{align*}
    \inf_{\betabar \in \R^d} \Lossrobpi(\betabar; \Invset, \Mtest). 
\end{align*}
In this part, we will only consider the case $R \neq 0$, since the case $R = 0$ corresponds to the (discrete) anchor regression-like setting, where both the robust risk and its minimizer are uniquely identifiable, and computable from training data. We will distinguish between two cases.
\paragraph{Case 1: $\| R^\top \betabar \|_2 = 0$.}  In this case, $\betabar$ is fully located in the orthogonal complement of $R$, which consists of $\cS$ and $\Rtilde$ (the orthogonal complement or $R$ in $\cSperp$). We will denote (the basis of) this subspace by $\Stot = \cS \oplus \Rtilde$. Thus, $\Stot$ is the "total" stable subspace consisting of identified directions in $\cS$ and non-identified, but unperturbed directions $\Rtilde$. We will parameterize $\betabar$ as $\betabar = \Stot w$. Thus, we are looking to solve the optimization problem 
\begin{align*}
   \betarobpi =  \argmin_{w} \, (\betastarS - \Stot w)^\top (\gamma \Mseen^\top + \noisecovxxstar) (\betastarS -  \Stot w) + 2 (\betastarS -  \Stot w)^\top \noisecovxyS + \noisecovyyS.
\end{align*}
Setting the gradient to zero yields the \emph{asymptotic worst-case robust estimator} 
\begin{equation}\label{eq:apx-pi-robust-formula}
\begin{aligned}
        \betarobpi &= \betastarS + \Stot [ \Stot^\top (\gamma \Mseen^\top + \noisecovxx) \Stot ]^{-1} \Stot^\top \noisecovxyS,
\end{aligned}
\end{equation}
which corresponds to the loss value of 
\begin{align*}
    \Lossrobpi(\betarobpi; \Invset, \Mtest) = \gammaprime \Cker^2 +  \noisecovyyS - 2 {\noisecovxyS}^\top \Stot [ \Stot^\top (\gamma \Mseen^\top + \noisecovxx) \Stot ]^{-1} \Stot^\top \noisecovxyS.
\end{align*}
As we observe, this quantity grows linearly in $\gammaprime$. However, as $\gamma \to \infty$, the quantity \emph{saturates} and is upper-bounded by $\noisecovyyS$.
\paragraph{Case 2:$\| R^\top \betabar \|_2 \neq 0$.} Since for $\| R^\top \betabar \|_2 \neq 0$, the objective function is differentiable, we compute its gradient to be
\begin{align*}
    \nabla  \Lossrobpi(\beta; \Invset, \Mtest) &= 2 \gammaprime R R^\top \beta / \| R R^\top \beta \| + 2 \gammaprime R R^\top \beta + \nabla \Lossrob(\beta; \thetastarS, \gamma \Mseen) \\ 
    &= 2 \gammaprime R R^\top \beta / \| R R^\top \beta \| + 2 \gammaprime R R^\top \beta  + 2(\noisecovxxstar + \gamma \Mseen) (\beta - \betastarS) - 2 \noisecovxyS. 
\end{align*}
This equation is, in general, not solvable w.r.t. $\beta$ in closed form. Instead, we provide the limit of the optimal value of the function when the strength of the unseen shifts is small, i.e. $\gammaprime \to 0$. We know that for $\gammaprime = 0$, the minimizer of the worst-case robust risk is given by the anchor estimator
\begin{align*}
    \betaa = \betastarS + (\noisecovxxstar + \gamma \Mseen)^{-1} \noisecovxyS. 
\end{align*}
Instead, we lower bound the non-differentiable term $2 \gammaprime \Cker \| R^\top \beta \|$ by the scalar product $2 \gammaprime \Cker \scalar{R^\top \beta}{R^\top \betaa}/ \| \betaa \|$ and expect it to be tight for small $\gammaprime$. After inserting this lower bound in \cref{eqn:proofs-id-robust-risk} we obtain the minimizer of the lower bound of form
\begin{align*}
    \beta_{LB} = \betastarS + (\noisecovxxstar + \gamma M + \gammaprime R R^\top)^{-1}(\noisecovxyS - \gammaprime \Cker R R^\top (\noisecovxxstar + \gamma M)^{-1} \noisecovxyS).
\end{align*}
We can now lower bound $\| R R^\top \beta_{LB} \|$ as 
\begin{equation}\label{eqn:small-gammaprime-lower-bound}
    \| R R^\top \beta_{LB} \| \geq \| R R^\top (\noisecovxxstar + \gamma M)^{-1} \noisecovxystar \| - \gammaprime \cdot \text{const}.
\end{equation}
Thus, the $\gammaprime$-rate of the \idRRs of $\beta_{LB}$ is at least $\gammaprime (\Cker + \| R R^\top (\noisecovxxstar + \gamma M)^{-1} \noisecovxystar \|)^2 + \mathcal{O}(\gammaprime^2)$,
from which the claim for small $\gammaprime$ follows. For \cref{sec:comp-with-finite-robustness-methods}, the lower bound directly implies optimality of the worst-case robust risk of the anchor estimator when the strength of the unseen shifts $\gammaprime$ is small. Additionally. if $\gamma = 0$, i.e. only unseen test shifts occur, we conclude that the OLS and anchor estimators have the same rates. 
\paragraph{Lower bound $\gammath$ for $\gammaprime$.}
Finally, we want to derive a lower bound on the shift strength  $\gammaprime$ such that for all $\gammaprime \geq \gammath$ Case 1 of our proof is valid, i.e. it holds that $\betarobpi$ is given by the closed form "abstaining" estimator \eqref{eq:apx-pi-robust-formula}. For this, we find $\gammath$ such that for all $\gammaprime \geq \gammath$ zero is contained in the subdifferential of$\Lossrobpi(\betarobpi;\Invset,\Mtest)$ at $\betarobpi$. Then the KKT conditions are met, and $\betarobpi$ is the unique minimizer of the worst-case robust risk due to strong convexity of the objective. We compute the subdifferential to be
\begin{align*}
    S = \gammaprime \Cker \{ R R^\top \beta: \| \beta \|_2 \leq 1 \} + \nabla \Lossrob(\betarobpi; \thetastarS, \gamma M).
\end{align*}
Since $\betarobpi$ is the minimizer of $ \Lossrob(\beta; \thetastarS, \gamma \Mseen)$ under the constraint $R^\top \beta = 0$, the gradient is zero in $R^\perp$ and it remains to show that 
\begin{align*}
    \| R R^\top \nabla \Lossrob(\betarobpi; \thetastarS, \gamma \Mseen) \| \leq \gammaprime \Cker,
\end{align*}
or 
\begin{align*}
    \gammaprime \geq \| R R^\top \nabla \Lossrob(\betarobpi; \thetastarS, \gamma \Mseen) \| / \Cker. 
\end{align*}
Via an upper bound on the projected gradient, we derive the stricter condition
\begin{align*}
    \gammaprime \geq \frac{\| R R^\top \noisecovxyS\| (1 + \kappa(\noisecovxxstar)) }{\Cker},
\end{align*}
where $\kappa(\noisecovxxstar)$ is the condition number of the covariance matrix. 

\subsection{Proof of \cref{cor:estimators}}\label{sec:apx-proof-of-corollary}
To obtain a new formulation for the \idRR, we start with \eqref{eqn:detailed-id-robust-risk} and expand 
\begin{align}
\begin{split}
    \Lossrobpi(\betabar; \Invset, \Mtest) &= \gammaprime  (\Cker + \| R^\top \betabar \|_2)^2 + \gamma (\betastarS - \betabar)^\top \Manchor (\betastarS-\betabar) + \Loss_0 (\betabar,\thetastarS) \\
    &= \gammaprime  (\Cker + \| R^\top \betabar \|_2)^2 + \gamma (\betastarS - \betabar)^\top \Manchor (\betastarS-\betabar) \\&+ (\betastarS - \betabar)^\top \noisecovxxstar (\betastarS-\betabar) + 2 (\betastarS - \beta)\noisecovxyS + \noisecovyyS \\ &= \gammaprime  (\Cker + \| R^\top \betabar \|_2)^2 + (\gamma - 1)(\betastarS - \betabar)^\top \Manchor (\betastarS-\betabar) + \Loss(\beta,\ptrain) \\
    &= \Lossrob(\beta, \thetastarS, \gamma \Manchor) + \gammaprime  (\Cker + \| R^\top \betabar \|_2)^2,
\end{split}
\end{align}
where we have used that the pooled second moment of $X$ equals to $\noisecovxxstar + \sum_e w_e (\mu_e \mu_e^\top) = \noisecovxxstar + \gamma \Manchor - (\gamma - 1) \Manchor$.
This reformulation shows that the \idRRs is equal to the anchor population loss (cf. \cite{rothenhausler2021anchor}) with an additional non-identifiability penalty term $\gammaprime  (\Cker + \| R^\top \betabar \|_2)^2$. 

We now want to evaluate the rates of the anchor and OLS estimators in terms of the magnitude $\gammaprime$ of unseen shift directions. We observe that only the non-identifiability term depends on $\gammaprime$, whereas the second term only depends on $\gamma$. First, we compute the closed-form anchor regression estimator, which reads 
\begin{equation}
    \betaa = \argmin_{\beta \in \R^d} \Lossrob(\beta, \thetastarS, \gamma \Manchor) = \betastarS + (\noisecovxxstar + \gamma \Manchor)^{-1} \noisecovxyS.
\end{equation}
Since $\betaOLS$ equals to the anchor estimator with $\gamma = 1$, we obtain 
\begin{equation*}
    \betaOLS = \betastarS + (\noisecovxxstar + \Manchor)^{-1} \noisecovxyS.
\end{equation*}
The claim of the corollary now follows by computing $\| R R^\top \betaa \|$ and $\| R R^\top \betaOLS \|$ and observing that the rest of the terms is constant in $\gammaprime$. Additionally, we observe that $\betaa$ is the minimizer of $\Lossrob(\beta, \thetastarS, \gamma \Manchor)$, and it is known (cf. e.g. \cite{rothenhausler2021anchor}) that $\Lossrob(\betaa, \thetastarS, \gamma \Manchor)$ is asymptotically constant in $\gamma$ and upper bounded by $\noisecovyyS$. On the other hand, the term $\Lossrob(\betaOLS, \thetastarS, \gamma \Manchor)$ is linear in $\gamma$. In total, we obtain 
\begin{equation*}
    \begin{aligned}
        \Lossrobpi(\betaa; \Invset, \Mtest) &= (\Cker + \| R R^\top \betaa \| )^2\gammaprime + c_1(\gamma); \\ 
        \Lossrobpi(\betaOLS; \Invset,\Mtest) &= (\Cker + \| R R^\top \betaOLS \| )^2\gammaprime + c_2(\gamma),
    \end{aligned}
\end{equation*}
where $c_1(\gamma) \leq \noisecovyyS$ and $c_2(\gamma) = \Omega(\gamma)$.
Comparing to the lower bound \eqref{eqn:small-gammaprime-lower-bound} for the minimax quantity for the case of $\gammaprime \to 0$, we observe that the anchor estimator is optimal (achieves the minimax rate) in the limit $\gammaprime \to 0$. Additionally, if $\gamma = 0$ (only new shifts occur during test time), anchor and OLS have identical rates in $\gammaprime$ and, in particular, OLS (corresponding to vanilla empirical risk minimization) is minimax-optimal in the limit of small unseen shifts. 
In the proof of \cref{thm:pi-loss-lower-bound} in \cref{sec:apx-proof-of-main-prop}, we show that for $\gammaprime \geq \gammath$, it holds that $R R^\top \betarobpi$ = 0, and thus the worst-case robust risk of the worst-case robust predictor equals 
\begin{equation*}
    \Lossrobpi(\betarobpi; \Invset, \Mtest) = \gammaprime \Cker^2 +  \noisecovyyS - o(\gamma) = \gammaprime \Cker^2 + c_3(\gamma),
\end{equation*}
where $c_3(\gamma) \leq \noisecovyyS$.
In total, we observe that the worst-case robust risk of \emph{all} considered prediction models grows linearly with the unseen shift strength $\gammaprime$, albeit with different rates. The terms $\| R R^\top \betaa \|$ and $\| R R^\top \betaOLS \|$ can be particularly large, for instance, when there is strong confounding aligned with the unseen shift directions which causes the empirical risk minimizer (OLS) to have a strong signal in these directions. The worst-case robust predictor $\betarobpi$, however, abstains in these directions, thus achieving a smaller rate.
% \begin{lemma}\label{lm:upper-bound}
    
% Let $\Mtest \coloneqq V \Lambda V^\top \succeq 0$ with 
% $V = [v_1, \dots, v_k] \in \R^{d \times k}$, $\Lambda = \diag(\lambda_1, \dots, \lambda_k)$, and define
% $\range\(\Mtest) = \cM$.
% Define $Q = (S, R) \in \R^{d \times q}$ such that $\range\(S) = \mathcal{S}$, 
% $\range\(R) \subset \cSperp$,
% and $\range\(Q) \supseteq \cM$. Furthermore define $\gamma := \max\{\lambda_j : j = 1, \dots, k\}$.
% Then, $\Mtest \preceq \gamma QQ^\top$.
% \end{lemma}
% \begin{proof}
%     Since $\range\(Q) \supseteq \cM$, we can write every eigenvector of $\Mtest$ as a linear combination of the columns of $Q$. That is, for every $j = 1, \dots, k$, there exists a vector $\tilde{v}_j \in \R^q$ such that $v_j = Q \tilde{v}_j$. 
%     Moreover, since $v_j \in \range\(Q)$, it follows that $QQ^\top v_j = v_j$, and by the orthonormality of the columns of $Q$, $\tilde{v}_j = Q^\top v_j$.
%     These two facts imply that $\norm{\tilde{v}_j}^2 = \tilde{v}_j^\top \tilde{v}_j = v_j^\top QQ^\top v_j = v_j^\top v_j = \norm{v_j}^2 = 1$.
%     Define $\tilde{V} \coloneqq [\tilde{v}_1, \dots, \tilde{v}_k] \in \R^{q \times k}$ and note that $\Mtest = V\Lambda V^\top = Q \tilde{V} \Lambda \tilde{V}^\top Q^\top$, so we can rewrite the matrix 
%     \begin{align}\label{eqn:psd-mat}
%     \gamma QQ^\top - \Mtest = Q (\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top) Q.
%     \end{align}
    
%     We will now show that the matrix $\gamma QQ^\top - \Mtest \succeq 0$.
%     \begin{enumerate}[i)]
%         \item First, we establish that 
%     $\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top \succeq 0$, by showing that its eigenvalues are non-negative.
%     Notice that the eigenvectors of $\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top$ are $(\tilde{V}, \tilde{U}) \in \R^{q \times q}$ where $\tilde{U} \coloneqq [\tilde{u}_1, \dots, \tilde{u}_{q-k}]\in\R^{q\times (q - k)}$ with $\tilde{U}^\top \tilde{V} = 0$.
%     Let $\tilde{v}\in \R^q$ be an eigenvector of $\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top$. If $\tilde{v}=\tilde{v}_j$ for some $j = 1, \dots, k$, we have that $(\gamma I_q - \tilde{V}\Lambda \tilde{V}^\top) \tilde{v} = (\gamma - \lambda_j)\tilde{v}$, which is associated to the eigenvalue $\gamma - \lambda_j \geq 0$.
%     If $\tilde{v} = \tilde{u}_j$, for some $j = 1, \dots, {q-k}$, we have that $(\gamma I_q - \tilde{V}\Lambda \tilde{V}^\top) \tilde{v} = \gamma \tilde{v}$, which is associated to the eigenvalue $\gamma \geq 0$.

%     \item Second, we establish that $\gamma QQ^\top - \Mtest = Q (\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top) Q^\top \succeq 0$.
%     Fix $x \in \R^d$ and define $y \coloneqq Q^\top x \in \R^q$. Then
%     \begin{align*}
%         x^\top (\gamma QQ^\top - \Mtest )x = &\
%         x^\top Q (\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top) Q^\top x = y^\top (\gamma I_q - \tilde{V} \Lambda \tilde{V}^\top) y \geq 0,
%     \end{align*}
%     where the first equality follows from \cref{eqn:psd-mat} and the last inequality follows from i).
%     \end{enumerate}
    
% \end{proof}

