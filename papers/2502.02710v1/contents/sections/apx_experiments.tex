
In this section, we provide more details of the data generation for our synthetic finite-sample experiments as well as data processing for the real-world data experiments.
\subsection{Synthetic experiments}\label{sec:apx-synthetic-exps}

For the synthetic experiments, we generate a random SCM which satisfies our assumptions. For $d = 15$, we randomly sample the joint covariance $\Sigmastar$ of $(\eta,\xi)$, fixing its total variance and the eigenvalues. We consider 7 environments including the reference environment, and for each environment except the reference, we randomly generate mean shifts $\mue$ of fixed norm $1$. Since we have $6$ non-zero random Gaussian mean shifts, it holds a.s. that $\dim \cS = 6$. We then randomly generate an "initial guess" for $\betastar \in \R^d$ of fixed norm $C = 10$. Now, with respect to the space $\cS$ of the identifiable directions induced by the mean shifts, we choose the most "adversarial" causal parameter $\betaadv$ which is equal to $\betastar$ on $\cS$, but on $\cSperp$ has the opposite direction of the noise OLS estimator ${\noisecovxxstar}^{-1} \noisecovxystar$. We ensure that $\| \betaadv \|_2 = C$. Note that under the observed shifts, $\betastar$ and $\betaadv$ are observationally equivalent. We complete $\betaadv$ to the set $\thetaadv$ of observationally equivalent model parameters and generate the multi-environment training data according to $\thetaadv$ and the collection of mean shifts. 

For \cref{fig:synthetic-experiments} (left), we define the test shift upper bound as $\Manchor = \gamma \frac{1}{7} \sum_{e} \mu_e \mu_e^\top$. We vary $\gamma$ from $0$ to $10$, and for each $\gamma$, we compute the oracle anchor regression estimator by minimizing the discrete anchor regression loss with the correct $\gamma$. Additionally, we compute the pooled OLS estimator and the worst-case robust predictor $\betarobpi$ as described in \cref{sec:apx-empirical-estimation}. Finally, we generate test data with a Gaussian additive shift $\Atest \sim \cN(0, \Manchor)$. We evaluate the loss of $\betaOLS$, $\betaa$ and $\betarobpi$ on this test environment and include the population lower bound. 

For \cref{fig:synthetic-experiments} (right), we define the test shift upper bound as $\Mnew = \gamma \frac{1}{7} \sum_{e} \mu_e \mu_e^\top + \gammaprime R R^\top$, where $R$ is a 2-dimensional subspace of the space $\cSperp$. We fix the magnitude $\gamma$ of the ''seen'' test shift directions at $\gamma = 40$ and set vary $\gammaprime$ from $0$ to $2$ to showcase the effect of small unseen shifts compared to large identified shifts. We compute the oracle anchor regression estimator by minimizing the discrete anchor regression loss. Additionally, we compute the pooled OLS estimator and the worst-case robust predictor $\betarobpi$ as described in \cref{sec:apx-empirical-estimation}, for which we use the oracle $\gammaprime$, given $\Manchor$ and empirical estimates of the spaces $\cS$, $\cSperp$, $R$. \\
Finally, we generate test data with a Gaussian additive shift $\Atest \sim \cN(0, \Mnew)$. We evaluate the loss of $\betaOLS$, $\betaa$ and $\betarobpi$ on this test environment, plot the resulting test losses for different estimators and include the population lower bound. 

\subsection{Real-world data experiments}\label{sec:apx-real-world}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{contents/images/training-data.pdf}
        \caption{}
        \label{fig:fig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{contents/images/test-data-strength_01.pdf}
        \caption{}
        \label{fig:fig2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{contents/images/test-data-strength_02.pdf}
        \caption{}
        \label{fig:fig3}
    \end{subfigure}
    \caption{The figures illustrate the structure of the (a) training-time shifts and (b-c) test-time shifts for different perturbation strengths on the example of two covariates. Panel (a) shows the training data containing two environments--observational (blue) and shifted (orange) corresponding to the knockout of the gene ENSG00000089009. 
    Panels~(b) and~(c) show the training data in grey and test data from a previously unseen environment (green). 
    Panel~(b) depicts the top $10\%$ test data points closest to the training support (perturbation strength = $0.1$).
    Panel~(c) illustrates the full test data (perturbation strength = 1.0).
    }
    \label{fig:genes}
\end{figure}

We consider the K562 dataset from \cite{replogle2022mapping} and perform the preprocessing as done in \cite{chevalley2022causalbench}.
The resulting dataset consists of $n = 162,751$ single-cell observations over $d = 622$ genes collected from observational and several interventional environments. 
% We now consider the Causalbench single-cell dataset introduced by \citep{chevalley2022causalbench}.
% The dataset consists of single-cell observations of 622 genes.
The interventional environments arise by knocking down a single gene at a time using the CRISPR interference method \citep{qi2013repurposing}. Following \citep{schultheiss2024assessing}, we select only always-active genes in the observational setting, resulting in a smaller dataset of 28 genes. For each gene $j = 1, \ldots, 28$, we set $Y:= X_j$ as the target variable and select the three genes $X_{k_1}, \ldots, X_{k_3}$ most strongly correlated with $Y$ (using Lasso), resulting in a prediction problem over $Y, X_{k_1}, \ldots, X_{k_3}$.
Given this prediction problem, we construct the training and test datasets as follows. Let $\Iobs$ denote the 10,691 observations collected from the observational environment, and let $\Iint_{i}$ denote the observations collected from the interventional environment where the gene $k_i$ was knocked down. We will denote by $\Iint_{i,s}$ the $s \times 100$ percent of datapoints in $\Iint_{i}$ that are closest to the mean of gene $k_i$ in the observational environment $\Iobs$. 
For example, $\Iint_{i,0.1}$ consists of the 10\% of datapoints in $\Iint_{i}$ closest to the observational mean of gene $k_i$.
Thus, 
% $s$ acts as a corresponds to the fraction of test points closest to the observational mean
% These are the $s \times 100\%$ of datapoints with the \emph{weakest} shift compared to the observational mean
% of the gene $k_i$, and thus 
the parameter $s \in [0,1]$ acts as a proxy for the \emph{strength} of the shift. 
Denote by $\Iint_{i, s}^*$ a random sample of $\Iint_{i, s}$ of a certain size.
For each $i \in \{1, 2, 3\}$, we fit the methods on the training data  $\Dtrain_i \coloneqq \Iobs \cup \Iint_{i, 1}^*$, with $|\Iint_{i, 1}^*| = 20$. \cref{fig:genes}(a) illustrates an example of training data  $\Dtrain_i$.
Having fitted the methods on $\Dtrain_i$, we evaluate them on test datasets constructed as follows.
For each 
shift strength $s \in \{0.1, \dots, 0.9\}$ and proportion $\pi \in \{0, .33, .67, 1\}$, define the test dataset $\mathcal{D}_{\pi, s}^{\mathrm{test}}$ consisting of $\pi$ observations from $\cup_{\ell\neq i}\Iint_{\ell, s}$ and $1-\pi$ (out-of-training) observations from $\Iint_{i, s}$.
% $\Dtest_{j, s} \coloneqq \Iint_{j, s}^*$. If $j = i$, this corresponds to a shift seen during training of potentially differing strength. If $j \neq i$, the test data contains a previously unseen distribution shift. 
An example of a test dataset for different shift strengths $s$ and previously unseen directions (i.e., $\pi = 1$) is shown in \cref{fig:genes}(b-c).  
We compare our method Worst-case Rob., defined as the minimizer of the empirical worst-case robust risk \eqref{eqn:rob-loss-ell-sample}, with anchor regression \citep{rothenhausler2021anchor}, invariant causal prediction (ICP) \citep{peters2016causal},  Distributional Robustness via Invariant Gradients (DRIG) \citep{shen2023causalityoriented}, and OLS (corresponding to vanilla ERM).
We use the following parameters for Worst-case Rob.: $\gamma = 50$, $\Cker = 1.0$, and $M = \Id$. For anchor regression and DRIG, we select $\gamma = 50$. For ICP, we set the significance level for the invariance tests to $\alpha = 0.05$.

These numerical experiments are computationally light and can be run in $\approx 5$ minutes on a personal laptop.\footnote{We use a 2020 13-inch MacBook Pro with a 1.4 GHz Quad-Core Intel Core i5 processor, 8 GB of RAM, and Intel Iris Plus Graphics 645 with 1536 MB of graphics memory.}