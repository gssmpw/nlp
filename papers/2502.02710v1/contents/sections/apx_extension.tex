
% \begin{enumerate}
%     \item  What if there is no reference environment?
%     \item We show that instead betastar can be identified on $\cup_{e} (\mu_e - \mu_0)  (\mu_e - \mu_0)^\top + (\Sigma_e - \Sigma_0) $.
%     \item Reader can check: the results hold the same, just with new $\cS$
%     \item Highlight: $\noisecovxx$ cannot be identified, instead $\noisecovxx + \Sigma_0$
% \end{enumerate}

We discuss how our setting changes when we relax the assumptions on the existence of the reference environment. We consider the data-generating process in \cref{eqn:SCM}, where $\Ecaltrain = [m]$, $m \in \mathbb{N}$. If no environment $e$ exists with $\mue = 0$ and $\Sigmae = 0$, we first pick an arbitrary distribution $\PrefXY$ as the reference environment\footnote{In practice, it is useful to pick a distribution with the smallest covariance, i.e. $\trace \Cov(\Xref) \leq \trace \Cov(\Xe)$ for all $e$.} .
We denote $\noisecovxx' := \noisecovxxstar + \Sigmaref$. 

First, we show we can express the space $\cS$ of training additive shift directions defined in \cref{eqn:def-S} in the general case. We center all distributions by $\muref$  to obtain centered distributions $\tilde{\Pr}$ that $\EE_{X\sim \tilde{P}_e}[X]=0$. With respect to the arbitrary reference environment, we now define
\begin{align*}
    \tilde{\cS} \coloneqq \range \left[\bigcup_{e \in \Ecaltrain} \left(\Sigmae - \Sigmaref + (\mue - \muref)(\mue - \muref)^\top \right)\right]\subset \R^d.
\end{align*}
We now consider test shifts with respect to the environment $\PrefXY$\footnote{In other words, we require that the test distribution is a shifted version of the (arbitrarily) chosen reference distribution.}. We define the test shift upper bound $\Mtest = \gamma \Mseen + \gammaprime R R^\top$, where $\range(\Mseen) \subset \cS$ and $\range(R) \subset \cSperp$. Again, we can decompose the parameter $\betastar$ as $\betastar = \betastarS + \betastarperp$.
The projection $\betastarS$ of the causal parameter onto the relative training shifts induces the following observationally equivalent parameters corresponding to the reference distribution:
%\fy{still unsure if it should be in prop. shorter prop just make it seem less important as a result} also define the following set of parameters projected onto S ... 
\begin{align*}
    \thetastarS := (\betaS, \noisecovxx', \noisecovxyS, \noisecovyyS) = (\betastarS, \noisecovxx', \noisecovxystar + \noisecovxx' \betastarperp, \noisecovyystar + 2 \langle \noisecovxystar, \betastarperp\rangle + \langle \betastarperp, \noisecovxx' \betastarperp\rangle).
\end{align*}
Again, $\thetastarS$ can be identified from the training distributions and is referred to as the \emph{identified model parameters}. The following adapted version of \cref{prop:invariant-set} shows that assuming shifts on $\PrefXY$, the robust prediction model is only identifiable if the test shifts are in the direction of the relative training shifts:
\begin{proposition}[Identifiability of reference distribution parameters and robust prediction model]
\label{prop:invariant-set-general} Suppose that the set of training and test distributions is generated according to \Cref{eqn:SCM,eqn:testAbound}.
%, and the test distribution is generated under an unknown additive shift bounded by \cref{eqn:testAbound}. 
%$\betastarS$ induces \fy{didn't get "induce" here} a unique tuple of model parameters $\thetacS$ 
Then, $\thetacS$ is observationally equivalent
to $\thetastar$ and computable from training distributions.
%The parameters $\thetacS = (\betastarS, \SigmaS)$ are computable from training data, thus, we will refer to them as the \emph{identified model parameters}. 
Furthermore, it holds that
\begin{enumerate}[(a)]
 \item %\textbf{Identifiability of the model parameters.} 
 the model parameters %$\theta = (\beta, \noisecovxx, \noisecovxy, \noisecovyy)$ 
 generating the reference distribution can be identified up to the following \idset: 
\begin{align*}
   \Invset = \{ \betastarS + \alpha, \noisecovxx', \noisecovxyS - \noisecovxx' \alpha, \noisecovyyS - 2 \alpha^\top \noisecovxyS + \alpha^\top \noisecovxx' \alpha \colon \alpha \in \cSperp \}  \ni \thetastar 
\end{align*}
\item %\textbf{Identifiability of the robust predictor.}
the robust prediction model $\betarob$ as defined in \cref{eqn:formula-robust-predictor} is identified up to the set
    \begin{align*}
      \betastarS + (\gamma \projM + \noisecovxx')^{-1} \noisecovxyS + \{ (\gamma \projM + \noisecovxx')^{-1} \alpha\colon \, \alpha \in \range(R)  \} \ni \betarob 
    \end{align*}
\end{enumerate}
\end{proposition}
The proof is analogous to \cref{sec:apx-proof-invariant-set}. A version of \cref{thm:pi-loss-lower-bound} for perturbations on the reference environment follows accordingly. 


% As a last comment, another difference between this relaxed setting and the one presented in the main text is that $\EE(X_0X_0^\top) = \Sigma_0 + \noisecovxxstar$, and thus we can only identify $\Sigma_0 + \noisecovxxstar$. This, however, has no consequences on the results of \cref{sec:main-results}.

