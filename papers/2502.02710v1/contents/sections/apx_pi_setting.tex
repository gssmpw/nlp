\nico{attempt}

Let $\Theta$ denote the parameter space and $\mathcal{E}$ the environment space.
Recall that for each environment $e \in \Ecaltrain \subseteq \mathcal{E}$, the data are generated according to the SCM indexed by $\thetastar := (\betastar, \noisecovxxstar, \noisecovxystar, \noisecovxystar) \in \Theta$ so that
\begin{align*}
    \Xe &= \Ae + \eta, \
    \Ye = \betastar^\top X^e + \xi
\end{align*}
induces a distribution $\prob_{X, Y}^{\thetastar, e}$.
Define the set of distributions observed at training time
\begin{align}
    \mathcal{P}_\mathrm{tr} \coloneqq \left\{\prob_{X, Y}^{\thetastar, e} \colon e \in \Ecaltrain\right\}.
\end{align}
Moreover, define the set of \nico{training} equivalent models
\begin{align}
    \Theta_\mathrm{tr} \coloneqq \left\{\theta \in \Theta \colon \forall\+ e \in \Ecaltrain\; \prob_{X, Y}^{\theta, e} = \prob_{X, Y}^{\thetastar, e}\right\},
\end{align}
which consists of all models that are compatible with the observed distributions.
By definition, for every environment $e \in \Ecaltrain$, we have that $\prob_{X, Y}^{\thetastar, e} = \prob_{X, Y}^{\theta, e}$ for all training equivalent models $\theta \in \Theta_{\mathrm{tr}}$.
So, in a sense, we can say that $\Ecaltrain$ preserves the training equivalence property, by definition.
We can take a step further, and define the largest set of environments $\bar{\mathcal{E}}$ that preserves training equivalence, that is
\begin{align}\label{eq:larg-set}
    \bar{\mathcal{E}} \coloneqq \left\{e \in \mathcal{E} \colon \forall\+\theta \in \Theta_{\mathrm{tr}}\; \prob_{X, Y}^{\thetastar, e} = \prob_{X, Y}^{\theta, e}\right\},
\end{align}
and its corresponding set of distributions 
\begin{align}
    \bar{\mathcal{P}} \coloneqq \left\{ \prob_{X, Y}^{\thetastar, e} \colon e \in \bar{\mathcal{E}} \right\}.
\end{align}
At test time, environments will be collected from $\Ecaltest \subseteq \mathcal{E}$, and the corresponding set of test distributions will be 
\begin{align}
    \mathcal{P}_\mathrm{test} \coloneqq \left\{ \prob_{X, Y}^{\thetastar, e} \colon e \in \Ecaltest \right\}.
\end{align}
In principle, we don't know $\thetastar$, so we might want to ask what alternative sets of test distributions are induced by training equivalent models. To do so, one can define for all $\tilde{\theta} \in \Theta_{\mathrm{tr}}$, define
\begin{align}\label{eq:rob-set-test}
    \mathcal{P}_{\mathrm{test}}^{\tilde{\theta}} \coloneqq \left\{ \prob_{X, Y}^{\theta, e} \colon e \in \Ecaltest \right\}.
\end{align}
Depending on the structure of $\Ecaltest$, the test distributions induced by training equivalent models may or may not coincide with the test distribution induced by the true unknown $\thetastar$.
The answer to this question depends on the interplay between $\Ecaltest$ and $\Theta_{\mathrm{tr}}$ and is tightly connected to the relation between observationally and interventionally equivalent models \cite{bongers2021foundations}.
In particular, if $\Ecaltest \subseteq \bar{\mathcal{E}}$, by the definition of $\bar{\mathcal{E}}$ in \prettyref{eq:larg-set} we have that $\mathcal{P}_{\mathrm{test}}^{\theta} = \mathcal{P}_{\mathrm{test}}$ for all training equivalent models $\theta \in \Theta_{\mathrm{tr}}$, and we say that the \nico{robustness?} set is identifiable from the training distribution. 
This, however, is a very special case, and it is the one considered by invariant-based methods for distributional robustness \cite{rothenhausler2021anchor, jakobsen2022distributional, shen2023causality, christiansen2021causal}.
In all other cases, all we can do is to take the union of \prettyref{eq:rob-set-test} over training equivalent models and define the \nico{partially identified robust set}
\begin{align}
    \mathcal{P}_{\mathrm{test}}^{\mathrm{PI}} \coloneqq \bigcup_{\theta \in \Theta_{\mathrm{tr}}}\mathcal{P}_{\mathrm{test}}^{\theta}.
\end{align}
