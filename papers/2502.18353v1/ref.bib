@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@article{wang2024towards,
  title={Towards Real World Debiasing: A Fine-grained Analysis On Spurious Correlation},
  author={Wang, Zhibo and Kuang, Peng and Chu, Zhixuan and Wang, Jingyi and Ren, Kui},
  journal={arXiv preprint arXiv:2405.15240},
  year={2024}
}

@ARTICLE{10416838,
  author={Dogra, Varun and Verma, Sahil and Kavita and Wo≈∫niak, Marcin and Shafi, Jana and Ijaz, Muhammad Fazal},
  journal={IEEE Access}, 
  title={Shortcut Learning Explanations for Deep Natural Language Processing: A Survey on Dataset Biases}, 
  year={2024},
  volume={12},
  number={},
  pages={26183-26195},
  keywords={Natural language processing;Task analysis;Predictive models;Training;Machine learning;Data models;Behavioral sciences;Transfer learning;Dataset biases;deep learning;natural language processing;shortcut learning;transfer learning},
  doi={10.1109/ACCESS.2024.3360306}}

@inproceedings{liu2024self,
  title={Self-Supervised Position Debiasing for Large Language Models},
  author={Liu, Zhongkun and Chen, Zheng and Zhang, Mengqi and Ren, Zhaochun and Ren, Pengjie and Chen, Zhumin},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={2897--2917},
  year={2024}
}
@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@article{jiang2020robust,
  title={Robust pre-training by adversarial contrastive learning},
  author={Jiang, Ziyu and Chen, Tianlong and Chen, Ting and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2010.13337},
  year={2020}
}
@inproceedings{verma2019manifold,
  title={Manifold mixup: Better representations by interpolating hidden states},
  author={Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={6438--6447},
  year={2019},
  organization={PMLR}
}
@article{aghajanyan2021muppet,
  title={Muppet: Massive Multi-task Representations with Pre-Finetuning},
  author={Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2101.11038},
  year={2021}
}
@article{goel2021robustness,
  title={Robustness Gym: Unifying the NLP Evaluation Landscape},
  author={Goel, Karan and Rajani, Nazneen and Vig, Jesse and Tan, Samson and Wu, Jason and Zheng, Stephan and Xiong, Caiming and Bansal, Mohit and R{\'e}, Christopher},
  journal={NAACL demo},
  year={2021}
}
@article{ribeiro2020beyond,
  title={Beyond accuracy: Behavioral testing of NLP models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  journal={58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{pham2020out,
  title={Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?},
  author={Pham, Thang M and Bui, Trung and Mai, Long and Nguyen, Anh},
  journal={arXiv preprint arXiv:2012.15180},
  year={2020}
}
@article{lee2018simple,
  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},
  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  journal={arXiv preprint arXiv:1807.03888},
  year={2018}
}

@article{mccoy2019berts,
  title={Berts of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2019}
}

@article{chang2017active,
  title={Active bias: Training more accurate neural networks by emphasizing high variance samples},
  author={Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}
@article{gardner2020evaluating,
  title={Evaluating Models' Local Decision Boundaries via Contrast Sets},
  author={Gardner, Matt and Artzi, Yoav and Basmova, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and others},
  journal={arXiv preprint arXiv:2004.02709},
  year={2020}
}
@article{d2020underspecification,
  title={Underspecification presents challenges for credibility in modern machine learning},
  author={D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
  journal={arXiv preprint arXiv:2011.03395},
  year={2020}
}
@article{du2021towards,
  title={Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models},
  author={Du, Mengnan and Manjunatha, Varun and Jain, Rajiv and Deshpande, Ruchi and Dernoncourt, Franck and Gu, Jiuxiang and Sun, Tong and Hu, Xia},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2021}
}
@article{niven2019probing,
  title={Probing neural network comprehension of natural language arguments},
  author={Niven, Timothy and Kao, Hung-Yu},
  journal={57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={NeurIPS Workshop},
  year={2019}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={NeurIPS Deep Learning Workshop},
  year={2015}
}
@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={34th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@article{prasanna2020bert,
  title={When bert plays the lottery, all tickets are winning},
  author={Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}
@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={33rd Conference on Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}
@inproceedings{bian2021attention,
  title={On Attention Redundancy: A Comprehensive Study},
  author={Bian, Yuchen and Huang, Jiaji and Cai, Xingyu and Yuan, Jiahong and Church, Kenneth},
  booktitle={North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year={2021}
}
@article{hooker2020characterising,
  title={Characterising bias in compressed models},
  author={Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
  journal={arXiv preprint arXiv:2010.03058},
  year={2020}
}
@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning (ICML)},
  year={2016}
}
@article{utama2020towards,
  title={Towards debiasing NLU models from unknown biases},
  author={Utama, Prasetya Ajie and Moosavi, Nafise Sadat and Gurevych, Iryna},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@article{tu2020empirical,
  title={An empirical study on robustness to spurious correlations using pre-trained language models},
  author={Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  year={2020}
}
@article{liang2021super,
  title={Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization},
  author={Liang, Chen and Zuo, Simiao and Chen, Minshuo and Jiang, Haoming and Liu, Xiaodong and He, Pengcheng and Zhao, Tuo and Chen, Weizhu},
  journal={59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2021}
}
@InProceedings{pawsx2019emnlp,
  title = {{PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification}},
  author = {Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2019}
}
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={Findings of EMNLP},
  year={2020}
}
@article{sun2020mobilebert,
  title={Mobilebert: a compact task-agnostic bert for resource-limited devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  journal={58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{shah2019predictive,
  title={Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview},
  author={Shah, Deven and Schwartz, H Andrew and Hovy, Dirk},
  journal={The 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{clark2019don,
  title={Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases},
  author={Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019}
}
@inproceedings{utama2021avoiding,
  title={Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning},
  author={Utama, Prasetya and Moosavi, Nafise Sadat and Sanh, Victor and Gurevych, Iryna},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={9063--9074},
  year={2021}
}
@article{zhou2020towards,
  title={Towards Robustifying NLI Models Against Lexical Dataset Biases},
  author={Zhou, Xiang and Bansal, Mohit},
  journal={58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{he2019unlearn,
  title={Unlearn dataset bias in natural language inference by fitting the residual},
  author={He, He and Zha, Sheng and Wang, Haohan},
  journal={2019 EMNLP workshop},
  year={2019}
}
@inproceedings{cadene2019rubi,
  title={Rubi: Reducing unimodal biases for visual question answering},
  author={Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  booktitle={Advances in neural information processing systems (NeurIPS)},
  year={2019}
}
@inproceedings{wang2020high,
  title={High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks},
  author={Wang, Haohan and Wu, Xindi and Huang, Zeyi and Xing, Eric P},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}
@article{geirhos2018imagenet,
  title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
  author={Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019}
}
@article{turc2019well,
  title={Well-read students learn better: On the importance of pre-training compact models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962},
  year={2019}
}
@article{schuster2019towards,
  title={Towards debiasing fact verification models},
  author={Schuster, Tal and Shah, Darsh J and Yeo, Yun Jie Serene and Filizzola, Daniel and Santus, Enrico and Barzilay, Regina},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019}
}
@article{conneau2017supervised,
  title={Supervised learning of universal sentence representations from natural language inference data},
  author={Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2017}
}
@article{sundararajan2017axiomatic,
  title={Axiomatic Attribution for Deep Networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  journal={International Conference on Machine Learning (ICML) },
  year={2017}
}
@article{toneva2018empirical,
  title={An empirical study of example forgetting during deep neural network learning},
  author={Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019}
}
@article{shah2020pitfalls,
  title={The pitfalls of simplicity bias in neural networks},
  author={Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@article{mudrakarta2018did,
  title={Did the Model Understand the Question?},
  author={Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  journal={56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2018}
}
@inproceedings{manjunatha2019explicit,
  title={Explicit bias discovery in visual question answering models},
  author={Manjunatha, Varun and Saini, Nirat and Davis, Larry S},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2019}
}
@article{sharma2019natural,
  title={Natural language understanding with the quora question pairs dataset},
  author={Sharma, Lakshay and Graesser, Laura and Nangia, Nikita and Evci, Utku},
  journal={arXiv preprint arXiv:1907.01041},
  year={2019}
}
@inproceedings{mahabadi2020end,
  title={End-to-End Bias Mitigation by Modelling Biases in Corpora},
  author={Mahabadi, Rabeeh Karimi and Belinkov, Yonatan and Henderson, James},
  booktitle={58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{utama2020mind,
  title={Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance},
  author={Utama, Prasetya Ajie and Moosavi, Nafise Sadat and Gurevych, Iryna},
  journal={58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2018}
}
@article{thorne2018fever,
  title={FEVER: a large-scale dataset for fact extraction and verification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2018}
}
@article{zellers2018swag,
  title={Swag: A large-scale adversarial dataset for grounded commonsense inference},
  author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  journal={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2018}
}
@inproceedings{muller2019does,
  title={When does label smoothing help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
@article{stacey2020there,
  title={Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training},
  author={Stacey, Joe and Minervini, Pasquale and Dubossarsky, Haim and Riedel, Sebastian and Rockt{\"a}schel, Tim},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2019}
}
@article{du2019techniques,
  title={Techniques for interpretable machine learning},
  author={Du, Mengnan and Liu, Ninghao and Hu, Xia},
  journal={Communications of the ACM},
  year={2019}
}
@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}
@inproceedings{fan2019rethinking,
  title={Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks},
  author={Fan, Lixin and Ng, Kam Woh and Chan, Chee Seng},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
@inproceedings{uchida2017embedding,
  title={Embedding watermarks into deep neural networks},
  author={Uchida, Yusuke and Nagai, Yuki and Sakazawa, Shigeyuki and Satoh, Shin'ichi},
  booktitle={Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
  year={2017}
}
@article{boenisch2020survey,
  title={A Survey on Model Watermarking Neural Networks},
  author={Boenisch, Franziska},
  journal={arXiv preprint arXiv:2009.12153},
  year={2020}
}
@article{chan2020poison,
  title={Poison attacks against text datasets with conditional adversarially regularized autoencoder},
  author={Chan, Alvin and Tay, Yi and Ong, Yew-Soon and Zhang, Aston},
  journal={EMNLP Findings},
  year={2020}
}
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019}
}
@inproceedings{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
@article{chen2017neural,
  title={Neural natural language inference models enhanced with external knowledge},
  author={Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
  journal={56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2018}
}
@article{kurita2020weight,
  title={Weight poisoning attacks on pre-trained models},
  author={Kurita, Keita and Michel, Paul and Neubig, Graham},
  journal={58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2020}
}
@article{jo2017measuring,
  title={Measuring the tendency of CNNs to learn surface statistical regularities},
  author={Jo, Jason and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1711.11561},
  year={2017}
}
@article{wang2019learning,
  title={Learning robust representations by projecting superficial statistics out},
  author={Wang, Haohan and He, Zexue and Lipton, Zachary C and Xing, Eric P},
  journal={International Conference on Learning Representations (ICLR)},
  year={2019}
}
@article{jia2017adversarial,
  title={Adversarial examples for evaluating reading comprehension systems},
  author={Jia, Robin and Liang, Percy},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2017}
}
@article{mihaylov2018knowledgeable,
  title={Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge},
  author={Mihaylov, Todor and Frank, Anette},
  journal={56th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2018}
}
@inproceedings{kim2019learning,
  title={Learning not to learn: Training deep neural networks with biased data},
  author={Kim, Byungju and Kim, Hyunwoo and Kim, Kyungsu and Kim, Sungjin and Kim, Junmo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  year={2019}
}
@inproceedings{selvaraju2019taking,
  title={Taking a hint: Leveraging explanations to make vision and language models more grounded},
  author={Selvaraju, Ramprasaath R and Lee, Stefan and Shen, Yilin and Jin, Hongxia and Ghosh, Shalini and Heck, Larry and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year={2019}
}
@article{si2019does,
  title={What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?},
  author={Si, Chenglei and Wang, Shuohang and Kan, Min-Yen and Jiang, Jing},
  journal={arXiv preprint arXiv:1910.12391},
  year={2019}
}
@article{bahng2019learning,
  title={Learning De-biased Representations with Biased Representations},
  author={Bahng, Hyojin and Chun, Sanghyuk and Yun, Sangdoo and Choo, Jaegul and Oh, Seong Joon},
  journal={International Conference on Machine Learning (ICML)},
  year={2020}
}
@article{minervini2018adversarially,
  title={Adversarially regularising neural nli models to integrate logical background knowledge},
  author={Minervini, Pasquale and Riedel, Sebastian},
  journal={The SIGNLL Conference on Computational Natural Language Learning (CoNLL)},
  year={2018}
}
@article{montavon2018methods,
  title={Methods for interpreting and understanding deep neural networks},
  author={Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Digital Signal Processing},
  year={2018},
  publisher={Elsevier}
}
@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics (TACL)},
  year={2019},
  publisher={MIT Press}
}
@article{li2020backdoor,
  title={Backdoor learning: A survey},
  author={Li, Yiming and Wu, Baoyuan and Jiang, Yong and Li, Zhifeng and Xia, Shu-Tao},
  journal={arXiv preprint arXiv:2007.08745},
  year={2020}
}
@article{evert2005statistics,
  title={The statistics of word cooccurrences: word pairs and collocations},
  author={Evert, Stefan},
  year={2005}
}
@inproceedings{du2020towards,
  title={Towards Generalizable Deepfake Detection with Locality-aware AutoEncoder},
  author={Du, Mengnan and Pentyala, Shiva and Li, Yuening and Hu, Xia},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management (CIKM)},
  year={2020}
}
@inproceedings{tang2020embarrassingly,
  title={An embarrassingly simple approach for trojan attack in deep neural networks},
  author={Tang, Ruixiang and Du, Mengnan and Liu, Ninghao and Yang, Fan and Hu, Xia},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining (KDD)},
  year={2020}
}
@article{deng2020unified,
  title={A Unified Taylor Framework for Revisiting Attribution Methods},
  author={Deng, Huiqi and Zou, Na and Du, Mengnan and Chen, Weifu and Feng, Guocan and Hu, Xia},
  journal={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2021}
}
@article{tang2020deep,
  title={Deep Serial Number: Computational Watermarking for DNN Intellectual Property Protection},
  author={Tang, Ruixiang and Du, Mengnan and Hu, Xia},
  journal={arXiv preprint arXiv:2011.08960},
  year={2020}
}
@inproceedings{wang2020score,
  title={Score-CAM: Score-weighted visual explanations for convolutional neural networks},
  author={Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  year={2020}
}
@article{sanh2020learning,
  title={Learning from others' mistakes: Avoiding dataset biases without modeling them},
  author={Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}
@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}
@article{cheng2020posterior,
  title={Posterior differential regularization with f-divergence for improving model robustness},
  author={Cheng, Hao and Liu, Xiaodong and Pereira, Lis and Yu, Yaoliang and Gao, Jianfeng},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2021}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}
@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
  journal={34th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}
@inproceedings{liu2021just,
  title={Just Train Twice: Improving Group Robustness without Training Group Information},
  author={Liu, Evan Z and Haghgoo, Behzad and Chen, Annie S and Raghunathan, Aditi and Koh, Pang Wei and Sagawa, Shiori and Liang, Percy and Finn, Chelsea},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}
@article{xu2021beyond,
  title={Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Xu, Ke and McAuley, Julian and Wei, Furu},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2021}
}
@article{jin2019bert,
  title={Is bert really robust? natural language attack on text classification and entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  journal={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2020}
}
@article{yaghoobzadeh2019robust,
  title={Robust natural language inference models with example forgetting},
  author={Yaghoobzadeh, Yadollah and Tachet, Remi and Hazen, Timothy J and Sordoni, Alessandro},
  journal={arXiv e-prints},
  pages={arXiv--1911},
  year={2019}
}
@inproceedings{sun2019patient,
  title={Patient Knowledge Distillation for BERT Model Compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019}
}
@article{chen2020lottery,
  title={The lottery ticket hypothesis for pre-trained bert networks},
  author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
  journal={34th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision (ICCV)},
  year={2017}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@article{hooker2019compressed,
  title={What do compressed deep neural networks forget?},
  author={Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
  journal={arXiv preprint arXiv:1911.05248},
  year={2019}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{phang2018sentence,
  title={Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},
  author={Phang, Jason and F{\'e}vry, Thibault and Bowman, Samuel R},
  journal={arXiv e-prints},
  pages={arXiv--1811},
  year={2018}
}

@inproceedings{liu2020mmtdnn,
  title={The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding},
  author={Liu, Xiaodong and Wang, Yu and Ji, Jianshu and Cheng, Hao and Zhu, Xueyun and Awa, Emmanuel and He, Pengcheng and Chen, Weizhu and Poon, Hoifung and Cao, Guihong and others},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={118--126},
  year={2020}
}
@article{du2022shortcut,
  title={Shortcut learning of large language models in natural language understanding},
  author={Du, Mengnan and He, Fengxiang and Zou, Na and Tao, Dacheng and Hu, Xia},
  journal={Communications of the ACM (CACM)},
  year={2023}
}
@inproceedings{Zhu2020FreeLB,
title={FreeLB: Enhanced Adversarial Training for Natural Language Understanding},
author={Chen Zhu and Yu Cheng and Zhe Gan and Siqi Sun and Tom Goldstein and Jingjing Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BygzbyHFvB}
}

@inproceedings{wang2021infobert,
title={InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
booktitle={International Conference on Learning Representations},
year={2021}}

@misc{chen2021chasing,
      title={Chasing Sparsity in Vision Transformers:An End-to-End Exploration}, 
      author={Tianlong Chen and Yu Cheng and Zhe Gan and Lu Yuan and Lei Zhang and Zhangyang Wang},
      year={2021},
      eprint={2106.04533},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{li2021select,
  title={How to Select One Among All? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding},
  author={Li, Tianda and Rashid, Ahmad and Jafari, Aref and Sharma, Pranav and Ghodsi, Ali and Rezagholizadeh, Mehdi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={750--762},
  year={2021}
}
@article{xu2021rethinking,
  title={Rethinking Network Pruning--under the Pre-train and Fine-tune Paradigm},
  author={Xu, Dongkuan and Yen, Ian EH and Zhao, Jinxi and Xiao, Zhibin},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2021}
}
@article{huang2021sparse,
  title={Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm},
  author={Huang, Shaoyi and Xu, Dongkuan and Yen, Ian EH and Chang, Sung-en and Li, Bingbing and Chen, Shiyang and Xie, Mimi and Liu, Hang and Ding, Caiwen},
  journal={arXiv preprint arXiv:2110.08190},
  year={2021}
}
@article{weiemergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research}
}
@article{wang2023robustness,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others},
  journal={arXiv preprint arXiv:2302.12095},
  year={2023}
}
@article{chen2023robust,
  title={How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks},
  author={Chen, Xuanting and Ye, Junjie and Zu, Can and Xu, Nuo and Zheng, Rui and Peng, Minlong and Zhou, Jie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2303.00293},
  year={2023}
}

@article{gururangan2018annotation,
  title={Annotation artifacts in natural language inference data},
  author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel R and Smith, Noah A},
  journal={arXiv preprint arXiv:1803.02324},
  year={2018}
}

@article{poliak2018hypothesis,
  title={Hypothesis only baselines in natural language inference},
  author={Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1805.01042},
  year={2018}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{jia2017adversarial,
  title={Adversarial examples for evaluating reading comprehension systems},
  author={Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1707.07328},
  year={2017}
}

@article{alzantot2018generating,
  title={Generating natural language adversarial examples},
  author={Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1804.07998},
  year={2018}
}



@inproceedings{guo2022auto,
  title={Auto-debias: Debiasing masked language models with automated biased prompts},
  author={Guo, Yue and Yang, Yi and Abbasi, Ahmed},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1012--1023},
  year={2022}
}

@article{wang2021identifying,
  title={Identifying and mitigating spurious correlations for improving robustness in nlp models},
  author={Wang, Tianlu and Sridhar, Rohit and Yang, Diyi and Wang, Xuezhi},
  journal={NAACL 2022 Findings},
  year={2022}
}

@article{lyu2022feature,
  title={Feature-Level Debiased Natural Language Understanding},
  author={Lyu, Yougang and Li, Piji and Yang, Yechang and de Rijke, Maarten and Ren, Pengjie and Zhao, Yukun and Yin, Dawei and Ren, Zhaochun},
  journal={arXiv preprint arXiv:2212.05421},
  year={2022}
}
@inproceedings{lyu2023feature,
  title={Feature-level debiased natural language understanding},
  author={Lyu, Yougang and Li, Piji and Yang, Yechang and de Rijke, Maarten and Ren, Pengjie and Zhao, Yukun and Yin, Dawei and Ren, Zhaochun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13353--13361},
  year={2023}
}

@article{mccoy2019right,
  title={Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  journal={57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@article{zhang2019paws,
  title={PAWS: Paraphrase adversaries from word scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  journal={arXiv preprint arXiv:1904.01130},
  year={2019}
}

@article{he2023mitigating,
  title={Mitigating Shortcuts in Language Models with Soft Label Encoding},
  author={He, Zirui and Deng, Huiqi and Zhao, Haiyan and Liu, Ninghao and Du, Mengnan},
  journal={LREC-COLING 2024},
  year={2024}
}

@article{meissner2022debiasing,
  title={Debiasing masks: A new framework for shortcut mitigation in NLU},
  author={Meissner, Johannes Mario and Sugawara, Saku and Aizawa, Akiko},
  journal={arXiv preprint arXiv:2210.16079},
  year={2022}
}

@misc{lyu2023featurelevel,
      title={Feature-Level Debiased Natural Language Understanding}, 
      author={Yougang Lyu and Piji Li and Yechang Yang and Maarten de Rijke and Pengjie Ren and Yukun Zhao and Dawei Yin and Zhaochun Ren},
      year={2023},
      eprint={2212.05421},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{huang2022biaspad,
  title={BiasPAD: A Bias-Progressive Auto-Debiasing Framework},
  author={Huang, Hao and Zhou, Tianyi and Long, Guodong and Shen, Tao and Jiang, Jing and Zhang, Chengqi},
  year={2022}
}


@inproceedings{utama-etal-2020-mind,
    title = "Mind the Trade-off: Debiasing {NLU} Models without Degrading the In-distribution Performance",
    author = "Utama, Prasetya Ajie  and
      Moosavi, Nafise Sadat  and
      Gurevych, Iryna",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.770",
    doi = "10.18653/v1/2020.acl-main.770",
    pages = "8717--8729",
    abstract = "Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.",
}

@inproceedings{clark-etal-2019-dont,
    title={Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases},
  author={Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  journal={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019}
    
}

@article{mahabadi2019end,
  title={End-to-end bias mitigation by modelling biases in corpora},
  author={Mahabadi, Rabeeh Karimi and Belinkov, Yonatan and Henderson, James},
  journal={arXiv preprint arXiv:1909.06321},
  year={2019}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{fuglede2004jensen,
  title={Jensen-Shannon divergence and Hilbert space embedding},
  author={Fuglede, Bent and Topsoe, Flemming},
  booktitle={International symposium onInformation theory, 2004. ISIT 2004. Proceedings.},
  pages={31},
  year={2004},
  organization={IEEE}
}

@inproceedings{srivastava2020robustness,
  title={Robustness to spurious correlations via human annotations},
  author={Srivastava, Megha and Hashimoto, Tatsunori and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={9109--9119},
  year={2020},
  organization={PMLR}
}

@article{wang2020identifying,
  title={Identifying spurious correlations for robust text classification},
  author={Wang, Zhao and Culotta, Aron},
  journal={arXiv preprint arXiv:2010.02458},
  year={2020}
}

@article{huangbiaspad,
  title={BiasPAD: A Bias-Progressive Auto-Debiasing Framework},
  author={Huang, Hao and Zhou, Tianyi and Long, Guodong and Shen, Tao and Jiang, Jing and Zhang, Chengqi}
}

@article{du2022less,
  title={Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation},
  author={Du, Yanrui and Yan, Jing and Chen, Yan and Liu, Jing and Zhao, Sendong and Wu, Hua and Wang, Haifeng and Qin, Bing},
  journal={arXiv preprint arXiv:2205.12593},
  year={2022}
}