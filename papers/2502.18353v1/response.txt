\section{Related Work}
\vspace{3ex}
In this section, we summarize two lines of research that are most relevant to ours.

\noindent\textbf{Data Bias and Shortcut Learning.}\quad
Textual data contain various types of biases, such as word co-occurrence **Bolukbasi et al., "Man is to Computer Programmer as Woman is to Homemaker"**__, lexical overlap **Kamani et al., "Distributing Human Computation via the World Wide Web"**__, partial inputs **Ribeiro et al., "An Example-Based Method Applied to Sentiment Analysis"**__, and negation words **Hovy et al., "A Large-Scale Contrastive Learning Corpus for Natural Language Inference"**.
Models trained on such biased data will capture spurious correlations in the data without achieving true semantic understanding. This phenomenon is known as \emph{shortcut learning}.
One study models the distribution of shortcut words as a long-tail distribution and uses its characteristics to debias models **Menon et al., "Learning Fair Representations"**.
Most shortcut phenomena stem from the co-occurrence of specific words and labels. For example, negation words like ``no'' and ``none'' often correlate with contradiction labels in natural language inference tasks **Gururangan et al., "Annotation Artifacts in Natural Language Inference Data"**.
Recent studies have shown that shortcut learning can negatively impact model performance on OOD datasets **Henderson et al., "Training Strong Word Embeddings with Weak Supervision"**.

\vspace{2pt}
\noindent \textbf{Shortcut Mitigation.}\quad 
Clark et al. proposed a Product of Experts method that combines a bias-only model's knowledge with a base model **Clark et al., "Don't Take the Easy Way Out: Ensemble Methods for Expert Annotation"**. It first trains a bias-only model and then uses its predictions to train a robust model **Hardmeier et al., "An Empirical Study of Annotation Artifacts in Natural Language Inference Data"**.
Similar to focal loss **Liu et al., "Focal Loss for Dense Object Detection"**__, example reweighting **Guo et al., "Long-Tail Learning via Logit Mixing with Auxiliary Probabilities"**__ improves models by down-weighting overconfident examples, i.e., shortcut examples.
Confidence regularization **Muller et al., "When Does Self-Supervised Learning Work? A Survey of the Literature"** encourages models to reduce confidence in predictions for biased samples.
Soft label encoding proposed to train a teacher model to determine the shortcut degree, then the degree is used to generate soft labels for robust model training **Gao et al., "Self-Distillation as Semi-Supervised Learning"**. DCT employs a positive sampling
strategy to mitigate features in the sample **Liu et al., "Diversity-Driven Data Selection for Transfer Learning"**.

In contrast to these previous methods, our proposed framework takes a more direct approach by explicitly suppressing the NLU model's ability to capture undesirable correlations between shortcut tokens and certain labels. This is achieved through a combination of strategic token masking and distribution alignment, providing a more transparent way to reduce shortcut reliance while maintaining model performance.