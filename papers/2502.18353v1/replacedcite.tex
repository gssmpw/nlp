\section{Related Work}
\vspace{3ex}
In this section, we summarize two lines of research that are most relevant to ours.

\noindent\textbf{Data Bias and Shortcut Learning.}\quad
Textual data contain various types of biases, such as word co-occurrence____, lexical overlap____, partial inputs____, and negation words____.
Models trained on such biased data will capture spurious correlations in the data without achieving true semantic understanding. This phenomenon is known as \emph{shortcut learning}.
One study models the distribution of shortcut words as a long-tail distribution and uses its characteristics to debias models____.
Most shortcut phenomena stem from the co-occurrence of specific words and labels. For example, negation words like ``no'' and ``none'' often correlate with contradiction labels in natural language inference tasks____.
Recent studies have shown that shortcut learning can negatively impact model performance on OOD datasets____.

\vspace{2pt}
\noindent \textbf{Shortcut Mitigation.}\quad 
Clark et al. proposed a Product of Experts method that combines a bias-only model's knowledge with a base model____. It first trains a bias-only model and then uses its predictions to train a robust model ____.
Similar to focal loss ____, example reweighting ____ improves models by down-weighting overconfident examples, i.e., shortcut examples.
Confidence regularization ____ encourages models to reduce confidence in predictions for biased samples.
Soft label encoding proposed to train a teacher model to determine the shortcut degree, then the degree is used to generate soft labels for robust model training____. DCT employs a positive sampling
strategy to mitigate features in the sample____.

In contrast to these previous methods, our proposed framework takes a more direct approach by explicitly suppressing the NLU model's ability to capture undesirable correlations between shortcut tokens and certain labels. This is achieved through a combination of strategic token masking and distribution alignment, providing a more transparent way to reduce shortcut reliance while maintaining model performance.