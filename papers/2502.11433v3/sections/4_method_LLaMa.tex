\section{LoRA-PPO}
\textcolor{blue}{
In this section, we are going to explain LoRA-PPO methods based on the explicit structure of \textsc{LLaMA} model.}

\textcolor{blue}{
Based on our proposed method, our algorithm utilizes the large language model (LLM) as generalized policy neural networks of deep reinforcement learning (DRL) agent for financial trading task. In particular, \textsc{LLaMA} model has been chosen as the major example for our experiment, so our theoretical analysis is based on its structure. To be specific, if we denote by $F_\theta$ the \textsc{LLaMA} model, the structure of model can be summarized as follows:
\begin{itemize}
    \item[1.] Token Embeddings: The input state $s_t$ is converted into a tokenized sequence $\{w_t^{1},\cdots,w_t^{n}\}$ and then embedded into a high-dimensional vector such that $x_t^{i}=E(w_t^{i}),i=1,\cdots,n$, where $E$ is the token embedding matrix. Also, we denote $x_t=\{x_t^{1},\cdots,x_t^{n}\}$
    \item[2.] Transformer Layers: Suppose that we have total $L$ layers. For each layer $l\in\{1,2,\cdots,L\}$, the \textbf{Attention} sub-layer admits several matrices: Query ($W_Q^{(l)}$), Key ($W_K^{(l)}$), Value ($W_V^{(l)}$), and Output ($W_O^{(l)}$). More, the \textbf{Feedforward Network (FFN)} sub-layer admits two weight matrices $W_1^{(l)}$ and $W_2^{(l)}$. Then, the hidden state updates can be expressed as:
    \begin{equation*}
        h_t^{(l)}=\text{LayerNorm}(\text{Attention}(Q^{(l)},V^{(l)},K^{(l)})+\text{FFN})
    \end{equation*}
    where\begin{equation*}
        Q^{(1)}=W_Q^{(1)}x_t, K^{(1)}=W_K^{(1)}x_t,
        V^{(1)}=W_V^{(1)}x_t.
    \end{equation*}
    and\begin{equation*}
        Q^{(l)}=W_Q^{(l)}h_t^{(l-1)}, K^{(l)}=W_K^{(l)}h_t^{(l-1)},
        V^{(l)}=W_V^{(l)}h_t^{(l-1)}.
    \end{equation*} 
    \item[3.] Output Projection: It maps the last hidden states to vocabulary logits, that is,
    \begin{equation*}
        F_\theta(s_t)=W_o h_t^{(L)} + b_o
    \end{equation*}
    which is a vector on space $\mathbb{R}^{|\mathcal{A}|}$
\end{itemize}
Then, the action $a_t$ will be decided by the logits from \textsc{LLaMA} model's output, that is, the probability distribution of action $a\in\mathcal{A}$ is computed by
\begin{equation*}
    \pi_\theta(a|s_t) = \frac{\exp{F_\theta(s_t)[a]}}{\sum_{a'\in\mathcal{A}}\exp{F_\theta(s_t)[a']}}
\end{equation*}
and the action with highest probability will be the chosen action $a_t$.
}

\textcolor{blue}{
Now, let move to the discussion of LoRA-PPO. We denote by $W^{(l)}$ all matrices at layer $l\in\{1,2,\cdots,L\}$. If we update the parameters of \textsc{LLaMA}, then the principle of LoRA yields that
\begin{equation*}
    W^{(l)'} = W^{(l)} + A^{(l)}B^{(l)}
\end{equation*}
where $A^{(l)}$ and $B^{(l)}$ should be the trainable low-rank matrices. Then, we denote by $A=\{A^{(1)},\cdots,A^{(L)}\}$ and $B=\{B^{(1)},\cdots,B^{(L)}\}$. Then, the policy gradient in LoRA-PPO should be:
\small
\begin{equation*}
    \nabla_{A,B}\log{\pi_\theta(a|s_t)}=\nabla_{A,B}F_\theta(s_t)[a]-\sum\limits_{a'\in\mathcal{A}}\pi_\theta(a'|s_t)\nabla_{A,B}F_\theta(s_t)[a']
\end{equation*}
}