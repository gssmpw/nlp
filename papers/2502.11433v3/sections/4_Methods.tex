\section{\textsc{FLAG-Trader}}
To tackle the challenge of directly fine-tuning an LLM for both alignment and decision-making, we introduce \textsc{FLAG-Trader}, a fused LLM-agent and RL framework for financial stock trading. In \textsc{FLAG-Trader}, a partially fine-tuned LLM serves as the policy network, leveraging its pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning, as shown in Figure \ref{fig:AC-network}. The model processes financial information using a textual state representation, allowing it to interpret and respond to market conditions effectively. Instead of fine-tuning the entire network, only a subset of the LLM’s parameters is trained, striking a balance between adaptation and knowledge retention. In the following, we will present the prompt input design and the detailed architecture of \textsc{FLAG-Trader}.







\subsection{Prompt Input Design}

The first stage of the pipeline involves designing a robust and informative prompt, denoted as \texttt{lang}($s_t$), which is constructed based on the current state 
$s_t$ to guide the LLM in making effective trading decisions. The prompt is carefully structured to encapsulate essential elements that provide context and ensure coherent, actionable outputs. It consists of four key components: a \emph{task description}, which defines the financial trading objective, outlining the problem domain and expected actions; a \emph{legible action space}, specifying the available trading decisions (\texttt{Sell,'' Hold,'' ``Buy''}); a \emph{current state representation}, incorporating market indicators, historical price data, and portfolio status to contextualize the decision-making process; and an \emph{output action}, which generates an executable trading decision. This structured prompt ensures that the LLM receives comprehensive input, enabling it to produce well-informed and actionable trading strategies, as illustrated in Figure \ref{fig:prompt}.




\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/FinRL_Prompt.pdf}
    \caption{The format of input prompt. It contains the task description, the legible action set, the current state description, and the output action format.}
    \label{fig:prompt}
    \vspace{-0.2cm}
\end{figure}

\subsection{\textsc{FLAG-Trader} Architecture}

To incorporate parameter-efficient fine-tuning into the policy gradient framework, we partition the intrinsic parameters of the LLM into two distinct components: the frozen parameters inherited from pretraining, denoted as 
$\theta_{\texttt{forzen}}$, and the trainable parameters, denoted as 
$\theta_{\texttt{train}}$. This separation allows the model to retain general language understanding while adapting to financial decision-making with minimal computational overhead.
Building upon this LLM structure, we introduce a policy network and a value network, both of which leverage the trainable top layers of the LLM for domain adaptation while sharing the frozen layers for knowledge retention. The overall architecture is illustrated in Figure \ref{fig:AC-network}.




\subsubsection{Policy Network Design}
The policy network is responsible for generating an optimal action distribution over the trading decision space $\cA$, conditioned on the observed market state. It consists of three main components:



\emph{State Encoding.}
To effectively process financial data using the LLM, the numerical market state 
$s$ is first converted into structured text using a predefined template\footnote{To simplify notation, we use \texttt{lang}($s_t$) to represent both the state encoding and the prompt, acknowledging this slight abuse of notation for convenience.}
\begin{align}
\texttt{lang}(s) = \text{"Price: \$}p\text{, Vol: }v\text{, RSI: }r\text{,..."}.
\label{eq:template}
\end{align}
This transformation enables the model to leverage the LLM’s textual reasoning capabilities, allowing it to understand and infer trading decisions in a structured, language-based manner.

\emph{LLM Processing.} The tokenized text representation of the state is then passed through the LLM backbone, which consists of:
1) \textbf{Frozen layers} (preserve general knowledge):
Token embeddings $E = \text{Embed}(\texttt{lang}(s))$ pass through LLM frozen layers, i.e.,
\begin{equation}
h^{(1)} = \text{LLM}_{1:N}(E;\theta_{\texttt{frozen}}).
\end{equation}
These layers preserve general knowledge acquired from pretraining, ensuring that the model maintains a strong foundational understanding of language and reasoning.
2) \textbf{Trainable layers} (domain-specific adaptation): The output from the frozen layers is then passed through the trainable layers, which are fine-tuned specifically for financial decision-making, i.e.,
\begin{equation}
h^{(2)} = \text{LLM}_{N+1:N+M}(h^{(1)};\theta_{\text{train}}).
\label{eq:trainable_layer}
\end{equation}
This structure enables efficient adaptation to the financial domain without modifying the entire LLM, significantly reducing training cost while maintaining performance.

\emph{Policy Head.} 
Finally, the processed representation is fed into the policy head, which outputs a probability distribution over the available trading actions according to
\begin{equation}
\texttt{logits} = \textsc{Policy\_Net}(h^{(2)},\theta_P)\in\mathbb{R}^{|\mathcal{A}|},
\label{eq:policy_dist}
\end{equation}
where $\theta_P$ is the parameter of \textsc{Policy\_Net},
with action masking for invalid trades: 
\begin{equation} 
\pi(a|s)\! =\!\! \begin{cases}
0 & a \notin \mathcal{A},\\
\frac{\exp(\texttt{logits}(a))}{\sum_{a'\in \mathcal{A}}\exp(\texttt{logits}(a^\prime))} & \text{otherwise}.
\end{cases}
\label{eq:masking}
\end{equation}
This ensures that actions outside the valid set 
$\cA$ (e.g., selling when no stocks are held) have zero probability, preventing invalid execution.

\subsubsection{Value Network Design}
The value network serves as the critic in the RL framework, estimating the expected return of a given state to guide the policy network's optimization. To efficiently leverage the shared LLM representation, the value network shares the same backbone as the policy network, processing the textual state representation through the frozen and trainable layers  \eqref{eq:template}–\eqref{eq:trainable_layer}. This design ensures efficient parameter utilization while maintaining a structured and informative state encoding. 
After passing through the LLM processing layers, the output 
$h^{(2)}$  is fed into a separate value prediction head, which maps the extracted features to a scalar value estimation:
\begin{equation}
V(s) = \textsc{Value\_Net}(h^{(2)}, \theta_V)\in\mathbb{R}^{1},
\label{eq:value_pred}
\end{equation}
where $\theta_V$ is the parameter of \textsc{Value\_Net}.


\subsection{Online Policy Gradient Learning}
The policy and value networks in \textsc{FLAG-Trader} are trained using an online policy gradient approach, ensuring that the model continuously refines its decision-making ability. The learning process follows an iterative cycle of state observation, action generation, reward evaluation, and policy optimization. The parameters of the model are updated using stochastic gradient descent (SGD), leveraging the computed policy and value losses to drive optimization.

At each training step, we define two key loss functions, i.e.,
\emph{policy loss} 
$\mathcal{L}_P$: measures how well the policy network aligns with the expected advantage-weighted log probability of actions; 
\emph{value loss} $\mathcal{L}_V$: ensures that the value network accurately estimates the expected return.

\begin{remark}
The definitions of \emph{policy loss} and \emph{value loss} may vary across different actor-critic (AC) algorithms. Here, we present a general formulation for clarity and ease of expression. Notably, our framework is designed to be flexible and adaptable, making it compatible with a wide range of AC algorithms.
\end{remark}



Based on these loss functions, the model updates the respective network parameters using backpropagation as follows. 

\textbf{Update Policy Head.} 
The policy network parameters 
 $\theta_P$ are updated via SGD to minimize the \emph{policy loss} $\mathcal{L}_P$ 
\begin{align}\label{eq:P}
    \theta_P\leftarrow\theta_P-\eta \nabla_{\theta_P}\mathcal{L}_P,
\end{align}
where $\eta$ is the learning rate for updating policy head $\theta_P$.

\textbf{Update Value Head.} The value network parameters 
$\theta_V$ are optimized via SGD to minimize the temporal difference (TD) error over policy loss $\mathcal{L}_V$
\begin{align}\label{eq:V}
    \theta_V\leftarrow\theta_V-\eta \nabla_{\theta_V}\mathcal{L}_V.
\end{align}

\textbf{Update Trainable LLM Layers.}
The trainable LLM parameters 
$\theta_{\texttt{train}}$ are updated via SGD jointly based on both the policy and value losses, i.e., $\mathcal{L}_P$ and $\mathcal{L}_V$, allowing the shared LLM representation to align with optimal decision-making:  
\begin{align}\label{eq:train}
    \theta_{\texttt{train}}\leftarrow\theta_{\texttt{train}}-\beta \nabla_{\theta_{\texttt{train}}}(\mathcal{L}_P+\mathcal{L}_V),
\end{align}
where $\beta$ is the learning rate for LLM parameter $\theta_{\texttt{train}}$. 

The updates in \eqref{eq:P}–\eqref{eq:train} are performed iteratively until the stopping criteria are met, as outlined in Algorithm \ref{alg:1}. This iterative learning process effectively balances exploration and exploitation, enhancing policy performance while maintaining stability. To mitigate overfitting and policy divergence, we employ Proximal Policy Optimization (PPO), which constrains updates by limiting the divergence from previous policies, ensuring more controlled and reliable learning. The detailed procedure of how to compute \emph{policy loss} $\cL_P$ and \emph{value loss} $\cL_P$ can be found in Appendix \ref{sec:Appendix_A}.
 






\begin{algorithm}[ht]
\caption{\textsc{FLAG-Trader}}
\begin{algorithmic}[1]
\STATE \textbf{Require:} Pre-trained LLM with parameter  $\theta:=(\theta_{\texttt{frozen}}, \theta_{\texttt{train}})$, environment dynamics $\cT$, reward function $\mathcal{R};$
\STATE Initialize policy network $\theta_P$ and value network $\theta_V$ with shared LLM trainable layers $\theta_{\texttt{train}}$;
\STATE Initialize experience replay buffer $B \leftarrow \emptyset$

\FOR{iteration $t=1,2,\ldots$,}
    \STATE Fetch the current state $s_t$ from the environment and construct an input prompt \texttt{lang}($s_t$);
    \STATE Pass prompt \texttt{lang}($s_t$) through LLM;
    \STATE \textsc{Policy\_Net} outputs $a_t$ from action space $\{\texttt{``buy,'' ``sell,'' ``hold''}\}$ based on \eqref{eq:masking};
    \STATE Execute action $a_t$ in the environment and observe reward $r(s_t, a_t)$ and transition to new state $s_{t+1}$;
    \STATE Store experience tuple $(s_t, a_t, r_t, s_{t+1})$ in replay buffer $B$;
    
    \IF{ $t \mod \tau=0$}
        \STATE Update policy head $\theta_P$ according to \eqref{eq:P};
        \STATE Update value head $\theta_V$ according to \eqref{eq:V};
        \STATE Update the trainable LLM layers $\theta_{\texttt{train}}$ according to \eqref{eq:train}.
        
    \ENDIF
\ENDFOR

\STATE \textbf{Return:} Fine-tuned \textsc{Policy\_Net}($\theta_P$).
\end{algorithmic}
\label{alg:1}
\end{algorithm}


