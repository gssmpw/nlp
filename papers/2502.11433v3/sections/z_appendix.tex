\section{Additional Algorithmic  Details: \textsc{FLAG-Trader} with PPO}\label{sec:Appendix_A}



In this section, we outline a detailed procedure for training the \textsc{FLAG-Trader} architecture via PPO, where the \textsc{Policy\_Net} (actor) and the \textsc{Value\_Net} (critic) share a subset of trainable parameters from a LLM, with
$\theta = \big( \theta_{\texttt{train}}, \theta_P, \theta_V\big)$.
We define $\theta_{policy} = \big( \theta_{\texttt{train}}, \theta_P)$ and $\theta_{value} = \big( \theta_{\texttt{train}}, \theta_V)$ for simplicity.

\textbf{Advantage Estimation.}
We use the Generalized Advantage Estimation (GAE) to compute the advantage function \( A_t \):
\begin{align}
    A_t \;=\; \sum_{k=0}^{T-1} (\gamma \lambda)^k \bigl[r_{t+k} + \gamma V_{\theta_{value}}(s_{t+k+1}) - V_{\theta_{value}}(s_{t+k})\bigr],
\end{align}
where \( \gamma \) is the discount factor, and \( \lambda \) is the GAE parameter.

\textbf{Probability Ratio.}
Let \(\theta_{policy, \mathrm{old}}\) denote the parameters before the current update. The PPO probability ratio is
\begin{align}
    r_t(\theta_{policy}) \;=\; \frac{\pi_{\theta_{policy}}(a_t \mid s_t)}{\pi_{\theta_{policy,\mathrm{old}}}(a_t \mid s_t)}.
\end{align}


\textbf{PPO Clipped Objective.}
PPO clips this ratio to prevent overly large updates. The surrogate objective is
\begin{align}
    \mathcal{L}_P(\theta_{policy}) \;=\; \mathbb{E}_t \Bigl[
  \min\bigl(r_t(\theta_{policy})\,A_t,\; \text{clip}\bigl(r_t(\theta_{policy}),\,1-\varepsilon,\,1+\varepsilon\bigr)\,A_t\bigr)
\Bigr],
\end{align}
where \(\varepsilon\) is a hyperparameter.

\textbf{Value Function Loss.}
The critic (value network) is updated by minimizing the difference between the predicted value \(V_{\theta_{value}}(s_t)\) and the target return \(R_t\). A common choice is:
\begin{align}
    \mathcal{L}_{V}(\theta_{value}) \;=\; \mathbb{E}_t\Bigl[(V_{\theta_{value}}(s_t) - R_t)^2\Bigr].
\end{align}


\textbf{Combined Loss.}
We often add an entropy term to encourage exploration, yielding the overall objective:
\begin{align}
    \mathcal{L}_{\text{total}}(\theta)
\;=\;
-\,\mathcal{L}_{P}(\theta_{policy})
\;+\;
c_1\,\mathcal{L}_{V}(\theta_{value})
\;-\;
c_2\,\mathcal{H}\bigl(\pi_{\theta_{policy}}\bigr),
\end{align}
where \(c_1\) and \(c_2\) are weighting coefficients, and \(\mathcal{H}(\pi_{\theta_{policy}})\) represents the policy entropy.

\textbf{Parameter Updates.}
At each iteration, we apply gradient descent on the total loss:
\begin{align}
\theta_P       &\leftarrow \theta_P       \;-\; \eta \;\nabla_{\theta_P}\,\mathcal{L}_P, \\
\theta_V       &\leftarrow \theta_V       \;-\; \eta \;\nabla_{\theta_V}\,\mathcal{L}_V, \\
\theta_{\texttt{train}} &\leftarrow \theta_{\texttt{train}} - \beta \;\nabla_{\theta_{\text{train}}}\,\mathcal{L}_{\text{total}},
\end{align}
where \(\eta\) and \(\beta\) are learning rates for the policy head, value head, and trainable LLM layers respectively. The algorithm is summerized in Algorithm \ref{alg:flagtrader-ppo}.

\begin{algorithm}[H]
\caption{FLAG-TRADER with PPO}
\label{alg:flagtrader-ppo}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Pre-trained LLM parameters $(\theta_{\texttt{frozen}}, \theta_{\texttt{train}})$; actor parameters $\theta_P$; critic parameters $\theta_V$; environment $\mathcal{E}$; discount factor $\gamma$; GAE parameter $\lambda$; PPO clip $\varepsilon$; learning rates $\eta, \beta$;
\STATE Initialize $\theta_{\text{train}}, \theta_P, \theta_V$; let $\theta_{\mathrm{old}} \leftarrow \theta$
\STATE Initialize replay buffer $B \leftarrow \emptyset$
\FOR{iteration = 1 to \text{max\_iters}}
  \STATE // \textit{Collect Rollouts}
  \FOR{t = 1 to T}
    \STATE Fetch the current state $s_t$ from the environment and construct an input prompt \texttt{lang}($s_t$);
    \STATE Pass prompt \texttt{lang}($s_t$) through LLM;
    \STATE \textsc{Policy\_Net} outputs $a_t$ from action space $\{\texttt{``buy,'' ``sell,'' ``hold''}\}$ based on \eqref{eq:masking};
    \STATE Execute action $a_t$ in the environment and observe reward $r(s_t, a_t)$ and transition to new state $s_{t+1}$;
    \STATE Store experience tuple $(s_t, a_t, r_t, s_{t+1})$ in replay buffer $B$;
    
  \ENDFOR

  \STATE // \textit{Compute Advantage and Targets}
  \FOR{\textbf{each} transition in $B$}
    \STATE Compute $V_{\theta_{value}}(s_t)$ and advantage $A_t$ (e.g., via GAE)
  \ENDFOR

  \STATE // \textit{Perform PPO Updates}
  \FOR{update\_epoch = 1 to K}
    \STATE Sample mini-batch $\mathcal{M}$ from $B$
    \STATE Compute probability ratio $r_t(\theta_{policy}) \;=\; \frac{\pi_{\theta_{policy}}(a_t \mid s_t)}{\pi_{\theta_{policy,\mathrm{old}}}(a_t \mid s_t)}$;
    \STATE Compute PPO loss $\mathcal{L}_P(\theta_{policy}) \;=\; \mathbb{E}_t \Bigl[
  \min\bigl(r_t(\theta_{policy})\,A_t,\; \text{clip}\bigl(r_t(\theta_{policy}),\,1-\varepsilon,\,1+\varepsilon\bigr)\,A_t\bigr)
\Bigr]$;
    \STATE Compute Value loss $\mathcal{L}_{V}(\theta_{value}) \;=\; \mathbb{E}_t\Bigl[(V_{\theta_{value}}(s_t) - R_t)^2\Bigr]$;
    \STATE Compute total loss $\mathcal{L}_{\text{total}}(\theta)
\;=\;
-\,\mathcal{L}_{P}(\theta_{policy})
\;+\;
c_1\,\mathcal{L}_{V}(\theta_{value})
\;-\;
c_2\,\mathcal{H}\bigl(\pi_{\theta_{policy}}\bigr)$;
    \STATE Perform gradient descent on each parameter group:
    \begin{align*}
\theta_P       &\leftarrow \theta_P       \;-\; \eta \;\nabla_{\theta_P}\,\mathcal{L}_P, \\
\theta_V       &\leftarrow \theta_V       \;-\; \eta \;\nabla_{\theta_V}\,\mathcal{L}_V, \\
\theta_{\texttt{train}} &\leftarrow \theta_{\texttt{train}} - \beta \;\nabla_{\theta_{\text{train}}}\,\mathcal{L}_{\text{total}};
\end{align*}
  \ENDFOR

  \STATE // \textit{Update old policy parameters}
  \STATE Update $\theta = \big( \theta_{\texttt{train}}, \theta_P, \theta_V\big)$ by $\theta_{\mathrm{old}} \leftarrow \theta$;
\ENDFOR
\STATE \textbf{Return:} Fine-tuned \textsc{Policy\_Net}($\theta_P$).
\end{algorithmic}
\end{algorithm}




% \begin{algorithm}
% \caption{FinRL-LLM Pipeline with PPO}
% \label{alg:FinRL-LLM-PPO}
% \begin{algorithmic}[1]
% \STATE  \textbf{Input:} Pre-trained LLM policy $\pi_{\theta}$, environment dynamics, reward function $r(s_t, a_t)$, learning parameters (replay buffer size, batch size, PPO clipping parameter $\epsilon$, discount factor $\gamma$, GAE parameter $\lambda$, learning rate $\eta$)
% \STATE  \textbf{Output:} Fine-tuned LLM policy $\pi_{\theta}$

% \STATE  Initialize LLM policy $\pi_{\theta}$ with pre-trained weights
% \STATE  Initialize experience replay buffer $B \leftarrow \emptyset$
% \FOR{each training iteration}
%     \STATE  Fetch the current state $s_t$ from the environment
%     \STATE  Convert $s_t$ into structured text $\text{lang}(s_t)$
%     \STATE  Encode $\text{lang}(s_t)$ into token embeddings $E_t \leftarrow \text{Embed}(\text{lang}(s_t))$
%     \STATE  Pass through LLM:
%     \STATE  $h_t^{(1)} \leftarrow \text{LLM}_{\text{frozen}}(E_t)$
%     \STATE  $h_t^{(2)} \leftarrow \text{LLM}_{\text{trainable}}(h_t^{(1)})$
%     \STATE  Compute action probability distribution:
%     \STATE  $\text{logits}_t \leftarrow \text{POLICY\_NET}(h_t^{(2)})$
%     \STATE  $\pi_{\theta}(a_t | s_t) \leftarrow \text{Softmax}(\text{logits}_t)$
%     \STATE  Sample action $a_t \sim \pi_{\theta}(a_t | s_t)$
%     \STATE  Execute $a_t$ in the environment and observe reward $r_t$ and transition to $s_{t+1}$
%     \STATE  Compute state value $V(s_t) \leftarrow \text{VALUE\_NET}(h_t^{(2)})$
%     \STATE  Store experience $(s_t, a_t, r_t, s_{t+1}, V(s_t), \pi_{\theta}(a_t | s_t))$ in replay buffer $B$

%     \IF{replay buffer $B$ is full}
%         \STATE  Sample a mini-batch of experiences $\{(s_i, a_i, r_i, s_{i+1}, V(s_i), \pi_{\theta_{\text{old}}}(a_i | s_i))\}$ from $B$
%         \STATE  Compute advantage estimation using GAE:
%         \STATE  $A_i = \sum_{l=0}^{T-t} (\gamma \lambda)^l (r_i + \gamma V(s_{i+1}) - V(s_i))$
%         \STATE  Compute log-likelihood ratio:
%         \STATE  $r_i(\theta) = \frac{\pi_{\theta}(a_i | s_i)}{\pi_{\theta_{\text{old}}}(a_i | s_i)}$
%         \STATE  Compute PPO clipped objective:
%         \STATE  $L_{\text{PPO}}(\theta) = \mathbb{E}_t [\min( r_i(\theta) A_i, \text{clip}(r_i(\theta), 1 - \epsilon, 1 + \epsilon) A_i )]$
%         \STATE  Compute value loss:
%         \STATE  $L_V = (V(s_i) - V_{\text{target}}(s_i))^2, \quad V_{\text{target}}(s_i) = r_i + \gamma V(s_{i+1})$
%         \STATE  Compute entropy bonus:
%         \STATE  $S(\pi_{\theta}) = - \sum_a \pi_{\theta}(a | s) \log \pi_{\theta}(a | s)$
%         \STATE  Compute total loss:
%         \STATE  $L_{\text{total}} = -L_{\text{PPO}} + c_1 L_V - c_2 S(\pi_{\theta})$
%         \STATE  Perform gradient descent on $\theta$ to minimize $L_{\text{total}}$
%         \STATE  Update $\pi_{\theta}$, VALUE\_NET, and trainable LLM layers
%         \STATE  Clear replay buffer $B$
%     \ENDIF
% \ENDFOR

% \STATE  \textbf{Return:} Fine-tuned LLM policy $\pi_{\theta}$
% \end{algorithmic}
% \end{algorithm}

% The online PPO module is responsible for training the LLM to improve its policy over time. The learning process involves the following steps:
% \begin{enumerate}
%     \item \textbf{Log-Likelihood Computation}: The LLM generates an action \(a_t\) conditioned on the state \(s_t\) and the prompt. The log-likelihood \(\log \pi_\theta(a_t | s_t, \text{prompt})\) is computed to measure the policy's performance.
%     \item \textbf{Environment Interaction}: The action \(a_t\) is executed in the environment, which transitions to a new state \(s_{t+1}\) and returns a reward \(r(s_t, a_t)\).
%     \item \textbf{Experience Replay}: The observed experience tuple \((s_t, a_t, r_t, s_{t+1})\) is stored in the replay buffer for batch updates.
%     \item \textbf{PPO Objective}: The policy is updated using the PPO objective:
%     \[
%     L_{\text{PPO}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right],
%     \]
%     where \(r_t(\theta)\) is the probability ratio, \(A_t\) is the advantage function, and \(\epsilon\) is a hyperparameter controlling the update clipping.
% \end{enumerate}

\section{Additional Experimental Details}


% \subsection{Details on Evaluation Metrics}
% Below is a brief overview of these metrics:\par
% \noindent\textbf{Cumulative Return (CR) \%} measures the total value change of an investment over time by summing daily logarithmic returns, shown in \eqref{eq:cum_return}: 
% \begin{align}
%    \label{eq:cum_return}
%    \textbf{CR} &= \sum_{t=1}^{n} r_i = \sum_{t=1}^{n} \left[ \ln\left(\frac{p_{t+1}}{p_t}\right) \cdot \text{action}_t \right],
% \end{align}
% where $r_i$ is the logarithmic return from day $t$ to $t+1$, $p_t$ and $p_{t+1}$ are the closing prices on days $t$ and $t+1$, respectively, and $\text{action}_t$ is the model's trading decision for day $t$.
% Notice that higher values indicate better strategy effectiveness.

% \noindent\textbf{Sharpe Ratio (SR)} assesses risk-adjusted returns by dividing the average excess return ($R_p$) over the risk-free rate ($R_f$) by its volatility ($\sigma_p$), detailed in \eqref{eq:sharpe}: 
% \begin{equation}
%     \textbf{SR} = \frac{R_p - R_f}{\sigma_p}.
%     \label{eq:sharpe}
% \end{equation}  
% Notice that higher ratios signify better performance.
  
% \noindent\textbf{Annualized Volatility (AV) \% and Daily Volatility (DV) \%} quantify return fluctuations; AV is derived by scaling DV (\textit{standard deviation of daily logarithmic returns}) by the square root of the annual trading days (252), as in \eqref{eq:annuaVol}. This metric highlights potential return deviations across the year.
% \begin{align}
%    \label{eq:annuaVol}
%     \textbf{AV} &= \textbf{DV} \times \sqrt{252}. 
% \end{align} 

% \noindent\textbf{Max Drawdown (MDD) \%} calculates the largest portfolio value drop from peak to trough, as given in \eqref{eq:maxdrawdown}. Lower values indicate lesser risk and higher strategy robustness. 
%     \begin{align}
%     \label{eq:maxdrawdown}
%     \textbf{MDD} = \text{max}(\frac{P_{\text{peak}} - P_{\text{trough}}}{P_{\text{peak}}}).
%     \end{align}



\subsection*{Hyperparameters for Finetuening \textsc{FLAG-Trader} with PPO in Algorithm \ref{alg:flagtrader-ppo}}
\begin{table}[!ht]
\centering
\caption{\textsc{FLAG-Trader} with PPO Finetuning Hyperparameters and Settings.}
\label{tab:parameters:lora}
\scalebox{0.8}{
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Default Value} & \textbf{Description} \\
\toprule
\texttt{total\_timesteps} & 13860 & Total number of timesteps \\
\texttt{learning\_rate} & \(5 \times 10^{-4}\) & Learning rate of optimizer \\
\texttt{num\_envs} & 1 & Number of parallel environments \\
\texttt{num\_steps} & 40 & Steps per policy rollout \\
\texttt{anneal\_lr} & True & Enable learning rate annealing \\
\texttt{gamma} & 0.95 & Discount factor \(\gamma\) \\
\texttt{gae\_lambda} & 0.98 & Lambda for Generalized Advantage Estimation \\
\texttt{update\_epochs} & 1 & Number of update epochs per cycle \\
\texttt{norm\_adv} & True & Advantages whitening \\
\texttt{clip\_coef} & 0.2 & Surrogate clipping coefficient \\
\texttt{clip\_vloss} & True & Clipped loss for value function \\
\texttt{ent\_coef} & 0.05 & Coefficient of entropy term \\
\texttt{vf\_coef} & 0.5 & Coefficient of value function \\
\texttt{kl\_coef} & 0.05 & KL divergence with reference model \\
\texttt{max\_grad\_norm} & 0.5 & Maximum gradient clipping norm \\
\texttt{target\_kl} & None & Target KL divergence threshold \\
\texttt{dropout} & 0.0 & Dropout rate \\
\texttt{llm} & "SmolLM2-135M-Instruct" & Model to fine-tune \\
\texttt{train\_dtype} & "float16" & Training data type \\
\texttt{gradient\_accumulation\_steps} & 8 & Number of gradient accumulation steps \\
\texttt{minibatch\_size} & 32 & Mini-batch size for fine-tuning \\
\texttt{max\_episode\_steps} & 65 & Maximum number of steps per episode \\
\bottomrule
\end{tabular}
}
\end{table}

