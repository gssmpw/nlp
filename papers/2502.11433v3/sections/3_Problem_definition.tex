\section{Problem Statement}\label{Sec:Problem Formulation}

We define the financial decision-making process as a finite horizon partially observable Markov decision process (MDP) with time index $\{0,\cdots,T\}$, represented by the tuple:
$\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{T}, R, \gamma),$
where each component is described in detail below.

\textbf{State.} The state space $\cS=\cX\times\cY$ consists of two components: market observations and trading account balance, i.e.,
$s_t = (m_t, b_t) \in \mathcal{S}.$ Specifically,
     $ m_t = (P_t, N_t) \in \cX$ represents the \textit{market observation process}, includingstock price $P_t$ at time $t$, and financial news sentiment or macroeconomic indicators $N_t$;
   $ b_t = (C_t, H_t) \in \mathcal{Y}$ represents the \textit{trading account balance}, including available cash $C_t$ at time $t$, and number of stock shares $H_t$.

\textbf{Action.} The agent chooses from a discrete set of trading actions
$\mathcal{A} = \{\texttt{Sell}: -1, \texttt{Hold}: 0, \texttt{Buy}: 1\},$
where $a_t=-1$ denotes selling all holdings (liquidate the portfolio),
$a_t=0$ denotes holding (no trading action), and
$a_t=1$ represents buying with all available cash (convert all cash into stocks).


 \textbf{State Transition.} The state transition dynamics are governed by a stochastic process
$s_{t+1} \sim \cT(\cdot | s_t, a_t).$
% Specifically, the stock price $P_t$ follows a stochastic process:
%     $
%     P_{t+1} = P_t + f_m(m_t, a_t) + \xi_t$ with $\quad \xi_t \sim \mathcal{N}(0, \sigma^2).$
The trading account evolves according to the following equations:
    \begin{itemize}
        \item If \texttt{Sell}:
        $        C_{t+1} = C_t + H_t P_{t+1}, \quad H_{t+1} = 0.
        $
        \item If \texttt{Hold}:
        $ C_{t+1} = C_t, \quad H_{t+1} = H_t.$
        \item If \texttt{Buy}:
       $        C_{t+1} = 0, \quad H_{t+1} = H_t + \frac{C_t}{P_{t+1}}.$ 
    \end{itemize}

\textbf{Reward.} 
The agent receives a reward  based on the daily trading profit \& loss (PnLs):
\begin{align*}
    R(s_t, a_t) = SR_t-SR_{t-1},
\end{align*}
where $SR_t$ denotes the Sharpe ratio at day $t$, computed by using the historical PnL from time 0 to time $t$. Moreover, PnL at time $t$ is calculated as
\begin{align*}
pnl_t:=(C_t-C_{t-1})+(H_tP_t-H_{t-1}P_{t-1}).   
\end{align*}
Then, the Sharpe ratio $SR_t$ at time $t$ can be calculated as:
\begin{align}
    \label{eq:sharpe}
    SR_t:=\frac{\mathbb{E}[pnl_1,\cdots,pnl_t]-r_f}{\sigma[pnl_1,\cdots,pnl_t]},
\end{align}
where $\mathbb{E}[pnl_1,\cdots,pnl_t]$ is the sample average of daily PnL up to time $t$, $r_f$ is the risk-free rate, and $\sigma[pnl_1,\cdots,pnl_t]$ is the sample standard deviation of daily PnL up to time $t$.

 
The goal is to find an admissible policy $\pi$ to maximize the expected value of cumulative discounted reward, i.e., 
\begin{align}\label{eq:global_obj}
   \max_{\pi} V^{\pi}(s) = \mathop{\mathbb{E}}\limits_{\substack{s_0=s,a_t\sim \pi(\cdot|s_t)\\ s_{t+1}\sim \cT(\cdot|s_t,a_t)}} \left[ \sum_{t=0}^T \gamma^t R_t \right],
\end{align}
where $R_t$ is a shortened version $R(s_t, a_t)$ and  $ \gamma \in (0,1]$ is the discount factor controlling the importance of future rewards.



Our goal is to train an LLM agent parameterized by $\theta$ to find the optimized policy $\pi_\theta$ for \eqref{eq:global_obj}, i.e.,
\begin{align}
a_t\sim\pi_\theta(\cdot|s_t)=\text{LLM}(\texttt{lang}(s_t);\theta),
\end{align}
where $\texttt{lang}(s_t)$ are the prompts generated by converting state $s_t$ into structured text. The proposed pipeline is illustrated in Figure \ref{fig:FinRL_LLM}. 




% Formally, we model a financial decision-making process as infinite horizon POMDP with time index $\mathbb{T}=\{0,1,2,\cdots, \infty\}$ and discount factor $\gamma\in(0,1]$. This POMDP contains: (1) a state space $\mathcal{S}:=\mathcal{X}\times\mathcal{Y}$ where $\mathcal{X}$ is the observable component and $\mathcal{Y}$ is unobservable component of the financial market; (2) the action space of the agent is $\mathcal{A}$, which is modeled as $\{\textit{``Buy",~``Sell",~``Hold"}\}$; (3) the reward function $R(o,b,a):\mathcal{X}\times\mathcal{Y}\times\mathcal{A}\to\mathbb{R}$ uses daily profit \& loss (PnL) as the output; (4) the observation process $\{o_t\}_{t\in\mathbb{T}}\subseteq\mathcal{X}$ is a multi-dimensional process (5) the reflection process $\{b_t\}_{t\in\mathbb{T}}\subseteq\mathcal{Y}$ represents the agent's self-reflection, which is updated from $b_t$ to $b_{t+1}$ on daily basis \cite{griffiths2023bayes}; (6) the action $a_t\sim\pi_\theta(\cdot|\text{prompt})$ represents the way to make investment decision driven by the language conditioned policy $\pi_\theta$ parameterized by $\theta$. By denoting daily profit \& loss (PnLs) by $r_t(s_t,a_t)=R(s_t:=(o_t, b_t), a_t)$, the optimization objective is to maximize the cumulative reward over a sequence of trading decisions, defined as
% \begin{align}
%    J(\theta) = \mathbb{E}_{(s_t, a_t)\sim \pi_\theta} \left[ \sum_{t=0}^\infty \gamma^t r_t(s_t,a_t) \right].
% \end{align}
% To improve the policy, the gradient of the policy is optimized using the policy gradient loss:
% \begin{align}
%     L(\pi_\theta) = -\mathbb{E}_t \left[ \log \pi_\theta(a_t | s_t) A_t \right],
% \end{align}
% where $A_t$ represents the advantage function evaluating the relative value of choosing a specific action.




% The objective of this research is to fine-tune large language models (LLMs) for financial decision-making in sequential trading environments. The model is designed to generate interpretable, full-sentence outputs such as \textit{"buy AAPL 100 shares,"} which are subsequently mapped into a simplified action space of \textbf{``buy,'' ``sell,'' or ``hold.''} Each decision is based on the current state, represented as $s_t = \{m_t, p_t, h_t\} $, where $m_t$ denotes market indicators, $p_t$ represents historical price information, and $h_t$ reflects the portfolio status.





