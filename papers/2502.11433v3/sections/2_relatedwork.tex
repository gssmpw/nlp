% \section{Related Work}
% Popular deep reinforcement learning (DRL) algorithms: value-based methods (Q-learning \cite{watkins1989learning}, SARSA \cite{rummery1994line}, DQN \cite{mnih2015human}), policy-based methods (RRL \cite{moody1998reinforcement}, REINFORCE \cite{williams1992simple}, TRPO \cite{schulman2015trust}) and actor-critic-based methods (PPO \cite{schulman2017proximal}, A2C \cite{mnih2016asynchronous}, SAC \cite{haarnoja2018soft}, DPG \cite{silver2014deterministic}, DDPG \cite{lillicrap2015continuous}). 

% Our study is inspired by the idea of utilizing large language model (LLM) as generalized policy networks \cite{szot2023large, zhai2024fine}.

% \subsection{RL in Finance}
% It is worth noting that there are some open source projects that provide full pipelines for implementing different DRL algorithms in financial applications \cite{liu2022finrl,liu2021finrl}. Literature of DRL algorithms in financial decision-making tasks can be categorized by the type of algorithms:\\
% (1) Value-based methods: \cite{gao2000algorithm}, \cite{jangmin2006adaptive}, \cite{lee2002multi}, \cite{lim2018reinforcement}, \cite{riva2021learning}, \cite{de2020tabular}, \cite{jeong2019improving}, \cite{niu2022metatrader}, \cite{sun2022deepscalper}, \cite{yuan2020using}\\
% (2) Policy-based methods: \cite{dempster2006automated}, \cite{deng2016deep}, \cite{shi2019multi}, \cite{si2017multi}, \cite{xu2021relation}, \cite{wang2021commission}, \cite{zhang2020cost}, \cite{wang2021deeptrader} \\
% (3) Actor-critic-based methods: \cite{liu2018practical}, \cite{zarkias2019deep}, \cite{ye2020reinforcement}, \cite{liang2018adversarial}, \cite{lin2021end}, \cite{yang2020deep}, \cite{sawhney2021quantitative}, \cite{Zhang2019DeepRL}, \cite{huang2024novel}, \cite{avramelou2024deep}.\\
% For further interests in DRL algorithms for financial applications, check surveys: \cite{hambly2023recent}, \cite{sun2023reinforcement}, \cite{sahu2023overview}

% \subsection{LLM in Finance}
% The rapid developement of general-domain language models (LMs) has stimulated the exploration of financial LMs, such as pre-trained LMs: \textsc{FinBert} \cite{liu2021finbert,yang2020finbert,araci2019finbert,huang2023finbert}, \textsc{FinBERT-MRC} \cite{zhang2023finbert}, \textsc{FLANG} \cite{shah2022flue}, and several financial LLMs: \textsc{FinGPT}\cite{liu2023fingpt}, \textsc{FinMA} \cite{xie2023pixiu}, \textsc{InvestLM} \cite{yang2023investlm}, {BloombergGPT} \cite{wu2023bloomberggpt}, which leverage extensive training on diverse financial datasets (e.g. stock price data, financial news and analyst reports) and adapt the capabilities of LMs to the unique needs of financial applications.

% \subsection{LLM agent for Sequential Decision Making}
% Moreover, there is emergence of financial LLM-based agent frameworks such as \textsc{FinMem} \cite{yu2024finmem}, \textsc{FinAgent} \cite{zhang2024finagent}, \textsc{CryptoTrade} \cite{li2024cryptotrade}, \textsc{FinRobot} \cite{yang2024finrobot}, and \textsc{FinCon} \cite{yu2024fincon} has presented a variety of architectural approaches tailored to specific financial tasks. 
\section{Related Work}
\label{sec:related-work}

\textbf{RL in Finance.}
RL has shown promise for financial decision-making, spanning Q-learning approaches for Sharpe ratio maximization \cite{gao2000algorithm}, dynamic asset allocation \cite{jangmin2006adaptive}, deep Q-learning \cite{jeong2019improving}, tabular SARSA \cite{de2020tabular}, policy-based portfolio optimization \cite{shi2019multi}, and actor-critic methods \cite{ye2020reinforcement} enhanced by adversarial training \cite{liang2018adversarial} and transformer-based architectures \cite{huang2024novel}. 
Recent research efforts in RL for financial applications have been greatly aided by open-source frameworks like FinRL \cite{liu2022finrl}, which standardize implementations and provide reproducible benchmarks. Comprehensive surveys \cite{hambly2023recent,sun2023reinforcement} further showcase advances in both methodological rigor and real-world deployment.
Despite these advances, RL-based trading still requires large training data, struggles with non-stationary markets, and faces challenges incorporating multimodal information in real time.



\textbf{LLMs in Finance.}
A growing trend is the integration of LLMs into financial decision-making. Hybrid systems like FinCon \cite{yu2025fincon} and TradingGPT \cite{li2023tradinggpt} leverage language understanding to enhance trading agents, while domain-specific models such as \textsc{FinBERT} \cite{araci2019finbert,yang2020finbert}, \textsc{FLANG} \cite{shah2022flue} and \textsc{Open-FinLLMs} \cite{xie2024open} have excelled at financial text tasks through specialized pre-training. Recent efforts include machine reading comprehension \cite{zhang2023finbert}, open-source financial LLMs \cite{liu2023fingpt}, BloombergGPT with domain-adapted tokenization \cite{wu2023bloomberggpt}, and InvestLM \cite{yang2023investlm} featuring numerical reasoningâ€”achieving strong results in sentiment analysis \cite{huang2023finbert}, earnings call interpretation \cite{xie2023pixiu}, and regulatory document processing. Additionally, \textsc{FinBen} \cite{xie2024finben}, benchmark study for LLMs in finance, have emerged to comprehensively evaluate model performance across various financial tasks. However, LLM-based methods often lack sequential decision-making mechanisms, are computationally expensive (especially with RL), and struggle with non-stationary market conditions.

\textbf{LLM Agents for Sequential Decision Making.}
The integration of LLMs with agentic frameworks has opened new avenues for financial decision-making. For instance, \textsc{FinMem} \cite{yu2024finmem} introduced memory-augmented LLM agents for portfolio management, \textsc{FinAgent} \cite{zhang2024finagent} leveraged hierarchical structures in high-frequency trading, and multi-agent systems like \textsc{FinRobot} \cite{yang2024finrobot} and \textsc{FinCon} \cite{yu2024fincon} emphasize contextual adaptation and collaboration. Meanwhile, fine-tuning LLMs and vision-language models (VLMs) with reinforcement learning has proven effective in complex tasks: LLaRP \cite{szot2023large} positions LLMs as generalizable policies for embodied tasks, and RL-tuned VLMs \cite{zhai2024fine} enhance multi-step decision-making.
However, LLMs remain computationally expensive for real-time deployment, and risk-sensitive trading demands robustness to non-stationary markets, calling for careful model complexity and balanced exploration-exploitation.

 \begin{figure*}
     \centering
     \includegraphics[width=0.9\linewidth]{figures/Finance-PPO.pdf}
     \caption{The \textsc{FLAG-Trader} pipeline for financial trading, utilizing an LLM-based actor-critic architecture.  The LLM consists of \textbf{frozen base layers} $\theta_{\texttt{frozen}}$ that retain pre-trained knowledge and \textbf{trainable top layers} $\theta_{\texttt{train}}$ for financial decision-making. Both the \textsc{Policy\_Net}  and \textsc{Value\_Net}  share these trainable layers while maintaining separate \textit{policy head} $\theta_P$ and \textit{value head} $\theta_V$, which are updated by policy gradient method.}
     \label{fig:AC-network}
 \end{figure*}