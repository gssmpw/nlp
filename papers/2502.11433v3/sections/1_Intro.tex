\section{Introduction}

\begin{figure*}
     \centering
     \includegraphics[width=0.9\linewidth]{figures/FinRL_LLM.pdf}
     \caption{A high-level overview of our LLM-based reinforcement learning setup for financial trading. The environment provides the current state 
$s_t$. A prompt containing task details, the action space, and the current state is fed into the LLM, which outputs a trading action 
$a_t$. The action is executed in the environment, yielding a reward 
$r(s_t, a_t)$ and next state 
$s_{t+1}$. The log-likelihood 
$\log_{\pi_\theta}(a_t|\texttt{lang}(s_t))$ is then leveraged by a policy gradient method (e.g., PPO), with experience tuples stored in a replay buffer for iterative updates.}
     \label{fig:FinRL_LLM}
 \end{figure*}
Algorithmic financial trading represents a critically complex decision-making domain that perpetually grapples with the intertwined challenges of synthesizing heterogeneous market signals and dynamically refining strategies \cite{hambly2023recent,yu2024fincon,li2023tradinggpt}. Traditional reinforcement learning (RL) approaches, despite their theoretical grounding in Markov Decision Processes (MDPs), confront three fundamental limitations when deployed in financial markets. Firstly, their inability to coherently model multimodal market states—spanning sequential price movements, quantitative technical indicators, and unstructured textual sentiments—compromises data integration \cite{Zhang2019DeepRL,nassirtoussi2014text}. Secondly, non-stationary data distributions inherent to financial systems systematically erode strategy generalizability across market regimes \cite{Zhang2019DeepRL}. Thirdly, the heavy reliance on manually crafted technical indicators (e.g., MACD, RSI) and complex feature engineering \cite{liang2018adversarial} introduces subjective biases, leads to information loss, and reduces the robustness of real-time decision-making, especially in volatile market conditions.

% \textcolor{red}{Algorithmic financial trading operates within a highly intricate decision-making environment, continually challenged by the integration of heterogeneous market signals and the adaptive refinement of trading strategies. Recently,  an increasing number of studies have explored deep reinforcement learning (DRL) techniques for financial trading, leveraging their ability to model complex patterns and adapt to dynamic market conditions \cite{hambly2023recent}. Traditional DRL approaches, despite their theoretical foundation in Markov Decision Processes (MDPs), face three critical limitations in financial applications. First, they heavily rely on manually crafted technical indicators (e.g., MACD, RSI) or complex feature engineering \cite{liang2018adversarial}, which can introduce subjective biases and lead to information loss \cite{Zhang2019DeepRL}. Second, conventional RL models, particularly those employing standard multilayer perceptrons (MLPs), exhibit poor generalization outside the training data distribution, struggling to maintain stability across different market cycles \cite{Zhang2019DeepRL}. Third, traditional RL lacks the capacity to effectively incorporate unstructured financial data -- such as news articles and social media sentiment -- resulting in limited ability to synthesize multimodal information from diverse sources like textual reports and sequential price movements \cite{Zhang2019DeepRL}, since the architectures of traditional neural networks (e.g. LSTM, RNN) brings limitations \cite{nassirtoussi2014text}. These constraints collectively hinder the robustness and adaptability of RL-based trading strategies in real-world financial environments.}

The emergence of Large Language Models (LLMs) offer significant potential for financial decision-making by addressing key limitations of RL-based trading strategies. Leveraging their transformer architecture, they serve as multimodal feature extractors, integrating time-series and textual data, capturing long-range dependencies, and generalizing across market regimes, while also extracting nuanced sentiment signals without relying on manually crafted features \cite{chen2021decision,yang2023fingpt,jin2023time,wood2021trading,yu2024finmem,deng2023llms}. Nonetheless, adapting LLMs for trading presents key challenges. First, their deployment often relies on agentic frameworks \cite{li2024cryptotrade,li2023tradinggpt,yu2025fincon}, which incur high implementation and operational costs due to their complex architecture. Second, LLMs are primarily trained for static text generation, making them ill-suited for sequential decision-making in trading.  This prompts us to the following question:

% \textcolor{red}{The emergence of Large Language Models (LLMs) presents potential for financial decision systems by addressing key limitations of traditional RL-based trading strategies. LLMs (and its intrinsic transformer architecture) excel as multimodal feature extractors, seamlessly integrating time-series and textual data \cite{chen2021decision,yang2023fingpt}. They capture long-range dependencies in financial time series \cite{jin2023time}, outperforming LSTMs in decision-making, and generalize across market regimes, enhancing adaptability \cite{wood2021trading,yu2024finmem}. Additionally, LLMs extract nuanced market sentiment from news and social media without manually crafted features \cite{deng2023llms}. However, directly adapting LLMs for trading faces several challenges: it typically requires the use of an agentic framework \cite{li2024cryptotrade,li2023tradinggpt,yu2025fincon}, which often incurs relatively high implementation and operational costs due to its complex architecture and decision-making structure; conventional training focuses on static text generation rather than sequential decision-making, and financial rewards (e.g., Sharpe ratios) are not optimization signals.}

\vspace{-0.1in}
\begin{tcolorbox}[colback=white!5!white,colframe=white!75!white]
\textit{
Can we design a framework that seamlessly integrates LLMs' reasoning with RL's reward-driven optimization to tackle the challenges of financial sequential decision-making? }
\end{tcolorbox}
\vspace{-0.15in}



To resolve these interconnected challenges, we \textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven RL policy optimization, as shown in Figure \ref{fig:FinRL_LLM}. This framework advances two synergistic innovations: a parameter-efficient fine-tuning module that jointly encodes temporal market data and textual streams into unified state representations and a hybrid RL component that explicitly incorporates external environment reward gradients into policy updates, ensuring alignment with trading performance metrics.  Our contributions are summarized as follows.

First, we propose the \textsc{FLAG-Trader} framework, where a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. The model processes market data using a textual state representation, enabling it to interpret and respond to market conditions effectively. Rather than fine-tuning the entire LLM, only a subset of its parameters is updated, balancing domain adaptation and knowledge retention. This design allows \textsc{FLAG-Trader} to make informed trading decisions while remaining computationally efficient and preserving the LLM’s general reasoning capabilities.

Second, we conduct extensive experiments to evaluate \textsc{FLAG-Trader} across multiple financial trading tasks. Our results demonstrate that \textsc{FLAG-Trader} consistently outperforms both the buy-and-hold strategy and LLM-agentic baselines, particularly in terms of cumulative return and Sharpe ratio, which we prioritize for financial performance assessment. Notably, our approach enables a small-scale (135M parameter) open-source LLM to surpass much larger proprietary models, highlighting the effectiveness of RL fine-tuning in optimizing LLM-driven trading strategies. These findings underscore the potential of integrating LLMs with RL to enhance financial decision-making while maintaining computational efficiency.



