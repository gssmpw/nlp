\section{Related Works}
Language models vary in size to suit different scenarios. According to the scaling law~\cite{kaplan2020scaling}, increasing the size of LLMs often enhances their performance in downstream tasks. However, larger models require more computational resources, slower inference speeds, and greater GPU memory. To facilitate the deployment of large models in resource-constrained environments, smaller-sized models like Qwen~\cite{bai2023qwen, chu2023qwen, yang2024qwen2} and MiniCPM~\cite{hu2024minicpm} have been introduced. Qwen, for instance, offers a model with 0.5 billion parameters. To balance resource usage and performance, we propose a 3-billion-parameter language model. 


Moreover, the emergent capabilities of large language models (LLMs) have expanded to include the understanding of visual and audio information~\cite{flamingo, chen2022pali, li2023blip, huang2023language, peng2023kosmos, zhu2023minigpt, ye2023mplug, chen2023shikra, zhang2023video, sun2023generative}. Notable models such as CLIP~\cite{clip} and OFA~\cite{wang2022ofa} project visual and textual information into a unified representation space, facilitating downstream multimodal tasks. 
There are two primary methods for integrating vision features into LLMs: aligning vision feature encoders via (i) transformer layers or (ii) multi layer perceptrons. LLaVA~\cite{liu2023improvedllava, liu2024llavanext, liu2023llava} employs a simple projection matrix to connect the pre-trained CLIP ViT-L/14 visual encoder with the Vicuna LLM. Qwen-VLs are a series of high-performance and versatile vision-language foundation models that use transformer layers to link ViT and LLM. In this paper, we deliver the transformer solution for dynamic vision resolution inputs.

For the audio modality, researchers have attempted using well-trained audio foundation models, such as AudioGPT~\cite{huang2024audiogpt} and HuggingGPT~\cite{shen2024hugginggpt}, as tools while employing LLMs as flexible interfaces. These efforts typically involve directing LLMs to generate commands for external tools or converting human speech to text before feeding it into the LLMs. Recent works explored building end-to-end audio-text LLMs for direct speech interaction, such as SpeechGPT~\cite{zhang2023speechgpt}, BLSP~\cite{wang2024blspbootstrappinglanguagespeechpretraining}, and LLaSM~\cite{shu2023llasm}. Furthermore, Qwen-Audio~\cite{chu2023qwen} leveraged the LLaVA architecture, which has been successfully applied in vision-text LLMs, to develop a unified audio-text multi-task multilingual LLMs capable of perceiving and understanding audio inputs while preserving the textual conversational abilities.  

With the development of multimodality, integrating both vision and audio into LLMs has become an attempt to enhance the capabilities of large language models. For example, Vita~\cite{fu2024vita} propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and audio information. Baichuan-omni~\cite{li2024baichuanomni} is an open-source 7B Multimodal Large Language Model (MLLM) which processes and analyzes modalities of image, video, audio, and text. Different from previous work, we introduce Megrez-3B-Omni, an on-device multimodal large language model. Our proposed model has the ability to process visual and audio information, without diminishing the ability of handling text.