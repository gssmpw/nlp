\section{Introduction}


    With the development of large language models (LLMs) and multimodal large language models (MLLMs), there has been significant progress in the model's abilities for understanding, reasoning, and interaction. However, the huge cost of running a LLM or MLLM with massive number of parameters and extensive computation remains a significant challenge.
    Most LLMs and MLLMs are deployed on high-performance cloud servers, which limits their application on edge-side scenarios such as mobile phones, personal computers, vehicles, and robotics.
    
    To adapt to these scenarios, many lightweight LLM models have been proposed. Owing to the progressive enhancement of training corpus quality and the burgeoning availability of distilled data, the performance of relative small models has been significantly augmented~\cite{hu2024minicpm,abdin2024phi3,yang2024qwen2,yang2024qwen2_5}. Edge-side language models can break free from the limitations of remote computing resources and network constraints, allowing users to enjoy the capabilities of language models anytime and anywhere.

    Currently, the development of small language models primarily focuses on understanding and generating purely text-based content~\cite{mit2025techreview}. However, small multimodal models capable of processing text, vision, and speech simultaneously are still significantly lacking. This stands in stark contrast to the application requirements of edge devices. 
    To tackle the need for visual and speech information analysis on edge devices, we specifically designed \ours. The proposed Megrez-3B-Omni is an on-device multimodal understanding LLM model. It is an extension of the Megrez-3B-Instruct model and supports analysis of image, text, and audio modalities. The model achieves state-of-the-art accuracy in all three domains:
    
\begin{itemize}
    \item Language Understanding: Megrez-3B-Omni retains text understanding capabilities without significant trade-offs. Compared to its single-modal counterpart (Megrez-3B-Instruct), the accuracy variation is less than 2\%, maintaining state-of-the-art performance on benchmarks like C-EVAL, MMLU/MMLU Pro, and AlignBench. It also outperforms previous-generation models with 14B parameters.
    
    \item Image Understanding: By utilizing SigLip-400M for constructing image tokens, Megrez-3B-Omni outperforms models with more parameters such as LLaVA-NeXT-Yi-34B. It is one of the best image understanding models among multiple mainstream benchmarks, including MME, MMMU, and OCRBench. It demonstrates excellent performance in tasks such as scene understanding and OCR.
    
    \item  Speech Understanding: Equipped with the encoder head of Qwen2-Audio/whisper-large-v3, the model supports both Chinese and English speech input, multi-turn conversations, and voice-based questions about input images. It can directly respond to voice commands with text and achieved leading results across multiple benchmarks.
\end{itemize}


Through an optimized data processing workflow, strategic selection of training methodologies, and balanced data ratios across modalities, we demonstrate the feasibility of deploying multimodal models at the edge devices. We hope Megrez series can serve as an example for unveiling the potential of edge-side omni models, and help draw more attention to improve the research in this area.