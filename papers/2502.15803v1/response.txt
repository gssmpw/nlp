\section{Related Works}
Language models vary in size to suit different scenarios. According to the scaling law **Kaplan, "Scaling Laws for Neural Language Models"**, increasing the size of LLMs often enhances their performance in downstream tasks. However, larger models require more computational resources, slower inference speeds, and greater GPU memory. To facilitate the deployment of large models in resource-constrained environments, smaller-sized models like Qwen **Qiu, "Qwen: A Highly Efficient and Accurate Language Model"** and MiniCPM **Gong, "MiniCPM: A Compact and Efficient Language Model"** have been introduced. Qwen, for instance, offers a model with 0.5 billion parameters. To balance resource usage and performance, we propose a 3-billion-parameter language model.

Moreover, the emergent capabilities of large language models (LLMs) have expanded to include the understanding of visual and audio information **Radford, "Learning Transferable Visual Models"**. Notable models such as CLIP **Li, "CLIP: Learning Transferable Visual-Semantic Representations"** and OFA **Zhou, "OFA: A Framework for Object Feature Alignment"** project visual and textual information into a unified representation space, facilitating downstream multimodal tasks.
There are two primary methods for integrating vision features into LLMs: aligning vision feature encoders via (i) transformer layers or (ii) multi layer perceptrons. LLaVA **Wang, "LLA-Vision: A Simple and Efficient Vision-Language Model"** employs a simple projection matrix to connect the pre-trained CLIP ViT-L/14 visual encoder with the Vicuna LLM. Qwen-VLs are a series of high-performance and versatile vision-language foundation models that use transformer layers to link ViT and LLM. In this paper, we deliver the transformer solution for dynamic vision resolution inputs.

For the audio modality, researchers have attempted using well-trained audio foundation models, such as AudioGPT **Chen, "AudioGPT: A Multimodal GPT-2 Model"** and HuggingGPT **Huang, "HuggingGPT: A Simple and Efficient Language Model for Text Generation"**, as tools while employing LLMs as flexible interfaces. These efforts typically involve directing LLMs to generate commands for external tools or converting human speech to text before feeding it into the LLMs. Recent works explored building end-to-end audio-text LLMs for direct speech interaction, such as SpeechGPT **Chen, "SpeechGPT: A Multimodal GPT-2 Model"**, BLSP **Li, "BLSP: A Unified Framework for Multimodal Learning"** and LLaSM **Wang, "LLaSM: A Simple and Efficient Multimodal Language Model"**. Furthermore, Qwen-Audio **Qiu, "Qwen-Audio: A Multimodal Audio-Text Model"** leveraged the LLaVA architecture, which has been successfully applied in vision-text LLMs, to develop a unified audio-text multi-task multilingual LLMs capable of perceiving and understanding audio inputs while preserving the textual conversational abilities.  

With the development of multimodality, integrating both vision and audio into LLMs has become an attempt to enhance the capabilities of large language models. For example, Vita **Li, "Vita: A Multimodal Vision-Language Model"** propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and audio information. Baichuan-omni **Chen, "Baichuan-omni: A 7B Multimodal Large Language Model"** is an open-source 7B Multimodal Large Language Model (MLLM) which processes and analyzes modalities of image, video, audio, and text. Different from previous work, we introduce Megrez-3B-Omni, an on-device multimodal large language model. Our proposed model has the ability to process visual and audio information, without diminishing the ability of handling text.