\begin{table}[!ht]
    \centering
    \caption{Experimental results on OCR and Hallucination benchmarks. $\dagger$: The official report lacks precise information regarding the model's total parameter. The best open-source results of models below 4B parameters are highlighted in \textbf{bold}.}
    \label{tab:mllm_other_comparison}
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{lccccccccc}
        \toprule
        \multirow{2}{*}{Model} & {Size} & {OpenCompass} & {OCRBench} & {TextVQA} & {DocVQA} & {AI2D} & {HallusionBench} \\
        & {\small(B)} &  &  & {\small(val)} & {\small(test)} & {\small(test)} & {\small(avg)} \\
        \midrule
        Qwen2-VL-2B-Instruct & 2.21 & 57.2 & 79.4 & 79.7 & 90.1 & 74.7 & 41.7 \\
        InternVL2.5-2B & 2.21 & 59.9 & 80.4 & 74.3 & 88.7 & 74.9 & 42.6 \\
        BlueLM-V-3B & 3.1 & 66.1 & \textbf{82.9} & 78.4 & 87.8 & \textbf{85.3} & 48 \\
        InternVL2.5-4B & 3.71 & 65.1 & 82.8 & 76.8 & \textbf{91.6} & 81.4 & 46.3 \\
        \midrule
        Baichuan-Omni & 7$^\dagger$ & {-} & 70 & 74.3 & {-} & {-} & 47.8 \\
        MiniCPM-V-2.6 & 8.1 & 65.2 & 85.2 & 80.1 & 90.8 & 82.1 & 48.1 \\
        Qwen2-VL-7B-Instruct & 8.29 & 67 & 84.5 & 84.3 & 94.5 & 83 & 50.6 \\
       MiniCPM-Llama3-V-2.5 & 8.54 & 58.8 & 72.5 & 76.6 & 84.8 & 78.4 & 42.4 \\
        VITA & 8$\times$7 & 67.8 & 71.8 & {-} & {-} & 39.7 \\
        GLM-4V-9B & 13.9 & 59.1 & 77.6 & {-} & {-} & 81.1 & 46.6 \\
        LLaVA-NeXT-Yi-34B & 34 & 55 & 57.4 & 69.3 & {-} & 78.9 & 34.8 \\
        Qwen2-VL-72B-Instruct & 73.4 & 74.8 & 87.7 & 85.5 & 96.5 & 88.1 & 58.1 \\
        \midrule
       Megrez-3B-Omni & 3.38 & 66.2 & 82.8 & \textbf{80.3} & \textbf{91.6} & 82.1 & \textbf{50.1} \\
        \bottomrule
    \end{tabular}}
\end{table}