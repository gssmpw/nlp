[
  {
    "index": 0,
    "papers": [
      {
        "key": "kaplan2020scaling",
        "author": "Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario",
        "title": "Scaling laws for neural language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      },
      {
        "key": "chu2023qwen",
        "author": "Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others",
        "title": "Qwen2 technical report"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2024minicpm",
        "author": "Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others",
        "title": "{MiniCPM}: Unveiling the Potential of Small Language Models with Scalable Training Strategies"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      },
      {
        "key": "chen2022pali",
        "author": "Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others",
        "title": "Pali: A jointly-scaled multilingual language-image model"
      },
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "key": "huang2023language",
        "author": "Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and others",
        "title": "Language is not all you need: Aligning perception with language models"
      },
      {
        "key": "peng2023kosmos",
        "author": "Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu",
        "title": "Kosmos-2: Grounding multimodal large language models to the world"
      },
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      },
      {
        "key": "ye2023mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others",
        "title": "mplug-owl: Modularization empowers large language models with multimodality"
      },
      {
        "key": "chen2023shikra",
        "author": "Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui",
        "title": "{Shikra}: Unleashing Multimodal {LLM's} Referential Dialogue Magic"
      },
      {
        "key": "zhang2023video",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding"
      },
      {
        "key": "sun2023generative",
        "author": "Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong",
        "title": "Generative pretraining in multimodality"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2022ofa",
        "author": "Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia",
        "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2023improvedllava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "huang2024audiogpt",
        "author": "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others",
        "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shen2024hugginggpt",
        "author": "Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting",
        "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2023speechgpt",
        "author": "Dong Zhang and Shimin Li and Xin Zhang and Jun Zhan and Pengyu Wang and Yaqian Zhou and Xipeng Qiu",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2024blspbootstrappinglanguagespeechpretraining",
        "author": "Chen Wang and Minpeng Liao and Zhongqiang Huang and Jinliang Lu and Junhong Wu and Yuchen Liu and Chengqing Zong and Jiajun Zhang",
        "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "shu2023llasm",
        "author": "Shu, Yu and Dong, Siwei and Chen, Guangyao and Huang, Wenhao and Zhang, Ruihua and Shi, Daochen and Xiang, Qiqi and Shi, Yemin",
        "title": "Llasm: Large language and speech model"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chu2023qwen",
        "author": "Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "fu2024vita",
        "author": "Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and others",
        "title": "Vita: Towards open-source interactive omni multimodal llm"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2024baichuanomni",
        "author": "Yadong Li and Haoze Sun and Mingan Lin and Tianpeng Li and Guosheng Dong and Tao Zhang and Bowen Ding and Wei Song and Zhenglin Cheng and Yuqi Huo and Song Chen and Xu Li and Da Pan and Shusen Zhang and Xin Wu and Zheng Liang and Jun Liu and Tao Zhang and Keer Lu and Yaqi Zhao and Yanjun Shen and Fan Yang and Kaicheng Yu and Tao Lin and Jianhua Xu and Zenan Zhou and Weipeng Chen",
        "title": "Baichuan-Omni Technical Report"
      }
    ]
  }
]