\section{Large Language Model}

\subsection{Tokenizer}

In designing tokenizers, it is crucial to balance maintaining an appropriately sized vocabulary for effective word embedding training and achieving a high compression rate for enhanced inference efficiency. Following the design principles of the Qwen2 tokenizer~\cite{yang2024qwen2}, we excluded tokens for languages other than Chinese and English, resulting in a vocabulary size of approximately 120,000 tokens. This reduction from 151,643 to 120,000 tokens aims to balance computational efficiency and model performance. To further improve the compression rate for Chinese, we significantly increased the proportion of Chinese in the training corpus. \autoref{tokenizer_compression_rate} provides a detailed comparison of Megrez's tokenizer with others, demonstrating that our tokenizer achieves the highest compression ratio, particularly in the Chinese corpus.

\input{tokenizer}


\subsection{Architecture}

At the beginning of the model design, we debated whether to design a unique but efficient model structure (such as sparse activation and linear attention) or use a widely adopted structure for ease of development. We chose the latter one and adopted the standard LLaMA structure. Our goal is to benefit developers to deploy the model on various platforms without modifications and minimize the complexity of future development.

\subsection{Data}

\subsubsection{Pretrain}

To create an effective pretraining dataset, it is essential that the data be diverse and cover a wide range of types, domains, and tasks. Our dataset is carefully curated to meet these criteria, incorporating public web documents, encyclopedias, books, code samples, and other sources. Additionally, our dataset is multilingual, with a significant portion of the data in English and Chinese.
We implement a comprehensive data preprocessing procedure. We begin by identifying the language using specialized tools. To enhance data diversity, we apply deduplication techniques, such as exact-match deduplication after normalization and fuzzy deduplication using MinHash and LSH algorithms. We then filter out low-quality data through a combination of rule-based and machine-learning-based methods. Specifically, we utilize multiple models to score the content, including language models, text-quality scoring models, and models designed to identify potentially offensive or inappropriate content. Additionally, we manually sample and review texts from various sources to ensure their quality. Finally, we selectively up-sample data from certain sources to ensure that our models are trained on a diverse range of high-quality content.

Inspired by the methods outlined in deepseek-coder~\cite{guo2024deepseek}, we implemented strict filtering and cleaning of GitHub code data. For Python code, we developed more refined readability rules to clean the data, which allowed us to extract high-quality datasets. Based on these cleaned data, we analyzed the reference relationships within individual repositories and applied topological sorting to the code.

\subsubsection{SFT}

Our supervised fine-tuning process is divided into two stages. In the first stage, we perform fine-tuning using a large amount of data with a short context length of 4K. In the second stage, we fine-tune using a mixture of short and long text conversation data with a long context length of 32K.

For our short text data, we utilized open-source instructional datasets that encompass various domains such as dialogue, code, mathematics, instruction following, and complex reasoning. To enhance the complexity of the instructions, we employed a self-evolution method, ensuring that our instructional data spans a diverse range of difficulty levels. 

For our long text data, we curated a selection of high-quality open-source datasets, including tasks such as long text summarization and long text question-answering. To further diversify our long text instructional data, we sourced high-quality long text data. 
Building upon this foundation, we employed LLMs to auto-generate both instructions and responses.

\subsection{Training Stages}

\subsubsection{Pretrain}

We utilized a multi-stage training paradigm, involving different distributions of pretrain data at different stages. Since our base language model was trained from scratch, we aimed to inject and enhance abilities of the model from different perspectives at each stage, which will be detailed as follows.

\textbf{Pretain Stage.} The primary goal of the Pretrain Stage in Megrez is to establish robust language modeling capabilities. At this stage, the majority of the training data is derived from Common Crawl (CC) in both Chinese and English, GitHub code repositories, and a corpus of books. The learning rate was initially set to \(3e{-5}\), then gradually increased to \(3e{-4}\) during the first 3\% of the total training steps in this stage, after which it remained constant until the model was trained on \textasciitilde 2 trillion tokens. Finally, the learning rate was decayed to \(3e{-5}\) following a cosine schedule. During the decay phase, the model was trained on the last part of tokens from high-quality datasets. To optimize training efficiency, the sequence length was set to 4k tokens for this training stage. In anticipation of extending the model's context length to 32k tokens in later stages, the base value in the Rotary Position Embedding (RoPE) framework~\cite{su2024roformer} was set to 5,000,000.

\textbf{Continue Pretrain Stage.} Our objective at this stage was to enhance the model's ability of long-context understanding. To achieve this, we curated a corpus with longer sequence lengths, drawn from books and other high-quality data sources. Since this stage served as an additional training phase following the model's pretraining, a small fraction of the training data from previous stages was also included to prevent any degradation of the model's general capabilities while extending its context length. The initial learning rate for this stage was set to \(3e{-5}\), which decayed to \(3e{-6}\) using a cosine decay schedule. 

\subsubsection{SFT}

\textbf{Megrez-Instruct-4K.} In the first stage, the pre-trained model was fine-tuned using millions of examples with a context length of 4K that cover skills such as instruction following, coding, mathematics, logical reasoning, role-playing, and multilingualism. This fine-tuning was conducted over 3 epochs with a learning rate set to \(1e{-5}\).

\textbf{Megrez-Instruct-32K.} In the second stage, the model was further fine-tuned using mixed instructional data points, which included both short and long text data, with a context length of 32K. This fine-tuning was also conducted over 3 epochs, but with a reduced learning rate of \(5e{-6}\). This stage aimed to improve the model's capability to handle long text data while maintaining its performance on short text data.

By employing this two-stage fine-tuning strategy, we ensured that the model achieved a balanced and robust performance across both short and long text data.

\subsubsection{Web Search}

We have developed a pipeline that enables large language models (LLMs) to utilize web search when answering questions\footnote{https://github.com/infinigence/InfiniWebSearch}. This approach helps reduce hallucinations and improves the quality of the model's responses. Within this pipeline, the LLM can autonomously decide whether to use web search based on the user's query. If web search is employed, the LLM extracts relevant information from the search results and summarizes it. The final answer to the user's question is then formulated based on the summarized information from multiple web sources.

Regarding model training, we introduced a lightweight post-training phase after the supervised fine-tuning (SFT) stage to enhance the model's function-calling capabilities. The training data for this phase includes only a few of the general SFT training data and web search task synthetic data.