\newcommand{\ouromni}[0]{Megrez-3B-Omni\ }
\newcommand{\ouromnis}[0]{Megrez-3B-Omni's\ }

\section{Experiment}

\subsection{Language Performance}
We conducted a comprehensive evaluation of the model's capabilities, encompassing conversational performance, instruction-following proficiency, general understanding, coding skills, and mathematical reasoning. Specifically, conversational capabilities were assessed using MT-Bench~\cite{zheng2024judging} and AlignBench (ZH)~\cite{liu2023alignbench}, while instruction-following abilities were evaluated through IFEval~\cite{zhou2023instruction}. General understanding was measured using C-EVAL (ZH)~\cite{huang2024c}, CMMLU (ZH)~\cite{li2023cmmlu}, MMLU~\cite{hendrycks2020measuring}, and MMLU-Pro~\cite{wang2406mmlu}. Coding proficiency was tested using HumanEval~\cite{chen2021codex} and MBPP~\cite{austin2021program}, and mathematical reasoning was examined through GSM8K~\cite{cobbe2021gsm8k} and MATH~\cite{hendrycks2021measuring}.

\input{llm_eval}


The results demonstrate that Megrez-3B-Instruct exhibits superior performance in conversational capabilities, instruction-following abilities, and comprehensive understanding, surpassing other models of similar parameter scales, such as Qwen2.5-3B-Instruct and MiniCPM3-4B. However, its performance in coding and mathematical reasoning remains less competitive.

Additionally, Megrez-3B-Omni shows notable proficiency as a general-purpose large language model (LLM), holding its ground even in comparisons with larger models exceeding 7B parameters, such as Baichuan-Omni and VITA.


\subsection{Vision Performance}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{case4.png}
  \caption{Qualitative results of Megrez-3B-Omni in captioning, reading chart in images and general chat.}
  \label{fig:case4}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{case5.png}
  \caption{Qualitative results of Megrez-3B-Omni in math, OCR, GUI information, conversation and Reasoning.}
  \label{fig:case5}
\end{figure}


  To rigorously evaluate \ouromnis performance on vision tasks, we employ a diverse suite of benchmark datasets. These include established, classic datasets as well as novel benchmarks provided by VLMEvalKit~\citep{duan2024vlmevalkit}. This comprehensive selection encompasses a wide range of categories, ensuring a thorough and balanced assessment of \ouromnis capabilities across various vision-related tasks. A more intuitive understanding of Megrez-3B-Omni capabilities on multiple vision tasks is shown in  \autoref{fig:case4} and \autoref{fig:case5}.

\textbf{Benchmarks.} \quad 
Refer to OpenCompass~\citep{2023opencompass}, We evaluate \ouromnis performance across 12 representative vision-language benchmarks as follows:
\begin{itemize}
    \item \textbf{General Benchmarks.} Our evaluation includes a comprehensive evaluation of visual question answering, multimodal conversation, knowledge, reasoning and spatial understanding capabilities. Specifically, we employ the following benchmarks: MME~\citep{fu2024mme}, MMBench-EN, MMBench-CN~\citep{liu2023mmbench}, MMMU~\citep{yue2023mmmu}, MMVet~\citep{yu2023mm}, MathVista~\citep{lu2023mathvista}, MStar~\citep{chen2024we}, and RealWorldQA~\citep{grok15}.
    \item \textbf{OCR Benchmarks.} To evaluate OCR capabilities, we utilize four established benchmark, including OCRBench~\citep{liu2024ocrbench}, TextVQA~\citep{singh2019textvqa}, DocVQA~\citep{mathew2021docvqa}, and AI2D~\citep{kembhavi2016diagram}.
    \item \textbf{Hallucination Benchmark.} We incorporate HallusionBench~\citep{guan2024hallusionbench} to assess the trustworthiness and factual correctness of the model's responses.
\end{itemize}

\input{evaluate_vision_general}

We compare with strong baselines in different series, including Qwen2-VL-2B-Instruct~\citep{Qwen2VL}, InternVL2.5-2B~\citep{chen2024expanding}, BlueLM-V-3B~\citep{lu2024bluelmv3b}, InternVL2.5-4B~\citep{chen2024expanding}, Baichuan-Omni~\citep{li2024baichuanomni}, MiniCPM-V-2.6~\citep{yao2024minicpmv}, Qwen2-VL-7B-Instruct~\citep{Qwen2VL}, MiniCPM-Llama3-V-2.5~\citep{yao2024minicpmv}, VITA~\citep{fu2024vita}, GLM-4V-9B~\citep{glm2024chatglm}, LLaVA-NeXT-Yi-34B~\citep{ai2024yi} and Qwen2-VL-72B-Instruct~\citep{Qwen2VL}.

\textbf{Results on General vision-language Benchmarks.} \quad 
As shown in Table~\ref{tab:mllm_general_comparison}, our analysis reveals several key observations. Firstly, \ouromni demonstrates superior performance compared to strong open-source models of comparable size. Secondly, it achieves competitive results with significantly larger models (below 10B parameters), such as MiniCPM-V-2.6 and Qwen2-VL-7B-Instruct, even achieving state-of-the-art performance on MMBench-CN, MathVista, and RealWorldQA benchmarks. Finally, compared to prominent open-source model in the industry, such as Qwen2-VL-72B-Instruct, \ouromni achieves over 80\% accuracy with a parameter count that is an order of magnitude smaller. In summary, these findings indicate that \ouromni achieves a favorable balance between performance and efficiency, making it suitable for broader community adoption and diverse applications.

\input{evaluate_vision_other} 

\textbf{Results on OCR and Hallucination Benchmarks.} \quad 
The experimental results presented in Table~\ref{tab:mllm_other_comparison} indicate that \ouromni demonstrates robust OCR and hallucination mitigation capabilities at comparable parameter sizes. This includes understanding of scene text, documents, chart and screenshots.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{assets_opencompass.jpg}
  \caption{The performance of Megrez-3B-Omni on the OpenCompass test set.}
  \label{fig:assets_opencompass}
\end{figure}



\subsection{Audio Performance}

\input{asr_performance}

We evaluated the Chinese ASR performance of Megrez-3B-O on the Fleur-test-zh and WenetSpeech-test-meeting datasets. The comparison of results with other multi-modal large language models (LLMs) is presented in \autoref{tab:asr_perf}. Benefiting from targeted training on Chinese ASR datasets, our model achieves superior performance compared to Whisper-large-v3. However, when compared to Qwen2-audio, our model exhibits slightly limited performance due to the disparity in dataset size.


\subsection{Speed Performance}

We conducted speed tests on various language models using the NVIDIA A100 GPU. To ensure fair comparison, all models were deployed using the vLLM inference framework. Both the input and output token counts were set to 128, and the batch size was set to 8\footnote{For more details please refer to:  https://huggingface.co/Infinigence/Megrez-3B-Instruct/blob/main/README\_SPEED.md}. As shown in \autoref{Megrez-3B-Instruct}, our model achieves the fastest inference speed among models with a parameter size around 3 billion. 
