\section{Multimodal Large Language Model}

\subsection{Multimodal Model Architecture}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{architecture_detail.jpg}
  \caption{Megrez-O architecture. During the training stage, the white module is frozen and the blue module is trained.}
  \label{fig:architecture}
\end{figure}

\input{model_param}
     
 The overall architecture of \ours, as shown in \autoref{fig:architecture}, follows the design philosophy of the LLaVA series model, employing a ``Multimodal Encoder-Connector-LLM'' configuration to enhance unified comprehension of image, audio, and language. Megrez-3B-Omni consists of five key components: a visual encoder, a visual compression layer, an audio encoder, an audio compression layer, and an LLM. The input image is processed by the visual encoder using an adaptive encoding method, and the audio input is similarly encoded by the audio encoder. For visual encoding, we use the SigLIP SoViT-400m model, and the generated visual tokens are compressed by a perceiver resampler structure with a single cross-attention layer. For audio, we use the whisper-large-v3 model, trained for speech recognition and translation, and apply a linear transformation to map audio features into the word embedding space. The compressed visual and audio tokens, along with the textual input, are fed into the LLM for conditional text generation.

 Another important setting is in the vision encoder, to address resolution and computational problem in images, we slice original image to keep detail information in vision. For each image slice, the visual output of each section occupies 64 tokens in the LLM. For a single image, we can divide it into a maximum of 9 slices to get local information. In addition, the original image will be resized to around 448$\times$448 to capture global information. A single image can contain a maximum of 9$\times$64 tokens from the slices and 64 tokens from the global image. 

 Regarding the audio encoder, we adopt the architecture of Whisper's encoder and incorporate an MLP projector to align the output features with the embedding space of the LLM. An input audio clip is truncated to a maximum duration of 30 seconds, after which its mel-spectrogram is obtained via a short-time Fourier transform. The output sequence from the Whisper encoder is then compressed to match the dimension of the LLM's embedding space through the MLP projection, producing 50 continuous tokens per second of audio. For raw inputs exceeding 30 seconds, the audio can be segmented into multiple clips and subsequently concatenated within the embedding space to preserve the complete information.

 
\subsection{Training Data}



\subsubsection{Vision}

The image training data, as outlined in \autoref{tab:vision-train-data}, is composed of diverse categories including caption and question-answering (QA) datasets in both Chinese and English. Throughout various training phases, specific subsets of the dataset are sampled to fulfill distinct objectives. The datasets are organized into the following categories:

\textbf{Image Captioning.} This category includes datasets such as LAION~\cite{schuhmann2022laion}, SA1B~\cite{kirillov2023segany}, ShareGPT4V~\cite{chen2024sharegpt4videoimprovingvideounderstanding}, Objects365-Caption~\cite{shao2019objects365}, etc. These are instrumental in training the model to produce descriptive language for images, enhancing its ability to generate captions that accurately reflect the visual content.

\textbf{Image QA Data.} Comprising datasets like MMInstruct~\cite{liu2024mminstruct}, COCO-QA~\cite{ren2015cocoqa}, VQA~\cite{antol2015vqa}, and ScienceQA~\cite{lu2022scienceqa}, this category focuses on training the model to answer questions based on images and perform visual reasoning tasks. These datasets include both general image QA and science reasoning tasks, which are crucial for developing a model capable of understanding and interpreting visual information in the context of questions.

\textbf{OCR.} This category is designed to support the model's comprehension of Optical Character Recognition (OCR). It utilizes datasets such as SynthText~\cite{gupta2016synthtext}, SynthDoG~\cite{kim2022ocr}, DocVQA~\cite{mathew2021docvqa}, TextVQA~\cite{singh2019textvqa}, and OCR-VQA~\cite{mishra2019ocrvqa}. These datasets are essential for training the model to interpret and understand text within images and diagrams, which is a critical skill for applications involving document analysis and data extraction from visual formats.

By strategically sampling from these datasets during different training stages, we aim to develop a model that is not only capable of generating descriptive language for images but also proficient in answering image-based questions and interpreting visual content. This approach ensures that the model is well-rounded and can handle a variety of tasks related to image understanding and processing.

\input{vision_train_data}

\subsubsection{Audio}

The audio training data are divided into two categories: pretraining ASR data and audio instruction data, corresponding to the two phases of audio module training. 

Regarding the audio instruction data, we adopted a synthetic approach to ensure efficiency and cost-effectiveness. First, we selected realistic dialogue data from our text SFT data sources to guarantee high-quality conversational content. Text-to-speech (TTS) generation was then applied to all instructions. The TTS model plays a critical role in audio data synthesis, ensuring diversity and naturalness in the generated voices, thus simulating realistic conversational scenarios involving different users. We choose the ChatTTS model due to its fast processing speed, diverse voice styles, and natural tone. In addition, we incorporated human speech recordings to better align with the distribution of realistic applications. To achieve this, we select instruction-like audio clips from the ASR datasets using a combination of rule-based filtering and LLM-based judgement. For these selected audio clips, GPT-4o was employed to generate responses based on their text transcripts. 

\subsection{Training Strategies}
    

\subsubsection{Vision Alignment}

The alignment process for the vision modality is divided into two distinct stages. Initially, we train the visual connector to establish an initial alignment between image representations and text using an image captioning dataset. During this phase, the visual encoder and the LLM are frozen, while the visual connector is trained with a learning rate of \(3e{-4}\). In the subsequent stage, we freeze the LLM and train both the visual encoder and the connector with a reduced learning rate of \(1e{-4}\). In addition to general VQA tasks, we have specifically synthesized high-quality QA data points for OCR and charts to enhance the model's abstract visual understanding. 



\subsubsection{Audio Alignment}
Similar to the vision branch, the alignment of the audio modality also involves two stages. We start to finetune the MLP connector using our ASR dataset while keeping the audio encoder and LLM frozen with a learning rate of \(3e{-4}\). This step aims to align the semantic representations of the audio and text modalities within the embedding space. To maintain consistency in task type and improve generalization in subsequent stages, we reformulate the ASR task into an instruction-following format. Specifically, given an audio clip and a transcription instruction, the model generates the corresponding transcript. Then, we freeze the LLM and train both the visual encoder and the connector with a reduced learning rate of \(3e{-5}\) using audio SFT data. 

\subsubsection{Omni Instruction Tuning}

Since the audio encoder has been pre-trained on extensive audio datasets encompassing ASR, S2TT, AAC, and other audio-related tasks, our model has already achieved satisfactory performance on many speech tasks after the alignment of the audio modality. Nonetheless, a significant semantic gap persists between the visual and textual modalities. Even after the alignment of the visual modality, the model is only capable of performing simple image caption task, and continues to struggle with complex vision tasks, such as OCR, VQA and multi turn conversations.

During the multimodal instruction tuning stage, both the visual encoder and connector, the audio encoder and connector, as well as the LLM are trainable, aiming to improve the visual understanding capabilities. To preserve the capabilities of the text and audio modalities, we employ a smaller learning rate of \(1e{-5}\) to train all parameters with a diverse set of open-source, synthetic, and internally annotated data, across text, audio, image-text, and image-audio modalities.
To ensure the model's seamless transition between various modalities, we randomly shuffle and concatenate data from different modalities to a length of 4K tokens. The data concatenation strategy significantly reduces the training time.
 
