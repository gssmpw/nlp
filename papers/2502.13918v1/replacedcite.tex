\section{Related Work}
%\subsection{Background}

The solution we developed has strong foundations on previous research. The system is mostly based on the work done by \textit{Deepmind} with AlphaZero ____, and the research provided by \textit{Deep Thinking systems} with the papers in ____ and ____. In this section, we provide a more in-depth look at those systems and finish with a brief overview of previous work done in similar games.

\subsection{AlphaZero}

Deepmind developed a series of algorithms to play complex board games. Each of these algorithms iterated upon the research obtained from the previous solutions. AlphaZero is one of those algorithms and mostly bases itself on the previous systems: AlphaGo ____ and AlphaGo Zero ____. These systems demonstrated great success, as they achieved super-human play in their respective domain. However, while the two first systems were designed solely for the game of Go, AlphaZero is a general reinforcement learning algorithm that can be applied to a large set of different games.

AlphaZero's algorithm makes use of a neural network with two heads: a policy head and a value head. Given a neural network policy prediction $\pi$ and value prediction $v$, the algorithm utilizes a modified Monte Carlo Tree Search (MCTS) to obtain both an improved policy and new value estimates. These two new quantities are then used to update the neural network, which can, once again, be used to provide new value and policy predictions ($v$ and $\pi$), repeating the cycle, to obtain an increasingly better network.

To achieve this AlphaZero utilizes two independent phases: network training and self-play data generation. These two phases can happen asynchronously, and they only exchange information by: sending the most recent version of the network from training to self-play through a shared network storage, and by sending the completed games from self-play to training via a replay buffer.

\subsection{Deep Thinking}

A group of researchers from the University of Maryland developed \textit{Deep Thinking systems} to study methods of achieving logic extrapolation using Recurrent Neural Networks. The team published two papers ____, that study three distinct problems: prefix sum computation, mazes, and chess.

For each of these problems, a fully convolutional neural network was trained employing a recurrent module that can be repeatedly used during inference for arbitrary numbers of iterations. The training is conducted on small/simple problems, utilizing a relatively low number of iterations and, afterwards, the network is tested on larger or more complex problems using larger numbers of recurrent module iterations in inference.

This research demonstrated the capacity to obtain models that, not only extrapolate to larger problems but also increase their performance with higher numbers of recurrent iterations.

In the second paper ____, the researchers addressed some of the limitations introduced in the first publication ____ and looked to achieve even better performance by creating an improved network architecture and a new training methodology, both of which we now describe.

\textit{Recall} was the name given to the new architecture (Figure \ref{input_and_recurrent_modules}), and it is characterized by a new connection that concatenates the input to the beginning of the recurrent module, followed by a convolutional layer to compress the representation back to the original number of channels.


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.49\textwidth]{Images/input_and_recurrent_module_compressed.png}
\caption{Input and recurrent modules of the \textit{Recall} architecture.}\label{input_and_recurrent_modules}
\end{center}
\end{figure}

The new training methodology modified loss calculation by introducing ``progressive loss``. To calculate the progressive loss we choose two numbers at random $n$ and $k$, such that $ (n + k) < T$, where $T$ is the maximum number of iterations we wish to perform. We start by passing the input through the input module and through the recurrent module for $n$ iterations, discarding all the gradients. Then we pass the outcome of this operation through the recurrent module for $k$ more iterations and finally through the output module. In the end, we obtain the progressive loss by calculating the loss function between this last output and the target. This progressive loss is then combined with the regular loss obtained from the output after the maximum number of iterations, using a parameter $\alpha$:

\begin{equation}\label{eq:progressive_loss}
L = (1 - \alpha) \times L_{maxIters} + \alpha \times L_{progressive}
\end{equation}



\subsection{Previous Work}

The Wargame landscape remains very diversified, despite attempts at unification and formalization ____, with environments ranging from realistic air combat simulation for pilot training ____ to simple family board games like Risk ____.

%Throughout the years many different algorithms and techniques have been used to study and analyse the large variety of Wargames. Due to their complexity, it is hard to develop rule-based agents capable of performing at a high level. On top of it, this kind of approach cannot usually adapt to different environments. The usage of machine learning solutions seems quite promising for Wargames as they can potentially solve both problems.


Several properties of these games make them particularly challenging for AI approaches ____, leading a lot of research to focus on human-crafted knowledge as a way to address this obstacles, either by directly enforcing optimal actions in certain board states ____, or by simplifying in-game features using hand-crafted functions ____. More recently, however, ____ has demonstrated promising results, utilizing new methods and relying on human knowledge just to accelerate early training using Supervised Learning.

Other research have focused on the usage of Reinforcement Learning, such as the paper ____, which provided important contributions to the study of unit combat in wargames. This research was conducted on a 10x10 featureless map and demonstrated the impact of different combat models and policy optimization algorithms on the training of Reinforcement Learning agents.

%As a means to address these challenges, approaches that rely on human-crafted knowledge have been used in the past, either by directly enforcing optimal actions in certain board states ____, or by simplifying in-game features using hand-crafted functions ____. More recently, however, ____ has demonstrated promising results, utilizing new methods that only rely on human knowledge as a means to accelerate early training through Supervised Learning.



On the other hand, in 2019, Glennn Moy and Slava Shekh published  research____ that proposed the use of AlphaZero in Wargaming. This paper showed that AlphaZero was capable of reaching optimal play in a simple 10x10 environment with two units and a single terrain type, but demonstrated that human-crafted heuristics were still needed in order to accelerate training.

There have also been several master thesis released by students of the Naval Post Graduate School ____, which address Wargames of a very similar nature with techniques such as Feudal Reinforcement Learning, Deep Q-Networks and AlphaZero.

Beyond Wargames, there have been other games where similar Reinforcement Learning techniques have been utilized, such as \textit{FreeCiv} ____ and \textit{Terra Mistica}. The research in ____ applied AlphaZero to the game of \textit{Terra Mistica} and gives important insights into possible action and state space representations, since the game shares multiple characteristics with Wargames such as the adversarial nature, hexagon-based boards and multi-phased turns.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%