\section{Naive Shuffle-DP binary Search for the Median}
\label{sec:naive-shuffle}
This section is dedicated to proving~\cref{thm:main-shuffle}.


The naive binary search with errors algorithm tests each coin up to $\acc$-accuracy and a $\failp/\log\ab$ failure probability, such that a simple union bound over all $\log\ab$ steps of binary searching will yield an $(\acc,\failp)$-accurate estimate. This algorithm is suboptimal up to logarithmic factors, although there are indications that its strong constant factors can make up the difference in some parameter regimes~\cite{karp2007noisy,gretta2023sharp}. The simple fact that this algorithm runs in deterministic number of rounds, with a deterministic number of samples per round, allows for a straightforward application of amplification by shuffling~\cite{feldman21shuffle}, something we could not achieve with the fully adaptive Bayesian updates algorithm. 

We consider both statistical error, where samples are assumed to be drawn from some unknown distribution with mean $p$, and we are interested in an estimate $\hat{p}$ which is close to that true mean, and the empirical setting where we make no assumption on the distribution of the samples, and are interested in how close our estimate $\hat{p}$ is to the ``best-case'' sample mean $\frac{1}{n}\sum_{i=1}^nx_i$.


\begin{lemma}[Sample complexity of learning one coin to its statistical mean.] 
\label{lemma: one-coin-statistical-mean}
    Given samples $\{x_i\}_{i=1}^n$ from a Bernoulli random variable $X$ with mean $p$ received through a binary randomized response channel $\pmech$ such that $y_i\sim \pmech(x_i)$, we can estimate $\hat{p}=\frac{1}{n}\frac{e^\priv + 1}{e^\priv - 1}\sum_{i=1}^n y_i - \frac{1}{e^\priv - 1}$. In order to learn an $(\acc,\failp)$-estimate of $p$, $\Pr[|\hat{p}-p|>\acc]<\failp$ it suffices to use $n$ samples where,
    $$
    n\leq\left(\frac{2p(1-p)}{\acc^2} + \frac{e^\priv}{\acc^2(e^\priv - 1)^2} + \frac{2(e^\priv + 1)}{4\acc(e^\priv - 1)}\right)\log(1/\failp).
    $$
    In other words, the sample complexity of learning one coin to its statistical mean with constant failure probability is $O\left(\frac{1}{\acc^2\priv^2} +\frac{p(1-p)}{\acc^2}\right)$, when $\priv<1$, or $O\left(\frac{1}{\acc^2e^\priv} +\frac{p(1-p)}{\acc^2}\right)$, when $\priv\geq 1$.
\end{lemma}
\begin{proof}
    Given a Bernoulli random variable $x$ with mean $p$, and a binary randomized response channel $\pmech$ (see~\autoref{def: binary rr}) the distribution induced by applying $\pmech$ to $x$ is:
    \begin{equation*}
    \label{eq:rr-bern-induced}
    y=\pmech(x)\sim\operatorname{Bern}\left(\frac{e^\priv}{e^\priv + 1}p + (1-p)\frac{1}{e^\priv + 1}\right)=\operatorname{Bern}\left(\frac{e^\priv-1}{e^\priv + 1}p + \frac{1}{e^\priv + 1}\right).
    \end{equation*}
    The variance of this distribution is 
    \begin{align*}
\sigma^2=\operatorname{Var}(y)&=\left(\frac{1}{e^\priv + 1}+\frac{e^\priv-1}{e^\priv + 1}p \right)\left( \frac{e^\priv}{e^\priv + 1}-\frac{e^\priv-1}{e^\priv + 1}p\right)\notag\\
&=\left(\frac{e^\priv - 1}{e^\priv + 1}\right)^2p(1-p) + \frac{e^\priv}{(e^\priv + 1)^2}.  \label{eq:rr-bern-var}
    \end{align*}
    We then proceed by simple rearranging, substitution, and application of Bernstein's inequality~\cref{fact: bernstein}.
    \begin{align*}
        \Pr\left[|\hat{p}-p| >\acc\right]&=\Pr\left[\bigg|\frac{1}{n}\frac{e^\priv + 1}{e^\priv - 1}\sum\limits_{i=1}^n y_j - \frac{1}{e^\priv - 1} - \left(\frac{e^\priv + 1}{e^\priv - 1}\bEE{y} - \frac{1}{e^\priv - 1}\right)\bigg| >\acc\right]\\
&=\Pr\left[\bigg|\frac{e^\priv + 1}{e^\priv -1}\left(\frac{1}{n}\sum\limits_{i=1}^n y_i -\bEE{y}\right)\bigg|>\acc\right]\\
&=\Pr\left[\bigg|\frac{1}{n}\sum\limits_{j=1}^ny -\bEE{y}\bigg|>t\right]\tag*{$\left(t=\acc\frac{e^\priv -1}{e^\priv +1}\right)$}\\
\failp&\leq\exp\left(\frac{-nt^2}{2\sigma^2 + \frac{2t}{3}}\right)\tag{Bernstein's Inequality}\\
n&\leq\left(\frac{2\sigma^2}{t^2} +\frac{2}{3t} \right)\log(1/\failp)\\
    &=\left( \frac{2p(1-p)}{\acc^2} +\frac{2e^\priv}{\acc^2(e^\priv - 1)^2}+\frac{2(e^\priv + 1)}{3\acc(e^\priv - 1)}\right)\log(1/\failp).\tag{Substituting $t$ and $\sigma^2$}
    \end{align*}
\end{proof}

\begin{lemma}[Sample complexity of learning one coin to its sample mean.]
\label{lem:empirical-coin-learn-rr}
    Given samples $\{x_i\}_{i=1}^n$ where each $x_i\in\{0,1\}$, and private outputs $y_i\sim \pmech(x_i)$, the true sample mean is $P=\frac{1}{n}\sum_{i=1}^n x_i$. Denote the sample mean of the collected private outputs $Y=\frac{1}{n}\sum_{i=1}^n y_i$. Our estimator of the sample mean will be similar to the statistical case, where $\widehat{P}=\frac{e^\priv + 1}{e^\priv - 1}Y - \frac{1}{e^\priv - 1}$. In order to learn an $(\acc,\failp)$-estimate of $P$ it is sufficient to use $n$ samples such that 
    \begin{equation*}
        n\leq\left( \frac{2e^\priv}{\acc^2 (e^\priv - 1)^2} + \frac{2(e^\priv + 1)}{3\acc (e^\priv - 1)} \right)\log(1/\failp).
    \end{equation*}
    Therefore, the sample complexity of learning the sample mean with constant failure probability is $O\left(\frac{1}{\acc^2\priv^2}\right)$, when $\priv<1$, or $O\left(\frac{1}{\acc^2e^\priv} \right)$, when $\priv\geq 1$. It is pleasing to note that this recovers the sample complexity of learning in the statistical case, up to the additive sampling error.
\end{lemma}

\begin{proof}
    The proof will proceed similarly to the statistical case. The key difference will be the variance of $y$ in this case which is
    \[
    \sigma^2=\sigma^2(\pmech(0))=\sigma^2(\pmech(1))=\frac{e^\priv}{(e^\priv + 1)^2}.
    \]
    The derivation then proceeds as in the statistical case.
    \begin{align*}
        \Pr[|\widehat{P}-P|>\acc] &= \Pr\left[\bigg| \frac{e^\priv + 1}{e^\priv - 1}Y - \frac{1}{e^\priv - 1} - P\bigg| >\acc\right]\\
            &=\Pr\left[\bigg| \frac{e^\priv + 1}{e^\priv - 1}Y - \frac{1}{e^\priv - 1} - \left( \frac{e^\priv + 1}{e^\priv - 1}\bEE{Y} - \frac{1}{e^\priv - 1} \right)\bigg|>\acc \right]\\
            &= \Pr\left[\frac{e^\priv + 1}{e^\priv - 1}\bigg| \left(Y-\bEE{Y} \right)\bigg|>\acc \right]\\
            &=\Pr\left[\bigg| Y-\bEE{Y}\bigg|> t \right]\tag*{$\left(t=\acc\frac{e^\priv - 1}{e^\priv + 1}\right)$}\\
            &\leq \exp\left(\frac{-nt^2}{2\sigma^2 + \frac{2t}{3}}\right)\tag{Bernstein's Inequality}\\
        n   &\leq \left( \frac{2\sigma^2}{t^2} + \frac{2}{3t} \right)\log(1/\failp)\\
            &=\left(\frac{2e^\priv}{\acc^2 (e^\priv - 1)^2} + \frac{2(e^\priv + 1)}{3\acc(e^\priv - 1)} \right)\log(1/\failp).\tag{Substituting $t$ and $\sigma^2$}
    \end{align*}
\end{proof}

With this we can now formally state the sample complexity of a naive binary search for the median under local differential privacy. We will focus on the empirical case for this result. 
\begin{theorem}[Naive Binary Search for the Median under Local Differential Privacy]
\label{thm:ldp-nbs-naive}
    The naive algorithm as described by~\citet{karp2007noisy}, under the constraints of $\priv$-local differential privacy, has sample complexity
    \[
    n\leq \left( \frac{2e^\priv}{\acc^2(e^\priv - 1)^2} + \frac{2(e^\priv + 1)}{3\alpha(e^\priv - 1)} \right)\log(\ab)\log\left(\frac{\log\ab}{\failp}\right).
    \]
    We can therefore say that for $\priv<1$, the naive approach has sample complexity $O\left(\frac{\log\ab}{\acc^2\priv^2}\log\left(\frac{\log\ab}{\failp}\right)\right)$, and for $\priv\geq 1$ it has sample complexity $O\left(\frac{\log\ab}{\acc^2e^\priv}\log\left(\frac{\log\ab}{\failp}\right)\right)$.
\end{theorem}
\begin{proof}
    Given $n$ total users, let $n'=n/\log(\ab)$ and let $\failp'=\failp/\log(\ab)$, apply~\autoref{lem:empirical-coin-learn-rr} with $n',\failp'$ to get sample complexity.
    \[
    n\leq \left( \frac{2e^\priv}{\acc^2(e^\priv - 1)^2} + \frac{2(e^\priv + 1)}{3\alpha(e^\priv - 1)} \right)\log(\ab)\log\left(\frac{\log\ab}{\failp}\right).
    \]
    By a union bound over all $\log\ab$ rounds of the binary search, the final estimate will be an $(\acc,\failp)$-approximate median.
\end{proof}

As stated in the introduction, the primary motivation for this approach is that by dividing the algorithm into a few deterministic stages, with many samples tested at each stage, we can hope to apply amplification by shuffling~\cite{feldman21shuffle}. We state the amplification by shuffling result here, and a subsequent lemma that will be useful to our analysis.
\begin{theorem}[{\citet*[Theorem 3.1]{feldman21shuffle}}]
    \label{theorem: amplification by shuffling}
    For any domain $\mathcal{X}$, let $\pmech_t:\pmech_1\times\ldots\times\pmech_{t-1}\times\mathcal{X}\rightarrow\mathcal{Y}$  for $t\in [n]$ be a sequence of randomizers such that $\pmech_t(y_{1:t-1},\cdot)$ is $\priv_L$-local DP; and let $S$ be the algorithm that given a tuple of $n$ messages, outputs a uniformly random permutation of said messages. Then for any $\privdelta\in(0,1]$ such that $\priv_L\leq\log\frac{n}{16\log(2/\privdelta)}$, $S\circ \mathcal{Y}^n$ is is $(\priv,\privdelta)$-DP, where
    \[
    \priv\leq\log\left(1 + 8\frac{e^{\priv_L}-1}{e^{\priv_L}+1}\left(\sqrt{\frac{e^{\varepsilon_L}\log(4/\delta)}{n}}+
\frac{e^{\varepsilon_L}}{n}\right)\right)
    \]
\end{theorem}
This implies the following useful lemma,
\begin{lemma}[Amplification by shuffling]
\label{lemma: amplification by shuffling}
    Fix any $\privdelta\in(0,1]$, $\priv\in(0,1]$, and $n$ such that $\priv>16\sqrt{\log(4/\privdelta)/n}$. Then, for
    \[
    \priv_L\coloneqq\log\frac{\priv^2 n}{80\log(4/\privdelta)}
    \]
    Shuffling the messages of $n$ users using the same $\priv_L$-LDP randomizer satisfies $(\priv,\privdelta)$-shuffle differential privacy.
\end{lemma}
\begin{proof}
    For $\priv,\privdelta$ and $\priv_L$ as above we have $0<\priv_L\leq\log\frac{n}{16\log(2/\privdelta)}$. Applying~\autoref{theorem: amplification by shuffling}, we get $(\priv',\privdelta)$-differential privacy for
    \[
    \priv' \leq \log\left(1 + 8\underbrace{\frac{e^{\priv_L}-1}{e^{\priv_L} + 1}}_{<1}\underbrace{\left(\frac{\priv}{\sqrt{80}}+\frac{\priv^2}{80\log(4/\delta)}\right)}_{< \priv/8}\right)\leq \priv
    \]
    Proving the lemma.
\end{proof}

%
We can now prove~\cref{thm:main-shuffle}.
\begin{theorem}[Restatement of~\Cref{thm:main-shuffle}]\label{thm:restated-shuffle}
    Let $r=\log_2 B$. There exists a protocol for \texttt{shuffle\--emp\--median}$(\{x_i\}_{i=1}^n,\alpha,\eps,\delta,r)$ with success probability $1-\failp$ provided that
    \[
    n=O\left( \left(\frac{1}{\acc^2} +\frac{1}{\priv^2}\right)\log\ab\sqrt{\log(1/\privdelta)\log\frac{\log\ab}{\failp}} \right).
    \]
    The protocol has $r=\log_2\ab$ rounds of adaptivity and queries shuffled batches of $n/\log_2(\ab)$ users. 
\end{theorem}
%
%
%
%
%
%
%
%
\begin{proof}[Proof of~\Cref{thm:main-shuffle}]    
    Take the sample complexity achieved in~\autoref{thm:ldp-nbs-naive}, and note that we are in the $\priv\gg 1$ regime as we will be applying taking $\priv_L\in O(\log n)$. We therefore have
    \[
    n= O\left( \frac{\log\ab}{\acc^2 e^{\priv_L}}\log\frac{\log\ab}{\failp} \right)
    \]
    We apply~\autoref{lemma: amplification by shuffling} while noting that at each stage we shuffle $n'=n/\log(\ab)$ users. Setting $\priv_L=\log\frac{\priv^2 n}{80\log(\ab)\log(4/\privdelta)}$ and rearranging gives that for each step of the binary search we have enough users to accurately learn the CDF of the remaining suffix of users within error $\alpha/2$ with probability $\beta/\log B$. Union bounding over all $\log B$ steps of the binary search, we conclude that with probability $1-\beta$, every step succeeds. This gives sample complexity,
    \[
    n= O\left(\frac{\log\ab}{\acc\priv} \sqrt{\log(1/\privdelta)\log\frac{\log\ab}{\failp}} \right),
    \]
    but we are not finished. We have to handle the multiple restrictions on parameter regimes 
    \[
    O\left(\frac{\log\ab}{\acc\priv} \sqrt{\log(1/\privdelta)\log\frac{\log\ab}{\failp}} \right)\geq n >\max\left\{\frac{\log\ab}{\acc^2},\frac{{\log\ab\log(1/\privdelta)}}{\priv^2}\right\}.
    \]
    The right hand side of this inequality comes from restrictions present in~\cref{lemma:CDF-bound,lemma: amplification by shuffling} on $n$ and $\priv$ respectively, the latter comes from using $n'=n/\log(\ab)$ in the restriction on $\priv$. 
    A trivial solution is be to take $1/(\acc\priv)$ and replace it with $1/\min\{\acc^2,\priv^2\}$, which gives
    \[
    n=O\left( \left(\frac{1}{\acc^2} +\frac{1}{\priv^2}\right)\log\ab\sqrt{\log(1/\privdelta)\log\frac{\log\ab}{\failp}} \right).
    \]
    %

   
\end{proof}

This result has an improved dependence in $\priv$ and $\acc$, and could be preferable from a communication perspective. Rounds of adaptivity are a restricting factor in distributed learning, and our goal was to understand the trade offs possible under privacy constraints. It is of practical interest to know whether the constraint on $n$ in~\cref{lemma: amplification by shuffling} can be improved from $n=\Omega\left(\log(1/\privdelta)/\priv^2\right)$ to $\Omega\left(\log(1/\privdelta)/\priv\right)$. This, in combination with a strengthening of~\cref{lemma:CDF-bound} to have a linear dependence on $\acc$, would allow the analysis to go through with only a $1/(\acc\priv)$ dependence.