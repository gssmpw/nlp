\section{A Note on the Continuous Case}\label{sec:continuous} If we replace the discrete domain $[B]$ with a continuous one, say $[0,1]$, it is generally impossible to obtain quantile error $o(1)$ using a finite number of samples under LDP. This follows from our lower bounds by discretizing $[0,1]$ into $[B]$ buckets and letting $B\to \infty$. In fact, this is a general issue for quantile or range estimation problems in DP (even beyond the local model), which is why related work studies the discrete setting~\cite{BeimelNS16twotologstar,Bun2015logstar,Kaplan2020closinggap,kulkarni2019answering}. On a more positive note, if we impose mild guarantees on the family of possible distributions the samples can come from, our result has implications in the continuous setting as well. For instance, if we assume that there are (known) numbers $-\infty=y_0<y_1<\cdots <y_B=\infty$ such that in any interval $[y_i,y_{i+1}]$, the emperical CDF increases by at most $\alpha/2$, then we can again obtain quantile error $\alpha$ with $O(\frac{\log B}{\eps^2\alpha^2})$ users using our algorithm and bucketing users in the same interval $[y_i,y_{i+1})$. As the dependency on $B$
 in the number of samples is logarithmic, this might allow 
 $B$ to be quite large, with a correspondingly small quantile error $\alpha$. We note that if the assumption on the CDF is incorrect, only the accuracy is affected while the algorithm remains private.