\section{Technical Contribution}\label{sec:tech-contributions}
In this section we give a high-level discussion our technical contribution for designing algorithms and proving lower bounds. For simplicity, we focus on the high privacy setting $\eps\leq 1$.
\subsection{Adaptive LDP Median Estimation via Noisy Binary Search (\cref{thm:main-emp})}\label{sec:tech-contributions-1}
At the heart of our LDP median protocol of~\cref{thm:main-emp} is an algorithm for the noisy binary search problem from~\cite{karp2007noisy}: Given an ordered set of $\ab$ coins with unknown head probabilities $\{p_i\}_{i=1}^\ab$ such that $p_1\leq \cdots \leq p_\ab$, a target $\tau \in (0,1)$, and an error $\alpha>0$, our goal is to find any coin $i$ such that 
\begin{equation}\label{eq:good-coin}
[p_i, p_{i+1}]\cap (\tau-\alpha, \tau+\alpha)\neq \emptyset,
\end{equation}
which intuitively means that the desired probability $\tau$ lies between coin $i$ and $i+1$ (up to error $\alpha$).  
We refer to a coin satisfying the above property as \emph{$(\tau, \alpha)$-good}. At each round, we may query a coin with index $j$, and we receive the result of the flipped coin.
This problem generalizes classic binary search, where for the query $t$, one would have $p_i = 0$ for all $i \leq t$ and $p_i = 1$ for all $i > t$. We will denote the general problem as \texttt{MonotonicNBS}$(\{p_i\}_{i=1}^n, \tau, \alpha)$ (omitting $\{p_i\}_{i=1}^n$ when they are clear from context).
%
The state-of-the-art algorithm for \texttt{MonotonicNBS} is the \emph{Bayesian Screening Search} (\texttt{BayeSS}) due to \cite{gretta2023sharp}. Their algorithm finds a $(\tau,\alpha)$-good coin using $O(\frac{\log \ab}{\alpha^2})$ samples with high probability in $\ab$ \footnote{In fact, they obtain stronger guarantees. For any $\alpha,\tau$, their algorithm uses $\frac{1}{C_{\tau,\alpha}}\left(\log \ab+O(\log^{2/3} \ab \log^{1/3}(\frac{1}{\delta})+\log(\frac{1}{\delta}))\right)$ where $C_{\tau,\alpha} = \Theta\left(\frac{\alpha^2}{\tau(1-\tau)}\right)$ for sufficiently small $\alpha$.  Moreover, by information theoretic lower bounds, any algorithm must use $\frac{1}{C_{\tau,\alpha}}\log \ab$ coin flips.}. %

To see how noisy binary search algorithms relate to median estimation under LDP, it is instructive to consider \texttt{LDPstat-median}$(\mathcal{D}, n, \alpha, \varepsilon)$.
%
%
Concretely, any sample $x$ from $\mathcal{D}$ gives a coin flip with head probability $p_i = \Pr_{x\sim \mathcal{D}}[x\leq i]$ for any $i\in [B]$.
%
%
It is a useful warmup problem, to show that one can solve \texttt{LDPstat-median} using an algorithm for \texttt{MonotonicNBS}. Plugging in the algorithm of Gretta and Price gives an algorithm for \texttt{LDPstat-median}$(\mathcal{D},n,\alpha,\eps)$ if $n \geq C \frac{\log \ab}{\eps^2\alpha^2}$ for a constant $C$. We show the precise details in Appendix~\ref{sec:statistical-median}.




For \texttt{LDPemp-median}, the situation is more complicated.  
A first idea is to reduce to the statistical setting by sampling users with replacement, thus sampling i.i.d from the empirical distribution. However, in sequentially adaptive protocols, users may only be queried once but sampling with replacement may sample a single user many times\footnote{If we allow for multiple queries to the same user, we can indeed reduce to the statistical setting by sampling users with replacement. However, some users would then be sampled up to $O(\log n/\log\log n)$ times and to maintain $\eps$-LDP, their reports would have to be made more noisy, thereby increasing the number of users needed to get an $\alpha$-approximate median. Thus, even allowing for users to be queried multiple times, it is unclear how to get optimal bounds via algorithms for \texttt{MonotonicNBS}.}. 
To resolve this issue, our main idea is to go through the users in a \emph{random order} or equivalently sample users \emph{without} replacement. Ideally, we would like to maintain the guarantees of algorithms for \texttt{MonotonicNBS}, but this problem assumes that the coin probabilities are unchanging over time. However, when sampling without replacement, the empirical CDF of the remaining users, and thus the coin probabilities, change over time. Our main technical contribution is two-fold. We first show that throughout the process, no coin probability is altered too much.
\begin{lemma}\label{lemma:CDF-bound}
Let $x_1,\dots,x_n\in[\ab]$ and let $y_i=x_{\pi(i)}$ where $\pi:[n]\to [n]$ is a random permutation. For $0\leq t< n$ and $j\in [\ab]$, we define $p_j^t=\frac{|\{t< i\leq n\mid y_i\leq j\}|}{n-t}$. Suppose that $n\geq C\frac{\log \ab}{\alpha^2}$ for a sufficiently large constant $C$. Then with high probability in $\ab$, we have for all $0\leq t\leq n/2$ and all $j\in[\ab]$ that $|p^{t}_j-p^{0}_j|\leq \alpha$.
\end{lemma}
Then, we show that %
the algorithm by Gretta and Price in fact also solves an \emph{adversarial} version of \texttt{MonotonicNBS} which we denote \texttt{AdvMonotonicNBS}. Here, in each round, if coin $j$ is selected to be flipped, an adversary may instead flip a coin with a bias $p$ such that $|p_j - p| \leq c\alpha$ for some $c$. The goal is to return a $(\tau,\alpha (1+c))$-good coin. A formal definition can be found in \autoref{def:adversarial}. Our result, which may be of independent interest is as follows.

%

\begin{theorem}\label{thm:NBS-changing-probabilities}
Let $0<\alpha\leq \frac{1}{4}$ and suppose $c\leq 1$. 
There exists an algorithm for \texttt{AdvMonotonicNBS}$(1/2, \alpha, c)$ which uses $O(\frac{\log \ab}{\alpha^2})$ coin flips and returns an $(1/2,\alpha(1+c))$-good with high probability in $\ab$.
\end{theorem}

Now by~\cref{lemma:CDF-bound}, as we sample users without replacement, the CDF of the remaining users never changes by more than $\alpha$ at any point. In particular, for the data $x_j$ of a newly sampled user and a threshold $t\in [B]$, the probability of observing a one when applying randomized response to $[x_j\leq i]$ never varies by more than $\alpha\eps$. Denoting this probability $p_i$, we are exactly in a position to apply~\cref{thm:NBS-changing-probabilities} to conclude that $O(\frac{\log B}{\alpha^2\eps^2})$ users suffices to find a $(1/2,2\alpha\eps)$ good coin. But this translates exactly to $i$ being an $O(\alpha)$-approximate median.

The proof of~\cref{lemma:CDF-bound} and~\cref{thm:NBS-changing-probabilities} can be found in~\cref{sec:proof-of-main-adaptive-up} and~\cref{app: proof theorem 3.1}.


%
%
%
%



%


\subsection{Lower bounds for Adaptive and Non-Adaptive Median Estimation (\cref{thm:main-lower,thm:intro-lower-non-interactive})}
We next describe the main ideas for the lower bounds of~\cref{thm:main-lower,thm:intro-lower-non-interactive}, the full proofs are available in~\cref{sec:lower-bound}.

\paragraph{Lower Bound for Adaptive Protocols (\cref{thm:main-lower})} 
In fact, we provide a lower bound for the general quantile estimation problem, demonstrating that all quantiles (not too close to the $0$ or $1$) are as hard as the median. %
To prove this lower bound, we first prove a lower bound in the statistical setting of~\cref{def:med-stat} and then reduce to the empirical setting of~\cref{def:med-emp}.
Our building block for the statistical lower bound is the framework in~\cite{duchi2013local}, which uses the fact that a protocol attaining low error on the quantile problem, can distinguish distributions with different $q$th quantiles from each other, even from a ``hard'' family of distributions. Our hard family of distributions will be close in statistical distance, but still has different $q$th quantiles:
    \[
        P_\beta(i) = \begin{cases}
        q - 2\alpha & i = 1 \\
        4\alpha & i = \beta \\
        1-q-2\alpha & i = \ab,
    \end{cases}
    \]
for $\beta \in \{2, \ldots, \ab-2\}$. If $\beta$ is chosen uniformly at random, then our LDP distinguishing mechanism will be able to deliver $\log(\ab)$ bits of information (measured with the mutual information), by Fano's inequality. However, there is an upper bound on the amount of mutual information possible with an LDP protocol, as first established in~\cite{duchi2013local} and this leads to our desired result. %

%
   %
%

To get a lower bound in the empirical setting, we observe that a low-error algorithm for empirical quantile estimation can be applied to also get low-error in the statistical setting by just applying it on the data sampled from $\mathcal{D}$. The approximation guarantee follows from the fact that we have enough users that the empirical $q$-quantile of the samples is an $\alpha/2$ approximation to the true $q$-quantile of the distribution $\mathcal{D}$.


\paragraph{Lower Bound for Non-Adaptive Protocols (\cref{thm:intro-lower-non-interactive})}
It turns out more challenging to obtain a lower bound for non-interactive protocols. Our proof is via a reduction to the problem of privately learning a CDF under non-interactive LDP with $\ell_\infty$-error below $\alpha$. For small $\eps$ and $\alpha$, it is known~\cite{edmondsNU20} that any such algorithm requires $\Omega(\frac{\log^2 B}{\eps^2\alpha^2\log^2 (1/\alpha)})$ users\footnote{In fact, their bound is $\Omega(\log^2 B)$, but it is relatively simple to check that their proof extends to general $\eps,\alpha\leq 1$ with mild assumptions on these parameters.}. 

Our reduction works as follows: Given a non-interactive $\eps$-LDP algorithm for median estimation which succeeds with probability $2/3$, we first boost this success probability to $1-\alpha^2$ with $O(\log 1/\alpha)$ independent repetitions and the median trick. The privacy of this protocol is thus $\eps_1=O(\eps\log(1/\alpha))$. Second, assuming access to such an algorithm succeeding with high probability, we design a non-interactive CDF approximation algorithm as follows. First, we add $2n$ dummy users $n$ of which are $0$ and $n$ of which are $1$. We run the LDP median estimation algorithm on this new set of users and by selecting how many dummy users to include from the left and from the right, we can use their responses to estimate any quantile with error probability $\alpha_1=O(\alpha)$ with probability $1-O(\alpha^2)$. Union bounding over the equally spread $O(1/\alpha)$ quantiles $\alpha,2\alpha,\dots, \lfloor 1/\alpha\rfloor\cdot \alpha$, we obtain a CDF estimation algorithm which has error $\alpha_1$ with probability $1-O(\alpha)$. In particular, the expected error of this non-interactive protocol is $O(\alpha)$. Now the lower bound from~\cite{edmondsNU20} kicks in which in turn gives the lower bound for median estimation, with the $\log^4(1/\alpha)$ stemming from the fact that we have to apply their lower bound with $\eps_1=O(\eps\log(1/\alpha))$.

\subsection{Shuffle DP for Median Estimation (\cref{thm:main-shuffle})}
Our core contribution with~\cref{thm:main-shuffle} is to demonstrate explicit trade-offs which exist when considering trust models, and rounds of adaptivity. While adaptive algorithms which query $O(1)$ users per round are extremely sample efficient, they remain fundamentally incompatible with the shuffle model. We introduce protocols which exchange the benefits of faster learning for larger groups amenable to shuffling, and show that such protocols can compete in practical parameter regimes.

Building on the ``near-optimal'' analysis of~\cite{feldman21shuffle}, we introduce protocols with $r=\log_2\ab$ rounds of adaptivity which sample batches of $n/r$ users at each round. Our ``binary search with repetitions'' algorithm~\cref{thm:main-shuffle} iteratively draws $n/r$ users at random, and after shuffling their private outputs, learns one of the $r$ pivots up to accuracy $\acc$ and failure probability $\failp/r$. Union bounding over all $r$ steps ensures we return an $\acc$-approximate quantile with probability $1-\failp$. 

The full proof of~\cref{thm:main-shuffle} can be found in~\cref{sec:naive-shuffle}.

%
%
%

%
%
%
%
%
%
%
%



%
   %
%
%

\section{Median Estimation with Adaptive LDP (\cref{thm:main-emp})}\label{sec:proof-of-main-adaptive-up}

In this section we prove~\cref{lemma:CDF-bound} and~\cref{thm:main-emp}, postponing the proof of \cref{thm:NBS-changing-probabilities} to \cref{app: proof theorem 3.1}.
%
We start with the following technical lemma.

\begin{restatable}[]{lemma}{cdfperm}\label{lemma:azuma-perm}\
Let $b_1,\dots,b_{2n}\in \{0,1\}$, $\pi: \{1,\dots,2n\}\to \{1,\dots,2n\}$ a random permutation, and $c_i=b_{\pi(i)}$ for $0\leq i < 2n$. Let $Y_i=|\{i< j\leq 2n \mid c_j=0\}|$. Further define $X_i=\frac{Y_i}{2n-i}-\frac{Y_0}{2n}$. For any $t\geq 0$,
\[
\Pr\left[\max_{1\leq i \leq n} |X_i|\geq t\right]\leq 2\exp\left(\tfrac{-t^2n}{2} \right).
\]
\end{restatable}
\begin{proof}
We first note that $(X_i)_{i=0}^n$ forms a martingale. To see this, first observe that
\[
\E[Y_{i+1}\mid (X_j)_{j\leq i}]=Y_i-\frac{Y_i}{2n-i}.
\]
Indeed, conditioning on $\pi(1),\dots, \pi(i)$, the probability that $c_{i+1}=b_{\pi(i+1)}=0$ is exactly $\frac{Y_i}{2n-i}$. Thus, 
\begin{align*}
\E[X_{i+1}\mid (X_j)_{j\leq i}]&=\frac{1}{2n-i-1}\left(Y_i-\frac{Y_i}{2n-i}\right)-\frac{Y_0}{2n}\\
&=\frac{Y_i}{2n-i}-\frac{Y_0}{2n}=X_i
\end{align*}
Moreover, writing $Y_{i+1}=Y_i-b$ where $b\in \{0,1\}$ for a given $i< n$, we have
\[
|X_{i+1}-X_i|
=\frac{Y_i}{(2n-i)(2n-i-1)},
\]
if $b=0$, and 
\[
|X_{i+1}-X_i|=\frac{2n-i-Y_i}{(2n-i)(2n-i-1)},
\]
if $b=1$. Now, $Y_i$ is exactly the number of zeros among the $2n-i$ values $\pi(i+1),\dots,\pi(2n)$, so trivially $0\leq Y_i\leq 2n-i$. It follows that for $i<n$, in either of the cases $b\in\{0,1\}$,
\[
|X_{i+1}-X_i|\leq\frac{1}{2n-i-1}\leq\frac{1}{n}.
\]
Finally, $X_0=0$, so we may apply Azuma's inequality (Theorem~\ref{thm:azuma} of~\cref{app:add-def}) with an appropriate rescaling of the $X_i$'s to obtain that
\[
\Pr\left[\max_{1\leq i \leq n} |X_i|\geq t\right]\leq 2\exp\left(\frac{-t^2n}{2} \right),
\]
as desired.
\end{proof}
It is now easy to obtain~\cref{lemma:CDF-bound}.
\begin{proof}[Proof of~\cref{lemma:CDF-bound}]
Suppose without loss of generality that $n=2n'$ is even. Fix $j\in \ab$ and define $b_i=[x_i\leq j]$ and $c_i=b_{\pi(i)}=[y_i\leq j]$ for $i\in [n]$. Let $Y_t=|\{t< i\leq 2n \mid c_i=0\}|$. Then $p_j^t=\frac{Y_t}{n-t}$, so plugging into Lemma~\ref{lemma:azuma-perm}, we find that,
\begin{align*}
\Pr[\max_{0\leq t\leq n'}|p^{t}_j-p^{0}_j|\geq \alpha]&\leq 2\exp\left( \tfrac{-\alpha^2n}{2}\right)\\&\leq 2\exp\left(\tfrac{-C\log \ab}{2}\right)\leq 2\ab^{-C/2}.
\end{align*}
Choosing $C$ sufficiently large and union bounding over all $j \in [\ab]$, the result follows.
\end{proof}
Finally, assuming~\cref{thm:NBS-changing-probabilities}, we can prove our main theorem~\cref{thm:main-emp}.
\begin{proof}[Proof of Theorem~\ref{thm:main-emp}]
We pick a random permutation $\pi:[n]\to [n]$ and define $y_t=x_{\pi(t)}$, the input of user $\pi(t)$. For $j\in [\ab]$ and $t<n$, we define $q_j^t=\frac{|\{t< i\leq n\mid y_i\leq j\}|}{n-t}$ and $q_0^t=0$. Thus the map $j\mapsto q_j^t$ is the empirical CDF of the users $y_{t+1},\dots, y_n$. 

Our algorithm uses the algorithm of~\cref{thm:NBS-changing-probabilities} to solve \texttt{AdvMonotonicNBS}$(1/2,\alpha\eps/8,1)$ with the adversarial probabilities $\{p_j^t\}_{j=1}^\ab$ to be described shortly. To do so, whenever the algorithm calls for flipping a coin $j$ at step $t$, we sample a new user $x_{\pi(t)}$ and apply randomized response to the variable $[x_{\pi(t)} \leq j]$, retaining the bit with probability $\frac{e^\eps}{1+e^\eps}$ and flipping it otherwise, to get a variable $z_j^t$. By standard properties of randomized response, this protocol satisfies the $\eps$-LDP requirement. Moreover, the probability $p_j^t$ that $z_j^t=1$ is $p_j^t=q_j^t\cdot \frac{e^\eps}{1+e^\eps}+(1-q_j^t)\cdot \frac{1}{1+e^\eps}$ and so
\begin{align}\label{eq:use-GP2}
|p_j^t-1/2|=\left|\lambda_j^t\cdot \frac{e^\eps-1}{1+e^\eps}\right|\geq \frac{\eps|\lambda_j^t|}{4},
\end{align}
where we have written $q_j^t=1/2+\lambda_j^t$.
Using that $n\gg \frac{\log \ab}{\eps^2\alpha^2}\gg \frac{\log \ab}{\alpha^2}$, it follows from Lemma~\ref{lemma:CDF-bound}, that $|q^{t}_j-q^{0}_j|\leq \alpha/5$ for all $t\leq n/2$ and $0\leq j\leq \ab$ with high probability in $\ab$. Thus,
\[
|p_j^t-p_j^0|=\frac{|q_j^t-q_j^0|(e^\eps-1)}{1+e^\eps}\leq \frac{\eps\alpha}{10},
\]
where the bound $\frac{e^\eps-1}{1+e^\eps}\leq \eps/2$ follows from a second degree Taylor expansion of the maps $f:\eps\mapsto \frac{e^\eps-1}{1+e^\eps}$ observing that $f'(0)=1/2$ and $f''(\eps)<0$.

It now follows from Theorem~\ref{thm:NBS-changing-probabilities}, that using the noisy feedback from at most $n/2$ of the users, the algorithm finds an $(1/2, \frac{\alpha\eps}{4})$-good coin $j^*$ with high probability in $\ab$. In particular  $p_{j^*}^0\leq \frac{1}{2}+\frac{\alpha\eps}{4}$ and $p_{j^*+1}^0\geq \frac{1}{2}-\frac{\alpha\eps}{4}$. It thus follows from equation~\eqref{eq:use-GP2} that $q_{j^*}^0\leq 1/2+\alpha$ and $q_{j^*+1}^0\geq 1/2-\alpha$. Therefore $j^*+1$ is an $\alpha$-approximate median of $\{x_i\}_{i=1}^n$ completing the proof.
\end{proof}