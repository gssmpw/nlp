\section{Preliminaries}

{\bf Diffusion models.} A diffusion model consists of a forward diffusion process that converts original data $\rvx_0$ to its latent $\rvx_t$ (where $t$ denotes the timestep) via progressive noise addition and a reverse sampling process that starts from latent $\rvx_t$ and generates data $\hat{\rvx}_0$ via sequential denoising steps. 

Take the denoising diffusion probabilistic model (DDPM)~\citep{ho2020denoising} as an example. Given $\rvx_0$ sampled from the real data distribution $q_\mathrm{data}$, the diffusion process is formulated as a Markov chain: 
\begin{equation}
q(\rvx_t | \rvx_{t-1}) = \gN(\rvx_t ; \sqrt{1 - \beta_t}\rvx_{t-1}, \beta_t \bm{\text{I}}) 
\end{equation}
where $\{\beta_t\in (0,1)\}_{t=1}^T$ specifies the variance schedule. For sufficiently large $T$, the latent $\rvx_T$ approaches an isotropic Gaussian distribution. Starting from $p(\rvx_T) = \gN(\rvx_T; \bm{0}, \bm{\text{I}})$, the sampling process maps latent $\rvx_T$ to data $\hat{\rvx}_0$ in $q_\mathrm{data}$ as a Markov chain with a learned Gaussian transition:
\begin{equation}
p_\theta(\rvx_{t-1} | \rvx_t) = \gN( \rvx_{t-1}; \bm{\mu}_\theta(\rvx_t, t), \bm{\Sigma}_\theta(\rvx_t, t)) 
\end{equation}
To train the diffusion model $\rveps_\theta(\rvx_t, t)$ that predicts the cumulative noise up to timestep $t$ for given latent $\rvx_t$, DDPM aligns the mean of the transition $p_\theta(\rvx_{t-1} | \rvx_t)$ with the posterior $q(\rvx_{t-1}| \rvx_t, \rvx_0)$: 
\begin{equation}
\label{eq:mean-align}
\min_\theta \E_{\rvx_0 \sim q_\mathrm{data}, t \sim \gU(1, T), \rveps \sim \gN(\mathbf{0},  \mathbf{I})} \| \rveps - \rveps_\theta(\sqrt{\bar{\alpha}_t} \rvx_0 + \sqrt{1-\bar{\alpha}_t}\rveps, t)  \|^2  \nonumber  \quad \text{where} \quad  \bar{\alpha}_t = \prod_{\tau=1}^t (1  - \beta_\tau)
\end{equation}
Once trained, starting from $\rvx_T \sim \gN(\mathbf{0}, \mathbf{I})$, the sampling process iteratively invokes $\rveps_\theta$ to sample $\hat{\rvx}_0$.



\subsection{Retrieval-Augmented Diffusion Model}

Professional artists often begin their creative process by sketching a rudimentary outline, setting a foundational layout before embellishing the intricate details~\citet{wolters2012drawing}. Similarly, recent research has shown that many image generative models such as Variational Autoencoders (VAE)~\citet{kim2018disentangling}, Generative Adversarial Networks (GAN)~\citet{karras2020analyzing}, and the Diffusion Models~\citet{yang2023disdiff} have exhibited the same disentanglement property. Specifically, within diffusion models, this characteristic allows the early stages of the sampling steps to establish the foundational layouts of the image, and it is possible to start from an intermediate step and produce images with minor changes while retaining the same semantic layout~\citet{wu2023uncovering}. Further research has also revealed that from the same intermediate diffusion step $x_{t^{*}}$, it is possible to interpolate and generate diverse visual expressions—varying in details such as hair length or facial features—while maintaining similar layouts~\citet{wang2023infodiffusion}. This further demonstrates that similar intermediate steps can be shared by the generation processes of many different semantically similar images. 

Following this assumption,~\citet{zhang2023redi} introduced an innovative approach in which some of the early steps in the diffusion model are substituted with pregenerated diffusion trajectories, thereby conserving computational resources. To be more specific, they first employed a pre-trained Stable Diffusion model to generate the full diffusion trajectory and store all the intermediate denoising results as a knowledge base. When generating images using the same model, Redi will denoise the encoded latent embedding from a Gaussian vector $X_T$ for the first 20 percent of the diffusion timesteps and use them as keys. The generated keys will be used to find matching nearest neighbors with the lowest L2 distance in the knowledge base. Then, they replaced the next 60 percent of the diffusion steps with those that were already stored in the knowledge base. In the end, the Stable Diffusion model will resume the denoising of the last 20 percent of the image generation process. Their proposed method speeds up the image generation process by more than two times while maintaining high quality. 

However, the early stages of the denoising results are comprised of a large amount of Gaussian noises and only a small portion of predicted $X_0$. And matching neighbors with the lowest $L2$ distances between the early stages of diffusion denoising results can be similar to random drawing. To circumvent this issue, ~\citet{zhang2023redi} restricted their experiments to a deterministic generation of a pre-trained Stable Diffusion Model with a fixed random seed, ensuring all the diffusion trajectories will start from the same Gaussian Vector $x_T$ and follow the same trajectory every time. In order to apply Redi to an unrestricted Stable Diffusion model, we will have to generate a brand new trajectory knowledge base to match the new initialization starting point ${{x_i}_T}$ for each generation. Given that it takes hours to generate the full trajectory knowledge base by running a pre-trained Stable Diffusion model, the wide use of Redi becomes highly infeasible. 

Expanding upon Redi's idea, our approach proposes leveraging the vast quantities of publicly available image datasets to construct a more versatile and rapidly accessible knowledge base for the retrieval of the diffusion trajectories. Instead of using a pre-trained model for generating all the diffusion trajectory, we propose to take advantage of the inherently lower computational complexity of the forward process in diffusion models compared to the reverse process. Our approach significantly reduces the time required to generate a new trajectory knowledge base from hours to mere seconds, enhancing the feasibility of generating diverse and unique images for different initial vectors $X_T$ at each sampling iteration.

