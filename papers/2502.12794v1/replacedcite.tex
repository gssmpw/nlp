\section{Related Work}
{\bf Differentially private data generation.} As an important yet challenging problem, enforcing DP into training a variety of advanced generative models has attracted intensive research effort____, including generative adversarial networks____, variational autoencoders____, and customized architectures____. For instance, ____ pre-train perceptual features using public data and fine-tune only data-dependent terms using maximum mean discrepancy under the DP constraint. 

{\bf Differentially private diffusion models.} In contrast, the work on privatizing diffusion models is relatively limited. Notably, \dpdm____ integrate DP-SGD____ with a score-based diffusion model____; ____ propose to pre-train a diffusion model with public data and then fine-tune the model using DP-SGD on private data; \dpldm____ apply a similar fine-tuning strategy to a latent diffusion model____; PrivImage____ queries the private data distribution to select semantically similar public samples for pretraining, followed by DP-SGD fine-tuning on the private data. However, all these methods share the drawbacks of significant utility loss, excessive batch sizes, or expensive inference costs. Beyond the pre-training/fine-tuning paradigm, recent work also explores synthesizing DP datasets by querying commercial image generation APIs (e.g., Stable Diffusion and DALL-E2) to approximate the distributions of private data____. 


{\bf Retrieval augmented generation.} Initially proposed to enhance the generative quality of NLP models by retrieving related information from external sources____, RAG has been extended to utilize local cohorts in the training data to facilitate image synthesis. For instance, rather than directly outputting the synthesized sample, ____ compute the average of the sample's nearest neighbors in the training data. ____ use an external image dataset to provide enhanced conditional guidance, augmenting the text prompt during the text-to-image generation. ____ explore RAG to accelerate the inference process of a diffusion model by reusing pre-computed sample trajectories as surrogates for skipping intermediate sampling steps. However, existing work primarily focuses on employing RAG in the inference stage to improve generative quality or efficiency.

% instead of an external knowledge base. For example, Instance-conditioned GAN (ICGAN)____ used an averaged instance of the nearest neighbors in training set $ h_i = \frac{1}{n} \prod_{i=1}^n \phi{x_i} = f_{\phi}(x_i) $ instead of a single instance $x_i$ for generating a new sample. The first attempt to apply retrieval augmentation in diffusion models____ utilized an external image dataset as external knowledge base. The retrieved nearest neighbors will serve as an enhanced conditional guidance to augment the text prompt during a text-to-image generation. The results show that the retrieval-augmented model can achieve comparable results with a smaller scale and fewer parameters.  

To our best knowledge, this presents the first work on integrating RAG in the DP training of diffusion models, aiming to improve the generative quality, memory footprint, and inference efficiency over the existing DPDM approaches. 


% Most of the existing works on differentially private diffusion models provide the DP privacy guarantee through the utilization of DP-SGD____ during training. \dpdm____ is the first attempt to integrate DP-SGD with a score-based diffusion model____. The paper demonstrates an obvious trade-off between the performance of the model and the privacy guarantee. Given the equation to calculate the privacy budget: 
% $\epsilon \geq \frac{c \cdot \frac{L}{N} \cdot \sqrt{T \log(1/\delta)}}{\sigma}$ with L being the batch size and N being the total amount of data, we can see that to reduce the scale of noises $\sigma$ added to the gradient under the same privacy budget $\epsilon$, one feasible way is to increase the batch size L. Therefore, to improve the performance of the model,____ chose to utilize an excessively large batch size (8192) for training. In addition, to reduce the effect of noise,\dpdm repeated the gradient calculation of each batch for eight times which bears additional load to the training process. To find a way around this bottleneck, ____ was the first to propose to pre-train a diffusion model with public data, then fine-tune the model with DP-SGD on private data. Similarly, they used an extremely large batch size (16384) to achieve superior results. The work only explored how to pre-train a probabilistic diffusion model with the Imagenet dataset and privately fine-tune the model with either Cifar10 or Camelyon17____. More recently, \dpldm____ applied a similar fine-tuning technique to Latent Diffusion Models (LDM)____ which are getting the most popularity for its wide usage academically and commercially. Moreover, the authors found out that the privately fine-tuned LDM can perform better by modifying only the selected attention layers while freezing other parameters of the pre-trained model. Although \dpldm still used an excessively large batch size for training, it outperforms the previous work by ____ and demonstrates more utilities on other datasets such as CelebA and Mnist. In addition to the pre-training/fine-tuning scheme, other works have explored the possibility of using a publicly available API (Stable Diffusion/DALLE) to approximate the distributions of private data, or consider the noise-adding forward process as part of the privacy guarantee as well____.


% {\bf Retrieval augmented generation.} Retrieval-Augmented Generation (RAG) was first designed to enhance Natural Language Processing (NLP) models by gathering related information from external memory____. Earlier attempts to apply similar ideas to the image domain utilized local cohorts within the training data instead of an external knowledge base. For example, Instance-conditioned GAN (ICGAN)____ used an averaged instance of the nearest neighbors in training set $ h_i = \frac{1}{n} \prod_{i=1}^n \phi{x_i} = f_{\phi}(x_i) $ instead of a single instance $x_i$ for generating a new sample. The first attempt to apply retrieval augmentation in diffusion models____ utilized an external image dataset as external knowledge base. The retrieved nearest neighbors will serve as an enhanced conditional guidance to augment the text prompt during a text-to-image generation. The results show that the retrieval-augmented model can achieve comparable results with a smaller scale and fewer parameters.  

% In a recent work____, differentially private retrieval augmentation was implemented in Latent Diffusion Models in an effort to improve the DP-model's performance while providing some privacy guarantees. The retrieval augmentation in this work follows the previous work____ that augment the text-prompt with images from a image data base. Both the text prompt and the retrieved images will be encoded by the CLIP encoder and used as guidance during the sampling process. In addition to retrieving data from a public dataset. The differentially private retrieval-augmented diffusion model____ applied a DP-retrieval algorithm to retrieve data from a predefined private dataset and interpolate them with the data retrieved from a public dataset. However, in both cases____ the retrieval augmentation was only augmenting the text prompt while keeping the original diffusion model intact. Therefore, the differtially private retrieval does not provide privacy guarantee for the diffusion model as you receive a non-private unconditional diffusion model if choosing the empty string "" as the text prompt. 



% In our work, we used the pre-generated checkpoints of \dpdm and trained \dpldm with smaller epochs as benchmarks for comparison. Our retrieval augmentations are directly applied to the trained models of \dpldm for fair comparison. We did not include the DP-diffusion by ____ because \dpldm outperform its setting across the broad and we can hardly create an environment to support a batch size of 16k+.