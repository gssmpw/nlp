
\section{Evaluation}


% \textbf{Datasets.} We conduct the evaluation across 5 sentence classification datasets (SST-2, MR, CR, SUBJ, TREC) widely used to benchmark prompt-based few-shot learning methods\mcite{gao2021making,liu2021pre,zhang2021differentiable}. We follow the same setting of LM-BFF\mcite{gao2021making}, which samples $K = 16$ samples per class to form the training and validation sets respectively. The dataset statistics are summarized in Table\mref{tab:datasets}. 

% \subsection{Experimental Setting}
% \begin{table}[h!]
% \centering
% \begin{tabular}{ |p{4.5cm}|p{4.5cm}|p{4.5cm}|  }
%  \hline
%  \multicolumn{3}{|c|}{Public and Private training datasets for Our work and DP-LDM} \\
%  \hline
%  Public training dataset\newline & Trajectory knowledge base\newline & Private training dataset \newline\\
%  \hline
%  EMNIST~\citet{cohen2017emnist} (training) & EMNIST (validation)  &MNIST (3$\times$32$\times$32)\newline~\citet{deng2012mnist}\\
%  \hline
%  Imagenet32~\citet{deng2009imagenet} (training)& imagenet32 (equivalent classes in validation)  & Cifar10~\citet{krizhevsky2009learning} \\
%  \hline
%  FFHQ32~\citet{karras2019style} (training) & FFHQ32 (validation) & CelebA32~\citet{liu2015faceattributes}\\
%  \hline
%  FFHQ64~\citet{karras2019style} (training)& FFHQ64 (validation)& CelebA64~\citet{liu2015faceattributes} \\
%  \hline
% \end{tabular}
% \caption{Table of datasets used in experiments}
% \label{table:1}
% \end{table}

\subsection{Experimental Setting}

{\bf Datasets.} We focus on the image synthesis task. In each task, we use the public dataset $\gD^\mathrm{pub}$ to pre-train the diffusion model and build the trajectory knowledge base for the retrieval-augmented generation, and use the private dataset $\gD^\mathrm{pub}$ to further fine-tune/train the diffusion model in a differentially private manner. Specifically, we consider the following 4 settings: i) EMNIST~\citep{cohen2017emnist} (public) and MNIST~\citep{deng2012mnist} (private), ii) ImageNet32~\citet{deng2009imagenet} (public) and CIFAR10~\citep{krizhevsky2009learning} (private), iii)  FFHQ32~\citep{karras2019style} (public) and CelebA32~\citep{liu2015faceattributes} (private), and iv) FFHQ64~\citep{karras2019style} (public) and CelebA64~\citep{liu2015faceattributes} (private). More details of these datasets are deferred to Table~\ref{table:setting}.


% , and FFHQ64 as public data to pre-train the public model. For FFHQ image dataset, the first 50,000 images are defined as training set and the rest of 10,000 images are defined as the validation set. Then we use the validation sets of EMNIST, FFHQ32 and FFHQ64 to build the trajectory knowledge base. For ImageNet32 dataset, we manually select 33,173 images from the equivalent classes of classes in cifar10. During the training phase of the private diffusion model, we select MNIST~\citet{deng2012mnist}, Cifar10~\citet{krizhevsky2009learning},  and CelebA64~\citet{liu2015faceattributes} as our private dataset. In order to encode MNIST images into three-dimensional embeddings, we adopt a similar approach as DP-LDM~\citet{lyu2023differentially} that duplicate and reshapes the one-dimensional Mnist images into the 3$\times$32$\times$32 format. 

{\bf Diffusion models.} We primarily use the latent diffusion model~\citep{rombach2022high} as the underlying diffusion model and DDIM~\citep{song2020denoising} as the default sampler.

{\bf Baselines.} We mainly consider two state-of-the-art DPDM methods as baselines: differential private diffusion model (\dpdm)~\citep{dockhorn2022differentially} and differential private latent diffusion model (\dpldm)~\citet{lyu2023differentially}.

{\bf Metrics.} Following prior work~\citep{dockhorn2022differentially,lyu2023differentially}, we use the Frechet Inception Distance (FID) score to measure the generative quality of different methods. In addition, we adopt the coverage metric~\citep{naeem2020reliable} to measure the generative diversity. Intuitively, given a reference real dataset $\gD^\mathrm{real}$, the coverage is measured by the proportion of samples from $\gD^\mathrm{real}$ that have at least one sample from the synthesized data $\gD^\mathrm{syn}$ in their neighborhood (with neighborhood size fixed as 5~\citep{lebensold2024dp}). Formally, 
% In addition to reporting the FID score, which reflects the quality of the generated images, we are also curious about the diversity of our generated samples as it might be a weakness for models using RAGs. One recent work on RAG~\citep{lebensold2024dp} adopted the Coverage metric~\citep{naeem2020reliable} as a proxy for measuring image diversity. As shown in the equation below, the diversity metric is obtained by measuring the proportion of real samples that have at least one fake sample in their neighborhood. The neighborhood size is fixed as 5 to be consistent with the previous work~\citep{lebensold2024dp}. 
\begin{equation}
\text{Coverage} = \frac{1}{|\gD^\mathrm{real}|} \sum_{\rvx \in \gD^\mathrm{real}} \bm{1}_{\exists \rvx' \in \gD^\mathrm{syn} \wedge \rvx' \in \gN_\rvx}
\end{equation}
where $\bm{1}$ is the indicator function and $\gN_\rvx$ denotes $\rvx$'s neighborhood.

{\bf Privacy.} We use {\sc Opacus}~\citet{opacus}, a DP-SGD library, for DP training and privacy accounting. Following prior work~\citep{dockhorn2022differentially}, we fix the setting of $\delta$ as $10^{-5}$ for the CIFAR10 and MNIST datasets and $10^{-6}$ for CelebA dataset so that $\delta$ is smaller than the reciprocal of the number of training samples. Similar to existing work, we also do not account for the (small) privacy cost of hyper-parameter tuning.

% Both \dpdm and \dpldm employ excessively large batch sizes (e.g., 8,192). In addition, \dpdm repeats the same gradient computation for $k=8$ times, creating additional memory overhead to the training. 
To simulate settings with modest compute resources, all the experiments are performed on a workstation running one Nvidia RTX 6000 GPU. 

\subsection{Main Results}

We empirically evaluate \system and baselines. To make a fair comparison, we fix the default batch size as 64 for \system and \dpldm; we do not modify the batch size (i.e., 8,192) for \dpdm because the impact of batch size on its performance is so significant that it stops generating any recognizable images with smaller batch sizes. By default, we fix the sampling timesteps as 100 across all the methods. For MNIST, we train the diffusion model under three privacy settings $\epsilon = \{0.2, 1, 10\}$, corresponding to the low, medium, and high privacy budgets; for the other datasets, we vary the privacy budget as $\epsilon = \{1, 10\}$.

% We empirically demonstrate the performance of our Retrieval-Augmented Differentially Private Diffusion model by comparing with other state-of-the-art DP diffusion models. The main competitors are DPDM~\citet{dockhorn2022differentially} and DP-LDM~\citet{lyu2023differentially}. In both DPDM and DP-LDM, the authors adopted excessively large batch sizes of 8192. In addition, DPDM repeated the same gradient calculation step k times (k=8) which created additional memory overhead to the training. In our experiment, to establish a fair environment for competition, we used a fixed batch size of 32 to train the models and generate results for DP-LDM and our model. We did not modify the batch size for DPDM because the impact to the model is too large at small batch sizes that it stops generating any sensible image. During the sampling process, we enforce using 100 ddim timesteps for all three models.  




\begin{table}[!ht]\small
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{c|c|c|c|c}
Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm & \system  \\
\hline
\multirow{3}{*}{EMNIST$\rightarrow$MNIST} & 0.2 & 125.7 & 50.8  & \cellcolor{Red}24.0 \\
& 1  & 50.5 & 34.9  & \cellcolor{Red}18.5 \\
& 10  & \cellcolor{Red}12.9 & 27.2  & 14.1 \\ 
\hline
\multirow{2}{*}{ImageNet32$\rightarrow$CIFAR10} & 1 & $\backslash$ & 79.1  & \cellcolor{Red}63.2 \\
& 10 & 109.9 & 33.3 & \cellcolor{Red}25.4 \\
\end{tabular}
\caption{FID scores of class-conditional generation by different methods. \label{tab:cond}}
\end{table}


% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c|c|c}
% Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm & \system (training) & \system (fine-tuning) \\
% \hline
% \multirow{3}{*}{EMNIST-MNIST} & 0.2 & 125.7 & 50.8 & 89.9 & \cellcolor{Red}24.0 \\
% & 1  & 50.5 & 34.9 & 41.3 & \cellcolor{Red}18.5 \\
% & 10  & 12.9 & 27.2 & 25.4 & \cellcolor{Red}14.1 \\ 
% \hline
% \multirow{2}{*}{ImageNet32-CIFAR10} & 1 & $\backslash$ & 79.1 & 135.3 & \cellcolor{Red}63.2 \\
% & 10 & 109.9 & 33.3 & 54.1 & \cellcolor{Red}25.4 \\
% \end{tabular}
% \caption{FID scores of class-conditional generation (\dpdm: training from scratch; \dpldm: fine-tuning; $\backslash$: unusable \ting{what is unusable?}). \ting{check whether the explanation is correct} \label{tab:cond}}
% \end{table}


{\bf Class-conditional generation.} We evaluate the quality of class-conditional generation by different methods, in which, besides the input image $\rvx$, a guidance signal $y$ (e.g., $\rvx$'s class label) is also provided for training (and inference). We consider the EMNIST$\rightarrow$MNIST and ImageNet32$\rightarrow$CIFAR10 settings. Table~\ref{tab:cond} compares the generative quality of different methods. 
Observe that, under the same privacy budget, \system considerably outperforms the baselines across most cases. For instance, under the ImageNet32$\rightarrow$CIFAR10 setting, with $\epsilon =1$, \system attains an FID score of 63.2, while \dpdm fails to produce any sensible outputs, highlighting the effectiveness of RAG in facilitating the DP training of diffusion models. 

%We measure the performance of \system under two settings: ``training'' indicates that the private model is trained from scratch using the private dataset before performing RAG; ``fine-tuning'' indicates that we first pre-train a public model using the public dataset as defined in Table~\ref{table:1} and then fine-tune the model (i.e., its attention layers in particular) using the private dataset, similar to \dpldm. We then perform RAG using the fine-tuned model to generate samples. 


\begin{table}[!ht]\small
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{c|c|c|c|c}
Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm  & \system \\
\hline
\multirow{3}{*}{EMNIST$\rightarrow$MNIST (CNN)}  & 0.2  & 85.77$\%$ &  11.35$\%$ & \cellcolor{Red}96.43$\%$\\
& 1  & 95.18$\%$ & 74.62$\%$  & \cellcolor{Red}98.11$\%$\\
& 10  & 98.06$\%$ & 95.54$\%$  & \cellcolor{Red}99.04$\%$\\ 
\hline
\multirow{2}{*}{ImageNet32$\rightarrow$CIFAR10 (ResNet)} & 1 & $\backslash$ & 50.39$\%$ & \cellcolor{Red}63.61$\%$\\
& 10 & 30.41$\%$ & 66.02$\%$  & \cellcolor{Red}67.37$\%$\\
\end{tabular}
\caption{Downstream accuracy of classifiers trained on synthesized data. \label{tab:Downstream}}
\end{table}

% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c|c|c}
% Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm & \system (Unlabeled Knowledge Base) & \system (Labeled Knowledge Base)\\
% \hline
% \multirow{3}{*}{EMNIST-MNIST (CNN)}  & 0.2  & 85.77$\%$ &  11.35$\%$ & \cellcolor{Red} 90.98$\%$ & \cellcolor{Red} 96.43$\%$\\
% & 1  & 95.18$\%$ & 74.62$\%$  & \cellcolor{Red}91.99$\%$  & \cellcolor{Red}98.11$\%$\\
% & 10  & 98.06$\%$ & 95.54$\%$  & \cellcolor{Red} 92.36$\%$ & \cellcolor{Red} 99.04$\%$\\ 
% \hline
% \multirow{2}{*}{ImageNet32-CIFAR10 (ResNet-9)} & 1 & $\backslash$ & 50.39$\%$ & \cellcolor{Red}34.06$\%$ & \cellcolor{Red}63.61$\%$\\
% & 10 & 30.41$\%$ & 66.02$\%$ & \cellcolor{Red}38.91$\%$ & \cellcolor{Red}67.37$\%$\\
% \end{tabular}
% \caption{Downstream Classification Task Performance \label{tab:Downstream}}
% \end{table}

We further evaluate the utility of the data synthesized by different methods to train downstream classifiers. For a fair comparison, we use the same classifier architecture to measure the downstream accuracy. For the synthesized MNIST data, we train a Convolutional Neural Network (CNN)~\citep{krizhevsky2012imagenet} and test its performance on the MNIST testing set. For the synthesized CIFAR10 data, we train a ResNet-9~\citep{he2016deep} and evaluate its accuracy on the CIFAR10 testing set, with results summarized in Table~\ref{tab:Downstream}. Observe that the classifier trained on \system's synthesized data
largely outperforms the other methods. For instance, under the ImageNet32$\rightarrow$CIFAR10 setting with $\epsilon=1$, \system attains 63.6\% downstream accuracy with 10\% higher than the baselines, indicating the high utility of the data synthesized by \system. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/cifar10-sample.pdf}
    \caption{Random samples synthesized by \system and baselines trained under the ImageNet32$\rightarrow$CIFAR10 setting with $\epsilon = 10$.
    \label{fig:Cifar10}}
\end{figure}


We also qualitatively compare the class-conditional samples generated by \dpdm, \dpldm, and \system trained under the ImageNet32$\rightarrow$CIFAR10 setting (with $\epsilon = 10$), as shown in Figure\mref{fig:Cifar10}. It is observed that across different classes, \system tends to produce samples of higher visual quality, compared with the baselines. 

% Differentially Private Diffusion Models trained on CelebA32~\ref{fig:CelebA32}, CelebA64~\ref{fig:CelebA64} and Cifar10~\ref{fig:Cifar10} at $\epsilon = 10$. 





\begin{table}[!t]\small
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{c|c|c|c|c}
Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm &  \system \\
\hline
\multirow{2}{*}{FFHQ32$\rightarrow$CelebA32} & 1  & 135.9 | 0.087 & 65.3 | 0.74 & \cellcolor{Red}52.8 | 0.96\\
& 10  & 29.8 | 0.55 & 38.0 | 0.98  & \cellcolor{Red}28.0 | 0.98\\
\hline
\multirow{2}{*}{FFHQ64$\rightarrow$CelebA64} & 1 & $\backslash$ &  72.2 | 0.59    & \cellcolor{Red}60.5 | 0.90 \\ 
& 10 & 80.8 | 0.094 & 45.2 | 0.94 & \cellcolor{Red}37.3 | 0.93 \\ 
\end{tabular}
\caption{FID (left) and coverage (right) scores of unconditional generation by different methods. \label{tab:uncond}}
\end{table}


% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c|c|c}
% Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm & \system (training) & \system (fine-tuning) \\
% \hline
% \multirow{2}{*}{FFHQ32-CelebA32} & 1  & 135.9 & 65.3 & 248.4 & \cellcolor{Red}52.8\\
% & 10  & 29.8 & 38.0 & 58.0 & \cellcolor{Red}28.0 \\
% \hline
% \multirow{2}{*}{FFHQ64-CelebA64} & 1 & $\backslash$ &  72.2 & 349.8      & \cellcolor{Red}60.5 \\ 
% & 10 & 80.8 & 45.2 & 69.9 & \cellcolor{Red}37.3 \\ 
% \end{tabular}
% \caption{FID scores of unconditional generation (\dpdm: training from scratch; \dpldm: fine-tuning; \system: fine-tuning; $\backslash$: unavailable) \label{tab:uncond}}
% \end{table}



{\bf Unconditional generation.} We further evaluate the unconditional generation by different methods under the FFHQ32$\rightarrow$CelebA32 and FFHQ64$\rightarrow$CelebA64 settings. Table~\ref{tab:uncond} summarizes the FID and coverage scores of different methods. Observe that \system outperforms the baselines by large margins in terms of generative quality (measured by the FID score). For instance, in the case of FFHQ32$\rightarrow$CelebA32 under $\epsilon = 10$, \system achieves an FID score of 37.3, which is 19.1\% and 51.8\% lower than \dpldm and \dpdm, respectively, highlighting its superior generative quality. Meanwhile, across all the cases, \system attains the highest (or the second highest) generative diversity (measured by the coverage score). Overall, \system strikes the optimal balance between generative quality and diversity among all three methods.


% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c|c}
% Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm & \system \\
% \hline
% \multirow{2}{*}{FFHQ32-CelebA32} & 1  & 0.93 & 0.97 & \cellcolor{Red}0.96\\
% & 10  & 0.98 & 0.98 & \cellcolor{Red}0.98 \\
% \hline
% \multirow{2}{*}{FFHQ64-CelebA64}& 10 & 0.91 & 0.94 & \cellcolor{Red}0.93 \\ 
% \end{tabular}
% \caption{Coverage scores of unconditional generation by different methods.  \label{tab:diversity}}
% \end{table}



\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/celeba64.pdf}
    \caption{Random samples synthesized by \system and baselines trained under the setting of FFHQ64$\rightarrow$CelebA64 (with $\epsilon = 10$).}
    \label{fig:CelebA64}
\end{figure}


We also qualitatively compare the unconditional samples generated by different methods trained on the CelebA64 dataset (with $\epsilon = 10$). Figure\mref{fig:Cifar10} shows random samples synthesized by \dpdm, \dpldm, and \system (more samples in \msec{sec:additional}). Observe that in general \system tends to produce unconditional samples of higher visual quality, compared with the baselines. 



\subsection{Ablation Studies}
\label{sec:ablation}

Next, we conduct ablation studies to understand the impact of various key factors, such as batch size and knowledge base size, on \system's performance. We use the FFHQ32$\rightarrow$CelebA32 (with $\epsilon = 10$) %\ting{check if correct}% 
as the default setting.


{\bf Retrieval accuracy.} Recall \system relies on retrieving the most similar trajectory from the knowledge base. A crucial question is thus whether \system indeed retrieves semantically relevant neighbors. To answer this question, under the class-conditional generation, we evaluate the accuracy of \system in retrieving the neighbors from the class corresponding to the given label (e.g., ``automobile''). 
% have designed a down-stream task for the nearest neighbor search as shown in figure {}. Given a pre-trained class-conditional diffusion model trained in CIFAR10, we generate the first 20$\%$ of the denoising trajectory with a certain class label (e.g. ``automobile''). Then we used the generated trajectory as "keys" to search for top 100 nearest trajectories in the trajectory knowledge base. Then, similar to a KNN classifier of the SimCLR~\citet{chen2020simple} model, 
We calculate the top-$k$ ($k = 1, 5$) accuracy based on the true labels of the retrieved neighbors. Under the setting of ImageNet32$\rightarrow$CIFAR10, \system attains 81.2$\%$ top-1 accuracy and 94.4$\%$ top-5 accuracy, respectively, indicating its effectiveness. % Overall, the feature extractor achieved more than 40$\%$ top1 accuracy and over 85$\%$ top5 accuracy when classifying across all classes in Cifar10 dataset with early trajectories. 



% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c|c|c}
% Model & Batch size = 16 & Batch size = 32 & Batch size = 64 & Batch size = 128 & Batch size = 256 \\
% \hline
% \dpldm & 43.4215,42.6609  & 39.8486,38.2896 & 38.3910,37.1322 & 36.3844,36.2427 & 35.1083,36.0950\\
% \system  & 29.4865,28.2899 & 28.5530,27.3846 & 27.9487,27.0438 & 27.3621,26.6403 & 26.7170,26.3205 \\
% %\hline 
% \end{tabular}
% \caption{Impact of batch size.\label{tab:ablation_batch}}
% \end{table}

% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c|c|c}
% Model & Database size = 50000 & Database size = 10000 & Database size = 2000 & Database size = 500 & Database size = 100 \\
% \hline
% FID  & 27.2346,26.8340 & 27.9487,27.7187 & 28.6275,28.1590 & 30.0893,30.6383 & 38.7700, 41.2479 \\
% \hline 
% Density & 1.25696,1.35464 & 1.378,1.41406 & 1.43942,1.39368 & 1.3097,1.31904 & 1.3378,1.28156 \\
% \hline 
% Coverage & 0.9816,0.9771 & 0.982,0.9661 & 0.959,0.9562 & 0.9425, 0.9422 & 0.902,0.8971 \\

% \end{tabular}
% \caption{Impact of Knowledge base size.\label{tab:ablation_knowledge}}
% \end{table}


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
   \vspace{5pt}
\includegraphics[width=0.49\textwidth]{figs/batchsize.pdf}
    \caption{Impact of batch size.}
    \label{fig:batchsize}
\end{wrapfigure}
\textbf{Batch size.} In contrast to existing methods (e.g., \dpdm and \dpldm) that typically require excessively large batch sizes (e.g., 8,192 samples per batch), by fully utilizing the public data using its RAG design, \system can generate high-quality samples under small batch sizes. Here, we evaluate the impact of batch-size setting on the performance of \system and \dpldm, with results illustrated in Figure~\ref{fig:batchsize}. It can be noticed that \system attains an FID score of 29.5 under a batch size of 16 while its score steadily improves to 26.67 as the batch size increases from 16 to 256, highlighting its superior performance under small batch sizes. 

\begin{table}[!ht]\small
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{c|c|c|c|c}
Setting & Privacy ($\epsilon)$ & \dpdm & \dpldm & \system \\
\hline
\multirow{2}{*}{FFHQ32$\rightarrow$CelebA32} & 1  & 153.1 & 72.2 & \cellcolor{Red}56.6\\
& 10  & 33.0 & 42.6 & \cellcolor{Red}30.3 \\
\hline
\multirow{2}{*}{FFHQ64$\rightarrow$CelebA64} & 1 & $\backslash$ &  78.7  & \cellcolor{Red}68.9 \\ 
& 10 & 86.2 & 50.3 & \cellcolor{Red}41.1 \\ 
\end{tabular}
\caption{FID scores of unconditional generation with the PNDM sampler. \label{tab:pbdm}}
\end{table}

{\bf Sampler.} By default, we use DDIM~\citep{song2020denoising} as the underlying sampler. Here, we evaluate the influence of the sampler on \system. Specifically, we consider the state-of-the-art PNDM sampler~\citep{liu2022pseudo} and evaluate the performance of different methods in unconditional generation tasks. By comparing Table~\ref{tab:uncond} and Table~\ref{tab:pbdm}, it is observed that \system achieves similar FID scores in both cases, indicating its insensitivity to the underlying sampler.


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
   \vspace{5pt}
\includegraphics[width=0.49\textwidth]{figs/kb.pdf}
    \caption{Impact of knowledge-base size.}
    \label{fig:kb}
\end{wrapfigure}
\textbf{Knowledge base size.} One key component of \system is its trajectory knowledge base that supports RAG. We now evaluate the impact of the knowledge base size. As shown in Figure~\ref{fig:kb}, we evaluate how \system's performance varies as the knowledge-base size grows from 100 to 50,000. As expected, both \system's FID and coverage scores improve greatly with the knowledge-base size. Meanwhile, even under a small knowledge base (e.g., of size 100), \system attains satisfactory performance (e.g., with an FID score of about 40 and a coverage of about 0.9). 



\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
   \vspace{-25pt}
\includegraphics[width=0.49\textwidth]{figs/steps.pdf}
    \caption{Impact of the fraction of privatized steps.}
    \label{fig:steps}
\end{wrapfigure}
\textbf{Fraction of privatized steps.} Recall that, over the sample trajectory, \system samples the initial $(T - k)$ steps, skips the intermediate $(k - v)$ steps, and privatizes the later $v$ steps. By default, we set $k/T$ = 0.8 and $v/T$ = 0.2. We now evaluate the influence of the fraction of privatized steps $v/T$ on \system.
Figure~\ref{fig:steps} shows how \system's FID score varies as $v/T$ increases from 0.3 to 0.7 (with $k/T$ fixed as 0.8) under the FFHQ32$\rightarrow$CelebA32 setting (with $\epsilon = 10$). Notably, as more steps are privatized, \system's FID score deteriorates while its coverage score improves marginally, suggesting an interesting trade-off between the generative quality and diversity. Intuitively, a larger fraction of $v/T$ indicates less reliance on the retrieved trajectory, encouraging more diverse generations but negatively impacting the generative quality.

% \begin{table}[!ht]\small
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{c|c|c|c}
% Setting & No. of Private steps ($\epsilon= 10)$ &  \system (training) & \system (fine-tuning) \\
% \hline
% \multirow{2}{*}{FFHQ32-CelebA32} & 70 & 301.6 & 33.2\\
% & 60  &  257.4 & 31.3 \\
% & 50  &  198.5 & 30.1 \\
% & 40  &  121.0 & 28.7 \\
% & 30  &  58.0 & 28.0 \\
% \end{tabular}
% \caption{FID scores of unconditional generation of \system with different total private denoising steps\label{tab:steps}}
% \end{table}


% \ting{Impact of the size of knowledge base}

% %\ting{Impact of batch size}

% \ting{Use other latent diffusion models (e.g., https://arxiv.org/pdf/2106.05931)?}



