




\section{Background}
\label{sec:obs}




\subsection{Threat Model} 

Consider a harmful query $Q$ (e.g. "how to build a bomb") that is initially rejected by the LLM's safeguards. The adversary conceals $Q$ with a jailbreak prompt $J$, creating an input $X = \langle J, Q\rangle$. The LLM generates a response $R$ when presented with $X$. The objective of the jailbreak attack is to optimize $J$ such that $R$ provides a meaningful answer to $Q$. While directly optimizing $J$ for this goal is challenging, a common approach~\citep{gcg,autodan,autodan2} is to aim for an affirmative $R$, typically beginning with pharses like ``{\em Sure, here is how to $[Q]$}''.

Formally, given $X = \langle x_1, x_2, \ldots, x_n \rangle$, the LLM generates $R = \langle r_1, r_2, \ldots \rangle$ by iterative sampling from the next-token distribution over the vocabulary:
\begin{equation}
r_{i} \sim P(\cdot  | x_1, x_2, \ldots, x_n, r_{1}, \ldots, r_{i-1})
\end{equation}
The goal is thus to maximize the joint probability 
\begin{equation}
P(r_1, \ldots, r_k) = \prod_{i=1}^k  P(r_i | x_1, x_2, \ldots, x_n, r_{1}, \ldots, r_{i-1})
\end{equation}
with $r_1, r_2, \ldots$ designated as ``{\em Sure, here is how to $[Q]$}''. For instance, AutoDAN$_{\rm b}$ applies genetic search over a population of handcrafted templates to find the most effective jailbreak prompt $J$. 

\subsection{Token Importance} 

A KV eviction method estimates the importance of tokens in the prompt (or newly generated tokens), typically based on their attention weights, and selectively removes the KVs of less significant tokens from the KV cache. Formally, let $X = \langle x_1, \ldots, x_n \rangle$ be a given prompt and $Y = \langle y_1, \ldots, y_m \rangle$ be an observation window. 
At each attention head, it computes the softmax-normalized key-query attention map $\mathrm{attn}(\cdot, \cdot)$ between tokens in $X$ and $Y$, with $x \in X$ as the key and $y \in Y$ as the query.
%Specifically, let ${\sf emb}(x_i)$ and ${\sf emb}(y_j)$ be $x_i$'s and $y_j$'s embeddings, respectively, and $W_{\rm k}$ and $W_{\rm q}$ be the key- and query- projection matrices. 
% \begin{equation}
% {\sf attn}(x_i, y_j) = {\sf softmax}( \frac{W_k {\sf emb}(x_i) W_q {\sf emb}(y_j)}{\sqrt{d}} )
% \end{equation}
A score function $\mathrm{score}(\cdot)$ then calculates the importance of each token $x \in X$: $S = \mathrm{score}(\mathrm{attn}(X, Y))$. The KVs of the least important tokens are evicted from the cache. Typically, KV eviction is applied at each attention head independently.



Various eviction methods differ in their implementation. For instance, H$_2$O~\citep{h2o} focuses on the generation stage and defines the observation window to include both the prompt and the tokens generated thus far, with its score function aggregating attention maps across this window. In contrast, SnapKV~\citep{snapkv} focuses on the long-context setting and defines the observation window as the last segment of the prompt.

% the attention scores are calculated using a specialized mechanism focusing on an ``observation window'' at the end of the input sequence. This process can be broken down into the following steps:





% Surprisingly, when applying some of the jailbreaking prompts to large language models that are streaming with optimized KV caches such as H2O~\cite{h2o}, SnapKV~\cite{snapkv}, or KIVI~\cite{kivi}, most of the jailbreaking prompts stopped working. And it is almost impossible to find a successful jailbreaking prompt without significantly modifying the attack structure.
% In this paper, we are going to first find a way to still jailbreak the steaming llms with kv cache optimization. Then, we are going to discuss how to manipulate the kv cache to make the model more robust against jailbreaking attempts. 

% To deeply understand the mechanisms of how the jailbreaking prompts impact the attention of transformers, we calculated and ranked each token of the successful jailbreaking prompts on each attention head of each layer of the LLM. We discovered that on most of the successful jailbreaking prompts, the average attention scores on the tokens of evil prompts such as "how to steal someone's identity" becomes significantly lower than the attention scores on the rest of the tokens in the same input. 
