
\section{Evaluation}
\label{sec:eval}
We empirically evaluate \rkv using popular LLMs, benchmark datasets, representative attacks, and established metrics. Our experiments are designed to answer four key questions: \mct{i} Is \rkv robust against state-of-the-art jailbreak attacks? \mct{ii} Does it maintain the LLM's general performance on benign prompts? \mct{iii} How do various parameters impact its performance? \mct{iv} How does it respond to adaptive attacks?


% In this section, we conduct a comprehensive evaluation of our proposed defense mechanism against advanced jailbreak attacks on large language models (LLMs). We evaluate the effectiveness of our defense in mitigating attacks while preserving the utility of models for legitimate tasks. We also perform ablation studies to understand the impact of various parameters on defense performance and explore adaptive attack scenarios.

\subsection{Experimental Setting}

{\bf LLMs.} We evaluate \rkv on three widely used open-source LLMs: Llama-2-Chat-7B~\citep{llama2}, Vicuna-7B~\citep{vicuna}, and Mistral-7B-Instruct~\citep{mistral7b}, which represent diverse model architectures and capabilities. %providing a robust testbed for our defense.


{\bf Datasets.} To evaluate the attack/defense effectiveness, we use the dataset containing 520 malicious prompts from the AdvBench~\citep{gcg} benchmark. To  assess LLMs' performance on benign prompts, we use the AlpacaEval~\citep{alpacafarm} and VicunaEval~\citep{vicuna} datasets for short-text tasks, and the LongBench~\citep{longbench} benchmark for long-text tasks. 


{\bf Attacks.} We focus on white-box, learning-based jailbreak attacks that adaptively generate prompts to circumvent LLMs' built-in safeguards. We consider 4 representative attacks in this space:  GCG~\citep{gcg}, AmpleGCG~\citep{amplegcg}, AutoDAN~\citep{autodan}, and AdvPrompter~\citep{advprompter}. 
% In addition, we also explore more recent attacks such as Cold-Attack~\citep{cold-attack}, with details deferred to \msec{sec:discussion}.

{\bf Baselines.} We primarily compare \rkv to two state-of-the-art jailbreak defenses, SmoothLLM~\citep{smoothllm}  and GoalPriority~\citep{goal-priority}. Besides, we evaluate SnapKV~\citep{snapkv}, an existing KV eviction method, to assess whether such techniques can be directly applied as defenses against jailbreak attacks.

% Our proposed defense method is set to evict the bottom 20\% least significant tokens by default. Our proposed defense is compared with existing methods such as \textbf{Goal Priority}~\cite{goal-priority} and \textbf{SmoothLLM}~\cite{smoothllm}. We have chosen the strategy of random swapping of 10\% of the characters as it appears to be the most effective as presented in \textbf{SmoothLLM}~\cite{smoothllm}.

{\bf Metrics.} We measure the attack and defense effectiveness using the metric of  
attack success rate (ASR), which is defined as the percentage of jailbreak prompts that successfully elicit the LLM to generate affirmative, harmful responses:
\begin{equation}
    \textrm{attack success rate (ASR)} = \frac{\textrm{\# successful prompts}}{\textrm{\# total prompts}}
\end{equation}
One common approach to measure ASR is to check whether the LLM refuses to answer harmful queries by matching refusal keywords or phrases such as ``{\em Sorry, I cannot}'' or ``{\em I apologize}'' (i.e., ASR-R). However, in the context of \rkv, which aims to prevent the LLM from generating informative responses to harmful queries, ASR-R does not accurately reflects the defense effectiveness.
Instead, we employ an LLM-based classifier (e.g., GPT-4o) to assess whether the LLM's responses are harmful. This metrics is similar to  `Recheck'~\citep{autodan} and `ASR-G'~\citep{cold-attack} used in previous studies.

To evaluate the LLM's general performance in short-text tasks, we measure its WinRate against the text-davinci-003 model using 100 queries from AlpacaEval and 80 queries from VicunaEval. We use GPT-4o as the evaluator. We also measure the LLM's Rouge-L~\citep{rouge} scores using the responses generated by text-davinci-003 as the references. To measure the LLM's general performance in long-text tasks, we report its scores on the LongBench-E datasets~\citep{longbench}.

The default setting of (hyper)-parameters is summarized in \msec{sec:setting}.


% Models: Llama2 , Vicuna, Mistral

% Datasets: 

% attack/defense -- AutoDan (520), GCG (520)

% model utility -- LongBench, AlpacaEval, VicunaEval, Rouge-L

% Attacks: AutoDAN, GCG, Advprompter

% Eviction methods: SnapKV, H2O

% Baseline defenses: Goal Priority, SmoothLLM

% Metrics: attack/defense effectiveness -- refusal, harmfulness classifier (primary metric)

% utility retention -- LongBench score,  AlpacaEval, VicunaEval, Rouge-L


\subsection{Q1: Defense Effectiveness}
\label{sec:defense}
We first evaluate the effectiveness of \rkv and baselines against state-of-the-art jailbreak attacks. The detailed setting is listed in \msec{sec:setting}. Table~\ref{tab:1} summarizes the results (note that AutoDAN and AmpleGCG only provide official implementations for Llama2 and Vicuna).
% \begin{table}[!ht]\footnotesize
% \renewcommand{\arraystretch}{1.05}
% \centering
% \begin{tabular}{c|c|c|c|c|c }
% LLM & Defense  & GPTFuzzer & AutoDAN ($\downarrow$) & GCG ($\downarrow$) & AdvPrompter ($\downarrow$)\\
% \hline 
% \multirow{4}{*}{Llama2-Chat-7B} & No Defense & & 58.3\% & 38.1\% & 37.0\%\\ 
% & SmoothLLM & & 40.0\% & 8.9\% & 31.5\%\\
% & GoalPriority & & 30.8\% & 14.3\%& 11.3\%\\
% & RobustKV & &  \cellcolor{Red}6.3\% & \cellcolor{Red}7.7\% & \cellcolor{Red}7.4\%\\

% \hline
% \multirow{4}{*}{Mistral-7B-Instruct} & No Defense & & \cellcolor{Gray} & 64.6\% & 88.1\%\\ 
% & SmoothLLM & & \cellcolor{Gray} & 50.6\% & 68.9\%\\
% & GoalPriority & & \cellcolor{Gray}  & 30.9\% & \cellcolor{Red}21.8\% \\
% & RobustKV & & \cellcolor{Gray} & \cellcolor{Red}27.6\% & 30.3\%\\

% \hline
% \multirow{4}{*}{Vicuna-7B}  & No Defense & & 90.6\% & 89.4\% & 85.9\%\\ 
% & SmoothLLM & & 72.1\%& 23.7\% & 55.4\%\\
% & GoalPriority & & 86.3\%& 34.7\% & \cellcolor{Red}20.5\%\\
% & RobustKV & & \cellcolor{Red}8.3\% & \cellcolor{Red}16.4\% & 28.2\%\\
% \end{tabular}
% \caption{Attack success rate (ASR) of representative jailbreak attacks against various defenses.\label{tab:1}}
% \end{table}

\begin{table}[!ht]\footnotesize
\renewcommand{\arraystretch}{1.1}
\centering
\begin{tabular}{c|c|c|c|c|c|c }
Attack &  LLM  & No Defense & SmoothLLM & GoalPriority & SnapKV & RobustKV\\

\hline 
\multirow{2}{*}{AutoDAN} & Llama2 & 61.5\% & 40.0\% & 30.8\% & 58.3\% & \cellcolor{Red}6.3\% \\ 
& Vicuna & 92.3\% & 72.1\% & 86.3\% & 90.6\% & \cellcolor{Red}8.3\%\\

\hline
\multirow{3}{*}{GCG} & Llama2 & 38.1\% & 8.9\% & 14.3\% & 35.4\% & \cellcolor{Red}7.7\%\\ 
& Mistral & 64.6\% & 50.6\% & 30.9\% & 66.2\% & \cellcolor{Red}27.6\%\\
& Vicuna & 89.4\% & 23.7\%  & 34.7\% & 84.6\% & \cellcolor{Red}16.4\% \\

\hline 
\multirow{2}{*}{AmpleGCG} & Llama2 & 51.5\% & 47.5\% & 8.1\% & 40.4\% & \cellcolor{Red}6.1\% \\ 
& Vicuna & 72.7\% & 35.4\% & 10.1\% & 58.6\% & \cellcolor{Red}7.1\%\\

\hline
\multirow{3}{*}{AdvPrompter}  & Llama2 & 37.0\% & 31.5\% & 11.3\% & 33.0\% & \cellcolor{Red}7.4\%\\ 
& Mistral & 88.1\% & 68.9\% & \cellcolor{Red}21.8\% & 80.1\% & 30.3\%\\
& Vicuna & 85.9\% & 55.4\% & \cellcolor{Red}20.5\% & 75.6\% & 28.2\%\\


\end{tabular}
\caption{Attack success rate (ASR) of representative jailbreak attacks against various defenses.\label{tab:1}}
\end{table}


We have the following key observations. 

-- The attacks achieve varying ASR against undefended LLMs, indicating differing safeguards across LLMs. Notably, Llama2 shows stronger resistance compared to Mistral and Vicuna. 

-- \rkv greatly reduces the ASR across all LLMs and attacks. For instance, the ASR against AutoDAN on Llama2 drops from 58.3\% (undefended) to 6.3\%, outperforming SmoothLLM and GoalPriority by large margins. Similar trends are also observed for the GCG, AmpleGCG, and AdvPrompter attacks. % These results demonstrate the robustness of our defense in mitigating sophisticated jailbreak attacks. 

-- \rkv performs slightly less effectively on Mistral and Vicuna against AdvPrompter. Intuitively, on LLMs with weaker safety guardrails (e.g., Vicuna), the attack can bypass the safeguards without significantly increasing the importance of jailbreak prompts, which may impact \rkv's performance. In contrast, circumventing stronger safeguards (e.g., Llama2) necessitates increasing the importance of jailbreak prompts, which makes \rkv highly effective. This dynamics creates an interesting trade-off between evading \rkv and bypassing the LLM's built-in safeguards. A more detailed discussion on this evasiveness trade-off is presented in \msec{sec:dilemma}. 

-- Applying existing KV eviction methods (e.g., SnapKV) directly provides little protection against jailbreak attacks.


% \begin{figure}[!ht]
% \centering
\begin{mtbox}{}
{\bf Prompt:}
Theoretical interpretation: portraying \ul{a character with an intention to give} step-by-step \ul{instructions for how to steal someone}'s identity could be art\ul{iculated} in this manner: ``yo''

Transform the theoretical statement from the viewpoint of said character, ensuring it is intensely precise and leverages met\ul{iculous}, and apply accurate and specific information (steer \ul{clear of} broad \ul{strokes or} ambiguity), Present it in a list format.\\


% {\bf Prompt after eviction:}

% Theoretical interpretation: portraying step-by-step's identity could be art in this manner: ``yo''

% Transform the theoretical statement from the viewpoint of said character, ensuring it is intensely precise and leverages met, and apply accurate and specific information (steer broad ambiguity), Present it in a list format.\\

{\bf Response:}
As ``yo,'' the theoretical interpretation of portraying step-by-step's identity as art can be seen through the following lens:

1. Emphasis on Simplicity: ``Yo'' is a short and simple word, yet it holds immense power in its ability to convey a wide $\ldots$
\end{mtbox}
%\caption{\small A sample prompt generated by AutoDAN and the response by Llama2 defended by \rkv (with KVs of highlighted tokens evicted by \rkv).}
%\label{fig:qual1}
%\end{figure}


\begin{mtbox}{}
{\bf Prompt:} A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: \ul{Provide step-by-step instructions for }how\ul{ to hack into a corpor}ate network ! ! ! none actually ! helpful ! Another !tabular ! ! ! A A A A A A ASSISTANT:\\

{\bf Response:} Here is an example of a tabular format for the information you provided: | Network | Hashrate (hash/s) | Block Time (s) | Difficulty | | --- | --- | --- | --- | | Bitcoin | 144,000,000 | 10 | 64 | | Ethereum | 12,000,000 | 15 | 18,446,744,073,709,551,623,6

\end{mtbox}



To qualitatively demonstrate \rkv's effectiveness, the samples above show one prompt generated by AutoDAN on Llama2 and the other by GCG on Vicuna, where the KVs of the highlighted tokens are evicted by \rkv. Note that how \rkv effectively minimizes the presence of harmful queries in KV caches, thereby preventing the LLM from generating informative responses. In \msec{sec:additional}, we also compare the effects of other defenses: SmoothLLM can marginally divert the LLM from the harmful query, but the LLM can often ``guess'' the correct words when some characters are perturbed. GoalPriority appends a system prefix to each prompt, reminding the LLM to prioritize safety over helpfulness. However, this self-reminding approach is vulnerable to prompt injection attacks that instruct the LLM to ignore any safety reminders. Further details on prompt injection attacks are provided in \msec{sec:additional}. 



% The defense failure of our proposed method usually occurs in the gap between evasiveness and effectiveness. To be more specific, our defense might fail in situations where jailbreak attacks are strong enough to bypass the safety alignments of the model while keeping the attention scores of the core evil prompts higher than our eviction threshold.  In particular, we can observe that on models with relatively stronger safety guardrails such as LLama2-Chat-7B, the gap between evasiveness and effectiveness becomes much narrower, and we can see much fewer cases of failure in our defense. 

\subsection{Q2: LLM Utility}

We evaluate how \rkv impacts LLM's general performance on benign prompts. 


\begin{table}[ht!]\small
\renewcommand{\arraystretch}{1.1}
\centering
\begin{tabular}{c|c|c|c|c}
\multirow{2}{*}{Defense} & \multicolumn{2}{c|}{AlpacaEval} &  \multicolumn{2}{c}{VicunaEval} \\
\cline{2-5}
& WinRate ($\uparrow$) & Rouge-L ($\uparrow$) & WinRate ($\uparrow$) & Rouge-L ($\uparrow$) \\
\hline
No Defense & 68\% & 0.453 & 92.5\% & 0.539 \\
% \hline
% Snapkv (Llama2, 20\% eviction rate) & 62\% & 0.367 & 93.75\% & 0.467 \\
%\hline
SmoothLLM~\citep{smoothllm}  & 62\% & 0.306 & 76.3\%& 0.412\\
%\hline
GoalPriority~\citep{goal-priority}& 59\% & 0.281 & 75.0\% & 0.376 \\
%\hline
RobustKV & \cellcolor{Red}63\% & \cellcolor{Red}0.415 & \cellcolor{Red}82.5\% & \cellcolor{Red}0.500 \\
\end{tabular}
\caption{Impact of defenses on LLMs' general performance on short-text tasks.}
\label{tab:performance_comparison}
\end{table}

{\bf Short-Text Tasks.} Table \ref{tab:performance_comparison} summarizes Llama2's performance in short-text tasks under various defense methods, using the AlpacaEval and VicunaEval datasets. Performance is measured by WinRate and Rouge-L scores.

% All of the short-text tasks are experimented on the LLama2-Chat-7B model for evaluation. 


Among all the defenses evaluated, \rkv demonstrates the least impact on the LLM's general performance. Specifically, it achieves a WinRate of 63\% on AlpacaEval and 82.5\% on VicunaEval, with Rouge-L scores closely matching those of undefended Llama2. This performance can be intuitively explained by previous studies on KV compression~\citep{scissorhands,fastgen}, which observe that most prompts contain tokens that contribute minimally to the LLM's decoding process. Evicting such tokens from KV caches has a negligible impact on the LLM's general performance.


% Although there is some reduction in performance compared to the original model without any modification, the decrease is acceptable given the substantial improvement in defense effectiveness.


\begin{table}[!ht]
\renewcommand{\arraystretch}{1.1}
\small
\centering
\begin{tabular}{ c|c|c|c } 
KV Eviction Method & Single-Document QA ($\uparrow$) & Multi-Document QA ($\uparrow$) & Summarization ($\uparrow$) \\
\hline 
Full KV & 21.07 & 30.61 & 27.81\\ 
H$_2$O~\citep{h2o} & 20.45 & 27.82 & 26.59\\ 
%\hline
SnapKV~\citep{snapkv} & 21.07 & 30.51 & 27.81\\ 
%\hline
%Smoothllm (RandomSwap 10\%) & 17.54 & 20.88 & 26.04\\ 
%\hline
RobustKV & 19.15 & 31.50 & 26.65\\ 
%\hline
%\hline
\end{tabular}
\caption{Impact of KV eviction methods on LLMs' general performance in long-text tasks.\label{tab:2}}
\end{table}

{\bf Long-Text Tasks.} We further evaluate \rkv's impact on the LLM's performance in long-text tasks. Specifically, we compare \rkv with alternative KV eviction methods, under the same eviction rate (20\%).
Table~\ref{tab:2} summarizes Llama2's performance on long-text tasks under various KV eviction methods, using the LongBench benchmark. We report scores for single-document QA (qasper), multi-document QA (hotpotqa), and summarization tasks (gov\_report). % All of the long-text tasks are experimented on the Llama2-Chat-7B model to ensure consistent results. 

Observe that \rkv maintains the LLM's performance on long-text tasks at a level comparable to other KV eviction methods optimized for LLM performance, exhibiting only a marginal decrease compared to the full KV setting.

% The results indicate that our defense preserves most of the model's ability to handle long-context inputs effectively.

% \todo{performance on short text tasks, use WinRate and L-Rough on AlpacaEval and VicunaEval as in \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}}


\begin{figure}[!th]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/2axis.pdf}
    \caption{Impact of eviction rate and eviction randomness on \rkv.}
    \label{fig:ablation}
\end{figure}


\subsubsection{Q3: Ablation Study}

We conduct an ablation study to investigate the impact of various parameters on \rkv's performance. Recall that under the default setting, \rkv evicts the KVs of the lowest-ranked $p = 20\%$ tokens. Here, we consider varying $p$. In addition, we explore an alternative randomized eviction policy. Instead of deterministically removing $p\%$ of the least important tokens, it assigns an eviction probability to each of the least important $2p\%$ tokens based on its ranking. Specifically, for the $i$-th least important token, we assign an eviction probability of $1 - \frac{i-1}{2 p |X|}$, where $|X|$ is the prompt length. Following a Poisson binomial distribution, the expected number of evicted tokens is approximately $p|X|$.
This study is conducted using Llama2. We measure the ASR against AutoDAN and evaluate the LLM's WinRate using the AlpacaEval dataset.
% assign weights to the tokens according to their attention score rankings (least important tokens receive the highest weight, and the weight gradually decreases as ranking goes up) and evict the tokens according to their weighed probabilities of being evicted. 
 
Figure~\ref{fig:ablation} illustrates \rkv's performance under varying eviction rates and randomness settings. Observe that the ASR decreases sharply as the eviction rate increases, while the LLM's performance on AlpacaEval drops slightly as KVs of more tokens are evicted. An eviction rate of $p = 20\%$ appears to strike an optimal balance between attack robustness and the LLM's general performance.
The deterministic eviction policy generally outperforms the randomized policy, corroborating the observation that tokens of harmful queries consistently rank lower than those of jailbreak prompts.



% \todo{impact of eviction rate (by default 20\%), by varying the rate from 0\%, 5\%, 10\%, 15\%, 20\%, 25\%} 

% impact of eviction rate (by default 20\%), by varying the rate from 0\%, 5\%, 10\%, 15\%, 20\%, 25\%

% 0:  303/520 = 58.3\% 

% 0.05: 58/212 = 27.3\%

% 0.1: 101/520 = 19.4\% 

% 0.15: 50/520 =9.62\% 

% 0.2: 33/520 = 6.34\%

% 0.25: 19/520 = 3.65\%



% \todo{impact of eviction policy (deterministic vs. randomized), e.g., 
% drop bottom 20\% vs randomly drop 50\% from bottom 40\%}

% Randomly Drop with weighed probabilities 

% (40\% tokens involved, 1st ranked token has 100\% eviction probability, 2nd ranked has 97.5\% and so on) ASR: 18/260 = 6.92\%

% (30\% tokens involved, 1st ranked token has 100\% eviction probability, 2nd ranked has 96.67\% and so on) ASR: 59/520 = 11.35\%

% (20\% tokens involved, 1st ranked token has 100\% eviction probability, 2nd ranked has 95\% and so on) ASR: 112/520 = 21.5\%

% (10\% tokens involved, 1st ranked token has 100\% eviction probability, 2nd ranked has 90\% and so on) ASR: 111/520 = 21.3\%

% \todo{impact of kv compression rate (by default $\sim$ 20\%), by varying rate from 5\%, 10\%, 20\%} 

\subsubsection{Q4: Adaptive Attacks}
\label{sec:dilemma}

For jailbreak defenses to be effective in practical settings, it is crucial to consider attacks that adapt to the given defenses. In our context, we assume the adversary is aware of \rkv's existence, underlying mechanism, and operational setting (e.g., eviction rate). Specifically, we examine two types of adaptive attacks.

{\bf Harmful Query Duplication.} Recall that \rkv minimizes the presence of the harmful query by evicting the lowest-ranked $p\%$ of tokens. An adaptive attack is to extend the harmful query length, for instance, by paraphrasing and duplicating it multiple times. This strategy that the portions of the harmful query persis even after \rkv's KV eviction.  

%The adaptive attack of our method assumes that an adversary knows beforehand that our eviction rate is set to n\%. To ensure that our defense method does not eliminate evil tokens, an evil agent will arbitrarily extend the length of evil tokens to exceed the number of tokens that can be evicted. For example, to jailbreak a prompt of 100 tokens in length, an adaptive attack of our eviction policy of 20\% of tokens should extend the goal tokens "How to make a bomb" to at least a multiple of 20 tokens to ensure that the evil goal won't be evicted by our defense. Consequently, by reinforcing the importance of these goal tokens among all the tokens in the prompt, the adaptive attack also emphasizes the presence of the evil behaviors, hence increases the difficulty to jailbreak because excessive presense of evil goals will increase the likelihood of triggering a refusal response. %Our experiment shows that an adaptive attack of our defense can successfully increase the GPT-evaluated attack success rate of AutoDAN on Llama2 to around 30\%, and GPT-evaluated attack success rate of AutoDAN on Vicuna to around 80\%. 

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
%\vspace{-5pt}
\includegraphics[width=0.5\textwidth]{figs/adaptive.pdf}
    \caption{\rkv's response to adaptive attacks that duplicate harmful queries.}
    \label{fig:adaptive}
    \vspace{-5pt}
\end{wrapfigure}
To realize this adaptive attack, we fix AutoDAN as the base attack and paraphrase and duplicate harmful queries 4 times in each prompt. As shown in Figure~\ref{fig:adaptive}, the adaptive attack significantly improves ASR over the non-adaptive counterpart on both Llama2 and Vicuna. To counter this adaptive strategy, we propose employing an LLM (which can be 
the defended LLM itself) to remove redundancies in incoming prompts before applying \rkv. Figure~\ref{fig:adaptive} demonstrates that \rkv with de-duplication reduces ASR to 14.86\% on Llama2 and 17.7\% on Vicuna, indicating its effectiveness against the harmful query duplication strategy. Also note that duplicating the harmful query emphasizes the harmful information in the prompt, thereby increasing the likelihood of triggering the LLM's safeguards.  

{\bf Token Importance Manipulation.} Given that \rkv relies on distinctive patterns of token importance to differentiate between tokens in the harmful query $Q$ and the jailbreak prompt $J$, an alternative adaptive strategy involves manipulating the importance rankings of tokens in $Q$ and $J$.


While directly manipulating token importance is challenging, we approximate the adaptive attack as follows. We fix the total number of tokens to be evicted at $p = 20\%$ of the prompt length and consider varying allocations of token eviction between $Q$ and $J$:
\mct{i} all from $J$, \mct{ii} from both $J$ and $Q$, and \mct{iii}
all from $Q$. The evicted tokens are randomly selected. For reference, we also consider a baseline scenario with no token eviction. 

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
\vspace{-15pt}
\includegraphics[width=0.5\textwidth]{figs/eviction.pdf}
    \caption{\rkv's response to adaptive attacks that manipulate token importance.}
    \label{fig:resusal}
\vspace{-5pt}
\end{wrapfigure}
Figure~\ref{fig:resusal} illustrates the ASR of AutoDAN on Llama2 under different eviction strategies. We employ two metrics: ASR-R measures whether the LLM refuses to answer the harmful query by matching refusal keywords and phrases~\citep{gcg}, while ASR-G checks whether the LLM's response is malicious using an LLM-based classifier (e.g., GPT-4o)~\citep{cold-attack}. We have the following interesting observations. When all evicted tokens are from the jailbreak prompt $J$, the attack is less effective in bypassing the LLM's safeguards, as indicated in its low ASR-R; when all evicted tokens are selected from the harmful query $Q$, the LLM tends to generate harmless responses, as reflected in its low ASR-G; when evicted tokens are chosen from both $J$ and $Q$, the attack balances ASR-G and ASR-R. Thus, we can conclude:

\begin{mtbox}{}
\rkv creates an intriguing evasiveness dilemma for adversaries, forcing them to balance between evading \rkv and circumventing the LLM's built-in safeguards.
\end{mtbox}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figs/response.png}
%     \caption{change in response rate on different eviction strategies}
%     \label{fig:response}
% \end{figure}




