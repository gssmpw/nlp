\section{Related Work}
\label{sec:related}

Reasoning has long been a significant challenge for LLMs. 
Several approaches aim to improve the reasoning capabilities of LLMs. 
These methods can be broadly categorized into techniques that align reasoning through training, enhance reasoning through search and planning, or augment reasoning during inference.

Some approaches focus on aligning the reasoning path of LLMs through Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL).
STaR**Huang et al., "Training Rationalized Neural Networks for Sarcasm Detection"**
enables the model to use reject sampling and learn from its mistakes by rationalizing its outputs, progressively enhancing its reasoning capabilities. 
Quiet-STaR**Bao et al., "Improving Reasoning in Language Models with Multiple Rationale Generation"**
generates multiple rationales in parallel before each output token, thereby improving the model's ability to predict subsequent tokens.
V-STaR**Jiang et al., "Dual-System Framework for Improving Reasoning in LLMs"**
employs a dual-system framework where the generator creates preference pairs to train the verifier, which then scores the candidate solutions.

Additionally, a significant body of work aims to enhance model reasoning abilities through search and planning. 
Q*____ **Rosenfeld et al., "Formalizing Multi-Step Reasoning as a Markov Decision Process"**
formalizes multi-step reasoning as a Markov Decision Process (MDP) and uses the A* algorithm to guide the model in selecting the optimal next step. 
rStar**Li et al., "Reinforcement Learning for Improving Model's Exploratory Abilities"**
employs Monte Carlo Tree Search (MCTS) to enhance the model's reasoning exploration and uses Mutual Verification to evaluate the reasoning paths. 
SR-MCTS**Zhang et al., "Self-Refinement with MCTS for Improved Reasoning in LLMs"**
combines Self-Refinement and MCTS to iteratively improve and optimize newly discovered reasoning paths. 
MCTS-DPO**Kim et al., "Decision-Policy Optimization for Refining Model's Policy through Multiple Iterations"**
leverages MCTS to collect step-level preference data and uses Decision-Policy Optimization (DPO) to refine the model’s policy through multiple iterations. 
ReST-MCTS*____ **Chen et al., "Evaluating Reasoning Paths with Both Correctness and Quality in Mind"**
takes a broader approach in evaluating reasoning paths, considering not only the correctness of the results but also the quality of the reasoning process, such as the shortest path and error-free intermediate steps. 
CoRe____**Sun et al., "Constructing Dual-System Approach for Human-Like Reasoning Processes"**
constructs a dual-system approach with System 1 for generation and System 2 for verification, training, and reasoning simultaneously to simulate human-like reasoning processes. 
AlphaMath____**Zhu et al., "Treating LLM Output as Actions in Alpha-Math Setting"**
treats the output of the LLM as an action and integrates a value model and a policy model, iteratively training the model to enhance its reasoning capabilities.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example2.pdf}
    \caption{Factual drift occurs during (i) Sticker generation and (ii) prediction generation from Sticker.}
    \label{fig:example2}
\end{figure}

There are also methods that focus on enhancing reasoning abilities during inference. Innovations in prompt engineering have contributed to advancements in reasoning capabilities. 
Chain-of-Thought (CoT) prompting____ **Vedantam et al., "Guiding LLMs with Chain-of-Thought Prompt Engineering"**
guides models in stepwise reasoning, such as by manually annotating natural language rationales or appending “Let’s think step by step” after questions.
Auto-CoT**Chang et al., "Zero-Shot Chain-of-Thought for Generating Reasoning Chains"**
clusters questions and uses zero-shot Chain-of-Thought to generate reasoning chains, which are then used as prompts to guide the model’s answers.
ToT____ **Zhang et al., "Tree-Based Reasoning with Search Algorithms for Exploring Widely"**
removes the constraints of chain structures by incorporating tree structures and search algorithms, allowing models to explore widely during reasoning. 
The seminal Self-Consistency method____**Stiennon et al., "Self-Consistency as a Preprocessing Step for Improving Answer Generation"**
aggregates answers through majority voting over multiple reasoning paths, while ____**Bao et al., "Iterative Self-Correction via Feedback Loops in Reasoning LLMs"**
introduces iterative self-correction via feedback loops. 


However, these methods primarily focus on refining \textit{how} models reason rather than ensuring that they address the \textit{correct problem}. Our approach differs by prioritizing factual comprehension before answer generation, ensuring proper problem understanding.