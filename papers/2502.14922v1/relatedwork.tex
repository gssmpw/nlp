\section{Related Work}
\label{sec:related}

Reasoning has long been a significant challenge for LLMs. 
Several approaches aim to improve the reasoning capabilities of LLMs. 
These methods can be broadly categorized into techniques that align reasoning through training, enhance reasoning through search and planning, or augment reasoning during inference.

Some approaches focus on aligning the reasoning path of LLMs through Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL).
STaR~\citep{zelikman2022star} enables the model to use reject sampling and learn from its mistakes by rationalizing its outputs, progressively enhancing its reasoning capabilities. 
Quiet-STaR~\citep{zelikman2024quiet} generates multiple rationales in parallel before each output token, thereby improving the model's ability to predict subsequent tokens.
V-STaR~\citep{hosseini2024v} employs a dual-system framework where the generator creates preference pairs to train the verifier, which then scores the candidate solutions.

Additionally, a significant body of work aims to enhance model reasoning abilities through search and planning. 
Q*~\citep{wang2024q} formalizes multi-step reasoning as a Markov Decision Process (MDP) and uses the A* algorithm to guide the model in selecting the optimal next step. 
rStar~\citep{qi2024mutual} employs Monte Carlo Tree Search (MCTS) to enhance the model's reasoning exploration and uses Mutual Verification to evaluate the reasoning paths. 
SR-MCTS~\citep{zhang2024llama} combines Self-Refinement and MCTS to iteratively improve and optimize newly discovered reasoning paths. 
MCTS-DPO~\citep{xie2024monte} leverages MCTS to collect step-level preference data and uses Decision-Policy Optimization (DPO) to refine the model’s policy through multiple iterations. 
ReST-MCTS*~\citep{zhang2025rest} takes a broader approach in evaluating reasoning paths, considering not only the correctness of the results but also the quality of the reasoning process, such as the shortest path and error-free intermediate steps. 
CoRe~\citep{zhu2022solving} constructs a dual-system approach with System 1 for generation and System 2 for verification, training, and reasoning simultaneously to simulate human-like reasoning processes. 
AlphaMath~\citep{chen2024alphamath} treats the output of the LLM as an action and integrates a value model and a policy model, iteratively training the model to enhance its reasoning capabilities.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example2.pdf}
    \caption{Factual drift occurs during (i) Sticker generation and (ii) prediction generation from Sticker.}
    \label{fig:example2}
\end{figure}

There are also methods that focus on enhancing reasoning abilities during inference. Innovations in prompt engineering have contributed to advancements in reasoning capabilities. 
Chain-of-Thought (CoT) prompting~\citep{NEURIPS2022_9d560961,kojima2022large} guides models in stepwise reasoning, such as by manually annotating natural language rationales or appending “Let’s think step by step” after questions.
Auto-CoT~\citep{zhang2022automatic} clusters questions and uses zero-shot Chain-of-Thought to generate reasoning chains, which are then used as prompts to guide the model’s answers.
ToT~\citep{yao2023tree} removes the constraints of chain structures by incorporating tree structures and search algorithms, allowing models to explore widely during reasoning. 
The seminal Self-Consistency method~\citep{wang2023selfconsistency} aggregates answers through majority voting over multiple reasoning paths, while \citet{madaan2024self} introduces iterative self-correction via feedback loops. 


However, these methods primarily focus on refining \textit{how} models reason rather than ensuring that they address the \textit{correct problem}. Our approach differs by prioritizing factual comprehension before answer generation, ensuring proper problem understanding.