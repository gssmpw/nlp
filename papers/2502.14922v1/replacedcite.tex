\section{Related Work}
\label{sec:related}

Reasoning has long been a significant challenge for LLMs. 
Several approaches aim to improve the reasoning capabilities of LLMs. 
These methods can be broadly categorized into techniques that align reasoning through training, enhance reasoning through search and planning, or augment reasoning during inference.

Some approaches focus on aligning the reasoning path of LLMs through Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL).
STaR____ enables the model to use reject sampling and learn from its mistakes by rationalizing its outputs, progressively enhancing its reasoning capabilities. 
Quiet-STaR____ generates multiple rationales in parallel before each output token, thereby improving the model's ability to predict subsequent tokens.
V-STaR____ employs a dual-system framework where the generator creates preference pairs to train the verifier, which then scores the candidate solutions.

Additionally, a significant body of work aims to enhance model reasoning abilities through search and planning. 
Q*____ formalizes multi-step reasoning as a Markov Decision Process (MDP) and uses the A* algorithm to guide the model in selecting the optimal next step. 
rStar____ employs Monte Carlo Tree Search (MCTS) to enhance the model's reasoning exploration and uses Mutual Verification to evaluate the reasoning paths. 
SR-MCTS____ combines Self-Refinement and MCTS to iteratively improve and optimize newly discovered reasoning paths. 
MCTS-DPO____ leverages MCTS to collect step-level preference data and uses Decision-Policy Optimization (DPO) to refine the model’s policy through multiple iterations. 
ReST-MCTS*____ takes a broader approach in evaluating reasoning paths, considering not only the correctness of the results but also the quality of the reasoning process, such as the shortest path and error-free intermediate steps. 
CoRe____ constructs a dual-system approach with System 1 for generation and System 2 for verification, training, and reasoning simultaneously to simulate human-like reasoning processes. 
AlphaMath____ treats the output of the LLM as an action and integrates a value model and a policy model, iteratively training the model to enhance its reasoning capabilities.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example2.pdf}
    \caption{Factual drift occurs during (i) Sticker generation and (ii) prediction generation from Sticker.}
    \label{fig:example2}
\end{figure}

There are also methods that focus on enhancing reasoning abilities during inference. Innovations in prompt engineering have contributed to advancements in reasoning capabilities. 
Chain-of-Thought (CoT) prompting____ guides models in stepwise reasoning, such as by manually annotating natural language rationales or appending “Let’s think step by step” after questions.
Auto-CoT____ clusters questions and uses zero-shot Chain-of-Thought to generate reasoning chains, which are then used as prompts to guide the model’s answers.
ToT____ removes the constraints of chain structures by incorporating tree structures and search algorithms, allowing models to explore widely during reasoning. 
The seminal Self-Consistency method____ aggregates answers through majority voting over multiple reasoning paths, while ____ introduces iterative self-correction via feedback loops. 


However, these methods primarily focus on refining \textit{how} models reason rather than ensuring that they address the \textit{correct problem}. Our approach differs by prioritizing factual comprehension before answer generation, ensuring proper problem understanding.