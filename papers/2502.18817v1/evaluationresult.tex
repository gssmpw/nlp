\section{Evaluation Result}
In this section, we first show the performance of \method{} by regarding it as a reward model to optimize the RAG model. Then we conduct the ablation studies to show the effectiveness of different modules in \method{}. Subsequently, we evaluate the judgment quality generated by \method{} and explore the consistency of judgments across different evaluation dimensions. Finally, case studies are conducted.

\subsection{Overall Performance}
\label{sec:5.1}
In this experiment, we treat these judgment models as reward models for optimizing the RAG model~\cite{rag-ddr2024Li} and evaluate their effectiveness by examining the RAG performance.

As shown in Table~\ref{table1:overall}, we compare \method{} with three judgment models, including Raw Metric, vanilla LLMs, and SFT. Specifically, the Raw Metric model uses an automatic metric as the reward model, while vanilla LLMs and SFT rely on LLMs as reward models. In our experiments, we use Llama3-8B and Qwen2.5-14B to implement the judgment models, and both MiniCPM-2.4B and Llama3-8B to build the RAG model.

Overall, \method{} outperforms all baseline models across different RAG models, demonstrating its effectiveness in judging sampled responses of RAG models. Compared to the Raw Metric model, LLM-based judgment models achieve better optimization performance for the RAG model, even when the Raw Metric model directly optimizes RAG models to align with final evaluation metrics. It shows that LLMs have the ability to produce high-quality judgments, benefiting the training and evaluation processes of RAG systems.
In comparison to vanilla LLMs, the SFT model uses labels produced by GLM-4-plus for training, while \method{} introduces a judge-consistency based optimization method that trains LLMs without relying on additional training signals. \method{} not only outperforms the SFT model but also achieves more improvements over the Raw Metric model, highlighting the effectiveness of our judge-as-a-judge mechanism in enhancing LLMs through a self-improvement approach. Notably, the evaluation results demonstrate that the advantages of \method{} can be extended to various RAG scenarios and judgment models of different scales.





\subsection{Ablation Study}
\label{sec:5.2}
%先讲去掉consistency效果
This experiment conducts ablation studies to investigate the contribution of different modules in \method{}.

As shown in Table~\ref{table1:ablation}, we evaluate the judgment performance of three variants of the \method{} model by reranking the sampled responses from vanilla RAG models and calculating the accuracy of the top-1 ranked responses. The three models compared in the experiments are: \method{} w/o Consistency, \method{} w/o Query, and \method{} w/o Ground Truth. Specifically, \method{} w/o Consistency randomly selects the chosen and rejected responses for DPO training. Both \method{} w/o Query and \method{} w/o Ground Truth models are evaluated by removing the query and ground truth from the input prompts, respectively.

\input{table/ablation}

Compared to \method{} w/o Consistency, \method{} achieves a higher accuracy score. This demonstrates that the judge-consistency method enhances the ability of LLMs to select higher-quality responses, thereby benefiting the RAG training process. Furthermore, removing the query or the ground truth answer from the evaluation prompts results in a performance decrease for \method{}, although the performance gap is narrowed when using larger-scale LLMs. This suggests that both the query and ground truth help the LLM-based judgment models produce more comprehensive evaluations. Additionally, LLMs of a larger scale can leverage their parametric knowledge to assess the quality of RAG responses.

\input{figure/analysis_result}
\input{figure/consjudge_vs_vanilla_consistency}

\subsection{The Judgment Quality of \method{}} \label{sec:5.3}
In this section, we first present the judge agreement across different models. Then, we analyze the consistency of judgments generated by different models based on different evaluation dimensions.

\textbf{Judge Agreement.} First, we sample 100 queries from each of the datasets--NQ, HotpotQA, TriviaQA, MARCO QA, and WoW--to construct the evaluation dataset. We then collect responses from different models and ask judgment models to evaluate these responses. As shown in Figure~\ref{fig:result:quality}, we use four judgment models: GLM-4-plus~\cite{du2022glm}, \method{}, vanilla LLM, and Raw Metrics, to select the best response for each query.

Figure~\ref{fig:result:quality:agreement} presents the judge agreement between different models. Among all judgment models, the Raw Metric shows the lowest agreement with the others, highlighting that string matching alone does not provide high-quality judgment for evaluation. Notably, \method{} not only demonstrates the highest agreement with the superior LLM, GLM-4-plus, but also conducts the agreement with GLM-4-plus when evaluating the judgments produced by vanilla LLM. This illustrates that \method{} conducts more consistent judgments with GLM-4-plus. Furthermore, we evaluate the judge agreement with GLM-4-plus of both Raw Metric and \method{} models in Figure~\ref{fig:result:quality:glm}. Across all datasets, \method{} achieves higher agreement scores with GLM-4-plus, demonstrating its effectiveness in optimizing LLMs to generate more accurate judgments in various scenarios.

\textbf{Judgment Consistency.} We next randomly sample 1,000 queries each from HotpotQA and TriviaQA to construct a dataset for evaluating judgment consistency. We then ask both vanilla LLM and \method{} to perform judgments for each query, using different hybrid evaluation aspects, and compute the consistency scores of these judgments.

As shown in Figure~\ref{fig:consjudge_vs_vanilla_consistency}, the consistency scores of judgments generated by \method{} outperform those generated by vanilla LLM across both datasets. These results indicate that \method{} achieves higher consistency with different evaluation dimensions, demonstrating its ability to comprehensively incorporate tailored evaluation dimensions to produce reliable evaluation results. Notably, the advantages of \method{} are consistent across the LLMs of different scales, further illustrating its generalization ability.


\input{table/casestudy}

\subsection{Case Study}
\label{sec:5.4}
In Table~\ref{table1:casestudy}, we present two cases to illustrate the effectiveness of the \method{} method.

In the first case, the Raw Metric selects the response with the most matched phrases to the ground truth, which results in unreliable evaluation for long ground truths~\cite{EnablingLargeLanguageModelstoGenerateTextwithCitations2023GaoTianyu}. Additionally, the vanilla LLM prioritizes factual correctness, while the SFT model focuses on coherence aspects, leading to incorrect judgments. In contrast, \method{} incorporates multiple evaluation dimensions, such as ``Semantic'', ``Completeness'' and ``Coherence'', allowing for more effective and consistent evaluations. This demonstrates the effectiveness of \method{} in choosing appropriate dimensions during evaluating the responses, which helps to provide a more comprehensive assessment.

In the second case, all models select the Choice B and Choice C, which contains the ground truth answer ``(C)''. It illustrates that these models, even for these LLM-based judgment models, focus more on matching responses with the ground truth answer to conduct the judgment. However, the reasoning processes in these responses of both Choice B and Choice C contain errors. In contrast, \method{} shifts its attention from matching the ground truth answer to the ``Semantic'' dimension, showing its effectiveness.
