\section{Related Work}
Retrieval-Augmented Generation (RAG) has proven its effectiveness in various tasks, such as open-domain question answering~\cite{trivedi2023interleaving}, dialogue systems~\cite{cai2019skeleton}, and code generation~\cite{li2025building}. By integrating external knowledge into the input context~\cite{Ram2023Incontextlearning}, Large Language Models (LLMs) can alleviate hallucination issues and produce more accurate responses~\cite{RALM2024Asai}. To evaluate the performance of LLM responses, most existing RAG models rely on automatic metrics like Exact-Match during both evaluation~\cite{Radit2023Lin,Retrieval2023Gao} and training~\cite{rag-ddr2024Li}. However, these metrics may fail to offer a fair assessment when the LLM-generated responses are lengthy or semantically similar to the ground truth but not character-matched~\cite{EnablingLargeLanguageModelstoGenerateTextwithCitations2023GaoTianyu}.

Recently, LLMs, such as ChatGPT~\cite{openai2023gpt}, have demonstrated human-equivalent performance~\cite{chiang2023can,zheng2023judging,sottana2023evaluation} and are now commonly used as judgment models to assess generated responses in various tasks~\cite{chen2023huatuogpt,chen2023teaching,an2023eval,chanchateval}. These studies typically prompt LLMs to evaluate generated responses based on specific evaluation dimensions~\cite{chiang2023closer}. However, using APIs of these closed-source models incurs significant costs and reduces reproducibility, as the models may evolve behind the API~\cite{SurveyonLLM-as-a-Judge2024Gu}. To address this, some researchers are turning to open-source LLMs as alternatives, using judgments from closed-source LLMs to fine-tune open-source models and improve their evaluation capabilities~\cite{wangpandalm,zheng2023judging}.

Moreover, RAG models also employ LLMs as judges to assess the quality of generation, focusing on the relevance and faithfulness of the responses~\cite{saad2024ares,friel2024ragbench,adlakha2023evaluating}. These models typically design prompts to instruct LLMs to determine whether the generated response aligns with the facts in the retrieved document and whether all relevant information has been fully extracted and integrated~\cite{jin2024rag,jacovi2025facts,Rageval2024Zhu}. However, LLM-based evaluations are highly sensitive to prompt designs~\cite{zhou2023survival,liu2024aligning}, making the judgments inconsistent when using different dimensions for evaluating RAG responses. To mitigate this issue, \method{} introduces a judge-consistency approach to self-improve the judgment performance of LLMs in RAG systems, avoiding distillation from larger-scale LLMs~\cite{zhang2025rag}.
% the ability of judgment models by selecting more appropriate evaluation dimensions to optimize the RAG system.










% However, LLMs as the judgment model still suffer issues such as position bias, verbosity bias, and self-enhancement bias, which can compromise the accuracy of judgment model~\cite{zheng2023judging,chen2024humans,li2024llms}.

% % Compared to human evaluation and automated metrics such as ROUGE~\cite{christiano2023deep} and BERTScore~\cite{christiano2023deep}, LLMs as judgment models incur lower costs and deliver higher performance~\cite{zheng2023judging,Rageval2024Zhu}. 

% To address these issues, some works choose to optimize evaluation prompts of judgment models, such as incorporating annotated high-quality demonstrations in the prompt~\cite{min2023factscore,li2024salad} or further refining the evaluation dimensions~\cite{liu2024hd,hu2024llm}. However, many LLMs face conceptual and evaluation criteria confusion, which limits the effectiveness of prompt optimization~\cite{hu2024llm}. Besides, Some works use the judgments generated by the human or powerful LLMs like GPT-4 to fine-tune the judgment model, improving its evaluation performance~\cite{wangpandalm,zheng2023judging}. \citet{zhang2025rag} use training data generated by GPT-4o to optimize the judgment model and apply it to optimize the RAG system. However, these methods require high-quality annotated and generated training data, which is costly. In contrast to existing works, \method{} self-improves the ability of judgment models by selecting more appropriate evaluation dimensions to optimize the RAG system.


% Human evaluation is the gold standard method to judge the responses of large language models (LLMs) in various NLP tasks. Many studies collect human judgments to further improve the model performance~\cite{zhou2020learning,ouyang2022training,christiano2023deep}. To reduce the cost of judgment, some works use BERTScore~\cite{christiano2023deep}, ROUGE~\cite{lin2004rouge}, and other automation metrics as the judgment methods~\cite{rag-ddr2024Li,trung2024reft}. However, the effectiveness of these methods is less than human judgment. 




% As the performance of the judgment model improves, its application scenarios have also been further expanded, including serving as an evaluator for various NLP tasks~\cite{van2024field,SurveyonLLM-as-a-Judge2024Gu} and providing feedback for model optimization~\cite{yuan2024self,zhang24generative}. Beyond this, some works begin to apply them to retrieval-augmented generation (RAG)~\citet{jin2024rag,zhang2025rag}, a technique that integrates external knowledge retrieval to enhance response quality and reduce hallucinations. ~\citet{zhang2025rag} use training data generated by GPT-4 to train a judgment model, which was then applied to optimize the retrieval-augmented system.



% As the performance of the judgment models improves, they are not only used as evaluators for various NLP tasks~\cite{van2024field,SurveyonLLM-as-a-Judge2024Gu} but also to provide feedback signals for model optimization~\cite{yuan2024self,zhang24generative}. 


% Retrieval-Augmented Generation (RAG) is widely used in various real-world applications and improves the performance of Large Language Models (LLMs) by enriching the input with relevant external texts~\cite{Retrieval2020Lewis,REALM2020Guu} from external sources. 



% RAG-based LLMs can efficiently leverage information from retrieved documents by either directly inserting them into the prompt~\cite{Ram2023Incontextlearning} or through specialized training~\cite{Shao2023Enhancing-RAG-with-IRG-Synergy}. The method can effectively alleviate hallucination and outdated knowledge of LLMs and help LLMs generate responses with greater confidence and closer to human preferences~\cite{ActiveRetrievalAugmentedGeneration2023Jiang,RECOMP2023Xu}. 

% However, with the widespread application of RAG, some limitations of the RAG system have started to emerge. ~\cite{RAGgedEdgesTheDouble-EdgedSwordofRetrieval-AugmentedChatbots2024Feldman,RetrievalAugmentationReducesHallucinationinConversation2021Shuster} have found that the knowledge in the documents retrieved by RAG may conflict with the parameterized knowledge of LLM. To address these limitations, ~\cite{CorrectiveRetrievalAugmentedGeneration2024Yuan} trains a better retrieval model to retrieve documents that are closer to the query, ~\cite{RALM2024Asai} trains RAG model using datasets specifically designed for RAG scenarios, enabling them to process retrieved information more efficiently. ~\cite{Chain-of-Note2023Yu} further remove noise from the documents by summarizing the retrieved documents. Although every approach is effective, they lack strong alignment with human preferences, making it challenging for RAG model to distinguish between responses. 

% Some works~\cite{Privacyimplicationsofretrieval-basedlanguagemodels2023Huang,RLinRAG2024Song} have begun to try to use reinforcement learning to optimize the RAG model so that its responses are more aligned with human preferences. ~\cite{RAG-RewardBench2024Jin} summarizes the existing reward models and evaluates their performance on Question-Answering tasks in RAG scenario. But these works have not provided an general and efficient reward model for RAG, it is still a challenge. 


% With the emergence of more and more reward models, another important challenge is how to evaluate the performance of these reward models, because the reward model plays a crucial role in reinforcement learning, Its performance directly affects the effect of reinforcement learning. ~\cite{Rewardbanch2024Lambert} proposes the first benchmark that can comprehensively evaluate the performance of reward models, but it is unable to expand to multiple task scenarios. Inspired by this work, ~\cite{M-Rewardbanch2024Gureja,RM-bench2024Liu,RMB2024Vu} propose multiple comprehensive benchmarks to evaluate the performance of reward models. However, none of these works proposes a benchmark that is suitable for RAG scenario. ~\cite{RAG-RewardBench2024Jin} solves this problem and proposes the first reward model benchmark in the RAG scenario. 


%GPT-4花费较大
%放到RAG场景下
%规避一下SFT问腿(RAG-reward)
%通用方法和RAG分隔开
%evaluation方法，为什么用LLM评价，为什么训练(ddr)



%1段 RAG乱七八糟的删掉，讲一句话“metric为什么不好”，引一篇1篇文章，引出evauation
%删除ROUGE等等
%2段 LLM评价, 被广泛使用  闭源模型成本高，采用开源模型，一些通用训练提升方法
%3段 RAG场景下，我们大模型关注的点，讲出RAG ebaluatin跟通用场景下不同，(我们的方法的不同)


%找到写论文的思路，动机写明白，找引用最高的，然后看被引的论文
%challenge 1: evaluation提示设计敏感
%challenge 2: 闭源模型花费大，api背后模型升级导致不可复现
%优点 1: 不需要大量的标记数据
%优点 2: RAG中有很多维度，很敏感，容易出现bias. 我们的方法能缓解不一致性
%不同prompt影响很大，所以做一致性分析, 消除这个bias，用一致性提升

