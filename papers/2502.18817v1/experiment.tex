
\section{Experimental Methodology}
This section describes the datasets, evaluation metrics, baselines, and implementation details used in our experiments. More implementation details are shown in Appendix~\ref{app:dataset details}.

\input{table/testdataset}

\textbf{Datasets.} We describe the datasets used for training \method{} and RAG training and evaluation.


\textit{\method{} Training.} For training \method{}, we collect 11 knowledge-intensive tasks from previous works~\cite{Chung2022flan_t5,izacard2022few} to collect 73,831 instances and 3,886 instances to construct both training and development sets.

% use 11 knowledge-intensive tasks to construct the training and development sets for optimization. The full dataset statistics are provided in Table~\ref{table1:traindataset}.

\textit{RAG Training \& Evaluation.} To retrieve documents for constructing the RAG datasets, we use BGE-large~\citep{bge_embedding} with the MS MARCO V2.1~\citep{bajaj2016ms} corpus. During RAG training, we collect seven datasets from ~\citet{rag-ddr2024Li} and randomly sample 20,805 samples for the training set and 1,400 samples for the development set. For RAG evaluation, we select knowledge-intensive tasks from prior work~\citep{rag-ddr2024Li, xu2024unsupervised}, including open-domain QA tasks (NQ~\citep{nq2019Kwiatkowski}, TriviaQA~\citep{triviaqa2017Joshi}, MARCO QA~\citep{bajaj2016ms}), multi-hop QA (HotpotQA~\cite{hotpotqa2018Yang}), factoid QA (ASQA~\cite{ASQA2022Stelmakh}), and dialogue tasks (WoW~\cite{wow2019Dinan}). The data statistics are shown in Table~\ref{table1:testdataset}.

% The dataset statistics of RAG Evaluation datasets are shown in Table~\ref{table1:testdataset}.

\textbf{Evaluation Metrics.} 
For tasks with longer outputs, automated evaluation metrics, such as ROUGE, cannot evaluate the quality of outputs fairly, which has been proven by previous work~\cite{EnablingLargeLanguageModelstoGenerateTextwithCitations2023GaoTianyu,llmeval2024zhang}. Thus, we adopt the LLM-as-a-Judge method~\citep{llmeval2024zhang}, which employs GLM-4-plus\footnote{\url{https://open.bigmodel.cn/}} for evaluation MARCO QA and WoW. Besides, we use StringEM as the evaluation metric for the ASQA dataset. For other evaluation tasks, we evaluate performance using Accuracy. The prompt using GLM-4-plus to evaluate is shown in Appendix~\ref{sec:prompt details}.

\textbf{Baselines.} In our experiments, we compare \method{} with three judgment models, including the Raw Metric model and two LLM-based judgment models. For the Raw Metric model, we utilize the automatic evaluation metrics, ROUGE-L and Accuracy, as the judgment model to optimize the RAG system. Specifically, the Raw Metric model uses ROUGE-L for MARCO QA, Yahoo!QA and WikiQA datasets, and also uses Accuracy for the remaining datasets, which is the same as previous work~\cite{rag-ddr2024Li}. Additionally, we employ two LLM-based judgment model baselines: Vanilla LLM and SFT. The Vanilla LLM method directly uses the LLM as the judgment model and then leverages the evaluation prompts to ask them to produce the judgments. The SFT method further fine-tunes LLMs based on judgment results generated by a superior LLM, GLM-4-plus, which has been used in previous work~\cite{zhang2025rag} to improve the judgment performance of LLMs.

\input{table/overall}

\textbf{Implementation Details.} In our experiments, we leverage LoRA~\citep{hulora} for efficient training LLMs. We set max\_epoch to 3, learning rate to 5e-5, and the warmup ratio to 0.1. For the generation model in the RAG system, we employ the MiniCPM-2.4B~\cite{minicpm-2b2024Hu} and Llama3-8B-Instruct~\cite{touvron2023llama} as the generation models. For the judgment model, we use Llama3-8B-Instruct and Qwen2.5-14B-Instruct~\cite{qwen2.5-14b2023Bai} as the backbone models. While training the judgment model, we synthesize 8 different hybrid evaluation aspects for generating the judgment results. We use MiniCPM-Embedding\footnote{\url{https://huggingface.co/openbmb/MiniCPM-Embedding}} to assess the similarity among judgments.  




% We used 4 different models to generate 12 responses: MiniCPM-2.4B, MiniCPM3-4B, Lama3-8B-Instruct and Qwen1.5-14B-Chat, every model uses 3 different temperatures:0.5, 0.6, 0.7. In addition, we select 8 representative evaluations from a large number of hybrid evaluations. The evaluation dimensions required for these 8 hybrid evaluations are:(1) Hallucination.(2) Completeness.(3) Coherence.(4) Semantic consistency.(5) Hallucination and Completeness.(6) Coherence and Semantic consistency.(7) Hallucination, Completeness and Semantic consistency.(8) Hallucination, Completeness, Coherence and Semantic consistency. More details are shown in Appendix~\ref{sec:prompt details} and Appendix~\ref{sec:parameter details}.




%bge相似度，abcd一个，雷达图一个,训练前和训练后