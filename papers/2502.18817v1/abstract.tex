\begin{abstract}
Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. 
This paper introduces the Judge-Consistency (\method{}) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, \method{} prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. 
Our experiments show that \method{} can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by \method{} have a high agreement with the superior LLM. All codes are available at \url{https://github.com/OpenBMB/ConsJudge}.
\end{abstract}










% However, it is still a challenge to evaluate the outputs of RAG systems. Current evaluation methods mainly rely on automated metrics or use LLM-as-a-Judge method. But both methods have limitations: automated metrics often provide inaccurate evaluation results, while LLM-as-a-Judge method requires significant cost and LM-based evaluations are highly sensitive to prompt designs, which may lead to inconsistencies in judgment when using different dimensions to evaluate. In this paper, we introduce the Judge-Consistency (\method{}) method, which aims to enhance LLM-based judgment models to generate more accurate assessments for RAG models without additional labeled data. Our experiments show that \method{} effectively optimizes LLMs to generate more accurate judgments across various testing scenarios and is applicable to LLMs of different scales, demonstrating its effectiveness and generalization ability. All codes and data will be released via GitHub.
% \end{abstract}