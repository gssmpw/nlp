\section{Introduction}
Retrieval-Augmented Generation (RAG)~\cite{REALM2020Guu,Retrieval2020Lewis,asai2023self,shi2023replug} has proven effective in mitigating hallucinations~\cite{elazar2021measuring,ji2023survey,shuster2021retrieval,huang2023survey} in Large Language Models (LLMs). RAG retrieves relevant knowledge from the knowledge base and incorporates the external information as the input context~\cite{Ram2023Incontextlearning}, benefiting various knowledge-intensive tasks~\cite{trivedi2023interleaving,izacard2022few,he2021efficient}. Existing studies typically employ automated evaluation metrics to assess the outputs of RAG systems during both training~\cite{rag-ddr2024Li} and evaluation~\cite{Radit2023Lin,Retrieval2023Gao} phases. These metrics primarily focus on string-level exact matching, which is less effective for determining whether the LLM-generated responses align with the ground truth~\cite{EnablingLargeLanguageModelstoGenerateTextwithCitations2023GaoTianyu,saad2024ares}, posing a challenge for RAG systems.

\input{figure/example}


To enable more accurate evaluation of RAG models, some works incorporate LLMs as judgment models to assess the quality of generated responses~\cite{saad2024ares,friel2024ragbench,adlakha2023evaluating}, relying on their human-equivalent evaluation capabilities~\cite{chiang2023can,zheng2023judging,sottana2023evaluation}. These judgment models introduce specific evaluation dimensions, such as hallucination and comprehensiveness, to prompt LLMs to verify whether the generated responses align with the facts in the retrieved documents and whether all relevant information has been properly extracted and integrated~\cite{jin2024rag,jacovi2025facts,Rageval2024Zhu}. However, LLM-based judgment models are highly sensitive to the design of evaluation prompts~\cite{zhou2023survival,liu2024aligning}, which may lead to inconsistencies in judgments when different evaluation dimensions are employed.

In this paper, we introduce Judge-Consistency (\method{}), a method that enhances LLM-based judgment models to generate more accurate evaluations for RAG models in a self-improvement framework. \method{} incorporates specialized evaluation dimensions and employs a multiple-choice selection strategy for evaluation modeling. Additionally, \method{} leverages the Direct Preference Optimization (DPO) method~\cite{DPO2023Rafailov} to enhance the judgment capabilities of LLMs, while also implementing a ``judge as a judge'' mechanism during choosing preference pairs of judgments. Specifically, \method{} encourages LLMs to generate judgment results based on various combinations of judgment dimensions, evaluates the quality of these judgments using judge-consistency, and selects the chosen and rejected judgments for DPO training. This approach enhances the performance of the LLM-based judgment model without necessitating distillation from more powerful LLMs~\cite{zhang2025rag}.

Our experiments demonstrate the effectiveness of the \method{} method as a reward model for optimizing the RAG model, resulting in significant improvements over vanilla LLMs. Further analysis shows that \method{} helps LLMs select appropriate evaluation dimensions for assessing response quality by incorporating the agreements and consistency across different evaluation dimensions during optimization. Additionally, compared to other baselines, \method{} exhibits higher judgment consistency with the superior LLM, GLM-4-plus~\cite{du2022glm}, across various RAG evaluation datasets, further highlighting its effectiveness in optimizing LLMs to produce more accurate judgments on diverse RAG tasks.
% Our experiments demonstrate the effectiveness of \method{} method, achieving a improvement compared to baseline model. Further analysis reveals that this improvement stems from the judgment model, which aims to generate more accurate assessments to further optimize RAG models. Specifically, the judgment model is trained using \method{} method, enabling it to self-improve the evaluation performance without distillation from larger scale LLMs, and achieving more efficient gains at a lower cost.



% This process refines the judgment model by selecting the most appropriate evaluation dimensions, ultimately achieving more accurate assessments.


% designs four specific 
% utilizes the LLM itself to 

% error exposure model to generate high-quality reasoning errors that enhance the reasoning capabilities of LLMs. EULER utilizes the LLM itself as the error exposure model and applies the Direct Preference Optimization (DPO) method~\cite{rafailov2023direct} to train the exposure model to assign a higher probability to these LLM produced reasoning errors than the ground truth solutions. Subsequently, EULER employs the optimized error exposure model to generate erroneous reasoning examples for fine-tuning LLMs, while also providing valuable error examples within the prompt during inference. This approach guides LLMs in learning mathematical knowledge through errors and helps them avoid potential mistakes when solving mathematical problems.



% These models typically design prompts to instruct LLMs to determine whether the generated response aligns with the facts in the retrieved document and if all relevant information has been fully extracted and integrated~\cite{jin2024rag,jacovi2025facts,Rageval2024Zhu}. 




% when evaluating a RAG response based on whether it remains faithful to the context versus whether it contains redundant information, the LLM may produce different results.



% When LLMs use different evaluation dimensions to judge RAG responses, the judgments will be inconsistent. For example, for the given RAG responses, LLM judges it from two different evaluation dimensions, whether it is faithful to the context and whether it contains redundant information, respectively, which will produce different results. 

% avoid the significant costs and non-reproducibility of closed-source LLMs, some studies have turned to open-source as an alternative. 


% Besides, some studies have turned to open-source LLMs, such as LLaMA~\cite{touvron2023llama}, as alternatives



% However, calling the APIs of these closed-source models incurs high costs and reduces the reproducibility of evaluation results~\cite{SurveyonLLM-as-a-Judge2024Gu}. As a result, some studies have turned to open-source LLMs, such as LLaMA~\cite{touvron2023llama}, as an alternative. They fine-tune these open-source LLMs using the judgments generated by closed-source LLMs to bridge the gap in evaluation capabilities between them~\cite{wangpandalm,zheng2023judging}, which further extends the application scenarios of LLMs as the judgment models.

% Inspired by the success of LLMs as reward models in various tasks, RAG models also begin applying them to evaluate generated responses.



% To overcome this limitation, recent studies have explored the use of large language models as judges, leveraging their human-like understanding to evaluate generated responses. However, evaluation methods based on closed-source models come with high usage costs and low reproducibility. As a result, some researchers have turned to open-source models, such as LLaMA, as an alternative. They fine-tune these open-source models using evaluation results from closed-source models to bridge the gap in evaluation capabilities between the two.


% evaluating the responses of RAG systems remains a challenge. Automated evaluation metrics, such as exact match, typically focus on character-level similarity between the LLM-generated response and the ground truth, making them ineffective in assessing responses that are semantically correct but do not match the ground truth at the character level.








% Some approaches directly integrate vanilla LLMs into RAG systems to generate responses~\cite{izacard2022few}, but this may expose the model to noisy information, leading to factual errors. 







%challenge 1: evaluation提示设计敏感
%challenge 2: 闭源模型花费大，api背后模型升级导致不可复现
%优点 1: 不需要大量的标记数据
%优点 2: RAG中有很多维度，很敏感，容易出现bias. 我们的方法能缓解不一致性
%不同prompt影响很大，所以做一致性分析, 消除这个bias，用一致性提升
%引用格式再规范一下，缩写

%RAG怎么做，一笔带过
%丢进GPT里过一遍