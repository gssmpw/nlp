\section{Methodology}
In this section, we introduce the Judge-Consistency (\method{}) method, which optimizes Large Language Models (LLMs) as judgment models to evaluate the effectiveness of the Retrieval-Augmented Generation (RAG) system. First, we discuss the evaluation methods used in \method{} (Sec.\ref{sec:3.1}). Next, we present how the \method{} method is utilized to optimize the judgment model, improving its judgment accuracy (Sec.\ref{sec:3.2}). Finally, we apply the \method{} method as the reward model to optimize the RAG system (Sec.~\ref{sec:3.3}). \input{figure/model}

\subsection{Preliminary of RAG Evaluation Methods} \label{sec:3.1}
Recent work, such as LLM-as-a-Judge~\cite{zheng2023judging}, typically regards LLMs as judgment models for rating responses in various NLP tasks. These methods use specially designed prompts to employ LLMs in evaluating generated responses across different dimensions, such as hallucination and accuracy~\cite{li2024llms}. In this section, we describe the evaluation dimensions and modeling methods used in \method{}.

\textbf{Evaluation Dimensions.} Some studies~\cite{Rageval2024Zhu,zhang2025rag} evaluate the generation quality of RAG models based on multiple criteria. \method{} follows these approaches by designing different evaluation prompts for four key dimensions: hallucination, completeness, coherence, and semantic consistency.

\textit{Hallucination.} Hallucination refers to the inclusion of information in the response that contradicts the ground truth. This dimension aims to detect whether the generated responses contain factual errors due to hallucinations~\cite{xu2024hallucination}.

\textit{Completeness.} Completeness evaluates whether the generated responses contain as much relevant information as possible from the ground truth. This dimension primarily focuses on identifying whether the responses omit some key points from the ground truth answers.

\textit{Coherence.} Coherence evaluates whether the responses are logically consistent and whether the language flows fluently between sentences. This dimension is primarily concerned with ensuring that the responses are both coherent and fluent.

\textit{Semantic Consistency.} Semantic consistency checks whether the generated response is semantically aligned with the ground truth answer, rather than simply matching it lexically. This dimension helps avoid misjudging responses that differ from the ground truth in terms of tokens but share the same meaning with ground truth answers.

\textbf{Evaluation Modeling.} For evaluation modeling, our \method{} method uses a multiple-choice selection approach~\cite{SurveyonLLM-as-a-Judge2024Gu}. In this method, LLMs evaluate all candidate responses and choose the best or the worst based on different evaluation dimensions. This facilitates RAG training using the judgment model (Sec.~\ref{sec:3.3}).

Existing methods typically rely on Pointwise Evaluation or Listwise Comparison for candidate response evaluation. Pointwise Evaluation directly prompts LLMs to score each candidate based on predefined evaluation dimensions.
However, this method fails to capture the differences between responses, leading to evaluation bias~\cite{kim2023prometheus,wang2023learning}. 
In contrast, Listwise Comparison prompts LLMs to evaluate an entire list of candidate responses and rank them~\cite{niu2024judgerank,yan2024consolidating}, allowing for a more comprehensive evaluation~\cite{li2024llms}. The \method{} model adopts a multiple-choice selection method, which performs Listwise Comparison to evaluate all candidate responses.



% prompting LLMs to score each candidate responses, or the listwise comparison method~\cite{niu2024judgerank,yan2024consolidating}, prompting LLMs to rank the candidate responses and select the best and worst ones. 


%some studies train judgment models using high-quality human-annotated data to align the judgment model with human judgments~\cite{ouyang2022training,Rewardbanch2024Lambert}. However, this method is both expensive and time-consuming~\cite{zhang2025rag}. Inspired by the previous work~\cite{self-improve2023Li,wangself},
% we propose the \method{} method to prompt the judgment model to sample multiple evaluation responses and synthesize the training data based on the consistency among them, thereby applying preference optimization to train the judgment model $R$. 

\subsection{Training Evaluation Models through Judge-Consistency} \label{sec:3.2}
Although LLMs have demonstrated effectiveness in evaluating their own responses, judgment models may still suffer from issues such as Position Bias, Verbosity Bias, and Evaluation Metric Bias, which can compromise the quality of judgments~\cite{zheng2023judging,chen2024humans,li2024llms}. To address these challenges, we propose the Judge-Consistency method to optimize the judgment model $\mathcal{M}$ based on the consistency of judgments across different evaluation dimensions. This process self-improves the judgment model by selecting more suitable evaluation dimensions, ultimately allowing for more precise assessments.


\textbf{Evaluation of LLM Responses.} To optimize the judgment model, we begin by selecting $m$ LLMs and sampling responses from each using different temperatures. Next, we randomly select one response $y$ from each LLM, forming a response set $Y=\{y_1, \dots, y_m\}$. We then combine the four evaluation dimensions introduced in Sec.~\ref{sec:3.1}, generating $k$ distinct hybrid evaluation aspects:
\begin{equation}\small
\mathcal{I} = \{I_1,...,I_k\},
\end{equation}
where $I_i$ represents a hybrid evaluation aspect, which could be a single evaluation dimension or a combination of multiple dimensions. For each evaluation aspect $I_i \in \mathcal{I}$, we create an evaluation prompt, yielding $k$ distinct prompts:
\begin{equation}\small
\mathcal{P} = \{P^1,...,P^k\},
\end{equation}
where $P^i$ is the $i$-th evaluation prompt. The judgment model $\mathcal{M}$ then generates a judgment result $r_i$ for evaluating LLM-generated responses $Y$, based on the $i$-th evaluation aspect $I_i$:
\begin{equation}\small
r_i = \mathcal{M}(P^i,q,y^*, Y),
\end{equation}
where $y^*$ is the ground truth for question $q$. The judgment result $r_i$ includes both the best ($y^+$) and the worst ($y^-$) selections from the candidate responses $Y = \{y_1, \dots, y_m\}$, along with chain-of-thoughts~\cite{Chain-of-Thought2022Wei} for the judgment. By utilizing all evaluation prompts in $\mathcal{P}$, we obtain $k$ judgment results, denoted as $R = \{r_1, \dots, r_k\}$.


\textbf{Judge Consistency Evaluation.} We follow the previous work~\cite{self-improve2023Li} to introduce a ``Judge as a judge'' approach that evaluates the consistency of judgments across different prompts.

Specifically, after obtaining the judgment results $R = \{r_1, \dots, r_k\}$, we use a text embedding model $\text{Emb}(\cdot)$ to compute the similarity score between the $i$-th judgment $r_i$ and all other judgment results $R$. The average of these similarity scores provides the consistency score $S_i$ for the judgment $r_i$:
\begin{equation}\small\label{eq:score}
S_i = \frac{1}{k} \sum_{j=1}^{k} \cos (\text{Emb}(r_i),\text{Emb}(r_j)).
\end{equation}
Judgments exhibiting higher consistency scores are considered positive results, while those with lower consistency scores are interpreted as negative results, indicating potential judge bias.



\textbf{Judgment Model Optimization.} We then optimize the judgment model $\mathcal{M}$ to better select the appropriate evaluation aspect from the set $\mathcal{I}$ to make more accurate judgments.

To achieve this, we treat the judgment with the highest consistency score as a positive judgment $r^+$, and the judgment with the lowest score as the negative one $r^-$. We collect the instance $(q, y^*, r^+, r^-)$ to form the training dataset $\mathcal{T}$. The judgment model $\mathcal{M}$ then uses prompts $\mathcal{P}$ for evaluating the quality of responses $Y$ and is optimized to assign a higher probability to the positive judgment $r^+$ than to the negative judgment $r^-$. This is accomplished through the Direct Preference Optimization (DPO) method~\cite{DPO2023Rafailov}:
\begin{equation}\small\label{eq:dpo}
\begin{aligned}
 & \mathcal{L}= - \mathbb{E}_{(q, y^*, r^+, r^-) \sim \mathcal{T}} [\log \sigma(\beta \\ &\log \frac{\mathcal{M}(r^+ \mid q, y^*)}{\mathcal{M}^\text{ref}(r^+ \mid q, y^*)} - \beta \log \frac{\mathcal{M}(r^- \mid q, y^*)}{\mathcal{M}^\text{ref} (r^- \mid q, y^*)})],
\end{aligned}
\end{equation}
where $r^+, r^- \in R$, and $\beta$ is a hyperparameter. $\mathcal{M}^\text{ref}$ denotes the reference model, which remains fixed during training. 

These judgment results in $R$ are generated using different evaluation prompts $\mathcal{P}$ that combine various evaluation dimensions. \method{} optimizes the judgment model $\mathcal{M}$ to reproduce the positive judgment $r^+$ that shares the most consistency score with others, making the judgment model $\mathcal{M}$ select more appropriate dimensions to evaluate the response quality of LLMs.









% Then, based on Multiple-Choice Selection method, we formulate a multiple-choice question $Q$ by combining the candidate responses $Y$, the question $q$, and the ground truth $q^*$, which requires the LLM to select the best and worst responses from $Y$:


%use $m$ different LLMs to generate $m$ responses $R=\left\{r_1,...,r_m \right\}$ for the given question $q$. 



% Specifically, we use existing QA datasets to feed queries into existing LLMs to obtain multiple responses. In order to increase the diversity of responses,we selected multiple popular models and set different temperatures for each model to sample. This is defined as:
% \begin{equation}\small
% R\ =\ LLM\left( Q,M,T \right) 
% \end{equation}
% where \(Q\) represents query, \(R=\left\{ r_1,r_2,...,r_m \right\} \) represents the \(m\) responses obtained after sending query to LLMs, \(M=\left\{ m_1,m_2,...,m_n \right\} \) represents the \(n\) LLMs we use to generate multiple responses, \(T=\left\{ t_1,t_{2,},...,t_k \right\} \) represents the k different temperatures set when each model is sampled. The relationship between the parameters is: \(m=n\times k\).



% The query in the original QA dataset is used to synthesize multiple different responses, and the answer in original QA pair can be used as the ground truth \(G\). Then the data now becomes a multi-tuple \(<Q,G,r_1,r_2,...,r_m>\). To simplify the task, we only randomly sample one response from a model. So now the data become \(<Q,G,r_1,r_2,...,r_n>\).

% we use the model which will be fine-tuned to evaluate the quality of the responses itself. Since the dimensional preference required by the model for making fine-grained evaluations is not fixed, in order to simulate this preference, we carefully designed several different prompts. Each prompt is a hybrid evaluation of the four evaluation dimensions.
% \begin{equation}\small
% P=\ Combine\left( Hall,Com,Coh,Sem \right) 
% \end{equation}
% In the above formula, \(P=\left\{ p_1,p_2,...,p_n \right\} \) represents the many prompts obtained by free combination. 
% After that, we use the ground truth \(G\) and query \(Q\) as references, and use the model to be fine-tuned to evaluate responses comprehensively through multiple fine-grained evaluation prompts to obtain multiple evaluations \(E\). In all these hybrid evaluations, some of the dimensions used in this evaluation may be truly needed, thus giving a correct evaluation,while some are not, thus may lead to an incorrect evaluation.


% Our goal is to build a preference dataset, so we need to select the best and worst evaluations from the lots of evaluations \(E\). So we adopted the Judge-Consistency mechanism, A correct evaluation will have a higher semantic similarity with most evaluations,conversely,a worse evaluation will have a low semantic similarity with other evaluations.The semantic similarity is calculated as:
% \begin{equation}\small
% f\left( y,\widehat{y} \right) =Sim\left( Emb\left( y \right) ,Emb\left( \tilde{y} \right) \right) 
% \end{equation}
% where \(Emb(y)\) represents the embedding representation of evaluation \(y\), we employ a lightweight MiniCPM-Embedding model to obtain embedding representation. Then, We use the voting mechanism, which prioritizes evaluations that exhibit higher similarity with others. This process can be described simply as:
% \begin{equation}\small
% s\left( y \right) \approx \frac{1}{N}\sum_{i=1}^n{\left[ f\left( y,\tilde{y_i} \right) \right]}
% \end{equation}
% Here, \(s(y)\) is the score assigned to evaluation \(y\). Therefore, we only need to sort the scores \(s(y)\) obtained by each evaluation, select the ones with the highest score as the positive example of the preference dataset, and select the ones with the lowest score as the negative example. So, the preference training dataset will be used to train our judgment model.


\subsection{Applying Judgment Models to Optimize Retrieval-Augmented Generation Systems}
\label{sec:3.3}
To evaluate the effectiveness of judgment model $\mathcal{M}$, we use it as the reward model and apply DPO to optimize the RAG system~\cite{rag-ddr2024Li}.

For a given query $q$, the current RAG system typically utilizes a dense retriever model to retrieve the Top-$n$ relevant documents $\mathcal{D} = \{d_1, \dots, d_n\}$ from the external knowledge bases. The generation model ($\text{Gen}$) then samples outputs $\Tilde{y}$, either with or without the retrieved documents $\mathcal{D}$:
\begin{equation}\small
\begin{aligned}
    \Tilde{y} &\sim \text{Gen} (\mathcal{D} \oplus q), \\
    \Tilde{y} &\sim \text{Gen} (q),
\end{aligned}
\end{equation}
where $\oplus$ is the concatenation operation. Then, we collect all sampled responses $\Tilde{y}$ in the set $\Tilde{Y}$ and utilize the judgment model $\mathcal{M}$ to generate the judgment result $r_\text{all}$ based on all evaluation dimensions:
\begin{equation}\small
r_\text{all} = \mathcal{M}(P_\text{all}, q,y^*,\Tilde{Y}),
\end{equation}
where $P_\text{all}$ indicates the evaluation prompt that involves all evaluation dimensions. Based on $r_\text{all}$, we select the best response $\Tilde{y}^+$ and worst response $\Tilde{y}^-$ from $\Tilde{Y}$, respectively. This allows us to construct a preference dataset $(q, \mathcal{P}, \Tilde{y}^+, \Tilde{y}^-)$ to optimize the generation model ($\text{Gen}$) via DPO training. If the optimized generation model ($\text{Gen}$) demonstrates improved performance on the downstream RAG tasks, this indicates that the judgment model $\mathcal{M}$ provides more precise judgments, effectively serving as the reward for optimizing the RAG system.



%To further improve the effectiveness of the RAG system $V$, some work uses reinforcement learning methods~\cite{RLRAGforchatbots2024Kulkarni}, such as Direct Preference Optimization~\cite{DPO2023Rafailov} (DPO) to optimize the generation module $G$ in RAG system. In these methods, the judgment model $\mathcal{R}$ plays a crucial role, which evaluates the candidate responses of the RAG system and generates reward signals to optimize the LLM $\mathcal{M}$, guiding its output preference toward to responses with higher reward. However, evaluating the quality of the judgment model in the RAG system remains a challenge~\cite{jin2024rag}. Since the quality of the judgment model directly affects the optimization effectiveness of the RAG system $V$, its quality can be evaluated based on how well it optimizes the RAG system.








% Inspired by~\cite{rag-ddr2024Li}, we use DPO to optimize the LLM in the generation module $G$, evaluating the effectiveness of the judgment model $R$. The generation module $G$ utilizes the query $q$ with retrieved documents $D$ or the query alone as inputs and prompts the LLM for sampling:
% \begin{equation}\small
% \Tilde{y} \sim \mathcal{M} (D \oplus q), \Tilde{y} \sim \mathcal{M} (q),
% \end{equation}
% where $\Tilde{y}$ is sampled output. Then, we utilize the judgment model $\mathcal{R}$ to calculate the reward for the $\Tilde{y}$ and judge the outputs with the highest and lowest rewards, $y^+$ and $y^-$, respectively:
% \begin{equation}\small
% (y^+, y^-) = \mathcal{R}(\Tilde{y}).
% \end{equation}
% The LLM $\mathcal{M}$ in generation module $G$ can be trained using the DPO training loss, guiding the LLM to better balance internal and external knowledge by comparative learning from the positive ($\Tilde{y}_t^+$) and negative ($\Tilde{y}_t^-$) outputs:
% \begin{equation}\small\label{eq:dpo2}
% \begin{aligned}
%  & \mathcal{L}= -\mathbb{E}_{(x, \Tilde{y}^+,\Tilde{y}^-) \sim \mathcal{I}} [\log \sigma (\beta \log \frac{\mathcal{M}(\Tilde{y}_t^+ \mid x)}{\mathcal{M}^\text{ref}(\Tilde{y}^+ \mid x)}  \\ &- \beta \log \frac{\mathcal{M}(\Tilde{y}^- \mid x)}{\mathcal{M}^\text{ref} (\Tilde{y}^- \mid x)})],
% \end{aligned}
% \end{equation}


% where $\beta$ is a hyperparameter. $\mathcal{I}$ is the dataset containing the query $q$ and its corresponding preference data pairs $(\Tilde{y}_t^+, \Tilde{y}_t^-)$. $V_t^\text{ref}$ is the reference model, which is frozen during training. By this method, we can obtain the optimized LLM $\mathcal{M}^*$. Then, according to Eq~\ref{eq:rag}, $\mathcal{M}^*$ can generate a response $y$ to answer the given question $q$ with the retrieved documents $D$. Finally, we use the evaluation metric corresponding to question $q$ to compute an evaluation score $S$ for $y$:
% \begin{equation}\small
% S =\text{Metric}(y,y^*),
% \end{equation}
% where $\text{Metric}$ can be evaluation metrics such as accuracy. $y^*$ is the ground truth answer of the question $q$. Finally, we can evaluate the quality of the judgment model based on the score $S$. If the score $S$ is higher, it indicates that the judgment model $\mathcal{R}$ generates more accurate rewards for the sampled output $\Tilde{y}$ and the quality of the judgment model is higher.

 

% Inspired by~\cite{}, we incorporate the judgment model into the optimization of the RAG system and evaluate the quality of the judgment model based on how well the RAG system is optimized.


% primarily of rewarding in rag system.
% %如何评价专门用于RAG的rm性能
% %我们是为了评价rm才做强化学习
% With the rise of reinforcement learning in LLM training, some studies have recognized the potential of reinforcement learning and have attempted to incorporate it into the RAG system~\cite{RewardRAG2024Thang}. In reinforcement learning within RAG system, the judgment model plays a crucial role by providing feedback to the responses generated by $G$, and we can use the feedback to optimize $G$. A good judgment model should give higher rewards to responses that better align with human preferences. However, it is still a challenge that how to evaluate the quality of the feedback provided by the judgment model. ~\cite{rag-ddr2024Li} propose a new RAG optimization method: Differentiable Data Rewards(DDR), DDR uses a judgment model to provide rewards to the texts generated by $G$, and uses the reward signals to build a new preference dataset to optimize $G$. In DDR, the quality of the judgment model effects the optimization of $G$ directly, thus, we can follow the same approach to evaluate the performance of our judgment model.




% \subsection{Optimizing RAG Models Using Reinforcement Learning}
% \label{sec:Optimizing RAG Models Using Reinforcement Learning}

% RAG is widely applied in a variety of real-world scenarios. However, RAG may not fully meet our actual needs in certain scenarios, so additional training methods are required to enhance the performance of RAG systems.Conventional supervised fine-tuning~\cite{Radit2023Lin} methods play a limited role in improving RAG systems. So, ~\cite{rag-ddr2024Li} proposed a new RAG optimization method: Differentiable Data Rewards(DDR).DDR aims to optimize the modules by aligning preferences with RAG system, ensuring that the RAG system provides a higher quality response. To achieve this goal, DDR utilizes system reward \(r\left( x,\tilde{y} \right) \) to train the target module.Specifically,it instructs module to input \(x\) and sample diverse responses \(\tilde{y}\) through diverse settings.
% Then using raw metric to provide system reward and constructing a preference training dataset.
% \begin{equation}\small
% r\left( x,\tilde{y} \right) =Metric\left( x,\tilde{y} \right) 
% \end{equation}
% Finally, this module is trained using the DPO to improve performance.
% The whole process can be iterated multiple times to greatly improve the performance of the RAG system.

% However, using raw metric to evaluate the outputs of RAG systems and serve as reward signals for training has its limitations.So LLM-as-a-judge has become a new evaluation approach. ~\cite{Rageval2024Zhu} proposes a relatively complete RAG evaluation framework. For the generator module in the RAG system, they proposed fine-grained evaluation dimensions that can comprehensively evaluate the quality of the text generated by the generator. Firstly, they use GPT-4o to extract 3-5 keywords from the generated text and the ground truth respectively, and then define three evaluation metrics which utilizes the keywords to judge whether the generated responses are informative, accurate, and relevant.

% Therefore,it is a good choice to optimize RAG models using LLMs as judgment models which have better evaluation capabilities. Specifically,We train a more powerful judge model as a judgment model by aligning the model's dimensional preferences during fine-grained evaluation. Then system reward in RAG system can be provided this judge model:
% \begin{equation}\small
% r\left( x,\tilde{y} \right) =Judge\left( x,\tilde{y} \right) 
% \end{equation}
% Finally, we can use this better system reward to obtain better RAG models.

