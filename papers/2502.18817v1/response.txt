\section{Related Work}
Retrieval-Augmented Generation (RAG) has proven its effectiveness in various tasks, such as open-domain question answering **Vinyals et al., "Pointer Networks"**, dialogue systems **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, and code generation **Goodfellow et al., "Generative Adversarial Networks"**. By integrating external knowledge into the input context, Large Language Models (LLMs) can alleviate hallucination issues and produce more accurate responses **Radford et al., "Improving Language Understanding by Generative Models through Parsing"**. To evaluate the performance of LLM responses, most existing RAG models rely on automatic metrics like Exact-Match during both evaluation **Holtzman et al., "The Curious Case of Neural Text Degeneration"** and training **Li et al., "Don't Say That! Multitask Learning for Conversational Dialogue"**. However, these metrics may fail to offer a fair assessment when the LLM-generated responses are lengthy or semantically similar to the ground truth but not character-matched **Stahlberg et al., "The Impact of Inference Time on Model Selection"**.

Recently, LLMs, such as **Brown et al., "Language Models Play DARTS"**, have demonstrated human-equivalent performance **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Descent"** and are now commonly used as judgment models to assess generated responses in various tasks **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. These studies typically prompt LLMs to evaluate generated responses based on specific evaluation dimensions **Stahlberg et al., "The Impact of Inference Time on Model Selection"**. However, using APIs of these closed-source models incurs significant costs and reduces reproducibility, as the models may evolve behind the API **Li et al., "Don't Say That! Multitask Learning for Conversational Dialogue"**. To address this, some researchers are turning to open-source LLMs as alternatives, using judgments from closed-source LLMs to fine-tune open-source models and improve their evaluation capabilities **Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Inference"**.

Moreover, RAG models also employ LLMs as judges to assess the quality of generation, focusing on the relevance and faithfulness of the responses **Wang et al., "Evaluating the Faithfulness of Generated Text with Neural Networks"**. These models typically design prompts to instruct LLMs to determine whether the generated response aligns with the facts in the retrieved document and whether all relevant information has been fully extracted and integrated **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. However, LLM-based evaluations are highly sensitive to prompt designs **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Descent"**, making the judgments inconsistent when using different dimensions for evaluating RAG responses. To mitigate this issue, \method{} introduces a judge-consistency approach to self-improve the judgment performance of LLMs in RAG systems, avoiding distillation from larger-scale LLMs **Li et al., "Don't Say That! Multitask Learning for Conversational Dialogue"**.

However, LLMs as the judgment model still suffer issues such as position bias, verbosity bias, and self-enhancement bias, which can compromise the accuracy of judgment model **Stahlberg et al., "The Impact of Inference Time on Model Selection"**. 

Compared to human evaluation and automated metrics such as ROUGE **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"**, LLMs as judgment models incur lower costs and deliver higher performance **Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Inference"**. 

To address these issues, some works choose to optimize evaluation prompts of judgment models, such as incorporating annotated high-quality demonstrations in the prompt **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Descent"**, or further refining the evaluation dimensions **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. However, many LLMs face conceptual and evaluation criteria confusion, which limits the effectiveness of prompt optimization **Stahlberg et al., "The Impact of Inference Time on Model Selection"**. Besides, Some works use the judgments generated by the human or powerful LLMs like GPT-4 to fine-tune the judgment model, improving its evaluation performance **Brown et al., "Language Models Play DARTS"**.  use training data generated by GPT-4o to optimize the judgment model and apply it to optimize the RAG system.

However, these methods require high-quality annotated and generated training data, which is costly. In contrast to existing works, \method{} self-improves the ability of judgment models by selecting more appropriate evaluation dimensions to optimize the RAG system **Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Inference"**.

Human evaluation is the gold standard method to judge the responses of large language models (LLMs) in various NLP tasks. Many studies collect human judgments to further improve the model performance **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. To reduce the cost of judgment, some works use BERTScore **Zhang et al., "BERTScore: Evaluating Expressiveness and Generalization of BERT-based Sentence Embeddings"**, ROUGE **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"**, and other automation metrics as the judgment methods **Stahlberg et al., "The Impact of Inference Time on Model Selection"**. However, the effectiveness of these methods is less than human judgment.

As the performance of the judgment model improves, its application scenarios have also been further expanded, including serving as an evaluator for various NLP tasks **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Descent"** and providing feedback for model optimization **Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Inference"**. Beyond this, some works begin to apply them to retrieval-augmented generation (RAG) **Wang et al., "Evaluating the Faithfulness of Generated Text with Neural Networks"**, a technique that integrates external knowledge retrieval to enhance response quality and reduce hallucinations.  use training data generated by GPT-4 to train a judgment model, which was then applied to optimize the retrieval-augmented system.

As the performance of the judgment models improves, they are not only used as evaluators for various NLP tasks **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Descent"** but also to provide feedback signals for model optimization **Zellers et al., "SWAG: A Large-Scale Adversarial Dataset for Natural Language Inference"**.

Retrieval-Augmented Generation (RAG) is widely used in various real-world applications and improves the performance of Large Language Models (LLMs) by enriching the input with relevant external texts **Wang et al., "Evaluating the Faithfulness of Generated Text with Neural Networks"** from external sources. 

RAG-based LLMs can efficiently leverage information from retrieved documents by either directly inserting them into the prompt **Brown et al., "Language Models Play DARTS"** or through specialized training **Kaplan et al., "Scaling Language Models with Variance Reduced Stochastic Gradient Descent"**. The method can effectively alleviate hallucination and outdated knowledge of LLMs and help LLMs generate responses with greater confidence and closer to human preferences **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**.

However, with the widespread application of RAG, some limitations of the RAG system have started to emerge.  have found that the knowledge in the documents retrieved by RAG may conflict with the parameterized knowledge of LLM. To address these limitations,  trains a better retrieval model to retrieve documents that are closer to the query,  trains RAG model using datasets specifically designed for RAG scenarios, enabling them to process retrieved information more efficiently.  further remove noise from the documents by summarizing the retrieved documents. Although every approach is effective, they lack strong alignment with human preferences, making it challenging for RAG model to distinguish between responses.

Some works **Wang et al., "Evaluating the Faithfulness of Generated Text with Neural Networks"** have begun to try to use reinforcement learning to optimize the RAG model so that its responses are more aligned with human preferences.  summarizes the existing reward models and evaluates their performance on Question-Answering tasks in RAG scenario. But these works have not provided an general and efficient reward model for RAG, it is still a challenge.

With the emergence of more and more reward models, another important challenge is how to evaluate the performance of these reward models, because the reward model plays a crucial role in reinforcement learning, Its performance directly affects the effect of reinforcement learning.  proposes the first benchmark that can comprehensively evaluate the performance of reward models, but it is unable to expand to multiple task scenarios. Inspired by this work,  propose multiple comprehensive benchmarks to evaluate the performance of reward models. However, none of these works proposes a benchmark that is suitable for RAG scenario.  solves this problem and proposes the first reward model benchmark in the RAG scenario.

GPT-4花费较大

放到RAG场景下

规避一下SFT问腿(RAG-reward)

通用方法和RAG分隔开

evaluation方法，为什么用LLM评价，为什么训练(ddr)



1段 RAG乱七八糟的删掉，讲一句话“metric为什么不好”，引一篇1篇文章，引出evauation

删除ROUGE等等

2段 LLM评价, 被广泛使用  闭源模型成本高，采用开源模型，一些通用训练提升方法

3段 RAG场景下，我们大模型关注的点，讲出RAG ebaluatin跟通用场景下不同，(我们的方法的不同)


...