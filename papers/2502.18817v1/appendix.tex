\section{Appendix}
\label{sec:appendix}

\input{table/reward_and_traindataset}


\subsection{License}
We show the licenses of the datasets that we use. ELI5 and Yahoo!QA do not report the license of the dataset in the paper or a repository. ELI5 shows its terms of use at website\footnote{\url{https://facebookresearch.github.io/ELI5/}}. Yahoo!QA shows its terms of use at website\footnote{\url{https://tensorflow.google.cn/datasets/community_catalog/huggingface/yahoo_answers_qa}}. We summarize the licenses of the remaining datasets as follows:

All of these licenses and agreements allow their data for academic use: NQ (CC BY-SA 3.0 license); FiQA (CC BY-SA 4.0 license); ScienceQA, CNNSum, MuSiQue, Web Questions and HotpotQA (CC BY 4.0 license); WoW (CC BY-NC license); FinQA, MeDiaQA, PubMedQA, PopQA, MARCOQA, WiKiQA, and StrategyQA (MIT license); NarrativeQA, AQUA-RAT, TriviaQA and ASQA (Apache 2.0 license); ECQA (CDLA-Sharing 1.0 license); 


% nq cc-by-sa-3.0
% Finqa MIT
% Fiqa cc-by-sa-4.0
% mediaqa MIT
% pubmedqa MIT
% sciqa cc-by-4.0
% Eli5 uunknown
% narrativeqa Apache-2.0
% popqa MIT
% CNNsum cc-by-4.0
% Musique cc-by-4.0
% commonsenseqa MIT
% ECQA CDLA-sharing-1.0
% marcoqa MIT
% webquestions cc-by-4.0
% wikiqa MIT
% yahoo unknown
% gsm8k MIT
% mathqa Apache-2.0
% strategyqa MIT
% aquarat Apache-2.0
% triviaqa Apache-2.0
% asqa  Apache-2.0
% hotpotqa cc-by-4.0
% trex cc-by-sa-4.0
% wow cc-by-nc
\input{figure/consistency_distribution}
% \input{figure/human_consistency}
\input{figure/human_consistency}
\subsection{Judgment Consistency Score Distribution of Vanilla LLMs and \method{}}
\label{sec:consistency score distribution}
In this section, we further analyze the consistency score distributions of judgments generated by the vanilla LLM and \method{} based on different hybrid evaluation aspects. To construct a dataset for this analysis, we randomly sample 1,000 queries from both HotpotQA and TriviaQA. We employ both vanilla LLM and \method{} to generate judgments for each query, using different hybrid evaluation aspects. Then, we refer to Eq.~\ref{eq:score} to use MiniCPM-Embedding to compute the consistency scores of these judgments.

As shown in Figure~\ref{fig:consistency:distribution}, the results demonstrate that \method{} not only achieves higher consistency scores but also exhibits a more concentrated distribution of consistency scores compared to the vanilla LLM. Notably, \method{} consistently maintains its advantage across LLMs of different scales, highlighting its robust generalization ability.


\subsection{Judge Agreement Evaluation between Human and \method{}}
In this section, we further analyze the agreement between different judgment models and human evaluators.

First, we randomly sample 200 queries from the RAG training dataset to assess the agreement in judgment, aiming to evaluate the effectiveness of \method{} in assisting the RAG training process. We then collect responses from various models and ask both judgment models and human evaluators to assess these responses. As shown in Figure~\ref{fig:human_consist}, we use four different judgment methods: Human, GLM-4-plus~\cite{du2022glm}, \method{}, and Raw Metrics, to select the best response for each query. For human annotators, we also provide them with the instructions shown in Table~\ref{table1:eightprompt} to guide their evaluation.

The Raw Metric exhibits the lowest agreement with the other judgment methods, underscoring that character-matching metrics are inadequate for fairly evaluating generated responses. In contrast, \method{} shows higher agreement with both humans and the superior LLM, GLM-4-plus. Furthermore, the agreement between \method{} and humans is comparable to that between GLM-4-plus and humans. This indicates that \method{} has the ability to produce judgments that are more consistent with human evaluators, making it an effective tool for constructing high-quality preference pairs during training RAG models~\cite{rag-ddr2024Li}.

\input{table/hyb_dimensions}

\subsection{More Experimental Details}
\label{app:dataset details}
In this section, we introduce more details of our experiments. We first show the details of training \method{}. Then, we describe the details of applying \method{} to optimize the RAG model. 

\textbf{\method{} Training.} To construct the \method{} Training dataset, as shown in Table~\ref{table1:reward_and_traindataset}, we collect multiple queries from these datasets and use four different LLMs, MiniCPM-2.4B~\cite{minicpm-2b2024Hu}, MiniCPM3-4B~\cite{minicpm-2b2024Hu}, Llama3-8B-Instruct~\cite{touvron2023llama} and Qwen1.5-14B-Chat~\cite{qwen2.5-14b2023Bai} to generate responses for each query. Specifically, each LLM generates three responses using three different temperatures, 0.5, 0.6, and 0.7 and we randomly sample one response from them, resulting in a total of four responses for each query. Furthermore, we combine the four different evaluation dimensions, resulting in eight hybrid evaluation aspects. As shown in Table~\ref{table1:hyb_dimensions}, these include four individual evaluation dimensions, two combinations of two dimensions, one combination of three dimensions, and one that integrates all evaluation dimensions. For the hybrid evaluation aspects combining two evaluation dimensions, one integrates Coherence and Semantic Consistency, focusing on evaluating the logical coherence and fluency of the response, while another combines Hallucination and Completeness, emphasizing whether the response is factually accurate and complete. For the hybrid aspects involving three dimensions, we exclude the Coherence, as it is less relevant in the RAG scenario than the other evaluation dimensions.


\input{table/eight}
\textbf{RAG Training.} To construct the DPO training data to optimize the RAG models, we employ \method{} to select the best and worst responses from the sampling responses generated by RAG models. As shown in Table~\ref{table1:reward_and_traindataset}, we collect multiple queries from these datasets and use bge-large~\cite{bge_embedding} to retriever top-$5$ relevant documents for each query. To enhance sampling diversity, RAG models generate responses under two different input conditions: the query alone (without RAG) and the query with the top-$5$ retrieved documents. RAG model samples two responses for each different input, yielding a total of four sampled responses. After that, we use the judgment model model to select the best and worst responses from them to construct the DPO training dataset.



\input{figure/RAGtrain_evaluation_prompt}
\input{figure/RAGinference_prompt}
\input{figure/overall_eval_prompt}


\subsection{Prompt Templates Used in Experiment}
\label{sec:prompt details}
In this section, we present the prompt templates used in our experiment. 

First, we present the prompt designed for \method{} to evaluate the responses generated by RAG models, as shown in Table~\ref{table1:eightprompt}. Next, as illustrated in Figures~\ref{fig:RAGtrain_and_evaluation_prompt} and \ref{fig:RAGevaluation_prompt}, we introduce the prompts used for training and evaluating the RAG models. These prompt templates are based on RA-DIT~\cite{Radit2023Lin} and RAG-DDR~\cite{rag-ddr2024Li}, specifically tailored to different LLMs and tasks to facilitate the generation of more effective responses. Additionally, the prompt designed for evaluating the performance of RAG models across the MARCO QA and WoW datasets using the GLM-4-plus model is displayed in Figure~\ref{fig:overall_eval_prompt}. Finally, Figure~\ref{fig:GLM_eval_prompt} presents the prompt used to instruct the GLM-4-plus to compare the judge quality between different judgment models.

\input{figure/LLM_evaluation_prompt}





























%从两个不同维度区分大一点，hallu和comp判断有没有幻觉
%三个的是在RAG场景下
%cohe和sem从文本的角度是否流利，想不想人说的，回答精准性，内部逻辑性,形式层面
%剩下的两个有没有幻觉，语义上