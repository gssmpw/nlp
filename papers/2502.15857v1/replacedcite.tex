\section{Related Work}
\subsection{Differential Privacy}
\label{sec:dp}
In this section, We briefly revisit two important definitions of differential privacy: $\epsilon$-Differential Privacy and Exponential Mechanism (EM).

\textbf{$\epsilon$-Differential Privacy (DP)}.
\textit{The Definition of $\epsilon$-Differential Privacy (DP)}____. 
A randomized algorithm $M: D \to S $ is $\epsilon$-Differential Privacy if for any two neighboring datasets $D_1, D_2 \in D$ that differ exactly in a single data sample, and for any output $O \subseteq S$:

\begin{equation}\label{eq:dp-0}
\begin{aligned}
     P_r[M(D_1) \in O] \le e^{\epsilon} P_r[M(D_2) \in O]
\end{aligned}
\end{equation}
where $\epsilon$ is a privacy parameter. Smaller values of $\epsilon$ imply stronger privacy guarantees.


\textbf{Exponential Mechanism}.
\textit{The Definition of Exponential Mechanism}____. For a given scoring function $u: X\times Y \to  R$, a randomized mechanism $M(X, u, Y)$ is $\epsilon$-DP compliant if it satisfies: 

\begin{equation}\label{eq:dp-1}
\begin{aligned}
     P_r[y|x]\propto exp(\frac{\epsilon \cdot u(x,y) }{2\bigtriangleup u }  )
\end{aligned}
\end{equation}
where the sensitivity $\bigtriangleup u$ is defined as:
\begin{equation}\label{eq:dp-2}
\begin{aligned}
    \bigtriangleup u = \max_{x,x^{'} \in X, y\in Y} |u(x,y) - u(x^{'},y)| 
\end{aligned}
\end{equation}


\subsection{Differential Privacy Synthetic Data}

A practical approach to generating private synthetic data involves training a language model, such as LLaMa2-7B____, on private data using DP through DP-SGD____. Subsequently, the DP model is sampled repeatedly to produce synthetic data ____. 
% These DP-generated synthetic texts can be utilized in any number of downstream tasks without additional privacy loss, as they do not require private training.
Research conducted by____ demonstrates that training downstream models on DP synthetic data achieves performance comparable to training directly on real data with DP, thereby underscoring the high quality of the synthetic data.

However, a significant challenge arises because cutting-edge LLMs, like GPT-4, do not offer model weights, making DP fine-tuning impractical. Even for open-source LLMs, such as LLaMa3-70B____, the process is resource-intensive. 
Meanwhile, these DP fine-tuning methods inherently rely on a trusted server to gather data from data owners for model training____, significantly limiting their applicability in scenarios where such trusted servers are not available, as is the case in our research context.
In the context of this work, we operate within a client-server architecture where fine-tuning the LLM on the server is not an option. 



\subsection{Model Pruning}

Model pruning, initially proposed by____ and subsequently enhanced by____, stands as a resilient and efficient strategy for mitigating model redundancy and attaining compression. This methodology branches into two primary techniques: \textit{unstructured pruning and structured pruning}.

Unstructured pruning ____ can obtain highly compressed models by directly pruning neurons, disregarding the model's internal architecture, which also causes unstructured sparsity and hard deployment.
A more pragmatic and structured option is structured pruning. \textit{Structured pruning} targets organized patterns for removal, encompassing entire layers____, attention heads within Multi-Head Attention (MHA) mechanisms____, hidden sizes in Feedforward Neural Networks (FFN)____, as well as hybrid configurations____. 
In recent times, there has been a surge in structured pruning research tailored specifically for LLMs. For example, ShortGPT____, LaCo____, and Shortened LLaMa____ concentrate solely on pruning depth (i.e., layer-wise). LLM-Pruner____ eliminates coupled structures in relation to network width while preserving the layer count. Sheared-LLaMA____ introduces a mask learning phase that is designed to pinpoint prunable components in both network width and depth.
Our work falls in the category of structured pruning of LLMs.