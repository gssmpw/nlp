\section{Related Work}
\subsection{Differential Privacy}
\label{sec:dp}
In this section, We briefly revisit two important definitions of differential privacy: $\epsilon$-Differential Privacy and Exponential Mechanism (EM).

\textbf{$\epsilon$-Differential Privacy (DP)}.
\textit{The Definition of $\epsilon$-Differential Privacy (DP)}~\cite{dwork2006differential}. 
A randomized algorithm $M: D \to S $ is $\epsilon$-Differential Privacy if for any two neighboring datasets $D_1, D_2 \in D$ that differ exactly in a single data sample, and for any output $O \subseteq S$:

\begin{equation}\label{eq:dp-0}
\begin{aligned}
     P_r[M(D_1) \in O] \le e^{\epsilon} P_r[M(D_2) \in O]
\end{aligned}
\end{equation}
where $\epsilon$ is a privacy parameter. Smaller values of $\epsilon$ imply stronger privacy guarantees.


\textbf{Exponential Mechanism}.
\textit{The Definition of Exponential Mechanism}~\cite{mcsherry2007mechanism,tong2023privinfer}. For a given scoring function $u: X\times Y \to  R$, a randomized mechanism $M(X, u, Y)$ is $\epsilon$-DP compliant if it satisfies: 

\begin{equation}\label{eq:dp-1}
\begin{aligned}
     P_r[y|x]\propto exp(\frac{\epsilon \cdot u(x,y) }{2\bigtriangleup u }  )
\end{aligned}
\end{equation}
where the sensitivity $\bigtriangleup u$ is defined as:
\begin{equation}\label{eq:dp-2}
\begin{aligned}
    \bigtriangleup u = \max_{x,x^{'} \in X, y\in Y} |u(x,y) - u(x^{'},y)| 
\end{aligned}
\end{equation}


\subsection{Differential Privacy Synthetic Data}

A practical approach to generating private synthetic data involves training a language model, such as LLaMa2-7B~\cite{touvron2023llama}, on private data using DP through DP-SGD~\cite{song2013stochastic,bassily2014private,abadi2016deep}. Subsequently, the DP model is sampled repeatedly to produce synthetic data ~\cite{mattern2022differentially,yue2022synthetic,kurakin2023harnessing}. 
% These DP-generated synthetic texts can be utilized in any number of downstream tasks without additional privacy loss, as they do not require private training.
Research conducted by~\cite{mattern2022differentially,yue2022synthetic,kurakin2023harnessing} demonstrates that training downstream models on DP synthetic data achieves performance comparable to training directly on real data with DP, thereby underscoring the high quality of the synthetic data.

However, a significant challenge arises because cutting-edge LLMs, like GPT-4, do not offer model weights, making DP fine-tuning impractical. Even for open-source LLMs, such as LLaMa3-70B~\cite{touvron2023llama}, the process is resource-intensive. 
Meanwhile, these DP fine-tuning methods inherently rely on a trusted server to gather data from data owners for model training~\cite{chen-etal-2023-customized}, significantly limiting their applicability in scenarios where such trusted servers are not available, as is the case in our research context.
In the context of this work, we operate within a client-server architecture where fine-tuning the LLM on the server is not an option. 



\subsection{Model Pruning}

Model pruning, initially proposed by~\cite{lecun1989optimal} and subsequently enhanced by~\cite{han2015learning}, stands as a resilient and efficient strategy for mitigating model redundancy and attaining compression. This methodology branches into two primary techniques: \textit{unstructured pruning and structured pruning}.

Unstructured pruning ~\cite{dong2017learning,lee2018snip,wang2020picking,sun2023simple,frantar2023sparsegpt} can obtain highly compressed models by directly pruning neurons, disregarding the model's internal architecture, which also causes unstructured sparsity and hard deployment.
A more pragmatic and structured option is structured pruning. \textit{Structured pruning} targets organized patterns for removal, encompassing entire layers~\cite{jha2023train}, attention heads within Multi-Head Attention (MHA) mechanisms~\cite{michel2019sixteen}, hidden sizes in Feedforward Neural Networks (FFN)~\cite{nova2023gradient}, as well as hybrid configurations~\cite{kurtic2024ziplm}. 
In recent times, there has been a surge in structured pruning research tailored specifically for LLMs. For example, ShortGPT~\cite{men2024shortgpt}, LaCo~\cite{yang2024laco}, and Shortened LLaMa~\cite{kim2024shortened} concentrate solely on pruning depth (i.e., layer-wise). LLM-Pruner~\cite{ma2023llm} eliminates coupled structures in relation to network width while preserving the layer count. Sheared-LLaMA~\cite{xia2023sheared} introduces a mask learning phase that is designed to pinpoint prunable components in both network width and depth.
Our work falls in the category of structured pruning of LLMs.