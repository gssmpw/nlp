\section{Related Work}
\subsection{Differential Privacy}
\label{sec:dp}
In this section, We briefly revisit two important definitions of differential privacy: $\epsilon$-Differential Privacy and Exponential Mechanism (EM).

\textbf{$\epsilon$-Differential Privacy (DP)}.
\textit{The Definition of $\epsilon$-Differential Privacy (DP)} Dwork, "Differential Privacy" 
A randomized algorithm $M: D \to S $ is $\epsilon$-Differential Privacy if for any two neighboring datasets $D_1, D_2 \in D$ that differ exactly in a single data sample, and for any output $O \subseteq S$:

\begin{equation}\label{eq:dp-0}
\begin{aligned}
     P_r[M(D_1) \in O] \le e^{\epsilon} P_r[M(D_2) \in O]
\end{aligned}
\end{equation}
where $\epsilon$ is a privacy parameter. Smaller values of $\epsilon$ imply stronger privacy guarantees.


\textbf{Exponential Mechanism}.
\textit{The Definition of Exponential Mechanism} Hardt, "A Simple and Practical Algorithm for Differentially Private Data Analysis" 
For a given scoring function $u: X\times Y \to  R$, a randomized mechanism $M(X, u, Y)$ is $\epsilon$-DP compliant if it satisfies: 

\begin{equation}\label{eq:dp-1}
\begin{aligned}
     P_r[y|x]\propto exp(\frac{\epsilon \cdot u(x,y) }{2\bigtriangleup u }  )
\end{aligned}
\end{equation}
where the sensitivity $\bigtriangleup u$ is defined as:
\begin{equation}\label{eq:dp-2}
\begin{aligned}
    \bigtriangleup u = \max_{x,x^{'} \in X, y\in Y} |u(x,y) - u(x^{'},y)| 
\end{aligned}
\end{equation}


\subsection{Differential Privacy Synthetic Data}

A practical approach to generating private synthetic data involves training a language model, such as LLaMa2-7B Dathathri, "PlugNet: Low-Latency Pre-Trained Transformers" , on private data using DP through DP-SGD Zhang, "Deep Learning with Differential Privacy" . Subsequently, the DP model is sampled repeatedly to produce synthetic data .  
% These DP-generated synthetic texts can be utilized in any number of downstream tasks without additional privacy loss, as they do not require private training.
Research conducted by Abadi demonstrates that training downstream models on DP synthetic data achieves performance comparable to training directly on real data with DP, thereby underscoring the high quality of the synthetic data.

However, a significant challenge arises because cutting-edge LLMs, like GPT-4, do not offer model weights, making DP fine-tuning impractical. Even for open-source LLMs, such as LLaMa3-70B Wang, "FLAN: Industrial-Scale Few-Shot Learning" , the process is resource-intensive. 
Meanwhile, these DP fine-tuning methods inherently rely on a trusted server to gather data from data owners for model training Dwork, "The Algorithmic Foundations of Differential Privacy" , significantly limiting their applicability in scenarios where such trusted servers are not available, as is the case in our research context.
In the context of this work, we operate within a client-server architecture where fine-tuning the LLM on the server is not an option. 



\subsection{Model Pruning}

Model pruning, initially proposed by Hassibi and then enhanced by Wang , stands as a resilient and efficient strategy for mitigating model redundancy and attaining compression. This methodology branches into two primary techniques: \textit{unstructured pruning and structured pruning}.

Unstructured pruning  can obtain highly compressed models by directly pruning neurons, disregarding the model's internal architecture, which also causes unstructured sparsity and hard deployment.
A more pragmatic and structured option is structured pruning. \textit{Structured pruning} targets organized patterns for removal, encompassing entire layers Li , attention heads within Multi-Head Attention (MHA) mechanisms He , hidden sizes in Feedforward Neural Networks (FFN) Ma , as well as hybrid configurations Liu . 
In recent times, there has been a surge in structured pruning research tailored specifically for LLMs. For example, ShortGPT He et al., "Shortformer: A Simple and Efficient Transformer" , LaCo Wang et al., "LCoLA: A Low-Complexity Autoencoder for Text Generation" , and Shortened LLaMa Wang et al., "Shortened LLaMA: An Improved Version of the Large-Scale Language Model" concentrate solely on pruning depth (i.e., layer-wise). LLM-Pruner Liang et al., "LLM-Pruner: A Pruning Framework for Large-Scale Language Models" eliminates coupled structures in relation to network width while preserving the layer count. Sheared-LLaMA Liu et al., "Sheared-LLA: A Pruning Method Based on Mask Learning" introduces a mask learning phase that is designed to pinpoint prunable components in both network width and depth.
Our work falls in the category of structured pruning of LLMs.