@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th annual symposium on foundations of computer science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}

@inproceedings{chen-etal-2023-customized,
    title = "A Customized Text Sanitization Mechanism with Differential Privacy",
    author = "Chen, Sai  and
      Mo, Fengran  and
      Wang, Yanhao  and
      Chen, Cen  and
      Nie, Jian-Yun  and
      Wang, Chengyu  and
      Cui, Jamie",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.355/",
    doi = "10.18653/v1/2023.findings-acl.355",
    pages = "5747--5758",
    abstract = "As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on a relaxed notion of metric local differential privacy (MLDP) do not apply to non-metric semantic similarity measures and cannot achieve good privacy-utility trade-offs. To address these limitations, we propose a novel Customized Text sanitization (CusText) mechanism based on the original $\epsilon$-differential privacy (DP) definition, which is compatible with any similarity measure.Moreover, CusText assigns each input token a customized output set to provide more advanced privacy protection at the token level.Extensive experiments on several benchmark datasets show that CusText achieves a better trade-off between privacy and utility than existing mechanisms.The code is available at \url{https://github.com/sai4july/CusText}."
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal=NIPS,
  volume={30},
  year={2017}
}

@inproceedings{dwork2006differential,
  title={Differential privacy},
  author={Dwork, Cynthia},
  booktitle={International colloquium on automata, languages, and programming},
  pages={1--12},
  year={2006},
  organization={Springer}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{jha2023train,
  title={How to train your (compressed) large language model},
  author={Jha, Ananya Harsh and Sherborne, Tom and Walsh, Evan Pete and Groeneveld, Dirk and Strubell, Emma and Beltagy, Iz},
  journal={arXiv preprint arXiv:2305.14864},
  year={2023}
}

@article{kim2024shortened,
  title={Shortened llama: A simple depth pruning for large language models},
  author={Kim, Bo-Kyeong and Kim, Geonmin and Kim, Tae-Ho and Castells, Thibault and Choi, Shinkook and Shin, Junho and Song, Hyoung-Kyu},
  journal={arXiv preprint arXiv:2402.02834},
  year={2024}
}

@article{kurakin2023harnessing,
  title={Harnessing large-language models to generate private synthetic text},
  author={Kurakin, Alexey and Ponomareva, Natalia and Syed, Umar and MacDermed, Liam and Terzis, Andreas},
  journal={arXiv preprint arXiv:2306.01684},
  year={2023}
}

@article{kurtic2024ziplm,
  title={Ziplm: Inference-aware structured pruning of language models},
  author={Kurti{\'c}, Eldar and Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{mattern2022differentially,
  title={Differentially private language models for secure data sharing},
  author={Mattern, Justus and Jin, Zhijing and Weggenmann, Benjamin and Schoelkopf, Bernhard and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.13918},
  year={2022}
}

@inproceedings{mcsherry2007mechanism,
  title={Mechanism design via differential privacy},
  author={McSherry, Frank and Talwar, Kunal},
  booktitle={48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)},
  pages={94--103},
  year={2007},
  organization={IEEE}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{nova2023gradient,
  title={Gradient-free structured pruning with unlabeled data},
  author={Nova, Azade and Dai, Hanjun and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={26326--26341},
  year={2023},
  organization={PMLR}
}

@inproceedings{song2013stochastic,
  title={Stochastic gradient descent with differentially private updates},
  author={Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D},
  booktitle={2013 IEEE global conference on signal and information processing},
  pages={245--248},
  year={2013},
  organization={IEEE}
}

@article{sun2023simple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{tong2023privinfer,
  title={Privinfer: Privacy-preserving inference for black-box large language model},
  author={Tong, Meng and Chen, Kejiang and Qi, Yuang and Zhang, Jie and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2310.12214},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{yang2024laco,
  title={Laco: Large language model pruning via layer collapse},
  author={Yang, Yifei and Cao, Zouying and Zhao, Hai},
  journal={arXiv preprint arXiv:2402.11187},
  year={2024}
}

@article{yue2022synthetic,
  title={Synthetic text generation with differential privacy: A simple and practical recipe},
  author={Yue, Xiang and Inan, Huseyin A and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Shajari, Hoda and Sun, Huan and Levitan, David and Sim, Robert},
  journal={arXiv preprint arXiv:2210.14348},
  year={2022}
}

