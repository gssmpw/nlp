
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{yang2019federated,
  title={Federated learning},
  author={Yang, Qiang and Liu, Yang and Cheng, Yong and Kang, Yan and Chen, Tianjian and Yu, Han},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={13},
  number={3},
  pages={1--207},
  year={2019},
  publisher={Morgan \& Claypool Publishers}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{hardy2017private,
  title={Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption},
  author={Hardy, Stephen and Henecka, Wilko and Ivey-Law, Hamish and Nock, Richard and Patrini, Giorgio and Smith, Guillaume and Thorne, Brian},
  journal={arXiv preprint arXiv:1711.10677},
  year={2017}
}

@inproceedings{chen2021homomorphic,
  title={When homomorphic encryption marries secret sharing: Secure large-scale sparse logistic regression and applications in risk control},
  author={Chen, Chaochao and Zhou, Jun and Wang, Li and Wu, Xibin and Fang, Wenjing and Tan, Jin and Wang, Lei and Liu, Alex X and Wang, Hao and Hong, Cheng},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2652--2662},
  year={2021}
}

@article{yang2019quasi,
  title={A quasi-newton method based vertical federated learning framework for logistic regression},
  author={Yang, Kai and Fan, Tao and Chen, Tianjian and Shi, Yuanming and Yang, Qiang},
  journal={arXiv preprint arXiv:1912.00513},
  year={2019}
}

@article{cheng2021secureboost,
  title={Secureboost: A lossless federated learning framework},
  author={Cheng, Kewei and Fan, Tao and Jin, Yilun and Liu, Yang and Chen, Tianjian and Papadopoulos, Dimitrios and Yang, Qiang},
  journal={IEEE Intelligent Systems},
  volume={36},
  number={6},
  pages={87--98},
  year={2021},
  publisher={IEEE}
}

@article{chen2021secureboost+,
  title={SecureBoost+: A High Performance Gradient Boosting Tree Framework for Large Scale Vertical Federated Learning},
  author={Chen, Weijing and Ma, Guoqiang and Fan, Tao and Kang, Yan and Xu, Qian and Yang, Qiang},
  journal={arXiv preprint arXiv:2110.10927},
  year={2021}
}

@inproceedings{zhang2018gelu,
  title={GELU-Net: A Globally Encrypted, Locally Unencrypted Deep Neural Network for Privacy-Preserved Learning.},
  author={Zhang, Qiao and Wang, Cong and Wu, Hongyi and Xin, Chunsheng and Phuong, Tran V},
  booktitle={IJCAI},
  pages={3933--3939},
  year={2018}
}

@article{zhang2020additively,
  title={Additively homomorphical encryption based deep neural network for asymmetrically collaborative machine learning},
  author={Zhang, Yifei and Zhu, Hao},
  journal={arXiv preprint arXiv:2007.06849},
  year={2020}
}
@inproceedings{fu2022blindfl,
  title={Blindfl: Vertical federated machine learning without peeking into your data},
  author={Fu, Fangcheng and Xue, Huanran and Cheng, Yong and Tao, Yangyu and Cui, Bin},
  booktitle={Proceedings of the 2022 International Conference on Management of Data},
  pages={1316--1330},
  year={2022}
}

@article{kang2022privacy,
  title={Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability},
  author={Kang, Yan and He, Yuanqin and Luo, Jiahuan and Fan, Tao and Liu, Yang and Yang, Qiang},
  journal={IEEE Transactions on Big Data},
  year={2022},
  publisher={IEEE}
}

@article{liu2020secure,
  title={A secure federated transfer learning framework},
  author={Liu, Yang and Kang, Yan and Xing, Chaoping and Chen, Tianjian and Yang, Qiang},
  journal={IEEE Intelligent Systems},
  volume={35},
  number={4},
  pages={70--82},
  year={2020},
  publisher={IEEE}
}



@article{vepakomma2018split,
  title={Split learning for health: Distributed deep learning without sharing raw patient data},
  author={Vepakomma, Praneeth and Gupta, Otkrist and Swedish, Tristan and Raskar, Ramesh},
  journal={arXiv preprint arXiv:1812.00564},
  year={2018}
}

@inproceedings{thapa2022splitfed,
  title={Splitfed: When federated learning meets split learning},
  author={Thapa, Chandra and Arachchige, Pathum Chamikara Mahawaga and Camtepe, Seyit and Sun, Lichao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={8},
  pages={8485--8493},
  year={2022}
}

@inproceedings{paillier1999public,
  title={Public-key cryptosystems based on composite degree residuosity classes},
  author={Paillier, Pascal},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={223--238},
  year={1999},
  organization={Springer}
}
@article{shamir1979share,
  title={How to share a secret},
  author={Shamir, Adi},
  journal={Communications of the ACM},
  volume={22},
  number={11},
  pages={612--613},
  year={1979},
  publisher={ACm New York, NY, USA}
}
@inproceedings{damgaard2012multiparty,
  title={Multiparty computation from somewhat homomorphic encryption},
  author={Damg{\aa}rd, Ivan and Pastro, Valerio and Smart, Nigel and Zakarias, Sarah},
  booktitle={Annual Cryptology Conference},
  pages={643--662},
  year={2012},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@article{liu2019communication,
  title={A communication efficient vertical federated learning framework},
  author={Liu, Yang and Kang, Yan and Li, Liping and Zhang, Xinwei and Cheng, Yong and Chen, Tianjian and Hong, Mingyi and Yang, Qiang},
  journal={Unknown Journal},
  year={2019},
  publisher={Cambridge University Press}
}

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@incollection{zhu2020deep,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Han, Song},
  booktitle={Federated Learning},
  pages={17--31},
  year={2020},
  publisher={Springer}
}

@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
  pages={1322--1333},
  year={2015}
}

@article{gupta2018distributed,
  title={Distributed learning of deep neural network over multiple agents},
  author={Gupta, Otkrist and Raskar, Ramesh},
  journal={Journal of Network and Computer Applications},
  volume={116},
  pages={1--8},
  year={2018},
  publisher={Elsevier}
}

@article{gu2021federated,
  title={Federated Deep Learning with Bayesian Privacy},
  author={Gu, Hanlin and Fan, Lixin and Li, Bowen and Kang, Yan and Yao, Yuan and Yang, Qiang},
  journal={arXiv preprint arXiv:2109.13012},
  year={2021}
}


@article{liu2021fate,
  title={FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection.},
  author={Liu, Yang and Fan, Tao and Chen, Tianjian and Xu, Qian and Yang, Qiang},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={226},
  pages={1--6},
  year={2021}
}


@article{yang2023harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
  journal={arXiv preprint arXiv:2304.13712},
  year={2023}
}

@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{Chatgpt,
  title={Chatgpt},
  author={OpenAI},
  year={2022},
  publisher={OpenAI}
}

@article{Gpt-4,
  title={Gpt-4},
  author={OpenAI},
  year={2023},
  publisher={OpenAI}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and Yang, Fan and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{cai2022autofednlp,
  title={Autofednlp: An efficient fednlp framework},
  author={Cai, Dongqi and Wu, Yaozong and Wang, Shangguang and Lin, Felix Xiaozhu and Xu, Mengwei},
  journal={arXiv preprint arXiv:2205.10162},
  year={2022}
}

@article{zhao2022reduce,
  title={Reduce Communication Costs and Preserve Privacy: Prompt Tuning Method in Federated Learning},
  author={Zhao, Haodong and Du, Wei and Li, Fangqi and Li, Peixuan and Liu, Gongshen},
  journal={arXiv preprint arXiv:2208.12268},
  year={2022}
}

@article{zhang2022federated,
  title={When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods},
  author={Zhang, Zhuo and Yang, Yuanhang and Dai, Yong and Qu, Lizhen and Xu, Zenglin},
  journal={arXiv preprint arXiv:2212.10025},
  year={2022}
}

@article{wu2022communication,
  title={Communication-efficient federated learning via knowledge distillation},
  author={Wu, Chuhan and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={2032},
  year={2022},
  publisher={Nature Publishing Group UK London}
}


@article{li2022fedipr,
  title={FedIPR: Ownership verification for federated deep neural network models},
  author={Li, Bowen and Fan, Lixin and Gu, Hanlin and Li, Jie and Yang, Qiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}


@article{shao2019long,
  title={Long and diverse text generation with planning-based hierarchical variational model},
  author={Shao, Zhihong and Huang, Minlie and Wen, Jiangtao and Xu, Wenfei and Zhu, Xiaoyan},
  journal={arXiv preprint arXiv:1908.06605},
  year={2019}
}

@inproceedings{du2022glm,
  title={GLM: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}


@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}


@article{shen2020federated,
  title={Federated mutual learning},
  author={Shen, Tao and Zhang, Jie and Jia, Xinkang and Zhang, Fengda and Huang, Gang and Zhou, Pan and Kuang, Kun and Wu, Fei and Wu, Chao},
  journal={arXiv preprint arXiv:2006.16765},
  year={2020}
}

@inproceedings{briggs2020federated,
  title={Federated learning with hierarchical clustering of local updates to improve training on non-IID data},
  author={Briggs, Christopher and Fan, Zhong and Andras, Peter},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--9},
  year={2020},
  organization={IEEE}
}

@article{xiao2023offsite,
  title={Offsite-tuning: Transfer learning without full model},
  author={Xiao, Guangxuan and Lin, Ji and Han, Song},
  journal={arXiv preprint arXiv:2302.04870},
  year={2023}
}

@inproceedings{huang2022cheetah,
  title={Cheetah: Lean and Fast Secure $\{$Two-Party$\}$ Deep Neural Network Inference},
  author={Huang, Zhicong and Lu, Wen-jie and Hong, Cheng and Ding, Jiansheng},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={809--826},
  year={2022}
}

@article{liu2022vertical,
  title={Vertical Federated Learning},
  author={Liu, Yang and Kang, Yan and Zou, Tianyuan and Pu, Yanhong and He, Yuanqin and Ye, Xiaozhou and Ouyang, Ye and Zhang, Ya-Qin and Yang, Qiang},
  journal={arXiv preprint arXiv:2211.12814},
  year={2022}
}


@inproceedings{zhang2018lq,
  title={Lq-nets: Learned quantization for highly accurate and compact deep neural networks},
  author={Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={365--382},
  year={2018}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{wang2023can,
  title={Can Public Large Language Models Help Private Cross-device Federated Learning?},
  author={Wang, Boxin and Zhang, Yibo Jacky and Cao, Yuan and Li, Bo and McMahan, H Brendan and Oh, Sewoong and Xu, Zheng and Zaheer, Manzil},
  journal={Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning (ICML)},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}


@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}


@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{welbl2017crowdsourcing,
  title={Crowdsourcing multiple choice science questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  journal={arXiv preprint arXiv:1707.06209},
  year={2017}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{he2021towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}


@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}


@inproceedings{yang2021regularized,
  title={Regularized mutual learning for personalized federated learning},
  author={Yang, Ruihong and Tian, Junchao and Zhang, Yu},
  booktitle={Asian Conference on Machine Learning},
  pages={1521--1536},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2018deep,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4320--4328},
  year={2018}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@article{chen2017learning,
  title={Learning efficient object detection models with knowledge distillation},
  author={Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{meng2019conditional,
  title={Conditional teacher-student learning},
  author={Meng, Zhong and Li, Jinyu and Zhao, Yong and Gong, Yifan},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6445--6449},
  year={2019},
  organization={IEEE}
}

@article{adriana2015fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Adriana, Romero and Nicolas, Ballas and Ebrahimi, K Samira and Antoine, Chassang and Carlo, Gatta and Yoshua, Bengio},
  journal={Proc. ICLR},
  volume={2},
  number={3},
  pages={1},
  year={2015}
}

@article{zagoruyko2016paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1612.03928},
  year={2016}
}

@inproceedings{chen2021cross,
  title={Cross-layer distillation with semantic calibration},
  author={Chen, Defang and Mei, Jian-Ping and Zhang, Yuan and Wang, Can and Wang, Zhe and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={8},
  pages={7028--7036},
  year={2021}
}

@inproceedings{passban2021alp,
  title={Alp-kd: Attention-based layer projection for knowledge distillation},
  author={Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun},
  booktitle={Proceedings of the AAAI Conference on artificial intelligence},
  volume={35},
  number={15},
  pages={13657--13665},
  year={2021}
}


@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3967--3976},
  year={2019}
}


@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{li2019fedmd,
  title={Fedmd: Heterogenous federated learning via model distillation},
  author={Li, Daliang and Wang, Junpu},
  journal={arXiv preprint arXiv:1910.03581},
  year={2019}
}

@article{cho2022heterogeneous,
  title={Heterogeneous ensemble knowledge transfer for training large models in federated learning},
  author={Cho, Yae Jee and Manoel, Andre and Joshi, Gauri and Sim, Robert and Dimitriadis, Dimitrios},
  journal={arXiv preprint arXiv:2204.12703},
  year={2022}
}

@article{yi2023fedlora,
  title={Fedlora: Model-heterogeneous personalized federated learning with lora tuning},
  author={Yi, Liping and Yu, Han and Wang, Gang and Liu, Xiaoguang},
  journal={arXiv preprint arXiv:2310.13283},
  year={2023}
}

@inproceedings{jang2022fedclassavg,
  title={Fedclassavg: Local representation learning for personalized federated learning on heterogeneous neural networks},
  author={Jang, Jaehee and Ha, Heoneok and Jung, Dahuin and Yoon, Sungroh},
  booktitle={Proceedings of the 51st International Conference on Parallel Processing},
  pages={1--10},
  year={2022}
}

@article{liu2022completely,
  title={Completely heterogeneous federated learning},
  author={Liu, Chang and Yang, Yuwen and Cai, Xun and Ding, Yue and Lu, Hongtao},
  journal={arXiv preprint arXiv:2210.15865},
  year={2022}
}

@article{fan2023fate,
  title={Fate-llm: A industrial grade federated learning framework for large language models},
  author={Fan, Tao and Kang, Yan and Ma, Guoqiang and Chen, Weijing and Wei, Wenbin and Fan, Lixin and Yang, Qiang},
  journal={arXiv preprint arXiv:2310.10049},
  year={2023}
}

@article{kang2023grounding,
  title={Grounding foundation models through federated transfer learning: A general framework},
  author={Kang, Yan and Fan, Tao and Gu, Hanlin and Fan, Lixin and Yang, Qiang},
  journal={arXiv preprint arXiv:2311.17431},
  year={2023}
}

@article{wan2024knowledge,
  title={Knowledge fusion of large language models},
  author={Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
  journal={arXiv preprint arXiv:2401.10491},
  year={2024}
}

@inproceedings{fu2023specializing,
  title={Specializing smaller language models towards multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  booktitle={International Conference on Machine Learning},
  pages={10421--10430},
  year={2023},
  organization={PMLR}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{wang2022benchmarking,
  title={Benchmarking generalization via in-context instructions on 1,600+ language tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  volume={2},
  year={2022}
}

@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@article{ho2022large,
  title={Large language models are reasoning teachers},
  author={Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  journal={arXiv preprint arXiv:2212.10071},
  year={2022}
}

@article{li2023symbolic,
  title={Symbolic chain-of-thought distillation: Small models can also" think" step-by-step},
  author={Li, Liunian Harold and Hessel, Jack and Yu, Youngjae and Ren, Xiang and Chang, Kai-Wei and Choi, Yejin},
  journal={arXiv preprint arXiv:2306.14050},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{klein2020contrastive,
  title={Contrastive self-supervised learning for commonsense reasoning},
  author={Klein, Tassilo and Nabi, Moin},
  journal={arXiv preprint arXiv:2005.00669},
  year={2020}
}

@article{dong2023puma,
  title={Puma: Secure inference of llama-7b in five minutes},
  author={Dong, Ye and Lu, Wen-jie and Zheng, Yancheng and Wu, Haoqi and Zhao, Derun and Tan, Jin and Huang, Zhicong and Hong, Cheng and Wei, Tao and Cheng, Wenguang},
  journal={arXiv preprint arXiv:2307.12533},
  year={2023}
}

@article{hou2023ciphergpt,
  title={Ciphergpt: Secure two-party gpt inference},
  author={Hou, Xiaoyang and Liu, Jian and Li, Jingyu and Li, Yuhan and Lu, Wen-jie and Hong, Cheng and Ren, Kui},
  journal={Cryptology ePrint Archive},
  year={2023}
}

@inproceedings{dwork2006differential,
  title={Differential privacy},
  author={Dwork, Cynthia},
  booktitle={International colloquium on automata, languages, and programming},
  pages={1--12},
  year={2006},
  organization={Springer}
}

@inproceedings{yao1986generate,
  title={How to generate and exchange secrets},
  author={Yao, Andrew Chi-Chih},
  booktitle={27th annual symposium on foundations of computer science (Sfcs 1986)},
  pages={162--167},
  year={1986},
  organization={IEEE}
}

@book{gentry2009fully,
  title={A fully homomorphic encryption scheme},
  author={Gentry, Craig},
  year={2009},
  publisher={Stanford university}
}

@article{yue2021differential,
  title={Differential privacy for text analytics via natural text sanitization},
  author={Yue, Xiang and Du, Minxin and Wang, Tianhao and Li, Yaliang and Sun, Huan and Chow, Sherman SM},
  journal={arXiv preprint arXiv:2106.01221},
  year={2021}
}

@article{chen2022customized,
  title={A customized text sanitization mechanism with differential privacy},
  author={Chen, Huimin and Mo, Fengran and Wang, Yanhao and Chen, Cen and Nie, Jian-Yun and Wang, Chengyu and Cui, Jamie},
  journal={arXiv preprint arXiv:2207.01193},
  year={2022}
}

@inproceedings{zhou2023textobfuscator,
  title={Textobfuscator: Making pre-trained language model a privacy protector via obfuscating word representations},
  author={Zhou, Xin and Lu, Yi and Ma, Ruotian and Gui, Tao and Wang, Yuran and Ding, Yong and Zhang, Yibo and Zhang, Qi and Huang, Xuan-Jing},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={5459--5473},
  year={2023}
}

@article{tong2023privinfer,
  title={Privinfer: Privacy-preserving inference for black-box large language model},
  author={Tong, Meng and Chen, Kejiang and Qi, Yuang and Zhang, Jie and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2310.12214},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{mcsherry2007mechanism,
  title={Mechanism design via differential privacy},
  author={McSherry, Frank and Talwar, Kunal},
  booktitle={48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)},
  pages={94--103},
  year={2007},
  organization={IEEE}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{zhang2021survey,
  title={A survey on multi-task learning},
  author={Zhang, Yu and Yang, Qiang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={34},
  number={12},
  pages={5586--5609},
  year={2021},
  publisher={IEEE}
}

@inproceedings{lhoest-etal-2021-datasets,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      {\v{S}}a{\v{s}}ko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.21",
    pages = "175--184",
    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
    eprint={2109.02846},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}


@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{kim2024shortened,
  title={Shortened llama: A simple depth pruning for large language models},
  author={Kim, Bo-Kyeong and Kim, Geonmin and Kim, Tae-Ho and Castells, Thibault and Choi, Shinkook and Shin, Junho and Song, Hyoung-Kyu},
  journal={arXiv preprint arXiv:2402.02834},
  year={2024}
}


@article{yang2024laco,
  title={Laco: Large language model pruning via layer collapse},
  author={Yang, Yifei and Cao, Zouying and Zhao, Hai},
  journal={arXiv preprint arXiv:2402.11187},
  year={2024}
}


@article{jha2023train,
  title={How to train your (compressed) large language model},
  author={Jha, Ananya Harsh and Sherborne, Tom and Walsh, Evan Pete and Groeneveld, Dirk and Strubell, Emma and Beltagy, Iz},
  journal={arXiv preprint arXiv:2305.14864},
  year={2023}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{nova2023gradient,
  title={Gradient-free structured pruning with unlabeled data},
  author={Nova, Azade and Dai, Hanjun and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={26326--26341},
  year={2023},
  organization={PMLR}
}

@article{kurtic2024ziplm,
  title={Ziplm: Inference-aware structured pruning of language models},
  author={Kurti{\'c}, Eldar and Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{song2013stochastic,
  title={Stochastic gradient descent with differentially private updates},
  author={Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D},
  booktitle={2013 IEEE global conference on signal and information processing},
  pages={245--248},
  year={2013},
  organization={IEEE}
}

@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th annual symposium on foundations of computer science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{mattern2022differentially,
  title={Differentially private language models for secure data sharing},
  author={Mattern, Justus and Jin, Zhijing and Weggenmann, Benjamin and Schoelkopf, Bernhard and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.13918},
  year={2022}
}

@article{yue2022synthetic,
  title={Synthetic text generation with differential privacy: A simple and practical recipe},
  author={Yue, Xiang and Inan, Huseyin A and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Shajari, Hoda and Sun, Huan and Levitan, David and Sim, Robert},
  journal={arXiv preprint arXiv:2210.14348},
  year={2022}
}

@article{kurakin2023harnessing,
  title={Harnessing large-language models to generate private synthetic text},
  author={Kurakin, Alexey and Ponomareva, Natalia and Syed, Umar and MacDermed, Liam and Terzis, Andreas},
  journal={arXiv preprint arXiv:2306.01684},
  year={2023}
}


@article{touvron2023llama2,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@inproceedings{biderman2023pythia,
  title        = {Pythia: A suite for analyzing large language models across training and scaling},
  author       = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2397--2430},
  year         = {2023},
  organization = {PMLR}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}


@article{OpenAI2023GPT4TR,
  title   = {GPT-4 Technical Report},
  author  = {OpenAI},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2303.08774}
}


@inproceedings{zhang2024loraprune,
  title={LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
  author={Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={3013--3026},
  year={2024}
}


@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal=NIPS,
  volume={30},
  year={2017}
}

@inproceedings{
lee2018snip,
title={SNIP: Single-shot network pruning based on connection sensitivity},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
booktitle=ICLR,
year={2019},
}

@article{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:2002.07376},
  year={2020}
}

@article{sun2023simple,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}


@inproceedings{yue-etal-2023-synthetic,
    title = "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe",
    author = "Yue, Xiang  and
      Inan, Huseyin  and
      Li, Xuechen  and
      Kumar, Girish  and
      McAnallen, Julia  and
      Shajari, Hoda  and
      Sun, Huan  and
      Levitan, David  and
      Sim, Robert",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.74",
    doi = "10.18653/v1/2023.acl-long.74",
    pages = "1321--1342",
    abstract = "Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.",
}

@article{samragh2023weight,
  title={Weight subcloning: direct initialization of transformers using larger pretrained ones},
  author={Samragh, Mohammad and Farajtabar, Mehrdad and Mehta, Sachin and Vemulapalli, Raviteja and Faghri, Fartash and Naik, Devang and Tuzel, Oncel and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2312.09299},
  year={2023}
}

@inproceedings{maia201818,
  title={Www'18 open challenge: financial opinion mining and question answering},
  author={Maia, Macedo and Handschuh, Siegfried and Freitas, Andr{\'e} and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1941--1942},
  year={2018}
}


@inproceedings{chen-etal-2023-customized,
    title = "A Customized Text Sanitization Mechanism with Differential Privacy",
    author = "Chen, Sai  and
      Mo, Fengran  and
      Wang, Yanhao  and
      Chen, Cen  and
      Nie, Jian-Yun  and
      Wang, Chengyu  and
      Cui, Jamie",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.355/",
    doi = "10.18653/v1/2023.findings-acl.355",
    pages = "5747--5758",
    abstract = "As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on a relaxed notion of metric local differential privacy (MLDP) do not apply to non-metric semantic similarity measures and cannot achieve good privacy-utility trade-offs. To address these limitations, we propose a novel Customized Text sanitization (CusText) mechanism based on the original $\epsilon$-differential privacy (DP) definition, which is compatible with any similarity measure.Moreover, CusText assigns each input token a customized output set to provide more advanced privacy protection at the token level.Extensive experiments on several benchmark datasets show that CusText achieves a better trade-off between privacy and utility than existing mechanisms.The code is available at \url{https://github.com/sai4july/CusText}."
}


@article{chen2022customized,
  title={A customized text sanitization mechanism with differential privacy},
  author={Chen, Huimin and Mo, Fengran and Wang, Yanhao and Chen, Cen and Nie, Jian-Yun and Wang, Chengyu and Cui, Jamie},
  journal={arXiv preprint arXiv:2207.01193},
  year={2022}
}

@article{li2024common,
  title={Common 7b language models already possess strong math capabilities},
  author={Li, Chen and Wang, Weiqi and Hu, Jingcheng and Wei, Yixuan and Zheng, Nanning and Hu, Han and Zhang, Zheng and Peng, Houwen},
  journal={arXiv preprint arXiv:2403.04706},
  year={2024}
}


@article{yu2024privacy,
  title={Privacy-Preserving Instructions for Aligning Large Language Models},
  author={Yu, Da and Kairouz, Peter and Oh, Sewoong and Xu, Zheng},
  journal={arXiv preprint arXiv:2402.13659},
  year={2024}
}