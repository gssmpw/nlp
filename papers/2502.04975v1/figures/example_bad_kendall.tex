
\begin{figure}
    \centering    
    \includegraphics[width=0.95\columnwidth]{data/ImageNet16-120_filtered_figure1.eps} \vspace{-5pt}
    \caption{Training-free NAS methods on 
ImageNet16-120 \cite{dong2020bench}. Methods are compared by Normalized Discounted Cumulative Gain (see Sec. \ref{sec:evaluationmetrics}), our method (VKDNW) is the best also measured by Kendall's $\tau$ and Spearman's $\rho$ correlations (see Table \ref{tab:table1}). Also note that simple \textit{number of trainable layers} (below denoted $\aleph$) is significantly better trivial proxy than the number of FLOPs.
\vspace{-5pt}}
    \label{fig:example_bad_kendall}
\end{figure}