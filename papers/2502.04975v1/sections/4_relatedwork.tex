\section{Related Work}
\label{sec:relatedwork}
\vspace{-5pt}

Zero-shot NAS aims to rank given networks in a training-free manner based on their (a priori unknown) final performance, which allows to prune the huge search space with limited costs when seeking for optimal architecture \cite{ying2019bench, dong2020bench, liu2018darts} and other configuration~\cite{cai2018proxylessnas, lin2021zen, sandler2018mobilenetv2}.
\input{tables/table1_filtered}
Throughout previous works different approaches for ranking computation can be found. In many of the works the gradient with respect to the network weights is investigated (GradNorm, GraSP, SNIP, Synflow, \cite{tanaka2020pruning, wang2020picking, lee2018snip, abdelfattah2021zero}), i.e. leveraging the first order approximation of the network. In Jacov \cite{abdelfattah2021zero} correlations of the Jacobian matrices among various input samples are compared; in NASWOT \cite{mellor2021neural} the linear maps induced by data points are examined; Zen \cite{lin2021zen} uses the approximation of gradient with respect to featuremaps; GradSign \cite{zhang2021gradsign} compares the optimization landscape at the level individual training samples; or in ZiCo \cite{li2023zico} the gradients from multiple forward and backward passes preferring large magnitude and low variance.



Other methods aggregate multiple sources aiming at obtaining a better informed ranks: TE-NAS \cite{chen2021neural} uses both the number of linear regions \cite{hanin2019complexity, xiong2020number} and the condition number of Neural Tangent Kernel \cite{jacot2018neural, lee2019wide}. However, it is well known that the kernel computation is highly computationally demanding \cite{novak2022fast}. AZ-NAS \cite{lee2024az} assesses expressivity, trainability and progressivity via examination of feature distribution across all orientations and the Jacobian. However, despite AZ-NAS outperforming previous works in some metrics, it is worse than \cite{li2023zico} in key NAS-related aspects such as the cumulative gain (see \cref{subsec:ranking}).

Despite a tremendous effort of the community, it was shown that most of the zero-shot NAS methods perform worse than a simple proxy given just by FLOPs or \#params \cite{ning2021evaluating, white2022deeper}. Thus, there is still a gap for improvements also driven by the need of the ranking explainability that would have a satisfactory theoretical support.

Furthermore, \cite{kadlecova2024surprisingly} uses a model-driven ranking that however needs a set of networks for which the validation accuracies are known to train the model and the ability of this ranking to generalize as it is fitted on a specific network collection only is disputable.

Finally, let us discuss the current practice in the NAS methods comparison. Methods are compared either by means of correlations of their scores to accuracy or by reporting the accuracy of the top-ranked network \cite{lee2024az, li2023zico}. However, while the first is not tailor-made for the architecture search task and therefore does not assess the desired ranking properties (see Sec. \ref{subsec:ranking}), comparing performance just by the accuracy of a single network is too vulnerable e.g. to the random seed choice.  %as e.g. in \cite{lee2024az} the results are not significant if one considers also the standard deviation.
