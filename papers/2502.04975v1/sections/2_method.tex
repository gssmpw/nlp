\section{Method}
\label{sec:method}

Our zero-shot proxy for image classification accuracy builds on Fisher Information theory~\cite{ly2017tutorial}, therefore we begin by a thorough analysis of Fisher Information Matrix (FIM) of the network weights estimation problem (see Sec. \ref{subsec:fim}). We proceed by discussing challenges involved in practical application of FIM in context of large over-parametrized models that lead to only limited success in previous works and present our contributions to overcome these limitations (see Sec.~\ref{subsec:empiricalFIM}). Finally. we propose a novel FIM-based proxy for NAS algorithms (Sec. \ref{subsec:vkdnw}).

\subsection{Fisher Information}
\label{subsec:fim}

The problem of finding the optimal weights of a neural network $f$ for the task of $C$-class image classification can be seen as a maximum likelihood estimation with a statistical model
\begin{align}
    \sigma_\theta(c\,|\,x)=\frac{\exp\left({\Psi_c(x, \theta)}\right)}{\sum_{d=1}^C\exp\left({\Psi_d(x, \theta)}\right)}, \quad c=1,\dots, C
\end{align}
where $\sigma_\theta(\cdot\,|\,x)$ denotes the a posteriori distribution of the labels given input image $x$ and $\Psi(x,\theta)\in\mathbb{R}^C=\left( \Psi_1(x,\theta),\dots ,\Psi_C(x,\theta)\right)$ is the network output (logits) given the weight vector $\theta\in\mathbb{R}^p$, i.e. the network weights. 
We describe the process of training as finding the optimal weight vector $\theta^*$ that fits our data and \textit{we posit that network architectures should be characterised by how easy it is to estimate their optimal network weights} $\theta^*$. 
We build upon statistical learning theory and use Fisher Information~\cite{ly2017tutorial} framework to formally describe expected behaviour of the training process of a given deep network $f$.

The Fisher Information Matrix (FIM) encompasses information on the difficulty of the parameter estimation problem and it plays a crucial role in several fundamental results, which we apply below in the context of deep networks. The FIM of a network $f$ and its set of weights  $\theta\in\mathbb{R}^p$ is given as
\begin{align}
\label{eq:fim}
F(\theta)\coloneq\mathbb{E}\left[\nabla_{\theta}\sigma_\theta(c\,|\,x)\,\nabla_{\theta}\sigma_\theta(c\,|\,x)^T\right]\in\mathbb{R}^{p\times p},
\end{align}
where we take the expected value $\mathbb{E}$ with respect to the joint distribution of $(x,c)$. For more detailed account on Fisher Information theory and its applications in the context of  machine learning, we kindly refer reader to \cite{karakida2019universal, martens2020new, lee2022masking, pennington2018spectrum, park2019adaptive}.

\paragraph{Cramér-Rao bound.} The first part of estimation theory we build upon is the inverse of the FIM, known as the Cramér–Rao bound (see \cite{frieden2010exploratory}), which is the asymptotic variance of the estimated weights (i.e. the uncertainty coming from the data variability). Thus, the larger the matrix norm of the FIM, the more certain we are about the weights $\theta$. More specifically, any data-dependent estimator $\Hat{\theta}_n=\Hat{\theta}_n(x_1,\dots,x_n)$ is a random vector with randomness coming from the (independent) choice of input images $x_1,\dots,x_n$ and as such has some variance matrix $\text{Var}\left(\Hat{\theta}_n\right)$. The famous result (see \cite{cramer1999mathematical, rao1992information}) named in honour of H. Cramér and C. R. Rao states that if $\Hat{\theta}_n$ is unbiased then the variance is bounded from below as
\begin{align}
    \text{Var}\left(\Hat{\theta}_n\right)\geq \frac{1}{n}F^{-1}(\theta)
\end{align}
and for maximum likelihood estimation this bound is attained as the number of input images grows to infinity $n\to\infty$. 

We formally show  (see Supplementary material) that  for each weight $\theta(j)$ the mean square error of our estimation is controlled by the diagonal element of the FIM inverse as
\begin{align}
    \mathbb{E}\left( \Hat{\theta}_n(j)-\theta(j)\right)^2\geq \frac{1}{n}\left(F^{-1}(\theta)\right)_{jj}.
\end{align}
Therefore, knowing the FIM allows us to evaluate how certain we are about the weight estimates. We can go even further by inspecting the eigenvalues of $F(\theta)$. Denoting the largest and smallest eigenvalue of the FIM as $\lambda_{\min}$ and $\lambda_{\max}$ respectively, then we have that there exist linear combination coefficients $e_{\min}$ and $e_{\max}$ of unit size so that
\begin{align}
    \mathbb{E}\left( e_{\min}^T\Hat{\theta}_n-e_{\min}^T\theta\right)^2\geq &\frac{1}{n\lambda_{\min}},\nonumber\\ 
    \mathbb{E}\left( e_{\max}^T\Hat{\theta}_n-e_{\max}^T\theta\right)^2\geq &\frac{1}{n\lambda_{\max}},
\end{align}
indicating that if the difference between the largest and smallest eigenvalue is \textit{large} then there exist combinations of weights with \textit{very large difference} in the estimation certainty. Altogether, the more the eigenvalues of the FIM are similar, the more similar is also the variance in the weight estimation across all model weights.

\paragraph{Prediction sensibility.}
The change of the model prediction measured by the KL-divergence when subject to a small perturbations of the weights is given as
\begin{align}
    D_{KL}(\sigma_{\theta+\theta_\delta}(\cdot\,|\,x), \sigma_{\theta}(\,\cdot|\,x))\approx\frac{1}{2}\theta_\delta^T\,F(\theta)\,\theta_\delta
\end{align}
for a small weight perturbation vector $\theta_\delta\in\mathbb{R}^p$. Thus, $F(\theta)$ measures volatility of the predictions subject to the weight change -- if the difference between the largest and the smallest eigenvalues of the FIM is large, then there exist perturbation directions $\theta_{min}, \theta_{max}$ corresponding to $\lambda_{min}, \lambda_{max}$ such that
\begin{align}
    D_{KL}&(\sigma_{\theta+\theta_{min}}(\cdot\,|\,x), \sigma_{\theta}(\cdot\,|\,x)) 
    \approx\frac{1}{2}\theta_{min}^T\,F(\theta)\,\theta_{min} = \nonumber\\
    &=\frac{1}{2}\lambda_{min}\|\theta_{min}\|^2\ll\frac{1}{2}\lambda_{max}\|\theta_{max}\|^2 = \\
    &=\frac{1}{2}\theta_{max}^T\,F(\theta)\,\theta_{max}
    \approx D_{KL}(\sigma_{\theta+\theta_{max}}(\cdot\,|\,x), \sigma_{\theta}(\cdot\,|\,x)) \nonumber
\end{align}
and therefore in some directions a small change of the weights has much larger impact on the prediction than in others, making the model less balanced. For further discussion, see the Supplementary material.

\subsection{Empirical Fisher Information Matrix implementation}
\label{subsec:empiricalFIM}
When independent and identically distributed sample images $x_n$ are available, empirical FIM $\Hat{F}(\theta)$ is defined as
%\footnote{The empirical FIM is a special case of the FIM in \eqref{eq:fim} when the distribution of the input images is given by the sample images and therefore this distinction is not strictly needed in our considerations. We stick to it to follow the general statistical terminology.}
\begin{align}
\Hat{F}(\theta)&\coloneq\frac{1}{n}\sum_{n=1}^N\mathbb{E}_{\sigma_\theta}\left[\nabla_{\theta}\sigma_\theta(c\,|\,x_n)\,\nabla_{\theta}\sigma_\theta(c|\,x_n)^T\right]
    \label{eq:fim_epirical}
\end{align}
where $\mathbb{E}_{\sigma_\theta}$ now denotes the expectation with respect to the model prediction $\sigma_\theta$.

We would like to first emphasize several crucial aspects and \textit{contributions of this paper} that lead to the first success of the Fisher Information theory (having e.g. \cite{abdelfattah2021zero} in mind) in the context of Neural Architecture Search, despite Fisher Information being one of the first obvious choices for deep network analysis:

1. Following \cite{kunstner2019limitations} we write the empirical FIM \eqref{eq:fim_epirical} as
\begin{align}
    \Hat{F}(\theta)=\frac{1}{n}\sum_{n=1}^{N}&\left[\nabla_\theta\Psi (x_n,\theta)^T\left(\diag(\sigma_\theta(\cdot, x_n))-\right.\right.\nonumber\\
    &\left.\left.\sigma_\theta(\cdot, x_n)\sigma_\theta(\cdot, x_n)^T\right)\nabla_\theta\Psi (x_n,\theta)\right].
\end{align}
and we further decompose the inner matrix $\diag(\sigma_\theta(\cdot, x_n))-\sigma_\theta(\cdot, x_n)\sigma_\theta(\cdot, x_n)^T$ using analytical formulas from \cite{tanabe1992exact} to avoid numerical instability of the computation as we arrive at a feasible representation
\begin{align}
    \Hat{F}(\theta)=\frac{1}{n}\sum_{n=1}^{N}A_n^TA_n
\end{align}
for some matrices $A_n\in\mathbb{R}^{C\times p}$. We observed that networks typically yield very imbalanced outputs at initialization (i.e. every network prioritizes few classes over the rest) making it extremely important not to exclude the factor $\diag(\sigma_\theta(\cdot, x_n))-\sigma_\theta(\cdot, x_n)\sigma_\theta(\cdot, x_n)^T$, which is however difficult to compute without underflow/overflow.

2. The dimension of FIM is equal to $p$ (the number of all trainable parameters) and therefore direct computation of the eigenvalues is numerically intractable and unstable. However, our results show that if a small number of representative parameters is drawn, then computation becomes stable while the discrimination power does not suffer. We used a simple rule where a single parameter from each trainable layer (not including batch normalization) is chosen. Stability with respect to choice of such parameters can be found in Supplementary material.
    
3. FIM of large networks typically suffers from having pathological spectrum, i.e. there usually exist zero eigenvalue and its multiplicity is large (see \cite{karakida2019pathological}) and thus the eigenvalues estimation due to a large condition number is imprecise. However, as we deal with a symmetric positive-semidefinite matrix, the eigenvalues actually coincide with the singular values (see \cite{leon2006linear}), for which the estimation algorithm performs better.


Let us also emphasize that there is a common misconception in part of the community as it is often mistakenly assumed that one can simply use the true labels of $x_n$ in \eqref{eq:fim_epirical} in place of $c$. However, such definition is then meaningless as it does not approximate the FIM in the classical Monte Carlo sense (see Supplementary material and \cite{kunstner2019limitations} for a comparison). That means that the empirical FIM \textit{does not} depend on the true labels and therefore our method \textit{does not} require real data - we use random input instead.


\subsection{Variance of Knowledge for Deep Network Weights}
\label{subsec:vkdnw}

In order to characterize properties of the parameter estimation process for a given network $f$ through the lens of Fisher Information theory, we inspect the eigenvalues of the empirical FIM $\Hat{F}(\theta_{\text{init}})$ and define the entropy of \textit{Variance of Knowledge for Deep Network Weights (VKDNW)} as
\begin{align}
\label{eq:vkdnw}
    \text{VKDNW}\big(f\big) &\coloneq -\sum_{k=1}^9\tilde{\lambda}_k\log\Tilde{\lambda}_k \nonumber\\
    \tilde{\lambda}_k&=\frac{\lambda_k}{\sum_{j=1}^9\lambda_j}, \quad k=1,\dots, 9,
\end{align}
where $\lambda_k$ denotes the $k$-th decile of the FIM eigenvalues as the representation of the FIM spectrum\footnote{As explained in Sec. \ref{subsec:empiricalFIM}, the smallest eigenvalue $\lambda_0$ is usually equal to zero and thus we exclude it for stability reasons (similarly with the maximal eigenvalue $\lambda_{10}$)}, and $\theta_{\text{init}}$ denotes network weights at initialization.

Our score therefore measures the diversity of the FIM eigenvalues, and from the entropy theory we know that VKDNW attains its maximum exactly when all the eigenvalues $\lambda_k$ are equal and VKDNW gets lower as the eigenvalues become \textit{more different}. Based on the discussion in Sec. \ref{subsec:fim} we see that VKDNW is high when the uncertainty in all model weight combinations are similar (see Cramér-Rao bound) and there are no directions in the weight space that would influence the network prediction substantially differently than others. Due to the fact that we have normalized both the number of eigenvalues under consideration (by taking a fixed number of representatives irrespective of the network size) and the magnitude of the eigenvalues, VKDNW is independent of network size (number of network weights $p$).


Let us note, that we are familiar with the fact that even though our motivation was (among others) based on Cramér-Rao bound that assumes evaluation of FIM at the \textit{correct} weight vector $\theta_X$ that fits the data, which is typically far away from the weights given at initialization. However, our empirical results below support the hypothesis that the evaluation despite being in the wrong point brings valuable information.

\paragraph{Ranking networks for NAS.}
The proposed VKDNW score is independent of network size, which is extremely beneficial to compare individual structures of network architectures. Thus, it does not aim to capture capacity, rather it targets feasibility of the computation graph given number of operations. However, when comparing different network structures and different network sizes together as it is done in NAS, one indeed needs to take network size into account as well, because naturally larger networks have bigger capacity and therefore tend to have higher accuracy.  To capture also the capacity we proxy the network size by the number of layers with weights that we denote as $\aleph \big(f\big)$ for a network $f$ and we introduce the ranking
\begin{align}
\text{VKDNW}_{\text{single}}\big(f\big)\coloneq\aleph\big(f\big)+\text{VKDNW}\big(f\big)
\label{eq:vkdnwrank}
\end{align}
%
%\todo{\begin{align}
%    \#\textit{trainable\_layers}(f_1)&<\#\textit{trainable\_layers}(f_2) \nonumber\\
%    &\text{or} \nonumber\\
%    \#\textit{trainable\_layers}(f_1)&=\#\textit{trainable\_layers}(f_2) \quad \text{and} \nonumber \\ \quad
%    \text{VKDNW}(f_1)&<\text{VKDNW}(f_2)
%\end{align}}
Here we leverage the fact that VKDNW as an entropy of some quantity is always between 0 and 1 and by summing it with an integer-valued quantity we in fact obtain that we have first grouped networks by our size proxy $\aleph$ and then within each group of similar networks sizes we order them by VKDNW. 
