\vspace{-5pt}
\section{Introduction}
\label{sec:intro}

In most instances, neural network architectures are designed by authors following the field's ``best-practices'' or their experience, without any formal and repeatable procedure. This is however inconvenient especially in applications on a large scale. Neural Architecture Search (NAS) aims to bridge this gap by following a well-defined optimization paradigm which systematically looks for the best architecture, given objective criterion such as maximal accuracy.

\input{figures/example_bad_kendall}

The main limitation of Neural Architecture Search (NAS) is however the computational cost, because in the most basic NAS setup, it is required to train thousands or more of different deep network architectures from scratch in order just to calculate a single scalar  -- the objective function value, such as the classification accuracy. This severely limits practical applications of NAS as the size of feasible architecture search space is only a small fraction of the overall space of all networks.

Training-free Neural Architecture Search (TF-NAS) aims to alleviate this limitation by introducing an \textit{objective function proxy} which -- unlike the actual objective function -- does not require training the network. As a result, a good proxy allows the TF-NAS algorithm to explore significantly bigger portion of the network architecture search space compared to traditional NAS, and to find the best network architecture without training a single network. The crucial question is however finding an appropriate objective function proxy.

In this paper, we present a novel, principled \textit{objective function proxy} called \textit{Variance of Knowledge of Deep Network Weights (VKDNW)} for image classification accuracy, which allows us to \textbf{find optimal network architectures for image classification without training} them first. Our method is, to the best of our knowledge, the first successful application of Fisher Information theory~\cite{ly2017tutorial} in the context of large deep neural networks, and as such allows us to formally describe and quantify the difficulty of network parameters' estimation process. In other words, \textit{given a network architecture, our method estimates how easy or hard it will be to train the network}.


Additionally, we also observe that the evaluation metrics used in the TF-NAS community are not well-suited to the problem at hand, because it unnecessarily penalizes for bad proxy accuracy for networks which are not interesting, and vice-versa it does not sufficiently reward proxies which are able to accurately pick out good network architectures. Following this observation, we propose that the Normalized Discounted Cumulative Gain should be used in companion with other TF-NAS metrics, and show indeed that there are significant differences amongst previously proposed TF-NAS methods when the new metric is considered. 

To summarize, we make the following contributions:
\begin{enumerate}
    \item We introduce a novel algorithm for estimation of Fisher Information Matrix spectrum, which is tractable even for models with large number of parameters such as deep networks, that overcomes the usual problems of numerical stability.
    \item We introduce a novel principled VKDNW proxy for image classification accuracy. The proxy is based on strong theoretical background and captures uncertainty in weight estimation process. It brings information that is orthogonal to the model size which then allows for efficient combination with previously proposed proxies, leading to state-of-the-art results.
    \item We propose a new evaluation metric for TF-NAS proxies which is more relevant to the actual NAS objective as it concentrates on ability of given proxy to identify good networks.
    %\item We show that using our VKDNW proxy, which originated purely on theoretical grounds, leads to state-of-the-art results in practical NAS applications.
    % \item Previous metrics can be explained by VKDNW dim?
\end{enumerate}



% \textbf{Claims}
% \begin{itemize}
%     \item Previous proxy-free methods are very complicated, often based on heuristics
%     \item Our method is much simpler, principled and based on strong theoretical background
% \end{itemize}
