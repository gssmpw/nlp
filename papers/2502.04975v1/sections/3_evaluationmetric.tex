\section{Evaluation Metrics}
\label{sec:evaluationmetrics}
\input{figures/toy_example_ranking}
In this section, we dive into the problem of evaluating (image) classification accuracy proxies for training-free NAS (TF-NAS). 
%Consider an optimal proxy for image classification accuracy in the context of NAS     evaluation and motivated by the task of seeking the best architecture we suggest several metrics that every thorough analysis should involve. 
In NAS algorithms, the proxy actually does not have to predict the classification accuracy in absolute terms -- since we are after finding the ``best'' network for given task -- it's sufficient that the proxy properly ranks the networks, ordering them from worst to the best.

Suppose that a collection of $K$ networks with validation accuracies $\text{acc}_1, \dots, \text{acc}_K$ have been ranked by a TF-NAS proxy as $r_1,\dots,r_K\in\mathbb{R}$. The standard way of evaluating~\cite{li2023zico, lee2024az,kadlecova2024surprisingly} how well the ranks correspond to the accuracies is to compute Kendall’s $\tau$ (KT, \cite{kendall1938new}) and Spearman’s $\rho$ (SPR, \cite{spearman1961proof}) rank correlation coefficients. Kendall's $\tau$ is given as
\begin{align}
    \tau\coloneq\frac{n_c-n_d}{\sqrt{n_c+n_d+n_1}\sqrt{n_c+n_d+n_2}},
\end{align}
where $n_c$ denotes the number of concordant pairs $\left(\text{acc}_k, r_k\right)$, $n_d$ the number of discordant pairs and $n_1$ (resp. $n_2$) denote the number of ties in $\text{acc}_k$ only (resp. in $r_k$ only). In our context, (KT) relates after some normalization to a probability that two randomly chosen network rankings $r_k, r_l$ are ordered correctly according to their accuracies $\text{acc}_k, \text{acc}_l$. In contrast, (SPR) is defined to account more for outliers, i.e. it tends to put a large penalization if there are some networks for which the difference between the rank of the accuracy $\text{acc}_k$ and the rank $r_k$ is large as it is just the classical correlation coefficient however applied on the orders of the assessed quantities.

\paragraph{Normalized Discounted Cumulative Gain.}
Both (KT) and (SPR) are good evaluation metrics when our interest lies in comparison of ranking proxies when \textit{all networks involved are of the same importance}. However, in NAS we are primarily interested if a ranking helps us to pick best networks from a given collection and less interested how well the ranking compares two networks with low accuracy. 
%An extreme approach how to put emphasize only on the networks with high accuracy is to evaluate two rankings (as is often done) by comparing the two accuracies of networks being ranked highest by the respective systems, however such comparison suffers from large uncertainty by the fact that only a single network is used to evaluate the whole collection.
%We are  motivated to introduce an evaluation metric that inflates the importance of the networks with high performance while keeping a non-trivial number of the networks in the play. We propose the following.
Finding inspiration in information retrieval where one measures the quality of a system that retrieves resources relevant to an input query, we propose to use Normalized Discounted Cumulative Gain (nDCG)~\cite{burges2005learning} as a more relevant metric to measure quality of TF-NAS proxies. The nDCG metric is defined with a key requirement that highly relevant documents are more valuable when they appear earlier in the search engine results (i.e., in higher-ranking positions). This requirement can be reformulated for the task of neural architecture search as \textit{networks with high accuracies are more valuable when they appear on higher-ranking positions}. 

We thus define the metric as follows: first we order the networks by their ranking $r_1,\dots, r_K$ so that $r_{k_1}=K, r_{k_2}=K-1,\dots$ and we compute
\begin{align}
    \text{nDCG}_{P}\coloneq\frac{1}{Z}\left(\sum_{j=1}^P\frac{2^{\text{acc}_{k_j}}-1}{\log_2\left(1+j\right)}\right),
    \label{eq:gain}
\end{align}
where $P\in\mathbb{N}$ is a parameter determining how many top-ranked networks do we consider (e.g. it corresponds to the population size in an evolution algorithm) and $Z$ is a normalization factor that represents the ideal discounted cumulative gain so that nDCG is equal to one for a perfect fit. 

The higher nDCG the better the ranking is as it is a weighted average of transformed top-ranked accuracies. In \cite{yining2013theoretical} it is shown that the choice of the discount factor of given by inverse logarithm is a good choice as it nDCG then well separates different ranking systems.
%\footnote{One may also consider other discount factors. Assume our architecture search proceeds in iterations: we rank all available networks and we train one epoch for only $N_0$ top-ranked of them; in each following iteration $i$ we recompute the ranking, again filter $N_i$ top-ranked models and continue with training of one epoch. At any point of the search we could then easily estimate accuracy of the final model if the discount would correspond to the probability that a network at given ranking position will be kept even in further iterations.}
\footnote{In case of ties we take random ordering within the groups (parameter \texttt{ignore\_ties=True} in the scikit-learn implementation).}



\paragraph{Toy Example.} Let us briefly demonstrate the weak ability of (KT) and (SPR) to distinguish rankings that poorly discriminate networks with high accuracy. Suppose a collection of $10$ networks with their validation accuracies 
$100, 90, \cdots, 10$ is given and our aim is to rank them from worst to the best. We evaluate two different toy rankings: a) ranking $\texttt{damaged\_top}$ which perfectly fits the accuracies, only the two top networks are swapped with the third and fourth, b) ranking $\texttt{perfect\_top}$ which fits perfectly for the top-performing networks, however, now the order of the four worst networks is reversed (see Figure~\ref{fig:toy_example_raking}). The $\texttt{damaged\_top}$ ranking is an example that should be evaluated worse for the architecture search task compared to $\texttt{perfect\_top}$ as in the first case the best networks are not placed first, while in the second they are. On the other hand, even though $\texttt{perfect\_top}$ is not a perfect fit, it still discriminates the top networks ideally and o\textit{nly struggles for networks with low accuracy}, which are not interesting for NAS which is after the best networks. From the correlation perspectives of (KT), (SPR) $\texttt{damaged\_top}$ is better than $\texttt{perfect\_top}$, therefore, if we rely only on these two evaluation metrics we prefer ranking that is worse for the architecture search task as the top networks are ranked worse. On the other hand, for $\texttt{perfect\_top}$ we obtain ideal $\text{nDCG}_{5}$ while it drops to 0.5 for $\texttt{damaged\_top}$. We conclude that using $\text{nDCG}_{5}$ we choose ranking that has higher discriminatory power for good networks. 

We also note that from statistical perspective the (KT) and (SPR) of $\texttt{damaged\_top}$ is significantly higher than of a random ranking\footnote{That is, the $p$-value for the hypothesis that $\texttt{damaged\_top}$ is assigned independently of the accuracies is below 0.005.}, and therefore we'd conclude that $\texttt{damaged\_top}$ is not independent of ground truth accuracy. On the other hand, when we perform the same statistical test on $\texttt{damaged\_top}$ using $\text{nDCG}_{5}$, we conclude that the $\texttt{damaged\_top}$ is not significantly better than ranking networks randomly\footnote{Running 1000 samples on a random ranking we obtain that $\texttt{damaged\_top}$ has $\text{nDCG}_{5}$ around 75th percentile of such random evaluations and it is not significantly better than a random ranking.}.


