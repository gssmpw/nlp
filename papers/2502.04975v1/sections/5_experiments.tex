\section{Experiments}
\label{sec:experiments}


\subsection{Ranking aggregation}
\label{subsec:ranking}


In addition to using the single ranking of \cref{eq:vkdnwrank}, we also experiment with multiple rankings  (as in~\cite{li2023zico, lee2024az, kadlecova2024surprisingly}) to order network architectures by their accuracy. We use two different options: non-linear and model-driven aggregation.

\paragraph{Non-linear aggregation.} The aggregation~\cite{lee2024az} uses multiplication to combine multiple ranks into a single one, which means a network is highly-ranked if and only if it is highly-ranked in \textit{all} subsidiary rankings, keeping their influence balanced. That is, denoting rankings $\rank_1,\dots, \rank_m$ for some $m\in\mathbb{N}$ we define the aggregated ranking
\begin{align}
    \rank_{\agg}(f):=\log{\Pi_{j=1}^m\rank_j(f)}
\end{align}
for each network $f$. This aggregation is therefore possible only in the context of a given network collection, such as in evolutionary search. 

In our case, $\text{VKDNW}_{\text{agg}}$ aggregates these five proxies:
\begin{itemize}
    \item $\text{VKDNW}_{\text{single}}$ (V) ranking (\cref{eq:vkdnwrank}),
    \item Jacov (J)~\cite{abdelfattah2021zero} measures the activations correlation when exposed to various inputs, %and lower correlation indicates more ability to distinguish between different inputs which in turn associates with higher accuracy,
    \item Expressivity (E)~\cite{lee2024az} assesses isotropy and uniformity of the features distribution across all orientations, %and with high isotropy the features are less correlated resulting in better storing capacity of the network
    \item Trainability (T)~\cite{lee2024az} captures ability of the network to keep stable gradient propagation between the layers by inspecting the spectrum of the Jacobian matrix,
    \item FLOPs (F) is the number of FLOPs for one forward pass.
\end{itemize}

\paragraph{Model-driven aggregation.}
When accuracies of a sufficient number of architectures in the search space are known, 
model-driven aggregation can be used to train a regression model to combine individual rankings. The trained model is then used to predict accuracy for unseen networks in the same search space. We evaluate three different models:
\begin{itemize}
    \item $\text{VKDNW}_{m}$ where the eigenvalues $\lambda_k$ of the FIM matrix are used directly as features in companion with $\aleph$ (number of trainable layers) and FLOPs to allow a more complex proxy of the diversity of the eigenvalues and the network complexity than the simple entropy \cref{eq:vkdnw},
    \item $\text{(VKDNW+ZCS)}_{m}$ where we additionally include all other zero-cost scores available (see Table \ref{tab:table1}),
    \item $\text{(VKDNW+ZCS+GRAF)}_{m}$ where we add network graph features from \cite{kadlecova2024surprisingly}. 
%These are computed based on the graph representation of the network: a) number of times a given operation is used in the network, b) minimum/maximum path length from input to the output node using only specified operations, c) input/output degree of the output/input node counting only specified operations, d) average input/output degree of intermediate nodes counting only specified operations.
\end{itemize}


\subsection{Results}
\label{subsec:results}

We have conducted experiments in the NAS-Bench-201 \cite{dong2020bench} and MobileNetV2 \cite{lin2021zen, sandler2018mobilenetv2} search spaces. To obtain easily comparable results, we used 64 randomly generated input images to compute our score as in \cite{lee2024az}. For the methods that rely on the knowledge of true labels, input data from the respective datasets were used.

\input{tables/mobileNetQuantitative}

\paragraph{NAS-Bench-201.} The dataset consists of 15,625 networks for which validation accuracies for CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning} and ImageNet16-120 \cite{chrabaszcz2017downsampled} after training for 200 epochs are provided. The networks are characterized by unique cell structures comprising of several types of operation choices. As one of the possible choices is zero operation, it's possible that some computation edges don't receive any input or cannot propagate their results to the output, leading to same computation graphs for different architectures, thus duplicating networks. Following the practice from NAS-Bench-101 \cite{ying2019bench}, we report results on 9,445 unique structures (also in \cite{mehrotra2021bench, kadlecova2024surprisingly}) and we refer the reader to Supplementary material for results on all networks. 
We measure proxies performance via Kendallâ€™s $\tau$ (KT) and Spearman's $\rho$ (SPR) correlations with validation accuracies together with Normalized Discounted Cumulative Gain ($\text{nDCG}_{1000}$, we write nDCG for short) from \cref{sec:evaluationmetrics}, averaged over 5 independent runs. For the model-driven aggregation, we trained a random forest model on 1024 networks for 100 iterations, and used the rest for testing.
\input{figures/boxplot}
In \Cref{tab:table1}, we compare our method with existing NAS methods and show  that $\text{VKDNW}_{\text{agg}}$ outperforms all methods with a significant margin in all three metrics. The benefit of our method is two-fold: a) it identifies the best networks due to its high performance in $\text{nDCG}$, which is the desirable property in NAS b) the ranking is consistent across the whole network search space due to high KT and SPR correlations. Note that we can only make these observations by combining standard correlation metrics and our newly proposed $\text{nDCG}$. We also show that $\text{VKDNW}_{\text{single}}$ outperforms all other single-rank proxies in all metrics on ImageNet16-120, in SPR on all three datasets, while being among the highest on \mbox{CIFAR-10 and CIFAR-100}. 

\paragraph{MobileNetV2.}
We search for the best network configuration in the MobileNetV2 space~\cite{sandler2018mobilenetv2}, while constraining the model size to approximately 450M FLOPs. We ran 100,000 iterations of the evolutionary search algorithm~\cite{lee2024az} for approximately 10 hours, meaning 100,000 different architectures were evaluated. We then took the best network from the search and trained it for 480 epochs on  ImageNet-1K~\cite{deng2009imagenet}, using the same hyper-parameter setting as in \cite{lee2024az, li2023zico}. The final training of the model took 7 days on 8xNVidia A100 GPUs.

As seen in \Cref{tab:mobileNertQuantiative}, our method $\text{VKDNW}_{\text{agg}}$ outperforms all prior approaches -- even train-based approaches (denoted MS and OS) that incur much higher computational costs for the search.

\input{tables/ablationAggComponentsSmall}

\subsection{Ablations}
\label{subsec:ablation}



We ablate our method in the NAS-Bench-201 search space \cite{dong2020bench} on ImageNet16-120 \cite{chrabaszcz2017downsampled} validation accuracies.
\paragraph{Orthogonality of VKDNW.} Our score VKDNW has a desirable property that it is based on information orthogonal to the size of the network: in Figure \ref{fig:boxplot_per_trainable_layers} we can see that unlike previous work, VKDNW is not correlated with the network size measured by $\aleph$ (number of trainable layers). We believe this property is key when stepping into much larger search spaces, however components of the previous state-of-the-art method AZ-NAS lack such a property (also \cref{fig:boxplot_per_trainable_layers} and \cref{tab:kendall_by_trainable_layers}). We conjecture that improvement in the key metrics by VKDNW is caused by this orthogonality feature. In Table \ref{tab:table1} we also provide comparison to previous model-driven method and show that adding our feature significantly improves the ranking in all considered metrics.
\input{tables/ablationTrainableLayers}



\paragraph{Components of aggregated rank.} Our single-rank variant $\text{VKDNW}_{\text{single}}$ is the strongest component of the aggregated $\text{VKDNW}_{\text{agg}}$ as can be seen in Table \ref{tab:ablationComponentsSmall} where it outperforms rest of the rankings with especially large margin in nDCG, thus, it strongly discriminates high-accuracy networks compared to others. 

\input{tables/ablationRandomVsReal}
\paragraph{Random or real input.} As our method is computed using generated random data (white noise) it does not rely on any real dataset and is therefore applicable also in situations where no reliable data are available. Table \ref{tab:real_vs_random} shows that by relying just on random data we do not lose any performance with respect to all key metrics. Moreover, the performance remains relatively stable across different batch sizes. We chose batch size 64 as larger batch size does not bring better results.

Due to lack of space, we kindly refer the reader to Supplementary material for further ablations.




