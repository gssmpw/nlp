\clearpage
%\setcounter{page}{1}
%\maketitlesupplementary
\twocolumn[
\centering
\Large
\vspace{0.5em}Supplementary Material \\
\vspace{1.0em}
]

In the supplement material, we elaborate our formal arguments and provide additional results and ablations.

\section{Fisher Information}
\label{suppl:fim}

In this section, we provide a more detailed inspection on the Fisher Information matrix (FIM) in the context of neural networks as an extension of \cref{subsec:fim}. The \Cref{subsec:cramer-rao} extends some results from the main paper, the \Cref{subsec:ngd} provides another motivation on why the FIM should be considered as a tool for analysis of neural networks and finally \Cref{subsec:montecarlo} summarizes terminology issues within the community. 

\subsection{Cramér-Rao bound}
\label{subsec:cramer-rao}
Consider the same setting as in \cref{subsec:fim}, that is a deep network $f$ is given with the unknown (deterministic) weight vector $\theta\in\mathbb{R}^p$ for some parameters $p\in\mathbb{N}$, whose estimation is the subject of our interest. Take any estimator $\Hat{\theta}_n$ that is computed from $n$ independently-drawn input images and which is unbiased, i.e.
\begin{align}
    \mathbb{E}\Hat{\theta}_n=\theta.
    \label{supp:unbiased}
\end{align}
Then we have the lower bound for the variance matrix of $\Hat{\theta}_n$ given as
\begin{align}
    \text{Var}\left(\Hat{\theta}_n\right)\geq \frac{1}{n}F^{-1}(\theta),
    \label{supp:cramer_rao}
\end{align}
which in turn gives also an estimate for the diagonal elements
\begin{align}
    \left(\text{Var}\left(\Hat{\theta}_n\right)\right)_{jj}\geq \frac{1}{n}\left(F^{-1}(\theta)\right)_{jj},
    \label{supp:cramer_rao_diag}
\end{align}
where we use the standard notation $A_{ij}$ for the entry at the position $i,j$ for any matrix $A$.

We now show the relation between \eqref{supp:cramer_rao} and mean-square error of the weight estimator. First, consider a weight $\theta(j)$ for some $j\in\lbrace 1,\dots, p\rbrace$. Then we shall write $\theta(j)=e_j^T\theta$, where $e_j$ is the $j$th unit vector consisting only of zeros and single one at the $j$th position: $e_j=(0,\dots,1,\dots, 0)$. Now recall that that if a random $d$-dimensional vector $X$ has a variance matrix $\text{Var}\left( X\right)$ and $e\in\mathbb{R}^d$ then the linear combination $e^TX$ has the variance
\begin{align}
    \text{Var}\left(e^TX\right)=e\text{Var}\left( X\right)e^T,
\end{align}
from which it easily follows that the variance of the random scalar $\Hat{\theta}_n(j)$ is
\begin{align}
    \text{Var}\left( \Hat{\theta}_n(j)\right)=&\text{Var}\left( e_j^T\Hat{\theta}_n\right)\nonumber\\
    =&e_j\text{Var}\left( \Hat{\theta}_n\right)e_j^T\nonumber\\
    =&\left(\text{Var}\left( \Hat{\theta}_n\right)\right)_{jj}.
    \label{supp:variance_diagonal}
\end{align}
Next, we use the bias-variance decomposition of the mean-square error: if $\Hat{X}$ is an estimator of an unknown scalar value $X\in\mathbb{R}$, then
\begin{align}
    \mathbb{E}\left(\Hat{X}-X\right)^2=&\mathbb{E}\left(\Hat{X}-\mathbb{E}\Hat{X}+\mathbb{E}\Hat{X}-X\right)^2\nonumber\\
    =&\mathbb{E}\left(\Hat{X}-\mathbb{E}\Hat{X}\right)^2+2\mathbb{E}\left(\Hat{X}-\mathbb{E}\Hat{X}\right)\mathbb{E}\left(\Hat{X}-X\right)\nonumber\\
    +&\mathbb{E}\left(\mathbb{E}\Hat{X}-X\right)^2\nonumber\\
    =&\text{Var}\Hat{X}+\left(\mathbb{E}\Hat{X}-X\right)^2\nonumber\\
    =&\text{Var}\Hat{X}+\left(\text{Bias}\hat{X}\right)^2.
    \label{supp:tradeoff}
\end{align}
Combining \eqref{supp:unbiased} with \eqref{supp:variance_diagonal}, \eqref{supp:tradeoff} and the Cramér-Rao bound \eqref{supp:cramer_rao} we obtain
\begin{align}
    \mathbb{E}\left( \Hat{\theta}_n(j)-\theta(j)\right)^2=&\text{Var}\left({\Hat{\theta}_n(j)}\right)+\left(\text{Bias}\left(\Hat{\theta}_n(j)\right)\right)^2\nonumber\\
    =&\text{Var}\left({\Hat{\theta}_n(j)}\right)\nonumber\\
    =&\left(\text{Var}\left( \Hat{\theta}_n\right)\right)_{jj}\nonumber\\
    \geq&\frac{1}{n}\left(F^{-1}(\theta)\right)_{jj}.
\end{align}
Summing now over all indices $j$ we finally obtain a lower bound of for the mean-square error of the entire weight vector $\Hat{\theta}_n$
\begin{align}
    \mathbb{E}\|\theta_1-\theta_2\|^2=&\sum_{j=1}^p\mathbb{E}\left( \Hat{\theta}_n(j)-\theta(j)\right)^2\nonumber\\
    \geq&\frac{1}{n}\sum_{j=1}^p\left(F^{-1}(\theta)\right)_{jj}.
    \label{supp:mean_square}
\end{align}
The quantity on the right-hand-side of \eqref{supp:mean_square} is the trace of the matrix $F^{-1}(\theta)$ and it coincides with sum of the eigenvalues of $F^{-1}(\theta)$, which are just the reciprocals of the eigenvalues of $F(\theta)$ \cite{leon2006linear}. 

We have shown that eigenvalues of the FIM determine the least-possible mean-square error for any unbiased estimator of the network weight vector $\theta$ and its components. The case of a biased estimator is more delicate and we kindly refer the reader to \cite{frieden2010exploratory}.

\subsection{Natural Gradient Descent}
\label{subsec:ngd}
Natural Gradient Descent (see \cite{martens2020new}) is an improvement of the classical Stochastic Gradient Descent that is proven to have faster and more stable convergence, but for the price of significantly increased computation costs. In Natural Gradient Descent, the weight updates are governed by a transformed loss gradient as
\begin{align}
    \theta_{n+1}=\theta_n-F^{-1}(\theta_n)\nabla_\theta \mathcal{L}(\theta_n),
\end{align}
where $F^{-1}(\theta)$ is the inverse of the FIM and $\mathcal{L}(\theta)$ is the loss function. 

We also take the steepest descent direction of the loss function, but now we do not measure the distance in the space of weights by means of the Euclidean distance but we adjust the curvature by measuring the KL-divergence of the output distributions. In other words, Natural Gradient Descent is just what happens to Stochastic Gradient Descent if we say that two weight vectors $\theta_1, \theta_2$ are close to each other if
\begin{align}
    D_{KL}&(\sigma_{\theta_1}(\cdot|x), \sigma_{\theta_2}(\cdot|x))
\end{align}
is small, in contrast to the usual case when we consider $\|\theta_1-\theta_2\|$ instead.  And again similarly as above, after inspecting the eigenvalues of the FIM we can conclude that the more different the eigenvalues are the more difficult is to train the model as the weight updates are much larger in some directions than in others. 

Even though we later use the FIM in the applications where the networks have been trained with the classical Stochastic Gradient Descent, the curvature given by the FIM still provides a valuable information -- if the network is more difficult to train using Natural Gradient Descent, it's unlikely that when using a simpler optimisation method, the network would yield stronger performance after training.

\subsection{Monte Carlo estimation of the Fisher Information Matrix (FIM)}
\label{subsec:montecarlo}

We now follow \cite{kunstner2019limitations} and outline details of the common misconception in the terminology within the community that might lead to incorrect estimation of the FIM. 

In our setting, the FIM is given as
\begin{align}
\label{supp:fim}
F(\theta)\coloneq\mathbb{E}\left[\nabla_{\theta}\sigma_\theta(c\,|\,x)\,\nabla_{\theta}\sigma_\theta(c\,|\,x)^T\right]\in\mathbb{R}^{p\times p},
\end{align}
where the expectation $\mathbb{E}$ is taken with respect to the join distribution of the image-label pair $(x,c)$. Recall that the joint distribution can be decomposed into the prior distribution for $x$, usually unknown, and the conditional distribution distribution for the label $c$ given $x$
\begin{align}
    \sigma_\theta(c\,|\,x)=\frac{\exp\left({\Psi_c(x, \theta)}\right)}{\sum_{d=1}^C\exp\left({\Psi_d(x, \theta)}\right)}, \quad c=1,\dots, C
\end{align}
where $\Psi(x,\theta)\in\mathbb{R}^C=\left( \Psi_1(x,\theta),\dots ,\Psi_C(x,\theta)\right)$ is the network output (logits) given the weight vector $\theta\in\mathbb{R}^p$, i.e. the network weights. We might deal with missing information on the prior distribution of $x$ by simply replacing it with the empirical distribution given by independently drawn examples $x_1,\dots, x_n$ which then yields a Monte Carlo estimate
\begin{align}
\Hat{F}(\theta)&\coloneq\frac{1}{n}\sum_{n=1}^N\mathbb{E}_{\sigma_\theta}\left[\nabla_{\theta}\sigma_\theta(c\,|\,x_n)\,\nabla_{\theta}\sigma_\theta(c|\,x_n)^T\right]
    \label{supp:fim_empirical}
\end{align}
where $\mathbb{E}_{\sigma_\theta}$ now denotes the expectation with respect to the model prediction $\sigma_\theta$, which is in the statistical community denoted as the empirical FIM. From the strong law of large numbers it follows that the empirical FIM converges to the FIM almost surely as the number of samples $n$ tends to infinity. Therefore, it is reasonable to replace \eqref{supp:fim} in the applications by \eqref{supp:fim_empirical}.

However, in some methods (see \cite{kunstner2019limitations} and references therein) the expectation in \eqref{supp:fim_empirical} with respect to the model prediction $\sigma_\theta$ is often replaced by the empirical distribution $\sigma$ of the labels given the images which leads to a different definition
\begin{align}
G(\theta)&\coloneq\frac{1}{n}\sum_{n=1}^N\mathbb{E}_{\sigma}\left[\nabla_{\theta}\sigma_\theta(c\,|\,x_n)\,\nabla_{\theta}\sigma_\theta(c|\,x_n)^T\right]\nonumber\\
&=\frac{1}{n}\sum_{n=1}^N\left[\nabla_{\theta}\sigma_\theta(c_n\,|\,x_n)\,\nabla_{\theta}\sigma_\theta(c_n|\,x_n)^T\right],
    \label{supp:fim_empirical_wrong}
\end{align}
where now $(x_j\,c_j)$ are the observed image-label pairs. The difference between \eqref{supp:fim_empirical} and \eqref{supp:fim_empirical_wrong} is that in the former we sum the multiplied gradients over all categories $c$ weighted by the network-predicted probability, while in the later we use only single class as if the network correctly classified the sample with zero error. At the initialization stage, the network prediction is however far from the ground-truth distribution and therefore $G(\theta)$ is indeed very different from the Monte Carlo approximation $\Hat{F}(\theta)$ (and also from the FIM $F(\theta)$ itself). In our method we used $\Hat{F}(\theta)$.

\section{Experiments}

\input{tables/table1_unfiltered.tex}

\paragraph{NAS-Bench-201.}

In \cref{tab:table1}, we provide results for the NAS-Bench-201 architecture search space where we adopted the practice of NAS-Bench-101 \cite{ying2019bench, kadlecova2024surprisingly, mehrotra2021bench} where only unique graph structures are considered. As we described in  \cref{sec:experiments}, the entire search space in NAS-Bench-201 contains also networks where some computation edges don't receive any input or their output cannot be propagated through the network due to the existence of zero operation nodes. By filtering these networks, the number of architectures drops from 15,625 to 9,445 unique architectures. We argue that this is indeed good practice as networks with unreachable parameters should not be used in practice as the energy costs rise without improved performance. Moreover, many of the ranking scores (such as FLOPs or \#params) do not make sense in such cases, because parameters/operations are not used in network output yet they are still included in these metrics. 

For the sake of completeness however, in \Cref{tab:table1_unfiltered} we provide results of our experiments on full NAS-Bench-201 search space, using all 15,625 architectures. We can see that both $\text{VKDNW}_{\text{single}}$ and $\text{VKDNW}_{\text{agg}}$ outperform all other simple rankings in all metrics on CIFAR-100 and ImageNet16-120. On CIFAR-10 dataset AZ-NAS\cite{lee2024az} achieves similar Kendall's $\tau$ and Spearman's $\rho$ correlations as $\text{VKDNW}_{\text{agg}}$, however $\text{VKDNW}_{\text{agg}}$ leads in nDCG with a considerable margin.

\paragraph{MobileNetV2.}
In this experiment, we search for the best network configuration in the MobileNetV2 space~\cite{sandler2018mobilenetv2}. The search space is much larger as it consists of different architectures with inverted residual blocks, where depth, width, and expansion ratio of the blocks is altered. We constrained the model size to approximately 450M FLOPs and number of layers to 14. We adapt the evolutionary search algorithm \cite{lee2024az} by replacing the objective function in the search algorithm with our $\text{VKDNW}_{\text{agg}}$.
We then ran 100,000 iterations of the algorithm, always keeping top 1,024 best architectures, measured by $\text{VKDNW}_{\text{agg}}$. In each iteration, one mutation operation randomly changes one element in one of the top 1,024 architectures, and the newly created architecture is again ranked using $\text{VKDNW}_{\text{agg}}$. As a result, 100,000 iterations of architecture evaluations were made in the search process, leaving us with a shortlist of 1,024 architectures in the end.

Out of these final 1,024 architectures, we then again picked the one with the highest $\text{VKDNW}_{\text{agg}}$ rank and trained it for 480 epochs on ImageNet-1K~\cite{deng2009imagenet} in the same teacher-student setting as \cite{li2023zico, lee2024az}. We used vanilla SGD optimizer with LR=0.2 and single-cycle cosine learning rate schedule. The final training of the model took 7 days on 8xNVidia A100 GPUs.

\section{Ablations}

\input{tables/ablationFIMDimension}
\paragraph{Fisher Information matrix size.}
In \cref{tab:fim_dimension}, we evaluate our method with varying number of trainable layers considered in the computation of the FIM (see \cref{eq:fim}). We can see that initial 16 layers of the network already carry enough information, even comparable to when we use 256 layers. In our method, we set this parameter to 128 to maximize for (nDCG) while keeping other metrics high.

\input{tables/ablationIndexChoice}
\paragraph{Parameter sampling policy.}
To make the dimension of the FIM feasible for computation of eigenvalues, we use only a small portion of the network weights. More specifically, instead of taking the full matrix of dimension $p$ (number of trainable parameters), we only sample one weight from each trainable layer from the first 128 layers and compute the FIM as if the network did not have any other parameters. In \Cref{tab:fim_index_choice}, we compare performance of our method $\text{VKDNW}_{\text{single}}$ as we vary the number of weights per layer and their sampling policy. We can see that the performance as measured by (nDCG) is roughly the same when taking anything between one and four weights per layer, and then starts to slowly decrease with a higher number of weights per layer. Secondly, our method is robust against choice of the policy as the performance for the case of one weight per layer with changing position of the weight within each layer does not change significantly. To further show that we do not lose any performance when dealing only with limited number of initial layers, we show in \cref{tab:fim_dimension} that our method is also robust against change of number of considered layers (the highest number of layers we tested was 256 as the number of larger networks in NAS-Bench-201 is small).

\input{figures/boxplot_params}
\paragraph{Orthogonality of VKDNW.}

Our score VKDNW is based on information orthogonal to the size of the network: in \Cref{fig:boxplot_per_trainable_layers}, we show that unlike previous work, VKDNW is not correlated with the network size measured by $\aleph$ (number of trainable layers). In \Cref{fig:boxplot_per_trainable_parameters}, we present similar results where we now measure the network size by the number of trainable parameters. We can see that VKDNW keeps the orthogonality property even after change of the size proxy.

\input{tables/ablationAggComponentsFull}
%\input{tables/ablationAggComponentsCorrelation}
\paragraph{Components of the aggregated rank.}
Our aggregated rank $\text{VKDNW}_{\text{agg}}$ combines information from 5 different sources: our $\text{VKDNW}_{\text{single}}$, Jacov, expressivity, trainability and FLOPs (see \cref{subsec:ranking}). In \Cref{tab:ablationComponentsFull}, all $2^5$ combinations of keeping/dropping every of the 5 sources are evaluated on ImageNet16-120. We can see that our ranking $\text{VKDNW}_{\text{single}}$ is the strongest component as it has the highest marginal performance in all three considered metrics. The lowest performance drop is observed for expressivity: without this component the method would even perform better in the (nDCG) metric than the original variant $\text{VKDNW}_{\text{agg}}$. We decided to include expressivity in the final ranking as we optimized for all three metrics (KT), (SPR) and (nDCG) simultaneously.


