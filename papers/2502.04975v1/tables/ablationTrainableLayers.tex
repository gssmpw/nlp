\begin{table}
  \centering
  \small
  \begin{tabular}{l|ccc}
  \hline
  & \multicolumn{3}{c}{(KT)}\\
  & CIFAR-10 & CIFAR-100 & ImageNet16-120 \\  
\hline
VKDNW & \applygradient{0.041}{0}{1}0.041 & \applygradient{0.090}{0}{1}-0.090 & \applygradient{0.107}{0}{1}-0.107 \\
trainability & \applygradient{0.220}{0}{1}0.220 & \applygradient{0.220}{0}{1}0.220 & \applygradient{0.253}{0}{1}0.253 \\
expressivity & \applygradient{0.539}{0}{1}0.539 & \applygradient{0.539}{0}{1}0.539 & \applygradient{0.539}{0}{1}0.539 \\
progressivity & \applygradient{0.398}{0}{1}0.398 & \applygradient{0.398}{0}{1}0.398 & \applygradient{0.385}{0}{1}0.385 \\ \hline
  \end{tabular}%\vspace{-5pt}
  \caption{Kendall's $\tau$ (KT) of $\aleph$ (number of trainable layers) with components of AZ-NAS compared to our new method VKDNW on three datasets on NAS-Bench-201 search space \cite{dong2020bench} and multiple image datasets.%\vspace{-5pt}
  }
    \label{tab:kendall_by_trainable_layers}
\end{table}
