%%%%%%%%% TITLE
%\section*{Supplementary Material for \texttt{NAISR}}
\onecolumn
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thefigure}{S.\arabic{figure}}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thesection}{S.\arabic{section}}
% \captionsetup[figure]{font=small,labelfont=small}
% \captionsetup[table]{font=small,labelfont=small}
\section*{\centering{Supplementary Material for \texttt{LucidAtlas}}}
%\AtBeginEnvironment{tabular}{\small}

\section{Extended Related Work}
\label{supp.sec.supp_related_work}
\paragraph{Epistemic Uncertainty versus Aleatoric Uncertainty.}
Epistemic and aleatoric uncertainties are two different kinds of uncertainties. Epistemic uncertainty relates to model parameters and stems from limited model knowledge, which is reducible with more data or better modeling. Important techniques include the Laplace approximation~\citep{daxberger2021laplace}, Ensembling~\citep{hullermeier2021survey} and MC-Dropout~\citep{gal2016dropout}. Aleatoric uncertainty arises from inherent data randomness and is irreducible. Important techniques includes a line of Bayesian Neural Networks~\citep{stirn2022faithfulheteroscedasticregressionneural, immer2024effectiveheterbayes}. DeepEnsembles~\citep{lakshminarayanan2017deepemsemble} can handle both epistemic and aleatoric uncertainties.

Regarding uncertainty estimation for interpretable models, more attention is paid to epistemic uncertainties. NAMs used ensembling to estimate and decrease model uncertainties~\citep{agarwal2020neural}. LA-NAM used Laplace approximations for uncertainty estimation~\citep{bouchiat2023lanam} with NAMs. In atlas construction, aleatoric uncertainty is especially important when individual differences are large. Capturing aleatoric uncertainty is crucial in medicine to understand population variations. NAMLSS can model aleatoric uncertainty using NAMs to approximate the parameters $\{\theta^{k}\}$ of a data distribution ~\citep{thielmann2024namlss}, as
\begin{small}
\begin{equation}
\theta^{(k)}=h^{(k)}\left(\beta^{(k)}+\sum_{i=1}^{N} f_{i}^{(k)}\left(c_{i}\right)\right) 
\label{supp.eq.namlss}
\end{equation}
\end{small}
where $\theta^{(k)}$ can, for example, be the mean and variance of Gaussian distributions; $\beta^{(k)}$ denotes the parameter-specific intercept and $f_{i}^{(k)}$ represents the feature network for parameter $k$ for the $i$-th feature. \emph{\texttt{LucidAtlas} extends NAMLSS to a more versatile representation, enabling individualized prediction, incorporating prior knowledge, and capturing spatial dependence.}
%~\citep{monteiro2020stochasticsegmentationnetworksmodelling,valiuddin2021improvingaleatoricuncertaintyquantification,Wang_2019,kawashima2020aleatoricuncertaintyestimationusing,Martin_2022,nevin2024deepuqassessingaleatoricuncertainties}


\paragraph{Monotonicity.}
Monotonic neural networks ensure that a network's output changes monotonically with respect to certain inputs. Research has focused on two lines of approaches:  architectures such as Deep Lattice Networks~\citep{you2017deep} that guarantee monotonicity but may lack expressiveness, and heuristic methods such as Certified Monotonic Neural Networks \citep{liu2020certified} that use regularization but can be computationally expensive. Recent advancements, including Constrained Monotonic Neural Networks \citep{runje2023constrained}, aim to balance monotonicity, expressiveness, and efficiency. Additionally, research in normalizing flows~\citep{de2020block} has contributed to developing monotonic functions in neural networks to ensure invertibility. Expressive monotonic neural networks~\citep{kitouni2023mono} are constructed using Lipschitz-constrained neural networks, ensuring monotonicity by design while preserving expressiveness.
\emph{We use the Lpipshitz-contrained neural networks to ensure monotonicity in \texttt{LucidAtlas} to follow prior / domain knowledge.}

\paragraph{Disentangled Representation Learning.}
Disentangled representation learning (DRL) has been explored in a variety of domains, including computer vision~\citep{shoshan2021gan, ding2020guided, zhang2018unsupervised, zhang2018learning, xu2021learning, yang2020dsm}, natural language processing~\citep{john2018disentangled}, and medical image analysis~\citep{chartsias2019disentangled, bercea2022federated}. 

Medical data is typically associated with various covariates which should be taken into account during analyses. Taking~\citep{chu2022disentangled} as an example, when observing a tumor's progression, it is difficult to know whether the variation of a tumor's progression is due to time-varying covariates or due to treatment effects. Therefore, being able to disentangle different effects is highly useful for a representation to promote understanding and to be able to quantify the effect of covariates on observations. \emph{\texttt{LucidAtlas} disentangles covariate effects in terms of their contribution to population trends and uncertainties.}


\paragraph{Explainable Artificial Intelligence.}
The goal of eXplainable Artificial Intelligence (XAI) is to provide human-understandable explanations for the decisions and actions of AI models. Various approaches to XAI have been proposed, including counterfactual inference~\citep{berrevoets2021learning, moraffah2020causal, thiagarajan2020calibrating, chen2022covariate}, attention maps~\citep{zhou2016cvpr, jung2021towards, woo2018cbam}, feature importance~\citep{arik2021tabnet, ribeiro2016should, agarwal2020neural}, and instance retrieval~\citep{Crabbe2021Simplex}. A neural additive model (NAM) \citep{agarwal2020neural, jiao2023naisr} is an important XAI method that achieves interpretability through a linear combination of neural networks, each focusing on a \emph{single} input feature. 
NAISR pioneers the use of NAMs for modeling medical shapes to enable scientific discoveries in the medical domain ~\citep{jiao2023naisr}; however, it does not account for heteroskedasticity in its shape representation and does not consider uncertainties. \emph{\texttt{LucidAtlas} extends this concept by integrating NAMs to construct an atlas that captures population trends and uncertainties with spatial dependencies}.

\begin{comment}
    

\section{Monotonic Neural Networks}
\label{supp.sec.monoNN}
We use expressive monotonic Lipschitz networks proposed in \citep{kitouni2023mono} to introduce prior knowledge by construction. This section introduces the principle of the monotonic neural network from \citep{kitouni2023mono}.


\subsection{Theory Preparation}
The goal of a monotonic network model is to develop a neural network architecture representing a vector-valued function

\begin{equation}
f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{n}, \quad d, n \in \mathbb{N} 
\label{supp.eq.def_f}
\end{equation}

that is provably monotonic in any subset of its inputs. We first define a few ingredients.

\paragraph{Definition of Monotonicity.} Let $\boldsymbol{x} \in \mathbb{R}^{d}, \boldsymbol{x}_{\mathbb{S}} \equiv \mathbf{1}_{\mathbb{S}} \odot \boldsymbol{x}$, as the Hadamard product of $\boldsymbol{x}$ with the indicator vector $\mathbf{1}_{\mathbb{S}}(i)=1$ if $i \in \mathbb{S}$ and 0 otherwise for a subset $\mathbb{S} \subseteq\{1, \cdots, d\}$.
We say that outputs $\mathbb{Q} \subseteq\{1, \cdots, n\}$ of $f$ are monotonically increasing in features $\mathbb{S}$ if

\begin{equation}
f\left(\boldsymbol{x}_{\mathbb{S}}^{\prime}+\boldsymbol{x}_{\overline{\mathbb{S}}}\right)_{i} \leq f\left(\boldsymbol{x}_{\mathbb{S}}+\boldsymbol{x}_{\overline{\mathbb{S}}}\right)_{i} \quad \forall i \in \mathbb{Q} \text { and } \forall \boldsymbol{x}_{\mathbb{S}}^{\prime} \leq \boldsymbol{x}_{\mathbb{S}} 
\label{supp.eq.def_mono}
\end{equation}

where $\overline{\mathbb{S}}$ denotes the complement of $\mathbb{S}$ and the inequality on the right uses the product (or componentwise) order.


\paragraph{Definition of $\operatorname{Lip}^{p}$ function.} $g: \mathbb{R}^{d} \rightarrow \mathbb{R}^{n}$ is $\mathrm{Lip}^{p}$ if it is Lipschitz continuous with respect to the $L^{p}$ norm in every output dimension, i.e.,

\begin{equation}
\|g(\boldsymbol{x})-g(\boldsymbol{y})\|_{\infty} \leq \lambda\|\boldsymbol{x}-\boldsymbol{y}\|_{p}\,, \quad \forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^{n} \,.
\label{supp.eq.lip_p}
\end{equation}



\subsection{Lipschitz Monotonic Networks (LMN)}

We will henceforth and without loss of generality only consider scalar-valued functions $(n=1)$. We start with a model $g(\boldsymbol{x})$ that is Lip ${ }^{1}$ with Lipschitz constant $\lambda$. Note that the choice of $p=1$ is crucial for decoupling the magnitudes of the directional derivatives in the monotonic features.  With a model $g(\boldsymbol{x})$ we can define an architecture with built-in monotonicity by adding a term that has directional derivative $\lambda$ for each coordinate in $\mathbb{S}$ :

\begin{equation}
f(\boldsymbol{x})=g(\boldsymbol{x})+\lambda\left(\mathbf{1}_{\mathbb{S}} \cdot \boldsymbol{x}\right)=g(\boldsymbol{x})+\lambda \sum_{i \in \mathbb{S}} x_{i} \,.
\label{supp.eq.fx_from_gx}
\end{equation}


This residual connection $\lambda\left(\mathbf{1}_{\mathbb{S}} \cdot \boldsymbol{x}\right)$ enforces monotonicity in the input subset $\boldsymbol{x}_{\mathbb{S}}$ :

\begin{equation}
\begin{split}
& \frac{\partial g}{\partial x_{i}} \in[-\lambda, \lambda], \forall i \in \mathbb{N}_{1: n}  \\
\Rightarrow & \frac{\partial f}{\partial x_{i}}=\frac{\partial g}{\partial x_{i}}+\lambda \geq 0 \quad \forall \boldsymbol{x} \in \mathbb{R}^{n}, i \in \mathbb{S} . 
\end{split}
\label{supp.eq.gradient_of_fx}
\end{equation}




\subsection{$\operatorname{Lip}^{p=1}$ Approximations}

Our goal is to construct a universal approximator of $\mathrm{Lip}^{1}$ functions, i.e., we would like the hypothesis class to have two properties:
\begin{itemize}
    \item[1.] It always satisfies Eq.~\eqref{supp.eq.lip_p}, i.e., be $\operatorname{Lip}^{1}$. 
    \item[2.] It is able to fit all possible $\operatorname{Lip}^{1}$ functions. In particular, the bound in Eq.~\eqref{supp.eq.lip_p} needs to be attainable $\forall \boldsymbol{x}, \boldsymbol{y}$.
\end{itemize}





\paragraph{$\operatorname{Lip}^{1}$ Constrained Models.} To satisfy the first requirement, fully connected networks can be Lipschitz bounded by constraining the matrix norm of all weight matrices~\citep{kitouni2023robustmono}. We recursively define the layer $l$ of the fully connected network of depth $D$ with activation $\sigma$ as

\begin{equation}
\boldsymbol{z}^{l}=\sigma\left(\boldsymbol{z}^{l-1}\right) \boldsymbol{W}^{l}+\boldsymbol{b}^{l} 
\label{supp.eq.z_l}
\end{equation}

where $\boldsymbol{z}^{0}=\boldsymbol{x}$ is the input and $f(\boldsymbol{x})=z^{D}$ is the output of the neural network. It follows that $g(\boldsymbol{x})$ satisfies Eq.~\eqref{supp.eq.lip_p} if

\begin{equation}
\prod_{i=1}^{D}\left\|\boldsymbol{W}^{i}\right\|_{1} \leq \lambda 
\label{supp.eq.weight_leq_lambda}
\end{equation}


and $\sigma$ has a Lipschitz constant less than or equal to 1 . There are multiple ways to enforce Eq.~\eqref{supp.eq.weight_leq_lambda}. Two existing possibilities that involve scaling by the operator norm of the weight matrix~\citep{gouk2020regularisationneuralnetworksenforcing} are:

\begin{equation}
\boldsymbol{W}^{i} \rightarrow \boldsymbol{W}^{\prime i}=\lambda^{1 / D} \frac{\boldsymbol{W}^{i}}{\max \left(1,\left\|\boldsymbol{W}^{i}\right\|_{1}\right)} \quad \text { or } \quad W^{i} \rightarrow W^{\prime i}=\frac{\boldsymbol{W}^{i}}{\max \left(1, \lambda^{-1 / D} \cdot\left\|\boldsymbol{W}^{i}\right\|_{1}\right)} \,.
\label{supp.eq.scale12}
\end{equation}


\cite{kitouni2023mono} proposes that the latter variant seems to train slightly better. In order to satisfy Eq.~\eqref{supp.eq.weight_leq_lambda}, it is not necessary to divide the entire matrix by its 1-norm. It is sufficient to ensure that the absolute sum over each column is constrained as:

\begin{equation}
\boldsymbol{W}^{i} \rightarrow \boldsymbol{W}^{\prime i}=\boldsymbol{W}^{i} \operatorname{diag}\left(\frac{1}{\max \left(1, \lambda^{-1 / D} \cdot \sum_{j}\left|W_{j k}^{i}\right|\right)}\right)\,. 
\label{supp.eq.norm_weight}
\end{equation}

With this normalization, we obtain $\operatorname{Lip}^{1}$ constrained models.

\paragraph{Preserving Expressive Power.} Guaranteeing that the model is Lipschitz bounded is not sufficient, it must also able to saturate the bound to be able to model all possible $\operatorname{Lip}^{1}$ functions. Some Lipschitz network architectures, e.g. Miyato et al. (2018), tend to over-constrain the model such that it cannot fit all  $\operatorname{Lip}^{1}$ functions due to gradient attenuation. 

The reason lies in the fact that ReLU, and most other commonly used activations, do not have unit gradient with respect to the inputs over their entire domain. While monotonic element-wise activations like ReLU cannot have unit gradient almost everywhere without being exactly linear, \cite{huster2018limitationslipschitzconstantdefense} explores activations that introduce non-linearities by reordering elements of the input vector. They propose GroupSort as an alternative to point-wise activations, and it is defined as follows:

\begin{equation}
\begin{split}
\sigma_{G}(\boldsymbol{x}) & =\operatorname{sort}_{1: G}\left(\boldsymbol{x}_{1: G}\right)+\operatorname{sort}_{G+1: 2 G}\left(\boldsymbol{x}_{G+1: 2 G}\right)+\ldots \\
& =\sum_{i=0}^{n / G-1} \operatorname{sort}_{i G+1:(i+1) G}\left(\boldsymbol{x}_{i G+1:(i+1) G}\right) 
\end{split}
\label{supp.eq.groupsort}
\end{equation}

where $\boldsymbol{x} \in \mathbb{R}^{n}, \boldsymbol{x}_{i: j}=\mathbf{1}_{i: j} \odot \boldsymbol{x}$, and sort ${ }_{i: j}$ orders the elements of a vector from indices $i$ to $j$ and leaves the other elements in place. This activation sorts an input vector in chunks (groups) of a fixed size $G$. The GroupSort operation has a gradient of unity with respect to every input, giving architectures constrained with Eq.~\eqref{supp.eq.weight_leq_lambda} greatly increased expressive power. \\

In summary, \cite{kitouni2023mono} have constructed a neural network architecture $f(\boldsymbol{x})$ via Eq.~\eqref{supp.eq.fx_from_gx} that can provably approximate all monotonic Lipschitz bounded functions. %The Lipschitz constant of the model can be increased arbitrarily by controlling the parameter $\lambda$ in our construction.
\end{comment}


\section{Method}

\subsection{Notations}
Table~\ref{supp.tab.exp_notations} shows the notations used in this paper. 
\begin{table}[]
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l|l}
\hline
Notations                         & Explanations                                                                          \\ \hline
$y$                               & Observed variable, i.e., target variable to model                                     \\ \hline
$\boldsymbol{c}$                  & A vector containing all $N$ covariates, e.g, $\boldsymbol{c}=[age, weight,...]$       \\ \hline
$f^m(\boldsymbol{c}, x)$ or $f^m$ & Prediction of mean population trend given $\boldsymbol{c}$ at location $x$            \\ \hline
$f^m_i(c_i, x)$ or $f^m_i$       & Additive effects predicted from $i^{th}$ subsnetwork $f_i$ for mean \\ \hline
$f^v(\boldsymbol{c}, x)$ or $f^v$ & Prediction of population variance given $\boldsymbol{c}$ at location $x$              \\ \hline
$f^v_i(c_i, x)$ or $f^v_i$ & \begin{tabular}[c]{@{}l@{}}Additive effects predicted from $i^{th}$ subsnetwork $f_i$ for variance\end{tabular} \\ \hline
$g^m_{i,k}(c_i)$                  & The predicted mean of $c_k$ given $c_i$                                               \\ \hline
$g^v_{i,k}(c_i)$                  & The predicted variance of $c_k$ given $c_i$                                           \\ \hline
$p(y|c_i,x)$                      & Marginalized covariate effects: how $c_i$ affect $y$ at location $x$                  \\ \hline
$\mathrm{E}[y|c_i,x]$             & The expectation of $y$ when $c_i$ and $x$ are fixed                                   \\ \hline
$\mathrm{Var}(y|c_i,x)$           & The variance of $y$ when $c_i$ and $x$ are fixed                                      \\ \hline
\end{tabular}}
\caption{Illustrations of the Notations.}
\label{supp.tab.exp_notations}
\end{table}


\subsection{Expanded Discussion on the Toy Example}
\label{supp.subsec.toy_dataset}
%We investigated this problem with a toy example.
Assuming $c_1$ and $c_2$ are covariates that influence the observed result $y$, a NAM fits well whose subnetworks capture $f_1(c_1) = \sin(c_1)$ and $f_2(c_2) = c_2$ and thus approximate $y$ with $y \approx f(c_1, c_2) + \epsilon = f_1(c_1) + f_2(c_2) + \epsilon$, where $\epsilon$ is  Gaussian noise with mean zero.

If we want to interpret the population trend of $y$ with only $c_1$, we need to marginalize $c_2$ out as
\begin{small}
\begin{equation}
\begin{split}
F_1(c_1)&=\int_{-\infty}^{\infty} [f_1(c_1) + f(c_2)]p(c_2|c_1) \dif c_2 \\
&=\underbrace{f_1(c_1)}_{\text{Interpretation from NAMs}}+\underbrace{\int_{-\infty}^{\infty} f_2(c_2)p(c_2|c_1) \dif c_2}_{\text{Interpretation from Dependence: } :=h_1(c_1)} \\
&= \sin(c_1) + \int_{-\infty}^{\infty} c_2p(c_2|c_1) \dif c_2
\end{split}
\label{supp.eq.source_of_intr1}
\end{equation}
\end{small}

where $h_1(c_1)$ measures how the dependence between $c_1$ and $c_2$ influences the marginalization $F_1(c_1)$. We can see from Eq.~\ref{supp.eq.source_of_intr1} that $F_1(c_1)$ is composed of the interpretation from the NAM's subnetwork plus the interpretation from the dependence between $c_1$ and $c_2$ as $h_1(c_1)$.

If we want to interpret the population trend of $y$ with only $c_2$, we need to marginalize $c_1$ out as
\begin{small}
\begin{equation}
\begin{split}
F_2(c_2)&=\int_{-\infty}^{\infty} [f_2(c_2) + f(c_1)]p(c_1|c_2) \dif c_1 \\
&=\underbrace{f_2(c_2)}_{\text{Interpretation from NAMs}}+\underbrace{\int_{-\infty}^{\infty} f_1(c_1)p(c_1|c_2) \dif c_1}_{\text{Interpretation from Dependence: } :=h_2(c_2)} \\
&= c_2 + \int_{-\infty}^{\infty} \sin(c_1)p(c_1|c_2) \dif c_1\,.
\end{split}
\label{supp.eq.source_of_intr2}
\end{equation}
\end{small}



\textbf{If $c_1$ and $c_2$ are \textit{independent}}, \\

\begin{small}
\begin{equation}
\begin{split}
h_1(c_1)=\int_{-\infty}^{\infty} f_2(c_2)p(c_2|c_1) \dif c_2 =\int_{-\infty}^{\infty} f_2(c_2)p(c_2) \dif c_2=\mathrm{E}_{p(c_2)}[f_2(c_2)]= f_2(\mathrm{E}[c_2])=\mathrm{E}[c_2]=constant\,, 
\end{split}
\label{supp.eq.h1}
\end{equation}
\end{small}



\begin{small}
\begin{equation}
\begin{split}
h_2(c_2)=\int_{-\infty}^{\infty} f_1(c_1)p(c_1|c_2) \dif c_1 =\int_{-\infty}^{\infty} f_1(c_1)p(c_1) \dif c_1=\mathrm{E}_{p(c_1)}[f_1(c_1)]= f_1(\mathrm{E}[c_1])=\sin(\mathrm{E}[c_1])=constant\,. 
\end{split}
\label{supp.eq.h2}
\end{equation}
\end{small}


Thus

\begin{small}
\begin{equation}
\begin{split}
&F_1(c_1)=\sin(c_1) + \mathrm{E}[c_2] \\
&F_2(c_2)=c_2 + \sin(\mathrm{E}[c_1]) 
\end{split}
\label{supp.eq.F1F2}
\end{equation}
\end{small}

which means the marginalization is the actual covariate disentanglement in Sec.~\ref{sec.dist_cov_effects} plus a constant. \\


\textbf{If $c_1$ and $c_2$ are \textit{dependent}} 

$h(c_1)$ is a function of $c_1$ which is controlled by the dependence between $c_1$ and $c_2$. 


For example, assume the relationship between  $c_1$ and $c_2$ are at one extreme of dependence in the sense that $c_2 $ is a deterministic function of $c_1$  as 

\begin{small}
\begin{equation}
c_2 = \exp(c_1)\,.
\label{supp.eq.exp_c1}
\end{equation}
\end{small}

Then
\begin{small}
\begin{equation}
\begin{split}
F_1(c_1) &= \sin(c_1) + \int_{-\infty}^{\infty} c_2p(c_2|c_1) \dif c_2 \\
&=\sin(c_1) + \exp(c_1)
\end{split}
\label{supp.eq.F1_with_corr}
\end{equation}
\end{small}


\begin{small}
\begin{equation}
\begin{split}
F_2(c_2) &= c_2 + \int_{-\infty}^{\infty} \sin(c_1)p(c_1|c_2) \dif c_1\\
&= c_2 + \sin(\log(x_2))\,.
\end{split}
\label{supp.eq.F2_with_corr}
\end{equation}
\end{small}


Therefore, considering the relationship between $c_1$ and $c_2$ is crucial when using either covariate to interpret the population trend.

In summary, disentangled covariate effects of NAMs, combined with those effects contributed by covariate dependence, shape human-understandable explanations aligned with population trends. \emph{While ignoring potential dependencies in NAMs may not impact prediction performance, it can result in ambiguous or misleading interpretations when analyzing population trends.} 






\subsection{Covariate Marginalization}
\label{supp.subsubsec.how_marg}
This section introduces our proposed marginalization approach to improve the trustworthiness of NAMs when trying to understand the dependency of a covariate on the response.  The dependency of covariates can be modeled with a multivariate Gaussian distribution as $p(\boldsymbol{c}|c_i)=\mathcal{N}(\hat{\boldsymbol{\mu}}(c_i), \hat{\boldsymbol{\Sigma}}(c_i))$ where $\hat{\boldsymbol{\mu}}(c_i)$ represents the mean vector and  $\hat{\boldsymbol{\Sigma}}(c_i)$ the covariance matrix conditioned on $c_i$. From $p(\boldsymbol{c}|c_i)$, one can extract the distribution of an individual covariate $c_k$ conditioned on $c_i$, as $p(c_k|c_i)=\mathcal{N}(\hat{\mu}_k(c_i), \hat{\Sigma}_{k,k}(c_i))$, e.g., how age $c_i$ determines weight $c_k$. One can also extract the joint distribution of $c_{K_1}$ and $c_{K_2}$ conditioned on $c_i$ as $p(c_{K_1}, c_{K_2}|c_i)$ from $p(\boldsymbol{c}|c_i)$, i.e.,
\begin{small}
\begin{equation}
\begin{split}
p(c_{K_1}, c_{K_2}|c_i) =
\mathcal{N}
\left(
\begin{bmatrix}
\hat{\mu}_{K_1}(c_i) \\
\hat{\mu}_{K_2}(c_i) 
\end{bmatrix},
\begin{bmatrix}
\hat{\Sigma}_{K_1, K_1}(c_i) & \hat{\Sigma}_{K_1, K_2}(c_i)  \\
\hat{\Sigma}_{K_2, K_1}(c_i) & \hat{\Sigma}_{K_2, K_2}(c_i)
\end{bmatrix}
\right),
\end{split}
\label{supp.eq.mgd}
\end{equation}
\end{small}
where $\hat{\Sigma}_{K_1, K_2}(c_i)$ is the covariance between $c_{K_1}$ and $c_{K_2}$. 

If the covariates are independent with each other, $\hat{\boldsymbol{\Sigma}}$ is a diagonal matrix. 

We employ subnetworks $\{g_i(c_i)\}$, each controlled by an individual covariate $c_i$, to model the corresponding conditional distributions $\{p(\boldsymbol{c}|c_i)\}$. Specifically, each subnetwork $g_i(c_i)$ captures a multivariate Gaussian distribution, expressed as: $p(\boldsymbol{c}|c_i) = \mathcal{N}(g^m_i(c_i), g^v_i(c_i))$, where the mean vector $g^m_i(c_i)$ and the covariance matrix $g^v_i(c_i)$ are predicted by \(g_i(c_i)\), as illustrated in Fig.~\ref{fig.method}. %\textcolor{red}{While the above might seem vaguely similar to modeling the dependence between the co-variates as an $N$-dimensional jointly multivariate  normal distribution, since most of the ensuing downstream tasks considered in this paper only require understanding specific functionals of pairwise dependence, we use the above approach to learn the relevant sub-networks needed for the response. }

Next, from Eq.~\eqref{supp.eq.namlss}, the observation $y$ can be formulated as 
\begin{small}
\begin{equation}
y=f^m(\boldsymbol{c},x) + f^v(\boldsymbol{c},x)\cdot \epsilon \\, ~\epsilon \sim \mathcal{N}(0,1) \\.%\sum_{i=1}^Nf^m_i(c_i,x)
\end{equation}
\end{small}

With trained $\{f_i\}$ and $\{g_i\}$, we now investigate how $c_i$ influences the distribution of the observation $y$ as $p(y|c_i,x)= \mathcal{N}(\Tilde{\mu}(c_i, x), \Tilde{\sigma}^2(c_i, x))$, where $\Tilde{\mu}(c_i, x)$ is the expectation of $y$ when fixing $c_i$ and $x$, i.e. $\mathrm{E}[y|c_i,x]$; and $\Tilde{\sigma}^2(c_i, x)$ is the variance of $y$ when fixing $c_i$ and $x$, i.e. $\mathrm{Var}(y|c_i,x)$. 


%We use subnetworks $\{g_i(c_i)\}$ controlled by individual covariates $\{c_i\}$ to capture those conditional disributions where each subnetwork $g_i(c_i)$ captures a series of conditional distributions, i.e., $\{p(c_k|c_i)= \mathcal{N}(g^m_{i,k}(c_i), g^v_{i,k}(c_i))\}$, where the mean $\{g^m_{i,k}(c_i)\}$ and variance $\{g^v_{i,k}(c_i)\}$ of the Gaussian distribution are predicted by $g_i(c_i)$, as illustrated in Fig.~\ref{fig.method}. For example, if $c_i$ represents age and $c_k$ weight, $p(c_k|c_i)$ models how weight is determined by age and the uncertainty when we use age to predict weight. With trained $\{f_i\}$ and $\{g_i\}$, we now investigate how $c_i$ influences the observation $y$ as $p(y|c_i,x)= \mathcal{N}(\Tilde{\mu}, \Tilde{\sigma}^2)$. 
%$p(y|c_i)= \mathcal{N}(\mathrm{E}[y|c_i], \mathrm{Var}(y|c_i))$. 



\paragraph{Mean of $p(y|c_i,x)$.}
We expand the two variable case in Sec.~\ref{subsec.why_mrg} to multi-covariates, with the \emph{law of total expectation}~(1)
\begin{small}
\begin{equation}
\begin{split}
\Tilde{\mu}(c_i, x) &= \mathrm{E}[y|c_i,x] = \mathrm{E}[(f^m(\boldsymbol{c},x) + f^v(\boldsymbol{c},x)\cdot \boldsymbol{\epsilon})|c_i,x] \\& \overset{(1)}{=} \mathrm{E}[f^m(\boldsymbol{c},x)|c_i,x] = \int_{-\infty}^{\infty} f^m(\boldsymbol{c},x) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= \int_{-\infty}^{\infty} (\sum_{k=1}^{N} f^m_{k}(c_{k},x)) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= f^m_i(c_i,x) + \underbrace{\int_{-\infty}^{\infty} (\sum_{k \neq i} f^m_{k}(c_{k},x)) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i}}_{:=H} 
\label{supp.eq.E_y_given_ci_1}
\end{split}
\end{equation}
\end{small}


\begin{comment}

\begin{small}
\begin{equation}
\begin{split}
\mathrm{E}[y|c_i] &= \int_{-\infty}^{\infty} f^m(\boldsymbol{c}) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= \int_{-\infty}^{\infty} (\sum_{k=1}^{N} f^m_{k}(c_{k})) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= f^m_i(c_i) + \underbrace{\int_{-\infty}^{\infty} (\sum_{k \neq i} f^m_{k}(c_{k})) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i}}_{:=H} 
\label{supp.eq.E_y_given_ci_1}
\end{split}
\end{equation}
\end{small}
\end{comment}


where $f^m_k(c_k)$ represents the interpretation from the additive subnetwork $f_i$ of \texttt{LucidAtlas}, while $H$ accounts for the contributions from the dependencies between the covariates which can be further simplified as follows  
\begin{small}
\begin{equation}
\begin{split}
& H = \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k},x) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k},x)(\int_{-\infty}^{\infty}  p(\boldsymbol{c}_{j \neq \{i,k\}}, c_k|c_i) \dif \boldsymbol{c}_{j \neq \{i,k\}}) \dif c_k \\
&= \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k},x) p(c_k|c_i) \dif c_k 
\label{supp.eq.H}
\end{split}
\end{equation}
\end{small}
where $\boldsymbol{c}_{k \neq i}=[c_1, ..., c_{i-1}, c_{i+1}, ..., c_N]$.

Eq.~\eqref{supp.eq.H} indicates that even when multiple covariates are involved, only conditional dependencies with respect to individual covariates ($p(c_k|c_i)$) are required to compute $\Tilde{\mu}(c_i, x)$ as a consequence of the additive model for a NAM, which simplifies computations.
% We found that $H$ can be obtained by independently sampling the output of the subnetworks $f^m_k(c_k)$ with $p(c_k|c_i)$ when modeling the atlas with NAMs.

Therefore, 
\begin{small}
\begin{equation}
\begin{split}
& \Tilde{\mu}(c_i, x) = f^m_i(c_i,x) + \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k},x) p(c_k|c_i) \dif c_k\,. 
\label{supp.eq.E_y_given_ci}
\end{split}
\end{equation}
\end{small}




\begin{comment}
\paragraph{Expectation $\mathrm{E}[y|c_i]$}
We expand the two variable case in Sec.~\ref{subsec.why_mrg} to multi-covariates, with the \emph{law of total expectation} and the \emph{law of the unconscious statistician}
\begin{small}
\begin{equation}
\begin{split}
\mathrm{E}[y|c_i] &= \int_{-\infty}^{\infty} f^m(\boldsymbol{c}) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= \int_{-\infty}^{\infty} (\sum_{k=1}^{N} f^m_{k}(c_{k})) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= f^m_i(c_i) + \underbrace{\int_{-\infty}^{\infty} (\sum_{k \neq i} f^m_{k}(c_{k})) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i}}_{:=H} 
\label{supp.eq.E_y_given_ci_1}
\end{split}
\end{equation}
\end{small}

where $f^m_k(c_k)$ represents the interpretation from the additive subnetwork $f_i$ of \texttt{LucidAtlas}, while $H$ accounts for the contributions from covariate dependences. 
\begin{small}
\begin{equation}
\begin{split}
& H = \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k}) p(\boldsymbol{c}_{k \neq i}|c_i) \dif \boldsymbol{c}_{k \neq i} \\
&= \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k})(\int_{-\infty}^{\infty}  p(\boldsymbol{c}_{j \neq \{i,k\}}, c_k|c_i) \dif \boldsymbol{c}_{j \neq \{i,k\}}) \dif c_k \\
&= \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k}) p(c_k|c_i) \dif c_k 
\label{supp.eq.H}
\end{split}
\end{equation}
\end{small}
where $\boldsymbol{c}_{k \neq i}=[c_1, ..., c_{i-1}, c_{i+1}, ..., c_N]$.


 We found that $H$ can be obtained by independently sampling the output of the subnetworks $f^m_k(c_k)$ with $p(c_k|c_i)$ when modeling the atlas with NAMs.

Therefore, 
\begin{small}
\begin{equation}
\begin{split}
& \mathrm{E}[y|c_i] = f^m_i(c_i) + \sum_{k \neq i} \int_{-\infty}^{\infty} f^m_{k}(c_{k}) p(c_k|c_i) \dif c_k\,. 
\label{supp.eq.E_y_given_ci}
\end{split}
\end{equation}
\end{small}
\end{comment}

\paragraph{Variance of $p(y|c_i,x)$.}

The \emph{law of total variance} is $\mathrm{Var}(Y) =\mathrm{E}[\mathrm{Var}(Y|X)] + \mathrm{Var}(\mathrm{E}[Y|X])$ which states that the total variance of a random variable $Y$ can be broken into two parts: \textcircled{1} the \textbf{expected variance of $Y$ given $X$}, which represents how much $Y$ fluctuates around its mean for each specific value of $X$; and \textcircled{2} The variance of the \textbf{expected value of $Y$ given $X$}, which measures how much the mean of $Y$ changes as $X$ varies.
With the \emph{law of total variance}, 
\begin{small}
\begin{equation}
\mathrm{Var}(y|c_i,x)= \underbrace{\mathrm{E}[\mathrm{Var}(y|\boldsymbol{c}_{k \neq i}, c_i,x)]}_{ \text{\textcircled{1}} :=\Tilde{\sigma}^2_E(c_i, x)} + \underbrace{\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}_{k \neq i}, c_i,x])}_{\text{\textcircled{2}}:=\Tilde{\sigma}^2_V(c_i, x)}
\label{supp.eq.Var_dist}
\end{equation}
\end{small}
The expected variance of $f^v(\boldsymbol{c}, x)$ given $c_i$ and $x$ is
\begin{small}
\begin{equation}
\begin{split}
&\Tilde{\sigma}^2_E(c_i, x) = \mathrm{E}[\mathrm{Var}(y|\boldsymbol{c}_{k \neq i}, c_i,x)] = \mathrm{E}[f^v(\boldsymbol{c},x)|c_i,x]  \\
&= \int_{-\infty}^{\infty} f^v(\boldsymbol{c},x) p(\boldsymbol{c}_{k \neq i}|c_i) d\boldsymbol{c}_{k \neq i} \\
&= f^v_i(c_i,x) + \sum_{k \neq i} \int_{-\infty}^{\infty} f^v_{k}(c_{k},x) p(c_k|c_i) \dif c_k\,.
\end{split}
\label{supp.eq.E_of_V}
\end{equation}
\end{small}

And the variance of the expected value of $f^m(\boldsymbol{c},x)$ given $c_i$ and $x$ can be computed as
\begin{small}
\begin{equation}
\begin{split}
&\Tilde{\sigma}^2_V(c_i, x) =\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}_{k \neq i}, c_i,x]) = \mathrm{Var}(f^m(c_i, \boldsymbol{c}_{k \neq i},x)|c_i) \\
&=\mathrm{Var}(f^m_i(c_i, x) + \sum_{k \neq i}f^m_k(c_k, x)|c_i, x) \\
&=\mathrm{Var}(\sum_{k \neq i}f^m_k(c_k, x)|c_i, x) \\ &= \sum_{k \neq i} \underbrace{\mathrm{Var}(f^m_k(c_k,x)|c_i, x)}_{\text{\textcircled{3}}} \\ &+ \mathop{\sum\sum}_{K_1\neq K_2 \neq i} \underbrace{\mathrm{Cov}(f^m_{K_1}(c_{K_1}, x), f^m_{K_2}(c_{K_2}, x) | c_i, x)}_{\text{\textcircled{4}}}
\end{split}
\label{supp.eq.V_of_E}
\end{equation}
\end{small}
where
\begin{small}
\begin{equation}
\begin{split}
&\text{\textcircled{3}} = \int_{-\infty}^{\infty} (f^m_k(c_k, x) - \Tilde{\mu}_k(c_i, x))^2p(c_k| c_i) \dif c_k ,\\
& \Tilde{\mu}_k(c_i, x) = \int_{-\infty}^{\infty} f^m_k(c_k, x)p(c_k|c_i)dc_k
\end{split}
\label{supp.eq.V_of_E_part1}
\end{equation}
\end{small}

\begin{equation}
\begin{split}
\text{\textcircled{4}} = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f^m_{K_1}(c_{K_1}, x)f^m_{K_2}(c_{K_2}, x)p(c_{K_1}, c_{K_2}|c_i)\dif c_{K_1} \dif c_{K_2} \\
 - \Tilde{\mu}_{K_1}(c_i, x) \Tilde{\mu}_{K_2}(c_i, x) 
\end{split}
\label{supp.eq.V_of_E_part2}
\end{equation}


%&= \int_{-\infty}^{\infty} (f^m(c_i, \boldsymbol{c}_{k \neq i},x) - \Tilde{\mu})^2 p(\boldsymbol{c}_{k \neq i}|c_i) d\boldsymbol{c}_{k \neq i} 

Eqs.~\eqref{supp.eq.E_of_V}-\eqref{supp.eq.V_of_E_part2} imply that instead of sampling the entire covariate space, one only needs to sample from the joint Gaussian distribution between the two covariates, conditioned on the individual covariates, to obtain the marginalized distribution $p(y|c_i,x)$.





\paragraph{Approximation.} 
The integrals in $\Tilde{\mu}(c_i, x)$ (in Eq.~\eqref{supp.eq.E_y_given_ci}), $\Tilde{\sigma}^2_E(c_i, x)$ (in Eq.~\eqref{supp.eq.E_of_V}) and $\Tilde{\sigma}^2_V(c_i, x)$ (in Eqs.~\eqref{supp.eq.E_of_V}-\eqref{supp.eq.V_of_E_part2}) can be approximated using Monte Carlo sampling. E.g. for $\Tilde{\mu}(c_i, x)$, for each covariate $c_k$ one can sample $L$ values $\{\hat{c}_k^l\}_{l=1, ..., L}$ from the distribution of covariates $p(c_k|c_i)$ given by $g_k(\cdot)$ to approximate $\Tilde{\mu}(c_i, x)$ as  
\begin{small}
\begin{equation}
\begin{split}
\Tilde{\mu}(c_i, x) \approx f^m_i(c_i) + \frac{1}{L} \sum_{k \neq i}\sum_{l=1}^{L} f^m_k(\hat{c}_{k}^{l},x)\,,
\label{supp.eq.sample_for_num}
\end{split}
\end{equation}
\end{small}  
where the $\{\hat{c}_{k}^{l}\}_{l=1}^{L}$ are sampled from $p(c_k|c_i)$. 



%Specifically, for each covariate $c_k$ one can sample $M$ values $\{\hat{c}_k^m\}_{m=1, ..., M}$ from the distribution of covariates $p(c_k|c_i)$ given by $g_k(\cdot)$ to approximate $\Tilde{\mu}(c_i, x)$ as  
% \begin{small}
% \begin{equation}
% \begin{split}
% &\Tilde{\mu}(c_i, x) \approx f^m_i(c_i) + \frac{1}{M} \sum_{k \neq i}\sum_{m=1}^{M} f^m_k(\hat{c}_{k}^{m},x)  \\
% &\Tilde{\sigma}^2_E(c_i, x) \approx f^v_i(c_i,x) + \frac{1}{M} \sum_{k \neq i} \sum_{m=1}^{M} f^v_{k} ,(\hat{c}_{k}^{m},x),
% \label{supp.eq.sample_for_num}
% \end{split}
% \end{equation}
% \end{small}  
% where $\{\hat{c}_{k}^{m}\}_{m=1}^{M}$ are sampled from $p(c_k|c_i)$. 

%The integrals in $\Tilde{\sigma}^2_V(c_i, x)$ (in Eq.~\eqref{supp.eq.V_of_E}) requires for the joint conditional distribution $p(\boldsymbol{c}_{k \neq i}|c_i)$ which is intractable with the learned $\{p(c_k|c_i)\}$. Inspired by Gibbs sampling, We proposed a heuristic sampling approach to approximate this integral in Algorithm~\ref{alg.sampling}. Specifically, we extract $M$ samples $\{\hat{\boldsymbol{c}}^m\}_{m=1, ..., M}$ with Algorithm~\ref{alg.sampling}. \textcolor{red}{Assuming that Algorithm~\ref{alg.sampling} guarantee each $\hat{\boldsymbol{c}}^m$ from $p(\boldsymbol{c}|c_i)$}, we approximate $\Tilde{\sigma}^2_V(c_i, x)$ as


\begin{comment}
\paragraph{Variance $\mathrm{Var}[y|c_i]$}

% Var(Y|C1) = E[Var(Y|C1,C2)|C1] + Var(E[Y|C1,C2]|C1)

With the \emph{law of total variance}, 
\begin{small}
\begin{equation}
\mathrm{Var}(y|c_i) = \underbrace{\mathrm{E}[\mathrm{Var}(y|\boldsymbol{c})|c_i]}_{\text{expectation of variance}} + \underbrace{\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}]|c_i)}_{\text{variance of expectation}}
\label{supp.eq.Var_dist}
\end{equation}
\end{small}
\begin{small}
\begin{equation}
\begin{split}
\mathrm{E}[\mathrm{Var}(y|\boldsymbol{c})|c_i] &= \mathrm{E}[f^v(\boldsymbol{c})|c_i]  \\
&= \int_{-\infty}^{\infty} f^v(\boldsymbol{c}) p(\boldsymbol{c}_{k \neq i}|c_i) d\boldsymbol{c}_{k \neq i} \\
&= f^v_i(c_i) + \sum_{k \neq i} \int_{-\infty}^{\infty} f^v_{k}(c_{k}) p(c_k|c_i) \dif c_k 
\end{split}
\label{supp.eq.E_of_V}
\end{equation}
\end{small}
\begin{small}
\begin{equation}
\begin{split}
\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}]|c_i) &= \mathrm{Var}(f^m(c_i, \boldsymbol{c}_{k \neq i})|c_i) \\
&= \int_{-\infty}^{\infty} (f^m(c_i, \boldsymbol{c}_{k \neq i}) - \mathrm{E}[y|c_i])^2 p(\boldsymbol{c}_{k \neq i}|c_i) d\boldsymbol{c}_{k \neq i} 
\end{split}
\label{supp.eq.V_of_E}
\end{equation}
\end{small}

\paragraph{Approximation} 
The integrals in $\mathrm{E}[y|c_i]$ (in Eq.~\eqref{supp.eq.E_y_given_ci}) and $\mathrm{E}[\mathrm{Var}(y|\boldsymbol{c})|c_i]$ (in Eq.~\eqref{supp.eq.E_of_V}) can be approximated using the Monte Carlo sampling approach. Specifically, for each covariate $c_k$ one can sample $M$ values $\{\hat{c}_k^m\}_{m=1, ..., M}$ from the distribution of covariates $p(c_k|c_i)$ given by $g_k(\cdot)$ to approximate $\mathrm{E}[y|c_i]$ as  
\begin{small}
\begin{equation}
\begin{split}
&\mathrm{E}[y|c_i] \approx f^m_i(c_i) + \frac{1}{M} \sum_{k \neq i}\sum_{m=1}^{M} f^m_k(\hat{c}_{k}^{m})  \\
&\mathrm{E}[\mathrm{Var}(y|\boldsymbol{c})|c_i] \approx f^v_i(c_i) + \frac{1}{M} \sum_{k \neq i} \sum_{m=1}^{M} f^v_{k}(\hat{c}_{k}^{m}),
\label{supp.eq.sample_for_num}
\end{split}
\end{equation}
\end{small}  

where $\{\hat{c}_{k}^{m}\}_{m=1}^{M}$ are sampled from $p(c_k|c_i)$. 

The integrals in $\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}]|c_i)$ (in Eq.~\eqref{supp.eq.V_of_E}) requires for the joint conditional distribution $p(\boldsymbol{c}_{k \neq i}|c_i)$ which is intractable with the learned $\{p(c_k|c_i)\}$. Inspired by Gibbs sampling, We proposed a heuristic sampling approach to approximate this integral in Algorithm~\ref{alg.sampling}. Specifically, we extract $M$ samples $\{\hat{\boldsymbol{c}}^m\}_{m=1, ..., M}$ with Algorithm~\ref{alg.sampling}. \textcolor{red}{Assuming that Algorithm~\ref{alg.sampling} guarantee each $\hat{\boldsymbol{c}}^m$ from $p(\boldsymbol{c}|c_i)$}, we approximate $\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}]|c_i)$ as
\begin{small}
\begin{equation}
\mathrm{Var}(\mathrm{E}[y|\boldsymbol{c}]|c_i) =\frac{1}{M}\sum_{m=1}^M (f^m(\boldsymbol{\hat{c}}^m) - \mathrm{E}[y|c_i])^2
\label{supp.eq.V_of_E_approx}  
\end{equation}
\end{small}

\end{comment}


\paragraph{Computational Complexity.}
Suppose there are $N$ covariates and $L$ samples. The computational complexity of marginalizing the NAM for a covariate is \(\mathcal{O}(LN)\), making it feasible in practice. In contrast, for a black-box model, which does not assume our additive structure, the complexity is exponentially higher at \(\mathcal{O}(L^N)\), making direct computation infeasible for large \(N\).




\begin{comment}
    

\begin{algorithm}[tb]
\caption{Sequential Covariate Sampling Process}
\label{alg.sampling}
\begin{algorithmic}
\STATE \textbf{Initialize:} $\mathcal{K} = \{i\}$, $s \gets i$.
\STATE \textbf{Define:} $c_s$ as the source covariate, $c_t$ as the target covariate, $c_i$ is the initial source covariate. 
\WHILE{$|\mathcal{K}| < $ total number of covariates}
    \STATE Given $c_s$, obtain $\{{g^m_{s,k}(c_s)}\}_{k \notin \mathcal{K}}$ and $\{{g^v_{s,k}(c_s)}\}_{k \notin \mathcal{K}}$ from $g_s(c_s)$
    \STATE $t \gets \arg\min_{k \notin \mathcal{K}} \{{g^v_{s,k}(c_s)}\}$
    \STATE Sample $c_t \sim \mathcal{N}(g^m_{s,t}(c_s), g^v_{s,t}(c_s))$
    \STATE $\mathcal{K} \gets \mathcal{K} \cup \{t\}$
    \STATE $s \gets t$ {Update source covariate for next iteration}
\ENDWHILE
\STATE \textbf{Return:} Sampled covariates as a vector $\boldsymbol{\hat{c}}=[c_1, ..., c_N]$ %$\{c_k\}_{k \in \mathcal{K}}$
\end{algorithmic}
\end{algorithm}
\end{comment}

As a result, we obtain $\Tilde{\mu}(c_i, x)$ and $\Tilde{\sigma}^2(c_i, x)=\Tilde{\sigma}^2_E(c_i, x)+\Tilde{\sigma}^2_V(c_i, x)$ to parameterize $p(y|c_i) = \mathcal{N}(\Tilde{\mu}(c_i, x), \Tilde{\sigma}^2(c_i, x))$, capturing the influence of a single covariate $c_i$ on the observation $y$. Our approach aligns with NAM interpretations and can be applied post-hoc.


\subsubsection{Imputation}
\label{supp.sec.imputation}
Our approach naturally facilitates the imputation of missing covariates, as it inherently predicts the conditional distributions \(\{p(c_k \mid c_i)\}\), enabling a principled way to estimate missing values.
 Specifically, if $c_i$ is missing, one can choose the $g_s$ whose uncertainty is the smallest as the predictor for $c_i$ as $s \gets \arg\min_{k,k \neq i} \{{g^v_{k,i}(c_k)}\}$. 


\subsubsection{Individualized Prediction}
\label{supp.sec.ind_pred}
One challenge in the context of atlas discovery is to make individualized predictions when observations are predominantly limited to a single time point, i.e., when the atlas is built from cross sectional data. \texttt{LucidAtlas} provides an approach for individualized prediction based on previous observations. Note that this approach is not based on true longitudinal data  (as such data is frequently not available) but instead aims to predict individual future responses based on the cross-sectional population trend.

We define our problem as follows. Given an observation $y^t$ at $x$ with its corresponding covariates $\boldsymbol{c}^t$  at time $t$, how will a subject's response change when $\boldsymbol{c}^t$ changes to $\boldsymbol{c}^{t+1}$ at time $t+1$?

First, we can obtain the probability distribution with  \texttt{LucidAtlas}, as $p(y^t|\boldsymbol{c}^t,x)=\frac{1}{\sqrt{2\pi f^v(\boldsymbol{c}^t,x)}}\exp(-\frac{(y^t-f^m(\boldsymbol{c}^t,x))^2}{2 \cdot f^v(\boldsymbol{c}^t,x)})$.

\begin{assumption}
We assume that the percentile of a subject remains stationary between  observations at two nearby time points, i.e., the cumulative distribution, $\mathrm{F}$, should be stationary: $\mathrm{F}(y^t) = \mathrm{F}(y^{t+1})$.
\label{supp.asp.sig}
\end{assumption}
An intuitive example for the Assumption~\ref{supp.asp.sig} is that if a child has the largest airway among all 2-year-olds, it is likely that this child's airway will remain the largest over a short period of time. Now we have
\begin{small}
\begin{equation}
\begin{split}
\mathrm{F}(y^t)=\frac{1}{2}[1+\mathrm{erf}(\frac{y^t-f^m(\boldsymbol{c}^t,x)}{\sqrt{2f^v(\boldsymbol{c}^t,x)}})]\,, \\
\mathrm{F}(y^{t+1})=\frac{1}{2}[1+\mathrm{erf}(\frac{y^{t+1}-f^m(\boldsymbol{c}^{t+1},x)}{\sqrt{2f^v(\boldsymbol{c}^{t+1},x)}})]\,.
\end{split}
\label{supp.eq.F_cum}
\end{equation}
\end{small}
\begin{small}
\begin{equation}
\begin{split}
\mathrm{F}(y^t) = \mathrm{F}(y^{t+1}) \Rightarrow  \frac{y^t-f^m(\boldsymbol{c}^t,x)}{\sqrt{2f^v(\boldsymbol{c}^t,x)}} = \frac{y^{t+1}-f^m(\boldsymbol{c}^{t+1},x)}{\sqrt{2f^v(\boldsymbol{c}^{t+1},x)}}  \Rightarrow \\
y^{t+1} = f^m(\boldsymbol{c}^{t+1},x) + {\sqrt{\frac{f^v(\boldsymbol{c}^{t+1},x)}{f^v(\boldsymbol{c}^{t},x)}}(y^t-f^m(\boldsymbol{c}^t,x))}\,.
\end{split}
\end{equation}
\end{small}

Therefore, an approximate individualized prediction can be obtained as $y^{t+1} \approx f^m(\boldsymbol{c}^{t+1},x) + {\sqrt{\frac{f^v(\boldsymbol{c}^{t+1},x)}{f^v(\boldsymbol{c}^{t},x)}}(y^t-f^m(\boldsymbol{c}^t,x))}$\,.








\section{Network Architecture}



\begin{figure*}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/f_i.pdf}
    \caption{Network Architecture of Additive Subnetwork $f_i$ in \texttt{LucidAtlas}. }
    \label{supp.fig.f_i}
\end{figure*}






\begin{figure*}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/g_i.pdf}
    \caption{Network Architecture of $g_i$ in \texttt{LucidAtlas} for Modeling Covariate Dependencies.}
    \label{supp.fig.g_i}
\end{figure*}


Fig.~\ref{supp.fig.f_i} shows the network architecture of the additive subnetwork $f_i$, which receives the anatomical location $x$ and covariate $c_i$ to predict the additive contribution $f^m_i(c_i, x)$ and $f^v_i(c_i, x)$ to the mean and variance for the distributional parameters $\mu(\boldsymbol{c}, x)$ and $\sigma^2(\boldsymbol{c}, x)$ respectively. Specifically, if there is prior knowledge, we use a monotonic neural network~\citep{kitouni2023mono} as the backbone to predict $f^m_i(c_i, x)$; if there is no prior knowledge, we use an MLP to predict $f^m_i(c_i, x)$. Another MLP receives the $f^m_i(c_i, x)$ with $c_i$ and $x$ to predict the contribution $f^v_i(c_i, x)$ to the 
 overall variance $\sigma^2(\boldsymbol{c}, x)$. Considering that the variance should be a number $\geq 0$, a $softplus$ activation layer is used at the output of the MLP for $f_v^i(c_i, x)$ to ensure $f_v^i(c_i, x)$ is a non-negative number.


Fig.~\ref{supp.fig.g_i} shows the network architecture for the dependence modeling network $g_i$, which receives the covariate $c_i$ to predict the parameters of the conditional distributions $\{p(c_k| c_i)\}$, whose distributional parameters are $\{g^m_{i,k}\}$ for means and $\{g^v_{i,k}\}$ for variances. One can also use monotonic neural networks if prior knowledge exists about the covariate dependence. For example, for children, on average, weight increases with age. Suppose there is no prior knowledge about monotonicity. In that case, one can use $\operatorname{Lip}^1$ constrained network discussed in~\cite{kitouni2023mono} for modeling covariates dependence, which prevents sharp and abrupt changes which are not likely to happen. 




\section{Datasets}
\label{supp.sec.dataset}



\begin{comment}
\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/toy_fx_subnet.pdf}
    \caption{Visualizations of the Learned $f_1(X1)$ and $f_2(X2)$. The \textcolor{BrickRed}{red} lines with 
 \textcolor{Gray}{gray} shadows are the learned population trends with uncertainties ($\pm$ 2 standard deviations). The \textcolor{Dandelion}{yellow} lines indicate exact true knowledge of how the dependent covariates ($X_1$ and $X_2$) contribute to the outcome $Y$. \textbf{A good interpretation should either recover the \textcolor{Dandelion}{yellow} lines.} \texttt{LucidAtlas} is the only method that successfully discovers the exact true knowledge about monotonicity for both $X_1$ and $X_2$. }
    \label{fig.toy_example}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/toy_marg.pdf}
    \caption{Visualizations of the Marginzalizations $F_1(X1)$ and $F_2(X2)$. The \textcolor{BrickRed}{red} lines with 
 \textcolor{Gray}{gray} shadows are the learned population trends with uncertainties ($\pm$ 2 standard deviations). \textcolor{SeaGreen}{Green} dots indicate training examples while \textcolor{Orchid}{purple} dots indicate testing examples. The \textcolor{Dandelion}{yellow} lines indicate exact true knowledge of how the dependent covariates ($X_1$ and $X_2$) contribute to the outcome $Y$. The \textcolor{blue}{blue} lines indicate the marginalized expectation $\mathrm{E}(Y|X_1)$ and $\mathrm{E}(Y|X_2)$. \textbf{A good interpretation should either recover the \textcolor{blue}{blue} lines or the \textcolor{Dandelion}{yellow} lines.} \texttt{LucidAtlas} is the only method that successfully discovers the exact true knowledge and the correct marginalized expectation for both $X_1$ and $X_2$. }
    \label{fig.toy_example}
\end{figure*}

As shown in Fig.~\ref{fig.toy_example}, we investigated this problem with a toy example. Suppose $X_1$ and $X_2$ are highly dependent covariates, and both will influence the outcome $Y$. Our prior knowledge is that $Y$ is monotonically and positively dependent with $X_2$. Note that this is a frequent setting in reality because monotonic relationships are oftentimes known are considered reasonably assumptions, e.g., it is a common sense that weight will increase with age for children. The Supplementary Material includes the parameterization details and the real-world problem behind it in Sec.~\ref{supp.subsec.toy_dataset}, also plotted with the yellow lines in Fig.~\ref{fig.toy_example}. We expect the model to interpret the unknown $X_1$ and to match our understanding of $X_2$.

In this case, one should marginalize $f(X_1, X_2)$ if one wants to interpret exactly how $X_1$ effects $Y$ as $\mathrm{E}_{p(X_2|X_1)}[f(X_1, X_2)] = \int f(X_1, X_2)p(X_2|X_1) \dif X_2$\label{supp.eq.gen_marg_X12}.

%\[\mathrm{E}_{p(X_2|X_1)}[f(X_1, X_2)] = \int f(X_1, X_2)p(X_2|X_1) \dif X_2.\]
% \begin{equation}
% \mathrm{E}_{p(X_2|X_1)}[f(X_1, X_2)] = \int f(X_1, X_2)p(X_2|X_1) \dif X_2.
% \label{supp.eq.toy_marg_x2_out}
% \end{equation}

If $f(\cdot)$ is a NAM, then 
\begin{small}
\begin{equation}
\begin{split}
F_1(X_1)&=\int [f_1(X_1) + f(X_2)]p(X_2|X_1) \dif X_2 \\
&=f(X_1)+\underbrace{\int f_2(X_2)p(X_2|X_1) \dif X_2}_{h(X_1)} ,
\end{split}
\end{equation}
\end{small}

where $h(X_1)$ measures how the dependence between $X_1$ and $X_2$ influences the marginalization $F_1(X_1)$. If $X_1$ and $X_2$ independent, $h(X_1)=\int f_2(X_2)p(X_2|X_1) \dif X_2=\int f_2(X_2)p(X_2) \dif X_2=\mathrm{E}_{p(X_2)}[f_2(X_2)]=constant$, which means $h(X_1)$ keeps the same constant for every $X_1$ and NAM learns the unique feature interpretation for $X_1$ as $f_1(X_1)$ . If $X_1$ and $X_2$ dependent, $h(X_1)$ is a function of $X_1$ which should be obtained by marginalizing $X_2$ out of $f(X_1, X_2)$.


In summary, when covariates are dependent, the NAM interpretation is not unique and may contradict our prior knowledge. Introducing prior knowledge helps to discover the actual mechanisms. Marginalization is critical to exclude the confusions from the covariates' dependences to get unique covariate interpretations as $\mathrm{E}[Y|X_i]$. \emph{{LucidAtlas} introduces the option of introducing prior knowledge and develops a marginalization theory in Sec.~\ref{subsubsec.how_marg}.} 






We fit a NAM model (with Exu activation and ensembles)~\citep{agarwal2020neural} to this dataset, as well as NAMLSS~\citep{thielmann2024namlss}, and our \texttt{LucidAtlas} (which considers monotonicity of $X_2$) and plot the models' interpretations as the outputs of the subnetworks in Fig.~\ref{fig.toy_example}(a,b,c). We observe that the interpretation from NAM and NAMLSS contradicts our prior knowledge about $X_2$ and fails to follow the marginal expectation $\mathrm{E}[Y|X_1]$ and $\mathrm{E}[Y|X_2]$. At the same time, \texttt{LucidAtlas} matches prior knowledge (in correct monotonicity) in its local explanation and discovers the correct marginalized distribution in its global explanation because \texttt{LucidAtlas} uses monotonic-aware subnetwork for $X_2$.

\end{comment}







\subsection{OASIS Brain}
\label{supp.subsec.brain_dataset}
The Open Access Series of Imaging Studies (OASIS) is a project aimed at making MRI data sets of the brain freely available to the scientific community~\citep{marcus2007oasis}. 

The OASIS Brain dataset we use is publicly available in a preprocessed form~\footnote{\url{https://www.kaggle.com/datasets/jboysen/mri-and-alzheimers}}. The OASIS Brain dataset consists of two sets, i.e., 

\begin{itemize}
    \item[1] \textbf{A Cross-Sectional MRI Dataset (416 Subjects, Ages 18–96).} 
    100 of the included subjects are over the age of 60 and have been clinically diagnosed with very mild to moderate Alzheimer’s disease (AD). Additionally, a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session.
    \item[2] \textbf{A Longitudinal MRI Dataset in Nondemented and Demented Older Adults (150 Subjects, Ages 60–96).}
    This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer’s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.
\end{itemize}




Our experiments include four covariates: age, socioeconomic status (SES), mini-mental state examination (MMSE), and clinical dementia rating (CDR). The outcome variable is normalized whole brain volume (nWBV), which is a scalar.  

\emph{We aim to investigate the relationships between these covariates and brain volume.} Based on prior knowledge, the atlas brain volume should not increase with age or CDR, nor decrease when mental state improves.  







\subsection{Pediatric Airway}
\label{supp.subsec.airway_dataset}

\begin{table}[htbp!]
\centering
\begin{tabular}{l r r r r r r r r r r} 
\toprule
    \# observations     &  1 & 2 & 3 & 4 & 5 & 6 & 7 & 9 & 11\\ 
\hline\hline
    \# patients   &  230 & 12 &  6 & 8 & 3 & 2 & 1 & 1 & 1 \\
\bottomrule
\end{tabular}
\caption{Number of patients for a given number of observations for the pediatric airway dataset. For example, the 1st column indicates that there are 230 patients who were only observed once. }
\label{tab.num_of_observations}
\end{table}

\begin{table*}

\resizebox{\textwidth}{!}{%
\begin{tabular}{p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}p{0.11\textwidth}}
\toprule
P- &      0&      10  &      20  &      30  &      40  & 50 & 60 & 70 & 80 & 90 & 100\\
\midrule
 &
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/0.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/1.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/2.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/3.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/4.png} & 
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/5.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/6.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/7.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/8.png} &  
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/9.png} &
\includegraphics[width=0.2\columnwidth]{figures/airway_age_perct/10.png} \\

\bottomrule
\end{tabular}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}}
\toprule
P- &      0&      10  &      20  &      30  &      40  & 50 & 60 & 70 & 80 & 90 & 100\\
\midrule

age         &     1.00 &    23.00 &    55.00 &    71.00 &    89.00 &   111.00 &   129.00 &   161.00 &   179.00 &   199.00 &   233.00 \\
m-vol &     4.56 &    16.84 &    29.53 &    28.91 &    27.31 &    70.90 &    71.23 &    43.34 &    78.63 &   102.35 &   113.84 \\

\bottomrule

\end{tabular}}
\caption{Visualization and demographic information of our 3D airway shape dataset. Shapes of $\{0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\}$-th age percentiles 
 are plotted with their covariates (age/month) printed in the table. M-vol (measured volume) is the volume ($cm^3$) of the gold standard shapes based on the actual imaging. }
\label{supp.tab.vis_demo_dataset}
\end{table*}





The airway shapes are extracted from computed tomography (CT) images. The real CT images are from children ranging in age from 1 month to $\sim$19 years old. Acquiring CT images is costly. Further, CT uses ionizing radiation which should be avoided, especially in children, due to cancer risks. Hence, it is difficult to acquire such CTs for many children. Instead, the data was acquired by serendipity from children who received CTs for reasons other than airway obstructions (e.g., because they had cancer)~\citep{jiao2023naisr}. This also explains why it is difficult to acquire longitudinal data. E.g., one of the patients has 11 timepoints because a very sick child was scanned 11 times. 


The pedatric airway dataset includes 230 cross-sectional observations (where a patient was only imaged once) and 34 longitudinal observations. 176 patients (i.e., 263 shapes) have all three covariates (age, weight, height) and 11 annotated anatomical landmarks. 6 landmarks are located on the upper airway section for this experiment. Errors in the shapes $\{\mathcal{S}^k\}$ may arise from image segmentation error, differences in head positioning, missing parts of the airway shapes due to incomplete image coverage, and dynamic airway deformations due to breathing. Table~\ref{tab.num_of_observations} shows the distribution of the number of observations across patients. Most of the patients in the dataset only have one observation; only 22 patients have $\geq 3$ observation times. Table~\ref{supp.tab.vis_demo_dataset} shows the shapes and demographic information at different age percentiles for the whole data set. Similar to the OASIS Brain dataset, the time span of the longitudinal data for each patient is far shorter than the time span across the entire dataset. 


\subsubsection{Data Preparation for Pediatric Airway Atlas}

The image processing pipeline includes three steps: 1) automatic airway segmentation from CT images; 2) airway representation with a centerline and cross sections.

\paragraph{Airway Segmentation.}
A deep learning-based approach is used for automatic upper airway segmentation from CT images. The segmentation model is trained in two steps. The first step predicts the segmentation using a coarse version of the scans. The second step makes the segmentation prediction on original images. This step takes in the image as input, but also uses the first step prediction as an additional input. Each step is implemented as a U-Net~\citep{unetcciccek20163d, unetronneberger2015u}.

The automatic segmentation model is developed based on a dataset containing 68 pairs of airway CT images and their corresponding manual segmentations.

\paragraph{Centerline and Cross Sections.}

The pediatric airway dataset is constructed by extracting 358 airway geometries from CT images with our automatic segmentation approach. The upper airways, like any tube-like structures, can be approximated by a centerline with cross sections~\citep{atlashong2013pediatric}. Following the approach in~\citep{atlashong2013pediatric}, the airway centerline is inferred based on the heat distribution along the airway provided by solving Laplace's equation. The iso-surfaces of heat values are extracted from the Laplace solution and the centerlines are considered as the centers of the iso-surfaces. Cross sections are cut from the airway geometry using planes that are orthogonal to the tangent of the centerline.

\paragraph{Pediatric Airway Atlas Construction.}
Similar as the approach in~\citep{atlashong2013pediatric}, the cross-sectional area is considered as the airway's main feature. For each point on the centerline, it has a distance $x$ from the nasal spine which is normalized to 1 over the length of the airway, and a cross-sectional area $y$. The 1D function for airway geometry is the curve $c(x)$ that smoothly passes through all these points on the centerline, as $y=c(x)$.

The airway curves are aligned based on six key anatomic landmarks $\{\boldsymbol{p}_i\}$: nasal spine, choana, epiglottic tip, true vocal cord (TVC), subglottis, and carina.


% {'carina': 1.0, 'choana': 0.36225048, 'epiglottic tip': 0.57822824, 'nasal spine': 0.0, 'subglottis': 0.72090906, 'TVC': 0.69001055}



Each landmark $\boldsymbol{p}_{i}=\left(p_{ix}, p_{iy}, p_{iz}\right)$ is projected onto the centerline to obtain the corresponding depth $x_{i}$ along the centerline. For example, the depth of nasal spine $x_{nasal spine}$ should be at 0 while the depth of carina $x_{\text {carina}}$ should be at 1. For each landmark, there is a mean position $\overline{\boldsymbol{p}}_{i}=\left(\bar{p}_{ix}, \bar{p}_{iy}, \bar{p}_{iz}\right)$ and the mean depth $\bar{d}_{i}$ of that landmark over all cases.

A landmark-based curve registration approach ~\citep{atlashong2013pediatric} is used to estimate a piece-wise linear warping function $h_{k}(\cdot)$ for each curve $c_{k}(\cdot)$, which is strictly monotonic and places the landmark points for a particular subject $k$ at the mean location of these landmarks in the atlas, $x_{i}=h_{k}\left(\bar{x}_{i}\right)$. With the constructed warping functions, curves can then be resampled to the normalized coordinate system with $C_{k}(x)=c_{k}\left(h_{k}(x)\right)$.









\section{Experimental Setup}


\begin{table*}[]
\centering
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{|l|cccccc|}
\hline
\multirow{3}{*}{Model} & \multicolumn{4}{c|}{\texttt{LucidAtlas}}                             & \multicolumn{1}{c|}{NAMLSS} & \multirow{3}{*}{MLP+NLL} \\ \cline{2-6}
                       & \multicolumn{2}{c|}{$f^m_i$ or $f^v_i$} & \multicolumn{2}{c|}{$g_i$} & \multicolumn{1}{c|}{$f_i$}  &                          \\ \cline{2-6}
 &
  \multicolumn{1}{c|}{Monotonic} &
  \multicolumn{1}{c|}{MLP} &
  \multicolumn{1}{c|}{Monotonic} &
  \multicolumn{1}{c|}{$Lip^1$ Constrained} &
  \multicolumn{1}{c|}{MLP} &
   \\ \hline
Layers &
  \multicolumn{1}{c|}{$[D_{in}, 128, D_{out}]$} &
  \multicolumn{1}{c|}{$[D_{in}, 128, D_{out}]$} &
  \multicolumn{2}{c|}{$[D_{in}, 128, D_{out}]$} &
  \multicolumn{1}{c|}{$[D_{in}, 128, D_{out}]$} &
  $[D_{in}, 128, D_{out}]$ \\ \hline
Activation &
  \multicolumn{1}{c|}{GroupSort} &
  \multicolumn{1}{c|}{GeLU} &
  \multicolumn{2}{c|}{GroupSort} &
  \multicolumn{1}{c|}{GroupSort} &
  GeLU \\ \hline
Learning Rate          & \multicolumn{6}{c|}{1e-2}                                                                                                     \\ \hline
Num of Epoch           & \multicolumn{6}{c|}{500}                                                                                                      \\ \hline
Output Activation      & \multicolumn{6}{c|}{Linear, Softplus}                                                                                         \\ \hline
Others                 & \multicolumn{6}{c|}{Adam optimizer, CosineAnnealingLR, Earlystopping}                                                         \\ \hline
\end{tabular}
}
\caption{Hyperparameter Settings for Comparison Methods. GroupSort and $Lip^1$-constrained networks were introduced in~\citep{kitouni2023mono}. Here, $D_{in}$ represents the input dimension, while $D_{out}$ denotes the output dimension. For details on $D_{in}$ and $D_{out}$, refer to Fig.~\ref{supp.fig.f_i} and Fig.~\ref{supp.fig.g_i}.}
\label{supp.tab.hyperparameter}
\end{table*}


Tab.~\ref{supp.tab.hyperparameter} illustrates the hyperparameter settings of our approach, for NAMLSS~\citep{thielmann2024namlss} and the \textit{MLP+NLL} comparison. For our approach and all comparison methods, we ues $15\%$ of the training data set by the patient as a validation set for early stopping. The batch size for the Pediatric Airway Dataset is set to $1024$, while for the OASIS Brain dataset it is set to 32. For other comparison methods, we use their publicly available implementation, which we describe in the  following.

\paragraph{NAM.} We use the official PyTorch implementation of NAM~\footnote{\url{https://github.com/lemeln/nam/tree/main?tab=readme-ov-file}}.
We evaluate the NAM using feature networks with one hidden layers (to keep consisent with hyperparameter settings in Table.~\ref{supp.tab.hyperparameter}), each containing $128$ units and employing ExU activation. A dropout rate of $0.1$ is applied, and the ensemble consists of $20$ learners. We use $15\%$ of the training data set by patient as a validation set for early stopping. All other experimental settings follow the recommended or default configurations.

\paragraph{Explainable Boosting Machine.} The explainable boosting machine (EBM) is an open-source Python implementation~\footnote{\url{https://github.com/interpretml/interpret/tree/3e810552f7fcae641bf6bd945f10c66bf56c424b}} of the gradient-boosting GAM that is available as a part of the InterpretML library~\citep{lou2013EBM, nori2019interpretmlunifiedframeworkmachine}.  We use $15\%$ of the training data set by patient as a validation set for early stopping. We use the default hyperparameter setting, because we did not find a significant improvement when tuning the hyperparameters. EBM allows for control of the monotonicity of features by using isotonic regression. We find introducing prior knowledge improves EBM's performance, and thus, we use the same prior knowledge for EBM as we used in our \texttt{LucidAtlas}.

\paragraph{LightGBM.} LightGBM is a gradient boosting framework that uses tree-based learning algorithms~\citep{ke2017lightgbm}. We use the open-source implementation~\footnote{\url{https://lightgbm.readthedocs.io/en/latest/index.html}}. We use $15\%$ of the training data set by patient as a validation set for early stopping. We find that the recommended or default configurations work well.


\paragraph{Hardware.} The deep learning models are trained on a single NVIDIA GeForce RTX 3090 GPU and an Intel(R) Xeon(R) Gold 6226R CPU. 



\section{More Results and Visualizations}
\label{supp.more_vis_exp}
Table~\ref{supp.exp.single_quant_ece} presents the quantitative evaluation of population distribution estimation based on the Expected Calibration Error (ECE), demonstrating that incorporating prior knowledge enhances performance on the pediatric airway dataset.

Table~\ref{supp.exp.oasis_cov_corr} evaluates the impact of accounting for covariate dependencies in marginalization. The results highlight that considering these dependencies is crucial for accurately interpreting the effects of individual covariates in neural additive models.

Fig.\ref{supp.fig.vis_whether_do_correlation_sgs}, Fig.\ref{supp.fig.vis_whether_do_correlation_epg} and Fig.\ref{supp.fig.vis_whether_do_correlation_nasal} illustrate covariate interpretations at the subglottis, epiglottic tip and nasal spine in the pediatric airway dataset. These visualizations emphasize the importance of modeling covariate dependencies when interpreting covariate effects.

Fig.~\ref{supp.fig.whether_use_prior} compares the covariate interpretations when ignoring or using covariate dependencies. 
Fig.\ref{supp.pairwise_cov_corr_airway} and Fig.\ref{supp.pairwise_cov_corr_brain} visualize the pairwise conditional distribution $p(c_k|c_i)$ for covariates in the pediatric airway dataset and the OASIS Brain dataset, respectively.

Fig.~\ref{supp.fig.vis_airway_shape} visualizes pediatric airway CSA functions with uncertainties across different ages, incorporating marginalized covariate interpretation while accounting for covariate dependence in \texttt{LucidAtlas}. The visualization reveals that both the average airway CSA and population variance increase as children grow.



\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/vis_shape.pdf}
    \caption{\small Visualization of Pediatric Airway CSA Functions with Uncertainties Across Different Ages.
In each subplot, the x-axis represents the normalized airway depth along the centerline (from the tip of the nose at 0 to the carina at 1), while the y-axis denotes the airway CSA at that depth. \textcolor{SeaGreen}{Green} and \textcolor{Orchid}{purple} lines indicate training and testing samples respectively. Both the average airway CSA and population variance increase as children grow. }
    \label{supp.fig.vis_airway_shape}
\end{figure}




\begin{table*}[t]
\centering
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{c|ccc|c|ccccccc}
\hline
 &
   &
   &
   &
  \cellcolor[HTML]{E9FEE9} &
  \multicolumn{7}{c}{\cellcolor[HTML]{E2E3F8}\textbf{Pediatric Airway}} \\ \cline{6-12} 
\multirow{-2}{*}{Methods} &
  \multirow{-2}{*}{Spa.} &
  \multirow{-2}{*}{Add.} &
  \multirow{-2}{*}{Mono.} &
  \multirow{-2}{*}{\cellcolor[HTML]{E9FEE9}\textbf{OASIS Brain}} &
  \multicolumn{1}{c|}{\textbf{Overall}} &
  nasal spine &
  choana &
  epiglottic tip &
  \cellcolor[HTML]{F8E5E4}TVC &
  \cellcolor[HTML]{F8E5E4}subglottis &
  \cellcolor[HTML]{F8E5E4}carina \\ \hline
MLP+NLL &
  \XSolidBrush &
  \XSolidBrush &
  \XSolidBrush &
  \textbf{0.0150} &
  \multicolumn{1}{c|}{0.0454} &
  0.0713 &
  0.052 &
  0.0727 &
  0.1023 &
  0.1071 &
  \textbf{0.0262} \\
NAMLSS &
  \XSolidBrush &
  \Checkmark &
  \XSolidBrush &
  {\color[HTML]{000000} 0.0189} &
  \multicolumn{1}{c|}{0.0572} &
  {\color[HTML]{000000} 0.116} &
  0.0316 &
  {\color[HTML]{9A0000} \textbf{0.0243}} &
  {\color[HTML]{000000} 0.1047} &
  {\color[HTML]{000000} 0.0949} &
  {\color[HTML]{000000} 0.158} \\ \hline
\rowcolor[HTML]{EFEFEF} 
Ours  np &
  \Checkmark &
  \Checkmark &
  \XSolidBrush &
  {\color[HTML]{9A0000} \textbf{0.0110}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} 0.0400}} &
  {\color[HTML]{9A0000} \textbf{0.0261}} &
  {\color[HTML]{9A0000} \textbf{0.0131}} &
  0.0673 &
  {\color[HTML]{000000} \textbf{0.0686}} &
  {\color[HTML]{9A0000} \textbf{0.0452}} &
  {\color[HTML]{000000} 0.0398} \\
\rowcolor[HTML]{EFEFEF} 
Our part &
  \Checkmark &
  \Checkmark &
  \Checkmark &
  {\color[HTML]{000000} 0.0161} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{9A0000} \textbf{0.0236}}} &
  \textbf{0.0266} &
  \textbf{0.0312} &
  0.1177 &
  {\color[HTML]{9A0000} \textbf{0.0545}} &
  \textbf{0.0467} &
  {\color[HTML]{9A0000} \textbf{0.0175}} \\
\rowcolor[HTML]{EFEFEF} 
Ours imp &
  \Checkmark &
  \Checkmark &
  \Checkmark &
  0.0187 &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}{\color[HTML]{000000} \textbf{0.0248}}} &
  0.0805 &
  0.0326 &
  \textbf{0.0553} &
  0.0732 &
  0.0673 &
  {\color[HTML]{000000} 0.0522} \\ \hline
\end{tabular}
}
\caption{\small Quantitative Evaluation of Population Distribution Estimation Using Expected Calibration Error (ECE). \textbf{\textcolor{purple}{Bold red values}} indicate the best scores across all methods. \textbf{Bold black values} indicate the 2nd best scores of all methods. Our approach %, trained with complete data,
achieves the best performance overall.  }
\label{supp.exp.single_quant_ece}
\end{table*}





\begin{table}[t]
\centering
\resizebox{0.2\textwidth}{!}{
\begin{tabular}{lcl}
\hline
\rowcolor[HTML]{E9FEE9} 
\multicolumn{3}{c}{\cellcolor[HTML]{E9FEE9}\textbf{OASIS Brain}}                                                                                    \\ \hline
\multicolumn{1}{c|}{Covariate}                   & \multicolumn{1}{c|}{Corr.}                              & \multicolumn{1}{c}{Overall}            \\ \hline
\multicolumn{1}{l|}{AGE}                         & \multicolumn{1}{c|}{\XSolidBrush}                       & 1.0233                                 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}AGE} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\Checkmark} & {\color[HTML]{9A0000} \textbf{0.9151}} \\ \hline
\multicolumn{1}{l|}{SES}                         & \multicolumn{1}{c|}{\XSolidBrush}                       & 1.9071                                 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}SES} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\Checkmark} & {\color[HTML]{9A0000} \textbf{1.2447}} \\ \hline
\multicolumn{1}{l|}{MMSE}                        & \multicolumn{1}{c|}{\XSolidBrush}                       & 1.8732                                 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}MMSE} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\Checkmark} & {\color[HTML]{9A0000} \textbf{1.0849}} \\ \hline
\multicolumn{1}{l|}{CDR}                         & \multicolumn{1}{c|}{\XSolidBrush}                       & 1.8208                                 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}CDR} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\Checkmark} & {\color[HTML]{9A0000} \textbf{1.0408}} \\ \hline
\end{tabular}
}
\caption{\small Quantitative Comparison of Different Ways of Marginalization for OASIS Brain Dataset. NLL is computed between the marginalized covariate interpretation and the data distribution. A \Checkmark in the \textbf{Corr.} column indicates that covariate dependence is considered, while \XSolidBrush signifies that it is ignored. Accounting for covariate dependence improves alignment between covariate interpretation and the data distribution.
}
\label{supp.exp.oasis_cov_corr}
\end{table}


\begin{figure}

    \centering
    \includegraphics[width=0.7\linewidth]{figures/sgs_com_vis.pdf}
    \caption{\small Visualizations of Covariate Interpretations from \texttt{LucidAtlas} for CSA Distribution at the Subglottis Landmark (Pediatric Airway Dataset). (1) $f_i(c_i)$ represents the disentangled covariate effect directly from a NAM as illustrated in Sec.~\ref{sec.dist_cov_effects}; (2) Marginalized covariate interpretation without accounting for covariate dependence; (3) Marginalized covariate interpretation incorporating covariate dependence. \textcolor{SeaGreen}{Green} dots denote training samples, while \textcolor{Orchid}{purple} dots indicate testing samples. The \textcolor{BrickRed}{red} lines represent the learned population trend, and the \textcolor{Gray}{gray} shading spans \(\pm 2 \times\) standard deviations. Considering covariate dependence is essential for accurately capturing how each covariate influences the population trend and associated uncertainties.} 
    \label{supp.fig.vis_whether_do_correlation_sgs}
\end{figure}




\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/epg_com_vis.pdf}
    \caption{\small Visualizations of Covariate Interpretations from \texttt{LucidAtlas} for CSA Distribution at the Epiglottic Tip Landmark (Pediatric Airway Dataset). (1) $f_i(c_i)$ represents the disentangled covariate effect directly from a NAM as illustrated in Sec.~\ref{sec.dist_cov_effects}; (2) Marginalized covariate interpretation without accounting for covariate dependence; (3) Marginalized covariate interpretation incorporating covariate dependence. \textcolor{SeaGreen}{Green} dots denote training samples, while \textcolor{Orchid}{purple} dots indicate testing samples. The \textcolor{BrickRed}{red} lines represent the learned population trend, and the \textcolor{Gray}{gray} shading spans \(\pm 2 \times\) standard deviations. Considering covariate dependence is essential for accurately capturing how each covariate influences the population trend and associated uncertainties.} 
    \label{supp.fig.vis_whether_do_correlation_epg}
\end{figure}






\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/nasal_comp_vis.pdf}
    \caption{\small Visualizations of Covariate Interpretations from \texttt{LucidAtlas} for CSA Distribution at the Nasal Spine Landmark (Pediatric Airway Dataset). (1) $f_i(c_i)$ represents the disentangled covariate effect directly from a NAM as illustrated in Sec.~\ref{sec.dist_cov_effects}; (2) Marginalized covariate interpretation without accounting for covariate dependence; (3) Marginalized covariate interpretation incorporating covariate dependence. \textcolor{SeaGreen}{Green} dots denote training samples, while \textcolor{Orchid}{purple} dots indicate testing samples. The \textcolor{BrickRed}{red} lines represent the learned population trend, and the \textcolor{Gray}{gray} shading spans \(\pm 2 \times\) standard deviations. Considering covariate dependence is essential for accurately capturing how each covariate influences the population trend and associated uncertainties. } 
    \label{supp.fig.vis_whether_do_correlation_nasal}
\end{figure}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/compare_prior_vis.pdf}
    \caption{\small Visualizations of the Effect of Prior Knowledge in \texttt{LucidAtlas} at the Subglottis Landmark (Pediatric Airway Dataset). The \textcolor{red}{$\times$} symbol indicates the covariate interpretation contradicts prior knowledge, such as the NAM incorrectly interpreting airway CSA as decreasing with a child's weight. Without incorporating prior knowledge, the model may deviate form our prior assumptions. Without marginalization, to account for covariate dependencies, the data may not be fit well.}
    \label{supp.fig.whether_use_prior}
\end{figure}




\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/Airway_covariate_correlation.pdf}
    \caption{Pairwise Conditional Distribution $p(c_k|c_i)$ of Age, Height and Weight in the Pediatric Airway Dataset learned by \texttt{LucidAtlas}. }
    \label{supp.pairwise_cov_corr_airway}
\end{figure}






\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/covariate_correlation_oasis.pdf}
    \caption{Pairwise Conditional Distribution $p(c_k|c_i)$ of Age, Clinical Dementia Rating (CDR) Socioeconomic Status (SES), Mini-Mental State Examination (MMSE)in the OASIS Brain Dataset learned by \texttt{LucidAtlas}. }
    \label{supp.pairwise_cov_corr_brain}
\end{figure}



%-------------------------------------------------------------------------
\clearpage
