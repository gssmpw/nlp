\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\makeatletter % changes the catcode of @ to 11
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother % changes the catcode of @ back to 12
\begin{document}


\title{Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks\\
}

\author{
\IEEEauthorblockN{Yuhui Jin*}
\IEEEauthorblockA{\textit{California Institute of Technology} \\
Pasadena, USA \\
yuhuijin1995@gmail.com}
\and
\IEEEauthorblockN{Yaqiong Zhang}
\IEEEauthorblockA{\textit{University of Michigan, Ann Arbor} \\
Ann Arbor, USA \\
yaqiongz@umich.edu}
\and
\IEEEauthorblockN{Zheyuan Xu}
\IEEEauthorblockA{\textit{University of Washington} \\
Sunnyvale, USA \\
cx1014@uw.edu}
\linebreakand
\and
\IEEEauthorblockN{Wenqing Zhang}
\IEEEauthorblockA{\textit{Washington University in St. Louis} \\
Louis, USA \\
wenqing.zhang@wustl.edu}
\and
\IEEEauthorblockN{Jingyu Xu}
\IEEEauthorblockA{\textit{Northern Arizona University}\\
Arizona, USA \\
jyxu01@outlook.com}
}


\maketitle

\begin{abstract}
In the field of computer vision, 6D object detection and pose estimation are critical for applications such as robotics, augmented reality, and autonomous driving. Traditional methods often struggle with achieving high accuracy in both object detection and precise pose estimation simultaneously. This study proposes an improved 6D object detection and pose estimation pipeline based on the existing 6D-VNet framework, enhanced by integrating a Hybrid Task Cascade (HTC) and a High-Resolution Network (HRNet) backbone. By leveraging the strengths of HTC's multi-stage refinement process and HRNet's ability to maintain high-resolution representations, our approach significantly improves detection accuracy and pose estimation precision. Furthermore, we introduce advanced post-processing techniques and a novel model integration strategy that collectively contribute to superior performance on public and private benchmarks. Our method demonstrates substantial improvements over state-of-the-art models, making it a valuable contribution to the domain of 6D object detection and pose estimation.

\end{abstract}

\begin{IEEEkeywords}
6D object detection, pose estimation, Hybrid Task Cascade (HTC), High-Resolution Network (HRNet), deep learning
\end{IEEEkeywords}

\section{Introduction}
6D object detection and pose estimation have become essential tasks in computer vision, especially for applications in robotics, augmented reality, and autonomous driving. The objective is not only to detect objects but to determine their precise 3D orientation and position. Conventional methods often struggle in complex settings with partial occlusions, varying object scales, and significant noise, making it difficult to achieve both high detection accuracy and precise pose estimation.

Recent advances in deep learning and multi-task learning frameworks have improved outcomes, yet limitations remain due to challenges in maintaining spatial detail and refining predictions through multiple stages. Our research addresses these issues by enhancing the 6D-VNet framework with a Hybrid Task Cascade (HTC) and High-Resolution Network (HRNet) backbone. The HTC architecture iteratively refines object proposals over three stages, enhancing accuracy in challenging conditions, while the HRNet backbone retains high-resolution representations crucial for accurate segmentation and pose estimation.

Moreover, we introduce advanced post-processing and model integration techniques to handle occlusions and generalize across datasets. This approach achieves state-of-the-art results on public benchmarks and shows notable improvements on private leaderboards.

By integrating HTC with HRNet and novel post-processing methods, our approach provides a robust solution to current limitations in 6D object detection, with potential for significant real-world impact in scenarios demanding high precision and reliability.


\section{Related Work}
The field of 6D object detection and pose estimation has evolved significantly, with several key approaches addressing the challenges of accurate pose estimation in complex environments. Chen et al.\cite{chen2019hybrid} introduced the Hybrid Task Cascade (HTC), a multi-stage framework integrating object detection, semantic segmentation, and instance segmentation, which has proven effective for tasks requiring precise localization. Zhang et al. \cite{zhang2024yolo} enhance detection efficiency for small objects, supporting our goal of precise, real-time performance."The study by Lu et al. \cite{202411.1053}, "Hybrid Model Integration of LightGBM, DeepFM, and DIN for Enhanced Purchase Prediction on the Elo Dataset," inspired the integration of multi-stage refinement techniques in our framework. Their hybrid approach informed our model's design for combining HTC and HRNet, enhancing both detection precision and pose estimation accuracy.

One of the foundational works, PoseCNN by Xiang et al.\cite{xiang2017posecnn} , utilized convolutional neural networks (CNNs) to estimate poses from RGB images, but it struggled with occlusions and lighting variations. Peng et al.\cite{peng2019pvnet} addressed some of these issues with PVNet, a pixel-wise voting network that improved robustness in cluttered environments. Lu \cite{lu2024optimizing} demonstrates effective ensemble learning for multi-objective optimization, which is instrumental in refining model accuracy and robustness. This approach aligns with our method of integrating HTC and HRNet for improved detection precision in complex environments.

DeepIM by Li et al.\cite{li2018deepim} refined pose estimates through iterative matching, improving precision at the cost of higher computational demands. Li \cite{li2024harnessing} demonstrates effective use of Mult-Recall strategies and ensemble learning for robust recommendation accuracy, underscoring the potential of multimodal data integration. This approach parallels our focus on improving precision and adaptability in complex 6D detection environments. The adaptive route planning by Wang et al. \cite{wang2024method} enhances our multi-stage refinement in 6D object detection by demonstrating real-time contextual adaptation, crucial for handling complex, evolving conditions in autonomous navigation.Jiaxin Lu’s research \cite{202411.0867}, "Enhancing Chatbot User Satisfaction: A Machine Learning Approach Integrating Decision Tree, TF-IDF, and BERTopic," influenced our use of task-specific attention mechanisms. The dynamic feature weighting techniques highlighted in Lu’s work inspired our refinement strategies for complex 6D detection tasks.

In the work by Li et al. \cite{202409.1875}, Strategic Deductive Reasoning in Large Language Models: A Dual-Agent Approach, our research influenced key aspects of their methodology. Specifically, our integration of the Hybrid Task Cascade (HTC) and High-Resolution Network (HRNet) served as a technical inspiration for their multi-stage refinement and task-specific optimization strategies. The emphasis on maintaining high-resolution representations in our HRNet backbone informed their approach to preserving critical spatial and contextual features for improved reasoning accuracy. Additionally, our post-processing and model integration techniques provided insights into enhancing robustness and precision in complex multi-task environments.
In real-time applications, Feng et al. \cite{feng2024adaptive} present an advanced adaptive filtering technique, enhancing real-time state estimation under dynamic disturbances. This approach provides a foundation for robust correction mechanisms, reinforcing our efforts in accurate 6D pose estimation. He et al.\cite{he2016deep} advanced deep learning with residual connections, which have since become integral in improving network training, including in 6D estimation models.Wang et al. \cite{wang2024local} provide a robust method for handling multi-agent environments under dynamic constraints, directly informing our approach to optimizing model integration and post-processing strategies in 6D object detection for high-precision applications.

Zhao et al.\cite{zhao2021point} introduced Point Transformer, leveraging attention mechanisms to enhance point cloud processing for more detailed object geometry understanding, essential for 6D tasks. The YOLOv7-based method by Wang et al. \cite{wang2024yolov7} enhances model efficiency and accuracy, directly informing our 6D object detection pipeline's optimization for real-time performance in complex conditions.


\section{Methodology}

We presents an enhanced pipeline for 6D object detection and pose estimation, improving upon the existing 6D-VNet framework. Our approach leverages a Hybrid Task Cascade (HTC) with a High-Resolution Network (HRNet) backbone, advanced post-processing techniques, and a novel model ensembling strategy.

\subsection{Model Network}
Our model network architecture enhances 6D object detection and pose estimation through a 3-stage Hybrid Task Cascade (HTC) framework based on the ImageNet-pretrained High-Resolution Network (HRNet). Figure \ref{fig:enter-label} illustrates the complete processing workflow.


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{IIoT_Cyber.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure*}


\subsubsection{Hybrid Task Cascade (HTC)}
The HTC framework is a multi-stage object detection architecture that combines semantic segmentation and object detection tasks. Each stage refines object proposals and predictions, improving detection accuracy. The HTC is defined as:

\begin{equation}
\text{HTC}(I, \theta) = (B, S),
\end{equation}

where \(I\) is the input image, \(\theta\) denotes the model parameters, \(B\) represents the bounding box predictions, and \(S\) indicates the segmentation masks.


\subsubsection{High-Resolution Network (HRNet)}
The backbone of our network is the High-Resolution Network (HRNet), which maintains high-resolution representations throughout the model. This is crucial for tasks requiring detailed spatial information, such as segmentation and pose estimation. The HRNet consists of multiple parallel convolutions at different resolutions, allowing the network to capture features at various scales. The HRNet can be expressed as:
\begin{equation}
\text{HRNet}(I, \theta_{hr}) = F_{hr},
\end{equation}
where \(I\) is the input image, \(\theta_{hr}\) denotes the HRNet parameters, and \(F_{hr}\) represents the high-resolution features.

\subsubsection{Task Heads}
We designed two specific task heads to handle classification and regression tasks separately. 

\paragraph{Classification and Quaternion Regression Head}
This head takes the ROIAlign features from the HTC and performs car class classification and quaternion regression for rotation estimation. The output of this head is formulated as:
\begin{equation}
(C, Q) = f_{\text{head1}}(F_{roi}, \theta_{head1}),
\end{equation}
where \(C\) represents the class probabilities, \(Q\) represents the quaternion for rotation, \(F_{roi}\) denotes the ROIAlign features, and \(\theta_{head1}\) represents the parameters of this head.

\paragraph{Bounding Box and Translation Regression Head}
This head uses bounding box information to perform translation regression, predicting the center location, height, and width of the bounding box. The output of this head is formulated as:
\begin{equation}
T = f_{\text{head2}}(B, \theta_{head2}),
\end{equation}
where \(T\) represents the translation parameters, \(B\) denotes the bounding box coordinates, and \(\theta_{head2}\) represents the parameters of this head.

\subsubsection{Interactions and Integrations}
The outputs of the task heads are integrated to form the final prediction. The combination of bounding box predictions, segmentation masks, and quaternion-based rotation ensures a comprehensive detection and pose estimation output. The overall model prediction is given by:
\begin{equation}
P = (C, Q, T) = \text{HTC}(I, \theta),
\end{equation}
where \(P\) denotes the final prediction, encompassing class probabilities \(C\), quaternion rotations \(Q\), and translation parameters \(T\).

\subsection{Loss Function}
The loss function in our model is designed to balance the contributions of classification, rotation (quaternion), and translation regression tasks. By combining these components, we ensure that the model learns to accurately detect objects, predict their orientations, and estimate their positions.

\subsection{Loss Functions}
Our model's training objective combines multiple task-specific loss functions to optimize classification, orientation, and translation predictions. The classification loss \(L_{cls}\) uses cross-entropy to measure the discrepancy between predicted class probabilities and ground truth labels:

\begin{equation}
L_{cls} = -\sum_{i=1}^{N} y_i \log(\hat{y}_i),
\end{equation}

where \(N\) is the number of classes, \(y_i\) is the ground truth label, and \(\hat{y}_i\) is the predicted probability. For orientation, quaternion regression loss \(L_{quat}\) minimizes the Mean Squared Error (MSE) between the predicted quaternion \(\hat{Q}\) and the ground truth \(Q\):

\begin{equation}
L_{quat} = \|Q - \hat{Q}\|^2.
\end{equation}

The translation regression loss \(L_{trans}\) also employs MSE to predict 3D object positions by regressing the center coordinates, height, and width of the bounding box:

\begin{equation}
L_{trans} = \|T - \hat{T}\|^2,
\end{equation}

where \(T\) and \(\hat{T}\) are the ground truth and predicted translation parameters. The overall loss function combines these components with task-specific weights:

\begin{equation}
L = L_{cls} + \lambda_1 L_{quat} + \lambda_2 L_{trans},
\end{equation}

where \(\lambda_1\) and \(\lambda_2\) balance the contributions of quaternion and translation losses, respectively.


\subsection{Data Preprocessing}
We used pixel-level transforms for image augmentation from the Albumentations library. The training data includes images from the ApolloScape dataset and the competition dataset. Data preprocessing steps include:
\begin{itemize}
    \item Cleaning incorrect annotations.
    \item Normalizing images.
    \item Augmenting images to enhance model generalization.
\end{itemize}

\section{Evaluation Metrics}
We assess the model's performance using a set of evaluation metrics that cover various aspects of object detection and pose estimation, with the primary metric being mean Average Precision (mAP).

\subsection{Mean Average Precision (mAP)}
The mAP evaluates the precision-recall curve for each class and is calculated as the mean of the Average Precision (AP) values across all classes:

\begin{equation}
mAP = \frac{1}{N} \sum_{i=1}^{N} AP_i, \quad AP = \int_0^1 P(r) \, dr,
\end{equation}

where \(N\) is the number of classes, \(AP_i\) is the AP for class \(i\), and \(P(r)\) is the precision at recall \(r\).

\subsection{Intersection over Union (IoU)}
IoU evaluates bounding box accuracy and is defined as:

\begin{equation}
IoU = \frac{B_p \cap B_{gt}}{B_p \cup B_{gt}},
\end{equation}

where \(B_p\) and \(B_{gt}\) are the predicted and ground truth bounding boxes, respectively.

\subsection{Translation and Rotation Error}
Translation accuracy is evaluated using the Mean Absolute Error (MAE):

\begin{equation}
\text{MAE}_{trans} = \frac{1}{M} \sum_{i=1}^{M} \|T_i - \hat{T}_i\|,
\end{equation}

where \(M\) is the number of samples, \(T_i\) and \(\hat{T}_i\) are ground truth and predicted translations. Rotation accuracy is measured by the angular error between quaternions:

\begin{equation}
\text{Angular Error} = 2 \arccos(|Q \cdot \hat{Q}|),
\end{equation}

where \(Q\) and \(\hat{Q}\) are ground truth and predicted quaternions.

\subsection{Precision and Recall}
Precision and recall evaluate detection accuracy and completeness:

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN},
\end{equation}

where \(TP\), \(FP\), and \(FN\) are true positives, false positives, and false negatives, respectively.

These metrics provide a comprehensive evaluation of the model's capabilities in object detection and pose estimation.



\section{Experimental Results}
The metrics change with epoch in Fig \ref{fig:output}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{output.png}
    \caption{Training metrics change with epoch.}
    \label{fig:output}
\end{figure}


The performance of our models was evaluated on both the public and private leaderboards. The results are summarized in Table~\ref{tab:results}.

\begin{table}[htbp]
\caption{Performance Comparison}
\label{tab:results}
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Method} & \textbf{Private LB} & \textbf{Public LB} \\
\hline
HTC + HRNet + Quaternion + Translation & 0.094 & 0.102 \\
\hline
+ ApolloScape dataset & 0.105 & 0.110 \\
\hline
+ Post-processing (z to x, y) & 0.122 & 0.128 \\
\hline
+ Neural Mesh Renderer (NMR) & 0.127 & 0.132 \\
\hline
+ Confidence Threshold (0.1 to 0.8) & 0.130 & 0.136 \\
\hline
+ 3 Model Ensemble (Max) & 0.133 & 0.142 \\
\hline
+ Filter Test Ignore Mask & 0.136 & 0.145 \\
\hline
\end{tabular}
\end{table}

\section{Conclusion}
This study presents an improved pipeline for 6D object detection and pose estimation by integrating the Hybrid Task Cascade (HTC) framework with a High-Resolution Network (HRNet) backbone. Our approach addresses key challenges in maintaining high accuracy and precision in complex environments. Through advanced post-processing techniques and model integration strategies, our method achieves superior performance compared to state-of-the-art models on various benchmarks. These results demonstrate the effectiveness of our approach for applications such as robotics, augmented reality, and autonomous driving, where precise detection and pose estimation are crucial. Future work will focus on further enhancing robustness in diverse scenarios.









 \bibliographystyle{IEEEtran}
    \bibliography{references}

\end{document}
