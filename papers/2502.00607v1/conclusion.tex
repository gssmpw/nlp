\section{Future Directions and Closing Thoughts}
I will have accomplished my main goal if at least some readers come away from this article thinking that the bipartite matching perspective on learning is at least sometimes useful. If not, then I will settle for the secondary goal of fostering a deeper appreciation --- among the uninitiated, at least --- of the transductive model and one-inclusion graphs as a lens into learning more broadly. Now I will take some time to speculate, perhaps irresponsibly, about possible future work that may benefit from the perspectives discussed in this article. %perhaps I accomplish my secondary goalAt the very least,  transductive model --- in its elegance and simplicity --- aI hope that the transductive model --- as elegant and simple as it is --- will hopefully  At the very least, perhapsI hope that some readers might  found that it simplifies learning into something much more approachable


% Local algorithms orientation
% Compactness vs dimensions
% Alternative characterizations of sample complexity, a-la hall complexity. Can't be uniform convergence or rademacher, since those are off.
% Transductive vs PAC, in agnostic case.
% Hat puzzles
% Local regularization, various questions. Conjecture, other stuff, power of SRM, proper learning.


\paragraph{Transductive vs PAC learning.} As described in Section~\ref{sec:trans}, transductive and PAC sample complexities are essentially equivalent (up to low-order terms) in the realizable setting, but may yet be separated by a multiplicative gap of up to $\frac{1}{\epsilon}$ in the agnostic setting. Can this gap be closed? In other words, are the transductive and PAC models also essentially equivalent in the agnostic setting of learning? 

\paragraph{Hat Puzzles and Learning.} The connection between hat puzzles and the transductive model, while easily recognizable as described in Section~\ref{sec:trans}, appears to not have been explored in the literature. Can ideas and techniques from one lead to progress in the other? 

\paragraph{Local Computation and Transductive Learning.} Classification in the transductive model can be viewed as a bipartite matching problem, as described in Section~\ref{sec:class}. This does not imply computational tractability, however, since the associated bipartite graph is exponentially large. Efficient learning here entails computing a node's match by examining only its local neighborhood, without constructing the entire graph. An algorithm precisely of this sort, for binary classification problems with an efficient ERM oracle, was recently shown by \citet{daskalakis_is_2024}. More generally, this is precisely the sort of problem tackled by the literature on \emph{local computation}, which was initiated by the work of \citet{rubinfeld_fast_2011}. Much work in that area has gone into designing local approximation algorithms for matching problems (e.g. \cite{kapralov_space_2020,levi_local_2015}). Can such local matching algorithms lead to computationally-efficient and near-optimal transductive learners? While this approach can not succeed in general due to hardness results for improper learning (e.g. \cite{daniely_local_2021,tiegel_improved_2024}), it may yet be plausible for large classes of  problems or under oracle assumptions as in~\cite{daskalakis_is_2024}.  %sMuch work in that area has gone into designing local approximation algorithms for matching problems (e.g. \cite{kapralov_space_2020,levi_local_2015}), though none of these algorithmic bounds are of the sort that immediately translates to efficient near-optimal transductive learners when applied to bipartite OIGs. Can Nonetheless, we anticipate that attacking the local matching question directly for bipartite OIGs, using techniques from local computation, might shed light on the tractability of improper learning for classification. Generalizing the matching question through \fdss then promises to extend those insights to other supervised learning settings.


% Next, the PI will employ the combinatorial lens to explore the computational complexity of learning. %under assumptions that are justified in practice, or for natural classes of problems.
% \begin{question}\label{q5}
%   Can we identify natural classes of learning problems, or assumptions typically satisfied in practice, so that near-optimal improper learning is computationally efficient?
% \end{question}
% The tractability landscape of general improper learning is less understood than that of its proper counterpart.  Indeed, computational hardness results for  improper learning have been harder to come by than for the proper setting. That said, recent work provides compelling evidence, albeit under non-standard complexity assumptions, that some simple binary classification problems like intersections of halfspaces and DNF formulas are hard to learn improperly \cite{daniely_local_2021,tiegel_improved_2024}. This suggests that any positive results must make strong assumptions. One assumption which sidesteps these hardness results, and moreover is often justifiable in practice through heuristics, is tractability of the ERM problem. Whether there exists an efficient reduction from generalization to  risk minimization in improper transductive learning  is a particularly intriguing instantiation of Question~\ref{q5}. Other instantiations could involve restricting the problem class, such as for example to regression or more generally to problems where ERM can be posed as a ``smooth'' convex optimization problem. Restricting the problem class might permit extracting an explicit and tractable functional form for a near-optimal local regularizer, perhaps one similar to that employed by the VAW learner.

% Our perspective on improper learning as an implicitly-described bipartite matching problem, or a generalization thereof, can help in attacking various instantiations of Question~\ref{q5}. In terms of the bipartite OIG for multiclass classification (Sections~\ref{sec:oig_bp} and~\ref{sec:oig_agnostic}), tractability entails identifying a left side node's partner via a local matching policy, without first constructing the full graph --- see the discussion on computational complexity in Section~\ref{sec:oig_plain}. This is precisely the sort of problem tackled by the literature on \emph{local computation}, which was initiated by the work of \citet{rubinfeld2011fast}. Much work in that area has gone into designing local approximation algorithms for matching problems (e.g. \cite{kapralov_space_2020,levi_local_2015}), though none of these algorithmic bounds are of the sort that immediately translates to efficient near-optimal transductive learners when applied to bipartite OIGs. Nonetheless, we anticipate that attacking the local matching question directly for bipartite OIGs, using techniques from local computation, might shed light on the tractability of improper learning for classification. Generalizing the matching question through \fdss then promises to extend those insights to other supervised learning settings.



\paragraph{Generalizing Hall Complexity.} The Hall complexity (Section~\ref{sec:class}) characterizes the sample complexity of multiclass classification ``on the nose'' in the transductive model, and close to it in the PAC model. For more general loss functions, the closest such quantity I am aware of in the realizable setting is the $\gamma$-OIG dimension of \citet{attias_optimal_2023}, though there is substantial slack on the associated bounds on sample complexity. It seems quite plausible that a natural measure of complexity which does not qualify as a dimension, much like Hall complexity, can more tightly characterize sample complexity in large classes of (realizable and agnostic) learning problems with general loss functions.\footnote{One might be tempted to try the Rademacher complexity, covering number, or other quantities closely related to uniform convergence. However, those fail to characterize learnability even for multiclass classification \cite{daniely_multiclass_2011}.} Since the Hall complexity can be viewed as dual to the optimization problem in multiclass classification, perhaps a dual perspective on the \fds assignment problem from Section~\ref{sec:general} would yield such a measure?

\paragraph{Templates for Learning.} Local regularization and unsupervised pre-training, taken together, characterize optimal multi-class learning as described in Section~\ref{sec:class}.  Two questions stand out here: (a) Does this same template characterize near-optimal learning for problems beyond classification? Generalizing the analysis from bipartite OIGs to \fdss seems plausible as a route to such a result.  (b) Whether the unsupervised component is necessary remains an open question as described in~\citet{asilis_open_2024}: Can local regularization alone serve as a blueprint for optimal learning in classification, or even more generally?

\paragraph{Compactness vs Dimensions.} What is the exact relationship between compactness and dimensions? 
Could a learning problem exhibit compactness despite the lack of a dimension which characterizes learning, whether qualitatively or quantitatively? While I am not aware of any definitive answers, such a separation of compactness and dimensional characterizations  certainly seems plausible. As evidence for this on the quantitative front, the DS dimension for multiclass classification and the $\gamma$-OIG dimension for realizable regression --- both representing the state-of-the-art dimensions in their respective domains --- feature considerable slack in their characterization of sample complexity, despite exact compactness holding. Qualitative and quantitative separation of the two notions seems especially plausible for \emph{distribution family learning} problems, which make more nuanced distributional assumptions than the realizable or agnostic settings. The recent work of \citet{lechner_inherent_2024}, which  rules out dimensional characterizations for  some problems in distribution family learning,   might shed light on this~question.\footnote{Whereas \cite{asilis_transductive_2024} extends compactness to some particularly ``well-behaved''   distribution family learning problems, those appear to be disjoint from the problems covered by the impossibility results of \cite{lechner_inherent_2024}.}


%For which supervised learning problems does compactness imply the existence of a dimension, and for which such problems are the two notions separated? Combining ideas from \cite{asilis_transductive_2024} and \cite{lechner_inherent_2024} might shed light on these questions. %Are there natural supervised learning problems which are compact yet lack a useful dimension, and if so what do those problems look like? %The generalization of bipartite one-inclusion graphs in Section~\ref{sec:general} might shed light on this question.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_matching"
%%% End:
