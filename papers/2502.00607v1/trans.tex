\section{A Transductive Model of Learning, and why it's Good Enough}
\label{sec:trans}

I will now present the learning model that will be our main playground in this article, then try to convince you that we lose little by restricting our attention to it. This model was first employed by \citet{haussler_predicting_1994}, and has been quite busy in recent years, with its learners used as precursors to PAC learners (e.g. \cite{daniely_optimal_2014,brukhim_characterization_2022,aden-ali_optimal_2023,daskalakis_is_2024,asilis_regularization_2024,montasser_adversarially_2022}).
%
I will describe the model as conceptualized by \citet{daniely_optimal_2014} and further developed by  \citet{asilis_regularization_2024}, and refer to it simply as \emph{the transductive model}  in keeping with some of the recent literature.   % I will call this ``the single-prediction adversarial transductive model'', or simply  ``the transductive model'' as it is referred to in recent literature.
%The transductive model  has been quite busy in recent years, with its learners used as precursors to PAC learners with various guarantees (e.g. \cite{daniely_optimal_2014,brukhim_characterization_2022,aden-ali_optimal_2023,daskalakis_is_2024,asilis_regularization_2024,montasser_adversarially_2022}).
%
Truth be told, however,  the transductive approach to learning --- first explicitly articulated by \citet[Chapter 6.1]{vapnik_nature_1998} --- is much broader than just this particular model. Most notably, the transductive model I use in this article is concerned with making only a single prediction, and presumes data is chosen by a particular strong adversary. We will come back to transduction more generally later.   %\footnote{One could argue that a more appropriate moniker would be ``the single-prediction adversarial transductive model'', to place it within the broader transductive approach  first explicitly articulated by \citet[Chapter 6.1]{vapnik_nature_1998}.}

\subsection{The Transductive Model}
The (single-prediction, adversarial) transductive model is  a game of ``fill in the blank'' played between a learner and an adversary. For a given positive integer $n$, the game proceeds as follows:
\begin{enumerate}
\item The adversary chooses a \emph{labeled dataset} $(x_1,y_1), \ldots,(x_n,y_n)$, with $x_i \in \X$ and $y_i \in \Y$. \\(The adversary may  be constrained in their choice of dataset --- more on this shortly.)
\item One label $y_i$, chosen uniformly at random, is hidden. The remaining data \[(x_1,y_1),\ldots,(x_{i-1},y_{i-1}),(x_i,?),(x_{i+1},y_{i+1}),\ldots,(x_n,y_n),\] where ``?'' is a ``blank'' symbol obscuring $y_i$, is displayed to the learner. 
%\item One example $(x_i,y_i)$ is chosen uniformly at random. The remaining examples $T_{-i} =(x_1,y_1),\ldots,(x_{i-1},y_{i-1}),(x_{i+1},y_{i+1}),\ldots,(x_n,y_n)$ are displayed to the learner. 
\item The learner is prompted to make a prediction $y'_i$ for the label of $x_i$, i.e., to ``fill in the blank'', incurring loss $\ell(y'_i,y_i)$.
\end{enumerate}
%TODO: Figure with cats and blank and such

The adversary in the transductive model chooses the training and test data, but cannot control which is which: the identity of the test example $(x_i,y_i)$ is chosen uniformly at random. Then, the learner is fed training data   $\set{(x_j,y_j) : j \neq i}$ and prompted with the test datapoint $x_i$. In relation to  PAC learning, where training and test data are sourced i.i.d.~from some distribution, the transductive model can be viewed as ``hard-coding'' a uniform distribution over $n$ examples then drawing training and test data \emph{without replacement}.






We will distinguish two settings of the transductive model, \emph{realizable} and \emph{agnostic}, analogous to the same in the PAC model. The transductive model was originally considered in the realizable setting~\cite{haussler_predicting_1994,daniely_optimal_2014}, where the adversary is constrained to instances $(x_1,y_1), \ldots, (x_n,y_n)$ which are perfectly explained by a some hypothesis $h \in \H$, meaning $h(x_j) = y_j$ for all examples. The (realizable) \emph{transductive error}  of a learner on datasets of size $n$ is simply the expected loss it incurs in this game, i.e., for a worst-case realizable instance $(x_1,y_1), \ldots, (x_n,y_n)$ chosen by the adversary. Here, the expectation is over the random identity of the test example $(x_i,y_i)$ as well as any internal randomness to the learner. In the agnostic setting of this model, as articulated in~\cite{asilis_regularization_2024}, the adversary is completely unconstrained in their choice of instance $(x_1,y_1), \ldots, (x_n,y_n)$. We then shift the goalposts by subtracting the best-in-class loss on the same instance, as necessary to allow for nontrivial  learning: the \emph{agnostic transductive error} is the expected loss incurred by the learner minus $\min_{h \in \H} \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),y_i)$.

Whether in the realizable or agnostic setting, we look for learners with small transductive error rate $\epsilon(n)$, as a function of the dataset size $n$. An equivalent perspective looks at the sample complexity $m(\epsilon)$, which is the minimum dataset size guaranteeing  error at most $\epsilon$. We say a problem is \emph{learnable} in the transductive model if its transductive sample complexity is finite for every $\epsilon > 0$.




\begin{figure}%[tbp]
  \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \scalebox{0.8}{\input{figures/rectangles.tikz}}
        \caption{Labeled dataset consistent with an axis-aligned rectangle.}
        \label{fig:rectangles_a}
    \end{subfigure}
   \hfill % Optional: this inserts space between the two figures
    \begin{subfigure}{0.3\textwidth}
        \centering
        \scalebox{0.8}{\input{figures/rectangles_b.tikz}}
        \caption{When any label is omitted, both + and - are consistent with some rectangle.}
        \label{fig:rectangles_b}
      \end{subfigure}
      \hfill
    \begin{subfigure}{0.3\textwidth}
      \centering
      \scalebox{0.8}{\input{figures/rectangles_c.tikz}}
      \caption{The minimal axis-aligned rectangle is sensitive only to the four + points defining it.}
      \label{fig:rectangles_c}
    \end{subfigure}
    \caption{Realizable binary classification for axis aligned rectangles in the transductive model.}
    \label{fig:rectangles}
\end{figure}

\paragraph{Example: Axis-aligned Rectangles.} To illustrate the transductive model, consider realizable binary classification of axis-aligned rectangles as described in~\cite{haussler_predicting_1994} and illustrated in Figure~\ref{fig:rectangles}. The adversary selects $n$ datapoints in Euclidean space $\RR^d$, and labels them  in a manner consistent with some axis-aligned rectangle as in Figure~\ref{fig:rectangles_a}. A random label is then omitted, and the learner is prompted to predict this label. A learner which merely minimizes empirical risk --- i.e., arbitrarily chooses an axis-aligned rectangle which is consistent with observed labels, and fills in the missing label accordingly --- can make a mistake with probability $1$. Such a scenario is illustrated in Figure~\ref{fig:rectangles_b}. This stands in contrast to the PAC model, where empirical risk minimization guarantees a misclassification rate of $O(d/n)$ with high probability, since axis-aligned rectangles in $\RR^d$ have a VC dimension of $2d$. A more careful choice of learning rule guarantees an error probability of $\frac{2d}{n}$ also in the transductive model: choose the \emph{smallest} axis-aligned rectangle consistent with the observed labels, and fill in the missing label accordingly. This is illustrated in Figure~\ref{fig:rectangles_c}.



\paragraph{Analogy to Hat Puzzles.} The reader familiar with \emph{hat puzzles} (see e.g. \cite{krzywkowski_hat_2010,butler_hat_2009}) might notice a similarity to the transductive model. There are many variations of hat puzzles, the most relevant to us of which look something like this: There are $n$ players and a supply of hats of $k$ different colors. An adversary places a hat on the head of each of the players. Each player must then guess the color of their own hat by looking at all, or in some formulations some, of the hats of the other players. The players formulate a joint strategy in advance, and a typical goal is to maximize the number of correct guesses.

When the transductive model is applied to classification in the realizable setting, the  analogy to hat puzzles is clear: the players are $x_1,\ldots,x_n$, the hat colors are $y_1,\ldots,y_n$, and player $i$'s guessing task corresponds to the scenario in which $y_i$ is hidden. The learner  therefore corresponds to the joint strategy of the players, and transductive error is proportional to the number of players guessing incorrectly. 

There are important differences between hat puzzles and the transductive model, however. To my knowledge, most hat puzzles permit the adversary to assign hat colors either arbitrarily or uniformly at random. The transductive model, in contrast, is constrained to the hat assignments permitted by the hypothesis class (in the realizable setting, at least). Another important difference is that hat puzzles are commonly concerned with deterministic guessing strategies.  This is because the optimal randomized strategy against such a powerful adversary is the trivial one: each player randomly guesses their hat color. Obtaining a similar guarantee with a deterministic strategy is therefore the more interesting puzzle here. %trivial randomized strategy whereby each player randomly guesses their strategy of randomly guessing the hat color is optimal since the optimal randomized strategies are trivial given the power of the unconstrained adversary.

To my knowledge, the analogy between hat puzzles and transductive learning appears completely absent from the literature. Despite their differences, there might yet be fruitful connections between the two areas. One tantalizing tidbit is that the hypercube  perspective on hat puzzles articulated by \citet{butler_hat_2009} appears to be a special case of the \emph{one-inclusion graphs} that are almost synonymous with the transductive model --- more on these graphs in Section~\ref{sec:class}.



\paragraph{A Note on Transduction vs Induction, more generally.} The usual  conception of supervised learning is as a problem of \emph{inductive inference}: reasoning from particular observations (the training data) to a general rule (the chosen predictor), which can then be applied to answer specific questions at test time (labeling the test data). A (seemingly) more permissive approach is to care not for the rule itself, but for its evaluation at the particular points of interest; i.e.,  we reason from particular observations (the training data) to answer specific questions (labeling the test data). This is \emph{transductive learning} as first explicitly articulated by~\citet{vapnik_nature_1998}, though the essential ideas date back to \citet{vapnik_theory_1974} and \citet{vapnik_estimation_1982}. Notice that this general transductive approach does not presuppose a particular data model or objective, and can be considered in other setups such as the PAC model, online learning, etc.

When there is only a single test point, and learners are allowed to be improper, induction and transduction are essentially equivalent. Indeed,  the formal distinction between the two becomes simply a matter of \emph{currying}\footnote{Currying is a notion common in functional programming, and amounts to the observation that a function \linebreak $g: A \times B \to \C$ can equivalently be viewed as a function $g': A \to (B \to C)$, with $g'(a)$ itself a function from $B$ to $C$.}  --- see the related discussion in~\cite{montasser_transductive_2022}. This equivalence no longer holds in general when multiple test datapoints must be labeled, nor in the proper regime. Since we operate in the improper regime with a single test example, the distinction between transduction and induction becomes merely a matter of perspective for our purposes. Nonetheless, the  transductive model is best thought of as reasoning for a specific test example, hence the name.
% TODO: Say something about how transductive and improper are essentially synonyms as far as we're concerned. Transductive is a perspective that enables improperness by default.
%By catering the learned rule to a specific test example, transduction can be viewed as ``defaulting'' to improper learners.

A related perspective on the distinction between transduction and induction  is the following. By catering the learned predictor to a specific test example, transductive learners are ``improper by default'' when viewed across all possible test points. Indeed, by focusing only on prediction and freeing the learner completely from representing any general rule, the transductive perspective unleashes the full power of improper learning. Induction, in contrast, is most meaningful when encoding an a-priori bias towards rules of a particular form, as is the case in the proper regime.



\subsection{Relationship to the PAC Model}
\label{subsec:pac_trans}
Despite seeming  incomparable at first glance, the PAC and transductive models turn out to be closely related.\footnote{This is specific to the  improper (i.e., unrestricted) regime of PAC learning, which is our focus in this article. The transductive model, in allowing improperness by default, is more permissive than proper PAC learning~\cite{daniely_optimal_2014}.} Speaking qualitatively, learnability is equivalent in the two models, and this holds for both realizable and agnostic learning problems with bounded loss functions.

Quantitatively, we can say something quite strong in the realizable setting with bounded losses: The two models are equivalent up to  low-order factors in the error and the sample complexity. This follows from  efficient black-box reductions  between the two models, one in each direction. We therefore lose very little, and gain quite a lot through the connection to matching and otherwise, by studying the transductive model instead of the PAC model for realizable learning problems.

The exact quantitative relationship between the two models  is more unsettled in the agnostic setting: Whereas it appears that PAC learning is essentially  no harder  transductive learning (up to low-order terms, for most natural loss functions, through an efficient black-box reduction), it remains plausible that transductive sample complexity exceeds its PAC counterpart by a multiplicative factor on the order of $1/\epsilon$, even for classification problems.
%
This gap of $1/\epsilon$ in agnostic sample complexities is the main asterisk to the stance I take in this article of treating the PAC and transductive models as interchangeable. Given  the transductive model's  fruitful connections to basic graph-theoretic questions, as well as the possibility that future work could eliminate this gap, I hope the reader finds this qualification forgivable.

I will now recap both the realizable and agnostic states of affairs in more detail.



\subsubsection*{The Realizable Setting}
  In the realizable setting with losses in $[0,1]$, the transductive and PAC models are equivalent up to  low-order factors in the error and the sample complexity. This is somewhat folklore knowledge in the field, but the proofs are written down explicitly in~\cite{asilis_regularization_2024}.  I'll recap those proofs informally next.

For one direction of the equivalence, a realizable PAC learner guaranteeing error $\epsilon$ with probability $1-\delta$ on $n$ samples can be converted to a realizable transductive learner guaranteeing error $O(\epsilon + \delta)$ on datasets consisting of $n$ samples. This is done in pretty much the obvious way: the PAC learner is trained on uniform draws from the transductive dataset, and invoked to make a prediction for the test point. Since there is a constant probability the test point is not in the training data, the PAC learner solves a strictly harder problem with constant probability, from which the bound follows by Markov's inequality.

For the other direction, a realizable transductive learner guaranteeing error $\epsilon$ on datasets of size $n$ can be  converted to a realizable PAC learner which guarantees error $O(\epsilon)$ with probability $1-\delta$ when given $O(n \log \frac{1}{\delta})$ samples. This again is a pretty elementary reduction: the transductive learner guarantees expected loss $\epsilon$ on a PAC instance via a \emph{leave one out} argument --- essentially conditioning on the training and test data and invoking the principle of deferred decisions with regards to which is which. This can then be boosted to a high probability guarantee by repetition $O(\log \frac{1}{\delta})$ times. Markov's inequality and the union bound show that one of the predictors has loss $O(\epsilon)$ with probability $1-\delta/2$, and a small hold-out set of examples can be used to identify such a predictor with probability $1-\delta/2$.

For the large class of metric loss functions, a more sophisticated  reduction gets rid of the multiplicative blowup in sample complexity \cite{aden-ali_optimal_2023,dughmi_is_2024}. Specifically, the number of samples is reduced from $O\left(n \log \frac{1}{\delta}\right)$ to $n+O\left(\frac{1}{\epsilon} \log \frac{1}{\delta}\right)$. This is possible through a more economical repetition construction which reuses examples across multiple invocations of the transductive learner.


\subsection*{The Agnostic Setting}
In relating the transductive and PAC models in the agnostic setting, the best we can currently say for general bounded loss functions is the following:  The previously-described realizable reductions go through with a degraded bound on the number of samples, by a factor of at most $O(1/\epsilon)$. To see why this might be, note that both those reductions rely on Markov's inequality, which guarantees that a (random) predictor with expected loss $\epsilon$ has loss at most $2 \epsilon$ with probability at least $1/2$. Agnostic error discounts the loss of a predictor by the best-in-class loss, and in doing so no longer enjoys the same Markov guarantee. Indeed, when $\epsilon$ denotes this discounted expected loss, the probability guarantee provided by Markov's inequality degrades from $1/2$ to $O(\epsilon)$. Carrying this through either reduction increases the sample complexity bound by a factor of~$O(1/\epsilon)$.

This multiplicative blowup of $1/\epsilon$ in sample complexity seems somewhat more fundamental in one of the directions than the other.  For the large class of metric loss functions, a transductive learner guaranteeing agnostic error $\epsilon$ on datasets of size $n$ can be converted to a PAC learner guaranteeing agnostic error $O(\epsilon)$ with probability $1-\delta$ when given $n+O\left(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon\delta}\right)$ samples. This is through an adaptation of the economical repetition construction of \citet{aden-ali_optimal_2023} to the agnostic setting by  \citet{dughmi_is_2024}.

This suggests that PAC learning is essentially no harder than transductive learning for most natural loss functions. Whether the converse is true remains very much in question,  as discussed and explored in \cite{dughmi_is_2024}. Specifically, it remains plausible that transductive sample complexity exceeds PAC sample complexity by a factor of up to $O(1/\epsilon)$, even for multiclass classification. Such a gap in sample complexity was ruled out for binary classification through a non-reduction approach in~\cite{dughmi_is_2024}, where we also conjecture that the gap can be  eliminated more generally.


%The equivalence is perhaps less tight for the agnostic setting, with the naive reductions suffering a blowup of $O\left(\frac{1}{\epsilon}\right)$ in sample complexity --- whether this is avoidable will be tackled  in  Section~\ref{sec:structural}. Nonetheless, learnability is equivalent for the PAC and transductive models, and the sample complexities are related through efficient reductions. We therefore often blur the distinction between the two models in this proposal.







%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_matching"
%%% End:
