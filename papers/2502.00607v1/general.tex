
\newcommand{\fds}{FDS\xspace}
\newcommand{\fdss}{FDSs\xspace}


\section{Learning with General Loss Functions}
\label{sec:general}
In this section I will show how the bipartite matching perspective, appropriately generalized, can lead to insights for supervised learning beyond classification. First I describe a generalization of the bipartite OIG, implicit in \citet{asilis_transductive_2024}, intended to capture learning beyond classification. I then overview one application of this approach, also from \cite{asilis_transductive_2024}: Appropriately extending Hall's theorem leads to a quite-general \emph{compactness result} for learning, one which relates the sample complexity of a problem to that of its finite projections, in both the transductive and PAC models.



\subsection{A Generalization of the One-Inclusion Graph}
\label{sec:fds}

%Describe the metric OIG

A \emph{Functional Dependency Structure (\fds)} is a potentially-infinite bipartite graph with \emph{input variables}   on the left, \emph{output variables}  on the right, and edges annotated with functions encoding the dependencies between inputs and outputs. Let $A$ and $B$ be index sets for the left and right side nodes of the graph, respectively, and let $E \sse A \cross B$ be the set of its edges. For each left side node $a \in A$ we associate an input variable $u_a$ which takes values in some set $\Z_a$. For each right side node $b \in B$ we associate an output variable $v_b$ which takes real number values. Each output variable depends on finitely many input variables, and we include the edge $(a,b)$ in $E$ whenever $v_b$ depends on $u_a$.  Notably, nodes on the right have finite degrees, but we make no such assumption for the degrees of nodes on the left.
Each edge $e=(a,b)$ is labeled with a \emph{cost function} $f_{e}: \Z_a \to \RR$, and the output variables average the incident cost functions. More concretely,  $v_b$ evaluates to the average, over all its incident edges $e=(a,b)$,  of $f_{e}(u_a)$. %We  also require some mild topological assumptions on the input domains $\Z_a$, as well as on the cost functions $f_e$; we defer discussion of those for now.
%We will also require some mild topological assumptions, which are benign for our applications to learning: Each domain $\Z_a$ is a Hausdorff space, and each cost function $f_{e}$ is \emph{proper} in that the preimage of any compact set is compact.% it reflects compact sets:  $f_{(a,b)}^{-1}(C)$ is a compact subset of $\Z_a$  whenever $C \sse \RR$ is compact. 
% satisfying some minimal topological assumptions,\footnote{It will suffice to assume that $\Z_\ell$ is a Hausdorff space. In fact, in our applications to supervised learning $\Z_\ell$ will be a metric space of labels, and it is known that all metrizable spaces are Hausdorff.}



Given an \fds, our objective is to assign values to the input variables so as to minimize the maximum output variable. If an assignment to the inputs maintains all outputs  weakly below $\epsilon$, we call it an \emph{$\epsilon$-assignment}.
%
%In the \emph{\fds assignment problem},  we seek an assignment of values to the input variables which minimizes the maximum output variable. If an assignment to the inputs maintains all outputs  weakly below $\epsilon$, we call it an \emph{$\epsilon$-assignment}.
%
Next, I describe how this \emph{\fds assignment problem} generalizes both bipartite matching and transductive learning with general losses. 


\paragraph{Infinite Bipartite Matching.}  Consider  matching one side of an infinite bipartite graph into the other, where the side  to be matched (say, the right side) has finite degrees. Let $G$ be a bipartite graph with left side $A$, right side $B$, and edges $E$. Let $\deg(b)$ denote the degree of a node $b \in B$, which we assume is finite.  We associate with each $a \in A$ a variable $u_a$ with domain $\Z_a \sse B$ equal to the neighbor set of $a$. For each edge $e=(a,b) \in E$, we associate the cost function $f_e(z) = [z \neq b] + \frac{1}{\deg(b)}$. A simple calculation shows that a matching of $B$ into $A$ corresponds to a $1$-assignment in this \fds. %there is an assignment to the input variables which maintains all output variables weakly below $1$. Therefore, the \fds assigment problem serves as a common generalization of both supervised learning and bipartite matching with finite degrees.



\paragraph{Classification.} As a warm-up, I will start with classification in the realizable setting. Given a hypothesis class  $\H \sse \Y^\X$ and  datapoints $S=(x_1,\ldots,x_n) \in \X^n$,  the bipartite OIG $G_{\bp}(\H,S)$ can easily be seen as an \fds.  For each partial labeling $\tilde{\y}=(y_1,\ldots, ?, \ldots, y_n)$ on the left side of $G_{\bp}$,  with ``?'' in the $i$th entry, we associate an input variable $u_{\tilde{\y}}$ with domain $\Y$. An assignment to $u_{\tilde{y}}$ is interpreted as a prediction for  $y_i$ when the learner is given the remaining labels $\set{y_j}_{j \neq i}$. For each full labeling $\y=(y_1,\ldots,y_n)$ on the right side of $G_{\bp}$ we associate an output variable $v_{\y}$, which measures the misclassification rate in  ground truth $\y$. The edges $(\tilde{\y},\y)$ of $G_{\bp}$, if labeled with $f_{\tilde{y},y}(z) = [z \neq y_i]$ when $i$ is the index of ``?'' in $\tilde{y}$, now appropriately encode the dependencies between input and output variables. Notice that the nodes on the right have finite degree $n$, as required in an \fds. An $\epsilon$-assignment for this \fds now corresponds to a  transductive learner with realizable error at most $\epsilon$.

For agnostic classification, the bipartite agnostic OIG $G_{\bp}^{\ag}(\H,S)$ can be similarly interpreted as an \fds, with one main difference: Since we benchmark our learner relative to $\H$, we appropriately ``discount'' each cost function  by the best-in-class loss. Specifically, we let $f_{\tilde{y},y}(z) = [z \neq y_i] - \min_{h \in \H} \frac{1}{n} \sum_{j=1}^n  [h(x_j)~\neq~y_j]$. An $\epsilon$-assignment for this \fds now corresponds to a transductive learner with agnostic error at most $\epsilon$.




\paragraph{Learning with General Losses.} %We now move beyond classification, to general loss functions.
More generally, consider a supervised learning problem in the transductive model with hypotheses $\H \sse \Y^\X$, loss function $\ell: \Y \times \Y \to \RR_{\geq 0}$, and datapoints $S=(x_1,\ldots,x_n) \in \X^n$. Whether in the realizable or agnostic setting, we use the same \fds as in the corresponding classification problem except for appropriately exchanging the loss functions. Specifically, for $\y=(y_1,\ldots,y_n)$ and $\tilde{\y}=(y_1,\ldots, ?, \ldots y_n)$ with ``?'' at the $i$th position, we let $f_{\tilde{y},y}(z) = \ell(z,y_i)$ for the realizable setting, and $f_{\tilde{y},y}(z) = \ell(z,y_i) - \inf_{h \in \H} \frac{1}{n} \sum_{j=1}^n \ell(h(x_j),y_j)$ in the agnostic setting.  In either setting, an $\epsilon$-assignment for the appropriate \fds now corresponds to a transductive learner with error at most $\epsilon$.














\subsection{Implication: Compactness of Learning}
\label{sec:compactness}

 \emph{Compactness} refers to situations where a property of a mathematical object is determined by finite parts of that object. For example, an infinite graph is $k$-colorable if and only if the same holds for its finite subgraphs~\cite{bruijn1951colour}, a set of formulae in propositional logic is satisfiable precisely when any finite sub-family is satisfiable, and so on. Given that learning problems often feature infinite data domains, label sets, and hypothesis classes, it is natural to ask whether the same holds for learning. Specifically, can the difficulty of a learning problem, as measured by its sample complexity, be determined by inspecting its \emph{finite projections}? By those, recall that we mean the learning problems obtained by restricting to a finite set of data points, labels, and hypotheses.


 
 
%Finite Characterization
Learning in the transductive model can be encoded exactly as an \fds assignment problem, as described in Section~\ref{sec:fds}.   The \fds assignment problem generalizes infinite bipartite matching problems of the form described in Section~\ref{sec:fds}, and those exhibit compactness via M. Hall's infinite marriage theorem~\cite{jr_distinct_1948} as discussed in Section~\ref{sec:hall}. Generalizing the infinite marriage theorem from bipartite graphs to \fdss would, therefore, extend compactness to learning.

This is precisely what we show in \citet{asilis_transductive_2024} by combining and extending ideas from three distinct proofs of the infinite marriage theorem \cite{jr_distinct_1948,rado_note_1967,halmos_marriage_1950}. Subject to mild topological assumptions on the input domains $\Z_a$ and cost functions $f_e$,\footnote{It suffices to assume that each $\Z_a$ has the topological structure of a metric space, and that each $f_e$ reflects compact sets, i.e.,  $f_e^{-1}(C)$ is compact whenever $C$ is compact.} an \fds admits an $\epsilon$-assignment precisely when the same holds for its subgraphs that are induced by a finite collection of right side nodes along with their neighbors. In other words, having an $\epsilon$-assignment is a compact property, being determined by the finite subgraphs of the \fds.  

Compactness of the \fds assignment problem implies a similar compactness result for most natural learning problems, including those with metric losses satisfying a mild topological assumption,\footnote{The metric space should satisfy the \emph{Heine-Borel} property, meaning that closed and bounded sets are compact.} as well as those with continuous losses on a compact label space. Such a problem is learnable with sample complexity $m(\epsilon)$ in the transductive model precisely when the same holds for all its finite projections. This holds for both the realizable and agnostic settings. %Therefore, learnability with a given rate is said to be a \emph{property of finite~character}.
%
These compactness results extend approximately to the PAC model, as per the relationships in Section~\ref{subsec:pac_trans}.


One interpretation of these results is the following: When learners are allowed to be improper, the obstacles to learning are finite in nature. This stands in contrast to influential recent work of \citet{ben-david_learnability_2019} which showed a different state of affairs when learners are constrained to be proper. There, a problem can be unlearnable --- or, even worse, its learnability may be independent from the axioms of ZFC set theory --- even though its finite projections are easily learned.\footnote{Note that \cite{ben-david_learnability_2019} do not pose their non-compact problem, which they term \emph{EMX learning}, as a supervised learning problem. However, it is not difficult to see that it can be rephrased as such.} 




\paragraph{Compactness vs Dimensions.} Compactness seems to be a weaker property than the existence of a \emph{dimension} for learning, in the usual sense of the term. Dimensions such as the VC dimension, DS dimension, Natarajan dimension, and others are typically defined to equal the largest (finite) number of datapoints over which finitely many hypotheses are ``sufficiently rich.'' For problems with continuous loss functions, the notion of sufficiently rich can be scale sensitive, being parametrized by the desired error; a typical example of this is the fat shattering dimension for regression.  Whether scale sensitive or not, such a dimension is a  \emph{compact} function of the hypothesis class, and therefore so is any bound on the sample complexity purely in terms of that dimension. %Any approximation slack in the expression for sample complexity may or may not be inherited by the compactness guarantee.

%Could a learning problem exhibit compactness despite the lack of a dimension which characterizes learning, whether qualitatively or quantitatively? While I am not aware of any definitive answers, such separation of compactness and dimensional characterizations  certainly seems plausible. As evidence for this on the quantitative front, the DS dimension for multiclass classification and the $\gamma$-OIG dimension for realizable regression --- both representing the state-of-the-art dimensions in their respective domains --- feature considerable slack in their characterization of sample complexity, despite exact compactness holding. Recent work of \citet{lechner_inherent_2024} ruling out dimensional characterizations for  \emph{distribution family learning}, which allows more nuanced distributional assumptions than the realizable or agnostic settings,    might shed light on this~question.\footnote{Whereas \cite{asilis_transductive_2024} extends compactness to some particularly ``well-behaved''   distribution family learning problems, those appear to be disjoint from the problems covered by the impossibility results of \cite{lechner_inherent_2024}.}

%For example, the VC dimension tightly characterizes the sample complexity of binary classification both in the PAC \cite{shalev-shwartz_understanding_2014} and transductive \cite{haussler_predicting_1994,equiv} models, and this implies compactness. For multiclass classification, the DS dimension characterizes the PAC sample complexity up to a constant in the exponent \cite{brukhim_characterization_2022}, and this implies compactness in the same approximate sense. A significantly tighter compact characterization for multiclass classification is not through the DS dimension, but rather through the average subgraph density as 


%  The final thrust of the proposed research concerns structural characterizations of learnability.
% The prime example of such a characterization is the \emph{VC dimension} of a binary classification problem~\cite{vapnik1971uniform}. This is defined as the maximum number $d$ of datapoints which are \emph{shattered} --- meaning that all $2^d$ classifications are possible --- by the hypotheses. A realizable or agnostic binary classification problem is learnable precisely when its VC dimension is finite, in which case its sample complexity is tightly characterized as a function of the VC dimension both in the PAC \cite{shalev-shwartz_understanding_2014} and transductive \cite{haussler_predicting_1994,equiv} models.   When the hypothesis class is itself parameterized by some measure $k$ of complexity (e.g. the intersection of $k$ halfspaces), an understanding of the VC dimension  can aid in choosing an appropriate model complexity for the available data.

% The VC dimension, and associated shattered sets, sharply describe the maximal  ``obstacles'' to learning in binary classification. A similar quantity for agnostic regression with $\ell_1$ loss, parameterized by a ``scale'' $\gamma$ of the desired error, is the \emph{$\gamma$-fat shattering dimension} \cite{bartlett_fat-shattering_1996,alon_scale-sensitive_1997}. The search for  characterizations beyond these settings is a very active area of research. Exciting recent progress includes the characterization of multiclass learnability in terms of the \emph{DS dimension} \cite{daniely_optimal_2014,brukhim_characterization_2022}, and of realizable regression in terms of the \emph{$\gamma$-OIG dimension} \cite{attias_optimal_2023}, with bounds relating those dimensions to the sample complexity. However, to date there is no ``dimension''  known to \emph{sharply} capture the sample complexity for a large class of supervised learning problems beyond binary classification and agnostic regression. We therefore take a different approach rooted in bipartite OIGs and \fdss (Section~\ref{sec:oig}), extracting from them simple quantities --- not necessarily ``dimension-like'' --- which exactly, or up to a constant, characterize the error rate of optimal transductive learning.  We start with the following question.%To further understand implications for PAC learning, we will also explore the exact relationship to the transductive setting.
% %validate the route we take through transductive learning,  the proposed research will also explore the exact relationship between transductive and PAC learning. % extracts, from bipartite OIGs and our proposed generalizations of them, simple quantities which sharply characterize optimal learning in large families of supervised learning problems. 



% For classificatoin, compactness follows from VC. For multiclass, approximate compactness from DS, and even better max subgraph density.
%For realizable regression, approximate compactness from Karbasi paper. For agnostic regression, from fat shattering maybe?
%Compactness vs dimension





% This question turns out to be quite subtle. Compactness can be seen to fail in the proper regime of learning, as implied by the work of \citet{ben-david_learnability_2019}.\footnote{We note that \cite{ben-david_learnability_2019} do not pose their non-compact problem, which they term \emph{EMX learning}, as a supervised learning problem. However, it is not difficult to see that it can be rephrased as such.}
% In the improper regime of learning, compactness TODO: CONTINUE HERE WITH HISTORY AND CONTEXT




%%%Local Variables:
%%% mode: latex
%%% TeX-master: "learning_matching"
%%% End:
