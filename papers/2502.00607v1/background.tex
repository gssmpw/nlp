\section{Basic Background: Supervised Learning and the PAC Model}
\label{sec:background}

At this point almost everyone has heard of machine learning (ML). Anyone likely to stumble upon this article will have also heard of its most influential special case, supervised learning, and those theoretically inclined will also be familiar with the PAC model. Nonetheless, I will set the stage by  recapping the basics.

\subsection{Basics of Supervised Learning}%Let's set the stage in any case

\emph{Supervised Learning} is the task of ``coming up'' with a function $f: \X \to \Y$ to ``explain'' or ``fit'' a sequence of input/output examples   $(x_1,y_1), \ldots, (x_n,y_n)$, with $x_i \in \X$ and $y_i \in \Y$.  Here $\X$ is a \emph{data domain} consisting of \emph{datapoints} $x \in \X$, $\Y$ is a \emph{label set} consisting of \emph{labels} $y \in \Y$, and the sequence $(x_1,y_1),\ldots,(x_n,y_n)$ is the \emph{training data} consisting of \emph{labeled examples (a.k.a. samples)}~$(x_i,y_i)$.  I~will refer to the chosen function $f$ as a \emph{predictor}, and to $n$ as the \emph{sample size}. A \emph{learning algorithm} takes as input training data, and outputs (some representation of) a predictor $f \in \Y^\X$.\footnote{Note that this describes the usual \emph{batch}, a.k.a.~\emph{offline}, setting of supervised learning. I do not discuss other paradigms such as online or active learning in this article.} 



Success in supervised learning is defined as \emph{generalization} to  future examples: For a typical \emph{test example}  $(x_{\tst},y_{\tst})$, the predicted label $y'_{\tst}=f(x_{\tst})$ should ``equal'' $y_{\tst}$, perhaps approximately. We usually assume the test example is drawn from the same  ``source'' as the training data  --- commonly, i.i.d.~from the same distribution. The quality of the prediction is quantified by $\ell(y'_{\tst},y_{\tst})$, where $\ell:~\Y~\times~\Y \to \RR_{\geq 0}$ is a \emph{loss function} chosen as part of the problem definition. Common loss functions include the 0-1 loss $\ell_{0-1}(y',y) = [y' \neq y]$ for \emph{classification} problems,\footnote{The notation $[P]$ denotes $1$ when predicate $P$ is true, and denotes $0$ when $P$ is false.} as well as the absolute loss $|y'-y|$ or squared loss $(y'-y)^2$ for \emph{regression problems} featuring $\Y  \sse \RR$.

Nontrivial generalization properties are typically only possible if one assumes something about the data.\footnote{The need for such an assumption is formalized by the  \emph{no free lunch theorems} of supervised learning \cite{wolpert_connection_1992,wolpert_lack_1996,schaffer_conservation_1994}.} The Bayesian approach to  machine learning, common in many applications, assumes some parametric form for the distribution generating the data, and postulates a prior on the parameters. This is not the approach I will take in this article. Instead, I will focus on the frequentist --- and some would say ``worst-case'' or ``adversarial'' ---  approach that is common in the computational learning theory community, embodied by the PAC model. Here we assume that the (training and test) data can be explained, perhaps approximately, by a function in some ``simple enough to learn'' class of functions $\H \sse \Y^\X$, often called the \emph{hypotheses}. Equivalently, we  seek a predictor which explains the unseen data roughly  as well as the best hypothesis $h^* \in \H$, whether or not we assume that $h^*$ itself provides a perfect explanation.



 \paragraph{Common Algorithmic Templates.} Perhaps the best known general-purpose supervised learning algorithm is \emph{empirical risk minimization (ERM)}, which chooses as its predictor a hypothesis $f \in \H$ minimizing $\frac{1}{n} \sum_{i=1}^n \ell(f(x_i),y_i)$ --- a quantity called the \emph{training error}, \emph{empirical error}, or \emph{empirical risk} of $f$. %\footnote{When multiple hypotheses minimize the empirical risk, we assume ERM breaks ties arbitrarily.}
A common template for generalizing ERM involves adding a \emph{regularization term} $\psi(f)$ to the  objective function, typically chosen to measure some notion of ``hypothesis complexity.'' An algorithm instantiating this template is known as a \emph{structural risk minimizer (SRM)}, and chooses as its predictor the hypothesis $f \in \H$ minimizing the \emph{structural risk} $\frac{1}{n} \sum_{i=1}^n \ell(f(x_i),y_i) + \psi(f)$. Other well-known algorithms, such as gradient descent and its variations,  can frequently be interpreted as approximate implementations of ERM or SRM.


\paragraph{Proper vs Improper Learning.} A learning algorithm is said to be \emph{proper} if its predictor $f$ is always chosen from the hypothesis class, i.e., $f \in \H$, otherwise it is said to be \emph{improper}. ERM  is an example of a proper learning algorithm, as are SRM algorithms of the form described above.  In the \emph{proper regime} of learning, algorithms are required to be proper. This article will be concerned with the more flexible \emph{improper regime} (a.k.a \emph{representation-independent learning}), where no such constraint is placed on the learner. In other words, all we care about is predictive power at test time, rather than any insights derived from the functional form or representation of the predictor~itself.


\subsection{The PAC Model}
A standard mathematical setup for evaluation of supervised learning algorithms, at least in the theoretical computer science community, is Valiant's \emph{Probably Approximately Correct (PAC) model} of learning (see e.g.~\cite{kearns_introduction_1994,mohri_foundations_2018}). Here, we assume there is an unknown distribution $\D$ on $\X \times \Y$ from which training and test data are  drawn.  Specifically, the labeled datapoints of the training set  $(x_1,y_1), \ldots, (x_n,y_n)$, as well as the test data  $(x_\tst,y_\tst)$, are i.i.d.~from $\D$. Often it is assumed that $\D$ lies in some class of distributions of interest. The \emph{true expected loss}, or simply \emph{loss}, of a predictor $f: \X \to \Y$ is the expected loss it incurs on draws from $\D$, written $L_\D(f) = \Ex_{(x,y) \sim \D} \ell(f(x),y)$.


There are two main ``settings'' in PAC learning. The  \emph{realizable setting} only requires that the data be perfectly explained by some hypothesis in $\H$. More generally, the \emph{agnostic setting} makes no assumption relating the data to the hypotheses, but shifts the goalposts as necessary to allow nontrivial guarantees: the expected loss at test time is evaluated only ``relative'' to that of the best hypothesis $h^* \in \H$. There are other settings which make more nuanced assumptions, such as $\D$ being of a particular parametric form or its support living in some (unknown) lower-dimensional space, etc. I will mostly discuss the realizable and agnostic settings in this article, those being the simplest and most studied from a theoretical perspective. %TODO:We will briefly discuss other settings in Section ??

The PAC model demands high probability guarantees of learners, in the worst case over distributions of interest. Consider first the realizable setting, where $\D$ is such that $\min_{h \in \H} L_{\D}(h) = 0$. A PAC learner has \emph{error} $\epsilon=\epsilon(n)$ and \emph{confidence} $\delta=\delta(n)$ if, when training data consists of $n$ i.i.d~samples from a realizable distribution $\D$, it produces a predictor $f$  satisfying $L_\D(f) \leq \epsilon$ with probability at least $1-\delta$. In the agnostic setting, where $\D$ can be arbitrary, we require $L_\D(f) - \min_{h \in \H} L_\D(h) \leq \epsilon$ with probability $1-\delta$.

In both the realizable and agnostic settings, we look for PAC learners with small $\epsilon$ and $\delta$ as a function of the sample size $n$. An equivalent perspective looks at the sample complexity $m(\epsilon,\delta)$, which is the minimum sample size which guarantees error  at most $\epsilon$ with probability at least $1-\delta$. We say a problem is \emph{PAC learnable} if its PAC sample complexity is finite whenever $\epsilon,\delta > 0$.

For most PAC learning problems, learnability and sample complexity are characterized in terms of a  ``dimension'' of the hypothesis class. Most prominently this is the \emph{VC dimension} for binary classification, the \emph{fat shattering dimension} for agnostic regression, and the \emph{DS dimension} for multiclass classification (see \cite{anthony_neural_1999,daniely_optimal_2014,brukhim_characterization_2022}). Treatment of these is beyond the scope of this article. The unfamiliar reader need not worry, however,  as dimensions will feature only tangentially in our~discussion.




%\paragraph{Learning settings: Realizable, Agnostic, etc.} In learning theory, evaluating a supervised learning algorithm requires specifying a data model and an objective. We will leave the details of the data model flexible for now, to allow for both the PAC model and the adversarial transductive model. Nonetheless we will describe two variations, which we call ``settings'', which cut across different models. The  \emph{realizable setting}  requires only that the data be perfectly explained by some hypothesis $h \in \H$ --- i.e., there exists a hypothesis which is guaranteed to suffer a loss of $0$ on training and test data. The performance of the learning algorithm is its expected loss at test time for some ``worst case'' realizable instance. More generally, the \emph{agnostic setting} makes no assumption relating the data to the hypotheses, but shifts the goalposts as necessary to allow nontrivial guarantees: the expected loss at test time is evaluated only ``relative'' to that of the best hypothesis $h^* \in \H$, again for some ``worst case'' instance. There are other settings which make more nuanced assumptions about the data, such as it is drawn from a distribution of a particular parametric form, or that it lives in some (unknown) lower-dimensional space, etc. We will mostly discuss the realizable and agnostic settings, those being the simplest and most studied from a theoretical perspective.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_matching"
%%% End:
