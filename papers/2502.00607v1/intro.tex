\section{Introduction}
The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to --- of all things --- bipartite matching! %I found this connection surprising and remarkably helpful, as a theoretical computer scientist who thinks primarily about discrete algorithms and optimization, in my  ongoing attempts at doing something plausibly useful or at least interesting in learning theory.
\mbox{En-route} from PAC learning to bipartite matching, I will overview a particular \emph{transductive model} of learning, and associated \emph{one-inclusion graphs}, which can be viewed as a generalization of some of the \emph{hat puzzles} that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. In hindsight, a secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.
%

I found these perspectives on learning surprising and relatable, as an outsider who primarily thinks about discrete algorithms and combinatorial optimization.
I hope this article  can help others of similar disposition approach learning theory, and perhaps even help researchers uncover more connections between machine learning and classical questions in combinatorics and optimization.

In what sense is PAC learning ``just matching''? For classification problems, quite literally: Multiclass classification in the PAC model is approximated by an implicitly-described bipartite matching problem, albeit a huge one. %In this bipartite matching problem, each node on the left corresponds to a realization of the (labeled) training data and the (unlabeled) test point, and each node on the right corresponds to a possible way of filling in the missing label for the test point. Learning algorithms are then simply assignments
Mathematicians and computer scientists know a lot about the structure of bipartite matchings, and I will demonstrate how we can invoke this understanding to deduce properties of learning problems and algorithms.

Outside of classification, e.g. for regression or problems with structured multivariate labels, the connection to matching is less direct. The learning task is approximated by an implicitly-described optimization problem which resembles a (non-linear) fractional generalization of bipartite matching. While this seems to preclude applying ideas from matching theory directly, it suggests that starting from what you know about matching and generalizing it appropriately might uncover useful structure. I will outline one example of this approach, and speculate wildly  about others.


\subsection*{Outline}
This article is structured as follows. First, I will recap the basics of supervised learning and the PAC model. Then I strip away many of the details of learning by reducing to a particular \emph{transductive model}. This is a bare-bones, distribution-free model of supervised learning which has its roots in the earliest days of the field \cite{vapnik_theory_1974,vapnik_estimation_1982,haussler_predicting_1994}. The model also bears striking similarities, but also important differences, with \emph{hat puzzles} from recreational mathematics.  I will try to convince you that this transductive model  is ``close enough,'' for most intents and purposes, to the PAC model.
% If you find the details and jargon of distributional machine learning overwhelming and dizzying, this simplification might calm your nerves.

Once in the transductive model, we will get into the most accessible of supervised learning problems: classification. I will review the \emph{one-inclusion graph} \cite{bondy_induced_1972,alon_partitioning_1987,haussler_predicting_1994}, which exactly encodes transductive classification as a combinatorial optimization problem. I will reinterpret this optimization problem as bipartite matching by a simple change of perspective \cite{asilis_regularization_2024}, and describe some implications of this matching perspective for understanding the space of near-optimal classification algorithms, as well as characterizing the optimal learning rate. 

We will then move on to more general supervised learning problems, to include things like regression or pretty much anything else. I will present a generalization of the one-inclusion graph that again exactly encodes transductive learning, and argue that this generalization is still intimately tied to bipartite matching. To drive that point home, I will present one example of how we start from a deep fact about bipartite matching --- its \emph{compactness} in a set-theoretic sense --- and generalize it appropriately to derive compactness for the sample complexity of learning~\cite{asilis_transductive_2024}.

Finally, I will close with irresponsible speculation about future applications of this perspective.

\subsection*{Disclaimer} This article explores one perspective on PAC learning which I personally found helpful,  and did not see written down in one place as I attempt to do here. More standard perspectives and techniques include dimensional characterizations of learnability, sample compression, uniform convergence, and others. Discussion of those at any length is beyond the scope of this article, and above my pay grade.  For a well rounded treatment of machine learning theory, by actual experts, refer to any of the excellent texts on the topic such as \citet{shalev-shwartz_understanding_2014}, \citet{mohri_foundations_2018}, and \citet{anthony_neural_1999}. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "learning_matching"
%%% End:
