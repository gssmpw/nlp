\section{Related Work}
\paragraph{3D Semantic Scene Completion.}
% The SSC task aimed to address the challenges of scene completion and semantic segmentation by predicting the occupancy and semantic categories of individual voxels within a 3D scene. This task received significant attention because of its crucial role in semantic occupancy prediction for autonomous driving.
% Existing methods for outdoor SSC were mainly divided into LiDAR-based and camera-based approaches, depending on their input modality. LiDAR-based methods focused on utilizing LiDAR for accurate 3D semantic occupancy predictions. **Kalogerakis, "Learning 3D Scene Completion from LiDAR Data"** was the first to define the semantic scene completion task, wherein both geometry and semantics were jointly inferred from incomplete visual observations.
% Subsequent research recognized the inherently 3D characteristics of semantic scene completion tasks, leading to numerous studies that directly employed 3D inputs **Qi, "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"** , such as depth information, occupancy meshes, and point clouds, to leverage their rich geometric cues. To incorporate additional texture information, many works **Kundu, "Joint Semantic Segmentation and Depth Estimation from a Single RGB Image"** explored the use of multi-modal inputs, combining RGB images with diverse geometric cues. Additionally, some research **Rhemeth, "Scene Completion for 3D Scenes"** explored the relationship between semantic segmentation and scene completion.

Semantic Scene Completion (SSC) aims to jointly predict the geometry and semantics of a scene in 3D space, addressing the challenges of both scene completion and semantic segmentation. **Kalogerakis, "Learning 3D Scene Completion from LiDAR Data"** was the first to introduce this task, inferring both occupancy and semantic labels from incomplete visual observations. Subsequent studies have leveraged explicit 3D representations, such as depth maps, occupancy grids, and point clouds **Qi, "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"** , which provide rich geometric cues. Meanwhile, multi-modal approaches **Kundu, "Joint Semantic Segmentation and Depth Estimation from a Single RGB Image"** have integrated RGB imagery with depth information to enhance texture representation. Additionally, several works **Rhemeth, "Scene Completion for 3D Scenes"** have explored the interdependence between semantic segmentation and scene completion.
% -------------------------------------------------
\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{figure3.pdf}
  \caption{The FlowScene framework is proposed for temproal 3D semantic scene completion.}
  \label{fig:figure3}
\end{figure*}
% \vspace{-3mm}
% -------------------------------------------------

With the advent of cost-effective, vision-based autonomous driving solutions, monocular SSC methods have gained traction. **Kundu, "Monocular 3D Scene Understanding"** was the first to infer dense 3D semantics from a single RGB image. **Li, "Temporal Pyramid VoxelFormer for Monocular 3D Scene Completion"** introduced a tri-perspective view (TPV) representation, extending BEV with two vertical planes. **Xu, "Occupancy Former: Occupancy Grid Prediction using Transformers"** proposed a dual-path transformer to encode voxel features, while **Wang, "VoxFormer: A Two-Stage Pipeline for Voxelized Semantic Scene Understanding"** introduced a two-stage pipeline for voxelized semantic scene understanding. **Chen, "Surround Occ: Progressive Voxel Upsampling and Dense SSC Ground Truth Generation"**
predicted the occupancy of voxels. 
  **Wang, "Octree-based Representation Learning for 3D Scenes"** utilized an octree-based representation for semantic occupancy prediction, while **Li, "Normalized Device Coordinates Scene Completion"** redefined spatial encoding by mapping 2D feature maps to normalized device coordinates (NDC) rather than world space.
**Xu, "Monocular Occupancy Prediction with Image-conditioned Cross-Attention"** enhanced 3D volumetric representations using an image-conditioned cross-attention mechanism. **Li, "Hierarchical Temporal Context Learning for Monocular 3D Scene Completion"** introduced a progressive feature reconstruction strategy to propagate 2D information across multiple viewpoints. **Wang, "Symphonize: Extracting High-level Instance Features for Cross-Attention"**
extracted high-level instance features to serve as key-value pairs for cross-attention. **Li, "Hierarchical Attention-based Scene Completion"** proposed a self-distillation framework to improve the performance of VoxFormer. Stereo-based methods, such as **Chen, "Bi-reconstruction Guidance for Monocular 3D Scene Completion"**, leveraged stereo depth estimation to resolve geometric ambiguities. 
   **Wang, "Mixing 2D and 3D Sparsity for Monocular 3D Scene Completion"** fused forward projection sparsity with the denseness of depth-prior backward projection.
**Li, "Context-aware Query Generator for Monocular 3D Scene Completion"**
utilized a context-aware query generator to initialize context-dependent queries tailored to individual input images, effectively capturing their unique characteristics and aggregating information within the region of interest. **Wang, "Hierarchical Temporal Context Learning for Monocular 3D Scene Completion"** decomposed temporal context learning into two hierarchical steps: cross-frame affinity measurement and affinity-based dynamic refinement.
\vspace{-4mm}
\paragraph{Optical Flow for Visual Perception.}
Optical flow estimation, a fundamental task in computer vision, aims to establish dense pixel-wise correspondences between consecutive frames. **Ranjan, "FlowNet: Learning Optical Flow with Convolutional Neural Networks"** introduced the first CNN-based end-to-end flow estimation pipeline, leveraging a hierarchical pyramid structure. **Liu, "Pyramid Stereo Matching Network for Real-time Dense Correspondence Estimation"** further refined this approach by incorporating multi-stage warping to handle large-displacement motion. **Teed, "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"** introduced an iterative, recurrent architecture that refines residual flow predictions in a fully convolutional manner. 
**Sun, "Global Matching for Real-time Dense Correspondence Estimation"**
reframed optical flow as a global matching problem, directly computing feature similarities to establish correspondences.

Beyond motion estimation, optical flow has been leveraged to enhance various vision tasks. **Wang, "Flow-Track: Improving Tracking Accuracy with Optical Flow"** used optical flow to enrich feature representations and improve tracking accuracy. 
  **Li, "Flow-guided Feature Aggregation for Video Object Detection"**
employed flow-guided feature aggregation for end-to-end video object detection. **Chen, "Label Propagation with Flow-based Warping for Referring Video Object Segmentation"** utilized flow-based warping to propagate annotations across temporal neighbors, thereby boosting referring video object segmentation. **Wang, "Diverse Moving Objects and Targets Motion Observation for Autonomous Vehicles"**
introduced a moving object detection and tracking framework tailored for autonomous vehicles. **Li, "Deep Visual Odometry and Scene Motion Modeling with Optical Flow prior"** incorporated optical flow into scene motion modeling, using it as a prior for learnable offsets in video segmentation.