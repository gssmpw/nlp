\section{Related Work}
\paragraph{3D Semantic Scene Completion.}
% The SSC task aimed to address the challenges of scene completion and semantic segmentation by predicting the occupancy and semantic categories of individual voxels within a 3D scene. This task received significant attention because of its crucial role in semantic occupancy prediction for autonomous driving.
% Existing methods for outdoor SSC were mainly divided into LiDAR-based and camera-based approaches, depending on their input modality. LiDAR-based methods focused on utilizing LiDAR for accurate 3D semantic occupancy predictions. SSCNet~\cite{song2017semantic} was the first to define the semantic scene completion task, wherein both geometry and semantics were jointly inferred from incomplete visual observations.
% Subsequent research recognized the inherently 3D characteristics of semantic scene completion tasks, leading to numerous studies that directly employed 3D inputs~\cite{rist2021semantic,zhang2018efficient,zou2021udnet,roldao2020lmscnet,zhang2018efficient}, such as depth information, occupancy meshes, and point clouds, to leverage their rich geometric cues. To incorporate additional texture information, many works~\cite{rist2021semantic, cai2021semantic, li2019rgbd} explored the use of multi-modal inputs, combining RGB images with diverse geometric cues. Additionally, some research~\cite{yan2021sparse,yang2021ssasc,mei2023sscrs} explored the relationship between semantic segmentation and scene completion.

Semantic Scene Completion (SSC) aims to jointly predict the geometry and semantics of a scene in 3D space, addressing the challenges of both scene completion and semantic segmentation. SSCNet~\cite{song2017semantic} was the first to introduce this task, inferring both occupancy and semantic labels from incomplete visual observations. Subsequent studies have leveraged explicit 3D representations, such as depth maps, occupancy grids, and point clouds~\cite{rist2021semantic,zhang2018efficient,Xia_2023_scpnet}, which provide rich geometric cues. Meanwhile, multi-modal approaches~\cite{cai2021semantic,li2019rgbd} have integrated RGB imagery with depth information to enhance texture representation. Additionally, several works~\cite{yan2021sparse,yang2021ssasc,mei2023sscrs} have explored the interdependence between semantic segmentation and scene completion.
% -------------------------------------------------
\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{figure3.pdf}
  \caption{The FlowScene framework is proposed for temproal 3D semantic scene completion.}
  \label{fig:figure3}
\end{figure*}
% \vspace{-3mm}
% -------------------------------------------------

With the advent of cost-effective, vision-based autonomous driving solutions, monocular SSC methods have gained traction. MonoScene~\cite{cao2022monoscene} was the first to infer dense 3D semantics from a single RGB image. TPVFormer~\cite{huang2023tri} introduced a tri-perspective view (TPV) representation, extending BEV with two vertical planes. OccFormer~\cite{zhang2023occformer} proposed a dual-path transformer to encode voxel features, while VoxFormer~\cite{li2023voxformer} introduced a two-stage pipeline for voxelized semantic scene understanding. SurroundOcc~\cite{wei2023surroundocc} employed 3D convolutions for progressive voxel upsampling and dense SSC ground truth generation.
OctOcc~\cite{ouyang2024octocc} utilized an octree-based representation for semantic occupancy prediction, while NDCScene~\cite{yao2023ndc} redefined spatial encoding by mapping 2D feature maps to normalized device coordinates (NDC) rather than world space.
MonoOcc~\cite{zheng2024monoocc} enhanced 3D volumetric representations using an image-conditioned cross-attention mechanism. H2GFormer~\cite{wang2024h2gformer} introduced a progressive feature reconstruction strategy to propagate 2D information across multiple viewpoints. Symphonize~\cite{jiang2024symphonize} extracted high-level instance features to serve as key-value pairs for cross-attention. HASSC~\cite{wang2024HASSC} proposed a self-distillation framework to improve the performance of VoxFormer. Stereo-based methods, such as BRGScene~\cite{li2023stereoscene}, leveraged stereo depth estimation to resolve geometric ambiguities. MixSSC~\cite{wang2025mixssc} fused forward projection sparsity with the denseness of depth-prior backward projection.
CGFormer~\cite{CGFormer} utilized a context-aware query generator to initialize context-dependent queries tailored to individual input images, effectively capturing their unique characteristics and aggregating information within the region of interest. HTCL~\cite{li2024htcl} decomposed temporal context learning into two hierarchical steps: cross-frame affinity measurement and affinity-based dynamic refinement.
\vspace{-4mm}
\paragraph{Optical Flow for Visual Perception.}
Optical flow estimation, a fundamental task in computer vision, aims to establish dense pixel-wise correspondences between consecutive frames. FlowNet~\cite{dosovitskiy2015flownet, ilg2017flownet2} introduced the first CNN-based end-to-end flow estimation pipeline, leveraging a hierarchical pyramid structure. PWC-Net~\cite{sun2018pwc} further refined this approach by incorporating multi-stage warping to handle large-displacement motion. RAFT~\cite{teed2020raft} introduced an iterative, recurrent architecture that refines residual flow predictions in a fully convolutional manner. GMFlow~\cite{xu2022gmflow} reframed optical flow as a global matching problem, directly computing feature similarities to establish correspondences.

Beyond motion estimation, optical flow has been leveraged to enhance various vision tasks. FlowTrack~\cite{zhu2018flowtrack} used optical flow to enrich feature representations and improve tracking accuracy. FGFA~\cite{zhu2017fgfa} employed flow-guided feature aggregation for end-to-end video object detection. LoSh~\cite{Yuan_2024_LoSh} utilized flow-based warping to propagate annotations across temporal neighbors, thereby boosting referring video object segmentation. DATMO~\cite{sormoli2024optical} introduced a moving object detection and tracking framework tailored for autonomous vehicles. DeVOS~\cite{Fedynyak_2024_DeVos} incorporated optical flow into scene motion modeling, using it as a prior for learnable offsets in video segmentation.