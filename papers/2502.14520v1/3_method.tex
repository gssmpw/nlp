
\section{Methodology}
\subsection{Preliminary}
\paragraph{Problem Setting.}
Given a set of RGB images \( I = \{I_{t-i}\}_{i=0}^{n} \), where \( n \) is the number of historical temporal images, the objective is to jointly infer the geometry and semantics of a 3D scene. This scene is represented as a voxel grid \( Y \in \mathbb{R}^{X \times Y \times Z \times (M+1)} \), where \( X, Y, Z \) represent the height, width, and depth in 3D space, respectively. Each voxel in the grid is assigned a unique semantic label from the set \( C \in \{C_0, C_1, ..., C_M\} \), where \( C_0 \) represents empty space and the remaining classes \( \{C_1, ..., C_M\} \) correspond to specific semantic categories. Here, \( M \) denotes the total number of semantic classes. The goal is to learn a transformation \( Y = \theta(I_{s}) \) that closely approximates the ground truth \( \hat{Y} \).

% \paragraph{Design Motivation.}
% 在详细介绍我们的框架之前，我们先阐述该方法设计的动机，
\subsection{Overview} 
% 历史时序特征
We illustrate our method in Figure~\ref{fig:figure3}. First, we use the image encoder RepViT~\cite{wang2023repvit} and FPN~\cite{lin2017feature} to extract the current image features, $F_t$, and the historical temporal features, $F_{temp}=\{F_{t-i}\}_{i=1}^n$. We then apply the pre-trained optical flow estimation model~\cite{xu2022gmflow} (Section~\ref{sec:Flow}) to generate bidirectional optical flow, $Flow = \{Flow_{fwd}^{t-i\rightarrow t}, Flow_{bwd}^{t-i\rightarrow t}\}_{i=1}^n$. The historical temporal features, $F_{temp}$, are warped using $Flow_{bwd}$ to obtain $F_{warp} = \{F_{warp}^{t-i \rightarrow t}\}_{i=1}^n$. The bidirectional optical flow is then used for occlusion detection through a forward and backward consistency check to obtain the cumulative mask, $M \in {0,1}^{1 \times h \times w}$. Subsequently, $F_{warp}$, $F_t$, and $M$ are passed into the FGTA module (Section~\ref{sec:FGTA}) to perform optical flow-guided temporal feature aggregation in the 2D image feature space, resulting in the aggregated feature $F_{agg}$.
Next, we apply the LSS view transformation~\cite{philion2020lift} to project $F_t$, $F_{agg}$, and $M$ into the 3D voxel space, obtaining $V_t$, $V_{agg}$, and $V_{mask}$, respectively. In the subsequent OGVR module (Section~\ref{sec:OGVR}), the two voxel features are fused based on the occlusion information, yielding the refined voxel features, $V_{fine}$. Finally, $V_{fine}$ passes through the voxel encoder, then undergoes upsampling and linear projection to output the dense semantic voxels, $Y$.
% -------------------------------------------------
\begin{figure}[t]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{figure4.pdf}
\end{center}
   \caption{Occlusion detection is performed using forward-backward consistency detection.}
\label{fig:figure4}
\end{figure}
% -------------------------------------------------
\subsection{Optical Flow Estimation}
\label{sec:Flow}
\paragraph{Flow-Guided Warping.}
Given a reference image frame $I_t$ and historical frames $I_{t-i}$, the flow field $Flow^{t \rightarrow t-i}=\mathcal{F}(I_t, I_{t-i})$ is estimated by a flow network $\mathcal{F}$ (e.g., GMFlow~\cite{xu2022gmflow}). The feature map from the historical frame is warped to the reference frame according to the flow. The warping function is defined as
\begin{equation}
    F_{warp}^{t-i\rightarrow t} = \mathcal{W}arp( F_{t-i},Flow^{t \rightarrow t-i})
\end{equation}
where $\mathcal{W}arp(\cdot)$ is a bilinear warping function applied to all locations of each channel in the feature map, and $F_{warp}^{t-i\rightarrow t}$ represents the feature map warped from frame $t-i$ to $t$.

\paragraph{Occlusion Detection.}
First, we note that there is relative motion between almost all frames in an autonomous driving scenario, which results in pixels in the current image that do not have corresponding matching pixels in the historical frames; these are referred to as occluded areas. To detect occlusion, as shown in Figure~\ref{fig:figure4}, we use the commonly employed forward and backward consistency check technique~\cite{sundaram2010dense,meister2018unflow}, which is implemented as:
\begin{equation}
    M = \mathcal{CC}(Flow_{bwd},Flow_{fwd}),
\end{equation}
where $\mathcal{CC}(\cdot)$ denotes the forward and backward consistency check function. For non-occluded pixels, the forward optical flow should be the inverse of the backward optical flow of the corresponding pixel in the second frame. A pixel is marked as occluded if the mismatch between the two flows exceeds a predefined threshold. Thus, we define the occlusion flag as 1 whenever the constraint is violated and 0 otherwise.

\subsection{Flow-Guided Temporal Aggregation}
\label{sec:FGTA}
Previous SSC methods either stacked historical frame features or estimated camera poses to align features, aiming to complement the current frame. However, this direct temporal modeling approach overlooks the scene motion context, fails to achieve temporal consistency, and inherently limits the ability to leverage additional effective cues. To better incorporate time- and motion-related cues, we propose a flow-guided temporal aggregation module in 2D space. This module leverages optical flow information to align and aggregate temporal features along the motion path. As illustrated on the right side of Figure~\ref{fig:figure3}.
% -------------------------------------------------
\begin{figure}[t]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{figure5.pdf}
\end{center}
   \caption{Projecting the occlusion mask into 3D voxel space with depth bins.}
\label{fig:figure5}
\end{figure}
% -------------------------------------------------

Specifically, guided by optical flow, the historical frame features are warped to the reference frame.
Features from different frames provide multiple information for the same object instance, such as motion, different viewpoints, deformations, textures, geometric structures, various lighting and occlusions.
First, we assign different weights to different spatial locations, while ensuring that the spatial weights remain the same across all feature channels.
At position $\textbf{P}$, if the warped feature $F_{warp}^{t-i\rightarrow t}(\text{P})$ is close to the feature $F_t(\text{P})$, it is assigned a larger weight. Otherwise, a smaller weight is assigned. Inspired by FGFA~\cite{zhu2017fgfa}, we use the cosine similarity~\cite{luo2018cosine} to measure the similarity between the warped features and the reference frame features: 
\begin{equation}
    w_{t-i\rightarrow t} (\textbf{P})= \operatorname{similarity}(F_{warp}^{t-i\rightarrow t}(\text{P}),F_t(\text{P})).
\end{equation}
Then, we use the similarity weights to aggregate these feature maps to enhance the scene motion context features. The aggregation feature ${F}_{agg}$ is obtained as:
\begin{equation}
    {F}_{agg}=\sum_{i=0}^{t}w_{t-i\rightarrow t}\cdot F^{t-i\rightarrow t}_{warp}.
\end{equation}

The non-occluded regions in the historical frames usually have richer texture and feature information, which may be missing in the current frame due to visual occlusion. To address this, we enhance the current frame features, we effectively fuse spatiotemporal information through the neighborhood cross-attention mechanism ~\cite{hassani_2023_neighborhood}.
First, we select reliable non-occluded region features in the historical frames based on the occlusion mask. The reference features $F_t$ are used as query, and the warp features $F_{warp}$ of the non-occluded regions serve as key and value. The specific operations are as follows:
\begin{equation}
    F_t = \operatorname{NCA}(F_t,(1-M)\cdot F_{warp}),
\end{equation}
where $\operatorname{NCA}(\cdot)$ is the neighborhood cross attention mechanism. After these operations, $F_t$ fuses the non-occluded region information from both the current and historical frames, providing more stable and accurate features that enhance the perception of dynamic scenes and occluded regions.


\subsection{Occlusion-Guided Voxel Refinement} 
\label{sec:OGVR}
After passing through the FGTA module, time- and motion-related cues are injected into the image features $F_t$ and the aggregate features $F_{agg}$. However, for the 3D voxel space, there is a lack of explicit geometric modeling. To incorporate occlusion and optical flow information into the 3D space, we introduce the occlusion-guided voxel refinement module.
This module enhances the semantic completion ability of the occluded region by employing a weighted strategy of the occlusion mask. As shown in the right side of Figure~\ref{fig:figure3}.

%-------------------------------------------------------------------------
\begin{table*}[ht]
  \centering
  \renewcommand
  \arraystretch{1.2}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l|l|c|c|rrrrrrrrrrrrrrrrrrr|r}
    \toprule
    \textbf{Methods} &\textbf{Venues}&\textbf{Input}&\textbf{IoU}   
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,0,255}{\textcolor[RGB]{255,0,255}{\rule{1px}{1px}}}} \textbf{road} (15.30$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{75,0,75}{\textcolor[RGB]{75,0,75}{\rule{1px}{1px}}}} \textbf{sidewalk} (11.13$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,150,255}{\textcolor[RGB]{255,150,255}{\rule{1px}{1px}}}} \textbf{parking} (1.12$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{175,0,75}{\textcolor[RGB]{175,0,75}{\rule{1px}{1px}}}} \textbf{other-grnd} (0.56$\%$)} 
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,200,0}{\textcolor[RGB]{255,200,0}{\rule{1px}{1px}}}} \textbf{building} (14.10$\%$)} 
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{100,150,245}{\textcolor[RGB]{100,150,245}{\rule{1px}{1px}}}} \textbf{car} (3.92$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{80,30,180}{\textcolor[RGB]{80,30,180}{\rule{1px}{1px}}}} \textbf{truck} (0.16$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{100,230,245}{\textcolor[RGB]{100,230,245}{\rule{1px}{1px}}}} \textbf{bicycle} (0.03$\%$)}  
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{30,60,150}{\textcolor[RGB]{30,60,150}{\rule{1px}{1px}}}} \textbf{motocycle} (0.03$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{0,0,255}{\textcolor[RGB]{0,0,255}{\rule{1px}{1px}}}} \textbf{other-vehicle} (0.20$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{0,175,0}{\textcolor[RGB]{0,175,0}{\rule{1px}{1px}}}} \textbf{vegetation} (39.3$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{135,60,0}{\textcolor[RGB]{135,60,0}{\rule{1px}{1px}}}}  \textbf{trunk} (0.51$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{150,240,80}{\textcolor[RGB]{150,240,80}{\rule{1px}{1px}}}} \textbf{terrain} (9.17$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,30,30}{\textcolor[RGB]{255,30,30}{\rule{1px}{1px}}}} \textbf{person} (0.07$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,40,200}{\textcolor[RGB]{255,40,200}{\rule{1px}{1px}}}} \textbf{bicylist} (0.07$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{150,30,90}{\textcolor[RGB]{150,30,90}{\rule{1px}{1px}}}}  \textbf{motorcyclist} (0.05$\%$)} 
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,120,50}{\textcolor[RGB]{255,120,50}{\rule{1px}{1px}}}} \textbf{fence} (3.90$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,240,150}{\textcolor[RGB]{255,240,150}{\rule{1px}{1px}}}} \textbf{pole} (0.29$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,0,0}{\textcolor[RGB]{255,0,0}{\rule{1px}{1px}}}} \textbf{traf.-sign} (0.08$\%$)}& \textbf{mIoU}  \\
    
    \midrule
    % LMSCNet$^\dagger$~\cite{roldao2020lmscnet}&3DV'2020  & 31.38  & 46.70 & 19.50 & 13.50 & 3.10 & 10.30 & 14.30 & 0.30 & 0.00 & 0.00 & 0.00 & 10.80 & 0.00 & 10.40 & 0.00 & 0.00 & 0.00 & 5.40 & 0.00 & 0.00  &7.07  \\
    % AICNet$^\dagger$~\cite{li2020aicnet}&CVPR'2020   & 23.93  & 39.30 & 18.30 & 19.80 & 1.60 & 9.60 & 15.30 & 0.70 & 0.00 & 0.00 & 0.00 & 9.60 & 1.90 & 13.50 & 0.00 & 0.00 & 0.00 & 5.00 & 0.10 & 0.00 &7.09  \\
    % JS3C-Net$^\dagger$~\cite{yan2021sparse}&AAAI'2021   & 34.00  & 47.30 & 21.70 & 19.90 & 2.80 & 12.70 & 20.10 & 0.80 & 0.00 & 0.00 & 4.10 & 14.20 & 3.10 & 12.40 & 0.00 & 0.20 & 0.20 & 8.70 & 1.90 & 0.30   &8.97\\
    MonoScene~\cite{cao2022monoscene}&CVPR'2022 &S& 34.16  & 54.70&27.10&24.80&5.70&14.40&18.80&3.30&0.50&0.70&4.40&14.90&2.40&19.50&1.00&1.40&0.40&11.10&3.30&2.10 & 11.08 \\
    
    TPVFormer~\cite{huang2023tri}& CVPR'2023&S& 34.25  &55.10&27.20&27.40&6.50&14.80&19.20&3.70&1.00&0.50&2.30&13.90&2.60&20.40&1.10&2.40&0.30&11.00&2.90&1.50& 11.26\\
    
    % SurroundOcc~\cite{wei2023surroundocc}& ICCV'2023&S&34.72&56.90&28.30&30.20&6.80&15.20&20.60&1.40&1.60&1.20&4.40&14.90&3.40&19.30&1.40&2.00&0.10&11.30&3.90&2.40&11.86\\

    OccFormer~\cite{zhang2023occformer}& ICCV'2023&S & 34.53  & 55.90&30.30&31.50&6.50&15.70&21.60&1.20&1.50&1.70&3.20&16.80&3.90&21.30&2.20&1.10&0.20&11.90&3.80&3.70& 12.32\\

    % MonoOcc~\cite{zheng2024monoocc}&ICRA'2024&S&-&55.20&27.80&25.10&9.70&21.40&23.20&{5.20}&2.20&1.50&5.40&24.00&8.70&23.00&1.70&2.00&0.20&13.40&5.80&6.40&13.80\\

    Symphonize~\cite{jiang2024symphonize}&CVPR'2024&S&42.19&58.40&29.30&26.90&{11.70}&24.70&23.60&3.20&{3.60}&2.60&5.60&24.20&10.00&23.10&{3.20}&1.90&{2.00}&16.10&7.70&8.00&15.04\\%59.31M
    BRGScene~\cite{li2023stereoscene}&IJCAI'2024&S & 43.34 & 61.90&31.20  &30.70 & 10.70 & 24.20 & 22.80 & 2.80 & 3.40 & 2.40 & 6.10 & 23.80 & 8.40 & 27.00 & 2.90 & 2.20 & 0.50 & 16.50 & 7.00 & 7.20 & 15.36\\
    CGFormer~\cite{CGFormer}&NIPS'2024&S&44.41&64.30&34.20&34.10&12.10&25.80&26.10&4.30&3.70&1.30&2.70&24.50&11.20&29.30&1.70&3.60&0.40&18.70&8.70&9.30&16.63\\
    
    VoxFormer-T~\cite{li2023voxformer}& CVPR'2023&T& 43.21& 54.10& 26.90& 25.10& 7.30& 23.50& 21.70& 3.60& 1.90& 1.60& 4.10& 24.40& 8.10& 24.20& 1.60& 1.10& 0.00& 13.10& 6.60& 5.70& 13.41\\
    H2GFormer-T~\cite{wang2024h2gformer}&AAAI'2024&T&43.52&57.90&30.40&30.00&6.90&24.00&23.70&5.20&0.60&1.20&5.00&25.20&10.70&25.80&1.10&0.10&0.00&14.60&7.50&9.30&14.60 \\
    HASSC-T~\cite{wang2024HASSC}&CVPR'2024&T&42.87&55.30&29.60&25.90&11.30&23.10&23.00&2.90&1.90&1.50&4.90&24.80&9.80&26.50&1.40&3.00&0.00&14.30&7.00&7.10&14.38 \\%58.43M
    SGN~\cite{mei2024sgn}&TIP'2024&T&43.71&57.90&29.70&25.60&5.50&27.00&25.00&1.50&0.90&0.70&3.60&26.90&12.00&26.40&0.60&0.30&0.00&14.70 &9.00& 6.40& 14.39\\
    HTCL~\cite{li2024htcl}&ECCV'2024&T&44.23&64.40&34.80&33.80&12.40&25.90&27.30&5.70&1.80&2.20&5.40&25.30&10.80&31.20&1.10&3.10&0.90&21.10&9.00&8.30&17.09 \\
    \hline
    \rowcolor{gray!20}\textbf{Ours}&  &T& \textbf{45.20} &64.10&\textbf{35.00}&\underline{33.70}&\textbf{13.00}&\textbf{27.70}&\underline{26.40}&\textbf{10.00}&\textbf{4.20}&\textbf{3.10}&\textbf{7.00}&\underline{26.30}&10.00&\textbf{30.20}&\underline{3.10}&\textbf{5.10}&\underline{1.10}&\underline{20.20}&\underline{8.90}&\underline{9.10}& \textbf{17.70}\\
    \bottomrule
  \end{tabular}}
    \caption{Quantitative results on the SemanticKITTI hidden test set. The best and the second best results are in \textbf{bold} and \underline{underlined}, respectively. The “S” and “T” denote single-frame images, and temporal images, respectively.}
  
  \label{tab:Test Quantitative Comparison}
\end{table*}
%-------------------------------------------------------------------------

Specifically, we follow the LSS view transformation paradigm and use depth bin assignment to project $F_t$, $F_{agg}$, and $M$into the 3D voxel space to obtain $V_t$, $V_{agg}$, and $V_{mask}$, respectively, as shown in Figure~\ref{fig:figure5}.
First, we use $V_{mask}$ to distinguish occluded and non-occluded regions in the 3D voxel space. For the non-occluded region, we use the aggregate features $V_{agg}$ that fuse multiple cues.
Subsequently, since the information from the corresponding position in the historical frame may be inaccurate due to occlusion, we use the voxel features from the current frame to update the occluded area to supplement the latest environmental information.
Finally, by constructing a weighted matrix, we normalize the fused voxel features to ensure that there is no mutation at the boundary between the occluded and non-occluded areas, thereby improving the smoothness of the features. The specific operation is as follows:
\begin{equation}
    V_{fine}=\frac{(1-V_{mask}) \cdot V_{agg} + V_t}{(1-V_{mask})+1}.
\end{equation}

Finally, $V_{fine}$ enters the sparse voxel encoder for feature extraction, and then performs linear prediction to output dense semantic voxels $Y$.


\subsection{Training Loss}
In the FlowScene framework, we adopt the scene-class affinity loss $\mathcal{L}_{scal}$ from MonoScene~\cite{cao2022monoscene} to optimize precision, recall, and specificity concurrently. The scene-class affinity loss is applied to semantic and geometric predictions, in conjunction with the cross-entropy loss weighted by class frequencies. Besides, the intermediate depth distribution for view transformation is supervised by the projections of LiDAR points, with the binary cross-entropy loss $\mathcal{L}_{d}$ following BEVDepth~\cite{li2023bevdepth}. The overall loss function is formulated as follows: 
\begin{equation}
    \mathcal{L} = \lambda_{sem}\mathcal{L}^{sem}_{scal} + \lambda_{geo}\mathcal{L}^{geo}_{scal} + \lambda_{ce}\mathcal{L}_{ce} +  \lambda_{d}\mathcal{L}_{d}, 
\end{equation}
where several $\lambda$ are balancing coefficients.
% -------------------------------------------------

%-------------------------------------------------------------------------
\begin{table*}[ht]
  \centering
  \renewcommand\arraystretch{1.2}
  \setlength{\tabcolsep}{2pt}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l|rrr|rrrrrrrrrrrrrrrrrr|r}
    \toprule
    \textbf{Methods}&\textbf{Prec.}&\textbf{Rec.} &\textbf{IoU}   
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{100,150,245}{\textcolor[RGB]{100,150,245}{\rule{1px}{1px}}}} \textbf{car} (2.85$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{100,230,245}{\textcolor[RGB]{100,230,245}{\rule{1px}{1px}}}} \textbf{bicycle} (0.01$\%$)}  
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{30,60,150}{\textcolor[RGB]{30,60,150}{\rule{1px}{1px}}}} \textbf{motocycle} (0.01$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{80,30,180}{\textcolor[RGB]{80,30,180}{\rule{1px}{1px}}}} \textbf{truck} (0.16$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{0,0,255}{\textcolor[RGB]{0,0,255}{\rule{1px}{1px}}}} \textbf{other-vehicle} (5.75$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,30,30}{\textcolor[RGB]{255,30,30}{\rule{1px}{1px}}}} \textbf{person} (0.02$\%$)} 
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,0,255}{\textcolor[RGB]{255,0,255}{\rule{1px}{1px}}}} \textbf{road} (14.98$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,150,255}{\textcolor[RGB]{255,150,255}{\rule{1px}{1px}}}} \textbf{parking} (2.31$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{75,0,75}{\textcolor[RGB]{75,0,75}{\rule{1px}{1px}}}} \textbf{sidewalk} (6.43$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{175,0,75}{\textcolor[RGB]{175,0,75}{\rule{1px}{1px}}}} \textbf{other-grnd} (2.05$\%$)} 
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,200,0}{\textcolor[RGB]{255,200,0}{\rule{1px}{1px}}}} \textbf{building} (15.67$\%$)} 
    &  \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,120,50}{\textcolor[RGB]{255,120,50}{\rule{1px}{1px}}}} \textbf{fence} (0,96$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{0,175,0}{\textcolor[RGB]{0,175,0}{\rule{1px}{1px}}}} \textbf{vegetation} (41.99$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{150,240,80}{\textcolor[RGB]{150,240,80}{\rule{1px}{1px}}}} \textbf{terrain} (7.10$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,240,150}{\textcolor[RGB]{255,240,150}{\rule{1px}{1px}}}} \textbf{pole} (0.22$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255,0,0}{\textcolor[RGB]{255,0,0}{\rule{1px}{1px}}}} \textbf{traf.-sign} (0.06$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{0, 150, 255}{\textcolor[RGB]{0, 150, 255}{\rule{1px}{1px}}}}  \textbf{other-struct.} (4.33$\%$)} 
    & \rotatebox{90}{\vcenteredbox{\colorbox[RGB]{255, 255, 50}{\textcolor[RGB]{255, 255, 50}{\rule{1px}{1px}}}} \textbf{other-obj.} (0.28$\%$)} &\textbf{mIoU}   \\
    
    \midrule
    MonoScene&56.73 &53.26&37.87&19.34&0.43&0.58&8.02&2.03&0.86&48.35&11.38&28.13&3.32&32.89&3.53&26.15&16.75&6.92&5.67&4.20&3.09&12.31\\
    VoxFormer&58.52 &53.44&38.76&17.84&1.16&0.89&4.56&2.06&1.63&47.01&9.67&27.21&2.89&31.18&4.97&28.99&14.69&6.51&6.92&3.79&2.43&11.91\\
    TPVFormer&59.32 &55.54&40.22&21.56&1.09&1.37&8.06&2.57&2.38&52.99&11.99&31.07& 3.78&34.83&4.80&30.08&17.52&7.46&5.86&5.48&2.70&13.64\\
    OccFormer&59.70 &55.31&40.27&22.58&0.66&0.26&9.89&3.82&2.77&54.30&13.44&31.53&3.55&36.42&4.80&31.00&19.51&7.77&8.51& 6.95&4.60&13.81\\
    % BRGScene&69.02& 57.53& 45.73 & 28.60 & 2.55 & 3.44 & 12.85 & 6.18 & 5.91 & 59.15 & 15.10 & 36.96 & 5.26 & 40.92 & 8.68 & 36.79 & 23.32 & 15.71 & 16.89 & 9.79 & 5.65 & 17.56\\
    Symphonies&69.24&54.88&44.12&{30.02}&1.85&{5.90}&{25.07}&{12.06}&{8.20}&54.94&13.83&32.76&{6.93}&35.11&8.58&{38.33}&11.52&14.01&9.57&{14.44}&{11.28}&18.58\\
    \hline
    \rowcolor{gray!20}\textbf{Ours}& \textbf{70.01} &\textbf{58.81} & \textbf{46.98} &\underline{29.83}&\textbf{4.44}&\underline{3.78}&\underline{16.71}&\underline{8.71}&\underline{7.77}&\textbf{60.70}&\textbf{16.99}&\textbf{39.59}&\underline{6.01}&\textbf{43.17}&\textbf{9.45}&\underline{37.32}&\textbf{25.14}&\textbf{17.35}&\textbf{18.12}&\underline{10.63}&\underline{7.56}& \textbf{19.12}\\
    \bottomrule
  \end{tabular}}
    \caption{Quantitative results on the SSCBench-KITTI360 test set. The best and the second best results are in \textbf{bold} and \underline{underlined}, respectively.}

  \label{tab:KITTI360Test Quantitative Comparison}
  \vspace{-3mm}
\end{table*}
%-------------------------------------------------------------------------