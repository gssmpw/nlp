\section{Introduction}
\label{sec:intro}

Finding cause and effect among and within a group of multivariate time series can lead to a better understanding of the dynamics of the involved individual time series. This can have an impact on several critical research areas, such as finance~\citep{masini2023machine}, climate science~\citep{mudelsee2019trend}, and industrial applications~\citep{strem2025apt}, to name a few. Although recent efforts have been made to improve the interpretability of time series models~\citep{ismail2020benchmarking,turbe2023evaluation}, most methods are usually restricted to finding post-hoc interpretations and only focus on short-term dependencies.

\newpage
The framework of Granger causality (GC)~\citep{granger1969investigating} was introduced to address the challenge of determining whether one variable's past values can help forecast another's future values, without implying direct causality. In terms of time series, GC is a statistical hypothesis test used to determine whether one time series can predict another. The test involves estimating a vector autoregressive model~(VAR) and examining whether lagged values of a time series improve or degrade the prediction of the other, while controlling the past behavior of both series. Although GC does not imply a direct cause-and-effect relationship between the involved time series~\citep{heckman2008econometric}, recognizing these interdependencies can lead to a better understanding of the dynamic relationships between variables over time~\citep{marcinkevivcsinterpretable,shojaie2022granger}.


Many families of deep learning architectures have been explored for time series analysis over the years, such as multilayer perceptrons \citep{zengAreTransformersEffective2023,dasLongtermForecastingTiDE2023}, recurrent neural networks~(RNNs) \citep{hochreiterLongShortTermMemory1997,choLearningPhraseRepresentations2014}, convolutional neural networks \citep{wuTimesNetTemporal2DVariation2022,wangMICNMultiscaleLocal2022}, Transformers \citep{vaswaniAttentionAllYou2017,nieTimeSeriesWorth2023}, state-space models \citep{wangMambaEffectiveTime2025}, or mixing architectures~\citep{wangTimeMixerDecomposableMultiscale2023}.
Throughout this, recurrent models remained a natural choice for time series data since their direction of computation aligns well with the forward flow of time.
This aligns particularly well with the goals of neural Granger causality.
Furthermore, their inference runtime is typically linear in the sequence length at constant memory cost, making them much more efficient than, for instance, Transformers with quadratic runtimes and memory requirements, while remaining highly expressive.
Recently, \citet{beckXLSTMExtendedLong2024} revisited recurrent models by borrowing insights gained from Transformers in many domains, specifically natural language processing.
Their proposed Extended Long Short-Term Memory~(xLSTM) models sparked a resurgence of interest in recurrent architectures for sequence modeling and have already proven highly suitable for time series forecasting \citep{krausXLSTMMixerMultivariateTime2024,alharthiXLSTMTimeLongTermTime2024}.
Their proposed Extended Long Short-Term Memory~(xLSTM) models sparked a resurgence of interest in recurrent architectures for sequence modeling and have already proven highly suitable for time series forecasting \citep{krausXLSTMMixerMultivariateTime2024,alharthiXLSTMTimeLongTermTime2024}.
We will recall the key structure of xLSTM in \Cref{sec:prelim_related:xlstm}.

Although most Granger causal machine learning methods assumed linearity in time series as a fundamental assumption~\citep{siggiridou2015granger,zhang2020cause}, recent efforts have been made to extend to capture non-linear dynamics in time series by using neural networks as the modeling choice instead of VARs~\citep{tank2021neural,lowe2022amortized,cheng2024cuts}.
Although successful, these non-linear methods require careful feature engineering to include time-based patterns and thus may not capture interactions between time series and external factors as effectively as xLSTMs, which can learn non-linear patterns and adapt to the non-stationary nature of time series data.

Specifically, we introduce \emph{\name}, a novel method that leverages xLSTMs to uncover the GC relations in the presence of complex data, which inherently can have long-range dependencies. \name{} first enforces sparsity between the time series components by using a novel lasso penalty on the initial projection layer of the xLSTM. We learn a weight per time series and then adapt them to find the relevant variates for that step. Note that our penalty is different from adaptive lasso~\citep{yuan2006model} where the weights are not learned, but rather are treated as fixed heuristics. Then, each time series component is modeled using a separate xLSTM model, enabling us to discover more interpretable GC relationships between the time series variables. After the forecast results by the individual xLSTM models, the important features are made more prominent, whereas the less important ones are diminished by a joint optimization technique, which includes using a novel reduction coefficient. Thus, the overall \name{} model can be trained end-to-end to uncover long-range Granger causal relations.

\textbf{Key Contributions.}
Our main research contributions can be summarized as follows:
\begin{enumerate}[label=(\roman*)]
    \item We propose \name{}\footnote{We make our code available at \url{https://anonymous.4open.science/r/GC-xLSTM-A9FE/}.}, a novel model that can uncover Granger causal relations in complex time series.
    \item Our novel algorithm jointly improves the forecasting model while adaptively enforcing strict sparsity.
    \item We show that our proposed training regime increases the overall robustness of the underlying model, making it insensitive to noise.
    \item Our empirical evaluations demonstrate that \name{} can uncover robust Granger causal relations in the presence of complex data.
\end{enumerate}

\paragraph{Outline.}
The remainder of this work is structured as follows:
We start by recalling preliminaries and reviewing related research to contextualize this work in the wider body of research on neural Granger causality in \Cref{sec:prelim_related}.
This allows us to introduce \name{} in \Cref{sec:method_gxlstm} and empirically evaluate in relation to other methods in \Cref{sec:exp}.
Finally, we conclude with an outlook to future work in \Cref{sec:conclusion}.

\section{Preliminaries and Related Work}
\label{sec:prelim_related}


Let us start by introducing the notation used throughout this work.
We are interested in datasets of time series $\bm S \in \R^{V \times T}$ of $V$ variates with length $T$.
Let $\bm{S}_t \in \R^V$ denote the value of $\bm S$ at time $t$.
A variate $v$ (sometimes called a channel) can be any scalar measurement, such as the chlorophyll content of a plant or the spatial location of some object being tracked.
Its value at time $t$ is $S_{v,t} \in \R$.
The measurements are assumed to be carried out jointly at $T$ regularly spaced time steps.
In forecasting, a model is presented with a time series of $L$ lookback steps $\bm{S}_{<L}$, from which it shall predict the next value $\bm{S}_{L+1} \in \R^V$.

\subsection{Granger Causality}
\label{sec:prelim_related:gc}

If the observed time series were generated by some underlying process $g$, which we can formalize as a structural equation model for all time steps $t$ as
$$
    S_{v,t} = g_v(\bm{S}_{1,<t}, \dots, \bm{S}_{V,<t}) + \epsilon_{v,t} \text{ for all } v \in \mathcal{V} ,
$$
where $\epsilon_{v,t}$ is some additive zero mean noise and $\mathcal V := \{1,\dots,V\}$ is the set of all variates.
In Granger causality~\citep{granger1969investigating}, we aim to determine whether past values $\bm{S}_{v,<t}$ of a variate $v$ are predictive for future values $\bm{S}_{w,\geq t}$ of another variate $w$. Following the notation of \citet{shojaie2022granger}, we formally define:
\begin{definition}[Granger Causality]
    Variate $v$ is \emph{Granger noncausal} for $w$ if and only if $g_v$ is invariant to $\bm{S}_{w,<t}$ for all $t \in \{1,\dots,T\}$, i.e., if and only if
    $$
        g_v(\bm{S}_{1,<t}, \dots, \bm{S}_{V,<t}) \neq g_v(\bm{S}_{1,<t}, \dots, \bm{S}_{V,<t} \setminus \bm{S}_{w,<t}) .
    $$
    Else, we call $v$ \emph{Granger causal} for $w$.
\end{definition}

The set of all such relationships are the directed edges $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ of the \emph{Granger causal graph} $(\mathcal{V}, \mathcal{E})$ of the variates, which we eventually aim to uncover.

\subsection{Neural Granger Causality}
\label{sec:prelim_related:ngc}

Unfortunately, however, we cannot explicitly access $g$ in most realistic settings.
Using machine learning methods, we can nonetheless estimate each of the $V$ process components $g_v$ by an autoregressive time series forecasting model $\mathcal{M}_{\bm\theta,v}(\bm{S}_{<t} ) \approx g_v(\bm{S}_{<t} )$.
We can do so by estimating the parameters $\bm\theta$ of the model based on the dataset of time series, minimizing the predictive mean squared error~(MSE) loss
\begin{equation}
    \mathcal{L}_\text{pred}\left(\bm\theta \right) = \sum_{v=1}^V \sum_{t=1}^T \left( S_{v,t} - \mathcal{M}_{\bm\theta,v}(\bm{S}_{<t} ) \right)^2 .
    \label{eq:loss_pred}
\end{equation}
In the case of using neural networks for $\mathcal{M}_{\bm\theta,v}$, this approach is called \emph{Neural Granger Causality} \citep{tank2021neural}.

It would be very costly to train a total of $V^2$ models to test if each variate Granger causes any other variate and thus construct the entire Granger causal graph.
To avoid this, we can train merely a single \emph{component-wise} model $\mathcal{M}_{\bm\theta,v}$ for each variate $v$ and inspect what inputs $w$ it is sensitive to.
This can be achieved by optimizing the predictive loss $\mathcal{L}_\text{pred}\left(\bm\theta \right)$ based on all model parameters $\bm\theta$ together with a regularizer $\Omega(\widehat{\bm{\theta}}_v )$ that enforces sparse usage of the input features:
\begin{equation}
    \min_{\bm\theta, \widehat{\bm{\theta}}} \;
    \mathcal{L}_\text{pred}\left(\bm\theta \right) +
    \lambda \sum_{v=1}^V \Omega\left(\widehat{\bm{\theta}}_v \right),
    \label{eq:minimization_general}
\end{equation}
where $\widehat{\bm{\theta}}_v$ are tunable parameters of the regularizer for variate $v$ and $\lambda \in \R_+$ is a hyperparameter to adjust the degree of sparsity.
One such approach are cMLPs~\citep{tank2021neural}; multilayer perceptrons where the first weight matrix $\bm W \in \R^{D \times V}$ projecting from $V$ features at each time step to $D$ hidden dimensions is regularized to encode a sparse selection of input features.
$\Omega$ is instantiated as an $\normltwo$ norm of its columns: $\sum_{v=1}^V \norm{ \bm{W}_v }_2$. Note that $\bm\theta$ and $\widehat{\bm{\theta}}$ can overlap.

Sparsity can then be extracted by binarizing the entries of $\bm W$ using a user-defined threshold $\tau$.
This is necessary as the $\normltwo$ penalty tends only to shrink parameters to small values near zero, yet not clamp them sharply to it.
This, however, allows subsequent layers to amplify the dampened signal again and still use it for forecasting.
We avoid this disadvantage in \name{} by explicitly optimizing the feature extractor for strict sparsity.
This more principled approach works without determining a sparsity threshold $\tau$.

Previous work has explored both more regularizers \citep{tank2021neural} and different means to extract Granger causal relationships, including using feature attribution via explainability \citep{atashgahiUnveilingPowerSparse2024} and interpretability \citep{marcinkevivcsinterpretable}. Furthermore, several works have gone towards learning relevant representations that respect the underlying Granger causality~\citep{xu2016learning,varando2021learning,dmochowski2023granger}. \citet{zoroddu2024learning} present another approach where prior knowledge is encoded in the form of a noisy undirected graph, which aids the learning of Granger causality graphs. A more recent approach~\citep{lin2024granger} employs Kolmogorov-Arnold networks~\citep{liu2024kan} to learn the Granger causal relations between time series.

\subsection{Extended Long Short-Term Memory (xLSTM)}
\label{sec:prelim_related:xlstm}

\citet{beckXLSTMExtendedLong2024} propose two building blocks to build up xLSTM architectures: the sLSTM and mLSTM modules for vector-valued (multivariate) sequences.
sLSTM cells improve upon classic LSTMs by exponential gating.
For parallelizable training, mLSTM cells move from memory mixing between hidden states to an associative matrix memory instead.
We will continue by recalling how sLSTM cells function since we found their mixing of consecutive memory states more effective in time series forecasting.

The standard LSTM architecture of \citet{hochreiterLongShortTermMemory1997} involves updating the cell state $\mathbf{c}_t$ through a combination of input, forget, and output gates, which regulate the flow of information across tokens.
sLSTM blocks enhance this by incorporating exponential gating and memory mixing~\citep{greffLSTMSearchSpace2017} to better handle complex temporal and cross-variate dependencies.
The sLSTM updates the cell $\bm{c}_t$ and hidden state $\bm{h}_t$ using three gates:
\begin{align}
    \bm{c}_t &= \bm{f}_t \odot c_{t-1} + \bm{i}_t \odot \bm{z}_t & \text{cell state} \label{eq:lstm:cell}\\
    \bm{n}_t &= \bm{f}_t \cdot \bm{n}_{t-1} + \bm{i}_t & \text{normalizer state} \\
    \bm{h}_t &= \bm{o}_t \odot \bm{c}_t \odot \bm{n}^{-1}_t & \text{hidden state} \\
    \bm{z}_t &= \tanh\bigl( \bm{W}_z \bm{x}_t + \bm{R}_z h_{t-1}+\bm{b}_z \bigr)\hspace{-10ex} & \text{cell input} \\
    \bm{\tilde{i}}_t &= \bm{W}_i \bm{x}_t + \bm{R}_i \bm{h}_{t-1} + \bm{b}_i & \nonumber\\
    \bm{i}_t &= \exp\bigl( \bm{\tilde{i}}_t - \bm{m}_t \bigr) & \text{input gate} \\
    \bm{\tilde{f}}_t &= \bm{W}_f \bm{x}_t + \bm{R}_f \bm{h}_{t-1} + \bm{b}_f & \nonumber\\
    \bm{f}_t &= \exp\bigl( \bm{\tilde{f}}_t + \bm{m}_{t-1} - \bm{m}_t \bigr) & \text{forget gate} \\[0.15em]
    \bm{f}_t &= \exp\bigl( \bm{\tilde{f}}_t + \bm{m}_{t-1} - \bm{m}_t \bigr) & \text{forget gate} \\[0.15em]
    \bm{o}_t &= \sigma\bigl( \bm{W}_o \bm{x}_t + \bm{R}_o \bm{h}_{t-1} + \bm{b}_o \bigr)\hspace{-5ex} & \text{output gate}\\
    \bm{m}_t &= \max\bigl( \bm{\tilde{f}}_t + \bm{m}_{t-1}, \bm{\tilde{i}}_t \bigr) & \text{stabilizer state} \label{eq:lstm:stabilizer}
\end{align}
In this setup, the matrices $\bm{W}_z, \bm{W}_i, \bm{W}_f,$ and $\bm{W}_o$ are input weights mapping the input token $\bm{x}_t$ to the cell input $\bm{z}_t$, input gate, forget gate, and output gate, respectively.
The states $\bm{n}_t$ and $\bm{m}_t$ serve as necessary normalization and training stabilization, respectively.
Bias terms $\bm{b}_z, \bm{b}_i, \bm{b}_f,$ and $\bm{b}_o$ are added to their respective gates.

As \citeauthor{beckXLSTMExtendedLong2024} have shown, it is sufficient and computationally beneficial to constrain the memory mixing performed by the recurrent weight matrices $\bm{R}_z, \bm{R}_i, \bm{R}_f,$ and $\bm{R}_o$ to individual \emph{heads}.
This is inspired by the multi-head setup of Transformers~\citep{zengAreTransformersEffective2023}, yet more restricted and efficient.
In particular, each token gets broken up into groups of features, where the input weights $\bm{W}_{z,i,f,o}$ act across all of them, but the recurrence matrices $\bm{R}_{z,i,f,o}$ are implemented as block-diagonal.
This permits specialization of the individual heads to patterns specific to the respective section of the tokens and does not sacrifice expressivity.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{content/figures/architecture.pdf}
    \caption{\textbf{\name{} performs three key steps to determine the Granger causal links:} Firstly, for each time series component, all variates are embedded with a sparse feature encoder $\bm W$ that is regularized through a novel sparsity loss with learned reduction coefficients $\bm\alpha$. xLSTM models then learn to autoregressively predict future steps from that embedding. Finally, once model estimation is complete, Granger causal dependencies can be extracted from $\bm W$.}
    \label{fig:architecture}
\end{figure*}

\newpage

\section{\name{}}
\label{sec:method_gxlstm}

We will now introduce the \name{} architecture and detail the optimization for strict sparsity jointly with the model parameters.

\subsection{Overall Architecture}

As \Cref{fig:architecture} shows, we estimate a pipeline of sparse feature selectors and xLSTM models to predict the multivariate time series.
Eventually, this allows us to derive Granger causal dependencies from the selected features.
Specifically, we learn for each variate $v$ a separate sparse projection and compute $\bm{x}_v = \bm{W}_v \bm{S} + \bm{b}_v$.
The matrix $\bm{W}_v$ is shared across all lags of the time series.
Note that $\bm b$ does not affect the sparse use of inputs.
We write $\bm\phi$ for the set of parameters $\bm{W}_v$ and $\bm{b}_v$ for all $v \in \mathcal{V}$.
In addition to selecting dependencies, the sparse projection embeds the data into $D$-dimensional hidden space.

For Neural Granger Causality to successfully and faithfully extract the proper underlying dependencies, it is essential to employ models that can capture the complete set of dependencies.
We, therefore, employ powerful deep-learning models with significantly higher capacity than the cMLPs and cLSTMs in prior work \citep{tank2021neural}.
In particular, we instantiate the individual time series forecasters $\mathcal{M}_{\bm\theta_v,v}$ with sLSTM blocks as introduced in \Cref{sec:prelim_related:xlstm}.
They can capture long-range dependencies in time series data, substantially enhancing the capabilities of traditional LSTMs in handling extended contexts.
\name{} consists of $V$ sLSTM models, each modeling a different time series component.
They are trained using established forecasting losses, such as the MSE loss $\mathcal{L}_\text{pred}\left(\bm\phi, \bm\theta \right)$ from \Cref{eq:loss_pred}.

\subsection{Optimizing for Strict Input Sparsity}

\begin{algorithm}[t]
    \caption{\name{} Granger causality detection}
    \label{alg:main}
    \begin{algorithmic}[1]
        \Input{
            Training data $\mathcal{D} = \left\{ \bm{S}^{(i)} \right\}_{i \in \{1,\dots,N \}}$,
            sparsity hyperparameter $\lambda \in \R_+$,
            learning rate $\eta \in \R_+$, and
            compression schedule start $K \in \N$
        }
        \Output{Granger causal graph}
        \algorithmicbreak
        \ForAll{$v \in \mathcal{V}$} \Comment{Training decomposes over $\mathcal V$}
            \State{$\bm{\phi}_v : \bm{W}_v, \bm{b}_v \sim \mathcal{U}\left(-\sqrt{\nicefrac{1}{V}}, \sqrt{\nicefrac{1}{V}}\right)$} \Comment{Kaiming/He}
            \State{$\bm{\alpha}_v \gets \nicefrac{1}{V} \mathds{1}$} \Comment{Uniform reduction coefficient}
            \State{$\bm{\theta}_v \gets \bm{\theta}^{(0)}$} \Comment{Standrad xLSTM initialization}
            \State{$k \gets 0$}
            \Repeat
                \State{Sample random mini-batch $\mathcal{B} \sim \mathcal{D}$}
                \Statex{\Comment{Estimate the gradients using $\mathcal{B}$}}
                \State{$\bm{g}_{\bm\phi, \bm\alpha, \bm\theta} \gets \nabla_{\bm{\phi}, \bm{\alpha}, \bm{\theta}} \frac{1}{|\mathcal{B}|} \sum_{\bm S \in \mathcal{B}} \Big[ \mathcal{L}_\text{pred}(\bm S ; \bm{\phi}_v, \bm{\theta}_v)$} \label{alg:main:gradient_start}
                \Statex{\hspace{7.2em} $+ \lambda \log\left(\sum_{w=1}^V \alpha_v^w \norm{\sg\left( \bm{W}_v^w \right)}_2 \right) \Big]$}
                \State{$\bm{\phi}_v \gets \bm{\phi}_v - \eta \bm{g}_{\bm\phi}$} \Comment{GD step}
                \State{$\bm{\theta}_v \gets \bm{\theta}_v - \eta \bm{g}_{\bm\theta}$} \Comment{GD step}
                \If{$k \geq K$} \Comment{Optimize $\bm\alpha$ after $K$ steps}  \label{alg:main:staged_alpha}
                    \State{$\bm{\alpha}_v \gets \bm{\alpha}_v - \eta \bm{g}_{\bm\alpha}$} \Comment{GD step}
                \EndIf \label{alg:main:gradient_end}
                \State{$\bm{W}_v \gets \left( 1 - \frac{\lambda \eta \bm{\alpha}_v}{\norm{\bm{W}_v}_2} \right)_+ \bm{W}_v$} \Comment{Prox. GD step} \label{alg:main:prox}
                \State{$k \gets k + 1$}
            \Until{convergence}
        \EndFor
        \State{$\mathcal E \gets \left\{ (v, w) \in \mathcal{V} \times \mathcal{V} \; \middle| \; \norm{\bm{W}_v^w}_2 > 0 \right\}$}
        \State\Return{extracted graph $(\mathcal V, \mathcal E)$}
    \end{algorithmic}
\end{algorithm}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{content/figures/method.pdf}
    \caption{\textbf{The optimization procedure for \name{}}. \name{} alternates between GD and proximal GD.}
    \label{fig:main_method}
\end{figure}

The purpose of the feature selector $\bm W$ is to only \enquote{pay attention} to as many variates as necessary for successful forecasting.
A common approach to achieve this sparsity on the Granger causal relationships is via the lasso regularization explained in \Cref{sec:prelim_related:ngc}.
We use the group lasso penalty \citep{yuan2006model,simonStandardizationGroupLasso2012} on the initial projection layer of \name{} as a structured sparsity-inducing penalty that encourages the selection of entire groups of variables, encoded as the columns of $\bm{W}_v$.
We adaptively compress by learning a reduction coefficient $\bm{\alpha}_v \in \R^V$ that selects which of the $V$ columns of $\bm{W}_v$ are redundant.
We perform this compression of $\bm{W}_v$ in a joint procedure with the general optimization of the forecasting model, as provided in \Cref{alg:main}.
Specifically, we perform two updates per optimization step for each of the variates, as \Cref{fig:main_method} depicts.
Firstly, we optimize the projection weights $\bm\phi$, the reduction coefficients $\bm\alpha$, and the xLSTM parameters $\bm\theta$ using mini-batch gradient descent.
This corresponds to \crefrange{alg:main:gradient_start}{alg:main:gradient_end} in \Cref{alg:main}.
This optimizes the following loss expected over the time series data $\bm S$:

\begin{equation} \label{eq:loss_gd}
    \min_{\bm{\phi}_v, \bm{\alpha}_v, \bm{\theta}_v} \mathcal{L}_\text{pred}(\bm S ; \bm{\phi}_v, \bm{\theta}_v) +
    \underbrace{
        \lambda \log\left( \sum_{w=1}^V \alpha_v^w \norm{\sg\left( \bm{W}_v^w \right)}_2 \right)
    }_{\mathcal{L}_\text{red}\left(\bm{\alpha}_v, \bm{W}_v \right)}
\end{equation}
Note that we, crucially, only descent on the reduction coefficient $\bm\alpha$ in $\mathcal{L}_\text{red}$ an not on $\bm{W}_v$, as the stop-gradient $\sg(\cdot{})$ denotes.
This sparsity optimization is instead performed by the second step in the procedure, shown in \cref{alg:main:prox}, where a proximal gradient descent step is taken to dynamically shrink $\bm{W}_v$ proportional to $\bm{\alpha}_v$.
The $(\cdot)_+$ denotes truncation as $\max(\cdot, 0)$.
The compression update takes a descent step towards the gradient of
\begin{equation} \label{eq:loss_prox_gd}
    \lambda \sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2 
\end{equation}
followed by a soft thresholding.
Intuitively, the $\mathcal{L}_\text{red}$ component of \Cref{eq:loss_gd} keeps $\bm{W}_v$ fixed while learning $\bm{\alpha}_v$, and \Cref{eq:loss_prox_gd} keeps $\bm{\alpha}_v$ fixed in the proximal step to compress $\bm{W}_v$.
\Cref{fig:compression_step_intuition} depicts the intuition of the proximal gradient step and soft-thresholding in \cref{alg:main:prox}.

\paragraph{Details on learning the reduction loss $\mathcal{L}_\text{red}$.}
It is worth briefly discussing the use of the logarithm in \Cref{eq:loss_gd}.
It mainly gives more equal weight to the decreases in $\bm{W}_v$ column norms and encourages learning of better sparse Granger causal relations.
It furthermore normalizes the gradient updates to $\bm{\alpha}_v$.
Empirically, this loss engineering allowed training models that were significantly more robust to noise and changes to the sparsity hyperparameter $\lambda$.
This was reflected by a more stable variable usage and predictive loss $\mathcal{L}_\text{pred}$.

\paragraph{Ensuring non-negativity of the reduction coefficients $\bm\alpha$.}
For the proximal update step to be well-behaved, we need to ensure that $\bm{\alpha}_v$ results in a convex combination of column weights, i.e., that $\bm{\alpha}_v^w > 0$ for all $w \in \mathcal{V}$ and $\bm{\alpha}_v^T \mathds{1} = 1$.
We achieve this by re-parameterizing it as $\bm{\alpha}_v = \softmax\left( \bm{\beta}_v \right)$, and learning $\bm{\beta}_v$ instead of $\bm{\alpha}_v$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{content/figures/proximal-intuition.pdf}
    \caption{\textbf{The intuition of the compression step in \cref{alg:main:prox}.} Lighter colors indicate matrix columns with smaller norms. In some cases, the matrix retains the high values, indicating its importance in uncovering the Granger causal relations while thinning out others.}
    \label{fig:compression_step_intuition}
\end{figure}

\paragraph{Intuitive dynamics of the gradient update step.}
The weights $\bm{W}_v$ in the reduction loss $\mathcal{L}_\text{red}$ of \Cref{eq:loss_gd} only serve to learn good reduction coefficients $\bm{\alpha}_v$.
Deriving the gradient of the penalty term $\mathcal{L}_\text{reg}$ with respect to the underlying $\bm\beta$ provides a helpful intuition of the training dynamics:
\begin{align*}
    \frac{\partial}{\partial \beta_v^w } \sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2
    &= \frac{\partial}{\partial {\beta_v^w} } \sum_{w=1}^V \softmax(\bm{\beta}_v)^w \norm{\bm{W}_v^w}_2 \\
    &= \alpha_v^w \hspace{-3pt} \left( \hspace{-2pt} \norm{\bm{W}_v^w}_2 - \sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2 \hspace{-2pt} \right)
\end{align*}
\vspace{-4ex}
\begin{align*}
    \Longrightarrow
    \frac{\partial}{\partial \beta_v^w } \mathcal{L}_\text{reg}
    &= \lambda \frac{\partial}{\partial \beta_v^w } \log\left( \sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2 \right) \\
    &= \lambda \alpha_v^w \frac{
        \norm{\bm{W}_v^w}_2 -
        \sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2
    }{
        \sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2
    } \\
    &= \lambda \alpha_v^w \left( \frac{\norm{\bm{W}_v^w}_2}{\sum_{w=1}^V \alpha_v^w \norm{ \bm{W}_v^w }_2}  - 1\right)
\end{align*}
We can see that if the norm of a column $\norm{\bm{W_v^w}}_2$ is large, that corresponding $\frac{\partial}{\partial \beta_v^w } \mathcal{L}_\text{reg}$ will be large.
Gradient descent will thus decrease $\alpha_v^w$ and effectively allocate less weight to its removal in the compression step.
Furthermore, $\frac{\partial}{\partial \beta_v^w } \mathcal{L}_\text{reg}$ also scales with $\alpha_v^w$, resulting in a self-reinforcing loop that aids learning sparse representations.

\begin{table*}[t]
    \centering
    \caption{Comparison of AUROC for Granger causality selection among different approaches, as a function of the forcing constant $F$ and the length of the time series $T$. \textbf{Bold} denotes best, \underline{underlined} denotes the runner-up.}
    \label{tab:auroc_lorenz}
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Model} & \multicolumn{3}{c}{$F = 10$} & \multicolumn{3}{c}{$F = 40$} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
         & $T = 250$ & $T = 500$ & $T = 1000$ & $T = 250$ & $T = 500$ & $T = 1000$ \\
        \midrule
        cMLP      & {86.6 ± 0.2} & \underline{96.6 ± 0.2} & \underline{98.4 ± 0.1} & \underline{84.0 ± 0.5} & \textbf{89.6 ± 0.2} & \underline{95.5 ± 0.3} \\
        cLSTM     & 81.3 ± 0.9 & 93.4 ± 0.7 & 96.0 ± 0.1 & 75.1 ± 0.9 & 87.8 ± 0.4 & 94.4 ± 0.5 \\
        GC-KAN  & \underline{86.9 ± 0.5} & 92.1 ± 0.3 & 96.2 ± 0.2 & \textbf{86.3 ± 0.2} & 87.1 ± 0.4 & \textbf{95.7 ± 0.2} \\
        \textbf{\name{}} (ours)  & \textbf{99.6 ± 0.2}& \textbf{99.3 ± 0.3} & \textbf{99.1 ± 0.2} & 71.1 ± 0.3 & \underline{88.0 ± 0.2} & 91.4 ± 0.2  \\
        \bottomrule
    \end{tabular}
\end{table*}

\paragraph{Practical Considerations.}
Furthermore, we perform staged optimization of $\bm\alpha$, which is initialized to a uniform distribution by setting all $\bm\beta = 0$. We only start training $\bm\alpha$ after exploring the prediction loss and having obtained a reasonably compressed forecaster, which is controlled by the hyperparameter $K$ in \Cref{alg:main} (see \cref{alg:main:staged_alpha}).
While we present the method with mini-batch gradient descent for conciseness, improved convergence can be achieved with more modern optimizers, such as Adam~\citep{kingmaAdamMethodStochastic2017}.

\section{Experimental Evaluation}
\label{sec:exp}

We conduct extensive experiments on three datasets to assess the practical effectiveness of \name{}. We will now explain the underlying parameters used to train the \name{} architecture and then discuss the used data sets before presenting our obtained results in detail.

\subsection{Model Details}
For our component-wise networks, we use a single xLSTM block comprised of one sLSTM layer, followed by a linear layer to predict the next step of the modeled variate.
The hidden dimension of the sLSTM layer is set to 32 for all datasets. We find that the presence of a gated MLP to up- and down-project the hidden states of the sLSTM layer does not significantly improve the performance of the model, so we choose to omit it in all our experiments for simplicity.
We do not use any mLSTM layers in our experiments, as we find that the sLSTM layer is sufficient to capture all relevant long-range dependencies in the data. 

We use the Adam optimizer with a weight decay of 0.1 for training.
We schedule the learning rate to follow a linear warmup of 2,000 iterations to $10^{-4}$, followed by cosine annealing until the end of training for a total of 13,000 steps.
We start learning the reduction coefficients after a warmup period of $K=1,500$ iterations, during which the uniform compression across all columns combined with the prediction loss gives reasonable priors for the gradient directions of the reduction coefficients.
Due to the moderate size of the datasets, we performed full-batch gradient descent.
Only the sparsity hyperparameter $\lambda$ was tuned specifically for each of the settings.
We will describe them next.

\subsection{Datasets}
We now introduce the three datasets used to test the efficacy of \name{}.

\paragraph{Lorenz-96.}
The $V$-dimensional Lorenz-96 model \citep{Karimi_2010} is a chaotic multivariate dynamical system governed by the differential equations 
\begin{equation}
    \frac{d x_{t, i}}{dt} = (x_{t, i+1} - x_{t, i-2})x_{t, i-1} - x_{t, i} + F ,
\end{equation}
where $F$ is the forcing constant that determines the degree of nonlinearity in the system.
Low values of $F$ correspond to near-linear dynamics, while higher values induce chaotic behavior.
Following the setting of \cite{tank2021neural} we best comparability, we simulate $V=20$ variates with a sampling rate of $\Delta t = 0.05$ for a total of $T \in \{ 250, 500, 1000 \}$ time steps. We use two forcing constants $F \in \{ 10, 40 \}$ to test our model under different levels of non-linearity. 

\paragraph{Moléne Dataset.}
The Moléne dataset \citep{molene2015dataset} contains hourly temperatures recorded by sensors at $V = 32$ locations in Brittany, France during $T = 744$ hours. The objective is to understand the spatio-temporal dynamics of the temperature and to assess the extent to which the model can uncover complex relationships in weather by considering only local observations and their geographical positioning.

\begin{figure*}[t]
    \centering
    \newcommand{\widthMolene}[0]{0.75\linewidth}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\widthMolene]{content/figures/results/molene_lam8.png}
        \caption{$\lambda$ = 8.}
        \label{fig:molene_dense}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\widthMolene]{content/figures/results/molene_lam10.png}
        \caption{$\lambda = 10$.}
        \label{fig:molene_sparse}
    \end{subfigure}
    \caption{\textbf{Granger causal relations on the Moléne Dataset.} \name{} uncovers several intersting Granger causal relations giving us an insight into the dynamic weather patterns. We can observe that the sparsity of the learned Granger causal relations increases with the value of $\lambda$.}
    \label{fig:_molene_combined}
\end{figure*}

\paragraph{Human Motion Capture.}
We next apply our methodology to detect complex, nonlinear dependencies in human motion capture (MoCap) recordings. In contrast to the Lorenz-96 simulated dataset results, this analysis allows us to more easily visualize and interpret the learned network. 
We consider a data set from the CMU MoCap database~\citep{cmu_mocap}. The data recorded from 30 unique regions in the body consists of $V=54$ joint angle and body position recordings across multiple different subjects.
Since some regions, like the neck, have multiple degrees of freedom in both translation and rotation, we consider the GC relations between two joints based on edges between all movement directions. The motion ranges from locomotion (e.g., running and walking) over physical activities such as gymnastics and dancing to day-to-day social interactions.

\begin{figure*}[t]
    \centering
    \newcommand{\widthMoCap}[0]{0.24\linewidth}
    \begin{subfigure}[t]{\widthMoCap}
        \centering
        \includegraphics[width=\linewidth]{content/figures/results/salsa_dense.png}
        \caption{Salsa with $\lambda = 4$.}
        \label{fig:salsa_A}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\widthMoCap}
        \centering
        \includegraphics[width=\linewidth]{content/figures/results/salsa_sparse.png}
        \caption{Salsa with $\lambda = 5$.}
        \label{fig:salsa_B}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\widthMoCap}
        \centering
        \includegraphics[width=\linewidth]{content/figures/results/run_dense.png}
        \caption{Running with $\lambda = 4$.}
        \label{fig:running_A}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{\widthMoCap}
        \centering
        \includegraphics[width=\linewidth]{content/figures/results/run_sparse.png}
        \caption{Running with $\lambda = 6$.}
        \label{fig:running_B}
    \end{subfigure}
    \caption{\textbf{\name{} captures complex human motions.} \name{} is able to uncover complex real-world dependencies in the Human Motion Capture dataset giving us an intiuative understanding of the learned interactions.}
    \label{fig:mocap}
\end{figure*}

\subsection{Experimental Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{content/figures/results/gc_est_f40_t1000.pdf}
    \caption{\textbf{\name{} uncovers the vast majority of GC edges.} In the highly chaotic $F = 40$ setting of the Lorenz-96 system \name{} is accurate in predicting the GC edges, shown in dark blue~\textcolor[RGB]{0,0,80}{\rule{0.875em}{0.875em}}. Errors are marked red~\textcolor{red}{\raisebox{0.65ex}{\fbox{\phantom{\rule{0.4ex}{0.4ex}}}}}.}
    \label{fig:lorenz96-accuracy}
\end{figure}

\paragraph{Lorenz-96.} We compare the results of \name{} with three baseline methods: (1) the cMLP model, (2) the cLSTM model, both proposed by \cite{tank2021neural}, and (3) GC-KAN, proposed by \cite{lin2024granger}. The baseline results are taken as reported in the original papers. \Cref{tab:auroc_lorenz} shows the AUROC scores for the Granger causality selection task on the Lorenz-96 dataset. \name{} significantly outperforms all the baseline methods for the $F = 10$ case, across various time series. This suggests that \name{} is able to capture the underlying Granger causal relationships in the presence of limited data more efficiently than other methods.
The more focused compression of norms of columns in the projection layer combined with the superior modeling power of xLSTM enables the \name{} to selectively squash false positives while retaining true positives. For $F = 40$, \name{} performs competitively to the baselines in the presence of high non-linearity but is outperformed for $T = 250$. We postulate this is because the learning of reduction coefficients does not have access to high-quality projection weight priors during the initial stages of training, due to less data.
\name{} substantially overcomes this challenge and displays similar improvement trends as the baselines with increasing time steps.
Furthermore, the model displays strong prediction accuracies as seen in \Cref{fig:lorenz96-accuracy}.

\paragraph{Moléne.}
Unlike \citet{zoroddu2024learning}, who incorporate graph prior knowledge based on sensor locations, our approach learns the GC structure solely from the temperature observations.
This ensures that \name{} does not inherently favor regional connections over long-range dependencies, allowing it to discover dominant weather patterns operating both locally and across broader spatial scales.
\Cref{fig:_molene_combined} shows the obtained Granger causal graphs under different sparsity constraints.
The dense structure of \ref{fig:molene_dense} exhibits a richer set of Granger causal interactions, while the sparser graph in \ref{fig:molene_sparse} highlights the most significant edges. By adjusting $\lambda$, we can balance granularity and interpretability, allowing for insights into both local and large-scale temperature dependencies.

\paragraph{Human Motion Capture.}
To analyze GC relations between body joints, we focus on two specific activities: Salsa dancing and running, which provide interpretable motion patterns. \Cref{fig:mocap} shows the results of the learned graphs for those activities. 
A closer look at the subfigures offers us an intuitive understanding of the learned interactions. For example, in the Salsa dance, we observe edges from the feet to the knees and the arms, supporting the characteristic movements of the lower driving the upper body.
We can also see the cross-limb correlation, with movements initiating on one side of the body and being propagated to the other. Similarly, the results for running strongly establish the lower limbs as primary motion drivers, with edges from the feet and knees to the arms. We also see cyclic dependencies between the knees, ankles, and feet, capturing the repetitive, alternating nature of the gait.

\subsection{Inspecting Training Dynamics}
Here, we elaborate on the utility of the logarithm in the reduction loss $\mathcal{L}_\text{red}$ of \Cref{eq:loss_gd}.
The logarithm term incentivizes the model to explore sparser solutions to Granger causal discovery by allowing the training to move forward over any local minima that make use of the full set of input variates.
As the projection matrix becomes sparser and the input variates vanish from consideration by the model, the prediction loss $\mathcal{L}_\text{pred}$ will slightly increase. 
As seen in \Cref{fig:training}, an increase in the sparsity of the feature selectors $\bm W$ drives down the loss and enables the model to learn more meaningful Granger causal relations.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{content/figures/results/training_f40_t1000-edited.pdf}
    \caption{\textbf{\Cref{alg:main} jointly optimizes the prediction loss and establishes sparsity.} Results show the different loss components on Lorenz-96 with $T=40$. \name{} finds the best model while adaptively enforcing sparsity.}
    \label{fig:training}
\end{figure}


\section{Conclusion}
\label{sec:conclusion}

We presented \name{}, a novel xLSTM-based model to uncover Granger causal relations from the underlying time series data. \name{} first enforces the sparsity between the time series components and then learns a weight per time series to decide the importance of each time series for the underlying task. Each time series component is then modeled using a separate xLSTM model, which enables us to discover more interpretable Granger causal relationships between the time series variables. We validated \name{} in three scenarios, showing its effectiveness and adaptability in uncovering Granger causal relationships even in the presence of complex and noisy data.

Future work includes adapting \name{} to identify individual lags by separating weights for each lag. Using more sophisticated architectures such as xLSTM-Mixer~\citep{krausXLSTMMixerMultivariateTime2024} and TimeMixer~\citep{wangTimeMixerDecomposableMultiscale2023} is another interesting direction for further improvements.
Finally, extending our evaluations to more real-world datasets encompassing domains such as climate change and finance is an important next step.
