\section{BACKGROUND AND RELATED WORK}
Here, we formulate the URL and OPE/L.

\subsection{Unbiased recommender learning}
URL, particularly when employing a pointwise loss function, aims to learn the relevance of each user-item pair using only biased data that arises from display position. We denote \( u \in \mathcal{U} \) and \( i \in \mathcal{I} \) as the indices for users and items, respectively. Additionally, we define random variables relevance \( R_{u,i} \in \mathbb{R} \) and observation \( O_{u,i} \in \{ 0, 1 \} \), each associated with parameters \( \gamma_{u,i} \) and \( \theta_{u,i} \), respectively, for all user-item pairs \( (u,i) \). Consequently, we define the ideal loss function to be estimated as follows:

\vspace{-5mm}
\begin{align*}
    \mathcal{J}(f_{\phi}) := \sum_{u \in \mathcal{U},i \in \mathcal{I}} \delta(\gamma_{u,i}, f_{\phi}(u,i)).
\end{align*}
\vspace{-5mm}

where \( f_{\phi} \) is a recommendation algorithm, such as matrix factorization, and \( \delta(\cdot,\cdot) \) is a loss function, such as squared and cross-entropy errors. However, because we cannot access all pairs of \( \gamma_{u,i} \), we must utilize an estimator using only biased data. The IPS for addressing observation bias \cite{schnabel2016recommendations,saito2020unbiased} is defined as follows:

\vspace{-5mm}
\begin{align}
    \hat{J}_{\text{IPS}}(f_{\phi};\mathcal{D}) := \sum_{(u,i) \in \mathcal{D}} \frac{O_{u,i}}{\theta_{u,i}} \delta(R_{u,i}, f_{\phi}(u,i)).
\end{align}
\vspace{-3mm}

where \( \mathcal{D} \) is the observable logged data. The IPS utilizing \( \theta_{u,i} \) as a propensity score is unbiased for any \( f_{\phi} \) (i.e., \( \mathbb{E}_{\mathcal{D}}[\hat{\mathcal{J}}_{\text{IPS}}(f_{\phi};\mathcal{D})] = \mathcal{J}(f_{\phi}) \)).

\subsection{Off-policy evaluation/learning}
OPE aims to accurately estimate the value of a new policy using only logged data, which contains the logging policies biases. We first begin the notation. We define the user context, \( x \in \mathcal{X} \), generated from unknown distribution \( p(x) \) and the ranking action (item), \( \boldsymbol{a} := (\boldsymbol{a}_1,\cdots,\boldsymbol{a}_K) \in \Pi(\mathcal{A}) \), generated from the known distribution of logging policies \( \pi_{0}(\boldsymbol{a}|x) \) given \( x \). Here, \( K \) is the length of the ranking. We denote the deterministic action embedding (e.g., category) as \( \boldsymbol{e}_{\boldsymbol{a}} := (\boldsymbol{e}_{\boldsymbol{a}_1},\cdots,\boldsymbol{e}_{\boldsymbol{a}_K}) \in \Pi(\mathcal{E}) \).
We then define reward \( \boldsymbol{r} \) generated from unknown distribution \( p(\boldsymbol{r}|x,\boldsymbol{a}) \) given \( x \) and \( \boldsymbol{a} \). The value of target policy \( \pi \) to be estimated is defined as \( V(\pi) = \sum_{k=1}^{K} V^{(k)}(\pi) \), which is the sum of the position-wise values, as follows:

\vspace{-5mm}
\begin{align}
    V^{(k)}(\pi) = \mathbb{E}_{p(x)\pi(\boldsymbol{a}|x)}[q_{k}(x, \boldsymbol{a})].
\end{align}
\vspace{-5mm}

where \( q_{k}(x, \boldsymbol{a}) = \mathbb{E}[\boldsymbol{r}_k|x,\boldsymbol{a}] \) is the expected reward for each position \( k \) given \( x \) and \( \boldsymbol{a} \). We can obtain \( n \) logged data \( \mathcal{D} := \{ (x_i, \boldsymbol{a}_i, \boldsymbol{e}_{\boldsymbol{a}_i}, \boldsymbol{r}_i) \}_{i=1}^{n} \). This data is generated from \( (x,\boldsymbol{a}, \boldsymbol{e}_{\boldsymbol{a}}, \boldsymbol{r}) \sim p(x)\pi_{0}(\boldsymbol{a}|x)p(\boldsymbol{r}|x,\boldsymbol{a}) \), independently and identically.

We then define the position-wise values of the MIPS estimator \cite{saito2022off}, which addresses the logging policies bias, as follows:

\vspace{-6mm}
\begin{align}
    \hat{V}^{(k)}_{\text{MIPS}}(\pi;\mathcal{D}) := \frac{1}{n}\sum_{i=1}^{n} w(x_i, \boldsymbol{e}_{\boldsymbol{a}_{i,k}}) \boldsymbol{r}_{i,k}.
\end{align}
\vspace{-4mm}

where \( w(x, \boldsymbol{e}_{\boldsymbol{a}_k}) := \pi(\boldsymbol{e}_{\boldsymbol{a}_k}|x) / \pi_{0}(\boldsymbol{e}_{\boldsymbol{a}_k}|x) \) is the marginalized importance weight over action embedding spaces, and \( \pi(\boldsymbol{e}_{\boldsymbol{a}_k}|x) \) denotes the marginalized distribution that determines the action embedding for each position. In OPE, to evaluate the performance of the estimator, it is common practice to assess the mean squared error (MSE), which can be decomposed into the squared bias and estimator variance terms, as follows:

\vspace{-5mm}
\begin{align}
    \text{MSE}(\hat{V}(\pi;\mathcal{D})) &:= \mathbb{E}_{\mathcal{D}} \Big[(V(\pi) - \hat{V}(\pi;\mathcal{D}))^2 \Big], \\
    &= \text{Bias}(\hat{V}(\pi;\mathcal{D}))^2 + \mathbb{V}_{\mathcal{D}}[\hat{V}(\pi;\mathcal{D})]. \notag
\end{align}
\vspace{-5mm}

where \( \text{Bias}(\hat{V}(\pi;\mathcal{D})) := V(\pi) - \mathbb{E}_{\mathcal{D}}[\hat{V}(\pi;\mathcal{D})] \) is the difference between the true policy value and expected value of the estimator, and \( \mathbb{V}_{\mathcal{D}}[\hat{V}(\pi;\mathcal{D})] := \mathbb{E}_{\mathcal{D}}[(\hat{V}(\pi;\mathcal{D}) - \mathbb{E}_{\mathcal{D}}[\hat{V}(\pi;\mathcal{D})])^2] \) is the estimator variance. Therefore, to minimize the MSE, we should consider the trade-off between the bias and variance of the estimator. In contrast, OPL aims to identify the optimal policy, \( \pi_{\phi}^{*} \), parameterized by \( \phi \) using only logged data \cite{swaminathan2015counterfactual}.

The MIPS becomes unbiased (i.e., \( \mathbb{E}_{\mathcal{D}}[\hat{V}^{(k)}_{\text{MIPS}}(\pi;\mathcal{D})] = V^{(k)}(\pi) \)) under a common embedding support \cite{saito2022off} when the following assumption of no direct effects is satisfied. Furthermore, under these assumptions, the MIPS reduces the variance, as compared to the IPS \cite{saito2022off}.

\textbf{Assumption 1.} (\textit{no direct effect}) Rewards \( \boldsymbol{r} \) is not influenced by ranking actions \( \boldsymbol{a} \) (i.e., \( \mathbb{E}[\boldsymbol{r}_k|x,\boldsymbol{a}] = \mathbb{E}[\boldsymbol{r}_k|x,\boldsymbol{e}_{\boldsymbol{a}_k}] \) ).

\textbf{Limitation of previous studies} When considering the IPS(Eq.(1)) in URL as a variant of DM in the context of OPE that predicts the reward function directly, the IPS is subject to significant bias owing to its prediction errors. Similarly, OPE estimators may suffer from significant bias, although they do not introduce the bias associated with logging policies when rewards are MNAR, which is a realistic scenario. Therefore, it is desirable to develop a novel estimator that can effectively address both biases to recommend relevant items to each user.