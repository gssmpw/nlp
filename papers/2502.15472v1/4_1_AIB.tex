\section{Information Bottleneck for ATROC}
\label{sec:AIB}
\subsection{Problem Description}
Following the standard \gls{ib} framework \cite{Alemi_2017_DVI, Tishby_1999_TIB}, we assume the joint distribution of the system variables as follows:
\begin{equation}
    p(\bm{a}, \bm{x}, \bm{z}, \hat{\bm{z}}, \bm{y}) = p(\bm{a})p(\bm{x}|\bm{a})p_{\phi}(\bm{z}|\bm{x})p(\hat{\bm{z}}|\bm{z})p_{\theta}(\bm{y}|\hat{\bm{z}}).
\end{equation}
This sets up the Markov chain depicted as:
\begin{align}
    A \leftrightarrow X \leftrightarrow Z \leftrightarrow \hat{Z} \leftrightarrow Y.
\end{align}
We introduce a performance metric \textit{bits per service} to measure communication efficiency, which is defined as $k\cdot c$, where $c$ represents bits per symbol. Thus, there exists a crucial trade-off between bits per service and inference accuracy. This relationship underpins the formulation of an \gls{ib}, where we seek to optimize the balance between information throughput and decision accuracy.

The transformation from reconstructed symbols $\hat{\bm{z}}$ to task-specific data $\bm{y}$ is designed to preserve task-specific information, aligning task-oriented paradigms with traditional and reconstruction-oriented approaches. Based on the \gls{ib} theory \cite{Tishby_1999_TIB, Alemi_2017_DVI}, we formulate the following optimization problem:
\begin{subequations}
    \begin{align}
    \min \quad&-I(A;Y) \\
    \text{s.t.} \quad&I(X;\hat{Z})-\zeta \leq 0, \\
    &I(A;Y) - I(A;\hat{Z})= 0, \label{eq_optim_data_pro}
    \end{align}
\end{subequations}
where $\zeta$ represents the upper bound of data rate depending on the channel. The data processing inequality \cite{Cover_1991_EoI} implies that, ideally, if $Y$ and $\hat{Z}$ contain equivalent information about the action $A$, the equality $I(A;Y) - I(A;\hat{Z})=0$ holds.

\begin{figure*}
    \begin{subequations}
    \begin{align}
    \mathcal{L}_{\text{IB}}(\bm{a}, \bm{x}; \phi,\theta)&= \underbrace{-I(A;Y)}_{\text{Distortion}}
+\beta_1(\underbrace{I(X;\hat{Z})}_{\text{Rate}}-\zeta) +\beta_2\underbrace{(I(A;Y)-I(A;\hat{Z}))}_{\text{Alignment}} \label{eq_IB_origin}\\
&\equiv -I(A;Y) + \hat{\beta}_{1}I(X;\hat{Z})- \hat{\beta}_{2}I(A;\hat{Z}) \label{eq_IB_beta_hat}\\
&\equiv \mathbb{E}_{\bm{a}, \bm{x}}[\mathbb{E}_{\bm{y}|\bm{x};\phi,\theta}[-\log p(\bm{a}|\bm{y})]
+\hat{\beta}_{1} D_{K L}(p_{\phi}(\hat{\bm{z}} | \bm{x}) \| p(\hat{\bm{z}})) 
+\hat{\beta}_{2} \mathbb{E}_{\hat{\bm{z}} \mid \bm{x};\phi}[-\log p(\bm{a}|\hat{\bm{z}})]] \label{eq_IB_simplify}
    \end{align}
    \label{eq_IB}
    \end{subequations}
    {\noindent} \rule[0pt]{17.8cm}{0.05em}
    % {\noindent} \rule[0pt]{16cm}{0.05em}
\end{figure*}

We further formulate this problem as \cref{eq_IB_origin}, where $\beta_1 > 0$ and $\beta_2 > 0$ are the Lagrange multipliers. The detailed derivation can be found in \cref{subsec:VIB}.
The first term $-I(A;Y)$ and the second term $I(X;\hat{Z})$ formalize the classic information bottleneck, meanwhile, the third term $[I(A;Y)-I(A;\hat{Z})]$ aligns the task-relevant information between the task-specific data $\bm{y}$ and the reconstructed symbols $\hat{\bm{z}}$.

In the case $\beta_2 \neq 1$, we define $\hat{\beta}_{1} = \frac{\beta_1}{1-\beta_2}$ and $\hat{\beta}_{2} = \frac{\beta_2}{1-\beta_2}$. Then \cref{eq_IB_origin} can be expressed as \cref{eq_IB_beta_hat}. In the case $\beta_2 = 1$, \cref{eq_IB_origin} is simplified to the classic \gls{ib} formulation \cite{Tishby_1999_TIB, Alemi_2017_DVI, Shao_2022_LTO}:
\begin{align}
    \mathcal{L}_{\text{IB}}(\bm{a}, \bm{x}; \phi,\theta)=& \underbrace{-I(A;\hat{Z})}_{\text{Distortion}}
+\beta_1\underbrace{I(X;\hat{Z})}_{\text{Rate}}. 
\end{align}
This extended \gls{ib} theory preserves more task-specific information, and the bits per service is the same as the previous \gls{ib} approaches. Meanwhile, it maintains the dimension and structure required for edge inference.

\subsection{Variational Approach}
\label{subsec:VIB}
With the objective function \cref{eq_IB_beta_hat}, we first illustrate how to compute each term for training \(\phi\) and \(\theta\). We start with the first term, $-I(A;Y)$, expressed as:
\begin{align}
    -I(A;Y)=& -\int p(\bm{a},\bm{y})\log{\frac{p(\bm{a}|\bm{y})}{p(\bm{a})}} \dif \bm{a} \dif \bm{y} \notag\\
    =& -\int p(\bm{a},\bm{y})\log{p(\bm{a}|\bm{y})} \dif \bm{a} \dif \bm{y} - H(A),
\end{align}
where $p(\bm{a}|\bm{y})$ is the posterior probability, which can be derived through the Markov Chain \cite{Alemi_2017_DVI, Shao_2022_LTO} as:
\begin{align}
    p(\bm{a}|\bm{y}) &= \int p(\bm{a},\bm{x},\bm{z},\hat{\bm{z}}|\bm{y})\dif{\bm{x}}\dif{\bm{z}}\dif{\hat{\bm{z}}} \notag\\
    =& \int\frac{p(\bm{a})p(\bm{x}|\bm{a})p_{\phi}(\bm{z}|\bm{x})p(\hat{\bm{z}}|\bm{z})p_{\theta}(\bm{y}|\hat{\bm{z}})}{p(\bm{y})}\dif{\bm{x}}\dif{\bm{z}}\dif{\hat{\bm{z}}}.
\end{align}
Given the complexity of this integration, we employ a neural network $q_{\psi}(\bm{a}|\bm{y})$ as a variational approximation to $p(\bm{a}|\bm{y})$. 

Denoting the \gls{kl} divergence as $D_{\text{KL}}$. According to the definition of \gls{kl} divergence \cite{Cover_1991_EoI}, we can derive the following expression:
\begin{align}
D_{\text{KL}}(p(\bm{a}|\bm{y}) \parallel &q_{\psi}(\bm{a}|\bm{y})) \notag\\
=&\int p(\bm{a},\bm{y})\log p(\bm{a}|\bm{y}) \dif\bm{a}\dif\bm{y} \notag\\
&- \int p(\bm{a},\bm{y}) \log q_{\psi}(\bm{a}|\bm{y}) \dif\bm{a}\dif\bm{y}.
\end{align}
Based on the fact that 
\begin{align}
    D_{\text{KL}}(p(\bm{a}|\bm{y}) \parallel q_{\psi}(\bm{a}|\bm{y})) \geq 0,
    \label{eq_KL_conditional}
\end{align} 
we have
\begin{align}
\int p(\bm{a},\bm{y})\log p(\bm{a}|\bm{y})& \dif\bm{a}\dif\bm{y} \notag\\
\geq &\int p(\bm{a},\bm{y})\log q_{\psi}(\bm{a}|\bm{y}) \dif\bm{a}\dif\bm{y},
\label{eq_KL_conditional_2}
\end{align}
which derives
\begin{align}
\mathbb{E}_{\bm{a},\bm{x}}\bigl[\mathbb{E}_{\bm{y}|\bm{x};\phi,\theta}&[-\log p(\bm{a}|\bm{y})]\bigr] \notag\\
&\leq \mathbb{E}_{\bm{a},\bm{x}}\left[\mathbb{E}_{\bm{y}|\bm{x};\phi,\theta}[-\log q_{\psi}(\bm{a}|\bm{y})]\right].
\label{eq_firstTerm_mean}
\end{align}
The detailed derivation of \cref{eq_firstTerm_mean} can be found in Appendix \ref{apd_fristTerm_mean}.

The second term $I(X;\hat{Z})$ \cite{Shao_2022_LTO} is formulated as:
\begin{align}
I(X;\hat{Z})=\mathbb{E}_{\bm{a},\bm{x}}\left[D_{\text{KL}}(p_{\phi}(\hat{\bm{z}} | \bm{x}) \| p(\hat{\bm{z}})) \right],
\end{align}
where the marginal probability is 
\begin{align}
    p(\hat{\bm{z}})=\int p(\bm{a})p(\bm{x}|\bm{a})p_{\phi}(\bm{z}|\bm{x})p(\hat{\bm{z}}|\bm{z}) \dif\bm{a}\dif\bm{x}\dif\bm{z}.
\end{align}
We adopt a Gaussian approximation $q(\hat{\bm{z}}) \sim \mathcal{N}(\bm{0}, I)$ as an estimation for $p(\hat{\bm{z}})$ \cite{Kingma_2013_Aev}. It is reasonable as the \gls{jscc} encoder generates a Gaussian distribution $p_{\phi}(\hat{\bm{z}}|\bm{x}) \sim \mathcal{N}(\bm{\mu}_{\phi}(\bm{x}), \bm{\sigma}_{\phi}^2(\bm{x})I)$, where $\bm{\mu}_{\phi}(\cdot)$ and $\bm{\sigma}_{\phi}(\cdot)$ are functions that map the input data $\bm{x}$ to the mean and standard deviation of the Gaussian distribution.

Since $D_{\text{KL}}(p(\hat{\bm{z}})\parallel q(\hat{\bm{z}})) \geq 0$, the following upper bound can be derived:
\begin{align}
    I(X;\hat{Z}) \leq \mathbb{E}_{\bm{a},\bm{x}}\left[D_{\text{KL}}(p_{\phi}(\hat{\bm{z}} | \bm{x}) \| q(\hat{\bm{z}})) \right],
\end{align}
where the \gls{kl} divergence can be calculated analytically by the method in \cite{Duchi_2007_DfL}.

Similar to \cref{eq_firstTerm_mean}, by using $q_{\delta}(\bm{a}|\hat{\bm{z}})$ as a variational approximation of $p(\bm{a}|\hat{\bm{z}})$, we have
\begin{align}
    \mathbb{E}_{\bm{a},\bm{x}}\bigl[\mathbb{E}_{\hat{\bm{z}}|\bm{x};\phi,\theta}[-&\log p(\bm{a}|\hat{\bm{z}})]\bigr] \notag\\
    &\leq \mathbb{E}_{\bm{a},\bm{x}}\left[\mathbb{E}_{\hat{\bm{z}}|\bm{x};\phi,\theta}[-\log q_{\delta}(\bm{a}|\hat{\bm{z}})]\right].
\end{align} 
The above extended \gls{vib} formulation determines the upper bound of the \gls{ib} objective function (\cref{eq_IB_simplify}), which can be expressed as:
\begin{align}
\mathcal{L}_{\text{VIB}}(\bm{a}, \bm{x};\phi, \theta)= \mathbb{E}_{\bm{a},\bm{x}}&\Bigl\{
\mathbb{E}_{\bm{y}|\bm{x};\phi,\theta}[-\log q_{\psi}(\bm{a}|\bm{y})] \notag\\
&+\hat{\beta}_1 D_{\text{KL}}(p_{\phi}(\hat{\bm{z}} | \bm{x}) \| q(\hat{\bm{z}})) \notag\\
&+\hat{\beta}_2 \mathbb{E}_{\hat{\bm{z}}|\bm{x};\phi,\theta}[-\log q_{\delta}(\bm{a}|\hat{\bm{z}})]
\Bigr\}.
\label{eq_VIB_theory}
\end{align}
Through Monte Carlo sampling, we train \(\phi\) and \(\theta\) by minimizing this objective function using stochastic gradient descent.
Specifically, given a mini-batch of data $\{(\bm{a}_i,\bm{x}_i)\}^\Omega_{i=1}$ with batch size $\Omega$, if the reconstructed \gls{jscc} symbols $\hat{\bm{z}}$ are sampled $J_1$ times and the task-specific data $\bm{y}$ are sampled $J_2$ times for each data pair, the following estimation can be obtained:
\begin{align}
    \mathcal{L}_{\text{VIB}}(\bm{a}, \bm{x};\phi, \theta)\cong \frac{1}{\Omega}\sum_{i=1}^{\Omega} 
&\left\{
\frac{1}{J_2}\sum_{j=1}^{J_2}[-\log q_{\psi}(\bm{a}_{i}|\bm{y}_{j})] \right. \notag\\ 
&\left. +\hat{\beta}_1 D_{\text{KL}}(p_{\phi}(\hat{\bm{z}} | \bm{x}_{i}) \| q(\hat{\bm{z}})) \right. \notag\\
&\left. +\frac{\hat{\beta}_2}{J_1}\sum_{j=1}^{J_1}[-\log q_{\delta}(\bm{a}_{i}|\hat{\bm{z}}_{j})]
\right\}.
\label{eq_VIB_sampling}
\end{align}
