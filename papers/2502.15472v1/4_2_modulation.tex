\section{JSCC Modulation}
\label{sec:modulation}
In existing communication standards, symbols are transmitted with specific constellation orders and designs.  
In this section, we develop a \gls{jscc} modulation scheme that can map arbitrary complex-valued \gls{jscc} symbols to a predefined constellation diagram with finite points, as shown in \cref{fig:Quantization}. In addition, we introduce a learning method to adjust the optimal constellation parameter according to the quantization loss. For clarity, we use \gls{qam} as an example. Note that our method can be easily extended to other modulation schemes.

\subsection{Quantization and Normalization}
\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=1\linewidth]{Figure/Quantization.pdf}
    \end{center}
       \caption{An example of the JSCC modulation and signal transmission procedure for $\bm{z} \in \mathbb{C}^4$ using 16-QAM.}
    \label{fig:Quantization}
\end{figure*}
To enable the quantization of arbitrary complex-valued \gls{jscc} symbols into a predefined constellation diagram, the following rule is applied to each symbol:
\begin{align}
    \bar{z}_i=Q(z_i)=\arg\mathop{\min}_{e_j}\|z_i-e_j \|_2^2,
    \label{eq_quantization}
\end{align}
where $z_i \in \mathbb{C}$ represents the original symbol, $\bar{z_i} \in \mathbb{C}$ represents the quantized symbol, $i\in \{1, \cdots,k \}$, $Q(\cdot)$ denotes the quantization function, and $\|\cdot \|_2$ denote the $\ell^2$-norm. $e_j \in \{e_1, \cdots,e_u\}$ represents the predefined constellation points, where $e_j\in \mathbb{C}$, and $u$ denote the number of constellation points. This quantization operation can be extended to a vector as follows,
\begin{align}
    \bar{\bm{z}}=Q(\bm{z})=[Q(z_1), \cdots, Q(z_k)].
    \label{eq_quantization_vector}
\end{align}

Since the transmitted symbols should satisfy the average power constraint:
\begin{align}
    \frac{1}{k}\sum_{i=1}^{k} |\bar{z}_i|^2 \leq P_{\text{target}},
\end{align}
the channel input (normalized symbols) are given by:
\begin{align}
\bm{z}_\text{in} = \frac{\sqrt{P_{\text{target}}}}{\sqrt{P_{\bar{\bm{z}}}}}\cdot\bar{\bm{z}},
\end{align}
where $P_{\bar{\bm{z}}} = \frac{1}{k} \sum_{i=1}^{k} |\bar{z}_i|^2$ denotes the power of quantized symbols $\bar{\bm{z}}$.

The channel input $\bm{z}_\text{in}$ is transmitted through the channel $\bm{z}_{\text{out}} = h\cdot\bm{z}_{\text{in}} + \bm{n}$.
Assume that the receiver has the full \gls{csi} knowledge and knows $P_{\bar{\bm{z}}}$, in the case of the static channel, it can perform channel equalization:
\begin{align}
    \check{\bm{z}} = \frac{h^*}{|h|^2}\bm{z}_{\text{out}},
\end{align}
where $h^*$ denotes the conjugate of channel coefficient $h$ and $\check{\bm{z}}$ denotes the equalized symbols.  After equalization, the equalized symbols should be scaled as
\begin{align}
    \tilde{\bm{z}} = \frac{\sqrt{P_{\bar{\bm{z}}}}}{\sqrt{P_{\text{target}}}}\cdot\check{\bm{z}},
\end{align}
where $\tilde{\bm{z}}$ denotes the scaled symbols. Then the reconstructed symbols $\hat{\bm{z}}=Q(\tilde{\bm{z}})$ can be obtained by \cref{eq_quantization_vector}.

\subsection{Learnable Constellation Diagram and Fine-Tuning}
\label{subsec:constellation}
Traditional modulation techniques, such as \gls{qam}, employ a lookup table that maps bits to constellation points. In contrast, the complex-valued channel symbols produced by the \gls{jscc} encoder are continuous, necessitating a different approach for their mapping.

\Cref{eq_quantization} demonstrates that the coordinates of each constellation point $e_j$ directly affect the quantization outcome. We propose a learnable constellation diagram that adapts to the observed space of \gls{jscc} symbols, minimizes quantization loss, and improves performance with the \gls{jscc} encoder and the information reshaper. Taking $u$-\gls{qam} as an example, where $u$ denotes the number of constellation points, the coordinates of each constellation point can be derived by the parameter $r$. This parameter specifies the distance between two constellation points located at the corners of one side, as illustrated in \cref{fig:Quantization}. Then, the real part and imaginary part of the constellation point \(e_j\) are given as follows:
\begin{align}
\Re(e_{j})=&-\frac{r}{2}+\frac{(j\bmod\sqrt{u})\cdot r}{\sqrt{u}-1}, \\
\Im(e_{j})=&\frac{r}{2}-\frac{\lfloor{j/\sqrt{u}}\rfloor\cdot r}{\sqrt{u}-1},
\end{align}
where ``\(\,\bmod\,\)'' denotes the modulo operation and $\lfloor\cdot\rfloor$ denotes the rounding down function.

The quantization loss is defined as
\begin{align}
    \mathcal{L}_{Q}(\bm{z};r)=\frac{1}{k}\sum_{i=1}^{k}\mathop{\min}_{e_j}\|z_i-e_j \|_2.
\end{align}

The training process for the learnable constellation diagram begins with the initialization of the constellation parameter \(r\) to a predefined value \(r_{\text{init}}\), along with loading a pre-trained \gls{jscc} encoder. Using an image dataset \(\mathcal{X}\) with corresponding ground truth actions \(\mathcal{A}\), mini-batches are sampled iteratively during training. For each mini-batch, images are encoded into \gls{jscc} symbols, and the average batch loss is computed based on the quantization error. The constellation parameter \(r\) is then updated by backpropagation until convergence. The output of this process is the optimal constellation parameter \(r^*\). The detailed constellation parameter training process is provided in \cref{alg_quantization}. Once the optimal $r^*$ is obtained, the \gls{jscc} encoder and the information reshaper are jointly fine-tuned using the extended loss function:
\begin{align}
    \mathcal{L}_{\text{VIB-}Q}(\bm{a}, \bm{x};\phi,\theta)=\mathcal{L}_{\text{VIB}}(\bm{a}, \bm{x};\phi,\theta) + \beta_{Q}\mathcal{L}_{Q}(\bm{z};r^*),
    \label{eq_loss_VIBQ}
\end{align}
where $\beta_{Q}$ is a hyperparameter that balances the quantization loss with the original \gls{vib} loss.

This method enhances the practical applicability of \gls{jscc} modulation by integrating it with established digital communication systems while preserving the benefits of customized encoding and decoding strategies.

\input{Algorithm/alg_quantization}






 