\section{Performance Evaluation}
\label{sec:result}
\subsection{Experiment Setup}
\subsubsection{Dataset}
We utilize the Car Learning to Act (CARLA) simulator, an open-source platform designed for autonomous driving research \cite{Dosovitskiy_2017_CAO}, which provides a variety of urban environments that simulate real-world traffic scenarios. The image dataset from \cite{Wu_2022_TgC}, comprising images from various urban maps, serves as the input data \(\bm{x}\) for our training. In our experiments, the training dataset consists of 189,524 images from four maps: Town01, Town03, Town04, and Town06. The test dataset includes 27,201 images from another four maps: Town02, Town05, Town07, and Town10.

\subsubsection{Evaluation Metrics}
Our evaluation focuses on comparing the driving performance of our \gls{atroc} framework against various baselines within the CARLA simulator. We use the commonly adopted driving score\footnote{\url{https://leaderboard.carla.org/}} to assess the vehicle's ability to navigate according to predetermined waypoints, destinations, and comply with traffic regulations. Each test is conducted three times in Town05 under four different weather scenarios: clear noon, cloudy sunset, soft rain at dawn, and hard rain at night.


\subsubsection{Basic Settings}
In our proposed framework, we configure the \gls{jscc} symbols dimension $k$ to $1024$, enabling us to achieve significantly low bits per service of $6144$ for 64-QAM. For training of the \gls{jscc} encoder and the information reshaper, we set the Lagrange multipliers $\hat{\beta}_1 = 1$, $\hat{\beta}_2 = 8192$, and the quantization loss hyperparameter $\beta_{Q}=10$, which is a good balance between fidelity and compression. Furthermore, we set the learning rate \(\eta\) to \(4\times10^{-5}\), and impose a power constraint with $P_{\text{target}} = 1$. 

The \gls{tgcp} model is trained following the instructions of \cite{Wu_2022_TgC}. In addition, the pre-trained parameters $\psi$ of \gls{tgcp} are kept fixed throughout all training phases. This pre-trained \gls{tgcp} model serves as the AI agent in the following experiments.

For simplicity, a deterministic information reshaper is used, allowing us to approximate $q_{\delta}(\bm{a}|\hat{\bm{z}})$ by $q_{\psi}(\bm{a}|\bm{y})$. The architecture and detailed parameters of the proposed JSCC encoder and the information reshaper are shown in \cref{fig_codec_structure}.

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=1.0\linewidth]{Figure/codec_structure.pdf}
    \end{center}
       \caption{Architecture of the proposed JSCC encoder and information reshaper. For example, \textit{ConvC 3-1} represents a convolutional layer with \(C\) channels, a \(3\times3\) kernel size, and padding of 1 on both sides. \(\downarrow\)2 denotes the strided down convolutions, while NN\(\uparrow\)2 denotes the nearest neighbor upsampling. \textit{FC2048} refers to a fully connected layer with an output size of 2048. \textit{BatchNorm} denotes batch normalization, \textit{LReLU} represents the leaky ReLU activation with \(\alpha=0.2\), and \(\Omega\) represents the batch size. The dimensions (number of channels) of the inputs and outputs for the \textit{ResBlock} remain unchanged.}
    \label{fig_codec_structure}
\end{figure*}

\subsubsection{Baseline Methods}
Three traditional image coding methods are included as baseline methods for comparison: (1) JPEG \cite{Wallace_1992_TJs}; (2) JPEG2000 \cite{Taubman_2002_JIc}; (3) and BPG \cite{BPG}. Each traditional image coding method is followed by (2048, 6144) \gls{ldpc} codes combined with a 64-QAM digital modulation scheme. The average bits per service for these methods range from 36,844 to 1,041,758. 

In addition, baseline methods also include two state-of-the-art reconstruction-oriented \gls{jscc} designs, with the legends ``ROC-AE'' \cite{Bourtsoulatze_2019_DJS} and ``ROC-VAE'' \cite{Saidutta_2021_JSC}, which represent traditional autoencoder and variational autoencoder approaches. Note that the training dataset for the ROC-AE, ROC-VAE, \gls{atroc}, and pre-trained \gls{tgcp} is identical. For a fair comparison, ROC-AE, ROC-VAE, and \gls{atroc} use the same network structure, resulting in the same bits per service (i.e., $6144$).
In particular, ROC-VAE and ROC-AE are also fine-tuned by our proposed \gls{jscc} modulation scheme for 64-\gls{qam}, where the optimal constellation parameters $r^*$ are 4 and 50.4, respectively.

\subsection{Results of JSCC Modulation}
The constellation parameter $r$ is trained using a pre-trained \gls{jscc} encoder, as described in \cref{alg_quantization}. \Cref{fig:Q_train} demonstrates that regardless of the initial value of the constellation parameter, $r_{\text{init}}\in \{1,\cdots,10 \}$, the optimal constellation parameter $r^*$ consistently converges, validating the effectiveness of the proposed modulation approach.
 
Driving scores from different fine-tuned models across various constellation parameters based on 64-\gls{qam} are presented in \cref{fig:Q_score}. The model fine-tuned with the optimal constellation parameter $r^*$ outperforms other models under the \gls{awgn} channel with SNRs range from -10 dB to 10 dB, showcasing the superiority of our proposed \gls{jscc} modulation scheme.

\begin{figure*}[htbp]
    \centering
    \subfloat[Training of 16-QAM]{
        \includegraphics[width=0.31\textwidth]{Figure/Constellation_Parameter_Epoch_z_qam16.pdf}
    }
    \hspace{0.001in}
    \subfloat[Training of 64-QAM.]{
        \includegraphics[width=0.31\textwidth]{Figure/Constellation_Parameter_Epoch_z_qam64.pdf}
    }
    \hspace{0.001in}
    \subfloat[Training of 256-QAM.]{
        \includegraphics[width=0.31\textwidth]{Figure/Constellation_Parameter_Epoch_z_qam256.pdf}
    }
    \caption{Training of the constellation parameter for 16-QAM, 64-QAM, and 256-QAM. Regardless of the initial value of the constellation parameter, the optimal value consistently converges.}
    \label{fig:Q_train}
\end{figure*}


\begin{figure}[t]
\centering
    \begin{center}
    \includegraphics[width=0.36\textwidth]{Figure/score_r_compare.pdf}
    \end{center}
       \caption{Driving score of fine-tuned models based on 64-QAM with different constellation parameters ($r\in \{1, r^*, 10\}$, where $r^*=3.04$) under the AWGN channel with SNR range from -10 dB to 10 dB.}
    \label{fig:Q_score}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation on CARLA}

\begin{figure}[t]
    \centering
    \subfloat[AWGN channel with SNR = 10 dB.]{
        \includegraphics[width=0.44\textwidth]{Figure/bandComp_awgn.pdf}
        \label{subfig:score_bps_AWGN}
    }
    \hfill
    \centering
    \subfloat[Rayleigh fading channel with SNR = 20 dB.]{
        \includegraphics[width=0.44\textwidth]{Figure/bandComp_rayleigh.pdf}
        \label{subfig:score_bps_Rayleigh}
    }
   \caption{Driving score of traditional reconstruction-oriented communication with varied bits per service under AWGN channel and Rayleigh channel. The ATROC with \(6144\) bits per service serves as a baseline for comparison across both channel conditions. In addition, the TCGP using raw RGB images (\(5.5296\times 10^{6}\) bits per service) for autonomous driving is also included as a baseline.}
    \label{fig:score_bps}
\end{figure}


\begin{figure}[t]
    \centering
    \subfloat[AWGN channel with SNRs range from -10 dB to 10 dB.]{
        \includegraphics[width=0.4\textwidth]{Figure/score_AWGN_9methods.pdf}
        \label{fig:score_AWGN}
    }
    \hfill
    \centering
    \subfloat[Rayleigh channel with SNRs range from 0 dB to 20 dB.]{
        \includegraphics[width=0.4\textwidth]{Figure/score_Rayleigh_9methods.pdf}
        \label{fig:score_Rayleigh}
    }
    \caption{Driving score with varied SNRs under AWGN channel and Rayleigh channel.}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{Figure/Qualitative_add_bps.pdf}
       \caption{A qualitative example of our proposed method and baseline methods under Rayleigh fading channel with SNR = 20 dB and SNR = 0 dB. The bits per service of each image are provided in the upper left corner. The details in the reconstructed image are highlighted on the right side of the image. 1) blue box: vehicle and road marks; 2) red box: traffic lights; 3) purple box: cyclist and road marks; 4) green box: fence in the distance. Since traditional reconstruction-oriented communication methods (JPEG, JPEG2000, and BPG) fail to reconstruct images when SNR = 0 dB, we use ``N/A'' (Not Applicable) to represent the corrupted images.}
    \label{fig:result_qualitative}
\end{figure*}

The impact of bits per service on the driving score is illustrated in \cref{fig:score_bps}, where the driving score of \gls{tgcp} using raw images without communication is 25.34.
Notably, our proposed method achieves significant bandwidth savings (99.19\% compared to existing methods) while maintaining a required driving score of 20 under both the \gls{awgn} and Rayleigh fading channels. This substantial reduction in bits per service not only illustrates the efficiency of our approach but also underscores its capability to operate effectively under stringent bandwidth constraints. 

Detailed results for specific channel conditions are shown in \cref{fig:score_AWGN} and \cref{fig:score_Rayleigh}, demonstrating the dependency of driving scores on \gls{snr}. For traditional image coding methods, such as JPEG, JPEG2000, and BPG, we apply two configurations for comparison: (1) the average bits per service are set to 961,484 for \(\text{JPEG}^{*}\), 1,041,758 for \(\text{JPEG2000}^{*}\), and 759,683 for \(\text{BPG}^{*}\); (2) the average bits per service are reduced to 524,996 for \(\text{JPEG}^{-}\), 472,958 for \(\text{JPEG2000}^{-}\), and 442,152 for \(\text{BPG}^{-}\). The first configuration highlights the optimal performance of traditional image coding methods, as shown in \cref{subfig:score_bps_AWGN} and \cref{subfig:score_bps_Rayleigh}. In contrast, the second configuration approximately halves the bits per service from the first, as a basis for further comparison.
In contrast, our method requires only 6144 bits per service, highlighting its superior compression and transmission efficiency. In addition, we compare the proposed method \gls{atroc} with the state-of-the-art reconstruction-oriented \gls{jscc} designs using the same neural network structure, with the legends ``ROC-AE'' \cite{Bourtsoulatze_2019_DJS} and ``ROC-VAE'' \cite{Saidutta_2021_JSC}.

Under \gls{awgn} channel conditions, our method significantly outperforms reconstruction-oriented communication methods with driving scores of 15.72 at \gls{snr} = 0 dB, 15.18 at \gls{snr} = -5 dB, and 10.67 at \gls{snr} = -10 dB, as shown in \cref{fig:score_AWGN}. Traditional methods (\(\text{JPEG}^{*}\), \(\text{JPEG}^{-}\),  \(\text{JPEG2000}^{*}\), \(\text{JPEG2000}^{-}\), \(\text{BPG}^{*}\), and \(\text{BPG}^{-}\),) show a dramatic decline in the driving score as \gls{snr} decreases (below 0 dB), emphasizing the robustness of our \gls{atroc} framework under challenging conditions.

Similarly, in Rayleigh fading channel scenarios (\cref{fig:score_Rayleigh}), our proposed method continues to demonstrate superior performance with driving scores of 18.57 at \gls{snr} = 10 dB, 13.1 at \gls{snr} = 5 dB, and 12.73 at \gls{snr} = 0 dB. However, traditional methods experience significant performance degradation when \gls{snr} is below 10 dB.

Moreover, \gls{jscc}-based reconstruction-oriented communication methods perform poorly under this extremely limited communication bandwidth, as these methods fail to preserve task-specific information.

These findings are further supported by qualitative analysis, as illustrated in \cref{fig:result_qualitative}. \gls{jscc}-based reconstruction-oriented communication methods, while capable of producing high-quality image reconstructions suitable for human vision, often fail to retain crucial task-specific information, such as vehicles, cyclists, road markers, and traffic lights. This deficiency leads to poor performance in edge-based autonomous driving applications, where precise detection of such elements is critical for safety and efficiency. In contrast, our proposed method can effectively preserve task-specific information, shown in the blue, red, and purple boxes of \cref{fig:result_qualitative}. To reduce the required bits per service, it ignores task-agnostic information, shown in the green box of \cref{fig:result_qualitative}.
Moreover, our proposed method demonstrates remarkable noise resistance under low \gls{snr} conditions. It effectively preserves task-specific information, maintaining its completeness even in challenging communication environments.

Furthermore, in \Cref{tab_result_metrics}, we evaluate additional performance metrics such as \gls{psnr}, \gls{msssim}, and \gls{fid}, which are typically used to assess image quality from a human perspective. The divergence in performance metrics between traditional reconstruction-oriented methods and our proposed method highlights the necessity of a communication design that prioritizes machine vision, particularly in applications where decision-making accuracy is critical.

\input{Table/tab_result_metrics}


