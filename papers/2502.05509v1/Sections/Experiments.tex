\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}



\noindent
We evaluate the effectiveness of the attack on two tasks: face recognition and digit classification. For face recognition, we use the AT\&T Face Database \cite{samaria1994orl}, which contains 400 grayscale images across 40 unique subjects. For digit classification, the MNIST \cite{deng2012mnist} dataset is utilized, containing 70,000 grayscale images of handwritten digits. For SNNs, static image data from both datasets are converted into 25-step spiking representations using a rate-encoding scheme, where pixel intensities are mapped to spike rates.


The target models, both ANNs and SNNs, share the same fully connected architecture with a single hidden layer of 3,000 neurons to allow fair comparisons. For SNNs, we employ LIF neurons with a leakage rate of $0.7$. The SNNs are trained using backpropagation through time (BPTT) \cite{bellec2018long} with a fast-sigmoid function (slope of 40) for surrogate gradient calculations \cite{neftci2019surrogate}. The loss function accumulates the softmax cross-entropy across all time steps, computed from the membrane potentials of the output neurons. Table~\ref{tab:target_accuracy} summarizes the test accuracy of the target models (ANNs and SNNs) trained on the MNIST and AT\&T Face datasets. 

We adopt the attack protocol from the GAMIN framework, which retains the best-performing model during training to prevent performance degradation from suboptimal updates, and reduce the impact of catastrophic forgetting.
The query budget is set to 1,280,000 queries (or 20,000 batches of 64), aligning with the original framework to provide adequate query capacity for comprehensive evaluation. All implementations were performed using PyTorch \cite{paszke2019pytorch} framework, with SNNtorch \cite{eshraghian2023training} employed for building and training the SNNs. 




\subsection{Evaluation Metrics}


We employ five metrics to analyze different aspects of the black-box MI attack and evaluate the performance of the generator, and surrogate models.
Global Convergence Score (\(M_{\text{global}}\)), Fidelity Score (\(F_S\)), and Combined Accuracy (\(A_{S \circ G}\)) are adopted from GAMIN \cite{aivodji2019gamin}, while Surrogate Test Accuracy (\(A_S\)) and Target Model Accuracy on Inverted Samples (\(A_T\)) are introduced to provide additional insights into the surrogate and reconstructed samples.


\subsubsection{Global Convergence Score (\(M_{\text{global}}\))}
Inspired by the equilibrium loss in BEGAN \cite{berthelot2017began}, this metric assesses the overall performance of the attack by combining the surrogate model's fidelity and the generator’s effectiveness. A lower \(M_{\text{global}}\) value reflects improved alignment between the surrogate and generator and is defined as:
\begin{equation}
    M_{\text{global}} = L_H(X_S, Y_S) - |\lambda_k L_H(X_S, Y_S) - L_H(X_G, Y_G)|,
\end{equation}
where \(L_H(X_S, Y_S)\) is the loss of the surrogate model on random inputs, \(L_H(X_G, Y_G)\) is the loss of the surrogate on inputs generated by the generator, and \(\lambda_k\) is the dynamically adjusted equilibrium factor. \(M_{\text{global}}\) is also used during training to retain the best-performing generator-surrogate pair.

\input{Sections/Tables/Table_Target_Accuracy}
\input{Sections/Tables/Table_Attack_Metrics}




\subsubsection{Surrogate Fidelity (\(F_S\))} This metric evaluates how well the surrogate model approximates the target model’s behavior. It is computed as:
\begin{equation}
    F_S = 1 - \text{MAE}(y, \hat{y}),
\end{equation}
where \(y\) and \(\hat{y}\) are the predictions of the target model and surrogate model, respectively, on a batch of random inputs. A higher \(F_S\) value indicates better fidelity and corresponds to a closer alignment with the target model’s decision boundaries.

\subsubsection{Surrogate Test Accuracy (\(A_S\))} To assess the surrogate model’s generalization ability, we compute its classification accuracy on the original test set. This complements \(F_S\) by evaluating how well the surrogate performs on real data, as opposed to random or generated inputs.

\subsubsection{Combined Accuracy (\(A_{S \circ G}\))}
This metric evaluates the ability of generator \(G\)  to synthesize inputs that the surrogate \(S\) model classifies as belonging to the target class. It is derived by computing the categorical accuracy of the surrogate on the generator’s outputs, representing the proportion of generated samples assigned to the intended label.

\subsubsection{Target Model Accuracy on Inverted Samples (\(A_T\))} This metric evaluates the quality of the reconstructed inputs by passing a batch of inverted samples through the target model \(T\). It measures how accurately the target model classifies the generator’s outputs, providing direct insight into the effectiveness of the inversion attack.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% %% Accuracy of Target
% \begin{table}[htbp!]
% \centering
% % \renewcommand{\arraystretch}{1.0}
% \caption{Accuracy of the target models on the test data}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}cccccc}
% \toprule
% \multicolumn{2}{c}{\textbf{Dataset}}   & \multicolumn{2}{c}{\textbf{MNIST}} & \multicolumn{2}{c}{\textbf{AT\&T Face}}   \\
% \multicolumn{2}{c}{{Model Type}}      & ANN              & SNN             & ANN                & SNN                                                 \\ \midrule
% \multicolumn{2}{c}{\textbf{Test Accuracy}} & 97.71\%& 97.67\%& 95.00\%& 91.25\%\\ \bottomrule
% \end{tabular}%
% }

% \label{tab:target_accuracy}
% \end{table}






% \begin{table*}[ht]
% \centering
% \renewcommand{\arraystretch}{1.1}
% \caption{Evaluation Metrics for GAMIN Black-Box Model Inversion Attack on ANNs and SNNs}
% \label{tab:evaluation_metrics}
% \resizebox{0.8\textwidth}{!}{%
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% Dataset                     & Model Type & $M_{global}$        & $F_S$  &  $A_S $    & $A_{S \circ G}$  & $A_T$         \\  \midrule
% \multirow{2}{*}{MNIST}      & ANN        & 1.0371 $\pm$ 0.12& 0.8929                 & 68.13   $\pm$ 6.9               & 100           & 100           \\
%                             & SNN        & 1.4904  $\pm$ 0.58               & \textbf{0.9541}        & 19.88  $\pm$ 4.7                & 70.0          & 70.0          \\ \midrule
% \multirow{2}{*}{AT\&T Face} & ANN        & 3.4326 $\pm$ 0.33               & 0.9868                 & 29.32   $\pm$ 6.6               & 92.5          & 97.5          \\
%                             & SNN        & 3.9378  $\pm$ 0.65              & 0.9948                 & \textbf{ 6.18} $\pm$ 4.5         & \textbf{37.5} & \textbf{49.2} \\ \bottomrule
% \end{tabular}%
% }
% \end{table*}