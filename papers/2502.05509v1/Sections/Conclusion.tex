\section{Conclusion}
\label{sec:conclusion}

\noindent
In this work, we investigated the privacy characteristics of SNNs under black-box MI attacks, assessing whether their discrete and event-driven computations offer inherent resistance to adversarial data reconstruction. Using GAMIN, a black-box agnostic MI framework, we systematically evaluated the effectiveness of the attack on SNNs compared to traditional ANNs. Our findings reveal that SNNs exhibit stronger resistance to model inversion, with reconstructions displaying severe degradation, unstable attack convergence, and reduced effectiveness across multiple evaluation metrics.

A key factor contributing to this resistance appears to be the structure of the SNN decision boundaries, which differ fundamentally from those of ANNs. The discrete and temporally distributed nature of spike-based computations introduces irregular decision regions that disrupt the surrogate model’s ability to generalize, limiting the attack’s ability to approximate the target model effectively. Furthermore, the encoding and decoding mechanisms inherent to SNNs may introduce an additional layer of obfuscation, restricting the generator’s capacity to synthesize meaningful training samples.

Future research will investigate whether alternative spiking encodings, such as latency coding \cite{auge2021survey}, impact inversion resistance, and assess attack performance on neuromorphic datasets with real temporal dependencies, such as event-based DVS datasets. 

This study contributes to the understanding of privacy in neuromorphic computing, providing new insights into the adversarial risks associated with SNNs. As these architectures gain traction in privacy-sensitive applications, a deeper exploration of their privacy-preserving properties will be essential for the development of privacy-aware and resilient neuromorphic systems.

