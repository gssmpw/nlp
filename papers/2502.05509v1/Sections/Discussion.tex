\section{Discussion}
\label{sec:discussion}



\noindent
Table~\ref{tab:evaluation_metrics} presents the evaluation metrics for GAMIN black-box MI attacks on ANNs and SNNs, highlighting key differences in attack performance across both architectures.

The M-global scores, which reflect overall attack performance by providing insight into the convergence of the surrogate and generator networks, are consistently higher for SNNs. Additionally, the combined accuracy \( A_{S \circ G} \), which measures how well the generator aligns its outputs with the surrogate, drops substantially for SNNs, indicating that the attack struggles to converge and generate high-quality reconstructions in the spiking domain.





The classification accuracy of the target model on reconstructed samples (\( A_T \)) reveals a stark contrast between ANNs and SNNs. In the ANN case, the target model classifies its own reconstructed samples with near-perfect accuracy (100\% on MNIST and 97.5\% on AT\&T Face), demonstrating that the generated samples align well with the target model’s learned decision boundaries. However, for SNNs, \( A_T \) is significantly lower (70\% for MNIST and 49.2\% for AT\&T Face), implying that the reconstructed images fail to retain essential class-discriminative features.
\( A_T \) also serves as an indicator of convergence, indirectly measuring how well the optimization problem in Eq.~\ref{eq:MI-opt} is addressed.



This is further supported by the qualitative assessment of reconstructed images in Figures~\ref{fig:visual_mnist} and~\ref{fig:visual_face}, which display reconstructions for MNIST and AT\&T Face, respectively. While ANN-based reconstructions retain more discernible features of the original data, SNN-based reconstructions are significantly degraded, and in some cases, the attack fails to converge entirely, producing incoherent outputs, as seen in the reconstructed digit ‘0’ in Figure~\ref{fig:visual_mnist}.









A surprising countertrend emerges in surrogate fidelity (\( F_S \)), which is higher for SNNs than for ANNs, despite the significantly lower surrogate test accuracy (\( A_S \)). This suggests that while the surrogate model effectively mimics the SNN’s decision boundaries when queried with random inputs, it generalizes poorly to structured inputs such as the test set.
This discrepancy indicates that SNNs have fundamentally different decision boundary characteristics compared to ANNs. One plausible explanation is that SNNs encode information in a more discrete and temporally distributed manner, leading to more fragmented or irregular decision boundaries. As a result, the surrogate can closely approximate these boundaries using randomly sampled inputs (resulting in high \( F_S \)) but fails to interpolate effectively when exposed to structured data distributions (leading to low \( A_S \)).


Furthermore, the encoding and decoding mechanisms in SNNs may contribute to attack resistance. Unlike ANNs, which compute activations in a single forward pass, SNNs accumulate information over multiple time steps. This distributed processing could act as an implicit form of obfuscation, making it more challenging for the generator to align its outputs with the target model’s decision patterns.




Overall, these findings provide strong evidence that SNNs exhibit an inherent degree of robustness against black-box MI attacks.
Unlike ANNs, where the generator can exploit smooth decision boundaries to refine reconstructions, the event-driven computations in SNNs appear to introduce irregularities that degrade the inversion process. However, the high surrogate fidelity on random inputs raises interesting questions about the nature of SNN decision boundaries and whether alternative encoding schemes (e.g., latency coding instead of rate coding) influence SNN privacy. Additionally, extending this analysis to neuromorphic datasets with real temporal dependencies, such as IBM DvsGesture \cite{amir2017low}, would provide a broader understanding of SNN privacy in real-world applications.



\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{Sections/Figs/MIA_SNN_BL_MNIST.pdf}
  \caption{Qualitative assessment of reconstructed MNIST samples.}
  \label{fig:visual_mnist}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth]{Sections/Figs/MIA_SNN_BL_Face.pdf}
  \caption{Qualitative assessment of reconstructed AT\&T Face dataset samples.}
  \label{fig:visual_face}
\end{figure}





By highlighting these distinctions, this work contributes to a deeper understanding of adversarial threats in neuromorphic computing and establishes a foundation for future research into privacy-preserving spiking architectures.

























%%%%%%%%%%%%%%

% \begin{table}[ht]
% \caption{Evaluation Metrics for Black-Box Model Inversion}
% \label{tab:eval_loss}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}cccc@{}}
% \toprule
% Dataset                     & Model Type & $M_{global}$        & $F_S$      \\ \midrule
% \multirow{2}{*}{MNIST}      & ANN        & 1.0371  & 0.8929 \\ 
%                             & SNN        & 1.4904  & \textbf{0.9541}     \\ 
% \multirow{2}{*}{AT\&T Face} & ANN        & 3.4326  & 0.9868   \\ 
%                             & SNN        & 3.9378  & 0.9948   \\ \bottomrule
% \end{tabular}%
% }
% \end{table}






% \begin{table}[ht]
% \caption{Evaluation Metrics for Black-Box Model Inversion}
% \label{tab:eval_acc}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% Dataset                     & Model Type & $A_S $    & $A_{S \circ G}$  & $A_T$        \\ \midrule
% \multirow{2}{*}{MNIST}      & ANN        & 68.13  & 100     & 100         \\ 
%                             & SNN        & 19.88  & 70.0   & 70.0       \\ 
% \multirow{2}{*}{AT\&T Face} & ANN        & 29.32 & 92.5 & 97.5     \\ 
%                             & SNN        & \textbf{6.18}  & \textbf{37.5} & \textbf{49.2} \\ \bottomrule
% \end{tabular}%
% }
% \end{table}




