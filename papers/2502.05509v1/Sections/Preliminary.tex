\section{Preliminary}
\label{sec:preliminary}


\subsection{Spiking Neural Networks (SNNs)}

\noindent
SNNs are biologically inspired computational models designed to emulate the temporal dynamics and communication mechanisms of biological neurons. Unlike ANNs, which rely on continuous-valued activations, SNNs process information via discrete, time-dependent spikes \cite{schuman2022opportunities}. Each neuron in an SNN accumulates input spikes over time. When the membrane potential crosses a threshold, the neuron generates an output spike that propagates to downstream neurons. This event-driven communication makes SNNs energy-efficient and well-suited for neuromorphic applications.


Among various spiking neuron models, the \textit{Leaky Integrate-and-Fire (LIF)} model is widely adopted due to its simplicity and biological relevance \cite{izhikevich2004model}. The membrane potential dynamics of a single LIF neuron in discrete time can be expressed as:

\begin{equation}
    \nu[n] = \alpha \cdot \nu[n-1] + \sum_{k} \omega_k \cdot I_k[n] - O[n-1] \cdot \eta \ , 
\end{equation}
where $\nu[n]$ represents the membrane potential at time step $n$, $\alpha$ is the leakage factor that models the decay of potential over time, $I_k[n]$ denotes the spike input from presynaptic neuron $k$, and $\omega_k$ is the corresponding synaptic weight. The neuron’s output spikes, $O[n]$, are determined using a thresholding function:



\begin{equation}
    O[n] = 
    \begin{cases} 
        1, & \text{if } \nu[n] > \eta \\ 
        0, & \text{otherwise}.
    \end{cases}
    \label{eq:thresholding}
\end{equation}
When the membrane potential exceeds the firing threshold $\eta$, the neuron generates a spike and undergoes a soft-reset mechanism, subtracting the threshold from the potential to avoid continuous firing.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Sections/Figs/Rate_Encoding_C.pdf}
  \caption{Illustration of rate encoding: Higher pixel intensities correspond to higher spike rates, while lower intensities produce sparser spike trains.}
  \label{fig:rate_encoding}
\end{figure}

\begin{figure*}[htbp!]
  \centering
  \includegraphics[width=\textwidth]{Sections/Figs/GAMIN_SNN.pdf}
  \caption{Workflow of the GAMIN framework applied to an SNN target model. The surrogate model and generator are trained iteratively, with dark blue arrows representing the surrogate training phase and orange arrows indicating the generator training phase.}
  \label{fig:Gamin_SNN}
\end{figure*}


For event-driven systems, input spikes can be directly obtained from neuromorphic sensors such as \textit{Dynamic Vision Sensors (DVS)}, which naturally generate spikes in response to changes in visual stimuli \cite{hu2016dvs}. These sensors provide asynchronous spike-based data, making them inherently compatible with SNNs without requiring additional encoding.

However, to process non-spiking data in SNNs, inputs must be converted into spike trains using an encoding mechanism. One commonly used approach is \textit{rate encoding}, where the spike frequency represents input intensity: higher input values correspond to higher spike rates, while lower values result in sparser spikes (see Figure~\ref{fig:rate_encoding}).  In this scheme, a continuous input value is mapped to a spike train by generating spikes with a probability proportional to the input magnitude over a fixed time window \cite{gerstner2014neuronal}.





Training SNNs with backpropagation-based learning algorithms poses unique challenges due to the non-differentiable nature of spike-based activity. Particularly, the discontinuity of the spike generation function in Equation~\ref{eq:thresholding} blocks gradient flow, making standard backpropagation ineffective. To address this, one approach is to use \textit{surrogate gradient methods}. These methods approximate the spiking function with a smooth, differentiable alternative during the backward pass, enabling techniques like \textit{Backpropagation Through Time (BPTT)} to be applied effectively \cite{bellec2018long, neftci2019surrogate}.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Model Inversion (MI) Attacks}

\noindent
MI attacks aim to extract sensitive information from machine learning models by leveraging their learned representations. Formally, given a target model $T: \mathbb{R}^d \to \mathbb{R}^k$ and an observed output $\mathbf{y} = T(\mathbf{X})$ the objective of an MI attack is to approximate the input  $\mathbf{X} \in \mathbb{R}^d$ that produced the output $\mathbf{y} \in \mathbb{R}^k$. This is often framed as an optimization problem \cite{struppek2022plug}:
\begin{equation}
\hat{\mathbf{X}} = \arg\min_{\mathbf{X} \in \mathbb{R}^d} \mathcal{L}(T(\mathbf{X}), \mathbf{y}),
\label{eq:MI-opt}
\end{equation}
where $\mathcal{L}(\cdot, \cdot)$ is a loss function measuring the similarity between the model’s output for a candidate input $\mathbf{X}$ and the target output $\mathbf{y}$. In white-box scenarios, attackers often leverage gradient information from the model to iteratively solve this optimization problem. Black-box scenarios, on the other hand, are more challenging, as they rely solely on input-output queries without access to internal parameters or gradients. 
One popular approach to tackle black-box MI attacks is to train a surrogate model that approximates the behavior of the target model. By querying the target model with synthetic inputs and observing its responses, the surrogate model learns to mimic the target model’s decision boundaries, enabling attackers to indirectly reconstruct input  data\cite{fang2024privacy}.
