\section{Black-Box Model Inversion on SNNs}
\label{sec: methodology}





\noindent
In this work, we utilize GAMIN (Generative Adversarial Model INversion) \cite{aivodji2019gamin}, a framework originally proposed for black-box model inversion on ANNs, and adapt it for the SNN domain. GAMIN works by training two neural networks in tandem: \textbf{a generator} $G$, which maps random noise \mbox{$Z_G \sim \mathcal{N}(0, 1)$} to synthetic inputs $X_G$, and \textbf{a surrogate model} $S$, which approximates the target model $T$. The goal is to generate inputs that, when passed through the surrogate, match the desired target label $y_t$ output by $T$. The training process consists of the following steps:



\begin{enumerate}
    \item The generator $G$ produces synthetic inputs $X_G = G(Z_G)$ from random noise.
    \item The target model $T$ is queried using both $X_G$ and additional random inputs $X_S$, generating predictions $Y_G = T(X_G)$ and $Y_S = T(X_S)$.
    \item The surrogate model $S$ is trained to mimic the target model $T$ while distinguishing noise inputs $X_S$ from generated samples $X_G$. This is achieved using a boundary-equilibrium loss:
    % \item The surrogate model $S$ is trained to mimic $T$ using a boundary-equilibrium loss:
    \begin{equation}
        L_S = L_H(X_S, Y_S) - k_t \cdot L_H(X_G, Y_G),
    \end{equation}
    
    where $L_H$ is the cross-entropy loss, and $k_t$ is a dynamically adjusted equilibrium factor updated as:
    \begin{equation}
        k_{t+1} = k_t + \lambda_k (\gamma_k L_H(X_S, Y_S) - L_H(X_G, Y_G)),
    \end{equation}
    
    Here, $\lambda_k$ and $\gamma_k$ are hyperparameters controlling the adjustment of $k_t$.

    \item The generator $G$ is optimized by minimizing the cross-entropy loss between the predictions of the combined model $S \circ G$ and the target label $y_t$:
    \begin{equation}
            L_G = L_H(S(G(Z_G)), y_t).
    \end{equation}

    During this step, the parameters of $S$ are kept fixed, ensuring that $G$ learns to generate inputs that are classified as $y_t$ by the surrogate model.

\end{enumerate}


\noindent
These steps are repeated iteratively until the generator produces high-quality inputs corresponding to the target label $y_t$, while the surrogate model closely approximates the decision boundaries of $T$.

To adapt GAMIN for SNNs, encoding and decoding mechanisms are utilized to handle the distinct input-output representations of these models. \textit{Rate encoding} transforms static input data, such as pixel intensities, into spike trains by mapping intensities to spike rates over a fixed temporal window  \cite{auge2021survey}. 

For output processing, \textit{decoding mechanisms} convert spiking activity into confidence scores that GAMIN can process. Common decoding methods include spike count, time-averaged firing rate, and membrane potential \cite{eshraghian2023training}.

It should be noted that encoding and decoding mechanisms are embedded components of the SNN model and do not interfere with GAMINâ€™s model-agnostic assumption. As a result, GAMIN treats SNNs and ANNs equivalently as queryable black-box models, operating independently of their underlying architecture or computational principles.

The application of GAMIN to SNNs, combined with its query-based agnostic framework, enables a direct investigation of the privacy vulnerabilities in spiking models. Figure \ref{fig:Gamin_SNN} illustrates the overall workflow, showing the interaction between the generator, surrogate model, and target model.




