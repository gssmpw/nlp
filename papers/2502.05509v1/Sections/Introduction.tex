\section{Introduction}
\label{sec:intro}

\noindent
Spiking Neural Networks (SNNs) have emerged as a promising computational paradigm, inspired by the biological processes of neuronal communication. By processing information through discrete spike events, SNNs offer a more biologically plausible and efficient alternative to traditional Artificial Neural Networks (ANNs) \cite{nunes2022spiking}. Their event-driven nature, in which neurons remain mostly inactive until a spike occurs, enables SNNs to excel in applications such as real-time object detection \cite{cordone2022object, barchid2021deep}, speech recognition\cite{wu2020deep}, and robotics \cite{bing2018survey}, where energy efficiency and temporal processing are crucial. However, as their use expands into critical areas, understanding and addressing their potential privacy risks is essential to maintain the integrity and confidentiality of sensitive information.



Among the various privacy threats, Model Inversion (MI) attacks stand out due to their potential to reconstruct sensitive training data from a model's outputs. These attacks exploit the learned patterns of the model to reverse-engineer sensitive information, often revealing training data or inferring private attributes \cite{fredrikson2015model}. MI attacks can be divided into two main categories: white-box and black-box. In white-box scenarios, attackers have full access to the model's architecture and parameters, often employing gradient-based optimization for precise reconstructions. In contrast, black-box attacks operate under more restrictive conditions, relying solely on input-output queries to infer private information  \cite{dibbo2023sok}. Given the real-world relevance of black-box scenarios, their impact on SNN privacy warrants deeper investigation \cite{dionysiou2023exploring}.

Recent advancements in black-box MI attacks have introduced diverse methodologies to extract sensitive information from machine learning models. A prominent approach involves generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which optimize in a latent space rather than directly in the input space \cite{nguyen2024label,han2023reinforcement, kahla2022label}. These models iteratively adjust their latent variables to reconstruct candidate inputs that align with the target model’s outputs. Some methods further enhance the reconstruction quality by leveraging a learned prior from auxiliary public datasets for pre-training or fine-tuning \cite{khosravy2022model, xu2023sparse,ye2023c2fmi}. While effective, this reliance on auxiliary datasets limits their applicability when such data are unavailable or do not align with the target domain. To address this, agnostic black-box methods have emerged, which operate independently of external datasets or assumptions about the target data distribution \cite{tramer2016stealing}. One such approach, GAMIN \cite{aivodji2019gamin}, combines adversarial optimization and surrogate modeling to infer sensitive inputs solely through direct queries to the target model.


In SNNs, however, the prior studies have primarily explored privacy-preserving mechanisms and adversarial robustness. For example, differential privacy has been integrated into SNNs to protect training data by introducing noise during gradient updates, as demonstrated by \cite{wang2022dpsnn}. Similarly, \cite{kim2022privatesnn} addresses data and class leakage during ANN-to-SNN conversion through synthetic data generation and temporal learning rules. Other efforts, such as \cite{moshruba2024neuromorphic}, have highlighted the relative resilience of SNNs against membership inference attacks due to their non-differentiable spiking dynamics. In the context of MI attacks, BrainLeaks \cite{poursiami2024brainleaks} developed a tailored gradient-based attack to overcome the challenges of SNNs' spiking mechanisms, demonstrating that while these models exhibit some resistance, they remain vulnerable to such privacy breaches. However, the privacy characteristics of SNNs under black-box MI attacks remain unexplored, leaving a significant gap in understanding their behavior in restricted-access scenarios.

This gap highlights the need to systematically evaluate whether SNNs' spiking mechanisms confer unique privacy-preserving properties or render them susceptible to black-box MI attacks. To address this challenge, this paper investigates the behavior of SNNs under black-box MI attacks using the GAMIN framework. By leveraging GAMIN’s agnostic black-box setting, we provide a detailed evaluation of how SNNs respond to these attacks compared to ANNs. The primary contributions of this work are as follows:
\begin{enumerate}
    \item Conducting the first comprehensive evaluation of SNNs under black-box MI attacks through extensive experiments.
    \item Analyzing and comparing the susceptibility of SNNs and ANNs to black-box MI attacks, with a focus on evaluating differences arising from their computational paradigms.
    \item Offering new insights into the interplay between spiking dynamics and privacy, advancing our understanding of SNN-specific privacy characteristics.
\end{enumerate}

By addressing this unexplored domain, this work establishes a foundation for future research in the privacy of SNNs and neuromorphic computing.



