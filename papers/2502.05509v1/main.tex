%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HaMi's IEEE Conference Template %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[conference]{IEEEtran}

\hyphenpenalty=4000

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{textcomp}
\usepackage{xcolor, soul}
\usepackage{booktabs}




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


\title{Do Spikes Protect Privacy? 

Investigating Black-Box Model Inversion Attacks 

in Spiking Neural Networks} % Will be changed later

% \author{\IEEEauthorblockN{Anonymous Authors}}

% \author{\IEEEauthorblockN{Full Name}
% \IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering} \\
% George Mason University, Fairfax, VA, USA\\
% Email: netID@gmu.edu}
% \and
% \IEEEauthorblockN{Full Name}
% \IEEEauthorblockA{\textit{Department of Electrical and Computer Engineering} \\
% George Mason University, Fairfax, VA, USA\\
% Email: netID@gmu.edu}
% }

\author{\IEEEauthorblockN{Hamed Poursiami}
\IEEEauthorblockA{\textit{ECE Department} \\
George Mason University \\ Fairfax, VA, USA\\
Email: hpoursia@gmu.edu}

\and
\IEEEauthorblockN{Ayana Moshruba}
\IEEEauthorblockA{\textit{ECE Department} \\
George Mason University \\ Fairfax, VA, USA\\
Email: amoshrub@gmu.edu}

\and
\IEEEauthorblockN{Maryam Parsa}
\IEEEauthorblockA{\textit{ECE Department} \\
George Mason University \\ Fairfax, VA, USA\\
Email: mparsa@gmu.edu}

}






\maketitle
\begin{abstract}

As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. Model Inversion (MI) attacks pose a significant privacy threat by enabling adversaries to reconstruct training data from model outputs. While MI attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking Neural Networks (SNNs) remain largely unexplored in this context. Due to their event-driven and discrete computations, SNNs introduce fundamental differences in information processing that may offer inherent resistance to such attacks. A critical yet underexplored aspect of this threat lies in black-box settings, where attackers operate through queries without direct access to model parameters or gradients—representing a more realistic adversarial scenario in deployed systems. This work presents the first study of black-box MI attacks on SNNs. We adapt a generative adversarial MI framework to the spiking domain by incorporating rate-based encoding for input transformation and decoding mechanisms for output interpretation. Our results show that SNNs exhibit significantly greater resistance to MI attacks than ANNs, as demonstrated by degraded reconstructions, increased instability in attack convergence, and overall reduced attack effectiveness across multiple evaluation metrics. Further analysis suggests that the discrete and temporally distributed nature of SNN decision boundaries disrupts surrogate modeling, limiting the attacker’s ability to approximate the target model.

\end{abstract}

\begin{IEEEkeywords}
Spiking Neural Networks, Black-Box Model Inversion Attacks, Neuromorphic Privacy, Adversarial Machine Learning
\end{IEEEkeywords}



\input{Sections/Introduction}
\input{Sections/Preliminary}
\input{Sections/Methodology}
\input{Sections/Experiments}
\input{Sections/Discussion}
\input{Sections/Conclusion}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=\linewidth]{ADDRESS}
%   \caption{CAPTION}
%   \label{fig:name}
% \end{figure}










\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
