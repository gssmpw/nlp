\section{Related Work}
% % \textbf{Parameter-efficient Fine-tuning.} Parameter-efficient fine-tuning methods____, a significant advancement over traditional full fine-tuning, offer the ability to achieve commendable results by training only a small subset of parameters, thereby reducing computational costs and improving efficiency. BitFit____ only updates the bias terms while freezing most of the pre-trained model's parameters, but it performs poorly on larger models____. Threshold-Mask____ constructs a binary mask matrix using a threshold to select pre-trained weights in the attention and feedforward layers through element-wise multiplication. Adapter-based methods, such as Adapters____, HyperFormer____and Compacter____, add trainable adapter modules to each Transformer block in the T5 model____. AdapterDrop____ improves efficiency by removing unimportant adapters for a given task in each layer of the Transformer. LoRA____ re-parameterizes incremental matrices through simple low-rank decomposition. Structurally similar to LoRA, KronA____ replaces the low-rank decomposition in LoRA with Kronecker product decomposition. PISSA____ also adopts the same architecture as LoRA, but it initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. LST____ reduces training memory by running a small ladder network alongside the pre-trained network. Prompt tuning____, a novel and promising parameter-efficient fine-tuning method, stands out from the rest by achieving good results with training very few parameters. Unlike other methods, it maintains a consistent number of trainable parameters even as the model size expands, making it a potential game-changer in the field.
% % \ \\ \noindent
% % \textbf{Soft Prompt-based Fine-tuning.} Soft prompt tuning inserts trainable continuous vectors, known as soft prompts, into the model's input or hidden states. Unlike hard prompts, soft prompts are generated by searching the discrete token space based on task-specific training data. In 2021, Li and Liang proposed prefix tuning____, which adds soft prompts at the input to adjust the model's behavior. Unlike prefix tuning, prompt tuning____ only adds soft prompts to the embedding layer of the model. Other methods have explored transfer learning-based prompt tuning, such as MPT____ and ATTEMPT____, which better initialize soft prompts through pre-training, but these methods require significant resources for the pre-training process.Xiao____ pointed out that soft prompts inherently exhibit low-rank properties, which can be leveraged to further enhance the method's parameter efficiency and performance through certain strategies. DePT____ decomposes the soft prompt into shorter prompts and pairs of low-rank matrices, which are then used to update the model's weights. DPT____ employs a re-parameterization strategy, using two low-rank matrices to replace the original soft prompt. However, these PT-based methods need more efficiency and richness of task-specific knowledge when dealing with long soft prompts.