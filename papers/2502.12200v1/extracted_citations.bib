@inproceedings{asai2022attempt,
  title={Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts},
  author={Asai, Akari and Salehi, Mohammadreza and Peters, Matthew E and Hajishirzi, Hannaneh},
  booktitle={Proceedings of EMNLP},
  pages={6655--6672},
  year={2022}
}

@inproceedings{ding2023sparse,
  title={Sparse Low-rank Adaptation of Pre-trained Language Models},
  author={Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of EMNLP},
  year={2023}
}

@article{edalati2022krona,
  title={Krona: Parameter efficient tuning with kronecker adapter},
  author={Edalati, Ali and Tahaei, Marzieh and Kobyzev, Ivan and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2212.10650},
  year={2022}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={Proceedings of ICML},
  pages={2790--2799},
  year={2019}
}

@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={Proceedings of ICLR},
  year={2021}
}

@article{karimi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1022--1035},
  year={2021}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of EMNLP},
  pages={3045--3059},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

@article{mahabadi2021parameter,
  title={Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks},
  author={Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  journal={arXiv preprint arXiv:2106.04489},
  year={2021}
}

@article{meng2024pissa,
  title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models},
  author={Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
  journal={arXiv preprint arXiv:2404.02948},
  year={2024}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{ruckle2020adapterdrop,
  title={Adapterdrop: On the efficiency of adapters in transformers},
  author={R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2010.11918},
  year={2020}
}

@article{sung2022lst,
  title={Lst: Ladder side-tuning for parameter and memory efficient transfer learning},
  author={Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12991--13005},
  year={2022}
}

@inproceedings{wang2023multitask,
  title={Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning},
  author={Wang, Zhen and Panda, Rameswar and Karlinsky, Leonid and Feris, Rogerio and Sun, Huan and Kim, Yoon},
  booktitle={Proceedings of ICLR},
  year={2023}
}

@inproceedings{xiao2023decomposed,
  title={Decomposed Prompt Tuning via Low-Rank Reparameterization},
  author={Xiao, Yao and Xu, Lu and Li, Jiaxi and Lu, Wei and Li, Xiaoli},
  booktitle={Proceedings of EMNLP},
  year={2023}
}

@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}

@inproceedings{zhao2020masking,
  title={Masking as an Efficient Alternative to Finetuning for Pretrained Language Models},
  author={Zhao, Mengjie and Lin, Tao and Mi, Fei and Jaggi, Martin and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of EMNLP},
  pages={2226--2241},
  year={2020}
}

@article{zhu2023sira,
  title={Sira: Sparse mixture of low rank adaptation},
  author={Zhu, Yun and Wichers, Nevan and Lin, Chu-Cheng and Wang, Xinyi and Chen, Tianlong and Shu, Lei and Lu, Han and Liu, Canoee and Luo, Liangchen and Chen, Jindong and others},
  journal={arXiv preprint arXiv:2311.09179},
  year={2023}
}

