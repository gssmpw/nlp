% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@inproceedings{guo2021parameter,
  title={Parameter-Efficient Transfer Learning with Diff Pruning},
  author={Guo, Demi and Rush, Alexander M and Kim, Yoon},
  booktitle={Proceedings of ACL},
  pages={4884--4896},
  year={2021}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of EMNLP},
  pages={3045--3059},
  year={2021}
}

@inproceedings{razdaibiedina-etal-2023-residual,
    title = {Residual Prompt Tuning: improving prompt tuning with residual reparameterization},
    author = {Razdaibiedina, Anastasiia  and
      Mao, Yuning  and
      Khabsa, Madian  and
      Lewis, Mike  and
      Hou, Rui  and
      Ba, Jimmy  and
      Almahairi, Amjad},
    booktitle = {Proceedings of ACL},
    year = {2023},
    pages = {6740--6757},
}

@article{doddapaneni2024user,
  title={User Embedding Model for Personalized Language Prompting},
  author={Doddapaneni, Sumanth and Sayana, Krishna and Jash, Ambarish and Sodhi, Sukhdeep and Kuzmin, Dima},
  journal={arXiv preprint arXiv:2401.04858},
  year={2024}
}

@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={SuB},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Proceedings of KR},
  year={2012}
}

@inproceedings{pilehvar2019wic,
  title={WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={Proceedings of NAACL},
  pages={1267--1273},
  year={2019}
}

@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of NAACL},
  pages={2924--2936},
  year={2019}
}

@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of NAACL},
  pages={252--262},
  year={2018}
}

@article{zhang2018record,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}

@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={Proceedings of ICLR},
  year={2021}
}

@inproceedings{wang2019superglue,
  title={SuperGLUE: a stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Proceedings of NeurIPS},
  pages={3266--3280},
  year={2019}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={Proceedings of EMNLP},
  pages={353--355},
  year={2018}
}

@inproceedings{
shi2024dept,
title={DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning},
author={Zhengxiang Shi and Aldo Lipani},
booktitle={Proceedings of ICLR},
year={2024}
}

@inproceedings{dettmers2023qlora,
  title={QLORA: efficient finetuning of quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Proceedings of NeurIPS},
  pages={10088--10115},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhong2024panda,
  title={Panda: Prompt transfer meets knowledge distillation for efficient model adaptation},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={TKDE},
  year={2024},
  publisher={IEEE}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{Vaswani2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Proceedings of NeurIPS},
  year={2017}
}

@inproceedings{xiao2023decomposed,
  title={Decomposed Prompt Tuning via Low-Rank Reparameterization},
  author={Xiao, Yao and Xu, Lu and Li, Jiaxi and Lu, Wei and Li, Xiaoli},
  booktitle={Proceedings of EMNLP},
  year={2023}
}

@inproceedings{giampiccolo2007third,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL},
  pages={1--9},
  year={2007}
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={Proceedings of AAAI},
  year={2011}
}

@inproceedings{aribandi2021ext5,
  title={ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
  author={Aribandi, Vamsi and Tay, Yi and Schuster, Tal and Rao, Jinfeng and Zheng, Huaixiu Steven and Mehta, Sanket Vaibhav and Zhuang, Honglei and Tran, Vinh Q and Bahri, Dara and Ni, Jianmo and others},
  booktitle={Proceedings of ICLR},
  year={2021}
}

@article{lan2024efficient,
  title={Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion},
  author={Lan, Pengxiang and Yang, Enneng and Liu, Yuting and Guo, Guibing and Jiang, Linying and Zhao, Jianzhe and Wang, Xingwei},
  journal={arXiv preprint arXiv:2405.11464},
  year={2024}
}


@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}

@inproceedings{zaken2022bitfit,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Zaken, Elad Ben and Goldberg, Yoav and Ravfogel, Shauli},
  booktitle={Proceedings of ACL},
  pages={1--9},
  year={2022}
}

@inproceedings{ding2023sparse,
  title={Sparse Low-rank Adaptation of Pre-trained Language Models},
  author={Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of EMNLP},
  year={2023}
}

@article{zhu2023sira,
  title={Sira: Sparse mixture of low rank adaptation},
  author={Zhu, Yun and Wichers, Nevan and Lin, Chu-Cheng and Wang, Xinyi and Chen, Tianlong and Shu, Lei and Lu, Han and Liu, Canoee and Luo, Liangchen and Chen, Jindong and others},
  journal={arXiv preprint arXiv:2311.09179},
  year={2023}
}

@inproceedings{zhao2020masking,
  title={Masking as an Efficient Alternative to Finetuning for Pretrained Language Models},
  author={Zhao, Mengjie and Lin, Tao and Mi, Fei and Jaggi, Martin and Sch{\"u}tze, Hinrich},
  booktitle={Proceedings of EMNLP},
  pages={2226--2241},
  year={2020}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={Proceedings of ICML},
  pages={2790--2799},
  year={2019}
}

@article{mahabadi2021parameter,
  title={Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks},
  author={Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  journal={arXiv preprint arXiv:2106.04489},
  year={2021}
}

@article{karimi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1022--1035},
  year={2021}
}

@article{ruckle2020adapterdrop,
  title={Adapterdrop: On the efficiency of adapters in transformers},
  author={R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2010.11918},
  year={2020}
}

@article{edalati2022krona,
  title={Krona: Parameter efficient tuning with kronecker adapter},
  author={Edalati, Ali and Tahaei, Marzieh and Kobyzev, Ivan and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2212.10650},
  year={2022}
}

@article{meng2024pissa,
  title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models},
  author={Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
  journal={arXiv preprint arXiv:2404.02948},
  year={2024}
}

@article{sung2022lst,
  title={Lst: Ladder side-tuning for parameter and memory efficient transfer learning},
  author={Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12991--13005},
  year={2022}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@inproceedings{asai2022attempt,
  title={Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts},
  author={Asai, Akari and Salehi, Mohammadreza and Peters, Matthew E and Hajishirzi, Hannaneh},
  booktitle={Proceedings of EMNLP},
  pages={6655--6672},
  year={2022}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL},
  pages={4171--4186},
  year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{wei2021finetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={Proceedings of ICLR},
  year={2021}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{wu2024personalized,
  title={Personalized Prompt for Sequential Recommendation},
  author={Wu, Yiqing and Xie, Ruobing and Zhu, Yongchun and Zhuang, Fuzhen and Zhang, Xu and Lin, Leyu and He, Qing},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@inproceedings{liu2024can,
  title={Can we soft prompt LLMs for graph learning tasks?},
  author={Liu, Zheyuan and He, Xiaoxin and Tian, Yijun and Chawla, Nitesh V},
  booktitle={Proceedings of WWW},
  pages={481--484},
  year={2024}
}

@article{yi2023contrastive,
  title={Contrastive graph prompt-tuning for cross-domain recommendation},
  author={Yi, Zixuan and Ounis, Iadh and Macdonald, Craig},
  journal={ACM Transactions on Information Systems},
  volume={42},
  number={2},
  pages={1--28},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2022multitask,
  title={Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning},
  author={Wang, Zhen and Panda, Rameswar and Karlinsky, Leonid and Feris, Rogerio and Sun, Huan and Kim, Yoon},
  booktitle={Proceedings of ICLR},
  year={2022}
}

@inproceedings{ma2022xprompt,
  title={XPrompt: Exploring the Extreme of Prompt Tuning},
  author={Ma, Fang and Zhang, Chen and Ren, Lei and Wang, Jingang and Wang, Qifan and Wu, Wei and Quan, Xiaojun and Song, Dawei},
  booktitle={Proceedings of EMNLP},
  pages={11033--11047},
  year={2022}
}

@inproceedings{yang2024graphpro,
  title={GraphPro: Graph Pre-training and Prompt Learning for Recommendation},
  author={Yang, Yuhao and Xia, Lianghao and Luo, Da and Lin, Kangyi and Huang, Chao},
  booktitle={Proceedings of WWW},
  pages={3690--3699},
  year={2024}
}

@inproceedings{wei2024promptmm,
  title={PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning},
  author={Wei, Wei and Tang, Jiabin and Xia, Lianghao and Jiang, Yangqin and Huang, Chao},
  booktitle={Proceedings of WWW},
  pages={3217--3228},
  year={2024}
}

@inproceedings{vu2022spot,
  title={SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer},
  author={Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel},
  booktitle={Proceedings of ACL},
  pages={5039--5059},
  year={2022}
}

@inproceedings{aghajanyan2021intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  booktitle={Proceedings of ACL},
  pages={7319--7328},
  year={2021}
}

@article{hansen1987truncated,
  title={The truncated SVD as a method for regularization},
  author={Hansen, Per Christian},
  journal={BIT Numerical Mathematics},
  volume={27},
  pages={534--553},
  year={1987},
  publisher={Springer}
}


@inproceedings{loshchilov2018decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={Proceedings of ICLR},
  year={2019}
}

@inproceedings{wang2023multitask,
  title={Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning},
  author={Wang, Zhen and Panda, Rameswar and Karlinsky, Leonid and Feris, Rogerio and Sun, Huan and Kim, Yoon},
  booktitle={Proceedings of ICLR},
  year={2023}
}

@inproceedings{choi2023smop,
  title={SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts},
  author={Choi, Joon-Young and Kim, Junho and Park, Jun-Hyung and Mok, Wing-Lam and Lee, SangKeun},
  booktitle={Proceedings of EMNLP},
  year={2023}
}

@inproceedings{li2018measuring,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={Proceedings of ICLR},
  year={2018}
}

@article{zhang2024multi,
  title={Multi-domain Knowledge Graph Collaborative Pre-training and Prompt Tuning for Diverse Downstream Tasks},
  author={Zhang, Yichi and Hu, Binbin and Chen, Zhuo and Guo, Lingbing and Liu, Ziqi and Zhang, Zhiqiang and Liang, Lei and Chen, Huajun and Zhang, Wen},
  journal={arXiv preprint arXiv:2405.13085},
  year={2024}
}

@inproceedings{hayoulora+,
  title={LoRA+: Efficient Low Rank Adaptation of Large Models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  booktitle={Proceedings of ICML},
  year={2024}
}

@inproceedings{liudora,
  title={DoRA: Weight-Decomposed Low-Rank Adaptation},
  author={Liu, Shih-yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  booktitle={Proceedings of ICML},
  year={2024}
}

@article{wang2024loraGA,
  title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  journal={arXiv preprint arXiv:2407.05000},
  year={2024}
}

@article{kalajdzievski2023rsLoRA,
  title={A rank stabilization scaling factor for fine-tuning with lora},
  author={Kalajdzievski, Damjan},
  journal={arXiv preprint arXiv:2312.03732},
  year={2023}
}

@inproceedings{williams2018broad,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
  booktitle={Proceedings of NAACL},
  pages={1112--1122},
  year={2018}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of EMNLP},
  pages={2383--2392},
  year={2016}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of EMNLP},
  pages={1631--1642},
  year={2013}
}

@inproceedings{bill2005automatical,
  title={Automatical ly constructing a corpus of sentential paraphrases},
  author={Bill, DOLAN William},
  booktitle={Proceedings of IWP},
  pages={9--16},
  year={2005}
}