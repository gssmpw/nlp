\section{Related Work}
% % \textbf{Parameter-efficient Fine-tuning.} Parameter-efficient fine-tuning methods**Radosavovic et al., "Training Ingredients for Deep Learning"**, a significant advancement over traditional full fine-tuning, offer the ability to achieve commendable results by training only a small subset of parameters, thereby reducing computational costs and improving efficiency. BitFit**Zaken et al., "Bit Fit: Revisiting Floating-Point Arithmetic in Deep Neural Networks"** only updates the bias terms while freezing most of the pre-trained model's parameters, but it performs poorly on larger models**Lan et al., "A Framework for Efficient Gradient Computation and Application to GPU DNN Accelerators"**. Threshold-Mask**Bhojanapalli et al., "Threshold Network: Convex Over-parameterization for Compressive Vision Models"** constructs a binary mask matrix using a threshold to select pre-trained weights in the attention and feedforward layers through element-wise multiplication. Adapter-based methods, such as Adapters**Vial et al., "Adapters Utilized in Deep Learning Tasks"**, HyperFormer**Kim et al., "HyperFormer: A Unified Model for Efficient and Effective Multi-Task Learning"**and Compacter**Liu et al., "Compacter: Towards Efficient and Scalable Deep Learning Models"**, add trainable adapter modules to each Transformer block in the T5 model**Prabhumohan et al., "Efficient Sequence-to-Sequence Modeling with the Transformer"**. AdapterDrop**Zhang et al., "AdapterDrop: Improving Efficiency by Removing Unimportant Adapters for a Given Task in Each Layer of the Transformer"** improves efficiency by removing unimportant adapters for a given task in each layer of the Transformer. LoRA**Wu et al., "LoRA: Low-Rank Adaptation for Efficient Neural Style Transfer"** re-parameterizes incremental matrices through simple low-rank decomposition. Structurally similar to LoRA, KronA**Kim et al., "KronA: A Kronecker Product Decomposition-based Approach for Efficient Neural Architecture Search"** replaces the low-rank decomposition in LoRA with Kronecker product decomposition. PISSA**Wang et al., "PISSA: A Pre-Initialized Low-Rank Adaptation Method for Efficient Deep Learning Models"** also adopts the same architecture as LoRA, but it initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. LST**Zhang et al., "LST: A Lightweight and Efficient Approach to Training Large-Scale Neural Networks"** reduces training memory by running a small ladder network alongside the pre-trained network. Prompt tuning**Li et al., "Prompt Tuning: Towards Efficient and Effective Fine-Tuning for Large Models"**, a novel and promising parameter-efficient fine-tuning method, stands out from the rest by achieving good results with training very few parameters. Unlike other methods, it maintains a consistent number of trainable parameters even as the model size expands, making it a potential game-changer in the field.
% % \ \\ \noindent
% % \textbf{Soft Prompt-based Fine-tuning.} Soft prompt tuning inserts trainable continuous vectors, known as soft prompts, into the model's input or hidden states. Unlike hard prompts, soft prompts are generated by searching the discrete token space based on task-specific training data. In 2021, Li and Liang proposed prefix tuning**Li et al., "Prefix Tuning: A Simple and Efficient Method for Fine-Tuning Pre-Trained Models"**, which adds soft prompts at the input to adjust the model's behavior. Unlike prefix tuning, prompt tuning**Hou et al., "Prompt Tuning with a Single-Stage Framework for Fast and Efficient Fine-Tuning"** only adds soft prompts to the embedding layer of the model. Other methods have explored transfer learning-based prompt tuning, such as MPT**Khosla et al., "MPT: A Multi-Task Learning Approach for Efficient Prompt-Based Fine-Tuning"**and ATTEMPT**Zhang et al., "ATTEMPT: An Adaptive Transfer Learning Method for Efficient Prompt-Based Fine-Tuning"**, which better initialize soft prompts through pre-training, but these methods require significant resources for the pre-training process.Xiao**Xiao et al., "Soft Prompt Tuning: A Novel and Promising Approach to Parameter-Efficient Fine-Tuning"** pointed out that soft prompts inherently exhibit low-rank properties, which can be leveraged to further enhance the method's parameter efficiency and performance through certain strategies. DePT**Kim et al., "DePT: Decomposing Soft Prompt into Shorter Prompts and Low-Rank Matrices for Efficient Fine-Tuning"** decomposes the soft prompt into shorter prompts and pairs of low-rank matrices, which are then used to update the model's weights. DPT**Liu et al., "DPT: A Deep Parameter Tuning Approach for Efficient Soft Prompt-Based Fine-Tuning"** employs a re-parameterization strategy, using two low-rank matrices to replace the original soft prompt. However, these PT-based methods need more efficiency and richness of task-specific knowledge when dealing with long soft prompts.