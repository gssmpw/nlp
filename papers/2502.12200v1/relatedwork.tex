\section{Related Work}
% % \textbf{Parameter-efficient Fine-tuning.} Parameter-efficient fine-tuning methods\citep{lester2021power,zaken2021bitfit,shi2024dept,ding2023sparse,zhu2023sira}, a significant advancement over traditional full fine-tuning, offer the ability to achieve commendable results by training only a small subset of parameters, thereby reducing computational costs and improving efficiency. BitFit\citep{zaken2021bitfit} only updates the bias terms while freezing most of the pre-trained model's parameters, but it performs poorly on larger models\citep{lialin2023scaling}. Threshold-Mask\citep{zhao2020masking} constructs a binary mask matrix using a threshold to select pre-trained weights in the attention and feedforward layers through element-wise multiplication. Adapter-based methods, such as Adapters\citep{houlsby2019parameter}, HyperFormer\citep{mahabadi2021parameter}and Compacter\citep{karimi2021compacter}, add trainable adapter modules to each Transformer block in the T5 model\citep{raffel2020exploring}. AdapterDrop\citep{ruckle2020adapterdrop} improves efficiency by removing unimportant adapters for a given task in each layer of the Transformer. LoRA\citep{hu2021lora} re-parameterizes incremental matrices through simple low-rank decomposition. Structurally similar to LoRA, KronA\citep{edalati2022krona} replaces the low-rank decomposition in LoRA with Kronecker product decomposition. PISSA\citep{meng2024pissa} also adopts the same architecture as LoRA, but it initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. LST\citep{sung2022lst} reduces training memory by running a small ladder network alongside the pre-trained network. Prompt tuning\citep{lester2021power}, a novel and promising parameter-efficient fine-tuning method, stands out from the rest by achieving good results with training very few parameters. Unlike other methods, it maintains a consistent number of trainable parameters even as the model size expands, making it a potential game-changer in the field.
% % \ \\ \noindent
% % \textbf{Soft Prompt-based Fine-tuning.} Soft prompt tuning inserts trainable continuous vectors, known as soft prompts, into the model's input or hidden states. Unlike hard prompts, soft prompts are generated by searching the discrete token space based on task-specific training data. In 2021, Li and Liang proposed prefix tuning\citep{li2021prefix}, which adds soft prompts at the input to adjust the model's behavior. Unlike prefix tuning, prompt tuning\citep{lester2021power} only adds soft prompts to the embedding layer of the model. Other methods have explored transfer learning-based prompt tuning, such as MPT\citep{wang2023multitask} and ATTEMPT\citep{asai2022attempt}, which better initialize soft prompts through pre-training, but these methods require significant resources for the pre-training process.Xiao\citep{xiao2023decomposed} pointed out that soft prompts inherently exhibit low-rank properties, which can be leveraged to further enhance the method's parameter efficiency and performance through certain strategies. DePT\citep{shi2024dept} decomposes the soft prompt into shorter prompts and pairs of low-rank matrices, which are then used to update the model's weights. DPT\citep{xiao2023decomposed} employs a re-parameterization strategy, using two low-rank matrices to replace the original soft prompt. However, these PT-based methods need more efficiency and richness of task-specific knowledge when dealing with long soft prompts.