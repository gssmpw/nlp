% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
% \usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{soul}
\usepackage{bigstrut}
\usepackage{url}
% \usepackage[hidelinks]{hyperref}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{amsthm}
% \usepackage{float}
% \usepackage{subfig}
% \usepackage{enumitem}
% \usepackage{booktabs}
% \usepackage{algorithm}
% \usepackage{amssymb}
% \usepackage{bbding}
% \usepackage{multirow}
% \usepackage{algorithmic}
% \usepackage{colortbl}
% \usepackage[table]{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{multirow}
% \usepackage[table]{xcolor}
\usepackage[switch]{lineno}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{subfig}
\usepackage{newfloat}
\usepackage{listings}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\newcommand{\question}[1]{{\color{red}{\bf\sf [Question: #1]}}}
\newcommand{\en}[1]{{\color{magenta}{[Yang: #1]}}}
\newcommand{\lan}[1]{{\color{blue}{#1}}}


% 添加行号
% \usepackage[switch]{lineno}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product}

\author{
 \textbf{Pengxiang Lan\textsuperscript{1}}\thanks{The first two authors contributed equally.},
 \textbf{Haoyu Xu\textsuperscript{1}}\footnotemark[1],
 \textbf{Enneng Yang\textsuperscript{1}},
 \textbf{Yuliang Liang\textsuperscript{1}},
\\
 \textbf{Guibing Guo\textsuperscript{1}}\thanks{Corresponding author.},
 \textbf{Jianzhe Zhao\textsuperscript{1}},
 \textbf{Xingwei Wang\textsuperscript{2}}
% \\
\\
\textsuperscript{1}Software College, Northeastern University, China,
\\
\textsuperscript{2}School of Computer Science and Engineering, Northeastern University, China
\\
\{pengxianglan, haoyuxu, ennengyang, liangyuliang\}@stumail.neu.edu.cn, 
\\
\{guogb, zhaojz\}@swc.neu.edu.cn, wangxw@mail.neu.edu.cn
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 % }
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Prompt tuning (PT) offers a cost-effective alternative to fine-tuning large-scale pre-trained language models (PLMs), requiring only a few parameters in soft prompt tokens added before the input text. However, existing PT approaches face two significant issues: (\textbf{i}) They overlook intrinsic semantic associations between soft prompt tokens, leading to high discreteness and limited interactions, thus reducing the model's comprehension and effectiveness in complex tasks.
% They overlook the intrinsic semantic associations between soft prompt tokens during training. We empirically reveal that prompt tokens exhibit high discreteness and lack interaction between tokens. This lack of associations limits the model's knowledge representation, reducing its effectiveness in understanding and handling complex tasks. 
(\textbf{ii}) Due to the complexity of downstream tasks, long soft prompt is necessitated to improve performance, but prompt length correlates positively with memory usage and computational costs. Achieving high efficiency and performance remains an ongoing challenge. To address these issues,  we propose a novel \textbf{L}ow-p\textbf{A}ra\textbf{M}eters \textbf{P}rompt Tuning (\textbf{LAMP}) method, which leverages prompt decomposition and compressed outer product.
% to improve model performance and efficiency. 
Specifically, the prompt decomposition module employs Truncated SVD to reduce training parameters and significantly lower the dimensionality of the soft prompt parameter space. It then utilizes a compressed outer product module to facilitate multiple interactions among prompt tokens, exploring their intrinsic associations to enhance knowledge representation. Finally, LAMP uses average pooling to reduce memory usage and training/inference time. Extensive experiments across six architectures and eight datasets demonstrate that LAMP outperforms state-of-the-art PT-based and LoRA-based methods in performance and efficiency. 
% Notably, on the T5-11B model, LAMP improved performance by 5.59\% compared to the vanilla PT while also increasing inference speed by 31\%, reducing trainable parameters by 91.21\%, shortening training time by 23.64\%, and lowering memory usage by 24.49\%.
\end{abstract}

\begin{figure}[t]
\centering
  \subfloat{
  \includegraphics[scale=0.188, trim={0mm 0mm 0mm 0mm}]{Fig/a_PT_tsne_multirc.png}}
  \subfloat{
   \includegraphics[scale=0.188, trim={0mm 0mm 0mm 0mm}]{Fig/b_PT_tsne_copa.png}}
  % \vspace{-5pt}
    % \caption{On T5-Base, (a) and (b) are the t-SNE clustering visualizations of the original PT on the MultiRC and COPA datasets, respectively. PT length is 100. PT is randomly initialized from the vocabulary. Our LPT method and vanilla PT visualization are compared in detail in Section \ref{interpretability}. }
    \caption{(a) and (b) show the t-SNE clustering visualizations of the original prompt tuning prompts after training on the MultiRC and COPA datasets using the T5-Base model. Source prompt tokens are initialized from sampled vocabulary and length is set to 100. }
    % \vspace{-10pt}
    \label{fig_tsne}
\end{figure}


\section{Introduction}
Pre-trained language models (PLMs) possess powerful learning capabilities to extract complex features and patterns from vast amounts of data \citep{devlin2019bert,radford2019language}. 
% attracting widespread attention from academia and industry. 
In recent years, as the scale of large PLMs has rapidly expanded, computational costs have surged dramatically \citep{lan2024efficient}. Although full fine-tuning parameters of PLMs yields satisfactory results, it has become impractical, e.g., PaLM has 540B parameters and requires 6144 TPU v4 chips to train for 1200 hours \citep{chowdhery2023palm}.
% FLAN with 137B parameters \citep{wei2021finetuned} and PaLM with 540B parameters \citep{chowdhery2023palm}.

Parameter-Efficient Fine-Tuning (PEFT) methods attempt to bridge this gap by achieving performance comparable to full fine-tuning with minimal computational resources and time costs~\citep{houlsby2019parameter,hu2021lora,lester2021power}. Among these methods, prompt tuning (PT) stands out for its efficiency and flexibility. It freezes the model parameters and exclusively trains the soft prompt tokens attached to the model's input, delivering performance on par with full fine-tuning \citep{lester2021power,xiao2023decomposed,razdaibiedina-etal-2023-residual, lan2024efficient}. Notably, PT's trainable parameters are much lower than other PEFT methods (e.g., Adapter~\citep{houlsby2019parameter} and LoRA \citep{hu2021lora}) and do not grow exponentially as the scale of PLMs. 

\begin{figure}[t]
\centering
  \includegraphics[scale=0.18]{Fig/inference_param.pdf}
  % \vspace{-10pt}
    \caption{(a) Average performance on the T5 models across the SuperGLUE benchmark. (b) Impact of prompt length on performance and trainable parameters in the WiC dataset of the SuperGLUE benchmark.}
    \label{fig_total}
   % \vspace{-10pt}
\end{figure}
 
Although these methods provide undeniable contributions, existing PT-based methods still suffer from two main challenges: \textbf{First}, in various tasks, PT initialization methods exhibit high discreteness, lacking exploration of the intrinsic semantic associations between tokens. The two mainstream prompt initialization methods—random initialization and initialization from sampled vocabulary (i.e., the 5,000 most common tokens)—help the model explore a broader parameter space during training. The sample vocabulary initialization, in particular, is widely utilized in pre-training for transfer learning \citep{vu2022spot,asai2022attempt} and multi-task learning \citep{wang2022multitask,zhong2024panda} due to its more informative nature. Nevertheless, as shown in Figure \ref{fig_tsne}, we found that although this informative initialization provides prompt tokens with rich knowledge, these tokens remain isolated without establishing semantic connections after training. In NLP tasks, intrinsic semantic associations form the context and meaning of language, helping models better understand and represent complex language structures \citep{mikolov2013efficient,devlin2019bert,Vaswani2017}. Prompt tokens leverage semantic knowledge to guide PLMs in producing outputs that better meet task requirements. Clearly, this intrinsic semantic association is essential for PT. This high discreteness overlooks capturing intrinsic semantic associations among soft prompt tokens, limiting the model's knowledge representation capabilities. \textbf{Second}, although PT does not require training the parameters of PLMs, adding soft prompts increases the total length of input embeddings. Figure \ref{fig_total} reveals the relationship between computational cost, trainable parameters, and prompt length. Previous research has shown that a long prompt 100 yields optimal PT performance, enabling PLMs to adapt to complex downstream tasks \citep{lester2021power,razdaibiedina-etal-2023-residual,xiao2023decomposed}. However, such a length renders PT inefficient. This inefficiency stems from the inherent high complexity of PLMs (e.g., the quadratic complexity of Transformers) \citep{Vaswani2017} and the fact that the storage of gradients and optimizer states is closely related to the number of trainable parameters \citep{guo2021parameter}. While some approaches \citep{xiao2023decomposed, shi2024dept,lan2024efficient} attempt to optimize standard PT, they still struggle with inefficiency and suboptimal performance.

To address the aforementioned challenges, we propose a novel efficient and effective low-parameters
prompt tuning (\textbf{LAMP}) method through prompt decomposition and compressed outer product.
Our motivation stems from the soft prompt exhibiting high dispersion and the ``intrinsic rank" \citep{aghajanyan2021intrinsic, hu2021lora} in PEFT, which indicates that model fine-tuning can occur in a low intrinsic dimensionality space. Specifically, LAMP first employs Truncated singular value decomposition (SVD) with its inherent structure—two low-dimensional singular vectors and singular values—to transform the loosely related semantic knowledge in PT tokens into a more structured and interrelated form, while simultaneously reducing trainable parameters. It then aggregates the semantic knowledge from the Truncated SVD and leverages the compressed outer product to enable multi-level interactions of intrinsic semantics, enhancing the model's knowledge representation. Finally, LAMP reduces computational load by applying average pooling, which does not increase training parameters. 
% Additionally, under the same trainable parameters as general PT, LAMP allows the inclusion of more semantic information to tailor prompts more precisely and effectively. 
Figure \ref{fig_total} demonstrates that the longer the prompt length, the more significant the reduction in computational cost and memory usage achieved by LAMP.
% As shown in Figure \ref{fig_total}, the longer the prompt, the more significant the reduction in computational costs and memory usage of LAMP.

% In summary, the main contributions of this paper are as follows:
The main contributions of this paper are:
\begin{itemize}[leftmargin=*]
\item Our empirical study reveals that tokens in soft prompts exhibit high dispersion during training, lacking inherent semantic interactions among tokens to assist the model in comprehending and handling complex tasks, thereby limiting the model's knowledge representation capability.
\item We propose a novel low-parameter prompt tuning (abbreviated as LAMP) method that captures potential semantic interactions between prompt tokens through prompt decomposition and compressed outer product. LAMP achieves robust performance while significantly reducing computational costs (e.g., training time, memory usage, and trainable parameters).
\item We comprehensively evaluated LAMP on the SuperGLUE benchmark. Experimental results demonstrate that LAMP outperforms other state-of-the-art PT methods and remains effective in few-shot scenarios. Notably, on the T5-11B model, LAMP improved performance by 5.59\% compared to the vanilla PT while also increasing inference speed by 31\%, reducing trainable parameters by 91.21\%, shortening training time by 23.64\%, and lowering memory usage by 24.49\%.
\end{itemize}

\section{Method}
% In this section, we first introduce the preliminaries of prompt tuning, then elaborate on our proposed LPT, as depicted in Figure \ref{fig_LPT}. 
% % It consists of two main modules: (1) Prompt Decomposition and (2) Reconstructed Soft Prompt.

\subsection{Preliminaries}
\label{PT}
% \textbf{Full Fine-tuning.} Fine-tuning all parameters of the PLMs is the primary method to better adapt to downstream tasks ~\cite{raffel2020exploring}. 
% Let labelled training data $(\boldsymbol{X}, \boldsymbol{Y})=\left\{\boldsymbol{x}_i, \boldsymbol{y}_i\right\}_{i=1}^N$ for one target task $\mathcal{T}$, the number of training data is $N$. The total parameters of PLM is $\Theta$ and each input text is $x_{i}$. The embedding of $x_{i}$ is represented as $\mathbf{E}_{i}\in \mathbb{R}^{{m \times d}}$, where $m$ is maximum sequence length and $d$ is the dimension of input embedding. The loss function of full fine-tuning parameters is formulated as follows:
% \begin{equation}
%     \mathcal{L}_{FT} = -\sum_{i} \log P \left(\mathbf{y}_{i}|\mathbf{E}_{i};\Theta \right)
% \end{equation}
% % \en{}
% while full fine-tuning achieves significant outcomes, its vast trainable parameters incur prohibitive training costs.

% Let labelled training data $(\boldsymbol{X}, \boldsymbol{Y})=\left\{\boldsymbol{x}_i, \boldsymbol{y}_i\right\}_{i=1}^N$ for one target task $\mathcal{T}$, the number of training data $N$. the total parameters $\Theta$ of PLM and each input text $x_{i}$. The embedding of $x_{i}$ is represented as $\mathbf{E}_{i}\in \mathbb{R}^{{m \times d}}$, where $m$ is maximum sequence length and $d$ is the dimension of input embedding.
\ \\ \noindent
\textbf{Prompt Tuning.} PT can maintain parameter efficiency as model size scales. This approach ensures that trainable parameters does not increase dramatically with model expansion, making it a preferred choice for many applications \citep{shi2024dept, lan2024efficient}. Let labelled training data $(\boldsymbol{X}, \boldsymbol{Y})=\left\{\boldsymbol{x}_i, \boldsymbol{y}_i\right\}_{i=1}^N$ for one target task $\mathcal{T}$, the number of training data is $N$. The total parameters of PLM is $\Theta$ and each input text is $\boldsymbol{x}_i$. The embedding of $\boldsymbol{x}_i$  is represented as $\mathbf{E}_{i}\in \mathbb{R}^{{m \times d}}$, where $m$ is maximum sequence length and $d$ is the dimension of input embedding. The target prompt $\mathbf{P} \in \mathbb{R}^{{l \times d}}$ is initialized, with $l$ as the hyper-parameter determining the length of the soft prompt. This prompt is then concatenated with the fixed embedding $\mathbf{E}_{i}\in \mathbb{R}^{{m \times d}}$. $\mathbf{E}_{i}$ remains unchanged during gradient updates in training, resulting in a new input embedding $\left [ \mathbf{P};\mathbf{E}_{i} \right]\in \mathbb{R}^{(l+m)\times d}$. The formulation for the target task is as follows:
\begin{equation}
    \mathcal{L}_{p} = -\sum_{i}\log P \left(\mathbf{y}_{i}|\left[\mathbf{P};\mathbf{E}_{i} \right];\Theta \right)
\label{eq2}
\end{equation}
where $\mathcal{L}_{p}$ is a loss function only optimized with the prompt $\mathbf{P}$. $P(\cdot)$ is maximizing the conditional probability of PT. The overall structure of PT is shown in Figure \ref{fig_LAMP}(a).

\begin{figure*}[h]
\centering
  \includegraphics[scale=0.85]{Fig/fig_2.png}
  % \vspace{2pt}
    \caption{(a) Conventional prompt tuning \citep{lester2021power}. (b) The overview of our proposed LAMP. It decomposes the vanilla prompt to construct a new low-dimensional prompt, captures the intrinsic semantic associations between prompt tokens, and finally reduces computational costs through average pooling.}
    \label{fig_LAMP}
  % \vspace{-10pt}
\end{figure*}


% \subsection{Prompt Decomposition}
\subsection{LAMP: Low-parameters Prompt Tuning}
% \subsubsection{Prompt initialization}
\textbf{Prompt initialization.} We initialize the source soft prompt $\mathbf{P}$ from sampled vocabulary \citep{lester2021power,vu2022spot,razdaibiedina-etal-2023-residual, asai2022attempt}, a strategy that embeds more semantic richness and prior knowledge than mere random initialization. Inadequate initialization can result in the discovery of suboptimal local minima, thus impairing the model's generalization capabilities.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Prompt decomposition}
\\ \ \noindent
\textbf{Prompt decomposition.} Our innovative motivation stems from two aspects: 
(1) Soft prompt tokens exhibit high dispersion (as shown in Figure~\ref{fig_tsne}), neglecting the knowledge interactions between tokens;
(2) Soft prompt also exhibit low "intrinsic rank" behavior \citep{xiao2023decomposed}.
% (2) in a particular task, prompt tokens' contribution to performance is not uniform; tokens may negatively influence outcomes \citep{ma2022xprompt}. 
Inspired by these findings and the core idea of Truncated SVD \citep{hansen1987Truncated}, we employ Truncated SVD to reduce training parameters by decomposing the soft prompt $\mathbf{P}\in \mathbb{R}^{l\times d}$. The original SVD of $\mathbf{P}$ is formulated as follows:
\begin{equation}
   \mathbf{P}=\mathbf{U}\text{diag}(\mathbf{Q})\mathbf{V}^\top
\end{equation}
where $\mathbf{U} \in \mathbb{R}^{l\times min(l,d)}$, $\mathbf{V}\in \mathbb{R}^{d\times min(l,d)}$ are the singular vectors with orthogonal columns, $\mathbf{U}$ and $\mathbf{V}$ transforms highly dispersed tokens from the original PT into interrelated representations. $\mathbf{\mathbf{Q}} \in \mathbb{R}^{min(l,d)}$ comprises the singular values arranged in descending order (the larger the singular value, the more information it contains). The operation $\text{diag}(\mathbf{Q})$ converts $\mathbf{Q}$ into a diagonal matrix, and $V^\top$ represents the transpose of $\mathbf{V}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Prompt reconstruction}
\\ \ \noindent
\textbf{Prompt reconstruction.} We define the soft prompt's low "intrinsic rank" as $r$ and select the top-$r$ singular values, $\mathbf{Q}_{[:r]}$, which contain a rich amount of information arranged in descending order. The remaining singular values, $\mathbf{Q}_{[r:]}$, are discarded and do not participate in training/inference. Consequently, as the $\mathbf{Q}$ dimension changes, singular values and vectors are redefined as $\left \{ \mathbf{U}_{[:r]}\in \mathbb{R}^{l\times r},\mathbf{Q}_{[:r]}\in \mathbb{R}^{r},\mathbf{V}_{[:r]}\in \mathbb{R}^{d\times r}\right \}$. We can approximate the original information by storing only $r$ (where $r \ll d$) singular values and their corresponding low-parameters singular vectors, achieving parameter compression, which is also why LAMP adopts Truncated SVD.

The trainable parameters are now reduced from `$l \times d$' to `$l \times r + r + r \times d$'. In experiments, we provide a detailed explanation of hyperparameter $r$ and its impact on model performance. 
% Since prompt length $l$ in LAMP is influenced by $r$ (where $r \ll d$), LAMP downplays the impact of $l$ on the total trainable parameters, allowing for the injection of more task-specific knowledge as soft prompt initialization.
For instance, on the Llama2-7B \citep{touvron2023llama}, when the prompt length increases from 100 to 500, traditional PT requires training 2,048k parameters, whereas our method, LAMP, only requires training ($500 \times 8+8+8 \times 4096=36.8$k) parameters. LAMP's advantage becomes more pronounced with longer prompt lengths or larger model scales, as it significantly reduces the computational cost by decreasing trainable parameters. LAMP has established a solid foundation for PT-based methods to excel across various domains.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{Compressed outer product}
\\ \ \noindent
\textbf{Compressed outer product.} Although using Truncated SVD to reduce the trainable parameters in prompt tuning is promising, directly applying dot products on the decomposed singular values and vectors can only partially capture the intrinsic associations between tokens. Specifically, dot products' inherent linear nature limits their ability to fully express the more complex, nonlinear interactions among prompt tokens. Considering that outer products can mine richer and more complex high-order interactions, we utilize the compressed outer product to further explore the intrinsic semantic associations between tokens in prompt tuning. 

Firstly, we utilize the dot product of singular values and singular vectors as the initialization input for the compressed outer product module:
\begin{equation}
   \mathbf{M}=\mathbf{U}_{[: r]} \operatorname{diag}\left(\mathbf{Q}_{[: r]}\right) \in \mathbb{R}^{l \times r},
\end{equation}
\begin{equation}
   \mathbf{I}=\operatorname{diag}\left(\mathbf{Q}_{[: r]}\right) \mathbf{V}_{[: r]}^\top \in \mathbb{R}^{r \times d} .
\end{equation}
This approach enables an initial aggregation of knowledge features between tokens, which helps to effectively represent and explore the underlying semantic knowledge associations. Due to compressed outer product can maintains the high-order structure while facilitating multiple layers of intrinsic semantic interactions. This approach effectively restores and enhances the complex information structures that might be lost in Truncated SVD, thereby enriching the knowledge representation capabilities of prompt tokens. The compressed outer product is formulated as follows:
\begin{equation}
\mathbf{C} = \sum_{i=1}^r \mathbf{M}_{[:, i]} \otimes \mathbf{I}_{[i, :]} \in \mathbb{R}^{l \times d}
\end{equation}
where $\mathbf{M}_{[:, i]}$ is the $i$-th column vector of $\mathbf{M}$, $\mathbf{I}_{[i, :]}$ is the $i$-th row vector of $\mathbf{I}$, $\otimes$ denotes the outer product of two vectors. $\mathbf{C} \in \mathbb{R}^{l \times d}$ is the resultant prompt after summing all the outer products. The introduction of compressed outer products enhances the representational power of prompt tuning. This approach enables the soft prompt to more effectively adapt to different downstream tasks through deep interactions between prompt tokens.

While introducing compressed outer product does not create new trainable parameters, it does entail a slight increase in computational overhead due to its engagement in more complex higher-order interactions. Additionally, given the transformers' quadratic complexity, the prompt's length is proportional to the training duration. We consider employing average pooling operation to reduce training time: 
\begin{equation}
    \mathbf{P'}_{i,j} = \frac{1}{p} \sum_{k=0}^{p-1} \mathbf{C}_{i\ast p+k, j}
% \mathbf{P}_{n}=\frac{1}{p} \sum_{t=1}^{l} \mathbf{C}_{t} \in \mathbb{R}^{\frac{l}{p} \times d}
\end{equation}
$\mathbf{P'}_{i,j}$ represents the elements of the tensor $\mathbf{P'}\in \mathbb{R}^{l/p \times d}$ after averaging pooling. This operation effectively compresses the $l$ elements along the first dimension into $l / p$. 
% Each element of the resulting tensor $\mathbf{P}_{n}$ represents the mean of $l / p$ elements from $\mathbf{C}$ across the feature dimension. 
It is also noteworthy that we explored a self-attention pooling strategy to dynamically filter prompt tokens in Appendix \ref{app_Self-Attention Pooling}; however, this strategy was not very effective and introduced additional trainable parameters.


\subsection{Training and Inference}
Only the parameters of $\small \mathbf{U}_{[:r]}\in \mathbb{R}^{l\times r}$, $\small \mathbf{Q}_{[:r]}\in \mathbb{R}^{r}$, and $\small \mathbf{V}_{[:r]}\in \mathbb{R}^{d\times r}$ are optimized during the training process, while the backbone model (i.e., $\Theta$ and $\mathbf{E}_{i}$) remained frozen as Figure~\ref{fig_LAMP}(b). The reconstructed prompt $\mathbf{P'}$ is inserted before the input text embeddings. By $\mathbf{P'}$, Eq.\ref{eq2} is displaced by:
\begin{equation}
\mathcal{L}_{PT} = -\sum_{i}\; \log P(\mathbf{y}_{i}|[\mathbf{P'};\mathbf{E}_{i}];\Theta)
\label{eq5}
\end{equation}
where $[\mathbf{P'};\mathbf{E}_{i}] \in \mathbb{R}^{(l/p+m)\times d}$ is a input embedding of PLMs through the connection of $\mathbf{P'}$ and $\mathbf{E}_{i}$. 

% \begin{table*}[t]
%   \centering
% \renewcommand{\arraystretch}{0.1}
% \setlength{\tabcolsep}{0.04cm}
% \begin{tabular}{l|l|cccccccc|>{\columncolor{gray!25}}c}
%     \toprule
%     \multirow{2}[2]{*}{\textbf{Method}} & \multirow{2}[2]{*}{\textbf{Params.}} & \textbf{CB} & \textbf{WSC} & \textbf{COPA} & \textbf{RTE} & \textbf{WiC} & \textbf{BoolQ} & \textbf{MultiRC} & \textbf{ReCoRD} & \multicolumn{1}{c}{{\textbf{Average}}} \\
%           &  & Acc.   & Acc.   & Acc.   & Acc.  & Acc.  & Acc.  & F1 & F1 & (\%)  \\
%     \midrule
%     \multicolumn{11}{c}{\textbf{T5-Small}} \\
%     Fine-Tuning$^{\dagger}$ & 60M   & 89.28  & 67.94  & 59.00  & 72.56  & 68.18  & 77.06  & 66.98  & 55.64  & 69.58  \\
%     \midrule
%     Prompt Tuning & 51K   & 71.43  & 59.62  & 58.33  & 66.91  & 63.95  & 66.12  & 63.31  & 50.11  & 62.47  \\
%     Residual PT$^{\dagger}$ & 462K  & 72.02  & 63.14  & 56.66  & 67.02  & 60.96  & \textbf{73.35 } & \underline{65.12}  & 53.08  & 63.91  \\
%     DePT  & 51K   & 75.00  & \textbf{67.31}  & 52.66  & 65.22  & 62.70  & 66.00  & 61.94  & \textbf{56.71 } & 63.44  \\
%     EPT   & 51K   & 78.57  & \textbf{67.31}  & 55.66  & \textbf{71.01}  & \textbf{67.39} & 69.17  & 64.46  & \underline{54.14}  & \underline{65.96}  \\
%     DPT   & 6K    & \underline{78.85}  & 60.53  & \underline{59.33}  & \underline{70.40}  & 64.26  & 72.17  & 64.61  & 53.60  & 65.47 \\
%     \midrule
%     % w/o pooling & 6K    & \textbf{85.71 } & \textbf{68.42 } & \underline{60.00 } & \textbf{71.12 } & 64.73  & \underline{73.30}  & \textbf{66.69 } & 53.30  & \textbf{67.91 } \\
%     \textbf{LAMP(ours)} & 5K   & \textbf{83.93}$_{1.54}$ & \textbf{67.31}$_{2.26}$  & \textbf{60.66}$_{0.47}$  & 69.31$_{0.78}$ & \underline{66.46}$_{0.71}$ & \underline{72.97}$_{0.23}$ & \textbf{66.25}$_{1.12}$ & 53.80$_{0.25}$ & \textbf{67.59} \\
%     \midrule
%     \multicolumn{11}{c}{\textbf{T5-Base}}\\
%     Fine-Tuning$^{\ddagger}$ & 220M  & 91.70  & 81.70  & 60.00  & 84.50  & 69.30  & 82.30  & 76.90  & 80.90  & 78.41  \\
%      \midrule
%     Prompt Tuning & 77K   & 78.57  & 61.54  & 55.00  & 67.63  & 62.38  & 77.00  & 72.37  & 71.32  & 68.27  \\
%     Residual PT$^{\dagger}$ & 693K  & 77.37  & \underline{67.94}  & \underline{56.66}  & \underline{81.70}  & 66.87  & 80.00  & 72.11  & 72.21  & 71.86  \\
%     DePT  & 77K   & 82.14  & 67.31  & 54.33  & 73.91  & 65.20  & 79.02  & \textbf{72.70} & 70.80  & 70.68  \\
%     EPT   & 77K   & \underline{85.71}  & 67.31  & \underline{56.00}  & 78.99  & 67.71  & 79.14  & \underline{72.62}  & 71.15  & \underline{72.33}  \\
%     DPT   & 9K    & 78.56  & 67.30  & \underline{56.66}  & 79.42  & \underline{68.49}  & \textbf{80.28}  & 72.50  & \textbf{72.56}  & 71.97  \\
%     \midrule
%     % w/o pooling & 9K    & \underline{92.86 } & \textbf{68.42 } & \underline{57.00 } & \textbf{82.31 } & \underline{69.28 } & \textbf{80.55 } & 72.25  & \textbf{72.66 } & \textbf{74.42 } \\
%     \textbf{LAMP(ours)} & 7K   & \textbf{94.64}$_{0.84}$ & \textbf{68.42}$_{2.15}$  & \textbf{58.66}$_{1.25}$  & \textbf{83.39}$_{0.51}$ & \textbf{70.06}$_{0.34}$ & \underline{80.24}$_{0.13}$ & 72.72$_{0.32}$ & \underline{72.55}$_{0.05}$ & \textbf{75.09} \\
%     \midrule
%     \multicolumn{11}{c}{\textbf{T5-Large}}\\
%     Fine-Tuning$^{\ddagger}$ & 770M  & 94.30  & 88.50  & 87.00  & 90.60  & 73.50  & 88.30  & 85.40  & 89.20  & 87.10  \\
%      \midrule
%     Prompt Tuning & 102K  & 82.35  & 65.38  & \underline{57.33}  & 88.45  & 70.69  & 84.28  & 76.37  & 74.36  & 74.90  \\
%     Residual PT$^{\dagger}$ & 925K  & 73.21  & \underline{70.50}  & \textbf{62.66} & \underline{88.92}  & \underline{72.25}  & \underline{85.04}  & 76.46  & \underline{84.36}  & 76.67  \\
%     DePT  & 102K  & 85.71  & 67.31  & 50.66  & 83.33  & 68.97  & 83.24  & 75.76  & 74.03  & 73.63  \\
%     EPT   & 102K  & \underline{89.29}  & 68.30  & 54.00  & 86.33  & 71.79  & 84.77  & 76.62  & 73.94  & 75.63  \\
%     DPT   & 11K   & \underline{89.29}  & 65.79  & \textbf{62.66} & 88.45  & 71.63  & 84.53  & \underline{76.72}  & 84.35  & \underline{77.93}  \\
%     \midrule
%     % w/o pooling & 11K   & \underline{94.64 } & \underline{73.68 } & \underline{59.00}  & \underline{89.89 } & \textbf{73.98 } & \textbf{85.20 } & \underline{76.81 } & \underline{84.41 } & \underline{79.70 } \\
%     \textbf{LAMP(ours)} & 9K   & \textbf{98.21}$_{1.37}$ & \textbf{78.95}$_{2.48}$ & \underline{57.33}$_{0.47}$  & \textbf{90.61}$_{0.31}$ & \textbf{73.35}$_{0.29}$ & \textbf{85.11}$_{0.07}$ & \textbf{76.94}$_{0.08}$ & \textbf{84.56}$_{0.07}$ & \textbf{80.63} \\
%     \bottomrule
%     \end{tabular}%
% \caption{For the performance comparison on the SuperGLUE benchmark, all experimental results are based on the T5-Small, T5-Base, and T5-Large. All scores represent the mean across three runs using distinct random seeds and subscripts indicate standard deviation. 
% % The evaluation metrics are F1 for MultiRc, ReCoRD, and accuracy for other datasets. 
% $^{\dagger}$ sourced from \citet{xiao2023decomposed}. $^{\ddagger}$ sourced from \citet{aribandi2021ext5}. The best result is marked in bold. The second-highest result is indicated by an underline. }
% % The figures \en{figures?} below the datasets represent the count of training samples in each dataset. \en{Does the number of samples need to be shown here? Is that enough in the appendix?}}
%   \label{tab1}%
%   \vspace{-8pt}
% \end{table*}%



% % Table generated by Excel2LaTeX from sheet 'Sheet4'
% \begin{table*}[t]
%   \centering
% \renewcommand{\arraystretch}{0.6}
% \setlength{\tabcolsep}{0.111cm}
% \begin{tabular}{l|l|cccccccc|>{\columncolor{gray!25}}c}
%     \toprule
%     \multirow{2}[2]{*}{\textbf{Method}} & \multirow{2}[2]{*}{\textbf{Params.}} & \textbf{CB} & \textbf{WSC} & \textbf{COPA} & \textbf{RTE} & \textbf{WiC} & \textbf{BoolQ} & \textbf{MultiRC} & \textbf{ReCoRD} & \multicolumn{1}{c}{{\textbf{Mean}}} \\
%           &  & Acc.   & Acc.   & Acc.   & Acc.  & Acc.  & Acc.  & F1 & F1 & (\%)  \\
%     \midrule
%     \multicolumn{11}{c}{\textbf{T5-Small}} \\
%     Fine-Tuning$^{\dagger}$ & 60M   & 89.28  & 67.94  & 59.00  & 72.56  & 68.18  & 77.06  & 66.98  & 55.64  & 69.58  \\
%     \midrule
%     Prompt Tuning & 51K   & 71.43  & 59.62  & 58.33  & 66.91  & 63.95  & 66.12  & 63.31  & 50.11  & 62.47  \\
%     Residual PT$^{\dagger}$ & 462K  & 72.02  & 63.14  & 56.66  & 67.02  & 60.96  & \textbf{73.35 } & \underline{65.12}  & 53.08  & 63.91  \\
%     DePT  & 51K   & 75.00  & \underline{67.31}  & 52.00  & 65.22  & 62.70  & 66.00  & 61.94  & \textbf{56.71 } & 63.36  \\
%     EPT   & 51K   & 78.57  & \underline{67.31}  & 55.00  & \underline{71.01}  & \textbf{67.39 } & 69.17  & 64.46  & \underline{54.14}  & \underline{65.88}  \\
%     DPT   & 6K    & \underline{78.85}  & 60.53  & \underline{59.00}  & 70.40  & 64.26  & 72.17  & 64.61  & 53.60  & 65.43  \\
%     \midrule
%     \textbf{LPT(Ours)} & 6K    & \textbf{85.71 } & \textbf{68.42 } & \textbf{60.00 } & \textbf{71.12 } & \underline{64.73}  & \underline{73.30}  & \textbf{66.69 } & 53.30  & \textbf{67.91 } \\
%     \midrule
%     \multicolumn{11}{c}{\textbf{T5-Base}}\\
%     Fine-Tuning$^{\ddagger}$ & 220M  & 91.70  & 81.70  & 60.00  & 84.50  & 69.30  & 82.30  & 76.90  & 80.90  & 78.41  \\
%      \midrule
%     Prompt Tuning & 77K   & 78.57  & 61.54  & 55.00  & 67.63  & 62.38  & 77.00  & 72.37  & 71.32  & 68.27  \\
%     Residual PT$^{\dagger}$ & 693K  & 77.37  & \underline{67.94}  & \underline{56.66}  & \underline{81.70}  & 66.87  & 80.00  & 72.11  & 72.21  & 71.86  \\
%     DePT  & 77K   & 82.14  & 67.31  & 54.00  & 73.91  & 65.20  & 79.02  & \textbf{72.70 } & 70.80  & 70.64  \\
%     EPT   & 77K   & \underline{85.71}  & 67.31  & 56.00  & 78.99  & 67.71  & 79.14  & \underline{72.62}  & 71.15  & \underline{72.33}  \\
%     DPT   & 9K    & 78.56  & 67.30  & 56.00  & 79.42  & \underline{68.49}  & \underline{80.28}  & 72.50  & \underline{72.56}  & 71.89  \\
%     \midrule
%     \textbf{LPT(Ours)} & 9K    & \textbf{92.86 } & \textbf{68.42 } & \textbf{57.00 } & \textbf{82.31 } & \textbf{69.28 } & \textbf{80.55 } & 72.25  & \textbf{72.66 } & \textbf{74.42 } \\
%     \midrule
%     \multicolumn{11}{c}{\textbf{T5-Large}}\\
%     Fine-Tuning$^{\ddagger}$ & 770M  & 94.30  & 88.50  & 87.00  & 90.60  & 73.50  & 88.30  & 85.40  & 89.20  & 87.10  \\
%      \midrule
%     Prompt Tuning & 102K  & 82.35  & 65.38  & 57.00  & 88.45  & 70.69  & 84.28  & 76.37  & 74.36  & 74.86  \\
%     Residual PT$^{\dagger}$ & 925K  & 73.21  & \underline{70.50}  & \textbf{62.66 } & \underline{88.92}  & \underline{72.25}  & \underline{85.04}  & 76.46  & \underline{84.36}  & 76.67  \\
%     DePT  & 102K  & 85.71  & 67.31  & 50.00  & 83.33  & 68.97  & 83.24  & 75.76  & 74.03  & 73.54  \\
%     EPT   & 102K  & \underline{89.29}  & 68.30  & 54.00  & 86.33  & 71.79  & 84.77  & 76.62  & 73.94  & 75.63  \\
%     DPT   & 11K   & \underline{89.29}  & 65.79  & \textbf{62.66 } & 88.45  & 71.63  & 84.53  & 76.72  & 84.35  & \underline{77.93}  \\
%     \midrule
%     \textbf{LPT(Ours)} & 11K   & \textbf{94.64 } & \textbf{73.68 } & \underline{59.00}  & \textbf{89.89 } & \textbf{73.98 } & \textbf{85.20 } & \textbf{76.81 } & \textbf{84.41 } & \textbf{79.70 } \\
%     \bottomrule
%     \end{tabular}%
% \caption{For the performance comparison on the SuperGLUE benchmark, all experimental results are based on the T5-Small, T5-Base, and T5-Large. All scores represent the mean across 3 runs using distinct random seeds. 
% % The evaluation metrics are F1 for MultiRc, ReCoRD, and accuracy for other datasets. 
% $^{\dagger}$ sourced from \citet{xiao2023decomposed}. $^{\ddagger}$ sourced from \citet{aribandi2021ext5}. The best result is marked in bold. The second-highest result is indicated by an underline. }
% % The figures \en{figures?} below the datasets represent the count of training samples in each dataset. \en{Does the number of samples need to be shown here? Is that enough in the appendix?}}
%   \label{tab1}%
%   \vspace{-10pt}
% \end{table*}%

\section{Experiments}
In this section, we will answer these key research questions by conduct extensive experiments: 
% \ \\ \noindent
\textbf{RQ1:} How does our LAMP performance compare with other SOTA baselines across different model scales and datasets?
% \ \\ \noindent
\textbf{RQ2:} How do few-shot adaptability and hyper-parameters optimization influence the LAMP?
% \ \\ \noindent
% \textbf{RQ3:} How to understand the advantages of LAMP through some hyper-parameters and model scales? 
% \ \\ \noindent
\textbf{RQ3:} How will the feature space of the soft prompt change after considering the intrinsic semantic associations between tokens?

\subsection{Evaluation Datasets and Metrics}
\textbf{Evaluation Datasets:} Building upon prior studies in prompt tuning \citep{xiao2023decomposed}, we employ eight NLP tasks from the SuperGLUE \citep{wang2019superglue} and GLUE \citep{wang2018glue} benchmark and conduct multi-aspect experiments to evaluate the high efficiency and effectiveness of LAMP. The SuperGLUE benchmark includes more complex and challenging tasks among eight datasets than GLUE \citep{wang2018glue}: CB ~\citep{de2019commitmentbank}, WSC ~\citep{levesque2012winograd}, COPA \citep{roemmele2011choice}, RTE \citep{giampiccolo2007third}, WiC ~\citep{pilehvar2019wic}, BoolQ ~\citep{clark2019boolq}, MultiRC ~\citep{khashabi2018looking} and ReCoRD \citep{zhang2018record}. MNLI \citep{williams2018broad}, QNLI \citep{rajpurkar2016squad}, SST-2 \citep{socher2013recursive} and  MRPC\citep{bill2005automatical} in GLUE benchmark. More details about datasets in Appendix \ref{app_Dataset Details}.
\ \\ \noindent
\textbf{Metrics:} Consistent with previous work \citep{razdaibiedina-etal-2023-residual,xiao2023decomposed}, the evaluation metric is F1 for MultiRC and ReCoRD, the evaluation metric is Accuracy for other tasks. 

\subsection{Baselines and Models}
We compare LAMP with the following baseline approaches: \textbf{Full Fine-tuning}, which updates all parameters of PLMs; \textbf{PT-based methods}, where PT \citep{lester2021power} inserts trainable continuous vectors, known as soft prompt, before the model's input, and its most advanced variants include Residual PT \citep{razdaibiedina-etal-2023-residual}, DePT \citep{shi2024dept}, EPT \citep{lan2024efficient} and DPT \citep{xiao2023decomposed}; \textbf{LoRA-based methods} include PiSSA \citep{meng2024pissa}, rsLoRA \citep{kalajdzievski2023rsLoRA}, LoRA+ \citep{hayoulora+}, DoRA \citep{liudora} and LoRA-GA \citep{wang2024loraGA} . More details about Baselines can be found in Appendix \ref{app_Baselines Details}.

We aim to explore a high-performance PEFT method that minimizes trainable parameters. \textbf{Trainable parameters are a crucial factor in our selection of baselines}; hence, methods with more significant trainable parameters and modifying the transformer layers are not included as baselines for comparison. Such as Adapter \citep{houlsby2019parameter} (prompt length is 100, 76.8k vs. 1.9M for T5-base) and its variant methods. Notably, EPT \citep{lan2024efficient} has demonstrated superior performance compared to these PEFT methods. Furthermore, Xprompt \citep{ma2022xprompt} underwent rewinding training, and transfer learning \citep{vu2022spot,asai2022attempt} and multi-task learning \citep{wang2022multitask} require pre-training. These methods are not directly comparable to LAMP.
% \ \\ \noindent
% \textbf{Models:} 
\subsubsection{Models Size}
PT tends to underperform in smaller-scale models. Thus, we conducted primary experiments employing three T5 model variants \citep{raffel2020exploring} (Small, 60M; Base, 220M; and Large, 770M) and validated the effectiveness of LAMP using T5-11B and Llama2-7B \citep{touvron2023llama}. 

\subsection{Training Details}
% LAMP is implemented by the Python library of PyTorch 2.0.0\footnote{\footnotesize \url{https://pytorch.org/}}, Huggingface Transformers \footnote{\footnotesize \url{https://github.com/huggingface/transformers}}. 
T5 model as the backbone for our experiments; the hidden dimensions for the T5-base, T5-small, and T5 large are 512, 768, and 1,024, respectively. Following the experimental setup from \citet{xiao2023decomposed}, we set the soft prompt length to 100, rank $r=8$ in Truncated SVD and batch size is 16. 
% Meanwhile, in Section \ref{subsec:few-shot}, we have demonstrated that LAMP can achieve satisfactory results even when $r$ is less than 10.
The models are trained 100 epochs using the AdamW \citep{loshchilov2018decoupled} optimizer with an initial learning rate of 0.3. We employ the double quantization operation in QLoRA\citep{dettmers2023qlora} for T5-11B and Llama2-7B.
% Further analysis of the impact of intrinsic rank $r$ on the training performance and parameters is detailed in Section \ref{subsec:few-shot} and Section \ref{subsec:r3}. 
Other training details in Appendix \ref{app_Implementation_Details}.



% \begin{table}[t]
% \renewcommand{\arraystretch}{0.1}
% \setlength{\tabcolsep}{0.02cm}
%   \centering
%     \begin{tabular}{c|c|cccccccc|>{\columncolor{gray!25}}c}
%     \toprule
%     \multicolumn{1}{c}{\multirow{2}[3]{*}{\textbf{K-Shot}}} & \multirow{2}[3]{*}{\textbf{Method}} & \multicolumn{9}{c}{\textbf{SuperGLUE}} \\
% \cmidrule{3-11}    \multicolumn{1}{c}{} &       & \textbf{CB} & \textbf{WSC} & \textbf{COPA} & \textbf{RTE} & \textbf{WiC} & \textbf{BoolQ} & \textbf{MultiRC} & \textbf{ReCoRD} & \textbf{Average (\%)} \\
%  \midrule
%     \multirow{7}[1]{*}{8} & Prompt Tuning    & 58.57  & 32.69  & 40.00  & 49.64  & 53.61  & 53.94  & 50.17  & 46.55  & 48.15  \\
%           & Res PT & 60.55    & 28.95    & 55.00    & 47.29    & 44.51    & 61.35    & 57.78    & 68.13    & 52.95 \\
%           & DePT  & 61.43  & 36.69  & 56.00  & 48.92  & 51.72  & 47.83  & 47.45  & 47.94  & 49.75  \\
%           & EPT   & 57.14  & 42.31  & 45.00  & 51.80  & 50.47  & 53.39  & 56.91  & 46.04  & 50.38  \\
%           & DPT   & 62.50  & 39.47  & 55.00  & 56.68  & 50.00  & 62.17  & 60.12  & 63.49  & 56.18  \\
%           \cmidrule{2-11}
%           % & \textbf{w/o pooling} & 66.10  & 42.11  & 55.00  & 53.07  & 50.00  & 61.62  & 59.97  & 69.79  & \textbf{57.21 } \\
%           & \textbf{LAMP (ours)} & 62.86$_{1.53}$  & 44.74$_{1.24}$  & 59.00$_{0.94}$  & 53.43$_{0.2}$  & 50.00$_{0.00}$  & 62.17$_{0.26}$  & 59.98$_{0.00}$  & 62.35$_{1.15}$ & \textbf{56.81}\\
%     \midrule
%     \multirow{7}[2]{*}{16} & Prompt Tuning    & 57.14  & 32.69  & 44.00  & 51.08  & 53.29  & 58.10  & 56.02  & 46.34  & 49.83  \\
%           & Res PT & 68.88    & 50.00    & 55.00    & 47.65    & 53.13    & 62.42    & 55.24    & 68.22    & 57.57 \\
%           & DePT  & 50.00  & 37.69  & 48.00  & 55.40  & 49.53  & 57.98  & 52.43  & 50.77  & 50.23  \\
%           & EPT   & 42.86  & 42.31  & 48.00  & 51.80  & 56.11  & 57.43  & 57.73  & 49.47  & 50.71  \\
%           & DPT   & 44.64  & 55.26  & 55.00  & 53.07  & 51.41  & 62.17  & 59.99  & 62.57  & 55.51  \\
%           \cmidrule{2-11}
%           % & \textbf{w/o pooling} & 67.86  & 42.11  & 55.00  & 52.71  & 55.49  & 62.17  & 59.95  & 69.79  & 58.14\\
%           & \textbf{LAMP (ours)} & 62.21$_{2.66}$  & 47.37$_{2.48}$  & 59.00$_{1.89}$  & 54.51$_{0.85}$  & 54.23$_{0.59}$ & 62.29$_{0.06}$  & 59.88$_{0.01}$  & 70.72$_{0.44}$  & \textbf{58.78 } \\
%     \midrule
%     \multirow{7}[2]{*}{32} & Prompt Tuning    & 57.14  & 32.69  & 46.00  & 54.68  & 55.49  & 59.94  & 51.39  & 49.15  & 50.81  \\
%           & Res PT & 69.21    & 47.37   & 58.00    & 51.99    & 53.34    & 63.09    & 56.15    & 68.15    & 58.41 \\
%           & DePT  & 53.57  & 32.69  & 52.00  & 50.36  & 52.98  & 58.72  & 49.89  & 48.91  & 49.89  \\
%           & EPT   & 67.85  & 40.38  & 56.00  & 51.80  & 55.80  & 60.12  & 49.74  & 46.05  & 53.47  \\
%           & DPT   & 62.50  & 52.63  & 55.00  & 53.79  & 52.35  & 62.17  & 59.95  & 63.34  & 57.72  \\
%           \cmidrule{2-11}
%           % & \textbf{w/o pooling} & 67.86  & 52.63  & 55.00  & 54.15  & 52.98  & 62.26  & 59.92  & 64.18  & 58.62 \\
%           & \textbf{LAMP (ours)} & 63.14$_{2.23}$  & 47.37$_{1.54}$  & 60.00$_{1.41}$  & 54.87$_{0.34}$  & 56.74$_{0.44}$  & 62.17$_{0.04}$  & 60.08$_{0.08}$  & 67.80$_{1.71}$  & \textbf{59.02 } \\
%     \bottomrule
%     \end{tabular}%
%   \caption{Few-shot adaptation results with $k = \{8, 16, 32\}$ on SuperGLUE benchmark. All results are presented as the average of three runs and subscripts indicate standard deviation, each with different random seeds on T5-Base.}
%   % The optimal results can be bold, similar to Table 1.}
%   \label{tab2}%
%   % \vspace{-8pt}
% \end{table}%

\begin{table*}[t]
  \centering
\renewcommand{\arraystretch}{0.95}
\setlength{\tabcolsep}{0.1cm}
\begin{tabular}{l|l|cccccccc|>{\columncolor{gray!25}}c}
    \toprule
    \multirow{2}[2]{*}{\textbf{Method}} & \multirow{2}[2]{*}{\textbf{Params.}} & \textbf{CB} & \textbf{WSC} & \textbf{COPA} & \textbf{RTE} & \textbf{WiC} & \textbf{BoolQ} & \textbf{MultiRC} & \textbf{ReCoRD} & \multicolumn{1}{c}{{\textbf{Average}}} \\
          &  & Acc.   & Acc.   & Acc.   & Acc.  & Acc.  & Acc.  & F1 & F1 & (\%)  \\
    \midrule
    \multicolumn{11}{c}{\textbf{T5-Small}} \\
    Fine-Tuning$^{\dagger}$ & 60M   & 89.28  & 67.94  & 59.00  & 72.56  & 68.18  & 77.06  & 66.98  & 55.64  & 69.58  \\
    \midrule
    Prompt Tuning & 51K   & 71.43  & 59.62  & 58.33  & 66.91  & 63.95  & 66.12  & 63.31  & 50.11  & 62.47  \\
    Residual PT$^{\dagger}$ & 462K  & 72.02  & 63.14  & 56.66  & 67.02  & 60.96  & \textbf{73.35 } & \underline{65.12}  & 53.08  & 63.91  \\
    DePT  & 51K   & 75.00  & \textbf{67.31}  & 52.66  & 65.22  & 62.70  & 66.00  & 61.94  & \textbf{56.71 } & 63.44  \\
    EPT   & 51K   & 78.57  & \textbf{67.31}  & 55.66  & \textbf{71.01}  & \textbf{67.39} & 69.17  & 64.46  & \underline{54.14}  & \underline{65.96}  \\
    DPT   & 6K    & \underline{78.85}  & 60.53  & \underline{59.33}  & \underline{70.40}  & 64.26  & 72.17  & 64.61  & 53.60  & 65.47 \\
    \midrule
    % w/o pooling & 6K    & \textbf{85.71 } & \textbf{68.42 } & \underline{60.00 } & \textbf{71.12 } & 64.73  & \underline{73.30}  & \textbf{66.69 } & 53.30  & \textbf{67.91 } \\
    \textbf{LAMP(ours)} & 5K   & \textbf{83.93} & \textbf{67.31}  & \textbf{60.66}  & 69.31 & \underline{66.46} & \underline{72.97} & \textbf{66.25} & 53.80 & \textbf{67.59} \\
    \midrule
    \multicolumn{11}{c}{\textbf{T5-Base}}\\
    Fine-Tuning$^{\ddagger}$ & 220M  & 91.70  & 81.70  & 60.00  & 84.50  & 69.30  & 82.30  & 76.90  & 80.90  & 78.41  \\
     \midrule
    Prompt Tuning & 77K   & 78.57  & 61.54  & 55.00  & 67.63  & 62.38  & 77.00  & 72.37  & 71.32  & 68.27  \\
    Residual PT$^{\dagger}$ & 693K  & 77.37  & \underline{67.94}  & \underline{56.66}  & \underline{81.70}  & 66.87  & 80.00  & 72.11  & 72.21  & 71.86  \\
    DePT  & 77K   & 82.14  & 67.31  & 54.33  & 73.91  & 65.20  & 79.02  & \textbf{72.70} & 70.80  & 70.68  \\
    EPT   & 77K   & \underline{85.71}  & 67.31  & \underline{56.00}  & 78.99  & 67.71  & 79.14  & \underline{72.62}  & 71.15  & \underline{72.33}  \\
    DPT   & 9K    & 78.56  & 67.30  & \underline{56.66}  & 79.42  & \underline{68.49}  & \textbf{80.28}  & 72.50  & \textbf{72.56}  & 71.97  \\
    \midrule
    % w/o pooling & 9K    & \underline{92.86 } & \textbf{68.42 } & \underline{57.00 } & \textbf{82.31 } & \underline{69.28 } & \textbf{80.55 } & 72.25  & \textbf{72.66 } & \textbf{74.42 } \\
    \textbf{LAMP(ours)} & 7K   & \textbf{94.64} & \textbf{68.42}  & \textbf{58.66}  & \textbf{83.39} & \textbf{70.06} & \underline{80.24} & 72.72 & \underline{72.55} & \textbf{75.09} \\
    \midrule
    \multicolumn{11}{c}{\textbf{T5-Large}}\\
    Fine-Tuning$^{\ddagger}$ & 770M  & 94.30  & 88.50  & 87.00  & 90.60  & 73.50  & 88.30  & 85.40  & 89.20  & 87.10  \\
     \midrule
    Prompt Tuning & 102K  & 82.35  & 65.38  & \underline{57.33}  & 88.45  & 70.69  & 84.28  & 76.37  & 74.36  & 74.90  \\
    Residual PT$^{\dagger}$ & 925K  & 73.21  & \underline{70.50}  & \textbf{62.66} & \underline{88.92}  & \underline{72.25}  & \underline{85.04}  & 76.46  & \underline{84.36}  & 76.67  \\
    DePT  & 102K  & 85.71  & 67.31  & 50.66  & 83.33  & 68.97  & 83.24  & 75.76  & 74.03  & 73.63  \\
    EPT   & 102K  & \underline{89.29}  & 68.30  & 54.00  & 86.33  & 71.79  & 84.77  & 76.62  & 73.94  & 75.63  \\
    DPT   & 11K   & \underline{89.29}  & 65.79  & \textbf{62.66} & 88.45  & 71.63  & 84.53  & \underline{76.72}  & 84.35  & \underline{77.93}  \\
    \midrule
    % w/o pooling & 11K   & \underline{94.64 } & \underline{73.68 } & \underline{59.00}  & \underline{89.89 } & \textbf{73.98 } & \textbf{85.20 } & \underline{76.81 } & \underline{84.41 } & \underline{79.70 } \\
    \textbf{LAMP(ours)} & 9K   & \textbf{98.21} & \textbf{78.95}& \underline{57.33}  & \textbf{90.61} & \textbf{73.35}& \textbf{85.11} & \textbf{76.94} & \textbf{84.56}& \textbf{80.63} \\
    \bottomrule
    \end{tabular}%
\caption{For the performance comparison on the SuperGLUE benchmark, all experimental results are based on the T5-Small, T5-Base, and T5-Large. All scores represent the mean across three runs using distinct random seeds. 
% The evaluation metrics are F1 for MultiRc, ReCoRD, and accuracy for other datasets. 
$^{\dagger}$ sourced from \citet{xiao2023decomposed}. $^{\ddagger}$ sourced from \citet{aribandi2021ext5}. The best result is marked in bold. The second-highest result is indicated by an underline. }
% The figures \en{figures?} below the datasets represent the count of training samples in each dataset. \en{Does the number of samples need to be shown here? Is that enough in the appendix?}}
  \label{tab1}%
  % \vspace{-5pt}
\end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet5'
\begin{table}[t]
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{0.3cm}
  \centering
     \begin{tabular}{c|ccc}
    \toprule
    \multicolumn{4}{c}{\textbf{K-Shot}} \\
    \midrule
    \textbf{Model} & \textbf{8} & \textbf{16} & \textbf{32} \\
    \midrule
    Prompt Tuning & 48.23 & 49.83 & 50.85 \\
    Residual PT & 52.95 & 57.57 & 58.50 \\
    DePT  & 49.83 & 50.31 & 49.93 \\
    EPT   & 50.42 & 50.71 & 53.51 \\
    DPT   & 56.26 & 55.60 & 57.72 \\
    \midrule
    \textbf{LAMP (ours)} & \textbf{57.21} & \textbf{58.14} & \textbf{59.25} \\
    \bottomrule
    \end{tabular}%
  \caption{Few-shot adaptation results with $k = \{8, 16, 32\}$ on SuperGLUE benchmark.}
  % All results are presented as the average of three runs and subscripts indicate standard deviation, each with different random seeds on T5-Base.
  \label{few-shot}%
  % \vspace{-10pt}
\end{table}%

% % Table generated by Excel2LaTeX from sheet 'Sheet2'
% \begin{table}[htbp]
%   \centering
%   \renewcommand{\arraystretch}{0.8}
%   \setlength{\tabcolsep}{0.03cm}
 
%     \begin{tabular}{l|l|cccc|c}
%     \toprule
%      \multirow{2}[2]{*}{\textbf{Model}} & \multicolumn{1}{l|}{\multirow{2}[2]{*}{\textbf{Par.}}} & \textbf{MNLI} & \textbf{QNLI} & \textbf{SST-2} & \textbf{MRPC} & \textbf{Mean} \\
%           &       & 393K  & 105K  & 67K   & 3.7K  & (\%) \\
%     \midrule
%     FT & 220M  & 86.33  & 93.19  & 94.75  & 84.56  & 89.71  \\
%     LoRA  & 3.8M  & 85.30  & 92.96  & 94.04  & 68.38  & 85.17  \\
%     PiSSA & 3.8M  & 85.75  & 93.15  & 94.07  & 76.31  & 87.32  \\
%     rsLoRA & 3.8M  & 85.73  & 93.12  & 94.19  & 52.86  & 81.48  \\
%     LoRA+  & 1.6M  & 85.81  & 93.14  & 93.85  & 74.43  & 86.81  \\
%     DoRA  & 3.8M  & 85.67  & 93.04  & 94.04  & 68.08  & 85.21  \\
%     L-GA  & 3.8M  & 85.70  & 93.18  & 94.11  & 85.29  & 89.57  \\
%     \midrule
%     \textbf{LAMP}  & 7K    & \textbf{85.82 } & \textbf{93.32 } & \textbf{94.50 } & \textbf{90.20 } & \textbf{90.96 } \\
%     \bottomrule
%     \end{tabular}%
%     \caption{Performance comparison between LAMP and LoRA-based methods on the GLUE benchmark, all experimental results are based on the T5-Base model. The baseline results are derived from LoRA-GA \citep{wang2024loraGA}. ``Par." is trainable parameters. }
%   \label{tab:lora}%
%   \vspace{-10pt}
% \end{table}%


% \begin{figure*}[t]
% \centering
%   \includegraphics[scale=0.227]{Fig/Length3_pooling.pdf}
%     \caption{Performance of different baselines varies with the prompt length $l \in \{20, 100, 200\}$. All results represent the average of three runs conducted with a different random seed.}
%     \label{fig_length}
%   \vspace{-10pt}
% \end{figure*}
% \begin{figure}[t]
% \centering
%   \includegraphics[scale=0.3]{Fig/few_shot.pdf}
%     \caption{Few-shot adaptation results with $k = \{8, 16, 32\}$ on SuperGLUE benchmark. All results are presented as the average of three runs, each with different random seeds on T5-Base.}
%     \label{fig_fewshot}
% \end{figure}

\begin{figure*}[t]
\centering
  \includegraphics[scale=0.215]{Fig/rank_length.pdf}
    \caption{(a) and (b), the performance of all baselines with the number of inherent ranks $r \in\{4, 6, 8, 10, 12, 20\}$ on the SuperGLUE benchmark. (c) and (d), the performance of different baselines varies with the prompt length $l \in \{20, 100, 200\}$. All results represent the average of three runs conducted with a different random seed.}
    \label{fig_rank_length}
  % \vspace{-8pt}
\end{figure*}

\begin{figure}[t]
\centering
  \includegraphics[scale=0.147]{Fig/T5_11B_pooling.pdf}
    \caption{(a) and (b), the performance changes of different methods at various datasets on the T5-11B and Llama2-7B. (c), the variation of training time and performance in different average pooling blocks $p$.}
    \label{Fig_11b_llama}
  % \vspace{-13pt}
\end{figure}

\subsection{Overall Performance Comparison (RQ1)}
Table \ref{tab1} presents a comparison of LAMP against baseline methods on the SuperGLUE benchmark using various T5 model sizes. Notably, LAMP requires the fewest training parameters and demonstrates exceptional average performance across different scales of T5 models. 
LAMP outperforms the original PT by 7.58\%, 9.08\%, and 7.11\% on T5-Small, T5-Base, and T5-Large, respectively. Detailed information on the standard deviation of LAMP can be found in Appendix \ref{app_Standard Deviation}. LAMP's performance improvement is attributed to enhancing the model's knowledge representation by uncovering the intrinsic semantic interactions between soft prompt tokens. LAMP achieves outstanding performance while significantly reducing training parameters and computational costs.

From the perspective of trainable parameters, LAMP and DPT are more efficient than other baselines; even though EPT outperforms DPT, EPT requires more trainable parameters. LAMP clearly outperforms DPT in several ways. First, DPT relies on randomly generated initial prompts that lack semantic richness, whereas LAMP leverages sample vocabularies to better aid the model in understanding complex language structures. Additionally, while DPT reduces trainable parameters, its prompt length remains at 100 when input into model, leading to inefficiency. In contrast, LAMP employs average pooling operations that neither harm performance nor increase trainable parameters, making it an efficient and effective novel prompt tuning method. \textbf{LAMP demonstrates superior performance and requires significantly fewer training parameters than the latest LoRA-based PEFT methods,} with detailed results provided in Appendix \ref{app_lora}.
% \begin{figure*}[t]
% \centering
%   \includegraphics[scale=0.216]{Fig/rank_pooling.pdf}
%     \caption{Compare the performance of all baselines with the number of inherent ranks $r \in\{4, 6, 8, 10, 12, 20\}$ on the SuperGLUE benchmark.}
%     \label{fig_rank}
%     \vspace{-8pt}
% \end{figure*}



% % Table generated by Excel2LaTeX from sheet 'Sheet5'
% \begin{table*}[t]
%   \renewcommand{\arraystretch}{0.5}
%   \setlength{\tabcolsep}{0.15cm}
%   \centering
%     \begin{tabular}{c|ccccc|ccccc}
%     \toprule
%     \multirow{2}[2]{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{T5-Base}   Prompt Length} & \multicolumn{5}{c}{\textbf{T5-Large}  Prompt Length} \\
%           & 20    & 100   & 1000  & 5000  & 10000 & 20    & 100   & 1000  & 5000  & 10000 \\
%     \midrule
%           & \multicolumn{5}{c|}{ \#Trainable Params (‰) } & \multicolumn{5}{c}{ \#Trainable Params (‰)} \\
%     \midrule
%     Full Fine-Tuning & 1000   & 1000   & 1000   & 1000   & 1000   & 1000   & 1000   & 1000   & 1000   & 1000 \\
%     Prompt Tuning & 0.069 & 0.344 & 3.444 & 17.221 & 34.442 & 0.028 & 0.139 & 1.389 & 6.940  & 13.879 \\
%     \textbf{LPT (Ours)} & \textbf{0.035} & \textbf{0.039} & \textbf{0.079} & \textbf{0.259} & \textbf{0.483} & \textbf{0.014} & \textbf{0.015} & \textbf{0.027} & \textbf{0.082} & \textbf{0.149} \\
%     \midrule
%     Multiple & \blue{1.95}  & \blue{8.84}  & \blue{43.41}  & \blue{66.56}  & \blue{71.32}  & \blue{1.96}  & \blue{9.10}  & \blue{50.57}  & \blue{84.98}  & \blue{92.88}  \\
%     \bottomrule
%     \end{tabular}%
%     \caption{Trainable parameters of different baselines varies with the prompt length. "Multiple" denotes the multiple of trainable parameters in vanilla prompt tuning (PT) relative to LPT.}
%   \label{tab3}%
%   \vspace{-10pt}
% \end{table*}%





\subsection{Ablation Experiment Analysis (RQ2)}
\label{subsec:few-shot}
% \lan{To more clearly demonstrate the impact of hyper-parameters tuning on various datasets within the SuperGLUE benchmark, without loss of generality, we conducted analyses using the CB, COPA, RTE, and WiC datasets within the SuperGLUE. }
% \begin{figure*}[t]
% \centering
%     \subfloat{
% 		\includegraphics[scale=0.199]{Fig/a_PT_tsne_multirc.png}}
%      \subfloat{
% 		\includegraphics[scale=0.199]{Fig/b_LPT_tsne_multirc.png}}
%     \subfloat{
% 		\includegraphics[scale=0.199]{Fig/c_PT_tsne_copa.png}}
%   \subfloat{
% 		\includegraphics[scale=0.199]{Fig/d_LPT_tsne_copa.png}}
% \caption{The comparison of the dispersion of PT tokens on the MultiRC and COPA datasets before and after considering the intrinsic semantic associations between soft prompt tokens on T5-Base.}
%     \label{fig_t-sne}
%   \vspace{-10pt}
% \end{figure*}
% \ \\ \noindent
\textbf{Few-shot adaptation.} 
% \subsubsection{Few-shot adaptation}
Following the few-shot experimental setup of \citet{xiao2023decomposed}, we randomly sampled 8, 16, and 32 training examples. Table \ref{few-shot} presents the results of all baselines on the SuperGLUE benchmark. All results are averaged over three runs with different random seeds on the T5-base model. We found that LAMP outperforms other baselines on most datasets in the few-shot setting, demonstrating its effectiveness. The few-shot performance of various methods across different datasets is detailed in Appendix \ref{app_Few-shot Details}.
% LAMP has better robustness; for instance, with fewer samples, such as on the xxx dataset, the 8-shot results are better than those at 32-shot.
\ \\ \noindent
\textbf{Sensitivity of Rank Size.}
% \subsubsection{Sensitivity of Rank Size}
The intrinsic rank $r$ is the primary factor influencing the total number of trainable parameters in LAMP. We analyzed the impact of $r \in\{4, 6, 8, 10, 12, 20\}$ on LAMP performance using the T5-base model on the CB and BoolQ datasets within the SuperGLUE benchmark. As shown in Figure \ref{fig_rank_length}(a) and Figure \ref{fig_rank_length}(b), despite the minimal differences in trainable parameters of LAMP and DPT across different $r$ values, LAMP consistently outperforms DPT and other baseline methods in most scenarios. This demonstrates the effectiveness of incorporating semantic knowledge into LAMP. The details of how the intrinsic rank $r$ affects the changes in training parameters can be found in the Appendix \ref{app_Intrinsic Rank}.
% Additionally, even when $r$ is less than 10, LAMP still has high robustness.
\begin{figure*}[t]
\centering
    \subfloat{
		\includegraphics[scale=0.2]{Fig/a_PT_tsne_multirc.png}}
     \subfloat{
		\includegraphics[scale=0.2]{Fig/b_Lamp_multirc.png}}
    \subfloat{
		\includegraphics[scale=0.2]{Fig/c_PT_tsne_copa.png}}
  \subfloat{
		\includegraphics[scale=0.2]{Fig/d_Lamp_copa.png}}
\caption{The comparison of the dispersion of PT tokens on the MultiRC and COPA datasets before and after considering the intrinsic semantic associations between soft prompt tokens on T5-Base.}
    \label{fig_t-sne}
  % \vspace{-10pt}
\end{figure*}
\ \\ \noindent
\textbf{Effect of Prompt Length.}
% \subsubsection{Effect of Prompt Length}
We conduct analyses using the RTE and WiC datasets within the SuperGLUE benchmark. To understand the impact of prompt length on the LAMP's performance, we maintained an "intrinsic rank" $r$ of 8 for the soft prompt on the T5-Base model, varying the prompt lengths $\in\{20, 100, 200\}$. Figure \ref{fig_rank_length}(c) and Figure \ref{fig_rank_length}(d) illustrates that LAMP consistently outperforms other baselines at prompt lengths of 20, 100, and 200. LAMP achieves optimal performance when the prompt length is set to 100, consistent with previous findings that 100 is the optimal hyperparameter for prompt length \citep{lester2021power,razdaibiedina-etal-2023-residual}. The experimental details of other datasets in the Appendix \ref{app_Rank+length}.
\ \\ \noindent
\textbf{Impact of Model Scale.} 
% \subsubsection{Impact of Model Scale}
Based on quantification, we conducted experiments on T5-11B and Llama2-7B using randomly selected datasets and compared LAMP against baselines that initialize prompts with semantic knowledge from samples. As shown in Figure \ref{Fig_11b_llama}(a) and Figure \ref{Fig_11b_llama}(b), across different model architectures with more than 7B parameters (T5 with an encoder-decoder structure and Llama2 with a decoder-only structure), LAMP consistently helps models adapt to various downstream tasks, achieving superior performance. The experimental details of T5-3B can be found in the Appendix \ref{app_T5-3B}. Meanwhile, Appendix \ref{app_Parameters} details the changes in training parameters and memory usage associated with the model scale.
\ \\ \noindent
\textbf{Blocks of Average Pooling.} 
% \subsubsection{Blocks of Average Pooling}
Figure \ref{Fig_11b_llama}(c) illustrates the changes in model training time and performance on the SuperGLUE benchmark as the number of average pooling blocks increases. The larger the pooling block $p$, the shorter the prompt length input to the model, resulting in a more noticeable reduction in training time. Furthermore, we were pleasantly surprised to find that average pooling had minimal impact on performance, yet it provided significant advantages for PT. This finding offers a promising mentality for extending PT to various domains.

\vspace{-13pt}
\subsection{Interpretability (RQ3)}
\label{interpretability}
Figure \ref{fig_t-sne} compares general PT before (i.e., blue points) and after (i.e., red points) extracting the intrinsic semantic associations between soft prompt tokens on the MultiRC and COPA datasets. To more intuitively reflect the discreteness of PT, we did not standardize the dimensions for comparison. The more extensive the x and y-axis ranges, the higher the degree of discreteness. The features extracted by LAMP exhibit more distinct clustering, indicating LAMP's superior ability to capture and represent the intrinsic structure and patterns of the data. By considering the interactions between prompt tokens, LAMP uncovers the intrinsic semantic associations among tokens to enhance knowledge representation. This enables PLMs to better grasp the semantic content of textual data. The comparative results of discreteness between LAMP and original prompt tuning (PT) across other datasets are detailed in Appendix \ref{app_visualization}.

% \subsection{Variation of LPT Parameters (RQ3)}
% % \subsection{In-depth Analysis (RQ3)}
% \label{subsec:r3}
% % \textbf{Change in LPT Parameters from Intrinsic Rank.} As shown in Figure \ref{fig_rank_size}(a), the impact of intrinsic rank $r \in \{5, 10, 20, 50, 80, 100\}$ on LPT parameters is illustrated across T5 models (T5-Small, T5-Base, and T5-Large). As $r$ increases, the trainable parameters in LPT gradually approach that of the vanilla PT. When all ranks in the Truncated SVD are retained, the trainable parameters of PT are fewer than those of LPT. 
% % % Notably, we use $r=10$ to maintain trainable parameters similar to DPT, only to verify that LPT outperforms DPT. Meanwhile, in Section \ref{subsec:few-shot}, we have demonstrated that LPT can achieve satisfactory results even when $r$ is less than 10. 
% % The number of trainable parameters in LPT can be further reduced by adjusting $r$.
% % \begin{figure}[t]
% % \centering
% %   \includegraphics[scale=0.208]{Fig/rank_model.pdf}
% %     \caption{(a) Trainable parameters with different intrinsic ranks on the T5 models. $\_S$ is T5-Small, $\_B$ is T5-Base and $\_L$ is T5-Large. (b) Trainable parameters with different model size. }
% %     \label{fig_rank_size}
% % \end{figure}
% % \ \\ \noindent
% % \textbf{Change in LPT Parameters from Model Size.} With intrinsic rank $r=10$, Figure \ref{fig_rank_size}(b) illustrates the effect of model size on LPT trainable parameters. As the model size increases, trainable parameters in original PT increases significantly, whereas LPT mitigates this issue. Table \ref{tab3} shows that when the prompt length and $r$ remain constant, the higher the model size, the more significant the reduction in trainable parameters achieved by LPT. LPT also mitigates the impact of the hidden dimension on trainable parameters.

% % \ \\ \noindent
% % \textbf{Change in LPT parameters from Prompt Length.} 
% Table \ref{tab3} presents the variation of LPT's trainable parameters with prompt length $l \in \{20, 100, 1000, 5000, 10000\}$ with $r=10$. As PT-based methods are applied in other domains (recommendation systems, NLP), prompt length significantly increases \citep{doddapaneni2024user,liu2024can}. Therefore, it is crucial to investigate LPT trainable parameters for long prompt lengths. The longer the prompt length, the more LPT downplays the impact of prompt length on trainable parameters, making LPT's advantages more evident. For instance, in T5-Large, the trainable parameters with a prompt length of 10,000 are only one-ninth of those in vanilla PT with a prompt length of 100. This allows LPT to incorporate richer task-specific knowledge and improves training efficiency. In summary, influenced by intrinsic rank $r$, prompt length $l$ , and model size, the efficiency of LPT compared to PT and other PEFT methods can be further expanded. The detailed changes in LPT parameters in terms of intrinsic rank $r$ and model size can be found in \ref{A_parameters}

% \subsection{Interpretability (RQ4)}
% \label{interpretability}
% Figure \ref{fig_t-sne} compares general PT before (i.e., blue points) and after (i.e., red points) extracting the intrinsic semantic associations between soft prompt tokens on the MultiRC and COPA datasets. To more intuitively reflect the discreteness of PT, we did not standardize the dimensions for comparison. The more extensive the x and y-axis ranges, the higher the degree of discreteness. The features extracted by LPT exhibit more distinct clustering, indicating LPT's superior ability to capture and represent the intrinsic structure and patterns of the data. By considering the semantic interactions between tokens, LPT enhances knowledge representation, enabling PLMs to understand textual data's semantic content better. The comparison of dispersion levels between LPT and original PT on other datasets can be found in Appendix \ref{A_visualization}.
% % \section{Related Work}
% % \textbf{Parameter-efficient Fine-tuning.} Parameter-efficient fine-tuning methods\citep{lester2021power,zaken2021bitfit,shi2024dept,ding2023sparse,zhu2023sira}, a significant advancement over traditional full fine-tuning, offer the ability to achieve commendable results by training only a small subset of parameters, thereby reducing computational costs and improving efficiency. BitFit\citep{zaken2021bitfit} only updates the bias terms while freezing most of the pre-trained model's parameters, but it performs poorly on larger models\citep{lialin2023scaling}. Threshold-Mask\citep{zhao2020masking} constructs a binary mask matrix using a threshold to select pre-trained weights in the attention and feedforward layers through element-wise multiplication. Adapter-based methods, such as Adapters\citep{houlsby2019parameter}, HyperFormer\citep{mahabadi2021parameter}and Compacter\citep{karimi2021compacter}, add trainable adapter modules to each Transformer block in the T5 model\citep{raffel2020exploring}. AdapterDrop\citep{ruckle2020adapterdrop} improves efficiency by removing unimportant adapters for a given task in each layer of the Transformer. LoRA\citep{hu2021lora} re-parameterizes incremental matrices through simple low-rank decomposition. Structurally similar to LoRA, KronA\citep{edalati2022krona} replaces the low-rank decomposition in LoRA with Kronecker product decomposition. PISSA\citep{meng2024pissa} also adopts the same architecture as LoRA, but it initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. LST\citep{sung2022lst} reduces training memory by running a small ladder network alongside the pre-trained network. Prompt tuning\citep{lester2021power}, a novel and promising parameter-efficient fine-tuning method, stands out from the rest by achieving good results with training very few parameters. Unlike other methods, it maintains a consistent number of trainable parameters even as the model size expands, making it a potential game-changer in the field.
% % \ \\ \noindent
% % \textbf{Soft Prompt-based Fine-tuning.} Soft prompt tuning inserts trainable continuous vectors, known as soft prompts, into the model's input or hidden states. Unlike hard prompts, soft prompts are generated by searching the discrete token space based on task-specific training data. In 2021, Li and Liang proposed prefix tuning\citep{li2021prefix}, which adds soft prompts at the input to adjust the model's behavior. Unlike prefix tuning, prompt tuning\citep{lester2021power} only adds soft prompts to the embedding layer of the model. Other methods have explored transfer learning-based prompt tuning, such as MPT\citep{wang2023multitask} and ATTEMPT\citep{asai2022attempt}, which better initialize soft prompts through pre-training, but these methods require significant resources for the pre-training process.Xiao\citep{xiao2023decomposed} pointed out that soft prompts inherently exhibit low-rank properties, which can be leveraged to further enhance the method's parameter efficiency and performance through certain strategies. DePT\citep{shi2024dept} decomposes the soft prompt into shorter prompts and pairs of low-rank matrices, which are then used to update the model's weights. DPT\citep{xiao2023decomposed} employs a re-parameterization strategy, using two low-rank matrices to replace the original soft prompt. However, these PT-based methods need more efficiency and richness of task-specific knowledge when dealing with long soft prompts.

\section{Related Work}
\textbf{Parameter-efficient Fine-tuning.} 
% \subsection{Parameter-efficient Fine-tuning}
Parameter-efficient fine-tuning achieves strong results by training a small subset of parameters, thereby reducing computational costs and improving efficiency. AdapterDrop \citep{ruckle2020adapterdrop} improves efficiency by removing unimportant adapters for a given task in each layer of the Transformer. BitFit \citep{zaken2021bitfit} only updates the bias terms while freezing most of the pre-trained model's parameters.
LST \citep{sung2022lst} reduces training memory by running a small ladder network alongside the pre-trained network. 
% Threshold-Mask\citep{zhao2020masking} constructs a binary mask matrix using a threshold to select pre-trained weights in the attention and feedforward layers through element-wise multiplication. 
% Adapter-based methods, such as Adapters\citep{houlsby2019parameter}, HyperFormer\citep{mahabadi2021parameter}and Compacter\citep{karimi2021compacter}, add trainable adapter modules to each Transformer block in the T5 model\citep{raffel2020exploring}. 
LoRA \citep{hu2021lora} re-parameterizes incremental matrices through simple low-rank decomposition. KronA \citep{edalati2022krona} replaces the low-rank decomposition in LoRA with Kronecker product decomposition;  PISSA \citep{meng2024pissa} initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. 
% PISSA\citep{meng2024pissa} also adopts the same architecture as LoRA, but it initializes the low-rank matrices with the weights of the pre-trained model, enhancing performance and efficiency. 
However, prompt tuning \citep{lester2021power} stands out from the rest by achieving good results with training very few parameters.
\ \\ \noindent
\textbf{Prompt-based Fine-tuning.} 
% \subsection{Prompt-based Fine-tuning}
Unlike other PEFT methods, prompt-based fine-tuning methods sustain a controlled increase in trainable parameters, even with substantial model scaling.
% Soft prompt tuning inserts trainable continuous vectors, known as soft prompts, into the model's input or hidden states. Unlike hard prompts, soft prompts are generated by searching the discrete token space based on task-specific training data. 
% In 2021, Li and Liang proposed prefix tuning\citep{li2021prefix}, which adds soft prompts at the input to adjust the model's behavior. Unlike prefix tuning, 
Prompt tuning \citep{lester2021power} only adds the soft prompt to the input embedding layer of the model.
% Other methods have explored transfer learning-based prompt tuning, such as ATTEMPT \citep{asai2022attempt} and MPT \citep{wang2023multitask}, which better initialize the soft prompt through pre-training, but these methods require significant resources for the pre-training process. 
% DPT \citep{xiao2023decomposed} pointed out that soft prompts inherently exhibit low-rank properties, which can be leveraged to further enhance the method's parameter efficiency and performance through certain strategies.
DPT\citep{xiao2023decomposed} employs a re-parameterization strategy, using two low-rank matrices to replace the original soft prompt. 
% Noteworthily,our method LAMP can integrate task-specific knowledge into the soft prompt, allowing for more precise and effective prompt customization and capturing semantic interactions to enhance knowledge representation. 
DPT relies solely on random number generation for the soft prompt, resulting in weaker generalization and higher sensitivity to initialization.
% SMoP \citep{choi2023smop} employs a gating mechanism to train multiple short soft prompts, maintaining the performance gains typically associated with longer soft prompts.
% Xprompt \citep{ma2022xprompt} proposes a prompt-tuning method based on the lottery hypothesis. It employs hierarchical structure pruning to eliminate negative prompt tokens in different granularity levels, resulting in prompts with higher parameter efficiency. 
DePT\citep{shi2024dept} decomposes the soft prompt into shorter prompts and pairs of low-rank matrices, which are then used to update the model's weights. 
EPT \citep{lan2024efficient} leverages multi-space projection and prompt fusion to refine soft prompt knowledge, enhancing flexibility and balancing accuracy with computational efficiency for diverse tasks.
Nevertheless, these PT-based methods need more efficiency and task-specific knowledge richness when dealing with long soft prompt. LAMP provides the ability to tailor prompts more precisely and effectively to the specific requirements of various tasks.
\section{Conclusions}
In this work, we observed that soft prompt tokens initialized randomly from the vocabulary lack intrinsic semantic associations to enhance knowledge representation. Additionally, PT-based methods face challenges balancing knowledge richness and computational cost in different tasks. Based on these issues, we approximate soft prompt by proposing a Low-parameter Prompt Tuning (LAMP) method, which utilizes two singular vectors and singular values. LAMP facilitates semantic knowledge interaction, allowing the soft prompt to incorporate more task-specific knowledge.  It can serve as an efficient plugin for various PT-based tasks. Experimental results across three model scales (T5-Small, T5-Base, T5-Large) demonstrate that LAMP achieves high effectiveness and robustness with fewer trainable parameters.

\section*{Acknowledgements}
This work is partially supported by the National Natural Science Foundation of China under Grant (No. 62032013, 62102074), the Science and Technology projects in Liaoning Province (No. 2023JH3/10200005).

\section*{Limitations}
While our method significantly reduces trainable parameters in NLP, its potential applications extend far beyond this domain. Its evaluation in areas beyond NLP and with other advanced large language models remains a work in the future.  While our approach significantly reduces trainable parameters, further quantification of model parameters will also be explored in future research. The intrinsic semantic interactions between soft prompt tokens can be more effectively mined without increasing the number of trainable parameters. In future work, we will explore methods to enhance knowledge representation for PT.

% \input{limitation}
% \include{limitation}

% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{reference}

\appendix

\begin{table*}
  \centering
  \renewcommand{\arraystretch}{1}
  \setlength{\tabcolsep}{0.1cm}
    \begin{tabular}{lrrlll}
    \toprule
    \multicolumn{6}{c}{\textbf{SuperGLUE Benchmark}} \\
    \midrule
    \textbf{Dataset} & \multicolumn{1}{l}{\textbf{\#Train}} & \multicolumn{1}{l}{\textbf{\#Dev}} & \textbf{Type} & \textbf{Domain} & \textbf{\#Metric} \\
    \midrule
    CB    & 250   & 56    & Natural Language Inference  & various & accuracy  \\
    WSC   & 259   & 104    & Common Sense Reasoning & fiction books & accuracy  \\
    COPA   & 400 & 100   & Question Answering & blogs, etc. & accuracy  \\
    RTE   & 2,490 & 277   & Natural Language Inference   & News, Wikipedia & accuracy  \\
    Wic   & 5,428 & 638   & Word Sense Disambiguation & lexical databases & accuracy  \\
    BoolQ & 9,427 & 3,270 & Question Answering & Wikipedia & accuracy  \\
    MulticRC & 27,243 & 4,848 & Question Answering & various & F1\\
    ReCoRD & 100,730 & 10,000 & Common Sense Reasoning & news (CNN, Daily Mail) & 
    F1\\
    \midrule
    MNLI  & 392,702 & 19,647 & NLI   & various & accuracy  \\
     QNLI  & 103,743 & 6,463 & NLI   & Wikipedia & accuracy  \\
     SST-2 & 66,349 & 1872   & Sentiment & Movie Reviews & accuracy \\
     MRPC  & 3,668 & 408   & Paraphrase & news  & 
    accuracy \\
    \bottomrule
    \end{tabular}%
    \caption{The details of the 8 datasets in SuperGLUE benchmark utilized in our experiment.}
    % excluding the test set counts.
  \label{tab_datasets}%
\end{table*}%

\section{Appendix}
\label{sec:appendix}

\subsection{Self-Attention Pooling}
\label{app_Self-Attention Pooling}
We propose utilizing a self-attention mechanism for the pooling operation to allow the soft prompt to adaptively assign different weights to prompt tokens —emphasizing key tokens while ignoring less important ones. The formula is expressed as follows:
\begin{eqnarray}
\begin{aligned}
    \mathbf{K} & =\mathbf{C} W_{s a} \\
    \mathbf{A}_{weight} & =\operatorname{Softmax}(\mathbf{K}) \\
\mathbf{P'} & =\mathbf{A}_{weight}^{\top} \mathbf{C}
\end{aligned}
\end{eqnarray}
where, $W_{sa} \in \mathbb{R}^{d \times l/p}$ is a learnable initialization weight matrix, and $A_{weight}\in \mathbb{R}^{l \times l/p}$ represents the attention weights. By applying $A_{weight}$ to $\mathbf{C} \in \mathbb{R}^{l \times d}$, we achieve adaptive selection and pooling operation of prompt tokens, resulting in $\mathbf{P'} \in \mathbb{R}^{l/p \times d}$. The self-attention pooling operation introduces additional trainable parameters $W_{sa} \in \mathbb{R}^{d \times l/p}$, and its performance is suboptimal. We hypothesize that this may be due to the compressed outer product effectively capturing the intrinsic semantic associations between prompt tokens. The dynamic weight assignment might disrupt these previously captured associations.

\subsection{Dataset Details}
\label{app_Dataset Details}
Table \ref{tab_datasets} provides detailed information on the 8 datasets we used in SuperGLUE benchmark. The processing of all datasets follows the approach of \citet{xiao2023decomposed}.
%


% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table*}[t]
  \renewcommand{\arraystretch}{1}
  \setlength{\tabcolsep}{0.2cm}
  \centering
    \begin{tabular}{cccccccccc}
    \toprule
    \textbf{Model} & \textbf{Params.} & CB    & WSC   & COPA  & RTE   & WiC   & BoolQ & MultiRC & ReCoRD \\
    \midrule
    \multicolumn{10}{c}{\textbf{T5-Small}} \\
    \textbf{LAMP\_S} & 6K    & 1.54  & 2.26  & 0.47  & 0.78  & 0.71  & 0.23  & 1.12  & 0.25 \\
    \midrule
    \multicolumn{10}{c}{\textbf{T5-Base}} \\
    \textbf{LAMP\_B} & 9K    & 0.84  & 2.15  & 1.25  & 0.51  & 0.34  & 0.13  & 0.32  & 0.05 \\
    \midrule
    \multicolumn{10}{c}{\textbf{T5-Large}} \\
    \textbf{LAMP\_L} & 11K   & 1.37  & 2.48  & 0.47  & 0.31  & 0.29  & 0.07  & 0.08  & 0.07 \\
    \bottomrule
    \end{tabular}%
    \caption{We report standard deviation of three runs for our method LAMP, where \text{\_S} is T5-Small, \text{\_B} is T5-Base and \text{\_L} is T5-Large.}
  \label{tab:Standard Deviation}%
\end{table*}%


% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table*}[t]
  \centering
  \renewcommand{\arraystretch}{1}
  \setlength{\tabcolsep}{0.45cm}
 
    \begin{tabular}{l|l|cccc|c}
    \toprule
     \multirow{2}[2]{*}{\textbf{Model}} & \multicolumn{1}{l|}{\multirow{2}[2]{*}{\textbf{Param.}}} & \textbf{MNLI} & \textbf{QNLI} & \textbf{SST-2} & \textbf{MRPC} & \textbf{Mean} \\
          &       & 393K  & 105K  & 67K   & 3.7K  & (\%) \\
    \midrule
    Fine-Tuning & 220M  & 86.33  & 93.19  & 94.75  & 84.56  & 89.71  \\
    LoRA  & 3.8M  & 85.30  & 92.96  & 94.04  & 68.38  & 85.17  \\
    PiSSA & 3.8M  & 85.75  & 93.15  & 94.07  & 76.31  & 87.32  \\
    rsLoRA & 3.8M  & 85.73  & 93.12  & 94.19  & 52.86  & 81.48  \\
    LoRA+  & 1.6M  & 85.81  & 93.14  & 93.85  & 74.43  & 86.81  \\
    DoRA  & 3.8M  & 85.67  & 93.04  & 94.04  & 68.08  & 85.21  \\
    LoRA-GA  & 3.8M  & 85.70  & 93.18  & 94.11  & 85.29  & 89.57  \\
    \midrule
    DEPT  & 76.8K  & 85.12  & 93.20  & 94.19  & 88.71   & 90.31  \\
    EPT  & 76.8K  & 85.63  & 93.15  & 94.21  & 89.20  & 90.55  \\
    DPT  & 9K  & 85.34  & 93.15 & 94.50  & 88.47  & 90.37  \\
    \midrule
    \rowcolor{gray!25}
    \textbf{LAMP}  & 7K    & \textbf{85.82 } & \textbf{93.32 } & \textbf{94.50 } & \textbf{90.20 } & \textbf{90.96 } \\
    \bottomrule
    \end{tabular}%
    \caption{The performance comparison between LAMP and the latest LoRA-based PEFT methods on the GLUE benchmark, all experimental results are based on the T5-Base model. The LoRA-based baseline results are derived from LoRA-GA \citep{wang2024loraGA}.  }
  \label{tab:lora}%
  \vspace{-10pt}
\end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet5'
\begin{table*}[t]
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{0.04cm}
  \centering
    \begin{tabular}{c|c|cccccccc|>{\columncolor{gray!25}}c}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[3]{*}{\textbf{K-Shot}}} & \multirow{2}[3]{*}{\textbf{Method}} & \multicolumn{9}{c}{\textbf{SuperGLUE}} \\
\cmidrule{3-11}    \multicolumn{1}{c}{} &       & \textbf{CB} & \textbf{WSC} & \textbf{COPA} & \textbf{RTE} & \textbf{WiC} & \textbf{BoolQ} & \textbf{MultiRC} & \textbf{ReCoRD} & \textbf{Average} \\
 \midrule
    \multirow{7}[1]{*}{8} & PT    & 58.57  & 32.69  & 40.66  & 49.64  & 53.61  & 53.94  & 50.17  & 46.55  & 48.23  \\
          & Res PT & 60.55    & 28.95    & 55.00    & 47.29    & 44.51    & 61.35    & 57.78    & 68.13    & 52.95 \\
          & DePT  & 61.43  & 36.69  & 56.66  & 48.92  & 51.72  & 47.83  & 47.45  & 47.94  & 49.83  \\
          & EPT   & 57.14  & 42.31  & 45.33  & 51.80  & 50.47  & 53.39  & 56.91  & 46.04  & 50.42  \\
          & DPT   & 62.50  & 39.47  & 55.66  & 56.68  & 50.00  & 62.17  & 60.12  & 63.49  & 56.26  \\
          \cmidrule{2-11}
          % & \textbf{w/o pooling} & 66.10  & 42.11  & 55.00  & 53.07  & 50.00  & 61.62  & 59.97  & 69.79  & \textbf{57.21 } \\
          & \textbf{LAMP} & 62.86$_{1.53}$  & 44.74$_{1.24}$  & 59.00$_{0.94}$  & 53.43$_{0.2}$  & 50.00$_{0.00}$  & 62.17$_{0.26}$  & 59.98$_{0.00}$  & 62.35$_{1.15}$ & \textbf{57.21}\\
    \midrule
    \multirow{7}[2]{*}{16} & PT    & 57.14  & 32.69  & 44.00  & 51.08  & 53.29  & 58.10  & 56.02  & 46.34  & 49.83  \\
          & Res PT & 68.88    & 50.00    & 55.00    & 47.65    & 53.13    & 62.42    & 55.24    & 68.22    & 57.57 \\
          & DePT  & 50.00  & 37.69  & 48.66  & 55.40  & 49.53  & 57.98  & 52.43  & 50.77  & 50.31  \\
          & EPT   & 42.86  & 42.31  & 48.00  & 51.80  & 56.11  & 57.43  & 57.73  & 49.47  & 50.71  \\
          & DPT   & 44.64  & 55.26  & 55.66  & 53.07  & 51.41  & 62.17  & 59.99  & 62.57  & 55.60  \\
          \cmidrule{2-11}
          % & \textbf{w/o pooling} & 67.86  & 42.11  & 55.00  & 52.71  & 55.49  & 62.17  & 59.95  & 69.79  & 58.14\\
          & \textbf{LAMP} & 62.21$_{2.66}$  & 47.37$_{2.48}$  & 59.00$_{1.89}$  & 54.51$_{0.85}$  & 54.23$_{0.59}$ & 62.29$_{0.06}$  & 59.88$_{0.01}$  & 70.72$_{0.44}$  & \textbf{58.14 } \\
    \midrule
    \multirow{7}[2]{*}{32} & PT    & 57.14  & 32.69  & 46.33  & 54.68  & 55.49  & 59.94  & 51.39  & 49.15  & 50.85  \\
          & Res PT & 69.21    & 47.37   & 58.66    & 51.99    & 53.34    & 63.09    & 56.15    & 68.15    & 58.50 \\
          & DePT  & 53.57  & 32.69  & 52.33 & 50.36  & 52.98  & 58.72  & 49.89  & 48.91  & 49.93  \\
          & EPT   & 67.85  & 40.38  & 56.33  & 51.80  & 55.80  & 60.12  & 49.74  & 46.05  & 53.51  \\
          & DPT   & 62.50  & 52.63  & 55.00  & 53.79  & 52.35  & 62.17  & 59.95  & 63.34  & 57.72  \\
          \cmidrule{2-11}
          % & \textbf{w/o pooling} & 67.86  & 52.63  & 55.00  & 54.15  & 52.98  & 62.26  & 59.92  & 64.18  & 58.62 \\
          & \textbf{LAMP} & 63.14$_{2.23}$  & 47.37$_{1.54}$  & 60.00$_{1.41}$  & 54.87$_{0.34}$  & 56.74$_{0.44}$  & 62.17$_{0.04}$  & 60.08$_{0.08}$  & 67.80$_{1.71}$  & \textbf{59.25 } \\
    \bottomrule
    \end{tabular}%
  \caption{Few-shot adaptation results (\%) with $k = \{8, 16, 32\}$ on SuperGLUE benchmark. All results are presented as the average of three runs and subscripts indicate standard deviation, each with different random seeds on T5-Base. The
best result is marked in bold.}
  % The optimal results can be bold, similar to Table 1.}
  \label{tab:app_few_shot}%
  % \vspace{-8pt}
\end{table*}%


\subsection{Baselines Details}
\label{app_Baselines Details}
Apart from full fine-tuning, due to the significant advantages of PT, all other baselines are variants based on PT. Descriptions of all \textbf{PT-based} baselines are as follows:
\begin{itemize}
\item \textbf{Full Fine-tuning:} Updating all model parameters in the T5-models \citep{raffel2020exploring} on each downstream task. It is the most fundamental method for comparing PEFT methods' performance and trainable parameters.
\item\textbf{Prompt tuning} \citep{lester2021power}: PT stands out in the PEFT approaches because it freezes the parameters of PLMs and only trains the attached soft (continuous) prompt vectors to the input text.
\item\textbf{Residual Prompt tuning} \citep{razdaibiedina-etal-2023-residual}: A PT-based variant (named Res PT) that utilizes a residual network to increase the flexibility of model selection for soft prompt token representations and improve the convergence rate.
\item\textbf{DePT} \citep{shi2024dept}: Decomposing the prompt into a shorter prompt and low-rank matrix pairs to reduce training time. It utilizes low-rank matrix pairs to update the input embedding.
\item \textbf{EPT} \citep{lan2024efficient}: This method utilizes multi-space projection and prompt fusion modules to enhance the soft prompt knowledge, making it more adaptable to various downstream tasks while balancing accuracy and efficiency.
\item \textbf{DPT} \citep{xiao2023decomposed}: A novel prompt initialization method involves replacing the prompt with two randomly initialized low-dimensional matrices.
 \end{itemize}
Descriptions of all \textbf{LoRA-based} baselines are as follows:
\begin{itemize}
\item\textbf{LoRA} \citep{hu2021lora}: A parameter-efficient approach focuses on updating only the low-rank matrices within the model.
\item\textbf{PiSSA} \citep{meng2024pissa}: It performs Truncated Singular Value Decomposition (SVD) on the model's weight matrix and uses the resulting low-rank matrices as the initialization for the low-rank matrices $A$ and $B$.
\item\textbf{rsLoRA} \citep{kalajdzievski2023rsLoRA}:  introduces a new scaling factor to stabilize LoRA's parameter scaling.
\item\textbf{LoRA+} \citep{hayoulora+}: It utilizes two different learning rates to control the updates of the low-rank matrices $A$ and $B$.
\item\textbf{DoRA} \citep{liudora}: It decomposes the pre-trained weight into two components, $magnitude$ and $direction$, for fine-tuning.
\item\textbf{LoRA-GA} \citep{wang2024loraGA}: It aligns low-rank gradient products with full fine-tuning gradients at the first step.
 \end{itemize}
\subsection{Implementation Details}
\label{app_Implementation_Details}
Weight decay of $1e-5$, and the maximum sequence length for the model is typically configured at 256. LAMP is implemented by the Python library of PyTorch 2.0.0 \footnote{\footnotesize \url{https://pytorch.org/}}, Huggingface Transformers 4.30.0 \footnote{\footnotesize \url{https://github.com/huggingface/transformers}}. All of our experiments were conducted with 8 GPUs, with 48 GB memory each. 

\subsection{Standard Deviation}
\label{app_Standard Deviation}
We present the standard deviation across three runs for our method on T5-Small, T5-Base and T5-Large. The outcomes are provided in Table \ref{tab:Standard Deviation}.

\subsection{Performance Comparison with LoRA Variants}
\label{app_lora}
Table \ref{tab:lora} presents a performance comparison between LAMP and LoRA, along with its variants on the GLUE benchmark. LAMP outperforms all other novel LoRA-based baselines. Notably, LAMP requires significantly fewer trainable parameters than all baselines, representing a unique advantage among PEFT methods.

\subsection{Few-shot Adaptions Details}
\label{app_Few-shot Details}
In Table  \ref{tab:app_few_shot}, we present the results of all baseline models across all SuperGLUE datasets, using T5-Base as the benchmark. The subscripts represent the standard deviation of our method LAMP across different K-shot settings.


\begin{figure}[htbp]
\centering
  \includegraphics[scale=0.26]{Fig/rank_size.pdf}
    \caption{Trainable parameters with different intrinsic ranks on the T5 models, where \text{\_S} is T5-Small, \text{\_B} is T5-Base and \text{\_L} is T5-Large. }
    \label{fig_rank_size}
    \vspace{-10pt}
\end{figure}

\subsection{Change in LAMP Parameters from Intrinsic Rank}
\label{app_Intrinsic Rank}
As shown in Figure \ref{fig_rank_size}, the impact of intrinsic rank $r \in \{5, 10, 20, 50, 80, 100\}$ on LAMP trainabel parameters is illustrated across T5 models (T5-Small, T5-Base, and T5-Large). As $r$ increases, LAMP's trainable parameters gradually approach that of the vanilla PT. When all ranks in the Truncated SVD are retained, the trainable parameters of PT are fewer than those of LAMP. 
% Notably, we use $r=10$ to maintain trainable parameters similar to DPT, only to verify that LAMP outperforms DPT. Meanwhile, in Section \ref{subsec:few-shot}, we have demonstrated that LAMP can achieve satisfactory results even when $r$ is less than 10. 
We downplay the impact of prompt length on the number of trainable parameters. The number of trainable parameters in LAMP can be further reduced by adjusting $r$.

% \begin{figure}[t]
% \centering
%   \includegraphics[scale=0.206]{Fig/rank_model.pdf}
%     \caption{(a) Trainable parameters with different intrinsic ranks on the T5 models, where \text{\_S} is T5-Small, \text{\_B} is T5-Base and \text{\_L} is T5-Large. (b) Trainable parameters with different model size. }
%     \label{fig_rank_size}
% \end{figure}

\begin{figure*}[t]
\centering
  \includegraphics[scale=0.21]{Fig/length+rank.pdf}
    \caption{(a) and (b), performance of different baselines varies with the prompt length $l \in \{20, 100, 200\}$ on the CB and COPA datasets. (c) and (d), compare the performance of all baselines with the number of inherent ranks $r \in\{4, 6, 8, 10, 12, 20\}$ on the RTE and WiC datasets.  All results represent the average of three runs conducted with a different random seed.}
    \label{fig_length+rank}
  % \vspace{-10pt}
\end{figure*}

\subsection{Impact of Rank Size and Prompt Length}
\label{app_Rank+length}
As shown in Figure \ref{fig_length+rank}, we present the effects of intrinsic rank and prompt length across various datasets. The experimental results indicate that LAMP consistently performs strongly across different intrinsic ranks $r$ and prompt lengths $l$.


\subsection{Performance in T5-3B}
\label{app_T5-3B}
For baseline selection, we similarly chose baselines initialized from sampled vocabulary. Table \ref{tab_T5-3B} shows that LAMP performs best on the T5-3B model, further validating the importance of incorporating intrinsic semantic associations between prompt tokens to enhance knowledge representation capacity.
% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table}[t]
  \centering
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{0.25cm}
\begin{tabular}{l|cccc}
    \toprule
    \multirow{2}[2]{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{T5-3B}} \\
    \cline{2-5}
          & PT   & DEPT   & EPT   & LAMP  \\
    \midrule
    MultiRC & 78.00& 77.95 & 78.51 & 78.83 \\
    WiC & 71.16 & 71.16 & 73.67 & 74.92 \\
    RTE & 91.31 & 92.75 & 92.75 & 93.48 \\
    BoolQ & 87.22 & 87.65 & 87.89 & 87.83 \\
    \midrule
    \rowcolor{gray!25}
    Mean & 81.93 & 82.38 & 83.21 & 83.77 \\
    \bottomrule
    \end{tabular}%
\caption{The performance changes of different
methods at various datasets on the T5-3B. }
  \label{tab_T5-3B}%
  % \vspace{-10pt}
\end{table}%

\subsection{Change in LAMP Parameters}
\label{app_Parameters}
% \textbf{Change in LAMP Parameters from Intrinsic Rank.} 

% Table generated by Excel2LaTeX from sheet 'Sheet5'
\begin{table*}[t]
  \renewcommand{\arraystretch}{1}
  \setlength{\tabcolsep}{0.1cm}
  \centering
    \begin{tabular}{c|ccccc|ccccc}
    \toprule
    \multirow{2}[2]{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{T5-Large}   Prompt Length} & \multicolumn{5}{c}{\textbf{Llama2-7B}  Prompt Length} \\
          & 20    & 100   & 1000  & 5000  & 10000 & 20    & 100   & 1000  & 5000  & 10000 \\
    \midrule
          & \multicolumn{5}{c|}{ \#Trainable Params  } & \multicolumn{5}{c}{ \#Trainable Params} \\
    \midrule
    Full FT & 0.7B   & 0.7B  & 0.7B   & 0.7B   & 0.7B   & 7B   & 7B    & 7B    & 7B    & 7B  \\
    PT & 0.02M & 0.1M & 1.02M & 5.12M & 10.24M & 0.08M & 0.41M & 4.10M & 20.48M  & 40.96M \\
    \textbf{LAMP} & \textbf{0.008M} & \textbf{0.009M} & \textbf{0.016M} & \textbf{0.048M} & \textbf{0.088M} & \textbf{0.033M} & \textbf{0.034M} & \textbf{0.041M} & \textbf{0.073M} & \textbf{0.113M} \\
    \midrule
    Ratio & \blue{2.45}  & \blue{11.38}  & \blue{63.21}  & \blue{106.22}  & \blue{116.10}  & \blue{2.49}  & \blue{12.20}  & \blue{100.45}  & \blue{281.41}  & \blue{363.20}  \\
    \bottomrule
    \end{tabular}%
    \caption{Trainable parameters of different baselines varies with the prompt length. ``Ratio" denotes the multiple of trainable parameters in vanilla prompt tuning (PT) relative to LAMP.}
  \label{tab3}%
  % \vspace{-10pt}
\end{table*}%

\subsubsection{Change in LAMP Parameters from Model Size}
With intrinsic rank $r=8$, Figure \ref{fig_model_size} illustrates the effect of model size on LAMP trainable parameters. As the model size increases, trainable parameters in original PT increases significantly, whereas LAMP mitigates this issue. Table \ref{tab3} shows that when the prompt length $l$ and  intrinsic rank $r$ remain constant, the higher the model size, the more significant the reduction in trainable parameters achieved by LAMP. LAMP also mitigates the impact of the hidden dimension on trainable parameters.
\begin{figure}[t]
\centering
  \includegraphics[scale=0.26]{Fig/model_size.pdf}
    \caption{Trainable parameters with different model size. }
    \label{fig_model_size}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[t]
\centering
  \includegraphics[scale=0.21]{Fig/memory_model.pdf}
    \caption{Comparison of memory usage using different methods on various model scales. We leverage quantization operation on T5-11B.}
    \label{fig_memory}
  % \vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
\centering
  \includegraphics[scale=0.65]{Fig/other_datasets_tsne.pdf}
    \caption{The comparison of the dispersion of PT tokens on the other datasets in SuperGLUE benchmark before and after considering the intrinsic semantic associations between soft prompt tokens on T5-Base.}
    \label{fig_visual}
\end{figure*}

\subsubsection{Variation of LAMP Parameters}
% \subsection{In-depth Analysis (RQ3)}
\label{subsec:r3}
Table \ref{tab3} presents the variation of LAMP's trainable parameters with prompt length $l \in \{20, 100, 1000, 5000, 10000\}$ with $r=8$. 
% As PT-based methods are applied in other domains (recommendation systems, NLP), prompt length significantly increases \citep{doddapaneni2024user,liu2024can}. Therefore, it is crucial to investigate LAMP trainable parameters for long prompt lengths. 
The longer the prompt length, the more LAMP downplays the impact of prompt length on trainable parameters, making LAMP's advantages more evident. For instance, in the Llama2-7B model, when the prompt length is set to 10,000, the number of trainable parameters is reduced to just one-three-hundred-sixtieth of the original PT, significantly boosting computational efficiency. 
% This allows LAMP to incorporate richer task-specific knowledge and improves training efficiency. 
In summary, influenced by intrinsic rank $r$, prompt length $l$ , and model size, the LAMP's efficiency compared to PT and other PEFT methods can be further expanded. 

\subsubsection{Change in LAMP memory usage from Model Size}
We visualized the memory usage of LAMP across different model scales, with the T5-11B results reflecting quantization. As shown in Figure \ref{fig_memory}, LAMP consistently has the lowest memory usage and highest computational efficiency across various model sizes.



\subsection{Visualization of Intrinsic Semantic Associations}
\label{app_visualization}
We visualized the comparison of general PT before and after extracting the intrinsic semantic associations between soft prompt tokens on other datasets within the SuperGLUE benchmark. Figure \ref{fig_visual} shows that LAMP also exhibits apparent clustering on these datasets, enhancing semantic knowledge representation. 
% Although the clustering effect in Figure \ref{fig_visual}(i) appears promising, the more extensive range of the x and y-axis after t-SNE dimensionality reduction indicates that the distances between tokens are farther, resulting in higher dispersion. In contrast, Figure \ref{fig_visual}(j) shows less dispersion, suggesting tighter clustering of the tokens.



\end{document}