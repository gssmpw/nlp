[
  {
    "index": 0,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      },
      {
        "key": "zaken2021bitfit",
        "author": "Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav",
        "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models"
      },
      {
        "key": "shi2024dept",
        "author": "Zhengxiang Shi and Aldo Lipani",
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"
      },
      {
        "key": "ding2023sparse",
        "author": "Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong",
        "title": "Sparse Low-rank Adaptation of Pre-trained Language Models"
      },
      {
        "key": "zhu2023sira",
        "author": "Zhu, Yun and Wichers, Nevan and Lin, Chu-Cheng and Wang, Xinyi and Chen, Tianlong and Shu, Lei and Lu, Han and Liu, Canoee and Luo, Liangchen and Chen, Jindong and others",
        "title": "Sira: Sparse mixture of low rank adaptation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zaken2021bitfit",
        "author": "Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav",
        "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lialin2023scaling",
        "author": "Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna",
        "title": "Scaling down to scale up: A guide to parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhao2020masking",
        "author": "Zhao, Mengjie and Lin, Tao and Mi, Fei and Jaggi, Martin and Sch{\\\"u}tze, Hinrich",
        "title": "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mahabadi2021parameter",
        "author": "Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James",
        "title": "Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "karimi2021compacter",
        "author": "Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian",
        "title": "Compacter: Efficient low-rank hypercomplex adapter layers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "raffel2020exploring",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ruckle2020adapterdrop",
        "author": "R{\\\"u}ckl{\\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna",
        "title": "Adapterdrop: On the efficiency of adapters in transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "edalati2022krona",
        "author": "Edalati, Ali and Tahaei, Marzieh and Kobyzev, Ivan and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi",
        "title": "Krona: Parameter efficient tuning with kronecker adapter"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "meng2024pissa",
        "author": "Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan",
        "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "sung2022lst",
        "author": "Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit",
        "title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa and Liang, Percy",
        "title": "Prefix-tuning: Optimizing continuous prompts for generation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wang2023multitask",
        "author": "Wang, Zhen and Panda, Rameswar and Karlinsky, Leonid and Feris, Rogerio and Sun, Huan and Kim, Yoon",
        "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "asai2022attempt",
        "author": "Asai, Akari and Salehi, Mohammadreza and Peters, Matthew E and Hajishirzi, Hannaneh",
        "title": "Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "xiao2023decomposed",
        "author": "Xiao, Yao and Xu, Lu and Li, Jiaxi and Lu, Wei and Li, Xiaoli",
        "title": "Decomposed Prompt Tuning via Low-Rank Reparameterization"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "shi2024dept",
        "author": "Zhengxiang Shi and Aldo Lipani",
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "xiao2023decomposed",
        "author": "Xiao, Yao and Xu, Lu and Li, Jiaxi and Lu, Wei and Li, Xiaoli",
        "title": "Decomposed Prompt Tuning via Low-Rank Reparameterization"
      }
    ]
  }
]