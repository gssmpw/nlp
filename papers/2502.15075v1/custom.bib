% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@article{liu-etal-2024-lost,
    title = "Lost in the Middle: How Language Models Use Long Contexts",
    author = "Liu, Nelson F.  and
      Lin, Kevin  and
      Hewitt, John  and
      Paranjape, Ashwin  and
      Bevilacqua, Michele  and
      Petroni, Fabio  and
      Liang, Percy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.9",
    doi = "10.1162/tacl_a_00638",
    pages = "157--173",
    abstract = "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",
}

@inproceedings{press-etal-2023-measuring,
    title = "Measuring and Narrowing the Compositionality Gap in Language Models",
    author = "Press, Ofir  and
      Zhang, Muru  and
      Min, Sewon  and
      Schmidt, Ludwig  and
      Smith, Noah  and
      Lewis, Mike",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.378",
    doi = "10.18653/v1/2023.findings-emnlp.378",
    pages = "5687--5711",
    abstract = "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask{'}s structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",
}

@misc{xu2024rereadingimprovesreasoninglarge,
      title={Re-Reading Improves Reasoning in Large Language Models}, 
      author={Xiaohan Xu and Chongyang Tao and Tao Shen and Can Xu and Hongbo Xu and Guodong Long and Jian-guang Lou and Shuai Ma},
      year={2024},
      eprint={2309.06275},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.06275}, 
}

@misc{yasunaga2024largelanguagemodelsanalogical,
      title={Large Language Models as Analogical Reasoners}, 
      author={Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and Jure Leskovec and Percy Liang and Ed H. Chi and Denny Zhou},
      year={2024},
      eprint={2310.01714},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01714}, 
}
@misc{gupta2024biasrunsdeepimplicit,
      title={Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs}, 
      author={Shashank Gupta and Vaishnavi Shrivastava and Ameet Deshpande and Ashwin Kalyan and Peter Clark and Ashish Sabharwal and Tushar Khot},
      year={2024},
      eprint={2311.04892},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.04892}, 
}
@misc{madaan2022textpatternseffectivechain,
      title={Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango}, 
      author={Aman Madaan and Amir Yazdanbakhsh},
      year={2022},
      eprint={2209.07686},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.07686}, 
}
@misc{su2023learningredteaminggender,
      title={Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models}, 
      author={Hsuan Su and Cheng-Chu Cheng and Hua Farn and Shachi H Kumar and Saurav Sahay and Shang-Tse Chen and Hung-yi Lee},
      year={2023},
      eprint={2310.11079},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11079}, 
}
@misc{koo2024benchmarkingcognitivebiaseslarge,
      title={Benchmarking Cognitive Biases in Large Language Models as Evaluators}, 
      author={Ryan Koo and Minhwa Lee and Vipul Raheja and Jong Inn Park and Zae Myung Kim and Dongyeop Kang},
      year={2024},
      eprint={2309.17012},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17012}, 
}
@misc{echterhoff2024cognitivebiashighstakesdecisionmaking,
      title={Cognitive Bias in High-Stakes Decision-Making with LLMs}, 
      author={Jessica Echterhoff and Yao Liu and Abeer Alessa and Julian McAuley and Zexue He},
      year={2024},
      eprint={2403.00811},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.00811}, 
}
@article{NGUYEN2024100971,
title = {Human bias in AI models? Anchoring effects and mitigation strategies in large language models},
journal = {Journal of Behavioral and Experimental Finance},
volume = {43},
pages = {100971},
year = {2024},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2024.100971},
url = {https://www.sciencedirect.com/science/article/pii/S2214635024000868},
author = {Jeremy K. Nguyen},
keywords = {Anchoring bias, Artificial intelligence},
abstract = {This study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.}
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{schaeffer2023emergentabilitieslargelanguage,
      title={Are Emergent Abilities of Large Language Models a Mirage?}, 
      author={Rylan Schaeffer and Brando Miranda and Sanmi Koyejo},
      year={2023},
      eprint={2304.15004},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.15004}, 
}

@article{NGUYEN2024100971,
title = {Human bias in AI models? Anchoring effects and mitigation strategies in large language models},
journal = {Journal of Behavioral and Experimental Finance},
volume = {43},
pages = {100971},
year = {2024},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2024.100971},
url = {https://www.sciencedirect.com/science/article/pii/S2214635024000868},
author = {Jeremy K. Nguyen},
keywords = {Anchoring bias, Artificial intelligence},
abstract = {This study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.}
}

@techreport{bick2024rapid,
  title={The Rapid Adoption of Generative AI},
  author={Bick, Alexander and Blandin, Adam and Deming, David J},
  year={2024},
  institution={National Bureau of Economic Research}
}

@misc{kim2024personadoubleedgedswordenhancing,
      title={Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts}, 
      author={Junseok Kim and Nakyeong Yang and Kyomin Jung},
      year={2024},
      eprint={2408.08631},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.08631}, 
}

@misc{dong2024personasettingpitfallpersistent,
      title={Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption}, 
      author={Wenchao Dong and Assem Zhunis and Dongyoung Jeong and Hyojin Chin and Jiyoung Han and Meeyoung Cha},
      year={2024},
      eprint={2409.03843},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.03843}, 
}

@misc{sakib2024challengingfairnesscomprehensiveexploration,
      title={Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations}, 
      author={Shahnewaz Karim Sakib and Anindya Bijoy Das},
      year={2024},
      eprint={2409.10825},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2409.10825}, 
}

@article{10.1145/3597307,
author = {Navigli, Roberto and Conia, Simone and Ross, Bj\"{o}rn},
title = {Biases in Large Language Models: Origins, Inventory, and Discussion},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3597307},
doi = {10.1145/3597307},
abstract = {In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {10},
numpages = {21},
keywords = {Bias in NLP, language models}
}

@article{gallegos-etal-2024-bias,
    title = "Bias and Fairness in Large Language Models: A Survey",
    author = "Gallegos, Isabel O.  and
      Rossi, Ryan A.  and
      Barrow, Joe  and
      Tanjim, Md Mehrab  and
      Kim, Sungchul  and
      Dernoncourt, Franck  and
      Yu, Tong  and
      Zhang, Ruiyi  and
      Ahmed, Nesreen K.",
    journal = "Computational Linguistics",
    volume = "50",
    number = "3",
    month = sep,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-3.8",
    doi = "10.1162/coli_a_00524",
    pages = "1097--1179",
    abstract = "Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
}

@techreport{NBERw32966,
 title = "The Rapid Adoption of Generative AI",
 author = "Bick, Alexander and Blandin, Adam and Deming, David J",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "32966",
 year = "2024",
 month = "September",
 doi = {10.3386/w32966},
 URL = "http://www.nber.org/papers/w32966",
 abstract = {Generative Artificial Intelligence (AI) is a potentially important new technology, but its impact on the economy depends on the speed and intensity of adoption. This paper reports results from the first nationally representative U.S. survey of generative AI adoption at work and at home. In August 2024, 39 percent of the U.S. population age 18-64 used generative AI. More than 24 percent of workers used it at least once in the week prior to being surveyed, and nearly one in nine used it every workday. Historical data on usage and mass-market product launches suggest that U.S. adoption of generative AI has been faster than adoption of the personal computer and the internet. Generative AI is a general purpose technology, in the sense that it is used in a wide range of occupations and job tasks at work and at home.},
}


@article{10.2307/1884852,
    author = {Simon, Herbert A.},
    title = "{A Behavioral Model of Rational Choice}",
    journal = {The Quarterly Journal of Economics},
    volume = {69},
    number = {1},
    pages = {99-118},
    year = {1955},
    month = {02},
    abstract = "{Introduction, 99. — I. Some general features of rational choice, 100.— II. The essential simplifications, 103. — III. Existence and uniqueness of solutions, 111. — IV. Further comments on dynamics, 113. — V. Conclusion, 114. — Appendix, 115.}",
    issn = {0033-5533},
    doi = {10.2307/1884852},
    url = {https://doi.org/10.2307/1884852},
    eprint = {https://academic.oup.com/qje/article-pdf/69/1/99/5413375/69-1-99.pdf},
}

@article{KAHNEMAN1972430,
title = {Subjective probability: A judgment of representativeness},
journal = {Cognitive Psychology},
volume = {3},
number = {3},
pages = {430-454},
year = {1972},
issn = {0010-0285},
doi = {https://doi.org/10.1016/0010-0285(72)90016-3},
url = {https://www.sciencedirect.com/science/article/pii/0010028572900163},
author = {Daniel Kahneman and Amos Tversky},
abstract = {This paper explores a heuristic—representativeness—according to which the subjective probability of an event, or a sample, is determined by the degree to which it: (i) is similar in essential characteristics to its parent population; and (ii) reflects the salient features of the process by which it is generated. This heuristic is explicated in a series of empirical examples demonstrating predictable and systematic errors in the evaluation of uncertain events. In particular, since sample size does not represent any property of the population, it is expected to have little or no effect on judgment of likelihood. This prediction is confirmed in studies showing that subjective sampling distributions and posterior probability judgments are determined by the most salient characteristic of the sample (e.g., proportion, mean) without regard to the size of the sample. The present heuristic approach is contrasted with the normative (Bayesian) approach to the analysis of the judgment of uncertainty.}
}

@article{KahnemanTversky1974,
author = {Amos Tversky  and Daniel Kahneman },
title = {Judgment under Uncertainty: Heuristics and Biases},
journal = {Science},
volume = {185},
number = {4157},
pages = {1124-1131},
year = {1974},
doi = {10.1126/science.185.4157.1124},
URL = {https://www.science.org/doi/abs/10.1126/science.185.4157.1124},
eprint = {https://www.science.org/doi/pdf/10.1126/science.185.4157.1124},
abstract = {This article described three heuristics that are employed in making judgments under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgments and decisions in situations of uncertainty.}
}

@article{FURNHAM201135,
title = {A literature review of the anchoring effect},
journal = {The Journal of Socio-Economics},
volume = {40},
number = {1},
pages = {35-42},
year = {2011},
issn = {1053-5357},
doi = {https://doi.org/10.1016/j.socec.2010.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1053535710001411},
author = {Adrian Furnham and Hua Chu Boo},
keywords = {Anchoring effects, Individual differences, Rewards},
abstract = {The anchoring effect is one of the most robust cognitive heuristics. This paper reviews the literature in this area including various different models, explanations and underlying mechanisms used to explain anchoring effects. The anchoring effect is both robust and has many implications in all decision making processes. This review paper documents the many different domains and tasks in which the effect has been shown. It also considers mood and individual difference (ability, personality, information styles) correlates of anchoring as well as the effect of motivation and knowledge on decisions affected by anchoring. Finally the review looks at the applicants of the anchoring effects in everyday life.}
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}



@misc{zhou2023leasttomostpromptingenablescomplex,
      title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models}, 
      author={Denny Zhou and Nathanael Schärli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc Le and Ed Chi},
      year={2023},
      eprint={2205.10625},
      archivePrefix={arXiv},
      primaryClass={cs.AI},}
@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}
@misc{ssa_babynames,
  author       = {{Social Security Administration}},
  title        = {Top Names of the 1900s},
  year         = {n.d.},
  url          = {https://www.ssa.gov/oact/babynames/decades/century.html},
  note         = {Accessed: 2024-10-08}
}

@misc{suri2023largelanguagemodelsdecision,
      title={Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5}, 
      author={Gaurav Suri and Lily R. Slater and Ali Ziaee and Morgan Nguyen},
      year={2023},
      eprint={2305.04400},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2305.04400}, 
}

@inproceedings{bai-etal-2024-longbench,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.172",
    doi = "10.18653/v1/2024.acl-long.172",
    pages = "3119--3137",
    abstract = "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs{'} long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",
}


@article{JacowitzKahneman1995,
author = {Karen E. Jacowitz and Daniel Kahneman},
title ={Measures of Anchoring in Estimation Tasks},
journal = {Personality and Social Psychology Bulletin},
volume = {21},
number = {11},
pages = {1161-1166},
year = {1995},
doi = {10.1177/01461672952111004},
URL = {https://doi.org/10.1177/01461672952111004},
eprint = {https://doi.org/10.1177/01461672952111004},
abstract = {The authors describe a method for the quantitative study of anchoring effects in estimation tasks. A calibration group provides estimates of a set of uncertain quantities. Subjects in the anchored condition first judge whether a specified number (the anchor) is higher or lower than the true value before estimating each quantity. The anchors are set at predetermined percentiles of the distribution of estimates in the calibration group (15th and 85th percentiles in this study). This procedure permits the transformation of anchored estimates into percentiles in the calibration group, allows pooling of results across problems, and provides a natural measure of the size of the effect. The authors illustrate the method by a demonstration that the initial judgment of the anchor is susceptible to an anchoring-like bias and by an analysis of the relation between anchoring and subjective confidence. },
}

@article{Critcheretal2008,
author = {Critcher, Clayton R. and Gilovich, Thomas},
title = {Incidental environmental anchors},
journal = {Journal of Behavioral Decision Making},
volume = {21},
number = {3},
pages = {241-251},
keywords = {basic anchoring, numeric priming, incidental anchoring, accessibility},
doi = {https://doi.org/10.1002/bdm.586},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bdm.586},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bdm.586},
abstract = {Abstract Three studies examined whether potential anchor values that are incidentally present in the environment can affect a person's numerical estimates. In Study 1, estimates of an athlete's performance were influenced by the number on his jersey. In Study 2, estimates of the proportion of sales in the domestic market were influenced by a product's model number. In Study 3, participants' estimates of how much they would spend at a restaurant were influenced by whether the restaurant was named “Studio 17” or “Studio 97.” These effects were not qualified by participants' expertise in the relevant domain (Study 1) or by their ability to subsequently recall the anchor value (Study 3). These findings document the existence of a new form of “basic anchoring” and suggest that not all basic anchoring effects are as fragile as the existing anchoring literature suggests. Copyright © 2007 John Wiley \& Sons, Ltd.},
year = {2008}
}

@article{Sherif_Taub_Hovland_1958, 
title={Assimilation and contrast effects of anchoring stimuli on judgments.}, volume={55}, 
DOI={10.1037/h0048784}, 
number={2}, 
journal={Journal of Experimental Psychology}, 
author={Sherif, Muzafer and Taub, Daniel and Hovland, Carl I.}, 
year={1958}, 
pages={150–155}
} 

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@ARTICLE{9361255,
  author={Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
  journal={IEEE Micro}, 
  title={NVIDIA A100 Tensor Core GPU: Performance and Innovation}, 
  year={2021},
  volume={41},
  number={2},
  pages={29-35},
  keywords={Graphics processing units;Tensors;Bandwidth;Throughput;Parallel processing;Benchmark testing;Artificial intelligence;GPU;A100;NVLink;Deep Learning;Tensor Core;CUDA;C++20},
  doi={10.1109/MM.2021.3061394}}

@ARTICLE{10070122,
  author={Choquette, Jack},
  journal={IEEE Micro}, 
  title={NVIDIA Hopper H100 GPU: Scaling Performance}, 
  year={2023},
  volume={43},
  number={3},
  pages={9-17},
  keywords={Graphics processing units;Instruction sets;Tensors;Memory management;Artificial intelligence;Transforms;Bandwidth},
  doi={10.1109/MM.2023.3256796}}

@misc{geminiteam2024geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={{Gemini Team} and Rohan Anil et al.},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{openai2024reasoning,
  author = {{OpenAI}},
  title = {Learning to reason with LLMs},
  year = {2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}

@misc{kamruzzaman2024promptingtechniquesreducingsocial,
      title={Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes}, 
      author={Mahammed Kamruzzaman and Gene Louis Kim},
      year={2024},
      eprint={2404.17218},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.17218}, 
}

@misc{dong2024ithemfluididentities,
      title={I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models}, 
      author={Wenchao Dong and Assem Zhunis and Hyojin Chin and Jiyoung Han and Meeyoung Cha},
      year={2024},
      eprint={2402.10436},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.10436}, 
}


@misc{aher2023usinglargelanguagemodels,
      title={Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies}, 
      author={Gati Aher and Rosa I. Arriaga and Adam Tauman Kalai},
      year={2023},
      eprint={2208.10264},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.10264},
}
@misc{haim2024whatsnameauditinglarge,
      title={What's in a Name? Auditing Large Language Models for Race and Gender Bias}, 
      author={Amit Haim and Alejandro Salinas and Julian Nyarko},
      year={2024},
      eprint={2402.14875},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14875}, 
}
@misc{arzaghi2024understandingintrinsicsocioeconomicbiases,
      title={Understanding Intrinsic Socioeconomic Biases in Large Language Models}, 
      author={Mina Arzaghi and Florian Carichon and Golnoosh Farnadi},
      year={2024},
      eprint={2405.18662},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.18662}, 
}

@misc{wang2024reallindastandupto,
      title={Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs}, 
      author={Pengda Wang and Zilin Xiao and Hanjie Chen and Frederick L. Oswald},
      year={2024},
      eprint={2404.01461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.01461}, 
}

@misc{moore2024reasoningbiasstudycounterfactual,
      title={Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning}, 
      author={Kyle Moore and Jesse Roberts and Thao Pham and Douglas Fisher},
      year={2024},
      eprint={2408.08651},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.08651}, 
}

@misc{ross2024llmeconomicusmappingbehavioral,
      title={LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory}, 
      author={Jillian Ross and Yoon Kim and Andrew W. Lo},
      year={2024},
      eprint={2408.02784},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.02784}, 
}
@article{brown2020language,
  author = {Brown, T. M. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Anguilm, K. and Desai, C. and Ledbetter, M. and Wang, H. and Polkosky, T. and Ritter, S. and Hampton, C. and Dathathreya, S. and Nie, A. and Nie, J. and Morgan, N. and Das, R. and Gao, D. and Mehrotra, S. and Fodor, P. and Civin, L.},
  title = {Language Models are Few-Shot Learners},
  year = {2020},
  eprint = {arXiv:2005.14165},
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{baker2002psychological,
  title={Psychological biases of investors},
  author={Baker, H. Kent and Nofsinger, John R.},
  journal={Financial Services Review},
  volume={11},
  number={2},
  pages={97--116},
  year={2002}
}


@incollection{haselton2015evolution,
  title = {The Evolution of Cognitive Bias},
  author = {Martie G. Haselton and Daniel Nettle and Damian R. Murray},
  booktitle = {The Handbook of Evolutionary Psychology},
  year = {2015},
  publisher = {Wiley},
  doi = {10.1002/9781119125563.evpsych241},
  url = {https://doi.org/10.1002/9781119125563.evpsych241}
}

@article{osullivan2018cognitive,
  title = {Cognitive Bias in Clinical Medicine},
  author = {ED O'Sullivan and SJ Schofield},
  journal = {Journal of the Royal College of Physicians of Edinburgh},
  volume = {48},
  number = {3},
  pages = {306--312},
  year = {2018},
  doi = {10.4997/jrcpe.2018.306},
  url = {https://doi.org/10.4997/jrcpe.2018.306}
}

@MISC{20.500.13051/3188,
	year = {2002},
	author = {Eskridge, William and Ferejohn, John},
	note = {The most interesting issues of public law (for us) are those relating to institutional design and function. When thinking about statutory interpretation, judicial review, and legislative and administrative procedures, it is useful to have a theory about how the governmental system works in our regulatory state, how it breaks down, and how it leads to decisions that do not serve the public interest. Hence, theories of regulatory pathology are useful. Within the academy, public choice theory has been particularly popular: selfish interest groups and public officials highjack the governmental process for their private gain, thereby undermining the public interest in efficient rules and distributions. The main regulatory pathology for public choice theory is rent-seeking, the private plunder of the public fisc. Republican theory offers a less cynical point of view. It maintains that politics is the forum where collective problems are resolved and values are advanced. The main regulatory pathology for republican theory is breakdowns in the deliberative process.},
	title = {Structuring Lawmaking to Reduce Cognitive Bias: A Critical View},
	url = {http://hdl.handle.net/20.500.13051/3188}
}

@article{weintein2002cognitive,
  title = {Don't Believe Everything You Think: Cognitive Bias in Legal Decision Making},
  author = {Ian Weinstein},
  journal = {Clinical Law Review},
  volume = {8},
  pages = {783},
  year = {2002},
  publisher = {Fordham University School of Law},
  url = {https://ir.lawnet.fordham.edu/faculty_scholarship/422}
}

@article{sawusch1977anchoring,
  title = {Anchoring effects and vowel discrimination},
  author = {J. R. Sawusch and H. C. Nusbaum},
  journal = {Journal of the Acoustical Society of America},
  volume = {62},
  number = {S1},
  pages = {S100--S101},
  year = {1977},
  doi = {10.1121/1.2016010},
  url = {https://doi.org/10.1121/1.2016010}
}

@ARTICLE{Monajemi, 
author = {Dargahi, Helen and Monajemi, Alireza and Soltani, Akbar and Hossein Nejad Nedaie, Hooman and labaf, Ali and },  
title = {Anchoring Errors in Emergency Medicine Residents and Faculties}, 
volume = {36}, 
number = {1},  
abstract ={Background: Clinical reasoning is the basis of all clinical activities in the health team, and diagnostic reasoning is perhaps the most critical of a physician&#39;s skills. Despite many advances, medical errors have not been reduced. Studies have shown that most diagnostic errors made in emergency rooms are cognitive errors, and anchoring error was identified as the most common cognitive error in clinical settings. This research intends to determine the frequency and compare the percentage of anchoring bias perceived among faculty members versus residents in the emergency medicine department.     Methods: In this quasi-experimental study, Emergency Medicine&#39;s Faculties and Residents are evaluated in clinical reasoning by nine written clinical cases. The clinical data for each clinical case was presented to the participants over three pages, based on receiving clinical and para-clinical information in real situations. At the end of each page, participants were asked to write up diagnoses. Data were analyzed using one-way ANOVA test.  The SPSS software (Version 16.0) was employed to conduct statistical tests, and a P value &lt; 0.05 was considered to be statistically significant.     Results: Seventy-seven participants of the residency program in the Emergency Medical group volunteered to participate in this study. Data showed Faculties were significantly higher in writing correct diagnoses than residents (66% vs. 41%), but the anchoring error ratio was significantly lower in residents (33% vs. 75%). In addition, the number of written diagnoses, time for writing diagnoses, and Clinical experience in faculties and residents were compared.     Conclusion: Findings showed that increasing clinical experience increased diagnostic accuracy and changed cognitive medical errors. Faculties were higher than residents in anchoring error ratio. This error could be the result of more exposure and more decision-making in the mode of heuristic or intuitive thinking in faculties. },  
URL = {http://mjiri.iums.ac.ir/article-1-6059-en.html},  
eprint = {http://mjiri.iums.ac.ir/article-1-6059-en.pdf},  
journal = {Medical Journal of the Islamic Republic Of Iran},   
doi = {10.47176/mjiri.36.124},  
year = {2022}  
}

@inproceedings{10.1145/2766462.2767841,
author = {Shokouhi, Milad and White, Ryen and Yilmaz, Emine},
title = {Anchoring and Adjustment in Relevance Estimation},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766462.2767841},
doi = {10.1145/2766462.2767841},
abstract = {People's tendency to overly rely on prior information has been well studied in psychology in the context of anchoring and adjustment. Anchoring biases pervade many aspects of human behavior. In this paper, we present a study of anchoring bias in information retrieval~(IR) settings. We provide strong evidence of anchoring during the estimation of document relevance via both human relevance judging and in natural user behavior collected via search log analysis. In particular, we show that sequential relevance judgment of documents collected for the same query could be subject to anchoring bias. That is, the human annotators are likely to assign different relevance labels to a document, depending on the quality of the last document they had judged for the same query.In addition to manually assigned labels, we further show that the implicit relevance labels inferred from click logs can also be affected by anchoring bias. Our experiments over the query logs of a commercial search engine suggested that searchers' interaction with a document can be highly affected by the documents visited immediately beforehand. Our findings have implications for the design of search systems and judgment methodologies that consider and adapt to anchoring effects.},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {963–966},
numpages = {4},
keywords = {anchoring, implicit relevance labels, offline evaluation, priming},
location = {Santiago, Chile},
series = {SIGIR '15}
}


@misc{perez2022ignorepreviouspromptattack,
      title={Ignore Previous Prompt: Attack Techniques For Language Models}, 
      author={Fábio Perez and Ian Ribeiro},
      year={2022},
      eprint={2211.09527},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.09527}, 
}

@article{Campbell_Sharpe_2009, title={Anchoring Bias in Consensus Forecasts and Its Effect on Market Prices}, 
volume={44}, 
DOI={10.1017/S0022109009090127}, 
number={2}, 
journal={Journal of Financial and Quantitative Analysis}, 
author={Campbell, Sean D. and Sharpe, Steven A.}, 
year={2009}, 
pages={369–390}
}

@inproceedings{NEURIPS2022_8bb0d291,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{agrawal2024multilingualevaluationlongcontext,
      title={Multilingual Evaluation of Long Context Retrieval and Reasoning}, 
      author={Ameeta Agrawal and Andy Dang and Sina Bagheri Nezhad and Rhitabrat Pokharel and Russell Scheinberg},
      year={2024},
      eprint={2409.18006},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.18006}, 
}

@inproceedings{lin-ng-2023-mind,
    title = "Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting",
    author = "Lin, Ruixi  and Ng, Hwee Tou",
    editor = "Rogers, Anna  and Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.324",
    doi = "10.18653/v1/2023.findings-acl.324",
    pages = "5269--5281",
    abstract = "We advocate the importance of exposing uncertainty on results of language model prompting which display bias modes resembling cognitive biases, and propose to help users grasp the level of uncertainty via simple quantifying metrics. Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in downstream tasks that are centered around people{'}s lives if users trust their results too much. In this work, we reveal two bias modes leveraging cognitive biases when we prompt BERT, accompanied by two bias metrics. On a drug-drug interaction extraction task, our bias measurements reveal an error pattern similar to the availability bias when the labels for training prompts are imbalanced, and show that a toning-down transformation of the drug-drug description in a prompt can elicit a bias similar to the framing effect, warning users to distrust when prompting language models for answers.",
}

@misc{bianchi2024llmsnegotiatenegotiationarenaplatform,
      title={How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis}, 
      author={Federico Bianchi and Patrick John Chia and Mert Yuksekgonul and Jacopo Tagliabue and Dan Jurafsky and James Zou},
      year={2024},
      eprint={2402.05863},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.05863}, 
}

@misc{kaneko2024evaluatinggenderbiaslarge,
      title={Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting}, 
      author={Masahiro Kaneko and Danushka Bollegala and Naoaki Okazaki and Timothy Baldwin},
      year={2024},
      eprint={2401.15585},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.15585}, 
}

@article{ji2024enhancing,
  title={Enhancing Equity in Large Language Models for Medical Applications},
  author={Ji, Yuelyue and Ma, Wenhe and Sivarajkumar, Sonish and Zhang, Hang and Sadhu, Eugene Mathew and Li, Zhuochun and Wu, Xizhi and Visweswaran, Shyam and Wang, Yanshan},
  journal={arXiv preprint arXiv:2410.05180},
  year={2024},
  url={https://arxiv.org/pdf/2410.05180v1}
}

@article{huang2024unveiling,
  title={Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education as an Example},
  author={Huang, Yuanning},
  journal={arXiv preprint arXiv:2409.09652},
  year={2024},
  url={https://arxiv.org/abs/2409.09652}
}

@misc{ranjan2024earlyreviewgenderbias,
      title={Early review of Gender Bias of OpenAI o1-mini: Higher Intelligence of LLM does not necessarily solve Gender Bias and Stereotyping issues}, 
      author={Rajesh Ranjan and Shailja Gupta and Surya Naranyan Singh},
      year={2024},
      eprint={2409.19959},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2409.19959}, 
}

@article{mirza2024evaluating,
  title={Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios},
  author={Mirza, Vishal and Kulkarni, Rahul and Jadhav, Aakanksha},
  journal={arXiv preprint arXiv:2409.14583},
  year={2024},
  url={https://arxiv.org/pdf/2409.14583}
}


