\section{Related Work}
\label{sec:related}

\paragraph{Model Quantization.} 
 for large models such as LLMs **Judd et al., "Neural Networks and Deep Learning"** __**, which is why post-training quantization (PTQ) techniques are generally preferred for quantizing these models.

Quantization reduces the computational cost of neural network inference by lowering model bit-precision **Hubara et al., "Quantized Neural Networks: Training Deep Neural Networks with Low-Precision Weights and Activations"** __. It can be divided into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ requires no re-training or labeled data **Jacob et al., "Quantization and Training of Neural Networks for Efficient Image Classification"** __ and is preferred for large models like LLMs, whereas QAT demands fine-tuning with labeled data **Gupta et al., "Deep Learning with Limited Numerical Precision"** __ but scales poorly for such models ____.

\paragraph{LLMs Quantization.} 


Researchers consider three post-training quantization settings for LLMs: weight-only, weight-activation, and KV cache quantization. Weight-only quantization focuses solely on weights **Courbariaux et al., "Training Deep Neural Networks with Low Precision Multiplication"** __, while weight-activation quantization targets both weights and activations **Chen et al., "Learning to Quantize Neural Networks by Empirical Risk Minimization"** __. Since longer inputs inflate the KV cacheâ€™s memory overhead **Hou et al., "Quantizing Deep Neural Networks with Limited Numerical Precision"** __, KV cache quantization offers memory savings similar to weight-activation methods while maintaining performance near that of weight-only quantization ____.

\paragraph{KV Cache Quantization.} 

Recent KV cache quantization methods reduce LLM memory and accelerate inference, falling into three categories: outlier redistribution, fixed-precision, and mixed-precision **Park et al., "Mixed Precision Quantization for Deep Neural Networks"** ____. Outliers in KV cache are addressed by redistributing them or applying smoothing transformations **Zhou et al., "Quantizing Neural Networks with Smoothing Transformations"** __. Fixed-precision methods use a uniform bit-width **Sun et al., "Fixed-Precision Quantization of Neural Networks for Efficient Inference"** __ but can overlook token importance and outliers. In contrast, mixed-precision allocates higher precision to critical tokens and lower precision elsewhere ____.
Existing methods like SKVQ and QAQ rely on specific engineering solutions or predefined parameters, whereas \mn{} uses a principled, analysis-driven framework that leverages the intrinsic properties of key and value caches for a more robust and adaptable quantization approach.