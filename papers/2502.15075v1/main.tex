% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\DeclareUnicodeCharacter{FF0C}{,}

% added 
\usepackage{amsmath, amssymb}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{afterpage}
\usepackage{enumitem}
\usepackage[dvipsnames]{xcolor}
\usepackage{url}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\usepackage{natbib}



% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}  % For better table rules (toprule, midrule, bottomrule)
\usepackage{tcolorbox}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{longtable}

%figures
\usepackage{float}
\usepackage{stfloats}
\usepackage{placeins}


\title{More for Keys, Less for Values: Adaptive KV Cache Quantization}



\author{
  Mohsen Hariri$^{1}$, 
  Lam Nguyen$^{1}$, 
  Sixu Chen$^{1}$, 
  Shaochen Zhong$^{2}$, 
  Qifan Wang$^{3}$, \\
  \textbf{Xia Hu}$^{2}$, 
  \textbf{Xiaotian Han}$^{1}$, 
  \textbf{Vipin Chaudhary}$^{1}$ \\[1ex]
  ${}^{1}$Case Western Reserve University; 
  ${}^{2}$Rice University; 
  ${}^{3}$Meta \\[1ex]
  \texttt{mohsen.hariri@case.edu}
}



% % Author information can be set in various styles:
% \author{Mohsen Hariri$^{1}$, Lam Nguyen$^{1}$, Sixu Chen$^{1}$, Shaochen Zhong$^{2}$, Qifan Wang$^{3}$, \\ Xia Hu$^{1}$， Xiaotian Han$^{1}$, Vipin Chaudhary$^{1}$
% % wqfcr@meta.com
% }


\def\mn{{\em KV-AdaQuant}}




\begin{document}
\maketitle


\begin{abstract}
This paper introduces an information-aware quantization framework that adaptively compresses the key-value (KV) cache in large language models (LLMs). Although prior work has underscored the distinct roles of key and value cache during inference, our systematic analysis—examining singular value distributions, spectral norms, and Frobenius norms—reveals, for the first time, that key matrices consistently exhibit higher norm values and are more sensitive to quantization than value matrices. Furthermore, our theoretical analysis shows that matrices with higher spectral norms amplify quantization errors more significantly. Motivated by these insights, we propose a mixed-precision quantization strategy, \mn{}, which allocates more bit-width for keys and fewer for values since key matrices have higher norm values. With the same total KV bit budget, this approach effectively mitigates error propagation across transformer layers while achieving significant memory savings. Our extensive experiments on multiple LLMs (1B - 70B) demonstrate that our mixed-precision quantization scheme maintains high model accuracy even under aggressive compression. For instance, using 4-bit for Key and 2-bit for Value achieves an accuracy of 75.2\%, whereas reversing the assignment—2-bit for Key and 4-bit for Value—yields only 54.7\% accuracy. The code is available at \url{https://tinyurl.com/kv-adaquant}.
\end{abstract}

 % \url{https://anonymous.4open.science/r/KV-AdaQuant-8F58}.

% For example,  
% we assign a higher bit-width (e.g., 4-bit) to keys and a lower bit-width (e.g., 2-bit) to values. 
% We validate the effectiveness of \mn{} through extensive experiments on multiple transformer architectures (1B - 70B) and by analyzing quantization error across various bit-width settings (from 2-bit to 8-bit). 


\section{Introduction}


\begin{figure}[t]
    \centering
    \begin{subfigure}[h]{0.27\textwidth}
        \centering
        \includegraphics[width=\linewidth]{./figures/key_value_statistics.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.20\textwidth}
        \centering
        \resizebox{!}{0.60in}{ 
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{lcc}
         \toprule
         Size & {\color{NavyBlue}K}\textsubscript{2}{\color{orange}V}\textsubscript{4} & \cellcolor[gray]{0.9}{\color{NavyBlue}K}\textsubscript{4}{\color{orange}V}\textsubscript{2} \\
         \midrule
         1B   & 0.06 & \cellcolor[gray]{0.9}0.34 \\
         8B   & 0.55 & \cellcolor[gray]{0.9}0.75 \\
         14B  & 0.78 & \cellcolor[gray]{0.9}0.91 \\
         14B  & 0.76 & \cellcolor[gray]{0.9}0.87 \\
         \bottomrule
        \end{tabular}
        }
        \vspace{10pt}
    \end{subfigure}
    \vspace{-8pt}
    \caption{\textbf{Key cache needs more bits.} (\textbf{Left}):
Spectral norms of the key cache (\textcolor{NavyBlue}{blue}) and value cache (\textcolor{orange}{orange}) across layers in Llama3.3-70B show that key caches consistently exhibit higher norms. 
(\textbf{Right}): GSM8k accuracy for two schemes: \(\text{K}_{2}\text{V}_{4}\), representing 2-bit allocation for the K cache and 4-bit allocation for the V cache and \(\text{K}_{4}\text{V}_{2}\), representing 4-bit allocation for the K cache and 2-bit allocation for the V cache, demonstrates that allocating more bits to the key cache maintains strong performance, confirming the efficacy of norm-aware, mixed-precision quantization.}
    \label{fig:intro}
\end{figure}




Large language models (LLMs) have advanced NLP by enabling parallel processing and capturing complex dependencies \cite{transformers, seq2seq}, which have scaled from GPT-1’s 117M \cite{gpt1}, GPT-2’s 1.5B \cite{gpt2} to GPT-3’s 175B \cite{gpt3} and GPT-4’s 1.8T parameters \cite{gpt4}. Open-source models like Llama have grown to 405B~\cite{Llama2}, with Mistral Large (123B) \cite{mistrallarge2} and DeepSeek V3 (671B) \cite{deepseekv3} also emerging. In parallel with increasing parameter counts, longer context windows are becoming essential for chain-of-thought reasoning, necessitating an expansion of the KV cache and contributing to memory bottlenecks  \cite{cot}. New systems support context windows of up to 1M tokens \cite{geminiflash2}, 200k tokens \cite{openaio1, openaio3mini}, and 130k tokens \cite{deepseekr1}. NVIDIA’s Blackwell architecture introduces low-precision formats like FP8, FP4, and INT4 \cite{nvidiablackwell}, emphasizing the need for precise quantization methods. The KV cache is a critical component in these models and its effective operation is paramount and significant.


To accommodate larger context windows, the KV cache can become a significant memory bottleneck. One effective solution is to quantize the key and value caches from high-precision formats (e.g., FP32 or BF16) to lower-precision ones (e.g., INT4 or INT2). However, we first discovered that the Key cache generally exhibits higher norms than the Value cache, making it more sensitive to uniform quantization. Since matrices with larger norms are more prone to quantization errors, bit-width allocation should be tailored to each matrix’s norm distribution. To confirm this, we conducted experiments with Llama3.3-70B (see Figures~\ref{fig:intro} (Left) and~\ref{fig:frobenius_norm}), as the Key cache (\textcolor{blue}{blue}) consistently shows elevated norm values compared to the Value cache (\textcolor{orange}{orange}). Furthermore, Figure~\ref{fig:intro} (Right) demonstrates that the K\textsubscript{(4bit)}V\textsubscript{(2bit)} scheme significantly outperforms the K\textsubscript{(2bit)}V\textsubscript{(4bit)} configuration on GSM8K dataset. These results underscore the need for a norm-aware, mixed-precision strategy that allocates higher precision to Keys and lower precision to Values.

Guided by these inspiring observations,  we propose a mixed-precision quantization strategy, \mn{}, which allocates different bit-width to more bits for keys and fewer bits for values. since key matrices consistently have higher norm values. With the same total KV bit budget, this approach effectively mitigates error propagation across transformer layers while achieving significant memory savings. \mn{} uses singular values, spectral norms, and Frobenius norms to assign more bits to Keys and fewer to Values, balancing memory savings with preserving critical information. This work makes the following contributions:
Guided by these insights, we propose a mixed-precision quantization strategy, \mn{}, which assigns a higher bit-width to key caches and a lower bit-width to value caches—reflecting the consistently higher norm values observed in key matrices. \mn{} leverages metrics such as singular values, spectral norms, and Frobenius norms to identify the critical information in KV cache. By maintaining the same overall KV bit budget, our approach effectively mitigates error propagation across transformer layers while offering significant memory savings. Our contributions include:
\begin{itemize}[leftmargin=0.5cm, itemindent=.0cm, itemsep=0.0cm, topsep=0.1cm]
    \item We systematically investigate the KV cache in transformers, uncovering structural patterns in how information is stored and retrieved.
    \item We introduce \mn{}, an information-aware quantization method that leverages singular value distributions to allocate mixed precision—assigning higher bit-widths to keys and lower bit-widths to values.
    \item We demonstrate the efficacy of our approach on the GSM8K benchmark, achieving substantial memory savings without compromising performance.
\end{itemize}


\section{Key and Value are Different}\label{sec:theory}
This section investigates the differences between key and value caches to enhance quantization strategies.

\subsection{Singular Value Distribution of the KV Cache}\label{sec:singular-value-dist}

An extensive collection of models, ranging in size up to 70B parameters (Llama 3.3 70B \cite{Llama3}), is examined to explore the singular values of the KV cache. Random batches are sampled from the C4 \cite{c4}, GSM8K \cite{gsm8k}, and MMLU \cite{mmlu} datasets, and KV caches are extracted across all attention heads and layers. Figure~\ref{fig:singular_value_distribution} illustrates the distribution of singular values for the KV cache, starting from the 5th largest singular value onward, highlighting how keys and values diverge. Additionally, Figure~\ref{fig:appendix_singular_value_distribution} in Appendix \ref{sec:appendix-sing-all} presents the full-range distribution, further revealing the discrepancy in larger singular values. Beyond individual singular values, the spectral norm ($\|M\|_{2}$) and the Frobenius norm ($\|M\|_{F}$) are computed, providing empirical evidence of the substantial gap in key and value matrix singular values and illustrating how this characteristic of the KV cache can inform the theoretical analysis of quantization error propagation in multi-layer attention networks. This discrepancy underscores the importance of careful precision allocation to mitigate performance degradation while saving more memory in the inference time.



\subsection{Rank of KV Cache}

The rank of a matrix corresponds to the number of nonzero singular values and determines the effective dimension of its column or row space. In multi-head self-attention, the key ($K$) and value ($V$) matrices possess one fixed dimension, $d_{\mathrm{head}}$, and one variable dimension corresponding to the sequence length, $\mathrm{seq\_len}$. Consequently, $K$ and $V$ can be analyzed in two distinct regimes: (a) when $\mathrm{seq\_len} < d_{\mathrm{head}}$, and (b) when $\mathrm{seq\_len} \ge d_{\mathrm{head}}$. In the first regime, the matrices are relatively short and thus offer limited opportunities for compression. Accordingly, many existing KV-cache compression methods either avoid compressing in this stage or employ specialized strategies that differ from those used later.

When $\mathrm{seq\_len} \ge d_{\mathrm{head}}$, the $K$ and $V$ matrices assume rectangular shapes with dimensions $\mathrm{seq\_len} \times d_{\mathrm{head}}$. Although the exact rank is data-dependent (since $K = X W_K$, where $X$ is drawn from a high-dimensional embedding space), empirical observations suggest that both key and value cache typically approach a rank close to $d_{\mathrm{head}}$. Therefore, in practical settings it is reasonable to assume that there are approximately $d_{\mathrm{head}}$ nonzero singular values for both $K$ and $V$. In the remainder of this section, we focus on this regime to analyze how spectral and Frobenius norm considerations guide our quantization strategy.



\subsection{Stability of the Dequantized KV Cache: Error Propagation}


The spectral norm of a matrix $M$, denoted $\|M\|_2$, quantifies the maximum stretching factor that $M$ can impart on any vector. In a multi-layer transformer (e.g., Llama 3.3 \cite{Llama3} with 80 layers or Phi-4 \cite{phi4} with 16 layers), the sequential application of transformations implies that quantization errors introduced at one layer can propagate and amplify in subsequent layers.

To illustrate this effect, consider a simplified layer without normalization or additional scaling \cite{elhage2021mathematical}, where the hidden state at layer $(l+1)$ is given by
\begin{equation}
    h_{l+1} = h_l + W_l h_l,
\end{equation}
Let $W_l$ denote the weight matrix at layer $l$. Define $\widetilde{W}_l$ as the result of quantizing the KV cache, and $W_l$ as the unquantized version. Then, the local error incurred at layer $l$ can be bounded as
\begin{equation}
\begin{split}
    \|h_{l+1} - \widetilde{h}_{l+1}\|_2 &= \|(W_l - \widetilde{W}_l) h_l\|_2 \\
    &\leq \|W_l - \widetilde{W}_l\|_2 \|h_l\|_2.
\end{split}
\end{equation}
Propagating such errors through multiple layers yields a cumulative effect that is proportional to the product of the spectral norms at each layer. In other words, even small perturbations introduced by quantization can be significantly amplified if the weight matrices exhibit large singular values. Empirical observations indicate that \textit{key} matrices consistently have higher spectral norms than \textit{value} matrices. This suggests that errors in key matrices are more prone to propagate and magnify throughout the transformer stack. As a result, it is advisable to employ less aggressive quantization (e.g., 4-bit rather than 2-bit) for key matrices, while value matrices—characterized by lower spectral norms—can tolerate more aggressive quantization without incurring substantial distortion.

\subsection{Total Energy of KV Cache (MSE Perspective)}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/norm_frob.pdf}
    \caption{Frobenius norm of key and value cache across layers for the Llama 3.3 70B model on the C4 dataset. The x-axis represents the layer index. The shaded region shows the min-max range across attention heads within each layer, and the solid line indicates the mean norm per layer. Key matrices exhibit significantly higher norms compared to value matrices, highlighting their stronger representational capacity.}
    \label{fig:frobenius_norm}
\end{figure}

The Frobenius norm of a matrix, denoted by $\|M\|_F$, is defined as the square root of the sum of the squared entries:
\[
\|M\|_F = \sqrt{\sum_{i,j} M_{i,j}^2}
\]

Equivalently, it can be viewed as the square root of the sum of the eigenvalues of $M^\top M$, meaning it captures the total ``energy'' of $M$. In the context of mean-squared error (MSE), the Frobenius norm provides a direct indicator of how much overall distortion might occur when quantizing the matrix.
\paragraph{Quantization Error and the Frobenius Norm.}
When a matrix $M$ is quantized to a lower-precision representation $\widetilde{M}$, the quantization error in MSE terms is often measured by the squared Frobenius norm:
\[
\|M - \widetilde{M}\|_F^2 = \sum_{i,j} (M_{i,j} - \widetilde{M}_{i,j})^2.
\]

A common assumption is that this error scales with both the range of the entries in $M$ and the granularity determined by the bit precision. Formally, one may write
\[
\mathbb{E}\bigl[\|M - \widetilde{M}\|_F^2\bigr] \;\propto\; \|M\|_F^2 \times 2^{-2b},
\]
where $b$ denotes the number of bits used per entry.\footnote{The proportionality depends on the quantizer design and the distribution of entries in $M$. For more details see Appendix \ref{appendix:proofs}.}
For matrices with the same dimensions (e.g., key and value cache), the MSE and squared Frobenius norm are directly proportional by a factor of \(1/mn\). Consequently, when analyzing quantization error for matrices of identical size:  

\begin{itemize}
    \item Minimizing \(\|M - \widetilde{M}\|_F^2\) is equivalent to minimizing \(\text{MSE}(M, \widetilde{M})\), as the proportionality constant \(1/mn\) does not affect optimization or relative comparisons.  
    \item The scaling relationship \(\mathbb{E}\bigl[\|M - \widetilde{M}\|_F^2\bigr] \propto \|M\|_F^2 \times 2^{-2b}\) holds equivalently for MSE, since both metrics share the same dependence on \(b\) and \(\|M\|_F^2\).  
\end{itemize}

Thus, the Frobenius norm of the quantization and mean square of the quantization error can be used interchangeably for key and value cache, as they encode the same error structure up to a constant scaling factor.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{./figures/Llama3.3-70B-it_5_to_end_L.pdf}
    \caption{\textbf{Singular value distribution of key and value cache} for the Llama 3.3 70B model on the C4 dataset. The x-axis represents the singular value indices, ordered from the 5th largest to the smallest, while the y-axis denotes the magnitude of the singular values. The shaded region indicates the minimum-to-maximum range across attention heads within each layer, and the solid lines represent the mean singular value magnitude at each index. This zoomed-in view reveals that, beyond the largest singular value (spectral norm), key matrices exhibit consistently higher singular values even in the lower ranges, underscoring their greater representational significance compared to value matrices. Consequently, key matrices should be quantized with less aggressive compression. See Figure~\ref{fig:appendix_singular_value_distribution} in Appendix~\ref{sec:appendix-sing-all} for the complete range.}
    \label{fig:singular_value_distribution}   
\end{figure*}
% \FloatBarrier % Prevents floats from moving past this point








\paragraph{Larger Frobenius Norms Require Higher Precision.}
As we discussed in ~\ref{sec:singular-value-dist} empirical measurements across multiple transformer models reveal that \emph{key matrices} have significantly larger Frobenius norms than \emph{value matrices}. This disparity suggests that if key matrices are quantized too aggressively (e.g., with fewer bits), the MSE incurred could be disproportionately high. Formally, for two matrices $K$ (key) and $V$ (value), if
\[
\|K\|_F \;\gg\; \|V\|_F,
\]
then allocating the same limited bit precision to both $K$ and $V$ would cause
\[
\|K - \widetilde{K}\|_F^2 \;\gg\; \|V - \widetilde{V}\|_F^2,
\]
since the error term for $K$ is amplified by its larger energy. Furthermore, errors introduced in earlier layers propagate through subsequent layers in a multi-layer transformer architecture, compounding the negative impact of coarse quantization.

\section{Method}

\subsection{Adaptive KV Cache Quantization (\mn{})}
Building on the theoretical findings that matrices with higher spectral norms propagate quantization error more strongly and that larger Frobenius norms incur greater quantization error, along with empirical evidence indicating that key matrices exhibit higher norms than value matrices, a straightforward strategy emerges: quantize keys with higher precision and values with fewer bits. In particular, allocating 4-bit precision to keys and 2-bit precision to values effectively controls the overall mean squared error (Figure~\ref{fig:quantization_error}), reducing memory usage while mitigating error propagation through successive transformer layers.






\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.44\textwidth}
        % \includegraphics[width=\textwidth]{./figures/2bit_error_larger.png}
        \includegraphics[width=\textwidth]{./figures/2bit_error_larger.pdf}
        
        \caption{Mean4squared quantization error across layers for 2-bit quantization.}
        \label{fig:2bit_quant}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.44\textwidth}
        \includegraphics[width=\textwidth]{./figures/4bit_error_larger.pdf}
        \caption{Mean squared quantization error across layers for 4-bit quantization.}
        \label{fig:4bit_quant}
    \end{subfigure}
    \caption{Quantization error (mean squared error) of key and value cache for the Llama 3.3 70B model on the C4 dataset across all transformer layers. The x-axis represents the layer index, and the y-axis indicates the mean squared quantization error. Key matrices exhibit consistently higher quantization error compared to value matrices, with larger discrepancies observed in lower-bit quantization.}
    \label{fig:quantization_error}
\end{figure*}



\subsection{Differences between \mn{} and Other Methods}\label{sec:difference}

The Hugging Face Transformers library implements a KV cache quantization technique that applies uniform precision to KV cache. Although straightforward, this approach can be suboptimal due to the distinct sensitivities of keys and values to quantization errors. Consequently, assigning the same bit-width to both key and value often introduces notable performance degradation. Furthermore, increasing the bit-width for value matrices offers minimal or no additional benefits, indicating that uniform quantization is not an ideal choice.

Other methods, such as  SKVQ \cite{skvq} and  QAQ \cite{qaq}, also propose low-bit quantization for the KV cache, leveraging mixed-precision settings. SKVQ, for example, quantizes the key cache to 2 bits and the value cache to 1.5 bits, demonstrating minimal accuracy loss; however, it relies on fixed hyperparameter configurations that do not deeply analyze the key and value cache' internal characteristics. QAQ similarly introduces quality-adaptive quantization to highlight the heightened sensitivity of key matrices to quantization errors, yet does not thoroughly investigate the underlying matrix properties, making generalization across diverse model architectures and tasks less certain.

By contrast, the proposed approach offers a more general framework of cache quantization that explicitly accounts for the distinct characteristics of key and value cache. This strategy reduces error propagation across transformer layers while achieving considerable memory savings. Additionally, it remains adaptable to further enhancements, such as calibration, grouping, and clustering, allowing the integration of complementary techniques to improve KV cache compression. This adaptability renders the framework orthogonal to existing methods, providing a more comprehensive solution to the challenges associated with KV cache quantization.




\section{Experiments}\label{sec:motivating}
A series of experiments is designed to investigate singular value patterns in KV cache and to demonstrate how these patterns can be leveraged to enhance practical quantization strategies. The experimental testbed consists of a diverse set of transformer models. Two distinct data setups are employed: one for analyzing the singular value distributions and another for evaluating the effectiveness of the proposed quantization approach.


\subsection{Experiment Settings}\label{sec:motivating}

\paragraph{Singular Value and Norm Analysis}\label{sec:svd_analysis}

This section examines structural differences between key and value cache by analyzing their singular values and norms. The process begins by feeding random batches (10 samples per batch) from the C4, MMLU, and GSM8K datasets into the model for each query, where every sample is padded to match the longest sequence to facilitate observations across diverse inputs. For each newly generated token (up to 1,000 tokens), the KV cache from each attention mechanism is collected, yielding matrices \(\mathbf{K}, \mathbf{V} \in \mathbb{R}^{d_{\text{head}} \times \text{seq\_len}}\) for each head and layer. Singular value decomposition is then performed offline in \texttt{float32} precision using the PyTorch \texttt{torch.linalg.svd} operator, and the resulting singular values, along with the spectral norms (\(\|\cdot\|_2\)) and Frobenius norms (\(\|\cdot\|_F\)), are computed and averaged across all heads and layers to capture global patterns. This approach reveals how \(\|\mathbf{K}\|_2\) and \(\|\mathbf{V}\|_2\) evolve in deeper layers, sheds light on the total “energy” in these matrices, and clarifies the distribution of larger and smaller singular values, offering insights into their susceptibility to quantization errors.




\paragraph{Quantization Error}\label{sec:quantization-error}

Hardware typically supports 2-bit, 4-bit, and 8-bit quantization, but in theory, any bit-width can balance compression and accuracy. We measure quantization error across bit-widths from 2 to 8 by quantizing and dequantizing key-value matrices, finding that MSE (and thus the Frobenius norm) behaves similarly across all bit-widths. As shown in Figure~\ref{fig:bit_error}, the key matrix consistently exhibits higher error.
\paragraph{Bit-Width Configurations.}  
The effect of different bit-widths used to represent each entry in \(\mathbf{K}\) and \(\mathbf{V}\) is quantified. Uniform settings include bit-widths ranging from 2 to 8 bits for \emph{both} keys and values (e.g., $(2,2)$, $(4,4)$, or $(8,8)$). Mixed-precision settings compare $(2,4)$ and $(4,2)$, assigning 2 bits to either values or keys and 4 bits to the other.  
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/error_larger.pdf}
    \caption{Mean squared error (MSE) of KV cache quantization for the Llama 3.3 70B model on the C4 dataset, using quantization bit-widths ranging from 2 to 8. The x-axis represents the quantization bit-width, while the y-axis shows the MSE on a logarithmic scale. A logarithmic scale is used to highlight differences at higher bit-widths, where MSE values decrease significantly and approach zero, particularly at 8-bit. Solid lines indicate the mean MSE across layers, while the shaded regions represent the min-max range of errors.}
    \label{fig:bit_error}
\end{figure}


\paragraph{Quantization Procedure.}  
Hugging Face’s \emph{Optimum} library is used for low-level CUDA kernels. No additional outlier or chunk-based grouping techniques are applied, so as to isolation of the effect of \(\mathbf{K}\)-vs-\(\mathbf{V}\) bit allocation.  


\paragraph{Error Metric: MSE.}
To measure the quantization distortion, we record
\[
\text{MSE}(\mathbf{K}, \widetilde{\mathbf{K}}) \;=\; \frac{1}{d_{\text{head}}\,\times\,\text{seq\_len}} \,\|\mathbf{K} - \widetilde{\mathbf{K}}\|_F^2,
\]
and likewise for \(\mathbf{V}\). In addition to per layer and per head comparison, we average MSE across all heads, layers, and tokens in a batch. This approach yields a global view of how severely each bit-width compresses \(\mathbf{K}\) vs. \(\mathbf{V}\).

\paragraph{Experimental Protocol.} For each model and dataset subset (C4, MMLU, GSM8K):
\begin{enumerate}[noitemsep,topsep=0pt]
    \item Run a forward pass to produce unquantized KV caches in bfloat16.
    \item Quantize to a chosen bit-width, then immediately dequantize to bfloat16.
    \item Compare the dequantized version \(\widetilde{\mathbf{K}} / \widetilde{\mathbf{V}}\) to the original \(\mathbf{K} / \mathbf{V}\) in MSE terms.
    \item Repeat for all bit-width settings of interest.
\end{enumerate}





\begin{table*}[t]
\centering
% \scriptsize % Use \scriptsize to make it even more compact
\setlength{\tabcolsep}{2.5pt} % Adjust horizontal spacing between columns if needed
\caption{Quantization error (mean ± std) for key and value cache at 2-bit, 3-bit, and 4-bit quantization ($K_i$ and $V_i$) across different datasets.}
\label{tab:merged_loss}\vspace{-10pt}
\begin{tabular}{ll c >{\columncolor[gray]{0.9}}c c c >{\columncolor[gray]{0.9}}c c}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Model}} &  \multicolumn{2}{c}{\textbf{2-bit}} & \multicolumn{2}{c}{\textbf{3-bit}} & \multicolumn{2}{c}{\textbf{4-bit}} \\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
 &  & $K_2$ & $V_2$ & $K_3$ & $V_3$ & $K_4$ & $V_4$ \\
\midrule
% ----------------- MMLU -----------------
\multirow{8}{*}{\centering\textbf{MMLU}}
& Llama3.2-1B     & 4.851 {\color{gray}{\scriptsize ± 1.037}} & 0.127 {\color{gray}{\scriptsize ± 0.101}} & 1.037 {\color{gray}{\scriptsize ± 0.265}} & 0.021 {\color{gray}{\scriptsize ± 0.015}} & 0.227 {\color{gray}{\scriptsize ± 0.059}} & 0.005 {\color{gray}{\scriptsize ± 0.003}} \\
& Llama3.1-8B-it      & 6.003 {\color{gray}{\scriptsize ± 1.782}} & 0.187 {\color{gray}{\scriptsize ± 0.127}} & 1.082 {\color{gray}{\scriptsize ± 0.244}} & 0.028 {\color{gray}{\scriptsize ± 0.019}} & 0.235 {\color{gray}{\scriptsize ± 0.055}} & 0.006 {\color{gray}{\scriptsize ± 0.004}} \\
& Llama3.3-70B-it     & 4.883 {\color{gray}{\scriptsize ± 1.106}} & 0.112 {\color{gray}{\scriptsize ± 0.093}} & 0.942 {\color{gray}{\scriptsize ± 0.198}} & 0.016 {\color{gray}{\scriptsize ± 0.012}} & 0.206 {\color{gray}{\scriptsize ± 0.043}} & 0.003 {\color{gray}{\scriptsize ± 0.003}} \\

& Phi4          & 5.929 {\color{gray}{\scriptsize ± 1.545}} & 0.657 {\color{gray}{\scriptsize ± 0.472}} & 1.306 {\color{gray}{\scriptsize ± 0.231}} & 0.103 {\color{gray}{\scriptsize ± 0.070}} & 0.286 {\color{gray}{\scriptsize ± 0.050}} & 0.022 {\color{gray}{\scriptsize ± 0.015}} \\
& Mistral0.3-7B     & 4.718 {\color{gray}{\scriptsize ± 1.340}} & 0.398 {\color{gray}{\scriptsize ± 0.405}} & 0.941 {\color{gray}{\scriptsize ± 0.240}} & 0.059 {\color{gray}{\scriptsize ± 0.059}} & 0.206 {\color{gray}{\scriptsize ± 0.053}} & 0.013 {\color{gray}{\scriptsize ± 0.013}} \\
& Qwen2.5-14B       & 5.184 {\color{gray}{\scriptsize ± 2.241}} & 1.270 {\color{gray}{\scriptsize ± 1.547}} & 1.005 {\color{gray}{\scriptsize ± 0.288}} & 0.182 {\color{gray}{\scriptsize ± 0.221}} & 0.223 {\color{gray}{\scriptsize ± 0.067}} & 0.040 {\color{gray}{\scriptsize ± 0.052}} \\
& DeepSeekR1Q-14B     & 5.126 {\color{gray}{\scriptsize ± 2.375}} & 1.406 {\color{gray}{\scriptsize ± 1.609}} & 0.900 {\color{gray}{\scriptsize ± 0.269}} & 0.198 {\color{gray}{\scriptsize ± 0.226}} & 0.199 {\color{gray}{\scriptsize ± 0.062}} & 0.044 {\color{gray}{\scriptsize ± 0.052}} \\
\midrule
% ----------------- C4 -----------------
\multirow{8}{*}{\centering\textbf{C4}}
& Llama3.2-1B & 4.885 {\color{gray}{\scriptsize ± 1.056}} & 0.207 {\color{gray}{\scriptsize ± 0.166}} & 1.074 {\color{gray}{\scriptsize ± 0.289}} & 0.030 {\color{gray}{\scriptsize ± 0.024}} & 0.233 {\color{gray}{\scriptsize ± 0.062}} & 0.006 {\color{gray}{\scriptsize ± 0.005}} \\
& Llama3.1-8B-it & 6.262 {\color{gray}{\scriptsize ± 1.789}} & 0.254 {\color{gray}{\scriptsize ± 0.185}} & 1.128 {\color{gray}{\scriptsize ± 0.249}} & 0.036 {\color{gray}{\scriptsize ± 0.026}} & 0.247 {\color{gray}{\scriptsize ± 0.056}} & 0.008 {\color{gray}{\scriptsize ± 0.005}} \\
& Llama3.3-70B-it & 4.391 {\color{gray}{\scriptsize ± 1.027}} & 0.121 {\color{gray}{\scriptsize ± 0.097}} & 0.847 {\color{gray}{\scriptsize ± 0.175}} & 0.017 {\color{gray}{\scriptsize ± 0.013}} & 0.186 {\color{gray}{\scriptsize ± 0.038}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
& Phi4 & 5.715 {\color{gray}{\scriptsize ± 1.442}} & 0.850 {\color{gray}{\scriptsize ± 0.684}} & 1.316 {\color{gray}{\scriptsize ± 0.245}} & 0.124 {\color{gray}{\scriptsize ± 0.093}} & 0.291 {\color{gray}{\scriptsize ± 0.056}} & 0.027 {\color{gray}{\scriptsize ± 0.020}} \\
& Mistral0.3-7B & 5.027 {\color{gray}{\scriptsize ± 1.332}} & 0.543 {\color{gray}{\scriptsize ± 0.493}} & 1.014 {\color{gray}{\scriptsize ± 0.269}} & 0.079 {\color{gray}{\scriptsize ± 0.068}} & 0.223 {\color{gray}{\scriptsize ± 0.060}} & 0.017 {\color{gray}{\scriptsize ± 0.015}} \\
& Qwen2.5-14B & 4.382 {\color{gray}{\scriptsize ± 2.170}} & 1.544 {\color{gray}{\scriptsize ± 1.872}} & 0.846 {\color{gray}{\scriptsize ± 0.250}} & 0.220 {\color{gray}{\scriptsize ± 0.265}} & 0.187 {\color{gray}{\scriptsize ± 0.060}} & 0.048 {\color{gray}{\scriptsize ± 0.060}} \\
& DeepSeekR1Q-14B & 4.832 {\color{gray}{\scriptsize ± 2.354}} & 1.651 {\color{gray}{\scriptsize ± 1.914}} & 0.927 {\color{gray}{\scriptsize ± 0.283}} & 0.232 {\color{gray}{\scriptsize ± 0.267}} & 0.201 {\color{gray}{\scriptsize ± 0.061}} & 0.051 {\color{gray}{\scriptsize ± 0.060}} \\
\midrule
% ----------------- GSM8K -----------------
\multirow{8}{*}{\centering\textbf{GSM8K}}
& Llama3.2-1B & 5.703 {\color{gray}{\scriptsize ± 1.557}} & 0.179 {\color{gray}{\scriptsize ± 0.136}} & 1.213 {\color{gray}{\scriptsize ± 0.352}} & 0.026 {\color{gray}{\scriptsize ± 0.020}} & 0.266 {\color{gray}{\scriptsize ± 0.078}} & 0.005 {\color{gray}{\scriptsize ± 0.004}} \\
& Llama3.1-8B-it & 6.445 {\color{gray}{\scriptsize ± 1.837}} & 0.213 {\color{gray}{\scriptsize ± 0.161}} & 1.184 {\color{gray}{\scriptsize ± 0.268}} & 0.030 {\color{gray}{\scriptsize ± 0.022}} & 0.257 {\color{gray}{\scriptsize ± 0.060}} & 0.007 {\color{gray}{\scriptsize ± 0.005}} \\
& Llama3.3-70B-it & 4.967 {\color{gray}{\scriptsize ± 1.127}} & 0.113 {\color{gray}{\scriptsize ± 0.091}} & 0.978 {\color{gray}{\scriptsize ± 0.203}} & 0.016 {\color{gray}{\scriptsize ± 0.012}} & 0.214 {\color{gray}{\scriptsize ± 0.044}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
& Phi4 & 6.610 {\color{gray}{\scriptsize ± 1.624}} & 0.785 {\color{gray}{\scriptsize ± 0.598}} & 1.498 {\color{gray}{\scriptsize ± 0.293}} & 0.116 {\color{gray}{\scriptsize ± 0.082}} & 0.330 {\color{gray}{\scriptsize ± 0.064}} & 0.025 {\color{gray}{\scriptsize ± 0.017}} \\
& Mistral0.3-7B & 5.308 {\color{gray}{\scriptsize ± 1.367}} & 0.461 {\color{gray}{\scriptsize ± 0.434}} & 1.065 {\color{gray}{\scriptsize ± 0.288}} & 0.067 {\color{gray}{\scriptsize ± 0.061}} & 0.232 {\color{gray}{\scriptsize ± 0.061}} & 0.015 {\color{gray}{\scriptsize ± 0.013}} \\
& Qwen2.5-14B & 4.829 {\color{gray}{\scriptsize ± 2.179}} & 1.736 {\color{gray}{\scriptsize ± 2.659}} & 0.979 {\color{gray}{\scriptsize ± 0.264}} & 0.241 {\color{gray}{\scriptsize ± 0.372}} & 0.214 {\color{gray}{\scriptsize ± 0.061}} & 0.051 {\color{gray}{\scriptsize ± 0.077}} \\
& DeepSeekR1Q-14B & 4.477 {\color{gray}{\scriptsize ± 2.176}} & 1.424 {\color{gray}{\scriptsize ± 1.752}} & 0.830 {\color{gray}{\scriptsize ± 0.256}} & 0.200 {\color{gray}{\scriptsize ± 0.242}} & 0.181 {\color{gray}{\scriptsize ± 0.058}} & 0.044 {\color{gray}{\scriptsize ± 0.056}} \\
\bottomrule
\end{tabular}
\end{table*}



\paragraph{Downstream Evaluation.}\label{sec:downstream_eval}

While MSE is a reasonable proxy for overall distortion, the ultimate question is whether the quantized KV cache preserves model quality on realistic tasks. We therefore evaluate the final model performance on GSM8K in two prompting modes: 1-shot and 8-shot prompting using 4 quantization settings:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \(\mathbf{K}_{\mathbf{(2bit)}}\mathbf{V}_{\mathbf{(2bit)}}\): Aggressive uniform quantization.
    \item \(\mathbf{K}_{\mathbf{(4bit)}}\mathbf{V}_{\mathbf{(4bit)}}\): More conservative uniform quantization.
    \item \(\mathbf{K}_{\mathbf{(2bit)}}\mathbf{V}_{\mathbf{(4bit)}}\) \textit{vs.} \(\mathbf{K}_{\mathbf{(4bit)}}\mathbf{V}_{\mathbf{(2bit)}}\): Mixed-precision assignments, giving 2 bits to either \(\mathbf{K}\) or \(\mathbf{V}\) and 4 bits to the other.
\end{itemize}

\subsection{Experiment Results}

\begin{table*}[t]
\centering
\caption{GSM8K 1-shot and 8-shot performance of various models under different key-value quantization settings. $\mathbf{K}_{i}\mathbf{V}_{j}$ denotes $i$-bit quantization for the K cache and $j$-bit for the V cache. The $\mathbf{K}_{\mathbf{(4bit)}}\mathbf{V}_{\mathbf{(2bit)}}$ column, highlighted below, represents our main finding. }
\label{tab:gsm8k}
\begin{tabular}{lrr>{\columncolor[gray]{0.9}}r r| rr>{\columncolor[gray]{0.9}}r r}
\toprule
 & \multicolumn{4}{c|}{\textbf{1-shot}} & \multicolumn{4}{c}{\textbf{8-shot}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
          Model &  $\mathbf{K}_{\mathbf{2}}\mathbf{V}_{\mathbf{2}}$ &  $\mathbf{K}_{\mathbf{2}}\mathbf{V}_{\mathbf{4}}$ &  \cellcolor[gray]{0.9}$\mathbf{K}_{\mathbf{4}}\mathbf{V}_{\mathbf{2}}$ &  $\mathbf{K}_{\mathbf{4}}\mathbf{V}_{\mathbf{4}}$ &  $\mathbf{K}_{\mathbf{2}}\mathbf{V}_{\mathbf{2}}$ &  $\mathbf{K}_{\mathbf{2}}\mathbf{V}_{\mathbf{4}}$ &  \cellcolor[gray]{0.9}$\mathbf{K}_{\mathbf{4}}\mathbf{V}_{\mathbf{2}}$ &  $\mathbf{K}_{\mathbf{4}}\mathbf{V}_{\mathbf{4}}$ \\
\midrule
 Llama3.2-1B-it & 0.033 & 0.035 &  \cellcolor[gray]{0.9}0.338 & 0.357 & 0.031 & 0.031 &  \cellcolor[gray]{0.9}0.289 & 0.369 \\
 Llama3.1-8B-it & 0.511 & 0.547 &  \cellcolor[gray]{0.9}0.752 & 0.754 & 0.408 & 0.441 &  \cellcolor[gray]{0.9}0.770 & 0.782 \\
           Phi4-14B & 0.759 & 0.783 &  \cellcolor[gray]{0.9}0.913 & 0.923 & 0.771 & 0.815 &  \cellcolor[gray]{0.9}0.927 & 0.931 \\
DeepSeekR1Q-14B & 0.772 & 0.775 &  \cellcolor[gray]{0.9}0.865 & 0.867 & 0.763 & 0.792 &  \cellcolor[gray]{0.9}0.876 & 0.875 \\
\bottomrule
\end{tabular}
\end{table*}




Table~\ref{tab:merged_loss} shows that across datasets and models, quantization error for the key matrices is consistently and substantially higher than that for the value matrices. For instance, in the MMLU dataset, Llama3.2-1B exhibits an error of $4.85 \pm 1.04$ for $\mathbf{K}_{2}$ versus only $0.13 \pm 0.10$ for $\mathbf{V}_{2}$. This trend persists for both the 3-bit and 4-bit settings, where increasing the bit-width reduces the error for both matrices, yet the keys still incur a notably higher error. Similar patterns are evident in the C4 and GSM8K datasets, highlighting that key matrices are inherently more sensitive to quantization noise. Furthermore, Figure~\ref{fig:layer-head-err} in the Appendix~\ref{appendix:b-quant-error-heads} illustrates the quantization error per attention head in each layer for the Llama3.1-8B model on C4, MMLU, and GSM8K, reinforcing our observation (see Appendix~\ref{appendix-d:quant-error} for extended results).
Table~\ref{tab:gsm8k} shows accuracy on GSM8K under 1-shot and 8-shot prompting. Uniform K$_{(2\text{bit})}$V$_{(2\text{bit})}$ settings degrade accuracy noticeably, especially in the longer 8-shot context. By contrast, K$_{(4\text{bit})}$V$_{(2\text{bit})}$ nearly matches the unquantized baseline, confirming that providing higher precision to keys preserves model quality while still compressing values aggressively. The reversed assignment K$_{(2\text{bit})}$V$_{(4\text{bit})}$ consistently underperforms, underscoring that allocating fewer bits to key matrices adversely affects accuracy.


\section{Related Work}\label{sec:related}

\paragraph{Model Quantization.} 
 for large models such as LLMs \cite{efficientllm}, which is why post-training quantization (PTQ) techniques are generally preferred for quantizing these models.

Quantization reduces the computational cost of neural network inference by lowering model bit-precision \cite{deepcompression, whitepaperquant}. It can be divided into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ requires no re-training or labeled data \cite{quantizationtrainingneuralnetworks, datafreequantizationweightequalization} and is preferred for large models like LLMs, whereas QAT demands fine-tuning with labeled data \cite{whitepaperquant, surveyquantizationmethodsefficient} but scales poorly for such models \cite{efficientllm}.

\paragraph{LLMs Quantization.} 


Researchers consider three post-training quantization settings for LLMs: weight-only, weight-activation, and KV cache quantization. Weight-only quantization focuses solely on weights \cite{gptq, obq, awq}, while weight-activation quantization targets both weights and activations \cite{llmint8, smoothquant, omniquant}. Since longer inputs inflate the KV cache’s memory overhead \cite{efficientlyscalingtransformerinference}, KV cache quantization offers memory savings similar to weight-activation methods while maintaining performance near that of weight-only quantization \cite{wkvquant}.

\paragraph{KV Cache Quantization.} 

Recent KV cache quantization methods reduce LLM memory and accelerate inference, falling into three categories: outlier redistribution, fixed-precision, and mixed-precision \cite{kvcachequantsurvey}. Outliers in KV cache are addressed by redistributing them or applying smoothing transformations \cite{smoothquant, awq, omniquant}. Fixed-precision methods use a uniform bit-width \cite{zeroquant, flexgen, kivi} but can overlook token importance and outliers. In contrast, mixed-precision allocates higher precision to critical tokens and lower precision elsewhere \cite{kvquant, wkvquant, snapkv, qaq, skvq}.
Existing methods like SKVQ and QAQ rely on specific engineering solutions or predefined parameters, whereas \mn{} uses a principled, analysis-driven framework that leverages the intrinsic properties of key and value caches for a more robust and adaptable quantization approach.



\section{Conclusion}

In this work, we systematically analyze KV cache matrices from the perspectives of linear algebra and information theory to uncover distinct patterns in the information carried by these matrices. Building on the singular value distribution of the KV cache, we propose an information-aware quantization framework for KV cache compression. We evaluate the effectiveness of our approach on two widely used datasets, GSM8K, demonstrating its practical impact. While our current study focuses on the per-token quantization methods, the proposed framework is general and can be applied to any key-value quantization technique.


\clearpage
\section*{Limitations}
Although our information-aware quantization framework is effective, there are aspects that warrant further consideration. First, our experiments utilize a maximum context length of 2K tokens. While this setting aligns with many practical inference scenarios, it may not fully capture the demands of tasks requiring larger context windows (e.g., 4K or 8K tokens). Extending our approach to support longer contexts could reveal additional challenges, such as increased quantization sensitivity and greater memory management overhead.


\section*{Ethical Considerations}
Our research aims to reduce the memory footprint and computational overhead of LLM in inference time through  KV cache quantization. Such advancements can potentially lower energy consumption and expand access to resource-constrained communities, contributing to more environmentally sustainable and inclusive deployment of language modeling based productions. However, the use of proposal KV cache compression method must be carefully monitored to ensure that accuracy trade-offs do not disproportionately affect critical applications such as healthcare, legal, and financial contexts.




\bibliography{references}


\clearpage

\appendix
% \tableofcontents

\section{Quantization and Dequantization Error Bounds}\label{appendix:proofs}


In this appendix we establishes upper bounds on the quantization error incurred when a matrix is represented using a finite bit-width in two's-complement format. Two theorems are presented: one that characterizes the error in terms of the spectral norm and another in terms of the Frobenius norm.

The first theorem demonstrates that the spectral norm of the quantization error is bounded by
\[
\|A - \widehat{A}\|_2 \;\lesssim\; \frac{\sqrt{m\,n}}{2^b}\,\|A\|_2.
\]
This result implies that, for a given bit-width \(b\), matrices with larger spectral norms incur proportionally larger quantization errors. Consequently, a matrix that exhibits a larger spectral norm is more susceptible to quantization errors. To control the error propagation in such matrices, a higher bit-width is necessary.

The second theorem provides an analogous bound for the Frobenius norm,
\[
\|A - \widehat{A}\|_F \;\lesssim\; \frac{\sqrt{m\,n}}{2^b}\,\|A\|_F.
\]
Similar to the spectral norm result, this bound indicates that the quantization error, measured in the Frobenius norm, is directly proportional to the norm of the original matrix. Hence, matrices with larger Frobenius norms are also more vulnerable to quantization errors and would benefit from a higher precision during quantization.


\subsection*{Preliminaries and Notation}

Let \(A \in \mathbb{R}^{m \times n}\) denote a real matrix whose entries are to be stored using a fixed number of bits in two's-complement representation. The following notation is adopted:
\begin{itemize}
  \item \textbf{Bit Depth.} Given \(b\) bits in two's-complement format, each representable integer \(q\) lies in the interval
  \[
    q \in \{-2^{b-1}, -2^{b-1}+1, \dots, 2^{b-1}-1\}.
  \]
  \item \textbf{Maximum Entry Magnitude.} Define
  \[
    M \;=\; \max_{1 \leq i \leq m,\, 1 \leq j \leq n} \;|A_{ij}|.
  \]
  \item \textbf{Scale Factor.} Set
  \[
    \alpha \;=\; \frac{M}{2^{b-1}-1}.
  \]
  This choice ensures that the scaled entries \(A_{ij}/\alpha\) lie within the representable range.
\end{itemize}

\medskip

\noindent
\textbf{Quantization.} Define the integer matrix \(Q \in \mathbb{Z}^{m \times n}\) by

\begin{equation}
  \begin{aligned}
    Q_{ij} &= \mathrm{round}\!\Bigl(\frac{A_{ij}}{\alpha}\Bigr), \\
    \text{with} \quad Q_{ij} &\in \{-2^{b-1}, \dots, 2^{b-1}-1\}.
  \end{aligned}
\end{equation}

\noindent
\textbf{Dequantization (Reconstruction).} The reconstructed matrix \(\widehat{A}\) is given by
\[
  \widehat{A}_{ij} \;=\; \alpha \, Q_{ij}.
\]

\medskip

\noindent
\textbf{Objective.} The aim is to bound the errors
\[
  \|A - \widehat{A}\|_2 \quad \text{and} \quad \|A - \widehat{A}\|_F,
\]
in terms of \(b\), \(m\), \(n\), and the norms of \(A\).

\subsection*{Spectral Norm Bound}

\begin{theorem}[Spectral Norm Quantization Error]
Let \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{N}\) be given. Consider two's-complement quantization with scale factor
\[
  \alpha \;=\; \frac{M}{2^{b-1} - 1}, \quad M \;=\; \max_{i,j} |A_{ij}|.
\]
Define 
\[
  Q_{ij} \;=\; \mathrm{round}\!\Bigl(\frac{A_{ij}}{\alpha}\Bigr)
  \quad \text{and} \quad
  \widehat{A}_{ij} \;=\; \alpha\,Q_{ij}.
\]
Then, the following bound holds:
\begin{equation}
  \begin{aligned}
    M &\le \|A - \widehat{A}\|_2 \le \sqrt{m\,n} \, \frac{M}{2(2^{b-1} - 1)} \\
      &\le \sqrt{m\,n} \, \frac{\|A\|_2}{2(2^{b-1} - 1)}.
  \end{aligned}
\end{equation}
In approximate form for large \(b\),
\[
  \|A - \widehat{A}\|_2 
  \;\lesssim\; 
  \frac{\sqrt{m\,n}}{2^b}\,\|A\|_2.
\]
\end{theorem}

\begin{proof}
\textbf{Entrywise Bound.} By construction,
\[
  \left|\frac{A_{ij}}{\alpha} - Q_{ij}\right| \;\le\; \frac{1}{2}.
\]
Multiplying by \(\alpha\) gives
\[
  |A_{ij} - \widehat{A}_{ij}| \;\le\; \frac{\alpha}{2} \;=\; \frac{M}{2\,(2^{b-1} - 1)}.
\]
Thus, 
\[
  \max_{i,j}\,|A_{ij} - \widehat{A}_{ij}| \;\le\; \frac{M}{2\,(2^{b-1} - 1)}.
\]

\medskip

\textbf{Conversion to the Spectral Norm.} Using the inequality
\[
  \|B\|_2 \;\le\; \sqrt{m\,n}\,\max_{i,j}|B_{ij}|,
\]
with \(B = A - \widehat{A}\), it follows that
\[
  \|A - \widehat{A}\|_2 \;\le\; \sqrt{m\,n}\,\frac{M}{2\,(2^{b-1} - 1)}.
\]

\medskip

\textbf{Relating \(M\) to \(\|A\|_2\).} Since
\[
  M \;=\; \max_{i,j}|A_{ij}| \;\le\; \|A\|_2,
\]
the bound can be written as
\[
  \|A - \widehat{A}\|_2 \;\le\; \sqrt{m\,n}\,\frac{\|A\|_2}{2\,(2^{b-1} - 1)}.
\]
For large \(b\), where \(2^{b-1} - 1 \approx 2^{b-1}\), the bound becomes
\[
  \|A - \widehat{A}\|_2 \;\lesssim\; \frac{\sqrt{m\,n}}{2^b}\,\|A\|_2.
\]
\end{proof}

\subsection*{Frobenius Norm Bound}

\begin{theorem}[Frobenius Norm Quantization Error]
Under the same setup, the Frobenius norm of the quantization error satisfies
\begin{equation}
\begin{split}
  \|A - \widehat{A}\|_F
  &\;\le\;
  \sqrt{m\,n}\,{\textstyle\frac{M}{2(2^{b-1}-1)}} \\
  &\;\le\;
  \sqrt{m\,n}\,{\textstyle\frac{\|A\|_F}{2(2^{b-1}-1)}}.
\end{split}
\end{equation}
In approximate form,
\[
  \|A - \widehat{A}\|_F
  \;\lesssim\;
  \frac{\sqrt{m\,n}}{2^b}\,\|A\|_F.
\]
\end{theorem}

\begin{proof}
\textbf{Entrywise Bound.} As established,
\[
  |A_{ij} - \widehat{A}_{ij}| \;\le\; \frac{M}{2\,(2^{b-1}-1)} \quad \text{for all } i,j.
\]

\medskip

\textbf{Conversion to the Frobenius Norm.} By definition,
\[
  \|A - \widehat{A}\|_F^2 \;=\; \sum_{i=1}^m \sum_{j=1}^n (A_{ij} - \widehat{A}_{ij})^2,
\]
which yields
\[
  \|A - \widehat{A}\|_F^2 \;\le\; m\,n \left(\frac{M}{2\,(2^{b-1}-1)}\right)^2.
\]
Taking square roots leads to
\[
  \|A - \widehat{A}\|_F \;\le\; \sqrt{m\,n}\,\frac{M}{2\,(2^{b-1}-1)}.
\]

\medskip

\textbf{Relating \(M\) to \(\|A\|_F\).} Since
\[
  M^2 \;\le\; \sum_{i=1}^m \sum_{j=1}^n A_{ij}^2 \;=\; \|A\|_F^2,
\]
it follows that \(M \leq \|A\|_F\) and hence
\[
  \|A - \widehat{A}\|_F \;\le\; \sqrt{m\,n}\,\frac{\|A\|_F}{2\,(2^{b-1}-1)}.
\]
For large \(b\), this simplifies to
\[
  \|A - \widehat{A}\|_F \;\lesssim\; \frac{\sqrt{m\,n}}{2^b}\,\|A\|_F.
\]
\end{proof}

\medskip

\noindent
\textbf{Remark.} The results indicate that both the spectral norm and Frobenius norm errors satisfy similar approximate bounds:
\begin{equation}
  \begin{aligned}
    \|A - \widehat{A}\|_2 &\lesssim \frac{\sqrt{m\,n}}{2^b}\,\|A\|_2,\\
    \|A - \widehat{A}\|_F &\lesssim \frac{\sqrt{m\,n}}{2^b}\,\|A\|_F.
  \end{aligned}
\end{equation}

\bigskip

\subsection*{Quantizing KV Cache}

Consider the key and value cache
\[
  \mathbf{V} \in \mathbb{R}^{L \times d_{\text{head}}}, \quad
  \mathbf{K} \in \mathbb{R}^{L \times d_{\text{head}}},
\]
with quantization bit-widths denoted by \(b_V\) and \(b_K\), respectively. Let \(\widehat{\mathbf{V}}\) and \(\widehat{\mathbf{K}}\) denote the dequantized matrices, and define the quantization errors as
\[
  \mathbf{E}_V \;=\; \mathbf{V} - \widehat{\mathbf{V}}, \quad
  \mathbf{E}_K \;=\; \mathbf{K} - \widehat{\mathbf{K}}.
\]
An empirical observation is that
\[
  \|\mathbf{V}\|_{\ast} \;<\; \|\mathbf{K}\|_{\ast},
\]
where \(\ast\) denotes either the spectral norm (\(\|\cdot\|_2\)) or the Frobenius norm (\(\|\cdot\|_F\)). In practice, \(\mathbf{K}\) typically exhibits a larger norm than \(\mathbf{V}\).

\subsection*{Spectral-Norm Perspective}

Standard quantization error bounds yield
\begin{equation}
  \begin{aligned}
    \|\mathbf{E}_V\|_2 &\lesssim \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_V}}\;\|\mathbf{V}\|_2, \\
    \|\mathbf{E}_K\|_2 &\lesssim \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_K}}\;\|\mathbf{K}\|_2.
  \end{aligned}
\end{equation}
To achieve comparable spectral-norm errors (i.e., \(\|\mathbf{E}_V\|_2 \approx \|\mathbf{E}_K\|_2\)), it is necessary that
\[
   \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_V}}\;\|\mathbf{V}\|_2 
   \;\approx\;
   \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_K}}\;\|\mathbf{K}\|_2.
\]
Cancelling the common factor \(\sqrt{L\, d_{\text{head}}}\) yields
\[
  2^{b_V}\,\|\mathbf{V}\|_2 \;\approx\; 2^{b_K}\,\|\mathbf{K}\|_2,
\]
or equivalently,
\[
  2^{\,b_K - b_V} \;\approx\; \frac{\|\mathbf{V}\|_2}{\|\mathbf{K}\|_2}.
\]
Since \(\|\mathbf{V}\|_2 < \|\mathbf{K}\|_2\), it follows that \(b_K > b_V\).

\subsection*{Frobenius-Norm (MSE) Perspective}

Similarly, the Frobenius norm error bounds are given by
\begin{equation}
  \begin{aligned}
    \|\mathbf{E}_V\|_F 
    &\;\lesssim\;
    \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_V}}\;\|\mathbf{V}\|_F, \\
    \|\mathbf{E}_K\|_F 
    &\;\lesssim\;
    \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_K}}\;\|\mathbf{K}\|_F.
  \end{aligned}
\end{equation}
To achieve comparable Frobenius errors,
\[
   \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_V}}\;\|\mathbf{V}\|_F 
   \;\approx\;
   \frac{\sqrt{L \, d_{\text{head}}}}{2^{b_K}}\;\|\mathbf{K}\|_F,
\]
which implies
\[
   2^{b_V}\,\|\mathbf{V}\|_F \;\approx\; 2^{b_K}\,\|\mathbf{K}\|_F,
\]
and therefore,
\[
   2^{\,b_K - b_V} \;\approx\; \frac{\|\mathbf{V}\|_F}{\|\mathbf{K}\|_F}.
\]
Again, because \(\|\mathbf{V}\|_F < \|\mathbf{K}\|_F\), it follows that \(b_K > b_V\).



% \section*{Appendix B: Quantization Error Across Layers and Heads}\label{appendix:b-quant-error-heads}
\section{Quantization Error Across Layers and Heads}\label{appendix:b-quant-error-heads}


Figure~\ref{fig:layer-head-err} displays the MSE of the quantized key and value caches in the Llama~3.1~8B model, evaluated layer by layer across 32 layers and differentiated by attention head. Results are presented for three datasets: C4, MMLU, and GSM8k. Each subfigure illustrates the 2-bit and 4-bit quantization configurations for the key and value caches. The error patterns vary across layers, suggesting that certain layers or heads may be more sensitive to aggressive quantization; however, the overall pattern indicates that assigning 4-bit to the key cache is comparable to assigning 2-bit to the value cache.

\begin{figure*}[ht]
    \centering
    % First subfigure
    \begin{subfigure}{.9\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{appendix/Llama3.1-8B-it_C4_plot.png}
        \includegraphics[width=\linewidth]{appendix/Llama3.1-8B-it_C4_plot.pdf}
        
        \caption{C4 dataset}
        \label{fig:c4-plot}
    \end{subfigure}
    % Second subfigure
    \begin{subfigure}{.9\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{appendix/Llama3.1-8B-it_MMLU_plot.png}
        \includegraphics[width=\linewidth]{appendix/Llama3.1-8B-it_MMLU_plot.pdf}
        \caption{MMLU dataset}
        \label{fig:mmlu-plot}
    \end{subfigure}
    % Third subfigure
    \begin{subfigure}{.9\textwidth}
        \centering
        % \includegraphics[width=\linewidth]{appendix/Llama3.1-8B-it_GSM_plot.png}
        \includegraphics[width=\linewidth]{appendix/Llama3.1-8B-it_GSM_plot.pdf}
        
        \caption{GSM8k dataset}
        \label{fig:gsm8k-plot}
    \end{subfigure}
    %
    \caption{Quantization error (MSE) of K and V caches in the Llama 3.1 8B model across 32 layers for the (a)~C4, (b)~MMLU, and (c)~GSM8k datasets. 
    The top row in each plot shows 2-bit quantization (k2, v2), while the bottom row shows 4-bit quantization (k4, v4). 
    Each point represents a different attention head in the corresponding layer. 
    The x-axis indicates the layer index, and the y-axis shows the quantization error in MSE.}
    \label{fig:layer-head-err}
\end{figure*}



\section{Extended Visualization of Singular Value Distributions}\label{sec:appendix-sing-all}


Figure~\ref{fig:appendix_singular_value_distribution} provides the full-range view of the singular value distribution for the key and value cache at various layers of the Llama~3.3~70B model on the C4 dataset. The horizontal axis enumerates the singular values in descending order, beginning with the largest (i.e., the spectral norm), and the vertical axis measures the corresponding magnitudes. The shaded region captures the minimum-to-maximum range of singular values across the attention heads within each layer, while the solid curves depict the mean singular value at each rank.

This figure underscores two main observations. First, key matrices systematically exhibit higher leading singular values than value matrices. In particular, the first singular value (the spectral norm) for key matrices is consistently larger, confirming that the key matrices possess a higher spectral norm. Second, this behavior (significant gap) remains evident across the entire spectrum of singular values, implying that the sum of the squares of these values (i.e., the Frobenius norm) is also larger for key matrices. Consequently, the stronger representational capacity of key matrices, as reflected by both their spectral and Frobenius norms, renders them more sensitive to quantization errors. 

\section{Extended Quantization Error Results}\label{appendix-d:quant-error}

Tables~\ref{tab:mmlu_loss_bits}, \ref{tab:c4_loss_bits}, and \ref{tab:gsm8k_loss_qbits} present extended quantization error results across a wider range of models and settings for the MMLU, C4, and GSM8K datasets, respectively. These tables offer a detailed breakdown of the error metrics under various bit-width configurations, complementing the analysis presented in the main text and in Table~\ref{tab:merged_loss}.
\label{sec:svd_analysis}
\FloatBarrier

% \begin{figure*}[t]
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{./figures/Llama3.3-70B-it_all_L.pdf}
  \caption{\textbf{Complete singular value distribution of key and value cache} for the Llama 3.3 70B model on the C4 dataset. The x-axis denotes the singular value indices, ordered from the largest (spectral norm) to the smallest, while the y-axis represents the corresponding magnitudes. The shaded region illustrates the range between the minimum and maximum singular values across attention heads within each layer, and the solid lines indicate the mean singular value magnitude at each index. This full-spectrum view highlights that key matrices consistently maintain significantly higher singular values throughout the entire distribution, further reinforcing their dominant representational capacity compared to value matrices.}

    \label{fig:appendix_singular_value_distribution}
\end{figure*}




\clearpage


\begin{table*}[ht]
\centering
% \scriptsize % Use \scriptsize to make it even more compact
\setlength{\tabcolsep}{2.5pt} % Adjust horizontal spacing between columns if needed
\caption{Quantization error (mean ± std) for the key and value caches ($K_i$ and $V_i$) at 2-bit, 3-bit, and 4-bit quantization, evaluated on the MMLU dataset.}
\label{tab:mmlu_loss_bits}
\begin{tabular}{ll>{\columncolor[gray]{0.9}}lll>{\columncolor[gray]{0.9}}ll}
\toprule
Model               &            K2 &            V2 &            K3 &            V3 &            K4 &            V4 \\
\midrule
Llama3.2-1B         & 4.851 {\color{gray}{\scriptsize ± 1.037}} & 0.127 {\color{gray}{\scriptsize ± 0.101}} & 1.037 {\color{gray}{\scriptsize ± 0.265}} & 0.021 {\color{gray}{\scriptsize ± 0.015}} & 0.227 {\color{gray}{\scriptsize ± 0.059}} & 0.005 {\color{gray}{\scriptsize ± 0.003}} \\
Llama3.2-1B-it      & 4.373 {\color{gray}{\scriptsize ± 1.034}} & 0.124 {\color{gray}{\scriptsize ± 0.090}} & 0.879 {\color{gray}{\scriptsize ± 0.218}} & 0.019 {\color{gray}{\scriptsize ± 0.013}} & 0.192 {\color{gray}{\scriptsize ± 0.047}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
Llama3.2-3B         & 3.943 {\color{gray}{\scriptsize ± 0.924}} & 0.193 {\color{gray}{\scriptsize ± 0.096}} & 0.849 {\color{gray}{\scriptsize ± 0.150}} & 0.030 {\color{gray}{\scriptsize ± 0.015}} & 0.183 {\color{gray}{\scriptsize ± 0.031}} & 0.007 {\color{gray}{\scriptsize ± 0.003}} \\
Llama3.2-3B-it      & 4.487 {\color{gray}{\scriptsize ± 1.180}} & 0.202 {\color{gray}{\scriptsize ± 0.100}} & 0.894 {\color{gray}{\scriptsize ± 0.180}} & 0.030 {\color{gray}{\scriptsize ± 0.015}} & 0.193 {\color{gray}{\scriptsize ± 0.037}} & 0.007 {\color{gray}{\scriptsize ± 0.003}} \\
Llama2-7B           & 3.190 {\color{gray}{\scriptsize ± 0.783}} & 0.259 {\color{gray}{\scriptsize ± 0.184}} & 0.769 {\color{gray}{\scriptsize ± 0.194}} & 0.042 {\color{gray}{\scriptsize ± 0.030}} & 0.168 {\color{gray}{\scriptsize ± 0.042}} & 0.009 {\color{gray}{\scriptsize ± 0.006}} \\
Llama3.1-8B-it      & 6.003 {\color{gray}{\scriptsize ± 1.782}} & 0.187 {\color{gray}{\scriptsize ± 0.127}} & 1.082 {\color{gray}{\scriptsize ± 0.244}} & 0.028 {\color{gray}{\scriptsize ± 0.019}} & 0.235 {\color{gray}{\scriptsize ± 0.055}} & 0.006 {\color{gray}{\scriptsize ± 0.004}} \\
Llama3.3-70B-it     & 4.883 {\color{gray}{\scriptsize ± 1.106}} & 0.112 {\color{gray}{\scriptsize ± 0.093}} & 0.942 {\color{gray}{\scriptsize ± 0.198}} & 0.016 {\color{gray}{\scriptsize ± 0.012}} & 0.206 {\color{gray}{\scriptsize ± 0.043}} & 0.003 {\color{gray}{\scriptsize ± 0.003}} \\
Memotron3.1-it     & 5.125 {\color{gray}{\scriptsize ± 1.284}} & 0.114 {\color{gray}{\scriptsize ± 0.094}} & 0.985 {\color{gray}{\scriptsize ± 0.207}} & 0.016 {\color{gray}{\scriptsize ± 0.012}} & 0.216 {\color{gray}{\scriptsize ± 0.046}} & 0.003 {\color{gray}{\scriptsize ± 0.003}} \\
Phi3-Medium-128K-it & 5.063 {\color{gray}{\scriptsize ± 1.914}} & 0.584 {\color{gray}{\scriptsize ± 0.559}} & 1.000 {\color{gray}{\scriptsize ± 0.319}} & 0.087 {\color{gray}{\scriptsize ± 0.083}} & 0.217 {\color{gray}{\scriptsize ± 0.068}} & 0.019 {\color{gray}{\scriptsize ± 0.018}} \\
Phi4                & 5.929 {\color{gray}{\scriptsize ± 1.545}} & 0.657 {\color{gray}{\scriptsize ± 0.472}} & 1.306 {\color{gray}{\scriptsize ± 0.231}} & 0.103 {\color{gray}{\scriptsize ± 0.070}} & 0.286 {\color{gray}{\scriptsize ± 0.050}} & 0.022 {\color{gray}{\scriptsize ± 0.015}} \\
Mistral0.3-7B       & 4.718 {\color{gray}{\scriptsize ± 1.340}} & 0.398 {\color{gray}{\scriptsize ± 0.405}} & 0.941 {\color{gray}{\scriptsize ± 0.240}} & 0.059 {\color{gray}{\scriptsize ± 0.059}} & 0.206 {\color{gray}{\scriptsize ± 0.053}} & 0.013 {\color{gray}{\scriptsize ± 0.013}} \\
Qwen2.5-14B         & 5.184 {\color{gray}{\scriptsize ± 2.241}} & 1.270 {\color{gray}{\scriptsize ± 1.547}} & 1.005 {\color{gray}{\scriptsize ± 0.288}} & 0.182 {\color{gray}{\scriptsize ± 0.221}} & 0.223 {\color{gray}{\scriptsize ± 0.067}} & 0.040 {\color{gray}{\scriptsize ± 0.052}} \\
DeepSeekR1L-8B      & 5.502 {\color{gray}{\scriptsize ± 1.549}} & 0.189 {\color{gray}{\scriptsize ± 0.118}} & 0.955 {\color{gray}{\scriptsize ± 0.204}} & 0.028 {\color{gray}{\scriptsize ± 0.017}} & 0.209 {\color{gray}{\scriptsize ± 0.046}} & 0.006 {\color{gray}{\scriptsize ± 0.004}} \\
DeepSeekR1Q-14B     & 5.126 {\color{gray}{\scriptsize ± 2.375}} & 1.406 {\color{gray}{\scriptsize ± 1.609}} & 0.900 {\color{gray}{\scriptsize ± 0.269}} & 0.198 {\color{gray}{\scriptsize ± 0.226}} & 0.199 {\color{gray}{\scriptsize ± 0.062}} & 0.044 {\color{gray}{\scriptsize ± 0.052}} \\
\bottomrule
\end{tabular}
\end{table*}
\FloatBarrier

\begin{table*}[ht]
\centering
% \scriptsize % Use \scriptsize to make it even more compact
\setlength{\tabcolsep}{2.5pt} % Adjust horizontal spacing between columns if needed
\caption{Quantization error (mean ± std) for the key and value caches ($K_i$ and $V_i$) at 2-bit, 3-bit, and 4-bit quantization, evaluated on the C4 dataset.}
\label{tab:c4_loss_bits}
\begin{tabular}{ll>{\columncolor[gray]{0.9}}lll>{\columncolor[gray]{0.9}}ll}
\toprule
Model               &            K2 &            V2 &            K3 &            V3 &            K4 &            V4 \\
\midrule
Llama3.2-1B         & 4.885 {\color{gray}{\scriptsize ± 1.056}} & 0.207 {\color{gray}{\scriptsize ± 0.166}} & 1.074 {\color{gray}{\scriptsize ± 0.289}} & 0.030 {\color{gray}{\scriptsize ± 0.024}} & 0.233 {\color{gray}{\scriptsize ± 0.062}} & 0.006 {\color{gray}{\scriptsize ± 0.005}} \\
Llama3.2-1B-it      & 4.524 {\color{gray}{\scriptsize ± 1.108}} & 0.193 {\color{gray}{\scriptsize ± 0.137}} & 0.925 {\color{gray}{\scriptsize ± 0.235}} & 0.028 {\color{gray}{\scriptsize ± 0.020}} & 0.201 {\color{gray}{\scriptsize ± 0.050}} & 0.006 {\color{gray}{\scriptsize ± 0.005}} \\
Llama3.2-3B         & 3.885 {\color{gray}{\scriptsize ± 0.777}} & 0.282 {\color{gray}{\scriptsize ± 0.150}} & 0.909 {\color{gray}{\scriptsize ± 0.168}} & 0.042 {\color{gray}{\scriptsize ± 0.023}} & 0.194 {\color{gray}{\scriptsize ± 0.032}} & 0.009 {\color{gray}{\scriptsize ± 0.005}} \\
Llama3.2-3B-it      & 4.135 {\color{gray}{\scriptsize ± 1.088}} & 0.274 {\color{gray}{\scriptsize ± 0.137}} & 0.912 {\color{gray}{\scriptsize ± 0.176}} & 0.039 {\color{gray}{\scriptsize ± 0.020}} & 0.195 {\color{gray}{\scriptsize ± 0.036}} & 0.009 {\color{gray}{\scriptsize ± 0.004}} \\
Llama2-7B           & 6.337 {\color{gray}{\scriptsize ± 1.710}} & 0.456 {\color{gray}{\scriptsize ± 0.247}} & 1.054 {\color{gray}{\scriptsize ± 0.263}} & 0.071 {\color{gray}{\scriptsize ± 0.038}} & 0.213 {\color{gray}{\scriptsize ± 0.052}} & 0.015 {\color{gray}{\scriptsize ± 0.008}} \\
Llama3.1-8B-it      & 6.262 {\color{gray}{\scriptsize ± 1.789}} & 0.254 {\color{gray}{\scriptsize ± 0.185}} & 1.128 {\color{gray}{\scriptsize ± 0.249}} & 0.036 {\color{gray}{\scriptsize ± 0.026}} & 0.247 {\color{gray}{\scriptsize ± 0.056}} & 0.008 {\color{gray}{\scriptsize ± 0.005}} \\
Llama3.3-70B-it     & 4.391 {\color{gray}{\scriptsize ± 1.027}} & 0.121 {\color{gray}{\scriptsize ± 0.097}} & 0.847 {\color{gray}{\scriptsize ± 0.175}} & 0.017 {\color{gray}{\scriptsize ± 0.013}} & 0.186 {\color{gray}{\scriptsize ± 0.038}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
Memotron3.1-it     & 5.367 {\color{gray}{\scriptsize ± 1.332}} & 0.127 {\color{gray}{\scriptsize ± 0.105}} & 1.049 {\color{gray}{\scriptsize ± 0.222}} & 0.018 {\color{gray}{\scriptsize ± 0.013}} & 0.231 {\color{gray}{\scriptsize ± 0.049}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
Phi3-Medium-128K-it & 4.831 {\color{gray}{\scriptsize ± 1.759}} & 0.788 {\color{gray}{\scriptsize ± 0.726}} & 1.022 {\color{gray}{\scriptsize ± 0.306}} & 0.109 {\color{gray}{\scriptsize ± 0.097}} & 0.220 {\color{gray}{\scriptsize ± 0.064}} & 0.023 {\color{gray}{\scriptsize ± 0.021}} \\
Phi4                & 5.715 {\color{gray}{\scriptsize ± 1.442}} & 0.850 {\color{gray}{\scriptsize ± 0.684}} & 1.316 {\color{gray}{\scriptsize ± 0.245}} & 0.124 {\color{gray}{\scriptsize ± 0.093}} & 0.291 {\color{gray}{\scriptsize ± 0.056}} & 0.027 {\color{gray}{\scriptsize ± 0.020}} \\
Mistral0.3-7B       & 5.027 {\color{gray}{\scriptsize ± 1.332}} & 0.543 {\color{gray}{\scriptsize ± 0.493}} & 1.014 {\color{gray}{\scriptsize ± 0.269}} & 0.079 {\color{gray}{\scriptsize ± 0.068}} & 0.223 {\color{gray}{\scriptsize ± 0.060}} & 0.017 {\color{gray}{\scriptsize ± 0.015}} \\
Qwen2.5-14B         & 4.382 {\color{gray}{\scriptsize ± 2.170}} & 1.544 {\color{gray}{\scriptsize ± 1.872}} & 0.846 {\color{gray}{\scriptsize ± 0.250}} & 0.220 {\color{gray}{\scriptsize ± 0.265}} & 0.187 {\color{gray}{\scriptsize ± 0.060}} & 0.048 {\color{gray}{\scriptsize ± 0.060}} \\
DeepSeekR1L-8B      & 4.575 {\color{gray}{\scriptsize ± 1.122}} & 0.204 {\color{gray}{\scriptsize ± 0.134}} & 0.817 {\color{gray}{\scriptsize ± 0.141}} & 0.030 {\color{gray}{\scriptsize ± 0.019}} & 0.179 {\color{gray}{\scriptsize ± 0.033}} & 0.006 {\color{gray}{\scriptsize ± 0.004}} \\
DeepSeekR1Q-14B     & 4.832 {\color{gray}{\scriptsize ± 2.354}} & 1.651 {\color{gray}{\scriptsize ± 1.914}} & 0.927 {\color{gray}{\scriptsize ± 0.283}} & 0.232 {\color{gray}{\scriptsize ± 0.267}} & 0.201 {\color{gray}{\scriptsize ± 0.061}} & 0.051 {\color{gray}{\scriptsize ± 0.060}} \\
\bottomrule
\end{tabular}
\end{table*}
\FloatBarrier

\begin{table*}[ht]
\centering
% \scriptsize % Use \scriptsize to make it even more compact
\setlength{\tabcolsep}{2.5pt} % Adjust horizontal spacing between columns if needed
\caption{Quantization error (mean ± std) for the key and value caches ($K_i$ and $V_i$) at 2-bit, 3-bit, and 4-bit quantization, evaluated on the GSM8K dataset.}
\label{tab:gsm8k_loss_qbits}
\begin{tabular}{ll>{\columncolor[gray]{0.9}}lll>{\columncolor[gray]{0.9}}ll}
\toprule
Model                &            K2 &            V2 &            K3 &            V3 &            K4 &            V4 \\
\midrule
Llama3.2-1B          & 5.703 {\color{gray}{\scriptsize ± 1.557}} & 0.179 {\color{gray}{\scriptsize ± 0.136}} & 1.213 {\color{gray}{\scriptsize ± 0.352}} & 0.026 {\color{gray}{\scriptsize ± 0.020}} & 0.266 {\color{gray}{\scriptsize ± 0.078}} & 0.005 {\color{gray}{\scriptsize ± 0.004}} \\
Llama3.2-1B-it       & 5.002 {\color{gray}{\scriptsize ± 1.383}} & 0.171 {\color{gray}{\scriptsize ± 0.130}} & 1.024 {\color{gray}{\scriptsize ± 0.287}} & 0.025 {\color{gray}{\scriptsize ± 0.020}} & 0.223 {\color{gray}{\scriptsize ± 0.061}} & 0.006 {\color{gray}{\scriptsize ± 0.004}} \\
Llama3.2-3B          & 4.840 {\color{gray}{\scriptsize ± 1.396}} & 0.261 {\color{gray}{\scriptsize ± 0.136}} & 1.045 {\color{gray}{\scriptsize ± 0.211}} & 0.038 {\color{gray}{\scriptsize ± 0.021}} & 0.226 {\color{gray}{\scriptsize ± 0.044}} & 0.008 {\color{gray}{\scriptsize ± 0.005}} \\
Llama3.2-3B-it       & 3.604 {\color{gray}{\scriptsize ± 0.850}} & 0.226 {\color{gray}{\scriptsize ± 0.129}} & 0.790 {\color{gray}{\scriptsize ± 0.135}} & 0.034 {\color{gray}{\scriptsize ± 0.019}} & 0.171 {\color{gray}{\scriptsize ± 0.028}} & 0.007 {\color{gray}{\scriptsize ± 0.004}} \\
Llama2-7B            & 5.081 {\color{gray}{\scriptsize ± 1.396}} & 0.405 {\color{gray}{\scriptsize ± 0.231}} & 0.969 {\color{gray}{\scriptsize ± 0.238}} & 0.065 {\color{gray}{\scriptsize ± 0.037}} & 0.205 {\color{gray}{\scriptsize ± 0.050}} & 0.014 {\color{gray}{\scriptsize ± 0.008}} \\
Llama3.1-8B-it       & 6.445 {\color{gray}{\scriptsize ± 1.837}} & 0.213 {\color{gray}{\scriptsize ± 0.161}} & 1.184 {\color{gray}{\scriptsize ± 0.268}} & 0.030 {\color{gray}{\scriptsize ± 0.022}} & 0.257 {\color{gray}{\scriptsize ± 0.060}} & 0.007 {\color{gray}{\scriptsize ± 0.005}} \\
Llama3.3-70B-it      & 4.967 {\color{gray}{\scriptsize ± 1.127}} & 0.113 {\color{gray}{\scriptsize ± 0.091}} & 0.978 {\color{gray}{\scriptsize ± 0.203}} & 0.016 {\color{gray}{\scriptsize ± 0.012}} & 0.214 {\color{gray}{\scriptsize ± 0.044}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
Memotron3.1-it      & 4.752 {\color{gray}{\scriptsize ± 1.124}} & 0.113 {\color{gray}{\scriptsize ± 0.089}} & 0.940 {\color{gray}{\scriptsize ± 0.194}} & 0.016 {\color{gray}{\scriptsize ± 0.012}} & 0.206 {\color{gray}{\scriptsize ± 0.042}} & 0.004 {\color{gray}{\scriptsize ± 0.003}} \\
Phi3-Medium-128K-it  & 4.940 {\color{gray}{\scriptsize ± 1.834}} & 0.605 {\color{gray}{\scriptsize ± 0.579}} & 1.042 {\color{gray}{\scriptsize ± 0.320}} & 0.088 {\color{gray}{\scriptsize ± 0.082}} & 0.227 {\color{gray}{\scriptsize ± 0.069}} & 0.019 {\color{gray}{\scriptsize ± 0.018}} \\
Phi4                 & 6.610 {\color{gray}{\scriptsize ± 1.624}} & 0.785 {\color{gray}{\scriptsize ± 0.598}} & 1.498 {\color{gray}{\scriptsize ± 0.293}} & 0.116 {\color{gray}{\scriptsize ± 0.082}} & 0.330 {\color{gray}{\scriptsize ± 0.064}} & 0.025 {\color{gray}{\scriptsize ± 0.017}} \\
Mistral0.3-7B        & 5.308 {\color{gray}{\scriptsize ± 1.367}} & 0.461 {\color{gray}{\scriptsize ± 0.434}} & 1.065 {\color{gray}{\scriptsize ± 0.288}} & 0.067 {\color{gray}{\scriptsize ± 0.061}} & 0.232 {\color{gray}{\scriptsize ± 0.061}} & 0.015 {\color{gray}{\scriptsize ± 0.013}} \\
Qwen2.5-14B          & 4.829 {\color{gray}{\scriptsize ± 2.179}} & 1.736 {\color{gray}{\scriptsize ± 2.659}} & 0.979 {\color{gray}{\scriptsize ± 0.264}} & 0.241 {\color{gray}{\scriptsize ± 0.372}} & 0.214 {\color{gray}{\scriptsize ± 0.061}} & 0.051 {\color{gray}{\scriptsize ± 0.077}} \\
DeepSeekR1L-8B       & 5.547 {\color{gray}{\scriptsize ± 1.517}} & 0.193 {\color{gray}{\scriptsize ± 0.129}} & 1.000 {\color{gray}{\scriptsize ± 0.212}} & 0.028 {\color{gray}{\scriptsize ± 0.018}} & 0.218 {\color{gray}{\scriptsize ± 0.049}} & 0.006 {\color{gray}{\scriptsize ± 0.004}} \\
DeepSeekR1Q-14B      & 4.477 {\color{gray}{\scriptsize ± 2.176}} & 1.424 {\color{gray}{\scriptsize ± 1.752}} & 0.830 {\color{gray}{\scriptsize ± 0.256}} & 0.200 {\color{gray}{\scriptsize ± 0.242}} & 0.181 {\color{gray}{\scriptsize ± 0.058}} & 0.044 {\color{gray}{\scriptsize ± 0.056}} \\
\bottomrule
\end{tabular}
\end{table*}
\FloatBarrier



\end{document}
