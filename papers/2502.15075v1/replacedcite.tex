\section{Related Work}
\label{sec:related}

\paragraph{Model Quantization.} 
 for large models such as LLMs ____, which is why post-training quantization (PTQ) techniques are generally preferred for quantizing these models.

Quantization reduces the computational cost of neural network inference by lowering model bit-precision ____. It can be divided into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ requires no re-training or labeled data ____ and is preferred for large models like LLMs, whereas QAT demands fine-tuning with labeled data ____ but scales poorly for such models ____.

\paragraph{LLMs Quantization.} 


Researchers consider three post-training quantization settings for LLMs: weight-only, weight-activation, and KV cache quantization. Weight-only quantization focuses solely on weights ____, while weight-activation quantization targets both weights and activations ____. Since longer inputs inflate the KV cacheâ€™s memory overhead ____, KV cache quantization offers memory savings similar to weight-activation methods while maintaining performance near that of weight-only quantization ____.

\paragraph{KV Cache Quantization.} 

Recent KV cache quantization methods reduce LLM memory and accelerate inference, falling into three categories: outlier redistribution, fixed-precision, and mixed-precision ____. Outliers in KV cache are addressed by redistributing them or applying smoothing transformations ____. Fixed-precision methods use a uniform bit-width ____ but can overlook token importance and outliers. In contrast, mixed-precision allocates higher precision to critical tokens and lower precision elsewhere ____.
Existing methods like SKVQ and QAQ rely on specific engineering solutions or predefined parameters, whereas \mn{} uses a principled, analysis-driven framework that leverages the intrinsic properties of key and value caches for a more robust and adaptable quantization approach.