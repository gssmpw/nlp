\section{Related Work}
\label{sec:related}

\paragraph{Model Quantization.} 
 for large models such as LLMs \cite{efficientllm}, which is why post-training quantization (PTQ) techniques are generally preferred for quantizing these models.

Quantization reduces the computational cost of neural network inference by lowering model bit-precision \cite{deepcompression, whitepaperquant}. It can be divided into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ requires no re-training or labeled data \cite{quantizationtrainingneuralnetworks, datafreequantizationweightequalization} and is preferred for large models like LLMs, whereas QAT demands fine-tuning with labeled data \cite{whitepaperquant, surveyquantizationmethodsefficient} but scales poorly for such models \cite{efficientllm}.

\paragraph{LLMs Quantization.} 


Researchers consider three post-training quantization settings for LLMs: weight-only, weight-activation, and KV cache quantization. Weight-only quantization focuses solely on weights \cite{gptq, obq, awq}, while weight-activation quantization targets both weights and activations \cite{llmint8, smoothquant, omniquant}. Since longer inputs inflate the KV cacheâ€™s memory overhead \cite{efficientlyscalingtransformerinference}, KV cache quantization offers memory savings similar to weight-activation methods while maintaining performance near that of weight-only quantization \cite{wkvquant}.

\paragraph{KV Cache Quantization.} 

Recent KV cache quantization methods reduce LLM memory and accelerate inference, falling into three categories: outlier redistribution, fixed-precision, and mixed-precision \cite{kvcachequantsurvey}. Outliers in KV cache are addressed by redistributing them or applying smoothing transformations \cite{smoothquant, awq, omniquant}. Fixed-precision methods use a uniform bit-width \cite{zeroquant, flexgen, kivi} but can overlook token importance and outliers. In contrast, mixed-precision allocates higher precision to critical tokens and lower precision elsewhere \cite{kvquant, wkvquant, snapkv, qaq, skvq}.
Existing methods like SKVQ and QAQ rely on specific engineering solutions or predefined parameters, whereas \mn{} uses a principled, analysis-driven framework that leverages the intrinsic properties of key and value caches for a more robust and adaptable quantization approach.