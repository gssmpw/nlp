[
  {
    "index": 0,
    "papers": [
      {
        "key": "efficientllm",
        "author": "Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and Zhang, Mi",
        "title": "Efficient Large Language Models: A Survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "deepcompression",
        "author": "Han, Song and Mao, Huizi and Dally, William J.",
        "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"
      },
      {
        "key": "whitepaperquant",
        "author": "Markus Nagel and Marios Fournarakis and Rana Ali Amjad and Yelysei Bondarenko and Mart van Baalen and Tijmen Blankevoort",
        "title": "A White Paper on Neural Network Quantization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "quantizationtrainingneuralnetworks",
        "author": "Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry",
        "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
      },
      {
        "key": "datafreequantizationweightequalization",
        "author": "Nagel, Markus and van Baalen, Mart and Blankevoort, Tijmen and Welling, Max",
        "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "whitepaperquant",
        "author": "Markus Nagel and Marios Fournarakis and Rana Ali Amjad and Yelysei Bondarenko and Mart van Baalen and Tijmen Blankevoort",
        "title": "A White Paper on Neural Network Quantization"
      },
      {
        "key": "surveyquantizationmethodsefficient",
        "author": "Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer",
        "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "efficientllm",
        "author": "Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and Zhang, Mi",
        "title": "Efficient Large Language Models: A Survey"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
      },
      {
        "key": "obq",
        "author": "Frantar, Elias and Singh, Sidak Pal and Alistarh, Dan",
        "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"
      },
      {
        "key": "awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "llmint8",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
      },
      {
        "key": "smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
      },
      {
        "key": "omniquant",
        "author": "Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping",
        "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "efficientlyscalingtransformerinference",
        "author": "Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff",
        "title": "Efficiently Scaling Transformer Inference"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wkvquant",
        "author": "Yuxuan Yue and Zhihang Yuan and Haojie Duanmu and Sifan Zhou and Jianlong Wu and Liqiang Nie",
        "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "kvcachequantsurvey",
        "author": "Haoyang Li and Yiming Li and Anxin Tian and Tianhao Tang and Zhanchao Xu and Xuejia Chen and Nicole Hu and Wei Dong and Qing Li and Lei Chen",
        "title": "A Survey on Large Language Model Acceleration based on KV Cache Management"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
      },
      {
        "key": "awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      },
      {
        "key": "omniquant",
        "author": "Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping",
        "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zeroquant",
        "author": "Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong",
        "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"
      },
      {
        "key": "flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y. and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E. and Liang, Percy and R\u00e9, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU"
      },
      {
        "key": "kivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kvquant",
        "author": "Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W. and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir",
        "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"
      },
      {
        "key": "wkvquant",
        "author": "Yuxuan Yue and Zhihang Yuan and Haojie Duanmu and Sifan Zhou and Jianlong Wu and Liqiang Nie",
        "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More"
      },
      {
        "key": "snapkv",
        "author": "Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen",
        "title": "SnapKV: LLM Knows What You are Looking for Before Generation"
      },
      {
        "key": "qaq",
        "author": "Shichen Dong and Wen Cheng and Jiayu Qin and Wei Wang",
        "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache"
      },
      {
        "key": "skvq",
        "author": "Duanmu, Haojie and Yuan, Zhihang and Li, Xiuhong and Duan, Jiangfei and Zhang, Xingcheng and Lin, Dahua",
        "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models"
      }
    ]
  }
]