\section{Methodology}
% SAT solvers and samplers have been optimized over decades to efficiently handle problems in CNF. CNF, a form that consists of a conjunction of disjunctions (i.e., clauses), is well-suited for SAT-solving algorithms like DPLL and CDCL. These algorithms take advantage of CNF's structure to systematically explore possible truth assignments, detect conflicts early, and prune the search space efficiently. By focusing on individual clauses, which define specific constraints on solutions, the use of CNF enables solvers to address highly complex problems in a manageable way.

% As discussed in Section \ref{sec:sat-sampling}, Boolean expressions are typically expressed in more abstract logical formats before being converted to CNF through techniques like logical simplifications and transformations such as the Tseitin transformation. This transformation preserves the satisfiability of the original formula while introducing auxiliary variables when needed. The conversion to CNF provides SAT solvers with a standardized problem representation that retains the essential constraints of the original problem.

In this section, we present a transformation algorithm paired with an optimization method to generate valid and diverse solutions to SAT problems. Our algorithm transforms a CNF from its flat, two-level structure into a more streamlined, multi-level, multi-output Boolean function, reducing both operations and logical constraints. We then apply gradient-based optimization to efficiently learn valid solutions using the simplified multi-level, multi-output function.

As discussed in Section \ref{sec:sat-sampling}, Boolean expressions are typically represented in more abstract logical formats before being converted to CNF. In other words, the sub-expressions (i.e., sub-clauses) in a CNF often result from the transformation of single logical operators or the combination of multiple operators. This transformation increases the size of the CNF. Therefore, a sampler that could operate directly on the logical operators represented by the sub-expressions in a CNF would be advantageous. Motivated by this, we introduce our transformation algorithm, which converts CNF into an equisatisfiable multi-level, multi-output Boolean function, and demonstrate how this function can be used to generate various valid and distinct solutions using gradient-based optimization.


\subsection{Transformation Algorithm}
Let us first review the clauses, known as the CNF signature \cite{roy2004restoring}, which represent primary logical operators as a result of the Tseitin transformation. The CNF structure of an inverter with the input $x$ and the output $f$, i.e., $f(x) = \neg x$, is given by 
\begin{equation}
    (f \vee x) \wedge (\neg f \vee \neg x), 
\end{equation} 
where $\vee$ and $\wedge$ denote logical OR and AND operators, respectively. The clauses representing the logical OR operator with $n$ inputs and an output $f$, i.e., $f(x_1, x_2, \dots, x_n) = \bigvee_{i=1}^n x_i$, can be expressed as
\begin{equation}
    \left(\neg f \vee \bigvee_{i=1}^n x_i \right) \wedge \left( \bigwedge_{i=1}^n (f \vee \neg x_i) \right).
\end{equation}
The clauses representing the logical AND operator with $n$ inputs and an output $f$, i.e., $f(x_1, x_2, \dots, x_n) = \bigwedge_{i=1}^n x_i$, are similar to those of the logical OR operator with its input and output variables inverted, i.e., 
\begin{equation}
    \left(f \vee \bigvee_{i=1}^n \neg x_i \right) \wedge \left( \bigwedge_{i=1}^n (\neg f \vee x_i) \right).
\end{equation}
The CNF structure of the logical NAND and NOR operators can be derived in a similar manner by inverting the output variable in the clauses associated with the logical AND and OR operators, respectively. The CNF signature of the logical XOR operator with $n$ inputs and an output $f$ is is given by \vspace{-0.2cm}
\begin{align}
    & (\neg f \vee \text{XOR}(x_1, \dots, x_n)) \wedge (f \vee \neg \text{XOR}(x_1, \dots, x_n)) \nonumber \\
    = & \neg \text{XOR}(x_1, \dots, x_n, f) = \text{XNOR}(x_1, \dots, x_n, f).\vspace{-0.2cm}
\end{align}
Similarly, the CNF structure for the logical XNOR operation can be described as $\text{XOR}(x_1, \dots, x_n, f)$.


While deriving logical operators from the CNF signatures described above is straightforward through pattern matching, this method is limited to identifying clauses linked to primary logical operators. It does not handle sub-expressions that may involve more complex Boolean expressions constructed from these operators. For example, the clauses\vspace{-0.2cm}
\begin{align}
    & (\neg x_4 \vee \neg x_{107} \vee x_5) \wedge (\neg x_4 \vee x_{107} \vee \neg x_5) \wedge \nonumber \\
    & (x_4 \vee \neg x_{108} \vee x_5) \wedge (x_4 \vee x_{108} \vee \neg x_5) \vspace{-0.2cm}
    \label{c1}
\end{align}
from the '$75$-$10$-$1$-q' CNF instance correspond to the function $x_5(x_4, x_{107}, x_{108}) = (x_{107} \wedge x_4) \vee (x_{108} \wedge \neg x_4)$, which cannot be identified using pattern matching alone, as it is impractical to store all possible Boolean patterns. This underscores the need for a general transformation algorithm capable of identifying the Boolean sub-expressions and constraints represented by the clauses.

Before introducing our algorithm, let us establish some key definitions. The primary objective of the algorithm is to transform a given CNF into an acyclic combinational structure while preserving all logical constraints in the original CNF. Variables that correspond to the inputs and outputs of this circuit are referred to as primary input variables and primary output variables, respectively. Variables representing the intermediate signals within the circuit are called intermediate variables. With these definitions in place, once a variable is identified as a primary input or intermediate variable, it cannot be redefined as an output variable in subsequent Boolean expressions, due to the acyclic nature of the circuit.

The main challenge in this transformation is first to identify sub-clauses corresponding to a Boolean expression and then to derive that expression. To achieve this, we begin by reading the first clause in the CNF. For each variable in the clause, if it has not already been defined as a primary input or intermediate variable, we treat it as a potential output of the clause or clauses read so far. We then derive its Boolean expression based on the clause or clauses (read so far) containing this variable in its negated form, and similarly derive its complementary expression from the clauses where the variable appears in its true form. If the resulting Boolean expressions are indeed complements of each other, we have successfully identified the Boolean expression for the current set of clauses. If not, we proceed to the next clause and repeat the process until the corresponding Boolean expression is found. Once the Boolean expression is identified, we designate its output variable as an intermediate variable. Additionally, if any input variables in the derived Boolean expression have not yet been classified as intermediate variables, they are now recognized as primary input variables. In case the resulting Boolean expression is identified to be a constant Boolean function with its output being a constant value of either $0$ or $1$, its output variable is recognized as a primary output variable. The obtained Boolean expression is simplified before adoption in the final circuit structure. For all Boolean manipulations, such as simplification and complement checking, we utilize the symbolic Boolean algebra system SymPy \cite{sympy}, a Python library for symbolic mathematics. It is worth noting that the resulting multi-level, multi-output Boolean function from our transformation can be further optimized by leveraging other techniques \cite{Robert2010ABC, Alan2006synthesis, Alan2011synthesis}, for reducing the complexity of multi-level logic circuits, potentially leading to even more compact Boolean functions. Our transformation process is summarized in Algorithm \ref{alg1}.





To clarify the transformation process, let us revisit the clauses in Eq. (\ref{c1}). Treating $x_5$ as a potential output variable, we derive its Boolean expression from the clauses where $x_5$ appears in its negated form (i.e., $(\neg x_4 \vee x_{107} \vee \neg x_5) \wedge (x_4 \vee x_{108} \vee \neg x_5)$). Since $\neg x_5 = 0$ in these clauses, it becomes non-contributory, resulting in the Boolean expression $x_5(x_4, x_{107}, x_{108}) = (x_{107} \wedge x_4) \vee (x_{108} \wedge \neg x_4)$. This is because we are determining the Boolean expression for $x_5 = 1$, and thus the remaining clauses containing $x_5$ in its true form are already satisfied and do not contribute to the expression.


Similarly, we derive the complementary Boolean expression using the clauses where $x_5$ appears in its true form (i.e., $(\neg x_4 \vee \neg x_{107} \vee x_5) \wedge (x_4 \vee \neg x_{108} \vee x_5)$). In this case, $x_5 = 0$ rendering it non-contributory and resulting in $\neg x_5(x_4, x_{107}, x_{108}) = (\neg x_{107} \wedge x_4) \vee (\neg x_{108} \wedge \neg x_4)$. Since these two expressions are complementary, it confirms the validity of the Boolean expression with the specified input and output variables. It is worth mentioning that repeating this process for other variables as potential output variables does not yield complementary expressions, thereby invalidating the assumption for those cases.  


\begin{algorithm}[t]
\caption{Pseudo Code of our Transformation Method}
\include{alg1}
\label{alg1}
\end{algorithm}

During the transformation process, some sub-clauses may be under-specified, making them difficult to translate directly into a Boolean expression. A simple example is an OR function $x_3(x_1, x_2) = x_1 \vee x_2$, where the output is constrained to 1. The full description of the associated clauses would be $(\neg x_3 \vee x_1 \vee x_2) \wedge (x_3 \vee \neg x_1) \wedge (x_3 \vee \neg x_2) \wedge x_3$. However, since $x_3$ is equal to 1 in this case, these clauses can be simplified to $(x_1 \vee x_2)$. In this simplified form, where the output variable is not explicitly specified, we cannot directly extract the function with its output constrained to 1. To handle such cases, if the current clauses do not share variables with subsequent clauses, we simplify these clauses to a Boolean expression as the result of the transformation. The variables of these clauses are then treated as input variables to the simplified Boolean expression, and an auxiliary variable is assigned to the output, which is constrained to $1$.

\begin{figure*}
    \centering
    \scalebox{0.85}{\input{fig1.tex}}
    \vspace{-0.25cm}
    \caption{The overall workflow of our sampling approach is illustrated including (a) a CNF example with comments highlighted in green, (b) its simplified multi-level, multi-output Boolean function in a circuit form for illustrative purposes with unconstrained and constrained paths highlighted in blue and red, respectively, and (c) its corresponding PyTorch description. }
    \label{fig1}
    \vspace{-0.5cm}
\end{figure*}


After transforming all the clauses, we integrate the resulting Boolean sub-expressions to construct a multi-level, multi-output Boolean function that is equisatisfiable with the original CNF. This function contains two types of paths: \textit{constrained paths} and \textit{unconstrained paths}. Constrained paths are those that run from primary inputs to primary outputs. The inputs along these paths must be adjusted to satisfy the explicit constraints applied to the primary output variables. Unconstrained paths originate from primary inputs and terminate at intermediate variables. Since they are not explicitly constrained to any values, any random initialization of their primary inputs will yield satisfying solutions for the variables on these paths. Fig. \ref{fig1}(a) and \ref{fig1}(b) present an example of a CNF and its corresponding circuit structure, produced through our transformation method. The figures highlight two types of paths: unconstrained paths in blue and constrained paths in red. In this example, any random assignment to the primary input variables $x_1$, $x_{11}$, and $x_{12}$ will satisfy the sub-clauses associated to the unconstrained path. Conversely, the primary input variables for the constrained path, namely $x_6$, $x_{13}$, and $x_{14}$, must be carefully selected such that $x_{10}$ is forced to be $1$.


With the definitions in place, the SAT solving/sampling problem now becomes a task of finding the primary inputs in the constrained paths to the multi-level, multi-output Boolean function resulting from the CNF transformation. Once the values of the primary input variables are determined to satisfy the constraints on the primary output variables, the intermediate variables can be computed using the corresponding Boolean operators in the function (see Fig. \ref{fig1}(c)).

To solve for the primary input variables in the constrained paths, we use GD optimization. Specifically, we reformulate the solving/sampling problem into a multi-output regression task, where the objective is to learn the primary input variables based on explicit constraints on the primary outputs and a differentiable model that maps the inputs to the outputs. While the multi-level, multi-output Boolean function fulfills the mapping requirements, it is not differentiable in its discrete form. To address this, we employ a probabilistic representation of logical operators \cite{Ardakani2017SC, NEURIPS2019_6562c5c1} such as AND, OR, NOT, XOR, and XNOR, enabling differentiability in the model as shown in Table \ref{tab1}. Such a representation allows us to relax the discrete operations into continuous ones.


\begin{table}
    \centering
    \caption{The probabilistic representation of logical operators.}
    \include{table1}
    \label{tab1}
    \vspace{-0.5cm}
\end{table}


With the probabilistic model derived from replacing the discrete logical operators of the multi-level, multi-output Boolean function with their probabilistic counterparts, we can now learn satisfying primary input variables given the constraints on the output variables. To find satisfying solutions, we define the primary input variables a matrix $\textbf{V} \in \mathbb{R}^{b \times n}$, where $n$ indicates the number of primary input variables and $b$ denotes the batch size. Since our continuous multi-level, multi-output Boolean function takes real values between $0$ and $1$ in a form of probability, we convert the primary input variables into probabilities using the sigmoid function $\sigma(\cdot)$, such that:\vspace{-0.2cm}
\begin{equation}
    \textbf{P} = \sigma(\textbf{V}),\vspace{-0.1cm}
\end{equation}
where $\textbf{P} \in [0,1]^{b \times n}$ represents the input probabilities fed into the model. We refer to this conversion process as a form of continuous embedding, as it transforms the input space into a continuous probability space, allowing the model to interpret the inputs probabilistically. The model's primary outputs are then computed as:
\begin{equation}\vspace{-0.1cm}
    \textbf{Y} = \mathcal{F}(\textbf{P}),\vspace{-0.1cm}
\end{equation}
where $\mathcal{F}:[0,1]^{b \times n} \rightarrow [0,1]^{b \times m}$ represents the probabilistic multi-level, multi-output Boolean function. The matrix $\textbf{Y} \in [0,1]^{b \times m}$ holds the $m$ primary output variables across the $b$ data batches.


To optimize the inputs, we define an $\ell_2$-loss function $\mathcal{L}$ that measures the difference between the computed outputs $\textbf{Y}$ and the target output matrix $\textbf{T} \in {0,1}^{b \times m}$:\vspace{-0.2cm}
\begin{equation}
    \mathcal{L} = \sum_{b,m} \left|\left| \textbf{Y} - \textbf{T} \right|\right|^2_2.\vspace{-0.2cm}
\end{equation}
By minimizing this loss function through GD, the primary input variables (i.e., $\textbf{V}$) are iteratively updated. Upon convergence, the $b$ solutions are determined by converting the soft input values (i.e., $\textbf{V}$) into hard binary values (i.e., $\widetilde{\textbf{V}} \in {0, 1}^{b \times n}$). Among these $b$ solutions, only those that are satisfiable and non-redundant are retained as satisfiable solutions.

In gradient descent optimization, the objective is to compute the derivative of the loss function with respect to each primary input variable associated with the constrained paths. This can be achieved using the chain rule, leveraging the derivatives of the logical operators presented in Table \ref{tab1}. For instance, in the constrained path highlighted in red in Fig. \ref{fig1}(b), the derivative of the loss function with respect to the primary input variable $x_{13}$ can be expressed as follows:
\begin{equation}
    \dfrac{\partial \mathcal{L}}{\partial x_{13}} = \dfrac{\partial \mathcal{L}}{\partial x_{10}} \cdot \dfrac{\partial x_{10}}{\partial x_{13}} = 2\cdot (x_{10} - 1) \cdot (1 - x_9 \cdot x_{14})\cdot (1 - x_9).
\end{equation}
For simplicity, we will exclude the embedding process from our calculations in this example. With the computed derivative, the value of $x_{13}$ is updated using the following equation:
\begin{equation}
    x_{13} = x_{13} - \gamma \cdot \dfrac{\partial \mathcal{L}}{\partial x_{13}},
\end{equation}
where $\gamma$ represents the learning rate.

The overall workflow is illustrated in Fig. \ref{fig1}. Our method integrates a parser that describes the probabilistic multi-level, multi-output Boolean function  in PyTorch. Since each solution is processed and learned independent from others, our approach is highly parallelizable and benefits from GPU acceleration, enables fast and scalable sampling by processing multiple data batches simultaneously.



