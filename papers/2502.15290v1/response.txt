\section{Related Work}
\subsection{Zero-shot Information Extraction}
%
Information extraction (IE) encompasses a range of tasks, notably named entity
typing and relation extraction, aimed at distilling structural information from
unstructured texts for the purpose of constructing comprehensive knowledge
graphs**Rastogi et al., "ZILLOW: Zero-Shot Learning for Textual Named Entity Typing"**
**Li et al., "Zero-Shot Relation Extraction with BERT"**.
%
Considering to recognize the unseen categories like: entity types or relations
without additional training, the zero-shot learning was introduced into
traditional information extraction (ZS-IE) tasks.
%
The vital challenge for ZS-IE is to learn generalizable representations of
entities and prototypical knowledge of categories.

For zero-shot named entity typing (ZS-ET), **Rastogi et al., "ZILLOW: Zero-Shot Learning for Textual Named Entity Typing"**
firstly proposed a label embedding method to encode the prototypical knowledge
of types with textual embeddings, and bridge the semantic correlation between
entity mentions and types.
%
**Li et al., "Zero-Shot Relation Extraction with BERT"** employed the attention mechanism to extract local
features that are relevant to the types, with a focus on the nuanced semantic
representations of both mentions and their contexts.
%
**Pan et al., "A Zero-Shot Named Entity Typing Model via Prototypical Knowledge Transfer"**
devised the ZS-ET model, augmented with
memory capabilities, to retain observed types as memory elements and facilitate
knowledge transfer from known to unknown types, thereby explicitly capturing the
semantic relationship between them.
%
Furthermore, auxiliary data including descriptions sourced from websites was
integrated into the ZS-ET model to augment the representation of mentions and
types**Rastogi et al., "ZILLOW: Zero-Shot Learning for Textual Named Entity Typing"**
.
%
For zero-shot relation extraction (ZS-RE), **Li et al., "Zero-Shot Relation Extraction with BERT"** initially
leveraged BERT to acquire two functions, which project entities and relation
descriptions into an embedding space by concurrently minimizing the distances
between them and subsequently categorizing their corresponding relations.
%
**Pan et al., "A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction"**
introduced a fine-grained semantic matching
method, which dissects the overall sentence-level similarity score into distinct
components for entity and context matching.
%
**Xu et al., "Prompt-Driven Zero-Shot Relation Extraction with Knowledge Graph Embeddings"**
presented a prompt-driven model that augments
semantic knowledge by creating instances featuring unseen relations from
existing instances with known relations.

In essence, the current ZS-IE approaches solely concentrate on textual modality,
neglecting the potential semantic enrichment from visual content. In contrast to
these endeavors, our focus lies in the ZS-MIE task, aimed at extracting
innovative structural knowledge embedded within multimodal data.

\subsection{Mulitmodal Information Extraction}
%
As the volume of multimodal data continues to expand, researchers have extended
the traditional IE tasks to encompass multimodal IE, resulting in improved
outcomes.
%
**Li et al., "Multimodal Named Entity Recognition via Modality Attention"**
initially broadened the scope of traditional
text-based named entity recognition to encompass multimodal named entity
recognition (MER), introducing the modality attention module to integrate
textual information with image one, thereby enhancing the accuracy of sequence
label predictions.
%
**Wang et al., "Multimodal Named Entity Typing via Cross-Modal Transformers"**
proposed to incorporate visual
objects and exploit the cross-modal transformer to obtain multimodal
representations for tackling the multimodal named entity typing (MET) task
firstly.
%
**Pan et al., "Multimodal Relation Extraction with Visual Modality"**
introduced the multimodal relation extraction
(MRE) task, leveraging visual modality to bolster the semantic representations
of textual modality.
%
**Xu et al., "Variational Information Bottleneck for Multimodal Information Extraction"**
exploited the variational information
bottleneck to extract effective multimodal representations for the MIE tasks.

In summary, the aforementioned multimodal information extraction tasks operate
within a supervised framework. However, our focus lies in zero-shot multimodal
information extraction (ZS-MIE). In contrast to supervised learning that relies
on abundant labeled data, zero-shot learning prioritizes the development of
generalizable representations for both samples and semantic labels, enabling the
inference of samples belonging to unobserved categories.