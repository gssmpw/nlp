{\appendices
\section{Proof of Theorem 1}
The theorem establishes the connection of our objective to the heuristics that we are optimizing the upper bound of the expected KL-divergence of the proposed distribution of true label $\z$ and its corresponding true distribution.

\begin{proof}
Let $\mathcal{H}(\cdot)$ denote the entropy function.
For the unsupervised learning, we obtain
\begin{align*}
&\kl[ q(\z, \zhat \mid \x, \yhat) \mid\mid p(\z, \zhat \mid \x, \yhat)] \\
&= \int \int q(\z, \zhat \mid \x, \yhat) \log \frac{q(\z, \zhat \mid \x, \yhat)}{p(\z, \zhat \mid \x, \yhat)} d\z d\zhat \\
&\ge \int \int q(\z \mid \zhat) q(\zhat \mid \x, \yhat) \log \frac{q(\z \mid \zhat) q(\zhat \mid \x, \yhat)}{p(\z \mid \x, \yhat)} d\z d\zhat \\
&= \int q(\zhat \mid \x, \yhat) \left( \int q(\z \mid \zhat) \log \frac{q(\z \mid \zhat)}{p(\z \mid \x, \yhat)} d\z \right) d\zhat \\
&\quad + \int q(\z \mid \zhat) d\z  \int q(\zhat \mid \x, \yhat)\log q(\zhat \mid \x, \yhat) d\zhat \\
&= \E_{\zhat \sim q(\zhat \mid \x, \yhat)}[\kl[q(\z \mid \zhat) \mid\mid p(\z \mid \x, \yhat) ]] + \underbrace{\mathcal{H}(q(\zhat \mid \x, \yhat))}_{\ge 0} \\
&\ge \E_{\zhat \sim q(\zhat \mid \x, \yhat)}[\kl[q(\z \mid \zhat) \mid\mid p(\z \mid \x, \yhat) ]] \,.
\end{align*}
% where the last line considers the fact that entropy is non-negative.
 
Applying a similar technique, we achieve the results for the supervised version as follows:
\begin{align*}
&\kl[ q(\z, \zhat \mid \x, \y, \yhat) \mid\mid p(\z, \zhat \mid \x, \y, \yhat)] \\
&= \int \int q(\z, \zhat \mid \x, \yhat) \log \frac{q(\z, \zhat \mid \x, \yhat)}{p(\z, \zhat \mid \x, \yhat)} d\z d\zhat \\
&\ge \E_{\zhat \sim q(\zhat \mid \x, \yhat)}[\kl[q(\z \mid \zhat, \x, \yhat) \mid\mid p(\z \mid \x, \yhat) ]] \,.
\end{align*}
The proof completes here.
\end{proof}

\section{Proof of Theorem 2}
\begin{proof} 
Our first step is to show 
\begin{align}
&\kl[ q(\z \mid \x, \yhat = \y_1) \mid\mid q(\z \mid \x, \yhat = \y_0)] \notag \\
&\le \kl[ q(\zhat \mid \x, \yhat = \y_1) \mid\mid q(\zhat \mid \x, \yhat = \y_0)] \,
\end{align}
% For the first inequality in~\cref{eq:gap_ineq}, 
For this inequality, we resort to the data processing inequality (DPI)~\cite{thomas2006elements}.
One form of DPI is as follows: given $P(X)$ and $Q(X)$ which share the same transition function mapping $X \to Y$, the data processing inequality with regard to KL-divergence is 
\begin{align}
  \kl[P(X) \mid \mid Q(X)] \geq \kl[P(Y) \mid \mid Q(Y)] \,. 
\end{align}  
We first notice that the distribution $q(\z \mid \x,  \yhat=\y_0)$ and $ q(\z \mid \x, \yhat=\y_1)$ might differ in the VAEs, since their parameter values could diverge given different inputs of $\yhat$ to the decoder function.
By replacing $X$ by $\zhat \mid \x, \yhat$ and $Y$ by $\z \mid \x, \yhat$, we obtain 
\begin{multline}
\kl[ q(\z \mid \x, \yhat=\y_1) \mid\mid q(\z \mid \x, \yhat=\y_0)] \\
\le \kl[ q(\zhat \mid \x, \yhat=\y_1) \mid\mid q(\zhat \mid \x, \yhat=\y_0)] \,,
\end{multline}  
regardless of the transition function being either $q(\z \mid \zhat)$ or $q(\z \mid \x, \y, \zhat)$ as long as they are applied consistently.
   
With regard to the second inequality in~\cref{eq:gap_ineq}, we apply the result in~\cref{lem:student_bound} and are able to complete the proof.
\end{proof}

Before presenting~\cref{lem:student_bound}, we first summarize the theoretical results of KL-divergence between two multivariate Student distributions from a recent work~\cite{huang2019novel}, shown in~\cref{thm:student_kl}.
\begin{theorem}
\label{thm:student_kl}
Given two multivariate Student distributions, respectively, defined as 
\begin{align*}
p_1(\x) &= \mathrm{Student}(\mu_1, \Sigma_1, \nu_1)  \\
p_2(\x) &= \mathrm{Student}(\mu_2, \Sigma_2, \nu_2) \,.
\end{align*}
Let us fix $\nu_1 > 2$ and denote $\tilde{\Sigma}_1 = \frac {\nu_1}{\nu_1 - 2}\Sigma_1$.
\begin{align}
&\kl [ p_1(\x) \mid \mid p_2(\x) ] \notag \\
&\le \frac 1 2 \log \frac{\mathrm{det}(\Sigma_2)}{\mathrm{det}(\Sigma_1)} + \frac 1 2 \log \frac{\nu_2}{\nu_1} +  \frac 1 2 \Gamma(\nu_2/2) - \frac 1 2 \Gamma(\nu_1/2) \notag \\
&\quad + \frac 1 2 \Gamma((\nu_2+m)/2) - \frac 1 2 \Gamma((\nu_1+m)/2) \notag \\
&\quad - \frac{\nu_1 + m}{2}\{\Psi({(\nu_1 + m)}/{2}) - \Psi(\nu_1/ 2)\} \notag \\
&\quad + \frac {\nu_2 + m} 2 \log \left\{ 1 + \frac{1}{\nu_2} \mathrm{tr}(\Sigma_2^{-1} \tilde{\Sigma}_1)  \right. \notag \\
&\qquad \left. + \frac{1}{\nu_2} \mathrm{tr}(\Sigma_2^{-1} (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T) \right\}
\end{align}
where $\Gamma(\cdot), \Psi(\cdot)$, and $\mathrm{tr}(\cdot)$ are respectively the gamma function, digamma function, and trace of the matrix.  
\end{theorem}

\begin{corollary}
\label{corol:student_kl}
For our case, the two distributions share the same value of $\nu$, i.e., $\nu=\nu_1=\nu_2$.
\cref{thm:student_kl} can be simplified to
\begin{align}
&\kl [ p_1(\x) \mid \mid p_2(\x) ] \notag \\
&\le \frac 1 2 \log \frac{\mathrm{det}(\Sigma_2)}{\mathrm{det}(\Sigma_1)} - \frac{\nu + m}{2}\left\{\Psi\left( \frac{\nu + m}{2} \right) - \Psi\left(\frac{\nu}{2} \right ) \right\} \notag \\
&\quad + \frac {\nu + m} 2 \log \left\{ 1 + \frac{1}{\nu} \mathrm{tr}(\Sigma_2^{-1} \tilde{\Sigma}_1)  \right. \notag \\
&\qquad \left. + \frac{1}{\nu} \mathrm{tr}(\Sigma_2^{-1} (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T) \right\} \,.
\end{align}
\end{corollary}
\begin{proof}
This is a straightforward extension of~\cref{thm:student_kl} by knowing $\nu_1 = \nu_2 = \nu$.
\end{proof}

\begin{lemma}
\label{lem:student_bound}
Consider Assumptions 1 to 3 satisfy. 
Fixing $\alpha = \frac{\sqrt{\nu \lambda}}{L}$, we have 
\begin{align}
\kl[ q(\zhat \mid \x, \y_1) \mid\mid q(\zhat \mid \x, \y_0)] 
\le C_1 + C_2 \Delta(\y_0, \y_1) \label{eq:gap_ineq} \,, 
\end{align}
where 
\begin{align*}
  C_1 &= \frac{\nu+m}{2} \left\{ \frac{M \sqrt{m} \alpha}{2(\nu-2) }  - \Psi\left( \frac{\nu + m}{2} \right) + \Psi\left(\frac{\nu}{2} \right) \right\} \\
  C_2 &= \frac{m M}{2e} + \frac{(\nu+m) \sqrt{m}}{2\alpha}  \,.
\end{align*}
\end{lemma}

\begin{proof}
For convenience, given an input $\x$ and its labels $\y$, we denote 
\begin{align*}
\mu(\y) &\coloneqq \mu_{\theta}(\x, \y) \\
\Sigma(\y) &\coloneqq \mathrm{diag}\{ \sigma^2_{\theta}(\x, \y) \} \, 
\end{align*}
which omits $\x$ as $\x$ is fixed in our studied case. 
It follows that, taking into account the diagonal property, the determinant is simplified to
\begin{align}
  \mathrm{det}(\Sigma(\y)) &= \prod_{j=1}^{m} \Sigma(\y)_{jj} = \prod_{j=1}^{m} \sigma^2_{\theta}(\x, \y)_{j} \label{eq:det} \,.
\end{align}
In addition, the trace of a matrix is the sum of its diagonal elements, such that 
\begin{align}
  \mathrm{tr}(\Sigma(\y)) &= \sum_{j=1}^{m} \Sigma(\y)_{jj} = \sum_{j=1}^{m} \sigma^2_{\theta}(\x, \y)_{j} \label{eq:trace} \,.
\end{align}

Employing the results in~\cref{corol:student_kl}, we obtain 
\begin{align}
&\kl[ q(\zhat \mid \x, \y_1) \mid\mid q(\zhat \mid \x, \y_0)] \notag \\
&\le \frac 1 2 \log \frac{\mathrm{det}(\Sigma(\y_0))}{\mathrm{det}(\Sigma(\y_1))} - \frac{\nu + m}{2}\left\{\Psi\left( \frac{\nu + m}{2} \right) - \Psi\left(\frac{\nu}{2} \right ) \right\} \notag \\
&\quad + \frac {\nu + m} 2 \log \left\{ 1 + \frac{1}{\nu} \mathrm{tr}(\Sigma(\y_0)^{-1} \tilde{\Sigma}(\y_1))  \right. \notag \\
&\qquad \left. + \frac{1}{\nu} \mathrm{tr}\left\{\Sigma(\y_0)^{-1} (\mu(\y_1) - \mu(\y_0))(\mu(\y_1) - \mu(\y_0))^T \right\} \right\} \label{eq:our_kl} \,.
\end{align}
Hence, combining \cref{prop:det,prop:trace},~\cref{eq:our_kl} becomes
\begin{align}
&\kl[ q(\zhat \mid \x, \y_1) \mid\mid q(\zhat \mid \x, \y_0)] \notag \\
&\le \frac{\nu+m}{2} \left\{ \frac{M \alpha}{2(\nu-2)}  - \Psi\left( \frac{\nu + m}{2} \right) + \Psi\left(\frac{\nu}{2} \right) \right\} \notag \\
&\quad + \left( \frac{m M}{2e} + \frac{(\nu+m) \sqrt{m}}{2\alpha}  \right) \Delta(\y_0, \y_1) \,.
\end{align}
It suffices to complete the proof with 
\begin{align*}
C_1 &= \frac{\nu+m}{2} \left\{ \frac{M \sqrt{m} \alpha}{2(\nu-2)}  - \Psi\left( \frac{\nu + m}{2} \right) + \Psi\left(\frac{\nu}{2} \right) \right\} \\
C_2 &= \frac{m M}{2e} + \frac{(\nu+m) \sqrt{m}}{2\alpha} \,.
\end{align*} 

\end{proof}


\begin{proposition}
\label{prop:det}
Given Assumptions 1 to 3 hold true, the term $\frac 1 2 \log\frac{\mathrm{det}(\Sigma(\y_0))}{\mathrm{det}(\Sigma(\y_1))}$ can be upper bounded as follows.
\begin{align}
\frac 1 2 \log\frac{\mathrm{det}(\Sigma(\y_0))}{\mathrm{det}(\Sigma(\y_1))} \le \frac{m M }{2e} \Delta(\y_0, \y_1) \,.
\end{align}
\end{proposition}

\begin{proof} 
Based on~\cref{eq:det}, one may be able to show 
\begin{align}
\frac 1 2 \log\frac{\mathrm{det}(\Sigma(\y_0))}{\mathrm{det}(\Sigma(\y_1))}
&= \frac 1 2 \log \prod_j \frac{\Sigma(\y_0)_{jj}}{\Sigma(\y_1)_{jj}} \notag \\
&\le \frac 1 2 \log\{ M \Delta(\y_0, \y_1) \}^{m} \notag \\  
&\le \frac{m M }{2e} \Delta(\y_0, \y_1) \,, 
\end{align} 
given $\log(x) \le e^{-1} x$ for any $x>0$. 
\end{proof}
 


\begin{proposition}
\label{prop:trace_terms}
Suppose Assumptions 1 to 3 satisfy.
The inequalities
\begin{align}
\frac{1}{\nu} \mathrm{tr}(\Sigma(\y_0)^{-1} \tilde{\Sigma}(\y_1)) \le \frac{M m}{\nu - 2} \Delta (\y_0, \y_1) 
\end{align}
and 
\begin{multline}
\mathrm{tr}(\Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T ) \\
\le \frac{m L^2}{\lambda} \Delta(\y_0, \y_1)^2  
\end{multline}
hold.
\end{proposition}

\begin{proof}  
We then analyze the first term and derive
\begin{align}
\frac{1}{\nu} \mathrm{tr}(\Sigma(\y_0)^{-1} \tilde{\Sigma}(\y_1))  
&=\frac 1 \nu \mathrm{tr} \left(\Sigma({\y_0})^{-1} \frac{\nu}{\nu-2} \Sigma(\y_1) \right) \notag \\
&=\frac 1 \nu \frac{\nu}{\nu-2} \mathrm{tr} \left(\Sigma({\y_0})^{-1} \Sigma(\y_1) \right) \notag \\
&=\frac{1}{\nu - 2} \mathrm{tr} \left\{ [\sigma^2(\x, \y_0)^{-1} \m{I}] [\sigma^2(\x, \y_1) \m{I}] \right\} \notag \\ 
&= \frac{1}{\nu - 2} \max_{j=1, \dots, m} \sum_{j=1}^m \frac{\sigma^2(\x, \y_0)_{j}}{\sigma^2(\x, \y_1)_{j}} \notag \\ 
&\le \frac{m}{\nu - 2} \max_{j=1, \dots, m} \frac{\sigma^2(\x, \y_0)_{j}}{\sigma^2(\x, \y_1)_{j}} \notag \\
&\le \frac{Mm}{\nu - 2} \Delta (\y_0, \y_1) \label{eq:first_term} \,. 
\end{align} 
Next, we bound $\mathrm{tr}\left(\Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T \right)$ by 
\begin{multline}
\mathrm{tr}\left \{ \Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1)) (\mu(\y) - \mu(\y_1))^T \right \} \\
\le \mathrm{tr}(\Sigma(\y_0)^{-1}) \mathrm{tr}\left \{ (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T \right \} \,.
\end{multline}
since a positive diagonal matrix and $\m{w} \m{w}^T$ for any $\m{w} \in \mathbb{R}^{a \times b}$ are both positive semi-definite; thus, the trace can be decomposed.
We hence derive
\begin{align*}
&\mathrm{tr} \left\{ (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T \right\} \\
&= \sum_{j=1}^{m} (\mu(\y_0) - \mu(\y_1))^2_{jj} \\
&= \left \| \mu(\y_0) - \mu(\y_1) \right \|^2_2 \\  
&\le L^2 \Delta(\y_0, \y_1)^2 \,.
\end{align*} 
On the other hand, for any given $\y$, 
\begin{align}
\mathrm{tr}(\Sigma(\y_0)^{-1}) 
&= \sum_{j=1}^{m} \frac{1}{\Sigma(\y_0)_{jj}} \le \frac{m}{\lambda} \,.
\end{align} 
Combining the above two inequalities, we get 
\begin{multline}
\mathrm{tr} \left\{ \Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T  \right\} \\
\le \frac{m L^2}{\lambda} \Delta(\y_0, \y_1)^2 \label{eq:second_term}\,.
\end{multline}
\cref{eq:first_term,eq:second_term} conclude the proof.
\end{proof}

\begin{corollary}
\label{prop:trace}
Suppose Assumptions 1 to 3  satisfy. 
The following inequality holds.
\begin{align}
&\frac {\nu + m} 2 \log \left\{ 1 + \frac{1}{\nu} \mathrm{tr}(\Sigma(\y_0)^{-1} \tilde{\Sigma}(\y_1))  \right. \notag \\
&\qquad \left. + \frac{1}{\nu} \mathrm{tr}(\Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1))(\mu(\y_0) - \mu(\y_1))^T \right\} \notag \\
&\le \frac{(\nu + m) \sqrt{m}}{2} \left( \frac{M \alpha}{2(\nu-2)} + \frac{1}{\alpha} \Delta(\y_0, \y_1) \right)\,,
\end{align}
where $\alpha = \frac{\sqrt{\nu \lambda}}{L}$.
\end{corollary}
\begin{proof}
First, we derive the inequality of $\log$ that we would apply throughout our proof. 
With $x > -1$, 
\begin{multline}
\log (1+x) 
\le \frac{x}{\sqrt{x+1}}  
= \frac{\sqrt{x} \sqrt{x}}{\sqrt{x+1}} \\
\le \frac{\sqrt{x} \sqrt{x+1}}{\sqrt{x+1}} 
= \sqrt{x} \,.
\end{multline}

Extending from~\cref{prop:trace_terms} with the above inequality, we therefore achieve  
\begin{align*}
&\frac{\nu+m}{2} \log \left\{ 1 + \frac{M m \Delta(\y_0, \y_1)}{\nu-2} + \frac{m L^2 \Delta(\y_0, \y_1)^2}{\nu \lambda} \right\} \\
&\le \frac{\nu+m}{2} \sqrt{ \frac{M m \Delta(\y_0, \y_1)}{\nu-2} + \frac{m L^2 \Delta(\y_0, \y_1)^2}{\nu \lambda} } \\
&= \frac{(\nu+m) \sqrt{m}}{2} \\
&\quad \times \sqrt{ \left(\frac{M \sqrt{\nu \lambda}}{ 2 (\nu - 2) L} + \frac{L}{\sqrt{\nu \lambda}} \Delta(\y_0, \y_1) \right)^2 - \mathrm{CONST.} } \\
&\le \frac{(\nu+m) \sqrt{m}}{2} \left( \frac{M \sqrt{\nu \lambda} }{2(\nu-2) L} + \frac{L}{\sqrt{\nu \lambda}}\Delta(\y_0, \y_1) \right) \\&= \frac{(\nu+m) \sqrt{m}}{2} \left( \frac{M \alpha}{2(\nu-2)} + \frac{1}{\alpha} \Delta(\y_0, \y_1) \right) \,, 
\end{align*} 
concerning $\alpha = \frac{\sqrt{\nu \lambda}}{L}$.
Moreover, any number is smaller than or equal to the result when it is decreased by a non-negative constant (i.e., CONST.).
The proof completes here. 
\end{proof}

\section{Multivariate Normal Replacing Multivariate Student as the Proposal Distribution}
\label{ap:normal_proposal}
Let us consider the case that the proposal distributions $q(\zhat \mid \x, \yhat=\y_0)$ and $q(\zhat \mid \x, \yhat=\y_1)$ are assumed to be multivariate Normal distributions, such that 
\begin{align}
q(\zhat \mid \x, \yhat=\y_0) &\coloneqq N(\mu(\y_0), \Sigma(\y_0)) \label{eq:normal_proposal0} \\ 
q(\zhat \mid \x, \yhat=\y_1) &\coloneqq N(\mu(\y_1), \Sigma(\y_1)) \label{eq:normal_proposal1}\,.
\end{align}
With this setting replacing the proposal being Student distributions, we have the following corollary.
\begin{corollary}
Assume the proposal distributions follow the multivariate Normal, such that~\cref{eq:normal_proposal0,eq:normal_proposal1} hold.
We get  
\begin{multline}
  D_{KL} [ q(\zhat \mid \x, \yhat=\y_1) \mid \mid q(\zhat \mid \x, \yhat=\y_0)] \\ 
  = O \left( m \Delta(\y_0, \y_1)^2 \right) \,.
\end{multline}
\end{corollary}

\begin{proof}
As known, the KL-divergence for two multivariate Normal distributions has a closed form.
That is, the KL-divergence is 
\begin{align}
&D_{KL} [ q(\zhat \mid \x, \yhat=\y_1) \mid \mid q(\zhat \mid \x, \yhat=\y_0)] \\
&=D_{KL} [ N(\mu(\y_1), \Sigma(\y_1)) \mid \mid N(\mu(\y_0), \Sigma(\y_0))] \notag \\
&= \frac 1 2 \log\frac{\mathrm{det}(\Sigma(\y_0))}{\mathrm{det}(\Sigma(\y_1))} - \frac{m}{2} + \frac 1 2 \mathrm{tr} \left( \Sigma(\y_0)^{-1} \Sigma(\y_1) \right) \notag \\
&\quad + \frac 1 2 \mathrm{tr} \left\{ \Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T  \right\} \,.
\end{align}
Applying~\cref{prop:det}, the first term follows
\begin{align}
\frac 1 2 \log\frac{\mathrm{det}(\Sigma(\y_0))}{\mathrm{det}(\Sigma(\y_1))} \le \frac{Mm}{2e} \Delta(\y_0, \y_1) \,.
\end{align}
Following the approach used in~\cref{eq:first_term}, we can derive
\begin{align}
\mathrm{tr} \left\{ \Sigma(\y_0)^{-1}  \Sigma(\y_1) \right\} 
\le M m \Delta(\y_0, \y_1)  \,.
\end{align}
Also, the following result can be found in~\cref{prop:trace_terms}.
\begin{align}
\mathrm{tr} \left\{ \Sigma(\y_0)^{-1} (\mu(\y_0) - \mu(\y_1)) (\mu(\y_0) - \mu(\y_1))^T  \right\} \notag \\
\le \frac{m L^2}{\lambda} \Delta(\y_0, \y_1)^2 \,.
\end{align}
Now, we combine all the terms and derive: 
\begin{align}
&D_{KL} [ q(\zhat \mid \x, \yhat=\y_1) \mid \mid q(\zhat \mid \x, \yhat=\y_0)] \notag \\
&\le \frac{M m}{2e} \Delta(\y_0, \y_1) - \frac{m}{2} + {M m} \Delta(\y_0, \y_1) \\
&\quad + \frac{m L^2}{\lambda} \Delta(\y_0, \y_1)^2 \notag \\
&= Mm \frac{1+2e}{2e} \Delta(\y_0, \y_1) - \frac{m}{2} + \frac{m L^2}{\lambda} \Delta(\y_0, \y_1)^2 \notag \\
&\le \frac{3Mm}{2} \Delta(\y_0, \y_1) - \frac{m}{2} + \frac{m L^2}{\lambda} \Delta(\y_0, \y_1)^2 \notag \\ 
&= m \left(\frac{L}{\sqrt{\lambda}} \Delta(\y_0, \y_1) + \frac{\sqrt{M \lambda}}{4 L} \right)^2 - m \left( \frac{9 M^2 \lambda + 8 L^2}{16 L^2} \right) \notag \\
&=O \left( m \Delta(\y_0, \y_1)^2 \right) \,,
\end{align}
since $m$ the dimension of $\z$ is also a variable in our task.
We finish the proof here.
\end{proof}

\section{Code and Implementation Details} \label{sec:impl}
Our code is publicly available at \emph{github}\footnote{
\url{https://github.com/huangweipeng7/lsnpc}
}. 
We implemented the $\beta$-VAE framework which could better learn the disentangled representations of data.
Additionally, Roskams-Hieter et al.~\cite{roskams2023leveraging} demonstrate its strong connection to \emph{power posteriors} that are more robust given mis-specified priors~\cite{li2023improved,friel2014improving}.
That is, with this implementation, the properties of our theoretical framework remain while the learned models could be more robust.
Note that $\beta$ will be only associated with the distributions centering on the latent variables $\z$ and $\zhat$.
The value of $\beta$ is fixed to be 0.01 in our implementation for all experiments.
Except for the sensitivity analysis, we set $\nu = \nu_0 = 2.01$ for all the experiments, which conforms to the requirements of $\nu = \nu_0 > 2$ as discussed in Theorem 2.
The hyperparameter $\eta$, the weights for the mixture proposal distribution in the supervised setting, was set to $0.5$ for all experiments.
All the experiments were conducted on a machine equipped with an Nvidia 4090D 24GB GPU, an Intel i7-13700KF CPU, and 32GB of RAM.

The label encoder is a 4-layer multilayer perceptron (MLP) that uses GELU~\cite{hendrycks2016gaussian} as its activation function. 
To improve stability and performance, Batch Normalization with a momentum of 0.01 is applied between the connecting layers.
The size of the fully connected layers in this MLP was fixed as 64.
In addition, the output label embedding size was 128.
The solution of merging the label embeddings and data feature embeddings is concatenation.
For all experimental configurations, we employed the AdamW optimizer along with a cosine annealing learning rate scheduler, which had a fixed cycle of 10 epochs (even when the training epochs were fewer than 10 in certain settings).
Additionally, we applied a batch size of 32.
All of the above hyperparameter settings were consistent for both the unsupervised and semi-supervised learning.
However, the numbers of training epochs and learning rates differ under the unsupervised and semi-supervised learning.
For all semi-supervised settings, the training epochs were fixed to 5.
While for the unsupervised settings, the numbers of training epochs for \vocseven{} and \voctwelve{} were respectively 20 and 15.
Furthermore, the epoch numbers were 20 and 15 for \tomato{} and \coco{} respectively.
More hyperparameter settings are separately listed in the public github repository for each configuration. 
The hyperparameters were manually optimized with regard to the performance of the validation sets.
Tuning the learning rate and number of training epochs were usually the most effective factors to improve the performance of \method{} in our experiments.

We employed the Adam optimizer throughout the training for base models.
The learning rate was fixed to 1e-5 for \tomato{} and 5e-5 for the other datasets.
Similarly, the batch size was set to 32 for \tomato{} and 128 for the other datasets.
Next, the number of training epochs was 20 for \vocseven{} and \voctwelve{}, 40 for \tomato{}, and 30 for \coco. 
For HLC, the number of epoch to start correcting was set to 5. 
These hyperparameter settings were mainly derived from~\cite{xia2023holistic,gehlot2023tomato} with minor modifications based on a rough grid search.
Since these approaches serve as the base models for assessing whether our method can further yield solid improvements, we did not put extensive effort into optimizing the hyperparameters.
Finally, for all configurations including the base models and \method{}, the checkpoint that achieves the best validation \microf{} was selected, since a high \macrof{} could only be obtained given that a high \microf is achieved. 
}
