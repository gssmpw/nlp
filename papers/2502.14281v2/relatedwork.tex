\section{Related Works}
\label{sec:related}
Noisy labels are inevitable in realworld datasets, posing a significant challenge to the generalization ability of deep neural networks trained on them. These deep neural networks can even easily fit randomly assigned labels in the training data, as demonstrated in~\cite{zhang2021understanding}. To tackle this challenge, many studies have been proposed for learning with noisy labels. For example, noise-cleansing methods primarily aim to separate clean data pairs from the corrupted dataset using the output of the noisy classifier~\cite{malach2017decoupling,wang2021proselflc,kim2021fine,han2018masking,tanaka2018joint,han2018co,yu2019does,wei2020combating,zheng2021meta,zheng2020error}. 
Another line of research focuses on designing either explicit regularization or robust loss functions for training to mitigate the problem of noisy labels~\cite{liu2020early,xia2020robust,zhang2018generalized,wang2019symmetric,ma2020normalized}. 
To explicitly model  noise patterns, another branch of research suggests using a transition matrix $T$ to formulate the noisy transition from true labels to noisy labels~\cite{patrini2017making,yao2020dual,zhang2021learning,xia2020part,berthon2021confidence}. 
Different from these lines of research, Bae et. al~\cite{Bae2022from} introduced NPC (Noisy Prediction Calibration), which is a new branch of method working as a post-processing scheme that corrects the noisy prediction from a pre-trained classifier to the true label.

Despite these endeavors of tackling learning with noisy labels, a recent survey~\cite{song2022learning} points out that the majority of the existing methods are applicable only for a
single-label multiclass classification problem, and more research is required for the multilabel classification problem where each example can be associated with multiple true class labels. 
Our approach focuses on multilabel classification along with recent studies.

More recently, several works have been proposed for multilabel classification with noisy labels.
For example, HLC~\cite{xia2023holistic} uses instance-label and label dependencies in an example for follow-up label correction during training. 
UNM~\cite{chen2024unm} uses a label-wise embedding network that semantically aligns label-wise features and label embeddings in a joint space and learns the co-occurrence of multilabels. The label-wise embedding network cyclically adjusts the fitting status to distinguish the clean labels and noisy labels, and generate pseudo labels for the noisy ones for training. 
Our approach is orthogonal to these studies, and can be seen as a post-processor similar to NPC~\cite{Bae2022from} for multiclass classification. 
In other words, our method can be used to correct the predictions of a pre-trained classifier such as HLC, and further improve the performance as a post-processor as we show in~\cref{sec:experiments}.