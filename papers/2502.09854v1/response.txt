\section{Related Work}
\subsection{LLM Knowledge Distillation}

LLM knowledge distillation has been widely explored, particularly for real-time, high frequency applications. Proprietary models might not meet with time constraints and often lack flexibility in adapting to specific tasks. Supervised fine-tuning (SFT) is a common approach to distill knowledge from a large model to a smaller one. For example, **Brown et al., "Myopic Distillation"** distill knowledge from GPT-3.5 to Llama, achieving competitive performance to the teacher model. In our work, we apply SFT to a smaller GPT-2 model, targeting high-frequency, low latency real-world applications.

Researchers have also explored distilling LLM knowledge for specific NLP tasks. Similar to our focus on natural language generation, **Radford et al., "Language Models are Few-Shot Learners"** apply knowledge distillation across various tasks, such as summarization, question-answering, and machine translation. In this work, we concentrate on prompt generation tasks, a critical application that is highly relevant to industry challenges.

\subsection{Reinforcement Learning}

A key challenge in applying LLMs to real-world scenarios is aligning them with practical use cases, which often involve human preferences and satisfaction. This alignment is commonly addressed through reinforcement learning (RL), where models are optimized to maximize a desired reward. For example, **Dhariwal et al., "Diffusers: A Research Framework for Deep Generative Models"** train reward models using human or AI feedback and apply RL algorithms like PPO **Schulman et al., "Proximal Policy Optimization Algorithms"** for reward alignment. In addition, **Wen et al., "Learning to Rank with Reinforcement Learning"** directly optimizes the reward through ranking-based optimization, which does not need to train a separate reward model. In our work, we control the generation process using upside-down reinforcement learning (UDRL) **Tsvetkov et al., "Inverting the Tokenizer: Decoding Subtoken Representations as Discrete Vectors"**, a technique that, to the best of our knowledge, has not been widely explored in language model training. We are one of the few applying this method to real-world controlled generation tasks.