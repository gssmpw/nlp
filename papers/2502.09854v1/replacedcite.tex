\section{Related Work}
\subsection{LLM Knowledge Distillation}

LLM knowledge distillation has been widely explored, particularly for real-time, high frequency applications. Proprietary models might not meet with time constraints and often lack flexibility in adapting to specific tasks. Supervised fine-tuning (SFT) is a common approach to distill knowledge from a large model to a smaller one. For example,____ distill knowledge from GPT-3.5 to Llama, achieving competitive performance to the teacher model. In our work, we apply SFT to a smaller GPT-2 model, targeting high-frequency, low latency real-world applications.

Researchers have also explored distilling LLM knowledge for specific NLP tasks. Similar to our focus on natural language generation,____ apply knowledge distillation across various tasks, such as summarization, question-answering, and machine translation. In this work, we concentrate on prompt generation tasks, a critical application that is highly relevant to industry challenges.

\subsection{Reinforcement Learning}

A key challenge in applying LLMs to real-world scenarios is aligning them with practical use cases, which often involve human preferences and satisfaction. This alignment is commonly addressed through reinforcement learning (RL), where models are optimized to maximize a desired reward. For example,____ train reward models using human or AI feedback and apply RL algorithms like PPO____ for reward alignment. In addition,____ directly optimizes the reward through ranking-based optimization, which does not need to train a separate reward model. In our work, we control the generation process using upside-down reinforcement learning (UDRL)____, a technique that, to the best of our knowledge, has not been widely explored in language model training. We are one of the few applying this method to real-world controlled generation tasks.