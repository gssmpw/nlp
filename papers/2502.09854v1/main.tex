\documentclass[11pt]{article}


\usepackage{amsmath}
\usepackage{amssymb}

% For multiple authors with affiliations
\usepackage{authblk}

% acl style
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% PRIMEArxiv style (When prompting errors/warnings, have to clear cache files!)
% \usepackage{PRIMEArxiv}
% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage[toc,page]{appendix}
% \PassOptionsToPackage{breakable,many}{tcolorbox}
% \usepackage{tcolorbox}
% \usepackage{xcolor}
% \usepackage{lipsum}
% \usepackage{fancyhdr}       % header
% \usepackage{graphicx}       % graphics
% \usepackage{array}
% \usepackage{adjustbox}
% \graphicspath{{media/}}     % organize your images and other figures under media/ folder
% \usepackage{enumitem}
% %Header
% \pagestyle{fancy}
% \thispagestyle{empty}
% \rhead{ \textit{ }} 

\newcommand{\hm}[1]{{\color{blue} [HM: #1]}}

\title{Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning}

\author[1,2]{Yu-Chen Lin\thanks{Work done during an internship at Adobe.}}
\author[1,3]{Sanat Sharma\thanks{Work done as a full-time employee at Adobe.}}
\author[1]{Hari Manikandan}
\author[1]{Jayant Kumar}
\author[1]{Tracy Holloway King}
\author[1]{Jing Zheng}
\affil[1]{Adobe}
\affil[2]{Carnegie Mellon University}
\affil[3]{Meta}

\date{October 2024}

\begin{document}

\maketitle
\begin{abstract}
In this work, we demonstrate that small language models (SLMs), specifically a 100M parameter GPT-2 model~\citep{gpt2}, can achieve competitive performance in multitask prompt generation tasks while requiring only a fraction of the computational resources needed by large language models (LLMs). Through a novel combination of upside-down reinforcement learning and synthetic data distillation from a powerful LLM, Llama-3~\citep{llama3}, we train an SLM that achieves relevance scores within 5\% of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite being up to 80 times smaller, making it highly suitable for resource-constrained and real-time applications. This study highlights the potential of SLMs as efficient multitask learners in multimodal settings, providing a promising alternative to LLMs for scalable, low-latency deployments.
\end{abstract}

\begin{figure*}[!tb]
\centering
\includegraphics[width=\textwidth]{figs/framework.png}
\caption{Our framework for developing a small language model (SLM) as an efficient multitask learner.}
\label{fig:framework}
\end{figure*}

\section{Introduction}

Large language models have revolutionized various applications of natural language processing, yet they come with substantial computational and memory costs, especially at inference time. These limitations hinder their deployment in high-frequency, real-time applications where efficiency is critical. We propose a small language model (SLM) framework that can serve as an efficient and effective multitask learner for multimodal prompt generation tasks, utilizing upside-down reinforcement learning to control the generation.

In Figure~\ref{fig:framework}, we demonstrate our overall pipeline. We start by generating synthetic data from a large language model (LLM) using prompt engineering and few-shot examples. This synthetic data undergoes upside-down reinforcement learning (UDRL)~\citep{udrl0, udrl}, enabling the training of a specialized SLM that is optimized for low latency, memory efficiency, targeted multitasking, and real-time multimodal generation tasks.

Our approach is built on three core contributions:

\begin{itemize}
    \item SLMs as Multitask Learners: We demonstrate that SLMs, trained with targeted synthetic data, can achieve multi-task capabilities often associated with larger models.
    \item Upside-down reinforcement learning for SLM Training: By using upside-down reinforcement learning, we optimize our SLM for various prompt generation tasks, achieving competitive relevance and diversity.
    \item Synthetic Dataset Distillation: Leveraging a large language model (Llama-3), we curate a high-quality training dataset for the SLM, enabling it to learn effectively from minimal resources.
\end{itemize}

Our experimental results show that the SLM can achieve comparable performance to an LLM with an 80-fold reduction in model size. This makes the SLM framework ideal for use in real-world, resource-constrained settings where both performance and efficiency are critical.

We highlight that this framework can be integrated with any commercial text-to-image generation system (e.g., Adobe Firefly) to provide multimodal prompt generation, making the SLM framework highly adaptable and efficient for real-world applications.

\section{Related Work}

\subsection{LLM Knowledge Distillation}

LLM knowledge distillation has been widely explored, particularly for real-time, high frequency applications. Proprietary models might not meet with time constraints and often lack flexibility in adapting to specific tasks. Supervised fine-tuning (SFT) is a common approach to distill knowledge from a large model to a smaller one. For example,~\citet{alpaca, vicuna} distill knowledge from GPT-3.5 to Llama, achieving competitive performance to the teacher model. In our work, we apply SFT to a smaller GPT-2 model, targeting high-frequency, low latency real-world applications.

Researchers have also explored distilling LLM knowledge for specific NLP tasks. Similar to our focus on natural language generation,~\citet{inheritsumm, recomp, mario, gkd} apply knowledge distillation across various tasks, such as summarization, question-answering, and machine translation. In this work, we concentrate on prompt generation tasks, a critical application that is highly relevant to industry challenges.

\subsection{Reinforcement Learning}

A key challenge in applying LLMs to real-world scenarios is aligning them with practical use cases, which often involve human preferences and satisfaction. This alignment is commonly addressed through reinforcement learning (RL), where models are optimized to maximize a desired reward. For example,~\citet{constitutionalai, ultrafeedback, rlaif, syntheticfeedback} train reward models using human or AI feedback and apply RL algorithms like PPO~\citep{ppo} for reward alignment. In addition,~\citet{dpo} directly optimizes the reward through ranking-based optimization, which does not need to train a separate reward model. In our work, we control the generation process using upside-down reinforcement learning (UDRL)~\citep{udrl0, udrl}, a technique that, to the best of our knowledge, has not been widely explored in language model training. We are one of the few applying this method to real-world controlled generation tasks.

\section{Methodology}

Our task involves generating prompts for generative models based on multimodal inputs. The process begins with detecting user intents using an in-house model. Once the intents are identified, we aim to generate prompts accordingly. Rather than relying solely on a large language model (LLM) or a multimodal LLM, we adopt a more efficient approach.

Specifically, our methodology focuses on training a small language model (SLM) for prompt generation with intents that emphasizes high efficiency and multitask capabilities. We achieve this by leveraging synthetic data distillation from a large language model (LLM) combined with upside-down reinforcement learning.

\begin{table*}[!tb]
\centering
\begin{tabular}{|p{3.5cm}|p{4cm}|p{7.5cm}|}
\hline
\textbf{Intents / Task} & \textbf{Generated Prompt} & \textbf{Training Data Format} \\ \hline
Topic: birthday \newline Scene object: balloon \newline Task: prompt for text-to-image generation & Whimsical birthday celebration featuring giant balloons in fun shapes and sizes, tied to a birthday child's arm or wrist. & \texttt{<|19|> <|intent|> Topic: birthday, Scene object: balloon <|IP|> whimsical birthday celebration featuring giant balloons in fun shapes and sizes, tied to a birthday child's arm or wrist.} \\ \hline
Topic: birthday party \newline Design type: invitation \newline Task: prompt for text-to-template generation & Create a whimsical birthday party invitation template with balloons, confetti, and a playful theme. & \texttt{<|14|> <|intent|> Topic: birthday party, Design Type: invitation <|TP|> create a whimsical birthday party invitation template with balloons, confetti, and a playful theme.} \\ \hline
\end{tabular}
\caption{Examples of intents and generated prompts for image and template generation, including format specifications. Having task, length, and intent tags helps guide the generations.}
\label{table:intents_prompts}
\end{table*}

\subsection{Synthetic Dataset Distillation}

To enable the SLM to capture complex task knowledge from a larger model, we first create a synthetic dataset using Llama-3. This involves generating high-quality, task-specific data that allows the SLM to learn from the representations of LLM effectively. We utilize vLLM~\citep{vllm} to parallelize our dataset generation process.

\subsubsection{Dataset Curation Process}

\begin{enumerate}
    \item \textbf{Intent and Prompt Pair Generation:} We curated a dataset of 52 million intent-prompt pairs by prompting the LLM with various intent descriptions and collecting its responses. Each sample consists of an intent description paired with a generated prompt, tailored to a range of real-world tasks. For example, intents such as “birthday celebration,” “holiday sale,” or “product launch” were used to generate prompts that align with these contexts.

    \item \textbf{Structured Prompting for Targeted Tasks:} To ensure diversity and relevance in generated prompts, we used structured prompting with few-shot examples and task specifications. Each prompt to Llama-3 included high-level task descriptions and few-shot examples to encourage contextually appropriate and diverse outputs. Different prompting strategies are used to address the specific requirements of different tasks. By specifying factors like tone, style, and context, we created a dataset that is comprehensive and task-aligned.

    \item \textbf{Labeling and Intent Selection:} The intent descriptions were sourced from in-house creative knowledge graph and augmented with diverse asset metadata from internal images and templates. This process provided a broad set of intents that accurately represent real-world use cases. Each intent was paired with multiple generated prompts, thus ensuring a rich dataset of prompt variations for each concept.
\end{enumerate}

This curated dataset of intent-prompt pairs serves as a distilled form of knowledge from Llama-3, allowing the SLM to learn diverse language patterns and contextual nuances without directly training on a massive language model. Examples of intents to generated prompts are shown in Table~\ref{table:intents_prompts}.

\subsection{Upside-Down Reinforcement Learning for SLM Optimization}\label{ssec:udrl}

To optimize the SLM's generation quality and control specific attributes like length and relevance, we utilize upside-down reinforcement learning (UDRL)~\citep{udrl0, udrl}. This approach allows the model to learn specific objectives based on desired outcomes rather than traditional reward structures.

\subsubsection{Reward-Based Prompt Generation}

Upside-down reinforcement learning frames the prompt generation task as an optimization problem where the SLM aims to achieve target specifications for each generated output. This process is as follows:

\begin{itemize}
    \item \textbf{Controlled-Length Generation:} The SLM is trained to produce prompts within a desired length range (e.g., 10 to 35 words). Tokens indicating target lengths are incorporated into the input, guiding the model towards generating responses that match the specified length with a mean squared error consistently under two words. More evaluations are in Section~\ref{sec:eval}.

    \item \textbf{Modality-Agnostic Prompting:} We trained the SLM to handle both text-to-image (T2I) and text-to-template (T2T) prompts within the same framework. This was achieved by adding modality tokens to each training instance, allowing the model to distinguish between generation tasks and tailor its output accordingly.

    \item \textbf{Contextual Relevance and Specificity:} By assigning relevance scores to generated prompts based on a predefined metric (e.g., alignment with target intent or clarity), the model learns to prioritize contextually relevant and specific responses. During training, prompts that meet these objectives are positively reinforced, improving the SLM's ability to generate accurate and contextually appropriate prompts. While we do not implement this aspect in our current pipeline, it could be valuable for other applications.
\end{itemize}

\subsection{Model Architecture and Key Capabilities}

Our SLM is based on nanoGPT\footnote{\url{https://github.com/karpathy/nanoGPT}}, a compact variant of GPT-2, with 104 million parameters, configured with 12 layers, 12 attention heads, and an embedding dimension of 768. The model architecture and training setup are designed to maximize efficiency and multitask performance.

\subsubsection{Model Specifications}\label{sssec:spec}

\begin{itemize}
    \item \textbf{Parameter Efficiency:} The SLM contains approximately 1/80th the parameters of Llama-3 8b and similar state-of-the-art LLMs, making it computationally efficient and suitable for deployment on standard hardware. 
    
    \item \textbf{Inference Speed:} Our SLM achieves a processing speed of up to 338 tokens per second on a single A10G GPU (non-batched, non-quantized, or accelerated), making it suitable for real-time applications. This performance is especially advantageous in resource-constrained environments where inference latency is a critical factor.

    \item \textbf{Multitask Learning Capabilities:} The SLM is trained to handle both T2I and T2T prompts, making it a versatile tool for multimodal applications. Through the integration of task-specific tokens, the model can generate contextually accurate prompts tailored to the input task, whether it’s for text-to-image generation or template-based design.
\end{itemize}

\subsubsection{Training with UDRL}

The training data are formatted as \texttt{<|\# words of the prompt|> <|intent|> INTENT <|prompt for T2I (IP) or T2T (TP)|> PROMPT}. As outlined in Section~\ref{ssec:udrl}, we combine the word count and modality tokens to create a single training instance; refer to Table~\ref{table:intents_prompts} for examples.

We utilize a vocabulary set with legal approval, ensuring it excludes any offensive, discriminatory, or biased language. We then train a BPE tokenizer with 25,600 tokens from scratch using our curated dataset. Subsequently, we train the nanoGPT model using next-token prediction, employing four A10G GPUs over 10 days, completing approximately 300,000 iterations with a batch size of 128 and a learning rate of 6e-4.

By combining this training approach with upside-down reinforcement learning, we ensure that our SLM can effectively manage length control and multitasking. During inference, we can easily control output length by specifying a length token and define the desired task using the modality token. Please refer to Section~\ref{sec:eval} for more evaluations of these abilities.

\begin{table*}[!tb]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline

\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Few-Shot}} & \multicolumn{2}{c|}{\textbf{Zero-Shot}} & \multirow{2}{*}{\textbf{\# Params}}\\ \cline{2-5}
& \textbf{T2I} & \textbf{T2T} & \textbf{T2I} & \textbf{T2T} & \\ \hline
\textbf{SLM (Ours)} & N/A & N/A & \textbf{7.94} & 7.79 & 104M \\ \hline
\textbf{Llama-3 8B~\citep{llama3}} & 8.24 & 8.01 & 7.93 & 7.79 & 8.0B \\ \hline
\textbf{Mistral 7B~\citep{mistral}} & \textbf{8.29} & 8.31 & 7.04 & 7.55 & 7.3B \\ \hline
\textbf{Gemma-1.1 7B~\citep{gemma}} & 7.97 & 8.13 & 7.57 & \textbf{7.96} & 8.5B \\ \hline
\textbf{Hermes-3 8B~\citep{hermes3}} & 8.21 & 8.33 & 7.12 & 7.63 & 8.0B \\ \hline
\textbf{Llama-3.2 1B~\citep{llama3}} & 7.09 & 7.49 & 5.99 & 6.48 & 1.2B \\ \hline
\textbf{Phi-3-Mini~\citep{phi3}} & 7.76 & 8.13 & 7.33 & 7.76 & 3.8B \\ \hline
\textbf{Qwen2 7B~\citep{qwen2}} & 8.20 & \textbf{8.35} & 7.14 & 7.75 & 7.6B \\ \hline
\end{tabular}

\caption{Relevance scores for prompt generation tasks, evaluated on text-to-image (T2I) and text-to-template (T2T) prompts under both few-shot and zero-shot prompting. Our SLM model, which operates in a zero-shot setting, demonstrates competitive performance with much larger models. With the UDRL training paradigm, SLM achieves efficiency suitable for enterprise use, outperforming or matching models with significantly higher parameter counts.}
\label{table:relevance_scores}
\end{table*}

\section{Experiments and Evaluations}\label{sec:eval}

We focus on both qualitative and quantitative evaluations to judge the quality of our model.

\subsection{Quantitative Evaluation}

The quantitative evaluation is divided into two primary areas: relevance evaluation, which assesses how well the generated prompts align with the specified intents and modality requirements, and task adherence evaluation, which measures the SLM’s accuracy in meeting specific prompt length requirements.

\subsubsection{Relevance Evaluation}

To evaluate relevance, we conducted experiments to measure how accurately the SLM-generated prompts aligned with the specified intents and task requirements. Relevance was assessed using an automatic method with LLM-as-a-judge, where GPT-4o~\citep{gpt4} served as the evaluator. Each generated prompt was rated on a scale from 1 to 10, where higher scores indicate stronger alignment with the target intent. We provided several examples and criterion to the LLM judge prior to the evaluation. Some of these include 
\begin{itemize}
    \item Correctness - does the prompt contain grammatical or semantic errors.
    \item Clarity - is the prompt clear to understand, is the grammar structure sound.
    \item Completeness - does the prompt utilize all of the context (intent) provided.
    \item Usefulness - is the prompt generated useful for the task provided.
\end{itemize}

For comparison, we evaluated our SLM with state-of-the-art LLMs ranging in size from 1B to 8B. Table~\ref{table:relevance_scores} displays the relevance scores for both text-to-image (T2I) and text-to-template (T2T) prompt generation tasks. We investigated the performance of these LLMs under both few-shot and zero-shot prompting. For our trained SLM, there is no need for demonstration, which places it in the zero-shot setting. We observe the following.
\begin{enumerate}
    \item \textbf{Effective Distillation:} In the few-shot setting, our SLM achieves relevance scores nearly on par with the teacher model, Llama-3 8B, with only a minor performance gap of approximately 3\%. In the zero-shot setting, the performance of our SLM matches Llama-3 8B, and it even outperforms Llama-3 8B in T2I prompt generation. This illustrates the success of our distillation process, effectively transferring high-performance prompt generation capabilities to a more efficient model, without the need for task-specific demonstrations.
    \item \textbf{Competitive Performance:} Despite having fewer parameters, our SLM performs competitively against the best LLMs tested. In the few-shot setting, it scores 7.94 on T2I tasks, closely trailing the top performer (8.29), and achieves 7.79 on T2T tasks, staying within reach of the best result (8.35). In the zero-shot setting, our SLM matches the performance of larger models, ranking first on T2I tasks and second on T2T tasks. This highlights the capability of our SLM to deliver strong performance across various prompt generation tasks, making it a robust solution for real-world applications.
    \item \textbf{Real-World Efficiency and Performance:} Considering the number of parameters, our SLM is the most efficient model in the comparison. Even when compared to the second-smallest model, Llama-3.2 1B, our SLM outperforms it significantly in both few-shot and zero-shot settings. Additionally, as detailed in Section~\ref{sssec:spec}, our SLM can operate with just a single GPU for real-time inference, while other models typically require multiple GPUs and may suffer from higher latency. This makes the SLM well-suited for practical deployment in resource-constrained environments.
\end{enumerate}

In summary, our SLM is highly effective in generating contextually accurate prompts that align closely with specified intents and task requirements. It combines efficiency, scalability, and competitive performance, positioning it as a compelling solution for real-world applications.

\subsubsection{Task Adherence Evaluation (Length)}

\begin{table}[!tb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Target Len.} & \multirow{2}{*}{10} & \multirow{2}{*}{20} & \multirow{2}{*}{30} & \multirow{2}{*}{35} \\
\textbf{(words)} & & & & \\ \hline
\textbf{Mean Len.} & \multirow{2}{*}{10.3} & \multirow{2}{*}{19.2} & \multirow{2}{*}{31.1} & \multirow{2}{*}{33.8} \\
\textbf{(words)} & & & & \\ \hline
\textbf{Mean} & \multirow{2}{*}{0.365} & \multirow{2}{*}{0.881} & \multirow{2}{*}{1.131} & \multirow{2}{*}{1.179} \\
\textbf{Square Err.} & & & & \\ \hline
\textbf{\% Prompts} & \multirow{3}{*}{98\%} & \multirow{3}{*}{96\%} & \multirow{3}{*}{93\%} & \multirow{3}{*}{95\%} \\
\textbf{Within} & & & & \\
\textbf{±2 Words} & & & & \\ \hline
\end{tabular}
\caption{Task adherence results for specified prompt lengths. MSE measures the deviation from the target length, and the final row shows the percentage of prompts within ±2 words of the target length.}
\label{table:length_adherence}
\end{table}

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=\textwidth]{figs/qualitative.png}
    \caption{Qualitative Results: Based on over 200 human evaluations, we found 87\% of all prompts to be very or somewhat relevant, with 96\% being somewhat or very correct.}
    \label{fig:qualitative}
\end{figure*}

In addition to relevance, we evaluated the SLM's ability to meet target length requirements for generated prompts. The model was trained to produce prompts within specified lengths, typically in the range of 10 to 35 words. Length adherence was measured by calculating the mean squared error (MSE) between the generated prompt length and the target length, and by recording the percentage of prompts that fell within an acceptable range (±2 words from the target length). The evaluation results are in Table~\ref{table:length_adherence}.

The SLM achieves precise control over prompt length, with MSE consistently around 1. These results indicate that the model can reliably generate prompts of desired lengths, which is essential for applications requiring specific or concise outputs.

\subsection{Qualitative Evaluation}

To complement the quantitative results, we conducted a qualitative evaluation with human reviewers and chose 18 human reviewers with experience in writing prompts for image and template generative models. The reviewers assessed the SLM-generated prompts on two criteria:
\begin{enumerate}
    \item Relevance - how closely does the prompt align with the intent.
    \item Correctness - is the prompt clear and easy to understand, with correct grammar and sound structure.
\end{enumerate}
Each criterion was rated on a scale from 1 to 3 (corresponding to not, somewhat, very, e.g., not relevant, somewhat relevant and very relevant respectively), and reviewers provided additional qualitative feedback on the prompts.

The human reviewers scored the prompts with an average relevance rating of \textbf{87\%}, highlighting the model's ability to generate prompts that align well with the provided intents and the task specification. For accuracy and correctness, the SLM received a score of \textbf{96\%} somewhat or very correct, indicating that the prompts were contextually accurate, grammatically correct and generally maintained coherence to the topic.

Overall, the qualitative feedback supports our quantitative findings, demonstrating that the SLM not only achieves high relevance and length adherence but also produces contextually rich prompts. This qualitative analysis underscores the practical applicability of SLMs in prompt generation tasks, particularly in scenarios where low-latency and computational efficiency are critical.

\section{Conclusion}

This work highlights the capabilities of small language models (SLMs) as efficient and effective multitask learners. By combining upside-down reinforcement learning with supervised training, we demonstrate that an SLM can achieve competitive performance against much larger models, such as Llama-3, with an 80-fold reduction in size on specific tasks. Our evaluations reveal that SLMs can maintain high contextual relevance, precision, and adherence to tasks, even in resource-constrained settings.

The significant improvements in efficiency and low latency underscore its suitability for deployment in enterprise scenarios where computational resources and speed are critical.

\bibliography{references}

\end{document}
