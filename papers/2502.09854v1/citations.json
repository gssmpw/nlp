[
  {
    "index": 0,
    "papers": [
      {
        "key": "alpaca",
        "author": "Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto ",
        "title": "Stanford Alpaca: An Instruction-following LLaMA model"
      },
      {
        "key": "vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.",
        "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "inheritsumm",
        "author": "Xu, Yichong  and\nXu, Ruochen  and\nIter, Dan  and\nLiu, Yang  and\nWang, Shuohang  and\nZhu, Chenguang  and\nZeng, Michael",
        "title": "{I}nherit{S}umm: A General, Versatile and Compact Summarizer by Distilling from {GPT}"
      },
      {
        "key": "recomp",
        "author": "Fangyuan Xu and Weijia Shi and Eunsol Choi",
        "title": "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"
      },
      {
        "key": "mario",
        "author": "Sahana Ramnath and Brihi Joshi and Skyler Hallinan and Ximing Lu and Liunian Harold Li and Aaron Chan and Jack Hessel and Yejin Choi and Xiang Ren",
        "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation"
      },
      {
        "key": "gkd",
        "author": "Rishabh Agarwal and Nino Vieillard and Yongchao Zhou and Piotr Stanczyk and Sabela Ramos and Matthieu Geist and Olivier Bachem",
        "title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "constitutionalai",
        "author": "Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan",
        "title": "Constitutional AI: Harmlessness from AI Feedback"
      },
      {
        "key": "ultrafeedback",
        "author": "Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Bingxiang He and Wei Zhu and Yuan Ni and Guotong Xie and Ruobing Xie and Yankai Lin and Zhiyuan Liu and Maosong Sun",
        "title": "UltraFeedback: Boosting Language Models with Scaled AI Feedback"
      },
      {
        "key": "rlaif",
        "author": "Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash",
        "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"
      },
      {
        "key": "syntheticfeedback",
        "author": "Kim, Sungdong  and\nBae, Sanghwan  and\nShin, Jamin  and\nKang, Soyoung  and\nKwak, Donghyun  and\nYoo, Kang  and\nSeo, Minjoon",
        "title": "Aligning Large Language Models through Synthetic Feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ppo",
        "author": "John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov",
        "title": "Proximal Policy Optimization Algorithms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dpo",
        "author": "Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "udrl0",
        "author": "Juergen Schmidhuber",
        "title": "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions"
      },
      {
        "key": "udrl",
        "author": "Rupesh Kumar Srivastava and Pranav Shyam and Filipe Mutz and Wojciech Ja\u015bkowski and J\u00fcrgen Schmidhuber",
        "title": "Training Agents using Upside-Down Reinforcement Learning"
      }
    ]
  }
]