\section{Problem formulation}
\label{problem_formulation}

\subsection{Background of LLM inference techniques}
High efficiency and low latency are critical in LLM inference, and they depend not only on performance of hardware, such as GPU, but also on how well the hardware is used. As pushing the limit of hardware computing power is costly and faced with slow progress, improved inference scheduling becomes a promising means of achieving the same outcome. Three factors are involved when designing inference scheduling methods: PD management approaches, cache management, and batching strategy.

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{graph//illustration.pdf}
    \caption{Illustration of LLM inference}
    \label{fig:illustration}
\end{figure}

LLM inference alternately goes through two stages, i.e., prefill and decode stages. In the prefill stage, initial input tokens are processed, and the LLM model's internal states, such as hidden states, are prepared. In the decode stage, the LLM model generates output tokens based on the context established during the prefill stage. The time lengths for prefill stage and decode stage are uncertain. The alternating process continues in parallel, until either the end of the sequence is reached or a predefined stopping criterion is satisfied. The process of LLM inference is illustrated in Fig. \ref{fig:illustration}, where requests, each with a prefill phase and a decode phase, are sent to clients and processed in parallel. The whole process is divided into multiple bins, and each bin consists of a prefill stage and a decode stage. Requests in prefill phase can be served only when the process is in prefill stage, and the same requirement is applied to the decode phase and stage. A client may be idle as either the prefill phase or decode phase is completed but the process still stays in the same stage, causing compromised utilization of computing resources. A single LLM inference in practice typically contains a large number of requests processed by parallel clients. Thus, there is great potential in better utilizing computing resources in LLM inference, but the scheduling problem has a high complexity and, as a real-time decision making in milliseconds, the computing time is limited. The most commonly used approaches to managing prefill are provided below.

% In the context of LLM inference, optimizing task scheduling is critical to maximize efficiency and minimize latency. The primary stages in LLM inference are the prefill and decode phases.

% Prefill Phase: During this stage, initial input tokens are processed, and the model's internal states, such as hidden states, are prepared. This phase establishes the necessary context for subsequent token generation.

% Decode Phase: In this phase, the model generates output tokens based on the context established during the prefill phase. This iterative process continues until either the end of the sequence is reached or a predefined stopping criterion is satisfied. In the Prefill-Decode Competition scenario, the decode phase may be preempted by the prefill phase to enhance hardware utilization.

% Given the distinct characteristics of the prefill and decode phases, previous studies [references] have proposed three strategies for managing these processes:

% 1) Prefill-Decode Competition: In this approach, only one of the prefill or decode operations is processed at any given time on a single hardware node (typically consisting of 8 GPUs). By carefully managing resource contention, this strategy can effectively utilize resources, reduce idle times, and potentially lower overall latency. However, the complexity of scheduling poses a significant challenge, potentially leading to suboptimal performance if not properly managed. Additionally, resource contention might result in bottlenecks during unexpected demand peaks. 

% 2) Prefill-Decode Fusion: This strategy integrates the prefill and decode processes into a single, cohesive operation. Fusion can reduce overhead and enhance throughput by streamlining the inference pipeline. It may also decrease latency due to the alignment of processes. However, this integration may compromise flexibility, restricting the ability to independently optimize each process or tailor responses to varying workload demands.

% 3) Prefill-Decode Separation: This approach separates the prefill and decode phases across multiple GPUs, allowing for independent optimization of each stage. This separation can lead to improvements in efficiency, scalability, and adaptability to different workload types. However, it may introduce additional communication or coordination overhead, which might increase latency if not managed effectively.



% \begin{figure}
%     \centering
%     \includegraphics[width=0.98\linewidth]{graph//PDfusion.pdf}
%     \caption{Illustration of PD fusion}
%     \label{fig:PDfusion}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.98\linewidth]{graph//PDseparate.pdf}
%     \caption{Illustration of PD Separate}
%     \label{fig:PDseparate}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{graph//PDcompetition.pdf}
    \caption{Illustration of PD Competition}
    \label{fig:PDcompetition}
\end{figure}

\begin{itemize}
    \item PD Competition: As illustrated in Fig. \ref{fig:PDcompetition}, in this approach, either prefill or decode stage is processed at any given time for all clients on a single hardware node (typically consisting of 8 GPUs). PD Competition allows decode stage to be preempted by the prefill stage to enhance hardware utilization.
%By carefully managing resource contention, this strategy can effectively utilize resources, reduce idle times, and potentially lower overall latency. However, the complexity of scheduling poses a significant challenge, potentially leading to suboptimal performance if not properly managed. Additionally, resource contention might result in bottlenecks during unexpected demand peaks. 
    \item PD Fusion: This approach integrates the prefill and decode stages into a single cohesive operation, aimed at reducing overhead and enhancing throughput by streamlining the inference pipeline. This approach also attempts to decrease latency through alignment of processes. However, this integration compromises flexibility, restricting the ability to independently optimize each process or tailor responses to varying workload demands.
    \item PD Separation: This approach separates the prefill and decode stages across exclusive sets of GPUs. However, it introduces additional communication or coordination overhead, which increases latency if not properly managed.
%, allowing for independent optimization of each stage. This separation can lead to improvements in efficiency, scalability, and adaptability to different workload types. 

\end{itemize}

As a widely used approach, PD Competition has a high flexibility in effectively utilizing computing resources. Such an approach also allows an inference scheduling method to fit in and further enhance its performance. As is aforementioned, inference scheduling for LLM inference is challenging. This study focuses on the inference scheduling under the PD Competition approach.

The second factor that influences the efficiency of LLM inference is cache management. The KV cache is instrumental during the decoding stage by storing intermediate hidden states from preceding token generation steps. It allows the LLM model to reuse states, significantly accelerating the inference process. Despite its advantages, the KV cache requires to be properly managed. First, the KV Cache size increases with the length of input and output sequences and the number of LLM model layers. This cache size growth results in significant memory consumption, especially in the context of large-sized models and extended sequences. Effective KV cache management avoids memory overflow and sustains high inference speed. Cache management may involve caching only the most relevant hidden states, while discarding or compressing less critical information to optimize resource use. Second, concurrency issues should be addressed. The complexity of managing the KV cache escalates with concurrent inference tasks. Ensuring consistent and conflict-free partitioning and access to the cache for each task is important for performance and accuracy of LLM inference. Besides, although the KV cache alleviates computational load during decoding, it introduces cache access and management overheads. Cache management requires taking into account overall latency. In this study, we proactively compute the KV cache and determine the optimal maximum number of parallel requests, equivalent to the number of clients handled in subsequent stages. Thus, we assume in the inference scheduling problem shown in Fig. \ref{fig:illustration} that the total number of clients is predetermined.

The third factor that influences LLM inference efficiency is batching strategy. Continuous batching, which is introduced by ORCA \cite{yu2022orca}, has emerged as an innovative technique to enhance the inference efficiency by dynamically aggregating incoming requests in real time. Unlike traditional static batching, which waits for a fixed number of requests before processing, continuous batching minimizes latency by reducing idle times between batch executions and adapting to fluctuating workloads. It ensures efficient use of computational resources. By maximizing parallel processing capabilities and aligning with the model architecture's latency and throughput trade-offs, continuous batching significantly enhances the scalability and responsiveness of LLM deployments. Using the continuous batching technique, decoding is allowed to be preempted by the prefill stage. Such a case is shown by the first request of client 2 in Fig. \ref{fig:illustration}, where the decode phase is separated by the prefill stage of bin 2 and is assigned to both the decode stages of bin 1 and 2. In this work, the decision-making process allows decoding to be preempted by the prefill stage, facilitated by the continuous batching technique.

% 注释这部分保留下，batching的文章可以用下
% However, there are specific limitations and challenges associated with batching:
% \begin{itemize}
%     \item Variable Length Sequences: LLMs often process sequences of varying lengths, which complicates the batching process. Batching sequences of different lengths requires padding shorter sequences to match the length of the longest sequence in the batch, which can lead to inefficient memory usage and computational overhead.
%     \item Memory Constraints: Batching large numbers of sequences can strain memory resources, especially when dealing with high-dimensional embeddings and large batch sizes. This can impact the ability to efficiently utilize hardware accelerators and may necessitate careful management of memory allocation.
%     \item Latency Considerations: While batching can improve throughput by processing multiple sequences simultaneously, it can also introduce latency, particularly when waiting for the completion of longer sequences before proceeding to the next batch. This trade-off between latency and throughput requires careful consideration in real-time or interactive applications.
% \end{itemize}

% Dynamic Batching Application: Refer to \href{https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/examples/jetson/concurrency_and_dynamic_batching/README.html}{this link}
% Dynamic batching is a strategy used in large model inference to optimize the processing of sequences by adjusting batch sizes dynamically based on the characteristics of the input data. Unlike fixed batching, where sequences are grouped into batches of predefined sizes, dynamic batching adapts batch sizes in real-time to improve resource utilization and efficiency.

% While dynamic batching offers flexibility and efficiency benefits, it also faces challenges in maintaining optimal parallelism between prefill and decode rounds, which can lead to inefficiencies and idle time.
% \begin{itemize}
%     \item Sequential Processing Dependencies: Large Language Model (LLM) inference typically involves sequential stages—prefill, where initial states are set up, and decode, where sequences are generated based on these states. These stages often require sequential execution on the same resources due to dependencies, limiting the ability to overlap their execution.

%     \item Variable Batch Completion Times: Even with dynamic batching, sequences within a batch may complete their prefill and decode stages at different times. This variability can result in situations where resources allocated for decode are idle while waiting for prefill to complete on other sequences within the same dynamically sized batch.

%     \item Resource Allocation Efficiency: Efficiently allocating resources between prefill and decode stages becomes crucial. If the system cannot effectively overlap these stages due to sequential dependencies or resource constraints, idle time (or "bubbles") occurs, reducing overall throughput and efficiency.
% \end{itemize}
% By implementing effective overlap strategies, adaptive resource allocation, and optimized batch scheduling policies, it is possible to mitigate these challenges and improve overall efficiency in large model inference tasks.

\begin{table*}
    \centering
    \caption{Notation Table: Sets, Parameters and Decision Variables}
    \begin{tabular}{cl}
    \toprule
        Sets    & Description \\
    \midrule
        $\mathcal{I}=\left\{1,2,\cdots \right\}$  & Set of requests. \\
        % $\mathcal{I}\left(t\right) \subseteq \mathcal{I}$ & Set of remaining requests to be scheduled at time $t$.\\
        $\mathcal{J}=\left\{1,2,\cdots \right\}$  & Set of clients.\\
        $\mathcal{K},\mathcal{K}^\text{p}, \mathcal{K}^\text{d}=\left\{1,2,\cdots \right\}$& Set of bins, prefill stages, and decode stages, respectively. $\mathcal{K} = \mathcal{K}^\text{p} = \mathcal{K}^\text{d}$.\\
        $\mathcal{L}=\left\{1,2,\cdots \right\}$  & Set of all possible levels for the prefill stage.\\
    \midrule
        Parameters          & \\
    \midrule
        $I=\mid \mathcal{I} \mid $                 & Total number of requests.\\
        $J=\mid \mathcal{J} \mid $                 & Total number of clients.\\
        $K=\mid \mathcal{K} \mid $                 & Total number of bins.\\
        $N^\text{p}_i \in \mathbb{Z}^+$             & Input token number of request $i$, for $i\in \mathcal{I}$. \\
        $N^\text{d}_i \in \mathbb{Z}^+$         & Output token number of request $i$, for $i\in \mathcal{I}$. \\
        $N^\text{cap}_l \in \mathbb{Z}^+$        & Maximum token capacity of level $l$, for $l\in \mathcal{L}$. \\  
        $T^\text{d} \in \mathbb{R}^+$            & Decode time per token.\\
        $T^\text{p} \in \mathbb{R}^+$            & Prefill time per token. \\
        $T^\text{p}_l \in \mathbb{R}^+$          & Prefill time of level $l$, for $l\in \mathcal{L}$. \\
    \midrule
        Decision Variables      & \\
    \midrule
        $p_{i,j,k}\in \{0,1\}$ & Assignment of prefill stage $k$ to request $i$ on client $j$ for prefill phase, for $i\in \mathcal{I}$, $j\in \mathcal{J}$, and $k\in \mathcal{K}^\text{p}$.  \\
        $d_{i,j,k}\in \{0,1\}$ & Assignment of decode stage $k$ to request $i$ on client $j$ for decode phase, for $i\in \mathcal{I}$, $j\in \mathcal{J}$, and $k\in \mathcal{K}^\text{d}$.  \\
        % $t^\text{s}_{k}\in \mathbb{R}^+$  & Start time of the $k$th bin, for $k\in \mathcal{K}$.  \\
        % $t^\text{e}_{k}\in \mathbb{R}^+$  & End time of the $k$th bin, for $k\in \mathcal{K}$.  \\
        $t^\text{s,p}_{k}\in \mathbb{R}^+$  & Start time of the $k$th prefill stage, for $k\in \mathcal{K}^\text{p}$.  \\
        $t^\text{s,d}_{k}\in \mathbb{R}^+$  & Start time of the $k$th decode stage, for $k\in \mathcal{K}^\text{d}$.\\
        $n^\text{p}_{k} \in \mathbb{R}^+$ & Time length of the $k$th  prefill stage, for $k\in \mathcal{K}^\text{p}$.  \\
        $n^\text{d}_{k} \in \mathbb{R}^+$ & Time length of the $k$th  decode stage, for $k\in \mathcal{K}^\text{d}$.  \\
        $w_{i,j,k}\in [0,1]$   & The proportion of the decoding phase of request $i$ executed in the $k$th decode stage on client $j$, for $i\in \mathcal{I}$, $j\in \mathcal{J}$, and $k\in \mathcal{K}^\text{d}$. \\
        $x_{i,j}\in \{0,1\}$ & The assignment of request $i$ to client $j$, for $i\in \mathcal{I}$ and $j\in \mathcal{J}$. \\
        $y_{k,l}\in \{0,1\}$    & Indicator variable specifying if the $k$th prefill stage is at level $l$, for $k\in \mathcal{K}^\text{p}$ and $l\in \mathcal{L}$.\\
        $t^\text{max} \in \mathbb{R}^+$ & Total inference time for all requests completed.\\
    \bottomrule
    \end{tabular}
    \label{notation table}
\end{table*}

\subsection{Inference scheduling problem description}
Inference scheduling aims to schedule inference requests using continuous batching and PD Competition, with the goal of minimizing the total inference time while adhering to operational constraints. The problem settings are given as follows and related notations are presented in TABLE \ref{notation table}.

\begin{enumerate}
    \item \textbf
{Requests and processing time:}
    \begin{itemize}
        \item Let \( \mathcal{I}=\left\{ 1,2,\cdots\right\} \) be the set of inference requests. Inference request $i$, for $i \in \mathcal{I}$, has a fixed input token number $N^\text{p}_i \in  \mathbb{Z}^+$ for prefill phase and output token number $N^\text{d}_i \in  \mathbb{Z}^+$ for decode phase. The input token number is assumed to be known, but the output token number is unknown.
        \item Each request has a known prefilling time, linearly related to the total number of input tokens in a bin.
        \item The decoding time is approximately linearly related to the output token number. Let $\mathcal{J}=\left\{1,2,\cdots \right\}$ be the set of clients and $T^\text{d}$ be the decode time per token. The minimal unit of decoding time is equal to the amount of time that each client processes a single token, i.e., $T^\text{d}|\mathcal{J}|$.
    \end{itemize}
    \item \textbf{Bin and batch size:}
    \begin{itemize}
        \item Let $\mathcal{K}=\left\{1,2,\cdots \right\}$ be the set of bins. The inference process is divided into a total of \( K=|\mathcal{K}| \) bins.
        \item The batch size \( |\mathcal{J}| \) represents the maximum number of requests that can be processed simultaneously in a batch.
    \end{itemize}
    \item \textbf{Stages:}
    \begin{itemize}
        \item There are two stages in hardware operation system: prefill and decode. Prefill and decode stages must alternate, ensuring that each stage is dedicated exclusively to one type of operation.
        \item At any given time, the system can perform either prefill or decode stages, but not both.
        \item At any time as the system starts a decode stage, the length of this operation is determined. Meanwhile, all requests processed at this stage will be scheduled. The decision is made based on system state, including information of the duration of the current bin, type of requests being processed, and the set of remaining requests. It is illustrated in Fig. \ref{fig:control}.
    \end{itemize}
    \item \textbf{Assignment and allocation:}
        \begin{itemize}
        \item Each request must be assigned to exactly one prefill stage for processing.
        \item For every request, the prefill phase must be completed by the same client that will subsequently carry out its decode phase. Hence, both phases must be allocated to the same client.
        \item  A client can process only one request at a time. Once a client begins processing a request, it remains occupied and cannot preempt tasks until both the prefill and decode stages of that request are completed.
    \end{itemize}
\end{enumerate}

% \kai{Machine, client, and batch may be confusing. We need to handle this later.}

% Despite the flexibility offered by dynamic batching, maintaining optimal parallelism between prefill and decode stages can be challenging. Variability in the completion times of requests within dynamically sized batches may lead to idle periods in resource utilization. Efficiently scheduling these operations to minimize such idle times while maximizing throughput is crucial for optimizing large model inference.




% \subsection{Model formulation}
% The LLM inference process is formulated as a sequential decision making problem under uncertainty. We use $t\in \mathbb{R}^+$ to denote time point.
% Let $\mathcal{I}\left(t\right) \subseteq \mathcal{I}$ be the set of remaining requests to be scheduled at time $t$. 
%The process of handling a single request is divided into two operations: prefill and decode. The duration of the prefill operation linearly depends on the number of tokens and is assumed to be known. The duration of the decode operation also positively depends on the number of tokens, but the relationship is uncertain and observed to be non-linear. However, since the decoding process outputs one token per client per round and can be preempted by the prefill operation at the end of each round, the time for each round of decoding is only related to the number of parallel clients and can be considered linear.

%Let $N^{\textrm{p}}_i \in \mathbb{Z}^+$ and $N^{\textrm{d}}_i \in \mathbb{Z}^+$, for $i\in \mathcal{I}$, be the token numbers of prefill and decode of request $i$, respectively. Given the number of tokens, denoted by $N\in \mathbb{Z}^+$, the duration of prefill is given by $G^{\textrm{p}}\left(N\right) \in \mathbb{R}^+$ and is linear and positively related with the number of input tokens $N$. Similarly, we use $G^{\textrm{d}}\left(N\right)$ to denote the duration of the decode, and it is also a linear function and positively related with the number of output number $N$. A client can be in either prefill or decode operations, and at any time all clients have to be in the same operation. The inference process is divided into $K\in \mathbb{Z}^+$ bins, and each bin has one prefill operation and then followed by a decode operation. For a request, when the prefill starts, it has to be finished within the same bin. However, the decode operation of a request can be carried out in several consecutive bins if one bin is not long enough. In other words, the decode operation can be preempted by prefill operations. We assume that $K$ is large enough to have all requests to fit in. The online inference process is illustrated by a Grantt chart in Fig. \ref{fig:illustration}. There may be multiple clients, and the bars for the first and second clients are presented. Three bins are shown in the example, and all the clients have to be in the same operation, i.e., either prefill or decode. Among three bins, the first client processes three requests, while the second client only processes two. The first request of the second client takes two bins. 





\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{graph//control.pdf}
    \caption{Online scheduling}
    \label{fig:control}
\end{figure}

\subsection{Deterministic equivalence}
In this hybrid offline-online problem, the offline decision component involves determining the assignment and sequence of given requests on clients.
%which pertains to the decision variables \(t^\text{max}, p_{i,j,k}, d_{i,j,k}, x_{i,j}\). 
As a sequential decision-making problem, the online decision component focuses on determining the length of each bin and the sequence of remaining requests in the future, with the aim of minimizing the total inference time. 
%This part is associated with the decision variables \(t^\text{s,p}{k}, t^\text{s,d}{k}, n^\text{p}{k}, n^\text{d}{k}, p_{i,j,k}, d_{i,j,k}, w_{i,j,k}, y_{k,l}\).

Without considering uncertainty, the offline-online inference scheduling can be formulated as a deterministic equivalence, which is a form of an MIP model as follows. 
%We assume that the prefilling and decoding tokens of each request are known in advance, denoted as $N^\text{p}_i, N^\text{d}_i$, respectively. The optimization of inference throughput concerns the processing time in the prefilling and decoding bins. The decoding time per token is a function of $|\mathcal{J}|$, hence we can treat it as a constant parameter, denoted as $T^\text{d}$. The prefilling time, however, is a linear function of the total number of tokens to be prefilled in a bin. This varying marginal cost implies a nonlinear model, but can be linearized. We define a set of levels $l \in \mathcal{L}$ for the prefilling bins, each with a token capacity $N^{cap}_l$ and the corresponding prefilling time $T^p_l$, and use binary variable $y_{k,l}$ to denote that decoding bin $k$ is of level $l$. Assuming a monotonically non-decreasing function for the prefilling time, in the optimal solution, the token load among the prefilling bins will be balanced, while automatically choosing the minimally containable level for each bin.

\begin{align}
    \label{obj:makespan}
    & \min t^\text{max} \\
    & \nonumber \text{s.t.} \\
    \label{cst:makespan_def}
    & t^\text{max} \geq t^\text{s,d}_{k} + n^\text{d}_{k}, \text{ for } k \in \mathcal{K}^\text{d}, \\
%    \label{cst:bin_def_1}
%    & t^s_0 = 0, \\
    \label{cst:bin_def_2}
    & t^\text{s,p}_{k} - \left(t^\text{s,d}_{k-1} + n^\text{d}_{k-1}\right) \geq 0, \text{ for } k = 2,...,K,\\
    \label{cst:bin_def_3}
    & t^\text{s,d}_{k} - \left(t^\text{s,p}_{k} + n^\text{p}_{k}\right) \geq 0, \text{ for } k = 1,2,...,K,\\
    \label{cst:prefill_length_def_1}
    & n^\text{p}_{k} \geq \sum_{l \in \mathcal{L}} T^\text{p}_l y_{k,l}, \text{ for } k \in \mathcal{K}^\text{p},\\
    \label{cst:prefill_length_def_2}
    & \sum_{i \in \mathcal{I}} \sum_{j \in \mathcal{J}} N^\text{p}_i p_{i,j,k} \leq \sum_{l \in \mathcal{L}} N^\text{cap}_l y_{k,l}, \text{ for } k \in \mathcal{K}^\text{p},\\ 
    \label{cst:level_def}
    & \sum_{l \in \mathcal{L}} y_{k,l} = 1,  \text{ for } k \in \mathcal{K}^\text{p}, \\
    \label{cst:decode_length_def_1}
    & n^\text{d}_{k} \geq T^\text{d} \sum_{i\in \mathcal{I}} N_i^\text{d} w_{i,j,k}, \text{ for }j \in \mathcal{J}, \text{ and } k \in \mathcal{K}^\text{d},\\
    \label{cst:immediate_prefill_decode}
    & d_{i,j,k} - p_{i,j,k} \geq 0, \text{ for }i \in \mathcal{I}, j \in \mathcal{J}, \text{ and } k \in \mathcal{K}^\text{d}, \\
    \label{cst:consecutive_decode_bin}
    \begin{split}
    &M\left(2 - d_{i,j,k_1} - d_{i,j,k_2}\right) + \sum_{k'=k1}^{k_2} d_{i,j,k'} \\ 
    &\quad \geq k_2 - k_1 + 1, \\
    &\quad \text{ for }i \in \mathcal{I}, j \in \mathcal{J}, k_1,k_2 \in \mathcal{K}^\text{d},\text{ and }k_1<k_2, 
    \end{split} \\
    \label{cst:prefill-decode-order}
    \begin{split}
    & M\left(p_{i,j,k_1}-1\right) +d_{i,j,k_2} \leq 0, \\
    &\quad \text{ for }i \in \mathcal{I}, j \in \mathcal{J}, k_1 \in \mathcal{K}^\text{p},k_2\in \mathcal{K}^\text{d}, \text{ and }k_1 > k_2, 
    \end{split} \\
    \label{cst:d_def_1}
    & \sum_{i\in \mathcal{I}} d_{i,j,k} \leq 1, \text{ for }j \in \mathcal{J}, \text{ and } k \in \mathcal{K}^\text{d}, \\
    \label{cst:d_def_2}
    & \sum_{k \in \mathcal{K}^\text{d}} d_{i,j,k} \leq K, \text{ for }i \in \mathcal{I}, \text{ and }j \in \mathcal{J}, \\
    \label{cst:w_def_1}
    & \sum_{k \in \mathcal{K}^\text{d}} w_{i,j,k} = x_{i,j}, \text{ for }i \in \mathcal{I}, \text{ and }j \in \mathcal{J}, \\
    \label{cst:w_def_2}
    & \sum_{j\in \mathcal{J}} \sum_{k \in \mathcal{K}^\text{d}} w_{i,j,k} = 1, \text{ for }i \in \mathcal{I}, \\
    \label{cst:p_def_1}
    & \sum_{i \in \mathcal{I}} p_{i,j,k} \leq 1, \forall j, k, \\
    \label{cst:p_def_2}
    & \sum_k p_{i,j,k} = x_{i,j}, \text{ for }j \in \mathcal{J}, \text{ and } k \in \mathcal{K}^\text{p}, \\
    \label{cst:loc_def}
    & \sum_{j \in \mathcal{J}} x_{i,j} = 1, \text{ for }i \in \mathcal{I}, \\
    \label{cst:var_def_start}
    & x_{i,j} \in \{0,1\}, \text{ for }i \in \mathcal{I}, \text{ and } j \in \mathcal{J}, \\
    & y_{k,l} \in \{0,1\}, \text{ for }k \in \mathcal{K}^\text{p}, \text{ and } j \in \mathcal{J}, \\
    & p_{i,j,k} \in \{0,1\}, \text{ for }i \in \mathcal{I}, j \in \mathcal{J}, k \in \mathcal{K}^\text{p}, \\
    & d_{i,j,k} \in \{0,1\}, \text{ for }i \in \mathcal{I}, j \in \mathcal{J}, k \in \mathcal{K}^\text{d}, \\
    & w_{i,j,k} \in [0,1], \text{ for }i \in \mathcal{I}, j \in \mathcal{J}, k \in \mathcal{K}^\text{d}, \\
    & t^\text{s,p}_{k}, n^\text{p}_{k}, \text{ for }k \in \mathcal{K}^\text{p},\\
    \label{cst:var_def_end}
    & t^\text{s,d}_{k}, n^\text{d}_{k}, \text{ for }k \in \mathcal{K}^\text{d}.
\end{align}
The total inference time is denoted by $t^\text{max}$, which is minimized in the objective function given in Eq. (\ref{obj:makespan}). We let $\mathcal{K}^\text{d}=\left\{1,2,\cdots \right\}$ be the set of decode stages. For any decode stage $k\in \mathcal{K}^\text{d}$, let $t^\text{s,d}_{k}\in \mathbb{R}^+$ and $n^\text{d}_{k}\in \mathbb{R}^+$ denote the start time and time length of the $k$th decode stage, respectively, and thus its end time is $\left(t^\text{s,d}_{k}+n^\text{d}_{k}\right)$. Eq. (\ref{cst:makespan_def}) requires that the total inference time is greater than or equal to the end time of any decode stage. Let $K \in \mathbb{Z}^+$ be the total number of bins, prefill stages, and also decode stages. Any prefill stage should start after the end of its previous decode stage, suggested by Eq. (\ref{cst:bin_def_2}). For any prefill stage $k\in \mathcal{K}^\text{p}$, let $n^\text{p}_{k}\in \mathbb{R}^+$ be the time length of the $k$th prefill stage. Thus, Eq. (\ref{cst:bin_def_3}) suggests that, within the same bin, the decode stage should start after the end of prefill stage. We use $\mathcal{L}=\left\{1,2,\cdots \right\}$  to represent the set of all possible levels for a prefill stage, and the time length of a prefill stage depends on the level. Let $y_{k,l}\in \left\{0,1\right\}$, for $k\in \mathcal{K}^\text{p}$ and $l \in \mathcal{L}$, be an indicator variable. Prefill stage $k$ is at level $l$, if $y_{k,l}=1$. Otherwise, the prefill stage is not at level $l$. We denote by $T^\text{p}_l \in \mathbb{R}^+$ the prefill time of level $l$, for $l\in \mathcal{L}$, and thus the time length of a prefill stage is determined in Eq. (\ref{cst:prefill_length_def_1}). We use $N^\text{cap}_l \in \mathbb{Z}^+$ to denote the maximum token capacity of level $l$, for $l\in \mathcal{L}$, and let $p_{i,j,k}\in \left\{0,1\right\}$ denote assignment of prefill stage $k$ to request $i$ on client $j$ for prefill phase, for $i\in \mathcal{I}$, $j\in \mathcal{J}$, and $k\in \mathcal{K}^\text{p}$. The relationship between $p_{i,j,k}$ and $y_{k,l}$ is given in Eq. (\ref{cst:prefill_length_def_2}). Eq. (\ref{cst:level_def}) suggests that any prefill stage can only be at one level in $\mathcal{L}$. Different than the prefill phase, the decode phase of a request can be served in multiple decode stages. Thus, we introduce $w_{i,j,k}\in [0,1]$ to represent the proportion of the decoding phase of request $i$ executed in the $k$th decode stage on client $j$, for $i\in \mathcal{I}$, $j\in \mathcal{J}$, and $k\in \mathcal{K}^\text{d}$. The time length of a decode stage is provided by Eq. (\ref{cst:decode_length_def_1}). Let $d_{i,j,k}\in \{0,1\}$ be the assignment of decode stage $k$ to request $i$ on client $j$ for decode phase, for $i\in \mathcal{I}$, $j\in \mathcal{J}$, and $k\in \mathcal{K}^\text{d}$. Eqs. (\ref{cst:immediate_prefill_decode})-(\ref{cst:prefill-decode-order}) together sets the rule that the decoding phase of a request must immediately follow the consecutive prefilling phase or its previous decoding phase. Eqs. (\ref{cst:d_def_1})-(\ref{cst:loc_def}) relate the assignment decisions among requests, clients, and bins. Eqs. (\ref{cst:var_def_start})-(\ref{cst:var_def_end}) define the domain for each set of variables, respectively.

Eqs (\ref{obj:makespan})-(\ref{cst:var_def_end}) will hereafter be referred to as ``the original model''. We attempted to solve the original model using commercial MIP solvers, such as Gurobi. However, we found it nearly impossible to solve such a large-scale problem directly. The number of requests, denoted by $|\mathcal{I}|$, is around 1,000 in small cases, while in larger cases, it can be up to 10,000. The number of batch sizes or client numbers, denoted by $|\mathcal{J}|$, can reach up to 200. The number of exclusive bins, denoted by $|\mathcal{K}|$, often is on the same order of magnitude as $|\mathcal{I}|$. %often exceeds the number of requests by a factor of three or more. 
To probe the solving cost, we solved a down-scaled toy case model with merely 100 requests and 20 clients, which took Gurobi more than 3,600 seconds to yield a near-optimal solution without fully closing the dual gap. Solving this problem in its original form is hence evidently impractical and cannot be solved in hours. Therefore, it is necessary to decompose this problem into stages and address it sequentially.
