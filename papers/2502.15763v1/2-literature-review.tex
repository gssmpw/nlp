\section{Literature review}
\label{literature}

% ref: Nvidia blog https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
% ref: Inference paper list https://github.com/DefTruth/Awesome-LLM-Inference
% ref: Review paper https://arxiv.org/abs/2404.14294

\begin{table*}[t]
    \centering
    \caption{Literature Review}
    \begin{tabular}{cccccccccc}
    \toprule
        Work & Throughput & Latency & \makecell{Cache \\ Management} & \makecell{Dynamic \\ Decision} & Uncertainty & \makecell{Request \\ Management} & \makecell{Iteration\\ Management} & Batching & \makecell{Distributed \\ Strategies}\\
    \midrule
        Orca & \checkmark & \checkmark &  &  & \checkmark  &  & \checkmark & \checkmark & \\
        vLLM & \checkmark  & \checkmark  & \checkmark  &  &  &  &  &  & \\
        Sarathi-Serve\cite{agrawal2024sarathi} & \checkmark & \checkmark &  &  &  &  & \checkmark & \checkmark & \\
        Llumnix\cite{sun2024llumnix} &  & \checkmark & \checkmark & \checkmark &  & \checkmark &  &  & \checkmark\\
        InfiniGen\cite{lee2024infinigen} &  & \checkmark & \checkmark &  &  &  &  &  & \\
        dLoRA\cite{wu2024dlora} & \checkmark & \checkmark  & \checkmark & \checkmark &  &  &  & \checkmark & \checkmark\\
        VTC\cite{sheng2024fairness} &  &  &  & \checkmark &  & \checkmark &  &  & \\
        FastServe\cite{wu2023fast} & \checkmark  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  &  & \\
        DistServe\cite{zhong2024distserve} & \checkmark & \checkmark & \checkmark & \checkmark &  &  & \checkmark &  & \checkmark\\
        MoonCake & \checkmark & \checkmark & \checkmark & \checkmark &  &  &  &  & \checkmark \\
    \midrule
        OURS & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \\
    \bottomrule
    \end{tabular}

    \label{table:literature_review}
\end{table*}
% Standards in the table:
% cache management: whether caring about KV cache
% dynamic decision: whether making decisons in different states
% uncertainty: whether caring about the ouput length
% request management: whether changing the order of requests
% iteration management: whether caring about the order of PD
% batching: batching strategies
% distributed strategies: whether using GPU clusters


This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly employed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain.


% 1. General inference optimization techniques
As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization \cite{jacob2018quantization}, sparsification \cite{child2019generating,bai2024sparsellm}, and distillation \cite{hinton2015distilling}. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention \cite{shazeer2019fast} or grouped-query attention \cite{ainslie2023gqa} in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) and model parallelism aim to fully leverage the computational power of the devices. In DP \cite{narayanan2021efficient}, the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per-device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism (PP) \cite{huang2019gpipe}, tensor parallelism (TP) \cite{shoeybi2019megatron}, sequence parallelism (SP) \cite{korthikanti2023reducing}, and context parallelism (CP) \cite{liu2023ring}. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency.

% vertical to our methods, we can use our methods along with these techniques

% Techniques such as model compression, pruning, quantization, pipeline parallelism, and distributed computing have been explored extensively. 
% However, these approaches often face limitations related to applicability, overhead, and real-time performance. 

% 2. LLM serving techniques
% Flash attention, speculative decoding, batching (continuous batching), memory management (paged attention), chunked prefill, distributed systems, scheduling
In addition to general model serving techniques, the optimization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion, Key-Value (KV) cache management, request management, iteration management, batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention \cite{dao2022flashattention} amalgamates the operations of data transfer between hardware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding \cite{leviathan2023fast, chen2023accelerating} employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca \cite{yu2022orca} pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM \cite{kwon2023efficient} introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe \cite{agrawal2024sarathi} introduces chunked-prefills, also known as dynamic SplitFuse or prefill-decode (PD) Fusion, batching together prefill and decode chunks to maximize both computation and bandwidth utilization. Since serving systems are commonly deployed on distributed platforms, numerous strategies have been proposed to exploit distributed characteristics. For example, recent works \cite{patel2024splitwise,zhong2024distserve,hu2024inference} advocate for separating prefill servers from decode servers, also known as PD Separation, due to the distinct computational and bandwidth characteristics of these two stages. For a comprehensive review of these techniques, we recommend \cite{zhou2024survey} for further reference.
These classic techniques form the foundation of inference services and our methods. 

% 3. Recent advancement
Recently, with ongoing advancements in AI system research, numerous innovative inference techniques have been developed, particularly those related to schedulers. We present some representative works and highlight the differences between these approaches and our method in TABLE~\ref{table:literature_review}. Inspired by context switching across CPU cores, Llumnix \cite{sun2024llumnix} proposes a live migration mechanism and a dynamic scheduling policy to reschedule requests across multiple model instances of LLM deployed on GPU clusters, thereby enhancing load balancing and isolation. InfiniGen \cite{lee2024infinigen} addresses the challenge of large KV cache sizes for long-text generation by speculating and prefetching critical KV cache entries. For LoRA models, dLoRA \cite{wu2024dlora} addresses the challenges of serving multiple LoRA models by dynamically merging and unmerging adapters with the base model and migrating requests and adapters between replicas. Sheng et al. \cite{sheng2024fairness} study the fairness problem in LLM serving concerning clients and proposes a novel scheduling algorithm called the virtual token counter (VTC). FastServe \cite{wu2023fast} proposed an innovative skip-join MLFQ scheduler to enable preemption during the autoregression process of LLM inference. In distributed systems, DistServe \cite{zhong2024distserve} tackles the issues of PD interference and resource coupling by disaggregating prefill and decoding computation, and proposes placement algorithms to optimize resource allocation and parallelism strategies for different phases. Mooncake \cite{qin2024mooncake} also proposes a disaggregated architecture that separates the prefill and decode clusters and utilizes a KV cache-centric scheduler to manage the cache flow. While these innovative techniques attempt to address the issues faced by the scheduler to some extent, they scarcely model the scheduling problem formally and are limited to solve the inference optimization problem theoretically.


% 4. Scheduling methods
In the domain of operations research, scheduling is a well-established and extensively utilized approach. For instance, offline scheduling methods are applied in manufacturing systems \cite{wang2020print3D}, healthcare systems \cite{pang2018surgery}, and operations management systems \cite{erdogan2015online}. To address real-time decision-making or multi-stage stochastic scenarios, online scheduling methods have been introduced in these systems \cite{pang2022dynamic, lee2019online}. Nevertheless, none of the systems we examined achieve the rapid decision frequency—down to 10 milliseconds—that is observed in LLM inference systems. Furthermore, traditional stochastic programming models are not suitable for sequential decision-making problems where real-time information is continuously revealed. Therefore, it is imperative to develop and tailor traditional methods for application in this emergent domain of LLM inference. Research on LLM inference optimization can not only enhance hardware resource utilization in the LLM domain but also expand the repertoire available to existing operations research algorithms.


% Scheduling methods.
% Should include two parts: LLM inference applications (fast serve, vllm); and scheduling applications in manufacture.

