\section{Conclusion and Future Work}
\label{conclusion}
In this paper, we study the inference optimization problem in the service system when deploying LLMs. To enhance the system throughput and better utilize the hardware, we formulate an MIP model to describe this inference optimization problem. To the best of our knowledge, this is the first formulation of the problem from a scheduling perspective. To tackle the complex and high real-time demands of LLM inference, we introduce a hybrid offline-online method.

In the offline method, we demonstrate how large-scale inference systems can be improved using a Minimizing Makespan Bin Packing Problem and how a theoretical lower bound can be provided. In the online request scheduling and iteration scheduling methods, the solution time efficiency is crucial. We propose a sorting and online preemptive method to more effectively utilize clients that finish early. Then, we focus on the iteration scheduling component of the original model and employ a Lagrangian method to compare the costs of adding a prefill stage versus a decode stage at each iteration. We provide a time efficient heuristic method to determine when to insert a prefill task and interrupt ongoing decoding tasks. 

In real-world experiments, we deploy the LlaMA-65B LLM model and infer the GSM 8K dataset, which includes 1,319 unique math problems. Our offline model increases machine utilization rates from a baseline of 80.2\% to 85.5\%, and reduces the total inference time from 201 seconds to 197 seconds. Utilizing the online scheduling methods, the system utilization rate can be further increased to 89.1\%, and the total inference time for the dataset can be reduced to 191 seconds. As demonstrated, if all our methods are implemented, system throughput can be improved by 5.46\%, and hardware utilization can increase by 11.0\%. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0\% on average.

The future directions of this research can be extended along three key aspects:

\begin{itemize}
    \item Stochastic Model \& Efficient Solution Method: The original MIP model we proposed is a deterministic equivalence formulation. While solving this model is already computationally challenging, developing a stochastic programming model could further enhance its accuracy by better accounting for uncertainties. Additionally, more efficient solution methods for tackling the original MIP model are needed to meet the millisecond-level real-time requirements of the decision-making process.
    \item Reinforcement Learning on Iteration Scheduling: the current iteration scheduling approach relies on a heuristic online method. Notably, the decision-making process in this method involves choosing between two options: prefill or decode. Since online state variables such as prefill task waiting time, the number of decoding clients, expected decoding time, and expected prefill time are relatively easy to derive, a simple reinforcement learning (RL) model could be trained to assist the scheduler in making decisions dynamically. 
    %Preliminary experiments using a basic RL model show some improvements. However, there are certain limitations to address before RL can be fully utilized: (1) the scalability of the RL modelâ€”an offline-trained RL model may not generalize well to other large language models or datasets; and (2) RL models consume GPU resources, which need to be factored into the cost-benefit analysis of this approach.
    \item Online Hardware Utilization Method: We observed during hardware experiments that the system's hardware is often underutilized when a static number of clients is employed, due to stochastic variations in the output length. In scenarios where dynamic and continuous batching methods are applicable, investigating online decision-making for hardware utilization could further optimize performance. Specifically, determining the optimal number of clients that can be allocated concurrently to the system at any given time could help enhance resource utilization and overall efficiency.
\end{itemize}

% \kai{Our method is also suitable for speculative decoding. Move this part to methods?}