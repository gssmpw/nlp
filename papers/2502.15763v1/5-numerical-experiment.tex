\section{Numerical Experiment}
\label{numerical-experiment}
\subsection{Experiment settings and baseline}
Before the model is solved by our hybrid method, extensive analysis is conducted to evaluate the time taken by the decode and prefill stages on hardware. This analysis provides crucial insights into the computational demands and performance characteristics of each stage. By quantifying these metrics, such as processing times and resource utilization, the data analysis establishes a solid foundation of empirical data. The data serve as reliable support for subsequent decision-making in optimizing scheduling strategies. The basic experiment setting is given in TABLE \ref{Experiment Settings}.


\begin{table}
    \centering
    \caption{Experiment Settings}
    \begin{tabular}{ccc}
    \toprule
    Parameter     & Number & Brief Description\\
    \midrule
    $|\mathcal{I}|$     & 1319 & The GSM8K dataset \\
    $|\mathcal{J}|$     & 200 & Due to hardware memory limit\\
    $\mathbf{E}(N_i^\text{p})$     & 68.43 & The GSM8K dataset input\\
    $\mathbf{E}(N_i^\text{d})$     & 344.83 & The Llama 65B output\\
    $T^\text{p}$     & 0.13 ms/token & The hardware performance on prefilling\\
    $T^\text{d}$     & 0.21 ms/token & The hardware performance on decoding\\
    \bottomrule

    \end{tabular}
    \label{Experiment Settings}
\end{table}

We demonstrate our improvements by utilizing the GSM8K dataset \cite{cobbe2021training}, a comprehensive collection of mathematical word problems specifically designed to assess the problem-solving and reasoning capabilities of language models. This dataset serves as a benchmark for evaluating both arithmetic and logical reasoning skills, essential attributes for advanced language models. Each problem in the dataset is intricately constructed to simulate real-world situations involving numerical relationships, necessitating that models comprehend the problem contextually and perform accurate calculations to derive the correct solution.

The GSM8K dataset comprises 1,319 unique problems as input requests, with an average input length of 68.43 tokens and a standard deviation of 25.04 tokens. For our experiments, we selected the LLaMA-65B language model due to its open-source nature and wide accessibility, making it a suitable candidate for academic and reproducible research. In our tests, the LLaMA-65B model generated responses averaging 344.83 tokens, with a standard deviation of 187.99 tokens. To ensure consistency and focus on quality responses, we constrained the maximum output length to 512 tokens during testing.

Our computational setup is characterized by a robust hardware configuration, consisting of eight Ascend processing units, each equipped with a maximum memory capacity of 64 GB. This formidable hardware infrastructure is essential for facilitating the efficient processing and testing necessary for our experiments. Additionally, we have assessed the KV cache usage for input in this experiment, establishing baseline settings that are also utilized in practical applications. The current hardware, along with the LLM employed, imposes a memory constraint of 1024 blocks of KV cache. Each block can accommodate a maximum of 128 tokens. For the GSM-8k benchmark, the combined maximum input and output for each request requires five blocks. Consequently, this configuration limits us to a maximum of approximately 200 clients running concurrently, calculated by the expression $1024/5 \approx 200 $.

In our experimental setup, we conduct an estimation of the operation time required for prefill and decode stages using over 400 data groups. We find that both prefill and decode times exhibit a linear relationship with the number of tokens involved. Specifically, the prefill time can be calculated as 0.13 milliseconds per token, plus a fixed overhead of 25 milliseconds. For the decode process, the time required for each batch of clients can be estimated as 0.21 milliseconds per token, with an additional fixed overhead of 29 milliseconds. For instance, when processing a parallel batched decode stage involving 200 clients, where each client produces one token per round, the operation would take approximately $200 \times 0.21 + 29 = 71$ milliseconds. In the case of prefill stages, if a batch consists of inputs totaling 5,000 tokens, the estimated time required would be $5000 \times 0.13 + 25 = 675$ milliseconds.

We present a Gantt chart in Fig. \ref{fig:baseline}, generated from an experiment using real-world data and open source LLM, to illustrate the current state of online inference services without the implementation of our proposed method. This chart demonstrates that, in practical scenarios, a significant number of idle periods, or ``bubbles'', occur when no scheduling strategy is employed. Furthermore, in offline scenarios, if the workload among clients is not evenly distributed, substantial machine idle time is observed after the early completion of some client's tasks. Our analysis of this Gantt chart reveals that the overall machine utilization rate is only 80.2\%.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{graph//result_gantt_baseline.pdf}
    \begin{flushleft}
    \vspace{-2em}
    \caption*{Utilization rate: 80.2\%. Total inference time: 201.00 seconds.}
    \end{flushleft}
    \vspace{-2em}
    \caption{Result Gantt: Baseline}
    \label{fig:baseline}
\end{figure}

\subsection{Offline request scheduling result}
This offline request scheduling model given by Eqs.  (\ref{offline_bin_packing_start})--(\ref{offline_bin_packing_end}) can be solved using open-source solver SCIP. Due to significantly reduced complexity, optimal solutions can be achieved within 20 minutes comparing to original problem which is not possible to be solved within hours. Although this offline model only addresses workload balancing using estimations of output length, its performance surpasses that of the original version. As illustrated in Fig. \ref{fig:offline_assignment}, the system shows a significant reduction of idle times, and machine utilization is enhanced to 85.5\%. Comparing to the baseline method, this method provides a more balanced request assignment across clients and reduce ``bubbles''. The total inference time can be reduced from 201.00 seconds to 197.08 seconds. Since solving the model still takes relatively long time, we list this method as optional and suggest practitioners use the offline model in typical scenarios such as RLHF training. 

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{graph//result_gantt_job_assignment.pdf}
    \begin{flushleft}
    \vspace{-2em}
    \caption*{Utilization rate: 85.5\%. Total inference time: 197.08 seconds.}
    \end{flushleft}
    \vspace{-2em}
    \caption{Result Gantt: Offline Request Scheduling}
    \label{fig:offline_assignment}
\end{figure}


\subsection{Online scheduling results}

Incorporating online requests and iteration scheduling methods, as depicted in Fig. \ref{fig:online+offline}, results in a marked improvement in total inference time, showing reductions 190.58s compared to 201.00s in the baseline scenario. Additionally, machine utilization is enhanced to 89.06\%. Comparing to offline scheduling method, these two online methods do not require additional computing and can be used for current online inference.

In Fig. \ref{fig:online_only}, we present the results obtained using only the online scheduling method, without employing the offline scheduling method. As shown, compared to the baseline, the utilization rate improves to 86.19\%, and the total inference time decreases to 193.33 seconds. These results demonstrate that the online method performs well even in the absence of prior knowledge about requests. This scenario is common in the area of LLM inference.

We also calculate the theoretical lower bound using Eq. (\ref{theoreticalLB}). In the specified numerical case utilizing GSM8K, the theoretical bound is 180 seconds, in which $T^{\text{p}*}=13$ seconds and $T^{\text{d}*}=167$ seconds. In this scenario, we reduce the total inference time from 201.00 seconds in the baseline to 190.08 seconds with the hybrid online-offline method. The gap to the optimal value is thus reduced from \(201 - 180 = 21\) seconds to \(190 - 180 = 10\) seconds, representing a reduction of 52.4\% in this ``primal dual'' gap.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{graph/result_gantt_online_request.pdf}
%     \begin{flushleft}
%     \vspace{-2em}
%     \caption*{Utilization rate: 88.21\%. Total inference time: 191.66 seconds.}
%     \end{flushleft}
%     \vspace{-2em}
%     \caption{Result Gantt: Offline+Online Request Scheduling}
%     \label{fig:online_request}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{graph/result_gantt_online_scheduling_only.pdf}
    \begin{flushleft}
    \vspace{-2em}
    \caption*{Utilization rate: 86.19\%. Total inference time: 193.33 seconds.}
    \end{flushleft}
    \vspace{-2em}
    \caption{Result Gantt: Online only Scheduling}
    \label{fig:online_only}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{graph/result_gantt_online_iteration.pdf}
    \begin{flushleft}
    \vspace{-2em}
    \caption*{Utilization rate: 89.06\%. Total inference time: 190.58 seconds.}
    \end{flushleft}
    \vspace{-2em}
    \caption{Result Gantt: Offline+Online Scheduling}
    \label{fig:online+offline}
\end{figure}


To better demonstrate the performance of our online scheduling methods, we present a numerical experiment involving 100 cases in Figs. \ref{fig:result_numerical_utilization} and  \ref{fig:result_numerical_generate_speed}. These cases are randomly generated with the input and output length distributions shown in TABLE \ref{Experiment Settings}. As illustrated in the figures, despite some variations across the 100 cases, our hybrid offline-online method consistently outperforms in both utilization and generation speed. The unit for generation speed is tokens per second, indicating how many tokens the LLM can generate each second. On average, our method achieves an 8.0\% improvement in utilization and an increase of 100.63 tokens per second in generation speed.



\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{graph/result_numerical_utilization.jpg}
    \caption{Utilization rate with 100 cases}
    \label{fig:result_numerical_utilization}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{graph/result_numerical_generate_speed.jpg}
    \caption{Generate speed with 100 cases}
    \label{fig:result_numerical_generate_speed}
\end{figure}