[
  {
    "index": 0,
    "papers": [
      {
        "key": "agrawal2024sarathi",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "Taming throughput-latency tradeoff in llm inference with sarathi-serve"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "sun2024llumnix",
        "author": "Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei",
        "title": "Llumnix: Dynamic Scheduling for Large Language Model Serving"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lee2024infinigen",
        "author": "Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong",
        "title": "$\\{$InfiniGen$\\}$: Efficient generative inference of large language models with dynamic $\\{$KV$\\}$ cache management"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2024dlora",
        "author": "Wu, Bingyang and Zhu, Ruidong and Zhang, Zili and Sun, Peng and Liu, Xuanzhe and Jin, Xin",
        "title": "$\\{$dLoRA$\\}$: Dynamically Orchestrating Requests and Adapters for $\\{$LoRA$\\}$$\\{$LLM$\\}$ Serving"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sheng2024fairness",
        "author": "Sheng, Ying and Cao, Shiyi and Li, Dacheng and Zhu, Banghua and Li, Zhuohan and Zhuo, Danyang and Gonzalez, Joseph E and Stoica, Ion",
        "title": "Fairness in serving large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wu2023fast",
        "author": "Wu, Bingyang and Zhong, Yinmin and Zhang, Zili and Liu, Shengyu and Liu, Fangyue and Sun, Yuanhang and Huang, Gang and Liu, Xuanzhe and Jin, Xin",
        "title": "Fast distributed inference serving for large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhong2024distserve",
        "author": "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao",
        "title": "$\\{$DistServe$\\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "jacob2018quantization",
        "author": "Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry",
        "title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "child2019generating",
        "author": "Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya",
        "title": "Generating long sequences with sparse transformers"
      },
      {
        "key": "bai2024sparsellm",
        "author": "Bai, Guangji and Li, Yijiang and Ling, Chen and Kim, Kibaek and Zhao, Liang",
        "title": "SparseLLM: Towards global pruning of pre-trained language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "shazeer2019fast",
        "author": "Shazeer, Noam",
        "title": "Fast transformer decoding: One write-head is all you need"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ainslie2023gqa",
        "author": "Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\\'o}n, Federico and Sanghai, Sumit",
        "title": "Gqa: Training generalized multi-query transformer models from multi-head checkpoints"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "narayanan2021efficient",
        "author": "Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others",
        "title": "Efficient large-scale language model training on gpu clusters using megatron-lm"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "huang2019gpipe",
        "author": "Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others",
        "title": "GPipe: Easy scaling with micro-batch pipeline parallelism"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "shoeybi2019megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "korthikanti2023reducing",
        "author": "Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "Reducing activation recomputation in large transformer models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2023ring",
        "author": "Liu, Hao and Zaharia, Matei and Abbeel, Pieter",
        "title": "Ring attention with blockwise transformers for near-infinite context"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "dao2022flashattention",
        "author": "Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher",
        "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "leviathan2023fast",
        "author": "Leviathan, Yaniv and Kalman, Matan and Matias, Yossi",
        "title": "Fast inference from transformers via speculative decoding"
      },
      {
        "key": "chen2023accelerating",
        "author": "Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John",
        "title": "Accelerating large language model decoding with speculative sampling"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "yu2022orca",
        "author": "Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon",
        "title": "Orca: A distributed serving system for $\\{$Transformer-Based$\\}$ generative models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "agrawal2024sarathi",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "Taming throughput-latency tradeoff in llm inference with sarathi-serve"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "patel2024splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\\'I}{\\~n}igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "Splitwise: Efficient generative llm inference using phase splitting"
      },
      {
        "key": "zhong2024distserve",
        "author": "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao",
        "title": "$\\{$DistServe$\\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      },
      {
        "key": "hu2024inference",
        "author": "Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others",
        "title": "Inference without interference: Disaggregate llm inference for mixed downstream workloads"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "zhou2024survey",
        "author": "Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others",
        "title": "A survey on efficient inference for large language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "sun2024llumnix",
        "author": "Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei",
        "title": "Llumnix: Dynamic Scheduling for Large Language Model Serving"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "lee2024infinigen",
        "author": "Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong",
        "title": "$\\{$InfiniGen$\\}$: Efficient generative inference of large language models with dynamic $\\{$KV$\\}$ cache management"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "wu2024dlora",
        "author": "Wu, Bingyang and Zhu, Ruidong and Zhang, Zili and Sun, Peng and Liu, Xuanzhe and Jin, Xin",
        "title": "$\\{$dLoRA$\\}$: Dynamically Orchestrating Requests and Adapters for $\\{$LoRA$\\}$$\\{$LLM$\\}$ Serving"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "sheng2024fairness",
        "author": "Sheng, Ying and Cao, Shiyi and Li, Dacheng and Zhu, Banghua and Li, Zhuohan and Zhuo, Danyang and Gonzalez, Joseph E and Stoica, Ion",
        "title": "Fairness in serving large language models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "wu2023fast",
        "author": "Wu, Bingyang and Zhong, Yinmin and Zhang, Zili and Liu, Shengyu and Liu, Fangyue and Sun, Yuanhang and Huang, Gang and Liu, Xuanzhe and Jin, Xin",
        "title": "Fast distributed inference serving for large language models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "zhong2024distserve",
        "author": "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao",
        "title": "$\\{$DistServe$\\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "qin2024mooncake",
        "author": "Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran",
        "title": "Mooncake: A kvcache-centric disaggregated architecture for llm serving"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "wang2020print3D",
        "author": "Wang, Feifan and Fathizadan, Sepehr and Ju, Feng and Rowe, Kyle and Hofmann, Nils",
        "title": "Print surface thermal modeling and layer time control for large-scale additive manufacturing"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "pang2018surgery",
        "author": "Pang, Bowen and Xie, Xiaolei and Song, Yongjia and Luo, Li",
        "title": "Surgery scheduling under case cancellation and surgery duration uncertainty"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "erdogan2015online",
        "author": "Erdogan, S Ayca and Gose, Alexander and Denton, Brian T",
        "title": "Online appointment sequencing and scheduling"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "pang2022dynamic",
        "author": "Pang, Bowen and Xie, Xiaolei and Ju, Feng and Pipe, James",
        "title": "A dynamic sequential decision-making model on MRI real-time scheduling with simulation-based optimization"
      },
      {
        "key": "lee2019online",
        "author": "Lee, Kangbok and Zheng, Feifeng and Pinedo, Michael L",
        "title": "Online scheduling of ordered flow shops"
      }
    ]
  }
]