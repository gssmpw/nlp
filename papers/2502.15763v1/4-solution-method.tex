\section{Solution Method}
\label{solution_method}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{graph//solution-method.pdf}
    \caption{Illustration of Solution Method}
    \label{fig:solution-method}
\end{figure*}

\subsection{Method overview}
The original model, provided by Eqs. (\ref{obj:makespan})-(\ref{cst:var_def_end}), demands a solution method capable of making decisions within 10 milliseconds in an asynchronous cycle, as the decoding time per client batch can be around 50 milliseconds. However, the original model is a large-scale MIP model with over 100,000 integer decision variables and constraints, making it difficult to solve even within several hours. Hence, it is vital to develop an efficient solution method that provides the best possible outcomes within the required time frame. As illustrated in Fig. \ref{fig:solution-method}, we propose a \textit{hybrid offline-online method} that structures the scheduling process for large model inference into two main methods: offline requests assignment and online scheduling. Each  method involves a subset of decision variables of the original model and provides timely solutions at each stage. In the figure, we illustrate how the offline-online information, as well as the decision making given by the scheduling models, is obtained and shared in the system.

\textbf{Offline Requests Scheduling}: In this method, a predetermined batch size of clients determines the number of parallel requests. Each client is allocated a balanced number of requests, resulting in an equitable task distribution. This method considers the assignment decisions in the original model as described in constraints (\ref{cst:makespan_def}), (\ref{cst:d_def_1}), and (\ref{cst:w_def_2})--(\ref{cst:p_def_1}). We isolate this part of the model to demonstrate offline training scenarios, such as RLHF (Reinforcement Learning with Human Feedback) training. In this task, requests are typically given and known in advance. Users can manually send their request prompts to the LLMs and wait to receive the outputs. These tasks can implement offline request assignment methods to achieve better throughput. However, for most scenarios such us user using GPT, using the offline method is still limited since solving the MIP model usually takes 10 minutes or more, which cannot meet the rapid iteration requirements of the LLM online decision-making process. Therefore, it is necessary to develop an online method to fill this gap.

\textbf{Online Scheduling}: The online scheduling process comprises two major parts corresponding to two types of online decisions. The first part, \textit{online requests scheduling}, determines which requests are scheduled in the upcoming bin and identifies the next client to serve once a previous request is completed. The second part, \textit{online iteration scheduling}, decides when to conclude the current decoding bin and start a preemptive prefilling bin to enhance overall utilization rates.

In the online requests scheduling part, heuristic methods are employed to determine the optimal order for processing requests in real-time. This approach considers factors such as task priority, current system load, and anticipated resource availability. By effectively prioritizing requests, the system can minimize waiting times and maximize throughput under dynamic operational conditions. This method emphasizes the implementation of the relaxed solutions provided by job assignment and illustrates the constraints (\ref{cst:bin_def_2})--(\ref{cst:prefill-decode-order}) in the original model.

The online iteration scheduling part aims to minimize idle time on machines by strategically allocating computational resources. By dynamically adjusting prefilling and decoding task priorities based on real-time feedback and system constraints, this method enhances overall system efficiency and responsiveness. This proactive scheduling approach minimizes machine idle time and optimizes the utilization of processing resources, thereby improving the overall performance of large language model inference tasks. This method underscores iteration-based optimization and considers the constraints (\ref{cst:w_def_1})--(\ref{cst:w_def_2}) and (\ref{cst:p_def_2})--(\ref{cst:loc_def}) in the original model.

% \textbf{Online Scheduling}: The online scheduling includes two major parts corresponding to two types of online decisions. Online requests scheduling decides the requests scheduled in the next bin and next clients when previous request is finished. Online iteration scheduling decides when to end current decoding bin to start a preemptive prefilling bin in order to improve total utilization rate. 

% The online requests scheduling phase employs heuristic methods to determine the optimal order of processing requests in real-time. This heuristic approach considers factors such as task priority, current system load, and anticipated resource availability. By prioritizing requests effectively, the system can minimize wait times and maximize throughput during dynamic operational conditions. This phase emphasizes the realization of the relaxed solutions given by job assignment and illustrate the constraints \ref{cst:bin_def_1}-\ref{cst:prefill-decode-order} in the original model.

% And the online iteration scheduling phase aims to reduce idle time on machines by strategically allocating computational resources. By dynamically adjusting task priorities based on real-time feedback and system constraints, the online operation scheduling phase enhances overall system efficiency and responsiveness. This proactive scheduling approach minimizes machine idle time and optimizes the utilization of processing resources, thereby improving the overall performance of large language model inference tasks. This phase emphasized the iteration-based optimization and illustrate the constraints \ref{cst:w_def_1}-\ref{cst:w_def_2} and \ref{cst:p_def_2}-\ref{cst:loc_def} in the original model.

The overarching goal of this structured approach is to minimize the total inference time for a specific benchmark, thereby maximizing throughput. By integrating thorough data analysis, efficient task allocation, and adaptive online scheduling strategies, this scheduling solution optimizes the performance of LLM inference processes. This holistic approach not only enhances system efficiency but also supports scalability and reliability in handling complex computational tasks.



\subsection{Offline requests scheduling \& theoretical lower bound}
As previously introduced, we begin by examining the offline request assignment decisions within the original model, with a specific focus on the constraints described in (\ref{cst:makespan_def}), (\ref{cst:d_def_1}), and (\ref{cst:w_def_2}--\ref{cst:p_def_1}). This part of the model is isolated to demonstrate offline training scenarios, such as RLHF training. In this scenario, we tackle the Minimizing Makespan Bin Packing Problem to efficiently address the workload balancing challenge. We assume that the output length is predetermined, and that prefill decode stages do not conflict during problem-solving. Nevertheless, in practical applications and simulations used to evaluate performance, we adhere to these constraints by allocating workload to clients without affecting the uncertainty of output length.

In the offline model outlined in Eqs. (\ref{offline_bin_packing_start})--(\ref{offline_bin_packing_end}), we introduce a new parameter, denoted by $T_i \in \mathbb{R}^+$ for $i\in \mathcal{I}$, representing the estimated decode completion time for request $i$. We also introduce a new decision variable, denoted by $t_j \in \mathbb{R}^+$ for $j\in \mathcal{J}$, to indicate the total decoding time for client $j$.

\begin{align}
    \label{offline_bin_packing_start}
    & \min \max_{j\in \mathcal{J}} t_j \\
    & \nonumber \text{s.t.} \\
    & \sum_{j \in \mathcal{J}} x_{i,j} = 1, \text{ for } i\in \mathcal{I}, \\
    & \sum_{i\in \mathcal{I}} x_{i,j} T_i \leq t_j, \text{ for } j\in \mathcal{J}, \\    
    & x_{i,j} \in \{0,1\}, \text{ for } i\in \mathcal{I},\text{ and }j\in \mathcal{J},\\
    \label{offline_bin_packing_end}
    & t^\text{max}, t_j \in \mathbb{R}^+, \text{ for } j \in  \mathcal{J}.
\end{align}

The offline model also provides a method to calculate the theoretical lower bound for a given set of requests, $\mathcal{I}$. In this method, we assume that prefill and decode phases for all the requests can be separated into two groups, and we calculate the optimal inference time for each group.

Let $t^{\text{p}*} \in \mathbb{R}^+$ and $t^{\text{d}*} \in \mathbb{R}^+$ represent the optimal total prefill and total decode times for all the requests in set $\mathcal{I}$, respectively. The value of $t^{\text{d}*}=\max_{j\in \mathcal{J}} t_j$ is obtained from the objective function value from Eqs. (\ref{offline_bin_packing_start})--(\ref{offline_bin_packing_end}). Let $L=\arg\max_{l \in \mathcal{L}} N_l^{\text{cap}}$, and then the largest prefill time across all levels in $T^\text{p}_l$ is denoted by $T^\text{p}_L$. $N^{\text{cap}}_L$ is the number of maximum number of tokens that can be processed in $T^\text{p}_L$. Then, $t^{\text{p}*}$ can be calculated by the following equation. 

\begin{align}
t^{\text{p}*} \geq T^\text{p}_L  \left\lfloor \frac{\sum_{i\in \mathcal{I}} N_i^{\text{p}}}{N^{\text{cap}}_L} \right\rfloor.
\end{align}
It yields a tight theoretical lower bound $T^{\text{LB}}$ as follows.

\begin{align}
\label{theoreticalLB}
T^{\text{LB}} = t^{\text{p}*} + t^{\text{d}*}.
\end{align}

\subsection{Online requests and iteration scheduling}
In this online part of the LLM inference optimization problem, two critical considerations arise. First, we need to determine which request to send to an available client once the previous request is completed. Second, when a round of decoding stage is finished, we must decide whether to send a preemptive prefill stage or continue this decode stage to the LLM workers for the subsequent time frame.

The first issue presents an online scheduling problem, as illustrated by Eqs. (\ref{cst:w_def_1})--(\ref{cst:w_def_2}) and (\ref{cst:p_def_1})--(\ref{cst:p_def_2}). The primary decision in this context is whether to select a new request to override the original assignment in order to achieve better machine utilization.

A sorting and online preemptive method is illustrated in Algorithm \ref{algorithm1}. This online algorithm first selects the future available requests, denoted as $I_j$, for client $j \in \mathcal{J}$. The set $I_j$ is sorted by $N_i^{\text{p}} + N_i^{\text{d}}$, that is, $N_{i_1}^{\text{p}} + N_{i_1}^{\text{d}}>N_{i_2}^{\text{p}} + N_{i_2}^{\text{d}} \forall i_1<i_2 \in I_j$. Then, for each client, the algorithm calculates future requests and counts the expected remaining tokens $remain\_token(j) \text{ for } j\in \mathcal {J}$ to be processed. Idle clients then greedily select the longest request from busy clients to process. This algorithm utilizes offline information on request assignment to provide timely online assignment decisions.

\begin{algorithm}
\caption{Sorting and Online Preemptive Method}
\label{algorithm1}
\begin{algorithmic}
\Require $\{ I_j = \{ i \mid x_{ij} = 1 \}, \forall j \in \mathcal{J} \}$ 
% \State \textbf{Sort:} $I_j = \{i_1, i_2, \ldots, i_n\}$ by $N_i^{\text{p}} + N_i^{\text{d}}$, $\forall j \in \mathcal{J}$
\For{client $j$ in clients $\mathcal{J}$}
    \If{queue for client $j$ is empty and $I_j \neq \emptyset$}
        \State pop $I_j$ to client $j$
        \State $remain\_token(j) \gets remain\_token(j) - (N_i^{\text{p}} + N_i^{\text{d}})$
    \ElsIf{$\max(remain\_token(j)) > 0$}
        \State pop $\arg\max(remain\_token)$ to client $j$
        \State $remain\_token(j) \gets remain\_token(j) - (N_i^{\text{p}} + N_i^{\text{d}})$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Continuing from the previous discussion, the second problem involves a sequential decision-making process, as outlined by Eqs. (\ref{cst:makespan_def})--(\ref{cst:prefill-decode-order}). The main challenge here is to deliver timely and efficient decisions in real time. As previously mentioned, each round of decoding takes approximately 50 milliseconds. Thus, it is essential to ensure that decisions are made within 10 milliseconds to sustain system efficiency. To achieve this, we employ the following method to integrate quick decision-making into the process.

This aspect of decision-making corresponds to the following problem.

\begin{align}
    &\nonumber \min t^\text{max} \\
    & \nonumber \text{s.t.} \\
    \label{tmax}
    & t^\text{max} \geq t_{k}^\text{s,d} + n_{k}^\text{d}, \text{ for } k \in \mathcal{K}^\text{d}, \\
    & t^{\text{s}}_{p,k} - (t^{\text{s}}_{d,k-1} + n^{\text{d}}_{k-1}) \geq 0,  \text{ for } k = 2,...,K,\\
    \label{tsdk}
    & t^{\text{s,d}}_{k} - (t^{\text{s,p}}_{k} + n^{\text{p}}_{k}) \geq 0,  \text{ for } k \in \mathcal{K}^\text{p},\\ 
    \label{npk}
    & n^{\text{p}}_{k} \geq \sum_{l\in \mathcal{L}} T^{\text{p}}_l y_{k,l}, \text{ for } k \in \mathcal{K}^\text{p},\\
    & \sum_{i\in \mathcal{I},j\in \mathcal{J}} N_i^{\text{p}} p_{i,j,k} \leq \sum_{l\in \mathcal{L}} N^{\text{cap}}_l y_{k,l}, \text{ for } k \in \mathcal{K}^\text{p}, \\ 
    & \sum_{l\in \mathcal{L}} y_{k,l} = 1, \text{ for } k \in \mathcal{K}^\text{p}, \\
    \label{ndk}
    & n^{\text{d}}_{k} \geq T^{\text{d}} \sum_{i} N_i^{\text{d}} w_{i,j,k}, \text{ for } j\in \mathcal{J}, \text{ and } k \in \mathcal{K}^\text{d},\\
    & d_{i,j,k} - p_{i,j,k} \geq 0, \text{ for } i\in \mathcal{I}, j\in \mathcal{J}, \text{ and } k \in \mathcal{K}.
\end{align}
By combining Eqs. (\ref{tmax}) and (\ref{tsdk}), we derive that $t^{\text{max}} \geq \max_{k \in \mathcal{K}} \left(t^\text{{s,p}}_{k} + n^\text{p}_{k} + n^\text{d}_{k}\right) $. In the context of online decision-making, the start time $t^\text{{s,p}}_{k}$ is typically influenced by the completion time of preceding tasks. The primary objective is to minimize the total time cost of prefill and decode stages. Consequently, we establish the following equation by integrating the calculations of $n^\text{p}_{k}$ and $n^\text{d}_{k}$ from Eqs. (\ref{npk}) and (\ref{ndk}).

\begin{align}
    \min t^\text{max} \geq \max_{k \in \mathcal{K}} \left( t^\text{s,p}_{k} + \sum_{l\in \mathcal{L}} T^\text{p}_l y_{k,l} + T^\text{d} \sum_{i \in \mathcal{I}, j\in \mathcal{J}} N_i^\text{d} w_{i,j,k} \right).
\end{align}
In this problem, the time cost is divided into two components. The cost for adding a prefill task at any point is given by

\begin{align}
    \frac{\partial t^\text{max}}{\partial y_{k,l}} = \sum_{l \in \mathcal{L}} T^\text{p}_l,
\end{align}
and the cost for adding a decode task at the decision-making moment is expressed by

\begin{align}
    \frac{\partial t^\text{max}}{\partial w_{i,j,k}} = T^\text{d} \sum_{i\in \mathcal{I}} N_i^\text{d}.
\end{align}
Thus, our heuristic method for deciding whether to dispatch a prefill or decode stage to the LLM worker involves comparing the prefill cost $C_p = \sum_l T^p_l$ with the waited decode time $C_d = T^d \sum_{i\in \mathcal {I}, j\in \mathcal{J}} N_i^d w_{i,j,k}$. If $C_p \geq C_d$, the algorithm advises continuing with a round of the decode task and waiting for additional prefill tasks; otherwise, the algorithm recommends executing a round of the prefill task.

% \subsection{An illustrative example}
% \label{a_case_study}

% Here is an example of a table.

% \begin{table}
% \caption{Parameter setting for illustrative example}
% \begin{centering}
% \begin{tabular}{ccccccccc}
% \hline 
% $i$         & 1    &   2  &   3  &   4  &   5  &   6  &   7  &   8 \tabularnewline
% \hline 
% $e_i$       & 0.62  & 0.75 & 0.85 & 0.73 & 0.68 & 0.91 & 0.81 & 0.72  \tabularnewline
% %p_i       & 0.082  & 0.044 & 0.051 & 0.089 & 0.081 & 0.020 & 0.119 & 0.121  \tabularnewline
% $r_i$       & 0.35 & 0.44 & 0.37 & 0.24 & 0.27 & 0.35 & 0.29 & 0.46  \tabularnewline
% $N_i$       &  5   &   6  &   7  &   7  &   7  &   5  &   6  &  6    \tabularnewline
% $T_{i,max}$ &  8   &   8  &   10  &   10 &   8  &   7  &   9  &  9 \tabularnewline
% $T_{i,min}$ &  1   &   1  &   2  &   2  &   1  &   1  &   2  &  1 \tabularnewline
% \hline 
% \end{tabular}
% \par\end{centering}
% \label{setting_illustrative_example}
% \end{table}




% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.99\linewidth]{graph//result_gantt1.png}\\
%   \caption{Baseline: Time Elapsed 201 Throughput 6.58 Utilization: 0.802}
%   \label{layout}
% \end{figure}

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.99\linewidth]{graph//result_gantt2.png}\\
%   \caption{Baseline: Time Elapsed 201 Throughput 6.58 Utilization: 0.802}
%   \label{layout}
% \end{figure}

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.99\linewidth]{graph//result_gantt3.png}\\
%   \caption{Simple assignment: Time Elapsed 197.08 Throughput 6.69 Utilization: 0.8549}
%   \label{layout}
% \end{figure}

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.99\linewidth]{graph//result_gantt4.png}\\
%   \caption{Adjustment: Time Elapsed 191.66 Throughput 6.88 Utilization: 0.8821}
%   \label{layout}
% \end{figure}

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.99\linewidth]{graph//result_gantt5.png}\\
%   \caption{Online: Time Elapsed 190.58 Throughput 6.92 Utilization: 0.8906}
%   \label{layout}
% \end{figure}

% \begin{figure}[t]
% \centering
%   \includegraphics[width=0.99\linewidth]{graph//result_gantt6.png}\\
%   \caption{RL: Time Elapsed 190.08 Throughput 6.94 Utilization: 0.893}
%   \label{layout}
% \end{figure}