Please help add the following items: label is given with #xxx

The followings are most related Memory management, batching and scheduling.
#vllm
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., ... & Stoica, I. (2023, October). Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles (pp. 611-626).
@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
#ORCA
G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, “Orca:A distributed serving system for transformer-based generative 34 models,” in Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation, 2022, pp. 521–538.
@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

#Lightllm
ModelTC, “Lightllm,” February 2024. [Online]. Available:
https://github.com/ModelTC/lightllm/
@misc{lightllm2024,
  author = {{ModelTC}},
  title = {{Lightllm}},
  year = {2024},
  month = feb,
  howpublished = {\url{https://github.com/ModelTC/lightllm/}},
  note = {[Online; accessed Day-Month-Year]}
}

#Deepspeed-fastgen
C. Holmes, M. Tanaka, M. Wyatt, A. A. Awan, J. Rasley, S. Rajbhandari, R. Y. Aminabadi, H. Qin, A. Bakhtiari, L. Kurilenko, and Y. He, “Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference,” arXiv preprint arXiv:2401.08671, 2024.
@article{holmes2024deepspeed,
  title={Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference},
  author={Holmes, Connor and Tanaka, Masahiro and Wyatt, Michael and Awan, Ammar Ahmad and Rasley, Jeff and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Qin, Heyang and Bakhtiari, Arash and Kurilenko, Lev and others},
  journal={arXiv preprint arXiv:2401.08671},
  year={2024}
}

#FastServe
B. Wu, Y. Zhong, Z. Zhang, G. Huang, X. Liu, and X. Jin, “Fast distributed inference serving for large language models,” arXiv preprint arXiv:2305.05920, 2023.
@article{wu2023fast,
  title={Fast distributed inference serving for large language models},
  author={Wu, Bingyang and Zhong, Yinmin and Zhang, Zili and Liu, Shengyu and Liu, Fangyue and Sun, Yuanhang and Huang, Gang and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2305.05920},
  year={2023}
}

%================================================================================
## Literature Review part, this part includes related technique paper and we compare them in table.

%----------
# General model serving methods
#Quantization
@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

#Sparsification
@inproceedings{bai2024sparsellm,
  title={SparseLLM: Towards global pruning of pre-trained language models},
  author={Bai, Guangji and Li, Yijiang and Ling, Chen and Kim, Kibaek and Zhao, Liang},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

#Distillation
@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

#MQA
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

#GQA
@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

#DP
@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

#TP
@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

#PP
@article{huang2019gpipe,
  title={GPipe: Easy scaling with micro-batch pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={proceeding of Computer Science> Computer Vision and Pattern Recognition},
  year={2019}
}

#SP
@article{korthikanti2023reducing,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={341--353},
  year={2023}
}

#CP
@article{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

%----------
#FalshAttention
@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

#Speculative decoding/sampling
@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@article{chen2023accelerating,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}

%----------
## The following recent papers are compared in table

#Llumnix: Dynamic Scheduling for Large Language Model Serving. Alibaba Group
@article{sun2024llumnix,
  title={Llumnix: Dynamic Scheduling for Large Language Model Serving},
  author={Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei},
  journal={arXiv preprint arXiv:2406.03243},
  year={2024}
}

#InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management. Seoul National University
@inproceedings{lee2024infinigen,
  title={$\{$InfiniGen$\}$: Efficient generative inference of large language models with dynamic $\{$KV$\}$ cache management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={155--172},
  year={2024}
}

#DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving. Peking University 
@inproceedings{zhong2024distserve,
  title={$\{$DistServe$\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={193--210},
  year={2024}
}

#dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving. Peking University
@inproceedings{wu2024dlora,
  title={$\{$dLoRA$\}$: Dynamically Orchestrating Requests and Adapters for $\{$LoRA$\}$$\{$LLM$\}$ Serving},
  author={Wu, Bingyang and Zhu, Ruidong and Zhang, Zili and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={911--927},
  year={2024}
}
#Fairness in Serving Large Language Models. UC Berkeley
@inproceedings{sheng2024fairness,
  title={Fairness in serving large language models},
  author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Zhu, Banghua and Li, Zhuohan and Zhuo, Danyang and Gonzalez, Joseph E and Stoica, Ion},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={965--988},
  year={2024}
}

#Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve. Microsoft Research India
@article{agrawal2024sarathi,
  title={Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2403.02310},
  year={2024}
}


@misc{nvidia2024finance,
  author = {{NVIDIA}},
  title = {{NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2024}},
  year = {2024},
  month = feb,
  howpublished = {{https://investor.nvidia.com/news/press-release-details/2024/NVIDIA-Announces-Financial-Results-for-Fourth-Quarter-and-Fiscal-2024/}},
  note = {[Online; accessed 09-01-2025]}
}

#### The followings are about distributed systems.

#PD separation
@inproceedings{patel2024splitwise,
  title={Splitwise: Efficient generative llm inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={118--132},
  year={2024},
  organization={IEEE}
}

@article{hu2024inference,
  title={Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
  journal={arXiv preprint arXiv:2401.11181},
  year={2024}
}

#Mooncake
@article{qin2024mooncake,
  title={Mooncake: A kvcache-centric disaggregated architecture for llm serving},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={arXiv preprint arXiv:2407.00079},
  year={2024}
}

#Survey
@article{zhou2024survey,
  title={A survey on efficient inference for large language models},
  author={Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others},
  journal={arXiv preprint arXiv:2404.14294},
  year={2024}
}


%----------
#### The followings are traditional OR scheduling methods

@article{wang2020print3D,
  title={Print surface thermal modeling and layer time control for large-scale additive manufacturing},
  author={Wang, Feifan and Fathizadan, Sepehr and Ju, Feng and Rowe, Kyle and Hofmann, Nils},
  journal={IEEE Transactions on automation science and engineering},
  volume={18},
  number={1},
  pages={244--254},
  year={2020},
  publisher={IEEE}
}

@article{pang2018surgery,
  title={Surgery scheduling under case cancellation and surgery duration uncertainty},
  author={Pang, Bowen and Xie, Xiaolei and Song, Yongjia and Luo, Li},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={16},
  number={1},
  pages={74--86},
  year={2018},
  publisher={IEEE}
}

@article{pang2022dynamic,
  title={A dynamic sequential decision-making model on MRI real-time scheduling with simulation-based optimization},
  author={Pang, Bowen and Xie, Xiaolei and Ju, Feng and Pipe, James},
  journal={Health Care Management Science},
  volume={25},
  number={3},
  pages={426--440},
  year={2022},
  publisher={Springer}
}

@article{yao2024novel,
  title={A novel mathematical model for the flexible job-shop scheduling problem with limited automated guided vehicles},
  author={Yao, Youjie and Liu, Qihao and Fu, Ling and Li, Xinyu and Yu, Yanbin and Gao, Liang and Zhou, Wei},
  journal={IEEE Transactions on Automation Science and Engineering},
  year={2024},
  publisher={IEEE}
}

@article{li2024jointly,
  title={Jointly Appointment Scheduling in a Two-Phase Service System With Two Types of Patients Considering Multiple Servers and Stochastic Service Time},
  author={Li, Na and Chen, Huangyu and Pei, Zhi and Wang, Tao},
  journal={IEEE Transactions on Automation Science and Engineering},
  year={2024},
  publisher={IEEE}
}

@article{lee2019online,
  title={Online scheduling of ordered flow shops},
  author={Lee, Kangbok and Zheng, Feifeng and Pinedo, Michael L},
  journal={European Journal of Operational Research},
  volume={272},
  number={1},
  pages={50--60},
  year={2019},
  publisher={Elsevier}
}

@incollection{albers2009online,
  title={Online scheduling},
  author={Albers, Susanne},
  booktitle={Introduction to scheduling},
  pages={71--98},
  year={2009},
  publisher={CRC Press}
}

@article{erdogan2015online,
  title={Online appointment sequencing and scheduling},
  author={Erdogan, S Ayca and Gose, Alexander and Denton, Brian T},
  journal={IIE Transactions},
  volume={47},
  number={11},
  pages={1267--1286},
  year={2015},
  publisher={Taylor \& Francis}
}

#GSM8K dataset
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}