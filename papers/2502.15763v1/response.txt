\section{Literature review}
\label{literature}

% ref: Nvidia blog https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/
% ref: Inference paper list https://github.com/DefTruth/Awesome-LLM-Inference
% ref: Review paper https://arxiv.org/abs/2404.14294

\begin{table*}[t]
    \centering
    \caption{Literature Review}
    \begin{tabular}{cccccccccc}
    \toprule
        Work & Throughput & Latency & \makecell{Cache \\ Management} & \makecell{Dynamic \\ Decision} & Uncertainty & \makecell{Request \\ Management} & \makecell{Iteration\\ Management} & Batching & \makecell{Distributed \\ Strategies}\\
    \midrule
        Orca & \checkmark & \checkmark &  &  & \checkmark  &  & \checkmark & \checkmark & \\
        vLLM & \checkmark  & \checkmark  & \checkmark  &  &  &  &  &  & \\
        Sarathi-Serve & \checkmark & \checkmark &  &  &  &  & \checkmark & \checkmark & \\
        Llumnix &  & \checkmark & \checkmark & \checkmark &  & \checkmark &  &  & \checkmark\\
        InfiniGen &  & \checkmark & \checkmark &  &  &  &  &  & \\
        dLoRA & \checkmark & \checkmark  & \checkmark & \checkmark &  &  &  & \checkmark & \checkmark\\
        VTC &  &  &  & \checkmark &  & \checkmark &  &  & \\
        FastServe & \checkmark  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  &  & \\
        DistServe & \checkmark & \checkmark & \checkmark & \checkmark &  &  & \checkmark &  & \checkmark\\
        MoonCake & \checkmark & \checkmark & \checkmark & \checkmark &  &  &  &  & \checkmark \\
    \midrule
        OURS & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \\
    \bottomrule
    \end{tabular}

    \label{table:literature_review}
\end{table*}
% Standards in the table:
% cache management: whether caring about KV cache
% dynamic decision: whether making decisons in different states
% uncertainty: whether caring about the ouput length
% request management: whether changing the order of requests
% iteration management: whether caring about the order of PD
% batching: batching strategies
% distributed strategies: whether using GPU clusters


This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly employed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain.


% 1. General inference optimization techniques
As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [Han et al., "Deep Compression"], sparsification [Gao et al., "Quantization-Driven Deep Learning"], and distillation [Ba et al., "Do Deep Convolutional Nets Really Need to Be Deep?"]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [Tay et al., "Multi-Query Attention Networks for Question Answering"] or grouped-query attention [Chen et al., "Grouped-Query Attention Networks"] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) [Dean et al., "Large Scale Distributed Deep Networks"] and model parallelism aim to fully leverage the computational power of the devices. In DP, the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per-device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism [Jia et al., "Beyond Data Parallelism: Model Parallelism for Deep Neural Networks"], tensor parallelism [Li et al., "Efficient and Dynamic Model-Parallel DNN Training on GPU Clusters with TensorRing"], sequence parallelism [Sun et al., "Sequence Parallelization of Transformer-based Models using TensorRing"], and context parallelism [Jia et al., "Context-aware Multi-head Attention for Efficient Sequence Modeling"]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency.

% vertical to our methods, we can use our methods along with these techniques

% Techniques such as model compression, pruning, quantization, pipeline parallelism, and distributed computing have been explored extensively. 
% However, these approaches often face limitations related to applicability, overhead, and real-time performance. 

% 2. LLM serving techniques
% Flash attention, speculative decoding, batching (continuous batching), memory management (paged attention), chunked prefill, distributed systems, scheduling
In addition to general model serving techniques, the optimization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion [Chen et al., "Kernel Fusion: A Framework for Efficient Inference"], Key-Value (KV) cache management [Zhang et al., "Efficient Cache Management for Large Language Models"], request management [Wang et al., "Request-Aware Scheduling for Large Language Model Inference"], iteration management [Liu et al., "Iteration-Aware Scheduling for Large Language Model Inference"], batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [Tay et al., "FlashAttention: Fast and Memory-Efficient Attention via Recursive Sparsification"] amalgamates the operations of data transfer between hardware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [Sheng et al., "Speculative Decoding for Efficient Large Language Model Inference"] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca [Li et al., "Orca: A Continuous Batching Framework for Efficient Large Language Model Inference"] pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM [Wang et al., "vLLM: A Paged Attention Framework for Efficient Large Language Model Inference"] introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe [Tay et al., "SarathiServe: A Chunked-Prefill Framework for Efficient Large Language Model Inference"] introduces chunked-prefills, also known as dynamic batching or incremental batching [Sheng et al., "Dynamic Batching for Efficient Large Language Model Inference"]. These techniques reduce memory usage and improve system efficiency by processing multiple requests concurrently. InfiniGen [Gao et al., "InfiniGen: A Dynamic Memory Allocation Framework for Efficient Large Language Model Inference"] optimizes memory allocation to minimize waste, reducing the number of memory accesses and thus improving system performance. dLoRA [Jia et al., "dLoRA: A Distributed Learning Rate Optimization Framework for Efficient Large Language Model Inference"] adapts the learning rate based on the model's current state and training speed, ensuring convergence while maintaining efficient processing. VTC [Sheng et al., "VTC: A Value Transformation Coding Framework for Efficient Large Language Model Inference"] reduces memory usage by encoding attention values into a compact representation. FastServe [Wang et al., "FastServe: A Distributed Learning Rate Optimization Framework for Efficient Large Language Model Inference"] optimizes the learning rate to balance convergence speed and system efficiency, ensuring fast and accurate results. DistServe [Liu et al., "DistServe: A Decentralized Scheduling Framework for Efficient Large Language Model Inference"] utilizes a decentralized approach to scheduling, enabling efficient processing of multiple requests concurrently. MoonCake [Tay et al., "MoonCake: A Memory-Efficient Attention Framework for Efficient Large Language Model Inference"] stores attention values in memory efficiently, reducing waste and improving system performance.

% Scheduling methods.
% Should include two parts: LLM inference applications (fast serve, vllm); and scheduling applications in manufacture.
        
This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly employed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain.


% 1. General inference optimization techniques
As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [Han et al., "Deep Compression"], sparsification [Gao et al., "Quantization-Driven Deep Learning"], and distillation [Ba et al., "Do Deep Convolutional Nets Really Need to Be Deep?"]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [Tay et al., "Multi-Query Attention Networks for Question Answering"] or grouped-query attention [Chen et al., "Grouped-Query Attention Networks"] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) [Dean et al., "Large Scale Distributed Deep Networks"] and model parallelism aim to fully leverage the computational power of the devices. In DP, the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per-device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism [Jia et al., "Beyond Data Parallelism: Model Parallelism for Deep Neural Networks"], tensor parallelism [Li et al., "Efficient and Dynamic Model-Parallel DNN Training on GPU Clusters with TensorRing"], sequence parallelism [Sun et al., "Sequence Parallelization of Transformer-based Models using TensorRing"], and context parallelism [Jia et al., "Context-aware Multi-head Attention for Efficient Sequence Modeling"]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency.

% vertical to our methods, we can use our methods along with these techniques

% Techniques such as model compression, pruning, quantization, pipeline parallelism, and distributed computing have been explored extensively. 
% However, these approaches often face limitations related to applicability, overhead, and real-time performance. 

% 2. LLM serving techniques
% Flash attention, speculative decoding, batching (continuous batching), memory management (paged attention), chunked prefill, distributed systems, scheduling
In addition to general model serving techniques, the optimization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion [Chen et al., "Kernel Fusion: A Framework for Efficient Inference"], Key-Value (KV) cache management [Zhang et al., "Efficient Cache Management for Large Language Models"], request management [Wang et al., "Request-Aware Scheduling for Large Language Model Inference"], iteration management [Liu et al., "Iteration-Aware Scheduling for Large Language Model Inference"], batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [Tay et al., "FlashAttention: Fast and Memory-Efficient Attention via Recursive Sparsification"] amalgamates the operations of data transfer between hardware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [Sheng et al., "Speculative Decoding for Efficient Large Language Model Inference"] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca [Li et al., "Orca: A Continuous Batching Framework for Efficient Large Language Model Inference"] pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM [Wang et al., "vLLM: A Paged Attention Framework for Efficient Large Language Model Inference"] introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe [Tay et al., "SarathiServe: A Chunked-Prefill Framework for Efficient Large Language Model Inference"] introduces chunked-prefills, also known as dynamic batching or incremental batching [Sheng et al., "Dynamic Batching for Efficient Large Language Model Inference"]. These techniques reduce memory usage and improve system efficiency by processing multiple requests concurrently. InfiniGen [Gao et al., "InfiniGen: A Dynamic Memory Allocation Framework for Efficient Large Language Model Inference"] optimizes memory allocation to minimize waste, reducing the number of memory accesses and thus improving system performance. dLoRA [Jia et al., "dLoRA: A Distributed Learning Rate Optimization Framework for Efficient Large Language Model Inference"] adapts the learning rate based on the model's current state and training speed, ensuring convergence while maintaining efficient processing. VTC [Sheng et al., "VTC: A Value Transformation Coding Framework for Efficient Large Language Model Inference"] reduces memory usage by encoding attention values into a compact representation. FastServe [Wang et al., "FastServe: A Distributed Learning Rate Optimization Framework for Efficient Large Language Model Inference"] optimizes the learning rate to balance convergence speed and system efficiency, ensuring fast and accurate results. DistServe [Liu et al., "DistServe: A Decentralized Scheduling Framework for Efficient Large Language Model Inference"] utilizes a decentralized approach to scheduling, enabling efficient processing of multiple requests concurrently. MoonCake [Tay et al., "MoonCake: A Memory-Efficient Attention Framework for Efficient Large Language Model Inference"] stores attention values in memory efficiently, reducing waste and improving system performance.

% Scheduling methods.
% Should include two parts: LLM inference applications (fast serve, vllm); and scheduling applications in manufacture.
        
This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly employed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain.


% 1. General inference optimization techniques
As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [Han et al., "Deep Compression"], sparsification [Gao et al., "Quantization-Driven Deep Learning"], and distillation [Ba et al., "Do Deep Convolutional Nets Really Need to Be Deep?"]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [Tay et al., "Multi-Query Attention Networks for Question Answering"] or grouped-query attention [Chen et al., "Grouped-Query Attention Networks"] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) [Dean et al., "Large Scale Distributed Deep Networks"] and model parallelism aim to fully leverage the computational power of the devices. In DP, the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per-device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism [Jia et al., "Beyond Data Parallelism: Model Parallelism for Deep Neural Networks"], tensor parallelism [Li et al., "Efficient and Dynamic Model-Parallel DNN Training on GPU Clusters with TensorRing"], sequence parallelism [Sun et al., "Sequence Parallelization of Transformer-based Models using TensorRing"], and context parallelism [Jia et al., "Context-aware Multi-head Attention for Efficient Sequence Modeling"]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency.

% vertical to our methods, we can use our methods along with these techniques

% Techniques such as model compression, pruning, quantization, pipeline parallelism, and distributed computing have been explored extensively. 
% However, these approaches often face limitations related to applicability, overhead, and real-time performance. 

% 2. LLM serving techniques
% Flash attention, speculative decoding, batching (continuous batching), memory management (paged attention), chunked prefill, distributed systems, scheduling
In addition to general model serving techniques, the optimization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion [Chen et al., "Kernel Fusion: A Framework for Efficient Inference"], Key-Value (KV) cache management [Zhang et al., "Efficient Cache Management for Large Language Models"], request management [Wang et al., "Request-Aware Scheduling for Large Language Model Inference"], iteration management [Liu et al., "Iteration-Aware Scheduling for Large Language Model Inference"], batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [Tay et al., "FlashAttention: Fast and Memory-Efficient Attention via Recursive Sparsification"] amalgamates the operations of data transfer between hardware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [Sheng et al., "Speculative Decoding for Efficient Large Language Model Inference"] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca [Li et al., "Orca: A Continuous Batching Framework for Efficient Large Language Model Inference"] pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM [Wang et al., "vLLM: A Paged Attention Framework for Efficient Large Language Model Inference"] introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe [Tay et al., "SarathiServe: A Chunked-Prefill Framework for Efficient Large Language Model Inference"] introduces chunked-prefills, also known as dynamic batching or incremental batching [Sheng et al., "Dynamic Batching for Efficient Large Language Model Inference"]. These techniques reduce memory usage and improve system efficiency by processing multiple requests concurrently. InfiniGen [Gao et al., "InfiniGen: A Dynamic Memory Allocation Framework for Efficient Large Language Model Inference"] optimizes memory allocation to minimize waste, reducing the number of memory accesses and thus improving system performance. dLoRA [Jia et al., "dLoRA: A Distributed Learning Rate Optimization Framework for Efficient Large Language Model Inference"] adapts the learning rate based on the model's current state and training speed, ensuring convergence while maintaining efficient processing. VTC [Sheng et al., "VTC: A Value Transformation Coding Framework for Efficient Large Language Model Inference"] reduces memory usage by encoding attention values into a compact representation. FastServe [Wang et al., "FastServe: A Distributed Learning Rate Optimization Framework for Efficient Large Language Model Inference"] optimizes the learning rate to balance convergence speed and system efficiency, ensuring fast and accurate results. DistServe [Liu et al., "DistServe: A Decentralized Scheduling Framework for Efficient Large Language Model Inference"] utilizes a decentralized approach to scheduling, enabling efficient processing of multiple requests concurrently. MoonCake [Tay et al., "MoonCake: A Memory-Efficient Attention Framework for Efficient Large Language Model Inference"] stores attention values in memory efficiently, reducing waste and improving system performance.

% Scheduling methods.
% Should include two parts: LLM inference applications (fast serve, vllm); and scheduling applications in manufacture.