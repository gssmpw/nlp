\section{Introduction}
\label{introduction}
%background introduction
Recent advancements in large language models (LLMs), including GPT-4, LLaMA, and Qwen, have significantly transformed the landscape of natural language processing by enabling more sophisticated text generation, comprehension, and interaction capabilities. These models serve as foundational technologies in a wide range of applications, such as chatbots, machine translation, and content creation. Despite their transformative potential, the inference process for LLMs is computationally intensive and resource-demanding, resulting in high costs and increased latency. These factors pose significant challenges when scaling their deployment across various applications. Effective inference scheduling is crucial for optimizing resource usage, reducing operational costs, and ensuring high-quality service delivery.
% \kai{Add simple explanation for the inference system, as well as prefill and decode.}

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{graph//intro_inference_service.pdf}
    \caption{Illustration of LLM inference service system}
    \label{fig:intro_inference_service}
\end{figure}

% problem definition
As illustrated in Fig. \ref{fig:intro_inference_service}, the current inference service system comprises three components: system resource, serving configuration, and inference service. System resource determines the extent of computational resources available in the system including GPU computing ability, memory, node size, network bandwidth, etc. The serving configuration specifies the desired model deployment method and service level expectations, such as data/tensor/pipeline parallel settings, quantization, batch size, and scheduling configurations. The inference service processes input requests from users, utilizes hardware capabilities, fulfills user configurations, and returns responses to LLM users. The scheduling strategy module is the core of the inference service, managing prefill and decode queues, offline and online request scheduling, and iteration-level hardware management. The offline scheduling method is optional, only for inference tasks where all requests are known in advance. Conversely, the two online scheduling methods, i.e., requests scheduling and iteration scheduling, are more versatile and applicable to a wide range of scenarios. The online methods acquire information from the online profiler and dispatch requests and inference tasks to LLM workers. In practice hardware utilization includes up to 20\% ``bubbles'', which means the hardware is idle during inference service, and detail is later shown in Section \ref{solution_method}. By designing an efficient LLM inference scheduling system to reduce these bubbles, computational resource consumption can be decreased, leading to reduced latency and increased throughput. For maintaining logic flow, detailed technical introductions to prefill and decode operations are provided in Section \ref{problem_formulation}.

% important literature
Current works, such as vLLM \cite{kwon2023efficient} and ORCA \cite{yu2022orca}, provide a robust foundation for LLM inference serving systems. The primary contributions of vLLM and ORCA to LLM inference are their enhancements in resource allocation and execution efficiency, which significantly increase throughput and decrease latency in large-scale language model deployments. These improvements are achieved through advanced memory management and continuous batching techniques, which enhance model parallelism and effectively leverage hardware resources \cite{kwon2023efficient, yu2022orca}.
% Motivation and objectives 
However, the state-of-the-art scheduler in vLLM predominantly employs a First-Come-First-Serve (FCFS) and prefill-first policy, prioritizing prefill tasks but not fully leveraging the scheduling system's potential. We identify the causes of low hardware utilization rates. From an offline scheduling perspective, client loads are unbalanced, and request sequencing can be improved. From an online scheduling angle, the prefill-first policy lacks parallel processing of multiple requests at prefill stage, and there are no dynamic sequencing adjustments or preemption methods.

% challenges and benefits
The optimization of LLM inference presents two major challenges. The first challenge pertains to the offline method, which involves managing a large number of requests from up to 200 parallel clients. This task is particularly time-consuming when using a mixed-integer programming (MIP) scheduling model. The second challenge is the need for much faster decision-making for online methods compared to traditional online scheduling problems. For example, in LLM inference scenarios, online decisions about whether to send prefill or decode requests to LLM workers typically occur every 50 milliseconds. In contrast, in healthcare or production systems, online decisions usually occur every thirty seconds or even several minutes. For online scheduling methods, the higher frequency of decision-making requires algorithms that are efficient and capable of delivering results within extremely short time frames.

% The challenges in LLM inference optimization involves two major aspects. The first challenge concerns the offline component, which involves managing a large number of requests across up to 200 parallel clients--an issue that is time-consuming using a mixed-integer programming (MIP) scheduling model. The second challenge requires much faster decision-making for online components compared to traditional online scheduling problems. For instance, in LLM inference scenarios, online decisions about whether to send prefill or decode requests to LLM workers typically occur once every 50 milliseconds, while in healthcare or production systems, online decisions usually occur every thirty seconds or even every a few minutes. For online scheduling methods, the higher frequency between two decision-making points means shorter solving time, which requires difficult low time-consuming algorithms.

Accompanied by these challenges, developing a scheduling method for LLM inference could yield substantial benefits. According to NVIDIA Financial Results in 2024 \cite{nvidia2024finance}, the revenue from GPU retail and cloud computation service has reached \$60.9 billion per year. Improving computational resource efficiency by 5\% will earn up to \$3.05 billion in revenue annually. Therefore, research in this area is crucial, especially for scheduling researchers.

%paper overview
In this paper, we aim at enhance service system throughput and optimize hardware utilization. We define the LLM inference optimization problem using a MIP model as its uncertainty equivalence. Then, we propose a hybrid offline-online method as solution. This work is the first to define this problem from a scheduling perspective. To address the high real-time demands in LLM inference, we propose a hybrid offline-online scheduling method. In numerical experiments, we demonstrate that our method can improve system throughput by 5.46\% and improve hardware utilization by 11.0\%. A 100-cases study shows that our method outperforms baseline method consistently with an improvement of 8.0\% on utilization rate.

%contribution
The major contributions of this work include the followings. To the best of our knowledge, we are the first to formulate a mathematical model that describe the scheduling problem in the area of LLM inference. We design a two-stage approach to manage both offline and online scheduling challenges. An Minimizing Makespan Bin Packing model is developed to efficiently solve the offline scheduling problem. Additionally, we introduce a sorting and preemption method to handle the online request scheduling problem, and we develop a Lagrangian-based heuristic technique for solving the online iteration scheduling issue. In the online scheduling module, our method can provide decisions within 5 milliseconds, meeting the real-time decision-making requirements of practical application in LLM inference.



%structure overview
The remainder of the paper is structured as follows. We review literature on efficient LLM inference in Section \ref{literature}. The problem definition and model formulation are presented in Section \ref{problem_formulation}. Then, we illustrate the difficulty of solving this problem and introduce our hybrid offline-online scheduling method to provide a timely solution in Section \ref{solution_method}. Numerical studies using real cases and 100 generated cases are presented in Section \ref{numerical-experiment}. Finally, the conclusion and future directions are provided in Section \ref{conclusion}.