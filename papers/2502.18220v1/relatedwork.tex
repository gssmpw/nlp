\section{RELATED WORK}
\subsection{Multi-modal Tracking}
In recent years, substantial research \cite{ostrack, STARK, ATOM} has been dedicated to visual object tracking, which has gained wide-ranging applications across various fields, such as autonomous driving, mobile robotics, video surveillance, and human-robot interaction. 
However, the performance and stability of visual object tracking remain constrained when confronted with challenges in complex scenarios.
Subsequently, multi-modal tracking \cite{MFGNet,cbpnet} incorporating additional auxiliary modalities \cite{apfnet,Lu_Li_Yan_Tang_Luo_2021,FANet,TFNet,MaCNet,cmpp, zhang2021object, zhu2024pr}, such as thermal, event, and depth, has emerged as a promising research.
Specifically, depth sensors \cite{rgbd1k} facilitate the handling of objects at varying geometric distances; thermal sensors \cite{GMMT} effectively address challenges such as low illumination; and event sensors, known for their low-latency motion capture capabilities (1 \textmu s) \cite{visevent} enhance high-speed awareness for improved tracking performance.
Therefore, multi-modal information can compensate for these deficiencies and enhance the robustness of visual object tracking networks when dealing with objects with large appearance variations.

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{architecture2.pdf}
\caption{Illustraction of our proposed UASTrack.}
%\tang{Try not to use the word 'Label' in MPL, the output of network should not be the label. The input image patches only contain template patches. Anyway, the output bb should be drawn on search patches. }}
\vspace{-3mm}
\label{fig:2}
\end{figure*}


%In practical application, RGB-T tracking \cite{GMMT} addresses the sensitivity of RGB images to illumination changes by incorporating thermal data. 
%RGB-D tracking \cite{rgbd1k} enhances the visibility of occluded objects by leveraging depth information. 
%Meanwhile, RGB-E tracking \cite{visevent} utilizes the event sensor, known for their low-latency motion capture capabilities (1 \textmu s), to improve tracking performance in low-light and high-speed scenarios through the integration of event flows.
However, existing approaches \cite{tenet,GMMT,zhu2024unimod1k,onetracker} often require training N times, using N distinct models for N tasks, leading to inefficiencies and poor generalization in practical application scenarios.
In contrast, our method introduces a unified multi-modal tracking framework, maintaining parameter consistency while ensuring effective adaptation to diverse modalities through a modality-customized mechanism.


\subsection{Learning A Single Set of Parameters for Any Modality}
Recently, there has been growing interest in establishing a unified object tracking with prompt-tuning paradigm for multi-modal object tracking. 
Several existing multi-modal tracking methods such as ProTrack \cite{protrack}, VIPT \cite{VIPT}, OneTracker, and SDSTrack \cite{sdstrack} combine cross-modal information to enhance tracking performance across RGB-D, RGB-E, and RGB-T tracking tasks. 
However, these approaches rely on $N$ sets of parameters for $N$ tasks, which limits their flexibility and adaptability to a wide range of real-world application scenarios within one joint training process.

Additionally, although Un-Track \cite{untrack} attempts to use a single set of parameters for any modality, fails to achieve task-adaptive selection due to relying on prior modality types to guide the flow of input modality. 
In contrast, our proposed UASTrack is the first unified RGB-X tracker to enable modality-adaptive perception by introducing a lightweight discriminative auto-selector.
Our method customizes the head adapter structure characteristics, helping to filter out noise redundancy. 
This operation allows the RGB-based pre-trained foundation network to adapt effectively to the spatial structures of the multi-modal domain.