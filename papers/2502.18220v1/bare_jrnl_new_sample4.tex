\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{framed,multirow}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{booktabs}
\usepackage{colortbl}
\newcommand{\eg}{\textit{e.g.},~}
\newcommand{\ie}{\textit{i.e.},~}
\newcommand{\etal}{\textit{et al.}~}
\usepackage[T1]{fontenc}

\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\newcommand{\tang}[1]{\textcolor{blue}{{}#1}}
\newcommand{\he}[1]{\textcolor{red}{{}#1}}
\newcommand{\g}[1]{\textcolor{green}{{}#1}}
\begin{document}

\title{UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking}

\author{He Wang, Tianyang Xu,~\IEEEmembership{Member,~IEEE}, Zhangyong Tang, Xiao-Jun Wu,  Josef Kittler, ~\IEEEmembership{Life Member,~IEEE}
        % <-this % stops a space
\thanks{
%This work is supported in part by the National Key Research and Development Program of China (2023YFF1105102, 2023YFF1105105), the National Natural Science Foundation of China (Grant NO. 62020106012, 62332008, 62106089, U1836218, 62336004), the 111 Project of Ministry of Education of China (Grant No.B12018), and the UK EPSRC (EP/N007743/1, MURI/EPSRC/DSTL, EP/R018456/1).


He Wang, Tianyang Xu, Zhangyong Tang, Shaochuan Zhao, and Xiao-Jun Wu (Corresponding author) are with the School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi 214122, China (e-mail: 7243115005@stu.jiangnan.edu.cn; tianyang.xu@jiangnan.edu.cn; zhangyong\_tang\_jnu@163.com; wu\_xiaojun@jiangnan.edu.cn).

Josef Kittler is with the Centre for Vision, Speech and Signal
Processing, University of Surrey, GU2 7XH Guildford, U.K. (e-mail: j.kittler@surrey.ac.uk).}}% <-this % stops a space


% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{
%0000--0000/00\$00.00~\copyright~2021 IEEE
}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.
\maketitle

\begin{abstract}
%In the field of multi-modal single object tracking (SOT), RGB data and auxiliary modalities, such as event, thermal, and depth, can complement each other to enhance tracking robustness.
%In multi-modal single object tracking (SOT), RGB data, combined with auxiliary modalities, effectively enhances tracking robustness.
%Multi-modal information is invaluable for visual tracking, as different sensor types provide distinct advantages in addressing specific challenges arising from variations in object appearance.
Multi-modal tracking is essential in single-object tracking (SOT), as different sensor types contribute unique capabilities to overcome challenges caused by variations in object appearance.
However, existing unified RGB-X trackers (X represents depth, event, or thermal modality) either rely on the task-specific training strategy for individual RGB-X image pairs or fail to address the critical importance of modality-adaptive perception in real-world applications.
%Most existing methods claim to implement a unified RGB-X tracker with a single set of parameters but fail to achieve true parameter unification and still depend on modality-specific signals.
In this work, we propose UASTrack, a unified adaptive selection framework that facilitates both model and parameter unification, as well as adaptive modality discrimination across various multi-modal tracking tasks.
%we propose a unified multi-modal tracking framework that enables the highest robust tracking across various tasks using a single set of parameters without modality prior information.
%To achieve discriminative auto-selector 模态, we design a modal-aware classifier capable of identifying modality labels to distinguish the data distributions of auxiliary modalities. 
To achieve modality-adaptive perception in joint RGB-X pairs, we design a Discriminative Auto-Selector (DAS) capable of identifying modality labels, thereby distinguishing the data distributions of auxiliary modalities. 
Furthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored to various modalities in the latent space. This strategy effectively filters noise redundancy and mitigates background interference based on the specific characteristics of each modality.
%More importantly, we propose a customized processing strategy, Task-Customized Optimization Adapter (TCOA), for various modalities in the latent space based on their characteristics, effectively filtering noise redundancy and background interference.
%Our approach introduces minimal additional training parameters amounting to 1.87M and flops of 0.95G while achieving superior Frames Per Second by 44.
%Our innovative approach achieves state-of-the-art (SOTA) performance across multiple datasets by only introducing additional training parameters (1.87M) and computational costs (0.95G).
%Our innovative approach achieves SOTA performance across multiple benchmarks by introducing only additional training parameters of 1.87M and flops of 1.95G.
Extensive comparisons conducted on five benchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering RGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach achieves comparative performance by introducing only additional training parameters of 1.87M and flops of 1.95G.
The code will be available at https://github.com/wanghe/UASTrack.

\end{abstract}

\begin{IEEEkeywords}
Multi-modal object tracking, Unified multi-modal tracking tasks, Adaptive task recognition.
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{V}isual object tracking \cite{siamfc, transt, Dimp, ATOM} is a crucial research area in computer vision, focusing on estimating the position and size of an object throughout a video sequence, beginning with the object initial state in the first frame.
Recent advancements highlight the limitations of relying solely on visible sensors, leading to increased interest in utilizing auxiliary modalities such as thermal (T) \cite{mat}, event (E) \cite{tenet}, and depth (D) \cite{rgbd1k}.
This shift propels multi-modal tracking \cite{VLCTrack, KSTrack, TGTrack} a pivotal research area due to the synergistic characteristics of the RGB modality and auxiliary modalities.
%For instance, while RGB data is highly sensitive to variations in lighting, T data remains consistent, enabling robust tracking even in adverse illumination conditions. 
For example, while RGB data is highly sensitive to lighting variations, thermal data remains stable, facilitating robust tracking even under challenging illumination conditions.
%Similarly, RGB-D tracking leverages the geometry information provided by depth data to improve tracking accuracy, especially in visual degradation scenarios with cluttered backgrounds or noisy occlusions. 
And RGB-D tracking utilizes the geometric information provided by depth modality to enhance tracking accuracy, particularly in scenarios involving cluttered backgrounds, or noisy occlusions.
In contrast, RGB-E tracking capitalizes on the superior temporal resolution and wide dynamic range of event-based data, enabling more precise object tracking even in scenarios involving rapid motion or sudden illumination changes.
%On the other hand, RGB-E tracking takes advantage of the superior temporal resolution and wide dynamic range inherent event-based data, facilitating more precise object tracking even under conditions of fast motion or sudden illumination changes. 
%These complementary features highlight the strengths of distinct multi-modal characteristics to overcome the limitations of single-modality systems.
These complementary features including RGB-X (X represents depth, event, or thermal) image pairs emphasize the strengths of distinct multi-modal characteristics in overcoming the limitations of single-modality systems.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{motivation5.pdf}
\caption{
A comparison between our unified tracker and previous modality-specific trackers. 
(a) N tasks with N models. (b) N tasks with one model but N sets of training parameters. (c) Our proposed method, UASTrack. 
UASTrack is a unified multi-modal tracker utilizing both a single model architecture and a single set of trainable parameters to dynamically accommodate any modality within the RGB-X sensory input.
UASTrack captures distinct modality inputs and applies modality-specific processing tailored to their unique characteristics, marking the first achievement of this capability in an RGB-X tracker.
The metric "PSR" (Prediction Success Rate) quantifies the tracker's capability to dynamically adjust to modality variations while maintaining robust recognition performance.}
\vspace{-3mm}
\label{fig0}
\end{figure}


Most existing methods \cite{tbsi, GMMT} process each RGB-X image pair independently.
Typically, these methods employ a task-specific training strategy, requiring $N$ separate sets of parameters for $N$ tasks, with each task necessitating a distinct model, as shown in Fig. \ref{fig0} (a).
%As shown in figure \ref{fig0} (a), most existing multi-modal trackers \cite{tbsi, GMMT} employ a \textbf{task-specific training strategy} and N tasks use N models, where each multi-modal tracking task requires a separate set of parameters. 
%and they lack the ability to perform adaptive inference across joint multi-modal benchmarks using a single set of parameters.
However, current advancements in multi-modal tracking are constrained by the lack of a comprehensive dataset that simultaneously encompasses all modalities containing depth, event, thermal, and RGB.
%As a result, there is interest research to focus on designing unified multi-modal tracking systems capable of leveraging paired multi-modal training data, while adaptively generalizing to any available modality during inference.
This limitation has been a growing research interest in developing unified multi-modal tracking systems capable of effectively utilizing paired multi-modal training data while adaptively generalizing to any available modality during inference.
Implementing unified multi-modal tracking systems for various tracking tasks offers several advantages:
Firstly, unified multi-modal tracking systems reduce the effort required for model and hyper-parameter tuning for each task, encouraging straightforward comparisons of algorithm performance across different modalities.
Moreover, unified multi-modal tracking systems enable the effective integration of shared information from various modalities into the tracking system.
Therefore, the adoption of unified multi-modal tracking systems enhances flexibility, enabling adaptation to diverse input types in practical applications.
 
%The adoption of unified training parameters enhances the scalability and flexibility of the algorithm, enabling it to adapt to various input data types in practical applications without the need for extensive hyper-parameter tuning for each modality.
%In real-world applications, different sensors or input types may be encountered. Unified parameter settings allow for seamless switching between different sensor data types without the need to retrain or re-adjust the model.

Recently, several methods have attempted to explore achieving unification across various multi-modal tracking tasks, which can generally be classified into two categories.
The first category focuses on employing a unified network architecture, as shown in Fig. \ref{fig0} (b). 
For instance, methods such as ProTrack, ViPT, SDS-Track, and OneTracker \cite{protrack, VIPT, sdstrack,onetracker} leverage the prompt-tuning paradigm to achieve a unified model but still need $N$ sets of training parameters for N RGB-X tracking tasks.
The second category addresses the limitations of the first by utilizing a single set of training parameters, as illustrated in Fig. \ref{fig0} (c).
%Although previous Un-Track \cite{untrack} achieves this, it still relies on prior modality types, limiting their ability to adaptively distinguish modalities without such prior guidance.
While previous Un-Track \cite{untrack} (Fig. \ref{fig0} (c1)) achieves this unification, it still depends on prior knowledge of modality types, which prevents its ability to adaptively distinguish any modalities.
Since various auxiliary modalities, such as thermal, event, and depth, exhibit significantly distinct characteristics, there is a need for a unified algorithm that can not only effectively leverage complementary information but also address domain gaps across modalities. 
However, existing approaches all overlook the unique properties of individual modalities and fail to dynamically adapt to the specific requirements of auxiliary modalities.
%Moreover, various modalities, such as thermal, event, and depth, possess significantly distinct characteristics, necessitating the design of a unified algorithm that can not only effectively leverage complementary information but also address domain gaps between modalities.Existing approaches overlook the unique properties of individual modalities and fail to adapt dynamically to the requirements of different multi-modal tasks.

To address the above challenges, we propose a unified adaptive selection framework with
modality-customization in Single Object Tracking (UASTrack), which not only achieves modality-adaptive perception but also incorporates modality-specific structures based on the characteristics of different RGB-X image pairs, as shown in Fig. \ref{fig0} (c2)). 
Specifically, we introduce a Discriminative Auto-Selector (DAS), which is designed to dynamically identify the input modality type, thereby guiding the adaptive selection of the most suitable network structures.
By employing a classification mechanism that distinguishes image pair combinations (e.g., RGB-T, RGB-D, or RGB-E), the DAS module establishes a robust foundation for adaptive processing modality-specific branches.
To enhance the DAS learning capability, we also incorporate Classification Constraint Loss (CCL) by using cross-entropy.
As illustrated in Fig. \ref{fig0}, our proposed DAS module effectively predicts various tasks, achieving prediction success rate (PSR) of 99.58\%, 99.62\%, and 99.96\% for RGB-T, RGB-D, and RGB-E tracking tasks, respectively.
In contrast, previous methods lack the capability to perform modality-adaptive predictions.
Although directly applying an RGB-based pre-trained head structure has proven effective in extracting robust multi-modal data, it often leads to sub-optimal performance due to differences in data distribution and modality-specific features.
To bridge the modality gap by transforming modality-specific features (thermal, event, or depth) into an RGB-based pre-trained feature space, our approach also proposes a novel Modality-Customized Adapter (TCOA) at the task level.
%The TCOA not only bridges the modality gap by transforming modality-specific features (thermal, event, or depth) into a RGB-based pre-trained feature space, but also enables dynamic adaptation to learn modality-specific transformations during training to enhance performance across tracking tasks.

Furthermore, since different modalities exhibit significant distributional differences and background redundancy characteristics, the optimization adapter for the prediction head is customized for each modality to maximize its effectiveness.
To be specific, in contrast to event and depth modalities, thermal data often contains more effective object information, particularly in scenes with limited illumination and occlusion challenges.
Therefore, a lightweight general adapter is introduced specifically for RGB-T tracking to amplify discriminative features while suppressing noise.
Due to the depth and event features being sparse, average pooling and max pooling mechanisms are additionally applied to reduce redundancy and effectively extract key modality cues.

To fully leverage the potential of RGB and auxiliary modalities while maintaining algorithmic efficiency, we adopt bidirectional adapters within Transformer Encoder blocks \cite{transformer, bat} to facilitate effective interactions between RGB and X features. 
Unlike previous works \cite{untrack,onetracker}, our approach aims to establish unified multi-modal tracking systems capable of adaptively recognizing multi-modal tasks, while integrating modality-specific refinements for each task.
%\textbf{UASTrack} is the first RGB-X tracker to achieve one single architecture in unified parameters. 
%Compared to the RGB-X baseline, which requires 56.44G flops and 92.13M parameters, our proposed UASTrack introduces only a increase of 1.87M parameters and 1.95G flops. 
In comparison to the RGB-X baseline, which requires 56.44G FLOPs and 92.13M parameters, our proposed UASTrack introduces a modest increase of only 1.87M parameters and 1.95G FLOPs, resulting in an absolute improvement of 8.5\% in Success Rate on LasHeR benchmark.


In summary, our contributions are as follows:

\begin{itemize}
    \item 
   We propose a unified RGB-X tracker that utilizes a Discriminative Auto-Selector, eliminating the need for prior modality types and enabling dynamic adaptation across various tracking tasks.
   %in a single training process. 
   Additionally, a classification constraint loss is incorporated to further enhance the Discriminative Auto-Selector learning capability.
    \item 
    %A Task Customization Optimization Adapter is proposed to refine multi-modal features, not only enhancing the adaptability of the foundation model pre-trained in the RGB domain but also enabling modality customization for different tasks, thereby delivering substantial performance enhancements for multi RGB-X tracking tasks.
    We propose a Task Customization Optimization Adapter, enhancing the adaptability of the foundation model to multi-modal space and enabling modality-specific customization for different tasks based on auxiliary modalities.
    \item 
    Extensive evaluations on five benchmarks confirm the effectiveness and efficiency of UASTrack, achieving a significant performance advantage over state-of-the-art trackers.
   
\end{itemize}

\section{RELATED WORK}

\subsection{Multi-modal Tracking}
In recent years, substantial research \cite{ostrack, STARK, ATOM} has been dedicated to visual object tracking, which has gained wide-ranging applications across various fields, such as autonomous driving, mobile robotics, video surveillance, and human-robot interaction. 
However, the performance and stability of visual object tracking remain constrained when confronted with challenges in complex scenarios.
Subsequently, multi-modal tracking \cite{MFGNet,cbpnet} incorporating additional auxiliary modalities \cite{apfnet,Lu_Li_Yan_Tang_Luo_2021,FANet,TFNet,MaCNet,cmpp, zhang2021object, zhu2024pr}, such as thermal, event, and depth, has emerged as a promising research.
Specifically, depth sensors \cite{rgbd1k} facilitate the handling of objects at varying geometric distances; thermal sensors \cite{GMMT} effectively address challenges such as low illumination; and event sensors, known for their low-latency motion capture capabilities (1 \textmu s) \cite{visevent} enhance high-speed awareness for improved tracking performance.
Therefore, multi-modal information can compensate for these deficiencies and enhance the robustness of visual object tracking networks when dealing with objects with large appearance variations.

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{architecture2.pdf}
\caption{Illustraction of our proposed UASTrack.}
%\tang{Try not to use the word 'Label' in MPL, the output of network should not be the label. The input image patches only contain template patches. Anyway, the output bb should be drawn on search patches. }}
\vspace{-3mm}
\label{fig:2}
\end{figure*}


%In practical application, RGB-T tracking \cite{GMMT} addresses the sensitivity of RGB images to illumination changes by incorporating thermal data. 
%RGB-D tracking \cite{rgbd1k} enhances the visibility of occluded objects by leveraging depth information. 
%Meanwhile, RGB-E tracking \cite{visevent} utilizes the event sensor, known for their low-latency motion capture capabilities (1 \textmu s), to improve tracking performance in low-light and high-speed scenarios through the integration of event flows.
However, existing approaches \cite{tenet,GMMT,zhu2024unimod1k,onetracker} often require training N times, using N distinct models for N tasks, leading to inefficiencies and poor generalization in practical application scenarios.
In contrast, our method introduces a unified multi-modal tracking framework, maintaining parameter consistency while ensuring effective adaptation to diverse modalities through a modality-customized mechanism.


\subsection{Learning A Single Set of Parameters for Any Modality}
Recently, there has been growing interest in establishing a unified object tracking with prompt-tuning paradigm for multi-modal object tracking. 
Several existing multi-modal tracking methods such as ProTrack \cite{protrack}, VIPT \cite{VIPT}, OneTracker, and SDSTrack \cite{sdstrack} combine cross-modal information to enhance tracking performance across RGB-D, RGB-E, and RGB-T tracking tasks. 
However, these approaches rely on $N$ sets of parameters for $N$ tasks, which limits their flexibility and adaptability to a wide range of real-world application scenarios within one joint training process.

Additionally, although Un-Track \cite{untrack} attempts to use a single set of parameters for any modality, fails to achieve task-adaptive selection due to relying on prior modality types to guide the flow of input modality. 
In contrast, our proposed UASTrack is the first unified RGB-X tracker to enable modality-adaptive perception by introducing a lightweight discriminative auto-selector.
Our method customizes the head adapter structure characteristics, helping to filter out noise redundancy. 
This operation allows the RGB-based pre-trained foundation network to adapt effectively to the spatial structures of the multi-modal domain.
\section{Methods}



% \begin{itemize}
%     \item left and right margins: .75$''$
%     \item column width: 3.375$''$
%     \item gap between columns: .25$''$
%     \item top margin---first page: 1.375$''$
%     \item top margin---other pages: .75$''$
%     \item bottom margin: 1.25$''$
%     \item column height---first page: 6.625$''$
%     \item column height---other pages: 9$''$
% \end{itemize}



\subsection{Overall Framework}
In this work, we propose a unified adaptive selection framework for any modality in single object tracking, as illustrated in Fig. \ref{fig:2}. 
The framework consists of a frozen Foundation Tracker and trained Discriminative Auto-Selector, Visual adapter, Modality Adaptive Selection Adapter, and Task-customized Optimization Adapter.
These trained components enable task-agnostic representation learning across diverse tracking scenarios.
We provide a detailed description of the foundation tracker architecture in Section \textit{B}, the task-agnostic representation learning in Section \textit{C}, and the objective loss formulation in Section \textit{D}.
%We will elaborate on the Foundation Tracker architecture in Section \textit{B}, the Task-Agnostic Representation Learning in Section \textit{C}, and the details about objective loss in Section \textit{D}.

\subsection{Foundation Tracker} 
As illustrated in Fig. \ref{fig:2}, UASTrack adopts an RGB-based pre-trained Transformer architecture \cite{transformer} as the backbone. 
Multi-modal tracking aims to predict the bounding box of the target in subsequent frames, based on its initial location and shape in the first frame of a video.
Robust tracking performance necessitates the effective integration of multi-modal inputs, including RGB images ${I}_{RGB}$ ${\in \mathbb{R}^{{H} \times {W} \times 3}}$ and auxiliary images ${I_{X}}$.  
%${\in \mathbb{R}^{{H} \times {W} \times 3}}$ ${(I_{D}, I_{T}, I_{E})}$.
Initially, the foundation network preprocesses input image pairs, converting them into a unified embedding format.
The embedding features are processed by the feature extractor $F$ to generate fused features denoted as $f$.
%The fused features are then passed to the task head $H$, where task-relevant information is extracted, and the final predictions $P$ are generated after post-processing.
The fused features are forwarded to the task head $H$, which extracts task-relevant information and generates the final predictions $P$ after post-processing.
The process of multi-modal tracking can be described as follows:
\begin{equation}
   \centering \label{equ:1}
{{P} = \mathrm{Head}(F({I}_{\text{RGB}},{I}_{\text{X}}))}.
\end{equation}

Considering the scarcity of comprehensive multi-modal training datasets, such as RGB-T, RGB-D, and RGB-E, and the lack of pre-trained multi-modal models, we adopt an RGB-based pre-trained Transformer as the backbone to mitigate over-fitting in downstream multi-modal tasks. 
The Transformer blocks are kept frozen, while task-agnostic representation learning adapters are fine-tuned. 
To address significant differences among modalities—such as variations in distributions, color characteristics, and data sparsity—an activated discriminative auto-selector is employed to effectively distinguish between different multi-modal tasks.
%Due to significant differences among modalities—such as distributions, color characteristics, and data sparsity—an activated discriminative auto-selector is employed to effectively identify different multi-modal tasks. 
This enables targeted processing by filtering modality-specific data and dynamically selecting the most relevant architecture, thereby ensuring efficient workflows.
%This facilitates the filtration of specific modality data for targeted processing, dynamically selecting the most relevant information to ensure efficient and focused downstream processing.

\subsection{Task-Agnostic Representation Learning
}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{classifier3.pdf}
\caption{Illustraction of the proposed Task-Customized Optimization Adapter. }
%\tang{Clarify the input modality of each branch. The direction of words should be same as figure 2.}}
\vspace{-3mm}
\label{fig:3}
\end{figure}

\textbf{Discriminative Auto-Selector.} 
To enable task-agnostic representation learning, we propose a Discriminative Auto-Selector (DAS) to predict a modality prediction (MP) which identifies auxiliary modalities and activates DAS during inference. 
Given the significant differences among auxiliary modalities, the simple and lightweight DAS effectively filters and distinguishes features from various modalities.
The structure of DAS is illustrated in Fig. \ref{fig:2} (b). 
The input, denoted as $f_{X}$, are auxiliary features processed after a patch embedding layer.
Initially, $f_{X}$ is passed through an adaptive average pooling layer ($AdaptiveAvgPool$), which adjusts its width and height to an output size of 1x1:

\begin{equation}
\centering \label{equ:2}
f_{X}^{'} = AdaptiveAvgPool(f_{X})
\end{equation}

Subsequently, the reshaped 
$f_{X}^{'}$ features are processed through two linear layers to obtain a modality-predicted probability $P_{m}$:

\begin{equation}
\centering \label{equ:3}
P_{m} = FC_{2}(Norm(FC_{1}(Reshape(f_{X}^{'})))
\end{equation}

Using the $Argmax$ operation, the index corresponding to the maximum value can be returned:
\begin{equation}
\centering \label{equ:4}
MP = Argmax(P_{m})
\end{equation}
$MP$ serves as a crucial input for subsequent multi-modal feature fusion and modality-specific optimization. 
The prediction success rates are presented in Fig. \ref{fig0}.
By applying the $MP$ predicted by the discriminative auto-selector, we can obtain the predicted types for input tasks without requiring prior types.

To further strengthen the DAS classification constraint, we utilize the predicted probability $P_{m}$ to compute the {Classification Loss} (CL) $L_{m}$ using cross-entropy loss against the true modality types $T_{m}$ for the three multi-modal tracking tasks.
\begin{equation}
\centering \label{equ:5}
L_{m} = - \sum_{i=1}^{N} T_{\text{m}, i} \log(P_{m})
\end{equation}
where $N$ is the number of multi-modal tracking tasks.

\textbf{Modality Adaptive Selection Adapter.}
As illustrated in Fig. \ref{fig0} (d), spatial interactions between RGB modality features are facilitated by a bidirectional adapter module inspired by \cite{bat}.
To accommodate the varying characteristics of different modalities, we design task-specific adapter structures with non-shared parameters.
Firstly, we identify and split $x$ modality data in $l-th$ encoder block, denoted as ${f_{x}^{l} \in \mathbb{R}^{{H} \times {W} \times C}}$, based on previous MP.
Then $f_{x}^{l}$ features are passed through a down-sampling layer $Down$ to reduce the feature channel dimension. 
Subsequently, a $Linear$ layer is applied to maintain the consistency of modality-specific features with a small number of trainable parameters.
The features then pass through an up-sampling layer, denoted as $Up$, to restore the original feature channel dimensions.
%${f_{x}\in \mathbb{R}^{{H} \times {W} \times C}}$ $(I_{D}, I_{T}, I_{E})$.
%\textbf{Taking the depth modality as an example, the mathematical formulation is as follows:}
The mathematical formulation of the task-specific sub-adapters is as follows. 
\begin{equation}
\centering \label{equ:6}
f^{'}_{x} = Up(Linear(Down(f^{l}_{x})))
% f{^}_{depth} = Up(Linear(Down(f^{l}_{Depth}))),
% f{^}_{depth} = Up(Linear(Down(f^{l}_{Depth}))),
\end{equation}
where there are $N$ sub-adapters for $N$ tasks.

For simplicity, the Visual Adapter (VA) maintains the same structure as the task-specific sub-adapters.

%\textbf{Visual Adapter.}

\textbf{Task-Customized Optimization Adapter.} 
On one hand, due to the limited adaptability of the RGB-based pre-trained network to downstream multi-modal data, we employ adapter learning with a small number of additional training parameters, without modifying the foundation structure. 
On the other hand, the significant variation of auxiliary modalities necessitates customized filtering for each modality.


\begin{table*}[]
\renewcommand{\arraystretch}{1.4}
\centering
\caption{A comparison with state-of-the-art methods on LasHeR, DepthTrack, and VisEvent benchmarks. 
The term "Separated" refers to trackers that perform different tasks by employing distinct training parameters. "Unified-Model" denotes trackers that utilize a single model but rely on varying training parameters to accomplish multiple tasks. Conversely, "Unified-All" represents trackers that employ a single model and a single set of training parameters to address various tasks.
Performance is denoted in \he{Red} for the best and in \tang{Blue} for the second-best, consistently throughout the table. 
}\vspace{0.3mm} 
\scalebox{1.2}{
\begin{tabular}{cccccccccccc}
\hline 
\multirow{2}{*}{Type} & \multirow{2}{*}{Method} & \multirow{2}{*}{Venue} & \multicolumn{3}{c}{LasHeR} & \multicolumn{2}{c}{VisEvent} & \multicolumn{3}{c}{DepthTrack} \\ 
 &  &  & SR & PR & NPR & SR & PR & Pr & Re & F-score \\ \hline \hline
\multirow{11}{*}{Separated} 
%&APFNet &AAAI 2022 &0.362 &0.50 &0.439 &- & - & - & - & - \\
%&DMCNet &TNNLS 2022 &0.355 &0.490 &0.431 & - & - & - & - & - \\
&TBSI &CVPR2023 &0.556 &0.692 &{0.657}& - & - & - & - & - \\
&LSAR &TCSVT 2023   &0.385 &0.460 &- & - & - & - & - & - \\
& GMMT & AAAI 2024 & \textcolor{blue}{\textbf{0.566}} & \textcolor{blue}{\textbf{0.707}} & \textcolor{blue}{\textbf{0.670}} & - & - & - & - & - \\
&ProFormer &TCSVT 2024 &0.533 &0.674 &0.630 & - & - & - & - & -\\
&MPT &TCSVT 2024 &0.313&0.355  &-& - & - & - & - & -\\
&QueryTrack &TIP 2024  &0.520 &0.660&- & - & - & - & - & -\\
%&CAT++  &TIP 2024 &0.356 &0.509 &0.444 & - & - & - & - & -\\
& BAT & AAAI 2024 &{0.563} & {0.702} &-  &-  &-  & - &-  &-  \\
&CEUTrack &ARXIV 2024 &-&-&-&0.531 &0.691&-&-& \\	
&MMHT & ARXIV 2024& - &- &- &0.551 &0.733&-&-&-\\
&TENeT &NN 2024 &- &-&- &0.601 &0.765 &-&-&-\\
&SPT &IJCV 2024 & -& -& -&- &-&0.527 &0.549 &0.538 \\
& CDAAT & SPL 2024 &- &- &- &- &- &0.578&0.603&0.590 \\
&TABBTrack &PR 2024 & & & & & &\textcolor{blue}{\textbf{0.622}}&\textcolor{blue}{\textbf{0.615}}&\textcolor{blue}{\textbf{0.618}}\\
\hline
\multirow{4}{*}{Unified-Model} 
& Protrack & ACMMM 2022 & 0.421 & 0.509 & - & 0.474 & 0.617 & 0.583 & 0.573 & 0.578 \\
& ViPT & CVPR 2023 & 0.525 & 0.651 & - & 0.589 & 0.756 & 0.561 & 0.581 & 0.571 \\
& OneTracker & CVPR 2024 & 0.538 & 0.672 & - & \textcolor{blue}{\textbf{0.608}} & \textcolor{blue}{\textbf{0.767}} & 0.607 & 0.604 & 0.609 \\
& SDSTrack & CVPR 2024 & 0.531 & 0.665 & 0.631 & 0.597 & 0.767 & 0.619 & 0.609 & 0.614 \\
\hline
\multirow{2}{*}{Unified-All} & Un-Track & CVPR 2024 & 0.511 & 0.604 & 0.640 & 0.592 & 0.735 & 0.566 & 0.588 & 0.577 \\
& UASTrack & - & \textcolor{red}{\textbf{0.570}} & \textcolor{red}{\textbf{0.711}} &\textcolor{red}{\textbf{0.675}}  & \textcolor{red}{\textbf{0.610}} & \textcolor{red}{\textbf{0.773}} & \textcolor{red}{\textbf{0.630}} & \textcolor{red}{\textbf{0.625}} & \textcolor{red}{\textbf{0.628}} \\ \hline
\end{tabular}}
\label{tab1}
\end{table*}

We analyze the characteristics of different modalities to determine appropriate processing approaches.
Compared to event and depth modalities, thermal modality features are dense and exhibit minimal redundancy.
Therefore, a general adapter module is sufficient to handle thermal features, ensuring a design that remains both effective and efficient without specialized processing 
In contrast, depth and event data exhibit significant sparsity and redundancy.
Depth data provides rich geometric information but may also include redundant features, such as excessive details from flat regions (e.g., walls and floors), whereas edge and object contour details are more critical.
%Event data generated through motion detection is inherently sparse and features highly uneven information distribution.
Event data, generated through motion detection, is inherently sparse and exhibits a highly uneven distribution of information.

To address these challenges, we design modality-customized adapters for the depth and event modalities to enable targeted processing, as illustrated in Fig. \ref{fig:2}. A max pooling operation is employed to extract high-response features, while an average pooling operation is used to retain global characteristics. These two mechanisms complement each other to achieve a balanced feature representation:
\begin{equation}
\centering \label{equ:7}
f^{'}_{x} = Up(Avg(Down(f_{x}))
+Max(Down(f_{x}))
\end{equation}
where $Avg$ and $Max$ represent average pooling and max pooling layers, respectively. 




% \begin{equation}
% \centering \label{equ:5}
% f^{'}_{thermal} = Up(Linear(Down(f_{thermal})))
% \end{equation}

\subsection{Objective Loss}
Consistent with OSTrack \cite{ostrack}, we employ focal loss as the classification loss $L_{cls}$ and adopt $L_{1}$ loss and $L_{GIoU}$ loss for regression. 
Additionally, we propose Classification Constraint
Loss that incorporates a cross-entropy loss to enhance the learning capability of the Discriminative Auto-Selector.
The overall loss $L$ is defined as:
\begin{equation}
\centering \label{equ:8}
L =  L_{cls}+\lambda_{1} L_{1}+\lambda_{2} L_{GIoU}+ \alpha * L_{m}
\end{equation}
where $\lambda_{1}$, $\lambda_{2}$, and $\alpha$ are set as 5, 2, and 0.1, respectively. 


\begin{table}[]
\renewcommand{\arraystretch}{1.5}
\centering
\caption{A comparison with state-of-the-art methods on other RGB-T tracking datasets including on RGBT234 and GTOT datasets.}\vspace{0.3mm}  
\scalebox{1}{
\begin{tabular}{cccccc}
\hline 
\multirow{2}{*}{Type} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{RGBT234} & \multicolumn{2}{c}{GTOT} \\
 &  & SR & PR & SR & PR \\ \hline \hline
\multirow{6}{*}{Separated} & APFNet & 0.579 & 0.827 & 0.739 & 0.905 \\
& TBSI & 0.637 & 0.871 & - & - \\
& BAT &\textcolor{blue}{\textbf{0.641}} &\textcolor{blue}{\textbf{0.868}} & \textcolor{blue}{\textbf{0.763}} &\textcolor{blue}{ \textbf{0.909}} \\
%& GMMT & \textcolor{blue}{\textbf{0.647}} & \textcolor{blue}{\textbf{0.879}} & 0.693 & 0.857 \\
& QueryTrack &0.600 &0.841 &0.759 &0.923 \\
& CAT++ &0.592 &0.840 &0.733 &0.915 \\
\hline
\multirow{3}{*}{Unified-Model}
& Protrack & 0.587 & 0.786 & -&-\\
& ViPT & 0.617 & 0.835 & - & - \\
& SDSTrack & 0.625 & 0.848 & 0.760 & 0.887 \\
\hline
\multirow{2}{*}{Unified-All} & Un-Track & 0.618 & 0.837 & - & - \\
 & Our & \textcolor{red}{\textbf{0.651}} & \textcolor{red}{\textbf{0.876}} & \textcolor{red}{\textbf{0.789}} & \textcolor{red}{\textbf{0.933}} \\ \hline
\end{tabular}}
\label{tab2}
\end{table}











% \begin{table}
%     \centering
%     \begin{tabular}{lrr}
%         \toprule
%         Scenario  & $\delta$ (s) & Runtime (ms) \\
%         \midrule
%         Paris     & 0.1          & 13.65        \\
%                   & 0.2          & 0.01         \\
%         New York  & 0.1          & 92.50        \\
%         Singapore & 0.1          & 33.33        \\
%                   & 0.2          & 23.01        \\
%         \bottomrule
%     \end{tabular}
%     \caption{Booktabs table}
%     \label{tab:booktabs}
% \end{table}



% \begin{algorithm}[tb]
%     \caption{Example algorithm}
%     \label{alg:algorithm}
%     \textbf{Input}: Your algorithm's input\\
%     \textbf{Parameter}: Optional list of parameters\\
%     \textbf{Output}: Your algorithm's output
%     \begin{algorithmic}[1] %[1] enables line numbers
%         \STATE Let $t=0$.
%         \WHILE{condition}
%         \STATE Do some action.
%         \IF {conditional}
%         \STATE Perform task A.
%         \ELSE
%         \STATE Perform task B.
%         \ENDIF
%         \ENDWHILE
%         \STATE \textbf{return} solution
%     \end{algorithmic}
% \end{algorithm}


\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{attr.pdf}
\caption{The Success Rate (SR) and Precision Rate (PR) of 19 different attributes on LasHeR dataset.}
\vspace{-3mm}
\label{fig:4}
\end{figure*}

\section{Experiments}\label{stylefiles}
%\tang{Clarify the training details, especially the usage datasets.}
To evaluate the advantages of our proposed UASTrack, we compare its performance against both separated training trackers and unified trackers. 
The comparison includes methods such as Un-Track \cite{untrack}, OneTracker \cite{onetracker}, ViPT \cite{VIPT}, SDSTrack \cite{sdstrack}, TBSI \cite{tbsi}, GMMT \cite{GMMT}, BAT \cite{bat}, APFNet \cite{apfnet}, LSAR \cite{lsar}, ProFormer \cite{proformer}, MPT \cite{mpt}, QueryTrack \cite{querytrack}, CAT++ \cite{cat++}, TENeT \cite{tenet}, SPT \cite{zhu2024unimod1k}, ProTrack \cite{protrack}, CEUTrack \cite{ceutrack}, MMHT \cite{MMHT}, TABBTrack \cite{TABBTrack}, CDAAT \cite{CDAAT}, and OSTrack \cite{ostrack}.
Our foundation network utilizes OSTrack-B224 \cite{ostrack} as the pre-trained model.
%These comparison trackers include Un-Track \cite{untrack}, OneTracker \cite{onetracker}, ViPT \cite{VIPT}, SDSTrack \cite{sdstrack}, TBSI \cite{tbsi}, GMMT \cite{GMMT}, BAT \cite{bat}, SDSTrack \cite{sdstrack}, APFNet \cite{apfnet}, DMCNet \cite{DMCNet}, LSAR \cite{lsar}, ProFormer \cite{proformer}, MPT \cite{mpt}, QueryTrack \cite{querytrack}, CAT++ \cite{cat++}, TENeT \cite{tenet}, SPT \cite{zhu2024unimod1k}, ProTrack \cite{protrack}, and OSTrack \cite{ostrack}. 
%Our foundation network employs OSTrack-B224 \cite{ostrack} as the pre-trained model.

\begin{table*}[]
\renewcommand{\arraystretch}{1}
\centering
\caption{Ablation study for our proposed components. The column $\sigma$ represents the average percentage change across all metrics compared to the baseline. Our plain version is highlighted in Bold.
%\tang{Mistakes on results, such as the performance on VisEvent.}
}\vspace{0.3mm} 
\scalebox{1.4}{
\begin{tabular}{ccccccccccc}
\hline
 &  &  & \multicolumn{2}{c}{LasHeR} & \multicolumn{2}{c}{VisEvent} & \multicolumn{3}{c}{DepthTrack} \\
\multirow{-2}{*}{DAS} & \multirow{-2}{*}{CCL} & \multirow{-2}{*}{TCOA}  & SR & PR & SR & PR & Pr & Re & F-score 
&\multirow{-2}{*}{$\sigma$} 
\\ \hline \hline
 &  &  & 0.482 & 0.609 & 0.588 & 0.754 & 0.577 & 0.582 & 0.579 &-\\
\checkmark &  &  & 0.535 & 0.678 & 0.591 & 0.760 & 0.583 & 0.585 & 0.584&+2.07\% \\
% &  & \checkmark  &0.492 & 0.633 & 0.592 & 0.725 & 0.596 & 0.585 & 0.590 \\
\checkmark &\checkmark  &  & 0.551 & 0.687 & 0.602 & 0.766 & 0.603 & 0.611 & 0.609 &+3.68\%  \\
\checkmark &  &\checkmark  & 0.547 & 0.682 & 0.595 & 0.766 & 0.601	&0.593	&0.597 &+3.00\%\\
\checkmark  &\checkmark   &\checkmark  & {\textbf{0.570}} & {\textbf{0.711}} &{\textbf{0.610}} & \textbf{0.773} & \textbf{0.630} & \textbf{0.625} & \textbf{0.628} & \textbf{+4.86}\% \\ \hline
\end{tabular}}
\label{tab3}
\end{table*}

\begin{table*}[]
\centering
\caption{Ablation study for our proposed Task-Customized Optimization Adapter (TCOA).}\vspace{0.3mm}
\renewcommand{\arraystretch}{1.4}
\scalebox{1.4}{
\begin{tabular}{ccccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{LasHeR} & \multicolumn{2}{c}{VisEvent} & \multicolumn{3}{c}{DepthTrack} \\
 & SR & PR & SR & PR & Pr & Re & F-score &\multirow{-2}{*}{\checkmark/$\times$} \\ \hline \hline
\multicolumn{1}{c}{w/o maxpool} & 0.558 & 0.695 & 0.603 &0.767  & 0.614 & 0.605 & 0.609 & $\times$ \\
\multicolumn{1}{c}{w/o avgpool} & 0.554 & 0.689  & 0.596 &0.764 & 0.602 & 0.596 & 0.599  &$\times$\\
\multicolumn{1}{c}{w/ avgpool+maxpool} &0.556  &0.692  & \textbf{0.610} &\textbf{ 0.773} & \textbf{0.630} & \textbf{0.625} & \textbf{0.628} & \checkmark\\
\multicolumn{1}{c}{w/ linear} &\textbf{ 0.570} & \textbf{0.711} & 0.602 & 0.766 & 0.606 & 0.611 & 0.609 & \checkmark \\ \hline
\end{tabular}}
\label{tab4}
\end{table*}

To train our proposed UASTrack, only the parameters in Discriminative Auto-Selector and modality-specific adapters are learnable, as shown in Fig. \ref{fig:2}. 
In addition to visual object tracking loss, we incorporate a cross-entropy loss that constrains DAS and specialization of modality-specific adapters. 
Our method is implemented using PyTorch and trained on a server equipped with a single NVIDIA 3090Ti GPU. 
We set the batch size to 32, training for 80 epochs. 
The learning rate for the backbone is set to 4e-4, with a decay ratio of 0.8.
We adopt the AdamW optimizer with a weight decay of 1e-4. 
Additionally, template feature dimensions are uniformly resized to 128×128, while the search search regions are resized to 256×256.

We jointly combine various multi-modal tracking benchmarks, including LasHeR \cite{lasher}, DepthTrack \cite{det}, and VisEvent \cite{visevent}, for the training process. 
UASTrack is evaluated on distributed multi-modal tasks across three RGB-T tracking benchmarks: LasHeR, RGBT234 \cite{RGBT234}, and GTOT \cite{GTOT}; one RGB-E benchmark: VisEvent; and one RGB-D benchmark: DepthTrack.


\subsection{Comparisons with State-of-the-art Approaches}
As presented in Table \ref{tab1}, our proposed UASTrack outperforms state-of-the-art methods, including both unified trackers and separated training trackers across RGB-T, RGB-E, and RGB-D tracking.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{response2.pdf}
\caption{Ablation study with visualized score map comparisons of our proposed method. 
"w/o TCOA," represents UASTrack without the TCOA module; "TCOA with linear" represents the TCOA module exclusively employs linear layers; and "TCOA with AvgPool+MaxPool" represents the TCOA module integrates both average pooling and max pooling operations. %\tang{The most highlighted areas seem not to be aligned with the input images.}
%"w/o TCOA" represents ASTrack without TCOA module, "TCOA with linear" represents TCOA modules are full of linear layers, "TCOA with
%Avgpool+maxpool" represents TCOA modules make up with average pooling and max pooling operations. 
}
\vspace{-3mm}
\label{fig:5}
\end{figure}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{tracking_result1.pdf}
% \caption{Visualisation of score maps compared our method components between different tasks.}
% \vspace{-3mm}
% \label{fig:5}
% \end{figure*}



\textbf{RGB-D Tracking.} 
DepthTrack is a comprehensive RGB-D dataset comprising 150 training sequences and 50 testing sequences, evaluated using F-score, Recall (Re), and Precision (Pr) metrics.
UASTrack sets a new state-of-the-art performance on DepthTrack benchmark.
Specifically, UASTrack achieves an F-score of 62.8\%, precision (Pr) of 63.0\%, and recall (Re) of 62.5\%. 
These results represent substantial improvements over the "Unified-All" tracker, Un-Track, with margins of 5.1\%, 3.7\%, and 7.4\% for F-score, Pr, and Re, respectively. 
Furthermore, UASTrack outperforms the "Unified-Model" tracker, SDSTrack by 1.4\%, 1.6\%, and 1.1\% for the same metrics.
%These results demonstrate significant improvements over the unified tracker Un-Track by margins of 5.1\%, 3.7\%, and 7.4\%, respectively, and outperform the task-specific tracker, SDSTrack, by 1.4\%, 1.6\%, and 1.1\%, respectively.


\textbf{RGB-T Tracking.}
LasHeR benchmark contains 979 training video sequences and 245 testing video sequences, evaluated using three metrics: Precision Rate (PR), Success Rate (SR), and Normalized Precision Rate (NPR). 
On the test dataset, UASTrack achieves the SR of 57.0\%, surpassing the best-performing "Separated" tracker, GMMT, by 0.4\%, and the unified tracker, Un-Track, by 5.9\%.

RGBT234 benchmark integrating both RGB and thermal images, includes a total of 234 video sequences with nearly 116.7k frames. 
As shown in Table \ref{tab2}, UASTrack achieves competitive performance compared with previous trackers, with an SR of 65.1\% and a PR of 87.6\%.  

GTOT benchmark, which is designed to evaluate the robustness of RGB-T trackers, consists of 50 diverse video sequences. 
As shown in Table \ref{tab2}, UASTrack sets a new SOTA with an SR of 78.9\% and a PR of 93.3\%.
%As shown in Table \ref{tab2}, UASTrack establishes a new state-of-the-art with a Success Rate (SR) of 78.9\% and a Precision Rate (PR) of 93.3\%. 
These results surpass the previous best-performing tracker, BAT, by margins of 2.6\% and 2.4\%, respectively.


\textbf{RGB-E Tracking.} 
As the largest RGB-E tracking dataset, VisEvent consists of 500 video pairs for training and 320 video pairs for testing. 
UASTrack achieves the top performance on VisEvent. 
UASTrack attains the highest Precision Rate (PR) of 77.3\% and Success Rate (SR) of 61.0\%.  %surpassing OneTracker by margins of 0.6\% and 0.2\%, and Un-Track by 3.8\% and 1.8\%, respectively.
These results surpass OneTracker by margins of 0.6\% and 0.2\%, and Un-Track by 3.8\% and 1.8\%, respectively.

\textbf{Attribute-Based Performance on LasHeR.}
Our method is evaluated on various challenging attributes in comparison with state-of-the-art trackers using the LasHeR dataset, as shown in Fig. \ref{fig:4}. 
These attributes include No Occlusion (NO), Partial Occlusion (PO), Total Occlusion (TO), Hyaline Occlusion (HO), Motion Blur (MB), Low Illumination (LI), High Illumination (HI), Abrupt Illumination Variation (AIV), Low Resolution (LR), Deformation (DEF), Background Clutter (BC), Similar Appearance (SA), Camera Movement (CM), Thermal Crossover (TC), Frame Loss (FL), Out-of-View (OV), Fast Motion (FM), Scale Variation (SV), and Aspect Ratio Change (ARC). 
The experimental results show that our method consistently outperforms existing state-of-the-art trackers across most attributes in terms of SR and PR. 
Notably, it demonstrates superior performance in scenarios involving significant DEF, FM, and SV, where the target objects experience drastic changes or blurring. 
Additionally, our tracker exhibits exceptional robustness in occlusion scenarios (HO, PO, and TO), effectively addressing complex occlusion challenges.
Even under illumination-changing environments, such as LI, HI, AIV, and TC, our tracker achieves significantly higher tracking accuracy compared to existing methods.

\subsection{Ablation Study}

% \begin{table*}[]
% \renewcommand{\arraystretch}{1}
% \centering
% \begin{tabular}{ccccccccc}
% \hline
% \multirow{2}{*}{Ratio} & \multicolumn{2}{c}{LasHeR} & \multicolumn{2}{c}{VisEvent} & \multicolumn{3}{c}{DepthTrack} \\
%  & SR & PR & SR & PR & Pr & Re & F-score \\ \hline
% 0 & \textbf{0.564} & \textbf{0.703}  & {0.599} & 0.763 & 0.611 & 0.602 & 0.607 \\
% 0.1 & 0.558 & 0.694  &0.599  &0.764  & 0.620 &0.621  &0.620  \\
% 0.2 & 0.556 & 0.690 & 0.598 & 0.763 & 0.620 & 0.573 & 0.578 \\
% 0.3 & 0.557 & 0.691  & \textbf{0.605} & \textbf{0.770} & 0.623 & 0.621 & 0.622 \\
% 0.4 & 0.556 & 0.694 & 0.604 & 0.770 & \textbf{0.624} & \textbf{0.622} & \textbf{0.623} \\
% 0.5 & 0.556 & 0.693 & 0.602 & 0.765 & 0.620 & 0.620 & 0.621 \\
% 0.6 & 0.555 & 0.692 & 0.601 & 0.763 &0.620  &0.620&0.621  \\
% 0.7 & 0.555 &0.691  &0.600 & 0.761  &0.619  &0.619 &0.620    \\
% 0.8 & 0.552 & 0.690 &  0.597 &0.658 & 0.619 &0.619  &0.619    \\
% 0.9 & 0.550 & 0.689  & 0.593 & 0.650 & 0.617 &0.618  &0.618   \\
% 1.0 & 0.541 & 0.673  & 0.590 &0.648  & 0.615 &0.616  &0.615   \\ \hline
% \end{tabular}
% \caption{}\vspace{0.3mm}
% \label{MEA}
% \end{table*}






\begin{table}[]
\centering
\caption{Ablation experiment for parameter $\alpha$}\vspace{0.3mm}
\renewcommand{\arraystretch}{1.4}
\scalebox{1.4}{
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{$\alpha$} & \multicolumn{3}{c}{LasHeR} \\
& SR & PR & NPR \\ \hline \hline
0.01 &0.566 & 0.706 & 0.669  \\
0.05 &0.564 & 0.703 & 0.665  \\
0.1 &\textbf{0.570} &\textbf{0.711} &\textbf{0.675}  \\
0.5 & 0.565 & 0.703 &0.665  \\
1 & 0.565 & 0.704 & 0.669 \\
5 & 0.563 & 0.701 &0.664  \\
10 &0.562  & 0.699 &0.663  \\ \hline
\end{tabular}}
\label{tab6}
\end{table}


\textbf{Component Analysis of UASTrack}.
We conduct an ablation experiment to evaluate the components of our proposed UASTrack on the VisEvent, LasHeR, and DepthTrack benchmarks, as shown in Table ~\ref{tab3}. 
Since both the Classification Constraint
Loss (CCL) and the Task-Customized Optimization adapter (TCOA)  rely on the prediction types from the Discriminative Auto-Selector (DAS) module, the validation results for individual modules are assessed based on the the DAS module.
The incorporation of DAS results in a significant improvement, with a 2.07\% increase for $\sigma$ compared to the baseline (first row). 
To be specific, the F-score on DepthTrack increases by 0.5\%, the Success Rate (SR) on LasHeR improves by 5.2\%, and the SR on VisEvent rises by 0.3\%.
Even without CCL, the network demonstrates superior performance in distinguishing the thermal modality compared to depth and event modalities.
This finding suggests that the thermal modality has inherent characteristics that make it more easily distinguishable by the network relative to the other modalities.
%Even in the absence of cCL constraints, the network demonstrates superior performance in distinguishing the thermal modality compared to depth and event modalities. 
%This finding suggests that the thermal modality possesses inherent characteristics that make it more easily distinguishable by the network relative to the other modalities.
Integrating CCL further enhances the network's performance, leading to notable improvements, including a 1.6\% increase in SR on LasHeR, a 1.1\% rise in SR on VisEvent, and a 2.5\% boost in F-score on DepthTrack.
Additionally, the incorporation of Task-Customized Optimization (TCO) improves tracker accuracy, contributing to a 1.2\% boost in SR on LasHeR, a 1.3\% increase in F-score on DepthTrack, and a 0.3\% improvement in SR on VisEvent.
When DAS, CCL, and TCOA are together integrated into the foundation network, optimal performance is achieved, with an SR of 56.4\% on LasHeR, an SR of 60.7\% on VisEvent, and an F-score of 62.8\% on DepthTrack.
%When the Discriminative Auto-Selector (DAS), CCL, and TCOA are fully integrated into the network, optimal performance is achieved, with SR values of 56.4\% on LasHeR and 60.7\% on VisEvent, and an F-score of 62.8\% on DepthTrack.

\textbf{Component analysis of the TCOA module.} 
We conduct an ablation experiment on the Task-Customized Optimization adapter module across different tasks to examine how variations in modality characteristics affect the adaptability of network structures. 
The results demonstrate that different modalities benefit from distinct optimization strategies. 
As shown in Table \ref{tab4}, employing a general sub-adapter composed of linear layers for the thermal modality achieves superior performance due to its ability to effectively capture thermal features, achieving a 1.4\% higher SR compared to "w avgpool+maxpool" on LasHeR. 
In contrast, depth and event data exhibit greater redundancy, which can introduce noise and hinder feature fusion.
To address this, integrating max pooling and average pooling operations into their respective sub-adapters enhances the TCOA module's ability to filter irrelevant information and extract salient features.
This approach yields substantial improvements, increasing the F-score by 1.9\% on DepthTrack and the SR by 0.5\% on VisEvent compared to "w linear".
These findings highlight the necessity of customizing network structures to the distinct characteristics of each modality, demonstrating that a one-size-fits-all approach is suboptimal for multi-modal tasks. 
%This highlights the potential for further exploration of modality-specific customizations to improve performance across diverse datasets and applications.

\begin{table}[]
\centering
\caption{Ablation experiment for low-rank dimensions. }\vspace{0.3mm}
\renewcommand{\arraystretch}{1.5}
\label{c}
\scalebox{1.3}{
\begin{tabular}{ccccc}
\multicolumn{5}{c}{(a) VA \& MASA} \\ \hline
 & 4 & {8} & 16 & 192 \\ \hline \hline
SR & 0.552 & \textbf{0.570} & 0.556 & 0.557 \\
PR & 0.690 & \textbf{0.711} & 0.694 & 0.691 \\ \hline
\multicolumn{5}{c}{(b) TCOA} \\ \hline
 & 8 & 96 & {\textbf{192}} & 384 \\ \hline \hline
SR & 0.541 & 0.555 & \textbf{0.570} & 0.550 \\
PR & 0.673 & 0.691 & \textbf{0.711} & 0.689 \\ \hline
\end{tabular}}
\label{tab8}
\end{table}
%\textbf{Different hidden layer dimensions in adapters. }



\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{tracking_result3.pdf}
\caption{Illustraction of tracking results comparison. From top to bottom, we show the results on three video sequences, "whitebag" from LasHeR dataset, "backpack\_indoor" from DepthTrack dataset, and "video\_0079" from VisEvent dataset.}
\vspace{-3mm}
\label{fig:6}
\end{figure*}


\textbf{Inﬂuence of parameter $\alpha$.}
The selection of hyperparameters is crucial for optimizing the object tracking performance.
We explore the effect of parameter $\alpha$, while keeping $L_{1}$ and $L_{GIoU}$ consistent with the OSTrack baseline.
The hyperparameter values for $L_{1}$ and $L_{GIoU}$ are set to 5 and 2, respectively.
The analysis focuses exclusively focuses on the effect of parameter $\alpha$.
As shown in Table \ref{tab6}, when $\alpha$ is set to 1, we explore values ranging from 1/100 to 10 times of it. 
To manage the wide range of potential values, we select the median values of the left and right intervals—0.05, 0.5, and 5—as candidate values for $\alpha$.
When $\alpha$ is set to 0.1, the SR improves by 0.4\%, 0.5\%, and 0.8\% compared to $\alpha$ values of 0.01 and 1, 10, respectively.
This indicates that, within this specific framework, a moderate value of $\alpha$ is most effective. 
Tracking accuracy decreases when $\alpha$ shifts away from 0.1, whether towards smaller values such as 0.01 or 0.05, or larger values such as 5 or 10.
This decline is likely due to an imbalance in the model: a smaller $\alpha$ may underweight critical components, leading to suboptimal feature utilization, while a larger $\alpha$ may overemphasize certain aspects, diminishing the model's effectiveness in addressing the diverse characteristics of the tracking task.


\begin{table}[]
\centering
\caption{A comparison for computational cost and the Frames Per Second (FPS) tracking speed of different trackers on LasHeR test set.}\vspace{0.3mm} 
\renewcommand{\arraystretch}{1.5}
\scalebox{1}{
\begin{tabular}{cccccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Params} & \multirow{2}{*}{Flops} & \multirow{2}{*}{FPS} & \multicolumn{2}{c}{LasHeR} \\
 &  &  &  & SR & PR \\ \hline \hline 
OSTrack-RGBT &92.13M &56.44G &71.28   & 0.479 & 0.590   
\\
OSTrack-RGB  &92.13M & 29.24G &107.10&0.470&0.583 
\\
OSTrack-TIR &92.13M &29.24G  &107.65&0.453 & 0.549 
\\ \hline
ViPT & 0.84M & 3.12G & 24.78 & 0.525 & 0.651 \\
SDSTrack & - & - & 20.90 & 0.531 & 0.665 \\
TBSI & 191.36M & 79.80G & 32.00 & 0.556 & 0.692 \\
Un-Track & 6.65M & 2.14G & - & 0.536 & 0.667 \\
Our & {1.87M}  & {1.95G} & 44.00 & \textbf{0.570} & \textbf{0.711} \\ \hline
\end{tabular}}
\label{tab7}
\end{table}


\textbf{Low-rank dimension analysis.}
We explore the effectiveness of different low-rank dimensions for Visual Adapter (VA), Modality Adaptive Selection Adapter (MASA), and Task-Customized Optimization Adapter (TCOA) in Table \ref{tab8}.
Since both the VA and MASA are applied during the feature extraction process, whereas the TCOA is applied after feature extraction, the VA and MASA utilize the same low-rank dimension.
From Table \ref{tab8} (a), we experiment by varying the ranks of the VA and MASA across four configurations: 4, 8, 16, and 192. 
The results reveal that lower ranks consistently demonstrate poor performance, while higher ranks tend to degrade performance. 
From Table \ref{tab8} (b), for the TCOA, we test ranks of 8, 96, 192, and 384. 
Our findings demonstrate that a rank of 192 achieves the best performance. 
This optimal configuration can be attributed to the balance in fine-tuning the head structure: excessively high ranks may increase the model's learning capacity but at the cost of overfitting, whereas excessively low dimensions risk losing critical feature information extracted earlier.

\begin{table*}[]
\centering
\caption{The ablation experiment for exploring cross-modal dependency on different RGB-X benchmarks. The \#1, \#4, and \#7 rows show the performance of our method on RGB-T, RGB-D, and RGB-E tracking tasks, respectively. Type "X$\rightarrow$Y" indicates that the X modality features are fed into the Y branch during testing. 
The column $\sigma$ represents the average percentage change across all metrics compared to the original task.
%The column $\sigma$ is the percentage of the decline in average performance metrics compared to the original task.
}\vspace{0.3mm} 
\renewcommand{\arraystretch}{1.3}
\scalebox{1.4}{
\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Row}&\multirow{2}{*}{Type} & \multicolumn{2}{c}{LasHeR} & \multicolumn{2}{c}{VisEvent} & \multicolumn{3}{c}{DepthTrack} & \multirow{2}{*}{$\sigma$} \\
& & SR & PR & SR & PR & Pr & Re & F-score &  \\ \hline \hline
\#1 &Thermal & 0.564 & 0.703 & - & - & - & - & - & - \\
\#2 &Thermal-\textgreater{}Event & 0.481 & 0.610 & - & - & - & - & - & -8.8\% \\
\#3 &Thermal-\textgreater{}Depth & 0.439 & 0.581 & - & - & - & - & - & -12.3\% \\
\#4 &Depth & - & - & - & - & 0.630 & 0.625 & 0.628 & - \\
\#5 &Depth-\textgreater{}Thermal & - & - & - & - & 0.521 & 0.518 & 0.520 & -10.8\% \\
\#6 &Depth-\textgreater{}Event & - & - & - & - & 0.548	&0.551	&0.550  & -18.5\% \\
\#7 &Event & - & - & 0.607 & 0.773 & - & - & - & - \\
\#8&Event-\textgreater{}Thermal & - & - & 0.535 & 0.709 & - & - & - & -6.8\% \\
\#9 &Event-\textgreater{}Depth & - & - & 0.531 & 0.712 & - & - & - & -6.9\% \\ \hline
\end{tabular}}
\label{tab5}
\end{table*}

\subsection{Qualitative Evaluation}
\textbf{Qualitative analysis about Task-Customized Optimization Adapter.}
To evaluate the effectiveness of the Task-Customized Optimization Adapter in achieving modality-specific customization for various tasks, we conduct a qualitative analysis using selected sequences from three datasets. 
Specifically, we select the sequence "ab\_motocometurn1" from LasHeR dataset, the sequence "00197\_driving\_outdoor3" from VisEvent dataset, and the sequence "adapter01\_indoor" from DepthTrack dataset, as shown in Fig. \ref{fig:5}.
Max pooling emphasizes prominent responses in sparse signals, retaining the most significant local features, making it particularly effective for capturing sparsity, such as locally active areas in event streams. 
In contrast, average pooling calculates regional averages, smooths data, and reduces redundancy, making it well-suited for processing local geometric information. 
The combination of these two pooling operations can complement each other, enabling the extraction of sparse, significant features while preserving smooth global information.
As illustrated in Fig. \ref{fig:5}, it is evident that TCOA module effectively optimizes multi-modal features whether implemented with all linear layers or enhanced with average and max pooling.
In the sequence "ab\_motocometurn1", unlike the sparse characteristics of depth and event data, the combination of RGB-T features provides richer target information. 
Consequently, the TCOA module with linear layers is sufficient for RGB-T tracking.
Conversely, from sequences "00197\_driving\_outdoor3" and "adapter01\_indoor," it can be concluded that incorporating average pooling and max pooling for RGB-E and RGB-D tracking effectively filters redundant data, enhancing the compatibility of multi-modal data with RGB-based pre-trained models.
This visual analysis of different TCOA configurations across various tasks further confirms that employing modality-specific sub-adapters for thermal, depth, and event data improves the adaptation of multi-modal features to RGB-based pre-trained networks. 
%Furthermore, it enhances feature representation, ultimately boosting task performance.
%$The visual analysis of different forms of TCO for various tasks further verifies that employing specially customized adapters for thermal infrared, depth, and event data not only improves the modality adaptation of multi-modal data to RGB-based pre-trained networks but also enhances the representation of feature information.



\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{Jointentropy.pdf}
\caption{Illustraction of Single-Modal Information Entropy and Cross-Modal Joint Entropy.}
\vspace{-3mm}
\label{fig:7}
\end{figure}

\textbf{Qualitative analysis about tracking results.}
We compare the tracking results of UASTrack with state-of-the-art trackers in Fig. \ref{fig:6}. 
In the sequence "whitebag", our tracker achieves superior tracking accuracy, despite challenges such as a cluttered background and rainy weather conditions. 
Similarly, in the sequence "backpack\_indoor", where the target object undergoes significant appearance changes and contains similar object interference, other methods fail to maintain reliable tracking. Furthermore, in the sequence "video\_0079", previous trackers struggle to address the combined challenges of occlusion, fast motion, and interference from similar objects. 
In contrast, our UASTrack demonstrates stronger robustness and significantly improved performance compared to SOTA methods in these extreme scenarios.


\subsection{Exploration Analysis}
\textbf{Cross-modal dependency analysis.}
The differences in cross-modality transferability arise from the distinctive characteristics of each modality, such as intrinsic information content, sparsity, redundancy, and task alignment.
Table \ref{tab5} presents the results of exploring dependency and transferability across modalities by sending auxiliary features to other branches.
Depth demonstrates the lowest transferability to event and thermal modalities, with the largest negative changes observed: a decrease of -10.8\% in $\sigma$ when depth features are transferred to the thermal branch and -18.5\% when transferred to the event branch.
This indicates that depth suffers significant performance degradation when its features are utilized in other modalities.
Although event and thermal modalities exhibit better cross-modality robustness, they still experience notable reductions in SR and PR.
The thermal modality demonstrates moderate transferability, with $\sigma$ reductions of -8.8\% when transferred to the event branch and -12.3\% when transferred to the depth branch.
The event modality exhibits the highest transferability, with $\sigma$ reductions of less than 7\%, highlighting its comparative resilience during cross-modality transfer.

These findings suggest that depth data exhibit limited generalizability, likely attributable to their strong dependence on structural and geometric information.
In contrast, event data demonstrate greater generalizability, potentially due to their sparse and dynamic characteristics, which enable more flexibility across diverse tasks.
Thermal features demonstrate moderate transferability, occupying an intermediate position relative to depth and event data.



\textbf{Comparison of computation cost and speed.} 
Table ~\ref{tab7} compares speed, training parameters, training flops, and performance of our proposed UASTrack with state-of-the-art trackers, including separated training tracker TBSI and unified trackers ViPT, SDSTrack, and Un-Track.
UASTrack, like ViPT, Un-Track, and SDSTrack, leverages prompt or adapter learning for fine-tuning multi-modal tracking models, significantly reducing training parameters compared to TBSI, which depends on full fine-tuning and incurs considerably higher training parameters and flops.
%significantly fewer training parameters compared to TBSI, which relies on full fine-tuning and consequently incurs substantially higher training parameters and flops.
UASTrack achieves an inference speed of 44 FPS, outperforming ViPT, SDSTrack, and TBSI by 19.22, 29.1, and 12, respectively. 
Moreover, UASTrack demonstrates superior accuracy, achieving SR and PR of 57.0\% and 71.1\%. 
UASTrack requires only 1.87M training parameters and 1.95G training flops, saving up to 99\% of training parameters compared to the full fine-tuning model TBSI. 
Compared to Un-Track, a unified model and unified parameter tracker, UASTrack achieves notable advancements in tracking accuracy, speed, and computational efficiency.

\textbf{Evaluation of information richness and complementarity across modalities.}
The analysis of information entropy \cite{wang2024kcdnet} provides valuable insights into the complexity and redundancy of information across different modalities. 
Fig. \ref{fig:7} compares the single-modal entropy and cross-modal joint entropy to evaluate the information richness and complementarity across modalities. 
As illustrated in Fig. \ref{fig:7} (a), RGB images exhibit the highest single-modal entropy value of 5.34, indicating a greater level of information richness and complexity.
Thermal images follow with an entropy of 4.68, reflecting their capacity to capture thermal variations, though with slightly less information density than RGB images.
Depth and event modalities show lower entropy values of 3.76 and 3.48, respectively.

In Fig. \ref{fig:7}(b), in terms of cross-modal joint entropy, the RGB-T pairs exhibit a joint entropy of 3.41. 
This value indicates that RGB and thermal modalities share substantial mutual information, leading to a reduction in their combined uncertainty. 
Similarly, the RGB-E pairs have a joint entropy of 2.44, which is the lowest among the considered pairs, suggesting significant redundancy between these modalities, likely due to the structural alignment of motion information with RGB content.
The RGB-D pairs exhibit a joint entropy of 2.46, slightly higher than RGB-E, indicating a moderate level of complementary information between RGB and depth data.
Through entropy-based analysis, we further validate the necessity of employing task-customized strategies designed for specific task requirements in our UASTrack, which effectively integrates multiple modalities to optimize performance across diverse scenarios.
%Through entropy-based analysis, we further validate the soundness of employing customized adapter strategies tailored to specific task requirements in our approach, which effectively integrates multiple modalities to optimize performance across diverse scenarios. 
%This underscores the significance of developing task-specific strategies in the design of multi-modal systems.



%These results highlight the varying degrees of redundancy and complementarity among the modalities. RGB-T tracking, as the most information-rich task, contributes extensively to joint representations, while T, D, and E provide varying levels of complementary information. The low joint entropy values across all pairs emphasize the potential for effective cross-modal fusion strategies, leveraging mutual information to enhance multimodal representations.
%\subsection{Limitation Analysis}

% \begin{table*}[]
% \renewcommand{\arraystretch}{1.2}
% \centering
% \begin{tabular}{cccccccc}
% \hline
% \multirow{2}{*}{} & \multicolumn{2}{c}{LasHeR} & \multicolumn{2}{c}{VisEvent} & \multicolumn{3}{c}{DepthTrack} \\
%  & SR & PR & SR & PR & Pr & Re & F-score \\ \hline
% baseline & 0.482 & 0.609 & 0.588 & 0.754 & 0.577 & 0.582 & 0.579 \\
% + Modality-aware Classifier & 0.551 & 0.687 & 0.602 & 0.766 & 0.606 & 0.611 & 0.609 \\
% + Task Customization Optimization Adapter &0.564 &0.703 &0.605 & 0.770
% &0.622&0.615	&0.619 \\
% + Modality Classification Loss & \textbf{0.564} & \textbf{0.703} &0.607 &0.773 & \textbf{0.630} & \textbf{0.625} & \textbf{0.628} \\ 
% \hline
% \end{tabular}
% \caption{}
% \label{}
% \end{table*}

\section{Conclusion}
In this paper, we present a novel unified RGB-X tracker that incorporates modality-customization and adaptive selection in single object tracking. 
Specifically, we propose a Discriminative Auto-Selector to enable dynamic adaptation across various RGB-X tracking tasks.
Additionally, we introduce a Task Customization Optimization Adapter to facilitate task-specific customization, thereby enhancing the robustness and accuracy of the tracker.
Our approach not only bridges the gap between single-modality pre-training and multi-modal deployment but also establishes the first unified RGB-X tracker capable of operating without prior modality types. 
Experimental results demonstrate the effectiveness of our method, showing significant improvements over SOTA trackers in all RGB-X tracking scenarios.


\section*{Acknowledgments}
This work is supported in part by the National Key Research and Development Program of China (2023YFF1105102, 2023YFF1105105), the National Natural Science Foundation of China (Grant NO. 62020106012, 62332008, 62106089, U1836218, 62336004), the 111 Project of Ministry of Education of China (Grant No.B12018), and the UK EPSRC (EP/N007743/1, MURI/EPSRC/DSTL, EP/R018456/1).



% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}



% %{\appendices
% %\section*{Proof of the First Zonklar Equation}
% %Appendix one text goes here.
% % You can choose not to have a title for an appendix if you want by leaving the argument blank
% %\section*{Proof of the Second Zonklar Equation}
% %Appendix two text goes here.}



% \section{References Section}
% You can use a bibliography generated by BibTeX as a .bbl file.
%  BibTeX documentation can be easily obtained at:
%  http://mirror.ctan.org/biblio/bibtex/contrib/doc/
%  The IEEEtran BibTeX style support page is:
%  http://www.michaelshell.org/tex/ieeetran/bibtex/
 

% \section{Simple References}
% You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
%  (used to reserve space for the reference number labels box).

% \begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibliography{ref}

% \end{thebibliography}

\begin{IEEEbiographynophoto}{Zhangyong Tang} is now a Ph.D. student with the School of Internet of Things Engineering,
Jiangnan University. His research interests include multi-modal object tracking and deep learning.\end{IEEEbiographynophoto}

\newpage

%\section{Biography Section}
%If you have an EPS/PDF photo (graphicx package needed), extra braces are needed around the contents of the optional argument to biography to prevent the LaTeX parser from getting confused when it sees the complicated
 % $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
 % your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
 % simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




% \vfill

\end{document}


