\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)

% *** CITATION PACKAGES ***
%
\usepackage{cite}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amssymb}


% \theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}


% *** SPECIALIZED LIST PACKAGES ***
\usepackage{algorithm, multirow, xcolor}
\usepackage{algorithmicx}
\usepackage{algpseudocode}



% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}

% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi

% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}

%\usepackage{stfloats}

%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi


% *** PDF, URL, AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.

\usepackage{booktabs}
\usepackage{cancel}

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% \newcommand{\yujie}[1]{{\color{blue}  \textbf{[yujie: #1]}}}
% \newcommand{\wei}[1]{{\color{cyan}  \textbf{[wei: #1]}}}
\newcommand{\qrz}[1]{\textcolor{red}{\emph{[QRZhang: #1]}}}
\newcommand{\dyz}[1]{\textcolor{red}{\emph{#1}}}
% \newcommand{\dyz}[1]{#1}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to, and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Learning Efficient Flocking Control based on Gibbs Random Fields}
% \title{Flocking Control Learning based on Gibbs Random Fields}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
% \thanks{M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}
\author{Dengyu Zhang$^{*}$, Chenghao Yu$^{*}$, Feng Xue, and Qingrui Zhang
% $^{\dagger}$
% \thanks{Manuscript received: Month, Day, Year; Revised Month, Day, Year; Accepted Month, Day, Year.}%Use only for final RAL version
% \thanks{
% This paper was recommended for publication by Editor FirstName A. EditorName upon evaluation of the Associate Editor and Reviewers' comments.
% This work was supported by (organizations/grants which supported the work.)} %Use only for final RAL version
\thanks{This work is supported by supported by the Key-Area Research and Development Program of Guangdong Province under Grant 2024B1111060003, in part by the National Nature Science Foundation of China under Grant 62103451, Guang Dong Basic and Applied Basic Research Foundation  under Grant 2024A1515012408, and Shenzhen Science and Technology Program JCYJ20220530145209021. (Corresponding author: Qingrui Zhang, zhangqr9@mail.sysu.edu.cn)}
\thanks{$^{1}$All Authors are with School of Aeronautics and Astronautics, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, P.R. China. $^{*}$Equal contributions.  }%

% \thanks{$^{2} $SecondAuthor is with the School of Engineering, Automation Department, University of Anywhere, Anyland
%         {\tt\footnotesize second.author@papercept.net}}%
% \thanks{Digital Object Identifier (DOI): see the top of this page.}
}
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a %, and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\markboth{IEEE Robotics and Automation Letters. 
Manuscript.
%Preprint Version. % Accepted Month, Year
}
{Zhang \MakeLowercase{\textit{et al.}}: Learning Efficient Flocking Control based on Gibbs Random Fields} 

% The only time the second header will appear is for the odd-numbered pages after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if you desire.

% If you want to put a publisher's ID mark on the page you can do it like this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second column for its text to clear the IEEEpubid mark.

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% Make the title area
\maketitle

% As a general rule, do not put math, special symbols, or citations in the abstract or keywords.
\begin{abstract}
Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with {%improved flocking performance
success rate around $99\%$}, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.

% Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments, showcasing remarkably improved flocking performance, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.
% Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework developed based on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is characterized by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism is realized using a GRF-based credit assignment method, which enhances the scalability of MARL concerning robot quantity. An action attention module is proposed to implicitly anticipate the motion intentions of neighboring robots, thereby mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with remarkably improved flocking performance over existing solutions, as demonstrated through extensive comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.
% Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. In this paper, these challenges are addressed through a multi-agent reinforcement learning (MARL) framework developed based on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is modeled as random variables conforming to a specific joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism is realized using a GRF-based credit assignment method, which enhances the scalability of MARL concerning robot quantity. An action attention module is proposed to implicitly anticipate the motion intentions of neighboring robots, thereby mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with remarkably improved flocking performance over existing solutions, as demonstrated through extensive comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.

% Distributed flocking control is one of the fundamental behaviors to support their applications in many tasks, but it remains a challenge in distributed flocking to simultaneously ensure efficiency, optimality, and safety for large-scale multi-robot systems in congested environments. In this paper, a reinforcement learning framework based on Gibbs random field (GRF) for distributed flocking is proposed. We designed GRF-based credit assignment to develop a decentralized training and decentralized executing framework. In addition, to enhance multi-robot coordinated control performance, we propose an action attention structure to enable implicit approximation of neighbors' intentions based on mean field approximation.
% Simulation results show that our method is computationally efficient and has a high collision avoidance success rate in various environments and flocking scales. Real-world experiment results demonstrate the competence of our method.
% Despite its competence
% Multi-robot 
% Flocking control is a pivotal technique for multi-robot systems in various applications.

% Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework developed based on Gibbs Random Fields (GRFs).  With GRFs, a multi-robot system is modeled as a set of random variables satisfying a certain joint probability distribution, which offers a new perspective on flocking reward design. A decentralized training and execution mechanism is realized based on a GRF-based credit assignment method, enhancing the scalability of MARL concerning robot quantity. An action attention module is proposed to implicitly anticipate the motion intentions of neighboring robots, thus mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with remarkably improved flocking performance over existing solutions, as demonstrated through extensive comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.

% produces efficient flocking control learning for multi-robot systems in challenging environments. With the proposed design, an efficient distributed control policy is learned with remarkably improved flocking performance over existing solutions. This design 

% The advantages of the proposed design are illustrated via extensive comparisons with the state-of-the-art algorithms in both simulations and experiments. Ablation studies are performed to show the efficiency of different modules of the presented framework. 

% Flocking control is one of the crucial techniques to support applications of multi-robot systems in various industries. However, it is challenging to accomplish efficient multi-robot flocking control in terms of computation burdens, performance optimality, and motion safety. In this paper, a multi-agent reinforcement learning (MARL) framework is developed based on Gibbs Random Fields (GRFs) for distributed multi-robot flocking control in congested environments. With GRFs, a multi-robot system is modeled as a set of random variables satisfying a certain joint probability distribution, thus presenting a new perspective on the flocking reward design. A decentralized training and decentralized execution mechanism is realized based on a GRF-based credit assignment method, which enriches the scalability of MARL with respect to robot quantity. An action attention module is proposed to implicitly anticipate the motion intention of neighboring robots, which is conducive to mitigating the potential non-stationarity issue in MARL. With the proposed design, an efficient distributed control policy is learned with improved flocking performance over existing solutions. The advantages of the proposed design are illustrated via extensive comparisons with the state-of-the-art algorithms in both simulations and experiments. Ablation studies are performed to show the efficiency of different modules of the presented framework. 

% is presented for decentralized training of MARL, which ensures scalability with respect to robot quantity at the training stage. An action attention module is proposed to implicitly anticipate the motion intention of neighboring robots, which is conducive to mitigating the potential non-stationarity issue in MARL with decentralized training. 


% are anticipated based on 

% mitigating the potential non-stationarity issue. The

% and decentralized execution mechanism 


% is thereafter presented based on  which support fully distributed  is thereafter designed, which 

% Both the robot-to-robot and robot-to-environment interactions are formulated as certain potential energies. The interactions are efficient, safe, and distributed flocking control. 


% Flocking control is one of the crucial techniques to support applications of multi-robot systems in various industries. However, the existing solutions, include rule-based methods and optimization-based approaches.  Reinforcement learning (RL) provides a promising solution to multi-robot flocking in a model-free manner. Hence, a multi-agent RL (MARL) framework is developed based on Gibbs Random Fields to 

% enable-based solution is developed for multi-robot flocking based on Gibbs Random Fields. 


% In this paper, a reinforcement learning solution is developed for efficient multi-robot flocking control 

% % Reinforcement learning provides a promising solution to multi-robot flocking in a model-free manner. Hence, 
% based on Gibbs Random Fields (GRF)

% celebrated it remains a challenge for both rule-based methods and optimization-based approaches to balance support their applications in various, despite their extraordinary competence in various applications,  remains a challenging task in congested environments. Conventional non-solutions, \emph{e.g.} rule-based approaches and optimization-based methods, to balance requirements on computation burdens, performance optimality, and motion safety for both rule-based methods and optimization-based solutions. In this paper, 

% Existing methods, efficiency, optimality, and safety in congested environments.
% Distributed flocking control is one of the fundamental problems in multi-robot systems, yet achieving efficiency, optimality, and safety remains challenging. This paper presents a novel reinforcement learning framework based on Gibbs Random Fields (GRF) for distributed flocking. The proposed approach introduces a GRF-based credit assignment mechanism to enable decentralized training. To further enhance coordination among robots, an action attention structure is incorporated, allowing implicit anticipation of neighbor intentions. Simulations demonstrate that our method achieves high computational efficiency and collision avoidance performance across various environments and flocking scales. Real-world experiments further validate the effectiveness of the proposed approach, highlighting its potential for practical applications.
\end{abstract}

% Multi-robot flocking possesses extraordinary advantages over a single-robot system in diverse domains, but it is challenging to ensure safe and optimal performance in congested environments. Hence, this paper is focused on the investigation of distributed optimal flocking control for multiple robots in crowded environments. A heuristic predictive control solution is proposed based on a Gibbs Random Field (GRF), in which bio-inspired potential functions are used to characterize robot-robot and robot-environment interactions. The optimal solution is obtained by maximizing a posteriori joint distribution of the GRF in a certain future time instant. A gradient-based heuristic solution is developed, which could significantly speed up the computation of the optimal control. Mathematical analysis is also conducted to show the validity of the heuristic solution. Multiple collision risk levels are designed to improve the collision avoidance performance of robots in dynamic environments. The proposed heuristic predictive control is evaluated comprehensively from multiple perspectives based on different metrics in a challenging simulation environment. The competence of the proposed algorithm is validated via the comparison with the non-heuristic predictive control and two existing popular flocking control methods. Real-life experiments are also performed using four quadrotor UAVs to further demonstrate the efficiency of the proposed design.

% \begin{IEEEkeywords}
% IEEE, IEEEtran, journal, \LaTeX, paper, template.
% \end{IEEEkeywords}
\begin{IEEEkeywords}
% List of keywords (from the RA Letters keyword list)
Reinforcement learning, robot learning, Gibbs random field, multi-robot systems, flocking control
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
% The very first letter is a 2-line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
% \IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
\IEEEPARstart{M}{ulti-robot} systems have shown great advantages over a single robot in terms of efficiency, robustness, and flexibility. In multi-robot systems, flocking is one of the fundamental behaviors to support their applications in many tasks, \emph{e.g.}, %reconstruction and mapping\cite{liu_towards_2016}, 
search-and-rescue operations\cite{tolstaya_learning_2020} and communication services\cite{bejaoui_qos-oriented_2020}. 
The burgeoning applications in diverse tasks put great demands on computational efficiency, performance optimality, and motion safety for multi-robot flocking.
In comparison with centralized flocking, a distributed solution is computationally more efficient\cite{park_online_2022}. However, it remains a challenge to simultaneously ensure efficiency, optimality, and safety for distributed large-scale multi-robot flocking in congested environments.
% For instance, in light-weight robot flocking, computational efficiency is critical for deployment on limited resources and performance optimality is important to reduce energy consumption. 
% For another instance, in dense or agile flocking, delayed reactions caused by inefficient algorithms can be critical for safety. 
% Hence, many applications need efficient, optimal, and safe distributed flocking control methods, but it's challenging to obtain them at the same time.
% However, it's challenging to obtain efficiency, optimality, and safety at the same time.

Tremendous algorithms have been proposed for distributed flocking control in past decades, many of which are bio-inspired methods stemming from the seminal work by Reynolds \cite{reynolds_flocks_1987}. Bio-inspired methods are rule-based, for instance, separation rules for collision avoidance, cohesion rules for robot congregation, and alignment rules for motion consensus \cite{olfati-saber_flocking_2006, roy_neural_2020}.
% Separation, cohesion, and alignment are three common rules designed for collision avoidance, robot congregation, and motion consensus. 
Typically, those rules in bio-inspired methods are designed based on artificial potential fields or certain velocity vectors \cite{vasarhelyi_optimized_2018, guo_collision-free_2023, mcguire_Viscoelastic_2022}. 
Bio-inspired methods are computationally efficient, but fall short of meeting various performance optimality requirements, \emph{e.g.} flocking order and {motion} tracking, \emph{etc}. Their collision avoidance behaviors will experience significant performance degeneration with the increase of robot quantity in congested environments, thus posing a safety issue.
% The collision avoidance behaviors of bio-inspired methods will experience significant performance degeneration with the increase of robot quantity in congested environments, thus posing a safety issue.
% Also, sometimes these methods can't satisfy real-world deployment operational constraints, further weakening safety. 

% To improve performance optimality and motion safety, optimization-based flocking control methods, such as model predictive control (MPC), have been investigated extensively. 
In optimization-based methods, such as model predictive control (MPC), multi-robot flocking is formulated as an optimization problem, which is resolved online in an iterative way \cite{soria_predictive_2021}. Flocking control by MPC is, therefore, guaranteed to be optimal to some extent. Despite their advantages in performance guarantee, optimization-based methods are computationally expensive in general \cite{soria_distributed_2022}. To reduce the computational requirements, several distributed MPC (DMPC) methods have been proposed for flocking control \cite{soria_distributed_2022, lyu_multivehicle_2021
}. However, DMPC is perplexed by the decision mutual influence issue, in which the optimal decision of one robot depends on the optimization results of neighbor robots. 
% The decision mutual influence issue could be mitigated in the sacrifice of optimality by using assumed optimized results of neighbors, \emph{e.g.} optimized inputs from the last time step \cite{soria_distributed_2022}. 
The computation burden of DMPC is also proportional to the complexity of objective functions and system dynamics.
In addition, all MPC-based flocking control methods are model-based, so their performance in real systems is heavily affected by model accuracy.
% Although optimize-based methods have advantages in performance optimality, they are computationally expensive due to online optimization. 
% In recent years, several methods of distributed MPC (DMPC) based flocking control have been proposed\cite{soria_distributed_2022, lyu_multivehicle_2021}. 
% In DMPC, one common technique to compute the local controller is to introduce the hypothetical information of neighbors, which requires recursive replanning to achieve consistency (\emph{i.e.} multi-robot coordination). Some methods abandoned replanning so they barely satisfy the requirement of real-time computation\cite{soria_distributed_2022}, but suffer from the decision conflict issue. Some methods leverage globally shared parameters instead of replanning to form consensus\cite{lyu_multivehicle_2021}, breaking the requirement of distributed flocking. 
% optimize-based methods are facing significant challenges in obtaining both efficiency and optimality in a fully distributed manner.


\begin{figure}[tbp]
    \centering
    \includegraphics[width=1\linewidth]{images/top_7v7_2.pdf}
    \caption{{Two flocks, each with 7 drones, move in opposite directions and avoid collisions in shared space using the proposed RL-based flocking controller.
    }}
    \label{fig:top_figure}
\end{figure}



Learning-based methods have recently emerged as a promising alternative to potentially achieve efficient, optimal, and safe flocking control in complex scenarios \cite{roy_neural_2020}.  In particular, reinforcement learning (RL) is capable of learning a policy by maximizing a return function using data samples collected through interactions with environments. The learning process is performed offline, so no more online optimization is needed. Hence, RL-based flocking control is potentially more time-efficient than optimization-based methods. Both the optimality and safety metrics are ensured by maximizing a specially designed reward function at the training stage. 
% {In learning-based methods, optimality is obtained by offline optimization on policy network, so it is more efficient than online optimization on control actions \cite{huang_collision_2024}. }
% In learning-based methods, control actions are generated by a neural network that directly takes in robot observation, therefore, they are efficient online.
% The optimality is obtained by offline optimization on the policy network instead of online optimization on control actions. 
% Previous works have shown that learning-based methods are efficient enough for microcontroller's on-board implementation \cite{huang_collision_2024}.
% {Moreover, a sophisticated dynamics model is not necessary for learning-based methods, further enhancing efficiency.}
% Although learning-based methods can be efficient and policy optimal, they also encountered similar challenges in distributed multi-robot coordination. 
% {Theoretically, RL-based methods can obtain efficiency and optimality at the same time after policy iteration, however, in actual training, the best policy is difficult to train due to nonstationarity, especially in multi-robot scenarios\cite{wong_deep_2023}.}
% However, multi-robot flocking is a much more challenging task for RL than a single-robot problem. 
However, multi-robot flocking is much more challenging for RL than single-robot control. Firstly, robots in flocking need to actively interact with their surroundings, including neighboring robots and local environments, which significantly increases the difficulty of the reward design.  Secondly, the number of robots in flocking tends to change, so the current centralized training mechanism cannot be applied. Thirdly, the decision policies of robots, which keep updating at training, have a mutual influence on one another, leading to the nonstationarity issue.
% MARL would experience nonstationarity issues at training due to the 
% mutual influence among the decisions of robots. 
% It is necessary to properly characterize the interaction among robots.
% it is challenging to characterize robot-to-robot interaction and robot-to-environment interaction. 

In this paper, we aim to develop an RL-based algorithm for distributed multi-robot flocking control to ensure efficiency, optimality, and safety in congested environments. In the first contribution, we present a multi-agent reinforcement learning framework based on Gibbs random fields (GRFs) for flocking control. The maximization objective of the RL-based flocking is, therefore, modeled by a set of pairwise energy functions. The interaction between robots is modeled by undirected edges on a graph, while the robot-to-environment interaction is characterized using unary energy functions. An energy normalization technique is designed to reduce the nonstationarity issue caused by the dynamic changes in the interaction topology among robots at training.

In the second contribution, a decentralized training and decentralized execution (DTDE) mechanism is developed for distributed flocking control policy learning in light of a GRF-based credit assignment solution. 
% Based on a GRF, robots in flocking are modeled by a dynamic random field with a joint probability distribution. 
The overall flocking objective is decomposed into a series of individual reward functions for each robot according to their contributions to return maximization, so the policy optimization is conducted in a distributed manner for every robot using their own rewards. The learning process is, therefore, scalable with the change of robot quantity in flocking. 
% With DTDE, the training process can be performed in parallel on multiple computation platforms, thereby reducing the training time costs. 
% Training results also show that the proposed DTDE mechanism is more data-efficient than a centralized training and decentralized execution (CTDE) framework with different setups.

In the third contribution, we present an action attention module, which enables robots to implicitly anticipate the motion intention of neighbors. The implicit motion intention anticipation is achieved via exchanging previous action distribution with neighbors, in which a scaled dot-product attention mechanism is implemented \cite{vaswani_attention_2017}. With the action attention mechanism, a robot can make proactive reactions to the potential motion changes in its neighbor robots. The efficiency of the action attention is verified through simulations and experiments at various conditions.
% environment conditions and flocking scales.
% Simulation results illustrate that flocking has a high collision avoidance success rate (over 95\%) with our action attention mechanism in various environments and flocking scales, demonstrating the effectiveness of this structure.

% \hfill mds
%  
% \hfill August 26, 2015

\section{Related works}

Recently, GRFs have been applied to coordinating multi-robot behaviors \cite{yu_grf-based_2024,rezeck_chemistry-inspired_2022,zhu_heuristic_2024}. In GRFs, robots are represented by random variables that adhere to a joint distribution \cite{guo_collision-free_2023}. The interactions among robots are characterized by specific energy functions. The primary objective of GRF-based flocking control is to determine the optimal control actions by maximizing a posteriori distribution.  However,  GRF-based flocking control is computationally intensive due to the action optimization process. The computation burdens are partially alleviated by using a mean-field approximation technique for the case of small-scale flocking  \cite{koller_probabilistic_2009, fernando_online_2021}.  {Additionally, a heuristic predictive flocking control (HPFC) has been proposed to further decrease the computational demands in GRF-based flocking control by leveraging prior knowledge \cite{zhu_heuristic_2024}}. 
Despite these advancements, the existing GRF-based flocking control methods struggle to scale effectively with an increasing number of robots. Intensive communication remains essential during the online inference process, increasing the difficulty of real-time implementation.

% Recently, GRFs have been studied for achieving coordinated flocking control of multi-robot systems \cite{yu_grf-based_2024,rezeck_chemistry-inspired_2022,zhu_heuristic_2024}. In a GRF, a multi-robot system is formulated as a set of random variables satisfying a joint distribution, where the interactions are characterized by certain potential energy functions \cite{guo_collision-free_2023}. The objective of GRF-based flocking control is to infer the best control by maximizing a posteriori distribution. Both the optimization and inference processes are computationally intensive in GRF-based flocking control \cite{guo_collision-free_2023}. To mitigate the computation burdens, mean-field approximation has been introduced to simplify graph inference \cite{koller_probabilistic_2009}. This approximation reduces computational complexity, making real-time, online flocking control more feasible for small-scale flocking \cite{fernando_online_2021}.  Additionally, a heuristic method has been introduced to further reduce the computational demands in GRF-based flocking control by leveraging certain prior knowledge \cite{zhu_heuristic_2024}. 
% Despite these advancements, the existing GRF-based flocking control methods still face the computational cost issue for a large group of robots. Also, intensive iterative communication is indispensable during the online action inference.

% so the flocking   By framing the flocking task on a GRF, coordinated behaviors of multiple robots are guided by energy functions. 

% In the past two decades, Gibbs Random Fields (GRF) has been widely studied for achieving coordinated flocking control in multi-robot systems \cite{rezeck_chemistry-inspired_2022}. %\cite{rezeck_cooperative_2021, rezeck_chemistry-inspired_2022}.
% GRF-based control frameworks model robot-to-robot and robot-to-environment interactions on an undirected graph, where the objective of flocking is formulated using Gibbs potentials.
 % However, due to the nature of GRF, flocking control requires computationally intensive iterative processes for inference and optimization.
 %, which significantly hampers scalability and efficiency.

% {Introduce MARL}
As a promising alternative, multi-{agent} reinforcement learning (MARL) offers potentially more adaptable and scalable solutions regarding computational burdens, performance optimality, and motion safety. Early works primarily focused on direct applications of classical reinforcement learning algorithms, \emph{e.g.}, Q-learning % \cite{chen_conceptual_2016, hung_q-learning_2017}
\cite{hung_q-learning_2017}, and deep deterministic policy gradient \cite{wang_deep_2018}, \emph{etc}. These approaches only demonstrated their effectiveness in small-scale flocking scenarios with fewer than ten robots. To achieve better performance for large-scale flocking, a supervised learning framework is introduced \cite{roy_neural_2020}, which leverages centralized MPC as a teacher model.  Other modifications, such as 
% convolutional global state matrices \cite{bezcioglu_self-organised_2021} and 
graph attention mechanisms \cite{xiao_graph_2023}, have further enhanced the performance of MARL in terms of generalization and robustness. 
However, in these works, the coordination of robots depends on pre-defined training strategies without online intention recognition, reducing their flexibility in handling unpredictable environments.

% Recent advances in MARL have sought to address these scalability issues. For example, supervised learning frameworks that leverage centralized MPC as a teacher model have shown improvements in learning-based flocking control\cite{roy_neural_2020}. Additionally, innovations such as convolutional global state matrices \cite{bezcioglu_self-organised_2021} and graph attention mechanisms \cite{xiao_graph_2023} have further enhanced the performance of MARL in terms of generalization and robustness. 
% However, in these works, the coordination of robots relies on pre-defined training strategies without online intention recognition, reducing their flexibility in handling unpredictable environments.

% \begin{table}[htbp]
%     \centering
%     \color{red}
%     \caption{Typical flocking control methods.}
%     \begin{tabular}{c|c|c|c|c}
%         \toprule
%         Algorithm & Type & Efficiency & Performance & Safety \\
%         \midrule
%         PPO-AA (ours) & RL-based &  high  &  high  &  high  \\
%         PPO  &  RL-based  &  high  &  medium  &  medium \\ 
%         DMPC\cite{soria_distributed_2022} & MPC-based & low & high & high \\ 
%         HPFC\cite{zhu_heuristic_2024_TMech} & GRF-based & low & high & low \\ 
%         CFDC\cite{guo_collision-free_2023} & Rule-based & high & low & low \\ 
%         Vásárhelyi\cite{vasarhelyi_optimized_2018} & Rule-based & high & low & low \\ 
%         Olfati-saber\cite{olfati-saber_flocking_2006} & Rule-based & high & low & low \\ 
%         \bottomrule
%     \end{tabular}
%     \label{tab:comparison}
% \end{table}

\section{Preliminaries}\label{sec:prelim}

\subsection{Gibbs random field}\label{sec:gibbs-random-field}
% A Gibbs random field (GRF) models a set of random variables with a spatial Markov property and strictly positive joint probability density. A GRF 
A Gibbs random field (GRF) is represented by an undirected graph $\mathcal G = (\mathcal V, \mathcal E)$, where $\mathcal V = \{1,\ldots,n\}$ is the random variable set and $\mathcal{E} = \{(i,j)\in \mathcal{V} \times \mathcal{V} | j\in \mathcal{N}_i, i\in\mathcal{V}\}$ denotes the dependence among variables with $\mathcal{N}_i$ as the neighbor set of a variable $i$ \cite{koller_probabilistic_2009}. In a GRF, the joint probability of random variables $X = \{X_v\}_{v\in\mathcal V}$ is delineated as 
\begin{equation}
% P(X) = \frac{1}{Z} \prod\nolimits_{c \in \mathcal{C}\text{, }X_c\subseteq X} \phi_c (X_c)
P(X) = \left.\prod\nolimits_{c \in \mathcal{C}\text{, }X_c\subseteq X} \phi_c (X_c)\right/Z
	\label{gibbs_def}
\end{equation}
where $\mathcal C \subseteq \mathcal V$ is a clique set, $\phi_c$ is a potential function for a clique $c\in \mathcal C $, $Z = \sum_{X} \prod_{c \in \mathcal{C}\text{, }X_c\subseteq X} \phi_c (X_c)$ is a normalization constant. All robots in a clique $c$ are neighbors of one another.

When $P(X)$ obeys a Gibbs distribution, $\phi_c(X_c)$ has an exponential form of  $\phi_c(X_c) = \exp \{ - \psi_c(X_c) \}$ with $\psi_c(X_c)$ interpreted as free energy, so \eqref{gibbs_def} is thus written as
\begin{equation}
	P(X) 
 % = \frac{1}{Z} \exp \{ - \sum_{c\in \mathcal{C}} \psi_c(X_c)\} 
 % = \frac{1}{Z} \exp \{ -H(X) \}
	% \label{gibbs_distribution}
  = \left.\exp \{ -H(X) \}\right/Z
	\label{gibbs_distribution}
\end{equation}
where $H(X) = \sum_{c\in \mathcal{C}} \psi_c(X_c)$. The derivation of Gibbs formulae defining a GRF is from the physical fact that the interaction between particles is described by a potential. In flocking, each robot can be treated as a random particle, so all robots as a group satisfy a certain Gibbs distribution.

 % A GRF has been widely used in statistical mechanics to model a system of particles interacting in two-dimensional or three-dimensional space. 

% A GRF, which is widely used in statistical mechanics to model a system of particles, is derived based on the physical fact that the interaction between particles is described by a potential.

 % It has been widely used in statistical mechanics to model a system of particles interacting in a two-dimensional or three-dimensional lattice.

% The spatial Markov property allows one to ignore more distant information as soon as immediate local information is provided \cite{zhu_heuristic_2024}.   A GRF with $n$ particles is described by an undirected graph model $\mathcal G = (\mathcal V, \mathcal E)$, where $\mathcal V = \{1,\dots,n\}$ is the random particle set and $\mathcal{E} = \{(i,j)\in \mathcal{V} \times \mathcal{V} | j\in \mathcal{N}_i, i\in\mathcal{V}\}$ represents the relationship among particles with $\mathcal{N}_i$ denoting the neighborhood of a variable $i$. 


% Gibbs random field (GRF), as an important model in statistical mechanics, is used to characterize systems of particles
% perhaps surprising since the usual derivation of the Gibbs formula
% Defining a G.R.F. depends on the physical fact that the interaction between
% particles are described by a potential.

% Consider a set $\mathcal V = \{1,\dots,n\}$ of $n$ agents, in which each robot interacts with its neighbors. we define the neighborhood $\mathcal{N}_i$ of robot $i$ as the set of neighboring robots within radius $r_s$, so the interaction between any two robots is bidirectional (\emph{i.e.} $j\in\mathcal{N}_i \iff i\in\mathcal{N}_j$). Therefore, the interaction between robots can be described as $\mathcal{E} = \{(i,j)\in \mathcal{V} \times \mathcal{V} | j\in \mathcal{N}_i, i\in\mathcal{V}\}$.
% In GRF, the joint probability of random variables $X = \{X_v\}_{v\in\mathcal V}$ is characterized as functions over cliques in an undirected graph $\mathcal G = (\mathcal V, \mathcal E)$ as
% \begin{equation}
% 	P(X_1, \dots, X_n) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \phi_c (X_c)
% 	\label{gibbs_def}
% \end{equation}
% where $\mathcal C \subset \mathcal V$ is a clique that all elements of $\mathcal C$ are neighbors of each other. $\phi_c$ is the positive potential function associated with a clique. $Z = \sum_{X_1, \dots, X_n} \prod_{c \in C} \phi_c (X_c)$ is a normalizing constant.
% Such distribution is a Gibbs distribution. A \emph{clique potential} $\phi_c(X_c)$ is represented by
% an unconstrained form using a real-value energy function $\phi_c(X_c) = \exp \{ - \psi_c(X_c) \}$, 
% which ensures a positive probability and gives the joint an additive structure\cite{koller_probabilistic_2009}
% \begin{equation}
% 	P(X) = \frac{1}{Z} \exp \{ - \sum_{c\in C} \psi_c(X_c)\} = \frac{1}{Z} \exp \{ -H(X) \}
% 	\label{gibbs_distribution}
% \end{equation}
% where $H(X) = \sum_{c\in \mathcal{C}} \psi_c(X_c)$ is the \emph{free energy}, hereinafter referred to as Gibbs energy. 


% The visibility or interaction sphere of robot $i$ is defined as a region surrounded by a ball with a constant radius $r_s$ also called the perception radius for a robot in flocking. The neighborhood of robot $i$ is chosen to be the set of neighboring robots within its interaction sphere. Mathematically, the neighborhood of robot $i$ is denoted as $\mathcal{N}_i = \{j \in \mathcal{A} \vert \Vert \mathbf{p}_{ji} \Vert \leq r_s\text{, } j \neq i\}$, where $\mathbf{p}_{ji} = \mathbf{p}_j - \mathbf{p}_i$ is the relative position of robot $j$ to $i$. It is assumed that the interaction between any two robots is bi-directional, so there exists $j \in\mathcal{N}_i \Leftrightarrow i \in \mathcal{N}_j$. The interactions among robots in flocking are, therefore, characterized by an undirected graph $\mathcal{G}(\mathcal{A},\mathcal{E})$, where $\mathcal{A}$ is the robot set and $\mathcal{E} = \{(i,j) \in \mathcal{A} \times \mathcal{A}\vert j \in \mathcal{N}_i\text{, } \forall i\in \mathcal{A} \}$ is the set of interaction edges. The neighbor set $\mathcal{N}_i$ depends on the relative positions among robots, so $\mathcal{G}$ is dynamically changing.

% The virtual robot concept in \cite{olfati-saber_flocking_2006}, denoted as a $\beta$-robot, is borrowed to characterize static obstacles in the environment. The $\beta$-robot denotes the closest point to the robot $i$ on the surface of an obstacle, so it has no size. The set of static obstacles perceived by robot $i$ are specified as $\mathcal{O}_{s,i} = \{\beta\in\mathcal{O}_{s} \vert \Vert \mathbf{p}_{\beta}-\mathbf{p}_{i} \Vert \leq r_s \}$, where $\mathcal{O}_{s}$ is the set of all static obstacles and $\mathbf{p}_{\beta}$ is the position of the $\beta$-robot \cite{olfati-saber_flocking_2006}. With the $\beta$-robot concept, both regular and irregular static obstacles can be easily addressed in the same framework. Similar concepts can also be found in \cite{cole2018reactive}. The dynamic obstacles, which are mostly non-cooperative robots, are also called $\beta$-robot with a little abuse of notations. However, all dynamic obstacles are assumed to be enclosed by a ball with a radius $r_\beta$.
% The dynamic obstacle set observed by robot $i$ is denoted by $\mathcal{O}_{d,i} = \{\beta \in\mathcal{O}_{d}\vert \Vert \mathbf{p}_{\beta}-\mathbf{p}_{i} \Vert \leq r_s + r_\beta \}$ where $\mathcal{O}_{d}$ is the set of all dynamic obstacles and $\mathbf{p}_{\beta}$ is the position of the dynamic obstacle $d$. Hence, the set of all perceived obstacles by robot $i$ is given by $\mathcal{O}_{i}=\mathcal{O}_{s,i}	\cup \mathcal{O}_{d,i}$. The robot is represented by a ball of radius $r_c$, so the safety constraint for robot $i$ is given by $\Vert \mathbf{p}_{ij} \Vert > 2r_c$, $j \in \mathcal{N}_i$; $\Vert \mathbf{p}_{i\beta} \Vert > r_c$, $\beta \in \mathcal{O}_{s,i}$; $\Vert \mathbf{p}_{i\beta} \Vert > r_c + r_\beta$, $\beta \in \mathcal{O}_{d,i}$. 


% Gibbs random field (GRF) is a set of random variables with a spatial Markov property and strictly positive joint probability density. The spatial Markov property allows one to ignore more distant information as soon as immediate local information is provided \cite{rezeck2021flocking}. In GRF, the joint probability of random variables is characterized as functions over cliques in an undirected graph. Consider the undirected graph  $\mathcal{G}(\mathcal{A},\mathcal{E})$ as given in Subsection \ref{subsec:System_Modeling}. A node subset $\mathcal{Q} \subseteq \mathcal{A}$ is called a clique if all elements in $\mathcal{Q}$ are neighbors to one another. A GRF on $\mathcal{G}$ is, therefore, a collection of random variables $X = \{X_i\}_{i\in\mathcal{A}}$ indexed by $\mathcal{A}$, where $X_i$ is the spatial site of the $i$-th random variable in $\mathcal{A}$. Hence, an instance of $X$ is a spatial configuration of the GRF, which should obey the following Gibbs distribution. 
% \begin{equation}\label{eq:GibbsDistr}
%     p(X) = Z^{-1}\prod _{\mathcal{Q} \in \mathcal{C}} \varPsi _{\mathcal{Q}} (X_{\mathcal{Q}})
% \end{equation}
% where $\mathcal{C}$ is the set of cliques on the graph $\mathcal{G}$, $X_{\mathcal{Q}}$ denotes random variables over $\mathcal{Q}$, $\varPsi _{\mathcal{Q}}$ is a non-negative clique potential, and $Z = \sum _{X} \prod _{\mathcal{Q} \in \mathcal{C}} \varPsi _{\mathcal{Q}} (X_{\mathcal{Q}})$ is a normalization factor. 

% The clique potential $\varPsi _{\mathcal{Q}}$ has an exponential form of $\varPsi _{\mathcal{Q}}=\exp (-\psi _{\mathcal{Q}}(X_{\mathcal{Q}}))$ with $\psi _{\mathcal{Q}}(X_{\mathcal{Q}})$ representing the free energy of the spatial configuration of $X_{\mathcal{Q}}$. In flocking, such a configuration of $X_{\mathcal{Q}}$ corresponds to the position configuration of neighboring robots. Minimum free energy corresponds to the optimal or stable flocking behaviors of robots. Hence, GRF provides a powerful framework to characterize interactions among neighboring robots. As can be seen, the joint probability is inversely proportional to the free energy function $\psi _{\mathcal{Q}}(X_{\mathcal{Q}})$. Hence, minimizing free energy functions $\psi _{\mathcal{Q}}(X_{\mathcal{Q}})$ is equivalent to obtaining the maximum a posteriori possibility (MAP) of the random variables $X$ on a GRF. The predictive control in this paper is designed to minimize the free energy of the spatial configurations of robot flocks at a future time instant. The free energy functions $\psi _{\mathcal{Q}}(X_{\mathcal{Q}})$ is defined by the combination of interaction potentials for flocking, collision avoidance potentials, and control performance potentials, \emph{etc}.  

% Consider the undirected graph  $\mathcal{G}(\mathcal{A},\mathcal{E})$ as given in Subsection \ref{subsec:System_Modeling}. A node subset $\mathcal{Q} \subseteq \mathcal{A}$ is called a clique if all elements in $\mathcal{Q}$ are neighbors to one another. A GRF on $\mathcal{G}$ is, therefore, a collection of random variables $X = \{X_i\}_{i\in\mathcal{A}}$ indexed by $\mathcal{A}$, where $X_i$ is the spatial site of the $i$-th random variable in $\mathcal{A}$. Hence, an instance of $X$ is a spatial configuration of the GRF, which should obey the following Gibbs distribution. 
% Gibbs random field is an effective model to represent a swarm with undirected inner interaction. 
% Individuals are not fully connected with each other, every individual only interacts with their neighbors, 
% such interactions are represented by edges on an undirected graph $\mathcal G = (\mathcal V, \mathcal E)$.
% $X = \{X_v\}_{v\in\mathcal V}$ is a random field on $\mathcal G$, represented by a collection of random variables.
% $x = \{(x_1, \dots, x_n) : x_v \in \Lambda_v, v \in \mathcal V\}$ is a 
% state or configuration of the random field established by an instance of $X$. 
% $\mathcal N_v \in \mathcal V$ is the set of neighbors for $v$. 
% $\mathcal C \subset \mathcal V$ is a clique, all elements of $\mathcal C$ are neighbors of each other. 
% An undirected graphical model represents a distribution $P(X_1 ,\dots,X_n)$
% defined by an undirected graph $\mathcal G$, and a set of positive clique potential functions
% $\phi_c$ associated with cliques of $\mathcal G$, \cite{koller_probabilistic_2009} $s.t.$
% \begin{equation}
% 	P(X_1, \dots, X_n) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \phi_c (X_c)
% 	% \label{gibbs_def}
% \end{equation}
% where $Z = \sum_{X_1, \dots, X_n} \prod_{c \in C} \phi_c (X_c)$ is a normalizing constant.
% Such distribution is a Gibbs distribution. A \emph{clique potential} $\phi_c(X_c)$ is represented by
% an unconstrained form using a real-value energy function $\phi_c(X_c) = \exp \{ - \psi_c(X_c) \}$, 
% which ensures a positive probability and gives the joint an additive structure
% \begin{equation}
% 	P(X) = \frac{1}{Z} \exp \{ - \sum_{c\in C} \psi_c(X_c)\} = \frac{1}{Z} \exp \{ -H(X) \}
% 	\label{gibbs_distribution}
% \end{equation}
% where $H(X) = \sum_{c\in \mathcal{C}} \psi_c(X_c)$ is the \emph{free energy}, hereinafter referred to as Gibbs energy. 


\subsection{Partially observable Markov decision process}
In distributed flocking control, global information is unobservable for each robot. Hence, the coordination of robots in flocking is modeled as a decentralized partially observable Markov decision process (Dec-POMDP) \cite{oliehoek_concise_2016}, which is 
defined by a tuple $\langle \mathcal V, \mathcal S, \{\mathcal A_i\}, \mathcal T, \{\mathcal O_i\}, \mathcal Z, r, \gamma \rangle$, 
where $\mathcal V = \{1, \dots, n\}$ is the set of robots, $\mathcal S$ is the global state space, 
$\mathcal A_i$ is the action space of robot $i$, and $\mathcal O_i$ is the observation space for robot $i$, $\mathcal T: \mathcal S \times \prod_{\forall i\in\mathcal V} \mathcal A_i \times \mathcal S \rightarrow [0,1]$ defines the transition probability from the current state to the next state, $\mathcal Z: \mathcal S \rightarrow \prod_{\forall i\in\mathcal V} \mathcal O_i$ is the observation function, $r: \mathcal S \times \prod_{\forall i\in\mathcal V} \mathcal A_i \rightarrow \mathbb{R}$ is a reward function, and $\gamma \in [0,1)$ is a discount factor. A joint policy for all robots in flocking is defined as $\boldsymbol{\pi}=\prod_{\forall i\in\mathcal V}\boldsymbol{\pi}_i$, where $\boldsymbol{\pi}_i\left(\left.\boldsymbol{a}_i\right| \boldsymbol{o}_i\right)$ is a policy for robot $i$ with $\boldsymbol{a}_i\in \mathcal A_i$ and $\boldsymbol{o}_i\in \mathcal O_i$.  A return function is defined as an accumulation of future rewards, namely $G_t = \sum_{l=0}^{\infty} \gamma^l r_{t+l}$,
where $r_t$ is the reward at the time step $t$. 
The objective of Dec-POMDP is to find a policy $\boldsymbol{\pi}^*$ that maximizes a value function  $V^{\boldsymbol{\pi}}(\boldsymbol{s}_t) = \mathbb E[G_t | \boldsymbol{s}_t]$ or an action-value function
$Q^{\boldsymbol{\pi}}(\boldsymbol{s}_t, \{\boldsymbol{a}_{i,t}\}_{i}^{n}) = \mathbb E[G_t | \boldsymbol{s}_t,\{\boldsymbol{a}_{i,t}\}_{i}^{n}]$.

% Note that joint state $\boldsymbol{s}_t\in\mathcal S $ at time step $t$ can be taken as a realization of the random variable set $X$ in a GRF.

% each robot draws individual observations $z_i \in \mathcal{Z}_i$ according to observation function, 
% The process for a swarm to form a configuration can be described as a \emph{decentralized partially observable Markov decision process} (Dec-POMDP) \cite{oliehoek_concise_2016}, 
% defined by a tuple $\langle \mathcal V, \mathcal S, \{\mathcal A^i\}, \mathcal T, \mathcal Z, R, \mathcal O, \gamma \rangle$, 
% where $\mathcal V = \{1, \dots, n\}$ is the set of agents, $\mathcal S$ is the global state space, 
% $\mathcal A^i$ is the action space of agent $i$, and $\mathcal Z$ is the observation space for an agent.
% The transition function defining the next state distribution is given by 
% $\mathcal T : \mathcal S \times \prod_i \mathcal A^i \times \mathcal S \rightarrow [0,1]$.
% The reward function is $R: \mathcal S \times \prod_i \mathcal A^i \rightarrow \mathbb{R}$, 
% and the discount factor is $\gamma \in [0,1)$. The discounted return is $G_t = \sum_{l=0}^{\infty} \gamma^l r_{t+l}$,
% where $r_t$ is the joint reward at time step $t$. The joint state $s_t$ at time step $t$ is established by an instance of the random field $X$.
% The joint policy $\pi$ induces a value function $V^\pi(s_t) = \mathbb [G_t | s_t]$ and an action-value function
% $Q^\pi(s_t, a_t) = \mathbb E[G_t | s_t, a_t]$, where $a_t$ is the joint action.

% At each time step $t$, the state $s_{t, i}$ of agent $i$ consists of its position $\boldsymbol p_i$, velocity $\boldsymbol v_i$, and acceleration $\boldsymbol a_i$, namely $o_{i} = (\boldsymbol p_i, \boldsymbol v_i, \boldsymbol a_i)$. Agent $i$ can take instant action $\boldsymbol {a}_{t, i}$ on its acceleration. The agents are modeled as rigid bodies with second-order translational dynamics.
% where $\boldsymbol p_i \in \mathbb{R}^{2}/\mathbb{R}^{3}$ is the position vector, $\boldsymbol v_i\in \mathbb{R}^{2}/\mathbb{R}^{3}$ is the velocity vector, and $\boldsymbol{a}_i\in \mathbb{R}^{2}/\mathbb{R}^{3}$is the acceleration of robot $i$.
\subsection{Robot dynamics and observations}
% The robots in flocking are assumed to have homogeneous second-order dynamics with its motion updated by changing the acceleration.   Hence, the robot model is given by
The robots in flocking are assumed to have homogeneous second-order dynamics given by $\dot{\boldsymbol p}_i = \boldsymbol v_i$, $\dot{\boldsymbol v}_i = \boldsymbol a_i$, 
where $\boldsymbol p_i$ is the position vector, $\boldsymbol v_i$ is the velocity vector, and $\boldsymbol{a}_i$ is the acceleration of robot $i$. For simplicity, the flocking behavior in two-dimensional space is considered, so $\boldsymbol p_i$, $\boldsymbol v_i$, $\boldsymbol{a}_{i} \in \mathbb{R}^{2}$. However, the proposed algorithm can be extended to a three-dimensional case with nearly no modifications.

In our proposed design{, robots can only change their accelerations, so the action space is represented by a set of acceleration vectors.} The action space is hybrid with $\boldsymbol a_i$ from either a discrete action set or a continuous policy set. Let $\mathcal{A}_i$ be the hybrid action space for robot $i$ with $ \mathcal{A}_i = \mathcal{A}_{i,d} \cup \mathcal{A}_{i,c}$, where $\mathcal{A}_{i,d}$ denotes the discrete action set and $\mathcal{A}_{i,c}$ is the continuous policy. The discrete action set $\mathcal{A}_{i,d}$ is obtained by 
% The discrete action set $\mathcal{A}_{i,d}$ is obtained by dividing the original action space of the robot model \eqref{eq:robotModel} according to certain rules, which are given by
\begin{equation*}
\mathcal{A}_{i,d} = \{\boldsymbol{a}_{i} = M \boldsymbol{e} | M \in \left\{M_1, \ldots, M_m\right\}, \boldsymbol{e} \in\left\{\boldsymbol{e}_1, \ldots, \boldsymbol{e}_{l}\right\} \}
\end{equation*}
where $M$ is the action magnitude, and $\boldsymbol{e}$ is the action direction.
The continuous policy set $\mathcal{A}_{i,c}$ contains two simple rules to enhance motion smoothness, so $\mathcal{A}_{i,c}=\{\boldsymbol{a}_{i,s}, \boldsymbol{a}_{i,f}\}$. The first rule $\boldsymbol{a}_{i, s} = -\boldsymbol{v}_i$ is used to decelerate a robot in a more smooth manner. The second rule $\boldsymbol{a}_{i, f}=\boldsymbol{v}_c - \boldsymbol{v}_i$ regulates the velocity tracking performance, where $\boldsymbol{v}_c$ is the desired flocking velocity. Other rules could be added to $\mathcal{A}_{i,c}$, if necessary.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{images/action_and_obstacle.pdf}
    \caption{{(a): Discrete action set $\mathcal{A}_{i,d}$ indicating acceleration vectors that robots can choose. (b): Obstacle observation $\boldsymbol{o}_{i,o}$ consists of the distances to obstacles in $l$ evenly divided sectors. The distances are defined as the minimum radius of the sector that doesn't cover any obstacles.}
    }
	\label{fig:perception}
\end{figure}
% In the hybrid action space, $\mathcal{A}_{i,d}$ represents the discrete action set, 


% The hybrid action space of robot $i$ is designed in discrete parts and continuous parts.
% The discrete part is vectors at several directions with several magnitudes, as shown in Fig. \ref{fig: perception}(b).
% % \begin{equation}
% % \mathcal{A}_{i,d} = \{\boldsymbol{a}_{i, m,\alpha} = M \boldsymbol{e} | M = M_1, \dots, M_m, \boldsymbol{e} = \boldsymbol{e}_0, \dots, \boldsymbol{e}_{\alpha}\}
% % \end{equation}
% where $M_m \in \{0.05, 0.1, 0.2, 0.5\}$, $\boldsymbol{e}_{l} = [\cos \alpha_l \ \sin \alpha_l]^\top$ is the unit vector at direction $\alpha_l=2 \pi k/l, k=1, ..., l $.
% The continuous part contains two simple rule, namely $\mathcal{A}_{i,c}=\{\boldsymbol{a}_{i}^{s}, \boldsymbol{a}_{i}^{f}\}$. $\boldsymbol{a}_{i}^{s} = -\boldsymbol{v}_i$ is the slowing acceleration of robot $i$ so robot can slow down precisely if necessary. $\boldsymbol{a}_{i}^{f}=\boldsymbol{v}_c - \boldsymbol{v}_i$ is the following acceleration to smoothly keep a constant speed, where $\boldsymbol{v}_c$ is the commanded velocity.
%where $\boldsymbol{v}_c$ is the commanded velocity and $\boldsymbol{p}_c$ is the commanded position. 
% $a_{i}^{s}$ is the slowing acceleration
% discretized as   $\mathcal{A}_i=\{\boldsymbol{a}_{i}^{slow}, \boldsymbol{a}_{i}^{follow}, \boldsymbol{0}, \{\boldsymbol{a}_{m, d}\}\}$.
% Denote the commanded velocity as $\boldsymbol{v}_c$ and the commanded position as $\boldsymbol{p}_c$.
% The second part is $\mathcal{A}_{i,2} = \{\boldsymbol{a}_{1, 1}, ..., \boldsymbol{a}_{n_m, n_d}\}$
% $\mathcal{A}_i=\{\boldsymbol{a}_{i}^{slow}, \boldsymbol{a}_{i}^{follow}, \boldsymbol{a}_{0}, \boldsymbol{a}_{1, 1}, ..., \boldsymbol{a}_{n_m, n_d}\}$ contains a slowing acceleration $\boldsymbol{a}_{i}^{slow}=-\boldsymbol{v}_i$, a following acceleration $\boldsymbol{a}_{i}^{follow}=(\boldsymbol{v}_c - \boldsymbol{v}_i)$, a zero acceleration $\boldsymbol{a}_{0}=\boldsymbol{0}$, and discrete accelerations at $n_{a,d}$ directions of $n_{a, a}$ magnitudes defined by
% \begin{equation}
% \boldsymbol{a}_{m, d} = M_m \boldsymbol{e}_d
% \end{equation}
% where $M_m \in \{0.05, 0.1, 0.2, 0.5\}$, $\boldsymbol{e}_d$ is the unit vector at direction defined by $2 \pi k/n_d, k=1, ..., n_d $, as shown in Fig. \ref{fig:perception}.
% The action space of robot $i$ is $\mathcal{A}_i = \mathcal{A}_{i,d} \cup \mathcal{A}_{i,c}$. 
% $\eta$ is a noise that follows a normal distribution $\eta \sim N(0,\sigma)$. 
 % , which can be decomposed into thrust $T$ and ZYX euler angles (roll angle $\phi$ and pitch angle $\theta$) \cite{greiff_modelling_2017}. 
%\begin{equation}
%	\boldsymbol R_B^G \boldsymbol T = \left[\begin{array}{ccc}
%		\cos\theta &\sin\theta \sin\phi &\sin\theta \cos\phi \\
%		0 &\cos\phi &- \sin\phi \\
%		-\sin\theta &\sin\phi \cos\theta &\cos\phi \cos\theta
%	\end{array}\right] \left[\begin{array}{c}
%	0 \\ 0 \\ T
%	\end{array}\right]
%\end{equation}
% The process of forming a swarm configuration can be described as a Dec-POMDP. 
The neighborhood for robot $i$ is defined by $\mathcal{N}_i = \{j | \Vert \boldsymbol{p}_i - \boldsymbol{p}_j \Vert \leq d_{in}, j\in \mathcal{V}\; \& \; j\neq i\}$, where $d_{in}$ is the interaction range. 
{The observation space of a robot is denoted as $\boldsymbol{o}_i = \{\boldsymbol{o}_{ii}, \{\boldsymbol{o}_{ij}\}_{j\in \mathcal{N}_i}, \{\boldsymbol{A}_{j}\}_{j\in \mathcal{N}_i}, \boldsymbol{o}_{io}\}$, where $\boldsymbol{o}_{ii}= \{\boldsymbol p_i, \boldsymbol v_i, \boldsymbol{p}_c, \boldsymbol{v}_c\}$  contains position and velocity of robot $i$ ($\boldsymbol p_i, \boldsymbol v_i$), and the desired position and velocity of the flock ($\boldsymbol{p}_c, \boldsymbol{v}_c$)}. $\boldsymbol{o}_{ij} = \{\boldsymbol p_j - \boldsymbol p_i, \boldsymbol{v}_j - \boldsymbol{v}_i\}$ denotes the relative state of a neighbouring robot $j$ to robot $i$. $\boldsymbol{A}_{j}$ is the action distribution of a neighbouring robot $j$ at the last timestep, and $\boldsymbol{o}_{io}= \{d_{i,1}, ..., d_{i,l}\}$ specifies the distances to obstacles in $l$ different directions. The obstacles are detected in a similar way as LiDAR. It is assumed that robot $i$ has the omnidirectional visibility of surrounding obstacles, so the obstacle detection has a span of $2\pi$ (360 degrees). We divide the detection span evenly into $l$ sectors, so $d_{i, k}$ represents the closest obstacle in the $k$-th sector with $d_{i, k}<d_{o, max}$, where $d_{o, max}$ specifies the maximum detection range. The obstacle detection mechanism is shown in Fig. \ref{fig:perception}(b).
% The For the detection of the obstacle distance $d_{i, k}$ with $k=1$, $\ldots$, $l$, the azimuth angle of the obstacle capability  The distance $d_{i, k}$ in $k$-th direction 
% measured by its proprioceptive and exteroceptive sensors, the states of its neighbors $\boldsymbol{o}_{ij} = \{\boldsymbol p_j - \boldsymbol p_i, \boldsymbol{v}_j - \boldsymbol{v}_i\}$ at timestep and the action distribution $\boldsymbol{A}_{j}$ of its neighbors at the last timestep.
% %We will further discuss the neighbor distribution in Sec. \ref{sec:action_attention}. 
% In detail, $\boldsymbol{o}_{ii}= \{\boldsymbol p_i, \boldsymbol v_i\}$ contains the position $\boldsymbol p_i$ and velocity $\boldsymbol v_i$ of robot $i$. $\boldsymbol{o}_{io}$ is the distances  to obstacles at $l$ directions defined as $\{d_{i,1}, ..., d_{i,l}\}$, a

% s shown in Fig. \ref{fig:perception}(a). Distance $d_{i, k}$ for robot $i$ in direction $k$ can be formulated as
% \begin{equation}
% 	\nonumber
% 	\mathcal{C}_k = \left\{\boldsymbol d = \left[\begin{array}{c}
% 	d\cos\theta\\ d\sin\theta\end{array}\right] \vert d\in (0, d_m] % \mathbb{R}^+
%  , 
% 	\theta\in \left[ \frac{2\pi (k-1)}{l}, \frac{2\pi k}{l}\right)\right\}
% \end{equation}
% \begin{equation}
% 	d_{i, k} = \min \left\{\Vert \boldsymbol{p}_o - \boldsymbol{p}_i \Vert \big\vert (\boldsymbol{p}_o - \boldsymbol{p}_i) \in \mathcal{C}_k\right\}
% \end{equation}
% The reward is represented by the joint probability density of the flocking, $r_t = P(s_t)$.




% The observation space of a robot is denoted as $\boldsymbol{o}_i = \{\boldsymbol{o}_{ii}, \{\boldsymbol{o}_{ij}\}_{j\in \mathcal{N}_i}, \{\boldsymbol{A}_{j}\}_{j\in \mathcal{N}_i}, \boldsymbol{o}_{io}\}$, where $\boldsymbol{o}_{ii}= \{\boldsymbol p_i, \boldsymbol v_i\}$ are states of robot $i$,  $\boldsymbol{o}_{ij} = \{\boldsymbol p_j - \boldsymbol p_i, \boldsymbol{v}_j - \boldsymbol{v}_i\}$ denotes the relative state of a neighbouring robot $j$ to robot $i$, $\boldsymbol{A}_{j}$ is the action distribution of a neighbouring robot $j$ at the last timestep, and $\boldsymbol{o}_{io}= \{d_{i,1}, ..., d_{i,l}\}$ specifies the distances to obstacles in $l$ different directions. The discrete action set $\mathcal{A}_{i,d}$ is obtained by dividing the original action space into accelerations of $m$ magnitudes in $l$ directions.

% \begin{table}[htbp]
%     \centering
%     \caption{Symbols}
%     \begin{tabular}{c|c}
%         \toprule
%          $\mathcal{G}$& graph \\
%          $\mathcal{V}$& the set of robots\\
%          $\mathcal{E}$ & edges on graph \\
%          $n$ & number of variables or robots \\
%          $\mathcal{N}_i$ & neighbor set \\
%          $X$ & joint variables \\
%          $X_v$, $X_c$ & a random variable \\
%          $P(X)$ & joint distribution\\
%          $Z$ & normalize constant\\
%          $\phi_c$ & clique potential\\
%          $\mathcal{C}$ & the set of cliques\\
%          $\psi_c$ & clique energy\\
%          $H(X)$ & total energy\\
%          $|\mathcal{V}|$ & number of robots \\
%          $|\mathcal{E}|$ & number of edges \\
%          $X_{\backslash i}$ & random variables except $i$ \\
%          $Q(X_i)$ & marginal distribution \\
%          $Z_i$ & normalize constant \\
%          \midrule
%          $\mathcal{S}$ & state space\\
%          $\{\mathcal{A}_i\}$ & action space\\
%          $\mathcal{T}$ & transition function\\
%          $\{\mathcal{O}_i\}$ & observation space\\
%          $\mathcal{Z}$ & observation function\\
%          $r$ & reward\\
%          $\gamma$ & discount factor\\
%          $\boldsymbol{\pi}$ & joint policy\\
%          $\boldsymbol{\pi}_i$ & policy of robot $i$\\
%          $\boldsymbol{a}_i$ & action of robot $i$\\
%          $\boldsymbol{o}_i$ & observation of robot $i$\\
%          $G_t$ & return at time step $t$\\
%          $r_t$ & reward at time step $t$\\
%          $t$ & time step\\
%          $\boldsymbol{\pi}^*$ & optimal policy\\
%          $V^{\boldsymbol{\pi}}$ & value function\\
%          $Q^{\boldsymbol{\pi}}$ & action-value function\\
%          $\boldsymbol{s}_t$ & joint state at $t$\\
%          $r_i$ & individual reward \\
%          \midrule
%          $\boldsymbol{p}_i$ & position of $i$\\
%          $\boldsymbol{v}_i$ & velocity of $i$\\
%          $\mathcal{A}_{i,d}$ & discrete action space\\
%          $\mathcal{A}_{i,c}$ & continuous action space\\
%          $M$ & magnitude of acceleration\\
%          $\boldsymbol{e}$ & unit vector at a direction\\
%          $\boldsymbol{a}_{i,s}$ & slowing acceleration\\
%          $\boldsymbol{a}_{i,f}$ & following acceleration\\
%          $\boldsymbol{v}_c$ & desired velocity\\
%          $d_{in}$ & interaction radius\\
%          $\boldsymbol{o}_i$ & observation \\
%          $\boldsymbol{o}_{ii}$ & self observation\\
%          $\boldsymbol{o}_{ij}$ & neighbor observation\\
%          $\boldsymbol{o}_{io}$ & obstacle observation\\
%          $\boldsymbol{A}_{j}$ & neighbor action distribution\\
%          $d_{i,l}$ & obstacle distance at direction $l$\\
%          $d_{o, max}$ & maximum perception range of obstacle\\
%          \bottomrule
%     \end{tabular}
%     \label{tab:symbols_prob}
% \end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{Symbols}
%     \begin{tabular}{c|c}
%         \toprule
%          $\psi_i$ & unary energy\\
%          $\psi_{ij}$ & pairwise energy\\
%          $\psi_{ij,p}$ & position alignment energy\\
%          $\psi_{ij,v}$ & velocity alignment energy\\
%          $\psi_{i,k}$ & motion smoothness energy\\
%          $\psi_{i,c}$ & control effort\\
%          $\psi_{i,t}$ & position tracking energy\\
%          $\psi_{i,o}$ & obstacle avoidance energy\\
%          $\psi_{i,b}$ & brake energy\\
%          $c_{p1}$,$c_{p2}$ & position alignment coefficient\\
%          $d_{ij}$ & distance between robots\\
%          $d_{r}$ & desired robot-to-robot distance\\
%          $c_v$ & velocity alignment coefficient \\
%          $\boldsymbol{v}_{ji}$ & relative velocity \\
%          $\boldsymbol{p}_{ji}$ & relative position \\
%          $c_k$ & kinetic coefficient \\
%          $c_{t1}$, $c_{t2}$ & tracking coefficient \\
%          $c_{o1}$, $c_{o2}$ & obstacle avoidance coefficient \\
%          $d_{io}$ & distance to obstacle \\
%          $d_{or}$ & reaction distance to obstacle \\
%          $c_b$ & brake energy coefficient \\
%          $\boldsymbol{v}_{oi}$ & relative velocity of obstacle \\
%          $\boldsymbol{p}_{oi}$ & relative position of obstacle \\
%         \midrule
%         $\boldsymbol{q}_j$ & query \\
%         $\boldsymbol{k}_j$ & key \\
%         $\boldsymbol{l}_j$ & value \\
%         $d_k$ & dimension of key \\
%         $\alpha_j$ & attention weight \\
%         $\boldsymbol{c}_i$ & weighted neighbor embedding \\
%         $\boldsymbol{e}_i$ & self observation embedding \\
%         $\boldsymbol{e}_j$ & neighbor observation embedding \\
%         $f$ & a 2-layer MLP with ReLU \\
%         $A_i'$ & new action distribution \\
%         \midrule
%         $\beta$ & learning rate \\
%         $n_b$ & batch size \\
%         $n_m$ & minibatch size \\
%         $n_p$ & epochs \\
%         $c_e$ & entropy coefficient \\
%         $L_c$ & critic loss \\
%         $L_a$ & actor loss \\
%         $L_{clip}$ & clipped surrogate objective \\
%         $k$ & episode index \\
%         $D_k$ & buffer for one episode \\
%         $T$ & length of a episode \\
%         $n_{s,max}$ & maximum training steps \\
%         $step$ & training step index \\
%         \midrule
%         $n_{o,max}$ & maximum obstacles at training \\
%         $v_{max}$ & maximum speed at training \\
%         $v_{c,max}$ & maximum desired speed \\
%         $\Delta t$ & step length \\
%         $v_{coll}$ & speed in collision radius \\
%         $\Phi_{order}$ & flocking order metric \\
%         $\Phi_{tracking}$ & tracking performance metric \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:symbols_method}
% \end{table}

% A GRF is utilized to characterize the robot-to-robot interactions and robot-to-environment interactions. 
\section{Methodology}\label{sec:method}
% {
% In this section, an RL-based algorithm is developed for flocking control in congested environments. Initially, we designed a GRF-based flocking reward function to train optimal policies. 
% Subsequently, we introduce a credit assignment mechanism that effectively distinguishes the contribution of each robot, facilitating a decentralized training mechanism. To further enhance the policy learning process, we devise an action attention structure within the policy network, enabling it to implicitly anticipate motion intentions. Ultimately, the policy network is trained in a decentralized manner using Proximal Policy Optimization (PPO).}

{In this section, an RL-based algorithm is designed for flocking control in congested environments. The optimal control is learned by maximizing a return function of Dec-POMDP, which corresponds to minimizing the total free energy in the GRFs. 
Hence, different free energy terms are presented, which is followed by the construction of a GRF.
A credit assignment mechanism is presented for decentralized training. An action attention structure is eventually developed for the policy network to achieve implicit motion intention anticipation. }

\subsection{Flocking reward}
\label{sec:flocking_reward}
The interactions in flocking are modeled as energy functions in a GRF. Two types of energy functions are rendered: pairwise energy for robot-to-robot interactions and unary energy for robot-to-environment interactions, so the total free energy is 
\begin{equation}
\label{eq:singleton_pairwise_potential}
    H(X) = \sum\nolimits_{i \in \mathcal V} \psi_i(\boldsymbol{o}_{i}, \boldsymbol{a}_{i}) + \sum\nolimits_{(i,j)\in \mathcal E} \psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})
\end{equation}
where $\psi_i(\boldsymbol{o}_{i}, \boldsymbol{a}_{i})$ is unary energy, $\psi_{ij}(\boldsymbol{o}_{i}\boldsymbol{o}_{j})$ is pairwise energy. In \eqref{eq:singleton_pairwise_potential}, $\boldsymbol{o}_i$ are regarded as random variables in GRFs, while $\boldsymbol{a}_i$ are taken as parameters.

% Here we design unary energy and pairwise energy in \eqref{eq:singleton_pairwise_potential} to describe the optimal configuration of the flocking, namely $s^* = \mathrm{argmin}_{s \in \mathcal S} H(s)$.
% \begin{equation}
% s^* = \mathrm{argmin}_{s \in \mathcal S} H(s)
% \label{eq:swarm_optimal}
% \end{equation}

Pairwise energy $\psi_{ij}$ contains position alignment energy and velocity alignment energy to ensure cohesion, separation, and alignment in flocking, which is defined as 
% For robots $i$ and $j$ with $j\in\mathcal{N}_i$, the pairwise energy is defined as 
\begin{equation}
	\psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j}) = \psi_{ij,p}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j}) + \psi_{ij,v}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})
\end{equation}
where $\psi_{ij,p}$ is the position alignment energy, $\psi_{ij, v}$ is the velocity alignment energy. Here we choose the Morse potential \cite{morse_diatomic_1929} as the position alignment energy to ensure flocking cohesion and separation, so 
\begin{equation} 
\psi_{ij,p} (\boldsymbol{o}_{i},\boldsymbol{o}_{j}) = c_{p1}\left[1 - \exp(-c_{p2}(d_{ij}-d_{r}))\right]^2 - c_{p1}
\label{position_alignment_energy}
\end{equation}
where $c_{p1}, c_{p2} > 0$ are coefficients, $d_{ij}=\Vert \boldsymbol p_i - \boldsymbol p_j \Vert$ is the distance between two robots with $d_{r}$ as the desired value. The energy $\psi_{ij,p}$  has three properties: 1) $\psi_{ij,p}$ is minimum at $d_{ij}=d_{r}$; 2) $\psi_{ij,p}(d_{r}) = -c_{p1} < 0$; 3) $\forall \epsilon \in (0$, $ d_{r})$, $\left.\psi_{ij,p}\right|_{d_{ij}=d_{r}-\epsilon} > \left.\psi_{ij,p}\right|_{d_{ij}=d_{r}+\epsilon}$.
% \begin{enumerate}
%     \item $\psi_{ij,p}$ is minimum at $d_{ij}=d_{r}$;
%     \item $\psi_{ij,p}(d_{r}) = -c_{p1} < 0$;
%     \item $\forall \epsilon \in (0$, $ d_{r})$, $\left.\psi_{ij,p}\right|_{d_{ij}=d_{r}-\epsilon} > \left.\psi_{ij,p}\right|_{d_{ij}=d_{r}+\epsilon}$.
% \end{enumerate}
 % $c_{p1}$ is the dissociation energy from the energy minimum, while $c_{p2}$ is the Morse parameter\cite{desai_new_2020}.
% The first property indicates that $d_{r}$ is the desired distance. The second property encourages robots to find neighbors. The third property ensures that repulsion is prioritized over attraction. 
% Velocity matching is one of the flocking rules proposed by Reynolds. 
% For velocity alignment, 
% % a simple velocity alignment energy can be formulated as a monotonically increasing function on velocity error (\emph{e.g.}, $\psi_{ij,v} = c_v \Vert \boldsymbol v_i - \boldsymbol v_j \Vert^2$). Here 
% we design a velocity alignment energy based on the projection of velocity.

The velocity alignment energy $\psi_{ij,v}$ is designed as
\begin{equation} 
\psi_{ij,v} (\boldsymbol{o}_{i}, \boldsymbol{o}_{j}) = c_v \max\left(0, -\boldsymbol v_{ji}^\top \boldsymbol p_{ji} / \Vert \boldsymbol p_{ji} \Vert^2 \right)
\label{velocity_alignment_energy}
\end{equation}
where $c_v>0$ is a coefficient, $\boldsymbol v_{ji}=\boldsymbol{v}_j - \boldsymbol{v}_i$ is the relative velocity, and $\boldsymbol p_{ji}=\boldsymbol{p}_j - \boldsymbol{p}_i$ is the relative position between robots. The velocity alignment energy is designed to minimize the velocity in the direction of relative position. 
% Moreover, given a certain relative velocity, this energy increases if robots get closer to each other, which reduces limitations for robots that are not too close. 
% This design is inspired by the braking curve \cite{vasarhelyi_optimized_2018}. 

Unary energy $\psi_i$ is given by a sum of several terms as
% contains free energy functions for motion smoothness, control optimization, position tracking, obstacle avoidance, and collision aversion. 
% so it is given by
\begin{equation}
	\psi_i(\boldsymbol{o}_{i}, \boldsymbol{a}_i) = \psi_{i,k} + \psi_{i,c} + \psi_{i,t} + \psi_{i,o} + \psi_{i,b}
\end{equation}
where $\psi_{i,k}$ is the energy for motion smoothness, $\psi_{i,c}$ is the control optimization energy, $\psi_{i,t}$ is the position tracking energy, $\psi_{i,o}$ is the obstacle avoidance energy, and $\psi_{i,b}$ is the collision aversion energy. 

The motion smoothness energy $\psi_{i,k}$ is beneficial to reducing oscillations, which is formulated as $\psi_{i,k}(\boldsymbol{o}_{i}) = c_k {\Vert \boldsymbol{v}_i - \boldsymbol{v}_c\Vert^2}$, 
% \begin{equation}
% 	\psi_{i,k}(\boldsymbol{o}_{i}) = c_k {\Vert \boldsymbol{v}_i - \boldsymbol{v}_c\Vert^2}
% 	\label{kinetic_energy}
% \end{equation}
where $c_k>0$ is a coefficient, $\boldsymbol{v}_c$ is the desired velocity{ of the flock}. 
The control optimization energy $\psi_{i,c}(\boldsymbol{o}_i)$ is defined as $\psi_{i,c}(\boldsymbol{a}_i) = c_{c}\boldsymbol{a}_i^2$.
% \begin{equation}
% 	\psi_{i,c}(\boldsymbol{a}_i) = c_{c}\boldsymbol{a}_i^2
% \end{equation}
The position tracking energy $\psi_{i,t}$ encourages a robot  to approach the flocking {desired} position, which is given by $\psi_{i, a}(\boldsymbol{o}_{i}) = c_{t1} (\Vert \boldsymbol p_i - \boldsymbol p_c \Vert - c_{t2})$, 
% \begin{equation}
% \psi_{i,a}(\boldsymbol{o}_{i}) = c_{t1} (\Vert \boldsymbol p_i - \boldsymbol p_c \Vert - c_{t2})
% 	\label{global_aiming_energy}
% \end{equation}
% The tracking energy has a constant magnitude of gradient.
where $c_{t1}$, $c_{t2}$ are constants, and $\boldsymbol p_c$ is the {desired position of flock}. 
The obstacle avoidance energy $\psi_{i,o}$ is designed similarly to the Morse potential, but only the repulsive part is left.
% \begin{equation} 
% \psi_{i,o} (\boldsymbol{o}_{i}) = \left\{\begin{aligned}
% &c_{o1}\left[1 - e^{-c_{o2}(d_{oi}-d_{or})}\right]^2, &d_{oi} < d_{or} \\
% &0, &\mathrm{otherwise}
% \end{aligned}\right.
% \label{eq:obstacle_energy}
% \end{equation}
\begin{equation}
    \psi_{i,o}(\boldsymbol{o}_i) = c_{o1} \left\{ 1 - \exp [-c_{o2} \min (0, d_{oi} - d_{or})] \right\}^2
\label{eq:obstacle_energy}
\end{equation}
where $c_{o1}, c_{o2} > 0$ are coefficients, $d_{oi} = \Vert \boldsymbol p_{o,min} - \boldsymbol p_{i} \Vert$ is the distance to nearest obstacle, $\boldsymbol p_{o,min}$ is the position of the nearest obstacle, $d_{or}$ is a constant indicating the reaction distance to obstacle. 
% The obstacle avoidance energy $\psi_{i,o}$ has two properties:
% \begin{enumerate}
% 	\item $\psi_{i,o} \geq 0$ and is minimum at $d_{io} = d_{or}$ ;
% 	\item $\left.\psi_{i,o}\left(\boldsymbol{o}_i\right)\right|_{d_{oi}=d_{or}} = 0$, $\left.\nabla \psi_{i,o}(\boldsymbol{o}_i) \right|_{d_{oi}=d_{or}} = 0$ .
% \end{enumerate}
% The first property encourages robots to move away from obstacles, while the second property results in a smooth transition.
A collision aversion energy $\psi_{i,b} (\boldsymbol{o}_i) $ is introduced to limit the velocity towards obstacles. 
\begin{equation}
\psi_{i,b} (\boldsymbol{o}_i) = c_{b} \max\left(0, -\boldsymbol v_{oi}^\top \boldsymbol p_{oi} / \Vert \boldsymbol p_{oi} \Vert^2 \right)
\label{brake_energy}
\end{equation}
where $c_{b}$ is a coefficient, $\boldsymbol p_{oi} = \boldsymbol p_{o,min} - \boldsymbol p_{i}$ is the relative position of the nearest obstacle, $\boldsymbol v_{oi} = \boldsymbol v_{o,min} - \boldsymbol v_{i}$ is the relative velocity of the nearest obstacle.

% or instance, when the pairwise energy $\psi_{ij,p}$ is negative, robots in flocking are encouraged to find as many neighbors as possible, implying more edges in $\mathcal{G}$. However, more edges might degenerate the collision avoidance performance, as shown in Fig. \ref{fig:energy_normalization}.

% The total free energy in a GRF is influenced by the number of interaction edges in graph $\mathcal{G}$. The interaction edges in $\mathrm{E}$ might change due to the variation of the neighbor robots of robot $i$, so the number of items to be summed up in \eqref{eq:singleton_pairwise_potential} can dynamically change.  For instance, with the pairwise energy $\psi_{ij,p}$, robots in flocking are encouraged to find as many neighbors as possible for cohesion, implying more edges in $\mathcal{G}$. However, more edges might degenerate the collision avoidance performance, as shown in Fig. \ref{fig:energy_normalization}. To balance the performance between cohesion and safety, the total free energy $H(X)$ in (\ref{eq:singleton_pairwise_potential}) is modified to be 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{images/Neighboring_normalization_v2.pdf}
    \caption{{Configurations with (a) 14 edges, (b) 18 edges. The red robot in (b) tends to collide with obstacles due to attractions from extra neighbors.}}
    \label{fig:energy_normalization}
\end{figure}
{The interaction edges in $\mathcal{E}$ might change due to the variation caused by distance change of the neighbor robots of robot $i$, so the number of items to be summed up in \eqref{eq:singleton_pairwise_potential} can dynamically change. Although more edges usually lead to better cohesion, more edges might degenerate safety and collision avoidance performance, as shown in Fig. \ref{fig:energy_normalization}}. To balance cohesion and safety, the total free energy $H(X)$ in (\ref{eq:singleton_pairwise_potential}) is modified to be 
\begin{equation}
\color{red}
\label{eq:normalized_energy}
    H(X) = H_u(X) + H_p(X)
\end{equation}
{where $H_u(X) = \sum_{i \in \mathcal V} \psi_i(\boldsymbol{o}_{i}, \boldsymbol{a}_i)$ is the unary energy, $H_p(X) = \frac{\vert \mathcal{V} \vert}{\vert \mathcal{E} \vert}\sum_{(i,j)\in \mathcal{E}} \psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})$ is the normalized pairwise energy,} $\vert \cdot \vert$ denotes the cardinal number of a set.
% the number of edges in the graph $\mathcal{G}$, and $\vert \mathcal{V} \vert$ is the total number of robots. 
% where $\vert \mathcal{E} \vert$ is the number of edges in the graph $\mathcal{G}$, and $\vert \mathcal{V} \vert$ is the total number of robots. 
The reward function for robot flocking is, therefore, designed to be 
\begin{equation}
\label{eq:collective_reward}
	r = \exp[-{H(X)}]  \propto P(X)
\end{equation}

% If only a collective reward is given, the training will probably encounter significant nonstationarity\cite{wong_deep_2023}.
\subsection{Credit assignment for decentralized training}
\label{sec:gibbs_payoff_distribution}

The flocking reward presented in Section IV-A is formulated from a centralized perspective, relying on global information. However, in a distributed system, robots only have access to information from their immediate neighbors, making it impossible for each robot to infer the global flocking reward solely from its local observations. 
%While using the global flocking reward directly to train a distributed policy is technically feasible in Centralized Training with Decentralized Execution (CTDE) frameworks, this approach may lead to reduced performance due to potential nonstationarity issues. 
Most RL algorithms assume a stationary environment, but the assumption is violated when the observable information cannot provide sufficient information for optimal decision-making \cite{wong_deep_2023}. 
To mitigate this issue, we propose a credit assignment method that constructs a series of local rewards based on each robot's local observations, leading to a DTDE mechanism. % In this context, credit assignment refers to assessing each robot's contribution to the team's success. 



In the global reward function \eqref{eq:collective_reward}, the couplings among robots arise from the pairwise energy terms in the global energy \eqref{eq:normalized_energy}. Hence, we introduce a decoupled pairwise energy.

\begin{equation}
\label{eq:decoupedPairwise}
    \hat{H}_p = \sum\nolimits_{i \in \mathcal{V}} \frac{1}{|\mathcal{N}_i|} \sum\nolimits_{j \in \mathcal{N}_i} \psi_{ij} (\boldsymbol{o}_{i}, \boldsymbol{o}_{j})
\end{equation}
where $\hat{H}_p$ is a decoupled pairwise energy which approximates the pairwise energy $H_p$ in \eqref{eq:normalized_energy}.

% \begin{equation}
% \color{red}
% \label{eq:globally_averaged}
%     H_p = \frac{|\mathcal{V}|}{|\mathcal{E}|}\sum\nolimits_{(i,j)\in \mathcal{E}} \psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})
% \end{equation}
{The approximation is valid because $H_p$ and $\hat{H}_p$ share the same minimum condition and minimum value, which is proved in Proposition \ref{Thm:same_minima}.}

\begin{proposition}
\label{Thm:same_minima}
The decoupled pairwise energy $\hat{H}_p $ given in \eqref{eq:decoupedPairwise} shares the same minimum condition and minimum value with the pairwise energy given in \eqref{eq:normalized_energy}.
\end{proposition}

% \begin{theorem}
% \label{Thm:total_minimum}
% %[minimum of globally averaged pairwise energy]
% The total pairwise energy $H_p $ given in \eqref{eq:globally_averaged} reaches minimum when $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and $\boldsymbol{v}_{ji} = \boldsymbol{0}$, $\forall (i,j) \in \mathcal{E}$.
% \begin{equation}
% \label{eq:globally_averaged}
%     H_p = |\mathcal{V}|\sum\nolimits_{(i,j)\in \mathcal{E}} \psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})\left/|\mathcal{E}|\right.
% \end{equation}
% % \begin{equation}
% % \label{eq:globally_averaged}
% %     H_p = \frac{|\mathcal{V}|}{|\mathcal{E}|} \sum\nolimits_{(i,j)\in \mathcal{E}} \psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})
% % \end{equation}
% \end{theorem}
% $\boldsymbol{v}_{ji} = \boldsymbol{0}$
\begin{IEEEproof}
{According to the first property of Eq. \eqref{position_alignment_energy}, the position alignment energy $\psi_{ij,p}$ reaches a minimum value of $-c_{p1}$ when  at $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$. The velocity alignment energy $\psi_{ij,v} (\boldsymbol{o}_{i}, \boldsymbol{o}_{j})$ reaches a minimum value of $0$ when $\boldsymbol v_{ji}^\top \boldsymbol p_{ji}\geq 0$. Hence, the minimum point for $H_p$ and $\hat{H}_p$ must satisfy the conditions that 1) $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and 2) $\boldsymbol v_{ji}^\top \boldsymbol p_{ji}\geq 0$, $\forall (i,j) \in \mathcal{E}$. The second condition holds if $\boldsymbol v_{ji}^\top \boldsymbol p_{ji}> 0$, $\boldsymbol v_{ji} \perp \boldsymbol{p}_{ji}$, or $\boldsymbol v_{ji}=\boldsymbol{0}$, $\forall (i,j) \in \mathcal{E}$. When $\boldsymbol v_{ji}^\top \boldsymbol p_{ji}> 0$, it implies that the angle between the velocity vector $\boldsymbol{v}_{ji}$ and the position vector $\boldsymbol{p}_{ji}$ is smaller than $\frac{pi}{2}$, namely $\angle\left(\boldsymbol{v}_{ji}, \boldsymbol {p}_{ji}\right)<\frac{\pi}{2}$. In this case, two robots would move towards each other, leading to an increase in $\psi_{ij,p}$. Hence, the case of $\boldsymbol v_{ji}^\top \boldsymbol p_{ji}> 0$ is not a minimum condition for $H_p$ or $\hat{H}_p$. When $\boldsymbol v_{ji} \perp \boldsymbol{p}_{ji} =0$, one robot would circle around another robot, by which the in-between distance could also be kept. Hence, both $H_p$ and $\hat{H}_p$ reaches a minimum value only if $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and $\boldsymbol v_{ji}^\top \boldsymbol p_{ji} = 0$. 
% When $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and $\boldsymbol{v}_{ji} = \boldsymbol{0}$ ($\forall (i,j) \in \mathcal{E}$), it is sufficient to conclude that both $H_p$ and $\hat{H}_p$ reaches minimum value.
The minimum value of them is $\sum_{i \in \mathcal{V}} -c_{p1} = -|\mathcal{V}|c_{p1}$
}
\end{IEEEproof}

{The minimum condition in Proposition \ref{Thm:same_minima} corresponds to configurations that every robot keeps the desired distance to its neighbors. A special case is the $\alpha$-lattice in \cite{olfati-saber_flocking_2006}, which is realizable when $d_{in}$ is properly selected, \emph{e.g.}, $d_r \leq d_{in} < \sqrt{2}d_{r}$ in a 2D space.}
Note that the circling motion due to $\boldsymbol v_{ji} \perp \boldsymbol{p}_{ji}$ is highly unlikely to happen for large-scale flocking. Other than the pairwise energy, there is also unary energy, such as motion smoothness $\psi_{i,k}(\boldsymbol{o}_{i})$ and position tracking {$\psi_{i, t}(\boldsymbol{o}_{i})$}, which would inhibit the occurrence of circling motion.
% The total pairwise energy $H_p$ {requires global knowledge of the flocking state}, hindering a decentralized training design. The next step is to find a potential decoupled energy that has some equivalent properties to  $H_p$. Via this decoupled energy, credit assignment can be readily achieved. The following theorem is introduced.
% Although the corresponding configuration may not be unique, they are all proper configurations in the flocking control problem. 
% We want to find decomposable pairwise energy that has the same minimum point with \eqref{eq:globally_averaged} to achieve credit assignment. Hence, we design a locally averaged pairwise energy.
% \begin{theorem}
% Suppose $\hat{H}_p$ is a decoupled pairwise energy function as given in (\ref{eq:decoupedPairwise}). $\hat{H}_p$ reaches its minimum $\hat{H}_{p,min}$ in the case of $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and $\boldsymbol{v}_{ji} = \boldsymbol{0}$, $\forall (i,j) \in \mathcal{E}$.
% % , and $\hat{H}_{p,min}={H}_{p,min}$.
%     %[minimum of locally averaged pairwise energy]
% % When $\forall (i,j) \in \mathcal{E}$, $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and $\boldsymbol{v}_{ji}^\top \boldsymbol{p}_{ji} \geq \boldsymbol{0}$, the locally averaged pairwise energy $\hat{H}_p$ is minimum, where $\hat{H}_p$ is given by
% \begin{equation}
% \label{eq:decoupedPairwise}
%     \hat{H}_p = \sum\nolimits_{i \in \mathcal{V}} \frac{1}{|\mathcal{N}_i|} \sum\nolimits_{j \in \mathcal{N}_i} \psi_{ij} (\boldsymbol{o}_{i}, \boldsymbol{o}_{j})
% \end{equation}
% \end{theorem}
% \begin{IEEEproof}
% It is obvious that $\psi_{ij} (\boldsymbol{o}_{i}, \boldsymbol{o}_{j})$ reaches their minimum value $-c_{p1}$ at the conditions of $\Vert \boldsymbol{p}_{ji} \Vert = d_r$, $\psi_{ij,p}$ and $\boldsymbol{v}_{ji}^\top \boldsymbol{p}_{ji} = 0$ for each robot $i$ and $ j \in \mathcal{N}_i$. $\hat{H}_p $ is a weighted sum of $\psi_{ij} (\boldsymbol{o}_{i}, \boldsymbol{o}_{j})$ with positive weights, $\forall (i,j) \in \mathcal{E}$. Hence, $\hat{H}_p $ should reaches its minimum when $\Vert \boldsymbol{p}_{ji} \Vert = d_r$, $\psi_{ij,p}$ and $\boldsymbol{v}_{ji}^\top \boldsymbol{p}_{ji} = 0$, $\forall (i,j) \in \mathcal{E}$. It is easy to conclude that $\hat{H}_p$ reaches a minimum, if $\Vert \boldsymbol{p}_{ji} \Vert = d_{r}$ and $\boldsymbol{v}_{ji} = \boldsymbol{0}$, $\forall (i,j) \in \mathcal{E}$. 
% % The minimum value is given by 
% % \begin{equation}
% %     \hat{H}_{p, \min} = -\sum_{i \in \mathcal{V}} \frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} c_{p1} =-|\mathcal{V}|c_{p1} ={H}_{p, \min}
% % \end{equation}
% % For each robot $i$ and its neighbor $j$, when $\Vert \boldsymbol{p}_{ji} \Vert = d_r$, $\psi_{ij,p}$ is minimum; when $\boldsymbol{v}_{ji}^\top \boldsymbol{p}_{ji} \geq \boldsymbol{0}$, $\psi_{ij,v} = 0$ is minimum. Hence, the locally averaged pairwise energy $\hat{H}_p$ is minimum and the minimum value is
% % \begin{equation}
% %     \min \hat{H}_p = \sum_{i \in \mathcal{V}} \frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} \psi_{ij,min} = |\mathcal{V}|\psi_{ij, min}
% % \end{equation}
% \end{IEEEproof}

{The total pairwise $H_p$ and the decoupled pairwise energy $\hat{H}_p$ share the same minimum conditions, so we can replace the total pairwise energy $H_p$ with $\hat{H}_p$ in \eqref{eq:normalized_energy}, leading to a series of local rewards $\{r_i\}$ for each robot.
}
\begin{equation}
\label{eq:local_reward}
    r_i = \exp{\left[-\psi_i(\boldsymbol{o}_{i}, \boldsymbol{a}_i) - \frac{1}{|\mathcal{N}_i|}\sum\nolimits_{j \in \mathcal{N}_i} \psi_{ij}(\boldsymbol{o}_{i}, \boldsymbol{o}_{j})\right]}
\end{equation}
% Then, the distributed policy can be trained in a decentralized way.

\subsection{Action attention structure}
\label{sec:action_attention}

In this subsection, an action attention structure is presented to implicitly anticipate the motion intention of neighboring robots. The action attention structure is inspired by the mean field theory, in which a joint coupled distribution could be locally approximated by a factored distribution in terms of Kullback–Leibler (KL) divergence. 
\begin{lemma}[Mean-field approximation \cite{koller_probabilistic_2009}]
\label{lemma:mean_field} Suppose $P(X)$ is a joint distribution and $Q(X)$ is a class of distributions specified as a product of independent marginals, namely $Q(X)=\prod_{i\in \mathcal{V}} Q(X_i)$. The factored distribution $Q(X)$ is a local optimal approximation of  $P(X)$ in terms of $\mathcal{D}_{KL}\left[Q(X)\Vert P(X)\right]$, if and only if
\begin{equation}
Q(X_i) = \frac{1}{Z_i}\exp\left\{ \mathbb{E}_{X_{-i}\sim Q}[\ln P(X_i | X_{-i})] \right\}
	% \label{eq:meanfield}
\end{equation}
where $Z_i$ is a normalization constant, $X_{-i}$ denotes all the random variables in $X$ except $X_i$, and $\mathcal{D}_{KL}\left[Q(X)\Vert P(X)\right]$ denotes the KL divergence between two distributions with $\mathcal{D}_{KL}\left[Q(X)\Vert P(X)\right]=\sum_{X}Q(X)\ln\left[\frac{Q(x)}{P(x)}\right]$. 
\end{lemma}
\begin{IEEEproof}
The proof of lemma \ref{lemma:mean_field} can be found in \cite{koller_probabilistic_2009}. 
\end{IEEEproof}

The modified energy in \eqref{eq:normalized_energy} leads to a joint distribution $P(X) \propto \exp[-H(X)]$. 
Based on Lemma \ref{lemma:mean_field}, the optimal choice $Q(\boldsymbol{o}_i)$ for robot $i$ to approximate $P(X)$ has the following formula.
% corresponding marginal distribution of robot $i$ is
\begin{equation}
    Q(\boldsymbol{o}_i) \propto \exp \left[-\psi_i - \frac{|\mathcal V|}{|\mathcal E|}\sum_{j \in \mathcal N_i} \sum_{\boldsymbol{o}_j \in \mathcal{O}_j} Q(\boldsymbol{o}_j) \psi_{ij}\right]
 \label{eq:meanfield}
\end{equation}
In \eqref{eq:meanfield}, a weighted sum of neighbor action distribution $Q(\boldsymbol{o}_j)$ is required to obtain $Q(\boldsymbol{o}_j)$.
% \begin{equation}
%     Q(\boldsymbol{o}_i) = \frac{1}{Z_i} \exp \left[-\psi_i - \frac{|\mathcal V|}{|\mathcal E|}\sum_{j \in \mathcal N_i} \sum_{s_j \in \mathcal{S}_j} Q(\boldsymbol{o}_j) \psi_{ij}\right]
%  \label{eq:meanfield}
% \end{equation}
% where $Z_i$ is a normalization constant, $Q(\boldsymbol{o}_j)$ is the marginal distribution of neighbor $j$.

To obtain such weights $\psi_{ij}$ in the policy network, we introduce the scaled dot-product attention module. In scaled dot-product attention, the network inputs contain queries $\boldsymbol{q}_j\in\mathbb{R}^{d_{k}\times 1}$, keys $\boldsymbol{k}_j\in\mathbb{R}^{d_{k}\times 1}$, and values $\boldsymbol{l}_j\in\mathbb{R}^{d_{l}\times 1}$ \cite{vaswani_attention_2017}. The attention weights are computed by a softmax of the dot product of queries and keys.
% \begin{equation}
$\mathrm{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{L}) = \mathrm{softmax}\Big(\boldsymbol{Q}^\top \boldsymbol{K}/\sqrt{d_k}\Big)\boldsymbol{L}$,
% \end{equation}
where $\boldsymbol{Q}=\left[\boldsymbol{q}_1,\ldots,\boldsymbol{q}_m\right]$, $\boldsymbol{K}=\left[\boldsymbol{k}_1,\ldots,\boldsymbol{k}_m\right]$, $\boldsymbol{L}=\left[\boldsymbol{l}_1,\ldots,\boldsymbol{l}_l\right]$.

The neighborhood information $\boldsymbol{o}_{ij}$ is firstly embedded into $\boldsymbol{e}_j$ by an embedding linear layer with a rectified linear unit (ReLU) activation function. The query $\boldsymbol{q}_j$ and value $\boldsymbol{l}_j$ are then extracted by two different linear layers from $\boldsymbol{e}_j$. The action distribution $\boldsymbol{A}_{j}$ of the neighbor robot $j$ is taken as the key $\boldsymbol{k}_j$, so $\boldsymbol{k}_j=\boldsymbol{A}_{j}$. The attention weight $\alpha_j$ is calculated by using a dot product of $\boldsymbol{q}_j$ and $\boldsymbol{A}_{j}$, namely $\alpha_j = \boldsymbol{q}_j^\top \boldsymbol{A}_{j} / \sqrt{|\boldsymbol{q}_j|}$.
% \begin{equation}
%     \alpha_j = \boldsymbol{q}_j^\top \boldsymbol{A}_{j} / \sqrt{|\boldsymbol{q}_j|}
% \end{equation}

In this structure, $\boldsymbol{q}_i$ can be interpreted as the awareness of each action. Thus, if an action input of a neighbor robot $j$ has a high probability (or larger action-state value), the corresponding attention weight will be high as well, implying robot $i$ should pay more attention to the high possible moves of its neighboring robots. Implicit intention approximation is therefore achieved based on the previous action distribution of neighbors.
The embedded feature $\boldsymbol{c}_i$ of neighbors is an attention-weighted sum of embedded value $\boldsymbol{l}_j$, so $\boldsymbol{c}_i = \sum\nolimits_{j \in \mathcal N_i} \alpha_j \boldsymbol{l}_j$.
% \begin{equation}
%     \boldsymbol{c}_i = \sum\nolimits_{j \in \mathcal N_i} \alpha_j \boldsymbol{l}_j
% \end{equation}
Information of robot $i$ itself, $\boldsymbol{o}_{ii}$ and $\boldsymbol{o}_{io}$, is concatenated and embedded into $\boldsymbol{e}_i$ by a linear layer with ReLU activation function. Then, $\boldsymbol{e}_i$ and $\boldsymbol{c}_i$ are concatenated and fed into a multi-layer perceptron (MLP) with ReLU activation functions to generate the new action distribution $\boldsymbol{A}_{i}' = \mathrm{softmax}[MLP(\boldsymbol{e}_i, \boldsymbol{c}_i)]$ for robot $i$.
% \begin{equation}
% 	\boldsymbol{A}_{i}' = \mathrm{softmax}[MLP(\boldsymbol{e}_i, \boldsymbol{c}_i)]
% \end{equation}
The action attention structure is shown in Fig. \ref{fig:framework}. 
% Similar to MF-AC, MF-PPO updates critic network with $L_c$ and actor network with $L_a$.
% \begin{equation}
% \begin{aligned}
%     L_c &= \mathbb{E} [R_{t,i} - V(o_{i}, \{a_{t-1, j}\};\phi)]^2 \\
%     L_a &= \mathbb{E} \{L^{CLIP} + c_e S[\pi_\theta]\}
% \end{aligned}
% \end{equation}
% where $R_{t,i}$ is GAE return, $L^{CLIP}$ is clipped surrogate objective\cite{schulman_proximal_2017}, $c_e$ is entropy coefficient, $S[\pi_\theta]$ is entropy bonus.
% The action attention structure is based on the dot-product attention mechanism.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{images/framework_v3.pdf}
\caption{{An action attention structure is designed for distributed policies. The attention weight $\alpha_j$ evaluating the importance of each neighbor is computed by both neighbor observation $\boldsymbol{o}_{ij}$ and neighbor action distribution $\boldsymbol{A}_j$. % Implicit motion intention anticipation on neighbors is achieved via the action attention structure.
}}
\label{fig:framework}
\end{figure}
% The attention weight is computed by a dot-product of query $\boldsymbol{q}_j$ and action distribution $\boldsymbol{A}_j$.
% The embedded feature of neighbors for robot $i$, $\boldsymbol{c}_i$, is an attention-weighted sum of embedded value $\boldsymbol{l}_j$. Finally, $\boldsymbol{e}_i$ and $\boldsymbol{c}_i$ are concatenated and fed into a 2-layer multi-layer perceptron with ReLU activation function to generate the new action distribution of robot $i$.
% However, other RL algorithms could also be used, such as Q learning, and D3 Many RL algorithms can be leveraged to optimize robot policy. Here we choose proximal policy optimization (PPO). 

\subsection{Decentralized policy optimization}
We choose the Proximal Policy Optimization (PPO) algorithm to solve the multi-robot policy learning problem, although {m}ost standard RL algorithms could be applied. Based on \eqref{eq:local_reward}, the state value function for each robot $i$ is approximated using a critic network $V_i^{\boldsymbol{\pi}_{i}}$, while the control policy $\boldsymbol{\pi}_{i}$ is constructed based on the action attention structure. 

% In PPO, $V^{\boldsymbol{\pi}}$ is approximated by a critic network, while the control policy is constructed based on the action attention structure. 
% % and a policy network is designed as the policy $\boldsymbol{\pi}$. 
% The parameters of the critic network $V_i^{\boldsymbol{\pi}_{i}}$ are optimized by minimizing a mean square error, and the parameters of the policy network $\boldsymbol{\pi}_{i}$ are optimized by maximizing a surrogate objective.
The parameters of $V_i^{\boldsymbol{\pi}_{i}}$ are trained by minimizing a mean square error $L_{c,i} = \mathbb{E} [G_{t,i} - V_i^{\boldsymbol{\pi}_i}]^2$, where $V^{\boldsymbol{\pi}_i}$ is the state value estimated by the critic network. The policy network $\boldsymbol{\pi}_{i}$ is optimized by maximizing a surrogate objective $L_{a,i} = \mathbb{E} \{L_{cl,i} + c_e S[\boldsymbol{\pi}_i]\}$, 
% \begin{equation}
% \begin{array}{cc}
%     L_{c,i} = \mathbb{E} [G_{t,i} - V_i^{\boldsymbol{\pi}_i}]^2 \text{,} 
%      &  L_{a,i} = \mathbb{E} \{L_{cl,i} + c_e S[\boldsymbol{\pi}_i]\}
% \end{array}
% \end{equation}
% \begin{equation}
% \begin{aligned}
%     L_c &= \mathbb{E} [G_{t} - V_i^{\boldsymbol{\pi}_i}]^2 \\
%     L_a &= \mathbb{E} \{L_{clip} + c_e S[\boldsymbol{\pi}_i]\}
% \end{aligned}
% \end{equation}
where % $L_{c,i}$ is the loss function for critic network, $L_{a,i}$ is the loss function for policy network, 
% $V^{\boldsymbol{\pi}_i}$ is the state value estimated by the critic network, 
$c_e$ is entropy coefficient, $S[\boldsymbol{\pi}_i]$ is an entropy bonus, and $L_{cl, i}$ is a clipped surrogate objective based on general advantage estimation (GAE) \cite{schulman_proximal_2017}. 

At training, the critic networks and policy networks for all robots are optimized with a common learning rate $\beta$. Networks are updated per batch for $n_p$ epochs with minibatch size $n_m$. In every batch, data from $n_b$ episodes is collected. % The training is finished after collecting data of $n_{s,max}$ steps. 

% The policy optimization workflow is shown in Algorithm \ref{alg:workflow}.
% \begin{equation}  \label{eq:SurrObj}
% \begin{array}{ll}
%      L^{CLIP} =& \mathbb{E}\left[\min\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t, \right.\right.\\
%      & \left.\left.{\rm clip}(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon)A_t\right)\right]
% \end{array}
% \end{equation}
% where $A_t$ is the advantage function computed by general advantage estimation(GAE) and $\epsilon$ is the clip parameter. 

% \begin{algorithm}[tbp]
%     \caption{Decentralized policy optimization}
%     \label {alg:workflow}
%     \begin{algorithmic}[1]
%         \State{Training initialization}
%         \For{$step \leq n_{s,max}$}
%         \For{$k = 1, \dots, n_{b}$}
%         \State {$D_k = []$ empty list}
% 		\State{Initialize environment and $Q(X)$}
% 		\For{$t = 1, \dots, T$}
% 		\For{$i \in \mathcal V$}
%         \State{Observe $\boldsymbol{o}_{i}=\{\boldsymbol{o}_{ii}, \{\boldsymbol{o}_{ij}\}, \{\boldsymbol{A}_{j}\}, \boldsymbol{o}_{io}\}$}
%         \State{Action distribution $\boldsymbol{A}_{i}' \leftarrow \boldsymbol{\pi}(\boldsymbol{o}_{i})$}
% 		\State{Sample action $a_{i}$ from $\boldsymbol{A}_{i}'$}
% 		\EndFor
% 		% \State{Central critic $v_{t} = V(s_{t};\phi)$}
%         \State{Calculate normalized energy in the GRF}
%         % \State{Energy normalization}
% 		\State{Credit assignment $\rightarrow r_{t,i}$}
% 		% \State{Execute actions $\{a_{i}\}$} % , observe $s', \{o'_{i}\}$}
% 		\State{$D_k += [s, \{\boldsymbol{o}_i\}, \{a_i\}, \{r_{i}\}, \{A_{i}'\}, s', \{o'_i\}]$}
%         \State{$step \leftarrow step+1$}
% 		\EndFor
% 		\EndFor

% 		% \State{Compute GAE in batch D and normalize advantage}

%         \State{Update networks for $n_p$ epochs with minibatch $n_m$}
% 		% \For{epochs $p = 1, ... P$}
% 		% % \For{mini-batch $m = 1, \dots, M$}
% 		% \State{$b \leftarrow$ random mini-batch from D}
%   %           \State{Update critic and policy network with data $b$}
% 		% \EndFor
		
% 		\EndFor
% 	\end{algorithmic}
% \end{algorithm}


\section{Results}

% In this section, both simulation and experimental results are presented to validate the efficiency of the proposed algorithm in various conditions. The training setup configurations are discussed at the beginning. It is followed by the framework validation, in which the proposed algorithm is compared with diverse benchmark methods. An ablation study is thereafter provided to show the efficiency of the proposed credit assignment and action attention module. In the end, real-world experiments are presented to demonstrate the feasibility and competence of our method in real robots.


\subsection{Training setup}
At the beginning of every episode, a random number of static obstacles (up to $n_{o, max}=50$) are randomly generated in a $15\mathrm{m} \times 15\mathrm{m}$ square arena. Robots are initially randomly generated in the same square arena with a random velocity (up to $v_{c, max}=0.4$) as shown in Fig. \ref{fig:cluster_crossing}. The desired position is initialized at the arena center with a random velocity (up to $v_{c, max}$). 
% In every episode, they have a fixed length $T$ and a fixed simulation timestep $\Delta t$.
At every training episode, a robot would instantly stop for $t_c = 5$ seconds, if it collides with an obstacle. The discrete action set $\mathcal{A}_{i,d}$ in Fig. \ref{fig:perception} are chosen such as $M \in \{0.05, 0.1, 0.2, 0.5\}$ with $\boldsymbol{e}\in\left\{\boldsymbol{e}_1,\ldots,\boldsymbol{e}_l,\ldots\boldsymbol{e}_{32}\right\}$ with $\boldsymbol{e}_l$ has an azimuth angle of $\frac{l\pi}{16}$.
The policy network is trained for $50000$ episodes ($25$ million steps) and the total number of steps in every episode is $500$. Training parameters are $\gamma=0.99$, $\beta=0.0005$, $c_e=0.001$, $c_c=0.00001$, $c_{v} = 0.001$, $c_{k} = 0.02$, $c_{t1}=0.005$, $c_{t2}=3$, $c_{p1} = 0.03$, $c_{p2} = 1.5$, $c_{o1} = 0.03$, $c_{o2} = 2$, $c_{o3} = 0.01$, $n_b = 5$, $n_m = 256$.
% Training parameters are summaried in table \ref{tab:parameters}. 

% \begin{table}
% 	\caption{Training parameters}
% 	\label{tab:parameters}
% 	\centering
% 	\begin{tabular}{cc|cc|cc}
% 		\toprule
%         parameter & value & parameter & value & parameter & value \\
% 		\midrule
% 	% discount factor 
%         $\gamma$ & 0.99 &
% 		% GAE factor 
%         $\epsilon$ & 0.2 &
% 		% learning rate 
%         $\beta$ & 0.0005 \\
% 		% entropy coefficient 
%         $c_e$ & 0.001 &
%         % control effort parameter 
%         $c_c$ & 0.00001 &
%         % tracking parameter 
%         $c_{t1}$ & 0.005 \\
%         % velocity matching parameter 
%         $c_v$ & 0.001 &
%         % kinetic parameter 
%         $c_k$ & 0.02 &
%         % Morse parameter 
%         $c_{p1}$ & 0.03 \\
%         % cropped Morse parameter 
%         $c_{o1}$ & 0.03 &
%         % episode length 
%         $T$ & 500 &
%         % brake parameter 
%         $c_{o3}$ & 0.01  \\
%         % mini-batch size 
%         $n_m$ & 256 &
% 	% maximum obstacle amount 
%         $n_{o,max}$ & 50 &
% 	% random init velocity 
%         $v_{c,max}$ & 0.4 \\
%         % Morse parameter 
%         $c_{p2}$ & 1.5&
%         % action directions 
%         $d_{o, max}$ & 0.6 &
%         % tracking parameter 
%         $c_{t2}$ & 3 \\
%         % collision penalty period
%         $t_{c}$ & $5$ &
%         % cropped Morse parameter 
%         $c_{o2}$ & 2 &
%        % batch size 
%         $n_b$ & 5 \\
% 	\bottomrule
% 	\end{tabular}
% \end{table}

% We designed randomized environments to test the performance of our flocking control framework. 
\subsection{Framework validation}
The proposed framework is evaluated in environments with various numbers of robots, \emph{e.g.} $10$, $30$, and $50$, and also different quantities of obstacles, \emph{e.g.}  $10$, $20$, and $50$. This setup is not used at training, so it is employed to test the efficiency and generalization of the proposed algorithm. 
% These environments encompass scenarios with  Algorithm performance tested $10$ times in each of the scenarios. 
A simulation case is shown in Fig. \ref{fig:cluster_crossing}. The proposed algorithm, which is termed as PPO-AA (PPO with action attention), is compared with state-of-the-art benchmarks, including PPO without action attention (PPO), {three rule-based methods (Olfati-saber's algorithm\cite{olfati-saber_flocking_2006}, Vásárhelyi's algorithm\cite{vasarhelyi_optimized_2018}, CFDC\cite{guo_collision-free_2023}), a distributed MPC method (DMPC) \cite{soria_distributed_2022}, and a GRF-based method (HPFC) \cite{zhu_heuristic_2024}.} The comparison is performed in terms of computation efficiency, flocking optimality, and motion safety.

% At the evaluation, 
All algorithms are repeatedly run $10$ times at each scenario with the same amount of robots and obstacles. For each run, the locations of all obstacles will be randomly generated. 
%  Hence, the testing scenario with the same number of robots and obstacles will also be different across different evaluation trials. 
It implies that the algorithms are sufficiently evaluated at $90$ different setups in total. The mean and variance of each performance metric are summarized for fair evaluation.

% The comprehensive performance is evaluated in terms of efficiency, optimality, and safety.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{images/examples-race4.pdf}
    \caption{{A validation environment with $50$ obstacles, in which robots move from the left to the right by six different algorithms:} (a) Olfati-saber, (b) V\'as\'arhelyi, (c) CFDC, (d) DMPC, (e) PPO, (f) PPO-AA.}
    \label{fig:cluster_crossing}
\end{figure}

\subsubsection{Computation efficiency}
The computation efficiency is evaluated based on the computation time of each algorithm in different environments. The computation costs of each algorithm for a different number of robots are shown in Fig. \ref{fig:computation}. All algorithms are tested on the same computer with Intel Core i7-10700K CPU.  PPO-AA and PPO are implemented in Python with PyTorch. DMPC is implemented in Python with OSQP solver\cite{stellato_osqp_2020}. 
HPFC, CFDC, Vásárhelyi{'s}, and Olfati-saber{'s} algorithm are implemented in C++. Results indicate that learning-based methods are more efficient compared to optimization-based methods. Furthermore, they are only slightly slower than rule-based methods. The computation time remains consistent regardless of the number of robots, demonstrating the scalability of the learning-based approach.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{images/computation.pdf}
    \caption{Computation time for each robot at different flocking scales.}
\label{fig:computation}
\end{figure}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=\linewidth]{images/metrics_v4.pdf}
    \caption{{Flocking order} $\Phi_{o}$ (a), {Tracking distance} $\Phi_{t}$ (b), and {Success rate} $ \Phi_{s}$ (c) at different environments with $(|\mathcal{V}|, |\mathcal{V}_o|)$, where $|\mathcal{V}|$ is the number of robots and $|\mathcal{V}_o|$ is the number of obstacles, $0\leq \Phi_{o}\leq 1$ and $0\leq \Phi_{s}\leq 1$. A larger value for $\Phi_{o}$ means better performance in terms of flocking order. A smaller $\Phi_{t}$ means better performance in terms of position tracking. A larger value for $\Phi_{s}$ means better performance in terms of motion safety. % {The mean control inputs for the 7 algorithms are $0.11$, $0.13$, $0.08$, $0.12$, $0.41$, $0.07$, $0.18$ $\mathrm{m/s^2}$, respectively.} 
    }
    \label{fig:metrics}
\end{figure*}

\subsubsection{Flocking optimality}
The optimality of flocking control is evaluated using two metrics: \emph{flocking order} and \emph{tracking distance}. The two metrics are assessed only for robots that do not collide with each other or obstacles. The \emph{flocking order} indicates the consistency of robots \cite{soria_distributed_2022}, which is defined as
\begin{equation}
\label{eq:order}
    \Phi_{o} = \sum\nolimits_{i\in\mathcal{V}} \sum\nolimits_{j\in\mathcal{N}_i} \frac{\boldsymbol{v}_i^\top \boldsymbol{v}_j } {|\mathcal{V}||\mathcal{N}_i|\Vert \boldsymbol{v}_i \Vert \Vert \boldsymbol{v}_j \Vert}
\end{equation}
The \emph{tracking distance} metric measures the tracking performance of flocking control, which is given by 
\begin{equation}
    \label{eq:tracking_distance}
    \Phi_{t} = \left.\sum\nolimits_{i\in\mathcal{V}} \Vert \boldsymbol{p}_i - \boldsymbol{p}_c \Vert\right/|\mathcal{V}|
\end{equation}
% \begin{equation}
%     \label{eq:tracking_distance}
%     \Phi_{t} = \frac{1}{|\mathcal{V}|} \sum\nolimits_{i\in\mathcal{V}} \Vert \boldsymbol{p}_i - \boldsymbol{p}_c \Vert
% \end{equation}
All algorithms are evaluated in $9$ different environment settings with the robot number of $10$, $30$, or $50$ and the obstacle number of $10$, $30$, or $50$, respectively.

The proposed learning-based methods, PPO-AA and PPO, demonstrate high flocking order across various tasks as illustrated in Fig. \ref{fig:metrics}(a), highlighting their effectiveness in maintaining cohesive robot groups. In comparison, CFDC and Vásárhelyi{'s algorithm exhibit} a lower flocking order due to their control rules not accounting for oscillations. DMPC shows a slightly higher flocking order than the learning-based methods at the cost of tracking distance and motion safety, which will % would 
be verified later on.

% indicating its optimality on this metric 

PPO-AA achieves the shortest tracking distance among the evaluated methods as shown in Fig. \ref{fig:metrics}(b), demonstrating its superior performance in minimizing the distance between robots and the flocking {desired position}. In contrast, DMPC struggles with tracking distance due to the inherent difficulty in balancing multiple objectives. DMPC’s performance is dependent on carefully tuned parameters, which require extensive expert knowledge. While rule-based methods can achieve short tracking distances when parameters are well-tuned (\emph{e.g.} CFDC and Olfati-Saber{'s} algorithm), they may perform poorly as observed in the Vásárhelyi{'s} algorithm. Overall, the learning-based method PPO-AA stands out for its exceptional tracking distance performance.

In summary, it is difficult for rule-based methods to balance multiple objectives of flocking, while {optimization-based} methods have the potential but require %requires 
extensive expert tuning efforts. In contrast, learning-based methods demonstrate competent performance across various metrics, showcasing their ability to achieve optimal results in diverse aspects. 

% This versatility highlights the effectiveness of learning-based approaches in handling complex, multi-faceted scenarios.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{images/order.pdf}
%     \caption{\textbf{Flocking order} $\Phi_{o}$ at different environments with $(|\mathcal{V}|, |\mathcal{V}_o|)$, where $|\mathcal{V}|$ is the number of robots and $|\mathcal{V}_o|$ is the number of obstacles. $0\leq \Phi_{o}\leq 1$. A larger value for $\Phi_{o}$ means better performance in terms of flocking order.}
%     \label{fig:order}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{images/tracking.pdf}
%     \caption{\textbf{Tracking distance} $\Phi_{t}$ at different environments with $(|\mathcal{V}|, |\mathcal{V}_o|)$, where $|\mathcal{V}|$ is the number of robots and $|\mathcal{V}_o|$ is the number of obstacles. $\Phi_{t}\geq 0$. A smaller $\Phi_{t}$ means better performance in terms of position tracking. }
%     \label{fig:tracking}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{images/trajectory_length.pdf}
%     \caption{Trajectory length of different algorithms in different environments labeled as $(|\mathcal{V}|, |\mathcal{V}_o|)$, where $|\mathcal{V}|$ is the number of robots and $|\mathcal{V}_o|$ is the number of obstacles.}
%     \label{fig:traj_length}
% \end{figure}

\subsubsection{Motion safety}
\label{sec:obstacle_avoidance_performance}
Motion safety is evaluated based on the success rate of collision avoidance for robots in flocking, whose metric is given by $\Phi_{s} = \left.{|\mathcal{V}_s|}\right/{|\mathcal{V}|}$, 
% \begin{equation}
%     \Phi_{s} = \left.{|\mathcal{V}_s|}\right/{|\mathcal{V}|}
% \end{equation}
where $|\mathcal{V}_s|$ is the number of robots without collision. Both rule-based methods and DMPC would experience apparent performance degeneration in terms of motion safety with the complexity increase of the environment, for example, the increase of robot numbers and the increase of obstacle numbers, as shown in Fig. \ref{fig:metrics}(c). This is due to the fact that parameters tuned for a simple scenario are barely adaptable to a complex one for both rule-based methods and DMPC. The vanilla PPO method also {experiences} significant performance degeneration due to the lack of motion intention anticipation. In contrast, PPO-AA consistently exhibits the highest collision avoidance success rate in most cases, underscoring the safety of the learning-based distributed flocking control framework.

% It indicates that the safety of rule-based methods is inconsistent across different environments. While DMPC generally achieves a higher success rate compared to rule-based methods, its performance diminishes in large-scale flocking scenarios due to the mutual influence issue. 

% The challenge is also faced by PPO. 


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{images/success_rate.pdf}
%     \caption{\textbf{Success rate} $ \Phi_{s}$ at different environments with $(|\mathcal{V}|, |\mathcal{V}_o|)$, where $|\mathcal{V}|$ is the number of robots and $|\mathcal{V}_o|$ is the number of obstacles. $0\leq \Phi_{s}\leq 1$. A larger value for $\Phi_{s}$ means better performance in terms of motion safety.}
%     \label{fig:success_rate}
% \end{figure}

\subsection{Ablation study}
The effectiveness of the credit assignment mechanism (described in Sec. \ref{sec:gibbs_payoff_distribution}) and the action attention structure (detailed in Sec. \ref{sec:action_attention}) is evaluated through an ablation study. For the credit assignment mechanism, we compare the performance of several CTDE frameworks: Multi-agent PPO with action attention (MAPPO-AA) \cite{yu_surprising_2022}, independent PPO with action attention (IPPO-AA) \cite{yu_surprising_2022}, and QMIX with action attention (QMIX-AA) \cite{rashid_qmix_2018}, as shown in Fig. \ref{fig:training_curve}. The results reveal that CTDE frameworks (MAPPO, IPPO, QMIX) struggle to learn effective distributed policies in our 30-robot training scenario. However, the DTDE by credit assignment can further improve data efficiency.
\begin{figure}[htbp]
\centering
    \includegraphics[width=1\linewidth]{images/training_curve_v2.pdf}
    \caption{{Learning curves} of different learning algorithms. }
    \label{fig:training_curve}
\end{figure}

For the action attention structure, the learning curves in Fig. \ref{fig:training_curve} demonstrate that PPO-AA outperforms PPO in terms of data efficiency and training variance. Also, PPO-AA exhibits higher optimality and safety, as illustrated in Fig. \ref{fig:metrics}. These results highlight the superior effectiveness of the action attention structure in enhancing both the performance and safety of the learning-based method.

\subsection{Generalization validation}
The algorithm generalization performance is further evaluated in %at 
a scenario involving two separate flocks moving in shared space as shown in Fig. \ref{fig:large_scale_crossing}. Each flock with $100$ robots moves at a speed of $0.1 \mathrm{m/s}$ in {the} opposite direction in an environment with $22$ obstacles (including $2$ non-convex obstacles). The results demonstrate that the learned flocking control by our method performs effectively in larger-scale and dynamically changing environments, highlighting its impressive generalization ability.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{images/RegularCrossingCompose_v2.pdf}
    \caption{{Generalization validation.} Two flocks move in an opposite direction in an environment with  $22$ obstacles (including $2$ non-convex obstacles), each of which contains $100$ robots. No collision is observed in 
    this simulation.}
    \label{fig:large_scale_crossing}
\end{figure}

\subsection{Real-world experiment}
To validate the algorithm performance in real-world conditions, we designed a complex environment featuring cylindrical obstacles, narrow passages, and a non-convex obstacle as shown in Fig. \ref{fig:experiment_compare}. The robot flocks are expected to move clockwise around a rectangular path within $1$ minute as illustrated in Fig. \ref{fig:experiment_compare}.
The experiments are conducted with Crazyflie drones and FZMotion\footnote{https://www.lusterinc.com/FZMotion/} motion capture system via Crazyswarm interface \cite{preiss_crazyswarm_2017}. The drone positions are tracked at $120\mathrm{Hz}$, while acceleration commands of flocking control algorithms are sent to drones at $10\mathrm{Hz}$. Onboard controller \cite{mellinger_minimum_2011} tracks the acceleration commands.

We compared the performance of our method (PPO-AA) with the two best benchmark algorithms in simulation, \emph{e.g.}, DMPC, and Olfati-saber{'s} algorithm. The experiments revealed that the Olfati-saber{'s} algorithm struggled to maintain the flock’s cohesion in densely obstructed environments, frequently losing track of the path. DMPC, on the other hand, maintained an optimal shape. The PPO-AA approach exhibited superior adaptability under uncertainty{, showing a success rate of about $99\%$}. When navigating narrow passages, the flock transformed into a linear formation, reducing the risk of collision. Once leaving the narrow passage, robots would recover a tight configuration, showcasing the robustness and collision avoidance capabilities of PPO-AA. % The real-world experiment results conclusively demonstrate PPO-AA’s superior collision avoidance success rate, as evident in Fig. \ref{fig:success_rate}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{images/experiment_compare.pdf}
    \caption{{Real-world experiments. Flocks with 7 robots are expected to move clockwise around a rectangular path. Metrics are shown in (d) control inputs ($\mathrm{m/s^2}$), (e) flocking order, and (f) tracking distance ($\mathrm{m}$).}}
    \label{fig:experiment_compare}
\end{figure}

% \subsubsection{Motion analysis}
We evaluated the motion performance of three algorithms in real-world experiments including control input, flocking order, and tracking distance. 
%As shown in Fig. \ref{fig:experiment_compare}, the Olfati-saber algorithm has the worst performance in flocking order and tracking distance. DMPC has the highest flocking order, demonstrating the optimality. The flocking order and tracking distance of PPO-AA are almost the same as DMPC, showing the optimality of our method.
{As shown in Fig. \ref{fig:experiment_compare}, Olfati-saber's algorithm has the worst performance in flocking order and tracking distance. DMPC boasts the highest flocking order, yet PPO-AA closely trails behind, not far off in performance. Notably, PPO-AA's tracking distance is highly competitive with DMPC, showcasing the strides our method has made.}
% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=1\linewidth]{images/real.pdf}
%      \caption{Motion performance of real-world experiments in terms of control efficiency (a), flocking order (b), and tracking distance (c).}
%     % \caption{Motion performance of real-world experiments in terms of control efficiency (a), flocking order (b), and tracking distance (c). Control efficiency shows the magnitude of acceleration commands. Flocking order shows the consistency of velocity among robots. Tracking distance shows the average distance to the desired position in a flock.}
%     \label{fig:real_metrics}
% \end{figure}
% In this experiment, one flock moves to the right, and another moves toward the left, as shown in Fig. \ref{fig:top_figure}. 
% \subsubsection{Dynamic obstacle avoidance}

Furthermore, we designed an experiment in which two flocks move in the same environment in an opposite direction and regard each other as dynamic obstacles, as shown in Fig. \ref{fig:top_figure}. The experiment demonstrates the ability of our method to avoid dynamic obstacles in a real-world implementation.

\section{Conclusion}
In this paper, we proposed a learning-based distributed flocking control framework based on a GRF. A credit assignment is introduced to achieve decentralized training and decentralized execution. Implicit intention anticipation is achieved via an action attention structure. Numerical simulation results demonstrated that our method is more computationally efficient than optimization-based methods. Our method shows better performance over both optimization-based approaches and rule-based solutions in terms of flocking performance and motion safety in various environments. The ablation study illustrated the effectiveness of the credit assignment and action attention structure.
Real-world experiments further demonstrated the competence of our method.  




% Can use something like this to put references on a page
% by themselves when using the end float and the captions off option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

\bibliography{reference}
\bibliographystyle{IEEEtran}



\end{document}


