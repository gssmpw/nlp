\section{Related works}
Recently, GRFs have been applied to coordinating multi-robot behaviors ____. In GRFs, robots are represented by random variables that adhere to a joint distribution ____. The interactions among robots are characterized by specific energy functions. The primary objective of GRF-based flocking control is to determine the optimal control actions by maximizing a posteriori distribution.  However,  GRF-based flocking control is computationally intensive due to the action optimization process. The computation burdens are partially alleviated by using a mean-field approximation technique for the case of small-scale flocking  ____.  {Additionally, a heuristic predictive flocking control (HPFC) has been proposed to further decrease the computational demands in GRF-based flocking control by leveraging prior knowledge ____}. 
Despite these advancements, the existing GRF-based flocking control methods struggle to scale effectively with an increasing number of robots. Intensive communication remains essential during the online inference process, increasing the difficulty of real-time implementation.

% Recently, GRFs have been studied for achieving coordinated flocking control of multi-robot systems ____. In a GRF, a multi-robot system is formulated as a set of random variables satisfying a joint distribution, where the interactions are characterized by certain potential energy functions ____. The objective of GRF-based flocking control is to infer the best control by maximizing a posteriori distribution. Both the optimization and inference processes are computationally intensive in GRF-based flocking control ____. To mitigate the computation burdens, mean-field approximation has been introduced to simplify graph inference ____. This approximation reduces computational complexity, making real-time, online flocking control more feasible for small-scale flocking ____.  Additionally, a heuristic method has been introduced to further reduce the computational demands in GRF-based flocking control by leveraging certain prior knowledge ____. 
% Despite these advancements, the existing GRF-based flocking control methods still face the computational cost issue for a large group of robots. Also, intensive iterative communication is indispensable during the online action inference.

% so the flocking   By framing the flocking task on a GRF, coordinated behaviors of multiple robots are guided by energy functions. 

% In the past two decades, Gibbs Random Fields (GRF) has been widely studied for achieving coordinated flocking control in multi-robot systems ____. %____.
% GRF-based control frameworks model robot-to-robot and robot-to-environment interactions on an undirected graph, where the objective of flocking is formulated using Gibbs potentials.
 % However, due to the nature of GRF, flocking control requires computationally intensive iterative processes for inference and optimization.
 %, which significantly hampers scalability and efficiency.

% {Introduce MARL}
As a promising alternative, multi-{agent} reinforcement learning (MARL) offers potentially more adaptable and scalable solutions regarding computational burdens, performance optimality, and motion safety. Early works primarily focused on direct applications of classical reinforcement learning algorithms, \emph{e.g.}, Q-learning % ____
____, and deep deterministic policy gradient ____, \emph{etc}. These approaches only demonstrated their effectiveness in small-scale flocking scenarios with fewer than ten robots. To achieve better performance for large-scale flocking, a supervised learning framework is introduced ____, which leverages centralized MPC as a teacher model.  Other modifications, such as 
% convolutional global state matrices ____ and 
graph attention mechanisms ____, have further enhanced the performance of MARL in terms of generalization and robustness. 
However, in these works, the coordination of robots depends on pre-defined training strategies without online intention recognition, reducing their flexibility in handling unpredictable environments.

% Recent advances in MARL have sought to address these scalability issues. For example, supervised learning frameworks that leverage centralized MPC as a teacher model have shown improvements in learning-based flocking control____. Additionally, innovations such as convolutional global state matrices ____ and graph attention mechanisms ____ have further enhanced the performance of MARL in terms of generalization and robustness. 
% However, in these works, the coordination of robots relies on pre-defined training strategies without online intention recognition, reducing their flexibility in handling unpredictable environments.

% \begin{table}[htbp]
%     \centering
%     \color{red}
%     \caption{Typical flocking control methods.}
%     \begin{tabular}{c|c|c|c|c}
%         \toprule
%         Algorithm & Type & Efficiency & Performance & Safety \\
%         \midrule
%         PPO-AA (ours) & RL-based &  high  &  high  &  high  \\
%         PPO  &  RL-based  &  high  &  medium  &  medium \\ 
%         DMPC____ & MPC-based & low & high & high \\ 
%         HPFC____ & GRF-based & low & high & low \\ 
%         CFDC____ & Rule-based & high & low & low \\ 
%         Vásárhelyi____ & Rule-based & high & low & low \\ 
%         Olfati-saber____ & Rule-based & high & low & low \\ 
%         \bottomrule
%     \end{tabular}
%     \label{tab:comparison}
% \end{table}