\section{Related Work}
\label{Sec. Related Work}

\textbf{Efficient On-Device and Over-Network LLM.} The rapid development of LLMs has profoundly influenced human-AI interaction while introducing significant challenges related to operational costs, energy consumption, and environmental impact. Researchers propose various solutions to enable deployment in resource-constrained devices \cite{chen2023frugalgpt}, such as architectural optimization \cite{chu2023mobilevlm} and quantization \cite{lin2024awq}. Furthermore, network-based approaches have emerged, such as \cite{shen2024large} proposed the client-edge co-inference method, which partitions LLM into lightweight layers running on devices and parameter-heavy layers operating at the edge to enhance inference efficiency.

However, existing frameworks primarily focus on either single-modal data sources or single-query accuracy \cite{zhang2024treacle}, neglecting the complex trade-off optimization of key resource indicators (e.g., response quality, latency, costs) \cite{dong2024creating} and failing to address the intricate relationships between multi-modal, multi-task, and multi-dialogue context (i.e., RQ1 and RQ2). Furthermore, \cite{zhang2024treacle, he2024large} employ datasets with fixed evaluation metrics, which may not fully represent the complexity in many real-world LLM applications. Moreover, online training methods incur prohibitively expensive costs and lead to irreproducible resource consumption, particularly when considering human subjects or alternative approaches for evaluating LLM inference (i.e., RQ3). In contrast to previous work, we propose an offline training approach to preserve LLM inference results. While this inevitably introduces uncertainties in the LLM inference data, we address this challenge by further incorporating an uncertainty estimator for response scores.


\noindent\textbf{Inference Selection and Offloading Optimization.} More generally, there is a rich set of literature on strategies for optimizing local-edge-cloud inference location selection/offloading, based on metrics such as performance, cost, and latency \cite{singhal2024resource}. These works explore diverse methodological approaches that can be categorized into network optimization strategies, such as confidence-based device-server selection \cite{al2024regret} and energy-delay considerations for LLM training \cite{liu2024resource}, as well as RL frameworks, such as accuracy-delay-fairness device-server selection \cite{beytur2024optimization} and multi-device offloading with quality-latency-energy considerations \cite{chen2024tilesr}.

However, existing hierarchical inference approaches \cite{al2024regret, beytur2024optimization}, which orchestrate offloading between local devices and cloud servers, only consider model size differences. They neglect fundamental distinctions in their input modalities, for example, one scenario is device-side LLMs being limited to text-only inputs, while server-side models can process multimodal inputs (i.e., RQ1). Furthermore, recall that the unique challenge in our settings is multi-dialogue interactions, yet these methods primarily focus on meeting specific performance standards or finding \textit{immediate} optimal solutions without considering \textit{cumulative} reward maximization throughout the decision-making process. \cite{he2024large} employed active inference for LLM inference offloading but disregarded multi-modal data and mutli-task in multi-dialogues, failing to align with practical LLM scenarios (i.e., RQ2). Differently from prior works, we implement these concepts in a multi-dialogue LLM scenario, not only maximizing cumulative rewards, but also considering the modality-task associations to select the optimal LLM and multi-modal data sources.