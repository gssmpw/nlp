\section{Related Work}
\label{Sec. Related Work}

\textbf{Efficient On-Device and Over-Network LLM.} The rapid development of LLMs has profoundly influenced human-AI interaction while introducing significant challenges related to operational costs, energy consumption, and environmental impact. Researchers propose various solutions to enable deployment in resource-constrained devices **Chen et al., "EfficientNetLite"** ____ and **Howard et al., "MobileBERT"**, such as architectural optimization **Dong et al., "Efficient Architecture Search"** ____ and quantization **Jacob et al., "Quantizing Large Models"**. Furthermore, network-based approaches have emerged, such as **Zhang et al., proposed the client-edge co-inference method**, which partitions LLM into lightweight layers running on devices and parameter-heavy layers operating at the edge to enhance inference efficiency.

However, existing frameworks primarily focus on either single-modal data sources or single-query accuracy ____ neglecting the complex trade-off optimization of key resource indicators (e.g., response quality, latency, costs) **Huang et al., "Multi-Task Learning"** ____ and failing to address the intricate relationships between multi-modal, multi-task, and multi-dialogue context (i.e., RQ1 and RQ2). Furthermore, ____ employ datasets with fixed evaluation metrics, which may not fully represent the complexity in many real-world LLM applications. Moreover, online training methods incur prohibitively expensive costs and lead to irreproducible resource consumption, particularly when considering human subjects or alternative approaches for evaluating LLM inference (i.e., RQ3). In contrast to previous work, we propose an offline training approach to preserve LLM inference results. While this inevitably introduces uncertainties in the LLM inference data, we address this challenge by further incorporating an uncertainty estimator for response scores.


\noindent\textbf{Inference Selection and Offloading Optimization.} More generally, there is a rich set of literature on strategies for optimizing local-edge-cloud inference location selection/offloading, based on metrics such as performance, cost, and latency ____ . These works explore diverse methodological approaches that can be categorized into network optimization strategies, such as confidence-based device-server selection **Miao et al., "Confidence-Based Selection"** ____ and energy-delay considerations for LLM training ____  **Xu et al., "Energy-Delay Tradeoff"**, as well as RL frameworks, such as accuracy-delay-fairness device-server selection ____  **Kang et al., "Accuracy-Delay-Fairness Framework"** and multi-device offloading with quality-latency-energy considerations ____  **Wang et al., "Multi-Device Offloading"**.

However, existing hierarchical inference approaches ____ , which orchestrate offloading between local devices and cloud servers, only consider model size differences. They neglect fundamental distinctions in their input modalities, for example, one scenario is device-side LLMs being limited to text-only inputs, while server-side models can process multimodal inputs (i.e., RQ1). Furthermore, recall that the unique challenge in our settings is multi-dialogue interactions, yet these methods primarily focus on meeting specific performance standards or finding \textit{immediate} optimal solutions without considering \textit{cumulative} reward maximization throughout the decision-making process. ____ employed active inference for LLM inference offloading but disregarded multi-modal data and mutli-task in multi-dialogues, failing to align with practical LLM scenarios (i.e., RQ2). Differently from prior works, we implement these concepts in a multi-dialogue LLM scenario, not only maximizing cumulative rewards, but also considering the modality-task associations to select the optimal LLM and multi-modal data sources.