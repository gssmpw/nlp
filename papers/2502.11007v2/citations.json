[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2023frugalgpt",
        "author": "Chen, Lingjiao and Zaharia, Matei and Zou, James",
        "title": "Frugalgpt: How to use large language models while reducing cost and improving performance"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chu2023mobilevlm",
        "author": "Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others",
        "title": "Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "shen2024large",
        "author": "Shen, Yifei and Shao, Jiawei and Zhang, Xinjie and Lin, Zehong and Pan, Hao and Li, Dongsheng and Zhang, Jun and Letaief, Khaled B",
        "title": "Large language models empowered autonomous edge ai for connected intelligence"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2024treacle",
        "author": "Zhang, Xuechen and Huang, Zijian and Taga, Ege Onur and Joe-Wong, Carlee and Oymak, Samet and Chen, Jiasi",
        "title": "TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dong2024creating",
        "author": "Dong, Qifei and Chen, Xiangliang and Satyanarayanan, Mahadev",
        "title": "Creating edge ai from cloud-based llms"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024treacle",
        "author": "Zhang, Xuechen and Huang, Zijian and Taga, Ege Onur and Joe-Wong, Carlee and Oymak, Samet and Chen, Jiasi",
        "title": "TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection"
      },
      {
        "key": "he2024large",
        "author": "He, Ying and Fang, Jingcheng and Yu, F Richard and Leung, Victor C",
        "title": "Large language models (llms) inference offloading and resource allocation in cloud-edge computing: An active inference approach"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "singhal2024resource",
        "author": "Singhal, Chetna and Wu, Yashuo and Malandrino, Francesco and Levorato, Marco and Chiasserini, Carla Fabiana",
        "title": "Resource-aware deployment of dynamic dnns over multi-tiered interconnected systems"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "al2024regret",
        "author": "Al-Atat, Ghina and Datta, Puranjay and Moharir, Sharayu and Champati, Jaya Prakash",
        "title": "Regret bounds for online learning for hierarchical inference"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2024resource",
        "author": "Liu, Chang and Zhao, Jun",
        "title": "Resource allocation for stable llm training in mobile edge computing"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "beytur2024optimization",
        "author": "Beytur, Hasan Burhan and Aydin, Ahmet Gunhan and de Veciana, Gustavo and Vikalo, Haris",
        "title": "Optimization of offloading policies for accuracy-delay tradeoffs in hierarchical inference"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024tilesr",
        "author": "Chen, Ning and Zhang, Sheng and Liang, Yu and Wu, Jie and Chen, Yu and Yan, Yuting and Qian, Zhuzhong and Lu, Sanglu",
        "title": "TileSR: Accelerate On-Device Super-Resolution with Parallel Offloading in Tile Granularity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "al2024regret",
        "author": "Al-Atat, Ghina and Datta, Puranjay and Moharir, Sharayu and Champati, Jaya Prakash",
        "title": "Regret bounds for online learning for hierarchical inference"
      },
      {
        "key": "beytur2024optimization",
        "author": "Beytur, Hasan Burhan and Aydin, Ahmet Gunhan and de Veciana, Gustavo and Vikalo, Haris",
        "title": "Optimization of offloading policies for accuracy-delay tradeoffs in hierarchical inference"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "he2024large",
        "author": "He, Ying and Fang, Jingcheng and Yu, F Richard and Leung, Victor C",
        "title": "Large language models (llms) inference offloading and resource allocation in cloud-edge computing: An active inference approach"
      }
    ]
  }
]