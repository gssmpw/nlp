\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}


\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{enumitem}

\newcommand{\cgb}[1]{{\textbf{\color{red}[cgb: #1]}}}

\captionsetup{font=small}

\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\Name}{\texttt{TMO}\xspace}
\newcommand{\DatasetName}{\textsc{M4A1}\xspace}


\usepackage[most]{tcolorbox}
\usepackage{colortbl}
\colorlet{Yes}{blue!15}
\colorlet{No}{red!15}

\usepackage{listings}
\colorlet{punct}{red!60!black}
\colorlet{numb}{magenta!80!black}

\lstdefinelanguage{json}{
    basicstyle=\small\ttfamily,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{white},
    showstringspaces=false,
    morestring=[b]",
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {e}{{{\color{numb}e}}}{1}
      {-}{{{\color{numb}-}}}{1}
      {+}{{{\color{numb}+}}}{1}
      {.}{{{\color{numb}.}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
}

\lstdefinelanguage{system prompt}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{white},
}

\setlength\emergencystretch{.5\textwidth}

\begin{document}

\title[Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings]{
% \Name: Local-Cloud LLM Inference for Multi-Modal Multi-Task Multi-Dialogue Offloading
Local-Cloud Inference Offloading for LLMs in \\ Multi-Modal, Multi-Task, Multi-Dialogue Settings}

\author{Liangqi Yuan$^*$, Dong-Jun Han$^\dagger$, Shiqiang Wang$^\ddagger$, and Christopher G. Brinton$^*$}
\affiliation{
    \institution{$^*$Purdue University, $^\dagger$Yonsei University, $^\ddagger$IBM T. J. Watson Research Center}
    \country{}
}

\begin{comment}
    
\author{Liangqi Yuan}
\email{liangqiy@purdue.edu}
\affiliation{
  \institution{Purdue University}
  \city{West Lafayette}
  \state{Indiana}
  \country{USA}
}

\author{Dong-Jun Han}
\email{djh@yonsei.ac.kr}
\affiliation{
  \institution{Yonsei University}
  \city{Seoul}
  \country{South Korea}
}

\author{Shiqiang Wang}
\email{wangshiq@us.ibm.com}
\affiliation{
  \institution{IBM T. J. Watson Research Center}
  \city{Yorktown Heights}
  \state{New York}
  \country{USA}
}

\author{Christopher G. Brinton}
\email{cgb@purdue.edu}
\affiliation{
  \institution{Purdue University}
  \city{West Lafayette}
  \state{Indiana}
  \country{USA}
}
\end{comment}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Yuan et al.}


\begin{abstract}
Compared to traditional machine learning models, recent large language models (LLMs) can exhibit multi-task-solving capabilities through multiple dialogues and multi-modal data sources. These unique characteristics of LLMs, together with their large model size, make their deployment more challenging. Specifically, (i) deploying LLMs on local devices faces computational, memory, and energy resource issues, while (ii) deploying them in the cloud cannot guarantee real-time service and incurs communication/usage costs.
In this paper, we design \Name, a local-cloud LLM inference system with Three-M Offloading: Multi-modal, Multi-task, and Multi-dialogue. \Name incorporates (i) a lightweight local LLM that can process simple tasks at high speed and (ii) a large-scale cloud LLM that can handle multi-modal data sources. We develop a resource-constrained reinforcement learning (RCRL) strategy for \Name that optimizes the inference location (i.e., local vs. cloud) and multi-modal data sources to use for each task/dialogue, aiming to maximize the long-term reward (response quality, latency, and usage cost) while adhering to resource constraints. 
We also contribute \DatasetName, a new dataset we curated that contains reward and cost metrics across multiple modality, task, dialogue, and LLM configurations, enabling evaluation of offloading decisions. We demonstrate the effectiveness of \Name compared to several exploration-decision and LLM-as-Agent baselines, showing significant improvements in latency, cost, and response quality.
\end{abstract}


% Compared to traditional machine learning models, recent large language models (LLMs) can exhibit multi-task-solving capabilities through multiple dialogues and multi-modal data sources. These unique characteristics of LLMs, together with their large model size, make their deployment more challenging. Specifically, (i) deploying LLMs on local devices faces computational, memory, and energy resource issues, while (ii) deploying them in the cloud cannot guarantee real-time service and incurs communication/usage costs. In this paper, we design TMO, a local-cloud LLM inference system with Three-M Offloading: Multi-modal, Multi-task, and Multi-dialogue. TMO incorporates (i) a lightweight local LLM that can process simple tasks at high speed and (ii) a large-scale cloud LLM that can handle multi-modal data sources. We develop a resource-constrained reinforcement learning (RCRL) strategy for TMO that optimizes the inference location (i.e., local vs. cloud) and multi-modal data sources to use for each task/dialogue, aiming to maximize the long-term reward (response quality, latency, and usage cost) while adhering to resource constraints. We also contribute M4A1, a new dataset we curated that contains reward and cost metrics across multiple modality, task, dialogue, and LLM configurations, enabling evaluation of offloading decisions. We demonstrate the effectiveness of TMO compared to several exploration-decision and LLM-as-Agent baselines, showing significant improvements in latency, cost, and response quality.



%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010219</concept_id>
       <concept_desc>Computing methodologies~Distributed artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Distributed artificial intelligence}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large Language Model, Reinforcement Learning, Multi-modal, Inference Offloading, Resource Constraint.}

\maketitle

\section{Introduction}
\label{Sec. Introduction}

Large language models (LLMs) have demonstrated remarkable general-purpose task-solving capabilities across various fields and have been gradually incorporated into daily life. The development of LLMs is thriving, yet deploying LLMs on local devices presents practical challenges, including computational, memory, and energy resource limitations \cite{su2024titanic, wang2024resource}. These issues hinder the deployment of large-scale LLMs on users' local devices. Industry giants have developed strategies that either (i) involve lightweight versions of LLMs, such as Google's Gemma and Meta's LLaMA, which offer solutions with varying numbers of parameters (typically 3B, 8B, 13B, etc.) for fast and cost-efficient response, or (ii) place LLMs in the cloud, where users interact with them via the Internet, as seen with the ChatGPT application on mobile phones. The former often suffers from inferior inference performance, while the latter requires substantial network bandwidth and incurs high costs from service providers. This creates a pressing need to build systems that combine the advantages of both strategies, i.e., enhancing response quality while ensuring lower latency and usage costs.

% However, existing solutions still rely on users to choose which LLM to select, rather than autonomously determining the most appropriate LLM or modality. For example, iPhone users must decide which data source (front camera, rear camera, or screenshot) best suits their current task before uploading to ChatGPT, creating unnecessary friction. Furthermore, LLMs over networks, such as the ChatGPT application on mobile phones, impose requirements on connectivity and service costs while presenting latency challenges.

\begin{figure}[t]
\tiny 
    \centering
    \includegraphics[width=1\linewidth]{Figure/Local-Cloud.png}
    \caption{Application scenario of the \Name system as an LLM Assistant. In this example, cloud LLM is selected along with first-person and overhead views, as the query requires visual information.}
    \label{Fig. TMO}
    \vspace{-4mm}
\end{figure}

\subsection{Research Challenges}
Optimizing LLMs over networks introduces unique challenges beyond traditional machine learning models, as LLMs operate with (i) multi-modal data sources (e.g., text, images), (ii) multi-task environments (e.g., editing, recommendations), and (iii) multi-dialogue contexts (i.e., ongoing conversations with users). Although these elements enrich the capabilities of LLMs, they introduce significant challenges, particularly in the inference stage:

\textit{\textbf{1. Determining the optimal inference location:}} Determining whether to process tasks using a local LLM or to offload them to a cloud LLM is non-trivial. This decision critically impacts not only the response quality but also the latency and costs. For example, local LLM inference may limit response quality due to the lightweight model deployed on local devices, whereas cloud LLM inference, although more powerful and capable of multi-modal processing, can introduce significant communication/computation delay.

\textit{\textbf{2. Understanding relationships between modalities, tasks, and dialogues:}} There are intricate associations between multi-modal data sources, inference tasks, and their temporal variation across multiple dialogues. Although various types of input data can improve the response quality of LLMs, not all tasks require comprehensive multi-modal data. For example, a message editing task may not require images as inputs, whereas some human activity tasks require various types of image modalities \cite{lee2024gazepointar}. Furthermore, when considering multi-dialogue scenarios, where each dialogue represents a task in a sequential order, earlier uploaded data modalities may provide sustained informational advantages, yet they might necessitate re-uploading if the information becomes outdated.

\textit{\textbf{3. Uncertainty issues in offline training with LLM inference data:}} It is often desirable to train auxiliary models using LLM inference results, e.g., to learn modality-task associations. The computationally expensive nature of LLMs renders it impractical to conduct online training of such models. Consequently, generating datasets for offline training becomes necessary. However, due to inherent uncertainty in LLM inference, which arises from the probabilistic nature of language model outputs \cite{hu2023uncertainty}, such datasets may contain multiple identical samples with varying response scores. This uncertainty substantially complicates the evaluation of LLM outputs, particularly in multi-task or generative scenarios where standard answers or evaluation metrics do not exist.

Motivated by these challenges, we aim to answer the following research questions. \textit{In the context of local-cloud LLM systems:}
\begin{itemize}
    \item[(RQ1)] \textit{How can we optimally select the appropriate LLM for inference (i.e., local LLM vs. multi-modal cloud LLM)?}
    
    \item[(RQ2)] \textit{How can we identify and utilize the most informative and relevant multi-modal data sources when necessary for tasks in dialogues, while balancing response quality, latency, and usage cost?}
    
    \item[(RQ3)] \textit{How can we effectively address the inherent uncertainty in LLM inference data during offline training?}
\end{itemize}

\subsection{Summary of Contributions}
In this paper, we develop \Name, a local-cloud LLM inference offloading optimization system shown in Fig. \ref{Fig. TMO}. \Name orchestrates ``Three-M'' Offloading -- multi-modal, multi-task, and multi-dialogue -- aimed at enhancing response quality and reducing latency and usage costs. We address the above research questions through a resource-constrained reinforcement learning (RCRL) methodology that learns optimal actions (i.e., RQ1 and RQ2) to respond to mutli-task and multi-dialogue scenarios. We develop an estimator of LLM inference through a nearest neighbor strategy for effective offline learning (i.e., RQ3). Our main contributions are as follows:
\begin{itemize}[leftmargin=5mm] 
    \item We construct \Name to account for adaptations between device-side LLM and cloud LLM inference within multi-modal, multi-task, multi-dialogue settings. This includes formalizing the interaction with both types of LLMs to dynamically adapt with diverse conversational demands \textbf{(Sec. \ref{Sec. Multi-Task/Dialogue with Multi-Modal Data Sources} and Sec. \ref{Sec. Local-Cloud LLM Inference})}. 
    
    \item We formulate \Name's decision engine in terms of resource-constrained reinforcement learning (RCRL) to maximize cumulative reward across multi-dialogue interactions by selecting the optimal LLMs and modalities for inference. This approach enables a concrete optimization of trade-offs between response quality, latency, and usage costs while incorporating task-modality associations into the decision-making process \textbf{(Sec. \ref{Sec. RL})}. 
    
    \item We devise a nearest neighbor strategy to overcome state-action pair uncertainty in estimating response quality during offline RL training. Our approach addresses two distinct types of uncertainty that manifest in LLM evaluation contexts: \textit{non-deterministic evaluation (NDE)}, where identical state-action pairs can yield varying response quality evaluations, and \textit{out-of-distribution (OOD)}, where the system estimates the response quality for previously unseen state-action pairs (\textbf{Sec. \ref{Sec. Response Score}}).
    
    \item We curate a new dataset, termed \DatasetName, which contains samples from three multi-modal data sources, four inference tasks, 2-5 dialogues, and four LLMs, with real-world measurements of latency and usage costs.\footnote{Our code and 100 samples of the \DatasetName dataset are available at \url{https://github.com/liangqiyuan/TMO}. We will release the full dataset for community use upon publication.} Through extensive evaluation with this dataset, we demonstrate \Name's superior performance in response quality, latency, and usage cost compared to two SOTA exploration-decision and 15 LLM-as-Agent baselines \textbf{(Sec. \ref{Sec. Experiment})}.
\end{itemize}


\section{Related Work}
\label{Sec. Related Work}

\textbf{Efficient On-Device and Over-Network LLM.} The rapid development of LLMs has profoundly influenced human-AI interaction while introducing significant challenges related to operational costs, energy consumption, and environmental impact. Researchers propose various solutions to enable deployment in resource-constrained devices \cite{chen2023frugalgpt}, such as architectural optimization \cite{chu2023mobilevlm} and quantization \cite{lin2024awq}. Furthermore, network-based approaches have emerged, such as \cite{shen2024large} proposed the client-edge co-inference method, which partitions LLM into lightweight layers running on devices and parameter-heavy layers operating at the edge to enhance inference efficiency.

However, existing frameworks primarily focus on either single-modal data sources or single-query accuracy \cite{zhang2024treacle}, neglecting the complex trade-off optimization of key resource indicators (e.g., response quality, latency, costs) \cite{dong2024creating} and failing to address the intricate relationships between multi-modal, multi-task, and multi-dialogue context (i.e., RQ1 and RQ2). Furthermore, \cite{zhang2024treacle, he2024large} employ datasets with fixed evaluation metrics, which may not fully represent the complexity in many real-world LLM applications. Moreover, online training methods incur prohibitively expensive costs and lead to irreproducible resource consumption, particularly when considering human subjects or alternative approaches for evaluating LLM inference (i.e., RQ3). In contrast to previous work, we propose an offline training approach to preserve LLM inference results. While this inevitably introduces uncertainties in the LLM inference data, we address this challenge by further incorporating an uncertainty estimator for response scores.


\noindent\textbf{Inference Selection and Offloading Optimization.} More generally, there is a rich set of literature on strategies for optimizing local-edge-cloud inference location selection/offloading, based on metrics such as performance, cost, and latency \cite{singhal2024resource}. These works explore diverse methodological approaches that can be categorized into network optimization strategies, such as confidence-based device-server selection \cite{al2024regret} and energy-delay considerations for LLM training \cite{liu2024resource}, as well as RL frameworks, such as accuracy-delay-fairness device-server selection \cite{beytur2024optimization} and multi-device offloading with quality-latency-energy considerations \cite{chen2024tilesr}.

However, existing hierarchical inference approaches \cite{al2024regret, beytur2024optimization}, which orchestrate offloading between local devices and cloud servers, only consider model size differences. They neglect fundamental distinctions in their input modalities, for example, one scenario is device-side LLMs being limited to text-only inputs, while server-side models can process multimodal inputs (i.e., RQ1). Furthermore, recall that the unique challenge in our settings is multi-dialogue interactions, yet these methods primarily focus on meeting specific performance standards or finding \textit{immediate} optimal solutions without considering \textit{cumulative} reward maximization throughout the decision-making process. \cite{he2024large} employed active inference for LLM inference offloading but disregarded multi-modal data and mutli-task in multi-dialogues, failing to align with practical LLM scenarios (i.e., RQ2). Differently from prior works, we implement these concepts in a multi-dialogue LLM scenario, not only maximizing cumulative rewards, but also considering the modality-task associations to select the optimal LLM and multi-modal data sources.



\section{Proposed \Name System}
\label{Sec. Proposed System}

\subsection{Multi-Modal, Multi-Task, Multi-Dialogue Interaction}
\label{Sec. Multi-Task/Dialogue with Multi-Modal Data Sources}
We consider a setup in which a user is aiming to solve multiple types of inference tasks through a series of dialogues with LLMs. Each dialogue $t$ involves:
\begin{itemize}[leftmargin=5mm]
\item a text prompt $P_t$ which describes the user's intent, e.g., ``Please recommend two dishes that I can cook based on the ingredients I have,'' and
\item a set of other modalities $\mathcal{M}_t \subseteq \mathcal{M}$, where $\mathcal{M}$ represents all modalities available to the user (e.g., different image views, 3D points, or other data formats describing the current situation).
\end{itemize}
During the conversation with LLMs, a user may ask additional questions using prompts potentially related to the previous dialogues (e.g., ``Where did I put my dishes?'' or ``Please turn on all the lights in the kitchen''). Here, we assume each text prompt belongs to one of $N$ different task categories (e.g., assistant, recommendation, query, and message editing). Moreover, distinct tasks have varying levels of difficulty and require different types of multi-modal information to solve. For example, the prompt ``Where did I put my dishes?'' requires additional modalities (e.g., images) for the LLM to solve the task, while the prompt ``Please turn on all the lights in the kitchen'' may not require other sources of data, indicating that the task is simpler. These characteristics within a multi-task, multi-dialogue scenario with multi-modal data sources distinguish our setup from existing works \cite{zhang2024treacle, he2024large, yang2024perllm} that consider simpler settings (e.g., a single dialogue with text only), motivating the need for a new solution.


\subsection{Local-Cloud LLM Inference}
\label{Sec. Local-Cloud LLM Inference}
As shown in Fig. \ref{Fig. TMO}, a key objective of \Name is to strategically distribute tasks between local and cloud platforms, to exploit their respective strengths. \Name consisiders two different LLMs:
\begin{itemize}[leftmargin=5mm]
\item The \textbf{local LLM} is a lightweight LLM (e.g., Phi-3-mini) deployed on local devices, intended to handle simple tasks efficiently and process text prompt understanding with high speed.

\item The \textbf{cloud LLM} is a large-scale multi-modal LLM (e.g., GPT-4o) situated in the cloud capable of managing complex tasks and processing information that exceeds local device capabilities, integrating multi-modal data input to provide richer and more accurate responses where required.
\end{itemize}
\Name determine whether to process LLM inference locally or offload to the cloud, depending on the task difficulty, available modalities, and resource requirements. Offloading computationally intensive tasks to the cloud enables preservation of local resources while still benefiting from the cloud powerful computational abilities.

Formally, we represent the user-LLM interaction at each dialogue $t$ in terms of three possibilities:
\begin{equation}
    R_t = \begin{cases}
    \text{LLM}_\text{Local}\left(P_t\right), \quad \text{for local inference}, \\
    \text{LLM}_\text{Cloud}\left(P_t\right), \quad \text{for cloud text-only inference}, \\
    \text{LLM}_\text{Cloud}\Big(P_t, \bigcup\limits_{m\in \mathcal{M}_t} \{D^{(m)}\}\Big), \\ \quad \quad \quad \text{for cloud inference with modalities in  $\mathcal{M}_t$},
    \end{cases}
\label{Eq. LLM_1}
\end{equation}
where $R_t$ represents the response to the particular combination of LLM and modalities provided for dialogue $t$. $D^{(m)}$ denotes the data source of modality $m\in\mathcal{M}_t$, while $\text{LLM}_\text{Local}(\cdot)$  and $\text{LLM}_\text{Cloud}(\cdot)$ are representations of the local LLM and the cloud LLM in functional form.
The transformer architecture of LLM is known for its capability of maintaining a contextual memory over extended dialogues, thereby allowing the LLM to generate responses for the current prompt based on the accumulated context of all previous prompts and responses, though the quality of the responses will differ between local and cloud LLMs. Hence, Eq. (\ref{Eq. LLM_1}) can be rewritten to reflect the cumulative nature of the prompts. For example, in the case of the cloud LLM with multi-modal inputs,
\begin{equation}
\small
R_t = \text{LLM}_\text{Cloud} \left(P_t, \;\; \bigcup_{i=0}^{t}\bigcup_{m \in \mathcal{M}_i}\{D^{(m)}\}, \;\; \bigcup_{i=0}^{t-1} \{P_i, R_i\}\right),
\label{Eq. LLM_2}
\end{equation}
where $t=1, 2, \dots, T$. This further shows the dependency on the previously selected modalities as well as prior prompts and responses from the series of earlier dialogues $i=0, 1, \dots, t-1$.

\noindent\textbf{\Name Objective and Rationale.}
The primary objective of \Name is to strategically decide (i) whether to use the local LLM or cloud LLM and (ii) which multi-modal data sources to utilize, for each task and dialogue. This decision-making process aims to enhance response quality while efficiently managing latency and costs throughout extended interactions. The goal is to ensure high response quality in complex multi-dialogue environments, particularly under the system constraints (e.g., latency and costs).

The complexity of LLM and modality selection can be illustrated by the human decision-making process in such a setting. For example, iPhone users encounter choices among multiple LLMs (e.g., local Siri and the ChatGPT application), while also navigating various multi-modal data sources on mobile devices (e.g., front camera, rear camera, and screenshots). Although users can make short-term decisions for current tasks (for example, intuitively deciding to use the local LLM for simple message editing), the challenge intensifies when considering the entire spectrum of interactions over prolonged multi-dialogues and evolving contexts. Individuals must determine which of many multi-modal data sources to transmit and balance the quality of the response with other metrics, such as latency and various costs. Furthermore, early uploading of modality data to the cloud LLM can facilitate quicker and more nuanced understanding of the environment, enhancing long-term multi-dialogue interaction management. Optimal decision-making in these multi-modal, multi-task, and multi-dialogue scenarios is thus difficult by human inspection.





\subsection{LLM and Modality Selection}
\label{Sec. RL}

We propose a resource-constrained RL (RCRL) approach to orchestrate the \Name system. Its key components are shown in Fig. \ref{Fig. RL}, and Alg. \ref{alg:system} summarizes the process.

\subsubsection{RL Formulation}
We first formulate different components of the RL policy as follows.

%\noindent\textbf{\textit{Environment:}} The environment is defined by dialogues between the user and the LLM services. \cgb{Just the dialogues? Isn't the environment the entire}

\noindent\textbf{\textit{State:}} The state captures both the history of previous LLM selections and the history of uploaded modalities, along with the current inference task category $n \in \{1,...,N\}$. Formally, each state in the state space $\mathcal{S}$ can be visualized as a repeated sequence of the tuple $(\ell, \mathcal{M}^{\ell}, n)$. Thus, we can represent the state space $\mathcal{S}$ over $\tau$ consecutive dialogues as follows:
\begin{equation}
\mathcal{S} = (\ell, \mathcal{M}^{\ell}, n)^\tau,
\label{Eq. State}
\end{equation}
where $\ell \in \{\text{LLM}_{\text{Local}}, \text{LLM}_{\text{Cloud}}\}$ indicates the choice between local LLM and cloud LLM. $\mathcal{M}^{\ell} = \mathcal{M}$ if $\ell = \text{LLM}_{\text{Cloud}}$ and $\mathcal{M}^{\ell} = \emptyset$ otherwise. (We will often refer to $\mathcal{M}^{\ell}$ as simply $\mathcal{M}$ when the context is clear.) The superscript $\tau$ indicates that there are $\tau$ state components over $\tau$ consecutive dialogues, showing the history of model and modality selections. The state $s_t \in \mathcal{S}$ for dialogue $t$ is then represented by the tuple $s_t = (\ell, \mathcal{M}, n)_t$.

\noindent\textbf{\textit{Action:}} The action taken for the next dialogue consists of the choice between utilizing local or cloud LLM services, as well as the selection of one or multiple data modalities for inference processing. Therefore, the action space $\mathcal{A}$ can be described as follows:
\begin{equation}
    \mathcal{A} = (\ell, \mathcal{M}^{\ell}),
\end{equation}
with the action $a_t = (\ell, \mathcal{M}^{\ell})_t$ for dialogue $t$ consisting of a choice of LLM and modalities to upload.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure/RL.png}
    \vspace{-6mm}
    \caption{Illustration of RCRL within the \Name System.}
    \label{Fig. RL}
    \vspace{-4mm}
\end{figure}

\noindent\textbf{\textit{Reward:}} In designing the reward function, we consider several key metrics to optimize the decision-making process regarding the choice of LLM service and modalities. Mathematically, the reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ that we propose is expressed as follows:
\begin{equation}
\small
r_t = \alpha S_{R_t} +  \beta_{\Lambda} \sum_{m \in \mathcal{M}_t} \Lambda\left(P_t, D^{(m)}\right) - \beta_{\psi} \psi_{a_t} - \beta_\phi \phi_{a_t},
\label{eq:reweard}
\end{equation}
where:
\begin{itemize}[leftmargin=5mm]
    \item The \textbf{response score} $S_{R_t}$ assesses how effectively the LLM response $R_t$ (from either local or cloud) meets the requirements of the text prompt. The response score is used solely for RL training purposes and is not required during the inference phase. 
    A more detailed description of this metric and how we estimate it is provided in Sec. \ref{Sec. Response Score}.
    \item The \textbf{association score} $\Lambda(P_t, D^{(m)})$ quantifies the relevance of multi-modal data $D^{(m)}$ to prompt $P_t$. We aggregate the association scores for the uploaded modality set $\mathcal{M}_t$, which depend on the current action $a_t$. Further details are provided in Sec. \ref{Sec. Association}.
    \item The \textbf{latency} $\psi_{a_t}$ represents the latency of the LLM inference under different actions $a_t$. This includes the computation time of the local LLM or the interaction time with the cloud LLM for uploading the selected data modalities. Refer to Sec. \ref{Sec. Latency Model} for the formal definition.
    \item The \textbf{usage cost} $\phi_{a_t}$ reflects the cost incurred by the LLM under action $a_t$, including the energy consumption of the local LLM or the service fee of the cloud LLM for various uploaded modalities, as described in Sec. \ref{Sec. Usage Cost Model}.
\end{itemize}
The response score $S_{R_t}$ and the association $\Lambda(P, D^{(m)})$ are thus positive indicators (higher is better, $\uparrow$). In contrast, latency $\psi_{a_t}$ and usage cost $\phi_{a_t}$ are negative indicators (lower is better, $\downarrow$). Variables $\alpha$, $\beta_\Lambda$, $\beta_\psi$, and $\beta_\phi$ are weighting factors that adjust the influence of each component on the total reward.

\begin{comment}
    
Based on our state and action definitions, we can model this problem as an MDP, where the learning objective is to maximize the cumulative reward, similar to other RL works. In each dialogue $t$, the RL algorithm observes state $s_t$, selects an action $a_t$, obtains a transition probability $P(s_{t+1} | s_t, a_t)$, receives a reward $r_t$, and transitions to the next state $s_{t+1} \sim P(\cdot | s_t, a_t)$. Given the current state $s\in\mathcal{S}$, the action $a\in\mathcal{A}$ is selected according to a specific policy $\pi$ as $a\sim\pi(\cdot|s)$, where $\pi(a|s)$ represents the probability of selecting the action $a$ in state $s$. The state value function represents the expected discounted reward when starting from state $s$ and following policy $\pi$:
\begin{equation}
    V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^T \gamma^t r_t \bigm| s_0 = s \right],
\end{equation}
where $r_t$ is the reward defined in Eq. \eqref{eq:reweard} and $\gamma\in (0,1)$ is the discount factor. The action value function quantifies the expected discounted rewards for taking action $a$ in state $s$ and subsequently adhering to policy $\pi$, which can be expressed as:
\begin{equation}
    Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^T \gamma^t r_t \mid s_0 = s, a_0 = a \right].
\end{equation}
The policy is parameterized by $\theta$ and can be written as $\pi_{\theta}$. Our goal is to determine $\theta$ that maximizes the sum of future discounted rewards, i.e.,
\begin{equation}
    \max_\theta J(\theta) \triangleq \mathbb{E}_{s,a \sim \pi_{\theta}} \left[\sum_{t=0}^T \gamma^t r_t\right].
\end{equation}
In deep RL optimization, various strategies can be employed to adjust the parameters $\theta$ of the policy $\pi_\theta(a|s)$ with the goal of maximizing $J(\theta)$. To achieve this, a tractable loss function $f(\theta)$ is defined such that $f(\theta)$ can be directly minimized to maximize $J(\theta)$. Taking Advantage Actor Critic (A2C) \cite{mnih2016asynchronous} as an example, the loss function $f_{\text{A2C}}(\theta)$ includes both the policy loss and the value loss, along with an entropy term to encourage exploration:
\begin{equation}
\begin{aligned}
f_{\text{A2C}}(\theta) &= \mathbb{E}_{s,a \sim \pi_\theta} [ -\log \pi_\theta(a|s) A(s, a) \\
&+ \frac{1}{2} (V^{\pi_\theta}(s) - r)^2 - \zeta H(\pi_\theta(\cdot|s)) ],
\end{aligned}\label{eq:A2Closs}
\end{equation}
where $A(s, a) = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)$ is an estimate of the advantage function, providing a measure of the relative value of taking action $a$ in state $s$. The coefficient $\zeta$ controls the weight of the entropy bonus $H(\pi_\theta(\cdot|s))$. In Sec.~\ref{Sec. Experiment Setup}, we also consider Proximal Policy Optimization (PPO) \cite{schulman2017proximal} and Deep Q Network (DQN) \cite{mnih2015human} versions of Eq.~\eqref{eq:A2Closs}.

\end{comment}


\subsubsection{MDP and Optimization}
Based on our state and action definitions, we model this problem as a markov decision process (MDP), where the learning objective is to maximize the cumulative reward. In each dialogue $t$, RL algorithms observe the state $s_t$, select an action $a_t$, obtain a transition probability $P(s_{t+1} | s_t, a_t)$, receive a reward $r_t$, and transition to the next state $s_{t+1} \sim P(\cdot | s_t, a_t)$. Given the current state $s_t$, the action $a_t$ is selected according to a specific policy $\pi$ as $a\sim\pi(\cdot|s)$, where $\pi(a|s)$ represents the probability of selecting action $a$ in state $s$. The value function $V^\pi(s)$ predicts the expected discounted rewards of states following the policy $\pi$:
\begin{equation}
\small
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^T \gamma^t r_t \bigm| s_0 = s \right]
\end{equation}
% \begin{equation}
% \small
% Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^T \gamma^t r_t \mid s_0 = s, a_0 = a \right],
% \end{equation}
with discount $\gamma$. The optimization of policy $\pi_{\theta}$ with respect to its parameterization $\theta$ aims to maximize the expected discounted rewards in LLM inference offloading and modality selection:
\begin{equation}
\small
\max_\theta J(\theta) \triangleq \mathbb{E}_{s,a \sim \pi_{\theta}} \left[\sum_{t=0}^T \gamma^t r_t\right].
\end{equation}
In deep RL optimization, various strategies can be employed to adjust the parameters $\theta$ of the policy $\pi_\theta(a|s)$ with the goal of maximizing $J(\theta)$. To achieve this, a loss function $f(\theta)$ is defined such that minimizing $f(\theta)$ leads to maximizing $J(\theta)$. Taking Advantage Actor Critic (A2C) \cite{mnih2016asynchronous} as an example, the loss function $f_{\text{A2C}}(\theta)$ includes both the policy loss and the value loss, along with an entropy term to encourage exploration:
\begin{equation}
\begin{aligned}
f_{\text{A2C}}(\theta) = \mathbb{E}_{s,a \sim \pi_\theta} [ -\log &\pi_\theta(a|s) A(s, a) \\
&+ \frac{1}{2} (V^{\pi_\theta}(s) - r)^2 - \zeta H(\pi_\theta(\cdot|s)) ],
\end{aligned}\label{eq:A2Closs}
\end{equation}
where $A(s, a) = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)$ for $s' \neq s$ is an estimate of the advantage function, providing a measure of the relative value of taking action $a$ in state $s$. The coefficient $\zeta$ controls the weight of the entropy bonus $H(\pi_\theta(\cdot|s))$. In Sec.~\ref{Sec. Experiment Setup}, we also consider Proximal Policy Optimization (PPO) \cite{schulman2017proximal} and Deep Q Network (DQN) \cite{mnih2015human} versions of Eq.~\eqref{eq:A2Closs}.

\begin{algorithm}[t]
\caption{Local-Cloud Inference Offloading in \Name}
\label{alg:system}
\raggedright
\textbf{Input:} M4A1 Dataset ($\mathcal{D}$), initial policy parameters ($\pi_\theta$), reward weights ($\alpha$, $\beta_{\Lambda}$, $\beta_\psi$, $\beta_\phi$), time span ($\tau$), resource constraint penalty coefficient ($\lambda$), number of neighbors for response score estimation ($k$), learning rate ($\eta$) \\
\textbf{Output:} Refined policy $\pi_\theta$ for LLM and modality selection

\begin{algorithmic}[1]
\For{each time step}
    \State Sample a mini-batch of transitions $\{(s, a, s')\}$ and corresponding metrics $\{(\Lambda, S_R, \psi_{a}, \phi_{a})\}$ from $\mathcal{D}$.
    \State Predict actions $a'$ for new states $s'$ via the policy $\pi_\theta(s')$.
    \State Estimate uncertain response scores $S_R'$ for pending state-action pairs $(s', a')$ via nearest neighbors. \Comment{Eq.~\eqref{eq:ResponseScore}}
    \State Compute rewards $r$ based on the estimated response scores $S_R'$, associations $\Lambda$, latencies $\psi_{a}$, and usage costs $\phi_{a}$. \Comment{Eq.~\eqref{eq:reweard}}
    \State Calculate the RL loss $f(\theta)$, latency penalty $g_1(\theta)$, and usage cost penalty $g_2(\theta)$. \Comment{Eq.~\eqref{eq:RCloss}}
    \State Update the policy parameters: $\theta \leftarrow \theta - \eta \nabla_\theta F(\theta)$.
\EndFor
\end{algorithmic}
\end{algorithm}


\subsubsection{Resource-Constrained RL} 
We consider RCRL to enhance decision-making capabilities for \Name in environments with limited resources. RCRL involves not only maximizing cumulative rewards, but also satisfying certain constraints on resource consumption such as latency or usage cost incurred from processing different modalities. Formally, we consider a resource consumption function $C_j(a)$, which quantifies the amount of resource $j \in \mathcal{J}$ consumed when action $a$ is taken. RCRL then aims to minimize the loss function $f(\theta)$ (e.g., Eq. \eqref{eq:A2Closs} for the A2C loss) while ensuring compliance with resource constraints, by solving
\begin{equation}
\min_{\theta} f(\theta) \quad \mathrm{s.t.} \quad \sum_{i=t}^{t+\tau} C_j(a_i) \leq \xi_j, \; \text{for all} \; j\in \mathcal{J},
\label{Eq. Objective}
\end{equation}
where $C_j(a)$ denotes the consumption of the $j$-th constrained resource with $\xi_j$ as the corresponding budget over the horizon, and $\mathcal{J}$ is the set of all constraints. $\tau$ is the planning horizon length, defined in Eq. (\ref{Eq. State}). We transform resource constraints into a policy optimization problem using a penalty term to enforce constraints. Specifically, we define $g_j(\theta) \triangleq \sum_{i=t}^{t+\tau} C_j(a_i) - \xi_j$ to penalize constraint violations and aim to minimize the overall loss $F(\theta)$:
\begin{equation}
    F(\theta) = f(\theta) + \lambda \sum_{j \in \mathcal{J}} \max \left\{ g_j(\theta), 0\right\},
\label{eq:RCloss}
\end{equation}
where $\lambda \geq 0$ denotes the penalty associated with the constraints. In our experiments, we will consider two constraint penalty terms $g_1(\theta)$ and $g_2(\theta)$ on latency and usage cost, respectively.



\subsection{Response Score Uncertainty Estimation}
\label{Sec. Response Score}

Due to the vast number of parameters in LLMs, offline training approaches have become essential to mitigate substantial inference costs and enable future reusability \cite{ouyang2022training}. \Name's RCRL procedure requires a specialized dataset for learning and exploration of LLM performance in multi-modal, multi-task, and multi-dialogue settings. Moreover, given the nature of multi-task settings, particularly in generative tasks, there often exist no explicit evaluation metrics, such as accuracy, task success rate, or error rate. Furthermore, the exploratory nature of RL, which frequently encounters new state-action pairs, presents significant evaluation challenges.

To address these issues, we propose leveraging the notion of \textit{LLM-as-Judge} to evaluate response quality. Following this approach, we employ a nearest neighbor strategy to estimate uncertain response scores, providing a method for handling the inherent uncertainties in four LLMs: the outputs of the local LLM, the cloud LLM, a reference LLM, and a judge LLM.

\subsubsection{Response Quality Evaluation}
The response score $S_{R_t}$ measures the response quality generated by the selected LLM in each dialogue. In multi-task scenarios, evaluating the response quality requires assessing the actual utility of responses in aiding human users, particularly in query tasks. We employ the quantitative evaluation scheme outlined by \cite{liu2024visual}, illustrated on the right side of Fig.~\ref{Fig. M4A1}. This involves using two additional \textit{text-only} LLMs specifically to assess the quality of candidate LLM services: 
\begin{itemize}[leftmargin=5mm]
\item The \textit{reference LLM} processes a prompt and a reference response to produce a polished reference response, serving as a benchmark for evaluation.
\item The \textit{judge LLM} then evaluates both the candidate's response and the reference's response, scoring them based on helpfulness, relevance, accuracy, and detail.
\end{itemize}
The response score $S_{R_t}$ for the dialogue $t$ is then calculated as the ratio of the candidate's LLM score to the reference score.
% Since the response score is a relevance score, so that the candidate LLM's response surpasses that of the reference LLM, the response score will exceed 1.

\subsubsection{Handling Uncertainty in Response Score Estimation}
Integrating RCRL directly into real-time LLM inference is impractical, especially in our scenario where four LLMs operate in tandem: local and cloud-based LLMs generate responses, while reference and judge LLMs evaluate their quality. Consequently, we archive the LLM inference processes into a dataset for offline RCRL. However, this approach introduces two main forms of uncertainty:
\begin{itemize}[leftmargin=5mm]
\item[(i)] \textit{non-deterministic evaluation (NDE)}, in which identical state-action pairs can yield different response quality scores; and
\item[(ii)] the potential presence of \textit{out-of-distribution (OOD)} state-action pairs in the dataset.
\end{itemize}
In contrast to \cite{ran2023policy}, which aims to avoid selecting OOD actions, our goal is to more accurately estimate the response quality under OOD actions rather than constrain them. This adds flexibility for datasets where the states and actions are randomly generated (e.g., the current task, selected LLM, and selected modality), making it inevitable that some state-action pairs will not appear in the dataset (such as for our \DatasetName dataset described in Sec.~\ref{Sec. Dataset}).

To address these uncertainties, we estimate the response score $S_R'$ for a pending state-action pair $(s', a')$ by leveraging its nearest neighbors in the dataset. Specifically, let $\{(s^i, a^i)\}_{i=1}^k$ be the $k$ state-action pairs nearest to $(s', a')$ in Euclidean distance, and let $\{S_{R^i}\}_{i=1}^k$ denote their known response scores. We estimate $S_R'$ as a weighted average over its neighbors:
\begin{equation}
\small
S_R' = \frac{\sum_{i=1}^k {S_{R^i}/d_i}}{\sum_{i=1}^k 1/d_i}, \quad \mathrm{where} \quad d_i = \left\|[s' \; a'] - [s^i \; a^i]\right\|
\label{eq:ResponseScore}
\end{equation}
represents the distance between the $k$ nearest state-action pairs and the pending state-action pair $(s', a')$, with $[x \; y]$ denoting vector concatenation. In this way, we achieve a more robust offline estimation of response quality, effectively handling both NDE and OOD uncertainties within RCRL for LLM inference. 
% Our ablation studies in Sec.~\ref{ssec:ablation} that this nearest neighbor strategy noticeably improves the RCRL reward in \Name.


\subsection{Task-Modality Association}
\label{Sec. Association}
\Name is designed to operate in multi-modal, multi-task scenarios. In such settings, quantifying the relevance between users' text prompts and multi-modal data sources enables selection of the most pertinent multi-modal data sources to offload to the cloud LLM. Consequently, we employ feature extractors to transform both tasks and modalities into feature vectors of identical dimensionality for similarity computation. Given this premise, we leverage the pre-trained CLIP model \cite{radford2021learning}, a transformer-based architecture with text and image encoders, to extract features from prompts and multi-model data sources. We employ a normalized cross-modality similarity metric to calculate the association score between a text prompt $P$ and data modality $m$, which is defined as:
\begin{equation}
\small
\Lambda\left(P, D^{(m)}\right) = \frac{\left\langle \omega_T(P), \omega_I(D^{(m)}) \right\rangle}{\left\|\omega_T(P)\right\| \cdot \left\|\omega_I(D^{(m)})\right\|},
\label{eq:Association}
\end{equation}
where $\omega_T: P \rightarrow \mathbb{R}^d$ and $\omega_I: D^{(m)} \rightarrow \mathbb{R}^d$ represent the text and image encoders, respectively, mapping inputs to a shared $d$-dimensional embedding space.


\subsection{Latency Model}
\label{Sec. Latency Model}

\subsubsection{Local Computation Latency $\psi_\text{Local}$}
The computational latency of the local LLM, or say inference time, is influenced not only by the number of LLM parameters, but also significantly by the length of the prompt, denoted $|P|$, and of the response, $|R|$. Following \cite{canziani2016analysis}, we assume that the computational latency is proportional to the number of parameters. If the local LLM is comprised of $|\text{LLM}_\text{Local}|$ parameters and requires $2|\text{LLM}_\text{Local}|$ floating point operations (FLOPS) for forward propagation, the computational demand for each token in a reference processing length $|P|_\text{ref}$ is calculated as $\frac{2|\text{LLM}_\text{Local}|}{|P|_\text{ref}}$ FLOPS. The latency calculation considers the theoretical peak performance of the local device's GPU, $TF_\text{peak}$ FLOPS. The computational latency can be computed as:
\begin{equation}
\small
    \psi_\text{Local} = \frac{2|\text{LLM}_\text{Local}| (|P|+|R|)}{|P|_\text{ref} \times TF_\text{peak}}.
    \label{eq:LocalLatency}
\end{equation}

\subsubsection{Cloud Interaction Latency $\psi_\text{Cloud}$} The interaction time with the cloud LLM encompasses the entire process from sending the prompt to receiving the response. This includes uploading the prompt and modalities, inference by the cloud LLM, and downloading the response. We directly recorded the interaction time with the cloud LLM for different sets of uploaded modalities $\mathcal{M}$ during the creation of our \DatasetName dataset.




\subsection{Usage Cost Model} 
\label{Sec. Usage Cost Model}
During the inference process, LLMs incur computational costs. From the user's perspective, for the local LLM, this manifests as energy consumption, while for the cloud LLM, it translates into the service fee paid to the provider. 

\subsubsection{Local LLM Usage Cost $\phi_\text{Local}$}
The energy consumption of the local LLM is influenced by inference time and the power consumption of the computing hardware, which are in turn affected by the local device's computational capacity, the sizes of the prompt and response, and the size of the LLM, all encapsulated within the inference time $\psi_\text{Local}$. Let the maximum power consumption for a given local device hardware be $W_\text{max}$ Watts. By utilizing a normalized energy cost factor $\kappa$ (capturing e.g., local electricity rates and device wear-and-tear), after appropriate unit conversion, the local LLM usage cost (in USD) is defined as:
\begin{equation}
    \phi_\text{Local} = \psi_\text{Local} \times W_\text{max} \times \kappa.
    \label{eq:LocalUsageCost}
\end{equation}

\subsubsection{Cloud LLM Usage Function $\phi_\text{Cloud}$}
For cloud-based LLMs, in addition to the inference costs of prompts and responses, the inference costs of data modalities must also be considered. These costs are typically available on the service provider's website. Taking GPT-4o as an example, the prompt cost rate is $\varphi_P = \$ 0.005 / 1K$ tokens, the response cost rate is $\varphi_R = \$ 0.015 / 1K$ tokens, and the data modality costs are proportional to their size and resolution at a rate $\varphi_m$ for modality $m \in \mathcal{M}$. Therefore, the usage cost of the cloud LLM can be defined as follows:
\begin{equation}
    \phi_\text{Cloud} = \varphi_P \cdot |P| + \varphi_R \cdot |R| + \sum_{m \in \mathcal{M}} \varphi_m \cdot \big|D^{(m)}\big|.
    \label{eq:CloudUsageCost}
\end{equation}

\noindent\textbf{Summary of Reward Function.} Revisiting our initially defined reward function in Eq. (\ref{eq:reweard}), we substitute the direct response score $S_{R_t}$ with the estimated response score $S_{R_t}'$ defined in Eq. (\ref{eq:ResponseScore}) to achieve a more precise representation. The association metric is quantified through Eq. (\ref{eq:Association}). Both the latency formulations ($\psi_\text{Local}$ and $\psi_\text{Cloud}$) and usage cost formulations ($\phi_\text{Local}$ and $\phi_\text{Cloud}$) are incorporated as $\psi_{a_t}$ and $\phi_{a_t}$ in the reward function, respectively.



\section{Experimental Results}
\label{Sec. Experiment}

\subsection{\DatasetName Dataset}
\label{Sec. Dataset}

To evaluate performance in practical settings, we develop \DatasetName, the first multi-modal, multi-task, multi-dialogue, and multi-LLM dataset, as shown in Fig. \ref{Fig. M4A1}. \DatasetName consists of 21,014 dialogues, each meticulously compiled by randomly and sequentially choosing tasks, prompts, modalities, and candidate LLMs to ensure comprehensive coverage across different configurations. The key characteristics of \DatasetName can be summarized as follows:
\begin{itemize}[leftmargin=5mm]
    \item \textbf{Three Multi-Modal Data Sources} consist of images from different views in a unified scenario, including first-person, side, and overhead views.
    \item \textbf{Four Tasks} include Assistant, Recommendation, Query, and Message Editing, represent varying levels of task difficulty, relevance to modalities, and the types of actions that the LLM will undertake.
    \item \textbf{Two to Five Dialogues} include pairs of text prompts and responses within a continuous context.
    \item \textbf{Four LLMs} are a local LLM (Phi-3-mini), a cloud LLM (GPT-4o), a reference LLM (GPT-4o), and a judge LLM (GPT-4o).
    \item \textbf{Four Metrics} include response score (Sec. \ref{Sec. Response Score}), association (Sec. \ref{Sec. Association}), latency (Sec. \ref{Sec. Latency Model}), and usage cost (Sec. \ref{Sec. Usage Cost Model}).
\end{itemize}
More details about \DatasetName and its construction can be found in our online technical report \cite{yuan2025local}.


\begin{figure}[t]
    \centering\includegraphics[width=\linewidth]{Figure/M4A1.png}
    \vspace{-6mm}
    \caption{Illustration of \DatasetName Dataset.}
    \vspace{-4mm}
    \label{Fig. M4A1}
\end{figure}


\subsection{Experiment Setup}
\label{Sec. Experiment Setup}

\noindent\textbf{Training Setup.} The \DatasetName dataset is split into a training set and a test set in an 8:2 ratio. The training set is used to train RLs while the test set is used to evaluate the performance at inference. When implementing our approach, we consider several popular discrete RL algorithms, including PPO \cite{schulman2017proximal}, DQN \cite{mnih2015human}, and A2C \cite{mnih2016asynchronous}. We consider these methods with and without resource constraints (corresponding to cases with $\lambda>0$ and $\lambda=0$ in Eq. \eqref{eq:RCloss}), and denote RC-PPO, RC-DQN, and RC-A2C as the methods with the resource constraints. The policies are parameterized by multilayer perceptrons (MLP), with a total of 30,000 time steps set for training. For each experiment, we repeat the process three times and report the standard deviation as the error bar. We implement the \Name system and the baselines in PyTorch and conducted the experiments on an NVIDIA A100 GPU with 40 GB of memory. 

\noindent\textbf{Baselines.} We compare the RL policies with three naive baselines: random (with random LLM and modality selection), local (using only the local LLM), and cloud (using only the cloud LLM, but with random modality selection). Inspired by recent advances in \textit{LLM-as-Agent} \cite{xi2023rise, liu2023agentbench}, we substitute the RL agent with an LLM agent as \Name's decision engine for the selection of LLMs and modalities. Specifically, current states and possible actions are translated into natural language descriptions as contextual input for the LLM agents, which then output an action in response to this prompt, following an RL-like procedure. The LLM agents include Phi-3-mini, Phi-3.5-mini, LLaMA-3.2-3B, LLaMA-3.1-8B, Mistral-7B-v0.3, FLAN-T5-large, FLAN-T5-xl, Gemma-2-2B, Gemma-2-9B, GPT-3.5-turbo, GPT-4o-mini, GPT-4o, OpenAI o1-mini, OpenAI o1, and OpenAI o3-mini. We reiterate that none of the existing works can be directly applied to our setting, which features multi-modal, multi-task, and multi-dialogue characteristics. Therefore, we reimplement AIwRG \cite{he2024large}, an active inference method, and PerLLM \cite{yang2024perllm}, a bandit-based approach using upper confidence bound (UCB) for exploration-exploitation trade-off, integrating the settings introduced in this paper to adapt them to our research problem.

\noindent\textbf{Default Settings.} %Given the multitude of variables in our experiments, we adopt a default setting to minimize uncertainties while focusing on the impact of specific variables. 
The default experimental settings are as follows unless otherwise stated; we also study the effects of various system parameters. Specifically, we use RC-A2C as the RL policy, with $\alpha=1$, $\beta_{\Lambda} = 1/3$, $\beta_\psi = 1/3$, and $\beta_\phi = 1/3$ for the reward function weights, a 30 second latency constraint, a 0.05 USD usage cost constraint, and Jetson TX2 as the device type. These settings serve as the baseline from which variations are systematically introduced to assess their effects. Additionally, we set the time span $\tau=5$, the penalty coefficient $\lambda=10$, $k=5$ nearest neighbors for estimating the response score, and $\kappa = 4.63 \times 10^{-8}$ (Joules per USD) based on the average electricity price in the US \footnote{\url{https://www.energybot.com/electricity-rates/}}.




\newcommand{\com}[1]{\tiny$\pm$#1}
\newcolumntype{a}{>{\columncolor{gray!15}}c}

\begin{table*}[t]
\caption{Main Result - \Name with RC-A2C optimally balances local/cloud LLMs and adaptively selects modalities, outperforming in response quality, latency, and costs without constraint violations. The highlighted row indicates the highest overall reward without constraint violations.}
\vspace{-2mm}
\label{Table Main Result}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{5pt}
\centering 
\resizebox{\linewidth}{!}{
% \scriptsize
\begin{tabular}{l|ccc|a|ccccc|cc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \textbf{Response} & \textbf{Latency} & \textbf{Usage Cost} & \textbf{Overall} & \multirow{2}{*}{\textbf{Local}} & \multicolumn{4}{c|}{\textbf{Cloud - Num. Selected Modalities}} & \multicolumn{2}{c}{\textbf{Constraint Violation} ($\downarrow$)} \\
& \textbf{Score} ($\uparrow$) & (s) ($\downarrow$) & (1e-3 USD) ($\downarrow$) & \textbf{Reward} ($\uparrow$) & & 0 (text-only) & 1 & 2 & 3 & Latency & Usage Cost \\
\midrule



Random & 0.86 \com{0.02} & 10.00 \com{0.25} & 12.32 \com{0.23} & 0.71 \com{0.01} & 2097 & 261 & 775 & 771 & 255 & \cellcolor{No} 0.95 \com{0.13} & \cellcolor{No} 0.89 \com{0.83} \\
Local & 0.74 \com{0.04} & 0.04 \com{0.00} & 0.00 \com{0.00} & 0.74 \com{0.04} & 4203 & 0 & 0 & 0 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
Cloud & 1.04 \com{0.01} & 20.24 \com{0.18} & 25.14 \com{0.32} & 0.75 \com{0.01} & 0 & 515 & 1583 & 1540 & 544 & \cellcolor{No} 1.11 \com{0.05} & \cellcolor{No} 2.69 \com{0.34} \\
\bottomrule 



\multicolumn{11}{c}{\textbf{\centering \rule{0pt}{10pt} LLM-as-Agent \cite{xi2023rise,liu2023agentbench} for Inference Offloading (Ignored LLM Agent's Latency and Usage Cost)}} \\ 
\toprule
Phi-3-mini & 0.81 \com{0.04} & 5.25 \com{0.31} & 7.54 \com{0.44} & 0.74 \com{0.04} & 3089 & 0 & 613 & 207 & 295 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
Phi-3.5-mini & 0.83 \com{0.04} & 5.63 \com{0.38} & 8.39 \com{0.57} & 0.76 \com{0.04} & 3118 & 0 & 42 & 1044 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
LLaMA-3.2-3B & 1.05 \com{0.02} & 22.04 \com{0.27} & 35.08 \com{0.39} & 0.74 \com{0.02} & 78 & 101 & 58 & 2955 & 1011 & \cellcolor{No} 2.11 \com{0.04} & \cellcolor{No} 4.02 \com{0.16} \\
LLaMA-3.1-8B & 0.89 \com{0.02} & 10.84 \com{0.18} & 12.41 \com{0.31} & 0.74 \com{0.02} & 1933 & 381 & 1017 & 545 & 326 & \cellcolor{No} 1.54 \com{0.22} & \cellcolor{No} 3.79 \com{1.02} \\
Mistral-7B-v0.3 & 0.83 \com{0.03} & 14.46 \com{0.28} & 1.10 \com{0.02} & 0.64 \com{0.03} & 1519 & 2684 & 0 & 0 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
FLAN-T5-large & 1.01 \com{0.04} & 24.45 \com{0.28} & 47.91 \com{0.55} & 0.68 \com{0.04} & 0 & 0 & 0 & 0 & 4203 & \cellcolor{No} 4.90 \com{0.01} & \cellcolor{No} 11.14 \com{0.36} \\
FLAN-T5-xl & 0.85 \com{0.05} & 12.95 \com{0.15} & 9.59 \com{0.27} & 0.65 \com{0.05} & 1452 & 1053 & 1408 & 0 & 289 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
Gemma-2-2b & 0.74 \com{0.04} & 0.04 \com{0.00} & 0.00 \com{0.00} & 0.74 \com{0.04} & 4203 & 0 & 0 & 0 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
Gemma-2-9b & 0.74 \com{0.04} & 0.04 \com{0.00} & 0.00 \com{0.00} & 0.74 \com{0.04} & 4203 & 0 & 0 & 0 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
GPT-3.5-turbo & 0.99 \com{0.02} & 19.96 \com{0.51} & 33.75 \com{0.66} & 0.73 \com{0.01} & 527 & 190 & 504 & 692 & 2292 & \cellcolor{No} 2.97 \com{0.15} & \cellcolor{No} 5.86 \com{0.39} \\
GPT-4o-mini & 0.74 \com{0.04} & 0.04 \com{0.00} & 0.00 \com{0.00} & 0.74 \com{0.04} & 4203 & 0 & 0 & 0 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
GPT-4o & 0.85 \com{0.05} & 12.92 \com{0.29} & 0.98 \com{0.02} & 0.67 \com{0.05} & 1807 & 2396 & 0 & 0 & 0 & \cellcolor{No} 2.26 \com{0.01} & \cellcolor{Yes} 0.00 \com{0.00} \\
OpenAI o1-mini \textsuperscript{*} & 0.77 \com{0.03} & 8.21 \com{0.43} & 0.62 \com{0.03} & 0.66 \com{0.03} & 222 & 127 & 0 & 0 & 0 & \cellcolor{No} 1.13 \com{0.81} & \cellcolor{Yes} 0.00 \com{0.00} \\
OpenAI o1 \textsuperscript{*} & 0.76 \com{0.02} & 5.18 \com{0.16} & 0.39 \com{0.01} & 0.69 \com{0.02} & 269 & 80 & 0 & 0 & 0 & \cellcolor{No} 0.38 \com{0.54} & \cellcolor{Yes} 0.00 \com{0.00} \\
OpenAI o3-mini \textsuperscript{*} & 0.86 \com{0.04} & 16.91 \com{1.18} & 1.28 \com{0.09} & 0.63 \com{0.03} & 87 & 262 & 0 & 0 & 0 & \cellcolor{No} 2.09 \com{0.06} & \cellcolor{Yes} 0.00 \com{0.00} \\
\bottomrule 



\multicolumn{11}{c}{\textbf{\centering \rule{0pt}{10pt} Comparison with SOTA Exploration-Decision Baselines}} \\ 
\toprule
AIwRG \cite{he2024large} & 1.02 \com{0.05} & 23.73 \com{0.98} & 44.01 \com{3.50} & 0.69 \com{0.06} & 113 & 244 & 0 & 0 & 3852 & \cellcolor{No} 4.68 \com{0.18} & \cellcolor{No} 9.26 \com{1.83} \\
PerLLM \cite{yang2024perllm} & 0.97 \com{0.06} & 21.12 \com{0.08} & 1.60 \com{0.01} & 0.66 \com{0.06} & 281 & 3922 & 0 & 0 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
\Name (PPO) & 1.02 \com{0.00} & 17.89 \com{0.82} & 22.48 \com{1.48} & 0.76 \com{0.01} & 142 & 40 & 2499 & 1355 & 121 & \cellcolor{No} 0.70 \com{0.77} & \cellcolor{No} 1.66 \com{2.35} \\
\Name (DQN) & 1.02 \com{0.08} & 19.57 \com{2.18} & 26.23 \com{6.36} & 0.75 \com{0.05} & 0 & 5 & 1820 & 2367 & 0 & \cellcolor{No} 0.81 \com{0.57} & \cellcolor{Yes} 0.00 \com{0.00} \\
\Name (A2C) & 1.05 \com{0.03} & 18.81 \com{3.44} & 26.44 \com{13.05} & 0.79 \com{0.07} & 98 & 0 & 2753 & 210 & 1168 & \cellcolor{No} 1.48 \com{2.09} & \cellcolor{No} 2.38 \com{3.36} \\
\Name (RC-PPO) & 1.03 \com{0.03} & 18.33 \com{1.87} & 22.43 \com{3.57} & 0.77 \com{0.04} & 90 & 105 & 2599 & 1309 & 110 & \cellcolor{No} 0.45 \com{0.64} & \cellcolor{Yes} 0.00 \com{0.00} \\
\Name (RC-DQN) & 1.05 \com{0.05} & 18.87 \com{0.81} & 23.29 \com{3.59} & 0.78 \com{0.06} & 46 & 174 & 2249 & 1636 & 85 & \cellcolor{No} 0.53 \com{0.33} & \cellcolor{No} 0.42 \com{0.60} \\
\rowcolor{Yes} \textbf{\Name (RC-A2C)} & 1.09 \com{0.08} & 16.63 \com{0.67} & 17.97 \com{1.18} & \textbf{0.85 \com{0.07}} & 74 & 12 & 3871 & 225 & 6 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
\bottomrule 

\multicolumn{11}{c}{\textbf{\centering \rule{0pt}{10pt} Our \Name System - Ablation Study (Using RC-A2C as Backbone)}} \\ 
\toprule

w/o LLM Sel. & 0.85 \com{0.00} & 9.25 \com{1.02} & 11.67 \com{3.14} & 0.72 \com{0.02} & 2128 & 6 & 1228 & 830 & 1 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
w/o Modality Sel. & 1.04 \com{0.02} & 20.13 \com{0.07} & 24.82 \com{0.21} & 0.75 \com{0.02} & 0 & 528 & 1591 & 1519 & 528 & \cellcolor{No} 1.17 \com{0.07} & \cellcolor{No} 2.74 \com{0.87} \\
w/o Score Est. & 1.01 \com{0.01} & 16.71 \com{0.28} & 17.76 \com{0.59} & 0.76 \com{0.01} & 0 & 0 & 4095 & 89 & 0 & \cellcolor{Yes} 0.00 \com{0.00} & \cellcolor{Yes} 0.00 \com{0.00} \\
\bottomrule
\end{tabular}
}

\vspace{2pt}
\justifying

\noindent{\footnotesize \textsuperscript{*} Due to the considerable inference time and usage cost of OpenAI o1-mini, OpenAI o1, and OpenAI o3-mini, we randomly sample 100 instances for evaluation.}
\vspace{-2mm}
\end{table*}




\subsection{Main Experimental Results}

Table \ref{Table Main Result} presents the main results of this study, with key takeaways described from four perspectives. 

\noindent\textbf{Comparison with Naive Baselines.} Compared to the local baseline, the proposed methods can significantly improve the response score by sacrificing latency and usage cost, thus enhancing the overall reward. Compared to the cloud baseline, we also boost the response score and conserve resources by selecting the appropriate modalities, which leads to improved rewards. In contrast to the random baseline, our approach strategically explores the relationships between tasks/dialogues and LLMs/modalities, thereby optimizing decision-making processes to better align with the specific requirements and constraints of each task.

\noindent\textbf{Comparison with SOTA LLM-as-Agent Baselines.} We replace the RL agent in \Name with an LLM agent for decision-making processes. Our goal is to evaluate the zero-shot LLM agent's ability to consider task-modality associations, understand contexts, and handle numerically sensitive tasks (e.g., avoiding resource constraint violations). As shown in Table \ref{Table Main Result}, different LLM agents exhibit substantial variability in performance. Some agents ignore all modalities (e.g., Mistral-7B-v0.3, Gemma-2-2b, Gemma-2-9b, GPT-4o-mini, GPT-4o, OpenAI o1-mini, OpenAI o1, and OpenAI o3-mini), while others choose to upload all available modalities for each task (e.g., FLAN-T5-large). Although some agents (e.g., Phi-3-mini, Phi-3.5-mini, and FLAN-T5-xl) appear to select different LLMs and modalities without violating any constraints, their response scores and overall rewards do not confer significant advantages. Thus, these zero-shot LLM agents do not demonstrate clear benefits. One reason is that they are not fine-tuned in the training dataset. Another reason is that, as generative models, LLM agents lack precise numerical calculation capabilities. Consequently, some agents violate constraints, whereas others adopt overly conservative strategies.

\noindent\textbf{Comparison with SOTA Exploration-Decision Baselines.} Existing works do not consider multi-task and multi-dialogue settings in a multi-modal scenario, and cannot be trivially deployed to our setting. For example, \cite{he2024large} utilize a benchmark with a fixed evaluation metric Pass@k that measures success rates. In contrast, our method addresses a multi-modal multi-task and multi-dialogue setting which introduces significant challenges related to response score evaluation and uncertainty. From the results, our \Name system significantly outperforms these SOTA baselines. \cite{he2024large} fails to efficiently select LLMs and modalities, frequently opting to upload all three types of modality data sources for most actions. This results in substantial resource waste and redundancy in uploading unnecessary information to the cloud. Conversely, \cite{yang2024perllm} rarely considers any modality data, relying solely on the local LLM and text-only cloud LLM. Although this approach avoids violating any resource constraints, it leads to lower response scores, as the LLM lacks access to any information derived from modality data.

\noindent\textbf{Analysis of Our \Name System.} Most RL algorithms prefer to utilize the cloud LLM rather than the local LLM, as the gain in response score from the cloud LLM is much greater than the resource savings achieved by the local LLM. Regarding modality selection, few RL algorithms choose a text-only cloud LLM, as it often results in interaction times comparable to or even longer than those with multi-modal inputs. This largely depends on the inference speed of the service provider rather than factors like network bandwidth. A potential reason is that text-only and multi-modal cloud LLMs may operate on different workflows and versions. However, in cases involving multi-modal inputs, we observe that single modality selection often receives preferential treatment by RL algorithms, as it provides valuable information with less latency and lower usage costs compared to selecting two or three modalities. RC-A2C demonstrates the best response scores and overall rewards while ensuring compliance with latency and usage cost constraints.

\subsection{Ablation Studies}
\label{ssec:ablation}
To validate our approach, we conduct ablation studies (Table \ref{Table Main Result}, last three rows) by removing key components: RQ1 -- LLM selection (substituting with random LLM selection), RQ2 -- modality selection (using random modality selection instead), and RQ3 -- uncertain response score estimation (using average action scores). Random LLM selection degrades response scores as local LLMs often lack task capabilities. Random LLM selection degrades response scores as local LLMs often lack task capabilities. Lastly, using average response scores fails to account for contextual information, particularly historical modality uploads, leading to biased estimations and degraded performance. Our results demonstrate that effective LLM selection matches tasks with appropriately capable LLMs, while modality selection enhances modality-task alignment, minimizing latency and costs. Additionally, uncertain response score estimation leverages contextual information for more accurate estimations and informed decisions.


\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\begin{table}[t]
\caption{Trade-off between Metrics in Reward Function.}
\vspace{-2mm}
\label{Table Trade-off}
\setlength{\tabcolsep}{5pt}
\centering 
\resizebox{\linewidth}{!}{
\begin{tabular}{C{15pt}C{15pt}C{15pt}cccc}
\toprule
\multirow{2}{*}{$\beta_{\Lambda}$} & \multirow{2}{*}{$\beta_\psi$} & \multirow{2}{*}{$\beta_\phi$} &\textbf{Response} & \textbf{Association} & \textbf{Latency} & \textbf{Usage Cost} \\
& & & \textbf{Score} ($\uparrow$) & ($\uparrow$) & (s) ($\downarrow$) & (1e-3 USD) ($\downarrow$) \\
\midrule

0 & 0.5 & 0.5 & 0.76 \com{0.06} & 0.02 \com{0.03} & 1.59 \com{2.20} & 1.63 \com{2.31} \\
0.5 & 0 & 0.5 & 1.05 \com{0.07} & 0.39 \com{0.10} & 21.04 \com{1.77} & 31.91 \com{7.11} \\
0.5 & 0.5 & 0 & 0.98 \com{0.04} & 0.55 \com{0.05} & 23.26 \com{0.92} & 42.97 \com{3.27} \\
\rowcolor{Yes} 1/3 & 1/3 & 1/3 & 1.09 \com{0.08} & 0.21 \com{0.01} & 16.63 \com{0.67} & 17.97 \com{1.18} \\

\bottomrule
\end{tabular}
}
\vspace{-5mm}
\end{table}


\begin{figure}[t]
    \centering
    \subfloat[Latency Constraint]{\includegraphics[width=0.5\linewidth]{Figure/Effect_Latency_Constraint.png}}
    \hfill
    \subfloat[Usage Cost Constraint]{\includegraphics[width=0.5\linewidth]{Figure/Effect_Usage_Constraint.png}}
\vspace{-3mm}
    \caption{Effect of Constraints. \Name demonstrates a faster reduction in resource constraint violations compared to baselines.}
    \label{Fig. Effect Constraints} 
\vspace{-5mm}
\end{figure}

    
\subsection{Trade-off Between Metrics}

In Table \ref{Table Trade-off}, we investigate the trade-offs among different metrics in the reward function by adjusting the weights for association, latency, and usage cost in Eq.~\eqref{eq:reweard}, denoted as $\beta_{\Lambda}$, $\beta_\psi$, and $\beta_\phi$, respectively. We fix the weight of the response score to $\alpha=1$. Due to the presence of resource constraints, simply setting $\beta_\psi=0$ or $\beta_\phi=0$ not only causes high latency or high usage cost, respectively, but also does not provide a benefit to the response score. Although the RL policy is penalized by the resource constraint in the loss function, it lacks explicit action-level penalties. As a result, the policy may only account for cumulative costs rather than recognizing individual action expenses, because uploading three modality data sources sequentially or simultaneously incurs roughly the same total resource cost. Consequently, when we balance the weights by setting $\beta_{\Lambda} = \frac{1}{3}, \beta_\psi = \frac{1}{3}, \beta_\phi = \frac{1}{3}$, we can attain a high response score while maintaining relatively lower latency and usage costs.


\subsection{Effect of Resource Constraints}

In Fig. \ref{Fig. Effect Constraints}, we evaluate the performance of different methods under varying latency and usage cost constraints, reflecting real-world scenarios where applications operate under different budgetary considerations. As latency and usage cost constraints increase, all methods demonstrate reduced constraint violations, with ours achieving more rapid reduction. This improvement stems from resource constraints incorporated into the loss function to balance the long-term cumulative rewards. In all different constraint settings, the proposed approach achieves the highest overall rewards, demonstrating its effectiveness in practical system deployments by considering response quality, latency, and usage cost altogether. However, under extremely limited resources, ours still struggles to effectively balance the trade-off between response score and resource costs. Therefore, in practical system deployments, beyond employing resource constraints to optimize long-term cumulative reward, establishing safety layers is also essential.


\subsection{Effect of Different Local Devices}

To further confirm the advantage of our approach, in Fig. \ref{Fig. Effect Device}, we explore the effects of using different types of local devices, with detailed specifications available in Table \ref{Table Device}. We infer that the performance of different devices does not significantly affect the operation of the \Name system, since their response scores and overall rewards do not show statistical significance. The underlying rationality is that for any device, latency and usage cost are normalized through min-max scaling and incorporated as part of the overall reward. The results are consistent with those in Table \ref{Table Main Result}, showing that our approach achieves (i) significant latency and cost savings compared to the cloud LLM baseline and (ii) significant improvements in response quality compared to the local LLM baseline, leading to the highest overall reward. Therefore, when the latency and usage cost of a local LLM are substantially lower than those of a cloud LLM, the \Name system can operate effectively on any device, a precondition that forms part of the background for the \Name system.

\begin{table}[t]
\caption{Local Devices Setting.}
\vspace{-2mm}
\label{Table Device}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Local Device}} & \textbf{Performance} & \textbf{Power} & \textbf{Latency} & \textbf{Usage Cost} \\ 
& (TFLOPS) & (Watts) & (s) & (1e-3 USD) \\ 
\midrule
Raspberry Pi-4B & 0.0135 & 8 & 1.12593 & 4.17e-4 \\
Raspberry Pi-5 & 0.0314 & 12 & 0.48408 & 2.69e-4 \\
Jetson Nano & 0.472 & 10 & 0.03220 & 1.49e-5 \\
Jetson TX2 & 1.33 & 15 & 0.01143 & 7.94e-6 \\
Jetson Xavier NX & 21 & 20 & 0.00072 & 6.71e-7 \\
Jetson Orin NX & 100 & 25 & 0.00015 & 1.76e-7 \\
iPhone 15 (A16) & 15.8 & 15 & 0.00096 & 6.69e-7 \\
iPhone 15 Pro (A17 Pro) & 35 & 15 & 0.00043 & 3.02e-7 \\
\bottomrule
\end{tabular}
}
\vspace{-5mm}
\end{table}


\begin{figure}[t]
    \centering
    \subfloat[Response Score \& Overall Reward]{\includegraphics[width=0.5\linewidth]{Figure/Effect_Devices_1.png}}
    \hfill
    \subfloat[Latency \& Usage Cost]{\includegraphics[width=0.5\linewidth]{Figure/Effect_Devices_2.png}}
\vspace{-3mm}
    \caption{Effect of local device performance. \Name achieves superior performance on all local device compared to baselines.}
    \label{Fig. Effect Device}
\vspace{-5mm}
\end{figure}





\subsection{Effect of Local LLM Response Quality}

Finally, we evaluate the \Name system's performance by simulating varying local LLM response qualities. Fig. \ref{Fig. Effect LLM Gap} demonstrates these results, wherein we modify the response scores in the \DatasetName dataset by applying specific adjustments (represented as the ``gap'' in the figure) to simulate different local LLM capabilities. The \Name system consistently outperforms all three baselines regardless of local and cloud LLM response quality, demonstrating its effectiveness across any combination of local and cloud LLMs. When the local LLM response quality is substantially lower than the cloud LLM quality, \Name consistently selects the cloud LLM and proceeds with the modality selection. As the quality of the local LLM response approaches the quality of the cloud LLM, \Name balances between the local and cloud options. 
An extreme scenario would be using identical LLMs locally and in the cloud, \Name is still capable of delivering a superior response quality. This holds under the assumption that the local LLM does not support multi-modal inputs (or, considering a broader perspective, the cloud LLM cannot access user multi-modal data due to privacy concerns), which reflects a distinctive challenge described in RQ1. These results confirm our methodology's applicability across diverse local LLM levels.


\begin{figure}[t]
    \centering
    \subfloat[Response Score]{\includegraphics[width=0.5\linewidth]{Figure/Effect_LLM_Gap_1.png}}
    \hfill
    \subfloat[Overall Reward]{\includegraphics[width=0.5\linewidth]{Figure/Effect_LLM_Gap_2.png}}
\vspace{-3mm}
    \caption{Effect of response quality of local LLM. \Name outperforms baselines across various deployments of local and cloud LLMs.}
    \label{Fig. Effect LLM Gap}
\vspace{-4mm}
\end{figure}
 
\section{Conclusion}
\label{Sec. Conclusion}
 
In this paper, we developed \Name, a novel local-cloud LLM inference offloading system designed specifically for multi-modal, multi-task, and multi-dialogue scenarios. We formulated the joint LLM and modality selection process as a resource-constrained reinforcement learning (RCRL) problem that maximizes long-term cumulative reward (response quality, latency, and usage cost) under practical user-defined constraints. Additionally, we curated a novel dataset, \DatasetName, enabling evaluation of \Name system performance across various configurations of modality availability, tasks and dialogues, resource constraints, and LLM response qualities. We demonstrated improvements in the overall reward obtained by \Name over several baselines. Future research will investigate multi-user collaborative inference through resource sharing and examine multi-server adaptive selection based on latency, cost, and other considerations.






\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}

% \begin{comment}
    
\appendix

\onecolumn

\section{\DatasetName Dataset}
\label{appendix:Dataset}

\subsection{\DatasetName Construction Details}

To construct the \DatasetName dataset, we integrate two predetermined datasets that provide complementary modalities, \textit{images} and \textit{text}. First, we use the \textbf{ActionSense} dataset 
% \cite{delpreto2022actionsense} 
to provide images, offering various camera views of human along with labels for each kitchen activity within the scenario, as illustrated in the bottom left of Fig. \ref{Fig. M4A1}. ActionSense is a large dataset of human kitchen activities that includes multi-modal wearable sensor data and environmental camera data, capturing 20 different kitchen activities such as peeling cucumbers, slicing potatoes, and cleaning plates with a sponge. These annotated kitchen activity labels function as responses to queries such as ``What activity am I doing right now?'' Given that current LLMs rarely process time-series sensor data (such as data from gyroscopes and accelerometers), we consider three types of image modalities: first-person images from a wearable sensor, and side and overhead views from environmental cameras. Second, the \textbf{instruction} dataset is used to provide structured text data, enriching the dataset with linguistic elements crucial for LLM inference and evaluation. This dataset provides text for four different tasks with prompts and reference responses. For each prompt, a reference response is also provided. Note that the reference responses do not affect the evaluation because the reference LLM will reprocess the content and format of the reference responses. 

Each sample in \DatasetName is constructed based on a set of three multi-modal data from ActionSense, paired with multiple randomly selected prompt and reference response pairs from the instruction dataset. The detailed data collection is presented in Alg. \ref{alg:dataset}.


\begin{algorithm*}[h]
\caption{\DatasetName Dataset Collection}
\label{alg:dataset}
\raggedright
\textbf{Input:} Local LLM, cloud LLM, reference LLM, judge LLM, image dataset (e.g., ActionSense), instruction dataset, time span $(\tau)$\\
\textbf{Output:} \DatasetName dataset

\begin{algorithmic}[1]
\While{collecting data}
    \State Randomly select a moment in the scenario and retrieve images with various modalities.
    \For{a random number of dialogues from 2 to $\tau$}
        \State Randomly select a user prompt from the instruction dataset.
        \State Randomly choose either the local LLM or the cloud LLM.
        \If{the chosen LLM is the cloud LLM}
            \State Randomly select which modality (text-only, a single view, or multiple views) to upload to the cloud LLM.
        \EndIf
        \State Generate a response using the selected LLM based on the context of prior dialogues, the current prompt, and the selected modality. 
        \Statex \Comment{Eqs.~\eqref{Eq. LLM_1},~\eqref{Eq. LLM_2}}
        \State The reference LLM generates a reference response based on the prompt and a human-generated reference response.
        \State The judge LLM assigns a response score given the prompt, the candidate response, and the reference response.
        \State Compute the local latency $\psi_{a}$ or record the real interaction time with the cloud LLM provider. \Comment{Eq.~\eqref{eq:LocalLatency}}
        \State Compute the usage cost $\phi_{a}$ for either the local or cloud LLM. \Comment{Eqs.~\eqref{eq:LocalUsageCost},~\eqref{eq:CloudUsageCost}}
    \EndFor
    \For{each prompt $P$ and each multimodal data $D^{(m)}$ in the current sample}
        \State Compute the association $\Lambda(P, D^{(m)})$ by the pre-trained CLIP model. \Comment{Eq.~(\ref{eq:Association})}
    \EndFor
\EndWhile

\end{algorithmic}
\end{algorithm*}


\subsection{Dataset Example}

Fig. \ref{Fig. Dataset Example} presents a representative sample from our dataset. It is important to note that this dataset was generated through random sampling to facilitate exploration and learning within the reinforcement learning (RL) framework. The ``action'' notation in this context refers to randomly configured actions rather than those selected by the RL policy. This randomly collected dataset encompasses diverse possibilities, enabling the RL algorithm to identify patterns and optimize decision-making processes.

\begin{tcolorbox}[boxrule = 0.5pt, breakable, box align=center,]


\textbf{Dialogue \#1:}
\begin{lstlisting}[language=JSON,escapeinside={(*}{*)}]
{
    "question": "Please draft a text message asking my friend to bring some ketchup on their way over.",
    "task_cat": "Message Editing",
    "action": 1,
    "image_index": [
        0,
        1,
        2
    ],
    "answer": "Here's a text message you can send to your friend:\n\n---\n\nHey [Friend's Name], could you bring over some ketchup when you come by? Thanks a lot! \n\n---\n\nFeel free to personalize it with your friend's name!",
    "score": 1.1666666666666667,
    "interaction_time": 5.88208794593811,
    "usage_cost": 0.013874999999999998
}
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Dialogue \#2:}
\begin{lstlisting}[language=JSON,escapeinside={(*}{*)}]
{
    "question": "Please create a text message to tell my friends what I am doing and I will get back to them later.",
    "task_cat": "Message Editing",
    "action": 1,
    "image_index": null,
    "answer": "Here's a text message you can send to your friends:\n\n---\n\nHey everyone! I'm currently working on something in the kitchen, so I'll get back to you later. Talk soon! \n\n---\n\nFeel free to adjust it if needed!",
    "score": 0.8888888888888888,
    "interaction_time": 9.501538038253784,
    "usage_cost": 0.00064
}
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Dialogue \#3:}
\begin{lstlisting}[language=JSON,escapeinside={(*}{*)}]
{
    "question": "I'm inviting friends over for a dinner party. Do you have any simple cocktail recommendations?",
    "task_cat": "Recommendation",
    "action": 0,
    "image_index": null,
    "answer": "Absolutely! Here's a text message you can send to your friends:\n\n---\n\nHi friends! I'm hosting a dinner party this weekend and I'm looking for some cocktail recommendations. Any suggestions? Cheers! \n\n---\n\nFeel free to personalize it with your friends' names if you'd like!",
    "score": 0.0,
    "interaction_time": 2.770137310028076,
    "usage_cost": 5.6000000000000006e-05
}
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Dialogue \#4:}
\begin{lstlisting}[language=JSON,escapeinside={(*}{*)}]
{
    "question": "Where did I put my dishes?",
    "task_cat": "Query",
    "action": 1,
    "image_index": null,
    "answer": "Based on the images, it looks like you might have been working on something in the kitchen recently. Here are some likely places where your dishes might be:\n\n- **On the countertops:** Check near the sink, stove, or prep area.\n- **In the sink:** They might be stacked there if you haven't washed them yet.\n- **In the dishwasher:** If you've already cleaned them, they could be in the dishwasher.\n- **In a drying rack or on a towel:** If you've washed them, they might be air-drying.\n- **In the cabinets:** If you've put them away, check your usual storage spots.\n\nI hope this helps you find your dishes!",
    "score": 1.2857142857142858,
    "interaction_time": 11.087376117706299,
    "usage_cost": 0.00034
}
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Multi-Modal Data Sources:}

\includegraphics[width=0.3\linewidth]{Figure/Modality_0.png}
\hfill
\includegraphics[width=0.3\linewidth]{Figure/Modality_1.png}
\hfill
\includegraphics[width=0.3\linewidth]{Figure/Modality_2.png}

\end{tcolorbox}
\vspace{-10pt}
\captionof{figure}{A representative example of the \DatasetName dataset.}
\label{Fig. Dataset Example}



\section{Prompt Template}

\subsection{System Prompt Configuration for \Name}

\colorlet{key}{red!80!black}

\begin{tcolorbox}[boxrule = 0.5pt, breakable, box align=center,]
\textbf{Cloud LLM:}
\begin{lstlisting}[language=system prompt,escapeinside={(*}{*)}]
(*\textcolor{key}{"system\_prompt"}*) = 
```
Imagine you are an advanced smart home assistant tasked with analyzing situations and providing feedback based on questions and visual data. Your capabilities include understanding complex scenarios from text descriptions, offering advice, helping edit texts, identifying objects or situations, and making predictions about potential next steps or actions. Please solve the problem to the best of your ability, assuming you are able to access, process, and operate various smart devices in your smart home, including all lights, thermostats, security cameras, and even residents' mobile phones. Given the current question, generate a response that aligns with your role as a smart home assistant. Given the current question and image, integrate this information to generate a response that aligns with your role as a smart home assistant. \n\n In generating a response, consider the sequence of questions and images provided, with newer questions and their associated images given more weight. Please also assess the relevance of past questions and images, especially in relation to their similarity to the current question and image. \n Current Image is the last image in the list.
```
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Local LLM:}
\begin{lstlisting}[language=system prompt,escapeinside={(*}{*)}]
(*\textcolor{key}{"system\_prompt"}*) = 
```
Imagine you are an advanced smart home assistant tasked with analyzing situations and providing feedback based on questions and visual data. Your capabilities include understanding complex scenarios from text descriptions, offering advice, helping edit texts, identifying objects or situations, and making predictions about potential next steps or actions. Please solve the problem to the best of your ability, assuming you are able to access, process, and operate various smart devices in your smart home, including all lights, thermostats, security cameras, and even residents' mobile phones. Given the current question, generate a response that aligns with your role as a smart home assistant. \n\n In generating a response, consider the sequence of questions provided, with newer questions given more weight. Please also assess the relevance of past questions, especially in relation to their similarity to the current question.
```
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Judge LLM:}
\begin{lstlisting}[language=system prompt,escapeinside={(*}{*)}]
(*\textcolor{key}{"system\_prompt"}*) = 
```
We would like to request your feedback on the performance of two smart home assistants in response to the user question displayed above. \n Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. \n Please output a single line containing only two values indicating the scores for Assistant 1 and 2 in the following format: 'Scores: [Assistant 1 score], [Assistant 2 score]'. The two scores are separated by a space. \n Please avoide any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. \n In addition, if an assistant's response indicates that it does not know the answer or is unable to provide an answer to the question, its score should be assigned as 0.
```
\end{lstlisting}

\rule{\linewidth}{0.5pt}

\textbf{Reference LLM:}
\begin{lstlisting}[language=system prompt,escapeinside={(*}{*)}]
(*\textcolor{key}{"system\_prompt"}*) = 
```
Given the following question and the detailed textual description of an image, provide an answer that reflects an understanding of both the question's intent and the details provided in the image description.
```
\end{lstlisting}

\end{tcolorbox}
\vspace{-10pt}
\captionof{figure}{Prompt template for four LLMs in our \Name system.}
\label{Fig. Our Prompt Template}


\subsection{System Prompt Configuration for LLM-as-Agent}

\begin{tcolorbox}[boxrule = 0.5pt, breakable, box align=center,]
\textbf{LLM-as-Agent:}
\begin{lstlisting}[language=system prompt,escapeinside={(*}{*)}]
(*\textcolor{key}{"system\_prompt"}*) = 
```
You are a decision-making assistant for an intelligent inference offloading system. Your role is to analyze observations and recommend the most appropriate action based on the given information. The action selection aims to maximize response quality while minimizing latency and usage costs. 

The actions are as follows (with their respective latencies and costs):
- **Action 0**: Use a small-scale local LLM for inference. Latency: **0.011 seconds**, Cost: **7.94285e-09 USD**.
- **Action 1**: Use a large-scale cloud-based LLM for pure text-based inference. Latency: **6.46030 seconds**, Cost: **0.00049 USD**.
- **Action 2**: Use a large-scale multimodal cloud-based LLM with the uploaded first-person view image (Image 0). Latency: **4.76134 seconds**, Cost: **0.00500 USD**.
- **Action 3**: Use a large-scale multimodal cloud-based LLM with the uploaded top-down view image (Image 1). Latency: **4.76134 seconds**, Cost: **0.00500 USD**.
- **Action 4**: Use a large-scale multimodal cloud-based LLM with the uploaded side view image (Image 2). Latency: **4.76134 seconds**, Cost: **0.00500 USD**.
- **Action 5**: Use a large-scale multimodal cloud-based LLM with the uploaded top-down (Image 1) and side view (Image 2) images. Latency: **6.24569 seconds**, Cost: **0.00945 USD**.
- **Action 6**: Use a large-scale multimodal cloud-based LLM with the uploaded first-person view (Image 0) and top-down view (Image 1) images. Latency: **6.24569 seconds**, Cost: **0.00945 USD**.
- **Action 7**: Use a large-scale multimodal cloud-based LLM with the uploaded first-person view (Image 0) and side view (Image 2) images. Latency: **6.24569 seconds**, Cost: **0.00945 USD**.
- **Action 8**: Use a large-scale multimodal cloud-based LLM with all three uploaded images (first-person view, top-down view, and side view). Latency: **6.98211 seconds**, Cost: **0.01368 USD**.

Observations include:
1. **Previous actions**: A history of all previously taken actions, which can be either local inference or cloud-based inference (with or without images).
2. **Uploaded images**: Status of image uploads, including whether the first-person view, top-down view, and/or side view images are available for use.
3. **Current task**: The type of task being performed, which could be one of the following:
   - Assistive System
   - Message Editing
   - Query
   - Recommendation

Your response must be **a single integer (0-8)** indicating the most appropriate action based on the observations provided. Provide no additional explanation, and ensure the action aligns with the task's requirements, available resources, and uploaded images.
```
\end{lstlisting}

\end{tcolorbox}
\vspace{-10pt}
\captionof{figure}{Prompt template for LLM agents.}
\label{Fig. Our Prompt Template}


    
% \end{comment}


\end{document}