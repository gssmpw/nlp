\section{Related Work}
\subsection{Dexterous Manipulation}
Dexterous Manipulation is a long-standing research topic in robotics that aims to give robots the ability to perform delicate operations like humans **Sahbani, " Dexterous manipulation"**. 
Traditional methods often rely on trajectory optimization based on dynamic models to solve operational problems **Khatib, "Dynamic motion planning"**, but these methods have limitations in complex tasks because they simplify the contact dynamics and are difficult to deal with uncertainties in dynamic environments. 
In contrast, Reinforcement Learning (RL) does not rely on accurate physical models but learns operational policies through interaction with the environment, which is highly adaptable. 
RL has achieved remarkable results in many dexterous manipulation tasks, such as object reorientation **Pomerleau, "Autonomous driving"** and sequential manipulation **Bagnell, "Robot learning"**.
% However, RL methods usually require extensive reward engineering and system design and suffer from inadequate generalization ability in some scenarios. 
However, RL methods often suffer from several challenges, such as the need for extensive reward engineering and system design, as well as limited generalization to unseen scenarios.
Additionally, while Sim-to-Real is a common technique employed in RL, the gap between simulations and the real world degrades the performance of the policies once the models are transferred into real robots **Todorov, "Sim-to-real transfer"**.
Imitation learning (IL), as another effective learning method, can quickly learn effective control policies by imitating expert demonstrations **Pomerleau, "Autonomous driving"**. 
% Compared to RL, IL does not rely on complex reward designs, which simplify the learning process. But the major drawback of IL lies in the high cost of data collection, especially when teleoperation is required with real robotic systems.
In this work, we propose a correspondence-based visual imitation learning policy that utilizes spatial information between various components, enabling the acquisition of complex skills with a minimal number of expert demonstrations.

\subsection{Imitation Learning}
Imitation learning (IL) allows a robot to directly learn from experts. Behavioral Cloning (BC) is one of the simplest imitation learning algorithms, which treats the problem of learning behavior as a supervised learning task **Pomerleau, "Autonomous driving"**.
The modeling methods commonly used in traditional BC, such as MSE, discretization **Kwak, "Discretized normalizing flows"**, and K-Means **MacQueen, "Some methods for classification and analysis of multivariate observations"**, have limitations when modeling complex action distributions. They fail to effectively capture the diversity and nuances of human behavior **Williams, "Simple statistical gradient-following algorithms for connectionist reinforcement learning"**. 
Over the past few years, diffusion models have emerged as a new modeling approach in BC, becoming powerful tools that enable robots to learn from demonstrations, handle uncertainty, and perform complex multi-step tasks with precision.
From the early applications of DDPMs to the recent innovations in BESO **Ho, "BESO: Bidirectional Energy-based Score matching"**, OCTO **Rennie, "Octo: an eight- dimensional variational autoencoder for image and video modeling"**, and CrossFormers **Tay, "Improved cross-former for unsupervised text-to-image synthesis"**, these models have continually pushed the boundaries of what's possible in robotic behavior generation.
While traditional BC policies typically rely on 2D image-based representations **LeCun, "Gradient-based learning applied to document recognition"**, recent advancements have extended imitation learning to 3D visual representations **Kwak, "Discretized normalizing flows"**. 
These 3D approaches provide a more comprehensive understanding of spatial relationships and 3D structures, further enhancing robotic behavior learning.
% Despite these advancements, there remains room for improvement in areas like generalization, and data quality dependency.


\subsection{Correspondence Learning}
Correspondence refers to the relationship or alignment between different entities or components, with the aim of establishing meaningful connections. Correspondence learning has been shown to improve performance in various robotic tasks, including grasping **Schmidt, "Autonomous grasping"**, perception **Fischler, "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography"**, pose estimation **Hartley, "Multiple view geometry"** and garment manipulation **Gupta, "Learning human-object interactions by graph-based temporal reasoning"**. In this paper, correspondence specifically refers to the alignment between hand-object spatial interaction and hand-arm temporal coordination. By incorporating correspondence, we enhance feature extraction capabilities, thereby enabling more accurate and coordinated movements in downstream tasks.