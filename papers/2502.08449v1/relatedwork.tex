\section{Related Work}
\subsection{Dexterous Manipulation}
Dexterous Manipulation is a long-standing research topic in robotics that aims to give robots the ability to perform delicate operations like humans \cite{chen2022towards, bai2014dexterous, qin2021dexmv, touch-dexterity}. 
Traditional methods often rely on trajectory optimization based on dynamic models to solve operational problems \cite{kumar2014real, mordatch2012contact, wang2022dexgraspnet}, but these methods have limitations in complex tasks because they simplify the contact dynamics and are difficult to deal with uncertainties in dynamic environments. 
In contrast, Reinforcement Learning (RL) does not rely on accurate physical models but learns operational policies through interaction with the environment, which is highly adaptable. 
RL has achieved remarkable results in many dexterous manipulation tasks, such as object reorientation \cite{chen2022system, qi2023hand, pitz2023dextrous, handa2023dextreme} and sequential manipulation \cite{chen2023sequential, gupta2021reset}.
% However, RL methods usually require extensive reward engineering and system design and suffer from inadequate generalization ability in some scenarios. 
However, RL methods often suffer from several challenges, such as the need for extensive reward engineering and system design, as well as limited generalization to unseen scenarios.
Additionally, while Sim-to-Real is a common technique employed in RL, the gap between simulations and the real world degrades the performance of the policies once the models are transferred into real robots \cite{zhao2020sim}.
Imitation learning (IL), as another effective learning method, can quickly learn effective control policies by imitating expert demonstrations \cite{10602544, wang2023mimicplay}. 
% Compared to RL, IL does not rely on complex reward designs, which simplify the learning process. But the major drawback of IL lies in the high cost of data collection, especially when teleoperation is required with real robotic systems.
In this work, we propose a correspondence-based visual imitation learning policy that utilizes spatial information between various components, enabling the acquisition of complex skills with a minimal number of expert demonstrations. 

\subsection{Imitation Learning}
Imitation learning (IL) allows a robot to directly learn from experts. Behavioral Cloning (BC) is one of the simplest imitation learning algorithms, which treats the problem of learning behavior as a supervised learning task \cite{pomerleau1988alvinn}.
The modeling methods commonly used in traditional BC, such as MSE, discretization \cite{lin2020limitations}, and K-Means \cite{guss2021towards}, have limitations when modeling complex action distributions. They fail to effectively capture the diversity and nuances of human behavior \cite{pearce2023imitating}. 
Over the past few years, diffusion models have emerged as a new modeling approach in BC, becoming powerful tools that enable robots to learn from demonstrations, handle uncertainty, and perform complex multi-step tasks with precision.
From the early applications of DDPMs to the recent innovations in BESO \cite{reuss2023goal}, OCTO \cite{team2024octo}, and CrossFormers \cite{doshi2024scaling}, these models have continually pushed the boundaries of what's possible in robotic behavior generation.
While traditional BC policies typically rely on 2D image-based representations \cite{chi2023diffusion, zhao2023learning, wang2023mimicplay, xia2024cage, liang2024dexhanddiffinteractionawarediffusionplanning}, recent advancements have extended imitation learning to 3D visual representations \cite{ze20243d, chen2024g3flow, gervet2023act3d, wang2024dexcap, wang2024rise, lu2024manicm}. 
These 3D approaches provide a more comprehensive understanding of spatial relationships and 3D structures, further enhancing robotic behavior learning.
% Despite these advancements, there remains room for improvement in areas like generalization, and data quality dependency.


\subsection{Correspondence Learning}
Correspondence refers to the relationship or alignment between different entities or components, with the aim of establishing meaningful connections. Correspondence learning has been shown to improve performance in various robotic tasks, including grasping \cite{patten2020dgcm, ding2024preafford}, perception \cite{lai2021functional, chen2024g3flow}, pose estimation \cite{haugaard2022surfemb} and garment manipulation \cite{Wu_2024_CVPR}. In this paper, correspondence specifically refers to the alignment between hand-object spatial interaction and hand-arm temporal coordination. By incorporating correspondence, we enhance feature extraction capabilities, thereby enabling more accurate and coordinated movements in downstream tasks.

%