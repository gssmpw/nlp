\section{Related Work}
\noindent\textbf{Hypergraph Expansions.} Unlike graphs where the edge connects two nodes, hypergraphs allow hyperedge to connect an arbitrary number of nodes which can depict high-order relationships. 
Most hypergraph representation learning methods first convert hypergraphs to graphs via classic expansion methods, i.e., clique expansion (CE)~\cite{CE}, star expansion (SE)~\cite{CESE}, 
line expansion (LE)~\cite{LEGNN} 
or its variants~\cite{HyperGCN,HGNN,zhou2006learning} and further feed the converted graph to neural networks for representation learning.
Clique expansion, one of the classic expansions, replaces hyperedges with cliques in which every node pair within the corresponding hyperedge is connected.
For instance, recent studies, i.e., HyperGCN~\cite{HyperGCN} and HGNN~\cite{HGNN} propose CE-based methods to convert hypergraphs into weighted graphs and learn the hypergraph representations for node classification tasks through graph neural networks (GNNs). 
Star expansion, another classic expansion, creates a set of nodes that represent hyperedges and further connects each new node with nodes that are in the corresponding hyperedge. 
Line expansion~\cite{LEGNN}, also called cross expansion~\cite{yan2024hypergraph}, constructs a graph with a new set of nodes where each node represents a node-hyperedge pair in the original hypergraph, and any two new nodes are connected if they share the same node or hyperedge on the hypergraph.
However, these methods fail to preserve high-order information within hypergraphs concisely and may lead to undesired losses in hypergraph conversion.  
Inspired by existing hypergraph expansions, this work proposes an adaptive hypergraph expansion to expand hypergraphs into weighted graphs to depict the higher-order relationships among nodes better. 

\noindent\textbf{Graph Neural Networks.}
 Graph neural networks (GNNs), considering both the node features and the graph structure, have become the state-of-the-art approach to learning graph representations~\cite{wang2025neuralgraphpatternmachine,wang2024learning,wang2024can}. 
 We would like to introduce several popular GNNs: GCN~\cite{GCN} implements a layer-wise propagation rule to learn the node embeddings from neighbors;
GAT~\cite{GAT} employs attention mechanisms to measure the importance of neighboring nodes when aggregating features;
GIN~\cite{GIN} is designed to utilize the Weisfeiler-Lehman Isomorphism Test~\cite{WLTest} for neighbor aggregation to enhance the capacity of GNNs in distinguishing different graph structures. 
Recently, more advanced GNNs~\cite{zhang2019heterogeneous,zhang2024ngqa,zhang2024mopi,GraphSage,wang2024gft,wang2024training} have been proposed, further demonstrating the effectiveness of GNNs.
 To leverage the powerful GNNs to learn the complex interaction in hypergraphs, inspired by existing works (e.g., HyperGCN and HGNN), we feed the weighted graph via our designed adaptive hypergraph expansion method to GNNs for downstream tasks. 



\begin{figure}
\vspace{-1mm}
\begin{center}
    \includegraphics[scale=0.45]{intro.png}
\end{center}
\vspace{-3mm}
\caption{Illustration of classic hypergraph expansion methods.}
\label{fig: intro}
\vspace{-5mm}
\end{figure}