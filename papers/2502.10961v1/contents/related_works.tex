\section{Related Works}\label{sec:related_works}

\textbf{Evaluation metrics on open-ended outputs from language models.}
Significant effort has been dedicated to creating effective evaluation metrics to measure the quality of open-ended outputs from language models. Early methods like BLEU \citep{Papineni2001-xx} and ROUGE \citep{lin2004rouge} rely on rule-based approaches that focus on lexical overlap to gauge similarity between generated responses and references. However, these methods may fail to capture the deeper semantic meaning of the text. This limitation led to research exploring the use of language model embeddings \citep{Zhang2019-qr,sellam2020bleurt,yuan2021bartscore} for evaluating generations. More recently, language models (LMs) have also been leveraged to score text. Broadly speaking, there are two types of approaches: training and training free. Specifically, training based approaches trains or finetunes LMs directly on ground truth scores \citep{Juraska2023-tk,wang2024interpretable,Kim2024-tn,Vu2024-nh} or performs RLHF to align with human preferences \citep{Ouyang2022-ea,Sun2023-qz,Li2023-ka,Yuan2024-jj,Zhang2024-bn,Shankar2024-al}. Training-free approach, however, directly leverages the instruction following capability of LMs and prompts the model to evaluate outputs via chain of thought \citep{Wei2022-kp}. Besides vanilla prompting LMs on text and other modalities \citep{Zheng2023-mh,Yu2023-tz}, aggregating ratings from a variety of LMs \citep{Verga2024-ct,Ning2024-ca}, generating reference answers \citep{Zeng2023-zt}, grounding quantitative reasoning \citep{Zhou2024-ql} and simulating debates among LMs \citep{Khan2024-jf} have been shown to further improve evaluation effectiveness. In this work, we do not train or finetune any models; instead, we show that privileged information improves automatic evaluations such that they outperform the best finetuned LMs and match expert human graders.

\textbf{Providing LM graders with additional information.} When asking LM-based grader to rate text, additional context can be provided to align with human. One prominent example is Constitutional AI \citep{Bai2022-uh} where human oversight are written in the form of rules or principles. The principles provided is a general set of principles without any variation for different queries. Others \citep{vu2023freshllms,Zeng2023-zt,Yu2023-tz,Bai2023-tp,Padlewski2024-ag} have explored generating or using reference answers to automatic graders for better decision making. \citet{finkelstein2024jack} constructs few-shot prompting examples from prior ratings while \citet{cook2024ticking,zhang2024reviseval} use grading checklist or criteria as additional information.
In this paper, we extend \PI beyond ground-truth references to more diverse and prompt-specific types of information, and establish its effectiveness to evaluate LMs on frontier problems.
