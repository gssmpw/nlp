
% \input{figures/odyssey_autoraters_eval}
\input{figures/reka_vibe_eval}

\section{Introduction}\label{sec:introduction}

% automatic graders fail on hard tasks
Automated evaluation metrics~\citep{Papineni2001-xx,Zheng2023-mh,Vu2024-nh} have become a cornerstone of natural language processing, serving as a cost-effective substitute for human evaluations.
The underlying idea is simple: replace the human grader with a language model (LM) and ask it to score the predictions of the candidate LMs.
While these metrics are crucial for tasks where human judgment is unavailable or impractical, they often fall short of matching the nuanced assessments of human experts, particularly on tasks that fall beyond the frontier of today's LM ability.
This discrepancy stems from a chicken-and-egg issue:
%
\begin{center}
    \emph{How can we trust LMs to grade themselves on tasks they don't master yet?}
\end{center}
%
% north star problems matter
These frontier tasks, like Olympiad-level or graduate-level STEM benchmarks, are not only inspiring but also serve as \NS for the development of LMs. \citep{Rein2023-ga, Fang2024-go, Trinh2024-ky, OpenAI-O1}
Therefore resolving this issue is paramount, as inaccurate evaluations hinder our ability to precisely gauge progress, particularly when the models are iteratively improved.

% introduce PI
We propose a novel approach to address these challenges: equip automatic graders with \emph{\PI} (PI) --- information only available to the grader and designed to ease the evaluation task.
Some examples of \PI include worked-out ground-truth solutions (\eg, for math prompts), prompt-specific rating guidelines (\eg, for cooking prompts), and detailed image description (\eg, for visual commonsense reasoning prompts).
We borrow the concept of \PI from Vapnik's work, where it refers to additional information for the learner to learn well, for example, rationales to solutions offered by a student to help students to learn better~\citep[Postscript]{Vapnik1982-lw}. 

% types of failures for graders
While \PI can be used on any evaluation task, it is particularly impactful for \NS problems which suffer from two main impediments.
First, as noted, LMs are by definition incapable of grading \NS problems by themselves.
Providing them with \PI enables the grader to specialize to the task at hand; thanks to \PI, the grader has become an expert on the given prompt and is now capable of judging candidate responses.

A second challenge arises for the most difficult \NS benchmarks where a majority of prompts are too difficult for today's LMs, resulting in evaluations dominated by noise.
In those cases, we automatically devise simpler variations of the same problems by providing the candidate LMs with hints synthesized from \PI.
These hints enable finer grained analyses and also come at no additional human labour cost once the \PI is collected.
More importantly, they allow directly hill-climbing on the \NS benchmark of interest, instead of relying on simplified proxy problems.
% (More in \cref{sec:method-hints}.)

% Our contributions
Concretely, our work highlights the value of \PI in automated evaluations.
We detail several kinds of \PI in \cref{sec:method}, as well as their use cases for both graders and candidate LMs.
\cref{sec:experiments} builds towards \cref{tab:math_odyssey} where we leverage \PI and show how to fully automate evaluations on \MathOdyssey~\citep{Fang2024-go}, a \NS benchmark of Olympiad-level math problems.
We demonstrate the effectiveness of the \PI in \cref{sec:experiments-pi}:
on \RewardBench~\citep{lambert2024rewardbench}, the graders closely match state-of-the-art and improve upon graders without \PI by more than $6\%$ accuracy points (\cref{tab:reward_bench});
on \VibeEval~\citep{Padlewski2024-ag}, they surpass individual human graders and improve correlation with the average human rating by more than $0.35$ points (\cref{fig:reka_vibe_eval});
and on \MathOdyssey, they exceed $0.7$ correlation thus approaching expert human graders (\cref{fig:odyssey_autoraters_eval}).
Finally, our analysis in \cref{sec:experiments-hints} shows that hints derived from \PI help separate models (\cref{fig:better_separation}) and uncover unknown trends w.r.t. problem difficulty (\cref{fig:difficulty_analysis}).
%Altogether, we hope this methodology can help build trustworthy automated evaluations that challenge our best models.
