\section{Experiments}\label{sec:experiments}

Our experiments show how privileged information can help improve automated evaluations. We ask the following questions:
\begin{itemize}
    \item How much do graders improve when given access to privileged information?
    \item Can privileged information help ease problem difficulty and thus improve separability?
    \item How to build expert-level evaluations with privileged information?
\end{itemize}
Unless otherwise specified, we refer to Gemini 1.5 Flash, Gemini 1.5 Pro, and Claude 3.5 Sonnet as Gemini Flash (\texttt{gemini-1.5-flash-001}), Gemini Pro (\texttt{gemini-1.5-pro-001}), and Sonnet (\texttt{claude-3-5-sonnet-20240620}), respectively.

\subsection{Better automatic graders with privileged information}\label{sec:experiments-pi}

We first study how the grading performance of various automatic graders change when given access to \PI.

\textbf{Datasets.} We study the performance of graders with \PI on two well-established benchmarks: \RewardBench \citep{lambert2024rewardbench} and \VibeEval \citep{Padlewski2024-ag}. \RewardBench has four categories: Chat, Chat Hard, Safety and Reasoning in total 2985 prompts. For each prompt, the grader is given two responses and asked to decide which one is more preferred by human. If the grader chooses the same response as the human label, it is considered correct. The benchmark also actively maintains a leaderboard for the average grading accuracy for the best graders\footnote{https://huggingface.co/spaces/allenai/reward-bench}. \VibeEval is a challenging visual question answering dataset consisting of 269 prompts, with 169 categorized as normal difficulty and the remaining as hard. Each prompt also comes with a golden reference answer written by human experts. In order to evaluate automatic graders, we first let Gemini Pro and GPT-4 Turbo to generate one response per prompt. Then we collect human ratings for each pairwise comparison between the model responses. More details can be found in Appendix \ref{app:vibe_eval_details}.

\input{tables/results_reward_bench}

\textbf{PI generation.} On \RewardBench, we obtain rating guidelines by distilling it from rated responses. Specifically, for each subset in Chat and Safety, we use 20 rated responses and ask Gemini Pro to synthesize generally applicable rating guidelines from them. The guidelines are then used to rate all prompts in the subset. Manually inspection show that these guidelines are generic enough. We obtain 10 subset-specific prompts (5 Chat and 5 Safety). For chat hard and reasoning subset, we manually craft one rating guideline and use it to grade all prompts in the two subsets. On \VibeEval, we leverage three types of \PI: reference answer, rating guideline and image caption. The reference answers are directly taken from the dataset and rating guidelines are explicitly written to focus on the correctness of the response rather than the verbosity. Finally, image captions are synthesized from Gemini Pro by asking the model to provide a description for the image. Examples of rating templates with \PI can be found in Appendix \ref{app:reward_vibe_templates}.

\textbf{Metrics.} On \RewardBench, we use the standard rating accuracy to evaluate the graders. On \VibeEval, since we not only know which response is preferred by human but also the extent of the preference, we use Spearman correlation between automatic graders and human graders as our evaluation metric. To reduce rating variance and position bias, each response pair is graded eight times, alternating the order in which the two responses are presented.

\textbf{Results.} In Table \ref{tab:reward_bench}, we compare the rating accuracy of Gemini Flash and Pro as graders, with and without PI. The top 5 models on the leaderboard as of February 13th, 2025 is also shown for reference. In Figure \ref{fig:reka_vibe_eval}, we show the performance of graders as well as human performance. We further analyze the effect of \PI on both human and LM graders along with different subsets.

\textbf{Graders with \PI outperform almost all specialized models and human graders.} As seen in both Table \ref{tab:reward_bench} and Figure \ref{fig:reka_vibe_eval} (left), providing \PI significantly improve the rating accuracy by more than 6\% on \RewardBench and more than double the Spearman correlation on \VibeEval. Moreover, the improvement is large enough on \RewardBench to closely match the SOTA result on the leaderboard for Gemini Pro. On \VibeEval, \PI enables both Gemini Flash and Pro to outperform human graders. This is particularly encouraging from a cost perspective. LM graders not only have the potential to support and partly substitute human graders in rating tasks, but also weaker and cheaper graders like Gemini Flash can be employed instead of the more expensive models when \PI is given.

% PI can be combined; PI helps human too
\textbf{Different sources of \PI can be used and combined.} Since we use three types of \PI for \VibeEval namely image caption, rating guideline and reference answer, we ablate the performance of automatic graders when given only a subset of \PI. As seen in Table \ref{tab:vibes_full_combinations}, the performance of both Gemini Flash and Gemini Pro benefit from more \PI. Moreover, it is clear that reference answer improves the rating effectiveness the most, with more than 0.20 point correlation improvement. When both reference answer and rating guideline are used, image caption adds marginal improvement. This can be explained since reference answer directly provides the correct answers to the question whereas image caption still may not be specific to the question. Additionally, when reference answer is not available, image caption improves as much as 0.07 point, suggesting that captions are still useful for visual question answering especially human written answers are not available. Interestingly, as Figure \ref{fig:reka_vibe_eval} (middle) shows, human graders also benefit from \PI just like their LM counterpart, further indicating the effectiveness of \PI.

% PI help the most on hard problems
\textbf{Privileged information helps especially on hard prompts.} In Table \ref{tab:reward_bench} and Figure \ref{fig:reka_vibe_eval} (right) automatic grading performance improves most significantly on challenging prompts when provided with \PI. For instance, Gemini Pro achieves over a 9\% increase in rating accuracy on the "Chat Hard" subset of \RewardBench and triples the Spearman correlation on the hard subset of \VibeEval. We hypothesize that this is because rating difficult prompts requires more complex reasoning, and \PI helps alleviate this cognitive load for automatic raters.

\input{tables/results_vibe_eval_debias}

\textbf{Privileged information ameliorates rating bias.} Lastly, we explore whether \PI can help mitigate several grading biases identified in previous work~\citep{Zheng2023-mh}. Specifically, we examine three types of biases: verbosity bias, where the LM grader favors longer responses; self-enhancement bias, where the LM grader prefers its own responses; and formatting bias, where the grader favors markdown formatting. In Table \ref{tab:debias}, we compare these biases with and without \PI on \VibeEval. To compute each entry, we first assess the total number of rating errors made by the grader, then determine how many of these errors are attributable to the bias \ie the number of mistakes the grader would make if it relied solely on the bias. The bias error rate is the ratio of these two values. Our results show that \PI significantly reduces verbosity and formatting biases, though it has no impact on self-enhancement bias. This suggests that self-enhancement bias may be inherently more challenging to address, and different models may be required for effective grading.

\subsection{Simplifying \NS problems with \PI}
\label{sec:experiments-hints}

\eat{
Summary:
\begin{itemize}
    \item MATH: on adversarially-chosen prompts, hints help separate Flash and Pro. Also, true regardless of hint-generating model.
    \item GPQA: separate Gemma / Flash; GPT-4o doesn't work as well with hints (confirmed on MATH too, in appendix);
    \item gotcha steps
\end{itemize}

which models are used to generate hints; number of hints to generate (in appendix?).

Hints also provide a sanity check: the more information you give, the wiser the model should become.
Get as quickly as possible to the plateau (sigmoid). 
}

We now show how to address the second challenge that \NS benchmarks pose: they are hard enough that we don't get meaningful signal to evaluate our models.

\textbf{Datasets and metrics.}
We use two widely-recognized reasoning datasets, \MATH~\citep{Hendrycks2021-rp} and GPQA~\citep{Rein2023-ga}, to evaluate model performance.
The \MATH dataset contains 5,000 open-ended problems from high school curricula and competitions spanning seven mathematical topics.
Since most \MATH problems are easily solved, we adversarially select problems that both Gemini 1.5 Flash and Pro solve less than $10\%$ of the time and call this subset \MATHAdv.
For \GPQA, we use all 448 questions across biology, chemistry, and physics.
These graduate-level problems are challenging: even human experts solve only $65\%$ of the time.
Both datasets provide step-by-step ground truth solutions created by human experts, which we use as privileged information.
For these studies, we measure accuracy against a known final answer to reduce variability and control for confounders due to an automatic grader.
We also sample eight model responses per problem and bootstrapping to compute $95\%$ confidence intervals.

\textbf{From \PI to hints.}
We use the \PI, here step-by-step solutions, to generate hints with a goal to simplify \NS problems.
Given the ground truth solution of a target problem, we ask Claude 3.5 Sonnet to breakdown the solution into three standalone hints.
We explicitly instruct not to reveal the final answer so that providing hints one-by-one incrementally eases the target problem. Example hint generation template can be found in Figure \ref{fig:example_hint_generation}.

\textbf{Results summary.}
\cref{fig:better_separation} shows that using our PI-generated hints is crucial to robustly separate models which would otherwise be indistinguishable on \MATHAdv and \GPQA.
Moreover these hints also uncover new insights in \cref{fig:difficulty_analysis}, namely that GPT-4o shines on harder problems while Gemini models can better take advantage of problem hints.

\input{figures/improve_separation}

\textbf{Hints from \PI ease problems and improve model separability.}
First, we see that performance monotonically increases when we provide more hints for both \cref{fig:better_separation,fig:difficulty_analysis}.
These hints significantly improve performance, \eg boosting from almost 0\% to 80\% on \MATHAdv.
Second, in \cref{fig:better_separation}, we observe that both candidate models yield very similar performance on the original problems (\ie, hints = $0$), with confidence intervals largely overlapping.
This is particularly true for \MATHAdv since the problems are deliberately selected to be difficult to both candidates.
Third, the gap between the two candidates first increases and then decreases, almost overlapping again when all the hints are provided to the models.
This illustrates the existing of an evaluation sweet spot and echoes the ``Goldilock zone'' message from \cite{Padlewski2024-ag}.
We observe similar trends regardless of the model used to generate hints and regardless of the number of hints generated (see \cref{fig:math_different_number_hints,fig:math_different_hint_models} in the Appendix).

\input{figures/difficulty_analysis}

\textbf{Hints reveal novel insights into LM capabilities.}
We now show how hints synthesized from \PI also enable us to gain new insight about the candidates' capabilities.
For example, in \cref{fig:difficulty_analysis}, we evaluate Gemini Flash, Gemini Pro, GPT-4o, and GPT-4 Turbo on \MATHAdv and \GPQA.
We see that GPT-4o performs at least as good as Gemini Pro or better on problems without hints; however, GPT-4o performance increases more slowly than the Gemini models --- so much so that Flash ultimately catches up, outperforming GPT-4o by more than $30\%$ accuracy points.
As a sanity check we also include GPT-4 Turbo, which scales as well as Gemini models.

\subsection{Expert-level evaluations with privileged information}

In this section, we propose to use \PI for both improving the reliability of automatic grader and easing problem difficulties for better performance separability on \NS problems. 

\textbf{Dataset.} We identify a recent and particularly challenging math reasoning dataset \MathOdyssey\emph{-Olympiad}, a subset of \MathOdyssey~\citep{Fang2024-go}. This subset has 148 very challenging high school competition level problems featuring both open-ended and multiple-choice types. \citet{Fang2024-go} evaluate GPT-4 Turbo on this subset and the accuracy based on final answer is only 10.14\%. The dataset also has huamn written reference solutions and answers.

Because close to 90\% of the problems cannot be solved by frontier models, accuracy based evaluation provides very limited information for assessing different frontier model performance. To this end, we propose to first leverage pairwise comparison between model solutions using automatic graders. Pairwise comparison can provide performance signals on problems where both models solve incorrectly. Then, since the original problems are still very hard for \NS LMs, we leverage the insights from Section \ref{sec:experiments-hints} to ease the problem difficulty levels for a more fine-grained evaluation.

\textbf{PI generation.} The \PI we leverage is the ground-truth solution. For grading model responses, the entire ground-truth solution is provided. For easing problem difficulties, we use the same technique from \ref{sec:experiments-hints} where we convert each ground-truth solution to three hints.

\input{figures/odyssey_autoraters_eval}

\textbf{LM grader correlation with human ratings.} Before applying LM graders to these challenging math problems, we first examine their correlation with human graders. Solutions were generated for the problems by Gemini 1.5 Pro, GPT-4o, and Claude 3.5 Sonnet at varying hint levels. We then had human experts evaluate 136 pairwise comparisons: Gemini 1.5 Pro vs. Claude 3.5 Sonnet, and GPT-4o vs. Claude 3.5 Sonnet, sampled equally across the hint levels. These comparisons were also graded by different LM graders, both with and without \PI, and their Spearman correlation with human ratings were computed. The results, shown in Figure \ref{fig:odyssey_autoraters_eval}, reveal that Claude 3.5 Sonnet performs best with \PI, achieving a Spearman correlation of 0.71 with human evaluations. Notably, Claude 3.5 Sonnet also shows the largest performance boost with \PI, improving by 0.37 points. All automatic graders benefit from \PI, significantly outperforming those that do not use it. Additionally, we implemented a symbolic grader that only assesses the final answer. This grader checks whether the solution is correct and assigns a tie if both responses are either correct or incorrect. If only one response is correct, it prefers that response. The Spearman correlation for this rule-based grader is 0.60, which lags behind the top three automatic graders with \PI. While the rule-based grader is a strong baseline due to its use of a Sympy symbolic equivalence checker, its rule-based nature limits future improvements, unlike automatic graders, which can continue to improve alongside advancements in LLMs and \PI.

\textbf{Model evaluation results.} In Table \ref{tab:math_odyssey}, we evaluate 8 models against Claude 3.5 Sonnet on pairwise win rate with various number of hints. The automatic grader is also Claude 3.5 Sonnet, the best grader from the above paragraph. The overall win rate shows that both Claude 3.5 Sonnet and Gemini 1.5 Pro are both strong and are preferred much more than other models. Looking at no hint column, which represents reasoning with only the original problem, all three GPT models also achieve strong performance against Claude 3.5 Sonnet. Their win rate, however, decrease with more hints. This is particularly true for GPT-4o who only has 26.6\% win rate with 3 hints (but more than 40\% win rate with no hint). The observation matches with what we have observed previously in Figure \ref{fig:difficulty_analysis} where GPT-4o excels at solving hard problems while Gemini 1.5 Pro leverage hints well, which solves easier versions of those hard problems more effectively.

\input{tables/results_odyssey_candidates}
