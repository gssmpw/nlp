% \begin{table*}[h]
% \vspace{0em}
% \begin{center}
% \setlength{\tabcolsep}{5pt}
% {\small
% \begin{tabular}{l  c  c | cccc}
% \addlinespace
% \toprule

% \textbf{Model}                              & \textbf{RM-tuned}     & \textbf{Overall}  & {Chat}     & {Chat Hard}    & {Safety}   & {Reasoning} \\
% \midrule 


% 1. Skywork-Reward-Gemma-2-27B               & \ding{51}             & 93.8\%            & 95.8\%            & \textbf{91.4\%}       & 91.9\%            & 96.1\%  \\
% 2. SFR-LLaMa-3.1-70B-Judge-r                & \ding{51}             & 92.7\%            & \textbf{96.9\%}   & 84.8\%                & 91.6\%            & \textbf{97.6\%}  \\
% 3. Skywork-Reward-Llama-3.1-8B              & \ding{51}             & 92.5\%            & 95.8\%            & 87.3\%                & 90.8\%            & 96.2\%  \\
% 4. Nemotron-4-340B-Reward                   & \ding{51}             & 92.0\%            & 95.8\%            & 87.1\%                & 91.5\%            & 93.6\%  \\
% 5. ArmoRM-Llama3-8B-v0.1                    & \ding{51}             & 90.4\%            & \textbf{96.9\%}   & 76.8\%                & 90.5\%            & 97.3\%  \\
% \midrule
% 13. Gemini Pro                              & \xmark                & 88.1\%            & 92.3\%            & 80.6\%                & 87.5\%            & 92.0\%  \\
% \,\,\,\,\,+ Privileged Info. (\textbf{$\rightarrow$ \#1})  
%                                             & \xmark                & \textbf{94.4\%}   & 96.6\%            & 89.7\%                & \textbf{94.7\%}   & 96.8\%  \\
% \midrule
% 34. Gemini Flash                            & \xmark                & 82.1\%            & 92.2\%            & 63.5\%                & 87.7\%            & 85.1\%  \\
% \,\,\,\,\,+ Privileged Info. (\textbf{$\rightarrow$ \#14})
%                                             & \xmark                & 88.0\%            & 95.0\%            & 77.2\%                & 90.2\%            & 89.6\%  \\

% \bottomrule
% \end{tabular}
% }
% \begin{minipage}[t]{0.95\textwidth}
%     \vspace{0.95em}
%     \caption{
%         \small
%         \textbf{RewardBench leaderboard.}
%         This table shows that generative LLMs excel at modelling human preferences when given privileged information.
%         In particular, they outperform SOTA reward models fine-tuned for RewardBench.
%     }
%     \label{tab:reward_bench}
% \end{minipage}
% \end{center}
% \vspace{0em}
% \end{table*}


\begin{table*}[h]
\vspace{0em}
\begin{center}
\setlength{\tabcolsep}{5pt}
{\small
\begin{tabular}{l  c  c | cccc}
\addlinespace
\toprule

\textbf{Model}                              & \textbf{RM-tuned}     & \textbf{Overall}  & {Chat}     & {Chat Hard}    & {Safety}   & {Reasoning} \\
\midrule 


1. INF-ORM-Llama3.1-70B               & \ding{51}             & \textbf{95.1\%}            & 96.6\%            & \textbf{91.0\%}       & 93.6\%            & \textbf{99.1\%}  \\
2. LDL-Reward-Gemma-2-27B-v0.1                & \ding{51}             & 95.0\%            & 96.4\%   & 90.8\%                & 93.8\%            & 99.0\%  \\
3. QRM-Gemma-2-27B              & \ding{51}             & 94.4\%            & 96.6\%            & 90.1\%                & 92.7\%            & 98.3\%  \\
4. Skywork-Reward-Gemma-2-27B-v0.2                   & \ding{51}             & 94.3\%            & 96.1\%            & 89.9\%                & 93.0\%            & 98.1\%  \\
5. Llama-3.1-Nemotron-70B-Reward                   & \ding{51}             & 94.1\%            & \textbf{97.5\%}   & 85.7\%                & \textbf{95.1\%}            & 98.1\%  \\
\midrule
35. Gemini Pro                              & \xmark                & 88.1\%            & 92.3\%            & 80.6\%                & 87.5\%            & 92.0\%  \\
\,\,\,\,\,+ Privileged Info. (\textbf{$\rightarrow$ \#3})  
                                            & \xmark                & 94.4\%   & 96.6\%            & 89.7\%                & 94.7\%   & 96.8\%  \\
\midrule
62. Gemini Flash                            & \xmark                & 82.1\%            & 92.2\%            & 63.5\%                & 87.7\%            & 85.1\%  \\
\,\,\,\,\,+ Privileged Info. (\textbf{$\rightarrow$ \#36})
                                            & \xmark                & 88.0\%            & 95.0\%            & 77.2\%                & 90.2\%            & 89.6\%  \\

\bottomrule
\end{tabular}
}
\begin{minipage}[t]{0.95\textwidth}
    \vspace{0.95em}
    \caption{
        \small
        \textbf{RewardBench leaderboard.}
        This table shows that generative LLMs excel at modelling human preferences when given privileged information.
        In particular, they are competitive against SOTA reward models fine-tuned for RewardBench.
    }
    \label{tab:reward_bench}
\end{minipage}
\end{center}
\vspace{0em}
\end{table*}


