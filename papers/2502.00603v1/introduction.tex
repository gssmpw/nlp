\section{Introduction}

\begin{figure}
  \centering
        \includegraphics[width=0.9\linewidth]{figs/cellEdge.png}
        \caption{Cell tower, Edge, Remote topology}
        \label{fig:disttopo}
\end{figure}

Traditional radio access network (RAN) realizations relied on proprietary hardware and software (i.e., baseband processing units (BBUs)) for performance and reliability, deployed near base stations (or cell towers) according to the required cell capacity. Relentless technology ``softwarization'' and virtualization trends have resulted in virtualized RAN (vRAN) architectures, enabling ``split'' BBU deployments, with some functionality executing on common-off-the-shelf (COTS) compute hardware using distributed topologies, as shown in Figure~\ref{fig:disttopo}.
vRAN deployment splits the RAN functionality into a radio unit (RU), a distributed unit (DU) and a centralized unit (CU)~\cite{ORAN} .\footnote{Strictly speaking the O-RAN architecture refers to these as O-RU, O-DU and O-CU respectively.}
%According to ORAN~\cite{ORAN}, a viable vRAN deployment for Enhanced Mobile Broadband (eMBB\footnote{The initial category of 5G, provides faster internet access with higher data bandwidth, but is less critical in terms of latency, making it the first 5G category to offer the advantages of 5G to the general public.}) splits the RAN into a distributed structure that comprises of the radio unit (RU), the (virtual) distributed unit (vDU), and the (virtual) centralized unit (vCU) as shown in Figure~\ref{fig:topo}.\footnote{The more recent Open RAN (O-RAN) architecture follows a similar approach and indeed has much in common with earlier vRAN (and Cloud-RAN) approaches. A key difference is that Open RAN is focused on realizing the distributed architecture through open interfaces. For the purposes of our work, this distinction is not important and we will use the more generic \textit{vRAN} terminology to refer to the softwarized/virtualized RAN architecture.} 
As shown in Figure~\ref{fig:topo}, RUs are located at cell sites (distributed throughout the deployment area) and connected to DUs at edge compute locations via a fronthaul network. DUs, in turn, are connected to CUs located at remote/centralized compute facilities via a midhaul (MH) network. The F1 interface enables this disaggregation for both control (F1-C) and user plane (F1-U) communication. vRAN aims to realize RAN functionality (for the DU and CU) in software on general-purpose compute platforms (as opposed to proprietary hardware). As with other virtualization efforts, i.e., cloud computing and network function virtualization, the expectation is that vRAN technology will result in a reduction of the total cost of ownership with unified hardware and software infrastructure, simplified network operation and maintenance through unified management, and, fostering flexibility and innovation~\cite{IntelvRAN,RT-OPEX,CloudIQ}.\footnote{Indeed vRAN architectures are seeing real-world deployments from service providers such as Rakuten, Dish and Verizon.}

\begin{table*}
    \centering
    \caption{Example deployment and resources at different locations for distributed vRAN}
    \label{tab:edge_remote}
    \begin{tabular}{ccccc}
        \toprule
        \thead{Location \\(Device)} & 
        \thead{Function} & 
        \thead{Space Cost \\(Relative)} & 
        \thead{Distance/Latency} & 
        \thead{Bandwidth} \\
        \midrule
        Cell-Site (RU) & RF frontend & Very high & -- & -- \\
        Edge (DU) & PHY, MAC, RLC & High & $<10\,\text{km} / <500\,\mu\text{s}$ & Very high \\
        Remote (CU) & PDCP, RRC/SDAP & Low & $<40\,\text{km} / <10\,\text{ms}$ & User bandwidth \\
        \bottomrule
    \end{tabular}
\end{table*}




\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs/topo_stack.png}
  \caption{Current vRAN deployments: RU at cell sites, DU at edge computing facilities, and CU at remote/centralized computing facilities.
  }
  \label{fig:topo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs/HadesStack1.png}
   \caption{\Name{}'s split-DU (edge DU(eDU) and remote DU(rDU) architecture}
  \label{fig:HadesStack}
\end{figure}





\iffalse
\begin{table}
  \small
  \caption{Example deployment and resources at different locations for distributed vRAN \\
  }
  \label{tab:edge_remote}
  \scalebox{1}{
      \begin{tabular}{ccccl}
        \toprule
        \vtop{\hbox{\strut location}\hbox{\strut (device)}} 
        &\thead{function}
        &\vtop{\hbox{\strut space cost}\hbox{\strut (relative)}}
        &\vtop{\hbox{\strut distance}\hbox{\strut /latency}}
        &\vtop{\hbox{\strut band-}\hbox{\strut width}}\\
        \midrule
        Cell-Site (RU) &RF frontend &very high & &\\
        Edge (DU)     &PHY, MAC, RLC   &high&~<10km/<500$\mu s$ &Very high\\
        Remote (CU)   &PDCP, RRC/SDAP   &low&<40km/<10$ms$ &~User bandwidth\\
        \bottomrule
      \end{tabular}}
\end{table}
\fi

In practice, however, the potential benefits of a virtualized/cloud-native RAN are hindered by 
\textit{the high costs of over-provision of edge computing resources to support the DU}.
Table~\ref{tab:edge_remote} outlines the space, distance/latency, and bandwidth requirements of a vRAN deployment. The strict latency requirements place low-layer RAN functions (PHY, MAC, RLC) at the edge, leading to significant computational overhead, particularly for uplink decoding, which can account for over 60\% of the vRAN workload~\cite{RT-OPEX,CloudIQ,foukas2021concordia,Nuberu}.
Edge capacity is further strained by the bursty nature of RAN traffic, requiring substantial over-provisioning of CPU resources. Even with workload consolidation, many CPU cycles in vRAN environments remain unused due to micro-scale workload fluctuations~\cite{foukas2021concordia}. Limited edge capacity\footnote{Edge facilities have the smallest available footprint, and the number of servers the edge site can host can range from more than 100 in the largest footprint to as low as three servers in its minimal form~\cite{CiscoReimaging}.}, high failure rates of standard nodes\footnote{According to one study general purpose compute nodes have an annual failure rate of 11\% for four-year-old equipment~\cite{x86_fail}.}, and the need for backup servers increase deployment costs and risk service disruptions. For example, data from Rakuten in Japan show only 99.7\% RAN availability without additional edge compute capacity~\cite{Rakuten}. Insufficient edge capacity leads to ungraceful network degradation during overloads, either by not admitting users or degrading service.

\iffalse
Edge compute capacity is further strained by the bursty nature of RAN traffic, necessitating significant over-provisioning of CPU resources. Even with workload consolidation, a large fraction of CPU cycles in vRAN environments goes unused due to micro-scale workload fluctuations~\cite{foukas2021concordia}. The limited edge capacity, high failure rates of standard computing nodes~\cite{x86_fail}, and the need for additional backup servers increase deployment costs and the risk of service disruptions.
%add costing analysis
Given these dynamics,
ultimately, vRAN infrastructure deployment requires excessive edge compute provisioning, with corresponding cost implications. Insufficient edge compute capacity\footnote{Edge facilities have the smallest available footprint, and the number of servers the edge site can host can range from more than 100 in the largest footprint to as low as three servers in its minimal form~\cite{CiscoReimaging}.} impacts network robustness and availability, as there is no spare capacity during node failures, and general-purpose nodes have high failure rates\footnote{According to one study general purpose compute nodes have an annual failure rate of 11\% for four-year-old equipment~\cite{x86_fail}.}. For example, data from edge-based vRAN deployments by Rakuten in Japan suggest RAN availability of only 99.7\% without deploying additional edge compute capacity~\cite{Rakuten}. Additionally, lack of edge computing capacity results in ungraceful network degradation during overload conditions (e.g., because of a flash crowd, or more slowly because of increased demand, and for various reasons can not be addressed through the deployment of additional edge computing capacity), where the only options are not admitting users or causing service degradation for admitted users.
\fi

\iffalse
Given these dynamics,
vRAN infrastructure deployment requires excessive edge compute provisioning, bearing economic ramifications. As Table~\ref{tab:edge_remote} suggests, edge deployment costs, in relative terms, significantly surpass those of remote compute locations due to space and power constraints\footnote{Edge facilities have the smallest available footprint, and the number of servers the edge site can host can range from more than 100 in the largest footprint to as low as three servers in its minimal form~\cite{CiscoReimaging}.}.
More seriously, however, the potential of \textit{under-provisioned edge compute capacity} presents in two additional operational manifestations. 
First, insufficient edge compute capacity impacts \textit{network robustness and availability}. Simply, without sufficient edge computing capacity, there is no spare capacity when nodes fail, and general-purpose compute nodes have relatively high failure rates. (According to one study general purpose compute nodes have an annual failure rate of 11\% for four-year-old equipment~\cite{x86_fail}.) For example, data from edge-based vRAN deployments by Rakuten in Japan suggest RAN availability of only 99.7\% without deploying additional edge compute capacity~\cite{Rakuten}. 
Second, lack of edge computing capacity results in \textit{ungraceful network degradation} during overload conditions. Without sufficient edge compute capacity the only recourse during overload is not to admit users into the network, or for admitted users to experience service degradation. Overload conditions might occur rapidly, e.g., because of a flash crowd, or more slowly because of increased demand, and for various reasons can not be addressed through the deployment of additional edge computing capacity. 
\fi


Given this context, we pose a crucial question: \textit{Can we strategically offload a segment of the DU's demanding workload from the edge to remote sites, effectively alleviating the hefty deployment costs at the edge intrinsic to current vRAN implementations?} As we explore this question, we make a number of observations.

\noindent\textbf{First}, 
in 5G, the end-to-end latency requirement of traffic depends on the packet content, and most uplink traffic is \textit{not} latency-sensitive.
For instance, video streaming applications can tolerate end-to-end delay up to 150ms~\cite{zoom}, successfully operating on 4G networks with end-to-end latency often exceeding 100$ms$. Thus, it is reasonable to expect that many applications will continue to function well with increased latency, such as an additional 10s of milliseconds over the middle-haul network.
Notably, control traffic over the F1-c interface, such as layer 3 Radio Resource Control (RRC) messages, can tolerate latencies higher than several hundred milliseconds~\cite{3gpp}.


\noindent\textbf{Second}, 
physical layer signal processing tasks, especially uplink decoding, occupy the majority of the vRAN compute time. Offloading latency-tolerant decoding to remote sites can significantly reduce edge DU deployment costs. Early decodability prediction work shows that only a few decoding iterations are required for HARQ feedback~\cite{EHARQ,FastHarqURLLC}.



\noindent\textbf{Third}, 
the MH network cost is more affordable than edge resource deployment in terms of power and space.\footnote{For example, decoding one cell at the edge needs 24W (a 400W Nokia Airframe server supports 10~cells), while offloading only needs 3.5W of power consumed by an optical module.}
This makes offloading latency-tolerant DU tasks from the edge to remote locations an attractive option.


Based on the above observations, 
we propose investing in possible trade-offs between edge computing resources and networking resources. Specifically, as shown in Figure~\ref{fig:HadesStack}, we introduce \Name{}, a system that splits DU functionality into edge and remote parts, allowing dynamic DU workload distribution to enhance efficiency and elasticity over edge infrastructure resources. \Name{} aims to optimize resource use by dividing the decoding process into a \textit{latency-critical early decoding phase} and a \textit{latency-tolerant completion decoding phase}, adaptively distributing the completion decoding processing between edge and remote clouds. 
This approach enhances uplink capacity while utilizing fewer edge computing resources. However, realizing such a design presents three main challenges, which we address as follows:




\begin{description}[wide, labelindent=0pt]
    \item[Identifying Decoding content]

    In realistic cellular scenarios, traffic with different latency requirements is multiplexed in a single layer-1 transmission block. Control messages need consistently low latency, and low-latency applications (e.g., AR/VR) cannot tolerate additional decoding delays. 
    Unfortunately, for 5G uplink traffic, the content of a packet is unknown by the base station before decoding, as the scheduling grant only specifies the data size, with the data content within a transmission left to the user device. 
    This necessitates a method to \textit{identify content} to decide whether a coding block can be offloaded.
    
    \textit{\Name{} addresses this by employing a pre-parsing method for MAC PDU subheaders, checking "bits of interest"(subheaders bits) during early decoding to identify content without completing full decoding.}

\iffalse
    In realistic cellular scenarios, traffic with different latency requirements is multiplexed in a single layer-1 transmission block and needs to be decoding to be demultiplexed.
    For example, messages related to control loops of the air interface, such as buffer status reports and power headroom, are delay-sensitive due to dynamic control based on the radio environment.
    These control messages need consistently low latency to avoid severe performance degradation or user session disconnect. 
    Similarly, user traffic from low latency applications (e.g., augmented/virtual reality (AR/VR)) cannot tolerate additional decoding delay and requires decoding to be retained at the edge.
    Unfortunately, for 5G uplink traffic, the content of a packet is unknown by the base station before decoding, as the scheduling grant only specifies the data size, with the data content within a transmission left to the user device. 
    This necessitates a method to \textit{identify content} to decide whether a coding block can be offloaded. 

    \textit{\Name{} addresses this challenge by employing a pre-parsing method for MAC PDU subheaders. It requires only the correctness of the bits present in the subheaders. \Name{} checks the likelyhood correctness of "bits of interest" during the early decoding phase, allowing pre-parsing of appropriate content within a PDU without completing the decoding of all bits of the PDU (with CRC passing)}.
\fi


    \item[Spliting DU]
    Naively moving all decoding to the remote location would require all remotely decoded traffic to be forwarded back to the edge. This would not only add latency for upper layer processing, but also increase MH bandwidth requirements. To prevent these inefficiencies, \Name{} employs a split DU design whereby DU functionality is duplicated across the edge and remote locations. 
    %For optimized implementation and deployment of split decoding, a splitting DU design is imperative.  A more efficient approach is to duplicate or separate DU components accordingly, 
    This design, however, necessitates meticulous synchronization and consistency management across both locations. 

    \textit{As show in Figure~\ref{fig:HadesStack}, \Name{} features a clean design of the split DU with a separate control plane interface at the edge (F1-c) and new data plane interface at both edge and remote (F1-u).
%    separate aggregated locations for control and data plane F1 interface to CU. 
    This design simplifies DU state management at the edge and avoids additional latency from user data traffic moving back and forth between edge and remote locations}.


    \item[Adaptable utilization of edge resources] 
    The bursty nature of uplink traffic at very fine time  granularities (millisecond)~\cite{foukas2021concordia}, makes it non-obvious how to distribute resources between early decoding tasks and completion decoding tasks and when completion decoding should be processed at the edge or offloaded to the remote location. 
    First, an efficient scheduling policy is needed to utilize edge computing power effectively. Second, naively sending all excess completion decoding of a burst to the remote location can increase communication and computation costs while reducing spectral efficiency. This is especially true in cases where the edge location might have enough compute cycles to spare in the upcoming transmission periods (idle period after traffic burst), with a similar (or even smaller) processing latency penalty. 
    Additionally, the midhaul link used for offloading decoding tasks is shared among cells, so its available capacity might fluctuate, constraining the amount of traffic that can be offloaded. 
    
    \textit{\Name introduces a resource allocation framework that dynamically distributes edge compute capacity and offloads excessive completion decoding to the remote. It applies an Earliest Deadline First (EDF) policy at the edge for scheduling, achieving low latency and fair resource sharing. \Name decides which packets to offload based on the edge's spare capacity.}

\iffalse
    %\item[Efficient load adaptation to maintaining target BLER]
    \textit{\Name introduces a resource allocation framework that dynamically distributes edge compute capacity between early and completion decoding and also dynamically offloads excessive completion decoding to the remote.
    \Name{} applies an Earliest Deadline First (EDF) policy at the edge for decoding task scheduling to enable high CPU usage and task completion rate.
    More specifically, EDF achieves low latency when there is sufficient resource and fair sharing between early decoding and completion decoding.
    \Name decides which packet (completion decoding) should be offloaded and which should be performed locally based on the spare capacity of the edge by checking the head-of-line deadline in the completion decoding queues.}
\fi
\end{description}

%To address these challenges, \Name{} proposes and adopts a set of synergistic mechanisms operating across the edge/remote deployment. I.e., adapting the DU to be more distributed deployment friendly, with a focus on uplink FEC decoding, and picking the right systems techniques, adapting them to the characters of FEC decoding in vRAN. Specifically,:

We use the open-source srsRAN~\cite{srsRAN} as the code base for the implementation of \Name{}.
We evaluate \Name{} with both over-the-air single-cell setup and aggregated multiple-cell-emulated traffic setup. 
The results show that \Name{} can significantly improve the efficiency and elasticity of edge infrastructure resources.
\Name{} has almost no decoding capacity decrease with only half of the edge resource needed to meet the RAN latency for eMBB services. 
\Name{} also proves to be particularly useful/elastic under variable edge and/or network conditions where available edge capacity is scarce.
It maintains 78\% of the maximum decoding throughput even in cases of no available edge decoding resources. 



