\section{Background}

%vDU/RAN stacks overview
%1) controlling... scheudling, mac control, RRC configuration...
%2) TB processing. decoding, and lower phy

%\subsection{5G primes}
\label{sec_background_primes}

In 5G networks, radio activities are organized into defined Transmission Time Intervals (TTIs) or slots, such as $1ms$ for eMBB service. The orchestration of RAN resources involves a complex blend of allocation, transmission, feedback, and control.


\begin{description}[wide, labelindent=0pt]
\item[Uplink Scheduling Efficiency:] 
A critical aspect of RAN efficiency is the proficient scheduling of Physical Resource Blocks (PRBs) and Modulation and Coding Schemes (MCS). PRBs, composed of several subcarriers within a TTI, are the basic resource units assigned to users. MCS, which determines the data rate per PRB, is selected based on user feedback like Buffer Status Report (BSR), Power Headroom Report (PHR), Quality of Service (QoS) requirements, and Channel State Information (CSI) assessments. After PRB allocation and MCS determination, the base station provides a scheduling grant for uplink data transmission.
A key mechanism in this process is the Hybrid Automatic Repeat Request (HARQ) protocol. If decoding issues arise, indicated by a NACK, retransmission is prompted. Delays in HARQ feedback processing can lead to unnecessary retransmissions, wasting resources~\cite{Nuberu, srsRAN, OAI}. Insufficient compute resources, cause high Block Error Rates (BLER), which impact MCS selection. The 3GPP standard aims for a BLER below 5\%~\cite{3gpp}.


\item[Importance of MAC-CE Processing:]
MAC Control Elements (MAC-CEs), including BSR and PHR, are crucial for MAC scheduling. Increased latency in processing these elements can significantly degrade network performance. This impact was demonstrated in an srsRAN-based experiment, with results shown in Figure~\ref{fig:BSRPHR_latency}. By intentionally delaying MAC-CE processing, we observed its effect on RAN performance. Delays in BSR processing led to inefficient resource allocation and higher packet latency. Similarly, delayed PHR processing, particularly for mobile users, directly reduced cell throughput. This was evident as mobile phones at the edge of a base station's coverage experienced decreased throughput due to delayed PHR processing.
\item[MAC PDU Decoding:] 
Uplink transmission entails the mobile device assembling a MAC PDU packet based on the resource assignment from the base station. Logical Channel IDs (LCIDs) within sub-PDU headers identify the SDUs contained, guiding the base station in extracting relevant data for further action or processing. This packet is then decoded by the base station, extracting MAC SDUs for higher-layer processing and MAC-CEs for control functions. If the packet size exceeds 8448 bits, it's segmented into multiple code blocks for independent decoding~\cite{3gpp}, as shown in Figure~\ref{fig:MACCE}. Both 5G NR and LTE employ iterative decoding algorithms~\cite{turbo, LDPC}, traditionally completing all iterations before generating HARQ feedback. Due to the unpredictable and computationally intensive nature of this process, over-provisioning of computational resources is a common practice to ensure low latency and reliable decoding. Decoding error prediction, which involves generating HARQ feedback based on early prediction rather than after complete iterations, is used to enhance performance and manage computational resource constraints~\cite{Nuberu,FastHarqURLLC}. This approach, is especially crucial in using Log-Likelihood Ratio (LLR) based methods, optimizing by assessing the likelihood of each transmitted bit (being '0' or '1') based on extrinsic information during decoding~\cite{EHARQ}, thus predicting decoding results early in the process.


\iffalse
\item[Efficient Scheduling for Uplink:]
For cellular networks, efficient radio resource management hinges on the efficient scheduling of PRBs and the selection of the right Modulation and Coding Schemes (MCS). 
Physic Resource Blocks (PRBs) represent the smallest unit of resources to be allocated to users.
One PRB consists of a certain number of subcarriers across a TTI.
And MCS determines the number of bits that can be carried in each PRB. 
PRB is allocated and MCS is selected for a user based on various factors.
These include users' feedback (e.g. Buffer Status Report (BSR), Power Headroom Report (PHR)), up layer configuration (e.g. specific Quality of Service (QoS) of data bearer), and related estimations from the base station itself (e.g. Channel State Information (CSI)).  Upon finalizing the PRB allocation and MCS selection, the base station issues a scheduling grant, detailing the assigned PRBs and MCS, enabling the UE to transmit its buffered data.
A quick retransmission mechanize is enabled at the MAC layer, called Hybrid Automatic Repeat Request (HARQ).
Upon the base station finishes the uplink reception processing in PHY, a HARQ acknowledgment (ACK) or non-acknowledgment (NACK) is sent back to the UE. An ACK indicates successful decoding, while a NACK indicates that there are bit errors that can not be recovered through decoding processes, prompting the UE to retransmit the data.
Mobile devices expect a response within a deadline (e.g. 3ms~\cite{Nuberu, srsRAN, OAI}) from the base station after sending an uplink transmission. Any delay in proceeding related to HARQ feedback can result in unfinished packet decoding being treated as corrupt, triggering unnecessary retransmissions and wasting spectrum resources. 
A high Block Error rate caused by insufficient computer power can reversely affect the MCS selection with a BLER-based link adaption at the MAC scheduler.
More specifically, the selection of MCS is further adjusted with consideration of the observed block error rate (BLER).  
3GPP~\cite{3gpp} specifies a target block error rate (BLER) of 5\%, MAC scheduler reacts to higher BLER with more robust MCS, which is more tolerant to air-channel distortion and requires fewer iterations of decoding.
So the link adaptation of the MAC layer provides a native interface for \Name{} to adapt the decoding load to the computing resource availability. This requires \Name{} to adapt the BLER to interact with the MAC scheduler.

Worth to note, both BSR and PHR are wrapted as 
MAC Control Elements (MAC-CEs) within the MAC PDU, the latency of them are critical for MAC scheduling.
In practical scenarios, as we observed using a srsRAN-based setup with a 10 MHz bandwidth and USRP, delays in processing these reports can have significant implications as shown in Figure~\ref{fig:BSRPHR_latency}
For BSR, any latency in processing affects not only the efficiency of scheduling decisions but also the latency of packet transmissions, potentially leading to suboptimal resource utilization.
For PHR, delayed processing, especially when UEs are on the move, directly impacts cell throughput. Our tests involved two commercial off-the-shelf (COTS) mobile phones connected to a srsRAN base station â€“ one engaged in Zoom video calls and the other handling UDP background traffic. We noted that as the mobile phones moved to the edge of the base station's coverage area, the delayed processing of PHR noticeably diminished cell throughput.
This real-world experiment underscores the importance of timely MAC-CE processing in maintaining optimal RAN performance, particularly in dynamic user environments.


%performance as shown with measurement in measurement with srsRAN (Figure~\ref{fig:BSRPHR_latency}). We set up a srsRAN base station with 10Mhz bandwidth with USRP and have two COTS mobile phones connected to it over the air. One mobile phone is running Zoom video calls, and the other is running a UDP client as background traffic. Delay of BSR both affects the scheduling efficiency and packet latency. We moving both mobile phones at the edge of the coverage of the base station to measure of impact of the delay of PHR. The delay in moving phones directly reduces the cell throughput significantly.
%A stale view of a mobile device's uplink buffer status can result in unacceptable RAN latency(e.g. the scheduling for an uplink grant can take more than 20ms with a MH latency of 10ms). Differently, delays in processing PHR can reduce the efficiency of estimating the power required for sending data to a mobile device, especially throughput decreasing for moving devices.

\item[Decoding MAC PDU:]
For UL transmission, the mobile device assembles the MAC PDU packet based on the grant (resource assignment) from the base station. 
While the base station does not regulate the contents (different user QoS traffic stream, MAC-CEs, RRC messages), it is up to the mobile device to fill the MAC PDU.
Each MAC PDU consists of a header and one or more MAC Service Data Units (SDUs) or MAC Control Elements (CEs).
The base station decodes the MAC PDU to extract the MAC SDUs (which will then be passed to the higher layers for further processing) and MAC-CEs (used for various MAC layer control purposes).
When encoding, the packet is broken into multiple code blocks if its size exceeds the maximum allowed packet size (8448 bits for 5g NR~\cite{3gpp}) as depicted in Figure~\ref{fig:MACCE}. Code blocks are decoded independently at the base station after receiving. 

Both NR and LTE employ iterative algorithms for decoding~\cite{turbo, LDPC}. ``Run-to-completion'' approach is used in traditional vRAN decoding implementation, which does not stop decoding iterations until the output bits of a PDU pass CRC checking or the maximum number of iterations is reached~\cite{LDPC2}.
Traditionally, HARQ feedback is generated after the completion of decoding. However, the decoding process loads are unpredictable and computationally intensive, as the number of iterations required depends on the Modulation and Coding Scheme (MCS) used in the transmission and the level of interference present during transmission. 
Computer resources are normally overprovisioned according to the worst execution time to guarantee low decoding latency and avoid decoding failure due to insufficient decoding iterations.

Decoding error prediction is used to enhance RAN performance. With that, HARQ feedback is generated based on the decoding error prediction without the need to wait for the complete iterations of decoding. For example, retransmission latency is reduced with fast HARQ management for ultra-low latency services
%~\cite{}
, and the reliability of vRAN is enhanced by providing prediction on feedback when the decoding deadline is reaching during deficiency of edge computing~\cite{Nuberu}.
And there are various approaches to predict the decoding results. For example,
the lightweight Channel estimation-based approaches can only provide low prediction accuracy, 
while Log-Likelihood Ratio (LLR) based approaches can provide high accuracy, but need few iterations of decoding.
The accuracy of prediction is critical for both end-to-end latency and spectrum resource efficiency. 
A false positive, i.e., acknowledging a packet that is not decodable, requires recovery of the uplink packet by higher layers such as RLC or even TCP with larger latency. false negative, i.e. NACK a packet that is decodable, triggers unnecessary transmission with wasting of the spectrum.
%Fig~\ref{fig:Hdynamic} shows the time needed for prediction compared with the traditional completion decoding. Feedback latency is significantly reduced but still fluctuates
\fi

\end{description}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/BSR_latency.png}
         \caption{Scheduling efficiency (BSR latency)}
         \label{fig:BSR_latency}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figs/PHR_latency.png}
         \caption{Throughput decrease (PHR latency)}
         \label{fig:PHR_latency}
     \end{subfigure}
        \caption{RAN performance impacted by increased latency in MAC-CE processing} 
        \label{fig:BSRPHR_latency}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.85\linewidth]{figs/MACCE.png}
  %\caption{Code block $n$ contains MAC-CE and MAC SDU}
  \caption{MAC-CE and MAC SDU are wrapped at the end of a MAC PDU, has to be decoded at the edge to guaranteeing low latency of MAC control messages.}
  \label{fig:MACCE}
\end{figure}





\iffalse
To verify the performance effects of delaying these messages we artificially delay actions on these messages with an experimental setup of a COTS mobile phone connected with a srsRAN base station over the air. figure~\ref{fig:BSRPHR_latency}~(a) shows a significant reduction in the efficiency even with a few milliseconds of latency increase. As shown in figure~\ref{fig:BSRPHR_latency}~(b), the throughput substantially decreases with a modest increase in delay for a moving device.
To avoid introducing offloading-related latency to BSR/PHR message processing, the decoding of these control messages needs to be performed at the edge. Unfortunately, these MAC control messages can not be decoupled from user traffic before decoding at the receiver side. As shown in figure~\ref{fig:MACCE}, MAC control messages are ``wrapped'' as a MAC structure (called MAC CE) in the same way as user-plane MAC SDUs within a single MAC PDU at the MAC layer.  The MAC PDU gets divided into Coding Blocks (packets) based on bit size. Thus, as shown in figure~\ref{fig:MACCE}, the last coding block $n$ contains both MAC control messages and UL user traffic and can only be multiplexed after decoding.
%\textit{Coding Blocks containing BSR/PHR messages should be identified/isolated and processed at the edge to satisfy the BSR/PHR latency requirement. In \Name{} we achieve this by predicting when BSR/PHR transmissions will occur and having the base station schedule small grants, thus forcing the mobile device to send BSR/PHR separate from other uplink traffic.}
\fi





%\Name{} resort to the link adaptation (LA) algorithm implemented in the MAC scheduler for load adaptation. 
%The key idea for load adaptation is that we adjust the priorities of the two queues which allows \Name{} to prioritize executing emerging run-to-completion tasks (e.g. tasks only have 1ms time budget left) over prediction tasks. This postpones prediction processing, and results in deadline missing (treated as NACKs) when the resource is in shortage.

\iffalse
\begin{figure}
  \includegraphics[width=0.5\linewidth]{figs/budget.png}
  \caption{The 3ms (k=3) HARQ deadline includes UL signal acquisition, front-haul transporting, UL sub-frame processing, and DL grant generation, so UL processing have approximately a 1.5ms time budget.}
  \label{fig:budget}
\end{figure}
\fi
\iffalse
\begin{figure}
  \includegraphics[width=0.5\linewidth]{figs/prediction.png}
  \caption{HARQ prediction accuracy relates to the prediction load; SNR-based prediction is used for 0 prediction load, others are LLR-based.}
  \label{fig:prediction}
\end{figure}
\begin{figure}
  \includegraphics[width=0.5\linewidth]{figs/edgeload_dynamic.png}
  \caption{H-iters prediction load varies over time.}
  \label{fig:Hdynamic}
\end{figure}
\fi
\iffalse
(add figure here for illustration)
\begin{figure}
  \includegraphics[width=0.4\linewidth]{figs/LLRs.png}
  \caption{convergence of LLRs}
  \label{fig:LLRs}
\end{figure}
\fi

\iffalse
\xenofon{I would restructure this section to only give details about the challenges we have to deal with. This needs to reflect the things we mentioned in the intro, only with more details and experimental results as proof. Most of the text of this section can be reused, but I would avoid mentioning anything that \Name{} does to deal with those challenges here (the current text has a lot of those references). This is what Section 3 introduces. Suggested restructuring:

\textbf{Section title:} UL decoding offloading challenges

\textbf{Subsection:} HARQ feedback deadlines and prediction -- Here we explain how the HARQ mechanism works. This should talk about how a MAC PDU is broken into code blocks, how those code blocks are decoded, how HARQ feedback has deadlines, and what happens if we miss the deadlines. Also, show through a small experiment and by citing all prior work, how early HARQ predictions work and their accuracy. I remember you had many results about this, so it would be good to establish that you verified the claims of past works experimentally.

\textbf{Subsection:} Inefficient use of compute/network resources -- Here, we need to show 2 experiments to back our claims from the intro: 

1) show that it is not always optimal to offload to the remote location all excess traffic of a burst, because in upcoming slots we might have spare capacity at the edge, which will go to waste (and we get the same result in terms of network performance). With this, we want to establish that some kind of control mechanism is needed to decide if/when we should offload.

2) show with numbers that there is a trade-off in the midhaul throughput requirements and compute at the edge and remote location based on what you are offloading (IQ samples vs LLRs). With this, you want to establish that there are different offloading options, which might be useful on different occasions. You already have some text about this in 2.2, but it has to be simplified (I have added some comments there on how you could do that).
\fi