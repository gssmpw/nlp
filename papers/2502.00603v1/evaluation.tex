\section{Implementation}
\Name{} is developed using the srsRAN library~\cite{srsRAN}, leveraging its finely tuned LDPC decoders for X86/ARM server architectures. Validated on x86 platforms, \Name{} can be easily ported to other architectures, such as ARM-based 5G accelerators, with minimal effort. Operating within a Linux environment, \Name{} conforms to the srsRAN’s recommended settings. While designed for 5G NR, its principles allow compatibility with 4G LTE systems, particularly in the context of decoding processes.
The detail of this implementation are explored in the following discussion.
%seperated decoding parallzied workers
%trnasporter
%scheduling frame work


\subsection{Decoding Scheduling}
In available RAN implementations~\cite{OAI,srsRAN}, a slot-based pipeline model for the PHY layer is commonly adopted, offering limited parallelism for vRAN. Specifically, PHY layer processing of sequential slots is pipelined with multiple slot worker threads since the slot arrival period is shorter than the processing time of a single slot~\cite{Nuberu}. To enable the decoding scheduling framework of \Name{}, we separated the decoding worker threads from the slot workers. Each decoder worker is assigned to a single CPU core, allowing the parallelization of decoding tasks across multiple CPU cores. Allocating more cores to the decoder pool increases \Name{}’ decoding capacity (bits/second).

Slot workers pass FEC decoding requests to edge decoder workers through the Early Queue and invoke callback functions when early decoding finishes. Completion decoding is then placed in the completion queues unless it is offloaded to the remote. Each edge worker has its own decoding task queues. For an edge decoding pool of 
$N$ CPU cores, decoding packets are distributed across the 
$N$  cores as fresh decoding tasks arrive. Within a core, the decoder worker executes tasks based on the scheduling of the early decoding queue and completion queues.

\subsection{Offloading Control}
\Name{} employs an adaptation offloading algorithm to dynamically distribute completion decoding tasks between edge and remote locations. Offloading transportation is managed by a dedicated CPU core, with an offload queue shared by edge decoding workers. LLRs and decoding context are transported between edge and remote using a data structure with typed indexes at the headers. Offloading data is managed by a separate thread, utilizing semaphores for thread synchronization.


\subsection{Link Adaptation}
For link adaptation, a timer enforces HARQ deadlines and decode time budgeting. Before each iteration, \Name{} checks the available slack time (the time before a task’s deadline) and determines if the execution time for another decoding iteration is less than the slack time. If the execution time exceeds the slack time, it is identified as a missed deadline. Based on the monitored BLER, the MAC scheduler can adjust the load through link adaptation. The current link adaptation implementation of srsRAN relies on a fixed mapping between expected SNR and MCS for simplicity. We have added a dynamic link adaptation algorithm to the MAC scheduler for \Name{}.




\iffalse
\textbf{vRAN implementation:}
To meet the processing deadline,
there are different implementation models for vRAN. For example, the slot-based pipeline model~\cite{OAI,srsRAN} overlaps the processing of one slot with that of others. This provides a simple parallelism for vRAN but can Hardly scale with the number of CPU cores available.
In existing open source 5G implementations~\cite{srsRAN, OAI}, parallelization of the physical layer is implemented based on this which has both DL and UL processing (including UL decoding) integrated within a single slot worker thread for simplicity. Because the subframe arrival period is smaller than the processing time of a single subframe (almost 3ms), sequential slots are pipe-lined with multiple subframe worker threads~\cite{OAI,srsRAN}. In such an implementation, connection failures can be caused by DL processing blocking due to UL deadline violation~\cite{Nuberu, RT-OPEX, CloudIQ}, and packets from a single subframe are sequentially decoded in a single thread.

An alternative is the task-queue-based worker thread model, which enables parallelism of tasks within one slot with more smaller granularity
%Prioritizing data parallelism within the processing of one frame yields better performance than using pipeline parallelism across multiple frames
So it yields better performance than using pipeline parallelism across multiple slots.
In a queue-based worker thread mode, incoming tasks or events are placed in a queue, and worker threads are used to process the tasks in the order they were received. This approach can help ensure that tasks are handled in a timely and efficient manner, and can improve overall system performance and responsiveness.
\fi


\section{Evaluation}
We evaluate \Name{} using off-the-shelf Intel Xeon Gold 6126 CPU (24 cores @ 2.6 GHz) servers on the edge and remote sides as vRAN servers, connected via a 100 GbE switch. To simulate edge computing resource constraints, we limit the number of cores for edge decoding workers, with each core capable of decoding at 250 bits/µs. Middlehaul (MH) network latency is emulated by adding a virtual queue at the remote server using a virtual interface and qdisc, \coloredtext{red}{
with a typical MH latency setup of 10ms~\cite{mhlatency}, adjustable to measure \Name{}’s performance under varying MH latencies}.


\coloredtext{red}{
We emulated variable decoding loads using multiple 20MHz cells. To test with realistic cell traffic pattens (packet size, MCS, intervals), we used a traffic generator based on traces collected from real-world cells via the Falcon sniffer~\cite{falcon}, capturing a variety of traffic patterns at peak times. This setup records packet size, MCS used, intervals between packets, and traffic follow can be identified with device ID in the grant.
To generate the load, we randomly pick traffic flows from the trace for each cell. The spectrum is fully loaded by adding as many flows as possible.
MCS adaptation for \Name{} is applied to for change packet size with link adaptation. For injecting emulated traffics, emulated radio front-ends are used.
}

It is important to note that the original srsRAN implementation lacks a deadline mechanism for uplink processes. Consequently, any uplink processing delays adversely affect downlink performance, leading to synchronization issues and reduced throughput~\cite{Nuberu}. Due to this inherent limitations, comparing against native srsRAN as a baseline is not possible. Instead, our baseline aligns with common practices observed in commercial RAN deployments for a more fair comparison. Specifically, we consider two non-\Name{} implementations of edge decoders for comparison purposes:

\textbf{BaseLine} has only a decoder pool at the edge with only traditional "run-to-completion" decoders. 
Upon missing the HARQ deadline, unfinished decoding packets are discarded as decoding failures, resulting in NACK feedback for HARQ.

\textbf{Nuberu}-like 
has an edge decoding pool with HARQ prediction and PRB-based load adaptation (or computing congestion controll~\cite{Nuberu})). It cannot offload any decoding loads. Decoding processing at the edge is performed in a traditional run-to-completion fashion, but prediction is allowed when edge decoding resources are deficient. Additionally, traffic is limited by capping the spectrum resource (PRBs) usage for load adaptation.

\subsection{Performance with decoding offloading}
\Name{} is designed to significantly enhance edge efficiency and provide greater elasticity for uplink decoding by leveraging available MiddleHaul (MH) bandwidth. Our evaluation of this part focuses on throughput and latency in a fully loaded UDP traffic scenario over an emulated 10-cell network, each cell operating at 20MHz. We vary the number of CPU cores and available MH bandwidth to assess performance. This controlled environment excludes the influence of MAC Control Elements (MAC-CE) and considers a single traffic type for a fair comparison against the native Nuberu framework, which also does not account for MAC CE and traffic Quality of Service (QoS).

With a decoding delay budget uniformly set at 20ms for the majority of traffic types~\cite{3gpp}—Figure~\ref{fig:heatmap} illustrates heat maps that reveal how throughput and latency shift in response to variable resource allocation. The evidence from these maps decisively indicates \Name{}'s superior capacity to improve resource efficiency and elasticity at the network edge compared to Nuberu and BaseLine. \Name{} is able to provide enough decoding capacity (white region in Figure~\ref{fig:heatmap}) in various resource combination. Both Nuberu and BaseLine are constrained by their inability to offload DU workloads to remote servers, tethering their throughput capabilities exclusively to the CPU cores at hand. BaseLine, in particular, encounters an early throughput cliff (90\% decrease) as CPU resources dwindle (10 cores), with an increasing number of packets failing to meet the HARQ deadline and subsequently being discarded as irrecoverable decoding errors. Nuberu exhibits a marginal improvement due to its predictive handling of packets still undergoing decoding, which permits these packets to await idle CPU slots within the allotted 20ms delay budget. However, when edge resources reach their saturation point - evident in the darker areas of Figure~\ref{fig:throughput_heatmap}—the minimal HARQ deadline misses achieved through Nuberu's predictive approach are insufficient to avert a decline in throughput (90\% decline with 5 cores), as available slots within the 20 ms decoding window become increasingly elusive.
In contrast, \Name{} distinguishes itself by not only realizing a higher decoding throughput that decreases incrementally as edge resources are depleted, but also by leveraging MH bandwidth to sustain considerable decoding throughput levels, even when edge computing resources are substantially limited. Notably, \Name{} maintains up to 75\% of the maximal throughput capacity by offloading DU workloads through the MH network bandwidth and eliminating early decoding load through link adaptation.

Figure~\ref{fig:latency_heatma} shows that increasing CPU cores improves latency, while offloading does not directly affect latency. Although \Name{} incurs a modest latency increase due to edge buffering, it successfully maintains a 99th percentile latency of 20ms, comparable to BaseLine and Nuberu. This consistency demonstrates the effectiveness of the Earliest Deadline First (EDF) scheduling employed by \Name{}.
Figure~\ref{fig:throughput-latency} illustrates the relationship between throughput and latency as a function of the number of edge CPU cores under fully provisioned MH network conditions. \Name{} outperforms Nuberu with higher throughput and lower latency in all conditions. We also compare \Name{} without offloading to Nuberu, finding that in such cases, Nuberu performs similarly to \Name{}

In summary, \Name{}' strategy of decoding splitting and offloading optimizes network resource utilization and ensures robust performance across varying operational demands and resource combinations, establishing it as a crucial innovation to enhance 5G RAN efficiency.




\begin{figure}
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/throughput_heat.png}
  \caption{Throughput}
  \label{fig:throughput_heatmap}
\end{subfigure}%
\hfill
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/latency_heat.png}
  \caption{Latency}
  \label{fig:latency_heatma}
\end{subfigure}
\begin{subfigure}{.9\textwidth}
     \centering
  \includegraphics[width=\linewidth]{figs/throughput-latency.png}
  \caption{Throughput vs. latency with abundant MH BW}
  \label{fig:throughput-latency}
\end{subfigure}
\caption{\coloredtext{red}{
Performance with limited resource: 10-cells, highlighting \Name{}' superior performance in resource-limited environments with a 20ms packet delay budget}}
\label{fig:heatmap}
\end{figure}



\subsection{Evaluating the Scheduling Framework of \Name{}}
The scheduling framework of \Name{} is put to the test through a series of benchmarks aimed at assessing its performance under various workloads.

\textbf{Deadline Missing Rate Control}
The ability of \Name{} to manage HARQ feedback deadlines is a critical aspect of its performance. Figure~\ref{fig:edge_dead} demonstrates that \Name{} consistently maintains a low deadline missing rate, significantly below the 0.1\% configuration level, and well within acceptable bounds compared to the 5\% target retransmission rate prescribed by standards~\cite{3gpp}. In contrast, BaseLine suffers from a high rate of missed deadlines due to resource scarcity and lack of adaptive rate control, resulting in increased NACKs and retransmissions. Nuberu shows similar improvement in managing missed deadlines and BLER, although its PRB-based load adaptation tends to overcompensate as we show next.

%without precisely meeting the BLER target.
%\Name{} properly control resource-->bler reflect --> load reduce

\textbf{Impact on Spectrum Usage}
\coloredtext{red}{
The deadline missing rate directly affects how load is adapted through spectrum usage based on decoding capacities. Figures~\ref{fig:spt} and~\ref{fig:spr} quantify the impact of load adaptations on spectrum usage. Spectrum efficiency for transmission, shown in Figure~\ref{fig:spt}, represents bit rates for transmission and is influenced by the load adapter. Without load adaptation, the full bit rate is enabled, as seen in the baseline. Nuberu starts its load adaptation by limiting spectrum allocation at 6 cells, while \Name{} triggers load adaptation at 8 cells. 
Spectrum efficiency for receiving, depicted in Figure~\ref{fig:spr}, represents the ratio of bits successfully decoded. Overloading is indicated when reception is lower than transmission, a scenario in which BaseLine experiences a significant decline in reception due to overloading, which is also confirmed by the significant BLER in Figure~\ref{fig:edge_dead}. \Name{} strategically lowers spectrum efficiency to reduce the load on edge decoding, contrasting with Nuberu's PRB-based adaptation, which restricts spectrum allocation and can lead to higher underutilization and inefficiency.
}

\textbf{Maximizing Edge Resource Utilization}
Figure~\ref{fig:edge_utlization} illustrates \Name{}' strategic utilization of edge resources. \Name{} effectively uses periods of inactivity at the edge to facilitate completion decoding, especially after offloading tasks to remote servers via MH networks. This adaptive utilization contrasts with BaseLine, which shows lower resource usage due to its rigid adherence to strict decoding delay budgets. Like \Name{}, Nuberu leverages extended decoding delays post-prediction to enhance resource utilization. \Name{} further distinguishes itself by efficiently prioritizing prediction decoding when necessary, achieving superior resource utilization rates and offloading excess completion decoding to remote servers.
There are no differences in edge utilization when the systems are underloaded(2/4-cells) or excessively overloaded(10-cells). When underloaded, \Name{} and Nuberu perform similarly to BaseLine, with no need for load adaptation or offloading.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.45\textwidth} % First subfigure
        \centering
        \includegraphics[width=\linewidth]{figs/deadline_missing.png}
        \caption{}
        \label{fig:edge_dead}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth} % Second subfigure
        \centering
        \includegraphics[width=\linewidth]{figs/spectrumEffi.png}
        \caption{}
        \label{fig:spt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth} % Third subfigure
        \centering
        \includegraphics[width=\linewidth]{figs/spectrumUsage.png}
        \caption{}
        \label{fig:spr}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth} % Fourth subfigure
        \centering
        \includegraphics[width=\linewidth]{figs/edge_utilization.png}
        \caption{}
        \label{fig:edge_utlization}
    \end{subfigure}
    \caption{A comprehensive assessment across various cell configurations, showcasing (a) the edge HARQ deadline missing rate, (b) spectrum efficiency for transmission, (c) spectrum efficiency for receiving, and (d) overall edge resource utilization. The bar graphs compare \Name{} with Nuberu and Baseline, indicating \Name{}'s ability to minimize deadline misses and optimize resource use while maintaining high transmission and receiving efficiency.}
    \label{fig:schedulingEvaluation}
\end{figure*}



Overall, \Name{}’s scheduling framework demonstrates robust performance, adeptly managing deadlines, optimizing spectrum usage, and maximizing edge resource utilization even under constraints. \Name{} proves to be a sophisticated solution capable of intelligently navigating network load and resource allocation challenges, ensuring efficient operation across varying conditions.



%\subsection{Throughput with different traffic}
\subsection{Traffic-Dependent Throughput and Efficiency}
The system throughput and efficiency of edge computing in uplink traffic decoding is highly influenced by the characteristics of the traffic. This section identifies the key factors that affect system performance in the context of uplink traffic.


%In our analysis, we prioritize enhancing edge computing efficiency, under the assumption that Fronthaul (FH) network resources are abundantly available. This focus is driven by the objective of reducing edge computing costs, contrasting with the accessibility of FH, which we consider to be more readily attainable.
%In the subsequent measurements, our emphasis remains on the potential efficiency gains in edge computing. We operate under the premise that FH network resources are abundant, as the constraints posed by FH availability have been addressed in a previous subsection. This allows us to concentrate on optimizing the edge computing aspect, which is a critical factor in our overall evaluation.


\textbf{Early Decoding Overhead}
When the system is constrained in edge decoding capacity, the overhead load of early decoding directly affects system throughput. \Name{}' early decoding phase, encompassing prediction and pre-parsing, significantly impacts edge decoding efficiency. The workload of pre-parsing is influenced by the proportion of subheader bits within the overall PDU. Smaller packets with a larger share of subheader bits require more processing for pre-parsing.
Figure~\ref{fig:throughput_overhead} shows how varying pre-parsing bit ratios affect decoding throughput under different load conditions (2-10 cells traffic load). When the system is lightly loaded (2 cells), throughput is not significantly affected by pre-parsing load due to sufficient edge decoding capacity. However, as the system becomes more loaded, \Name{} experiences a throughput decline with an increasing ratio of pre-parsing bits. This decline is more significant under heavier loads, decrease by 30\% with 15\% pre-parsing bits when we have 10cells load.


Figure~\ref{fig:early_overhead} further illustrates the load implications across different traffic scenarios, highlighting that a higher pre-parsing load correlates with a decline in throughput. This is because more workload needs to be processed under low latency constraints, exacerbating the impact of limited edge decoding capacity.



%Higher-order MCS with more bits per symbol can potentially increase the complexity and processing time of early decoding due to the denser information content. This also verify the rational of using adaptive link adaptation...lower-order MCS for scenarios with limited computing capacity and optimizing resource distribution to accommodate higher-order MCS when sufficient resources are available. 


\begin{figure}
%\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/throughput_vs_overhead.png}
  \caption{Throughput with different \\ pre-parsing ratios}
  \label{fig:throughput_overhead}
\end{subfigure}%
\hfill
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figs/early_overhead.png}
  \caption{Iteration overhead with different \\ pre-parsing ratios}
  \label{fig:early_overhead}
\end{subfigure}
%\caption{Effects of overhead due to different pre-parsing ratios}
\caption{Impact of Pre-parsing Ratios on Decoding Efficiency: (a) Shows the throughput achieved with varying ratios of pre-parsing, illustrating how the increase in pre-parsing content (from 5\% to 15\%) affects the relative throughput across different numbers of cells. (b) Depicts the average decoding overhead associated with prediction and different levels of pre-parsing,}
\label{fig:overhead}
\end{figure}

\begin{figure}
    \centering
        \begin{subfigure}[t]{\textwidth} % First sub-subfigure
            \centering
            \includegraphics[width=\linewidth]{figs/throughput_delaybudget.png}
            \caption{MH latency 10ms}
            \label{fig:throughput_delaybudget10}
        \end{subfigure}%
        \hfill
        \begin{subfigure}[t]{\textwidth} % Second sub-subfigure
            \centering
            \includegraphics[width=\linewidth]{figs/throughput_delaybudget25.png}
            \caption{\coloredtext{red}{MH latency 15ms}}
            \label{fig:throughput_delaybudget15}
        \end{subfigure}
        \caption{The relative decoding throughput achieved for different delay budgets (5ms, 20ms, 100ms) across an increasing number of cells, indicating how latency constraints affect throughput capacity.}
        \label{fig:throughput_delaybudget}
    \hfill
    \begin{subfigure}[t]{\textwidth} % Second subfigure
        \centering
        \includegraphics[width=\linewidth]{figs/MixedDelay.png}
        \caption{The decoding latency experienced by different types of traffic.}
        \label{fig:mixeddelay}
    \end{subfigure}
\end{figure}


\textbf{Impact of Delay Budgets on Throughput}
The delay budgets assigned to uplink traffic critically determine the potential for offloading and leveraging idle edge CPU cycles. Figure~\ref{fig:throughput_delaybudget} presents the relative throughput achievable with six CPU cores under varied decoding delay budgets. Larger delay budgets enable more efficient utilization of edge CPU resources by allowing longer waits for available CPU slots, leading to significant throughput improvements, especially when comparing $20ms$ to $5ms$ delay budgets. However, benefits diminish with delay budgets extending beyond $20ms$, as opportunities to offload to remote servers decrease. There is an almost doubling of throughput
%are almost twice throughput increase 
from $5ms$ traffic to $20ms$ traffic.
\coloredtext{red}{
Alternatively, large MH latency deters the offloading.
As we increase the MH latency from $10ms$ (~\ref{fig:throughput_delaybudget10}) to $15ms$ (~\ref{fig:throughput_delaybudget15}), the throughput of traffic with $20ms$ delay budget declines significant during overloaded situation (8cells and 10cells).
}

In real-world scenarios, uplink traffic often includes a variety of services, each with unique latency requirements. \Name{} effectively handles this diversity through its pre-parsing feature, which categorizes and manages traffic based on their delay budgets, as shown in Figure~\ref{fig:mixeddelay}. 
Our evaluation simulated a mix of service categories
: 5\% of the traffic was allocated to VR applications with a stringent 5ms delay budget, 50\% to video live streaming services like Zoom with a 20ms delay budget, and the remaining to best-effort traffic like Google Photos with a more lenient 100ms delay budget. 
The traffic characteristics, including data size and rate, were modeled on real application traces, leading to scenarios where a single MAC PDU packet might carry data from multiple service types.
Figure~\ref{fig:mixeddelay} demonstrates that both the mean latency and 99.9 percentile latency are differentiated for the three different traffic types and perform under the QoS requirements. Although our test distribution might not precisely reflect typical real-world traffic patterns, the results provide valuable insights. They demonstrate \Name{}’s capacity for adaptive traffic management, showcasing its ability to dynamically and efficiently fulfill the varying delay requirements of mixed traffic.


%JC{Adding another cluster of bars with limited MH bandwidth?}
%JC{To show achievable throughput with different MH latencies...?}
































