@article{10.1145/2912147,
author = {Sintsova, Valentina and Pu, Pearl},
title = {Dystemo: Distant Supervision Method for Multi-Category Emotion Recognition in Tweets},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2912147},
doi = {10.1145/2912147},
abstract = {Emotion recognition in text has become an important research objective. It involves building classifiers capable of detecting human emotions for a specific application, for example, analyzing reactions to product launches, monitoring emotions at sports events, or discerning opinions in political debates. Most successful approaches rely heavily on costly manual annotation. To alleviate this burden, we propose a distant supervision method—Dystemo—for automatically producing emotion classifiers from tweets labeled using existing or easy-to-produce emotion lexicons. The goal is to obtain emotion classifiers that work more accurately for specific applications than available emotion lexicons. The success of this method depends mainly on a novel classifier—Balanced Weighted Voting (BWV)—designed to overcome the imbalance in emotion distribution in the initial dataset, and on novel heuristics for detecting neutral tweets. We demonstrate how Dystemo works using Twitter data about sports events, a fine-grained 20-category emotion model, and three different initial emotion lexicons. Through a series of carefully designed experiments, we confirm that Dystemo is effective both in extending initial emotion lexicons of small coverage to find correctly more emotional tweets and in correcting emotion lexicons of low accuracy to perform more accurately.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {13},
numpages = {22},
keywords = {Distant supervision, emotion recognition, natural language processing, semi-supervised learning, text mining, twitter}
}

@inproceedings{10.1145/2971485.2971516,
author = {Bruun, Anders and Law, Effie Lai-Chong and Heintz, Matthias and Eriksen, Poul Svante},
title = {Asserting Real-Time Emotions through Cued-Recall: Is it Valid?},
year = {2016},
isbn = {9781450347631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971485.2971516},
doi = {10.1145/2971485.2971516},
abstract = {Asserting emotions through free-recall is commonly used to evaluate user experience (UX) of interactive systems. From psychology we know that free-recall of emotions leads to a significant memory bias where participants rely on a few of the most intense episodes when asserting an overall experience. It is argued that cued-recall can reduce the memory bias in UX evaluations. Yet, this has not been studied empirically. We present a systematic empirical study based on 38 participants. We measured emotions in terms of objective galvanic skin responses (GSR) and subjective Self-Assessment Manikin (SAM) ratings. We found significant correlations between emotions experienced in real-time and those experienced during cued-recall. This validates the use of cued-recall for UX evaluations. An implication is that HCI researchers and practitioners now have cued-recall as an alternative that significantly reduces the memory bias and enables highly detailed measurements of emotions while not disturbing participants during system interaction.},
booktitle = {Proceedings of the 9th Nordic Conference on Human-Computer Interaction},
articleno = {37},
numpages = {10},
keywords = {real-time, emotion, cued-recall, User experience},
location = {Gothenburg, Sweden},
series = {NordiCHI '16}
}

@inproceedings{10.1145/3442188.3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/3491102.3517453,
author = {Kaur, Harmanpreet and McDuff, Daniel and Williams, Alex C. and Teevan, Jaime and Iqbal, Shamsi T.},
title = {“I Didn’t Know I Looked Angry”: Characterizing Observed Emotion and Reported Affect at Work},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517453},
doi = {10.1145/3491102.3517453},
abstract = {With the growing prevalence of affective computing applications, Automatic Emotion Recognition (AER) technologies have garnered attention in both research and industry settings. Initially limited to speech-based applications, AER technologies now include analysis of facial landmarks to provide predicted probabilities of a common subset of emotions (e.g., anger, happiness) for faces observed in an image or video frame. In this paper, we study the relationship between AER outputs and self-reports of affect employed by prior work, in the context of information work at a technology company. We compare the continuous observed emotion output from an AER tool to discrete reported affect obtained via a one-day combined tool-use and diary study (N = 15). We provide empirical evidence showing that these signals do not completely align, and find that using additional workplace context only improves alignment up to 58.6\%. These results suggest affect must be studied in the context it is being expressed, and observed emotion signal should not replace internal reported affect for affective computing applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {199},
numpages = {18},
keywords = {workplace, emotion labeling, Affect},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3626772.3657740,
author = {Yang, Zhenyu and Xue, Dizhan and Qian, Shengsheng and Dong, Weiming and Xu, Changsheng},
title = {LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657740},
doi = {10.1145/3626772.3657740},
abstract = {Zero-Shot Composed Image Retrieval (ZS-CIR) has garnered increasing interest in recent years, which aims to retrieve a target image based on a query composed of a reference image and a modification text without training samples. Specifically, the modification text describes the distinction between the two images. To conduct ZS-CIR, the prevailing methods employ pre-trained image-to-text models to transform the query image and text into a single text, which is then projected into the common feature space by CLIP to retrieve the target image. However, these methods neglect that ZS-CIR is a typicalfuzzy retrieval task, where the semantics of the target image are not strictly defined by the query image and text. To overcome this limitation, this paper proposes a training-free LLM-based Divergent Reasoning and Ensemble (LDRE) method for ZS-CIR to capture diverse possible semantics of the composed result. Firstly, we employ a pre-trained captioning model to generate dense captions for the reference image, focusing on different semantic perspectives of the reference image. Then, we prompt Large Language Models (LLMs) to conduct divergent compositional reasoning based on the dense captions and modification text, deriving divergent edited captions that cover the possible semantics of the composed target. Finally, we design a divergent caption ensemble to obtain the ensemble caption feature weighted by semantic relevance scores, which is subsequently utilized to retrieve the target image in the CLIP feature space. Extensive experiments on three public datasets demonstrate that our proposed LDRE achieves the new state-of-the-art performance.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {80–90},
numpages = {11},
keywords = {composed image retrieval, multi-modal retrieval, zero-shot learning},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3629606.3629646,
author = {Zhang, He and Li, Xinyang and Qiu, Christine and Fu, Xinyi},
title = {Decoding Fear: Exploring User Experiences in Virtual Reality Horror Games},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629646},
doi = {10.1145/3629606.3629646},
abstract = {This preliminary study investigated user experiences in VR horror games, highlighting fear-triggering and gender-based differences in perception. By utilizing a scientifically validated and specially designed questionnaire, we successfully collected questionnaire data from 23 subjects for an early empirical study of fear induction in a virtual reality gaming environment. The early findings suggest that visual restrictions and ambient sound-enhanced realism may be more effective in intensifying the fear experience. Participants exhibited a tendency to avoid playing alone or during nighttime, underscoring the significant psychological impact of VR horror games. The study also revealed a distinct gender difference in fear perception, with female participants exhibiting a higher sensitivity to fear stimuli. However, the preference for different types of horror games was not solely dominated by males; it varied depending on factors such as the game’s pace, its objectives, and the nature of the fear stimulant.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {411–419},
numpages = {9},
keywords = {Virtual reality game, affective computing, coping strategy, fear},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@inproceedings{10.1145/3637528.3671552,
author = {Liu, Zhiwei and Yang, Kailai and Xie, Qianqian and Zhang, Tianlin and Ananiadou, Sophia},
title = {EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671552},
doi = {10.1145/3637528.3671552},
abstract = {Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of large language models (LLMs), researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on 3 classification tasks and 2 regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 8 regression tasks and 6 classification tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our models with a variety of LLMs and sentiment analysis tools on AEB, where our models outperform all other open-sourced LLMs and sentiment analysis tools, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools. This project is available at https://github.com/lzw108/EmoLLMs/.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5487–5496},
numpages = {10},
keywords = {affective evaluation benchmark, affective instruction dataset, emotion detection, large language models, sentiment analysis},
location = {Barcelona, Spain},
series = {KDD '24}
}

@INPROCEEDINGS{10150364,
  author={Stoev, Teodor and Yordanova, Kristina and Tonkin, Emma L.},
  booktitle={2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)}, 
  title={Experiencing Annotation: Emotion, Motivation and Bias in Annotation Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={534-539},
  keywords={Pervasive computing;Annotations;Bibliographies;Conferences;Task analysis;data annotation;data labeling;emotion;stress},
  doi={10.1109/PerComWorkshops56833.2023.10150364}}

@ARTICLE{10253654,
  author={Zhao, Sicheng and Hong, Xiaopeng and Yang, Jufeng and Zhao, Yanyan and Ding, Guiguang},
  journal={Proceedings of the IEEE}, 
  title={Toward Label-Efficient Emotion and Sentiment Analysis}, 
  year={2023},
  volume={111},
  number={10},
  pages={1159-1197},
  keywords={Training;Emotion recognition;Sentiment analysis;Complexity theory;Taxonomy;Speech recognition;Affective computing;Human factors;Artificial intelligence;Labeling;Affective computing;artificial emotional intelligence (AEI);emotion and sentiment analysis (ESA);label-efficient learning},
  doi={10.1109/JPROC.2023.3309299}}

@INPROCEEDINGS{1034632,
  author={Lee, C.M. and Narayanan, S. and Pieraccini, R.},
  booktitle={IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.}, 
  title={Recognition of negative emotions from the speech signal}, 
  year={2001},
  volume={},
  number={},
  pages={240-243},
  keywords={Emotion recognition;Speech recognition;Principal component analysis;Automatic speech recognition;Speech analysis;Man machine systems;Linear discriminant analysis;Probability distribution;Statistical distributions;Frequency},
  doi={10.1109/ASRU.2001.1034632}}

@INPROCEEDINGS{10388198,
  author={Yang, Vera and Srivastava, Archita and Etesam, Yasaman and Zhang, Chuxuan and Lim, Angelica},
  booktitle={2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
  title={Contextual Emotion Estimation from Image Captions}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  keywords={Computer vision;Affective computing;Annotations;Computational modeling;Natural languages;Estimation;Predictive models;Large language model;emotion estimation;image captioning;context;ChatGPT;GPT-3.5},
  doi={10.1109/ACII59096.2023.10388198}}

@ARTICLE{10416364,
  author={Bota, Patrícia and Cesar, Pablo and Fred, Ana and da Silva, Hugo Plácido},
  journal={IEEE Transactions on Affective Computing}, 
  title={Exploring Retrospective Annotation in Long-Videos for Emotion Recognition}, 
  year={2024},
  volume={15},
  number={3},
  pages={1514-1525},
  keywords={Annotations;Streaming media;Real-time systems;Emotion recognition;Motion pictures;Physiology;Electroencephalography;Annotation;emotion recognition;physiological signals;retrospective},
  doi={10.1109/TAFFC.2024.3359706}}

@INPROCEEDINGS{10447760,
  author={Du, Xingjian and Yu, Zhesong and Lin, Jiaju and Zhu, Bilei and Kong, Qiuqiang},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Joint Music and Language Attention Models for Zero-Shot Music Tagging}, 
  year={2024},
  volume={},
  number={},
  pages={1126-1130},
  keywords={Vocabulary;Tagging;Chatbots;Acoustics;Decoding;Multiple signal classification;Recording;Music tagging;joint music and language attention models;Music Foundation Model},
  doi={10.1109/ICASSP48485.2024.10447760}}

@INPROCEEDINGS{10494076,
  author={Zhang, He and Li, Xinyang and Sun, Yuanxi and Fu, Xinyi and Qiu, Christine and Carroll, John M.},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games}, 
  year={2024},
  volume={},
  number={},
  pages={320-330},
  keywords={Human computer interaction;Solid modeling;Three-dimensional displays;Metaverse;Games;Virtual reality;Predictive models;Human-centered computing;Human computer interaction (HCI);Virtual reality;HCI design and evaluation methods;Computing methodologies;Artificial intelligence;Activity recognition and understanding;Database},
  doi={10.1109/VR58804.2024.00054}}

@INPROCEEDINGS{10701433,
  author={Schneider, Piotr and Mikołajewski, Dariusz and Bryniarska, Anna and Igras-Cybulska, Magdalena and Cybulski, Artur and Marcinowicz, Walery and Janiszewski, Maciej and Kawala-Sterniuk, Aleksandra},
  booktitle={2024 Progress in Applied Electrical Engineering (PAEE)}, 
  title={Methods and tools for automatic or semi-automatic recognition of selected emotions using machine learning algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Emotion recognition;Machine learning algorithms;Accuracy;Video on demand;Speech recognition;Machine learning;Feature extraction;Recording;Convolutional neural networks;Web sites;machine learning algorithms;emotion recognition;artificial intelligence;automatic recognition},
  doi={10.1109/PAEE63906.2024.10701433}}

@ARTICLE{5585726,
  author={Mower, Emily and Matarić, Maja J and Narayanan, Shrikanth},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={A Framework for Automatic Human Emotion Classification Using Emotion Profiles}, 
  year={2011},
  volume={19},
  number={5},
  pages={1057-1070},
  keywords={Feature extraction;Humans;Accuracy;Support vector machines;Databases;Eyebrows;Hidden Markov models;Emotion profiles;multimodal emotion classification;nonprototypical emotions},
  doi={10.1109/TASL.2010.2076804}}

@INPROCEEDINGS{5771357,
  author={Gunes, Hatice and Schuller, Björn and Pantic, Maja and Cowie, Roddy},
  booktitle={2011 IEEE International Conference on Automatic Face \& Gesture Recognition (FG)}, 
  title={Emotion representation, analysis and synthesis in continuous space: A survey}, 
  year={2011},
  volume={},
  number={},
  pages={827-834},
  keywords={Speech;Visualization;Speech synthesis;Databases;Humans;Emotion recognition;Computational modeling},
  doi={10.1109/FG.2011.5771357}}

@ARTICLE{7112127,
  author={Soleymani, Mohammad and Asghari-Esfeden, Sadjad and Fu, Yun and Pantic, Maja},
  journal={IEEE Transactions on Affective Computing}, 
  title={Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection}, 
  year={2016},
  volume={7},
  number={1},
  pages={17-28},
  keywords={Videos;Electroencephalography;Feature extraction;Motion pictures;Databases;Tagging;Recurrent neural networks;Affect;EEG;facial expressions;video highlight detection;implicit tagging;Affect;EEG;facial expressions;video highlight detection;implicit tagging},
  doi={10.1109/TAFFC.2015.2436926}}

@ARTICLE{7160695,
  author={D’Mello, Sidney K.},
  journal={IEEE Transactions on Affective Computing}, 
  title={On the Influence of an Iterative Affect Annotation Approach on Inter-Observer and Self-Observer Reliability}, 
  year={2016},
  volume={7},
  number={2},
  pages={136-149},
  keywords={Observers;Reliability;Iterative methods;Affective computing;Face;Context;Speech;Affect annotation;ground truth;self;observers;affect detection;Affect annotation;ground truth;self;observers;affect detection},
  doi={10.1109/TAFFC.2015.2457413}}

@ARTICLE{8764449,
  author={Poria, Soujanya and Majumder, Navonil and Mihalcea, Rada and Hovy, Eduard},
  journal={IEEE Access}, 
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances}, 
  year={2019},
  volume={7},
  number={},
  pages={100943-100953},
  keywords={Emotion recognition;Task analysis;Context modeling;Taxonomy;Natural language processing;Pragmatics;Emotion recognition;sentiment analysis;dialogue systems;natural language processing},
  doi={10.1109/ACCESS.2019.2929050}}

@ARTICLE{8854185,
  author={Kossaifi, Jean and Walecki, Robert and Panagakis, Yannis and Shen, Jie and Schmitt, Maximilian and Ringeval, Fabien and Han, Jing and Pandit, Vedhas and Toisoul, Antoine and Schuller, Björn and Star, Kam and Hajiyev, Elnar and Pantic, Maja},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild}, 
  year={2021},
  volume={43},
  number={3},
  pages={1022-1040},
  keywords={Databases;Tools;Computational modeling;Biological system modeling;Sensors;Affective computing;Emotion recognition;SEWA;affect analysis;in-the-wild;emotion recognition;database;valence;arousal;facial action units},
  doi={10.1109/TPAMI.2019.2944808}}

@ARTICLE{9158345,
  author={Kollias, Dimitrios and Zafeiriou, Stefanos},
  journal={IEEE Transactions on Affective Computing}, 
  title={Exploiting Multi-CNN Features in CNN-RNN Based Dimensional Emotion Recognition on the OMG in-the-Wild Dataset}, 
  year={2021},
  volume={12},
  number={3},
  pages={595-606},
  keywords={Feature extraction;Estimation;Databases;Computer architecture;Visualization;Machine learning;Emotion recognition;Deep convolutional and recurrent neural architectures;CNN plus Multi RNN;low-, mid-, high-level features;multi-CNN feature extraction and aggregation;multi-task learning;facial image analysis;valence;arousal;emotion recognition in-the-wild;AffWildNet;AffWild and AffWild2 emotion databases;OMG-Emotion database and challenge},
  doi={10.1109/TAFFC.2020.3014171}}

@incollection{afzal2011natural,
  title={Natural affect data: Collection and annotation},
  author={Afzal, Shazia and Robinson, Peter},
  booktitle={New perspectives on affect and learning technologies},
  pages={55--70},
  year={2011},
  doi={https://doi.org/10.1007/978-1-4419-9625-1_5},
  publisher={Springer}
}

@article{batliner2003find,
  title={How to find trouble in communication},
  author={Batliner, Anton and Fischer, Kerstin and Huber, Richard and Spilker, J{\"o}rg and N{\"o}th, Elmar},
  journal={Speech communication},
  volume={40},
  number={1-2},
  pages={117--143},
  year={2003},
  doi={https://doi.org/10.1016/S0167-6393(02)00079-1},
  publisher={Elsevier}
}

@article{cheng2024emotion,
  title={Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning},
  author={Cheng, Zebang and Cheng, Zhi-Qi and He, Jun-Yan and Sun, Jingdong and Wang, Kai and Lin, Yuxiang and Lian, Zheng and Peng, Xiaojiang and Hauptmann, Alexander},
  journal={arXiv preprint arXiv:2406.11161},
  year={2024}
}

@article{devillers2005challenges,
  title={Challenges in real-life emotion annotation and machine learning based detection},
  author={Devillers, Laurence and Vidrascu, Laurence and Lamel, Lori},
  journal={Neural Networks},
  volume={18},
  number={4},
  pages={407--422},
  year={2005},
  doi={https://doi.org/10.1016/j.neunet.2005.03.007},
  publisher={Elsevier}
}

@article{hoelzemann2024matter,
  title={A matter of annotation: an empirical study on in situ and self-recall activity annotations from wearable sensors},
  author={Hoelzemann, Alexander and Van Laerhoven, Kristof},
  journal={Frontiers in Computer Science},
  volume={6},
  pages={1379788},
  year={2024},
  doi={https://doi.org/10.3389/fcomp.2024.1379788},
  publisher={Frontiers Media SA}
}

@article{nadeem2024vision,
  title={Vision-Enabled Large Language and Deep Learning Models for Image-Based Emotion Recognition},
  author={Nadeem, Mohammad and Sohail, Shahab Saquib and Javed, Laeeba and Anwer, Faisal and Saudagar, Abdul Khader Jilani and Muhammad, Khan},
  journal={Cognitive Computation},
  pages={1--14},
  year={2024},
  publisher={Springer}
}

@article{nimmi2022pre,
  title={Pre-trained ensemble model for identification of emotion during COVID-19 based on emergency response support system dataset},
  author={Nimmi, K and Janet, B and Selvan, A Kalai and Sivakumaran, N},
  journal={Applied Soft Computing},
  volume={122},
  pages={108842},
  year={2022},
  publisher={Elsevier}
}

@InProceedings{pmlr-v239-mohta23a,
  title = 	 {Are large language models good annotators?},
  author =       {Mohta, Jay and Ak, Kenan and Xu, Yan and Shen, Mingwei},
  booktitle = 	 {Proceedings on "I Can't Believe It's Not Better: Failure  Modes in the Age of Foundation Models" at NeurIPS 2023 Workshops},
  pages = 	 {38--48},
  year = 	 {2023},
  editor = 	 {Antorán, Javier and Blaas, Arno and Buchanan, Kelly and Feng, Fan and Fortuin, Vincent and Ghalebikesabi, Sahra and Kriegler, Andreas and Mason, Ian and Rohde, David and Ruiz, Francisco J. R. and Uelwer, Tobias and Xie, Yubin and Yang, Rui},
  volume = 	 {239},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v239/mohta23a/mohta23a.pdf},
  url = 	 {https://proceedings.mlr.press/v239/mohta23a.html},
  abstract = 	 {Numerous Natural Language Processing (NLP) tasks require precisely labeled data to ensure effective model training and achieve optimal performance. However, data annotation is marked by substantial costs and time requirements, especially when requiring specialized domain expertise or annotating a large number of samples. In this study, we investigate the feasibility of employing large language models (LLMs) as replacements for human annotators. We assess the zero-shot performance of various LLMs of different sizes to determine their viability as substitutes. Furthermore, recognizing that human annotators have access to diverse modalities, we introduce an image-based modality using the BLIP-2 architecture to evaluate LLM annotation performance. Among the tested LLMs, Vicuna-13b demonstrates competitive performance across diverse tasks. To assess the potential for LLMs to replace human annotators, we train a supervised model using labels generated by LLMs and compare its performance with models trained using human-generated labels. However, our findings reveal that models trained with human labels consistently outperform those trained with LLM-generated labels. We also highlights the challenges faced by LLMs in multilingual settings, where their performance significantly diminishes for tasks in languages other than English.}
}

@article{sapkota2024zero,
  title={Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development},
  author={Sapkota, Ranjan and Paudel, Achyut and Karkee, Manoj},
  journal={arXiv preprint arXiv:2411.11285},
  year={2024}
}

@article{sharma2020automated,
  title={Automated emotion recognition based on higher order statistics and deep learning algorithm},
  author={Sharma, Rahul and Pachori, Ram Bilas and Sircar, Pradip},
  journal={Biomedical Signal Processing and Control},
  volume={58},
  pages={101867},
  year={2020},
  doi={https://doi.org/10.1016/j.bspc.2020.101867},
  publisher={Elsevier}
}

@inproceedings{shvetsova2025howtocaption,
  title={Howtocaption: Prompting llms to transform video annotations at scale},
  author={Shvetsova, Nina and Kukleva, Anna and Hong, Xudong and Rupprecht, Christian and Schiele, Bernt and Kuehne, Hilde},
  booktitle={European Conference on Computer Vision},
  pages={1--18},
  doi={https://doi.org/10.1007/978-3-031-72992-8_1},
  year={2025},
  organization={Springer}
}

@inproceedings{tan-etal-2024-large,
    title = "Large Language Models for Data Annotation and Synthesis: A Survey",
    author = "Tan, Zhen  and
      Li, Dawei  and
      Wang, Song  and
      Beigi, Alimohammad  and
      Jiang, Bohan  and
      Bhattacharjee, Amrita  and
      Karami, Mansooreh  and
      Li, Jundong  and
      Cheng, Lu  and
      Liu, Huan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.54/",
    doi = "10.18653/v1/2024.emnlp-main.54",
    pages = "930--957",
    abstract = "Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field."
}

@inproceedings{tang2024pdfchatannotator,
  title={PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs},
  author={Tang, Yi and Chang, Chia-Ming and Yang, Xi},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages={419--430},
  year={2024}
}

@article{troiano2023dimensional,
  title={Dimensional modeling of emotions in text with appraisal theories: Corpus creation, annotation reliability, and prediction},
  author={Troiano, Enrica and Oberl{\"a}nder, Laura and Klinger, Roman},
  journal={Computational Linguistics},
  volume={49},
  number={1},
  pages={1--72},
  year={2023},
  doi={https://doi.org/10.1162/coli_a_00461},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{you2016building,
  title={Building a large scale dataset for image emotion recognition: The fine print and the benchmark},
  author={You, Quanzeng and Luo, Jiebo and Jin, Hailin and Yang, Jianchao},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  doi={https://doi.org/10.1609/aaai.v30i1.9987},
  year={2016}
}

@misc{zhang2024mmllmsrecentadvancesmultimodal,
      title={MM-LLMs: Recent Advances in MultiModal Large Language Models}, 
      author={Duzhen Zhang and Yahan Yu and Jiahua Dong and Chenxing Li and Dan Su and Chenhui Chu and Dong Yu},
      year={2024},
      eprint={2401.13601},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.13601}, 
}

@misc{zhang2024qualitativeresearchmeetslarge,
      title={When Qualitative Research Meets Large Language Model: Exploring the Potential of QualiGPT as a Tool for Qualitative Coding}, 
      author={He Zhang and Chuhao Wu and Jingyi Xie and Fiona Rubino and Sydney Graver and ChanMin Kim and John M. Carroll and Jie Cai},
      year={2024},
      eprint={2407.14925},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2407.14925}, 
}

