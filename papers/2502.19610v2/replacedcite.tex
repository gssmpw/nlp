\section{Related Work}
\label{sec:related_work}
% \subsection{Dialog Agents}

Many dialog agent tasks have been proposed, including offline task-oriented dialog ____ ____ and online user simulations using real humans or LM agents as responders ____ ____. 
Question generation is a related task where agents seek information relevant to a downstream task, such as user intent ____ or relevant facts ____.
Some task-oriented dialog datasets focus on clarification and information seeking, such as ____.
However, datasets such as ShARC ____ and ClariT ____ only require "yes" or "no" questions.
BeNYfits expands on these works by adding a highly realistic, multi-turn dialog agent task requiring logical reasoning and domain-specific knowledge.
Similar tasks include MediQ ____, which benchmarks medical diagnosis through dialog, and ClarQ-LLM ____, which focuses on discovering hidden information while playing an adventurer.
In comparison, BeNYfits focuses on logically reasoning legalistic tasks to reach a binary prediction.
% Additionally, the ability to combine overlapping opportunities into multi-label tasks (e.g., determine eligibility for school programs \textbf{and} child care) allows for the creation of increasingly complex tasks.
% These 
% \subsection{Code Generation-based Task-oriented Dialog}

Many works on tool-use have equipped language models with a code interpreter ____ ____, though fewer have specifically studied tool creation, e.g., ____.
Several prior works have established the efficacy of code generation in dialog systems. 
____ propose grounding in code generated based on partner utterances and using symbolic planning to reason over the code. 
____ find code translations an effective intermediate representation for natural language questions.
____ create an LLM agent framework for dynamically creating and composing subtask actions based on code.
To the best of our knowledge, no other code generation-based approaches have been proposed for question generation in dialog. 
% Since most modern language models are trained on both natural language and code, using code as a proxy for natural language allows them to structure information into a compositional knowledge representation. 
% In addition, grounding can be done by executing the code or using the code as reference in the reasoning prompt.