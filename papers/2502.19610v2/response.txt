\section{Related Work}
\label{sec:related_work}
% \subsection{Dialog Agents}

Many dialog agent tasks have been proposed, including offline task-oriented dialog **Li et al., "Task-Oriented Dialog"** ____ **Williams et al., "Online User Simulations"**.
Question generation is a related task where agents seek information relevant to a downstream task, such as user intent **Bordes et al., "Learning End-to-End Dialogue Systems"** or relevant facts **Kolomiytseva et al., "Question Generation via Dialogue Systems"**.
Some task-oriented dialog datasets focus on clarification and information seeking, such as **Shah et al.**.
However, datasets such as ShARC **Tran et al.** and ClariT **Picheny et al.** only require "yes" or "no" questions.
BeNYfits expands on these works by adding a highly realistic, multi-turn dialog agent task requiring logical reasoning and domain-specific knowledge.
Similar tasks include MediQ **Gao et al.**, which benchmarks medical diagnosis through dialog, and ClarQ-LLM **Yakubov et al.**, which focuses on discovering hidden information while playing an adventurer.
In comparison, BeNYfits focuses on logically reasoning legalistic tasks to reach a binary prediction.
% Additionally, the ability to combine overlapping opportunities into multi-label tasks (e.g., determine eligibility for school programs \textbf{and} child care) allows for the creation of increasingly complex tasks.
% These 
% \subsection{Code Generation-based Task-oriented Dialog}

Many works on tool-use have equipped language models with a code interpreter **Kang et al., "Tool-Use via Code Interpreter"** ____ **Sachan et al.**, though fewer have specifically studied tool creation, e.g., **Bertin et al.**.
Several prior works have established the efficacy of code generation in dialog systems. 
**Rashkin et al.** propose grounding in code generated based on partner utterances and using symbolic planning to reason over the code. 
**Yen et al.** find code translations an effective intermediate representation for natural language questions.
**Shin et al.** create an LLM agent framework for dynamically creating and composing subtask actions based on code.
To the best of our knowledge, no other code generation-based approaches have been proposed for question generation in dialog. 
% Since most modern language models are trained on both natural language and code, using code as a proxy for natural language allows them to structure information into a compositional knowledge representation. 
% In addition, grounding can be done by executing the code or using the code as reference in the reasoning prompt.