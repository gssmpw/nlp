\section{Evaluation}
To evaluate the performance of our IllusionCAPTCHA, we have structured our evaluation around four research questions:

\begin{itemize}[noitemsep,leftmargin=*] 
\item \textbf{RQ3: Human Identification of Illusionary Images.} Can the illusionary images generated by IllusionCAPTCHA remain identifiable to human users? 

\item \textbf{RQ4: LLM Deception by Illusionary Content.} Can the illusionary content effectively deceive LLMs into selecting a false answer? 

\item \textbf{RQ5: Inducement Prompts Effectiveness.} Can the CAPTCHA structure we designed compel bots to make targeted choices? 

\item \textbf{RQ6: Human Attempts to Pass CAPTCHA.} How many attempts do human users require to successfully pass our designed CAPTCHA? 
\end{itemize}






\subsection{RQ3: Human Identification of Illusionary Images}

\noindent\textbf{Motivation.} In this section, we examine whether illusionary images can effectively convey information to human users, a critical factor since a CAPTCHA image must clearly communicate its intended message to its target audience.

\noindent\textbf{Method.} To address RQ3, we designed a questionnaire to assess human users' ability to identify illusionary images. The questionnaire comprises two types of images—text-based and image-based illusionary images—each consisting of five samples. All ten samples were generated using the method described in Section~\ref{sec:method}, and to avoid copyright issues, the base images were produced using a diffusion technique. Below, we provide the details of our questionnaire.


\input{Table/RQ1}

% \noindent\textbf 

\begin{itemize}[leftmargin=*]
    \item \textbf{Perception of Illusion (Mandatory Question):} ``Do you notice any illusionary effect in this image?''
    
    \item \textbf{Uncertainty Clarification (Optional Question):} ``If you are uncertain, could you please explain why?''
    
    \item \textbf{Confidence Level (Mandatory Question):} ``If you answered `Yes' or `No' regarding the perception of an illusion, how confident are you in your response? Please rate on a scale from 1 (least confident) to 5 (most confident).''
    
    \item \textbf{Image Description (Mandatory Question):} ``What do you observe in this image?''
    
    \item \textbf{Description Confidence (Mandatory Question):} ``How confident are you in your description of the image? Rate from 1 (least confident) to 5 (most confident).''
\end{itemize}


\noindent\textbf{Result Analysis.} The key results from this survey are summarized in Table~\ref{tex:RQ1}, 10 participants taking part in this questionnaire. In terms of visibility, the data reveals that human users were able to accurately identify 83\% of illusionary text and 88\% of illusionary images on average. This suggests a relatively strong ability to recognize deceptive or distorted content in both formats of 
illusionary content.

Additionally, the confidence metric provides insight into the users' perception of their own performance. The majority of participants reported high levels of confidence in their selections, indicating that they believed they were making correct judgments, even when faced with illusionary or complex content. This confidence may play a crucial role in how users engage with tasks that involve visual and textual interpretation, highlighting the special structure of human vision.

\input{Table/RQ2}
\input{Table/RQ3}
\input{Table/RQ4}

\subsection{RQ4: LLM Deception by Illusionary Content} 

\noindent\textbf{Motivation.} In this section, we investigate whether illusionary content can effectively deceive the visual processing of LLMs, a critical requirement since a CAPTCHA image must successfully mislead AI systems.


\noindent\textbf{Method.} To rigorously test our generated illusionary content, we adopt the same settings as our empirical study in Section~\ref{sec:empirical_study}, employing 30 generated illusionary images. In contrast to our empirical study, this section aims to demonstrate that LLMs are unable to identify illusionary content. Additionally, unlike other studies, we require precise answers—for example, the correct response should be the name of a concrete bridge
rather than simply bridge.

\noindent\textbf{Result Analysis.} Table~\ref{tex:RQ2} presents the experimental results for LLMs in identifying both illusionary images and text. Our findings indicate that, under both Zero-Shot and COT reasoning settings, neither GPT nor Gemini successfully identified the illusionary images, achieving a 0\% success rate. Notably, when using COT, GPT was able to discern the shape of a hidden character within the image but failed to accurately name the character, even when provided with a hint. These results suggest that visual illusions are particularly challenging for current LLMs to identify, underscoring their effectiveness as natural CAPTCHAs.

\subsection{RQ5: Effectiveness of Inducement Prompts} 

\noindent\textbf{Motivation.} In this section, we explore whether our inducement prompts can effectively guide our intended attackers—GPT-4o and Gemini 1.5 pro 2.0—to select the options we designed.


\noindent\textbf{Method.} In this evaluation, we test GPT-4o and Gemini 1.5 Pro 2.0. We employ two prompt settings Zero-Shot and COT, to assess their performance. Additionally, we allow LLMs two attempts to identify CAPTCHAs, leveraging their ability to retain context across interactions. For this experiment, we utilize 30 IllusionaryCaptchas as the target images.

\noindent\textbf{Result Analysis.} From Table~\ref{tex:RQ3}, we can see that in both attempts, the LLMs consistently selected the option we predicted they would choose, suggesting that the models were identifying only the generated content and not focusing on what we intended human users to recognize. Additionally, we observed that the LLMs often selected the longest description of the images, indicating a tendency to overlook the core elements of the visual illusion. This behavior highlights a key limitation in the LLMs' ability to process visual context effectively, as they appear to prioritize the length or complexity of the descriptions rather than engaging with the nuanced visual details. This finding suggests that while LLMs perform well with textual analysis, they may struggle when tasked with interpreting visual content that requires deeper contextual understanding or inference, such as illusionary images.

\subsection{RQ6: Human Attempts to Pass CAPTCHA}

\textbf{Motivation.} One of the primary aims of our CAPTCHA is to facilitate easier identification of images by human users. Therefore, it is crucial to demonstrate that our CAPTCHA is more user-friendly. To achieve this, we need to assess the number of attempts required for human users to successfully pass the CAPTCHA.

\noindent\textbf{Method.} In this evaluation, we designed a questionnaire structure similar to the one used in Section~\ref{sec:empirical_study} consulting 23 participants to investigate how many attempts human users need to pass our IllusionCAPTCHA. 

\noindent\textbf{Result Analysis.} Table~\ref{tex:RQ4} presents the experimental results of our IllusionCAPTCHA for human users. In this survey, we consulted 23 participants, and we found that 86.95\% were able to pass the CAPTCHA on their first attempt, while 8.69\% succeeded on their second attempt. We also collected feedback on the reasons for failure and discovered that the primary reason participants could not pass was that they did not know the name of the character, although they recognized it as a character from television. Therefore, our CAPTCHA is more friendly for human users to identify, compared to current existing CAPTCHAs.

