
\section{Methodology}

As illustrated in Figure~\ref{fig:Illusion-based}, IllusionCAPTCHA generates CAPTCHA challenges through a three-step process. 
First, it blends a base image with a user-defined prompt, such as ``huge forest,'' to create a visual illusion that obscures the original content. With the prompt, the output image will be looked like the things in the prompt, hiding its true content from base image. This results in images that, while recognizable to humans, can confuse AI systems. 
Second, multiple-choice options are generated based on the altered images, forming the CAPTCHA challenge options.
Our empirical study indicates that humans may occasionally make errors similar to those of LLMs, suggesting that relying solely on illusionary images may not be sufficient to distinguish human users from bots. 
Therefore, we incoperate the third step of ``Inducement Prompt'' to induce our LLM-based attackers to choose the intended choice. Moreover, we utilize multimodel question to increase difficulty for attackers but easy for human users to identify. Below we detail the design of IllusionCAPTCHA.
%

\subsection{Illusionary Image Generation}

\begin{figure}[!t]
	\centering
    \includegraphics[width=0.8\linewidth]{Figure/Picture_1.eps}
	\caption{An example of the original and illusionary image}
     % \vspace{-10pt}
	\label{fig:Illusion-example}
\end{figure}
\label{sec:method}
The first objective is to create illusionary images that are easily recognizable by humans but difficult for AI systems to identify. This process involves tackling two primary challenges: (1) maintaining the context of the base image, and (2) add disturbance to the image particularly effective for AI systems to interfere with their capabilities while maintaining recognizability for humans. 

To address the first challenge, we employ an illusion diffusion model~\cite{AP123}, which generates images by blending two different types of content. Built upon ControlNet~\cite{zhang2023adding}, a framework that allows precise control over image generation through conditional inputs, this model ensures that the resulting images remain accessible to human viewers while being challenging for automated systems to interpret.
Figure~\ref{fig:Illusion-example} shows how a normal image is transferred into an illusionary one.
However, not all generated images will effectively balance recognizability for humans while fooling AI vision. To overcome the second challenge, we first generate 50 sample images using different seeds, all within the range of 0 to 5, at a fixed illusion strength level of 1.5â€”an optimal value for human identification in this context. We then calculate the cosine similarity between each generated image and the base image, selecting the one with the lowest similarity, which can be seen as the most diffcult images for bots to identify.

To enhance the perceptibility of the generated images, we develop tailored strategies for two types of illusion-based CAPTCHAs: text-based CAPTCHAs and image-based CAPTCHAs. In the first scenario, the base image contains a clear, readable word embedded within an illusion. To ensure that human users can still recognize the text with minimal effort, we opt for simple, familiar English words such as ``day'' or ``sun.'' 
In the second scenario, the base image features a well-known, easily recognizable character or object, such as an iconic symbol or a famous location (e.g. ``Eiffel Tower''). This ensures that human users can quickly identify the content, even with added illusionary elements. 
These strategies aim to strike a balance between maintaining human usability and introducing complexity that misleads AI systems. 


\subsection{Options Setup} 

Our options have been meticulously crafted to safeguard against LLM-based attacks. In our CAPTCHA, we offer four distinct options. One option represents the correct answer usually the hidden content of our image, while another is the input prompt we utilize in the generation of our image. The remaining two options consist of detailed descriptions of our prompt part without the correct answer, intentionally crafted without referencing any content from our true answer.

Unlike traditional CAPTCHAs that require users to type text or select multiple images to answer a question, our CAPTCHA asks users to choose the correct description of an image. This design simplifies the process by offering a hint, making it easier for users to identify the correct answer without needing to click through multiple images.

Compared to text-based CAPTCHAs, ours is more user-friendly, as it avoids the challenges posed by vague images. Additionally, in contrast to hCAPTCHA and reCAPTCHA, our approach reduces the difficulty of making a selection. Unlike reasoning-based CAPTCHAs that require users to manipulate images, which can lead to frustration, our design eliminates the need for such interactions, further improving user experience.


\subsection{Inducement Prompt}

Building on our empirical study, we discover that both LLMs and human users tend to make similar errors when presented with certain types of CAPTCHAs. Additionally, human users often require a second attempt to pass the CAPTCHA successfully. As a result, relying on a single question to differentiate between AI and human users proves insufficient.
To address this issue, we designed a system that aims to lure potential attackers, such as multimodal LLMs, into selecting predictable, bot-like answers. Our CAPTCHA format uses multiple-choice questions, each offering four answer options.

Our strategy centers on the idea to trick the LLM-based adversary to select the option that describes the illusionary element added, which is the object that LLMs typically fails to capture. Research~\cite{hu2024bliva} has shown that LLMs typically describe images with long, detailed sentences. To exploit this, we include one option that features an intentionally elaborate, detailed description of the illusionary elements in the image (e.g., "a vast forest filled with birds, depicting a beautiful and serene scene").

Additionally, to reduce the difficulty for human users, we embed hints within the questions that guide them toward the correct answer. Therefore, these hints(e.g. Tell us the \textbf{true} and \textbf{detailed} answer of this image) are crafted to trigger hallucinations in LLMs, further increasing the likelihood that bots will select incorrect responses, although they are in the prompt that attacker sets before.







