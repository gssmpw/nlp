\section{Additional details}\label{app:misc}

In this section, we provide an exact formulation for sequence-level divergences
\begin{equation}\label{eq:kl_divergence_lms}
    \KL_{\seq}(\pi,  \pi') \triangleq \E_{x \sim d(\cdot), y \sim p_{\pi}(\cdot | x)}\left[ \sum_{i=1}^{|y|}\KL(\pi(\cdot | x, y_{:i})), \pi'(\cdot | x, y_{:i})) \right]\,.
\end{equation}
In particular, to estimate this KL-divergence, we need to sample prompts $x$ and generate responses using a language model $\pi$. Additionally, we introduce a sequence-based Jensen-Shannon divergence, defined as
\begin{equation}\label{eq:js_divergence_lms}
    \begin{split}
    \JS_{\seq}(\pi, \pi') &\triangleq \E_{x \sim d(\cdot), y \sim p_{\pi}(\cdot | x), y' \sim p_{\pi'}(\cdot | x)}\bigg[ \frac{1}{2}\sum_{i=1}^{|y|}\KL(\pi(\cdot | x, y_{:i}),  m(\cdot | x, y_{:i}))  + \frac{1}{2}\sum_{i=1}^{|y'|}\KL(\pi'(\cdot | x, y'_{:i}), m(\cdot | x, y'_{:i})) \bigg]\,,
    \end{split}
\end{equation}
where $m(\omega | x, y) \triangleq 0.5 \cdot \pi(\omega| x,y) + 0.5 \cdot \pi'(\omega|x,y)$ is a mixture of two language models. To estimate this divergence, we need to use samples from both models $\pi$ and $\pi'$ and compute an average of two token-level KL-divergences. Notice that computation of a true Jensen-Shannon divergence between $p_{\pi}$ and $p_{\pi'}$ is computationally infeasible since the log-probabilities of the mixture $0.5 \cdot p_{\pi}(y|x) + 0.5 \cdot p_{\pi'}(y|x)$ does not satisfy a chain rule in terms of $\pi$ and $\pi'$.