\section{Introduction}\label{sec:intro}

% \textbf{Template for an intro.}
% \begin{enumerate}
%     \item What problem are you addressing? Deployment and infernece cost.
% \item Why is this an important problem? Compute cost, ecological. used a lot. But what happen when you do it at the limit?
% \item Why is this an unsolved problem? does distillation can be hacked? Do we always need to be as close to the teacher as possible?
% \item Describe your proposal. study reward hacking; to this end, what is it and etc.
% \item Describe your contributions. Good framework and new insights about distillation.
% \item What experiments. Experiments on XSum.
% \item Does the long-term impact of this idea excite you? Towards distilled everything. 
% \end{enumerate}

% \textbf{Intro generated by gpt.}
% \begin{itemize}
%     \item Auto-regressive language model achieves impressive results on numerous tasks, but is expensive;
%     \item Alternative: small on-device LLMs, one of the most well-known approaches: knowledge distillation \cite{hinton2015distilling}; Mention usage in pre-train for distilbert \cite{sanh2019distilbert} and in post-training pipeline of Gemma-2 \cite{team2024gemma}. Say we will focus on post-training.
%     \item Importantly, it adds an additional phase compared to a classical approach of post-training \cite{stiennon2020learning,ouyang2022training,bai2022training}.Final pipeline includes both distillation and RLHF;
%     \item Describe what knowledge distillation is, and put a large emphasis on the fact that the teacher model serves as a proxy.
%     \item RLHF is well-studied empirically and faces the problem of reward hacking \cite{amodei2016concrete,pan2022effects,gao2023scaling}. The reason for reward hacking: usage of an imperfect proxy for rewards.
%     \item Reward hacking in RLHF is super important: misleading humans \cite{wen2024language,hendrycks2021unsolved} and so on.
%     \item Underline the similarities between distillation and RLHF and the main one: usage of imperfect proxy.
%     \item Define teacher hacking in distillation \cite{hinton2015distilling} informally as an analogy to reward hacking for distillation. The main questions are: 1) Does it happen, and how can it be detected? 2) How do we reduce its effect when it happens?
%     \item Write down about the controlled setting and answers to these questions as a contribution sub-section.
%     \mb{briefly introduce the controlled setting too, as this is crucial and can also be seen as one of the contributions. we can call it  semi-synthetic data, in the sense that prompts are real but responses are generated.}
%     \mb{insist on us being the first paper on teacher hacking}
% \end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/oracle_pipeline.pdf}
    \caption{The schematic idea of the controlled experimental setup: instead of the use of true expert data for supervised fine-tuning, we employ an additional oracle model to replace it and measure the approximation quality.}
    \label{fig:oracle_pipeline}
\end{figure}


Auto-regressive language models (LMs) have achieved remarkable success across a wide range of natural language processing tasks, such as translation, summarization, and reasoning. These models demonstrate impressive generalization capabilities, but their high computational cost poses a significant challenge, particularly when deploying them on resource-constrained devices. This issue motivates the development of alternatives, such as small on-device language models, which can provide similar functionality at a fraction of the computational cost. One of the most popular approaches to achieve this goal is knowledge distillation \cite{hinton2015distilling}.


Knowledge distillation has been widely adopted at various stages of the machine learning pipeline. % For example, it has been used during pre-training to create smaller models such as DistilBERT \cite{sanh2019distilbert}.% and Baby Llama \cite{timiryasov2023baby}. 
Recently, it was successfully applied to the post-training pipeline of language model training, as demonstrated by Zephyr \cite{tunstall2023zephyr}, Gemma-2 \cite{team2024gemma}, and DeepSeek-V3 \cite{liu2024deepseek}. In this work, we focus on using knowledge distillation in this post-training stage.

In the classical setup, post-training typically involves supervised fine-tuning (SFT) followed by reinforcement learning from human feedback (RLHF) \cite{stiennon2020learning,ouyang2022training,bai2022training}. However, a growing body of research on knowledge distillation of LMs \cite{agarwal2024onpolicy,gu2024minillm,kim2024promptkd} also suggests integrating distillation into this pipeline, typically between SFT and RLHF phases, especially for training small LMs.


%\todoDT{Check \cite{li2024direct}}
%\todoDT{"Biased teachers" \cite{zhang2023not}}

% \todoDT{Documented usages of distillation in post-training pipeline: Gemma-2 \cite{team2024gemma} (both pre-train and post-train), Gemini-1.5 Flash \cite{team2024gemini} (we don't know exactly), Deepseek-V3 \cite{liu2024deepseek}, developments in this area: GKD \cite{agarwal2024onpolicy}}

Knowledge distillation involves training a smaller student model to replicate the behavior of a larger teacher model. However, as highlighted by \cite{menon2021statistical,zhang2023not}, the teacher model does not represent the ground-truth distribution but instead acts as an imperfect proxy for it. This viewpoint raises a question about the limiting behavior of training.

This issue draws parallels to a well-studied phenomenon in RLHF known as reward hacking \cite{amodei2016concrete,pan2022effects,gao2023scaling}. Reward hacking can be seen as a manifestation of Goodhart's law and arises when models over-optimize the reward signal, which is an imperfect proxy for the true task objective. The consequences of reward hacking in RLHF can be significant, leading to models that mislead humans or produce unsafe behaviors \cite{hendrycks2021unsolved,wen2024language}. 

Knowledge distillation shares a fundamental similarity with RLHF: both approaches rely on imperfect proxies. Inspired by the analogy to reward hacking, we define teacher hacking in the context of distillation as the phenomenon where the student model exploits imperfections in the teacher model rather than approximating better the true data distribution. This raises critical questions: \textit{(1) Does teacher hacking occur in practice, and how can it be detected? (2) When it does occur, what strategies can be employed to mitigate its effects?} 


To answer these questions, we propose a controlled experimental setup inspired by similar work in the setting of RLHF \cite{gao2023scaling} to measure the effects of teacher hacking. Specifically, we consider a two-stage post-training pipeline that consists of supervised fine-tuning and knowledge distillation. To measure the approximation error of the solution to the SFT task, we introduce an oracle model that generates the responses for the SFT dataset and serves as a ground truth; see \Cref{fig:oracle_pipeline} for an illustration. In this setup, the distance between student and oracle models serves as a ``golden metric'' representing the ground truth objective. Conversely, the distance between student and teacher models defines a ``proxy metric,'' which can be observed during training outside a controlled study. 
Our setup is semi-synthetic, in the sense that prompts are sampled from real datasets but ground-truth responses are sampled from the oracle model.
%By analyzing the evolution of these metrics during training, we can detect the presence and intensity of teacher hacking under various conditions and address our main questions.
% \todoDT{Write about experiments + modify the contribution section + add words about practical ideas}
% \todoDT{Emphasize that the setup is also a contribution}
% \todoDT{Mention about scaling law}
% \todoDT{The question: what is asymptotical limits of distillation given the limiting behavior}
% \todoDT{Parallel with model collapse}


Overall, we highlight our main contributions:
\begin{itemize}[itemsep=2pt,leftmargin=10pt]
    \item We introduce and formally define the phenomenon of teacher hacking in language model distillation, providing a novel controlled experimental framework to systematically measure its effects.
    \item We demonstrate that teacher hacking is absent when using online data generation methods, such as sampling directly from the teacher model during training or employing on-policy distillation. Furthermore, we show that when a fixed offline dataset is used for distillation, the proxy metric initially follows a polynomial convergence law. However, teacher hacking emerges when the optimization process deviates from this scaling behavior.
    \item We propose practical strategies to mitigate teacher hacking, including increasing prompt dataset diversity or using a larger amount of teacher-generated data during training.
\end{itemize}
