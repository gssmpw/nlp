\section{Introduction}\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/oracle_pipeline.pdf}
    \caption{\textbf{Overview of our controlled experimental setup.} Usually, the teacher model is trained on expert data before being distilled into the student LM. In the controlled setup of this paper, the teacher is  itself distilled from an additional oracle model. This oracle model allows us to measure the quality of the distillation process into the student, and to reveal \enquote{teacher hacking}.}
    \label{fig:oracle_pipeline}
\end{figure}



\textbf{Distillation for post-training LMs.}
Language models (LMs) have achieved remarkable success across a wide range of natural language processing tasks, such as translation, summarization, and reasoning.
Notably, large LMs demonstrate impressive generalization capabilities, but their high computational cost poses a significant challenge, particularly when deployed on resource-constrained devices. Efficiency considerations motivate the training of smaller LMs, that would ideally provide similar performance at a fraction of the computational cost. To this end, the most popular approach is knowledge distillation (KD) \cite{hinton2015distilling}, in which a smaller student LM is trained to imitate the larger teacher LM.
Distillation is increasingly studied \cite{agarwal2024onpolicy,gu2024minillm,kim2024promptkd} and used, notably for the post-training pipelines of LMs (as demonstrated by Zephyr \cite{tunstall2023zephyr}, Gemma-2 \cite{team2024gemma}, and DeepSeek-V3 \cite{liu2024deepseek}), just before the final reinforcement learning from human feedback (RLHF) \cite{stiennon2020learning,ouyang2022training,bai2022training} phase.

\textbf{Teacher as an imperfect proxy.}
However, a key understudied limitation of KD is that the teacher model does not represent the ground-truth distribution but instead acts as an imperfect proxy for it \citep{menon2021statistical,zhang2023not}. This viewpoint draws parallels to a well-studied phenomenon in RLHF known as reward hacking \cite{amodei2016concrete,pan2022effects,gao2023scaling}. Reward hacking is a manifestation of Goodhart's law arising when LMs over-optimize the reward model, trained to represent human preference, thus also an imperfect proxy for the true task objective. The consequences of reward hacking in RLHF can be significant, leading to models misleading humans and producing unsafe behaviors \cite{hendrycks2021unsolved,wen2024language}. 


\textbf{Teacher hacking.}
Inspired by the analogy with reward hacking in RLHF, we define teacher hacking as a possible phenomenon during distillation in which the student LM learns to imitate the teacher, not by better approximating the true data distribution, but by exploiting imperfections in the teacher.
This raises natural questions: \textit{(1) Does teacher hacking occur in practice?} and if so, \textit{(2) when does it appear?}, and \textit{(3) what strategies can mitigate it?}

\textbf{Controlled experimental setup.}
To answer these questions, we propose a controlled experimental setup. Specifically, we introduce an oracle model that represents the ground-truth; see \Cref{fig:oracle_pipeline} for an illustration. In this setup, the distance between the student and oracle LMs serves as a \enquote{golden metric}. Conversely, the distance between student and teacher LMs, optimized during the fine-tuning of the student, defines a \enquote{proxy metric}. 
Our setup is semi-synthetic, in the sense that prompts are sampled from real datasets but responses are sampled from the oracle LM to learn the teacher, and from the teacher LM to learn the student.

We summarize our main contributions as follows.
\begin{itemize}[topsep=0pt,itemsep=3pt,parsep=3pt,leftmargin=15pt]
% \begin{tcolorbox}[colback=colorblue,
%     colframe=black,
%     arc=4pt,
%     boxsep=0.3pt,
% ]%
% \textbf{Contribution 1.}
\item
We introduce and formally define the phenomenon of teacher hacking in LM distillation, providing a novel controlled experimental framework to systematically measure its importance.
% \end{tcolorbox}%
% \begin{tcolorbox}[colback=colorblue,
%     colframe=black,
%     arc=4pt,
%     boxsep=0.3pt,
% ]
% \textbf{Contribution 2.}
\item
We show that teacher hacking occurs when distillation is performed on a fixed offline dataset. Notably, we empirically found that the proxy metric initially follows a polynomial convergence law, and that teacher hacking emerges when the optimization process deviates from this law.
% \end{tcolorbox}%
% \begin{tcolorbox}[colback=colorblue,
%     colframe=black,
%     arc=4pt,
%     boxsep=0.3pt,
% ]
% \textbf{Contribution3.}
\item
We analyze strategies to mitigate teacher hacking.
Notably, we show that teacher hacking is absent when using online data generation methods, sampling directly from the teacher or the student during distillation. We relate this success to increased diversity in the dataset. We then propose cheaper practical alternative strategies to mitigate teacher hacking, such as increasing the diversity across prompts or using a larger amount of offline teacher-generated data.
% \end{tcolorbox}%
\end{itemize}
