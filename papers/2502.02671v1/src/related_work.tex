\section{Related work}\label{sec:related_work}


Goodhart's law states: \enquote{When a measure becomes a target, it ceases to be a good measure} \cite{strathern1997improving}. In particular, it manifests itself as reward hacking in RLHF \cite{amodei2016concrete,gao2023scaling,weng2024rewardhack}. 
% The active line of research attempts to mitigate the effects of reward hacking through better reward modeling or more robust optimization procedures \cite{chen2024odin,rame2024warm,liu2024rrm}. 
A line of works studied reward hacking under controlled experimental setups \cite{gao2023scaling,rafailov2024scaling}. 
Our setup closely resembles that of \citet{gao2023scaling}, where two types of reward models (RMs) are used: a golden reward model, which substitutes the ground-truth reward function, and a proxy reward model, trained on golden RM-preferred generations as ground-truth preferences. Specifically, we employ oracle and teacher models in the same roles as the golden and proxy RMs, respectively: the teacher model is trained on oracle-generated data, while the final student model is trained using the teacher's next-token distribution. 

Given possible negative consequences of reward hacking \cite{hendrycks2021unsolved,wen2024language}, another line of research attempts to mitigate its effects through better reward modeling or more robust training procedures \cite{chen2024odin,rame2024warm,liu2024rrm}.

Knowledge distillation (KD) was initially introduced as a method to compress a large teacher model into a smaller student model without much loss in performance \cite{bucilu«é2006model,hinton2015distilling}; it has then been successfully used to create smaller language models such as DistilBERT \cite{sanh2019distilbert},  Zephyr \cite{tunstall2023zephyr}, Gemma-2 \cite{team2024gemma}, Gemini-1.5 Flash \cite{team2024gemini}, and DeepSeek-V3 \cite{liu2024deepseek}. One of the actively used approaches to language model distillation is sequence-level KD \cite{kim2016sequence}. In our case, this approach corresponds to utilizing offline data sources and forward KL token-level loss (thanks to properties of the KL). Other approaches focus on matching different quantities of the teacher model by the student model, such as hidden states \cite{jiao2019tinybert} or attention scores \cite{wang2020minilm}.

% Scaling laws provide insights into model performance based on computational resources \cite{hestness2019beyond,kaplan2020scaling}. While these laws are well-known for training models, recent studies explore their application to distillation \cite{zhang2023towards,ildiz2024high}, focusing on the relationship between model size, training data, and distillation effectiveness.

Most of the previously-mentioned works implicitly or explicitly assume that the replication of the teacher model represents the final goal of the distillation process, contrary to the approach mentioned by \citet{menon2021statistical}, where the teacher is only an imperfect approximation of the true data distribution. Based on this perspective, \citet{zhang2023not} developed a perturbed loss that can be seen as training from a \enquote{proxy teacher}. However, they do not further investigate the consequences of over-optimizing the objective.
