\section{Conclusion}\label{sec:conclusion}
In this paper, we introduce and examine the phenomenon of teacher hacking in language model distillation by designing a semi-synthetic controlled experimental setup. This allows us to measure its effects, and validate experimentally its presence when using a fixed offline dataset for the distillation procedure. 

Fortunately, as a practical outcome of our study, we were able to identify several strategies to mitigate teacher hacking: (1) utilize online generations during the distillation process, (2) when the generation budget is fixed, prioritize increasing the diversity of the prompt dataset, and (3) if the prompt dataset is fixed and online generations are not feasible, generate multiple offline completions per prompt ahead of time to expand the dataset. We hope that these practical and methodological insights provide valuable guidance in extending the applicability and effectiveness of language model distillation in real-world scenarios.