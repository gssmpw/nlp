\section{Preliminaries}\label{sec:preliminaries}


Let $\cX$ and $\cY$ denote the spaces of all possible prompts and responses, assumed to be sentences in the vocabulary $\Sigma$. For two sets \(A\) and \(B\), \(\simplex(A)\) denotes the space of probability measures over \(A\), and \(\simplex(A|B)\) represents the space of conditional probability measures over \(A\) given \(B\). An auto-regressive language model \(\pi \in \LM_{\Sigma}(\cX)\) is defined as the conditional probability of the next token or the end-of-sequence token given a prompt \(x \in \mathcal{X}\) and a partial generation \(y \in \cY\), expressed as \(\pi(\omega | x, y)\), where $\omega \in \Sigma$. In practice, the probability $\pi(\omega | x,y)$ is defined using a softmax with temperature $\pi(\omega | x,y) \propto \exp( \frac{1}{\tau}z(\omega| x, y))$, where $z(\cdot | x,y)$ are the logits output by the neural network and $\tau$ is a temperature parameter.

For any language model \(\pi\), the induced distribution over responses is given by
$
    p_{\pi}(y|x) \triangleq \prod_{i=1}^{|y|} \pi(y_i | x, y_{:i})\,,
$
where $|y|$ denotes the length of the response \(y\), i.e., the number of tokens (non-empty characters) in \(y\), and $y_{:i} \triangleq (y_1, y_2, \ldots, y_{i-1})$.


\paragraph{Distances between language model distributions.} Let $d \in \simplex(\cX)$ be a distribution over prompts induced by the particular task dataset. To measure the convergence of one distribution, induced by a language model $\pi$, to a distribution induced by a language model $\pi'$, we use the (expected) forward and reverse KL between the corresponding conditional measures $p_{\pi}$ and $p_{\pi'}$, defined as $\KL_{\seq}(\pi,  \pi') \triangleq \E_{x \sim d(\cdot)}[\KL(p_{\pi}(\cdot|x) , p_{\pi'}(\cdot|x))]$. By the properties of the KL divergence, the sequence-level divergence can be estimated very efficiently using token-level KL divergences. Additionally, we use a sequence-level Jensen-Shannon divergence $\JS_{\seq}(\pi, \pi')$. We provide more details in~\Cref{app:misc}.


% where $x \sim d(\cdot)$ and $y \sim p_{\pi}(\cdot | x)$. Additionally, we introduce a sequence-based Jensen-Shannon-style divergence, defined as
% \begin{equation}\label{eq:js_divergence_lms}
%     \begin{split}
%     \JS_{\seq}(\pi, \pi') &\triangleq \E\bigg[ \frac{1}2{}\sum_{i=1}^{|y|}\KL(\pi(\cdot | x, y_{:i}),  m(\cdot | x, y_{:i})) \\
%     &\quad + \frac{1}{2}\sum_{i=1}^{|y'|}\KL(\pi'(\cdot | x, y'_{:i}), m(\cdot | x, y'_{:i})) \bigg]\,,
%     \end{split}
% \end{equation}
% where $x \sim d(\cdot), y \sim p_{\pi}(\cdot | x)$, $y' \sim p_{\pi'}(\cdot | x)$ are generations from the corresponding models, and $m(\omega | x, y) \triangleq 0.5 \cdot \pi(\omega| x,y) + 0.5 \cdot \pi'(\omega|x,y)$ is a mixture of two language models. Notice that computation of a true Jensen-Shannon divergence between $p_{\pi}$ and $p_{\pi'}$ is computationally infeasible since the log-probabilities of the mixture $0.5 \cdot p_{\pi}(y|x) + 0.5 \cdot p_{\pi'}(y|x)$ does not satisfy a chain rule in terms of $\pi$ and $\pi'$.

\paragraph{Supervised fine-tuning.} Let $\rho \in \simplex(\cY | \cX)$ be a conditional response distribution that encodes the ground-truth response distribution. Solving downstream tasks such as summarization, translation, reasoning, or instruction following is fundamentally equivalent to approximating $\rho$. Therefore, the ultimate goal of any post-training pipeline is to approximate $\rho$ in order to address these tasks effectively. 

One of the common approaches to this problem is supervised fine-tuning (SFT). Let us assume that we have a dataset of pairs $(x,y)$ for $x \sim d(\cdot)$ and $y \sim \rho(\cdot | x)$. Then, to find a language model $\pi$ such that its conditional distribution $p_{\pi}$ approximates $\rho$, it is common to use a simple log-loss
\begin{equation}\label{eq:sft_loss}
    \cL_{\mathrm{SFT}}(\pi) \triangleq \E_{x \sim d(\cdot), y \sim \rho(\cdot | x)}\left[ - \log p_{\pi}(y|x) \right]\,.
\end{equation}
This loss is equal, up to a constant factor, to an expected sequence-level forward KL divergence between $\rho$ and $p_{\pi}$: $\E_{x \sim d(\cdot)}[\KL(\rho, p_{\pi})]$.

% \paragraph{Language model distillation} 
% Define \(d \in \simplex(\mathcal{X})\) as a distribution over prompts induced by the dataset. We denote the student and teacher language models as \(\pi_{\student}, \pi_{\teacher} \in \LM_\Sigma(\mathcal{X})\), with their corresponding conditional response distributions represented by \(p_{\student}\) and \(p_{\teacher}\), respectively.

\paragraph{Language model distillation.}

We suppose that we have access to a teacher language model,
denoted $\pi_{\teacher} \in \LM_\Sigma(\cX)$, such that it approximates the ground-truth distribution $\rho$, that is, $p_{\teacher} = p_{\pi_{\teacher}} \approx \rho$. The goal of language model distillation is to train a student language model, denoted $\pi_{\student}$, so as to approximate the teacher model, that is, $p_{\student} = p_{\pi_{\student}} \approx p_{\teacher}$. We emphasize that the teacher-induced distribution $p_{\teacher}$ is not equal to a ground-truth $\rho$ but only approximates it. 

We usually distinguish between \textit{hard} and \textit{soft} distillation. In hard distillation, the student is only trained from the teacher's predicted next tokens, i.e., the student only sees the most likely tokens according to the teacher. In soft distillation, the student is trained from the teacher's predicted next token distributions, giving much more information to the student. In this work, we focus on soft distillation, and the loss function for this procedure takes the form
%The general form of the knowledge distillation loss is defined as
\begin{equation}\label{eq:distillation_loss}
    \cL_{\mathrm{KD}}(\pi_{\student}) \triangleq \E\left[ \frac{1}{|y|} \sum_{i=1}^{|y|} \ell_{\mathrm{token}}(\pi_{\student}(\cdot | x, y_{:i}), \pi_{\teacher}(\cdot | x, y_{:i})) \right]\,,
\end{equation}
where $x \sim d(\cdot)$ and $y \sim \nu(\cdot| x)$, \(\nu \in \simplex(\mathcal{Y} | \mathcal{X})\) is a \textit{data source}, and \(\ell_{\mathrm{token}}\) is a token-level loss function between two distributions over the vocabulary.

The token-level loss and the data source should satisfy two assumptions: (i) it is non-negative and satisfies the property $\ell_{\mathrm{token}}(p,q) = 0$ if and only if $p = q$ for any two distributions $p,q \in \simplex(\Sigma)$, and (ii) the support of $\nu(\cdot | x)$ includes the support of teacher-induced conditions measure $p_{\teacher}(\cdot | x)$ for almost all $x$, i.e. $d(x) > 0$.  Given these two assumptions, it is easy to show that a language model $\pi$ minimizes the loss $\cL_{\mathrm{KD}}(\pi_{\student}) = 0$ if and only if $\pi_{\student} = \pi_{\teacher}$. 

In particular, considering $\nu(\cdot|x)$ induced by an offline dataset generated by a teacher and using $\ell_{\mathrm{token}}(p,q) = \KL(p,q)$, we achieve the same expected loss as in the case of SFT. This approach corresponds to the works of \cite{hinton2015distilling, sanh2019distilbert}. However, we could consider different token-level losses, such as reverse KL divergence \cite{gu2024minillm}, generalized Jensen-Shannon divergence \cite{agarwal2024onpolicy}, or skewed KL divergence \cite{ko2024distillm}. The data source can be induced by sampling online from the teacher model \cite{kim2016sequence}, sampling online from the student model \cite{gu2024minillm}, or combining offline and online data \cite{lin2020autoregressive, agarwal2024onpolicy, ko2024distillm}. 


\paragraph{Offline vs. online data sources.}

In this paper, we distinguish between two different types of data sources: offline and online.
Offline data sources are based on a fixed dataset, denoted as $\cD_{\mathrm{offline}} = \{(x_i, y_i)\}_{i=1}^M$, where $x_i$ is sampled from $\cD_{\mathrm{prompt}}$, and $y_i \sim p_{\teacher}(\cdot | x_i)$ are responses generated by the teacher model. Importantly, the dataset $\cD_{\mathrm{offline}}$ does not need to have a one-to-one correspondence between prompts and responses. Instead, it may include multiple responses per prompt, and the number of prompts is not necessarily equal to the total number of available prompts $N$. Each training batch is sampled directly from this fixed dataset.

Online data sources, by contrast, involve generating responses dynamically through an online sampling procedure. For each training batch of prompts $\{x_j\}_{j=1}^B$ of size $B$, sampled from $\cD_{\mathrm{prompt}}$, a corresponding model (either the teacher or the student) generates a batch of responses $\{y_j\}_{j=1}^B$. While sampling from the student model is often referred to as on-policy generation in the literature \cite{agarwal2024onpolicy}, we use the term online student to emphasize its parallel to the online teacher data source. 

The distinction between offline and online data is particularly evident during subsequent epochs: for offline data sources, responses remain fixed across epochs, while for online data sources, new responses are generated for each epoch, independently drawn from the same distribution.



\paragraph{Teacher hacking.} 
As we already mentioned, the main goal of the post-training pipeline is to approximate the ground-truth distribution $\rho$ by the student model. This goal could be achieved by SFT given access to sufficiently many samples from $\rho$. However, recent works have shown that distilling from a teacher model can actually work better since the whole next-token distribution contains much richer information than only sampled tokens. An understudied problem is that the teacher model is only an imperfect proxy for $\rho$. This may lead to a problematic situation where the student LM learns to imitate the teacher, not by better approximating the true data distribution but by exploiting imperfections in the teacher.
We call this phenomenon teacher hacking, and give a more formal definition below.
% Let $\rho \in \simplex(\cY | \cX)$ represent the ground-truth conditional distribution of responses given a prompt, which reflects the behavior of a human expert. In particular, solving downstream tasks such as summarization, translation, reasoning, or instruction following is fundamentally equivalent to approximating $\rho$. As a corollary of this equivalence, the ultimate goal of any post-training pipeline, is to approximate $\rho$ in order to address these tasks effectively. In the case of language model distillation, however, the process depends on a teacher model that is only an imperfect proxy for $\rho$.
\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
\begin{definition}[Teacher hacking]\label{def:teacher_hacking}
    Let $\{p_{\student}^{(k)}\}_{k=1}^\infty$ be a sequence of conditional response distributions induced during the training of a student model, $p_{\teacher}$ the distribution induced by the teacher model, and $\rho$ the target human expert conditional distribution. We say that $\{p_{\student}^{(k)}\}_{k=1}^\infty$ exhibits the teacher hacking phenomenon with respect to a distance measure $\dist \colon \simplex(\cY | \cX) \times \simplex(\cY | \cX) \to \mathbb{R}_{+}$ if, as $k \to +\infty$, $\dist(p_{\student}^{(k)}, p_{\teacher})$ decreases while $\dist(p_{\student}^{(k)}, \rho)$ increases.
\end{definition}
\end{tcolorbox}

A simple example where this would occur is when the student model is initially closer to the target distribution $\rho$ than the teacher model. However, in more realistic scenarios that are closer to real-world applications, the teacher model is larger and provides a better approximation of $\rho$ than the student model.

\paragraph{Overfitting vs. teacher hacking.} 

We would like to clarify the difference between two related phenomena: classical overfitting and the newly-introduced teacher hacking. In the case of overfitting, the model continues to minimize the loss on the training set but fails to do so on the held-out validation set, which it never observes. In contrast, teacher hacking occurs when the model successfully achieves its objective from the teacher's perspective, even on the validation dataset, but fails to improve in terms of approximating the ground-truth behavior. The ultimate outcome of both phenomena is similar: the model fails to generalize to the ground truth, but the reasons differ.