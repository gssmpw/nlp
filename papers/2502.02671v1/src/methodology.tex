\section{Methodology}\label{sec:methodology}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/training_pipeline.pdf}
    \caption{\textbf{Overview of the training pipeline.} Two stages: (1) prompts $x$ from a task-specific real dataset are used by the oracle model to generate the oracle pairs $(x,y)$, and afterwards, this dataset is used to get initial SFT checkpoints for both teacher and student model; (2) prompts from the same distribution are used to perform knowledge distillation, where the teacher model serves as a proxy to train the student model.}
    \label{fig:training_pipeline}
\end{figure}



To analyze the effect of teacher hacking, we require a method to estimate the distance between the student model and the ground-truth distribution. For this purpose, we introduce the \textit{oracle model}, denoted as $\mu \in \LM_\Sigma(\cX)$, which is assumed to induce the target distribution $\rho$, i.e., $p_{\mu} = \rho$.

\paragraph{Golden and proxy metrics.} As outlined in \Cref{def:teacher_hacking}, evaluating the teacher hacking phenomenon requires computing two sets of metrics. 

Golden metrics are computed using the oracle model and reflect the performance with respect to the true objective. Specifically, we use three types of divergences: the forward KL divergence $\KL_{\seq}(\mu, \pi_{\student})$, the reverse KL divergence $\KL_{\seq}(\pi_{\student}, \mu)$, and a Jensen-Shannon-like distance $\JS_{\seq}(\pi_{\student}, \mu)$, which is closely related to the Jensen-Shannon divergence between the conditional distributions over the response space. These metrics are estimated using a held-out validation set of prompts, with sampling performed from the respective models.

Proxy metrics, in contrast, do not rely on the oracle model and instead measure the alignment between the student and teacher models. These metrics use the same types of distances: the forward KL divergence $\KL_{\seq}(\pi_{\teacher}, \pi_{\student})$, the reverse KL divergence $\KL_{\seq}(\pi_{\student}, \pi_{\teacher})$, and the Jensen-Shannon divergence $\JS_{\seq}(\pi_{\student}, \pi_{\teacher})$. Similar to the golden metrics, proxy metrics are estimated using the validation set of prompts and sampling from the models involved.



\paragraph{Training.} The training procedure for our experiments consists of two stages. 


In the first stage, supervised fine-tuning is performed on both the teacher and student models using a small oracle-generated dataset $\cD_{\mathrm{oracle}} = \{(x_i, y_i)\}_{i=1}^{N_{\mathrm{oracle}}}$. The prompts $x_i$ are sampled from the task distribution $d(\cdot)$, and the responses $y_i \sim \rho(\cdot | x_i)$ are generated using the oracle model. 
Our setup is semi-synthetic, in the sense that prompts are sampled from real datasets but responses (seen as labels) are sampled from the oracle LM to learn the teacher, and from the teacher LM to learn the student.
This stage is the only place where direct information from the oracle model is propagated to the teacher and student models. The fine-tuning process optimizes a usual SFT loss \eqref{eq:sft_loss} that in expectation equals to a sequence-level distance $\KL_{\seq}(\mu, \pi)$. The best checkpoint is selected based on the estimate of this quantity over the validation set.

In the second stage, distillation is conducted from the teacher to the student model by optimizing the soft-distillation loss \eqref{eq:distillation_loss}. The distillation process uses a training dataset of unlabeled prompts $\cD_{\mathrm{prompt}} = \{x_i\}_{i=1}^N$, where $N \gg N_{\mathrm{oracle}}$ and different data sources that define a distribution of $y_i \sim \nu(\cdot | x)$ in the loss. The final pipeline is summarized in \Cref{fig:training_pipeline}.

\paragraph{Evaluation.}
To investigate the teacher hacking phenomenon, we analyze two key types of curves: (1) the dependence of the training loss, proxy metrics, and golden metrics on the number of epochs completed, and (2) the proxy-golden curve, which illustrates the relationship between the golden metric (only accessible in our controlled experimental setup) and the proxy metric. Both proxy and golden metrics are computed using a held-out validation set of prompts. The final pipeline is summarized in \Cref{fig:evaluation_pipeline}.
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/evaluation_pipeline.pdf}
    \caption{\textbf{Overview of the evaluation pipeline.} We use the validation prompt dataset to measure the golden metric (the distance between the oracle and the student models) and the proxy metric (the distance between the teacher and the student models).}
    \label{fig:evaluation_pipeline}
\end{figure}

The epoch-dependence plots provide insights into scaling law phenomena \cite{kaplan2020scaling} and help to understand overall training dynamics. Proxy-golden curves are crucial for visually assessing the presence of teacher hacking: a U-shaped curve serves as a clear indicator. Indeed, we expect the proxy metric to be reduced during training, and if the golden metric first decreases and then increases, it directly shows teacher hacking. These proxy-golden plots can be compared to plots from \citet{gao2023scaling}, with one essential difference: our approach measures optimization progress as the distance to the teacher model rather than the distance from an initial reference policy.


