\section{Experimental results}\label{sec:results}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/plots/xsum_base_student_large_teacher_forward_kl_exp0.pdf}
%     \caption{\textbf{Offline vs. online data sources.} The epoch-dependence plots, XSUM dataset, forward KL token-level loss and corresponding golden and proxy metrics, T5-large serves as the teacher, and T5-base acts as the student. For the blue curve corresponding to using a fixed offline dataset, the proxy metric either decreases or stagnates, while the golden metric consistently increases, indicating teacher hacking.
%     }
%     \label{fig:exp_large_to_base_offline_vs_online_fwd_kl}
% \end{figure*}

% In this section, we present our experimental setup in more detail and present our empirical findings.

\paragraph{Oracle, teacher, and student models.} Our experiments use a family of encoder-decoder language models based on T5 \cite{raffel2020exploring,roberts2022t5x}. The oracle model is the Flan-T5-XL \cite{chung2024scaling}, a 3B-parameter model fine-tuned on the Flan dataset \cite{wei2021finetuned,longpre2023flan} for instruction-based tasks. For the teacher and student models, we use pretrained checkpoints of T5-1.1 in three configurations: small (77M parameters), base (250M parameters), and large (800M parameters). We always use temperature sampling with a temperature parameter $\tau = 1$ for generations from any model.

%\paragraph{Datasets} We use three datasets for training and evaluation: the summarization dataset XSUM \cite{narayan2018dont}, the translation dataset WMT-14 en-de \cite{bojar2014findings}, and an instruction-following dataset called Natural Instructions \cite{naturalinstructions,supernaturalinstructions}. According to our pipeline, we use only the contexts from this dataset with an additional task-dependent instruction.
\paragraph{Datasets.} Our experiments use three datasets for training and evaluation: the XSum summarization dataset \cite{narayan2018dont}, the WMT-14 en-de translation dataset \cite{bojar2014findings}, and the instruction-following dataset Natural Instructions \cite{naturalinstructions,supernaturalinstructions}. In alignment with our experimental setup, we use only the prompt data from these datasets, supplemented with task-specific instructions as needed for each task.

For the first stage of the training pipeline, where the oracle dataset is build and used for SFT, we use $N_{\mathrm{oracle}} = 25\,000$, $50\,000$, and $100\,000$ prompts from the XSum, WMT-14 en-de, and Natural Instructions datasets, respectively. For the second stage, which involves the knowledge distillation procedure, we use $N = 200\,000$, $450\,000$, and $500\,000$ prompts from these datasets. A single epoch is defined as one complete pass through all $N$ examples, corresponding to $\lceil N / B \rceil$ training steps, where $B$ denotes the batch size. For XSum and WMT-14 en-de, we use batch size $B=32$; for Natural Instructions, we use batch size $B=64$.


\subsection{Does teacher hacking appear?}\label{sec:online_vs_offline}



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/plots/proxy_golden/xsum_base_student_large_forward_kl_exp5.pdf}

    \caption{\textbf{Proxy-Golden plot (offline data source).}
    %% We already explained this in Figure 2 and this should be clear enough from the plot's title.
    % forward KL token-level loss and corresponding golden and proxy metrics. T5-large is used as a teacher, T5-base is used as a student. 
    We distill a T5-large teacher into a T5-base student on the XSUM dataset. The token-level training loss is the forward KL, the proxy metric is the distance to the teacher distribution and the golden metric is the distance to the ground-truth (oracle) distribution (available thanks to our semi-synthetic controlled experimental setup).
    In this plot, the $x$-axis (proxy metric) indicates optimization progress, and the $y$-axis shows the ground-truth performance (golden metric): lower is better. Teacher hacking occurs in the case of offline data source: the orange curve has a U-type shape, indicating that during optimization, the orange metric starts increasing, whereas the proxy metric continues to decrease.
    }
    \label{fig:proxy_golden_exp_large_to_base_offline_vs_online_fwd_kl}
\end{figure}




\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/plots/xsum_base_student_large_teacher_forward_kl_exp0.pdf}
    \caption{\textbf{Impact of using offline vs. online data sources.} 
     When using a fixed offline dataset, though the proxy metric continues to decrease, this is not visible in the golden metric, which continues to increase, a phenomenon we call teacher hacking. However, when using online response sampling, both from the teacher model or from the student model, this phenomenon does not occur.
    }
    \label{fig:main_exp_xsum_fwd_kl_base_large}
\end{figure*}

We begin by investigating whether teacher hacking appears. %This analysis compares the training dynamics under different data sources.

\paragraph{Setup.}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/plots/xsum_base_student_large_teacher_forward_kl_exp1.pdf}
    \caption{\textbf{Impact of diversity of offline data sources.} 
    %% Not needed, this was already explained in Figure 2.
    % The epoch-dependence plots, XSum dataset, forward KL token-level loss and corresponding golden and proxy metrics. T5-base is used as a teacher, and T5-small is used as a student. 
    We regulate the diversity of the dataset by decreasing the number of prompts in $2/5$ times and providing $2/5$-times more generations for each existing prompt, while preserving the size of the dataset.
    Whereas the dynamics of the train loss and proxy metric are almost the same, the effect of teacher hacking becomes more evident with a less diverse dataset.    
    }
    \label{fig:exp_large_to_base_diversity_fwd_kl}

\end{figure*}


For the first experiment, we use only \textit{offline data sources}: responses are pre-generated as $y_i \sim p_{\teacher}(x_i)$ for all $x_i \in \cD_{\mathrm{prompt}}$, and the dataset remains fixed throughout training. The learning rate for optimization is selected via a grid search over $\{ 10^{-4}, 3 \times 10^{-4}, 10^{-3} \}$.% to achieve the best proxy metric for the final checkpoint.

The distillation procedure starts from the SFT checkpoints of the teacher and student models. Training is carried out over 50 epochs to analyze long-term convergence behavior. 


\paragraph{Results.}
The results of distilling the T5-large teacher model into the T5-base student on the XSum dataset, using forward KL loss, along with the corresponding golden and proxy metrics, are shown in \Cref{fig:proxy_golden_exp_large_to_base_offline_vs_online_fwd_kl}.


% To further validate the presence of teacher hacking, we present the proxy-golden plot for offline and online data sources in . 

In this plot, the $x$-axis represents the optimization progress in terms of the distance to the teacher model (from left to right), while the $y$-axis shows the golden metric. The scatter plot shows the exact values of proxy and golden metrics, where the color demonstrates at which epoch this measurement was performed. The curve itself shows a relationship between smoothed values of the proxy and golden metric, where smoothing is performed by Gaussian smoothing.

For offline data source, the plot exhibits a U-shaped curve. This behavior indicates teacher hacking: as optimization progresses, the ground-truth performance (golden metric) initially improves but eventually deteriorates. Overall, the conclusion of this experiments is following.

\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
\textbf{Observation 1.} Teacher hacking exists and emerges after extended training on a fixed offline dataset.% In contrast, employing online data generation or limiting training to a few epochs effectively prevents this issue.
\end{tcolorbox}







\subsection{When does teacher hacking appear?}
In this subsection, we investigate the conditions under which teacher hacking occurs.



\paragraph{Setup.}

For this experiment, we evaluate three distinct data sources: (1) \textit{Offline data source};
(2) \textit{Online teacher data source}: for each batch of prompts sampled from $\cD_{\mathrm{prompt}}$, a new response $y_i \sim p_{\teacher}(x_i)$ is dynamically generated by the teacher model;
(3) \textit{Online student data source}: responses are generated on-the-fly as $y_i \sim p_{\student}(x_i)$ using the current student model.

As in the previous experiment, we use the forward KL divergence as a token-level loss. We refer to \Cref{app:add_plots} for different token-level loss functions, such as reverse KL divergence and Jensen-Shannon divergence.

We analyze the dynamics of proxy and golden metrics for each data source. For the proxy and golden metrics, we apply Gaussian smoothing to smooth the noisy behavior of the curves. We would also like to emphasize that the difference in scaling between the training loss and proxy/golden metrics is due to averaging over sentence length, which is used in training loss but not in proxy/golden metrics. The training loss indicates the token-level training convergence, whereas the proxy/golden metrics show the sentence-level validation convergence.   


\paragraph{Results.} 

The comparison results between offline and online data sources are shown in \Cref{fig:main_exp_xsum_fwd_kl_base_large}. Additional comparisons for other combinations of datasets, student/teacher model sizes, and loss functions are provided in \Cref{app:add_plots}.


We analyze the epoch dependence of the training loss, proxy metric, and golden metric. We present the dependencies on a log-log scale to track possible polynomial convergence laws: the polynomial dependence of the metric on the training time. Overall, we make the following observations.


\textbf{(i)} In the offline data scenario, we validate the presence of teacher hacking: the proxy metric decreases while the golden metric increases after a certain point. This phenomenon does not occur with online data sources.

\textbf{(ii)} We notice that the behavior of all curves for online and offline data sources is different. Overall, the training loss for the offline data sources decreases faster since the training loss is optimized multiple times over the same data, but the performance on the proxy/golden metrics is worse overall.

\textbf{(iii)} The proxy metric for online data sources follows a linear trend on the log-log scale, indicating a polynomial convergence law. In contrast, teacher hacking in offline data coincides with deviations in the proxy metric compared to online data. It gives a mechanism to \textit{detect} teacher hacking using only the proxy metric, that is measurable even in real scenarios (not only in our controlled experimental setup).


These results highlight that teacher hacking can harm ground-truth performance, particularly when training involves multiple epochs on the same dataset. However, we would like to emphasize that this issue is not present when training is limited to a small number of epochs (e.g., 1–3), as the golden metric remains stable in these cases. We summarize the conclusions of these first experiments as follows.



\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
\textbf{Observation 2.} Employing online data generation or limiting training to a few epochs effectively prevents teacher hacking.
\end{tcolorbox}


\subsection{How to mitigate teacher hacking?}\label{sec:diversity}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/plots/xsum_base_student_large_teacher_forward_kl_exp2.pdf}
    \caption{\textbf{Impact of generation budget for offline data sources.} 
    %% Not needed, already explained in Figure 2. 
    % The epoch-dependence plots, XSum dataset, forward KL token-level loss and corresponding golden and proxy metrics. T5-large is used as a teacher, T5-base is used as a student. 
    As the number of generations per prompt increases, both proxy and golden metrics improve, suggesting that the effect of teacher hacking is decreasing.
    }
    \label{fig:exp_large_to_base_budget_fwd_kl}
\end{figure*}


In the next experiment, we evaluate different methods for modifying the diversity and amount of offline data, in order to investigate how the properties of the offline data affect the teacher hacking phenomenon.

\paragraph{Setup.}

For this experiment, we evaluate different approaches to constructing an offline data source. We define an \textit{ordinary} offline data source as one that uses all available prompts and a single generation for each prompt: $\cD_{\mathrm{offline}} = \{ (x_i, y_i) \mid y_i \sim p_{\teacher}(\cdot \mid x_i) \}_{i=1}^N$, where $\cD_{\mathrm{prompt}} = \{ x_i \}_{i=1}^N$.

Our first objective is to study how the diversity of the offline dataset impacts the teacher hacking phenomenon under a fixed dataset generation budget. Let $k \in \mathbb{N}$ be a natural number. To construct a dataset with reduced diversity, we sub-sample $\lceil N / k \rceil$ prompts from $\cD_{\mathrm{prompt}}$ and generate $k$ responses for each sampled prompt using the teacher model. The resulting dataset maintains the same generation budget of $N$ total responses but exhibits reduced diversity because the $k$ generations for the same prompt $x$ are closer to each other compared to $k$ generations for $k$ different prompts. In our experiments, we apply this technique for $k = 2$ and $k = 5$.

Second, we investigate how increasing the generation budget influences teacher hacking. For a fixed integer $m \in \mathbb{N}$, we generate $m$ responses for each prompt in $\cD_{\mathrm{prompt}}$, resulting in a dataset of size $m \times N$. Despite the larger dataset size, we define epochs as passing through $N$ data points to ensure comparability across experiments. This setup enables us to interpolate between using an offline data source and an online teacher data source. We use values $m=2$ and $m=3$ for our experiments. The rest of the experimental setup follows the description in \Cref{sec:online_vs_offline}.


\paragraph{Diversity of offline data sources.}

We begin by examining the impact of dataset diversity on training dynamics and the previously observed teacher hacking phenomenon. The results are shown in \Cref{fig:exp_large_to_base_diversity_fwd_kl}. Notably, while the training loss and proxy metric dynamics remain nearly the same, the golden metric behaves differently: lower dataset diversity leads to worse golden metric performance, making the teacher hacking effect more evident.

\paragraph{Generation budget.}
Next, we examine the effect of a larger generation budget on training dynamics, as shown in \Cref{fig:exp_large_to_base_budget_fwd_kl}. Increasing the number of generations per prompt uniformly improves both the proxy and golden metrics over time and, oppositely, alters the training loss dynamics. Overall, this suggests an interpolation between the behavior of an ordinary offline and online teacher data sources.

\paragraph{Discussion.}

The results suggest two practical strategies to mitigate the effects of teacher hacking:

\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
\textbf{Observation 3.} \textit{Prioritize Prompt Diversity.} When the generation budget for the distillation dataset is fixed, focusing on increasing the diversity of prompts can help reduce the impact of teacher hacking.
\end{tcolorbox}

\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
\textbf{Observation 4.} \textit{Expand the Dataset with Multiple Completions.} If the prompt dataset is fixed, increasing the generation budget by generating multiple completions per prompt also helps diminish the effects of teacher hacking.
\end{tcolorbox}
