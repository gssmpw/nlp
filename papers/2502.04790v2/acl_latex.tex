% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{soul}

% ours
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{algorithm}  
% \usepackage{algorithmic}  
\usepackage{algorithmicx}
\usepackage{algpseudocode} 
% \usepackage{algorithm2e}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{stackengine}
\usepackage{xcolor,colortbl}
\usepackage{amsthm}     % for proof
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{arydshln}
\usepackage[most]{tcolorbox}
\usepackage{lipsum}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.85}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother

\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}



\title{${\text{S}^2\text{-MAD}}$: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency}



\author{
 \textbf{Yuting Zeng\textsuperscript{1,2}},
 \textbf{Weizhe Huang\textsuperscript{1}},
 \textbf{Lei Jiang\textsuperscript{1}},
 \textbf{Tongxuan Liu\textsuperscript{1,2*}},
 \textbf{Xitai Jin\textsuperscript{3}},
 \\
 \textbf{Chen Tianying Tiana\textsuperscript{4}},
 \textbf{Jing Li\textsuperscript{1*}},
 \textbf{Xiaohua Xu\textsuperscript{1*}}
\\
 \textsuperscript{1}University of Science and Technology of China,
 \textsuperscript{2}JD.com,
 \\
 \textsuperscript{3}Harbin Institute of Technology,
 \textsuperscript{4}National University of Singapore
\\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 % }
  \small{
  {$\{$yuting$\_$zeng, hwz871982879, jianglei0510, tongxuan.ltx$\}$@mail.ustc.edu.cn},
 }
 \\
 \small{
 {tianachen@u.nus.edu}, {$\{$lj, xiaohuaxu$\}$@ustc.edu.cn}, {2023212227@stu.hit.edu.cn}
 }
}

\begin{document}
% 
\maketitle
\begin{abstract}
Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\% in token costs while maintaining performance degradation below 2.0\%.
%We conduct comparative experiments across multiple datasets, demonstrating that our approach not only improves the reasoning capabilities of LLMs in complex scenarios but also effectively reduces the token costs associated with MAD.%
\end{abstract}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors.}

\input{sections/introduction.tex}
\input{sections/preliminary.tex}
\input{sections/method.tex}
\input{sections/experiment.tex}
\input{sections/related-work.tex}
\input{sections/conclusion.tex}
\input{sections/limitation}

\section*{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of
China(NSFC) with Grant No. 62172383 and No. 62231015, Anhui Provincial Key
R\&D Program with Grant No. S202103a05020098, Research Launch Project of University of Science and Technology of China(USTC) with Grant No. KY0110000049.

\bibliography{custom}
\input{sections/appendix}
\end{document}
