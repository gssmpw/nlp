\section{Experiments}
\label{sec:experiments}

\begin{table*}[t]
\centering
\setlength\tabcolsep{1.5 pt}
\resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccccc} 
    \toprule
    \multirow{2}{*}{Methods} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{MATH} & \multicolumn{2}{c}{MMLU} & \multicolumn{2}{c}{GPQA} & \multicolumn{2}{c}{Arithmetic}  \\ 
    \cmidrule{2-11}
    & ACC(\%)$\uparrow$  & Tokens($k$)$\downarrow$   & ACC(\%)$\uparrow$   & Tokens($k$)$\downarrow$   & ACC(\%)$\uparrow$   & Tokens($k$)$\downarrow$       & ACC(\%)$\uparrow$    & Tokens($k$)$\downarrow$      & ACC(\%)$\uparrow$   & Tokens($k$)$\downarrow$    \\ 
    \midrule
    % \rowcolor{lightgray}
    \rowcolor{Gray}
    \multicolumn{11}{c}{\texttt{GPT-3.5-turbo-0125}} 
    \\
    \midrule
    CoT                      & 78.8\scriptsize{$\pm 0.04$}  & 0.25\scriptsize{$\pm 0.03$}             & 35.2\scriptsize{$\pm 0.02$} & 0.37\scriptsize{$\pm 0.01$}         & 72.9\scriptsize{$\pm 0.02$} & 0.24\scriptsize{$\pm 0.00$}                & 31.2\scriptsize{$\pm 0.03$}  & 2.02\scriptsize{$\pm 0.02$}         & 82.2\scriptsize{$\pm 0.04$} & 0.16\scriptsize{$\pm 0.02$}           \\
    CoT-SC(40)               & \underline{\textbf{85.6}}\scriptsize{$\pm 0.01$} & 10.0\scriptsize{$\pm 0.02$}          & \underline{\textbf{48.2}}\scriptsize{$\pm 0.01$} & 14.6\scriptsize{$\pm 0.15$}        & \underline{\textbf{78.4}}\scriptsize{$\pm 0.01$}  & 9.64\scriptsize{$\pm 0.03$}                 & 32.0\scriptsize{$\pm 0.01$}  & 80.5\scriptsize{$\pm 0.27$}         & 95.0\scriptsize{$\pm 0.01$} & 6.25\scriptsize{$\pm 0.13$}           \\
    MAD(5,4)                 & 83.6\scriptsize{$\pm 0.01$} & 20.4\scriptsize{$\pm 0.12$}          & 40.5\scriptsize{$\pm 0.00$}  & 23.4\scriptsize{$\pm 0.24$}        & 74.1\scriptsize{$\pm 0.03$} & 26.9\scriptsize{$\pm 0.11$}                & 41.4\scriptsize{$\pm 0.03$} & 64.3\scriptsize{$\pm 3.06$}         & 96.2\scriptsize{$\pm 0.02$}  & 20.3\scriptsize{$\pm 0.32$}           \\ 
    S-MAD(5,4)                 & \underline{85.6}\scriptsize{$\pm 0.02$} & 17.9\scriptsize{$\pm 0.22$}          & 40.3\scriptsize{$\pm 0.01$}  & 19.0\scriptsize{$\pm 0.17$}        & 74.3\scriptsize{$\pm 0.01$} & 22.2\scriptsize{$\pm 0.23$}                & \underline{45.0}\scriptsize{$\pm 0.00$} & 57.3\scriptsize{$\pm 0.77$}         & 96.8\scriptsize{$\pm 0.01$}  & 9.83\scriptsize{$\pm 0.06$}           \\ 
    GD(5,4)                  & 85.4\scriptsize{$\pm 0.03$} & 15.5\scriptsize{$\pm 0.08$}          & \underline{40.7}\scriptsize{$\pm 0.01$} & 18.7\scriptsize{$\pm 0.13$}         & 76.0\scriptsize{$\pm 0.02$} & 16.1\scriptsize{$\pm 0.06$}                & \underline{\textbf{45.0}}\scriptsize{$\pm 0.03$} & 50.5\scriptsize{$\pm 0.67$}         & \underline{99.3}\scriptsize{$\pm 0.00$} & 13.7\scriptsize{$\pm 0.64$}           \\ 
    \({\text{S}^2\text{-MAD(5,4)}}\)                  & 84.8\scriptsize{$\pm 0.02$} & \underline{4.53}\scriptsize{$\pm 0.31$}          & 40.3\scriptsize{$\pm 0.00$} & \underline{11.4}\scriptsize{$\pm 0.29$}         & \underline{76.8}\scriptsize{$\pm 0.02$} & \underline{4.78}\scriptsize{$\pm 0.57$}                & 43.8\scriptsize{$\pm 0.01$} & \underline{23.6}\scriptsize{$\pm 7.05$}         & \underline{\textbf{99.6}}\scriptsize{$\pm 0.00$} & \underline{2.29}\scriptsize{$\pm 0.07$}           \\
    \midrule
    % \rowcolor{lightgray}
    \rowcolor{Gray}
    \multicolumn{11}{c}{\texttt{GPT-4-0613}} 
    \\
    \midrule
    CoT                      & 92.8\scriptsize{$\pm 0.01$} & 0.38\scriptsize{$\pm 0.00$}           & 73.0\scriptsize{$\pm 0.01$} & 0.71\scriptsize{$\pm 0.00$}         & 83.7\scriptsize{$\pm 0.01$} & 0.39\scriptsize{$\pm 0.00$}                & 47.2\scriptsize{$\pm 0.02$}  & 2.23\scriptsize{$\pm 0.10$}         & - & -          \\
    CoT-SC(40)               & \underline{\textbf{94.3}}\scriptsize{$\pm 0.00$} & 15.2\scriptsize{$\pm 0.03$}          & \underline{\textbf{81.0}}\scriptsize{$\pm 0.01$}  & 27.9\scriptsize{$\pm 0.00$}         & 88.4\scriptsize{$\pm 0.01$}     & 15.4\scriptsize{$\pm 0.02$}                & 53.0\scriptsize{$\pm 0.00$} & 90.0\scriptsize{$\pm 0.30$}         & - & -           \\
    MAD(5,4)                 & 93.3\scriptsize{$\pm 0.01$} & 50.4\scriptsize{$\pm 0.19$}          & 78.7\scriptsize{$\pm 0.00$} & 70.9\scriptsize{$\pm 0.20$}        & \underline{90.8}\scriptsize{$\pm 0.00$} & 61.7\scriptsize{$\pm 0.09$}                & 59.7\scriptsize{$\pm 0.01$} & 109.6\scriptsize{$\pm 5.22$}         & - & -           \\ 
    S-MAD(5,4)                 & 93.0\scriptsize{$\pm 0.02$} & 24.4\scriptsize{$\pm 0.04$}          & \underline{79.3}\scriptsize{$\pm 0.00$}  & 59.5\scriptsize{$\pm 0.53$}        & \underline{\textbf{91.5}}\scriptsize{$\pm 0.00$} & 48.2\scriptsize{$\pm 0.24$}                 & \underline{60.7}\scriptsize{$\pm 0.02$} & 97.8\scriptsize{$\pm 0.42$}         & -  & -           \\ 
    GD(5,4)                  & \underline{94.3}\scriptsize{$\pm 0.00$} & 21.4\scriptsize{$\pm 0.08$}           & 76.7\scriptsize{$\pm 0.01$} & 36.8\scriptsize{$\pm 0.28$}         & 87.8\scriptsize{$\pm 0.01$} & 25.3\scriptsize{$\pm 0.11$}                & \underline{\textbf{62.7}}\scriptsize{$\pm 0.02$} & 64.6\scriptsize{$\pm 1.25$}
    & - & - \\
    \({\text{S}^2\text{-MAD(5,4)}}\)                  & 94.2\scriptsize{$\pm 0.00$} & \underline{2.78}\scriptsize{$\pm 0.16$}          & 77.3\scriptsize{$\pm 0.01$} & \underline{11.2}\scriptsize{$\pm 0.79$}         & 88.1\scriptsize{$\pm 0.01$} & \underline{4.71}\scriptsize{$\pm 0.35$}                & 60.8\scriptsize{$\pm 0.04$} & \underline{27.1}\scriptsize{$\pm 9.5$}         & - & -           \\
    \midrule
    \rowcolor{lightgray}
    \rowcolor{Gray}
    \multicolumn{11}{c}{\texttt{Llama-3.1-8B-Instruct}} 
    \\
    \midrule
    CoT                      & 83.2\scriptsize{$\pm 0.01$} & 0.32\scriptsize{$\pm 0.00$}           & 32.0\scriptsize{$\pm 0.03$} & 0.53\scriptsize{$\pm 0.00$}         & 61.2\scriptsize{$\pm 0.04$} & 0.43\scriptsize{$\pm 0.00$}         & 19.7\scriptsize{$\pm 0.02$}                & 2.35\scriptsize{$\pm 0.01$}         & 74.0\scriptsize{$\pm 0.01$} & 0.19\scriptsize{$\pm 0.00$}          \\
    CoT-SC(40)               & \underline{\textbf{89.0}}\scriptsize{$\pm 0.01$} & 12.6\scriptsize{$\pm 0.00$}          & 43.0\scriptsize{$\pm 0.01$}  & 21.1\scriptsize{$\pm 0.02$}         & \underline{74.1}\scriptsize{$\pm 0.02$}     & 17.2\scriptsize{$\pm 0.04$}                & 33.5\scriptsize{$\pm 0.02$} & 93.9\scriptsize{$\pm 0.01$}         & 83.0\scriptsize{$\pm 0.01$} & 7.63\scriptsize{$\pm 0.00$}           \\
    MAD(5,4)                 & 86.7\scriptsize{$\pm 0.02$} & 31.4\scriptsize{$\pm 0.27$}          & \underline{\textbf{46.0}}\scriptsize{$\pm 0.01$} & 80.5\scriptsize{$\pm 0.20$}        & 73.4\scriptsize{$\pm 0.02$} & 54.7\scriptsize{$\pm 0.58$}                & 37.0\scriptsize{$\pm 0.01$} & 117.5\scriptsize{$\pm 4.23$}         & \underline{\textbf{91.0}}\scriptsize{$\pm 0.02$} & 76.1\scriptsize{$\pm 0.12$}           \\ 
    S-MAD(5,4)                 & \underline{87.3}\scriptsize{$\pm 0.02$} & 26.0\scriptsize{$\pm 0.36$}          & \underline{45.0}\scriptsize{$\pm 0.00$}  & 66.3\scriptsize{$\pm 0.43$}        & \underline{\textbf{74.5}}\scriptsize{$\pm 0.00$} & 43.3\scriptsize{$\pm 0.14$}                 & \underline{\textbf{40.0}}\scriptsize{$\pm 0.01$} & 102.2\scriptsize{$\pm 2.53$}         &  89.5\scriptsize{$\pm 0.01$} & 62.3\scriptsize{$\pm 0.53$}           \\ 
    GD(5,4)                  & 86.5\scriptsize{$\pm 0.00$} & 17.0\scriptsize{$\pm 0.07$}           & 44.0\scriptsize{$\pm 0.01$} & 39.9\scriptsize{$\pm 0.28$}         & 73.5\scriptsize{$\pm 0.01$} & 39.5\scriptsize{$\pm 0.11$}                & 37.0\scriptsize{$\pm 0.02$} & 71.6\scriptsize{$\pm 2.25$}
    & 89.3\scriptsize{$\pm 0.03$} & 33.4\scriptsize{$\pm 0.02$}  \\
    \({\text{S}^2\text{-MAD(5,4)}}\)                  & 85.7\scriptsize{$\pm 0.02$} & \underline{5.39}\scriptsize{$\pm 0.16$}          & 44.0\scriptsize{$\pm 0.05$} & \underline{21.9}\scriptsize{$\pm 0.17$}         & 73.8\scriptsize{$\pm 0.04$} & \underline{10.6}\scriptsize{$\pm 1.74$}                & \underline{39.0}\scriptsize{$\pm 0.04$} & \underline{19.3}\scriptsize{$\pm 1.5$}         & \underline{90.0}\scriptsize{$\pm 0.03$} & \underline{13.9}\scriptsize{$\pm 0.29$}          \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Comparison of Token Cost and Accuracy Between \({\text{S}^2\text{-MAD}}\) and Other Methods.} The results of highest accuracy are \textbf{bold} and the results of both highest accuracy and lowest token cost except from CoT are \underline{underlined}. The dash (-) indicates that the model achieved a correctness rate of 1 for all methods on this dataset.}
\label{tab:comparison_all}
\end{table*}


\subsection{Experimental Setup} 
\paragraph{Tasks and Metrics.} To evaluate the effectiveness and efficiency of \({\text{S}^2\text{-MAD}}\) in mathematical and logical reasoning tasks, we use total token cost and accuracy (ACC) as evaluation metrics across five representative tasks: (1) GSM8K \cite{cobbe2021training}: a dataset designed to assess the model's reasoning ability in complex mathematical problems. (2) MATH \cite{hendrycks2021measuring}: a dataset covers various branches of mathematics to evaluate the capacity to generate problem-solving logic and reasoning processes. (3) MMLU \cite{hendrycks2020measuring}: a dataset that aimed at evaluating the model's overall performance across diverse tasks. (4) GPQA \cite{rein2023gpqa}: a multiple-choice question dataset,  containing 448 questions across various disciplines. (5) Arithmetic \cite{brown2020language}: a datasets evaluates the model's fundamental mathematical reasoning abilities.
% Arithmetic \cite{brown2020language}, GSM8K \cite{cobbe2021training}, MMLU \cite{hendrycks2020measuring}, MATH \cite{hendrycks2021measuring} and \cite{rein2023gpqa}.

\paragraph{Baselines.} We compare our \({\text{S}^2\text{-MAD}}\) with the following baselines: (1) Chain-of-Thought (CoT) \cite{wei2023chainofthought}; (2) Self-Consistency with Chain-of-Thought (CoT-SC) \cite{wang2023selfconsistency}; (3) Multi-agent Debate (MAD) \cite{liang2023encouraging}; (4) Sparse MAD (S-MAD) \cite{li2024improving}; (5) GroupDebate (GD) \cite{liu2024GroupDebate}.Experiments are conducted with different numbers of agents, rounds, and group strategies. For example, (5,4) represents using 5 agents and 4 rounds, while CoT-SC(40) indicates CoT-SC with 40 reasoning paths.

\paragraph{Implementation Details.} We set the number of intra-group rounds to 2 and use a forgetting mechanism to retain the outputs from the previous round only. At the end of each intra-group discussion phase, we filter and summarize the results from the same groups. Our experiments use GPT-3.5-turbo-0301, GPT-4-0613 and Llama-3.1-8B-Instruct as agents, evaluating all baselines and our \({\text{S}^2\text{-MAD}}\) in a zero-shot setting. Since the accuracy rate of the Arithmetic dataset reached 100\% in a single GPT-4, no further comparison was conducted. Details about the prompts 
and additional results for GPT-4o-mini and GPT-4o-0806 are showed in the Appendix \ref{appendix:prompts} and Appendix \ref{sec:experiments_add}.

For the Similarity Calculation Module, we primarily use regular expression matching for the main results and cosine similarity for further analysis, which uses the Bert-base-uncased model to vectorize the agent’s responses and calculate the cosine similarity between the responses. 
%Specifically, for mathematical calculation problems, we use regular expression matching to obtain the agent’s calculated results. For multiple-choice questions, we match the agent’s final selected option. We also implement a method using the bert-base-uncased model to vectorize the agent’s responses and calculate the cosine similarity between the responses. If the similarity score exceeds the threshold $\tau$, we consider the answers to be similar.


\subsection{Main Result} 
In this section, we conducted a detailed comparison of our method with multi-agent debate methods (including MAD, S-MAD, GD) and single-agent methods (including CoT, CoT-SC). The main observations are as follows:
firstly, we compare \({\text{S}^2\text{-MAD}}\) with MAD. The results presented in Table \ref{tab:comparison_all} shows that \({\text{S}^2\text{-MAD}}\) consistently reduces total token cost across different models while maintaining comparable accuracy, it achieves a reduction of 94.5\%, 84.2\%, 92.4\%, 83.6\% and 88.7\% on the five datasets respectively compared to MAD. The variation in these percentages is due to the varying difficulty of the questions, which impacts model performance. Furthermore, compared to S-MAD and GD, our approach achieves up to 90.2\% and 87.0\% less token cost, respectively. This demonstrates that there is a significant amount of redundancy in the information exchange during multi-agent debate, leading to the inefficiency of token cost throughout the debate process.
%When the questions are harder, the model initializes with more viewpoints, resulting in relatively lower information redundancy. However, since there is still some information redundancy, our method remains effective. Furthermore, compared to S-MAD and GD, our approach achieves the lowest reduction of 72.3\% and 58.0\%, and the highest reduction of 90.2\% and 87.0\%, respectively. Additionally, our method maintains comparable accuracy to MAD on all five datasets.

We also conducted a comparison with the single-agent method CoT, achieving a significant improvement in accuracy across five datasets, especially achieving up to 19.3\% and 12.6\% on GPQA and MMLU dataset. Furthermore, when compared with the Cot-SC method, we successfully reached or exceeded Cot-SC's performance on certain datasets, such as GPQA and Arithmetic, while using relatively fewer token cost. %More experimental results can be found in the appendix \ref{result}.

\begin{table}[h]
  \centering
  \setlength\tabcolsep{1.5 pt}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Method} & \textbf{ACC (\%)} & \textbf{Token (k)} & \textbf{Cost Saving}\\ 
    \midrule
    MAD(5,4) & \underline{72.3}\scriptsize{$\pm 0.00$} & 78.7\scriptsize{$\pm 0.31$} & - \\
    \midrule
    \({\text{S}^2\text{-MAD}}\) & &\\
    \midrule
    RE-Matching & 70.7\scriptsize{$\pm 0.01$} & \underline{12.4}\scriptsize{$\pm 0.73$} & -84.2\%\\
    VecCS$_{\tau = 0.96}$ & 69.0\scriptsize{$\pm 0.02$} & 18.6\scriptsize{$\pm 0.67$} & -76.4\%\\
    VecCS$_{\tau = 0.40}$ & \textbf{74.5}\scriptsize{$\pm 0.02$} & \textbf{4.18}\scriptsize{$\pm 0.09$} & -94.7\%\\
    \bottomrule
    
  \end{tabular}
  \caption{\textbf{Comparison of different similarity calculation strategies on MATH using GPT-4o-mini.} RE-Matching refers to regular expression matching and VecCS$_{\tau = 0.96}$ means vectorization and cosine similarity calculation with $\tau = 0.96$. The results of highest accuracy or lowest token cost are \textbf{bold} and the suboptimal results are \underline{underlined}.}
  \label{tab:comparison_similarity}
\end{table}
\subsection{In-Depth Analysis} 
\label{sec:in-depth-analysis}

\paragraph{Similarity Calculation Strategy.} In this section, we conduct further comparison on similarity calculation strategies using GPT-4o-mini. As shown in Table \ref{tab:comparison_similarity}, the method of vectorizing the responses and calculating their cosine similarity can achieve the best accuracy and the lowest token cost at a specific threshold. Specifically, on the MATH dataset using GPT-4o-mini, setting $\tau$ to 0.40 results in a 2.2\% improvement in accuracy and a 94.7\% reduction in token cost compared to the MAD method. However, when $\tau$ is set to 0.96, the increased token cost actually leads to a decrease in ACC. Furthermore, as illustrated in the Figure \ref{fig:vect}, the token cost remains relatively low when $\tau < 0.85$, but increases sharply thereafter. This is attributed to the prompt's strict formatting constraints on the agent's output, which cause high similarity among outputs. Additionally, we observed that the relative optimal threshold values for accuracy vary across different datasets (e.g., approximately 0.1 for GSM8K and 0.4 for MATH), making it challenging to manually determine the optimal threshold settings.



\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./figs/vect.pdf}
    \caption{\textbf{The relationship between the threshold} $\tau$, ACC, and Token Cost on the GSM8K and MATH datasets.}
    \label{fig:vect}
    \vspace{-1.0 em}
\end{figure}


\begin{table}[h]
  \centering
  \setlength\tabcolsep{1.5 pt}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Method} & \textbf{ACC (\%)} & \textbf{Token (k)} & \textbf{Cost Saving}\\ 
    \midrule
    MAD(8,3) & 86.7\scriptsize{$\pm 0.02$} & 28.5\scriptsize{$\pm 0.08$} & - \\
    S-MAD(8,3) & 86.5\scriptsize{$\pm 0.01$} & 18.7\scriptsize{$\pm 0.04$}  & -34.4\%\\
    \midrule
    GD & &\\
    \midrule
    2+6 & 86.7\scriptsize{$\pm 0.00$} & 20.3\scriptsize{$\pm 0.08$} & -28.8\%\\
    4+4 & 87.3\scriptsize{$\pm 0.01$} & 19.1\scriptsize{$\pm 0.00$}  & -33.0\%\\
    2+3+3 & 87.3\scriptsize{$\pm 0.02$} & 18.0\scriptsize{$\pm 0.05$} & -36.8\%\\
    2+2+4 & \underline{87.7}\scriptsize{$\pm 0.00$} & 18.3\scriptsize{$\pm 0.03$} & -35.7\%\\
    2+2+2+2 & \underline{\textbf{87.8}}\scriptsize{$\pm 0.00$} & 17.4\scriptsize{$\pm 0.04$} & -38.9\%\\
    \midrule
    \({\text{S}^2\text{-MAD}}\) & &\\
    \midrule
    2+6 & 84\scriptsize{$\pm 0.00$} & 8.01\scriptsize{$\pm 0.13$} & -71.9\%\\
    4+4 & 84.6\scriptsize{$\pm 0.00$} & 7.28\scriptsize{$\pm 0.19$} & -74.5\%\\
    2+3+3 & 85.1\scriptsize{$\pm 0.00$} & \underline{6.97}\scriptsize{$\pm 0.14$} & -75.5\%\\
    2+2+4 & 84.5\scriptsize{$\pm 0.00$} & 7.02\scriptsize{$\pm 0.20$} & -75.4\%\\
    2+2+2+2 & 83.4\scriptsize{$\pm 0.02$} & \underline{\textbf{6.78}}\scriptsize{$\pm 0.12$} & -76.2\%\\
    \bottomrule
    
  \end{tabular}
  \caption{\textbf{Comparison of different group strategies with GD and \({\text{S}^2\text{-MAD}}\) on GSM8K datasets.} The notation 2+6 signifies two distinct groups containing 2 and 6 agents respectively. The results of highest accuracy or lowest token cost are \textbf{bold} and the suboptimal results are \underline{underlined}.}
  \label{tab:comparison}
\end{table}

\paragraph{Group Strategy.} To assess the impact of different grouping strategies on performance and token cost, we conducted experiments involving 8 agents across 3 rounds on the GSM8K. As shown in Table \ref{tab:comparison}, increasing the number of groups can reduce the total token cost as the quantity of information exchange is limited by communication constraints. However, when the number of agents within a group increases, agents can more effectively receive diverse information, achieving higher accuracy.  Our findings indicate a clear trade-off between optimizing token cost and maintaining high accuracy, emphasizing the importance of selecting an appropriate grouping strategy.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./figs/e_2.pdf}
    \caption{\textbf{Scaling study of Agents and Rounds.}}
    \label{fig:e_2}
    \vspace{-1.0 em}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]
{./figs/scale.pdf}
    \caption{\textbf{Scaling Study of Token Cost.}}
    \label{fig:scale}
    \vspace{-1.0 em}
\end{figure*}



\paragraph{Agent, Round and Token Cost Scaling.} To assess the impact of the number of rounds and agents on the accuracy and token cost across different methods, we analyze the trends in accuracy and token cost for different combinations of rounds and agents. As shown in the Figure \ref{fig:e_2}, with the increase in the number of agents and rounds, there is a noticeable enhancement in the overall performance of various methods; however, this also leads to a significant increase in token cost. Our approach maintains a certain level of performance while exhibiting a gradual increase in token cost as agents and rounds increase, achieving the lowest token cost across different setting, as shown in Figure \ref{fig:scale}. This indicates that there is a significant amount of redundant and repetitive information exchange during debates, resulting in higher token cost and less effective agent interactions.


\subsection{Ablation Study} 
To investigate the impact of different modules and strategies on performance, we conducted ablation experiments on the GSM8K dataset using GPT-3.5-turbo, are shown in Table \ref{tab:ablation}. Our \({\text{S}^2\text{-MAD}}\) achieved an accuracy of 85.6\% with a token cost of 4.73k, demonstrating a significant 72.7\% reduction compared to MAD. We further explored sparsity through constrained communication topologies, which slightly decreased accuracy to 84.7\% while retaining a similar token cost. Without the early stopping strategy led to a slight accuracy drop to 84.4\% but maintained a comparable token cost of 4.99k. In contrast, removing the jump strategy resulted in a more substantial decline in accuracy to 80.8\% and an increase in token usage to 9.45k, We hypothesize that this is due to insufficient information diversity, causing redundant checks that impact response accuracy. Finally, although removing the filtering module can increase accuracy to 87.6\%, it also leads to an increase in token cost of 13.4k. Although our method has not yet achieved optimal performance in terms of accuracy and token cost, it still shows a slight improvement over the MAD method while significantly reducing token usage, highlighting the efficiency of our proposed method in balancing accuracy and token saving.

