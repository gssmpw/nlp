\section{Method}

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\textwidth]{figures/model.pdf}
\end{center}
\caption{Our model, \model{}, pre-trains on instructional videos to predict: a) which procedural steps belong to each video clip (left), b) where in the video hierarchy the current video belongs to (center), and c) video clip augmentation and training procedures to learn the joint clip-video representations most effectively (right).}
\end{figure}

The goal of our approach is to use both prior procedural step-level information as well as task-level hierarchy metadata to efficiently pre-train video representations for downstream instructional tasks. 
% By leveraging local clip information as well as the global video context, we aim to learn contextualized clip and video embeddings to improve corresponding clip and video downstream tasks. This differs from previous works which focus on using simpler objectives to align or infer only clip features over a large pre-training corpus. 
This involves inferring the clip-level step labels given a set of prior procedural steps as done in prior work (\S \ref{methods:clip_kg}). Our novel contributions involve video-level hierarchical task prediction (\S \ref{methods:video_hier}), video clip ordering and selection (\S \ref{methods:clip_sel}), and analytical early stopping strategies (\S \ref{methods:early_stop}) to efficiently pre-train a video representation model. These contributions lead to improved downstream performance in task recognition, step recognition, as well as step forecasting objectives, which are important in recommending the correct instructional videos to users.

\subsection{Clip to Procedural Steps Alignment}
\label{methods:clip_kg}

The first pre-training task involves aligning the clip-level representations with steps associated with that task occurring in the instructional video. These associated steps can be derived from the videos themselves to identify a common subset of narrated steps (ex. cut the chicken, boil pasta) along the same video task (ex. making Chicken Alfredo). Similarly, the steps required for that task can be found on a how-to tutorial website, such as an online recipe or wikiHow. For each of these instructional tasks $T \in \mathcal{T}$, we leverage procedural knowledge by using the task steps $s \in S$ provided in a sequential order $T_i = \{s_1, s_2, \dots, s_k\}$. Here each $s$ is a text description of that step. The steps may occur in more than one task $T$ and each task may have a different number of steps.

These steps then have to be aligned with clips $x$ in a video $X$, where segments for pre-training are not previously annotated. In this case, each video is segmented into 9.6 second clips. Each clip is further made out of 3.2 second segments, where 32 frames are fed into the HowTo100M pre-trained MIL-NCE video branch \citep{miech19howto100m} at 10 fps to produce each segment. Then 3 segments are mean pooled to generate each 9.6 second clip representation $X_i = \{x_1, x_2, \dots, x_n\}$, as done in \cite{paprika}. 

For the alignment, we select the top-scoring pair alignment between the clip and any steps $S$. The embeddings are obtained using MPNet \citep{mpnet}, where cosine similarity (sim($\cdot, \cdot$)) is calculated between each corresponding ASR caption $C_i = \{c_1, c_2, \dots, c_n\}$ of the 9.6 second clips and each task step in $S$. These top scoring pairs constitute pseudo-labels used per clip during pre-training $y_i = \text{top-}k\ \text{sim}(\text{MPNet}(c_i), \text{MPNet}(s))\ \forall s \in S$ corresponding to each clip $x_i \in X$. Therefore we have $y_i \in Y$ as the complete set of labels for all clips in the video, where $|Y| = |X|$. 

To train \model{} the base architecture is a standard single  Transformer encoder layer (TF-Enc). The input to the model is the entire sequence of mean pooled embeddings $X$. Then each positional output $\model(X)_i$ corresponding to clip $x_i$ is fed into a task head, which consists of a two layer MLP with a ReLU activation function in between. The task head predicts that clip's distribution over all task steps $S$ to compute a binary cross entropy (BCE) loss $\mathcal{L}_{step} = \sum_{X}\sum_{y_i \in Y}\text{BCE}(\text{MLP}(\text{TF-Enc}(X)_i), y_i)$. Recall that each $y_i$ may contain multiple step pseudo-labels, depending on our $\text{top-}k$ setting.

\subsection{Video to Hierarchy Alignment}
\label{methods:video_hier}

For the video-level objective, we predict the video's categorization \emph{path} within a larger hierarchy. These paths are categorized by users or are automatically tagged on platforms such as YouTube when a video is uploaded. We take the hierarchy paths and use our model to predict each node within the hierarchy path for that video (ex. Food $\rightarrow$ Italian $\rightarrow$ Chicken Alfredo). The model that can predict these coarse-to-fine video topics guides the step-level predictions required, and vice versa. 

We define this hierarchy as $H$ and containing nodes $n \in H$. These hierarchy nodes $n$ are associated with a single parent node and children node(s). At the first level, the hierarchy contains root node(s), which have no parents. Each level may also contain leaf nodes that have parents, but no subsequent children. Each video $X$ is already associated with a corresponding hierarchy leaf node $n_l$ at level $l \in [1, L]$ in the hierarchy, where $L$ is the max depth of the hierarchy. Therefore, each video $X$ has a corresponding path in the hierarchy $P=\{n_1, n_2, \hdots, n_l\}$. This starts with the highest level root node $n_1$ and goes down to the leaf node $n_l$ associated with the video. 

% Here the leaf node $n_1$ is linked to one of the video topics $T \in \mathcal{T}$. The assignment of the video to the hierarchical category is previously provided through prior categorization systems. It can also be inferred by the similarity between the video title and node topic title $\text{top-}1\ \text{sim}(\text{MPNet}(X_\text{title}), \text{MPNet}(T_\text{title}))\ \forall T \in \mathcal{T}$. The path associated with $X$ is defined as $P=\{n_1, n_2, \hdots, n_l\}$. This starts with the highest level root node $n_1$ and goes down to the leaf node $n_l$ associated with the video. 

Predicting the path $P$ is done by first pooling the position-wise clip outputs $\text{TF-Enc}(X)_i$ to leverage the contextualized clip-level representations to generate a video-level embedding $v$. We test two pooling mechanisms. The first is average pooling the positional outputs $v = \frac{1}{|X|} \sum_i \text{TF-Enc}(X)_i$. The second is to stack another Transformer layer on top of the first one and use the first position classification output embedding $v = \text{TF-Enc}(\text{TF-Enc}(X))_1$. Using the single video-level embedding produced from either method, we use MLP task heads $H_i$ for each level of our hierarchy to predict the correct node at each level of the path using a cross-entropy loss (CE) $\mathcal{L}_{path} =\sum_v \sum_{n_i \in P} \text{CE}(H_i(v), n_i)$.

\subsection{Clip Selection and Ordering}
\label{methods:clip_sel}
During step prediction, \cite{lwds} and \cite{videotf} reported better results when predicting over all the steps $S$ across all tasks. However, we study clip selection in the context of our video-level alignment as well. In this context, we want to ensure that the clips associated with each hierarchy path are semantically meaningful to infer the video's topic $T$. To do so we remove and reorder the clips from the original sequence of clips by leveraging the procedural steps metadata.

\subsubsection{Clip Thresholding and Filtering}


To filter down which clips $x_i \in X$ are used, we first threshold any associated video captions $c_i \in C$ and steps $s \in S$ with a dot product above 1.0. This threshold works well in practice when using different magnitude embeddings from the captions and steps using MPNet. The new subset of clips can be defined as $X' = \{x_i \in X \mid \text{MPNet}(c_i) \cdot \text{MPNet}(s) > 1.0\}$. This improves the precision of the clips used and avoids any filler content unrelated to the video topic. 

The second filtering method only keeps clips that have step pseudo-labels belonging to the wikiHow steps of the video's associated topic $T$. To do this we find the closest topic that belongs to the video's node category. This is done as the hierarchical node names $n_l$ (Chicken Alfredo) and the external how-to task names $T$ (Grandma's Favorite Italian Alfredo) may not align. Concretely the topic is determined through
$\text{arg max}_{T \in \mathcal{T}}\ \text{sim}(\text{MPNet}(n_l), \text{MPNet}(T))$.
Then we take the complete set of pseudo-labels $Y$ containing task steps computed for the entire video $X$, as well as the steps associated with the topic $T$. From these pseudo-labels, we only keep the clips whose pseudo-label steps occur in the procedural steps of that topic $X' = \{x_i \in X \mid y_i \in T\}$. While the first level of filtering considers similar steps across all possible tasks, this further refines the selection of relevant clips for that topic. 

\subsubsection{Clip Reordering}

Instructional videos often contain sequences with long temporal gaps between a relevant sequence of clips. For example, when prepping Chicken Alfredo, one video may present cleaning and cutting the chicken at the start, while in another video this occurs right before seasoning and cooking. To efficiently learn the visual alignment between task steps and video clips, we reorder the clips chronologically using the pseudo-labels $Y$ based on the order present in the task steps $T$. We augment the reordering by randomly swapping neighboring clips with a probability of 0.15.  

The reordered clips often don't contain all the steps present in wikiHow and usually contain repeat clips of a step that occurs over a long duration. In cases with repeat steps, we experiment with randomly selecting a single step, such that the video contains unique steps.

Note that this selection and reordering augmentations are performed on the sequence of MIL-NCE input features $X$ before passing them into \model{} for pre-training. 

\subsection{Pre-training and Early Stopping}

\model{} is trained jointly with the clip-level pseudo-labels and the video-level path labels $\mathcal{L}_{joint} = \mathcal{L}_{step} + \mathcal{L}_{path}$, where the clips are filtered and reordered based on our selection strategy. The goal is to train generalized embeddings for auxiliary downstream tasks. These tasks may not share the same pre-training attributes (tasks, steps, and hierarchy) used during the pre-training procedure. In this case, we want to ensure that our model learns a generalized representation that does not overfit to our pre-training attributes.

Traditionally, we could keep track of the loss and stop the training once the improvement stops after a certain number of epochs. However, at this point, the model would have already overfit to our pre-training attributes. Instead, we observe if the model is capturing the general structure of our pre-training attributes. This is done by monitoring the model's accuracy on the clip-to-step prediction task, as these steps are more likely to overlap with downstream tasks than broader video hierarchy nodes in $H$ or how-to topics $\mathcal{T}$. 
However, we do not want to overfit on the step predictions when the downstream steps are different. Therefore we identify an inflection point of this step accuracy, up to which point the most generalizable features are learned. 

To determine this inflection point, the accuracy metric $M = \{m_1, m_2, \dots, m_{|M|}\}$ of step prediction per epoch number is recorded. From these discrete points, we first fit a polynomial $p(e) = a_0 + a_1 e + \dots + a_n e^n$ to estimate the functional form of the accuracy metric given the epoch by minimizing the least squares error: $\text{arg min}_p \sum_{e=1}^{|M|} |p(e) - m_e|^2$. In practice, we set our $n=10$. Given this function, we compute the first derivative and find the epoch that leads to the local maximum $p$: $e^* = \text{max}_{e \in [1, K]}\ p'(e)$. The model from this epoch $e^*$ is used for downstream fine-tuning. 

% This is done by identifying an inflection point model's accuracy or loss. We observe the accuracy of the hierarchy node prediction, where we select an intermediate level node level. Performance of the intermediate node level shows the model's generalizability to a range of topics that is greater than a small number of root nodes, but not too specific to overfit to the topic nodes. 

% Given the accuracy score of this node per epoch, we have to select which epoch to checkpoint the model. From these points we first fit a polynomial to estimate the functional form of the accuracy curve. Given this function, we compute the second derivative to find this inflection point in the accuracy. This is done by taking the epoch corresponding maximum value of this second derivative function. The model from this epoch is used for downstream fine-tuning. 
\label{methods:early_stop}