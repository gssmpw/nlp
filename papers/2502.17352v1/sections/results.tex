\section{Results}

\subsection{Baseline Performance}
\input{tables/baselines}
\input{tables/data_efficiency}
We compare our method against previous works in Table \ref{tab:baselines}. Without any task pre-training, MIL-NCE provides us with a baseline performance for both COIN and CrossTask. Paprika substantially improves upon this basis using only a shallow pre-trained embedding model. This further demonstrates that leveraging prior knowledge within the PKG and well defined objectives leads to efficient pre-training. With Transformer-based pre-trained models, LwDS improves upon Paprika using the distribution matching strategy over all step labels. In comparison, VideoTF had lower performance due to its masking learning strategy over our smaller basis of 30k videos. 

We test our framework initially with just the video hierarchy path alignment objective $\mathcal{L}_{path}$ using the Transformer pooling layer to compute the video level embedding $v$. In this setting, we see improvements over the baseline results, even when we do not leverage the clip-level labels. This indicates that the right model architecture and video-level hierarchical data provides the largest improvement in downstream model performance. 

When we use our full model with clip step alignment, we improve the performance further. For the augmentations we: 1) keep clips with dot product similarities above 1.0, 2) are part of the linked task steps, and 3) are ordered by the step order. We ablate these choices in \S \ref{sec:ablations}.

To understand the data efficiency of \model{} we report the performances of the baseline models given their original pre-training dataset sizes in Table \ref{tab:data_efficiency}. 
Compared to Paprika with 85k samples, \model{} outperforms it with 30k samples. While using the full 1.2M samples, \model{} still has comparable results, with strong task recognition performance. For LwDS and VideoTF reported with 1.2M samples, improvements are seen in the step and task recognition methods when simpler step prediction objectives have access to more pre-training data. An interesting observation is that by leveraging task-level objectives, \model{} and Paprika add significant gains in forecasting tasks. This is true even when step-objective only LwDS and VideoTF methods leverage the full data.


\subsection{Method Ablations}
\label{sec:ablations}
\input{tables/ablations}

We ablate the design choices used for our task and clip-level objectives in Table \ref{tab:ablations}. For the task-level objective, we ablate using a mean or a Transformer encoder (TF-Enc) pooler to predict the hierarchy path steps. For clip-level designs, we first test thresholding clips whose MPNet dot product between the caption top step instruction is greater than 1.0. We then test filtering steps that only belong to that video's task. The clips can also be sorted by the order in which the steps appear in the task wikiHow. Unique clips based on the step labels can also be chosen, and finally, neighboring clips can be randomly swapped to increase the input variance order of the steps per video. 

From the task-level pooling, using a second Transformer layer to compute a contextualized video embedding improves downstream performance using the same clip-level settings. 

For clip-level performance, just thresholding the clips already provides comparable performance to other clip ablation setups. This shows that even a minimum filtering effort of the video segments used during pre-training can have a large impact on our joint video-clip training. Note that the setting without thresholding would be the same setup as the LwDS baseline. Using thresholding, task steps and sorting provides similar performance to thresholding only. However, this setup speeds up pre-training by 2-3x due to the fewer clip samples required per video input, thus larger batch sizes available (256 versus 92). We observe that adding unique steps and swapping clips leads to diminishing returns. We hypothesize that this is due to drastic reduction in the number of clips per video, where the variance in the steps and their natural order in videos is lost. 

\subsection{Early Stopping Results}

\input{tables/early_stopping}
\begin{figure}[t]
\centering
\begin{subfigure}{.485\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/early_stop_full.pdf}
  % \caption{A subfigure}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.515\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/early_stop_thresh_task.pdf}
  % \caption{A subfigure}
  \label{fig:sub2}
\end{subfigure}

\bigskip
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/early_stop_thresh.pdf}
  % \caption{A subfigure}
  \label{fig:sub3}
\end{subfigure}
\caption{Pre-trained models from different epochs are tested on downstream COIN task recognition (red line) and step recognition (blue line) tasks. The derivative of the clip step accuracy $p'(x)$ is also plotted (black line), where the max value represents the analytical early stopping point (dashed line). The saturation based early stopping with no improvements over 50 epochs is also presented as a reference (dotted line). 
% This is carried out over three different video augmentation settings to help identify common patterns.
}
\label{fig:early_stopping}
\end{figure}

During the evaluation of our method, we took the early stopping pre-training checkpoint associated with the clip step prediction accuracy. We also test using the same early stopping approach for the baseline methods and report them in Table \ref{tab:early_stopping}. Here the average accuracy performance change from epoch 2000 to our analytical stopping checkpoint is -0.30 across all baseline methods and a change of 0.53 for our \model{} method. This indicates that the downstream performance is preserved even when running on average for 508 out of 2000 epochs. Note that during our experiments we saved the model checkpoint every 50 epochs, while the epoch metrics were kept every epoch. 

% In comparison the average change is () using a performance plateau method after 50 epochs while taking an average () more epochs to run.  

We also visualize the early stopping criterion versus downstream performance of COIN step and task recognition experiments in Figure \ref{fig:early_stopping}. The general trend across different pre-training setups is that downstream performance peaks around a certain epoch. This epoch is typically closer to our analytically computed early stopping point than a typical saturation-based stopping, which is determined by no step accuracy improvement over 50 epochs. We can observe here that in our post hoc evaluation of neighboring epochs, that optimal checkpoints may vary but remain close to the analytical epoch, under different pre-training settings. During our experimentation, we selected the saved checkpoint closest to this optimal epoch.

Here it is also shown that using our selected method of thresholding, in task, and sorting methods for clip tasks also have a faster convergence time while providing comparable performance to thresholding only augmentations. This adds to our previous motivation of using fewer clips during pre-training, to allow for \emph{faster training and faster stopping}. Removing the sorting leads to more variance for the model to learn, and doubles the epochs required. Using only thresholding requires the greatest number of epochs for convergence since each video may contain steps from different tasks and in different orders. In general, we show that it is possible to identify optimal stopping points when learning clip-level representations, thereby reducing computational costs. 

