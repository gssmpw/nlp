\section{Introduction}

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
\includegraphics[width=0.5\textwidth]{figures/overview.pdf}
\end{center}
\caption{We leverage both task hierarchical data as well as procedural step information to pre-train our instructional video model \model.}
\label{fig:overview}
\end{wrapfigure}

Instructional videos are a quick and convenient way to visually understand how to carry out tasks such as cooking a meal, filling out an application, or assembling furniture. A viewer then would want to find the most relevant video recommendations containing a certain cooking ingredient or a furniture fixture that they are interested in. However, the videos may not directly provide explicit annotations for these different steps. Therefore it is important that these instructional videos can be categorized within different tasks, and the distinct \emph{steps} occurring within the videos can be predicted explicitly.

% Instructional videos provide visual instruction of step-by-step procedural knowledge on how to complete a task. 
Even when distinct steps within individual videos are predicted, the video recommendations must be contextualized to the user's preferences. If a user is looking up recipes with chicken, videos related to Chicken Cesar Salad or Chicken Alfredo are equally relevant. If we also know the user was looking at Italian recipes, the understanding that Chicken Alfredo pertains to Italian cuisine would be presented. Similarly, if the user was looking at low-carbohydrate meals, then the additional categorizing salads and soups videos would lead to the recommendation of the Chicken Cesar Salad. In addition to individual steps within the videos, we benefit from models that can contextualize the steps within their video-level \emph{tasks}.

% The underlying steps are common across different tasks and related tasks. However, the same steps may occur across different videos and require different contextualized embeddings. For example the instructional video could be about making Chicken Cesar Salad or Chicken Alfredo. In both videos, a chicken is being cut, but each clip should be contextualized to their global recipes if a user would ask a system what the next step is, or what recipe does this clip belong to.

Typically video representation models learn task-level representations by \emph{implicitly} learning the relationships between different steps in different tasks. We \emph{explicitly} learn these step and task relationships by mining how-to procedural steps as well as video hierarchy data, illustrated in Figure \ref{fig:overview}. For example, a video discussing Chicken Alfredo would be under the categories: Food and then Italian. A model knowing which tasks and similar steps belong to food prep and Italian cooking leverages both global task information as well as local step information. 

This leads to more efficient video pre-training for instructional fine-tuning tasks, due to this joint contextualization of video and clip-level features on prior hierarchical knowledge and procedural steps respectively. If the output video embedding aligns with other videos of Italian cooking, then it provides a strong prior on the procedural steps expected in the video (ex. cut a chicken breast, boil pasta, add Alfredo sauce, etc.) for downstream step-wise tasks related to Italian cooking. Similarly, such clips observed in a video would provide a strong prior for downstream video-level tasks for classification and clustering (ex. find similar recipes). 

By explicitly leveraging these prior procedural steps and hierarchy structure, less pre-training data is required to train the video model from scratch, beneficial in low-resource settings where time, domain-specific data, or compute is limited. We incorporate these findings with a \underline{\textbf{P}}rocedural-\underline{\textbf{H}}ierarchical \underline{\textbf{I}}ntegrated \underline{\textbf{Vi}}deo \underline{\textbf{T}}ransformer (PHIViT or \model) model, with the following contributions:
\begin{itemize}
    \item Architecture: We infer the video's task hierarchy path in addition to predicting individual steps to perform joint video-clip pre-training. 
    \item Training: The model uses different video augmentation procedures during pre-training to identify salient steps and optimal early stopping strategies. 
    \item Evaluation: The model is evaluated on video, clip, and forecasting tasks on two downstream datasets. We improve performance over previous methods and show which \model{} pre-training settings work best.
    % .98 accuracy points, 1.5% percentage
\end{itemize}