\section{Conclusion}

Instructional videos provide viewers with a convenient way of learning new tasks based on their interests and the materials they have available to carry out the task. Therefore it is important to identify these video topics and the intermediate steps that pertain to them. We leverage two levels of prior knowledge through these how-to steps corresponding to videos in addition to the video's task in a larger task hierarchy structure. Leveraging these explicit knowledge structures allows our model to pre-train across different video topics more efficiently. This differs from implicitly learning task-level representations from individual clips, which requires more data, time, and compute to pre-train.

This is demonstrated by pre-training \model{} on 30k instructional videos and testing its capabilities on three downstream tasks involving task recognition, step recognition, and step forecasting across two different datasets. In these settings, \model{} outperforms previous procedural pre-training methods as it efficiently incorporates step and task-level supervision within a Transformer encoder. We also present practical pre-training augmentation strategies as well as early stopping analysis to improve the compute as well as performance efficiencies of our pre-training method. With this work, we push to further understand how to mine, and leverage structured data within models of complex modalities, such as video, in a generalizable manner.