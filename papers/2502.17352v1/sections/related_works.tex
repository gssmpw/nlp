\section{Related Works}
Pre-training a video model for clip and video-level representation learning relies on a large amount of unstructured video data. These datasets, such as HowTo100M \citep{miech19howto100m} contain automatic speech recognition captions over the entire continuous video, thus they do not contain explicit steps occurring within the videos. If steps occurring with videos are discretized, it provides a method of learning a better representation of instructional videos. 

\subsection{Mining Procedural Knowledge}
Existing works aim to obtain these discrete steps directly from videos. Earlier works aim to first identify the relevant scenes of the videos by clustering video embeddings to identify the salient steps across videos to use \citep{shah2023steps}. \cite{rohrbach2022tl} extracts salient steps by finding repeat steps occurring across multiple videos and are demonstrated verbally. Similarly \cite{wang2023self} performs this using an inverse optimal transport problem between text and visual semantic embeddings. Given key steps in the video, a hypergraph can be created that links the common steps between the videos \citep{bansal2024united}. This provides a video-oriented approach to mining step-by-step procedures. Paths, consisting of chains of steps, along these graphs are used to predict the step-by-step actions required. Here smaller thus more common sub-paths are a more reliable indicator of subsequent steps than any path between two steps \citep{li2023skip}. This is further developed by \cite{zou2024weakly} through adding ordering constraints of steps. To generalize the variance of clips representing the same discrete steps, \cite{yang2023implicit} learn a model with affordance knowledge to identify equivalent entities and behaviors from HowTo100M. 

Beyond the video data itself, external sources contain step-by-step procedures for a variety of instructional tasks, such as wikiHow \citep{koupaee2018wikihow}. \cite{zhou2023non} learns how to convert a linear chain of steps to create graphs, containing steps that are interchangeable or optional. Works leveraging distant supervision (LwDS) in \cite{lwds} and in \cite{videotf} Video Taskformer (VideoTF) align the wikiHow step captions to the clip speech transcripts to provide step pseudo-labels per video clip. \cite{paprika} leverages these pseudo-labels to create a Procedural Knowledge Graph (PKG) of step labels across aligned video clips, therefore leveraging both external knowledge as well as video data to create the step-by-step instructions for HowTo100M video tasks. These discrete structures can then be used to pre-train instructional video models. 

\subsection{Procedural Pre-training}
The contrastive pre-trained model (MIL-NCE) \cite{miech19howto100m} is used to align step description embeddings to the video captions embeddings for clip-level tasks, such as step prediction. \cite{paprika} uses the PKG to learn a clip lightweight procedural-aware model (Paprika) tuned on multiple video-level and clip-level pseudo-labels. This model is then used to efficiently contextualize clip embedding inputs into a shallow Transformer \citep{vaswani2017attention} for downstream tasks. 

Instead of only clip-level inputs, the entire video context consisting of multiple clips can be used for pre-training. LwDS pre-trains a video-level Transformer to infer the distribution of step pseudo-labels for each video clip input. VideoTF builds on top of this by testing a masking objective to recover the predicted steps for random clips. These models learn video-level information implicitly through clip-step prediction objectives. With \model, we jointly learn procedural steps in addition to video-level representations explicitly within a Transformer.