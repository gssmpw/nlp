\section{Experimental Setup}

\subsection{Pre-training Setup}
We pre-train our model on the HowTo100M dataset, where we use a subset of the data used in \cite{bertasius2021space}. Out of those videos, 30k videos were still available to download, which we use in all of our baselines, experiments, and ablations. 

To obtain the tasks $\mathcal{T}$ and instructional steps $S$, we use the wikiHow corpus \citep{koupaee2018wikihow}. We select the articles following \cite{paprika}, where each article contains a task name and contains high level step descriptions. 

For video hierarchy matching, we extract the hierarchy paths from the HowTo100M dataset \citep{miech19howto100m}, where each video is classified within a hierarchy $L = 3$ levels deep. This consists of the root nodes, child nodes which are subcategories of the root node, and task (leaf) nodes $n_l$ which are subcategories of the child nodes. The root, child, and task nodes consisted of 17, 105, and 1059 nodes respectively. These are also the corresponding output prediction sizes for our MLP classifier heads $H_1, H_2$, and $H_3$ when training our $\mathcal{L}_{path}$ objective.

We jointly train the video and clip level objectives given the clip selection and ordering augmentations $\mathcal{L}_{joint} = \mathcal{L}_{step} + \mathcal{L}_{path}$. We used an Adam optimizer \citep{kingma2014adam} with a learning rate of $1e^{-4}$, a decay of $1e^{-3}$, and a batch size of 256. For our proposed method, pre-training for 2000 epochs took 27 hours using four A40 GPUs and 16 CPU threads. The clip-to-step metric was tracked to determine an early stopping checkpoint. The epoch $e^*$ that maximized the first derivative function was used for downstream fine-tuning. 

\subsection{Fine-tuning Setup}

We evaluate our pre-training setup on two datasets: COIN \citep{tang2019coin} and CrossTask \citep{zhukov2019cross}. COIN contains 11.8k videos across 180 tasks arranged in a hierarchical fashion, where the hierarchy was manually curated (different from HowTo100M). Each clip within the video is annotated with a step belonging to the video task. CrossTask contains videos across 83 tasks. For evaluation purposes, we tested 18 subtasks with clip-level step annotations with 2.3k videos. 

For each dataset, we evaluate over three tasks:
\begin{itemize}
    \item \textbf{Task Recognition}: Predict the task label from the video using the pooled video-level outputs $v$.
    \item \textbf{Step Recognition}: Predict the step associated with each clip, where the position-wise clip embeddings from the base Transformer layer are fed directly into a task head for prediction.
    \item \textbf{Step Forecasting}: Predict the step for the clip, where the input clip embedding is masked. We ensure that there is always at least one prior clip in the forecasting history.
\end{itemize}   

% The model architecture for each task follows the one used in pre-training. For task recognition, we used the pooling strategy used in hierarchy alignment. For step recognition and forecasting, we directly use the position-wise outputs from base Transformer output to infer the labels from the task heads. Note that the task heads used in fine-tuning are newly initialized since they cover different tasks than those in pre-training.
Each dataset and task is split for training and evaluation following \cite{paprika}. Note that the task heads used in fine-tuning are newly initialized since they cover different tasks than those in pre-training. The Adam optimizer settings are kept the same as in pre-training, with a reduced batch size of 16.

% Each dataset and task is randomly split the videos into training and test splits with 80\% and 20\% of the video respectively. Note that the task heads used in fine-tuning are newly initialized since they cover different tasks than those in pre-training. The final checkpoint used for evaluation is the one with the best training accuracy, with early stopping occurring after 5 epochs after no improvement. The Adam optimizer settings are kept the same as in pre-training, with a reduced batch size of 16.


\subsection{Baseline Methods}

We compare our pre-training approach against several baselines. The first baseline is observing the performance using the original HowTo100M MIL-NCE embeddings \citep{miech19howto100m} in our architecture without any pre-training. 

We also test Paprika \citep{paprika}, which leverages the relationships between steps across different tasks within its mined procedural knowledge graph as pseudo-labels. These pseudo-labels are used to define four different objectives to train a lightweight MLP Procedure-Aware Model on individual clip inputs during pre-training. This MLP model is used to contextualize the input clip embeddings before feeding them into the Transformer for the downstream tasks.

Transformer-based baselines are also trained with LwDS \citep{lwds}, which also performs clip step prediction. For the output labels, it computes the similarity between the clip and all wikiHow steps $S$. It tests two different label strategies: taking the top-3 scoring discrete labels for step classification (SC) and continuous probability labels for distribution matching (DM). For fine-tuning it uses a second layer Transformer to pool the clip representations for inference. 

Another method is VideoTF \citep{videotf}, which builds on top of LwDS by inferring \emph{masked} clip labels instead of inferring all clip labels unmasked inputs. During pre-training it uses a two layer Transformer for step prediction. During fine-tuning it further tunes the corresponding task heads for downstream evaluation. 

All baselines used the same pre-training and fine-tuning optimization settings as our method. For pre-training, each baseline was trained for 2000 epochs. We refer back to the original papers for further implementation details regarding these baselines. 


