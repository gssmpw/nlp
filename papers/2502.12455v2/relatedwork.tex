\section{Related Work}
% 通过剪枝方法减少结构冗余
% Pruning represents effective approach to achieving sparse LLMs, which directly reduces model parameters while maintaining essential functionalities. Traditional pruning methods can be broadly categorized into unstructured and structured approaches. The former corresponds to specialized speedup ratios, while the latter is applicable to general acceleration scenarios. Unstructured pruning operates at the finest granularity by directly sparsifying at the weight level. This method allows for the removal of arbitrary weights in the network, typically selecting weights for pruning through optimization problems while minimizing performance degradation\cite{lee2018snip}. For large language models, pruned weights are directly set to zero\cite{frantar2023sparsegpt,sun2023simple}. Although unstructured pruning can achieve higher compression rates, the resulting irregular sparse patterns require specialized hardware and software support for actual acceleration\cite{han2015deep,wen2016learning,filters2016pruning,tang2021manifold}. Structured pruning adopts a coarser-grained approach by removing complete structural units, such as convolution kernels, channels, attention heads, or entire layers \cite{you2019gate,ashkboos2024slicegpt,liu2021group,ma2023llm,men2403shortgpt}. The primary advantage of this method lies in its ability to directly produce regular, narrow model architectures that can achieve model acceleration and size reduction without requiring specialized sparse computation libraries\cite{luo2017thinet,liu2021group,filters2016pruning,nonnenmacher2021sosp}. However, both approaches face a fundamental limitation: they achieve efficiency by permanently removing parameters, which may discard valuable knowledge and lose the ability to adapt computation based on input complexity. This limitation motivates our approach of maintaining all parameters while enabling dynamic, input-dependent sparsity within FFN layers.
Model pruning is an effective approach to achieving sparse LLMs while maintaining model functionality. Pruning methods can be categorized into two main types: unstructured and structured pruning. Unstructured pruning operates at the weight level, allowing for arbitrary weight removal \cite{lee2018snip}. In large language models, pruned weights are set to zero \cite{frantar2023sparsegpt,sun2023simple}. However, this method requires specialized hardware and software support for acceleration\cite{han2015deep,wen2016learning,filters2016pruning,tang2021manifold}. Structured pruning takes a coarser-grained approach by removing complete structural units such as convolution kernels, channels, attention heads, or entire layers \cite{you2019gate,ashkboos2024slicegpt,liu2021group,ma2023llm,men2403shortgpt}. Its main advantage is the ability to directly produce regular, narrow model architectures that can achieve acceleration without specialized sparse computation libraries \cite{luo2017thinet,liu2021group,filters2016pruning,nonnenmacher2021sosp}. However, both approaches face a fundamental limitation: achieving efficiency through permanent parameter removal may discard valuable knowledge and lose the ability to adapt computation based on input complexity.


% 通过MOE方法减少结构冗余
% 近年来，MOE由于其出色的表现和高效的推理性能而被人们广泛采用。传统的MOE方法使用条件计算的方式，在前向传播的过程中选择性地激活某个专家子集，包括激活top1/top2个专家，这种方法可以实现激活参数量比稠密模型更小的情况下，达到更出色的性能，从而客观上达到推理加速的效果。然而事实上，激活固定数量的专家并没有达到推理加速的上限。现有的研究探索根据token难度动态选择激活专家数量的方法，这可以降低低难度token选择专家的数量，从而达到推理加速的效果。上述方法中不同专家的参数量都是相同的，另外也有方法探索异构专家的影响，最新的方法在moe架构中引入了零计算的专家，这进一步减少了专家计算的开销。
% In recent years, the MoE model has gained widespread adoption due to its outstanding performance and efficient inference capabilities. Traditional MOE approaches use conditional computation to selectively activate a subset of experts during the forward propagation process. This often involves activating the top-1 or top-2 experts, which allows for improved performance with fewer activated parameters compared to a dense model, thus accelerating inference \cite{fedus2022switch, DBLP:conf/iclr/LepikhinLXCFHKS21}. However, activating a fixed number of experts does not fully maximize inference acceleration potential. Recent research has focused on dynamically selecting the number of activated experts based on the difficulty of the token, reducing the number of experts for low-difficulty tokens and further enhancing inference speed \cite{DBLP:journals/corr/abs-2403-07652}. While these methods typically involve the same number of experts across all participants, other approaches explore the use of heterogeneous experts \cite{sun2024hunyuan}. A recent innovation introduces zero-computation experts into the MOE architecture, significantly reducing the computational overhead \cite{jin2024moe++}.
In recent years, there has been growing interest in exploring sparse computation in large language models. Mixture-of-Experts (MoE) represents a pioneering approach that demonstrates how sparse activation can effectively balance model capacity and computational efficiency. In MoE architectures, only a subset of FFN modules (experts) are activated for each input token \cite{fedus2022switch, DBLP:conf/iclr/LepikhinLXCFHKS21, DBLP:journals/corr/abs-2403-07652}. This foundational idea of conditional computation has inspired various innovations in expert activation strategies. Some works explore heterogeneous expert architectures \cite{sun2024hunyuan} or introduce zero-computation experts \cite{jin2024moe++} to further optimize computational efficiency. These advances in MoE architectures demonstrate the potential of sparse computation and motivate our exploration of applying similar principles within individual FFN layers.