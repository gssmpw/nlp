% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{array}
\usepackage{subcaption}

% \title{From Dense to Sparse: Progressive Model Sparsification via Dynamic Expert Routing}
% \title{DSMoE: Converting Dense Models to Dynamic Sparse Models via Matrix Blocking and Adaptive Expert Routing}
% \title{Sparsifying LLMs via Dynamic Expert Routing} % TODO 感觉这个标题很容易被误会只是把dense进行upcycling到MoE，可以考虑用gpt-4给你起个名字，类似于：DSMoE: Sparsifying Dense LLM via Knowledge Partition and Dynamic Selection in an MoE-like Manner——done
% \title{DSMoE: Matrix-Partitioned Experts with MoE-style Dynamic Routing for Dense LLMs} 
% \title{DSMoE: Matrix-Partitioned Experts with MoE-style Dynamic Routing for Efficient Dense LLM Inference}
\title{DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}




% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\author{
    Minxuan Lv \textsuperscript{\rm 1,2}\footnotemark[1], Zhenpeng Su\textsuperscript{\rm 1,2,3}\footnotemark[1], Leiyu Pan\textsuperscript{\rm 4},Yizhe Xiong\textsuperscript{\rm 5},Zijia Lin\textsuperscript{\rm 3,5}\footnotemark[2],Hui Chen\textsuperscript{\rm 5}\footnotemark[2],Wei Zhou\textsuperscript{\rm 1,2}\\ \textbf{Jungong Han}\textsuperscript{\rm 5},\textbf{Guiguang Ding}\textsuperscript{\rm 5},\textbf{Cheng Luo}\textsuperscript{\rm 3},\textbf{Di Zhang}\textsuperscript{\rm 3},\textbf{Kun Gai}\textsuperscript{\rm 3},\textbf{Songlin Hu}\textsuperscript{\rm 1,2}\footnotemark[2] \\
    \textsuperscript{\rm 1}Institute of Information Engineering, Chinese Academy of Sciences\\
    \textsuperscript{\rm 2}University of Chinese Academy of Sciences \\
    \textsuperscript{\rm 3}Kuaishou Technology,
    \textsuperscript{\rm 4}Tianjin University,
    \textsuperscript{\rm 5}Tsinghua University \\
    \texttt{\{lvminxuan,husonglin\}@iie.ac.cn}  \quad \texttt{huichen@tsinghua.edu.cn} \\
    \texttt{\{suzhenpeng,linzijia\}@kuaishou.com}\\ 
}




%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}













\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}} %将脚注符号设置为fnsymbol类型，即特殊符号表示
\footnotetext[1]{These authors contributed equally to this work.} %对应脚注[1]
\footnotetext[2]{Corresponding authors.} %对应脚注[2]
\renewcommand{\thefootnote}{\arabic{footnote}}


\begin{abstract}
% As large language models continue to scale, computational costs and resource consumption have emerged as significant challenges. While structured optimization fundamentally reduces the computational overhead of large models, static pruning methods inevitably lead to performance degradation by directly discarding portions of model knowledge. Drawing inspiration from Mixture-of-Experts (MoE) architectures, this paper proposes a method for converting dense models to \textbf{D}ynamic \textbf{S}parse \textbf{M}ixture-\textbf{o}f-\textbf{E}xperts (\textbf{DSMoE}) through matrix blocking of MLP layers. DSMoE implements adaptive expert routing using sigmoid activation functions and straight-through estimators, enabling dynamic computation adjustment based on input complexity. Additionally, we introduce a sparsity loss term to balance model performance and computational efficiency. Extensive experiments demonstrate that under equivalent FLOPs constraints, our method achieves significant improvements in both language modeling perplexity and downstream task performance compared to existing pruning and MoE approaches, particularly excelling in generation tasks. Analysis reveals that DSMoE learns unique layerwise activation patterns, providing new insights for future MoE architecture design.
As large language models continue to scale, computational costs and resource consumption have emerged as significant challenges. While existing sparsification methods like pruning reduce computational overhead, they risk losing model knowledge through parameter removal. This paper proposes DSMoE (\textbf{D}ynamic \textbf{S}parse \textbf{M}ixture-\textbf{o}f-\textbf{E}xperts), a novel approach that achieves sparsification by partitioning pre-trained FFN layers into computational blocks. We implement adaptive expert routing using sigmoid activation and straight-through estimators, enabling tokens to flexibly access different aspects of model knowledge based on input complexity. Additionally, we introduce a sparsity loss term to balance performance and computational efficiency. Extensive experiments on LLaMA models demonstrate that under equivalent computational constraints, DSMoE achieves superior performance compared to existing pruning and MoE approaches across language modeling and downstream tasks, particularly excelling in generation tasks. Analysis reveals that DSMoE learns distinctive layerwise activation patterns, providing new insights for future MoE architecture design.
\end{abstract}



\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{overview_ffn}
% \caption{An overview of DSMoE. The structure shown in the figure is a simplified representation of the transformer backbone. The left portion illustrates how an existing dense model is converted into DSMoE architecture. In the MLP layer, we partition matrices along the intermediate dimension, where portions corresponding to the original matrix multiplication form new expert MLP layers. We have simplified the MLP layer structure here; the MLP layer also includes a gating matrix with dimensions matching the upper matrix, which performs Hadamard multiplication with the upper matrix without affecting our partitioning scheme. The right side shows the traditional MoE structure. Compared to DSMoE, their routing typically selects topK experts, while DSMoE uses sigmoid activation where experts with activation values greater than $\tau$ are activated.}
\caption{The Overview of DSMoE versus Traditional MoE Framework Architectures. The structure shown in the figure is a simplified representation of the transformer backbone. We have simplified the FFN layer structure here; the FFN layer also includes a gating matrix with dimensions matching the upper matrix, which performs Hadamard multiplication with the upper matrix without affecting our partitioning scheme. In the FFN layer, we partition matrices along the intermediate dimension, where portions corresponding to the original matrix multiplication form new expert FFN layers. }
\label{fig:overview}
\end{figure*}






\section{Introduction} % TODO intro里头有个很重要的问题：没有提，sparsify dense LLM是为了干啥？加速？你的目标应该是速度和performance的最佳tradeoff，得提目标，才能知道怎么看你的实验设置是否合理和有效
Large Language Models(LLM) have demonstrated remarkable performance across various downstream tasks\cite{touvron2023llama,dai2022stablemoe,anil2023gemini,biderman2023pythia}. However, as model sizes continue to expand, computational costs and resource consumption grow exponentially. How to improve computational efficiency while maintaining model performance has become a pressing challenge\cite{cheng2024survey}.


% To address this challenge, researchers have proposed various optimization approaches. Quantization methods reduce parameter precision\cite{dettmers2024qlora,shao2023omniquant}; model pruning decreases computation by removing redundant parameters\cite{ashkboos2024slicegpt,ma2023llm,frantar2023sparsegpt}; and Mixture of Experts (MoE) systems employ dynamic expert routing to improve computational efficiency\cite{jiang2024mixtral,dai2024deepseekmoe,liu2024deepseek}. While methods like quantization can optimize model performance from different dimensions, this paper focuses on the optimization potential of the model structure itself. Existing pruning methods reduce computation by statically removing parameters, which results in fixed model capacity and loss of adaptability to inputs of varying complexity, potentially sacrificing important model knowledge. Although MoE methods introduce expert mechanisms to increase model flexibility, current approaches are still limited by fixed expert numbers and inflexible computational resource allocation.
% To address this challenge, researchers have proposed various methods for sparsifying dense models. Traditional pruning methods reduce computation by statically removing parameters\cite{ashkboos2024slicegpt,ma2023llm,frantar2023sparsegpt}, which uniformly reduces model capacity for all inputs, potentially limiting the model's ability to handle complex inputs that require more extensive knowledge access. Mixture of Experts (MoE)  represents a promising approach that maintains model capacity while activating only a small subset of parameters during inference\cite{jiang2024mixtral,dai2024deepseekmoe,liu2024deepseek}. However, converting dense models into MoE architectures while preserving the rich knowledge acquired during pre-training remains a significant research challenge in model optimization.
% At the algorithmic level, approaches to model efficiency optimization generally follow two paradigms: post-training compression and acceleration of dense models, or training of Mixture of Experts (MoE) architectures. While compression methods like pruning achieve efficiency through permanent parameter removal\cite{ashkboos2024slicegpt,ma2023llm,frantar2023sparsegpt}, they may discard valuable knowledge and lack flexibility in handling inputs of varying complexity. Conversely, although effective, MoE approaches—whether trained from scratch\cite{fedus2022switch,dai2024deepseekmoe,liu2024deepseek} or warm-started from dense models\cite{jiang2024mixtral}—cannot reduce the computational cost of the original dense model. Given that the most widely used and effective foundation models still maintain dense architectures (such as LLaMA\cite{touvron2023llama}, Qwen\cite{bai2023qwen}), reducing their computational overhead while minimizing performance degradation becomes particularly crucial.
At the algorithmic level, approaches to model efficiency optimization generally follow two paradigms: post-training compression and acceleration of dense models, or training of Mixture of Experts (MoE) architectures. While compression methods like pruning achieve efficiency through permanent parameter removal\cite{ashkboos2024slicegpt,ma2023llm,frantar2023sparsegpt}, they may discard valuable knowledge and lack flexibility in handling inputs of varying complexity. Conversely, although effective, MoE approaches—whether trained from scratch\cite{fedus2022switch,dai2024deepseekmoe,liu2024deepseek} or warm-started from dense models\cite{jiang2024mixtral}—cannot reduce the computational cost of the original dense model. Given that the most widely used and effective foundation models still maintain dense architectures (such as LLaMA\cite{touvron2023llama}, Qwen\cite{bai2023qwen}), there is a crucial need to optimize these widely-adopted dense models for better efficiency without sacrificing their performance


% To address these limitations, we propose a novel model optimization method. Our approach divides matrices along the intermediate dimension in transformer MLP layers (also known as FFN layers), treating each division as an "expert". Through a learnable sigmoid activation gating mechanism, experts can be dynamically activated based on input complexity, rather than following fixed routing patterns. Specifically, we design a framework for converting dense models into dynamically sparse multi-expert models with selective activation. We maintain consistency between training and inference by introducing threshold-controlled expert activation during training. Simultaneously, we incorporate straight-through estimation to ensure gradient learning from non-activated experts, preventing the "dead expert" problem. We also employ sparsity loss to control the sparsity of dynamic activation. This design enables the model to preserve the fundamental knowledge of the dense model while achieving significant reduction in inference computation.
% To address this challenge, we propose DSMoE, a method that intelligently partitions transformer FFN layers into knowledge-specialized blocks. Each block inherits a portion of the original model's knowledge through matrix partitioning. A learnable routing mechanism dynamically determines which knowledge blocks each token should access, enabling flexible computation allocation while preserving model capabilities. This design allows the model to maintain the fundamental knowledge from pre-training while achieving significant reduction in inference computation.
To address these limitations, we propose DSMoE, which achieves model sparsification by partitioning pre-trained FFN layers into smaller computational blocks. Unlike existing approaches, DSMoE preserves all model parameters, eliminating the risk of knowledge loss, while achieving lower computational overhead than the original dense model through dynamic routing mechanisms. This design maintains the model's complete knowledge while improving computational efficiency.


Extensive experiments conducted on LLaMA-1B and LLaMA-7B models demonstrate encouraging results. Under equivalent computational constraints, our method achieves significant improvements in language modeling perplexity and downstream task performance compared to existing pruning and MoE approaches. Notably superior performance is observed in reasoning and question-answering tasks, particularly in generation tasks.

The main contributions of this work include:

\begin{itemize}
    % \item proposing a novel model optimization framework that enables smooth transition from dense to dynamically sparse models.
    \item proposing a novel approach that enables transition from dense to dynamically sparse models by preserving and partitioning pre-trained knowledge, enabling different tokens to adaptively access varying portions of model knowledge.
    % \item designing an adaptive expert routing mechanism that addresses the fixed expert number limitation in traditional MoE.
    \item validating the method's effectiveness across multiple benchmarks through extensive experimentation, providing new insights for MoE large model optimization.
\end{itemize}



\section{Related Work}
% 通过剪枝方法减少结构冗余
% Pruning represents effective approach to achieving sparse LLMs, which directly reduces model parameters while maintaining essential functionalities. Traditional pruning methods can be broadly categorized into unstructured and structured approaches. The former corresponds to specialized speedup ratios, while the latter is applicable to general acceleration scenarios. Unstructured pruning operates at the finest granularity by directly sparsifying at the weight level. This method allows for the removal of arbitrary weights in the network, typically selecting weights for pruning through optimization problems while minimizing performance degradation\cite{lee2018snip}. For large language models, pruned weights are directly set to zero\cite{frantar2023sparsegpt,sun2023simple}. Although unstructured pruning can achieve higher compression rates, the resulting irregular sparse patterns require specialized hardware and software support for actual acceleration\cite{han2015deep,wen2016learning,filters2016pruning,tang2021manifold}. Structured pruning adopts a coarser-grained approach by removing complete structural units, such as convolution kernels, channels, attention heads, or entire layers \cite{you2019gate,ashkboos2024slicegpt,liu2021group,ma2023llm,men2403shortgpt}. The primary advantage of this method lies in its ability to directly produce regular, narrow model architectures that can achieve model acceleration and size reduction without requiring specialized sparse computation libraries\cite{luo2017thinet,liu2021group,filters2016pruning,nonnenmacher2021sosp}. However, both approaches face a fundamental limitation: they achieve efficiency by permanently removing parameters, which may discard valuable knowledge and lose the ability to adapt computation based on input complexity. This limitation motivates our approach of maintaining all parameters while enabling dynamic, input-dependent sparsity within FFN layers.
Model pruning is an effective approach to achieving sparse LLMs while maintaining model functionality. Pruning methods can be categorized into two main types: unstructured and structured pruning. Unstructured pruning operates at the weight level, allowing for arbitrary weight removal \cite{lee2018snip}. In large language models, pruned weights are set to zero \cite{frantar2023sparsegpt,sun2023simple}. However, this method requires specialized hardware and software support for acceleration\cite{han2015deep,wen2016learning,filters2016pruning,tang2021manifold}. Structured pruning takes a coarser-grained approach by removing complete structural units such as convolution kernels, channels, attention heads, or entire layers \cite{you2019gate,ashkboos2024slicegpt,liu2021group,ma2023llm,men2403shortgpt}. Its main advantage is the ability to directly produce regular, narrow model architectures that can achieve acceleration without specialized sparse computation libraries \cite{luo2017thinet,liu2021group,filters2016pruning,nonnenmacher2021sosp}. However, both approaches face a fundamental limitation: achieving efficiency through permanent parameter removal may discard valuable knowledge and lose the ability to adapt computation based on input complexity.


% 通过MOE方法减少结构冗余
% 近年来，MOE由于其出色的表现和高效的推理性能而被人们广泛采用。传统的MOE方法使用条件计算的方式，在前向传播的过程中选择性地激活某个专家子集，包括激活top1/top2个专家，这种方法可以实现激活参数量比稠密模型更小的情况下，达到更出色的性能，从而客观上达到推理加速的效果。然而事实上，激活固定数量的专家并没有达到推理加速的上限。现有的研究探索根据token难度动态选择激活专家数量的方法，这可以降低低难度token选择专家的数量，从而达到推理加速的效果。上述方法中不同专家的参数量都是相同的，另外也有方法探索异构专家的影响，最新的方法在moe架构中引入了零计算的专家，这进一步减少了专家计算的开销。
% In recent years, the MoE model has gained widespread adoption due to its outstanding performance and efficient inference capabilities. Traditional MOE approaches use conditional computation to selectively activate a subset of experts during the forward propagation process. This often involves activating the top-1 or top-2 experts, which allows for improved performance with fewer activated parameters compared to a dense model, thus accelerating inference \cite{fedus2022switch, DBLP:conf/iclr/LepikhinLXCFHKS21}. However, activating a fixed number of experts does not fully maximize inference acceleration potential. Recent research has focused on dynamically selecting the number of activated experts based on the difficulty of the token, reducing the number of experts for low-difficulty tokens and further enhancing inference speed \cite{DBLP:journals/corr/abs-2403-07652}. While these methods typically involve the same number of experts across all participants, other approaches explore the use of heterogeneous experts \cite{sun2024hunyuan}. A recent innovation introduces zero-computation experts into the MOE architecture, significantly reducing the computational overhead \cite{jin2024moe++}.
In recent years, there has been growing interest in exploring sparse computation in large language models. Mixture-of-Experts (MoE) represents a pioneering approach that demonstrates how sparse activation can effectively balance model capacity and computational efficiency. In MoE architectures, only a subset of FFN modules (experts) are activated for each input token \cite{fedus2022switch, DBLP:conf/iclr/LepikhinLXCFHKS21, DBLP:journals/corr/abs-2403-07652}. This foundational idea of conditional computation has inspired various innovations in expert activation strategies. Some works explore heterogeneous expert architectures \cite{sun2024hunyuan} or introduce zero-computation experts \cite{jin2024moe++} to further optimize computational efficiency. These advances in MoE architectures demonstrate the potential of sparse computation and motivate our exploration of applying similar principles within individual FFN layers.



\section{Background}
For simplicity, we focus on the prevalent architecture of generative large language models while maintaining a concise mathematical formulation. In autoregressive generation tasks, given a sequence $X = (x_1, x_2, ..., x_T)$ of length $T$, the model iteratively produces a probability distribution over the vocabulary for each position conditioned on preceding tokens. This process can be formulated as:

% \begin{equation}
% P_{\cdot,t} = \text{softmax}(EH^L_{\cdot,t}) % TODO 这里h是不是少了上标L——done
% \end{equation}

% \begin{equation}
% H^L = \text{Transformer}(x_1, x_2, ..., x_{T-1})    % TODO 这里h_L是不是应该改成h^L，感觉跟下文的描述有冲突，而且公式1和公式2之间没有关系也很奇怪，加上上标之后，就好了——done
% \end{equation}

\begin{equation}
\begin{split}
P_{\cdot,t} = \text{softmax}(EH^L_{\cdot,t}) \quad \quad \quad \quad \quad \; \; \, \\
H^L = \text{Transformer}(x_1, x_2, ..., x_{T-1})
\end{split}
\end{equation}

Here, $L$ denotes the number of layers in the Transformer architecture. For any position $t$, $P_{\cdot,t}$ represents the probability distribution over the vocabulary, derived from the $t$-th column of the hidden state matrix $h^L$. Specifically, $H^L = [h_1^L, h_2^L, ..., h_{T-1}^L]$ contains the hidden representations from the final layer, where $h_t^L$ is the contextual embedding at position $t$. The probability of the ground-truth token $x_{t+1}$ is denoted as $P_{x_{t+1},t}$ in the distribution $P{\cdot,t}$. The transformation from hidden states to probability distributions is achieved through a linear projection matrix $E$, followed by a softmax operation.



In typical scenarios, we employ cross-entropy loss for autoregressive learning, which can be expressed as:
\begin{equation}
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^{T-1} \log P(x_{t+1}|x_{\leq t})
\end{equation}

The Transformer architecture consists of multiple layer-wise submodules, where each layer comprises a self-attention module and a Feed-Forward Network (FFN) module. The simplified mathematical formulation can be expressed as:
\begin{equation}
\hat{h^l_t} = \text{Attn}([h^{l-1}_1,h^{l-1}_2,...,h^{l-1}_t]) 
\end{equation}
\begin{equation}
h^l_t = \text{FFN}(\hat{h^l_t})
\end{equation}

% FFN typically consists of MLP layers with three essential matrices: the up-projection matrix $\mathbf{U}_{\text{up}}$, the down-projection matrix $\mathbf{V}_{\text{down}}$, and the gate matrix $\mathbf{W}_{\text{gate}}$. The up-projection matrix transforms the input to a higher dimensional space for richer feature representation, the down-projection matrix compresses the information back to the original dimension, and the gate matrix controls information flow through adaptive feature weighting. The FFN output is computed through the following operation: % TODO 这一段太specific了，这种设置，只是因为用了swiGLU的激活函数，如果不是，也不是公式6这种表示方式。上面的描述可以加上，“以最流行的swiGLU激活方式为例，***通常由三部分组成”——done
FFN modules typically consist of two matrix transformations with a non-linear activation function. In modern language models, the most prevalent FFN implementation uses SwiGLU activation, which involves three essential matrices: the up-projection matrix $\mathbf{U}_{\text{up}}$, the down-projection matrix $\mathbf{V}_{\text{down}}$, and the gate matrix $\mathbf{W}_{\text{gate}}$. The up-projection matrix transforms the input to a higher dimensional space for richer feature representation, the down-projection matrix compresses the information back to the original dimension, and the gate matrix controls information flow through adaptive feature weighting. The FFN output is computed through the following operation:


\begin{equation}
\label{eq:MLP}
h^l_t=(act(\hat{h^l_t}W_{gate}) \odot (\hat{h^l_t}U_{up})) V_{down}
\end{equation}

In this formulation, $\text{act}(\cdot)$ represents the activation function and $\odot$ denotes Hadamard product.

\section{Method}
% Although our method is termed DSMoE, its training approach differs from traditional methods like Switch Transformer\cite{fedus2022switch} and DeepSeeKMoE\cite{dai2024deepseekmoe}. DSMoE is divided into three modules and does not incorporate mechanisms for balancing expert token distribution.

Although our method is termed DSMoE, its training approach differs from traditional MoE methods such as Switch Transformer \cite{fedus2022switch} and DeepSeeKMoE \cite{dai2024deepseekmoe}. Our objective is to achieve sparsity through partitioning pre-trained models, where each expert inherits a distinct portion of the original model's knowledge. Our approach is based on the principle that the model should learn to selectively utilize different aspects of pre-trained knowledge based on input complexity, rather than routing tokens among independently trained experts. To implement this insight, we present our method in three modules.

\subsection{FFN Partitioning}
% The key insight behind our matrix partitioning approach is that FFN layer computations can be mathematically decomposed along the intermediate dimension while preserving the original transformation properties. As shown in Equation \ref{eq:MLP}, we can partition the matrices $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{W}$ into $n$ groups along the intermediate dimension, where each group can be viewed as an "expert" that naturally inherits a portion of the original transformation capabilities. The final output, computed as the summation of all expert outputs, is mathematically equivalent to the original MLP formulation:
% The widespread adoption of MoE architectures inspires our exploration of sparsity in FFN layers, suggesting that different parts of computation can be dynamically activated based on input patterns. Previous work has further revealed that FFN layers essentially operate as key-value memories, where different portions of the layer specialize in detecting and processing distinct input patterns\cite{geva2020transformer}. Building on these insights, we propose to directly partition pre-trained FFN layers. As shown in Equation \ref{eq:MLP}, we can partition the matrices $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{W}$ into $n$ groups along the intermediate dimension, where each group can be viewed as an "expert" that naturally inherits a portion of the original transformation capabilities. The final output, computed as the summation of all expert outputs, is mathematically equivalent to the original MLP formulation: % TODO “mathematically equivalent”非常严格，这个要做好验证，确保数学上等价，才可以这么说——done
The widespread adoption of MoE architectures inspires our exploration of sparsity in FFN layers, suggesting that different parts of computation can be dynamically activated based on input patterns. Previous work has further revealed that FFN layers essentially operate as key-value memories, where different portions of the layer specialize in detecting and processing distinct input patterns\cite{geva2020transformer}. Building on these insights, we propose to directly partition pre-trained FFN layers. As shown in Equation \ref{eq:MLP}, we partition the matrices $\mathbf{U}$, $\mathbf{V}$, and $\mathbf{W}$ into $n$ groups along the intermediate dimension, where each group can be viewed as an ``expert" that inherits a portion of the original transformation capabilities. When summing all expert outputs, this partitioned form yields identical results to the original FFN computation:


\begin{equation}
\begin{split}
h^l_t = (act(\hat{h^l_t}\begin{bmatrix}
W_1 & \cdots & W_n
\end{bmatrix}) \odot \\
(\hat{h^l_t}\begin{bmatrix}
U_1 & \cdots & U_n
\end{bmatrix})) \begin{bmatrix}
V_1  \\
\vdots  \\
V_n 
\end{bmatrix} \\
=(act(\hat{h^l_t}W_1) \odot \hat{h^l_t}U_1) V_1 + \cdots \\
+ (act(\hat{h^l_t}W_n) \odot \hat{h^l_t}U_n) V_n
\end{split}
\end{equation}



We can structurally split the original FFN layer matrix into multiple small FFN matrices. To enable dynamic expert activation based on input, we employ a gating network that determines which experts should be activated. The expert's output is propagated to the subsequent layer only when the corresponding gating activation value exceeds a certain threshold $\tau$. This can be formulated as:

\begin{equation}
\begin{split}
o_i = (act(\hat{h^l_t}W_i) \odot \hat{h^l_t}U_i) V_i \\
h^l_t=\sum^n_{i=1} o_i * G(\sigma(\hat{h^l_t}\mathbf{Y}_i)) \\
G(x) = \begin{cases} x & \text{if } x > \tau \\ 0 & \text{others }  \end{cases}
\end{split}
\label{eq:G_function}
\end{equation}
where $\mathbf{Y} = [\mathbf{Y}_1,\dots,\mathbf{Y}_n] \in \mathbb{R}^{d \times n}$ represents the parameters of the gating network, and $\sigma(\cdot)$ denotes the sigmoid activation function. 

To maintain consistent output norm regardless of the number of active experts, similar to dropout, we scale $h_t^l$ by the ratio of total expert count $n$ to the number of activated experts. This normalization can be expressed as:
\begin{equation}
h_t^l = \frac{n \cdot h_t^l}{\sum_{i=1}^n \mathbb{I}[\sigma(\hat{h}_t^l\mathbf{Y}_k) > \tau]}
\end{equation} % TODO 这里要一两句话说明一下如果没有激活的（sigmoid结果都小于公式8里头的tau，这一项相当于分母是0，应该怎么处理，其实如果都不激活，等价于h_t^l直接就是0了，这个可能也会引起concern，需要说明一下）


\begin{table*}[!t]
\centering
% \scalebox{0.77}{
% \begin{tabular}
% {llccrc}
% \toprule
% Model & Configuration & Params & Activated Params & FLOPs& PPL ($\downarrow$) \\
% \midrule
% LLaMA-1B& d=2048, D=8192 & 1.24B & 1.24B & 2.53 T & 5.67  \\
% LLaMA-7B& d=4096, D=11008 & 6.74B & 6.74B & 13.53 T & 3.40 \\
% \midrule
% \textit{LLaMA-1B} \\
% \midrule 
% LLM-Pruner-channel & d=1215, D=8192 & 889M & 889M & 1.50 T & 7.51 \\
% LLM-Pruner-block & d=2048, D=3896.4  & 735M & 735M & 1.50 T & 7.46 \\
% SparseGPT & d=2048, D=8192 & 1.24B & 1.24B & 2.53 T & 9.82 \\
% MoE& d=2048, D=1024 \ \ \ $\times 8$, topK=3  & 1.24B & 736M & 1.50 T & 7.45 \\
% DSMoE(ours) & d=2048, D=1024 \ \ \ $\times 8$, dynamic & 1.24B & 735M & 1.50 T & \textbf{7.41} \\
% \midrule
% \textit{LLaMA-7B} \\
% \midrule
% LLM-Pruner-channel & d=2401, \ \ D=11008 & 3.95B & 3.95B & 7.93 T & 4.01 \\
% LLM-Pruner-block & d=11008, D=6256.5  & 3.94B & 3.94B & 7.93 T & 4.01 \\
% SparseGPT & d=4096, \ \  D=11008 & 6.74B & 6.74B & 13.53 T & 3.96 \\
% MoE& d=2048, \ \  D=1376 \ \ \ $\times 8$, topK=3 & 6.74B & 3.98B & 7.99 T & 4.12  \\
% DSMoE(ours) & d=2048, \ \ D=1376 \ \ \ $\times 8$, dynamic & 6.74B & 3.93B & 7.91 T & \textbf{3.91} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Results of perplexity (PPL) across different language models. The \textbf{bold} values indicate the best-performing method among various acceleration approaches. The Configuration column describes the specific model architecture, where $d$ represents the hidden dimension, D denotes the expansion dimension in MLP layers (for LLM-Pruner-block method, this represents the average value), $\times$ n indicates the use of n parallel MLP layers, and topK specifies the number of activated experts per layer in the MoE architecture. The Params column shows the total number of model parameters, while Activated Params indicates the average number of parameters activated during inference. FLOPs (Floating Point Operations) represents the average number of floating-point operations per sample. The term "dynamic" indicates that the number of activated MLP layers is variable.}
\scalebox{0.77}{
\begin{tabular}
{llccrc}
\toprule
Model & Configuration & Params & Activated Params & FLOPs& PPL ($\downarrow$) \\
\midrule
LLaMA-1B& d=2048, D=8192 & 1.24B & 1.24B & 2.53 T & 5.67  \\
LLaMA-7B& d=4096, D=11008 & 6.74B & 6.74B & 13.53 T & 3.40 \\
\midrule
\textit{LLaMA-1B} \\
\midrule 
LLM-Pruner-channel & d=1215, D=8192 & 889M & 889M & 1.50 T & 7.51 \\
LLM-Pruner-block & d=2048, D=3896.4  & 735M & 735M & 1.50 T & 7.46 \\
SparseGPT & d=2048, D=8192 & 1.24B & 1.24B & 2.53 T & 9.82 \\
MoE& d=2048, D=1024 \ \ \ $\times 8$, topK=3  & 1.24B & 736M & 1.50 T & 7.45 \\
DSMoE(ours) & d=2048, D=1024 \ \ \ $\times 8$ & 1.24B & 735M & 1.50 T & \textbf{7.41} \\
\midrule
\textit{LLaMA-7B} \\
\midrule
LLM-Pruner-channel & d=2401, \ \ D=11008 & 3.95B & 3.95B & 7.93 T & 4.01 \\
LLM-Pruner-block & d=11008, D=6256.5  & 3.94B & 3.94B & 7.93 T & 4.01 \\
SparseGPT & d=4096, \ \  D=11008 & 6.74B & 6.74B & 13.53 T & 3.96 \\
MoE& d=2048, \ \  D=1376 \ \ \ $\times 8$, topK=3 & 6.74B & 3.98B & 7.99 T & 4.12  \\
DSMoE(ours) & d=2048, \ \ D=1376 \ \ \ $\times 8$ & 6.74B & 3.93B & 7.91 T & \textbf{3.91} \\
\bottomrule
\end{tabular}
}
\caption{Results of perplexity (PPL) across different language models. The \textbf{bold} values indicate the best-performing method among various acceleration approaches. The Configuration column describes the specific model architecture, where $d$ represents the hidden dimension, D denotes the expansion dimension in FFN layers (for LLM-Pruner-block method, this represents the average value), $\times$ n indicates the use of n parallel FFN layers, and topK specifies the number of activated experts per layer in the MoE architecture. The Params column shows the total number of model parameters, while Activated Params indicates the average number of parameters activated during inference. FLOPs (Floating Point Operations) represents the average number of floating-point operations per sample.}
\label{tab:PPL_results}
\end{table*}


\subsection{Straight-Through Estimator}
% One critical challenge in converting dense models to sparse ones is maintaining the learning capability of all experts. Unlike MoE models that train experts from scratch, our experts inherit knowledge that we wish to preserve and adapt. The straight-through estimator enables learning for non-activated experts while maintaining thresholded activation during inference, preventing the "dead expert" problem that would lead to knowledge loss. However, when directly using the gating function $G(x)$, experts with activation values below the threshold receive zero gradients during backpropagation:
A key challenge in converting dense models to sparse ones is maintaining the learning capability of all experts. During the forward pass, experts with activation values below the threshold $\tau$ do not participate in computation, as defined by the gating function $G(x)$ in Equation \ref{eq:G_function}. However, this thresholding operation creates a critical problem during backpropagation - experts that are not activated receive zero gradients:
\begin{equation}
\begin{split}
    \frac{\partial h_t^l}{\partial \mathbf{V}_i} = \frac{\partial h_t^l}{\partial \mathbf{W}_i} = \frac{\partial h_t^l}{\partial \mathbf{U}_i} = \\ 
    \frac{\partial h_t^l}{\partial \mathbf{Y}_i} = \mathbf{0},   \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) \leq \tau
\end{split}
\end{equation}

% In this scenario, experts and their corresponding gating matrices would not receive updates, effectively causing these experts to become "dead". Since the routers of DSMoE are randomly initialized (to avoid the "rich-get-richer" phenomenon), we employ the straight-through estimator technique. During training, while outputs from experts below the threshold do not participate in subsequent numerical computations, we maintain their gradients:
This gradient blocking prevents non-activated experts from receiving training signals, leading to a ``dead expert" problem where these experts become permanently inactive. Unlike traditional MoE models that train experts from scratch, our experts inherit pre-trained knowledge that we wish to preserve and adapt. To address this issue, we employ the straight-through estimator technique, which allows gradient flow through non-activated experts while maintaining thresholded activation during the forward pass:

\begin{equation}
S(x) = sg(G(x)) + x - sg(x)
\end{equation}
\begin{equation}
    h_t^l = \sum_{i=1}^n o_i \cdot S(\sigma(\hat{h}_t^l\mathbf{Y}_k))
\label{eq:S_function}
\end{equation}
where the operator ``$sg(\cdot)$" is the ``\texttt{stop gradient}" operator to prevent gradient back propagation. The partial derivatives for experts and their gates below the threshold are as follows. Let:

% \begin{equation}
%     \adjustbox{scale=0.7}{$
%     \frac{\partial h_t^l}{\partial \mathbf{V}_i} = \begin{cases}
%     (\text{act}(\hat{h}_t^l\mathbf{W}_i) \odot \hat{h}_t^l\mathbf{U}_i)^\top \cdot \sigma(\hat{h}_t^l\mathbf{Y}_i) & \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) > \tau \\
%     \mathbf{0} & \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) \leq \tau
%     \end{cases}
%     $}
% \end{equation}

% \begin{equation}
%     \adjustbox{scale=0.6}{$
%     \frac{\partial h_t^l}{\partial \mathbf{W}_i} = \begin{cases}
% (\hat{h}_t^l)^\top \odot \text{act}'(\hat{h}_t^l\mathbf{W}_i) \cdot ((\hat{h}_t^l\mathbf{U}_i \odot \mathbf{V}_i) \cdot \sigma(\hat{h}_t^l\mathbf{Y}_i)) & \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) > \tau \\
% \mathbf{0} & \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) \leq \tau
% \end{cases}
%     $}
% \end{equation}

% \begin{equation}
%     \adjustbox{scale=0.7}{$
%     \frac{\partial h_t^l}{\partial \mathbf{U}_i} = \begin{cases}
% (\hat{h}_t^l)^\top \cdot (\text{act}(\hat{h}_t^l\mathbf{W}_i) \odot \mathbf{V}_i \cdot \sigma(\hat{h}_t^l\mathbf{Y}_i)) & \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) > \tau \\
% \mathbf{0} & \text{if } \sigma(\hat{h}_t^l\mathbf{Y}_i) \leq \tau
% \end{cases}
%     $}
% \end{equation}

% \begin{equation}
%     \frac{\partial h_t^l}{\partial \mathbf{Y}_i} = (\hat{h}_t^l)^\top \cdot (o_i \cdot \sigma'(\hat{h}_t^l\mathbf{Y}_i))
% \end{equation}


% \begin{itemize}
%     \item $a_i = \text{act}(\hat{h}_t^l\mathbf{W}_i)$
%     \item $a'_i = \text{act}'(\hat{h}_t^l\mathbf{W}_i)$
%     \item $g_i = \sigma(\hat{h}_t^l\mathbf{Y}_i)$
%     \item $u_i=\hat{h}_t^l\mathbf{U}_i$ 
% \end{itemize}

\begin{equation}
\begin{aligned}
a_i = \text{act}(\hat{h}_t^l\mathbf{W}_i) \\[5pt]
a'_i = \text{act}'(\hat{h}_t^l\mathbf{W}_i) \\[5pt]
g_i = \sigma(\hat{h}_t^l\mathbf{Y}_i) \\[5pt]
u_i=\hat{h}_t^l\mathbf{U}_i
\end{aligned}
\end{equation}

The gradients for expert parameters and their gates can be derived as:

\begin{equation}
\adjustbox{scale=1}{$
\frac{\partial h_t^l}{\partial \mathbf{V}_i} = \begin{cases}
(a_i \odot u_i)^\top \cdot g_i & \text{if } g_i > \tau \\
\mathbf{0} & \text{if } g_i \leq \tau
\end{cases}
$}
\end{equation}
\begin{equation}
\adjustbox{scale=0.85}{$
\frac{\partial h_t^l}{\partial \mathbf{W}_i} = \begin{cases}
(\hat{h}_t^l)^\top \odot a'_i \cdot ((u_i \odot \mathbf{V}_i) \cdot g_i) & \text{if } g_i > \tau \\
\mathbf{0} & \text{if } g_i \leq \tau
\end{cases}
$}
\end{equation}
\begin{equation}
\adjustbox{scale=1}{$
\frac{\partial h_t^l}{\partial \mathbf{U}_i} = \begin{cases}
(\hat{h}_t^l)^\top \cdot (a_i \odot \mathbf{V}_i \cdot g_i) & \text{if } g_i > \tau \\
\mathbf{0} & \text{if } g_i \leq \tau
\end{cases}
$}
\end{equation}
\begin{equation}
\frac{\partial h_t^l}{\partial \mathbf{Y}_i} = (\hat{h}_t^l)^\top \cdot (o_i \cdot \sigma'(\hat{h}_t^l\mathbf{Y}_i))
\end{equation}


% Given that $\sigma'(\hat{h}_t^l\mathbf{Y}_i) > 0$, we can interpret the gradient dynamics as follows: regardless of whether expert $i$ is currently activated, if it produces meaningful output $o_i$ for the current input $(\hat{h}_t^l)^\top$, the gradients will drive $\mathbf{Y}_i$ to increase along that direction. This adaptive mechanism ensures that the expert becomes more likely to be activated when encountering similar input patterns in future iterations.
The gradient dynamics reveal an important property: since $\sigma'(\hat{h}_t^l\mathbf{Y}_i) > 0$, an expert that produces meaningful output $o_i$ for an input $(\hat{h}_t^l)^\top$ will receive gradients that increase its activation probability for similar inputs in future iterations, regardless of its current activation status. This adaptive mechanism ensures that experts can learn to specialize in processing specific input patterns while maintaining their inherited knowledge from pre-training.





\subsection{Sparse Loss}
% As we partition the original matrix and assign activation scores, the model naturally tends to activate all experts during training since the complete knowledge of the pre-trained dense model is only accessible when all experts are active. However, this contradicts our objective. Our goal is to encourage the model to learn sparse activation patterns, thereby reducing MLP computations during inference through selective expert activation. To achieve this, we introduce a sparse loss term with a tunable hyperparameter $\lambda$ to control the sparsity level:

Since our experts inherit from a dense model, the model naturally tends to activate all experts to access complete knowledge. However, this conflicts with our goal of sparse computation. We introduce a sparsity loss term that creates an adversarial effect with expert gate gradients, encouraging the model to learn which knowledge is truly necessary for different inputs. 

\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \mathcal{L}_{\text{sparse}}
\end{equation}
where $\mathcal{L}_{\text{sparse}}$ denotes the sparsity loss term, which we abbreviate as $\mathcal{L}{\text{s}}$ in subsequent equations. The hyperparameter $\lambda$ controls the strength of sparsity regularization, with larger values encouraging sparser activation patterns.

\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{LM}} + \frac{\lambda}{LN}\sum_{l=1}^L\sum_{n=1}^N\mathcal{L}_s(G(\sigma(\hat{h}^l_t\mathbf{Y}_n)))
\end{equation}

We employ $L1$ norm as the sparsity function $\mathcal{L}_s$. Given that our activation function $\sigma(x) > 0$, our final loss function becomes:

\begin{equation}
\mathcal{L} = \mathcal{L}_{LM} + \frac{\lambda}{LN}\sum_{l=1}^L\sum_{n=1}^N G(\sigma(\hat{h}^l_t\mathbf{Y}_n))
\end{equation}

The gradients introduced by this sparse loss term create an adversarial effect with the gate gradients, encouraging the model to actively suppress the output of less important experts across different layers.

It is worth noting that our approach differs fundamentally from the MoE framework and therefore does not require auxiliary load balancing losses. While load balancing losses in MoE aim to ensure uniform training across experts, our objective is solely focused on learning sparse activation patterns. Furthermore, unlike MoE which typically enforces a fixed number of active experts, our method allows for flexible activation patterns determined by the learned gating mechanism.





\section{Experiments}


\begin{table*}[!t]
\centering
\scalebox{0.70}{
\begin{tabular}{lp{1.40cm}<{\centering}p{1.40cm}<{\centering}p{1.40cm}<{\centering}p{1.30cm}<{\centering}p{1.30cm}<{\centering}p{1.40cm}<{\centering}p{1.30cm}<{\centering}p{1.40cm}<{\centering}p{1.40cm}<{\centering}p{1.40cm}<{\centering}p{1.40cm}<{\centering}}
\toprule
Model & Hellaswag & LAMBADA & PIQA & SIQA & StoryCloze & Wino & GSM8K & TriviaQA & WebQs & NatrualQs \\
\midrule
LLaMA-1B & 64.09 & 61.05 & 75.51 & 42.47 & 72.58 & 60.85 & 4.85 & 12.52 & 36.08 & 22.49 \\
LLaMA-7B & 76.39 & 72.34 & 79.05 & 44.67 & 79.15 & 70.87 & 14.70 & 26.28 & 61.89 & 32.82 \\
\midrule
\textit{LLaMA-1B} \\
\midrule
LLM-Pruner-channel & 53.44 & 45.04 & 71.43 & 40.94 & 68.67 & \textbf{58.45} & 1.44 & 6.98 & 17.46 & 14.56 \\
LLM-Pruner-block & 51.05 & 46.28 & 71.71 & 41.04 & 68.62 & 56.27 & 1.36 & 7.28 & 18.46 & 14.56 \\
SparseGPT & \textbf{54.01} & \textbf{56.49} & 71.10 & 40.68 & 68.05 & 57.30 & 1.51 & 5.29 & 14.44 & 11.61 \\
MoE & 49.06 & 44.84 & 70.02 & 41.05 & 65.47 & 55.64 & 1.62 & 5.76 & 13.49 & 11.27 \\
DSMoE(ours) & 50.92 & 48.12 & \textbf{72.36} & \textbf{41.14} & \textbf{68.78} & 56.35 & \textbf{1.67} & \textbf{8.17} & \textbf{25.52} & \textbf{18.21} \\
\midrule
\textit{LLaMA-7B} \\
\midrule
LLM-Pruner-channel & 66.41 & 61.63 & 74.97 & 43.19 & 75.30 & 66.85 & 4.85 & 12.63 & 36.02 & 20.57 \\
LLM-Pruner-block & 67.93 & 62.02 & 76.22 & 44.26 & 75.46 & 63.53 & 1.81 & 12.96 & 38.77 & 21.65 \\
SparseGPT & \textbf{73.60} & 67.43 & 77.36 & 44.21 & \textbf{76.37} & \textbf{70.48} & \textbf{8.33} & 17.61 & 47.83 & 24.90 \\
MoE & 63.89 & 60.49 & 74.10 & 43.29 & 72.90 & 61.17 & 3.26 & 11.58 & 31.25 & 19.09 \\
DSMoE(ours) & 70.22 & \textbf{67.61} & \textbf{78.12} & \textbf{44.31} & \textbf{76.37} & 66.77 & 6.41 & \textbf{22.04} & \textbf{57.94} & \textbf{29.92} \\
\bottomrule
\end{tabular}
}
\caption{Performances of language models on downstream tasks. The best score is marked in \textbf{bold}.}
\label{tab:benchmark_score}
\end{table*}


\subsection{Dataset}

% 我们收集了多个领域的数据集对模型进行预训练。在通用领域，我们收集了Fineweb-edu数据集，它是从fineweb数据集中筛选出的高质量的具有教育意义的网页。在数学和代码领域，我们分别选取了OpenWebmath和StarCoder数据集。OpenWebMath是从网页数据中过滤出的高质量数学文本数据。StarCoder数据集包含了种类丰富的代码数据，并且已经被证明能够基于该数据集能够预训练出表现良好的代码模型。另外，研究证明，添加合成数据有利于模型的预训练，所以我们还引入了Cosmopedia数据集。
We gathered datasets from various domains to continually pre-train the base model. For the general domain, we used the Fineweb-edu dataset, which consists of high-quality educational web pages filtered from the Fineweb dataset \cite{penedo2024fineweb}. In the math and coding domains, we selected the OpenWebMath \cite{DBLP:conf/iclr/PasterSAB24} and StarCoder \cite{DBLP:journals/tmlr/LiAZMKMMALCLZZW23} datasets respectively. The OpenWebMath dataset contains high-quality mathematical text data extracted from web pages, while the StarCoder dataset offers a diverse range of code data and has been demonstrated to effectively pre-train well-behaved code models. Furthermore, it has been demonstrated that  incorporating synthetic data enhances model pre-training performance \cite{abdin2024phi}. Therefore, we introduced the Cosmopedia dataset to leverage this advantage\cite{benallal2024cosmopedia}.

% 我们对不同领域的数据集进行混合。由于计算资源的限制，我们将训练数据总量定为10B tokens。最后，我们分别采用LLama2和Llama3的tokenzier对数据进行分词，并限制样本最大长度为1024。
Furthermore, we mixed datasets from different domains. Due to computational resource limitations, we set the total amount of training data to 10 billion tokens. % TODO 这里有个concern，是否你的baseline也都是10B 进行了continue pretraining，如果不是，会被质疑实验不solid，如果是，那需要一句话说明一下。改完之后，如果空间不太够，可以把figure里头的标题稍微精简一下
Finally, we used the tokenizers from LLaMA to segment the data, limiting the maximum sample length to 1024 tokens for each. We randomly sampled 5,000 non-overlapping instances from each dataset as the validation set, ensuring no intersection with the training set.

\subsection{Experimental Setup}
We evaluate DSMoE on two pre-trained models of different scales: Llama-7B\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}} and Llama-1B\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-1B}}. For our method's hyperparameters, we simply set the activation threshold $\tau=0.5$ and the sparsity regularization coefficient $\lambda=1.0$.

We compare our approach with several baselines: the channel-wise and block-wise methods from LLM-Pruner (a structured pruning approach), and SparseGPT (an unstructured pruning method). To ensure fair comparison, we first measure the FLOPs of our trained model, then estimate the pruning ratio for baseline methods to maintain a slightly higher FLOPs than our method. The FLOPs metric directly corresponds to the number of parameters involved in computation, providing a standardized measure of computational efficiency.

% Additionally, we explore the effectiveness of warm-starting a dense model into an MoE architecture to investigate whether the MoE framework better accommodates the warm-starting paradigm.

% Additionally, we explore the effectiveness of warm-starting a dense model into an MoE architecture through identical MLP partitioning to investigate whether the MoE framework better accommodates the warm-starting paradigm.% TODO 这里identical是指copy整个FFN？这个是传统dense2MoE的做法，可以直接写copy，更加清楚，否则有点搞不明白这个的意思——done
Additionally, we explore an alternative approach by applying the same FFN partitioning scheme but training it as a traditional MoE architecture (with fixed expert selection and standard MoE training objectives) to investigate whether the conventional MoE framework better accommodates the warm-starting paradigm.


\subsection{Main Results}





We first present the model's perplexity on the validation set. % TODO 这里需要说明一下validate set是啥？前面没讲 —— done
Following previous work\cite{touvron2023llama,brown2020language,su2024cartesianmoe,dai2024deepseekmoe}, we then evaluate the model's performance on downstream benchmarks, which includes zero-shot accuracy testing on HellaSwag\cite{zellers2019hellaswag}, LAMBADA\cite{paperno2016lambada}, SIQA\cite{sap2019socialiqa}, PIQA\cite{bisk2020piqa}, StoryCloze\cite{mostafazadeh2016corpus}, and Winogrande\cite{sakaguchi2021winogrande}. Additionally, we conduct 5-shot evaluation measuring exact match performance on TriviaQA\cite{joshi2017triviaqa}, WebQuestions (WebQs)\cite{berant2013semantic}, GSM8K\cite{cobbe2021training}, and Natural Questions (NaturalQs)\cite{kwiatkowski2019natural}.


\subsubsection{Perplexity Results}
% Table \ref{tab:PPL_results} presents the perplexity results of the baseline dense model and its pruned, sparsified variants. The results demonstrate that DSMoE consistently outperforms baseline models under equivalent activation constraints. Since SparseGPT acceleration requires specific pruning ratios and hardware support, we conducted our comparative analysis only on models with equivalent parameter pruning levels. Our experimental results indicate that DSMoE achieves superior efficiency compared to static parameter pruning. Furthermore, DSMoE exhibits better performance than fixed-activation methods like MoE, which can be attributed to the fact that knowledge from all experts contributes to the model's learning process, enabling it to develop the ability to flexibly select activations based on input. Additionally, DSMoE appears to have learned certain feature processing capabilities, such as more flexible activation of expert numbers across different layers—a characteristic absent in MoE approaches. We will examine these aspects in detail in the analysis section.
Table \ref{tab:PPL_results} presents the perplexity results of the baseline dense model and its pruned, sparsified variants. The results demonstrate that DSMoE consistently outperforms baseline models under equivalent activation constraints. Since SparseGPT acceleration requires specific pruning ratios and hardware support, we conducted our comparative analysis only on models with equivalent parameter pruning levels. Our experimental results indicate that DSMoE achieves superior efficiency compared to static parameter pruning. Furthermore, DSMoE exhibits better performance than fixed-activation methods like MoE, which can be attributed to the fact that knowledge from all experts contributes to the model's learning process, enabling it to develop the ability to flexibly select activations based on input. Additionally, DSMoE exhibits distinctive feature processing capabilities, learning layer-specific activation patterns that naturally emerge from the input complexity. We will examine these emergent patterns in detail in the analysis section.



In conclusion, DSMoE demonstrates consistent superiority across models of two different scales, highlighting its robust advantages.


\subsubsection{Benchmark Results}


% TODO 内容太短，需要补充。对于更大模型来说，这种优势倍进一步放大
% Table \ref{tab:benchmark_score} shows the benchmark performance of the dense model and its pruned, sparsified variants. We observe that our approach demonstrates substantial improvements over existing sparsification methods across the majority of evaluation metrics. Particularly on generative reasoning and question-answering benchmarks (GSM8K, TriviaQA, WebQuestions, and Natural Questions), it demonstrates superior performance compared to other baseline methods.

Table \ref{tab:benchmark_score} presents the benchmark performance of various pruning methods, traditional MoE approaches, and DSMoE. DSMoE achieved the best performance in 7 out of 10 benchmarks for both LLaMA-1B and LLaMA-7B model architectures, demonstrating superior effectiveness over existing sparsification methods across most evaluation metrics.

Specifically, DSMoE exhibited excellent performance on inference tasks (i.e., the first 6 benchmarks), achieving the best results on PIQA, SIQA, and StoryCloze test sets. While not achieving top performance on Hellaswag, LAMBADA, and Wino test sets, DSMoE still ranked among the leading models. For generation tasks (i.e., the last 4 benchmarks), DSMoE demonstrated remarkable effectiveness. Apart from slightly lower performance on GSM8K with LLaMA-7B compared to SparseGPT, it significantly outperformed other sparse methods on all other test sets, with performance only a few points below the dense model. These results highlight DSMoE's potential, particularly in generation tasks.

Furthermore, we observed that the performance gap between DSMoE and other sparse approaches was more pronounced in LLaMA-7B compared to LLaMA-1B. This may be attributed to greater model redundancy at larger parameter scales, enabling DSMoE to more effectively prune unnecessary information. This observation suggests the potential scalability of DSMoE to models with larger parameter counts.

\section{Analyses}
\subsection{Ablation Study: Removing Straight-Through Estimator}
To validate the necessity of the straight-through estimator mechanism in DSMoE, we conduct an ablation study by removing this component. Specifically, instead of using Equation (\ref{eq:S_function}) for training, we employ Equation (\ref{eq:G_function}). We perform this comparative analysis on the LLaMA-1B model.

\begin{table}[!t]
\centering
\scalebox{0.70}{
\begin{tabular}{lcc}
\toprule
Model & DSMoE & \textit{w/o $S(x)$} \\
\midrule
Hellaswag & 50.92 & 32.29 \\
LAMBADA & 48.12 & 27.79 \\
PIQA & 72.36 & 62.73 \\
SIQA & 41.14 & 39.30 \\
StoryCloze & 68.67 & 57.14 \\
Wino & 56.35 & 50.83 \\
GSM8K & 1.67 & 0.38 \\
TriviaQA & 8.17 & 2.47 \\
WebQs & 25.52 & 2.95 \\
NatrualQs & 18.21 & 1.00 \\
\midrule
PPL & 7.41 & 12.75 \\
\bottomrule
\end{tabular}
}
\caption{Ablation study of DMoE against the model without direct estimation function S(x), where G(x) is employed in place of S(x).}
\label{tab:ab_study_1}
\end{table}

% As shown in Table \ref{tab:ab_study_1}, the model without straight-through estimator significantly underperforms the complete model in terms of both perplexity and benchmark performance.  % TODO 但是table 3里头的标题是 without G(x)，跟下面这个章节才对应，跟这一段有点对不上，另外最好一句话说明一下为什么会导致performance下降这么厉害——done
As shown in Table \ref{tab:ab_study_1}, the model without straight-through estimator significantly underperforms the complete model in terms of both perplexity and benchmark performance. This substantial degradation occurs because routing parameters for non-activated experts receive zero gradients during backpropagation, preventing these routes from being adjusted to utilize more of the pre-trained knowledge inherited from the dense model. Without the ability to adaptively modify routing decisions, potentially valuable knowledge encoded in these experts becomes permanently inaccessible, leading to significant performance loss.


\subsection{Ablation Study: Training without Piecewise Function G(x)}
To validate the necessity of incorporating piecewise function learning during training, we conduct an ablation study by removing the piecewise function G(x) and using the following formula for training:

\begin{equation}
h^l_t=\sum^n_{i=1} o_i * \sigma(\hat{h^l_t}Y_i)
\end{equation}


\begin{figure}[htbp]
\includegraphics[width=0.8\linewidth]{diff_threshold}
\caption{During the training phase, G(x) is not utilized. In the inference phase, G(x) is employed for activation. The model's perplexity and the number of activated experts vary with the threshold $\tau$. The pentagram markers indicate the perplexity and number of activated experts achieved by DSMoE.}
\label{fig:diff_threshold}
\end{figure} % TODO 这个图不是很清晰，不是矢量图，最好改成matlab或者pyplot来画，更好一些，截图比较模糊——done，这个应该是配色的问题，果冻色看起来糊


Prior to inference, we determine the appropriate activation level by adjusting the threshold value on the validation set, with a step size of 0.05. Figure \ref{fig:diff_threshold} illustrates the relationship between perplexity and the average number of activated experts on the validation set.

The results clearly demonstrate that as the threshold increases, perplexity rises rapidly while the average number of activated experts decreases correspondingly. This observation indicates that without the piecewise function G(x), all experts participate in computation and gradient updates. Under the constraint of sparsity loss, the model tends to distribute activation values uniformly across all experts rather than learning to distinctively identify more important experts. This leads to two consequences: first, the activation values for each expert are suppressed to a relatively low level, and second, the learned importance of each expert becomes relatively uniform. Under the same activation constraints as DSMoE, the approach without the piecewise function G(x) exhibits higher perplexity, highlighting how this training-inference inconsistency significantly degrades model performance.


\subsection{Layer-wise Activation Patterns Analysis}


% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=0.8\linewidth]{hotmap1B}
%         \caption{Heatmap for 1B model}
%         \label{fig:hotmap1B}
%     \end{subfigure}
    
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=0.8\linewidth]{hotmap7B}
%         \caption{Heatmap for 7B model}
%         \label{fig:hotmap7B}
%     \end{subfigure}
%     \caption{Heatmap visualization of expert activation counts across different layers and average expert activations for LLaMA-7B and LLaMA-1B models on various validation sets.}
%     \label{fig:heatmaps}
% \end{figure}


\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{hotmap1B}
        \caption{Heatmap for 1B model}
        \label{fig:hotmap1B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.51\linewidth}
        \centering
        \includegraphics[width=\linewidth]{hotmap7B}
        \caption{Heatmap for 7B model}
        \label{fig:hotmap7B}
    \end{subfigure}
    \caption{Heatmap visualization of expert activation counts across different layers and average expert activations for LLaMA-7B and LLaMA-1B models on various validation sets.}
    \label{fig:heatmaps}
\end{figure}



We evaluated DSMoE across different validation sets and generated heatmaps to visualize the distribution of activated experts across network layers. Both model sizes exhibit a distinctive activation pattern: higher activation counts at both input and output layers, elevated activation in middle layers, and lower activation in remaining layers - forming a ``W-shaped" pattern.

The bottom layers, which typically encode fundamental features, demonstrate high expert activation. This suggests the model's tendency to activate multiple experts in parallel to process multi-dimensional input features, potentially serving as an ``information preservation mechanism" to retain critical base-level information. The top layers, responsible for final decision-making and output generation, show increased expert activation to enhance output robustness by reducing individual expert bias through collective decision-making. The elevated activation in middle layers suggests these layers serve as critical zones for feature transformation, integration, and processing of long-range dependencies. This bottom-middle-top activation pattern forms a complete information processing pipeline: bottom layers for extensive collection and processing of basic features, middle layers for feature transformation and information integration, and top layers for comprehensive decision-making and output generation.

Furthermore, we observed significant variations in both the average number of activated experts and activation patterns across different test sets. This indicates that DSMoE implements dynamic regulation mechanisms specific to different inputs rather than converging to a homogeneous learning pattern.

These observations provide novel insights for future MoE architectures, suggesting that expert activation counts can be strategically varied across different layers of the network.


\section{Conclusion}
% This paper introduces a method for transforming dense models into dynamically sparse models via matrix blocking and dynamic gating mechanisms to achieve expert dynamic activation. Experimental results demonstrate significant performance improvements over existing methods under equivalent computational constraints, while analysis provides new insights for future MoE architecture design.
This paper presents DSMoE, a novel approach that achieves model sparsification by partitioning pre-trained FFN layers into computational blocks. Experiments on LLaMA models demonstrate superior performance over existing pruning and MoE approaches under equivalent computational constraints, while revealing distinctive layerwise activation patterns for future MoE designs.


\section{Limitations}
Due to computational resource constraints, we were only able to evaluate DSMoE on language models up to 7B parameters. Future work with access to larger computational resources could explore the scalability and effectiveness of our approach on larger model architectures, which may reveal additional insights about the relationship between model scale and dynamic sparsification patterns.

% \section*{Acknowledgments}


\bibliography{custom}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
