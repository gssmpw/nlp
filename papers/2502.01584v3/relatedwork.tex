\section{Related Work}
\paragraph{Benchmarks that Require PhD Knowledge}

As models keep getting better, the benchmarks that we use to quantify their capabilities get \emph{saturated}. Several recent benchmarks have been explicitly designed with the belief that models are approaching superhuman capabilities in particular domains, and these benchmarks are thus designed to have extremely challenging domain-specific questions. GPQA (``Google-proof Q\&A'')~\citep{gpqa} is a recent example, where benchmark problems were created and vetted by teams of experts who had or were pursuing PhDs in physics, chemistry, and biology. Remarkably, the latest generation of reasoning models appear to be saturating GPQA in just a few months. HLE (``Humanity's Last Exam'')~\citep{phan:humanitys-last-exam} is a newer, larger, and harder benchmark that is similarly designed. HLE covers many more areas of knowledge, with questions written by people who hold advanced degrees, and even the latest models still perform very poorly: o3-mini-high achieves 14\% accuracy. These benchmarks are very valuable, but, by design, each problem is only comprehensible by people who have narrow subject-matter expertise. An individual cannot hope to answer or even understand most of the questions on these benchmarks.

\paragraph{Math Benchmarks}
A number of notable benchmarks, 
such as GSM8K \citep{cobbe:gsm8k} and MATH \citep{hendrycks_math2021},
evaluate the mathematical capabilities of the models. Although these benchmarks are now saturated~\citep{lei2024,zhong2024}, models do perform substantially worse on small variations of these problems~\cite{mirzadeh2024pre}.
\cite{gulati2024} build a benchmark with problems from the William Lowell Putnam Mathematical Competition and similarly find that o1-preview solves 41.95\% of the original problems, but its accuracy is 30\% lower on small variations. These phenomena are likely the result of data contamination or \textit{token bias} \citep{jiang_tokbias2024pre}.
While math benchmarks are invaluable in discovering the capabilities and limitations of logical reasoning in state-of-the-art models, the problems in such benchmarks inevitably require a strong mathematical background for a reader to appreciate, follow along, or catch errors in the models' reasoning.

\paragraph{Benchmarks That Do Not Require World Knowledge}
ARC-AGI~\citep{chollet:arc-agi} is perhaps the best known benchmark of reasoning and abstraction benchmark for AI. ARC-AGI is carefully constructed to require a small number of priors, whereas our benchmark 
tests both a model's reasoning ability and its ability to recall extensive general knowledge. The ARC-AGI tests can be challenging for people, but
``Each task included in ARC has been successfully solved by at least one member of a group of three high-IQ humans.''~\citep{chollet:arc-agi} In contrast, typically a few hundred people submit correct solutions to the Puzzle Challenges every week.


\paragraph{Benchmarks That Exercise General Knowledge}

There is a long tradition of using puzzles to evaluate language models. Most closely related to \benchmark{} are benchmarks based on cryptic crosswords~\citep{rozner:cryptic-crosswords}, logic and word puzzles~\citep{jiang:brainteaser,tyagi-etal-2024-step,connections}, and the ``on-air'' questions from the NPR Sunday Puzzle~\citep{zhao:puzzleqa}. While we also source our benchmark from the same show, our benchmarks are disjoint and very different. \citet{zhao:puzzleqa}'s benchmark is  easy for contemporary models (\Cref{on-air-challenges}) and human contestants can solve it live.  In contrast, \benchmark{} is derived from the off-air ``weekly challenges'' that are designed to be significantly harder. Some puzzles explicitly tell listeners to use a dictionary or atlas to help them work through the puzzle. As we shall see, the weekly challenges can stump frontier models too.