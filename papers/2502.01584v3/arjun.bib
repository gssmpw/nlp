
@misc{npr:high-count,
	title = {Maybe {Not} {So} {Easy} {As} {Pie}},
	url = {https://www.npr.org/2009/09/13/112759216/maybe-not-so-easy-as-pie},
	language = {en},
	urldate = {2025-03-18},
	journal = {Sunday Puzzle},
	collaborator = {Lynn, Neary and Shortz, Will and Huband, Charles},
	month = sep,
	year = {2009},
}

@misc{renner:npr1995,
	title = {My {Date} with the {NPR} {Puzzle}},
	url = {https://www.taterenner.com/NPRMYEX7.htm},
	urldate = {2025-03-18},
	author = {Renner, Richard},
	month = dec,
	year = {1995},
}

@misc{bryan:weekly-faq,
	title = {Frequently {Asked} {Questions} {About} {Soapbox}},
	url = {https://www.npr.org/sections/sundaysoapbox/2008/04/frequently_asked_questions_abo.html},
	abstract = {What is Soapbox?},
	language = {en},
	urldate = {2025-03-18},
	journal = {NPR},
	author = {Bryan, Wright},
	month = apr,
	year = {2008},
}

@misc{aggarwal:l1,
	title = {L1: {Controlling} {How} {Long} {A} {Reasoning} {Model} {Thinks} {With} {Reinforcement} {Learning}},
	shorttitle = {L1},
	url = {http://arxiv.org/abs/2503.04697},
	doi = {10.48550/arXiv.2503.04697},
	abstract = {Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Aggarwal, Pranjal and Welleck, Sean},
	month = mar,
	year = {2025},
	note = {arXiv:2503.04697 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{shao_deepseekmath_2024,
	title = {{DeepSeekMath}: {Pushing} the {Limits} of {Mathematical} {Reasoning} in {Open} {Language} {Models}},
	shorttitle = {{DeepSeekMath}},
	url = {http://arxiv.org/abs/2402.03300},
	doi = {10.48550/arXiv.2402.03300},
	abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
	month = apr,
	year = {2024},
	note = {arXiv:2402.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wu:phd-knowledge,
	title = {{PhD} {Knowledge} {Not} {Required}: {A} {Reasoning} {Challenge} for {Large} {Language} {Models}},
	shorttitle = {{PhD} {Knowledge} {Not} {Required}},
	url = {http://arxiv.org/abs/2502.01584},
	doi = {10.48550/arXiv.2502.01584},
	abstract = {Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot. Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.},
	urldate = {2025-03-07},
	publisher = {arXiv},
	author = {Anderson, Carolyn Jane and Biswas, Joydeep and Boruch-Gruszecki, Aleksander and Cassano, Federico and Feldman, Molly Q. and Guha, Arjun and Lucchetti, Francesca and Wu, Zixuan},
	month = feb,
	year = {2025},
	note = {arXiv:2502.01584 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{li_gift_2025,
	title = {{GiFT}: {Gibbs} {Fine}-{Tuning} for {Code} {Generation}},
	shorttitle = {{GiFT}},
	url = {http://arxiv.org/abs/2502.11466},
	doi = {10.48550/arXiv.2502.11466},
	abstract = {Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Li, Haochen and Feng, Wanjin and Zhou, Xin and Shen, Zhiqi},
	month = feb,
	year = {2025},
	note = {arXiv:2502.11466 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{o1-system-card,
	title = {{OpenAI} o1 {System} {Card}},
	url = {https://openai.com/index/openai-o1-system-card/},
	abstract = {This report outlines the safety work carried out prior to releasing OpenAI o1 and o1-mini, including external red teaming and frontier risk evaluations according to our Preparedness Framework.},
	language = {en-US},
	urldate = {2025-02-01},
	author = {OpenAI},
	year = {2024},
}

@misc{muennighoff:s1,
	title = {s1: {Simple} test-time scaling},
	shorttitle = {s1},
	url = {http://arxiv.org/abs/2501.19393},
	doi = {10.48550/arXiv.2501.19393},
	abstract = {Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27\% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50\% to 57\% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Candès, Emmanuel and Hashimoto, Tatsunori},
	month = jan,
	year = {2025},
	note = {arXiv:2501.19393 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{phan:humanitys-last-exam,
	title = {Humanity's {Last} {Exam}},
	author = {Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Shi, Sean and Choi, Michael and Agrawal, Anish and Chopra, Arnav and Khoja, Adam and Kim, Ryan and Ren, Richard and Hausenloy, Jason and Zhang, Oliver and Mazeika, Mantas and Yue, Summer and Wang, Alexandr and Hendrycks, Dan},
	year = {2025},
}

@misc{gemini2ft,
	title = {Gemini 2.0 {Flash} {Thinking} {Experimental}},
	url = {https://deepmind.google/technologies/gemini/flash-thinking/},
	abstract = {Gemini 2.0 Flash Thinking Experimental is our enhanced reasoning model, capable of showing its thoughts to improve performance and explainability.},
	language = {en},
	urldate = {2025-02-01},
	journal = {Google DeepMind},
	author = {Google},
	month = dec,
	year = {2024},
}

@misc{chollet:arc-agi,
	title = {On the {Measure} of {Intelligence}},
	url = {http://arxiv.org/abs/1911.01547},
	doi = {10.48550/arXiv.1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Chollet, François},
	month = nov,
	year = {2019},
	note = {arXiv:1911.01547 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{zhao:puzzleqa,
	title = {Solving and {Generating} {NPR} {Sunday} {Puzzles} with {Large} {Language} {Models}},
	abstract = {We explore the ability of large language models to solve and generate puzzles from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15 years of on-air puzzles. We evaluate four large language models using PUZZLEQA, in both multiple choice and free response formats, and explore two prompt engineering techniques to improve free response performance: chain-of-thought reasoning and prompt summarization. We find that state-of-the-art large language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5, achieves 50.2\% loose accuracy. However, in our few-shot puzzle generation experiment, we find no evidence that models can generate puzzles: GPT-3.5 generates puzzles with answers that do not conform to the generated rules. Puzzle generation remains a challenging task for future work.},
	language = {en},
	booktitle = {Conference on {Computational} {Creativity} ({ICCC})},
	author = {Zhao, Jingmiao and Anderson, Carolyn Jane},
	year = {2023},
}

@inproceedings{jiang:brainteaser,
	address = {Singapore},
	title = {{BRAINTEASER}: {Lateral} {Thinking} {Puzzles} for {Large} {Language} {Models}},
	shorttitle = {{BRAINTEASER}},
	url = {https://aclanthology.org/2023.emnlp-main.885},
	doi = {10.18653/v1/2023.emnlp-main.885},
	abstract = {The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model’s ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of reconstruction examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-ofthe-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across reconstruction formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Yifan and Ilievski, Filip and Ma, Kaixin and Sourati, Zhivar},
	year = {2023},
	pages = {14317--14332},
}

@inproceedings{rozner:cryptic-crosswords,
	title = {Decrypting {Cryptic} {Crosswords}: {Semantically} {Complex} {Wordplay} {Puzzles} as a {Target} for {NLP}},
	shorttitle = {Decrypting {Cryptic} {Crosswords}},
	url = {https://openreview.net/forum?id=Ah5CMODl52},
	abstract = {Cryptic crosswords, the dominant crossword variety in the UK, are a promising target for advancing NLP systems that seek to process semantically complex, highly compositional language. Cryptic clues read like fluent natural language but are adversarially composed of two parts: a definition and a wordplay cipher requiring character-level manipulations. Expert humans use creative intelligence to solve cryptics, flexibly combining linguistic, world, and domain knowledge. In this paper, we make two main contributions. First, we present a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. After showing that three non-neural approaches and T5, a state-of-the-art neural language model, do not achieve good performance, we make our second main contribution: a novel curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. We also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies. Although our curricular approach considerably improves on the T5 baseline, our best-performing model still fails to generalize to the extent that humans can. Thus, cryptic crosswords remain an unsolved challenge for NLP systems and a potential source of future innovation.},
	language = {en},
	urldate = {2025-01-31},
	author = {Rozner, Joshua and Potts, Christopher and Mahowald, Kyle},
	month = nov,
	year = {2021},
}

@inproceedings{gpqa,
	title = {{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\&{A} {Benchmark}},
	shorttitle = {{GPQA}},
	url = {https://openreview.net/forum?id=Ti67584b98#discussion},
	abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65{\textbackslash}\% accuracy (74{\textbackslash}\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34{\textbackslash}\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). When we released this dataset in November 2023, GPT-4 achieved 39{\textbackslash}\% accuracy. As of March 2024, Claude 3 Opus achieves a reported score of approximately 60{\textbackslash}\%, highlighting the rapid pace of progress in AI. If we are to use future AI systems to help us answer very hard questions—for example, when developing new scientific knowledge—we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA for skilled non-experts should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
	language = {en},
	urldate = {2025-01-30},
	author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
	month = aug,
	year = {2024},
}

@misc{deepseek-r1,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{fiotto-kaufman:ndif,
	title = {{NNsight} and {NDIF}: {Democratizing} {Access} to {Foundation} {Model} {Internals}},
	url = {https://openreview.net/forum?id=MxbEiFRf39&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)},
	abstract = {We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. NDIF is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the intervention graph, an architecture developed to decouple experiment design from model runtime. Together, this framework provides transparent and efficient access to the internals of foundation-size deep neural networks without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches. Code and documentation will be made available open-source.},
	urldate = {2025-01-23},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Fiotto-Kaufman, Jaden Fried and Loftus, Alexander Russell and Todd, Eric and Brinkmann, Jannik and Pal, Koyena and Troitskii, Dmitrii and Ripa, Michael and Belfki, Adam and Rager, Can and Juang, Caden and Mueller, Aaron and Marks, Samuel and Sharma, Arnab Sen and Lucchetti, Francesca and Prakash, Nikhil and Brodley, Carla E. and Guha, Arjun and Bell, Jonathan and Wallace, Byron C. and Bau, David},
	year = {2025},
}

@inproceedings{lucchetti:substance-beats-style,
	title = {Substance {Beats} {Style}: {Why} {Beginning} {Students} {Fail} to {Code} with {LLMs}},
	abstract = {Although LLMs are increasing the productivity of professional programmers, existing work shows that beginners struggle to prompt LLMs to solve text-to-code tasks. Why is this the case? This paper explores two competing hypotheses about the cause of student-LLM miscommunication: (1) students simply lack the technical vocabulary needed to write good prompts, and (2) students do not understand the extent of information that LLMs need to solve code generation tasks. We study (1) with a causal intervention experiment on technical vocabulary and (2) by analyzing graphs that abstract how students edit prompts and the different failures that they encounter. We find that substance beats style: a poor grasp of technical vocabulary is merely correlated with prompt failure; that the information content of prompts predicts success; that students get stuck making trivial edits; and more. Our findings have implications for the use of LLMs in programming education, and for efforts to make computing more accessible with LLMs.},
	booktitle = {Proceedings of the {Annual} {Conference} of the {Nations} of the {Americas} {Chapter} of the {ACL} ({NAACL})},
	author = {Lucchetti, Francesca and Wu, Zixuan and Guha, Arjun and Feldman, Molly Q. and Anderson, Carolyn Jane},
	year = {2025},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{noauthor_sky-t1_nodate,
	title = {Sky-{T1}: {Train} your own {O1} preview model within \$450},
	shorttitle = {Sky-{T1}},
	url = {https://novasky-ai.github.io/posts/sky-t1/},
	abstract = {We introduce Sky-T1-32B-Preview, our reasoning model that performs on par with o1-preview on popular reasoning and coding benchmarks.},
	language = {en},
	urldate = {2025-01-22},
}

@inproceedings{lee_llm2llm_2024,
	address = {Bangkok, Thailand},
	title = {{LLM2LLM}: {Boosting} {LLMs} with {Novel} {Iterative} {Data} {Enhancement}},
	shorttitle = {{LLM2LLM}},
	url = {https://aclanthology.org/2024.findings-acl.388/},
	doi = {10.18653/v1/2024.findings-acl.388},
	abstract = {Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2\% on the GSM8K dataset, 32.6\% on CaseHOLD, 32.0\% on SNIPS, 52.6\% on TREC and 39.8\% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available at https://github.com/SqueezeAILab/LLM2LLM.},
	urldate = {2025-01-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Nicholas and Wattanawong, Thanakul and Kim, Sehoon and Mangalam, Karttikeya and Shen, Sheng and Anumanchipalli, Gopala and Mahoney, Michael and Keutzer, Kurt and Gholami, Amir},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {6498--6526},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2025-01-15},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
}

@inproceedings{xu_demystifying_2024,
	title = {Demystifying {CLIP} {Data}},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Xu, Hu and Xie, Saining and Tan, Xiaoqing Ellen and Huang, Po-Yao and Howes, Russell and Sharma, Vasu and Li, Shang-Wen and Ghosh, Gargi and Zettlemoyer, Luke and Feichtenhofer, Christoph},
	year = {2024},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cui_ultrafeedback_2024,
	title = {{UltraFeedback}: {Boosting} {Language} {Models} with {Scaled} {AI} {Feedback}},
	shorttitle = {{UltraFeedback}},
	url = {http://arxiv.org/abs/2310.01377},
	doi = {10.48550/arXiv.2310.01377},
	abstract = {Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality {\textbackslash}textit\{AI feedback\} automatically for a scalable alternative. Specifically, we identify {\textbackslash}textbf\{scale and diversity\} as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present {\textbackslash}textsc\{UltraFeedback\}, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon {\textbackslash}textsc\{UltraFeedback\}, we align a LLaMA-based model by best-of-\$n\$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research. Our data and models are available at https://github.com/thunlp/UltraFeedback.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
	month = jul,
	year = {2024},
	note = {arXiv:2310.01377 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{lambert_tulu_2024,
	title = {Tulu 3: {Pushing} {Frontiers} in {Open} {Language} {Model} {Post}-{Training}},
	shorttitle = {Tulu 3},
	url = {http://arxiv.org/abs/2411.15124},
	doi = {10.48550/arXiv.2411.15124},
	abstract = {Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V. and Liu, Alisa and Dziri, Nouha and Lyu, Shane and Gu, Yuling and Malik, Saumya and Graf, Victoria and Hwang, Jena D. and Yang, Jiangjiang and Bras, Ronan Le and Tafjord, Oyvind and Wilhelm, Chris and Soldaini, Luca and Smith, Noah A. and Wang, Yizhong and Dasigi, Pradeep and Hajishirzi, Hannaneh},
	month = dec,
	year = {2024},
	note = {arXiv:2411.15124 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/1909.08593},
	doi = {10.48550/arXiv.1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	month = jan,
	year = {2020},
	note = {arXiv:1909.08593 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gehring_rlef_2024,
	title = {{RLEF}: {Grounding} {Code} {LLMs} in {Execution} {Feedback} with {Reinforcement} {Learning}},
	shorttitle = {{RLEF}},
	url = {http://arxiv.org/abs/2410.02089},
	doi = {10.48550/arXiv.2410.02089},
	abstract = {Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Gehring, Jonas and Zheng, Kunhao and Copet, Jade and Mella, Vegard and Cohen, Taco and Synnaeve, Gabriel},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02089 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_q3_2024,
	title = {Q3 earnings call: {CEO}’s remarks},
	shorttitle = {Q3 earnings call},
	url = {https://blog.google/inside-google/message-ceo/alphabet-earnings-q3-2024/},
	abstract = {Our Q3 results were led by great performance in Search, Cloud and YouTube.},
	language = {en-us},
	urldate = {2025-01-06},
	journal = {Google},
	month = oct,
	year = {2024},
}

@misc{ma_lingma_2024,
	title = {Lingma {SWE}-{GPT}: {An} {Open} {Development}-{Process}-{Centric} {Language} {Model} for {Automated} {Software} {Improvement}},
	shorttitle = {Lingma {SWE}-{GPT}},
	url = {http://arxiv.org/abs/2411.00622},
	doi = {10.48550/arXiv.2411.00622},
	abstract = {Recent advancements in LLM-based agents have led to significant progress in automatic software engineering, particularly in software maintenance and evolution. Despite these encouraging advances, current research faces two major challenges. First, SOTA performance primarily depends on closed-source models, which significantly limits the technology's accessibility, and potential for customization in diverse SE tasks. Second, these models are predominantly trained on static code data, lacking a deep understanding of the dynamic interactions, iterative problem-solving processes, and evolutionary characteristics inherent in software development. To address these challenges, our study adopts a software engineering perspective. We recognize that real-world software maintenance and evolution processes encompass not only static code data but also developers' thought processes, utilization of external tools, and the interaction between different functional personnel. Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma SWE-GPT 7B and 72B. By learning from and simulating real-world code submission activities, Lingma SWE-GPT systematically incorporates the dynamic interactions and iterative problem-solving inherent in software development process, thereby achieving a more comprehensive understanding of software improvement processes. We conducted experimental evaluations using SWE-bench Verified benchmark. The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30.20\% of the GitHub issues, marking a significant improvement in automatic issue resolution (22.76\% relative improvement compared to Llama 3.1 405B), approaching the performance of closed-source models (31.80{\textbackslash}\% issues of GPT-4o resolved). Notably, Lingma SWE-GPT 7B resolves 18.20\% of the issues, highlighting the potential for applying smaller models to ASE tasks.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Ma, Yingwei and Cao, Rongyu and Cao, Yongchang and Zhang, Yue and Chen, Jue and Liu, Yibo and Liu, Yuchen and Li, Binhua and Huang, Fei and Li, Yongbin},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00622 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{pan_training_2024,
	title = {Training {Software} {Engineering} {Agents} and {Verifiers} with {SWE}-{Gym}},
	url = {http://arxiv.org/abs/2412.21139},
	doi = {10.48550/arXiv.2412.21139},
	abstract = {We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to train language model based SWE agents , achieving up to 19\% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. We also experiment with inference-time scaling through verifiers trained on agent trajectories sampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve 32.0\% and 26.0\% on SWE-Bench Verified and Lite, respectively, reflecting a new state-of-the-art for open-weight SWE agents. To facilitate further research, we publicly release SWE-Gym, models, and agent trajectories.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Pan, Jiayi and Wang, Xingyao and Neubig, Graham and Jaitly, Navdeep and Ji, Heng and Suhr, Alane and Zhang, Yizhe},
	month = dec,
	year = {2024},
	note = {arXiv:2412.21139 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{yang_evaluating_2024,
	title = {Evaluating and {Aligning} {CodeLLMs} on {Human} {Preference}},
	url = {http://arxiv.org/abs/2412.05210},
	doi = {10.48550/arXiv.2412.05210},
	abstract = {Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.{\textbackslash}footnote\{{\textbackslash}url\{https://codearenaeval.github.io/ \}\}},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Yang, Jian and Yang, Jiaxi and Jin, Ke and Miao, Yibo and Zhang, Lei and Yang, Liqun and Cui, Zeyu and Zhang, Yichang and Hui, Binyuan and Lin, Junyang},
	month = dec,
	year = {2024},
	note = {arXiv:2412.05210 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{huang_n_2024,
	title = {The {N}+ {Implementation} {Details} of {RLHF} with {PPO}: {A} {Case} {Study} on {TL};{DR} {Summarization}},
	shorttitle = {The {N}+ {Implementation} {Details} of {RLHF} with {PPO}},
	url = {http://arxiv.org/abs/2403.17031},
	doi = {10.48550/arXiv.2403.17031},
	abstract = {This work is the first to openly reproduce the Reinforcement Learning from Human Feedback (RLHF) scaling behaviors reported in OpenAI's seminal TL;DR summarization work. We create an RLHF pipeline from scratch, enumerate over 20 key implementation details, and share key insights during the reproduction. Our RLHF-trained Pythia models demonstrate significant gains in response quality that scale with model size, with our 2.8B, 6.9B models outperforming OpenAI's released 1.3B checkpoint. We publicly release the trained model checkpoints and code to facilitate further research and accelerate progress in the field ({\textbackslash}url\{https://github.com/vwxyzjn/summarize\_from\_feedback\_details\}).},
	urldate = {2024-12-31},
	publisher = {arXiv},
	author = {Huang, Shengyi and Noukhovitch, Michael and Hosseini, Arian and Rasul, Kashif and Wang, Weixun and Tunstall, Lewis},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17031 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ding_semcoder_2024,
	title = {{SemCoder}: {Training} {Code} {Language} {Models} with {Comprehensive} {Semantics} {Reasoning}},
	shorttitle = {{SemCoder}},
	url = {http://arxiv.org/abs/2406.01006},
	doi = {10.48550/arXiv.2406.01006},
	abstract = {Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3\% on HumanEval (GPT-3.5-turbo: 76.8\%), 63.6\% on CRUXEval-I (GPT-3.5-turbo: 50.3\%), and 63.9\% on CRUXEval-O (GPT-3.5-turbo: 59.0\%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities. Our data, code, and models are available at: https://github.com/ARiSE-Lab/SemCoder.},
	urldate = {2024-11-23},
	publisher = {arXiv},
	author = {Ding, Yangruibo and Peng, Jinjun and Min, Marcus J. and Kaiser, Gail and Yang, Junfeng and Ray, Baishakhi},
	month = oct,
	year = {2024},
	note = {arXiv:2406.01006},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{zhang_bridge-coder_2024,
	title = {Bridge-{Coder}: {Unlocking} {LLMs}' {Potential} to {Overcome} {Language} {Gaps} in {Low}-{Resource} {Code}},
	shorttitle = {Bridge-{Coder}},
	url = {http://arxiv.org/abs/2410.18957},
	doi = {10.48550/arXiv.2410.18957},
	abstract = {Large Language Models (LLMs) demonstrate strong proficiency in generating code for high-resource programming languages (HRPLs) like Python but struggle significantly with low-resource programming languages (LRPLs) such as Racket or D. This performance gap deepens the digital divide, preventing developers using LRPLs from benefiting equally from LLM advancements and reinforcing disparities in innovation within underrepresented programming communities. While generating additional training data for LRPLs is promising, it faces two key challenges: manual annotation is labor-intensive and costly, and LLM-generated LRPL code is often of subpar quality. The underlying cause of this issue is the gap between natural language to programming language gap (NL-PL Gap), which is especially pronounced in LRPLs due to limited aligned data. In this work, we introduce a novel approach called Bridge-Coder, which leverages LLMs' intrinsic capabilities to enhance the performance on LRPLs. Our method consists of two key stages. Bridge Generation, where we create high-quality dataset by utilizing LLMs' general knowledge understanding, proficiency in HRPLs, and in-context learning abilities. Then, we apply the Bridged Alignment, which progressively improves the alignment between NL instructions and LRPLs. Experimental results across multiple LRPLs show that Bridge-Coder significantly enhances model performance, demonstrating the effectiveness and generalization of our approach. Furthermore, we offer a detailed analysis of the key components of our method, providing valuable insights for future work aimed at addressing the challenges associated with LRPLs.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Zhang, Jipeng and Zhang, Jianshu and Li, Yuanzhe and Pi, Renjie and Pan, Rui and Liu, Runtao and Zheng, Ziqiang and Zhang, Tong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.18957},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_facebook_nodate,
	title = {Facebook - log in or sign up},
	url = {https://www.facebook.com/},
	abstract = {Log into Facebook to start sharing and connecting with your friends, family, and people you know.},
	language = {en},
	urldate = {2024-10-22},
	journal = {Facebook},
}

@inproceedings{liu_tuning_2024,
	title = {Tuning {Language} {Models} by {Proxy}},
	url = {https://openreview.net/forum?id=dribhnhm1i#discussion},
	abstract = {Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce **proxy-tuning**, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a *smaller* LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88\% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. We then demonstrate the generality of proxy-tuning by applying it to domain adaptation on code, and task-specific finetuning on question-answering and math problems. Finally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing its knowledge about recent events. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.},
	language = {en},
	urldate = {2024-10-15},
	author = {Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A.},
	month = aug,
	year = {2024},
}

@inproceedings{perretta:mutant-hints,
	title = {Instructor-{Written} {Hints} as {Automated} {Test} {Suite} {Quality} {Feedback}},
	publisher = {ACM},
	author = {Perretta, James and DeOrio, Andrew and Guha, Arjun and Bell, Jonathan},
	year = {2025},
}

@inproceedings{wei:starcoder2-self-instruct,
	title = {Fully {Transparent} {Self}-{Alignment} for {Code} {Generation}},
	url = {https://openreview.net/forum?id=xXRnUU7xTL},
	abstract = {Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of Large Language Models (LLMs) to follow human instructions. For programming tasks, most models are fine-tuned using either costly human-annotated instruction-response pairs or those generated by large, proprietary LLMs, which may not be permitted. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiment, we employ SelfCodeAlign with CodeQwen1.5-7B, yielding a dataset of 74k instruction-response pairs. Finetuning CodeQwen1.5-7B on this dataset results in the creation of SelfCodeAlign-CQ-7B. Remarkably, SelfCodeAlign-CQ-7B achieves a pass@1 score of 67.1 on HumanEval+, even outperforming CodeLlama-70B-Instruct, which is ten times larger. Across all evaluated benchmarks, SelfCodeAlign-CQ-7B consistently outperforms CodeQwen1.5-7B trained with OctoPack, the prior state-of-the-art instruction-tuning method without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We also prove the effectiveness of different components in our pipeline and demonstrate that SelfCodeAlign outperforms the state-of-the-art, GPT-3.5-based distillation methods, including OSS-Instruct and Evol-Instruct. Overall, SelfCodeAlign shows for the first time that a strong instruction-tuned code LLM can result from self-alignment rather than distillation. We plan to open-source all code, data, and models.},
	language = {en},
	urldate = {2024-09-25},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Wei, Yuxiang and Cassano, Federico and Liu, Jiawei and Ding, Yifeng and Jain, Naman and Mueller, Zachary and Vries, Harm de and Werra, Leandro Von and Guha, Arjun and Zhang, Lingming},
	year = {2024},
}

@misc{arditi_refusal_2024,
	title = {Refusal in {Language} {Models} {Is} {Mediated} by a {Single} {Direction}},
	url = {http://arxiv.org/abs/2406.11717},
	abstract = {Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.},
	language = {en},
	urldate = {2024-09-13},
	publisher = {arXiv},
	author = {Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Panickssery, Nina and Gurnee, Wes and Nanda, Neel},
	month = jul,
	year = {2024},
	note = {arXiv:2406.11717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{openai:chatgpt-plugins,
	title = {{ChatGPT} {Plugins}},
	url = {https://openai.com/index/chatgpt-plugins/},
	urldate = {2024-08-20},
	author = {Agarwal, Sandhini and Akkaya, Ilge and Balcom, Valerie and Bavarian, Mo and Bernadett-Shapiro, Gabriel and {Greg Brockman} and {Miles Brundage} and {Jeff Chan} and {Fotis Chantzis} and {Noah Deutsch} and {Brydon Eastman} and {Atty Eleti} and {Niko Felix} and {Simón Posada Fishman} and {Isa Fulford} and {Christian Gibson} and {Joshua Gross} and {Mike Heaton} and {Jacob Hilton} and {Xin Hu} and {Shawn Jain} and {Joy Jiao} and {Haozhun Jin} and {Logan Kilpatrick} and {Christina Kim} and {Michael Kolhede} and {Andrew Mayne} and {Paul McMillan} and {David Medina} and {Jacob Menick} and {Andrey Mishchenko} and {Ashvin Nair} and {Rajeev Nayak} and {Arvind Neelakantan} and {Rohan Nuttall} and {Joel Parish} and {Alex Tachard Passos} and {Adam Perelman} and {Filipe de Avila Belbute Peres} and {Vitchyr Pong} and {John Schulman} and {Eric Sigler} and {Natalie Staudacher} and {Nicholas Turley} and {Jerry Tworek} and {Ryan Greene} and {Arun Vijayvergiya} and {Chelsea Voss} and {Jiayi Weng} and {Matt Wiethoff} and {Sarah Yoo} and {Kevin Yu} and {Wojciech Zaremba} and {Shengjia Zhao} and {Will Zhuk} and {Barret Zoph}},
	year = {2023},
}

@misc{benallal:smollm,
	title = {{SmolLM} - blazingly fast and remarkably powerful},
	url = {https://huggingface.co/blog/smollm},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-09},
	author = {Ben Allal, Loubna and Lozhkov, Anton and Bakouch, Elie},
	year = {2024},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	year = {2020},
	pages = {9459--9474},
}

@inproceedings{prather:widening,
	address = {New York, NY, USA},
	series = {{ICER} '24},
	title = {The {Widening} {Gap}: {The} {Benefits} and {Harms} of {Generative} {AI} for {Novice} {Programmers}},
	volume = {1},
	isbn = {9798400704758},
	url = {https://dl.acm.org/doi/10.1145/3632620.3671116},
	doi = {10.1145/3632620.3671116},
	abstract = {Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies. Previous research has shown that novices can encounter multiple metacognitive difficulties while programming, such as forming incorrect conceptual models of the problem or having a false sense of progress after testing their solution. Novices are typically unaware of how these difficulties are hindering their progress. Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages. Its impact on novice metacognition has only started to be explored. Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools. Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools. Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who did and did not struggle. Some students who did not struggle were able to use GenAI to accelerate, creating code they already intended to make, and were able to ignore unhelpful or incorrect inline code suggestions. But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties. Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence. Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work.},
	urldate = {2024-08-20},
	booktitle = {{ACM} {Conference} on {International} {Computing} {Education} {Research} ({ICER})},
	publisher = {Association for Computing Machinery},
	author = {Prather, James and Reeves, Brent N and Leinonen, Juho and MacNeil, Stephen and Randrianasolo, Arisoa S and Becker, Brett A. and Kimmel, Bailey and Wright, Jared and Briggs, Ben},
	month = aug,
	year = {2024},
	pages = {469--486},
}

@inproceedings{vadaparty:cs1-llm,
	address = {New York, NY, USA},
	title = {{CS1}-{LLM}: {Integrating} {LLMs} into {CS1} {Instruction}},
	isbn = {9798400706004},
	shorttitle = {{CS1}-{LLM}},
	url = {https://dl.acm.org/doi/10.1145/3649217.3653584},
	doi = {10.1145/3649217.3653584},
	urldate = {2024-08-20},
	booktitle = {Innovation and {Technology} in {Computer} {Science} {Education} ({ITiCSE})},
	publisher = {Association for Computing Machinery},
	author = {Vadaparty, Annapurna and Zingaro, Daniel and Smith IV, David H. and Padala, Mounika and Alvarado, Christine and Gorson Benario, Jamie and Porter, Leo},
	month = jul,
	year = {2024},
	pages = {297--303},
}

@inproceedings{lau:ban-it,
	address = {New York, NY, USA},
	title = {From "{Ban} {It} {Till} {We} {Understand} {It}" to "{Resistance} is {Futile}": {How} {University} {Programming} {Instructors} {Plan} to {Adapt} as {More} {Students} {Use} {AI} {Code} {Generation} and {Explanation} {Tools} such as {ChatGPT} and {GitHub} {Copilot}},
	volume = {1},
	isbn = {978-1-4503-9976-0},
	url = {https://dl.acm.org/doi/10.1145/3568813.3600138},
	doi = {10.1145/3568813.3600138},
	urldate = {2024-08-20},
	booktitle = {{ACM} {Conference} on {International} {Computing} {Education} {Research} ({ICER})},
	publisher = {Association for Computing Machinery},
	author = {Lau, Sam and Guo, Philip},
	month = sep,
	year = {2023},
	pages = {106--121},
}

@misc{copilot-workspace,
	title = {Copilot {Workspace}},
	url = {https://next.github.com/projects/copilot-workspace},
	abstract = {GitHub Next Project: A Copilot-native dev environment, designed for everyday tasks.},
	language = {en},
	urldate = {2024-08-20},
	journal = {GitHub Next},
	author = {{GitHub Next}},
}

@misc{wang:opendevin,
	title = {{OpenDevin}: {An} {Open} {Platform} for {AI} {Software} {Developers} as {Generalist} {Agents}},
	shorttitle = {{OpenDevin}},
	url = {http://arxiv.org/abs/2407.16741},
	doi = {10.48550/arXiv.2407.16741},
	abstract = {Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenDevin, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released under the permissive MIT license, OpenDevin is a community project spanning academia and industry with more than 1.3K contributions from over 160 contributors and will improve going forward.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F. and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and Tran, Hoang H. and Li, Fuqiang and Ma, Ren and Zheng, Mingzhang and Qian, Bill and Shao, Yanjun and Muennighoff, Niklas and Zhang, Yizhe and Hui, Binyuan and Lin, Junyang and Brennan, Robert and Peng, Hao and Ji, Heng and Neubig, Graham},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16741 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@inproceedings{jimenez:swe-bench,
	title = {{SWE}-bench: {Can} {Language} {Models} {Resolve} {Real}-world {Github} {Issues}?},
	shorttitle = {{SWE}-bench},
	url = {https://openreview.net/forum?id=VTF8yNQM66},
	abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96\% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
	language = {en},
	urldate = {2024-08-20},
	author = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik R.},
	month = oct,
	year = {2023},
}

@misc{xia:agentless,
	title = {Agentless: {Demystifying} {LLM}-based {Software} {Engineering} {Agents}},
	url = {http://arxiv.org/abs/2407.01489},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01489 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{zheng_opencodeinterpreter_2024,
	address = {Bangkok, Thailand and virtual meeting},
	title = {{OpenCodeInterpreter}: {Integrating} {Code} {Generation} with {Execution} and {Refinement}},
	shorttitle = {{OpenCodeInterpreter}},
	url = {https://aclanthology.org/2024.findings-acl.762},
	abstract = {The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreterbrings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.},
	urldate = {2024-08-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics} {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {12834--12859},
}

@misc{chatgpt-ada,
	title = {Data analysis with {ChatGPT} {\textbar} {OpenAI} {Help} {Center}},
	url = {https://help.openai.com/en/articles/8437071-data-analysis-with-chatgpt},
	abstract = {Feature and capabilities used when working with data in ChatGPT},
	language = {en},
	urldate = {2024-08-20},
}

@inproceedings{li:bird,
	title = {Can {LLM} {Already} {Serve} as {A} {Database} {Interface}? {A} {BIg} {Bench} for {Large}-{Scale} {Database} {Grounded} {Text}-to-{SQLs}},
	urldate = {2024-08-20},
	booktitle = {Neural {Information} {Processing} {Systems} {Track} on {Datasets} and {Benchmarks} ({NeurIPS} {Datasets} and {Benchmarks})},
	author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Ma, Chenhao and Li, Guoliang and Chang, Kevin and Huang, Fei and Cheng, Reynold and Li, Yongbin},
	month = nov,
	year = {2023},
}

@misc{yahav:fundamental-theorem-of-genai,
	title = {Eran {Yahav} on {X}: "{Which} should now be (obviously!) recasted as the "fundamental theorem of {GenAI}"" / {X}},
	shorttitle = {Eran {Yahav} on {X}},
	url = {https://x.com/yahave/status/1660729517210476544},
	language = {en},
	urldate = {2024-08-20},
	journal = {X (formerly Twitter)},
	month = nov,
	year = {2022},
}

@inproceedings{nam:gilt,
	title = {Using an {LLM} to {Help} {With} {Code} {Understanding}},
	isbn = {9798400702174},
	doi = {10.1145/3597503.3639187},
	urldate = {2024-06-24},
	booktitle = {{IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {Association for Computing Machinery},
	author = {Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
	month = apr,
	year = {2024},
	pages = {1--13},
}

@article{cassano:multipl-t,
	title = {Knowledge {Transfer} from {High}-{Resource} to {Low}-{Resource} {Programming} {Languages} for {Code} {LLMs}},
	volume = {8},
	abstract = {Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket. This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate tens of thousands of new, validated training items for Racket, OCaml, and Lua from Python. Moreover, we use an open dataset (The Stack) and model (StarCoderBase), which allow us to decontaminate benchmarks and train models on this data without violating the model license. With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase that achieve state-of-the-art performance for Racket, OCaml, and Lua on benchmark problems. For Lua, our fine-tuned model achieves the same performance as StarCoderBase as Python -- a very high-resource language -- on the MultiPL-E benchmarks. For Racket and OCaml, we double their performance on MultiPL-E, bringing their performance close to higher-resource languages such as Ruby and C\#.},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Anderson, Carolyn Jane and {Feldman, Molly Q} and Greenberg, Michael and {Jangda, Abhinav} and {Guha, Arjun}},
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{just:mutants,
	address = {New York, NY, USA},
	title = {Are mutants a valid substitute for real faults in software testing?},
	isbn = {978-1-4503-3056-5},
	url = {https://doi.org/10.1145/2635868.2635929},
	doi = {10.1145/2635868.2635929},
	booktitle = {{ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering} ({FSE})},
	publisher = {Association for Computing Machinery},
	author = {Just, René and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
	month = nov,
	year = {2014},
	keywords = {Test effectiveness, code coverage, mutation analysis, real faults},
	pages = {654--665},
}

@inproceedings{cassano:canitedit,
	title = {Can {It} {Edit}? {Evaluating} the {Ability} of {Large} {Language} {Models} to {Follow} {Code} {Editing} {Instructions}},
	shorttitle = {Can {It} {Edit}?},
	booktitle = {Conference on {Language} {Modelling} ({COLM})},
	author = {Cassano, Federico and Li, Luisa and Sethi, Akul and Shinn, Noah and Brennan-Jones, Abby and Lozhkov, Anton and Anderson, Carolyn Jane and Guha, Arjun},
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@phdthesis{yee:dissertation,
	title = {Predicting {typeScript} type annotations and definitions with machine learning},
	url = {https://repository.library.northeastern.edu/files/neu:4f241c784},
	abstract = {Type information is useful for developing large-scale software systems. Types help prevent bugs, but may be inflexible and hamper quick iteration on early prototypes. TypeScript, a syntactic superset of JavaScript, brings the best of both worlds, allowing programmers to freely mix statically and dynamically typed code, and choose the level of type safety they wish to opt into. However, type migration, the process of migrating an untyped program to a typed version, has remained a labour-intensive manual effort in practice. As a first step towards automated effective type migration, there has been interest in applying machine learning to the narrower problem of type prediction. In this dissertation, I propose to use machine learning to partially migrate JavaScript programs to TypeScript, by predicting type annotations and generating type definitions. To support this thesis, I make three contributions. First, I propose evaluating type prediction by type checking the generated annotations instead of computing accuracy. Second, I fine-tune a large language model with fill-in-the-middle capability to fill in the type and predict type annotations. Finally, I use a similar approach to fine-tune a large language model to generate missing type definitions.--Author's abstract},
	urldate = {2024-08-15},
	author = {Yee, Ming-Ho},
	year = {2024},
}

@inproceedings{ferdowsi:live-llm,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Validating {AI}-{Generated} {Code} with {Live} {Programming}},
	isbn = {9798400703300},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642495},
	doi = {10.1145/3613904.3642495},
	urldate = {2024-08-14},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ferdowsi, Kasra and Huang, Ruanqianqian (Lisa) and James, Michael B. and Polikarpova, Nadia and Lerner, Sorin},
	month = may,
	year = {2024},
	pages = {1--8},
}

@article{vasconcelos:gen-probs,
	title = {Generation {Probabilities} {Are} {Not} {Enough}: {Exploring} the {Effectiveness} of {Uncertainty} {Highlighting} in {AI}-{Powered} {Code} {Completions}},
	shorttitle = {Generation {Probabilities} {Are} {Not} {Enough}},
	url = {https://www.microsoft.com/en-us/research/publication/generation-probabilities-are-not-enough-exploring-the-effectiveness-of-uncertainty-highlighting-in-ai-powered-code-completions/},
	abstract = {Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed […]},
	language = {en-US},
	urldate = {2024-08-14},
	journal = {ToCHI},
	author = {Vasconcelos, Helena and Bansal, Gagan and Fourney, Adam and Liao, Q. Vera and Vaughan, Jennifer Wortman},
	month = aug,
	year = {2024},
}

@inproceedings{ross:programmer-assistant,
	address = {New York, NY, USA},
	series = {{IUI} '23},
	title = {The {Programmer}’s {Assistant}: {Conversational} {Interaction} with a {Large} {Language} {Model} for {Software} {Development}},
	isbn = {9798400701061},
	shorttitle = {The {Programmer}’s {Assistant}},
	url = {https://dl.acm.org/doi/10.1145/3581641.3584037},
	doi = {10.1145/3581641.3584037},
	abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
	urldate = {2024-08-14},
	booktitle = {International {Conference} on {Intelligent} {User} {Interfaces} ({IUI})},
	publisher = {Association for Computing Machinery},
	author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
	month = mar,
	year = {2023},
	pages = {491--514},
}

@inproceedings{ziegler_productivity_2022,
	address = {New York, NY, USA},
	series = {{MAPS} 2022},
	title = {Productivity assessment of neural code completion},
	isbn = {978-1-4503-9273-0},
	url = {https://dl.acm.org/doi/10.1145/3520312.3534864},
	doi = {10.1145/3520312.3534864},
	abstract = {Neural code synthesis has reached a point where snippet generation is accurate enough to be considered for integration into human software development workflows. Commercial products aim to increase programmers’ productivity, without being able to measure it directly. In this case study, we asked users of GitHub Copilot about its impact on their productivity, and sought to find a reflection of their perception in directly measurable user data. We find that the rate with which shown suggestions are accepted, rather than more specific metrics regarding the persistence of completions in the code over time, drives developers’ perception of productivity.},
	urldate = {2024-08-13},
	booktitle = {{ACM} {SIGPLAN} {International} {Symposium} on {Machine} {Programming} ({MAPS})},
	publisher = {Association for Computing Machinery},
	author = {Ziegler, Albert and Kalliamvakou, Eirini and Li, X. Alice and Rice, Andrew and Rifkin, Devon and Simister, Shawn and Sittampalam, Ganesh and Aftandilian, Edward},
	month = jun,
	year = {2022},
	pages = {21--29},
}

@inproceedings{brown:luau-part-two,
	title = {Goals of the {Luau} {Type} {System}, {Two} {Years} {On}},
	abstract = {Programming language designers seek to provide strong tools to help developers reason about their programs. For example, the formal methods community seeks to enable developers to prove correctness properties of their code, and type system designers seek to exclude classes of undesirable behavior from programs. The security community creates tools to help developers achieve their security goals. In order to make these approaches as effective as possible for developers, recent work has integrated approaches from human-computer interaction research into programming language design. This work ...},
	urldate = {2024-08-14},
	publisher = {ACM},
	author = {{Lily Brown} and {Andy Friesen} and {Alan Jeffery}},
	year = {2023},
}

@misc{netflix:ts,
	title = {{TypeScript} migration - {Strict} type of cocktails - {Front} {End} {Happy} {Hour}},
	url = {https://frontendhappyhour.com/episodes/typescript-migration-strict-type-of-cocktails},
	abstract = {Software Engineers deal with migrations at some point or another, and there are always a lot of lessons learned afterward. In this episode, we are joined by Sumana Mohan and Joe King from Netflix to talk about their recent migration to TypeScript on the Netflix website signup flow.},
	urldate = {2022-05-19},
	author = {{Sumana Mohan} and {Joe King} and {Ryan Burgess} and {Jem Young} and {Stacy London}},
	year = {2022},
}

@misc{stripe:sorbet,
	title = {Sorbet: {Stripe}’s type checker for {Ruby}},
	shorttitle = {Sorbet},
	url = {https://stripe.com/blog/sorbet-stripes-type-checker-for-ruby},
	abstract = {Stripe uses Sorbet to type check our 15 million lines of Ruby code, making engineers more productive. Here's why we love Sorbet.},
	language = {en-us},
	urldate = {2022-05-19},
	author = {{Jake Zimmerman}},
	year = {2022},
}

@misc{quip:ts,
	title = {The {Road} to {TypeScript} at {Quip}, {Part} {Two}},
	url = {https://quip.com/blog/the-road-to-typescript-at-quip-part-two},
	abstract = {The Road to TypeScript at Quip, Part Two},
	language = {en},
	urldate = {2022-05-19},
	journal = {Quip},
	author = {{Mihai Parparita}},
	year = {2020},
}

@misc{heap:ts,
	title = {How we failed, then succeeded, at migrating to {TypeScript}},
	url = {https://heap.io/blog/migrating-to-typescript},
	abstract = {Here's how we migrated from CoffeeScript to TypeScript and what we learned along the way.},
	language = {en},
	urldate = {2022-05-19},
	journal = {Heap},
	author = {{Luke Autry}},
}

@misc{slack:ts,
	title = {{TypeScript} at {Slack}},
	url = {https://slack.engineering/typescript-at-slack/},
	abstract = {When Brendan Eich created the very first version of JavaScript for Netscape Navigator 2.0 in merely ten days, it’s likely that he did not expect how far the Slack Desktop App would take his invention: We use one JavaScript code base to build a multi-threaded desktop application, routinely interacting with native code, targeting Windows, macOS, …},
	language = {en-US},
	urldate = {2022-05-19},
	journal = {Slack Engineering},
	author = {{Felix Rieseberg}},
	year = {2017},
	note = {Section: Uncategorized},
}

@misc{dropbox:mypy,
	title = {Our journey to type checking 4 million lines of {Python}},
	url = {https://dropbox.tech/application/our-journey-to-type-checking-4-million-lines-of-python},
	language = {en},
	urldate = {2022-05-19},
	author = {Lehtosalo, Jukka},
	year = {2019},
}

@misc{github-copilot,
	title = {{GitHub} {Copilot}: {Your} {AI} pair programmer},
	url = {https://github.com/features/copilot/},
	urldate = {2024-03-25},
	author = {{GitHub}},
	year = {2021},
}

@inproceedings{park_generative_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	isbn = {9798400701320},
	shorttitle = {Generative {Agents}},
	url = {https://doi.org/10.1145/3586183.3606763},
	doi = {10.1145/3586183.3606763},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2024-07-24},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2023},
	pages = {1--22},
}

@misc{noauthor_llama_nodate,
	title = {The {Llama} 3 {Herd} of {Models} {\textbar} {Research} - {AI} at {Meta}},
	url = {https://ai.meta.com/research/publications/the-llama-3-herd-of-models/},
	urldate = {2024-07-24},
}

@inproceedings{cassano:canitedit-llm4code,
	title = {Can {It} {Edit}? {Evaluating} the {Ability} of {Large} {Language} {Models} to {Follow} {Code} {Editing} {Instructions}},
	shorttitle = {Can {It} {Edit}?},
	abstract = {A significant amount of research is focused on developing and evaluating large language models for a variety of code synthesis tasks. These include synthesizing code from natural language instructions, synthesizing tests from code, and synthesizing explanations of code. In contrast, the behavior of instructional code editing with LLMs is understudied. These are tasks in which the model is instructed to update a block of code provided in a prompt. The editing instruction may ask for a feature to added or removed, describe a bug and ask for a fix, ask for a different kind of solution, or many other common code editing tasks. We introduce a carefully crafted benchmark of code editing tasks and use it evaluate several cutting edge LLMs. Our evaluation exposes a significant gap between the capabilities of state-of-the-art open and closed models. For example, even GPT-3.5-Turbo is 8.8\% better than the best open model at editing code. We also introduce a new, carefully curated, permissively licensed training set of code edits coupled with natural language instructions. Using this training set, we show that we can fine-tune open Code LLMs to significantly improve their code editing capabilities.},
	urldate = {2024-02-06},
	booktitle = {International {Workshop} on {Large} {Language} {Models} for {Code} ({LLM4Code})},
	author = {Cassano, Federico and Li, Luisa and Sethi, Akul and Shinn, Noah and Brennan-Jones, Abby and Lozhkov, Anton and Anderson, Carolyn Jane and Guha, Arjun},
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{pan:lost-in-translation,
	address = {New York, NY, USA},
	series = {{ICSE} '24},
	title = {Lost in {Translation}: {A} {Study} of {Bugs} {Introduced} by {Large} {Language} {Models} while {Translating} {Code}},
	isbn = {9798400702174},
	shorttitle = {Lost in {Translation}},
	url = {https://doi.org/10.1145/3597503.3639226},
	doi = {10.1145/3597503.3639226},
	abstract = {Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1\% to 47.3\% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5\% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.},
	urldate = {2024-07-10},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {Association for Computing Machinery},
	author = {Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh},
	month = apr,
	year = {2024},
	pages = {1--13},
}

@article{liu_no_2024,
	title = {No {Need} to {Lift} a {Finger} {Anymore}? {Assessing} the {Quality} of {Code} {Generation} by {ChatGPT}},
	volume = {50},
	issn = {1939-3520},
	shorttitle = {No {Need} to {Lift} a {Finger} {Anymore}?},
	url = {https://ieeexplore.ieee.org/document/10507163/?arnumber=10507163},
	doi = {10.1109/TSE.2024.3392499},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using ChatGPT, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by ChatGPT, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability, chatting between users and ChatGPT for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of ChatGPT in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) ChatGPT is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with 48.14\%48.14\% advantage in Accepted rate on judgment platform, but ChatGPT's ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with ChatGPT generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by ChatGPT has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than 89\%89\% of vulnerabilities successfully addressed; and (4) code generation may be affected by ChatGPT's non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the ChatGPT-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.},
	number = {6},
	urldate = {2024-07-09},
	journal = {IEEE Transactions on Software Engineering},
	author = {Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
	month = jun,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {ChatGPT, Chatbots, Codes, Complexity theory, Electronic mail, Large language model, Security, Task analysis, Transformers, code generation},
	pages = {1548--1584},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks’ internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, humanunderstandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task (Wang et al., 2022) to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	language = {en},
	urldate = {2024-06-28},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{xia:universal-fuzzing,
	address = {New York, NY, USA},
	title = {{Fuzz4All}: {Universal} {Fuzzing} with {Large} {Language} {Models}},
	isbn = {9798400702174},
	shorttitle = {{Fuzz4All}},
	doi = {10.1145/3597503.3639121},
	urldate = {2024-06-25},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {Association for Computing Machinery},
	author = {Xia, Chunqiu Steven and Paltenghi, Matteo and Le Tian, Jia and Pradel, Michael and Zhang, Lingming},
	month = apr,
	year = {2024},
	pages = {1--13},
}

@inproceedings{burns_weak--strong_2024,
	title = {Weak-to-{Strong} {Generalization}: {Eliciting} {Strong} {Capabilities} {With} {Weak} {Supervision}},
	shorttitle = {Weak-to-{Strong} {Generalization}},
	language = {en},
	urldate = {2024-06-25},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and Sutskever, Ilya and Wu, Jeffrey},
	month = jun,
	year = {2024},
}

@inproceedings{wei:magicoder,
	title = {Magicoder: {Empowering} {Code} {Generation} with {OSS}-{Instruct}},
	shorttitle = {Magicoder},
	abstract = {We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using **OSS-Instruct**, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.},
	language = {en},
	urldate = {2024-06-25},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
	month = jun,
	year = {2024},
}

@inproceedings{chen:drb-ml,
	address = {New York, NY, USA},
	title = {Data {Race} {Detection} {Using} {Large} {Language} {Models}},
	isbn = {9798400707858},
	url = {https://doi.org/10.1145/3624062.3624088},
	doi = {10.1145/3624062.3624088},
	abstract = {Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.},
	urldate = {2024-06-25},
	booktitle = {Workshops of {The} {International} {Conference} on {High} {Performance} {Computing}, {Network}, {Storage}, and {Analysis} ({SC}-{W})},
	publisher = {Association for Computing Machinery},
	author = {Chen, Le and Ding, Xianzhong and Emani, Murali and Vanderbruggen, Tristan and Lin, Pei-Hung and Liao, Chunhua},
	month = nov,
	year = {2023},
	pages = {215--223},
}

@inproceedings{chen_transferability_2022,
	title = {On the transferability of pre-trained language models for low-resource programming languages},
	isbn = {978-1-4503-9298-3},
	doi = {10.1145/3524610.3527917},
	abstract = {A recent study by Ahmed and Devanbu reported that using a corpus of code written in multilingual datasets to fine-tune multilingual Pre-trained Language Models (PLMs) achieves higher performance as opposed to using a corpus of code written in just one programming language. However, no analysis was made with respect to fine-tuning monolingual PLMs. Furthermore, some programming languages are inherently different and code written in one language usually cannot be interchanged with the others, i.e., Ruby and Java code possess very different structure. To better understand how monolingual and multilingual PLMs affect different programming languages, we investigate 1) the performance of PLMs on Ruby for two popular Software Engineering tasks: Code Summarization and Code Search, 2) the strategy (to select programming languages) that works well on fine-tuning multilingual PLMs for Ruby, and 3) the performance of the fine-tuned PLMs on Ruby given different code lengths.In this work, we analyze over a hundred of pre-trained and fine-tuned models. Our results show that 1) multilingual PLMs have a lower Performance-to-Time Ratio (the BLEU, METEOR, or MRR scores over the fine-tuning duration) as compared to monolingual PLMs, 2) our proposed strategy to select target programming languages to fine-tune multilingual PLMs is effective --- it reduces the time to fine-tune yet achieves higher performance in Code Summarization and Code Search tasks, and 3) our proposed strategy consistently shows good performance on different code lengths.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	publisher = {Association for Computing Machinery},
	author = {Chen, Fuxiang and Fard, Fatemeh H. and Lo, David and Bryksin, Timofey},
	month = oct,
	year = {2022},
	pages = {401--412},
}

@inproceedings{zhang_multilingual_2023,
	title = {Multilingual {Code} {Co}-evolution using {Large} {Language} {Models}},
	isbn = {9798400703270},
	doi = {10.1145/3611643.3616350},
	abstract = {Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C\#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Jiyang and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos},
	month = nov,
	year = {2023},
	pages = {695--707},
}

@inproceedings{ouyang:instructgpt,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{lee_deduplicating_2022,
	address = {Dublin, Ireland},
	title = {Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}},
	url = {https://aclanthology.org/2022.acl-long.577},
	doi = {10.18653/v1/2022.acl-long.577},
	abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets—for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation. Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.},
	urldate = {2024-06-24},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	month = may,
	year = {2022},
	pages = {8424--8445},
}

@inproceedings{qinkai:codegeex,
	title = {{CodeGeeX}: {A} {Pre}-{Trained} {Model} for {Code} {Generation} with {Multilingual} {Evaluations} on {HumanEval}-{X}},
	shorttitle = {{CodeGeeX}},
	doi = {10.1145/3580305.3599790},
	booktitle = {{ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} ({KDD})},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie},
	year = {2023},
	pages = {5673--5684},
}

@inproceedings{geva_transformer_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://aclanthology.org/2022.emnlp-main.3},
	doi = {10.18653/v1/2022.emnlp-main.3},
	abstract = {Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
	urldate = {2024-06-20},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	keywords = {To Read},
	pages = {30--45},
}

@misc{hong_intrinsic_2024,
	title = {Intrinsic {Evaluation} of {Unlearning} {Using} {Parametric} {Knowledge} {Traces}},
	url = {http://arxiv.org/abs/2406.11614},
	doi = {10.48550/arXiv.2406.11614},
	abstract = {The task of "unlearning" certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance for mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general methodology for eliciting directions in the parameter space (termed "concept vectors") that encode concrete concepts, and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors, while directly ablating these vectors demonstrably removes the associated knowledge from the LLMs and significantly reduces their susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parametric-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Hong, Yihuai and Yu, Lei and Ravfogel, Shauli and Yang, Haiqin and Geva, Mor},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11614 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, To Read},
}

@misc{anand_critical_2024,
	title = {A {Critical} {Study} of {What} {Code}-{LLMs} ({Do} {Not}) {Learn}},
	url = {http://arxiv.org/abs/2406.11930},
	doi = {10.48550/arXiv.2406.11930},
	abstract = {Large Language Models trained on code corpora (code-LLMs) have demonstrated impressive performance in various coding assistance tasks. However, despite their increased size and training dataset, code-LLMs still have limitations such as suggesting codes with syntactic errors, variable misuse etc. Some studies argue that code-LLMs perform well on coding tasks because they use self-attention and hidden representations to encode relations among input tokens. However, previous works have not studied what code properties are not encoded by code-LLMs. In this paper, we conduct a fine-grained analysis of attention maps and hidden representations of code-LLMs. Our study indicates that code-LLMs only encode relations among specific subsets of input tokens. Specifically, by categorizing input tokens into syntactic tokens and identifiers, we found that models encode relations among syntactic tokens and among identifiers, but they fail to encode relations between syntactic tokens and identifiers. We also found that fine-tuned models encode these relations poorly compared to their pre-trained counterparts. Additionally, larger models with billions of parameters encode significantly less information about code than models with only a few hundred million parameters.},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Anand, Abhinav and Verma, Shweta and Narasimhan, Krishna and Mezini, Mira},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11930 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, To Read},
}

@inproceedings{ellis_dreamcoder_2021,
	address = {New York, NY, USA},
	title = {{DreamCoder}: bootstrapping inductive program synthesis with wake-sleep library learning},
	isbn = {978-1-4503-8391-2},
	shorttitle = {{DreamCoder}},
	url = {https://doi.org/10.1145/3453483.3454080},
	abstract = {We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of "wake-sleep" approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.},
	urldate = {2022-04-27},
	booktitle = {{ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	publisher = {Association for Computing Machinery},
	author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sablé-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
	month = jun,
	year = {2021},
	pages = {835--850},
}

@inproceedings{xie_explanation_2021,
	title = {An {Explanation} of {In}-context {Learning} as {Implicit} {Bayesian} {Inference}},
	url = {https://openreview.net/forum?id=RdJVFCHjUMI},
	abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
	language = {en},
	urldate = {2024-06-19},
	author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
	month = oct,
	year = {2021},
}

@misc{le_rethinking_2022,
	title = {Rethinking {Data}-driven {Networking} with {Foundation} {Models}: {Challenges} and {Opportunities}},
	shorttitle = {Rethinking {Data}-driven {Networking} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2211.06494},
	abstract = {Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Le, Franck and Srivatsa, Mudhakar and Ganti, Raghu and Sekar, Vyas},
	month = nov,
	year = {2022},
	note = {arXiv:2211.06494 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture, LLM, Networking},
}

@misc{belrose_eliciting_2023,
	title = {Eliciting {Latent} {Predictions} from {Transformers} with the {Tuned} {Lens}},
	url = {http://arxiv.org/abs/2303.08112},
	doi = {10.48550/arXiv.2303.08112},
	abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the {\textbackslash}emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
	month = nov,
	year = {2023},
	note = {arXiv:2303.08112 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{conmy_towards_2023,
	title = {Towards {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html},
	language = {en},
	urldate = {2024-06-18},
	journal = {Advances in Neural Information Processing Systems},
	author = {Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
	month = dec,
	year = {2023},
	pages = {16318--16352},
}

@misc{zhou:raspl,
	title = {What {Algorithms} can {Transformers} {Learn}? {A} {Study} in {Length} {Generalization}},
	shorttitle = {What {Algorithms} can {Transformers} {Learn}?},
	url = {http://arxiv.org/abs/2310.16028},
	doi = {10.48550/arXiv.2310.16028},
	abstract = {Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the "min-degree-interpolator" model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16028 [cs, stat]},
	keywords = {To Read},
}

@inproceedings{pal_future_2023,
	address = {Singapore},
	title = {Future {Lens}: {Anticipating} {Subsequent} {Tokens} from a {Single} {Hidden} {State}},
	shorttitle = {Future {Lens}},
	url = {https://aclanthology.org/2023.conll-1.37},
	doi = {10.18653/v1/2023.conll-1.37},
	abstract = {We conjecture that hidden state vectors corresponding to individual input tokens encode information sufficient to accurately predict several tokens ahead. More concretely, in this paper we ask: Given a hidden (internal) representation of a single token at position t in an input, can we reliably anticipate the tokens that will appear at positions {\textbackslash}mbox{\textbackslash}geq t + 2? To test this, we measure linear approximation and causal intervention methods in GPT-J-6B to evaluate the degree to which individual hidden states in the network contain signal rich enough to predict future hidden states and, ultimately, token outputs. We find that, at some layers, we can approximate a model's output with more than 48\% accuracy with respect to its prediction of subsequent tokens through a single hidden state. Finally we present a “Future Lens” visualization that uses these methods to create a new view of transformer states.},
	urldate = {2024-06-06},
	booktitle = {Proceedings of the 27th {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Pal, Koyena and Sun, Jiuding and Yuan, Andrew and Wallace, Byron and Bau, David},
	editor = {Jiang, Jing and Reitter, David and Deng, Shumin},
	month = dec,
	year = {2023},
	pages = {548--560},
}

@misc{parth_thakkar_copilot_2024,
	title = {Copilot {Internals}},
	url = {https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html},
	abstract = {Hacky repo to see what the Copilot extension sends to the server},
	language = {en-US},
	urldate = {2024-06-03},
	journal = {copilot-explorer},
	author = {{Parth Thakkar}},
	month = jun,
	year = {2024},
	keywords = {LLM UX},
}

@inproceedings{schaeffer_are_2023,
	title = {Are {Emergent} {Abilities} of {Large} {Language} {Models} a {Mirage}?},
	url = {https://openreview.net/forum?id=ITw9edRDlD},
	abstract = {Recent work claims that large language models display {\textbackslash}textit\{emergent abilities\}, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their {\textbackslash}textit\{sharpness\}, transitioning seemingly instantaneously from not present to present, and their {\textbackslash}textit\{unpredictability\}, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due the researcher’s choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
	language = {en},
	urldate = {2024-06-03},
	author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	month = nov,
	year = {2023},
	keywords = {To Read},
}

@misc{marks_sparse_2024,
	title = {Sparse {Feature} {Circuits}: {Discovering} and {Editing} {Interpretable} {Causal} {Graphs} in {Language} {Models}},
	shorttitle = {Sparse {Feature} {Circuits}},
	url = {http://arxiv.org/abs/2403.19647},
	doi = {10.48550/arXiv.2403.19647},
	abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19647 [cs]},
	keywords = {Interpretability, To Read},
}

@misc{ahuja_learning_2024,
	title = {Learning {Syntax} {Without} {Planting} {Trees}: {Understanding} {When} and {Why} {Transformers} {Generalize} {Hierarchically}},
	shorttitle = {Learning {Syntax} {Without} {Planting} {Trees}},
	url = {http://arxiv.org/abs/2404.16367},
	doi = {10.48550/arXiv.2404.16367},
	abstract = {Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Ahuja, Kabir and Balachandran, Vidhisha and Panwar, Madhur and He, Tianxing and Smith, Noah A. and Goyal, Navin and Tsvetkov, Yulia},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16367 [cs]},
	keywords = {Interpretability, To Read},
}

@misc{shi_compositional_2022,
	title = {Compositional {Generalization} and {Decomposition} in {Neural} {Program} {Synthesis}},
	url = {http://arxiv.org/abs/2204.03758},
	doi = {10.48550/arXiv.2204.03758},
	abstract = {When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, what we can measure is whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we focus on measuring the ability of learned program synthesizers to compositionally generalize. We first characterize several different axes along which program synthesis methods would be desired to generalize, e.g., length generalization, or the ability to combine known subroutines in new ways that do not occur in the training data. Based on this characterization, we introduce a benchmark suite of tasks to assess these abilities based on two popular existing datasets, SCAN and RobustFill. Finally, we make first attempts to improve the compositional generalization ability of Transformer models along these axes through novel attention mechanisms that draw inspiration from a human-like decomposition strategy. Empirically, we find our modified Transformer models generally perform better than natural baselines, but the tasks remain challenging.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Shi, Kensen and Hong, Joey and Zaheer, Manzil and Yin, Pengcheng and Sutton, Charles},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03758 [cs, stat]},
	keywords = {To Read},
}

@misc{meng_locating_2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://arxiv.org/abs/2202.05262},
	doi = {10.48550/arXiv.2202.05262},
	abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	month = oct,
	year = {2022},
	note = {arXiv:2202.05262 [cs]},
}

@inproceedings{yasunaga_break-it-fix-it_2021,
	title = {Break-{It}-{Fix}-{It}: {Unsupervised} {Learning} for {Program} {Repair}},
	shorttitle = {Break-{It}-{Fix}-{It}},
	url = {https://proceedings.mlr.press/v139/yasunaga21a.html},
	abstract = {We consider repair tasks: given a critic (e.g., compiler) that assesses the quality of an input, the goal is to train a fixer that converts a bad example (e.g., code with syntax errors) into a good one (e.g., code with no errors). Existing works create training data consisting of (bad, good) pairs by corrupting good examples using heuristics (e.g., dropping tokens). However, fixers trained on this synthetically-generated data do not extrapolate well to the real distribution of bad inputs. To bridge this gap, we propose a new training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use the critic to check a fixer’s output on real bad inputs and add good (fixed) outputs to the training data, and (ii) we train a breaker to generate realistic bad code from good code. Based on these ideas, we iteratively update the breaker and the fixer while using them in conjunction to generate more paired data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new dataset we introduce where the goal is to repair Python code with AST parse errors; and DeepFix, where the goal is to repair C code with compiler errors. BIFI outperforms existing methods, obtaining 90.5\% repair accuracy on GitHub-Python (+28.5\%) and 71.7\% on DeepFix (+5.6\%). Notably, BIFI does not require any labeled data; we hope it will be a strong starting point for unsupervised learning of various repair tasks.},
	language = {en},
	urldate = {2024-06-03},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yasunaga, Michihiro and Liang, Percy},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {To Read},
	pages = {11941--11952},
}

@inproceedings{yasunaga_graph-based_2020,
	title = {Graph-based, {Self}-{Supervised} {Program} {Repair} from {Diagnostic} {Feedback}},
	url = {https://proceedings.mlr.press/v119/yasunaga20a.html},
	abstract = {We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2\% full repair rate on DeepFix (+22.9\% over the prior best), and 48.4\% synthesis success rate on SPoC (+3.7\% over the prior best).},
	language = {en},
	urldate = {2024-06-03},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yasunaga, Michihiro and Liang, Percy},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10799--10808},
}

@inproceedings{clement_pymt5_2020,
	address = {Online},
	title = {{PyMT5}: multi-mode translation of natural language and {Python} code with transformers},
	shorttitle = {{PyMT5}},
	url = {https://aclanthology.org/2020.emnlp-main.728},
	doi = {10.18653/v1/2020.emnlp-main.728},
	abstract = {Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1\% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.},
	urldate = {2024-06-03},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Clement, Colin and Drain, Dawn and Timcheck, Jonathan and Svyatkovskiy, Alexey and Sundaresan, Neel},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	keywords = {Code LLM},
	pages = {9052--9065},
}

@article{merrill_provable_2021,
	title = {Provable {Limitations} of {Acquiring} {Meaning} from {Ungrounded} {Form}: {What} {Will} {Future} {Language} {Models} {Understand}?},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {Provable {Limitations} of {Acquiring} {Meaning} from {Ungrounded} {Form}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00412/107385/Provable-Limitations-of-Acquiring-Meaning-from},
	doi = {10.1162/tacl_a_00412},
	abstract = {Abstract
            Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever “understand” raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of “assertions”: textual contexts that provide indirect clues about the underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation of languages that satisfy a strong notion of semantic transparency. However, for classes of languages where the same expression can take different values in different contexts, we show that emulation can become uncomputable. Finally, we discuss differences between our formal model and natural language, exploring how our results generalize to a modal setting and other semantic relations. Together, our results suggest that assertions in code or language do not provide sufficient signal to fully emulate semantic representations. We formalize ways in which ungrounded language models appear to be fundamentally limited in their ability to “understand”.},
	language = {en},
	urldate = {2024-06-02},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Merrill, William and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A.},
	month = sep,
	year = {2021},
	pages = {1047--1060},
}

@misc{oh_gecko_2024,
	title = {{GECKO}: {Generative} {Language} {Model} for {English}, {Code} and {Korean}},
	shorttitle = {{GECKO}},
	url = {http://arxiv.org/abs/2405.15640},
	abstract = {We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture. In this report, we share the experiences of several efforts to build a better data pipeline for the corpus and to train our model. GECKO shows great efficiency in token generations for both Korean and English, despite its small size of vocabulary. We measure the performance on the representative benchmarks in terms of Korean, English and Code, and it exhibits great performance on KMMLU (Korean MMLU) and modest performance in English and Code, even with its smaller number of trained tokens compared to English-focused LLMs. GECKO is available to the open-source community under a permissive license. We hope our work offers a research baseline and practical insights for Korean LLM research. The model can be found at: https://huggingface.co/kifai/GECKO-7B},
	urldate = {2024-06-02},
	author = {Oh, Sungwoo and Kim, Donggyu},
	month = may,
	year = {2024},
	note = {arXiv:2405.15640 [cs]},
}

@inproceedings{cosma_rocode_2024,
	address = {Torino, Italia},
	title = {{RoCode}: {A} {Dataset} for {Measuring} {Code} {Intelligence} from {Problem} {Definitions} in {Romanian}},
	shorttitle = {{RoCode}},
	url = {https://aclanthology.org/2024.lrec-main.1236},
	abstract = {Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.},
	urldate = {2024-05-27},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Cosma, Adrian and Iordache, Ioan-Bogdan and Rosso, Paolo},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {14173--14185},
}

@inproceedings{yao:react-prompting,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {https://openreview.net/forum?id=WE_vluYUL-X},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
	language = {en},
	urldate = {2024-05-19},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R. and Cao, Yuan},
	month = sep,
	year = {2022},
}

@inproceedings{shinn:reflexion,
	title = {Reflexion: language agents with verbal reinforcement learning},
	volume = {36},
	shorttitle = {Reflexion},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = dec,
	year = {2023},
	pages = {8634--8652},
}

@misc{cobbe:gsm8k,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	doi = {10.48550/arXiv.2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{brown:gpt3,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2024-05-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{gao:pal,
	title = {{PAL}: {Program}-aided {Language} {Models}},
	shorttitle = {{PAL}},
	url = {https://proceedings.mlr.press/v202/gao23f.html},
	abstract = {Large language models (LLMs) have demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and others. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on GSM8K, surpassing PaLM which uses chain-of-thought by absolute 15\% top-1.},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {10764--10799},
}

@inproceedings{babe:studenteval,
	title = {{StudentEval}: {A} {Benchmark} of {Student}-{Written} {Prompts} for {Large} {Language} {Models} of {Code}},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}},
	author = {Babe, Hannah McLean and Nguyen, Sydney and Zi, Yangtian and Guha, Arjun and Feldman, Molly Q. and Anderson, Carolyn Jane},
	year = {2024},
}

@inproceedings{liang:copilot-survey,
	address = {New York, NY, USA},
	title = {A {Large}-{Scale} {Survey} on the {Usability} of {AI} {Programming} {Assistants}: {Successes} and {Challenges}},
	isbn = {9798400702174},
	shorttitle = {A {Large}-{Scale} {Survey} on the {Usability} of {AI} {Programming} {Assistants}},
	url = {https://dl.acm.org/doi/10.1145/3597503.3608128},
	doi = {10.1145/3597503.3608128},
	abstract = {The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {Association for Computing Machinery},
	author = {Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.},
	month = feb,
	year = {2024},
	keywords = {AI programming assistants, usability study},
	pages = {1--13},
}

@inproceedings{dunay:multiline-code-compose,
	title = {Multi-line {AI}-assisted {Code} {Authoring}},
	abstract = {CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10’s of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers. First, we discuss how multi-line suggestions can have a "jarring" effect, as the LLM’s suggestions constantly move around the developer’s existing code, which would otherwise result in decreased productivity and satisfaction. Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.},
	booktitle = {{ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering} ({FSE})},
	author = {Dunay, Omer and Cheng, Daniel and Tait, Adam and Thakkar, Parth and Rigby, Peter C. and Chiu, Andy and Ahmad, Imad and Ganesan, Arun and Maddila, Chandra and Murali, Vijayaraghavan and Tayyebi, Ali and Nagappan, Nachiappan},
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{murali:code-compose,
	title = {{AI}-assisted {Code} {Authoring} at {Scale}: {Fine}-tuning, deploying, and mixed methods evaluation},
	shorttitle = {{AI}-assisted {Code} {Authoring} at {Scale}},
	urldate = {2024-04-05},
	booktitle = {{ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering} ({FSE})},
	author = {Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan and Rigby, Peter C.},
	month = feb,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{bird:taking-flight-with-copilot,
	title = {Taking {Flight} with {Copilot}: {Early} insights and opportunities of {AI}-powered pair-programming tools},
	volume = {20},
	issn = {1542-7730},
	shorttitle = {Taking {Flight} with {Copilot}},
	url = {https://dl.acm.org/doi/10.1145/3582083},
	doi = {10.1145/3582083},
	abstract = {Over the next five years, AI-powered tools likely will be helping developers in many diverse tasks. For example, such models may be used to improve code review, directing reviewers to parts of a change where review is most needed or even directly providing feedback on changes. Models such as Codex may suggest fixes for defects in code, build failures, or failing tests. These models are able to write tests automatically, helping to improve code quality and downstream reliability of distributed systems. This study of Copilot shows that developers spend more time reviewing code than actually writing code. As AI-powered tools are integrated into more software development tasks, developer roles will shift so that more time is spent assessing suggestions related to the task than doing the task itself.},
	number = {6},
	urldate = {2024-05-15},
	journal = {Queue},
	author = {Bird, Christian and Ford, Denae and Zimmermann, Thomas and Forsgren, Nicole and Kalliamvakou, Eirini and Lowdermilk, Travis and Gazit, Idan},
	month = jan,
	year = {2023},
	pages = {Pages 10:35--Pages 10:57},
}

@misc{claude3,
	title = {Introducing the next generation of {Claude}},
	url = {https://www.anthropic.com/news/claude-3-family},
	abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
	language = {en},
	urldate = {2024-05-15},
	author = {{Claude Team}},
}

@misc{rimsky:caa,
	title = {Steering {Llama} 2 via {Contrastive} {Activation} {Addition}},
	url = {http://arxiv.org/abs/2312.06681},
	doi = {10.48550/arXiv.2312.06681},
	abstract = {We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Rimsky, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
	month = mar,
	year = {2024},
	note = {arXiv:2312.06681 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{muennighoff:octopack,
	title = {{OctoPack}: {Instruction} {Tuning} {Code} {Large} {Language} {Models}},
	shorttitle = {{OctoPack}},
	url = {https://openreview.net/forum?id=mw1PWNSWZP},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and von Werra, Leandro and Longpre, Shayne},
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{lucchetti:steering-type-prediction,
	title = {Activation {Steering} for {Robust} {Type} {Prediction} in {CodeLLMs}},
	url = {http://arxiv.org/abs/2404.01903},
	doi = {10.48550/arXiv.2404.01903},
	abstract = {Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90\% of type mispredictions. Finally, we show that steering vectors calculated from Python activations reliably correct type mispredictions in TypeScript, and vice versa. This result suggests that LLMs may be learning to transfer knowledge of types across programming languages.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Lucchetti, Francesca and Guha, Arjun},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{hendel:task-vectors,
	address = {Singapore},
	title = {In-{Context} {Learning} {Creates} {Task} {Vectors}},
	url = {https://aclanthology.org/2023.findings-emnlp.624},
	doi = {10.18653/v1/2023.findings-emnlp.624},
	abstract = {In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the “standard' machine learning framework, where one uses a training set S to find a best-fitting function f(x) in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query x and a single “task vector' calculated from the training set. Thus, ICL can be seen as compressing S into a single task vector {\textbackslash}boldsymbolþeta(S) and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.},
	urldate = {2024-03-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hendel, Roee and Geva, Mor and Globerson, Amir},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	year = {2023},
	pages = {9318--9333},
}

@inproceedings{todd:function-vectors,
	title = {Function {Vectors} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=AwyxtyMwaG},
	abstract = {We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some extent they can be summed to create vectors that trigger new complex tasks. Our findings show that compact, causal internal vector representations of function abstractions can be explicitly extracted from LLMs.},
	language = {en},
	urldate = {2024-04-12},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Todd, Eric and Li, Millicent and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C. and Bau, David},
	year = {2024},
}

@misc{hernandez_linearity_2024,
	title = {Linearity of {Relation} {Decoding} in {Transformer} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.09124},
	abstract = {Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in LMs.},
	language = {en},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
	month = feb,
	year = {2024},
	note = {arXiv:2308.09124 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{laurencon_what_2024,
	title = {What matters when building vision-language models?},
	url = {http://arxiv.org/abs/2405.02246},
	abstract = {The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Laurençon, Hugo and Tronchon, Léo and Cord, Matthieu and Sanh, Victor},
	month = may,
	year = {2024},
	note = {arXiv:2405.02246 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao:code-gemma,
	title = {{CodeGemma}: {Open} {Code} {Models} {Based} on {Gemma}},
	url = {https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf},
	author = {Zhao, Heri and Hui, Jeffery and Nguyen, Nam and Siqi},
	year = {2024},
}

@inproceedings{ainslie:gqa,
	address = {Singapore},
	title = {{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}},
	shorttitle = {{GQA}},
	url = {https://aclanthology.org/2023.emnlp-main.298},
	doi = {10.18653/v1/2023.emnlp-main.298},
	abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {4895--4901},
}

@article{su:roformer,
	title = {{RoFormer}: {Enhanced} transformer with {Rotary} {Position} {Embedding}},
	volume = {568},
	issn = {0925-2312},
	shorttitle = {{RoFormer}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
	doi = {10.1016/j.neucom.2023.127063},
	abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model\_doc/roformer.},
	urldate = {2024-05-06},
	journal = {Neurocomputing},
	author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
	month = feb,
	year = {2024},
	keywords = {Natural language processing, Position information encoding, Pre-trained language models, Pre-training},
	pages = {127063},
}

@inproceedings{liu:generating-wikipedia,
	title = {Generating {Wikipedia} by {Summarizing} {Long} {Sequences}},
	abstract = {We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate ﬂuent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reﬂected in perplexity, ROUGE scores and human evaluations.},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
	year = {2018},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ding:crosscodeval,
	title = {{CrossCodeEval}: {A} {Diverse} and {Multilingual} {Benchmark} for {Cross}-{File} {Code} {Completion}},
	shorttitle = {{CrossCodeEval}},
	url = {https://openreview.net/forum?id=wgDcbBMSfh&noteId=HC5kQ6mgXn},
	abstract = {Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C\#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file. Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {Neural {Information} {Processing} {Systems} {Track} on {Datasets} and {Benchmarks} ({NeurIPS} {Datasets} and {Benchmarks})},
	author = {Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ding, Hantian and Tan, Ming and Jain, Nihal and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
	month = nov,
	year = {2023},
}

@inproceedings{macneil:code-explanations,
	address = {New York, NY, USA},
	title = {Generating {Diverse} {Code} {Explanations} using the {GPT}-3 {Large} {Language} {Model}},
	volume = {2},
	isbn = {978-1-4503-9195-5},
	doi = {10.1145/3501709.3544280},
	abstract = {Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing error-specific feedback [10, 16]. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Github's Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs' potential to support learning by explaining numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.},
	urldate = {2024-02-05},
	publisher = {Association for Computing Machinery},
	author = {MacNeil, Stephen and Tran, Andrew and Mogil, Dan and Bernstein, Seth and Ross, Erin and Huang, Ziheng},
	month = aug,
	year = {2022},
	keywords = {code explanations, computer science education, large language models, natural language processing},
	pages = {37--39},
}

@misc{wang_exploring_2024,
	title = {Exploring {Multi}-{Lingual} {Bias} of {Large} {Code} {Models} in {Code} {Generation}},
	url = {http://arxiv.org/abs/2404.19368},
	abstract = {Code generation aims to synthesize code and fulfill functional requirements based on natural language (NL) specifications, which can greatly improve development efficiency. In the era of large language models (LLMs), large code models (LCMs) have been recently proposed to generate source code. LCMs can generate highly feasible solutions for programming problems described in natural language. Despite the effectiveness, we observe a noticeable multilingual bias in the generation performance of LCMs. Specifically, LCMs demonstrate proficiency in generating solutions when provided with instructions in English, yet may falter when faced with semantically equivalent instructions in other NLs such as Chinese. Moreover, the ability of LCMs to generate code exhibits variety across different programming languages (PLs), such as Python and C++. The observed phenomenon indicates the presence of multi-lingual bias within the generative capabilities of LCMs, which has remained unexplored.},
	language = {en},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Wang, Chaozheng and Li, Zongjie and Gao, Cuiyun and Wang, Wenxuan and Peng, Ting and Huang, Hailiang and Deng, Yuetang and Wang, Shuai and Lyu, Michael R.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19368 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{szafraniec:transcoder-ir,
	title = {Code {Translation} with {Compiler} {Representations}},
	url = {https://openreview.net/forum?id=XomEU3eNeSQ},
	abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java → Rust pair with greedy decoding. With beam search, it increases the number of correct translations by 5.5\% in average. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.},
	urldate = {2024-02-05},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh James and Labatut, Patrick and Charton, Francois and Synnaeve, Gabriel},
	month = sep,
	year = {2022},
}

@misc{gu:cruxeval,
	title = {{CRUXEval}: {A} {Benchmark} for {Code} {Reasoning}, {Understanding} and {Execution}},
	shorttitle = {{CRUXEval}},
	url = {http://arxiv.org/abs/2401.03065},
	abstract = {We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75\% and 81\% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50\% and 46\% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple programs as a lens into its code reasoning capabilities and areas for improvement.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Gu, Alex and Rozière, Baptiste and Leather, Hugh and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida I.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03065 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{babe:studenteval-llm4code,
	title = {{StudentEval}: {A} {Benchmark} of {Student}-{Written} {Prompts} for {Large} {Language} {Models} of {Code}},
	booktitle = {International {Workshop} on {Large} {Language} {Models} for {Code} ({LLM4Code})},
	author = {Babe, Hannah McLean and Nguyen, Sydney and Zi, Yangtian and Guha, Arjun and Feldman, Molly Q. and Anderson, Carolyn Jane},
	year = {2024},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@article{carlsson_topology_2009,
	title = {Topology and data},
	volume = {46},
	issn = {0273-0979},
	url = {http://www.ams.org/journal-getitem?pii=S0273-0979-09-01249-X},
	doi = {10.1090/S0273-0979-09-01249-X},
	language = {en},
	number = {2},
	urldate = {2024-04-15},
	journal = {Bulletin of the American Mathematical Society},
	author = {Carlsson, Gunnar},
	month = jan,
	year = {2009},
	pages = {255--308},
}

@inproceedings{magai_deep_2023,
	address = {Haikou, China},
	title = {Deep {Neural} {Networks} {Architectures} from the {Perspective} of {Manifold} {Learning}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350325485},
	url = {https://ieeexplore.ieee.org/document/10331992/},
	doi = {10.1109/PRAI59366.2023.10331992},
	abstract = {Despite significant advances in the field of deep learning in applications to various areas, an explanation of the learning process of neural network models remains an important open question. The purpose of this paper is a comprehensive comparison and description of neural network architectures in terms of geometry and topology. We focus on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of a data manifold on different layers. In this paper, we use the concepts of topological data analysis (TDA) and persistent homological fractal dimension. We present a wide range of experiments with various datasets and configurations of convolutional neural network (CNNs) architectures and Transformers in CV and NLP tasks. Our work is a contribution to the development of the important field of explainable and interpretable AI within the framework of geometrical deep learning.},
	language = {en},
	urldate = {2024-04-15},
	booktitle = {2023 {IEEE} 6th {International} {Conference} on {Pattern} {Recognition} and {Artificial} {Intelligence} ({PRAI})},
	publisher = {IEEE},
	author = {Magai, German},
	month = aug,
	year = {2023},
	pages = {1021--1031},
}

@misc{levy:sametaskmoretokens,
	title = {Same {Task}, {More} {Tokens}: the {Impact} of {Input} {Length} on the {Reasoning} {Performance} of {Large} {Language} {Models}},
	shorttitle = {Same {Task}, {More} {Tokens}},
	url = {http://arxiv.org/abs/2402.14848},
	doi = {10.48550/arXiv.2402.14848},
	abstract = {This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs' in long input reasoning tasks. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14848 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{pinnaparaju:stablecode,
	title = {Stable {Code} {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.01226},
	doi = {10.48550/arXiv.2404.01226},
	abstract = {We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at https://huggingface.co/stabilityai/stable-code-3b and https://huggingface.co/stabilityai/stable-code-instruct-3b. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art open model under 3B parameters and even performs comparably to larger models of sizes 7 billion and 15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct also exhibits state-of-the-art performance on the MT-Bench coding tasks and on Multi-PL completion compared to other instruction tuned models. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and Datta, Ashish and Zhuravinskyi, Maksym and Mahan, Dakota and Bellagente, Marco and Riquelme, Carlos and Cooper, Nathan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01226 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lozhkov:starcoder2,
	title = {{StarCoder} 2 and {The} {Stack} v2: {The} {Next} {Generation}},
	shorttitle = {{StarCoder} 2 and {The} {Stack} v2},
	url = {http://arxiv.org/abs/2402.19173},
	doi = {10.48550/arXiv.2402.19173},
	abstract = {The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and Liu, Tianyang and Tian, Max and Kocetkov, Denis and Zucker, Arthur and Belkada, Younes and Wang, Zijian and Liu, Qian and Abulkhanov, Dmitry and Paul, Indraneil and Li, Zhuang and Li, Wen-Ding and Risdal, Megan and Li, Jia and Zhu, Jian and Zhuo, Terry Yue and Zheltonozhskii, Evgenii and Dade, Nii Osae Osae and Yu, Wenhao and Krauß, Lucas and Jain, Naman and Su, Yixuan and He, Xuanli and Dey, Manan and Abati, Edoardo and Chai, Yekun and Muennighoff, Niklas and Tang, Xiangru and Oblokulov, Muhtasham and Akiki, Christopher and Marone, Marc and Mou, Chenghao and Mishra, Mayank and Gu, Alex and Hui, Binyuan and Dao, Tri and Zebaze, Armel and Dehaene, Olivier and Patry, Nicolas and Xu, Canwen and McAuley, Julian and Hu, Han and Scholak, Torsten and Paquet, Sebastien and Robinson, Jennifer and Anderson, Carolyn Jane and Chapados, Nicolas and Patwary, Mostofa and Tajbakhsh, Nima and Jernite, Yacine and Ferrandis, Carlos Muñoz and Zhang, Lingming and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{li:instruction-backtranslation,
	title = {Self-{Alignment} with {Instruction} {Backtranslation}},
	url = {https://openreview.net/forum?id=1oijHJBRsT},
	abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Levy, Omer and Zettlemoyer, Luke and Weston, Jason E. and Lewis, Mike},
	month = oct,
	year = {2023},
}

@inproceedings{zhou:lima,
	title = {{LIMA}: {Less} {Is} {More} for {Alignment}},
	shorttitle = {{LIMA}},
	abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
	urldate = {2023-06-14},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
	year = {2023},
	note = {arXiv:2305.11206 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wu_os-copilot_2024,
	title = {{OS}-{Copilot}: {Towards} {Generalist} {Computer} {Agents} with {Self}-{Improvement}},
	shorttitle = {{OS}-{Copilot}},
	url = {https://arxiv.org/abs/2402.07456v2},
	abstract = {Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35\%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.},
	language = {en},
	urldate = {2024-04-07},
	journal = {arXiv.org},
	author = {Wu, Zhiyong and Han, Chengcheng and Ding, Zichen and Weng, Zhenmin and Liu, Zhoumianze and Yao, Shunyu and Yu, Tao and Kong, Lingpeng},
	month = feb,
	year = {2024},
}

@misc{replit_replit_2024,
	title = {Replit: {Building} {LLMs} for {Code} {Repair}},
	url = {https://blog.replit.com/code-repair},
	abstract = {Introduction

At Replit, we are rethinking the developer experience with AI as a first-class citizen of the development environment. Towards this vision, we are tightly integrating AI tools with our IDE. Currently, LLMs specialized for programming are trained with a mixture of source code and relevant natural languages, such as GitHub issues and StackExchange posts. These models are not trained to interact directly with the development environment and, therefore, have limited ability to understand events or use tools within Replit. We believe that by training models native to Replit, we can create more powerful AI tools for developers.

A simple example of a Replit-native model takes a session event as input and returns a well-defined response. We set out to identify a scenario where we could develop a model that could also become a useful tool for our current developers and settled on code repair. Developers spend a significant fraction of their time fixing bugs in software. In 2018, when Microsoft released “A Common Protocol for Languages,” Replit began supporting the Language Server Protocol. Since then, the LSP has helped millions using Replit to find errors in their code. This puts LSP diagnostics among our most common events, with hundreds of millions per day. However, while the LSP identifies errors, it can only provide fixes in limited cases. In fact, only 10\% of LSP diagnostic messages in Python projects on Replit have associated fixes. Given the abundance of training data, repairing code errors using LSP diagnostics is therefore the ideal setting to build our first Replit-native AI model.

Methodology

Data},
	urldate = {2024-04-07},
	journal = {Replit Blog},
	author = {Replit},
	month = apr,
	year = {2024},
}

@article{ghrist_barcodes_2007,
	title = {Barcodes: {The} persistent topology of data},
	volume = {45},
	issn = {0273-0979},
	shorttitle = {Barcodes},
	url = {http://www.ams.org/journal-getitem?pii=S0273-0979-07-01191-3},
	doi = {10.1090/S0273-0979-07-01191-3},
	abstract = {This article surveys recent work of Carlsson and collaborators on applications of computational algebraic topology to problems of feature detection and shape recognition in high-dimensional data. The primary mathematical tool considered is a homology theory for point-cloud data sets —persistent homology — and a novel representation of this algebraic characterization — barcodes. We sketch an application of these techniques to the classiﬁcation of natural images.},
	language = {en},
	number = {01},
	urldate = {2024-04-04},
	journal = {Bulletin of the American Mathematical Society},
	author = {Ghrist, Robert},
	month = oct,
	year = {2007},
	pages = {61--76},
}

@article{sakkas_seq2parse_2022,
	title = {{Seq2Parse}: neurosymbolic parse error repair},
	volume = {6},
	shorttitle = {{Seq2Parse}},
	url = {https://dl.acm.org/doi/10.1145/3563330},
	doi = {10.1145/3563330},
	abstract = {We present Seq2Parse, a language-agnostic neurosymbolic approach to automatically repairing parse errors. Seq2Parse is based on the insight that Symbolic Error Correcting (EC) Parsers can, in principle, synthesize repairs, but, in practice, are overwhelmed by the many error-correction rules that are not relevant to the particular program that requires repair. In contrast, Neural approaches are fooled by the large space of possible sequence level edits, but can precisely pinpoint the set of EC-rules that are relevant to a particular program. We show how to combine their complementary strengths by using neural methods to train a sequence classifier that predicts the small set of relevant EC-rules for an ill-parsed program, after which, the symbolic EC-parsing algorithm can make short work of generating useful repairs. We train and evaluate our method on a dataset of 1,100,000 Python programs, and show that Seq2Parse is accurate and efficient: it can parse 94\% of our tests within 2.1 seconds, while generating the exact user fix in 1 out 3 of the cases; and useful: humans perceive both Seq2Parse-generated error locations and repairs to be almost as good as human-generated ones in a statistically-significant manner.},
	number = {OOPSLA2},
	urldate = {2024-04-04},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Sakkas, Georgios and Endres, Madeline and Guo, Philip J. and Weimer, Westley and Jhala, Ranjit},
	month = oct,
	year = {2022},
	keywords = {Automated Program Repair, Error-Correcting Parsers, Machine Learning},
	pages = {167:1180--167:1206},
}

@article{brody_structural_2020,
	title = {A structural model for contextual code changes},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3428283},
	doi = {10.1145/3428283},
	abstract = {We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program’s Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task. We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers, and neural CRFs. Our experiments show that our model achieves a 28\% relative gain over state-of-the-art sequential models and 2× higher accuracy than syntactic models that learn to generate the edited code, as opposed to modeling the edits directly. Our code, dataset, and trained models are publicly available at {\textless}a{\textgreater}https://github.com/tech-srl/c3po/{\textless}/a{\textgreater} .},
	number = {OOPSLA},
	urldate = {2024-04-04},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	month = nov,
	year = {2020},
	keywords = {Edit Completions, Machine Learning, Neural Models of Code},
	pages = {215:1--215:28},
}

@article{alon_code2vec_2019,
	title = {code2vec: learning distributed representations of code},
	volume = {3},
	shorttitle = {code2vec},
	url = {https://dl.acm.org/doi/10.1145/3290353},
	doi = {10.1145/3290353},
	abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75\%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
	number = {POPL},
	urldate = {2024-04-04},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	month = jan,
	year = {2019},
	keywords = {Big Code, Distributed Representations, Machine Learning},
	pages = {40:1--40:29},
}

@inproceedings{weiss_thinking_2021,
	title = {Thinking {Like} {Transformers}},
	url = {https://proceedings.mlr.press/v139/weiss21a.html},
	abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
	language = {en},
	urldate = {2024-04-01},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11080--11090},
}

@article{palacio_toward_2024,
	title = {Toward a {Theory} of {Causation} for {Interpreting} {Neural} {Code} {Models}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {http://arxiv.org/abs/2302.03788},
	doi = {10.1109/TSE.2024.3379943},
	abstract = {Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces \$do\_\{code\}\$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. \$do\_\{code\}\$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of \$do\_\{code\}\$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of \$do\_\{code\}\$, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code ({\textbackslash}eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of \$do\_\{code\}\$ as a useful method to detect and facilitate the elimination of confounding bias in NCMs.},
	urldate = {2024-04-01},
	journal = {IEEE Transactions on Software Engineering},
	author = {Palacio, David N. and Velasco, Alejandro and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
	year = {2024},
	note = {arXiv:2302.03788 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Methodology},
	pages = {1--28},
}

@inproceedings{wright:soft-typing,
	title = {Soft typing},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	author = {Cartwright, Robert and Fagan, Mike},
	year = {1991},
}

@article{campora:migrating,
	title = {Migrating {Gradual} {Types}},
	volume = {2},
	number = {POPL},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Campora, John Peter and Chen, Sheng and Erwig, Martin and Walkingshaw, Eric},
	year = {2018},
}

@inproceedings{hellendoorn:dlti,
	title = {Deep {Learning} {Type} {Inference}},
	booktitle = {{ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
	year = {2018},
}

@article{hu:codebotler,
	title = {Deploying and {Evaluating} {LLMs} to {Program} {Service} {Mobile} {Robots}},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/10416558},
	doi = {10.1109/LRA.2024.3360020},
	abstract = {Recent advancements in large language models (LLMs) have spurred interest in using them for generating robot programs from natural language, with promising initial results. We investigate the use of LLMs to generate programs for service mobile robots leveraging mobility, perception, and human interaction skills, and where accurate sequencing and ordering of actions is crucial for success. We contribute CodeBotler, an open-source robot-agnostic tool to program service mobile robots from natural language, and RoboEval, a benchmark for evaluating LLMs' capabilities of generating programs to complete service robot tasks. CodeBotler performs program generation via few-shot prompting of LLMs with an embedded domain-specific language (eDSL) in Python, and leverages skill abstractions to deploy generated programs on any general-purpose mobile robot. RoboEval evaluates the correctness of generated programs by checking execution traces starting with multiple initial states, and checking whether the traces satisfy temporal logic properties that encode correctness for each task. RoboEval also includes multiple prompts per task to test for the robustness of program generation. We evaluate several popular state-of-the-art LLMs with the RoboEval benchmark, and perform a thorough analysis of the modes of failures, resulting in a taxonomy that highlights common pitfalls of LLMs at generating robot programs. We release our code and benchmark at https://amrl.cs.utexas.edu/codebotler/},
	urldate = {2024-02-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Hu, Zichao and Lucchetti, Francesca and Schlesinger, Claire and Saxena, Yash and Freeman, Anders and Modak, Sadanand and Guha, Arjun and Biswas, Joydeep},
	year = {2024},
	keywords = {Benchmark testing, Mobile robots, Natural languages, Python, Robots, Service robots, Software tools for benchmarking and reproducibility, Task analysis, human-centered robotics, service robotics, social HRI, software tools for robot programming},
	pages = {1--8},
}

@article{migeed:decidable,
	title = {What is {Decidable} about {Gradual} {Types}?},
	volume = {4},
	number = {POPL},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Migeed, Zeina and Palsberg, Jens},
	year = {2020},
}

@inproceedings{politz:lambda-py,
	address = {Indianapolis, IN, USA},
	title = {Python: the full monty},
	isbn = {978-1-4503-2374-1},
	shorttitle = {Python},
	url = {https://dl.acm.org/doi/10.1145/2509136.2509536},
	doi = {10.1145/2509136.2509536},
	abstract = {We present a small-step operational semantics for the Python programming language. We present both a core language for Python, suitable for tools and proofs, and a translation process for converting Python source to this core. We have tested the composition of translation and evaluation of the core for conformance with the primary Python implementation, thereby giving conﬁdence in the ﬁdelity of the semantics. We brieﬂy report on the engineering of these components. Finally, we examine subtle aspects of the language, identifying scope as a pervasive concern that even impacts features that might be considered orthogonal.},
	language = {en},
	urldate = {2024-03-28},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Object} {Oriented} {Programmingm}, {Systems}, {Languages} and {Applications} ({OOPSLA})},
	publisher = {ACM},
	author = {Politz, Joe Gibbs and Martinez, Alejandro and Milano, Mae and Warren, Sumner and Patterson, Daniel and Li, Junsong and Chitipothu, Anand and Krishnamurthi, Shriram},
	month = oct,
	year = {2013},
	pages = {217--232},
}

@inproceedings{rastogi:gti,
	title = {The {Ins} and {Outs} of {Gradual} {Type} {Inference}},
	booktitle = {{ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages} ({POPL})},
	author = {Rastogi, Aseem and Chaudhuri, Avik and Hosmer, Basil},
	year = {2012},
}

@inproceedings{siek:gtlc,
	title = {Gradual {Typing} for {Functional} {Languages}},
	booktitle = {Scheme {Workshop}},
	author = {Siek, Jeremy G. and Taha, Walid},
	year = {2006},
}

@inproceedings{siek:gti,
	title = {Gradual {Typing} with {Unification}-based {Inference}},
	booktitle = {{ACM} {SIGPLAN} {International} {Symposium} on {Dynamic} {Languages} ({DLS})},
	author = {Siek, Jeremy G. and Vachharajani, Manish},
	year = {2008},
}

@inproceedings{th:migration,
	title = {Interlanguage {Migration}: {From} {Scripts} to {Programs}},
	booktitle = {{ACM} {SIGPLAN} {International} {Symposium} on {Dynamic} {Languages} ({DLS})},
	author = {Tobin-Hochstadt, Sam and Felleisen, Matthias},
	year = {2006},
}

@article{pradel_deepbugs_2018,
	title = {{DeepBugs}: a learning approach to name-based bug detection},
	volume = {2},
	shorttitle = {{DeepBugs}},
	url = {https://dl.acm.org/doi/10.1145/3276517},
	doi = {10.1145/3276517},
	abstract = {Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.},
	number = {OOPSLA},
	urldate = {2024-03-29},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Pradel, Michael and Sen, Koushik},
	month = oct,
	year = {2018},
	keywords = {Bug detection, JavaScript, Machine learning, Name-based program analysis, Natural language},
	pages = {147:1--147:25},
}

@inproceedings{long_automatic_2016,
	address = {New York, NY, USA},
	series = {{POPL} '16},
	title = {Automatic patch generation by learning correct code},
	isbn = {978-1-4503-3549-2},
	url = {https://dl.acm.org/doi/10.1145/2837614.2837617},
	doi = {10.1145/2837614.2837617},
	abstract = {We present Prophet, a novel patch generation system that works with a set of successful human patches obtained from open- source software repositories to learn a probabilistic, application-independent model of correct code. It generates a space of candidate patches, uses the model to rank the candidate patches in order of likely correctness, and validates the ranked patches against a suite of test cases to find correct patches. Experimental results show that, on a benchmark set of 69 real-world defects drawn from eight open-source projects, Prophet significantly outperforms the previous state-of-the-art patch generation system.},
	urldate = {2024-03-29},
	booktitle = {Proceedings of the 43rd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Long, Fan and Rinard, Martin},
	month = jan,
	year = {2016},
	keywords = {Code correctness model, Learning correct code, Program repair},
	pages = {298--312},
}

@inproceedings{hindle_naturalness_2012,
	title = {On the naturalness of software},
	url = {https://ieeexplore.ieee.org/document/6227135},
	doi = {10.1109/ICSE.2012.6227135},
	abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.},
	urldate = {2024-03-26},
	booktitle = {2012 34th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
	month = jun,
	year = {2012},
	note = {ISSN: 1558-1225},
	keywords = {Computational modeling, Entropy, Java, Natural language processing, Software, Speech recognition, code completion, code suggestion, language models, n-gram, natural language processing},
	pages = {837--847},
}

@inproceedings{vig_investigating_2020,
	title = {Investigating {Gender} {Bias} in {Language} {Models} {Using} {Causal} {Mediation} {Analysis}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html},
	abstract = {Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model.  We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms that facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.},
	urldate = {2024-03-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
	year = {2020},
	pages = {12388--12401},
}

@article{harper:xml,
	title = {On the type structure of standard {ML}},
	volume = {15},
	issn = {0164-0925},
	url = {https://dl.acm.org/doi/10.1145/169701.169696},
	doi = {10.1145/169701.169696},
	number = {2},
	urldate = {2024-03-25},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Harper, Robert and Mitchell, John C.},
	month = apr,
	year = {1993},
	pages = {211--252},
}

@article{demillo:test-data-selection,
	title = {Hints on {Test} {Data} {Selection}: {Help} for the {Practicing} {Programmer}},
	volume = {11},
	issn = {1558-0814},
	shorttitle = {Hints on {Test} {Data} {Selection}},
	url = {https://ieeexplore.ieee.org/document/1646911},
	doi = {10.1109/C-M.1978.218136},
	abstract = {In many cases tests of a program that uncover simple errors are also effective in uncovering much more complex errors. This so-called coupling effect can be used to save work during the testing process.},
	number = {4},
	urldate = {2024-03-25},
	journal = {Computer},
	author = {DeMillo, R.A. and Lipton, R.J. and Sayward, F.G.},
	month = apr,
	year = {1978},
	note = {Conference Name: Computer},
	keywords = {Computer bugs, Computer errors, Error analysis, Error correction, Process design, Programming profession, Software debugging, Software reliability, System testing},
	pages = {34--41},
}

@article{alphacode,
	title = {Competition-level code generation with {AlphaCode}},
	volume = {378},
	url = {https://www.science.org/doi/full/10.1126/science.abq1158},
	doi = {10.1126/science.abq1158},
	abstract = {Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions.},
	number = {6624},
	urldate = {2024-03-25},
	journal = {Science},
	author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1092--1097},
}

@misc{tabnine,
	title = {Tabnine: {AI} {Assistant} for software developers},
	url = {https://www.tabnine.com/},
	urldate = {2024-03-25},
	author = {Weiss, Dror and Yahav, Eran},
	year = {2013},
}

@misc{allal:bigcode-leaderboard,
	title = {Big {Code} {Models} {Leaderboard}},
	url = {https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard},
	urldate = {2024-02-27},
	author = {Allal, Loubna Ben},
}

@misc{code-llama,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	shorttitle = {Code {Llama}},
	url = {http://arxiv.org/abs/2308.12950},
	abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67\% and 65\% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
	month = jan,
	year = {2024},
	note = {arXiv:2308.12950 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jain_coarse-tuning_2023,
	title = {Coarse-{Tuning} {Models} of {Code} with {Reinforcement} {Learning} {Feedback}},
	url = {http://arxiv.org/abs/2305.18341},
	doi = {10.48550/arXiv.2305.18341},
	abstract = {Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, these models are trained using next-token prediction, which ignores the syntax and semantics of code. We propose RLCF, that further trains a pre-trained LLM via reinforcement learning, using feedback from a grounding function that scores the quality of the code. The grounding function uses (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM that compares the generated code to a reference code. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF raises the odds that an LLM-generated program compiles, is executable, and produces the right output on tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Jain, Abhinav and Adiole, Chima and Chaudhuri, Swarat and Reps, Thomas and Jermaine, Chris},
	month = dec,
	year = {2023},
	note = {arXiv:2305.18341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{machalica:predictive-test-selection,
	title = {Predictive {Test} {Selection}},
	url = {https://ieeexplore.ieee.org/abstract/document/8804462},
	doi = {10.1109/ICSE-SEIP.2019.00018},
	abstract = {Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95\% of individual test failures and over 99.9\% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.},
	urldate = {2024-02-19},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Machalica, Mateusz and Samylkin, Alex and Porth, Meredith and Chandra, Satish},
	month = may,
	year = {2019},
	keywords = {Diamond, Facebook, Libraries, Machine learning, Metadata, Testing, Tools, continuous integration, flaky tests, machine learning, test selection},
	pages = {91--100},
}

@inproceedings{ryder:change-impact-analysis,
	address = {New York, NY, USA},
	series = {{PASTE} '01},
	title = {Change impact analysis for object-oriented programs},
	isbn = {978-1-58113-413-1},
	url = {https://doi.org/10.1145/379605.379661},
	doi = {10.1145/379605.379661},
	abstract = {Small changes can have major and nonlocal effects in object-oriented languages, due to the use of subtyping and dynamic dispatch. This complicates life for maintenance programmers, who need to fix bugs or add enhancements to systems originally written by others. Change impact analysis provides feedback on the semantic impact of a set of program changes. This analysis can be used to determine the regression test drivers that are affected by a set of changes. Moreover, if a test fails, a subset of changes responsible for the failure can be identified, as well as a subset of changes that can be incorporated safely without affecting any test driver.},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 2001 {ACM} {SIGPLAN}-{SIGSOFT} workshop on {Program} analysis for software tools and engineering},
	publisher = {Association for Computing Machinery},
	author = {Ryder, Barbara G. and Tip, Frank},
	month = jun,
	year = {2001},
	pages = {46--53},
}

@inproceedings{gligoric:pratical-regression-test-selection,
	address = {New York, NY, USA},
	series = {{ISSTA} 2015},
	title = {Practical regression test selection with dynamic file dependencies},
	isbn = {978-1-4503-3620-8},
	url = {https://doi.org/10.1145/2771783.2771784},
	doi = {10.1145/2771783.2771784},
	abstract = {Regression testing is important but can be time-intensive. One approach to speed it up is regression test selection (RTS), which runs only a subset of tests. RTS was proposed over three decades ago but has not been widely adopted in practice. Meanwhile, testing frameworks, such as JUnit, are widely adopted and well integrated with many popular build systems. Hence, integrating RTS in a testing framework already used by many projects would increase the likelihood that RTS is adopted. We propose a new, lightweight RTS technique, called Ekstazi, that can integrate well with testing frameworks. Ekstazi tracks dynamic dependencies of tests on files, and unlike most prior RTS techniques, Ekstazi requires no integration with version-control systems. We implemented Ekstazi for Java and JUnit, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC) with shorter- and longer-running test suites. The results show that Ekstazi reduced the end-to-end testing time 32\% on average, and 54\% for longer-running test suites, compared to executing all tests. Ekstazi also has lower end-to-end time than the existing techniques, despite the fact that it selects more tests. Ekstazi has been adopted by several popular open source projects, including Apache Camel, Apache Commons Math, and Apache CXF.},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 2015 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Gligoric, Milos and Eloussi, Lamyaa and Marinov, Darko},
	month = jul,
	year = {2015},
	keywords = {Regression test selection, file dependencies},
	pages = {211--222},
}

@inproceedings{havelund:monitoring-programs-using-rewriting,
	address = {USA},
	series = {{ASE} '01},
	title = {Monitoring {Programs} {Using} {Rewriting}},
	abstract = {We present a rewriting algorithm for efficiently testingfuture time Linear Temporal Logic (LTL) formulae on finiteexecution traces. The standard models of LTL are infinitetraces, reflecting the behavior of reactive and concurrentsystems which conceptually may be continuously alive. Inmost past applications of LTL, theorem provers and modelcheckers have been used to formally prove that down-scaledmodels satisfy such LTL specifications. Our goal is insteadto use LTL for up-scaled testing of real software applications,corresponding to analyzing the conformance of finitetraces against LTL formulae. We first describe whatit means for a finite trace to satisfy an LTL formula andthen suggest an optimized algorithm based on transformingLTL formulae. We use the Maude rewriting logic, whichturns out to be a good notation and being supported by anefficient rewriting engine for performing these experiments.The work constitutes part of the Java PathExplorer (JPAX)project, the purpose of which is to develop a flexible tool formonitoring Java program executions.},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 16th {IEEE} international conference on {Automated} software engineering},
	publisher = {IEEE Computer Society},
	author = {Havelund, Klaus and Rosu, Grigore},
	month = nov,
	year = {2001},
	pages = {135},
}

@inproceedings{huang:rosrv,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ROSRV}: {Runtime} {Verification} for {Robots}},
	isbn = {978-3-319-11164-3},
	shorttitle = {{ROSRV}},
	doi = {10.1007/978-3-319-11164-3_20},
	abstract = {We present ROSRV, a runtime verification framework for robotic applications on top of the Robot Operating System (ROS [8]), a widely used open-source framework for robot software development. ROSRV aims to address the safety and security issues of robots by providing a transparent monitoring infrastructure that intercepts and monitors the commands and messages passing through the system. Safety and security properties can be defined in a formal specification language, and are ensured by automatically generated monitors. ROSRV integrates seamlessly with ROS—no change in ROS nor the application code is needed. ROSRV has been applied and evaluated on a commercial robot.},
	language = {en},
	booktitle = {Runtime {Verification}},
	publisher = {Springer International Publishing},
	author = {Huang, Jeff and Erdogan, Cansu and Zhang, Yi and Moore, Brandon and Luo, Qingzhou and Sundaresan, Aravind and Rosu, Grigore},
	editor = {Bonakdarpour, Borzoo and Smolka, Scott A.},
	year = {2014},
	keywords = {Access Control Policy, Linear Temporal Logic, Robot Operating System, Runtime Verification, Virtual Machine Monitor},
	pages = {247--254},
}

@inproceedings{sun:long,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Do {Long}-{Range} {Language} {Models} {Actually} {Use} {Long}-{Range} {Context}?},
	url = {https://aclanthology.org/2021.emnlp-main.62},
	doi = {10.18653/v1/2021.emnlp-main.62},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
	year = {2021},
	pages = {807--822},
}

@inproceedings{holtzman:neural-text-degeneration,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {https://openreview.net/forum?id=rygGQyrFvH},
	abstract = {Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text.},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = sep,
	year = {2019},
}

@inproceedings{kwon:paged-attention,
	address = {New York, NY, USA},
	title = {Efficient {Memory} {Management} for {Large} {Language} {Model} {Serving} with {PagedAttention}},
	isbn = {9798400702297},
	url = {https://dl.acm.org/doi/10.1145/3600006.3613165},
	doi = {10.1145/3600006.3613165},
	abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
	urldate = {2024-02-19},
	booktitle = {Symposium on {Operating} {Systems} {Principles} ({SOSP})},
	publisher = {Association for Computing Machinery},
	author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
	month = oct,
	year = {2023},
	pages = {611--626},
}

@misc{dao:flash-attention-2,
	title = {{FlashAttention}-2: {Faster} {Attention} with {Better} {Parallelism} and {Work} {Partitioning}},
	shorttitle = {{FlashAttention}-2},
	url = {http://arxiv.org/abs/2307.08691},
	doi = {10.48550/arXiv.2307.08691},
	abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\${\textbackslash}times\$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40{\textbackslash}\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\${\textbackslash}times\$ speedup compared to FlashAttention, reaching 50-73{\textbackslash}\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72{\textbackslash}\% model FLOPs utilization).},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Dao, Tri},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08691 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{gupta:bias-runs-deep,
	title = {Bias {Runs} {Deep}: {Implicit} {Reasoning} {Biases} in {Persona}-{Assigned} {LLMs}},
	shorttitle = {Bias {Runs} {Deep}},
	abstract = {Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70\%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80\%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42\% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.},
	urldate = {2024-02-19},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
	month = jan,
	year = {2024},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{salewski:impersonation,
	title = {In-{Context} {Impersonation} {Reveals} {Large} {Language} {Models}' {Strengths} and {Biases}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/e3fe7b34ba4f378df39cb12a97193f41-Abstract-Conference.html},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Salewski, Leonard and Alaniz, Stephan and Rio-Torto, Isabel and Schulz, Eric and Akata, Zeynep},
	year = {2023},
}

@inproceedings{yao:tree-of-thoughts,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {https://openreview.net/forum?id=5Xc1ecxO1h},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4{\textbackslash}\% of tasks, our method achieved a success rate of 74{\textbackslash}\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik R.},
	month = nov,
	year = {2023},
}

@inproceedings{wei:chain-of-thought,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=_VjQlMeSB_J},
	abstract = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	month = may,
	year = {2022},
}

@misc{leaked-system-prompts,
	title = {jujumilk3/leaked-system-prompts},
	url = {https://github.com/jujumilk3/leaked-system-prompts},
	abstract = {Collection of leaked system prompts},
	urldate = {2024-02-19},
	author = {Lee, Donggyu},
	month = feb,
	year = {2024},
	note = {original-date: 2023-05-16T02:09:06Z},
	keywords = {llm, prompt},
}

@inproceedings{liu:what-it-wants-me-to-say,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {“{What} {It} {Wants} {Me} {To} {Say}”: {Bridging} the {Abstraction} {Gap} {Between} {End}-{User} {Programmers} and {Code}-{Generating} {Large} {Language} {Models}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {“{What} {It} {Wants} {Me} {To} {Say}”},
	url = {https://doi.org/10.1145/3544548.3580817},
	doi = {10.1145/3544548.3580817},
	abstract = {Code-generating large language models map natural language to code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the user’s natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users’ understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
	urldate = {2024-02-18},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
	month = apr,
	year = {2023},
	keywords = {Human-AI Interaction, Large Language Models, Natural Language Programming, Spreadsheets},
	pages = {1--31},
}

@misc{parnin:building-copilots,
	title = {Building {Your} {Own} {Product} {Copilot}: {Challenges}, {Opportunities}, and {Needs}},
	shorttitle = {Building {Your} {Own} {Product} {Copilot}},
	url = {http://arxiv.org/abs/2312.14231},
	doi = {10.48550/arXiv.2312.14231},
	abstract = {A race is underway to embed advanced AI capabilities into products. These product copilots enable users to ask questions in natural language and receive relevant responses that are specific to the user's context. In fact, virtually every large technology company is looking to add these capabilities to their software products. However, for most software engineers, this is often their first encounter with integrating AI-powered technology. Furthermore, software engineering processes and tools have not caught up with the challenges and scale involved with building AI-powered applications. In this work, we present the findings of an interview study with 26 professional software engineers responsible for building product copilots at various companies. From our interviews, we found pain points at every step of the engineering process and the challenges that strained existing development practices. We then conducted group brainstorming sessions to collaborative on opportunities and tool designs for the broader software engineering community.},
	urldate = {2024-02-18},
	publisher = {arXiv},
	author = {Parnin, Chris and Soares, Gustavo and Pandita, Rahul and Gulwani, Sumit and Rich, Jessica and Henley, Austin Z.},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14231 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bender:stochastic-parrots,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2024-02-18},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@misc{boston-dynamics:robots-that-can-chat,
	title = {Robots {That} {Can} {Chat}},
	url = {https://bostondynamics.com/blog/robots-that-can-chat/},
	abstract = {We created a robot tour guide using Spot integrated with Chat GPT and other AI models to explore the robotics applications of foundational models.},
	language = {en-US},
	urldate = {2024-02-18},
	journal = {Boston Dynamics},
}

@article{merken:lawyers-fake-chatgpt,
	chapter = {Legal},
	title = {New {York} lawyers sanctioned for using fake {ChatGPT} cases in legal brief},
	url = {https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/},
	abstract = {A U.S. judge on Thursday imposed sanctions on two New York lawyers who submitted a legal brief that included six fictitious case citations generated by an artificial intelligence chatbot, ChatGPT.},
	language = {en},
	urldate = {2024-02-18},
	journal = {Reuters},
	author = {Merken, Sara and Merken, Sara},
	month = jun,
	year = {2023},
}

@inproceedings{hendrycks:apps,
	title = {Measuring {Coding} {Challenge} {Competence} {With} {APPS}},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html},
	urldate = {2024-02-09},
	booktitle = {Neural {Information} {Processing} {Systems} {Track} on {Datasets} and {Benchmarks} ({NeurIPS} {Datasets} and {Benchmarks})},
	author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
	month = dec,
	year = {2021},
}

@inproceedings{pearce:asleep-at-the-keyboard,
	title = {Asleep at the {Keyboard}? {Assessing} the {Security} of {GitHub} {Copilot}’s {Code} {Contributions}},
	shorttitle = {Asleep at the {Keyboard}?},
	url = {https://ieeexplore.ieee.org/abstract/document/9833571},
	doi = {10.1109/SP46214.2022.9833571},
	abstract = {There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described ‘AI pair programmer’, GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, e.g. those from MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40\% to be vulnerable.},
	urldate = {2024-02-09},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Artificial Intelligence (AI), Codes, Common Weakness Enumerations (CWEs), Computational modeling, Computer crime, Cybersecurity, Keyboards, Open source software, Privacy, Software development management, code generation},
	pages = {754--768},
}

@inproceedings{liu:repobench,
	title = {{RepoBench}: {Benchmarking} {Repository}-{Level} {Code} {Auto}-{Completion} {Systems}},
	shorttitle = {{RepoBench}},
	url = {https://openreview.net/forum?id=pPjZIOuQuF},
	abstract = {Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems.},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Liu, Tianyang and Xu, Canwen and McAuley, Julian},
	month = oct,
	year = {2023},
}

@misc{guo:deepseek,
	title = {{DeepSeek}-{Coder}: {When} the {Large} {Language} {Model} {Meets} {Programming} -- {The} {Rise} of {Code} {Intelligence}},
	shorttitle = {{DeepSeek}-{Coder}},
	url = {http://arxiv.org/abs/2401.14196},
	doi = {10.48550/arXiv.2401.14196},
	abstract = {The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y. and Li, Y. K. and Luo, Fuli and Xiong, Yingfei and Liang, Wenfeng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14196 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{schick:toolformer,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {https://openreview.net/forum?id=Yacmpz84TH},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel. In this paper, we show that LMs can teach themselves to *use external tools* via simple APIs and achieve the best of both worlds. We introduce *Toolformer*, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	language = {en},
	urldate = {2024-02-08},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessi, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = nov,
	year = {2023},
}

@misc{karpas:mrkl,
	title = {{MRKL} {Systems}: {A} modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
	shorttitle = {{MRKL} {Systems}},
	url = {http://arxiv.org/abs/2205.00445},
	abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and Shalev-Shwartz, Shai and Shashua, Amnon and Tenenholtz, Moshe},
	month = may,
	year = {2022},
	note = {arXiv:2205.00445 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{huang:genai-act2,
	title = {Generative {AI}’s {Act} {Two}},
	url = {https://www.sequoiacap.com/article/generative-ai-act-two/},
	language = {en-US},
	urldate = {2024-02-08},
	journal = {Sequoia Capital},
	author = {Huang, Sonya and Grady, Pat},
	month = sep,
	year = {2023},
}

@misc{matlab-ai-chat,
	title = {{MATLAB} {AI} {Chat} {Playground}},
	url = {https://www.mathworks.com/matlabcentral/playground/},
	abstract = {Use AI to generate initial draft MATLAB code, and answer questions!},
	language = {en},
	urldate = {2024-02-08},
}

@inproceedings{luger:like-having-a-really-bad-pa,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {"{Like} {Having} a {Really} {Bad} {PA}": {The} {Gulf} between {User} {Expectation} and {Experience} of {Conversational} {Agents}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {"{Like} {Having} a {Really} {Bad} {PA}"},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858288},
	doi = {10.1145/2858036.2858288},
	abstract = {The past four years have seen the rise of conversational agents (CAs) in everyday life. Apple, Microsoft, Amazon, Google and Facebook have all embedded proprietary CAs within their software and, increasingly, conversation is becoming a key mode of human-computer interaction. Whilst we have long been familiar with the notion of computers that speak, the investigative concern within HCI has been upon multimodality rather than dialogue alone, and there is no sense of how such interfaces are used in everyday life. This paper reports the findings of interviews with 14 users of CAs in an effort to understand the current interactional factors affecting everyday use. We find user expectations dramatically out of step with the operation of the systems, particularly in terms of known machine intelligence, system capability and goals. Using Norman's 'gulfs of execution and evaluation' [30] we consider the implications of these findings for the design of future systems.},
	urldate = {2024-02-08},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Luger, Ewa and Sellen, Abigail},
	month = may,
	year = {2016},
	keywords = {conversational agents, evaluation, mental models},
	pages = {5286--5297},
}

@inproceedings{porcheron:voice-interfaces,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Voice {Interfaces} in {Everyday} {Life}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174214},
	doi = {10.1145/3173574.3174214},
	abstract = {Voice User Interfaces (VUIs) are becoming ubiquitously available, being embedded both into everyday mobility via smartphones, and into the life of the home via 'assistant' devices. Yet, exactly how users of such devices practically thread that use into their everyday social interactions remains underexplored. By collecting and studying audio data from month-long deployments of the Amazon Echo in participants' homes-informed by ethnomethodology and conversation analysis-our study documents the methodical practices of VUI users, and how that use is accomplished in the complex social life of the home. Data we present shows how the device is made accountable to and embedded into conversational settings like family dinners where various simultaneous activities are being achieved. We discuss how the VUI is finely coordinated with the sequential organisation of talk. Finally, we locate implications for the accountability of VUI interaction, request and response design, and raise conceptual challenges to the notion of designing 'conversational' interfaces.},
	urldate = {2024-02-08},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Porcheron, Martin and Fischer, Joel E. and Reeves, Stuart and Sharples, Sarah},
	month = apr,
	year = {2018},
	keywords = {amazon echo, collocated interaction, conversation analysis, conversational agent, conversational user interface, ethnomethodology, intelligent personal assistants},
	pages = {1--12},
}

@misc{umass-ocelot-story,
	title = {{CICS} {Student} and {Professors} {Develop} {JavaScript} {Classroom} {Teaching} {Tool} {That} {Shields} {Students} from “the {Bad} {Parts}”},
	url = {https://www.cics.umass.edu/news/cics-student-and-professors-develop-javascript-classroom-teaching-tool},
	abstract = {Computer science students increasingly need to learn JavaScript, one of the most widely-used programming languages in the world. Unfortunately, the language has a laundry list of bad features that get in the way of programming, leading to bugs in code and frustrations for novice programmers. So, how should we teach JavaScript? A team of researchers at the College of Information and Computer Sciences, UMass Amherst (CICS) think they have the answer.},
	language = {en},
	urldate = {2024-02-06},
	journal = {Manning College of Information \& Computer Sciences},
	month = apr,
	year = {2019},
}

@misc{wiggers:shopify-sidekick,
	title = {Shopify {Sidekick} is like {ChatGPT}, but for e-commerce merchants},
	url = {https://techcrunch.com/2023/07/26/shopify-sidekick-is-like-chatgpt-but-for-ecommerce-merchants/},
	abstract = {Shopify's new generative AI tool, Sidekick, is like OpenAI's ChatGPT, but tuned for ecommerce merchant use cases.},
	language = {en-US},
	urldate = {2024-02-05},
	journal = {TechCrunch},
	author = {Wiggers, Kyle},
	month = jul,
	year = {2023},
}

@misc{intuit-assist,
	title = {Introducing {Intuit} {Assist}: {The} {Generative} {AI}-{Powered} {Financial} {Assistant} for {Small} {Businesses} and {Consumers}},
	url = {https://www.intuit.com/company/press-room/press-releases/2023/introducing-intuit-assist-the-generative-ai-powered-financial-assistant-for-small-businesses-and-consumers/},
	urldate = {2024-02-05},
}

@misc{adobe-sensei,
	title = {Adobe {Announces} {New} {Sensei} {GenAI} {Services} to {Reimagine} {End}-to-{End} {Marketing} {Workflows}},
	url = {https://news.adobe.com/news/news-details/2023/Adobe-Announces-New-Sensei-GenAI-Services-to-Reimagine-End-to-End-Marketing-Workflows/default.aspx},
	abstract = {Adobe Sensei GenAI will leverage multiple large language models (LLMs) within Adobe Experience Platform, depending on unique business needs Adobe Firefly—Adobe’s new family of creative generative AI models first focused on image generation and text effects—will be integrated into Adobe Experience Cloud for businesses to generate content designed to be safe for commercial use Adobe unveiled the first set of Sensei GenAI services in Adobe Experience Manager, Adobe Journey Optimizer, Adobe Customer},
	language = {en-US},
	urldate = {2024-02-05},
}

@misc{roblox-assistant,
	title = {Introducing {Assistant} in {Studio} [{Beta}]},
	url = {https://devforum.roblox.com/t/introducing-assistant-in-studio-beta/2725977},
	abstract = {Hello creators,  Today, we are excited to bring Assistant to Studio as a beta feature, following the release of Assistant for Docs in October. In addition to answering documentation questions, the Studio version of Assistant can create and insert scripts in targeted cases, explain highlighted code, and generate new materials. This is just the beginning. Over time, Assistant will take more actions to help you accelerate your creative process.  Our vision is for Assistant to help you get from id...},
	language = {en},
	urldate = {2024-02-05},
	journal = {Developer Forum {\textbar} Roblox},
	month = dec,
	year = {2023},
}

@misc{spataro:office-copilot,
	title = {Introducing {Microsoft} 365 {Copilot} – your copilot for work},
	url = {https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/},
	abstract = {Humans are hard-wired to dream, to create, to innovate. Each of us seeks to do work that gives us purpose — to write a great novel, to make a discovery, to build strong communities, to care for the sick. The urge to connect to the core of our work lives in all of us. But...},
	language = {en-US},
	urldate = {2024-02-05},
	journal = {The Official Microsoft Blog},
	author = {Spataro, Jared},
	month = mar,
	year = {2023},
}

@misc{gpt-store,
	title = {Introducing the {GPT} {Store}},
	url = {https://openai.com/blog/introducing-the-gpt-store},
	abstract = {We’re launching the GPT Store to help you find useful and popular custom versions of ChatGPT.},
	language = {en-US},
	urldate = {2024-02-05},
}

@inproceedings{radford:clip,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
}

@inproceedings{roziere:transcoder,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'20},
	title = {Unsupervised translation of programming languages},
	isbn = {978-1-71382-954-6},
	abstract = {A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is time-consuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
	month = dec,
	year = {2020},
	pages = {20601--20611},
}

@inproceedings{roziere:transcoder-st,
	title = {Leveraging {Automated} {Unit} {Tests} for {Unsupervised} {Code} {Translation}},
	urldate = {2023-08-10},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Roziere, Baptiste and Zhang, Jie and Charton, Francois and Harman, Mark and Synnaeve, Gabriel and Lample, Guillaume},
	month = oct,
	year = {2021},
}

@article{schafer:testpilot,
	title = {Adaptive {Test} {Generation} {Using} a {Large} {Language} {Model}},
	volume = {50},
	doi = {10.1109/TSE.2023.3334955},
	number = {1},
	journal = {IEEE Transactions on Software Engineering (TSE)},
	author = {Schäfer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{villmow:contest-unit-test,
	address = {Online},
	title = {{ConTest}: {A} {Unit} {Test} {Completion} {Benchmark} featuring {Context}},
	shorttitle = {{ConTest}},
	url = {https://aclanthology.org/2021.nlp4prog-1.2},
	doi = {10.18653/v1/2021.nlp4prog-1.2},
	abstract = {We introduce CONTEST, a benchmark for NLP-based unit test completion, the task of predicting a test's assert statements given its setup and focal method, i.e. the method to be tested. ConTest is large-scale (with 365k datapoints). Besides the test code and tested code, it also features context code called by either. We found context to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoder-decoders, and study the effects of including syntactic information and context. Overall, our models achieve a BLEU score of 38.2, while only generating unparsable code in 1.92\% of cases.},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the 1st {Workshop} on {Natural} {Language} {Processing} for {Programming} ({NLP4Prog} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Villmow, Johannes and Depoix, Jonas and Ulges, Adrian},
	editor = {Lachmy, Royi and Yao, Ziyu and Durrett, Greg and Gligoric, Milos and Li, Junyi Jessy and Mooney, Ray and Neubig, Graham and Su, Yu and Sun, Huan and Tsarfaty, Reut},
	month = aug,
	year = {2021},
	pages = {17--25},
}

@misc{tufano:unit-tests,
	title = {Unit {Test} {Case} {Generation} with {Transformers} and {Focal} {Context}},
	url = {http://arxiv.org/abs/2009.05617},
	doi = {10.48550/arXiv.2009.05617},
	abstract = {Automated unit test case generation tools facilitate test-driven development and support developers by suggesting tests intended to identify flaws in their code. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult for developers to read or understand. In this paper we propose AthenaTest, an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written testcases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure consisting of denoising pretraining on a large unsupervised Java corpus, and supervised finetuning for a downstream translation task of generating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information surrounding the focal method. Both techniques provide improvements in terms of validation loss, with pretraining yielding 25\% relative improvement and focal context providing additional 11.1\% improvement. We also introduce Methods2Test, the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K test cases mined from 91K open-source repositories from GitHub. We evaluate AthenaTest on five defects4j projects, generating 25K passing test cases covering 43.7\% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3, finding that our approach outperforms GPT-3 and has comparable coverage w.r.t. EvoSuite. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated tests, showing overwhelmingly preference towards AthenaTest.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Deng, Shao Kun and Sundaresan, Neel},
	month = may,
	year = {2021},
	note = {arXiv:2009.05617 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{leinonen:llm-error-messages,
	address = {New York, NY, USA},
	series = {{SIGCSE} 2023},
	title = {Using {Large} {Language} {Models} to {Enhance} {Programming} {Error} {Messages}},
	isbn = {978-1-4503-9431-4},
	url = {https://dl.acm.org/doi/10.1145/3545945.3569770},
	doi = {10.1145/3545945.3569770},
	abstract = {A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix them. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.},
	urldate = {2024-02-05},
	booktitle = {{ACM} {Technical} {Symposium} on {Computer} {Science} {Education} ({SIGCSE})},
	publisher = {Association for Computing Machinery},
	author = {Leinonen, Juho and Hellas, Arto and Sarsa, Sami and Reeves, Brent and Denny, Paul and Prather, James and Becker, Brett A.},
	month = mar,
	year = {2023},
	keywords = {ai, codex, compiler error messages, large language models, programming error messages, syntax error messages},
	pages = {563--569},
}

@inproceedings{iyer:concode,
	address = {Brussels, Belgium},
	title = {Mapping {Language} to {Code} in {Programmatic} {Context}},
	url = {https://aclanthology.org/D18-1192},
	doi = {10.18653/v1/D18-1192},
	abstract = {Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to “return the smallest element” in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {1643--1652},
}

@inproceedings{fried:incoder,
	title = {{InCoder}: {A} {Generative} {Model} for {Code} {Infilling} and {Synthesis}},
	shorttitle = {{InCoder}},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{gulwani:synthesis,
	title = {Program {Synthesis}},
	volume = {4},
	issn = {2325-1107, 2325-1131},
	url = {https://www.nowpublishers.com/article/Details/PGL-010},
	doi = {10.1561/2500000010},
	abstract = {Program Synthesis},
	language = {English},
	number = {1-2},
	urldate = {2024-02-05},
	journal = {Foundations and Trends in Programming Languages},
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	month = jul,
	year = {2017},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--119},
}

@misc{singhal_nofuneval_2024,
	title = {{NoFunEval}: {Funny} {How} {Code} {LMs} {Falter} on {Requirements} {Beyond} {Functional} {Correctness}},
	shorttitle = {{NoFunEval}},
	url = {http://arxiv.org/abs/2401.15963},
	abstract = {Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics. We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their training setups. Surprisingly, even the classification accuracy on functional-correctness instances derived from the popular HumanEval benchmark is low, calling in question the depth of their comprehension and the source of their success in generating functionally-correct code in the first place. We will release our benchmark and evaluation scripts publicly at https://aka.ms/NoFunEval.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Singhal, Manav and Aggarwal, Tushar and Awasthi, Abhijeet and Natarajan, Nagarajan and Kanade, Aditya},
	month = jan,
	year = {2024},
	note = {arXiv:2401.15963 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{shetty_codescholar_2023,
	title = {{CodeScholar}: {Growing} {Idiomatic} {Code} {Examples}},
	shorttitle = {{CodeScholar}},
	url = {http://arxiv.org/abs/2312.15157},
	doi = {10.48550/arXiv.2312.15157},
	abstract = {Programmers often search for usage examples for API methods. A tool that could generate realistic, idiomatic, and contextual usage examples for one or more APIs would be immensely beneficial to developers. Such a tool would relieve the need for a deep understanding of the API landscape, augment existing documentation, and help discover interactions among APIs. We present CodeScholar, a tool that generates idiomatic code examples demonstrating the common usage of API methods. It includes a novel neural-guided search technique over graphs that grows the query APIs into idiomatic code examples. Our user study demonstrates that in 70\% of cases, developers prefer CodeScholar generated examples over state-of-the-art large language models (LLM) like GPT3.5. We quantitatively evaluate 60 single and 25 multi-API queries from 6 popular Python libraries and show that across-the-board CodeScholar generates more realistic, diverse, and concise examples. In addition, we show that CodeScholar not only helps developers but also LLM-powered programming assistants generate correct code in a program synthesis setting.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Shetty, Manish and Sen, Koushik and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2312.15157 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{wei_finetuned_2021,
	title = {Finetuned {Language} {Models} are {Zero}-{Shot} {Learners}},
	url = {https://openreview.net/forum?id=gEZrGCozdqR},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	language = {en},
	urldate = {2024-01-23},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = oct,
	year = {2021},
}

@inproceedings{liu:evalplus,
	title = {Is {Your} {Code} {Generated} by {ChatGPT} {Really} {Correct}? {Rigorous} {Evaluation} of {Large} {Language} {Models} for {Code} {Generation}},
	shorttitle = {Is {Your} {Code} {Generated} by {ChatGPT} {Really} {Correct}?},
	abstract = {Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based input generators to further validate the synthesized code. We extend the popular HUMANEVAL benchmark and build HUMANEVAL+ with 81x additionally generated tests. Our extensive evaluation across 14 popular LLMs demonstrates that HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by 15.1\% on average! Moreover, we even found several incorrect ground-truth implementations in HUMANEVAL. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis but also opens up a new direction to improve programming benchmarks through automated test input generation.},
	booktitle = {Conference on {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{barke:grounded-copilot,
	title = {Grounded {Copilot}: {How} {Programmers} {Interact} with {Code}-{Generating} {Models}},
	volume = {7},
	shorttitle = {Grounded {Copilot}},
	abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants--with a range of prior experience using the assistant--as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
	year = {2023},
	keywords = {Codexeval, Computer Science - Human-Computer Interaction, Computer Science - Programming Languages},
}

@inproceedings{athiwaratkun:mbxp,
	title = {Multi-lingual {Evaluation} of {Code} {Generation} {Models}},
	abstract = {We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks.},
	language = {en},
	author = {Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and Gonugondla, Sujan Kumar and Ding, Hantian and Kumar, Varun and Fulton, Nathan and Farahani, Arash and Jain, Siddhartha and Giaquinto, Robert and Qian, Haifeng and Ramanathan, Murali Krishna and Nallapati, Ramesh and Ray, Baishakhi and Bhatia, Parminder and Sengupta, Sudipta and Roth, Dan and Xiang, Bing},
	month = sep,
	year = {2022},
}

@misc{amini:mathqa,
	title = {{MathQA}: {Towards} {Interpretable} {Math} {Word} {Problem} {Solving} with {Operation}-{Based} {Formalisms}},
	shorttitle = {{MathQA}},
	url = {http://arxiv.org/abs/1905.13319},
	doi = {10.48550/arXiv.1905.13319},
	abstract = {We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/},
	urldate = {2024-01-20},
	publisher = {arXiv},
	author = {Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
	month = may,
	year = {2019},
	note = {arXiv:1905.13319 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{nguyen:charlie,
	title = {How {Beginning} {Programmers} and {Code} {LLMs} ({Mis})read {Each} {Other}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Sydney and Babe, Hannah McLean and Zi, Yangtian and Guha, Arjun and Anderson, Carolyn Jane and Feldman, Molly Q},
	month = may,
	year = {2024},
}

@misc{khare_understanding_2023,
	title = {Understanding the {Effectiveness} of {Large} {Language} {Models} in {Detecting} {Security} {Vulnerabilities}},
	url = {http://arxiv.org/abs/2311.16169},
	doi = {10.48550/arXiv.2311.16169},
	abstract = {Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection tools have made promising progress, their scalability and applicability remain challenging. Recently, Large Language Models (LLMs), such as GPT-4 and CodeLlama, have demonstrated remarkable performance on code-related tasks. However, it is unknown whether such LLMs can do complex reasoning over code. In this work, we explore whether pre-trained LLMs can detect security vulnerabilities and address the limitations of existing tools. We evaluate the effectiveness of pre-trained LLMs on a set of five diverse security benchmarks spanning two languages, Java and C/C++, and including code samples from synthetic and real-world projects. We evaluate the effectiveness of LLMs in terms of their performance, explainability, and robustness. By designing a series of effective prompting strategies, we obtain the best results on the synthetic datasets with GPT-4: F1 scores of 0.79 on OWASP, 0.86 on Juliet Java, and 0.89 on Juliet C/C++. Expectedly, the performance of LLMs drops on the more challenging real-world datasets: CVEFixes Java and CVEFixes C/C++, with GPT-4 reporting F1 scores of 0.48 and 0.62, respectively. We show that LLMs can often perform better than existing static analysis and deep learning-based vulnerability detection tools, especially for certain classes of vulnerabilities. Moreover, LLMs also often provide reliable explanations, identifying the vulnerable data flows in code. We find that fine-tuning smaller LLMs can outperform the larger LLMs on synthetic datasets but provide limited gains on real-world datasets. When subjected to adversarial attacks on code, LLMs show mild degradation, with average accuracy reduction of up to 12.67\%. Finally, we share our insights and recommendations for future work on leveraging LLMs for vulnerability detection.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Khare, Avishree and Dutta, Saikat and Li, Ziyang and Solko-Breslin, Alaia and Alur, Rajeev and Naik, Mayur},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16169 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@article{li:starcoder,
	title = {{StarCoder}: may the source be with you!},
	urldate = {2023-05-11},
	journal = {Transactions of Machine Learning Research (TMLR)},
	author = {Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and Liu, Qian and Zheltonozhskii, Evgenii and Zhuo, Terry Yue and Wang, Thomas and Dehaene, Olivier and Davaadorj, Mishig and Lamy-Poirier, Joel and Monteiro, João and Shliazhko, Oleh and Gontier, Nicolas and Meade, Nicholas and Zebaze, Armel and Yee, Ming-Ho and Umapathi, Logesh Kumar and Zhu, Jian and Lipkin, Benjamin and Oblokulov, Muhtasham and Wang, Zhiruo and Murthy, Rudra and Stillerman, Jason and Patel, Siva Sankalp and Abulkhanov, Dmitry and Zocca, Marco and Dey, Manan and Zhang, Zhihan and Fahmy, Nour and Bhattacharyya, Urvashi and Yu, Wenhao and Singh, Swayam and Luccioni, Sasha and Villegas, Paulo and Kunakov, Maxim and Zhdanov, Fedor and Romero, Manuel and Lee, Tony and Timor, Nadav and Ding, Jennifer and Schlesinger, Claire and Schoelkopf, Hailey and Ebert, Jan and Dao, Tri and Mishra, Mayank and Gu, Alex and Robinson, Jennifer and Anderson, Carolyn Jane and Dolan-Gavitt, Brendan and Contractor, Danish and Reddy, Siva and Fried, Daniel and Bahdanau, Dzmitry and Jernite, Yacine and Ferrandis, Carlos Muñoz and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
	month = dec,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{zhao_github_2023,
	title = {{GitHub} {Copilot} now has a better {AI} model and new capabilities},
	url = {https://github.blog/2023-02-14-github-copilot-now-has-a-better-ai-model-and-new-capabilities/},
	abstract = {We’re launching new improvements to GitHub Copilot to make it more powerful and more responsive for developers.},
	language = {en-US},
	urldate = {2023-12-12},
	journal = {The GitHub Blog},
	author = {Zhao, Shuyin},
	month = feb,
	year = {2023},
}

@inproceedings{edwards:testing-in-education,
	address = {Norfolk Virginia USA},
	title = {Using software testing to move students from trial-and-error to reflection-in-action},
	isbn = {978-1-58113-798-9},
	url = {https://dl.acm.org/doi/10.1145/971300.971312},
	doi = {10.1145/971300.971312},
	language = {en},
	urldate = {2023-12-11},
	booktitle = {Proceedings of the 35th {SIGCSE} technical symposium on {Computer} science education},
	publisher = {ACM},
	author = {Edwards, Stephen H.},
	month = mar,
	year = {2004},
	pages = {26--30},
}

@book{flatt:htdp,
	title = {How {To} {Design} {Programs}},
	publisher = {MIT Press},
	author = {Flatt, Matthew and Felleisen, Matthias and Findler, Robert Bruce and Krishnamurthi, Shriram},
	year = {2001},
}

@misc{openai:gpt4,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{kazemitabaar_studying_2023,
	address = {Hamburg Germany},
	title = {Studying the effect of {AI} {Code} {Generators} on {Supporting} {Novice} {Learners} in {Introductory} {Programming}},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580919},
	doi = {10.1145/3544548.3580919},
	language = {en},
	urldate = {2023-12-11},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi},
	month = apr,
	year = {2023},
	pages = {1--23},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{phung_generating_2023,
	title = {Generating {High}-{Precision} {Feedback} for {Programming} {Syntax} {Errors} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.04662},
	doi = {10.48550/arXiv.2302.04662},
	abstract = {Large language models (LLMs), such as Codex, hold great promise in enhancing programming education by automatically generating feedback for students. We investigate using LLMs to generate feedback for fixing syntax errors in Python programs, a key scenario in introductory programming. More concretely, given a student's buggy program, our goal is to generate feedback comprising a fixed program along with a natural language explanation describing the errors/fixes, inspired by how a human tutor would give feedback. While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms. The main research question we study is: Can we develop LLMs-based feedback generation techniques with a tunable precision parameter, giving educators quality control over the feedback that students receive? To this end, we introduce PyFiXV, our technique to generate high-precision feedback powered by Codex. The key idea behind PyFiXV is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student; notably, this validation mechanism also provides a precision knob to educators. We perform an extensive evaluation using two real-world datasets of Python programs with syntax errors and show the efficacy of PyFiXV in generating high-precision feedback.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Phung, Tung and Cambronero, José and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
	month = apr,
	year = {2023},
	note = {arXiv:2302.04662 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
}

@article{joshi:ring,
	title = {Repair {Is} {Nearly} {Generation}: {Multilingual} {Program} {Repair} with {LLMs}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Repair {Is} {Nearly} {Generation}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25642},
	doi = {10.1609/aaai.v37i4.25642},
	language = {en},
	number = {4},
	urldate = {2023-08-17},
	journal = {AAAI Conference on Artificial Intelligence},
	author = {Joshi, Harshit and Sanchez, José Cambronero and Gulwani, Sumit and Le, Vu and Verbruggen, Gust and Radiček, Ivan},
	month = jun,
	year = {2023},
	note = {Number: 4},
	keywords = {SNLP: Language Models},
	pages = {5131--5140},
}

@misc{geng:bert-ocaml-errors,
	title = {Novice {Type} {Error} {Diagnosis} with {Natural} {Language} {Models}},
	abstract = {Strong static type systems help programmers eliminate many errors without much burden of supplying type annotations. However, this flexibility makes it highly non-trivial to diagnose ill-typed programs, especially for novice programmers. Compared to classic constraint solving and optimization-based approaches, the data-driven approach has shown great promise in identifying the root causes of type errors with higher accuracy. Instead of relying on hand-engineered features, this work explores natural language models for type error localization, which can be trained in an end-to-end fashion without requiring any features. We demonstrate that, for novice type error diagnosis, the language model-based approach significantly outperforms the previous state-of-the-art data-driven approach. Specifically, our model could predict type errors correctly 62\% of the time, outperforming the state-of-the-art Nate's data-driven model by 11\%, in a more rigorous accuracy metric. Furthermore, we also apply structural probes to explain the performance difference between different language models.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Geng, Chuqin and Ye, Haolin and Li, Yixuan and Han, Tianyu and Pientka, Brigitte and Si, Xujie},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03682 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{polycoder,
	title = {A {Systematic} {Evaluation} of {Large} {Language} {Models} of {Code}},
	language = {en},
	urldate = {2023-11-01},
	booktitle = {Deep {Learning} for {Code} {Workshop} ({DL4C})},
	author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
}

@inproceedings{noauthor_flowmind_nodate,
	title = {{FlowMind}},
}

@misc{singh_codefusion_2023,
	title = {{CodeFusion}: {A} {Pre}-trained {Diffusion} {Model} for {Code} {Generation}},
	shorttitle = {{CodeFusion}},
	url = {http://arxiv.org/abs/2310.17680},
	doi = {10.48550/arXiv.2310.17680},
	abstract = {Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Singh, Mukul and Cambronero, José and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Verbruggen, Gust},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17680 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{babelcode,
	title = {Measuring the {Impact} of {Programming} {Language} {Distribution}},
	url = {https://proceedings.mlr.press/v202/orlanski23a.html},
	language = {en},
	urldate = {2023-11-01},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Orlanski, Gabriel and Xiao, Kefan and Garcia, Xavier and Hui, Jeffrey and Howland, Joshua and Malmaud, Jonathan and Austin, Jacob and Singh, Rishabh and Catasta, Michele},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {26619--26645},
}

@misc{bigquery-code-dataset,
	title = {{GitHub} on {BigQuery}: {Analyze} all the open source code},
	shorttitle = {{GitHub} on {BigQuery}},
	url = {https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code},
	language = {en-US},
	urldate = {2023-11-01},
	journal = {Google Cloud Blog},
	author = {{Felipe Hoffa}},
	year = {2016},
}

@misc{huang_large_2023,
	title = {Large {Language} {Models} {Cannot} {Self}-{Correct} {Reasoning} {Yet}},
	url = {http://arxiv.org/abs/2310.01798},
	abstract = {Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to selfcorrect their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.},
	language = {en},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01798 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zheng_lmsys-chat-1m_2023,
	title = {{LMSYS}-{Chat}-{1M}: {A} {Large}-{Scale} {Real}-{World} {LLM} {Conversation} {Dataset}},
	shorttitle = {{LMSYS}-{Chat}-{1M}},
	url = {http://arxiv.org/abs/2309.11998},
	abstract = {Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset’s content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.},
	language = {en},
	urldate = {2023-10-20},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion and Zhang, Hao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11998 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wang_mint_2023,
	title = {{MINT}: {Evaluating} {LLMs} in {Multi}-turn {Interaction} with {Tools} and {Language} {Feedback}},
	shorttitle = {{MINT}},
	url = {http://arxiv.org/abs/2309.10691},
	abstract = {To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1-8\% for each turn of tool use and 2-17\% with natural language feedback. (b) Better single-turn performance does not guarantee better multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.},
	language = {en},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
	month = oct,
	year = {2023},
	note = {arXiv:2309.10691 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_lemur_2023,
	title = {Lemur: {Harmonizing} {Natural} {Language} and {Code} for {Language} {Agents}},
	shorttitle = {Lemur},
	url = {http://arxiv.org/abs/2310.06830},
	abstract = {We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur’s superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments.},
	language = {en},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Xu, Yiheng and Su, Hongjin and Xing, Chen and Mi, Boyu and Liu, Qian and Shi, Weijia and Hui, Binyuan and Zhou, Fan and Liu, Yitao and Xie, Tianbao and Cheng, Zhoujun and Zhao, Siheng and Kong, Lingpeng and Wang, Bailin and Xiong, Caiming and Yu, Tao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06830 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{gunasekar:phi-1,
	title = {Textbooks {Are} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2306.11644},
	doi = {10.48550/arXiv.2306.11644},
	abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11644 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{heidorn:english-vhll,
	address = {New York, NY, USA},
	title = {English as a very high level language for simulation programming},
	isbn = {978-1-4503-7884-0},
	url = {https://dl.acm.org/doi/10.1145/800233.807050},
	doi = {10.1145/800233.807050},
	abstract = {An automatic programming system which produces simulation programs from information obtained through natural language dialogue has been implemented under CP/CMS on the IBM 360/67. In the current version the information obtained from an English conversation about a simple queuing problem is used to build a language independent entity-attribute-value data structure. From this structure both an English description of the problem and a GPSS simulation program for it can be produced. This processing is done by a FORTRAN program which interprets sets of decoding and encoding rules written in a specially developed grammar-rule language. The paper includes a complete sample problem with a discussion of its processing and examples of decoding and encoding rules.},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} symposium on {Very} high level languages},
	publisher = {Association for Computing Machinery},
	author = {Heidorn, George E.},
	month = mar,
	year = {1974},
	pages = {91--100},
}

@article{sammet:english-pl,
	title = {The use of {English} as a programming language},
	volume = {9},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/365230.365274},
	doi = {10.1145/365230.365274},
	abstract = {The purpose of this talk is to make a personal plea, backed up by some practical comments, for the use of English or anyone else's natural language as a programming language. This seems to be a suitable subject for the conference, since whatever definition of pragmatics is decided upon, it certainly seems to be tied in with the users of any programming language and what the language means to them.},
	number = {3},
	journal = {Communications of the ACM},
	author = {Sammet, Jean E.},
	month = mar,
	year = {1966},
	pages = {228--230},
}

@inproceedings{ballard:nlc,
	address = {New York, NY, USA},
	title = {Programming in natural language: “{NLC}” as a prototype},
	isbn = {978-0-89791-008-8},
	shorttitle = {Programming in natural language},
	url = {https://dl.acm.org/doi/10.1145/800177.810072},
	doi = {10.1145/800177.810072},
	booktitle = {Annual {Conference} of the {ACM}},
	publisher = {Association for Computing Machinery},
	author = {Ballard, Bruce W. and Biermann, Alan W.},
	year = {1979},
	pages = {228--237},
}

@article{touvron_llama_nodate,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	language = {en},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
}

@inproceedings{lai:ds1000,
	title = {{DS}-1000: {A} {Natural} and {Reliable} {Benchmark} for {Data} {Science} {Code} {Generation}},
	shorttitle = {{DS}-1000},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Scott Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{austin:mbpp,
	title = {Program {Synthesis} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2108.07732},
	doi = {10.48550/arXiv.2108.07732},
	urldate = {2023-09-01},
	publisher = {arXiv},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{bareis_code_2022,
	title = {Code {Generation} {Tools} ({Almost}) for {Free}? {A} {Study} of {Few}-{Shot}, {Pre}-{Trained} {Language} {Models} on {Code}},
	shorttitle = {Code {Generation} {Tools} ({Almost}) for {Free}?},
	url = {http://arxiv.org/abs/2206.01335},
	abstract = {Few-shot learning with large-scale, pre-trained language models is a powerful way to answer questions about code, e.g., how to complete a given code example, or even generate code snippets from scratch. The success of these models raises the question whether they could serve as a basis for building a wide range code generation tools. Traditionally, such tools are built manually and separately for each task. Instead, few-shot learning may allow to obtain different tools from a single pre-trained language model by simply providing a few examples or a natural language description of the expected tool behavior. This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose. We consider three code manipulation and code generation tasks targeted by a range of traditional tools: (i) code mutation; (ii) test oracle generation from natural language documentation; and (iii) test case generation. For each task, we compare few-shot learning to a manually built tool. Our results show that the model-based tools complement (code mutation), are on par (test oracle generation), or even outperform their respective traditionally built tool (test case generation), while imposing far less effort to develop them. By comparing the effectiveness of different variants of the model-based tools, we provide insights on how to design an appropriate input (“prompt”) to the model and what influence the size of the model has. For example, we find that providing a small natural language description of the code generation task is an easy way to improve predictions. Overall, we conclude that few-shot language models are surprisingly effective, yet there is still more work to be done, such as exploring more diverse ways of prompting and tackling even more involved tasks.},
	language = {en},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Bareiß, Patrick and Souza, Beatriz and d'Amorim, Marcelo and Pradel, Michael},
	month = jun,
	year = {2022},
	note = {arXiv:2206.01335 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{lemieux:codamosa,
	address = {Melbourne, Australia},
	title = {{CodaMosa}: {Escaping} {Coverage} {Plateaus} in {Test} {Generation} with {Pre}-trained {Large} {Language} {Models}},
	isbn = {978-1-66545-701-9},
	shorttitle = {{CodaMosa}},
	url = {https://ieeexplore.ieee.org/document/10172800/},
	doi = {10.1109/ICSE48619.2023.00085},
	language = {en},
	urldate = {2023-08-17},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
	month = may,
	year = {2023},
	pages = {919--931},
}

@misc{ziyang:wizard-coder,
	title = {{WizardCoder}: {Empowering} {Code} {Large} {Language} {Models} with {Evol}-{Instruct}},
	shorttitle = {{WizardCoder}},
	url = {http://arxiv.org/abs/2306.08568},
	doi = {10.48550/arXiv.2306.08568},
	abstract = {Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM},
	urldate = {2023-08-12},
	publisher = {arXiv},
	author = {Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08568 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{wang:self-instruct,
	title = {Self-{Instruct}: {Aligning} {Language} {Model} with {Self} {Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	urldate = {2023-04-02},
	booktitle = {Annual {Meeting} of the {Association} of {Computation} {Linguistics} ({ACL})},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{kocetkov:the-stack,
	title = {The {Stack}: 3 {TB} of permissively licensed source code},
	shorttitle = {The {Stack}},
	url = {http://arxiv.org/abs/2211.15533},
	urldate = {2023-08-12},
	booktitle = {Deep {Learning} for {Code} {Workshop} ({DL4C})},
	author = {Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Muñoz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and von Werra, Leandro and de Vries, Harm},
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{replit-code,
	title = {Replit {Code} v1.3},
	url = {https://huggingface.co/replit/replit-code-v1-3b},
	urldate = {2023-08-12},
	author = {{Replit}},
	month = may,
	year = {2023},
}

@misc{denny:promptly,
	title = {Promptly: {Using} {Prompt} {Problems} to {Teach} {Learners} {How} to {Effectively} {Utilize} {AI} {Code} {Generators}},
	shorttitle = {Promptly},
	url = {http://arxiv.org/abs/2307.16364},
	doi = {10.48550/arXiv.2307.16364},
	urldate = {2023-08-05},
	publisher = {arXiv},
	author = {Denny, Paul and Leinonen, Juho and Prather, James and Luxton-Reilly, Andrew and Amarouche, Thezyrie and Becker, Brett A. and Reeves, Brent N.},
	month = jul,
	year = {2023},
	note = {arXiv:2307.16364 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@article{cassano:multipl-e,
	title = {{MultiPL}-{E}: {A} {Scalable} and {Polyglot} {Approach} to {Benchmarking} {Neural} {Code} {Generation}},
	volume = {49},
	shorttitle = {{MultiPL}-{E}},
	number = {7},
	urldate = {2023-03-02},
	journal = {IEEE Transactions on Software Engineering (TSE)},
	author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q. and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
	year = {2023},
	pages = {3675--3691},
}

@article{wasmfx,
	title = {Continuing {WebAssembly} with {Effect} {Handlers}},
	volume = {7},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Phipps-Costin, Luna and Rossberg, Andreas and Guha, Arjun and Leijen, Daan and Hillerström, Daniel and Sivaramakrishnan, KC and Pretnar, Matija and Lindley, Sam},
	year = {2023},
}

@article{open-science,
	title = {Open science is a research accelerator},
	volume = {3},
	copyright = {2011 Springer Nature Limited},
	issn = {1755-4349},
	url = {https://www.nature.com/articles/nchem.1149},
	doi = {10.1038/nchem.1149},
	language = {en},
	number = {10},
	urldate = {2023-06-17},
	journal = {Nature Chemistry},
	author = {Woelfle, Michael and Olliaro, Piero and Todd, Matthew H.},
	month = oct,
	year = {2011},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Medicinal chemistry, Total synthesis},
	pages = {745--748},
}

@misc{noauthor_home_nodate,
	title = {Home},
	url = {https://www.cs.cornell.edu/~jnfoster/},
	abstract = {Home},
	language = {en},
	urldate = {2023-06-08},
	journal = {Nate Foster},
}

@misc{muennighoff_scaling_2023,
	title = {Scaling {Data}-{Constrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.16264},
	abstract = {The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.},
	language = {en},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
	month = may,
	year = {2023},
	note = {arXiv:2305.16264 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, To Read},
}

@misc{cassano:opentau,
	title = {Type {Prediction} {With} {Program} {Decomposition} and {Fill}-in-the-{Type} {Training}},
	url = {https://arxiv.org/abs/2305.17145v1},
	language = {en},
	urldate = {2023-06-03},
	journal = {arXiv.org},
	author = {Cassano, Federico and Yee, Ming-Ho and Shinn, Noah and Guha, Arjun and Holtzen, Steven},
	month = may,
	year = {2023},
}

@inproceedings{shambaugh:rehearsal,
	title = {Rehearsal: {A} {Configuration} {Verification} {Tool} for {Puppet}},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	author = {Shambaugh, Rian and Weiss, Aaron and Guha, Arjun},
	year = {2016},
}

@inproceedings{holtz:idips,
	title = {Iterative {Program} {Synthesis} for {Adaptable} {Social} {Navigation}},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Holtz, Jarrett and Andrews, Simon and Guha, Arjun and Biswas, Joydeep},
	year = {2021},
}

@inproceedings{yee:typeweaver,
	title = {Do {Machine} {Learning} {Models} {Produce} {TypeScript} {Types} that {Type} {Check}?},
	booktitle = {European {Conference} on {Object} {Oriented} {Programming} ({ECOOP})},
	author = {Yee, Ming-Ho and Guha, Arjun},
	year = {2023},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{berabi_tfix_2021,
	title = {{TFix}: {Learning} to {Fix} {Coding} {Errors} with a {Text}-to-{Text} {Transformer}},
	shorttitle = {{TFix}},
	url = {https://proceedings.mlr.press/v139/berabi21a.html},
	abstract = {The problem of fixing errors in programs has attracted substantial interest over the years. The key challenge for building an effective code fixing tool is to capture a wide range of errors and meanwhile maintain high accuracy. In this paper, we address this challenge and present a new learning-based system, called TFix. TFix works directly on program text and phrases the problem of code fixing as a text-to-text task. In turn, this enables it to leverage a powerful Transformer based model pre-trained on natural language and fine-tuned to generate code fixes (via a large, high-quality dataset obtained from GitHub commits). TFix is not specific to a particular programming language or class of defects and, in fact, improved its precision by simultaneously fine-tuning on 52 different error types reported by a popular static analyzer. Our evaluation on a massive dataset of JavaScript programs shows that TFix is practically effective: it is able to synthesize code that fixes the error in  67 percent of cases and significantly outperforms existing learning-based approaches.},
	language = {en},
	urldate = {2023-04-25},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Berabi, Berkay and He, Jingxuan and Raychev, Veselin and Vechev, Martin},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {780--791},
}

@inproceedings{nijkamp:codegen,
	title = {{CodeGen}: {An} {Open} {Large} {Language} {Model} for {Code} with {Multi}-{Turn} {Program} {Synthesis}},
	author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{codewhisperer,
	title = {{ML}-powered coding companion – {Amazon} {CodeWhisperer} – {Amazon} {Web} {Services}},
	url = {https://aws.amazon.com/codewhisperer/},
	language = {en-US},
	urldate = {2023-03-29},
	journal = {Amazon Web Services, Inc.},
}

@misc{chandra:neural-software-analysis,
	title = {Neural {Software} {Analysis}},
	url = {https://cacm.acm.org/magazines/2022/1/257449-neural-software-analysis/abstract},
	abstract = {Developer tools that use a neural machine learning model to make predictions about previously unseen code.},
	language = {en},
	urldate = {2023-03-21},
	author = {Chandra, Satish, Michael Pradel},
}

@misc{lahiri:ticoder,
	title = {Interactive {Code} {Generation} via {Test}-{Driven} {User}-{Intent} {Formalization}},
	url = {http://arxiv.org/abs/2208.05950},
	abstract = {Pre-trained large language models (LLMs) such as OpenAI Codex have shown immense potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, the code produced does not have any correctness guarantees around satisfying user's intent. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics. In this paper, we take a first step towards addressing the problem above by proposing the workflow of test-driven user-intent formalization (TDUIF), which leverages lightweight user feedback to jointly (a) formalize the user intent as tests (a partial specification), and (b) generates code that meets the formal user intent. To perform a scalable and large-scale automated evaluation of the algorithms without requiring a user in the loop, we describe how to simulate user interaction with high-fidelity using a reference solution. We also describe and implement alternate implementations of several algorithmic components (including mutating and ranking a set of tests) that can be composed for efficient solutions to the TDUIF problem. We have developed a system TICODER that implements several solutions to TDUIF, and compare their relative effectiveness on the MBPP academic code generation benchmark. Our results are promising with using the OpenAI Codex LLM on MBPP: our best algorithm improves the pass@1 code generation accuracy metric from 48.39\% to 70.49\% with a single user query, and up to 85.48\% with up to 5 user queries. Second, we can generate a non-trivial functional unit test consistent with the user intent within an average of 1.69 user queries for 90.40\% of the examples for this dataset.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Lahiri, Shuvendu K. and Naik, Aaditya and Sakkas, Georgios and Choudhury, Piali and von Veh, Curtis and Musuvathi, Madanlal and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05950 [cs]},
	keywords = {To Read},
}

@inproceedings{chen:codet,
	title = {{CodeT}: {Code} {Generation} with {Generated} {Tests}},
	shorttitle = {{CodeT}},
	url = {https://openreview.net/forum?id=ktrw68Cmu9c},
	language = {en},
	urldate = {2023-03-21},
	author = {Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
	month = feb,
	year = {2023},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{pinckney:npm-mining,
	title = {A {Large} {Scale} {Analysis} of {Semantic} {Versioning} in {NPM}},
	booktitle = {Mining {Software} {Repositories} ({MSR})},
	author = {Pinckney, Donald and Cassano, Federico and Guha, Arjun and Bell, Jonathan},
	year = {2023},
}

@inproceedings{allal:santacoder,
	title = {{SantaCoder}: don't reach for the stars!},
	booktitle = {Deep {Learning} for {Code} {Workshop} ({DL4C})},
	author = {Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and Umapathi, Logesh Kumar and Anderson, Carolyn Jane and Zi, Yangtian and Poirier, Joel Lamy and Schoelkopf, Hailey and Troshin, Sergey and Abulkhanov, Dmitry and Romero, Manuel and Lappert, Michael and De Toni, Francesco and del Río, Bernardo García and Liu, Qian and Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue and Yu, Ian and Villegas, Paulo and Zocca, Marco and Mangrulkar, Sourab and Lansky, David and Nguyen, Huu and Contractor, Danish and Villa, Luis and Li, Jia and Bahdanau, Dzmitry and Jernite, Yacine and Hughes, Sean and Fried, Daniel and Guha, Arjun and de Vries, Harm and von Werra, Leandro},
	year = {2023},
}

@article{wang_documentation_2022,
	title = {Documentation {Matters}: {Human}-{Centered} {AI} {System} to {Assist} {Data} {Science} {Code} {Documentation} in {Computational} {Notebooks}},
	volume = {29},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Documentation {Matters}},
	url = {https://dl.acm.org/doi/10.1145/3489465},
	doi = {10.1145/3489465},
	abstract = {Computational notebooks allow data scientists to express their ideas through a combination of code and documentation. However, data scientists often pay attention only to the code, and neglect creating or updating their documentation during quick iterations. Inspired by human documentation practices learned from 80 highly-voted Kaggle notebooks, we design and implement Themisto, an automated documentation generation system to explore how human-centered AI systems can support human data scientists in the machine learning code documentation scenario. Themisto facilitates the creation of documentation via three approaches: a deep-learning-based approach to generate documentation for source code, a query-based approach to retrieve online API documentation for source code, and a user prompt approach to nudge users to write documentation. We evaluated Themisto in a within-subjects experiment with 24 data science practitioners, and found that automated documentation generation techniques reduced the time for writing documentation, reminded participants to document code they would have ignored, and improved participants’ satisfaction with their computational notebook.},
	language = {en},
	number = {2},
	urldate = {2023-03-07},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Wang, April Yi and Wang, Dakuo and Drozdal, Jaimie and Muller, Michael and Park, Soya and Weisz, Justin D. and Liu, Xuye and Wu, Lingfei and Dugan, Casey},
	month = apr,
	year = {2022},
	pages = {1--33},
}

@misc{noauthor_intel_nodate,
	title = {Intel® {Programmable} {Acceleration} {Card} with {Intel}® {Arria}® 10 {GX} {FPGA}},
	url = {https://www.intel.com/content/www/us/en/products/details/fpga/platforms/pac/arria-10-gx.html},
	abstract = {Intel® Programmable Acceleration Card with Intel® Arria® 10 GX FPGA (Intel® PAC with Intel® Arria® 10 GX FPGA) provides both inline and lookaside acceleration.},
	language = {en},
	urldate = {2023-02-28},
	journal = {Intel},
}

@misc{noauthor_alveo_nodate,
	title = {Alveo},
	url = {https://www.xilinx.com/products/boards-and-kits/alveo.html},
	abstract = {Alveo Data Center accelerator cards with their ready to go applications deliver a much-needed increase in compute capability, at lowest TCO, for the broadest range of workloads.},
	language = {en},
	urldate = {2023-02-28},
	journal = {Xilinx},
}

@misc{apple:metal,
	title = {Metal {Overview}},
	url = {https://developer.apple.com/metal/},
	language = {en},
	urldate = {2023-02-26},
	journal = {Apple Developer},
	author = {Inc, Apple},
}

@misc{chetlur:cudnn,
	title = {{cuDNN}: {Efficient} {Primitives} for {Deep} {Learning}},
	shorttitle = {{cuDNN}},
	url = {http://arxiv.org/abs/1410.0759},
	urldate = {2023-02-26},
	publisher = {arXiv},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	month = dec,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{paszke:pytorch,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	urldate = {2023-02-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	keywords = {Deep learning},
}

@inproceedings{gao_refty_2022,
	title = {Refty: {Refinement} {Types} for {Valid} {Deep} {Learning} {Models}},
	shorttitle = {Refty},
	url = {https://www.microsoft.com/en-us/research/publication/refty-refinement-types-for-valid-deep-learning-models/},
	abstract = {Deep learning has been increasingly adopted in many application areas. To construct valid deep learning models, developers must conform to certain computational constraints by carefully selecting appropriate neural architectures and hyperparameter values. For example, the kernel size hyperparameter of the 2D convolution operator cannot be overlarge to ensure that the height and width of the […]},
	language = {en-US},
	urldate = {2023-02-24},
	author = {Gao, Yanjie and Li, Zhengxian and Lin, Haoxiang and Zhang, Hongyu and Wu, Ming and Yang, Mao},
	month = may,
	year = {2022},
}

@incollection{silva_checking_2021,
	address = {Cham},
	title = {Checking {Data}-{Race} {Freedom} of {GPU} {Kernels}, {Compositionally}},
	volume = {12759},
	isbn = {978-3-030-81684-1 978-3-030-81685-8},
	url = {https://link.springer.com/10.1007/978-3-030-81685-8_19},
	abstract = {GPUs oﬀer parallelism as a commodity, but they are diﬃcult to program correctly. Static analyzers that guarantee data-race freedom (DRF) are essential to help programmers establish the correctness of their programs (kernels). However, existing approaches produce too many false alarms and struggle to handle larger programs. To address these limitations we formalize a novel compositional analysis for DRF, based on access memory protocols. These protocols are behavioral types that codify the way threads interact over shared memory.},
	language = {en},
	urldate = {2023-02-22},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer International Publishing},
	author = {Cogumbreiro, Tiago and Lange, Julien and Rong, Dennis Liew Zhen and Zicarelli, Hannah},
	editor = {Silva, Alexandra and Leino, K. Rustan M.},
	year = {2021},
	doi = {10.1007/978-3-030-81685-8_19},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {NSF Large},
	pages = {403--426},
}

@inproceedings{gao:dnn-sat,
	title = {Resource-{Guided} {Configuration} {Space} {Reduction} for {Deep} {Learning} {Models}},
	doi = {10.1109/ICSE43902.2021.00028},
	abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity. In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Analytical models, Computational modeling, Deep learning, Software engineering, Software systems, Tools, Training, configurable systems, deep learning, AutoML, constraint solving},
	pages = {175--187},
}

@inproceedings{cao:dl-perf,
	title = {Understanding {Performance} {Problems} in {Deep} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2112.01771},
	doi = {10.1145/3540250.3549123},
	abstract = {Deep learning (DL) has been widely applied to many domains. Unique challenges in engineering DL systems are posed by the programming paradigm shift from traditional systems to DL systems, and performance is one of the challenges. Performance problems (PPs) in DL systems can cause severe consequences such as excessive resource consumption and financial loss. While bugs in DL systems have been extensively investigated, PPs in DL systems have hardly been explored. To bridge this gap, we present the first comprehensive study to i) characterize symptoms, root causes, and introducing and exposing stages of PPs in DL systems developed in TensorFLow and Keras, with 224 PPs collected from 210 StackOverflow posts, and to ii) assess the capability of existing performance analysis approaches in tackling PPs, with a constructed benchmark of 58 PPs in DL systems. Our findings shed light on the implications on developing high-performance DL systems, and detecting and localizing PPs in DL systems. To demonstrate the usefulness of our findings, we develop a static checker Deep-Perf to detect three types of PPs. It has detected 488 new PPs in 130 GitHub projects. 105 and 27 PPs have been confirmed and fixed.},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Cao, Junming and Chen, Bihuan and Sun, Chao and Hu, Longjie and Wu, Shuaihong and Peng, Xin},
	month = nov,
	year = {2022},
	note = {arXiv:2112.01771 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, D.2.5},
	pages = {357--369},
}

@article{gao:dnnperf,
	title = {Runtime {Performance} {Prediction} for {Deep} {Learning} {Models} with {Graph} {Neural} {Network}},
	abstract = {Deep learning (DL) models have been widely adopted in many application domains. Predicting the runtime performance of DL models such as GPU memory consumption and training time is important for boosting development productivity and reducing resource waste because improper configurations of hyperparameters and neural architectures can result in many failed training jobs or unsatisfactory models. However, the runtime performance prediction of DL models is challenging because of the hybrid DL programming paradigm, complicated hidden factors within the framework runtime, fairly huge model configuration space, and wide differences among models. In this paper, we propose DNNPerf, a novel Graph Neural Network-based tool for predicting the runtime performance of DL models using Graph Neural Network. DNNPerf represents a DL model as a directed acyclic computation graph and incorporates a rich set of performance-related features based on the computational semantics of both nodes and edges. We also propose a new Attention-based Node-Edge Encoder to better encode the node and edge features. DNNPerf is evaluated on thousands of configurations of real-world and synthetic DL models to predict their GPU memory consumption and training time. The experimental results show that DNNPerf achieves accurate predictions, with an overall error of 13.684\% for the GPU memory consumption prediction and an overall error of 7.443\% for the training time prediction, outperforming all the compared methods.},
	language = {en},
	journal = {IEEE International Conference on Software Engineering (ICSE) - Software Engineering in Practice (SIEP)},
	author = {Gao, Yanjie and Gu, Xianyu and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
}

@inproceedings{liu:tensorflow-bug-detector,
	title = {Detecting {TensorFlow} {Program} {Bugs} in {Real}-{World} {Industrial} {Environment}},
	doi = {10.1109/ASE51524.2021.9678891},
	abstract = {Deep learning has been widely adopted in industry and has achieved great success in a wide range of application areas. Bugs in deep learning programs can cause catastrophic failures, in addition to a serious waste of resources and time.This paper aims at detecting industrial TensorFlow program bugs. We report an extensive empirical study on 12,289 failed TensorFlow jobs, showing that existing static tools can effectively detect 72.55\% of the top three types of Python bugs in industrial TensorFlow programs. In addition, we propose (for the first time) a constraint-based approach for detecting TensorFlow shape-related errors (one of the most common TensorFlow-specific bugs), together with an associated tool, ShapeTracer. Our evaluation on a set of 60 industrial TensorFlow programs shows that ShapeTracer is efficient and effective: it analyzes each program in at most 3 seconds and detects effectively 40 out of 60 industrial TensorFlow program bugs, with no false positives. ShapeTracer has been deployed in the platform-X platform and will be released soon.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Liu, Chen and Lu, Jie and Li, Guangwei and Yuan, Ting and Li, Lian and Tan, Feng and Yang, Jun and You, Liang and Xue, Jingling},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Computer bugs, Constraint Solving, Deep learning, Industries, Python, Software engineering, TensorFlow Bugs, Time factors},
	pages = {55--66},
}

@inproceedings{gao:dnnmem,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Estimating {GPU} memory consumption of deep learning models},
	isbn = {978-1-4503-7043-1},
	url = {https://doi.org/10.1145/3368089.3417050},
	doi = {10.1145/3368089.3417050},
	abstract = {Deep learning (DL) has been increasingly adopted by a variety of software-intensive systems. Developers mainly use GPUs to accelerate the training, testing, and deployment of DL models. However, the GPU memory consumed by a DL model is often unknown to them before the DL job executes. Therefore, an improper choice of neural architecture or hyperparameters can cause such a job to run out of the limited GPU memory and fail. Our recent empirical study has found that many DL job failures are due to the exhaustion of GPU memory. This leads to a horrendous waste of computing resources and a significant reduction in development productivity. In this paper, we propose DNNMem, an accurate estimation tool for GPU memory consumption of DL models. DNNMem employs an analytic estimation approach to systematically calculate the memory consumption of both the computation graph and the DL framework runtime. We have evaluated DNNMem on 5 real-world representative models with different hyperparameters under 3 mainstream frameworks (TensorFlow, PyTorch, and MXNet). Our extensive experiments show that DNNMem is effective in estimating GPU memory consumption.},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Yanjie and Liu, Yu and Zhang, Hongyu and Li, Zhengxian and Zhu, Yonghao and Lin, Haoxiang and Yang, Mao},
	month = nov,
	year = {2020},
	keywords = {deep learning, estimation model, memory consumption, program analysis},
	pages = {1342--1352},
}

@inproceedings{tillet:triton,
	address = {Phoenix AZ USA},
	title = {Triton: an intermediate language and compiler for tiled neural network computations},
	isbn = {978-1-4503-6719-6},
	shorttitle = {Triton},
	url = {https://dl.acm.org/doi/10.1145/3315508.3329973},
	doi = {10.1145/3315508.3329973},
	language = {en},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages}},
	publisher = {ACM},
	author = {Tillet, Philippe and Kung, H. T. and Cox, David},
	month = jun,
	year = {2019},
	pages = {10--19},
}

@inproceedings{reed:torchfx,
	title = {torch.fx: {Practical} {Program} {Capture} and {Transformation} for {Deep} {Learning} in {Python}},
	abstract = {Modern deep learning frameworks provide imperative, eager execution programming interfaces embedded in Python to provide a productive development experience. However, deep learning practitioners sometimes need to capture and transform program structure for performance optimization, visualization, analysis, and hardware integration. We study the different designs for program capture and transformation used in deep learning. By designing for typical deep learning use cases rather than long tail ones, it is possible to create a simpler framework for program capture and transformation. We apply this principle in torch.fx, a program capture and transformation library for PyTorch written entirely in Python and optimized for high developer productivity by ML practitioners. We present case studies showing how torch.fx enables workﬂows previously inaccessible in the PyTorch ecosystem.},
	booktitle = {Machine {Learning} and {Systems} ({MLSys})},
	author = {Reed, James K and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
	year = {2022},
}

@inproceedings{chang:deep-learning-failures,
	address = {Seoul South Korea},
	title = {An empirical study on program failures of deep learning jobs},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380362},
	doi = {10.1145/3377811.3380362},
	abstract = {Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.},
	language = {en},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao},
	month = jun,
	year = {2020},
	pages = {1159--1170},
}

@inproceedings{abadi:tensorflow,
	address = {USA},
	series = {{OSDI}'16},
	title = {{TensorFlow}: a system for large-scale machine learning},
	isbn = {978-1-931971-33-1},
	shorttitle = {{TensorFlow}},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = nov,
	year = {2016},
	pages = {265--283},
}

@inproceedings{bergstra:theano,
	address = {Austin, Texas},
	title = {Theano: {A} {CPU} and {GPU} {Math} {Compiler} in {Python}},
	shorttitle = {Theano},
	url = {https://conference.scipy.org/proceedings/scipy2010/bergstra.html},
	doi = {10.25080/Majora-92bf1922-003},
	abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
	language = {en},
	urldate = {2023-02-21},
	author = {Bergstra, James and Breuleux, Olivier and Bastien, Frédéric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
	year = {2010},
	pages = {18--24},
}

@inproceedings{shajii_codon_2023,
	address = {New York, NY, USA},
	series = {{CC} 2023},
	title = {Codon: {A} {Compiler} for {High}-{Performance} {Pythonic} {Applications} and {DSLs}},
	isbn = {9798400700880},
	shorttitle = {Codon},
	url = {https://doi.org/10.1145/3578360.3580275},
	doi = {10.1145/3578360.3580275},
	abstract = {Domain-specific languages (DSLs) are able to provide intuitive high-level abstractions that are easy to work with while attaining better performance than general-purpose languages. Yet, implementing new DSLs is a burdensome task. As a result, new DSLs are usually embedded in general-purpose languages. While low-level languages like C or C++ often provide better performance as a host than high-level languages like Python, high-level languages are becoming more prevalent in many domains due to their ease and flexibility. Here, we present Codon, a domain-extensible compiler and DSL framework for high-performance DSLs with Python's syntax and semantics. Codon builds on previous work on ahead-of-time type checking and compilation of Python programs and leverages a novel intermediate representation to easily incorporate domain-specific optimizations and analyses. We showcase and evaluate several compiler extensions and DSLs for Codon targeting various domains, including bioinformatics, secure multi-party computation, block-based data compression and parallel programming, showing that Codon DSLs can provide benefits of familiar high-level languages and achieve performance typically only seen with low-level languages, thus bridging the gap between performance and usability.},
	urldate = {2023-02-20},
	booktitle = {Proceedings of the 32nd {ACM} {SIGPLAN} {International} {Conference} on {Compiler} {Construction}},
	publisher = {Association for Computing Machinery},
	author = {Shajii, Ariya and Ramirez, Gabriel and Smajlović, Haris and Ray, Jessica and Berger, Bonnie and Amarasinghe, Saman and Numanagić, Ibrahim},
	month = feb,
	year = {2023},
	keywords = {NSF Large, Python, domain-specific languages, intermediate representation, optimization, type checking},
	pages = {191--202},
}

@inproceedings{zhuang:macket,
	title = {A {Tool} for {Mutation} {Analysis} in {Racket}},
	booktitle = {International {Workshop} on {Mutation} {Analysis} ({Mutation})},
	author = {Zhuang, Bambi and Perretta, James and Guha, Arjun and Bell, Jonathan},
	year = {2023},
}

@inproceedings{perretta:student-mutants,
	title = {On the use of mutation analysis for evaluating student test suite quality},
	isbn = {978-1-4503-9379-9},
	doi = {10.1145/3533767.3534217},
	urldate = {2022-12-09},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Perretta, James and DeOrio, Andrew and Guha, Arjun and Bell, Jonathan},
	keywords = {mutation analysis, software faults, software testing},
	pages = {265--275},
}

@article{phipps-costin:typewhich,
	title = {Solver-based {Gradual} {Type} {Migration}},
	volume = {5},
	doi = {https://doi.org/10.1145/3485488},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Phipps-Costin, Luna and Anderson, Carolyn Jane and Greenberg, Michael and Guha, Arjun},
	year = {2021},
}

@inproceedings{pinckney:maxnpm,
	title = {Flexible and {Optimal} {Dependency} {Management} via {Max}-{SMT}},
	urldate = {2022-12-09},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Pinckney, Donald and Cassano, Federico and Guha, Arjun and Bell, Jonathan and Culpo, Massimiliano and Gamblin, Todd},
	year = {2023},
}

@inproceedings{pinckney:wasmk,
	title = {Wasm/k: {Delimited} {Continuations} for {WebAssembly}},
	doi = {10.1145/3426422.3426978},
	booktitle = {Dynamic {Languages} {Symposium} ({DLS})},
	author = {Pinckney, Donald and Brun, Yuriy and Guha, Arjun},
	year = {2020},
}

@inproceedings{jangda:nextdoor,
	title = {Accelerating {Graph} {Sampling} for {Graph} {Machine} {Learning} {Using} {GPUs}},
	doi = {10.1145/3447786.3456244},
	booktitle = {European {Conference} on {Computer} {Systems} ({EuroSys})},
	author = {Jangda, Abhinav and Polisetty, Sandeep and Guha, Arjun and Serafini, Marco},
	year = {2021},
}

@misc{wang_recode_2022,
	title = {{ReCode}: {Robustness} {Evaluation} of {Code} {Generation} {Models}},
	shorttitle = {{ReCode}},
	url = {http://arxiv.org/abs/2212.10264},
	abstract = {Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model's robustness performance. With human annotators, we verified that over 90\% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.},
	urldate = {2023-01-29},
	publisher = {arXiv},
	author = {Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and Nallapati, Ramesh and Ramanathan, Murali Krishna and Roth, Dan and Xiang, Bing},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10264 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Software Engineering, To Read},
}

@misc{dong_codescore_2023,
	title = {{CodeScore}: {Evaluating} {Code} {Generation} by {Learning} {Code} {Execution}},
	shorttitle = {{CodeScore}},
	url = {http://arxiv.org/abs/2301.09043},
	abstract = {A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing CEMs can be categorized into match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) and execution-based CEMs (e.g., AvgPassRatio and Pass@k), but both of them suffer from some issues. The former only measures differences in surface form regardless of the functional equivalence of codes, while the latter has huge execution overheads, including collecting expensive test cases, resolving tedious execution dependencies, and enormous execution time. To address these issues, in this paper, we propose CodeScore, an efficient and effective CEM for code generation, which estimates test case PassRatio of generated code without executing code. We also present a framework named UniCE for training unified code evaluation models by learning code execution, i.e., learning PassRatio and Executability of generated code. In order to learn code execution comprehensively, we construct more than 100 test cases for each task in several popular benchmark datasets, covering MBPP, APPS, and HumanEval. Experimental results show that CodeScore has obtained a state-of-the-art correlation with execution-based CEMs. CodeScore is strongly correlated with AvgPassPatio, and binary CodeScore is moderately correlated with Pass@1. In particular, CodeScore eliminates the need for test cases and execution dependencies in inference, and CodeScore reduces execution time by three orders of magnitude compared to AvgPassPatio and Pass@1.},
	urldate = {2023-01-29},
	publisher = {arXiv},
	author = {Dong, Yihong and Ding, Jiazheng and Jiang, Xue and Li, Zhuo and Li, Ge and Jin, Zhi},
	month = jan,
	year = {2023},
	note = {arXiv:2301.09043 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{aghakhani_trojanpuzzle_2023,
	title = {{TrojanPuzzle}: {Covertly} {Poisoning} {Code}-{Suggestion} {Models}},
	shorttitle = {{TrojanPuzzle}},
	url = {https://arxiv.org/abs/2301.02344v1},
	doi = {10.48550/arXiv.2301.02344},
	abstract = {With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training or fine-tuning phases by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior poisoning attacks explicitly inject the insecure code payload into the training data, making the poisoning data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel data poisoning attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poisoning data by never including certain (suspicious) parts of the payload in the poisoned data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based dataset-cleansing methods that identify and filter out suspicious sequences from the training data. Our evaluation against two model sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for how practitioners should select code used to train or tune code-suggestion models.},
	language = {en},
	urldate = {2023-01-29},
	author = {Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert},
	month = jan,
	year = {2023},
	keywords = {To Read},
}

@article{demchak:china-hijacks,
	title = {China’s {Maxim} – {Leave} {No} {Access} {Point} {Unexploited}: {The} {Hidden} {Story} of {China} {Telecom}’s {BGP} {Hijacking}},
	volume = {3},
	issn = {2378-0789},
	shorttitle = {China’s {Maxim} – {Leave} {No} {Access} {Point} {Unexploited}},
	url = {http://scholarcommons.usf.edu/mca/vol3/iss1/7/},
	doi = {10.5038/2378-0789.3.1.1050},
	language = {en},
	number = {1},
	urldate = {2023-01-13},
	journal = {Military Cyber Affairs},
	author = {Demchak, Chris and {U.S. Naval War College} and Shavitt, Yuval and {Tel Aviv University}},
	month = jun,
	year = {2018},
}

@misc{goodin:russia-hijack-2017,
	title = {“{Suspicious}” event routes traffic for big-name sites through {Russia}},
	url = {https://arstechnica.com/information-technology/2017/12/suspicious-event-routes-traffic-for-big-name-sites-through-russia/},
	abstract = {Google, Facebook, Apple, and Microsoft all affected by “intentional” BGP mishap.},
	language = {en-us},
	urldate = {2023-01-13},
	journal = {Ars Technica},
	author = {Goodin, Dan},
	month = dec,
	year = {2017},
}

@misc{goodin:russia-hijack-2022,
	title = {Some {Twitter} traffic briefly funneled through {Russian} {ISP}, thanks to {BGP} mishap},
	url = {https://arstechnica.com/information-technology/2022/03/absence-of-malice-russian-isps-hijacking-of-twitter-ips-appears-to-be-a-goof/},
	abstract = {Despite the timing, the 45-minute hijacking was most likely an error, not an attack.},
	language = {en-us},
	urldate = {2023-01-13},
	journal = {Ars Technica},
	author = {Goodin, Dan},
	month = mar,
	year = {2022},
}

@article{singel:pakistan-youtube,
	title = {Pakistan's {Accidental} {YouTube} {Re}-{Routing} {Exposes} {Trust} {Flaw} in {Net}},
	issn = {1059-1028},
	url = {https://www.wired.com/2008/02/pakistans-accid/},
	abstract = {A Pakistan ISP that was ordered to censor YouTube accidentally managed to take down the video site around the world for several hours Sunday. The Pakistani government ordered ISPs to censor YouTube to prevent Pakistanis from seeing a trailer to an anti-Islamic film by Dutch politician Geert Wilders. YouTube has since removed the clip for violating its terms of service, but a screenshot […]},
	language = {en-US},
	urldate = {2023-01-13},
	journal = {Wired},
	author = {Singel, Ryan},
	note = {Section: tags},
	keywords = {glitches and bugs, sunshine and secrecy, threat level},
}

@inproceedings{yap:espresso,
	address = {Los Angeles CA USA},
	title = {Taking the {Edge} off with {Espresso}: {Scale}, {Reliability} and {Programmability} for {Global} {Internet} {Peering}},
	isbn = {978-1-4503-4653-5},
	shorttitle = {Taking the {Edge} off with {Espresso}},
	url = {https://dl.acm.org/doi/10.1145/3098822.3098854},
	doi = {10.1145/3098822.3098854},
	abstract = {We present the design of Espresso, Google’s SDN-based Internet peering edge routing infrastructure. This architecture grew out of a need to exponentially scale the Internet edge cost-effectively and to enable application-aware routing at Internet-peering scale. Espresso utilizes commodity switches and host-based routing/packet processing to implement a novel fine-grained traffic engineering capability. Overall, Espresso provides Google a scalable peering edge that is programmable, reliable, and integrated with global traffic systems. Espresso also greatly accelerated deployment of new networking features at our peering edge. Espresso has been in production for two years and serves over 22\% of Google’s total traffic to the Internet.},
	language = {en},
	urldate = {2023-01-10},
	booktitle = {Proceedings of the {Conference} of the {ACM} {Special} {Interest} {Group} on {Data} {Communication}},
	publisher = {ACM},
	author = {Yap, Kok-Kiong and Motiwala, Murtaza and Rahe, Jeremy and Padgett, Steve and Holliman, Matthew and Baldus, Gary and Hines, Marcus and Kim, Taeeun and Narayanan, Ashok and Jain, Ankur and Lin, Victor and Rice, Colin and Rogan, Brian and Singh, Arjun and Tanaka, Bert and Verma, Manish and Sood, Puneet and Tariq, Mukarram and Tierney, Matt and Trumic, Dzevad and Valancius, Vytautas and Ying, Calvin and Kallahalla, Mahesh and Koley, Bikash and Vahdat, Amin},
	month = aug,
	year = {2017},
	pages = {432--445},
}

@inproceedings{lantz:mininet,
	address = {Monterey California},
	title = {A network in a laptop: rapid prototyping for software-defined networks},
	isbn = {978-1-4503-0409-2},
	shorttitle = {A network in a laptop},
	url = {https://dl.acm.org/doi/10.1145/1868447.1868466},
	doi = {10.1145/1868447.1868466},
	abstract = {Mininet is a system for rapidly prototyping large networks on the constrained resources of a single laptop. The lightweight approach of using OS-level virtualization features, including processes and network namespaces, allows it to scale to hundreds of nodes. Experiences with our initial implementation suggest that the ability to run, poke, and debug in real time represents a qualitative change in workﬂow. We share supporting case studies culled from over 100 users, at 18 institutions, who have developed SoftwareDeﬁned Networks (SDN). Ultimately, we think the greatest value of Mininet will be supporting collaborative network research, by enabling self-contained SDN prototypes which anyone with a PC can download, run, evaluate, explore, tweak, and build upon.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the 9th {ACM} {SIGCOMM} {Workshop} on {Hot} {Topics} in {Networks}},
	publisher = {ACM},
	author = {Lantz, Bob and Heller, Brandon and McKeown, Nick},
	month = oct,
	year = {2010},
	pages = {1--6},
}

@article{schneider:nal,
	title = {Nexus {Authorization} {Logic} ({NAL}): {Design} {Rationale} and {Applications}},
	volume = {14},
	language = {en},
	number = {1},
	journal = {ACM Transactions on Information and System Security},
	author = {Schneider, Fred B and Walsh, Kevin and Sirer, Emin Gün},
	month = may,
	year = {2011},
	pages = {30},
}

@inproceedings{mcclurg:updates,
	address = {New York, NY, USA},
	series = {{PLDI} '15},
	title = {Efficient synthesis of network updates},
	isbn = {978-1-4503-3468-6},
	url = {https://doi.org/10.1145/2737924.2737980},
	doi = {10.1145/2737924.2737980},
	abstract = {Software-defined networking (SDN) is revolutionizing the networking industry, but current SDN programming platforms do not provide automated mechanisms for updating global configurations on the fly. Implementing updates by hand is challenging for SDN programmers because networks are distributed systems with hundreds or thousands of interacting nodes. Even if initial and final configurations are correct, naively updating individual nodes can lead to incorrect transient behaviors, including loops, black holes, and access control violations. This paper presents an approach for automatically synthesizing updates that are guaranteed to preserve specified properties. We formalize network updates as a distributed programming problem and develop a synthesis algorithm based on counterexample-guided search and incremental model checking. We describe a prototype implementation, and present results from experiments on real-world topologies and properties demonstrating that our tool scales to updates involving over one-thousand nodes.},
	urldate = {2023-01-09},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	publisher = {Association for Computing Machinery},
	author = {McClurg, Jedidiah and Hojjat, Hossein and Černý, Pavol and Foster, Nate},
	month = jun,
	year = {2015},
	keywords = {LTL, SDN, model checking, network updates, software-defined networking, synthesis, verification},
	pages = {196--207},
}

@inproceedings{katz-bassett:revtr,
	address = {USA},
	series = {{NSDI}'10},
	title = {Reverse traceroute},
	abstract = {Traceroute is the most widely used Internet diagnostic tool today. Network operators use it to help identify routing failures, poor performance, and router misconfigurations. Researchers use it to map the Internet, predict performance, geolocate routers, and classify the performance of ISPs. However, traceroute has a fundamental limitation that affects all these applications: it does not provide reverse path information. Although various public traceroute servers across the Internet provide some visibility, no general method exists for determining a reverse path from an arbitrary destination. In this paper, we address this longstanding limitation by building a reverse traceroute system. Our system provides the same information as traceroute, but for the reverse path, and it works in the same case as traceroute, when the user may lack control of the destination. We use a variety of measurement techniques to incrementally piece together the path from the destination back to the source. We deploy our system on PlanetLab and compare reverse traceroute paths with traceroutes issued from the destinations. In the median case our tool finds 87\% of the hops seen in a directly measured traceroute along the same path, versus only 38\% if one simply assumes the path is symmetric, a common fallback given the lack of available tools. We then illustrate how we can use our reverse traceroute system to study previously unmeasurable aspects of the Internet: we present a case study of how a content provider could use our tool to troubleshoot poor path performance, we uncover more than a thousand peer-to-peer AS links invisible to current topology mapping efforts, and we measure the latency of individual backbone links with average error under a millisecond.},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the 7th {USENIX} conference on {Networked} systems design and implementation},
	publisher = {USENIX Association},
	author = {Katz-Bassett, Ethan and Madhyastha, Harsha V. and Adhikari, Vijay Kumar and Scott, Colin and Sherry, Justine and Van Wesep, Peter and Anderson, Thomas and Krishnamurthy, Arvind},
	month = apr,
	year = {2010},
	pages = {15},
}

@inproceedings{vermeulen:revtr2,
	address = {New York, NY, USA},
	series = {{IMC} '22},
	title = {Internet scale reverse traceroute},
	isbn = {978-1-4503-9259-4},
	url = {https://doi.org/10.1145/3517745.3561422},
	doi = {10.1145/3517745.3561422},
	abstract = {Knowledge of Internet paths allows operators and researchers to better understand the Internet and troubleshoot problems. Paths are often asymmetric, so measuring just the forward path only gives partial visibility. Despite the existence of Reverse Traceroute, a technique that captures reverse paths (the sequence of routers traversed by traffic from an arbitrary, uncontrolled destination to a given source), this technique did not fulfill the needs of operators and the research community, as it had limited coverage, low throughput, and inconsistent accuracy. In this paper we design, implement and evaluate revtr 2.0, an Internet-scale Reverse Traceroute system that combines novel measurement approaches and studies with a large-scale deployment to improve throughput, accuracy, and coverage, enabling the first exploration of reverse paths at Internet scale. revtr 2.0 can run 15M reverse traceroutes in one day. This scale allows us to open the system to external sources and users, and supports tasks such as traffic engineering and troubleshooting.},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the 22nd {ACM} {Internet} {Measurement} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Vermeulen, Kevin and Gurmericliler, Ege and Cunha, Italo and Choffnes, David and Katz-Bassett, Ethan},
	month = oct,
	year = {2022},
	keywords = {internet measurements, internet scale, reverse traceroute},
	pages = {694--715},
}

@inproceedings{ferguson:orion,
	title = {Orion: {Google}’s {Software}-{Deﬁned} {Networking} {Control} {Plane}},
	abstract = {We present Orion, a distributed Software-Deﬁned Networking platform deployed globally in Google’s datacenter (Jupiter) and Wide Area (B4) networks. Orion was designed around a modular, micro-service architecture with a central publish-subscribe database to enable a distributed, yet tightlycoupled, software-deﬁned network control system. Orion enables intent-based management and control, is highly scalable and amenable to global control hierarchies. Over the years, Orion has matured with continuously improving performance in convergence (up to 40x faster), throughput (handling up to 1.16 million network updates per second), system scalability (supporting 16x larger networks), and data plane availability (50x, 100x reduction in unavailable time in Jupiter and B4, respectively) while maintaining high development velocity with bi-weekly release cadence. Today, Orion enables Google’s Software-Deﬁned Networks, defending against failure modes that are both generic to large scale production networks as well as unique to SDN systems.},
	booktitle = {{USENIX} {Symposium} on {Networked} {Systems} {Design} and {Implementation} ({NSDI})},
	author = {Ferguson, Andrew D and Gribble, Steve and Hong, Chi-Yao and Killian, Charles and Mohsin, Waqar and Muehe, Henrik and Ong, Joon and Poutievski, Leon and Singh, Arjun and Vicisano, Lorenzo and Alimi, Richard and Chen, Shawn Shuoshuo and Conley, Mike and Mandal, Subhasree and Nagaraj, Karthik and Bollineni, Kondapa Naidu and Sabaa, Amr and Zhang, Shidong and Zhu, Min and Vahdat, Amin},
	year = {2021},
}

@inproceedings{ferguson:pane,
	title = {Participatory {Networking}: {An} {API} for {Application} {Control} of {SDNs}},
	booktitle = {{ACM} {SIGCOMM} {Conference}},
	author = {Ferguson, Andrew D. and Guha, Arjun and Liang, Chen and Fonseca, Rodrigo and Krishnamurthi, Shriram},
	year = {2013},
}

@inproceedings{gupta:sdx,
	address = {Chicago Illinois USA},
	title = {{SDX}: a software defined internet exchange},
	isbn = {978-1-4503-2836-4},
	shorttitle = {{SDX}},
	url = {https://dl.acm.org/doi/10.1145/2619239.2626300},
	doi = {10.1145/2619239.2626300},
	abstract = {BGP severely constrains how networks can deliver trafﬁc over the Internet. Today’s networks can only forward trafﬁc based on the destination IP preﬁx, by selecting among routes offered by their immediate neighbors. We believe Software Deﬁned Networking (SDN) could revolutionize wide-area trafﬁc delivery, by offering direct control over packet-processing rules that match on multiple header ﬁelds and perform a variety of actions. Internet exchange points (IXPs) are a compelling place to start, given their central role in interconnecting many networks and their growing importance in bringing popular content closer to end users.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {{ACM} {SIGCOMM} {Conference}},
	publisher = {ACM},
	author = {Gupta, Arpit and Vanbever, Laurent and Shahbaz, Muhammad and Donovan, Sean P. and Schlinker, Brandon and Feamster, Nick and Rexford, Jennifer and Shenker, Scott and Clark, Russ and Katz-Bassett, Ethan},
	month = aug,
	year = {2014},
	pages = {551--562},
}

@article{liu:diste,
	title = {Distributed {Traffic} {Engineering} for {Multi}-{Domain} {SDN} {Without} {Trust}},
	volume = {10},
	issn = {2168-7161},
	doi = {10.1109/TCC.2021.3067456},
	abstract = {In software defined networking, the flat design of distributed control plane enables the management of multi-domain networks that are incapable of deploying a root controller. However, it is very difficult to avoid policy conflicts between independent local controllers due to the lack of centralized arbitration. Moreover, domains without trust may not be always cooperative and could even cheat to maximize their own interests. In this article, we first consider the cooperative scenario and address the problem of traffic engineering in a flat distributed control plane. We propose a fully distributed algorithm, called DisTE, which can provide max-min fair bandwidth allocation for flows and maximize resource utilization. DisTE also preserves the local topology of each domain and achieves policy consistency by multiple rounds of synchronization. We then consider the non-cooperative scenario, where selfish domains may discriminate bandwidth requests from other domains or overstate theirs owns to squeeze more bandwidths.},
	number = {4},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Liu, Yangyang and Zhao, Laiping and Hua, Jingyu and Qu, Wenyu and Zhang, Suohao and Zhong, Sheng},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Cloud Computing},
	keywords = {Bandwidth, Channel allocation, Network topology, Reliability, Resource management, Routing, SDN, Topology, Traffic engineering, distributed control, selfish domain},
	pages = {2481--2496},
}

@inproceedings{santos:dsdn,
	title = {Decentralizing {SDN}'s control plane},
	doi = {10.1109/LCN.2014.6925802},
	abstract = {Motivated by the internets of the future, which will likely be considerably larger in size as well as highly heterogeneous and decentralized, we propose Decentralize-SDN, D-SDN, a framework that enables not only physical- but also logical distribution of the Software-Defined Networking (SDN) control plane. D-SDN accomplishes network control distribution by defining a hierarchy of controllers that can “match” an internet's organizational- and administrative structure. By delegating control between main controllers and secondary controllers, D-SDN is able to accommodate administrative decentralization and autonomy.It incorporates security as an integral part of the framework. This paper describes D-SDN and presents two use cases, namely network capacity sharing and public safety network services.},
	booktitle = {39th {Annual} {IEEE} {Conference} on {Local} {Computer} {Networks}},
	author = {Santos, Mateus A. S. and Nunes, Bruno A. A. and Obraczka, Katia and Turletti, Thierry and de Oliveira, Bruno T. and Margi, Cintia B.},
	month = sep,
	year = {2014},
	note = {ISSN: 0742-1303},
	keywords = {Ad hoc networks, Fault tolerance, Fault tolerant systems, Internet, Logic gates, Mobile computing, Protocols},
	pages = {402--405},
}

@inproceedings{phemius:disco,
	title = {{DISCO}: {Distributed} multi-domain {SDN} controllers},
	shorttitle = {{DISCO}},
	doi = {10.1109/NOMS.2014.6838330},
	abstract = {Software-Defined Networking (SDN) is now envisioned for Wide Area Networks (WAN) and constrained overlay networks. Such networks require a resilient, scalable and easily extensible SDN control plane. In this paper, we propose DISCO, an extensible DIstributed SDN COntrol plane able to cope with the distributed and heterogeneous nature of modern overlay networks. A DISCO controller manages its own network domain and communicates with other controllers to provide end-to-end network services. This east-west communication is based on a lightweight and highly manageable control channel. We implemented DISCO on top of the Floodlight OpenFlow controller and the AMQP protocol and we evaluated it through an inter-domain topology disruption use case.},
	booktitle = {2014 {IEEE} {Network} {Operations} and {Management} {Symposium} ({NOMS})},
	author = {Phemius, Kévin and Bouet, Mathieu and Leguay, Jérémie},
	month = may,
	year = {2014},
	note = {ISSN: 2374-9709},
	keywords = {Bandwidth, Computer architecture, Decentralized control, Monitoring, Network topology, Protocols, Wide area networks},
	pages = {1--4},
}

@article{jain_b4_2013,
	title = {B4: experience with a globally-deployed software defined wan},
	volume = {43},
	issn = {0146-4833},
	shorttitle = {B4},
	url = {https://doi.org/10.1145/2534169.2486019},
	doi = {10.1145/2534169.2486019},
	abstract = {We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100\% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.},
	number = {4},
	urldate = {2023-01-09},
	journal = {ACM SIGCOMM Computer Communication Review},
	author = {Jain, Sushant and Kumar, Alok and Mandal, Subhasree and Ong, Joon and Poutievski, Leon and Singh, Arjun and Venkata, Subbaiah and Wanderer, Jim and Zhou, Junlan and Zhu, Min and Zolla, Jon and Hölzle, Urs and Stuart, Stephen and Vahdat, Amin},
	month = aug,
	year = {2013},
	keywords = {centralized traffic engineering, openflow, routing, software- defined networking, wide-area networks},
	pages = {3--14},
}

@article{clauset:communities,
	title = {Finding community structure in very large networks},
	volume = {70},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.70.066111},
	doi = {10.1103/PhysRevE.70.066111},
	number = {6},
	urldate = {2023-01-09},
	journal = {Physical Review E},
	author = {Clauset, Aaron and Newman, M. E. J. and Moore, Cristopher},
	month = dec,
	year = {2004},
	note = {Publisher: American Physical Society},
	pages = {066111},
}

@inproceedings{beckett:propane,
	title = {Don't {Mind} the {Gap}: {Bridging} {Network}-wide {Objectives} and {Device}-level {Configurations}},
	shorttitle = {Don't {Mind} the {Gap}},
	url = {https://dl.acm.org/doi/10.1145/2934872.2934909},
	doi = {10.1145/2934872.2934909},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {{ACM} {SIGCOMM} {Conference}},
	author = {Beckett, Ryan and Mahajan, Ratul and Millstein, Todd and Padhye, Jitendra and Walker, David},
	year = {2016},
}

@inproceedings{reitblatt:network-updates,
	title = {Abstractions for {Network} {Update}},
	doi = {10.1145/2377677.2377748},
	booktitle = {{ACM} {SIGCOMM} {Conference}},
	author = {Reitblatt, Mark and Foster, Nate and Rexford, Jennifer and Schlesinger, Cole and Walker, David},
	year = {2012},
}

@inproceedings{katta:cacheflow,
	title = {{CacheFlow}: {Dependency}-{Aware} {Rule}-{Caching} for {Software}-{Defined} {Networks}},
	doi = {10.1145/2890955.2890969},
	urldate = {2023-01-08},
	booktitle = {Proceedings of the {Symposium} on {SDN} {Research}},
	author = {Katta, Naga and Alipourfard, Omid and Rexford, Jennifer and Walker, David},
	year = {2016},
	keywords = {Commodity Switch, OpenFlow, Rule Caching, Software-Defined Networking, TCAM},
}

@techreport{onf:openflow-spec,
	title = {{OpenFlow} {Switch} {Specification} 1.5.1},
	institution = {Open Networking Foundation},
	note = {https://opennetworking.org/software-defined-standards/specifications/},
}

@inproceedings{guha:coqnet,
	title = {Machine {Verified} {Network} {Controllers}},
	doi = {10.1145/2491956.2462178},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	author = {Guha, Arjun and Reitblatt, Mark and Foster, Nate},
	year = {2013},
}

@misc{dohan_language_2022,
	title = {Language {Model} {Cascades}},
	url = {http://arxiv.org/abs/2207.10342},
	doi = {10.48550/arXiv.2207.10342},
	abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10342 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, To Read},
}

@misc{krishna_rankgen_2022,
	title = {{RankGen}: {Improving} {Text} {Generation} with {Large} {Ranking} {Models}},
	shorttitle = {{RankGen}},
	url = {http://arxiv.org/abs/2205.09726},
	doi = {10.48550/arXiv.2205.09726},
	abstract = {Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and (2) sequences generated from a large language model conditioned on the prefix. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling, as well as contrastive decoding and search, on both automatic metrics (85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English writers (74.5\% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We release our model checkpoints, code, and human preference data with explanations to facilitate future research.},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Krishna, Kalpesh and Chang, Yapei and Wieting, John and Iyyer, Mohit},
	month = nov,
	year = {2022},
	note = {arXiv:2205.09726 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, To Read},
}

@inproceedings{reitblatt:fattire,
	title = {{FatTire}: {Declarative} {Fault} {Tolerance} for {Software} {Defined} {Networks}.},
	copyright = {All rights reserved},
	booktitle = {{ACM} {SIGCOMM} {Workshop} on {Hot} {Topics} in {Software} {Defined} {Networking} ({HotSDN})},
	author = {Reitblatt, Mark and Canini, Marco and Guha, Arjun and Foster, Nate},
	year = {2013},
}

@inproceedings{weiss:tortoise,
	title = {Automated {System} {Configuration} {Repair}},
	copyright = {All rights reserved},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Weiss, Aaron and Brun, Yuriy and Guha, Arjun},
	year = {2017},
}

@patent{swamy:ibex-patent,
	title = {Programming, verifying, visualizing, and deploying browser extensions with fine-grained security policies},
	copyright = {All rights reserved},
	author = {Swamy, Nikhil and Livshits, Benjamin and Guha, Arjun and Fredrikson, Matthew},
	year = {2015},
}

@inproceedings{spitzer:robojs,
	title = {Making {High}-{Performance} {Robots} {Safe} and {Easy} to {Use} for an {Introduction} to {Computing}},
	copyright = {All rights reserved},
	url = {https://arxiv.org/abs/1909.03110},
	booktitle = {Educational {Advances} in {Artificial} {Intelligence} ({EAAI})},
	author = {Spitzer, Joseph and Biswas, Joydeep and Guha, Arjun},
	year = {2020},
}

@inproceedings{smolka:fastnetkat,
	title = {A {Fast} {Compiler} for {NetKAT}},
	copyright = {All rights reserved},
	booktitle = {{ACM} {International} {Conference} on {Functional} {Programming} ({ICFP})},
	author = {Smolka, Steffen and Eliopoulos, Spiridon Aristides and Foster, Nate and Guha, Arjun},
	year = {2015},
}

@inproceedings{saur:morpheus,
	title = {Morpheus: {Safe} and {Flexible} {Controller} {Upgrades} for {SDNs}},
	copyright = {All rights reserved},
	booktitle = {Symposium on {SDN} {Research} ({SOSR})},
	author = {Saur, Karla and Collard, Joseph and Foster, Nate and Guha, Arjun and Vanbever, Laurent and Hicks, Michael},
	year = {2016},
}

@article{politz:adsafety-journal,
	title = {Type-based {Verification} of {JavaScript} {Sandboxing}},
	volume = {22},
	copyright = {All rights reserved},
	number = {4},
	journal = {Journal of Computer Security},
	author = {Politz, Joe Gibbs and Guha, Arjun and Krishnamurthi, Shriram},
	year = {2014},
	pages = {511--565},
}

@inproceedings{nelson:balance-of-power,
	title = {A {Balance} of {Power}: {Expressive}, {Analyzable} {Controller} {Programming}},
	booktitle = {{ACM} {SIGCOMM} {Workshop} on {Hot} {Topics} in {Software} {Defined} {Networking} ({HotSDN})},
	author = {Nelson, Tim and Guha, Arjun and Dougherty, Daniel J. and Fisler, Kathi and Krishnamurthi, Shriram},
	year = {2013},
}

@inproceedings{jangda:warp-tiling,
	title = {Model-{Based} {Warp}-{Level} {Tiling} for {Image} {Processing} {Programs} on {GPUs}},
	copyright = {All rights reserved},
	booktitle = {International {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques} ({PACT})},
	author = {Jangda, Abhinav and Guha, Arjun},
	year = {2020},
}

@inproceedings{jangda:not-so-fast,
	title = {Not {So} {Fast}: {Analyzing} the {Performance} of {WebAssembly} vs. {Native} {Code}},
	copyright = {All rights reserved},
	booktitle = {{USENIX} {Annual} {Technical} {Conference} ({ATC})},
	author = {Jangda, Abhinav and Powers, Bobby and Berger, Emery and Guha, Arjun},
	year = {2019},
}

@article{jangda:lambda-lambda,
	title = {Formal {Foundations} of {Serverless} {Computing}},
	volume = {3},
	copyright = {All rights reserved},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Jangda, Abhinav and Pinckney, Donald and Brun, Yuriy and Guha, Arjun},
	year = {2019},
}

@inproceedings{holtz:srtr-demo,
	title = {Demo: {Interactive} {Robot} {Transition} {Repair}},
	copyright = {All rights reserved},
	booktitle = {International {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} ({AAMAS})},
	author = {Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep},
	year = {2018},
}

@inproceedings{holtz:srtr,
	title = {Interactive {Robot} {Transition} {Repair} with {SMT}},
	copyright = {All rights reserved},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence} and the {European} {Conference} on {Artificial} {Intelligence} ({IJCAI}-{ECAI})},
	author = {Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep},
	year = {2018},
}

@inproceedings{holtz:ldips,
	title = {Robot {Action} {Selection} {Learning} via {Layered} {Dimension} {Informed} {Program} {Synthesis}},
	copyright = {All rights reserved},
	booktitle = {Conference on {Robot} {Learning} ({CoRL})},
	author = {Holtz, Jarrett and Guha, Arjun and Biswas, Joydeep},
	year = {2020},
}

@inproceedings{holtz:idips-workshop,
	title = {Iterative {Program} {Synthesis} for {Adaptable} {Social} {Navigation}},
	copyright = {All rights reserved},
	booktitle = {Robotics: {Science} and {Systems} {Workshop} on {Social} {Navigation}},
	author = {Holtz, Jarrett and Andrews, Simon and Guha, Arjun and Biswas, Joydeep},
	year = {2021},
}

@inproceedings{guha:web-api-verif,
	title = {Web {API} {Verification}: {Results} and {Challenges}},
	copyright = {All rights reserved},
	booktitle = {Analysis of {Security} {APIs} ({ASA})},
	author = {Guha, Arjun and Lerner, Benjamin S. and Politz, Joe Gibbs and Krishnamurthi, Shriram},
	year = {2012},
}

@inproceedings{guha:semantic-gap,
	title = {Minding the ({Semantic}) {Gap}: {Engineering} {Programming} {Language} {Theory}},
	copyright = {All rights reserved},
	booktitle = {Workshop on the {Future} of {Software} {Engineering} {Research} ({FoSER})},
	author = {Guha, Arjun and Krishnamurthi, Shriram},
	year = {2011},
}

@inproceedings{guha:fission,
	title = {Fission: {Secure} {Dynamic} {Code}-{Splitting} for {JavaScript}},
	copyright = {All rights reserved},
	booktitle = {Summit {oN} {Advances} in {Programming} {Languages} ({SNAPL})},
	author = {Guha, Arjun and Jeannin, Jean-Baptiste and Nigam, Rachit and Tangen, Jane and Shambaugh, Rian},
	year = {2017},
}

@inproceedings{guha:coqnets-ons,
	title = {Formal {Foundations} for {Software} {Defined} {Networks}},
	copyright = {All rights reserved},
	booktitle = {Open {Networking} {Summit} ({ONS}) {Research} {Track}},
	author = {Guha, Arjun and Reitblatt, Mark and Foster, Nate},
	year = {2013},
}

@inproceedings{frenetic-ocaml13,
	title = {The {Frenetic} {Network} {Controller}},
	copyright = {All rights reserved},
	booktitle = {The {OCaml} {Users} and {Developers} {Workshop}},
	author = {Foster, Nate and Guha, Arjun and {The Frenetic Contributors}},
	year = {2013},
}

@article{foster:sdn-langs,
	title = {Languages for {Software}-{Defined} {Networks}},
	volume = {51},
	copyright = {All rights reserved},
	number = {2},
	journal = {IEEE Communications Magazine},
	author = {Foster, Nate and Freedman, Michael J. and Guha, Arjun and Harrison, Rob and Katta, Naga Praveen and Monsanto, Christopher and Reich, Joshua and Reitblatt, Mark and Rexford, Jennifer and Schlesinger, Cole and Story, Alec and Walker, David},
	year = {2013},
	pages = {128--565},
}

@article{first:tactok,
	title = {{TacTok}: {Semantics}-{Aware} {Proof} {Synthesis}},
	volume = {4},
	copyright = {All rights reserved},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {First, Emily and Brun, Yuriy and Guha, Arjun},
	year = {2020},
}

@inproceedings{ferguson:pane-ons,
	title = {A {Northbound} {API} for {Sharing} {SDNs}},
	copyright = {All rights reserved},
	booktitle = {Open {Networking} {Summit} ({ONS}) {Research} {Track}},
	author = {Ferguson, Andrew D. and Guha, Arjun and Liang, Chen and Fonseca, Rodrigo and Krishnamurthi, Shriram},
	year = {2013},
}

@inproceedings{ferguson:pane-hotice,
	title = {Participatory {Networking}},
	copyright = {All rights reserved},
	booktitle = {{USENIX} {Workshop} on {Hot} {Topics} in {Management} of {Internet}, {Cloud}, and {Enterprise} {Networks} and {Services} ({HotICE})},
	author = {Ferguson, Andrew D. and Guha, Arjun and Place, Jordan and Fonseca, Rodrigo and Krishnamurthi, Shriram},
	year = {2012},
}

@inproceedings{ferguson:hft,
	title = {Hierarchical {Policies} for {Software} {Defined} {Networks}},
	copyright = {All rights reserved},
	booktitle = {{ACM} {SIGCOMM} {Workshop} on {Hot} {Topics} in {Software} {Defined} {Networking} ({HotSDN})},
	author = {Ferguson, Andrew D. and Guha, Arjun and Liang, Chen and Fonseca, Rodrigo and Krishnamurthi, Shriram},
	year = {2012},
}

@inproceedings{cooper:teachinggc,
	title = {Teaching {Garbage} {Collection} without {Implementing} {Compilers} or {Interpreters}},
	copyright = {All rights reserved},
	booktitle = {{ACM} {Technical} {Symposium} on {Computer} {Science} {Education} ({SIGCSE})},
	author = {Cooper, Gregory H. and Guha, Arjun and Krishnamurthi, Shriram and McCarthy, Jay and Findler, Robert Bruce},
	year = {2013},
}

@article{casado:sdn-abstractions-cacm,
	title = {Abstractions for {Software}-{Defined} {Networks}},
	copyright = {All rights reserved},
	journal = {Communications of the ACM (CACM)},
	author = {Casado, Martìn and Foster, Nate and Guha, Arjun},
	year = {2014},
}

@inproceedings{burroni:debugging-ppl,
	title = {Interactive {Writing} and {Debugging} of {Bayesian} {Probabilistic} {Programs}},
	copyright = {All rights reserved},
	booktitle = {Workshop on {Probabilistic} {Programming} {Languages}, {Semantics}, and {Systems} ({PPS})},
	author = {Burroni, Javier and Guha, Arjun and Jensen, David},
	year = {2018},
}

@inproceedings{baxter:stopify,
	title = {Putting in {All} the {Stops}: {Execution} {Control} for {JavaScript}},
	copyright = {All rights reserved},
	url = {https://arxiv.org/abs/1802.02974},
	doi = {10.1145/3192366.3192370},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	author = {Baxter, Samuel and Nigam, Rachit and Politz, Joe Gibbs and Krishnamurthi, Shriram and Guha, Arjun},
	month = jun,
	year = {2018},
	pages = {30 -- 45},
}

@article{aldrich:model-robots,
	title = {Model-{Based} {Adaptation} for {Robotics} {Software}},
	volume = {36},
	copyright = {All rights reserved},
	number = {2},
	journal = {IEEE Software},
	author = {Aldrich, Jonathan and Garlan, David and Kaestner, Christian and Goues, Claire Le and Mohseni-Kabir, Anahita and Ruchkin, Ivan and Samuel, Selva and Schmerl, Bradley and Timperley, Christopher Steven and Veloso, Manuela and Voysey, Ian and Biswas, Joydeep and Guha, Arjun and Holtz, Jarrett and Camara, Javier and Jamshidi, Pooyan},
	year = {2019},
	pages = {83--90},
}

@inproceedings{anderson:netkat,
	title = {{NetKAT}: {Semantic} {Foundations} for {Networks}},
	copyright = {All rights reserved},
	url = {https://dl.acm.org/doi/10.1145/2578855.2535862},
	doi = {10.1145/2578855.2535862},
	abstract = {Recent years have seen growing interest in high-level languages for programming networks. But the design of these languages has been largely ad hoc, driven more by the needs of applications and the capabilities of network hardware than by foundational principles. The lack of a semantic foundation has left language designers with little guidance in determining how to incorporate new features, and programmers without a means to reason precisely about their code.

This paper presents NetKAT, a new network programming language that is based on a solid mathematical foundation and comes equipped with a sound and complete equational theory. We describe the design of NetKAT, including primitives for filtering, modifying, and transmitting packets; union and sequential composition operators; and a Kleene star operator that iterates programs. We show that NetKAT is an instance of a canonical and well-studied mathematical structure called a Kleene algebra with tests (KAT) and prove that its equational theory is sound and complete with respect to its denotational semantics. Finally, we present practical applications of the equational theory including syntactic techniques for checking reachability, proving non-interference properties that ensure isolation between programs, and establishing the correctness of compilation algorithms.},
	language = {en},
	booktitle = {{ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages} ({POPL})},
	publisher = {Association for Computing Machinery},
	author = {Anderson, Carolyn Jane and Foster, Nate and Guha, Arjun and Jeannin, Jean-Baptiste and Kozen, Dexter and Schlesinger, Cole and Walker, David},
	year = {2014},
}

@misc{bibaev_all_2022,
	title = {All {You} {Need} {Is} {Logs}: {Improving} {Code} {Completion} by {Learning} from {Anonymous} {IDE} {Usage} {Logs}},
	shorttitle = {All {You} {Need} {Is} {Logs}},
	url = {http://arxiv.org/abs/2205.10692},
	doi = {10.48550/arXiv.2205.10692},
	abstract = {Integrated Development Environments (IDE) are designed to make users more productive, as well as to make their work more comfortable. To achieve this, a lot of diverse tools are embedded into IDEs, and the developers of IDEs can employ anonymous usage logs to collect the data about how they are being used to improve them. A particularly important component that this can be applied to is code completion, since improving code completion using statistical learning techniques is a well-established research area. In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832. The approach adheres to privacy requirements and legal constraints, since it does not require collecting personal information, performing all the necessary anonymization on the client's side. Importantly, it can be improved continuously: implementing new features, collecting new data, and evaluating new models - this way, we have been using it in production since the end of 2020.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Bibaev, Vitaliy and Kalina, Alexey and Lomshakov, Vadim and Golubev, Yaroslav and Bezzubov, Alexander and Povarov, Nikita and Bryksin, Timofey},
	month = may,
	year = {2022},
	note = {Number: arXiv:2205.10692
arXiv:2205.10692 [cs]},
}

@inproceedings{hellendoorn_when_2019,
	address = {Montreal, QC, Canada},
	title = {When {Code} {Completion} {Fails}: {A} {Case} {Study} on {Real}-{World} {Completions}},
	isbn = {978-1-72810-869-8},
	shorttitle = {When {Code} {Completion} {Fails}},
	url = {https://ieeexplore.ieee.org/document/8812116/},
	doi = {10.1109/ICSE.2019.00101},
	abstract = {Code completion is commonly used by software developers and is integrated into all major IDE’s. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientiﬁc understanding of developer needs and of the efﬁcacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artiﬁcial completions to inform future research and tools in this area. We ﬁnd that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data. Worse, on the few completions that consumed most of the developers’ time, prediction accuracy was less than 20\% – an effect that is invisible in synthetic benchmarks. Our ﬁndings have ramiﬁcations for future benchmarks, tool design and real-world efﬁcacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artiﬁcial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249].},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Hellendoorn, Vincent J. and Proksch, Sebastian and Gall, Harald C. and Bacchelli, Alberto},
	month = may,
	year = {2019},
	pages = {960--970},
}

@inproceedings{jesse:typebert,
	address = {Athens Greece},
	title = {Learning type annotation: is big data enough?},
	isbn = {978-1-4503-8562-6},
	shorttitle = {Learning type annotation},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473135},
	doi = {10.1145/3468264.3473135},
	abstract = {TypeScript is a widely used optionally-typed language where developers can adopt “pay as you go” typing: they can add types as desired, and benefit from static typing. The “type annotation tax” or manual effort required to annotate new or existing TypeScript can be reduced by a variety of automatic methods. Probabilistic machine-learning (ML) approaches work quite well. ML approaches use different inductive biases, ranging from simple token sequences to complex graphical neural network (GNN) models capturing syntax and semantic relations. More sophisticated inductive biases are hand-engineered to exploit the formal nature of software. Rather than deploying fancy inductive biases for code, can we just use “big data” to learn natural patterns relevant to typing? We find evidence suggesting that this is the case. We present TypeBert, demonstrating that even with simple token-sequence inductive bias used in BERT-style models and enough data, type-annotation performance of the most sophisticated models can be surpassed.},
	language = {en},
	urldate = {2022-11-28},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Jesse, Kevin and Devanbu, Premkumar T. and Ahmed, Toufique},
	month = aug,
	year = {2021},
	pages = {1483--1486},
}

@article{jesse:diversetyper,
	title = {Learning {To} {Predict} {User}-{Defined} {Types}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9785755/},
	doi = {10.1109/TSE.2022.3178945},
	abstract = {TypeScript is a widely adopted gradual typed language where developers can optionally type variables, functions, parameters and more. Probabilistic type inference approaches with ML (machine learning) work well especially for commonly occurring types such as boolean, number, and string. TypeScript permits a wide range of types including developer defined class names and type interfaces. These developer defined types, termed user-defined types, can be written within the realm of language naming conventions. The set of user-defined types is boundless and existing bounded type guessing approaches are an imperfect solution. Existing works either under perform in user-defined types or ignore user-defined types altogether. This work leverages a BERT-style pre-trained model, with multi-task learning objectives, to learn how to type user-defined classes and interfaces. Thus we present DIVERSETYPER, a solution that explores the diverse set of user-defined types by uniquely aligning classes and interfaces declarations to the places in which they are used.},
	language = {en},
	urldate = {2022-11-28},
	journal = {IEEE Transactions on Software Engineering},
	author = {Jesse, Kevin and Devanbu, Premkumar and Sawant, Anand Ashok},
	year = {2022},
	pages = {1--1},
}

@misc{meng_mass-editing_2022,
	title = {Mass-{Editing} {Memory} in a {Transformer}},
	url = {http://arxiv.org/abs/2210.07229},
	abstract = {Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07229 [cs]},
	keywords = {To Read},
}

@misc{shin_few-shot_2022,
	title = {Few-{Shot} {Semantic} {Parsing} with {Language} {Models} {Trained} {On} {Code}},
	url = {http://arxiv.org/abs/2112.08696},
	doi = {10.48550/arXiv.2112.08696},
	abstract = {Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Shin, Richard and Van Durme, Benjamin},
	month = may,
	year = {2022},
	note = {arXiv:2112.08696 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{madaan_language_2022,
	title = {Language {Models} of {Code} are {Few}-{Shot} {Commonsense} {Learners}},
	url = {http://arxiv.org/abs/2210.07128},
	abstract = {We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches “serialize” the output graph as a ﬂat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are ﬁne-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the fewshot setting. Our code and data are available at https://github.com/madaan/ CoCoGen .},
	language = {en},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07128 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{cheng_binding_2022,
	title = {Binding {Language} {Models} in {Symbolic} {Languages}},
	url = {http://arxiv.org/abs/2210.02875},
	doi = {10.48550/arXiv.2210.02875},
	abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02875 [cs]},
}

@inproceedings{an:rubydust,
	title = {Dynamic {Inference} of {Static} {Types} for {Ruby}},
	booktitle = {popl},
	author = {An, Jong-hoon David and Chauduri, Avik and Foster, Jeffrey S. and Hicks, Michael},
	year = {2011},
}

@inproceedings{furr:pruby,
	title = {Profile-{Guilding} {Static} {Typing} for {Dynamic} {Scripting} {Languages}},
	booktitle = {oopsla},
	author = {Furr, Michael and An, Jong-hoon David and Foster, Jeffrey S.},
	year = {2009},
}

@inproceedings{furr:druby,
	address = {Honolulu, Hawaii},
	title = {Static type inference for {Ruby}},
	isbn = {978-1-60558-166-8},
	url = {http://portal.acm.org/citation.cfm?doid=1529282.1529700},
	doi = {10.1145/1529282.1529700},
	abstract = {Many general-purpose, object-oriented scripting languages are dynamically typed, which provides ﬂexibility but leaves the programmer without the beneﬁts of static typing, including early error detection and the documentation provided by type annotations. This paper describes Diamondback Ruby (DRuby), a tool that blends Ruby’s dynamic type system with a static typing discipline. DRuby provides a type language that is rich enough to precisely type Ruby code we have encountered, without unneeded complexity. When possible, DRuby infers static types to discover type errors in Ruby programs. When necessary, the programmer can provide DRuby with annotations that assign static types to dynamic code. These annotations are checked at run time, isolating type errors to unveriﬁed code. We applied DRuby to a suite of benchmarks and found several bugs that would cause run-time type errors. DRuby also reported a number of warnings that reveal questionable programming practices in the benchmarks. We believe that DRuby takes a major step toward bringing the beneﬁts of combined static and dynamic typing to Ruby and other object-oriented languages.},
	language = {en},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the 2009 {ACM} symposium on {Applied} {Computing} - {SAC} '09},
	publisher = {ACM Press},
	author = {Furr, Michael and An, Jong-hoon (David) and Foster, Jeffrey S. and Hicks, Michael},
	year = {2009},
	pages = {1859},
}

@mastersthesis{saftoiu:jstrace,
	title = {{JSTrace}: {Run}-time type discovery for {JavaScript}},
	school = {Brown University},
	author = {Saftoiu, Claudiu},
	year = {2010},
}

@phdthesis{flanagan:thesis,
	type = {{PhD} {Thesis}},
	title = {Effective {Static} {Debugging} via {Componential} {Set}-based {Analysis}},
	school = {Rice University},
	author = {Flanagan, Cormac},
	year = {1997},
}

@inproceedings{vekris:two-phase-typing,
	title = {Trust, but {Verify}: {Two}-{Phase} {Typing} for {Dynamic} {Languages}},
	booktitle = {ecoop},
	author = {Vekris, Panagiotis and Cosman, Benjamin and Jhala, Ranjit},
	year = {2015},
}

@inproceedings{chugh:systemd,
	title = {Dependent {Types} for {JavaScript}},
	booktitle = {oopsla},
	author = {Chugh, Ravi and Herman, David and Jhala, Ranjit},
	year = {2012},
}

@inproceedings{chandra:static-js,
	title = {Type inference for static compilation of {JavaScript}},
	booktitle = {oopsla},
	author = {Chandra, Satish and Gordon, Colin S. and Jeannin, Jean-Baptiste and Schlesinger, Cole and Sridharan, Manu and Tip, Frank and Choi, Young-Il},
	year = {2016},
}

@inproceedings{anderson:inference,
	title = {Towards {Type} {Inference} for {JavaScript}},
	booktitle = {ecoop},
	author = {Anderson, Christopher and Giannini, Paola and Drossopoulou, Sophia},
	year = {2005},
}

@article{castagna:perspective,
	title = {Gradual {Typing}: {A} {New} {Perspective}},
	volume = {3},
	number = {POPL},
	journal = {pacmpl},
	author = {Castagna, Giuseppe and Lanvin, Victor and Petrucciani, Tommaso and Siek, Jeremy G.},
	year = {2019},
}

@article{miyazaki:dti,
	title = {Dynamic {Type} {Inference} for {Gradual} {Hindley}–{Milner} {Typing}},
	volume = {3},
	number = {POPL},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Miyazaki, Yusuke and Sekiyama, Taro and Igarashi, Atsushi},
	year = {2019},
}

@inproceedings{garcia:principal-gradual-types,
	title = {Principal {Type} {Schemes} for {Gradual} {Programs}},
	booktitle = {popl},
	author = {Garcia, Ronald and Cimini, Matteo},
	year = {2015},
}

@inproceedings{kazerounian:inferdl,
	title = {Sound, {Heuristic} {Type} {Annotation} {Inference} for {Ruby}},
	booktitle = {dls},
	author = {Kazerounian, Milod and Ren, Brianna M. and Foster, Jeffrey S.},
	year = {2020},
}

@article{kazerounian:simtyper,
	title = {{SimTyper}: sound type inference for {Ruby} using type equality prediction},
	volume = {5},
	issn = {2475-1421},
	shorttitle = {{SimTyper}},
	url = {https://dl.acm.org/doi/10.1145/3485483},
	doi = {10.1145/3485483},
	abstract = {Many researchers have explored type inference for dynamic languages. However, traditional type inference computes most general types which, for complex type systems—which are often needed to type dynamic languages—can be verbose, complex, and difficult to understand. In this paper, we introduce SimTyper, a Ruby type inference system that aims to infer usable types—specifically, nominal and generic types—that match the types programmers write. SimTyper builds on InferDL, a recent Ruby type inference system that soundly combines standard type inference with heuristics. The key novelty of SimTyper is type equality prediction, a new, machine learning-based technique that predicts when method arguments or returns are likely to have the same type. SimTyper finds pairs of positions that are predicted to have the same type yet one has a verbose, overly general solution and the other has a usable solution. It then guesses the two types are equal, keeping the guess if it is consistent with the rest of the program, and discarding it if not. In this way, types inferred by SimTyper are guaranteed to be sound. To perform type equality prediction, we introduce the deep similarity (DeepSim) neural network. DeepSim is a novel machine learning classifier that follows the Siamese network architecture and uses CodeBERT, a pre-trained model, to embed source tokens into vectors that capture tokens and their contexts. DeepSim is trained on 100,000 pairs labeled with type similarity information extracted from 371 Ruby programs with manually documented, but not checked, types. We evaluated SimTyper on eight Ruby programs and found that, compared to standard type inference, SimTyper finds 69\% more types that match programmer-written type information. Moreover, DeepSim can predict rare types that appear neither in the Ruby standard library nor in the training data. Our results show that type equality prediction can help type inference systems effectively produce more usable types. CCS Concepts: • Software and its engineering → Data types and structures.},
	language = {en},
	number = {OOPSLA},
	urldate = {2022-11-07},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Kazerounian, Milod and Foster, Jeffrey S. and Min, Bonan},
	month = oct,
	year = {2021},
	pages = {1--27},
}

@inproceedings{guria_rbsyn_2021,
	address = {Virtual Canada},
	title = {{RbSyn}: type- and effect-guided program synthesis},
	isbn = {978-1-4503-8391-2},
	shorttitle = {{RbSyn}},
	url = {https://dl.acm.org/doi/10.1145/3453483.3454048},
	doi = {10.1145/3453483.3454048},
	abstract = {In recent years, researchers have explored component-based synthesis, which aims to automatically construct programs that operate by composing calls to existing APIs. However, prior work has not considered efficient synthesis of methods with side effects, e.g., web app methods that update a database. In this paper, we introduce RbSyn, a novel typeand effect-guided synthesis tool for Ruby. An RbSyn synthesis goal is specified as the type for the target method and a series of test cases it must pass. RbSyn works by recursively generating well-typed candidate method bodies whose write effects match the read effects of the test case assertions. After finding a set of candidates that separately satisfy each test, RbSyn synthesizes a solution that branches to execute the correct candidate code under the appropriate conditions. We formalize RbSyn on a core, object-oriented language 𝜆𝑠𝑦𝑛 and describe how the key ideas of the model are scaled-up in our implementation for Ruby. We evaluated RbSyn on 19 benchmarks, 12 of which come from popular, open-source Ruby apps. We found that RbSyn synthesizes correct solutions for all benchmarks, with 15 benchmarks synthesizing in under 9 seconds, while the slowest benchmark takes 83 seconds. Using observed reads to guide synthesize is effective: using type-guidance alone times out on 10 of 12 app benchmarks. We also found that using less precise effect annotations leads to worse synthesis performance. In summary, we believe type- and effect-guided synthesis is an important step forward in synthesis of effectful methods from test cases.},
	language = {en},
	urldate = {2022-11-07},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Guria, Sankha Narayan and Foster, Jeffrey S. and Van Horn, David},
	month = jun,
	year = {2021},
	pages = {344--358},
}

@inproceedings{nguyen_empirical_2022,
	title = {An {Empirical} {Evaluation} of {GitHub} {Copilot}'s {Code} {Suggestions}},
	doi = {10.1145/3524842.3528470},
	abstract = {GitHub and OpenAI recently launched Copilot, an “AI pair programmer” that utilizes the power of Natural Language Processing, Static Analysis, Code Synthesis, and Artificial Intelligence. Given a natural language description of the target functionality, Copilot can generate corresponding code in several programming languages. In this paper, we perform an empirical study to evaluate the correctness and understandability of Copilot's suggested code. We use 33 LeetCode questions to create queries for Copilot in four different programming languages. We evaluate the correctness of the corresponding 132 Copilot solutions by running LeetCode's provided tests, and evaluate understandability using SonarQube's cyclomatic complexity and cognitive complexity metrics. We find that Copilot's Java suggestions have the highest correctness score (57\%) while JavaScript is the lowest (27\%). Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages. We also find some potential Copilot shortcomings, such as generating code that can be further simplified and code that relies on undefined helper methods.},
	booktitle = {2022 {IEEE}/{ACM} 19th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Nguyen, Nhan and Nadi, Sarah},
	month = may,
	year = {2022},
	note = {ISSN: 2574-3864},
	keywords = {Codes, Codex, Complexity theory, Empirical Evaluation, GitHub Copilot, Java, Measurement, Natural language processing, Program Synthesis, Software, Static analysis},
	pages = {1--5},
}

@misc{zhu_xlcost_2022,
	title = {{XLCoST}: {A} {Benchmark} {Dataset} for {Cross}-lingual {Code} {Intelligence}},
	shorttitle = {{XLCoST}},
	url = {http://arxiv.org/abs/2206.08474},
	abstract = {Recent advances in machine learning have significantly improved the understanding of source code data and achieved good performance on a number of downstream tasks. Open source repositories like GitHub enable this process with rich unlabeled code data. However, the lack of high quality labeled data has largely hindered the progress of several code related tasks, such as program translation, summarization, synthesis, and code search. This paper introduces XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for cross-lingual code intelligence. Our dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-lingual code tasks. To the best of our knowledge, it is the largest parallel dataset for source code both in terms of size and the number of languages. We also provide the performance of several state-of-the-art baseline models for each task. We believe this new dataset can be a valuable asset for the research community and facilitate the development and validation of new methods for cross-lingual code intelligence.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Zhu, Ming and Jain, Aneesh and Suresh, Karthik and Ravindran, Roshan and Tipirneni, Sindhu and Reddy, Chandan K.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08474 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{zan_cert_2022,
	title = {{CERT}: {Continual} {Pre}-{Training} on {Sketches} for {Library}-{Oriented} {Code} {Generation}},
	shorttitle = {{CERT}},
	url = {http://arxiv.org/abs/2206.06888},
	abstract = {Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large-scale unlabelled code corpora and perform well in code generation. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and the generator are continually pre-trained upon a base model using unlabelled data. Furthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate library-oriented code generation. Experimental results demonstrate the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67\% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
	month = jun,
	year = {2022},
	note = {arXiv:2206.06888 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{li_cctest_2022,
	title = {{CCTEST}: {Testing} and {Repairing} {Code} {Completion} {Systems}},
	shorttitle = {{CCTEST}},
	url = {http://arxiv.org/abs/2208.08289},
	abstract = {Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks like GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source codes. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion models often output suspicious results, and to date, an automated testing and enhancement framework for code completion models is not available. This research proposes CCTEST, a framework to test and repair code completion systems in blackbox settings. CCTEST features a novel mutation strategy, namely program structure-consistency (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing likely erroneous cases, from all the completed code cases. Moreover, CCTEST repairs the code completion outputs by selecting the output that mostly reflects the "average" appearance of all output cases, as the final output of the code completion systems. We detected a total of 33,540 inputs that can trigger likely erroneous cases from eight popular LLM-based code completion systems. With repairing, we show that the performance of code completion models notably increased by 53.51\% on average.},
	urldate = {2022-09-21},
	publisher = {arXiv},
	author = {Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Wang, Shuai and Gao, Cuiyun},
	month = aug,
	year = {2022},
	note = {arXiv:2208.08289 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{babeljs,
	title = {{BabelJS}},
	url = {https://babeljs.io/},
	urldate = {2022-04-24},
}

@article{jayagopal_study_nodate,
	title = {Study of {Program} {Synthesizers} \& {Novice} {Programmers}},
	language = {en},
	author = {Jayagopal, Dhanya},
	pages = {53},
}

@misc{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	shorttitle = {{LLM}.int8()},
	url = {http://arxiv.org/abs/2208.07339},
	doi = {10.48550/arXiv.2208.07339},
	abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs.},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	month = aug,
	year = {2022},
	note = {arXiv:2208.07339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, To Read},
}

@misc{pearce_examining_2022,
	title = {Examining {Zero}-{Shot} {Vulnerability} {Repair} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2112.02125},
	doi = {10.48550/arXiv.2112.02125},
	abstract = {Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, "off-the-shelf" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100\% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
	month = aug,
	year = {2022},
	note = {arXiv:2112.02125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, To Read},
}

@misc{evtikhiev_out_2022,
	title = {Out of the {BLEU}: how should we assess quality of the {Code} {Generation} models?},
	shorttitle = {Out of the {BLEU}},
	url = {http://arxiv.org/abs/2208.03133},
	abstract = {In recent years, researchers have created and introduced a significant number of various code generation models. As human evaluation of every new model version is unfeasible, the community adopted automatic evaluation metrics such as BLEU to approximate the results of human judgement. These metrics originate from the machine translation domain and it is unclear whether they are applicable for the code generation tasks and how well do they agree with the human evaluation on this task. There also are two metrics, CodeBLEU and RUBY, that were developed to estimate the similarity of code and take into account the code properties. However, for these metrics there are hardly any studies on their agreement with the human evaluation. Despite all that, minimal differences in the metric scores are used to claim superiority of some code generation models over the others. In this paper, we present a study on applicability of six metrics -- BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, RUBY -- for evaluation of the code generation models. We conduct a study on two different code generation datasets and use human annotators to assess the quality of all models run on these datasets. The results indicate that for the CoNaLa dataset of Python one-liners none of the metrics can correctly emulate human judgement on which model is better with \${\textgreater}95{\textbackslash}\%\$ certainty if the difference in model scores is less than 5 points. For the HearthStone dataset, which consists of classes of particular structure, the difference in model scores of at least 2 points is enough to claim the superiority of one model over the other. Using our findings, we derive several recommendations on using metrics to estimate the model performance on the code generation task.},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Evtikhiev, Mikhail and Bogomolov, Egor and Sokolov, Yaroslav and Bryksin, Timofey},
	month = aug,
	year = {2022},
	note = {arXiv:2208.03133 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, To Read},
}

@misc{reese:testing,
	title = {Best practices for writing unit tests - .{NET}},
	url = {https://docs.microsoft.com/en-us/dotnet/core/testing/unit-testing-best-practices},
	abstract = {Learn best practices for writing unit tests that drive code quality and resilience for .NET Core and .NET Standard projects.},
	language = {en-us},
	urldate = {2022-08-12},
	author = {Reese, John},
}

@inproceedings{alur:sygus,
	title = {Syntax-guided synthesis},
	booktitle = {Formal {Methods} in {Computer}-{Aided} {Design} ({FMCAD})},
	author = {Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo and Raghothaman, Mukund and Seshia, Sanjit A. and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
	year = {2013},
}

@article{chaudhuri:neurosymbolic,
	title = {Neurosymbolic {Programming}},
	volume = {7},
	issn = {2325-1107, 2325-1131},
	url = {http://www.nowpublishers.com/article/Details/PGL-049},
	doi = {10.1561/2500000049},
	abstract = {We survey recent work on neurosymbolic programming, an emerging area that bridges the areas of deep learning and program synthesis. Like in classic machine learning, the goal here is to learn functions from data. However, these functions are represented as programs that can use neural modules in addition to symbolic primitives and are induced using a combination of symbolic search and gradient-based optimization.},
	language = {en},
	number = {3},
	urldate = {2022-04-25},
	journal = {Foundations and Trends in Programming Languages},
	author = {Chaudhuri, Swarat and Ellis, Kevin and Polozov, Oleksandr and Singh, Rishabh and Solar-Lezama, Armando and Yue, Yisong},
	year = {2021},
	pages = {158--243},
}

@inproceedings{pradel:typewriter,
	title = {{TypeWriter}: {Neural} {Type} {Prediction} with {Search}-{Based} {Validation}},
	booktitle = {esecfse},
	author = {Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish},
	year = {2020},
}

@inproceedings{malik:nl2type,
	title = {{NL2Type}: {Inferring} {JavaScript} {Function} {Types} from {Natural} {Language} {Information}},
	booktitle = {icse},
	author = {Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael},
	year = {2019},
}

@article{drori:codex-univ-math,
	title = {A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2123433119},
	doi = {10.1073/pnas.2123433119},
	abstract = {We demonstrate that a neural network pretrained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI’s Codex transformer and execute them to solve course problems at 81\% automatic accuracy. We curate a dataset of questions from Massachusetts Institute of Technology (MIT)’s largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University’s Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pretrained on text automatically solves only 18.8\% of these university questions using zero-shot learning and 30.8\% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81\% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8 to 81.1\%. We perform a survey to evaluate the quality and difficulty of generated questions. This work automatically solves university-level mathematics course questions at a human level and explains and generates university-level mathematics course questions at scale, a milestone for higher education.},
	language = {en},
	number = {32},
	urldate = {2022-08-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and Wang, Roman and Singh, Nikhil and Patti, Taylor L. and Lynch, Jayson and Shporer, Avi and Verma, Nakul and Wu, Eugene and Strang, Gilbert},
	month = aug,
	year = {2022},
	pages = {e2123433119},
}

@inproceedings{wei:lambdanet,
	title = {{LambdaNet}: {Probabilistic} {Type} {Inference} using {Graph} {Neural} {Networks}},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
	year = {2020},
}

@misc{zhou_doccoder_2022,
	title = {{DocCoder}: {Generating} {Code} by {Retrieving} and {Reading} {Docs}},
	shorttitle = {{DocCoder}},
	url = {http://arxiv.org/abs/2207.05987},
	abstract = {Natural-language-to-code models learn to generate a code snippet given a natural language (NL) intent. However, the rapid growth of both publicly available and proprietary libraries and functions makes it impossible to cover all APIs using training examples, as new libraries and functions are introduced daily. Thus, existing models inherently cannot generalize to using unseen functions and libraries merely through incorporating them into the training data. In contrast, when human programmers write programs, they frequently refer to textual resources such as code manuals, documentation, and tutorials, to explore and understand available library functionality. Inspired by this observation, we introduce DocCoder: an approach that explicitly leverages code manuals and documentation by (1) retrieving the relevant documentation given the NL intent, and (2) generating the code based on the NL intent and the retrieved documentation. Our approach is general, can be applied to any programming language, and is agnostic to the underlying neural model. We demonstrate that DocCoder consistently improves NL-to-code models: DocCoder achieves 11x higher exact match accuracy than strong baselines on a new Bash dataset tldr; on the popular Python CoNaLa benchmark, DocCoder improves over strong baselines by 1.65 BLEU.},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Zhou, Shuyan and Alon, Uri and Xu, Frank F. and JIang, Zhengbao and Neubig, Graham},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05987 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, To Read},
}

@article{sun_treegen_2020,
	title = {{TreeGen}: {A} {Tree}-{Based} {Transformer} {Architecture} for {Code} {Generation}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{TreeGen}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6430},
	doi = {10.1609/aaai.v34i05.6430},
	abstract = {A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1\%) and GEO (89.6\%). We also conducted an ablation test to better understand each component of our model.},
	language = {en},
	number = {05},
	urldate = {2022-07-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Zeyu and Zhu, Qihao and Xiong, Yingfei and Sun, Yican and Mou, Lili and Zhang, Lu},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {8984--8991},
}

@inproceedings{cassola:gradual-elixir,
	address = {New York, NY, USA},
	series = {{SBLP} '20},
	title = {A {Gradual} {Type} {System} for {Elixir}},
	isbn = {978-1-4503-8943-3},
	url = {https://doi.org/10.1145/3427081.3427084},
	doi = {10.1145/3427081.3427084},
	abstract = {Elixir is a functional programming language with dynamic typing. In this paper we propose a type system that makes it possible to perform static type-checking on a significant fragment of Elixir. An important feature of the type system we introduce is that it does not require any syntactic change to the language. Type information is provided by means of function signatures which are declared in terms of Elixir typespec directives. The proposed type system is based on subtyping and is backward compatible, as it allows the presence of untyped code fragments. We have implemented a prototype of the type-checker in Elixir itself.},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the 24th {Brazilian} {Symposium} on {Context}-{Oriented} {Programming} and {Advanced} {Modularity}},
	publisher = {Association for Computing Machinery},
	author = {Cassola, Mauricio and Talagorria, Agustín and Pardo, Alberto and Viera, Marcos},
	month = oct,
	year = {2020},
	keywords = {elixir, functional programming, gradual typing, static typing},
	pages = {17--24},
}

@inproceedings{kuhlenschmidt:grift,
	title = {Toward {Efficient} {Gradual} {Typing} for {Structural} {Types} via {Coercions}},
	booktitle = {pldi},
	author = {Kuhlenschmidt, Andre and Almahallawi, Deyaaeldeen and Siek, Jeremy G.},
	year = {2019},
}

@article{muehlboeck:nom,
	title = {Sound gradual typing is nominally alive and well},
	volume = {1},
	url = {https://doi.org/10.1145/3133880},
	doi = {10.1145/3133880},
	abstract = {Recent research has identified significant performance hurdles that sound gradual typing needs to overcome. These performance hurdles stem from the fact that the run-time checks gradual type systems insert into code can cause significant overhead. We propose that designing a type system for a gradually typed language hand in hand with its implementation from scratch is a possible way around these and several other hurdles on the way to efficient sound gradual typing. Such a design process also highlights the type-system restrictions required for efficient composition with gradual typing. We formalize the core of a nominal object-oriented language that fulfills a variety of desirable properties for gradually typed languages, and present evidence that an implementation of this language suffers minimal overhead even in adversarial benchmarks identified in earlier work.},
	number = {OOPSLA},
	urldate = {2022-07-26},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Muehlboeck, Fabian and Tate, Ross},
	month = oct,
	year = {2017},
	keywords = {Gradual Typing, Immediate Accountability, Nominal, Transparency},
	pages = {56:1--56:30},
}

@article{lu:static-python,
	title = {Gradual {Soundness}: {Lessons} from {Static} {Python}},
	volume = {7},
	issn = {2473-7321},
	shorttitle = {Gradual {Soundness}},
	url = {https://programming-journal.org/2023/7/2},
	doi = {10.22152/programming-journal.org/2023/7/2},
	abstract = {Context Gradually-typed languages allow typed and untyped code to interoperate, but typically come with signiﬁcant drawbacks. In some languages, the types are unreliable; in others, communication across type boundaries can be extremely expensive; and still others allow only limited forms of interoperability. The research community is actively seeking a sound, fast, and expressive approach to gradual typing.},
	language = {en},
	number = {1},
	urldate = {2022-07-26},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Lu, Kuang-Chen and Greenman, Ben and Meyer, Carl and Viehland, Dino and Panse, Aniket and Krishnamurthi, Shriram},
	month = jun,
	year = {2022},
	pages = {2},
}

@inproceedings{bonnaire-sergeant:typed-clojure,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Practical {Optional} {Types} for {Clojure}},
	isbn = {978-3-662-49498-1},
	doi = {10.1007/978-3-662-49498-1_4},
	abstract = {Typed Clojure is an optional type system for Clojure, a dynamic language in the Lisp family that targets the JVM. Typed Clojure enables Clojure programmers to gain greater confidence in the correctness of their code via static type checking while remaining in the Clojure world, and has acquired significant adoption in the Clojure community. Typed Clojure repurposes Typed Racket’s occurrence typing, an approach to statically reasoning about predicate tests, and also includes several new type system features to handle existing Clojure idioms.},
	language = {en},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer},
	author = {Bonnaire-Sergeant, Ambrose and Davies, Rowan and Tobin-Hochstadt, Sam},
	editor = {Thiemann, Peter},
	year = {2016},
	keywords = {Java Virtual Machine, Proof System, Type Check, Type System, Typing Rule},
	pages = {68--94},
}

@inproceedings{ottoni:hhvm,
	address = {New York, NY, USA},
	series = {{PLDI} 2018},
	title = {{HHVM} {JIT}: a profile-guided, region-based compiler for {PHP} and {Hack}},
	isbn = {978-1-4503-5698-5},
	shorttitle = {{HHVM} {JIT}},
	url = {https://doi.org/10.1145/3192366.3192374},
	doi = {10.1145/3192366.3192374},
	abstract = {Dynamic languages such as PHP, JavaScript, Python, and Ruby have been gaining popularity over the last two decades. A very popular domain for these languages is web development, including server-side development of large-scale websites. As a result, improving the performance of these languages has become more important. Efficiently compiling programs in these languages is challenging, and many popular dynamic languages still lack efficient production-quality implementations. This paper describes the design of the second generation of the HHVM JIT and how it addresses the challenges to efficiently execute PHP and Hack programs. This new design uses profiling to build an aggressive region-based JIT compiler. We discuss the benefits of this approach compared to the more popular method-based and trace-based approaches to compile dynamic languages. Our evaluation running a very large PHP-based code base, the Facebook website, demonstrates the effectiveness of the new JIT design.},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the 39th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ottoni, Guilherme},
	month = jun,
	year = {2018},
	keywords = {Hack, PHP, code optimizations, dynamic languages, profile-guided optimizations, region-based compilation, web server applications},
	pages = {151--165},
}

@misc{pyre,
	title = {Pyre: {A} performant type-checker for {Python} 3},
	url = {https://pyre-check.org/},
	abstract = {A performant type-checker for Python 3.},
	language = {en},
	urldate = {2022-07-26},
}

@article{chauduri:fb-flow,
	title = {Fast and precise type checking for {JavaScript}},
	volume = {1},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Chauduri, Avik and Vekris, Panagiotis and Goldman, Sam and Roch, Marshall and Levi, Gabriel},
	year = {2017},
}

@inproceedings{bierman:ts,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Understanding {TypeScript}},
	isbn = {978-3-662-44202-9},
	doi = {10.1007/978-3-662-44202-9_11},
	abstract = {TypeScript is an extension of JavaScript intended to enable easier development of large-scale JavaScript applications. While every JavaScript program is a TypeScript program, TypeScript offers a module system, classes, interfaces, and a rich gradual type system. The intention is that TypeScript provides a smooth transition for JavaScript programmers—well-established JavaScript programming idioms are supported without any major rewriting or annotations. One interesting consequence is that the TypeScript type system is not statically sound by design. The goal of this paper is to capture the essence of TypeScript by giving a precise definition of this type system on a core set of constructs of the language. Our main contribution, beyond the familiar advantages of a robust, mathematical formalization, is a refactoring into a safe inner fragment and an additional layer of unsafe rules.},
	language = {en},
	booktitle = {{ECOOP} 2014 – {Object}-{Oriented} {Programming}},
	publisher = {Springer},
	author = {Bierman, Gavin and Abadi, Martín and Torgersen, Mads},
	editor = {Jones, Richard},
	year = {2014},
	keywords = {Call Signature, Object Type, Operational Semantic, Return Type, Type System},
	pages = {257--281},
}

@inproceedings{meyerovich:flapjax,
	title = {Flapjax: {A} {Programming} {Language} for {Ajax} {Applications}},
	copyright = {All rights reserved},
	doi = {10.1145/1640089.1640091},
	abstract = {Citation for the Most Influential Paper Award:

Web programming requires dedicated abstractions that facilitate communication intensive applications with interactive interfaces. Flapjax reimagines how to program client-based web applications by providing two central abstractions adopted from functional reactive programming: event streams and so-called behaviors, which model computations that automatically track data dependencies and propagate updates. Flapjax seamlessly builds on top of JavaScript without requiring a dedicated runtime and thus interoperates with existing applications written in JavaScript. This design choice lowers the barrier to entry and makes the language approachable to novice users. In terms of modern usage, the reactive style and event stream abstraction advocated by Flapjax have been adopted widely, both in mobile apps and on the web. Moreover, recent research has shown that beyond interactivity, these abstractions also facilitate correct-by-design distributed applications in presence of concurrency as well as crash and communication faults.},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Object} {Oriented} {Programmingm}, {Systems}, {Languages} and {Applications} ({OOPSLA})},
	author = {Meyerovich, Leo A. and Guha, Arjun and Baskin, Jacob and Cooper, Gregory H. and Greenberg, Michael and Bromfield, Aleks and Krishnamurthi, Shriram},
	year = {2009},
	note = {Best Student Paper Award (2009); Most Influential Paper Award (awarded 2019)},
	pages = {1 -- 20},
}

@inproceedings{politz:adsafety,
	title = {{ADsafety}: {Type}-based {Verification} of {JavaScript} {Sandboxing}},
	copyright = {All rights reserved},
	booktitle = {{USENIX} {Security} {Symposium}},
	author = {Politz, Joe Gibbs and Eliopoulos, Spiridon Aristides and Guha, Arjun and Krishnamurthi, Shriram},
	year = {2011},
}

@inproceedings{guha:ibex,
	title = {Verified {Security} for {Browser} {Extensions}},
	copyright = {All rights reserved},
	booktitle = {{IEEE} {Security} and {Privacy} ({Oakland})},
	author = {Guha, Arjun and Fredrikson, Matthew and Livshits, Benjamin and Swamy, Nikhil},
	year = {2011},
}

@inproceedings{guha:static-analysis-for-ajax-intrusion-detection,
	title = {Using {Static} {Analysis} for {Ajax} {Intrusion} {Detection}},
	copyright = {All rights reserved},
	booktitle = {World {Wide} {Web} {Conference} ({WWW})},
	author = {Guha, Arjun and Krishnamurthi, Shriram and Jim, Trevor},
	year = {2009},
}

@inproceedings{guha:poly-contracts,
	title = {Relationally-{Parametric} {Polymorphic} {Contracts}},
	copyright = {All rights reserved},
	booktitle = {Dynamic {Languages} {Symposium} ({DLS})},
	author = {Guha, Arjun and Matthews, Jacob and Findler, Robert Bruce and Krishnamurthi, Shriram},
	year = {2007},
}

@inproceedings{lerner:tejas,
	title = {{TeJaS}: {Retrofitting} {Type} {Systems} for {JavaScript}},
	copyright = {All rights reserved},
	booktitle = {Dynamic {Languages} {Symposium} ({DLS})},
	author = {Lerner, Benjamin S. and Politz, Joe Gibbs and Guha, Arjun and Krishnamurthi, Shriram},
	year = {2013},
}

@inproceedings{politz:first-class-member-names,
	title = {A {Tested} {Semantics} for {Getters}, {Setters}, and {Eval} in {JavaScript}},
	booktitle = {dls},
	author = {Politz, Joe Gibbs and Carroll, Matthew J. and Lerner, Benjamin S. and Krishnamurthi, Shriram},
	year = {2012},
}

@inproceedings{guha:flowtypes,
	title = {Typing {Local} {Control} and {State} {Using} {Flow} {Analysis}},
	copyright = {All rights reserved},
	booktitle = {European {Symposium} on {Programming} ({ESOP})},
	author = {Guha, Arjun and Saftoiu, Claudiu and Krishnamurthi, Shriram},
	year = {2011},
	keywords = {types},
}

@inproceedings{guha:js,
	title = {The {Essence} of {JavaScript}},
	copyright = {All rights reserved},
	booktitle = {European {Conference} on {Object} {Oriented} {Programming} ({ECOOP})},
	author = {Guha, Arjun and Saftoiu, Claudiu and Krishnamurthi, Shriram},
	year = {2010},
	keywords = {JavaScript},
}

@misc{chen:codex,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Codexeval, Computer Science - Machine Learning},
}

@misc{xu:in-ide-code-generation,
	title = {In-{IDE} {Code} {Generation} from {Natural} {Language}: {Promise} and {Challenges}},
	shorttitle = {In-{IDE} {Code} {Generation} from {Natural} {Language}},
	url = {http://arxiv.org/abs/2101.11149},
	doi = {10.48550/arXiv.2101.11149},
	abstract = {A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. We perform the first comprehensive investigation of the promise and challenges of using such technology inside the IDE, asking "at the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?" We first develop a plugin for the IDE that implements a hybrid of code generation and code retrieval functionality, and orchestrate virtual environments to enable collection of many user events. We ask developers with various backgrounds to complete 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Analysis identifies several pain points that could improve the effectiveness of future machine learning based code generation/retrieval developer assistants, and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies and development of better models.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Xu, Frank F. and Vasilescu, Bogdan and Neubig, Graham},
	month = sep,
	year = {2021},
	note = {arXiv:2101.11149 [cs]},
	keywords = {Codexeval, Computer Science - Software Engineering},
}

@inproceedings{vaithilingam:small-copilot-study,
	address = {New Orleans LA USA},
	title = {Expectation vs. {Experience}: {Evaluating} the {Usability} of {Code} {Generation} {Tools} {Powered} by {Large} {Language} {Models}},
	isbn = {978-1-4503-9156-6},
	shorttitle = {Expectation vs. {Experience}},
	url = {https://dl.acm.org/doi/10.1145/3491101.3519665},
	doi = {10.1145/3491101.3519665},
	abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.},
	language = {en},
	urldate = {2022-07-19},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
	month = apr,
	year = {2022},
	pages = {1--7},
}

@misc{titzer_fast_2022,
	title = {A fast in-place interpreter for {WebAssembly}},
	url = {http://arxiv.org/abs/2205.01183},
	abstract = {WebAssembly (Wasm) is a compact, well-specified bytecode format that offers a portable compilation target with near-native execution speed. The bytecode format was specifically designed to be fast to parse, validate, and compile, positioning itself as a portable alternative to native code. It was pointedly not designed to be interpreted directly. Instead, design considerations at the time focused on competing with native code, utilizing optimizing compilers as the primary execution tier. Yet, in JIT scenarios, compilation time and memory consumption critically impact application startup, leading many Wasm engines to later deploy baseline (single-pass) compilers. Though faster, baseline compilers still take time and waste code space for infrequently executed code. A typical interpreter being infeasible, some engines resort to compiling Wasm not to machine code, but to a more compact, but easy to interpret format. This still takes time and wastes memory. Instead, we introduce in this article a fast in-place interpreter for WebAssembly, where no rewrite and no separate format is necessary. Our evaluation shows that in-place interpretation of Wasm code is space-efficient and fast, achieving performance on-par with interpreting a custom-designed internal format. This fills a hole in the execution tier space for Wasm, allowing for even faster startup and lower memory footprint than previous engine configurations.},
	urldate = {2022-07-14},
	publisher = {arXiv},
	author = {Titzer, Ben L.},
	month = may,
	year = {2022},
	note = {arXiv:2205.01183 [cs]},
	keywords = {Computer Science - Performance, Computer Science - Programming Languages, D.3.4},
}

@inproceedings{subramanian:genesis,
	address = {Paris France},
	title = {Genesis: synthesizing forwarding tables in multi-tenant networks},
	isbn = {978-1-4503-4660-3},
	shorttitle = {Genesis},
	url = {https://dl.acm.org/doi/10.1145/3009837.3009845},
	doi = {10.1145/3009837.3009845},
	abstract = {Operators in multi-tenant cloud datacenters require support for diverse and complex end-to-end policies, such as, reachability, middlebox traversals, isolation, trafﬁc engineering, and network resource management. We present GENESIS, a datacenter network management system which allows policies to be speciﬁed in a declarative manner without explicitly programming the network data plane. GENESIS tackles the problem of enforcing policies by synthesizing switch forwarding tables. It uses the formal foundations of constraint solving in combination with fast off-the-shelf SMT solvers. To improve synthesis performance, GENESIS incorporates a novel search strategy that uses regular expressions to specify properties that leverage the structure of datacenter networks, and a divide-and-conquer synthesis procedure which exploits the structure of policy relationships. We have prototyped GENESIS, and conducted experiments with a variety of workloads on real-world topologies to demonstrate its performance.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {{ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages} ({POPL})},
	publisher = {ACM},
	author = {Subramanian, Kausik and D'Antoni, Loris and Akella, Aditya},
	month = jan,
	year = {2017},
	pages = {572--585},
}

@article{williams:ts-tpd,
	title = {Mixed {Messages}: {Measuring} {Conformance} and {Non}-{Interference} in {TypeScript}},
	abstract = {TypeScript participates in the recent trend among programming languages to support gradual typing. The DeﬁnitelyTyped Repository for TypeScript supplies type deﬁnitions for over 2000 popular JavaScript libraries. However, there is no guarantee that implementations conform to their corresponding declarations.},
	language = {en},
	author = {Williams, Jack and Morris, J Garrett and Wadler, Philip and Zalewski, Jakub},
	year = {2017},
	pages = {29},
}

@misc{abacus:ts,
	title = {How {We} {Completed} a ({Partial}) {TypeScript} {Migration} {In} {Six} {Months}},
	url = {https://blog.abacus.com/how-we-completed-a-partial-typescript-migration-in-six-months/},
	abstract = {How our Engineering team moved all our React-based code and all new front-end development to TypeScript, without delaying or postponing feature development.},
	language = {en-US},
	urldate = {2022-05-19},
	journal = {Abacus},
	author = {Abacus},
	month = feb,
	year = {2019},
	note = {Section: Developing In Real Time},
}

@misc{airbnb:ts-migrate,
	title = {ts-migrate: {A} {Tool} for {Migrating} to {TypeScript} at {Scale}},
	shorttitle = {ts-migrate},
	url = {https://medium.com/airbnb-engineering/ts-migrate-a-tool-for-migrating-to-typescript-at-scale-cd23bfeb5cc},
	abstract = {Learn about how we used codemods to accelerate migration from JavaScript to TypeScript at Airbnb.},
	language = {en},
	urldate = {2022-05-19},
	journal = {The Airbnb Tech Blog},
	author = {Rudenko, Sergii},
	month = aug,
	year = {2020},
}

@misc{tiobe,
	title = {{TIOBE} {Index}},
	url = {https://www.tiobe.com/tiobe-index/},
	language = {en-US},
	urldate = {2022-07-05},
	journal = {TIOBE},
}

@inproceedings{henglein:scheme-to-ml,
	title = {Safe polymorphic type inference for a dynamically typed language: {Translating} {Scheme} to {ML}},
	booktitle = {International {Conference} on {Functional} {Programming} {Languages} and {Computer} {Architecture} ({FPCA})},
	author = {Henglein, Fritz and Rehof, Jakob},
	year = {1995},
}

@inproceedings{th:typed-scheme,
	title = {The {Design} and {Implementation} of {Typed} {Scheme}},
	booktitle = {{ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages} ({POPL})},
	author = {Tobin-Hochstadt, Sam and Felleisen, Matthias},
	year = {2008},
}

@article{campora:pyhound,
	title = {Taming type annotations in gradual typing},
	volume = {4},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3428259},
	doi = {10.1145/3428259},
	language = {en},
	number = {OOPSLA},
	urldate = {2022-04-26},
	journal = {Proceedings of the ACM on Programming Languages (PACMPL)},
	author = {Campora, John Peter and Sheng, Chen},
	month = nov,
	year = {2020},
	pages = {1--30},
}

@inproceedings{hopjs,
	title = {A {Glimpse} of {Hopjs}},
	booktitle = {icfp},
	author = {Serrano, Manuel and Prunet, Vincent},
	year = {2016},
}

@inproceedings{imai_is_2022,
	title = {Is {GitHub} {Copilot} a {Substitute} for {Human} {Pair}-programming? {An} {Empirical} {Study}},
	shorttitle = {Is {GitHub} {Copilot} a {Substitute} for {Human} {Pair}-programming?},
	doi = {10.1109/ICSE-Companion55297.2022.9793778},
	abstract = {This empirical study investigates the effectiveness of pair programming with GitHub Copilot in comparison to human pair-programming. Through an experiment with 21 participants we focus on code productivity and code quality. For experimental design, a participant was given a project to code, under three conditions presented in a randomized order. The conditions are pair-programming with Copilot, human pair-programming as a driver, and as a navigator. The codes generated from the three trials were analyzed to determine how many lines of code on average were added in each condition and how many lines of code on average were removed in the subsequent stage. The former measures the productivity of each condition while the latter measures the quality of the produced code. The results suggest that although Copilot increases productivity as measured by lines of code added, the quality of code produced is inferior by having more lines of code deleted in the subsequent trial.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Imai, Saki},
	month = may,
	year = {2022},
	note = {ISSN: 2574-1926},
	keywords = {Codexeval},
	pages = {319--321},
}

@misc{inala_fault-aware_2022,
	title = {Fault-{Aware} {Neural} {Code} {Rankers}},
	url = {http://arxiv.org/abs/2206.03865},
	doi = {10.48550/arXiv.2206.03865},
	abstract = {Large language models (LLMs) have demonstrated an impressive ability to generate code for various programming tasks. In many instances, LLMs can generate a correct program for a task when given numerous trials. Consequently, a recent trend is to do large scale sampling of programs using a model and then filtering/ranking the programs based on the program execution on a small number of known unit tests to select one candidate solution. However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development. In this paper, we propose fault-aware neural code rankers that can predict the correctness of a sampled program without executing it. The fault-aware rankers are trained to predict different kinds of execution information such as predicting the exact compile/runtime error type (e.g., an IndexError or a TypeError). We show that our fault-aware rankers can significantly increase the pass@1 accuracy of various code generation models (including Codex, GPT-Neo, GPT-J) on APPS, HumanEval and MBPP datasets.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Inala, Jeevana Priya and Wang, Chenglong and Yang, Mei and Codas, Andres and Encarnación, Mark and Lahiri, Shuvendu K. and Musuvathi, Madanlal and Gao, Jianfeng},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.03865
arXiv:2206.03865 [cs]},
	keywords = {Codexeval, Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{hahn_formal_2022,
	title = {Formal {Specifications} from {Natural} {Language}},
	url = {http://arxiv.org/abs/2206.01962},
	abstract = {We study the ability of language models to translate natural language into formal specifications with complex semantics. In particular, we fine-tune off-the-shelf language models on three datasets consisting of structured English sentences and their corresponding formal representation: 1) First-order logic (FOL), commonly used in software verification and theorem proving; 2) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages; and 3) regular expressions (regex), frequently used in programming and search. Our experiments show that, in these diverse domains, the language models achieve competitive performance to the respective state-of-the-art with the benefits of being easy to access, cheap to fine-tune, and without a particular need for domain-specific reasoning. Additionally, we show that the language models have a unique selling point: they benefit from their generalization capabilities from pre-trained knowledge on natural language, e.g., to generalize to unseen variable names.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Hahn, Christopher and Schmitt, Frederik and Tillman, Julia J. and Metzger, Niklas and Siber, Julian and Finkbeiner, Bernd},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.01962
arXiv:2206.01962 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering, Joydeep},
}

@misc{ni_learning_2022,
	title = {Learning from {Self}-{Sampled} {Correct} and {Partially}-{Correct} {Programs}},
	url = {http://arxiv.org/abs/2205.14318},
	doi = {10.48550/arXiv.2205.14318},
	abstract = {Program synthesis aims to generate executable programs that are consistent with the user specification. While there are often multiple programs that satisfy the same user specification, existing neural program synthesis models are often only learned from one reference program by maximizing its log-likelihood. This causes the model to be overly confident in its predictions as it sees the single solution repeatedly during training. This leads to poor generalization on unseen examples, even when multiple attempts are allowed. To mitigate this issue, we propose to let the model perform sampling during training and learn from both self-sampled fully-correct programs, which yield the gold execution results, as well as partially-correct programs, whose intermediate execution state matches another correct program. We show that our use of self-sampled correct and partially-correct programs can benefit learning and help guide the sampling process, leading to more efficient exploration of the program space. Additionally, we explore various training objectives to support learning from multiple programs per example and find they greatly affect the performance. Experiments on the MathQA and GSM8K datasets show that our proposed method improves the pass@k performance by 3.1\% to 12.3\% compared to learning from a single reference program with MLE.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Ni, Ansong and Inala, Jeevana Priya and Wang, Chenglong and Polozov, Oleksandr and Meek, Christopher and Radev, Dragomir and Gao, Jianfeng},
	month = may,
	year = {2022},
	note = {Number: arXiv:2205.14318
arXiv:2205.14318 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, New Ideas},
}

@misc{zhong_active_2022,
	title = {Active {Programming} by {Example} with a {Natural} {Language} {Prior}},
	url = {http://arxiv.org/abs/2205.12422},
	doi = {10.48550/arXiv.2205.12422},
	abstract = {We introduce APEL, a new framework that enables non-programmers to indirectly annotate natural language utterances with executable meaning representations, such as SQL programs. Based on a natural language utterance, we first run a seed semantic parser to generate a prior over a list of candidate programs. To obtain information about which candidate is correct, we synthesize an input on which the more likely programs tend to produce different outputs, and ask an annotator which output is appropriate for the utterance. Hence, the annotator does not have to directly inspect the programs. To further reduce effort required from annotators, we aim to synthesize simple input databases that nonetheless have high information gain. With human annotators and Bayesian inference to handle annotation errors, we outperform Codex's top-1 performance (59\%) and achieve the same accuracy as the original expert annotators (75\%), by soliciting answers for each utterance on only 2 databases with an average of 9 records each. In contrast, it would be impractical to solicit outputs on the original 30K-record databases provided by SPIDER},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Zhong, Ruiqi and Snell, Charlie and Klein, Dan and Eisner, Jason},
	month = may,
	year = {2022},
	note = {Number: arXiv:2205.12422
arXiv:2205.12422 [cs]},
	keywords = {Codexeval, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
}

@misc{fan_improving_2022,
	title = {Improving automatically generated code from {Codex} via {Automated} {Program} {Repair}},
	url = {http://arxiv.org/abs/2205.10583},
	doi = {10.48550/arXiv.2205.10583},
	abstract = {Large language models, e.g., Codex and AlphaCode, have shown capability in producing working code for many programming tasks. However, the success rate of existing models remains low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics (e.g., type information), resulting in incorrect programs (or even programs which do not compile). In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance confidence in the code produced by language models. Our study revealed that: (1) automatically generated codes share some common programming mistakes with human-crafted solutions, indicating existing APR tools have the potential to fix auto-generated code; (2) TBar and Recoder, two well-known Java APR tools based on templates and learning respectively, increase the number of solved tasks from 37 to 42 on 60 easy level tasks, while increase from 5 to 9 on 53 medium-level programming tasks; (3) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports changing existing code, may outperform existing APR tools in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions on how to improve current APR tools.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Fan, Zhiyu and Gao, Xiang and Roychoudhury, Abhik and Tan, Shin Hwei},
	month = may,
	year = {2022},
	note = {Number: arXiv:2205.10583
arXiv:2205.10583 [cs]},
	keywords = {Alexandria, Codexeval, Computer Science - Software Engineering},
}

@misc{ahmad_summarize_2022,
	title = {Summarize and {Generate} to {Back}-translate: {Unsupervised} {Translation} of {Programming} {Languages}},
	shorttitle = {Summarize and {Generate} to {Back}-translate},
	url = {http://arxiv.org/abs/2205.11116},
	doi = {10.48550/arXiv.2205.11116},
	abstract = {Back-translation is widely known for its effectiveness for neural machine translation when little to no parallel data is available. In this approach, a source-to-target model is coupled with a target-to-source model trained in parallel. The target-to-source model generates noisy sources, while the source-to-target model is trained to reconstruct the targets and vice versa. Recent developments of multilingual pre-trained sequence-to-sequence models for programming languages have been very effective for a broad spectrum of downstream software engineering tasks. Hence, it is compelling to train them to build programming language translation systems via back-translation. However, these models cannot be further trained via back-translation since they learn to output sequences in the same language as the inputs during pre-training. As an alternative, we propose performing back-translation via code summarization and generation. In code summarization, a model learns to generate natural language (NL) summaries given code snippets. In code generation, the model learns to do the opposite. Therefore, target-to-source generation in back-translation can be viewed as target-to-NL-to-source generation. We show that our proposed approach performs competitively with state-of-the-art methods.},
	urldate = {2022-06-15},
	publisher = {arXiv},
	author = {Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
	month = may,
	year = {2022},
	note = {Number: arXiv:2205.11116
arXiv:2205.11116 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
}

@misc{chung_towards_nodate,
	title = {Towards a {Static} {Type} {System} for {Julia}},
	language = {en},
	author = {Chung, Benjamin},
	note = {Thesis Proposal},
}

@book{wojslaw_introducing_2017,
	address = {Berkeley, CA},
	title = {Introducing {ZFS} on {Linux}},
	isbn = {978-1-4842-3305-4 978-1-4842-3306-1},
	url = {http://link.springer.com/10.1007/978-1-4842-3306-1},
	language = {en},
	urldate = {2022-05-30},
	publisher = {Apress},
	author = {Wojsław, Damian},
	year = {2017},
	doi = {10.1007/978-1-4842-3306-1},
}

@article{larsen_allsynth_nodate,
	title = {{AllSynth}: {Transiently} {Correct} {Network} {Update} {Synthesis} {Accounting} for {Operator} {Preferences}},
	abstract = {The increasingly stringent dependability requirements on communication networks as well as the need to render these networks more adaptive to improve performance, demand for more automated approaches to operate networks. We present AllSynth, a symbolic synthesis tool for updating communication networks in a provably correct and eﬃcient manner. AllSynth automatically synthesizes network update schedules which transiently ensure a wide range of policy properties (expressed in the LTL logic), also during the reconﬁguration process. In particular, in contrast to existing approaches, AllSynth symbolically computes and compactly represents all feasible solutions. At its heart, AllSynth relies on a novel, two-level and parameterized use of BDDs which greatly improves performance. Indeed, AllSynth not only provides formal correctness guarantees and outperforms existing state-of-the-art tools in terms of generality, but often also in terms of runtime as documented by experiments on a benchmark of real-world network topologies.},
	language = {en},
	author = {Larsen, Kim Guldstrand and Mariegaard, Anders and Schmid, Stefan and Srba, Jiˇrı},
	keywords = {To Read},
	pages = {26},
}

@misc{elliott_you_2016,
	title = {You {Might} {Not} {Need} {TypeScript} (or {Static} {Types})},
	url = {https://medium.com/javascript-scene/you-might-not-need-typescript-or-static-types-aa7cb670a77b},
	abstract = {TypeScript has gained a lot of popularity since the Angular 2 project decided to adopt it and write all their documentation examples in…},
	language = {en},
	urldate = {2022-05-19},
	journal = {JavaScript Scene},
	author = {Elliott, Eric},
	month = dec,
	year = {2016},
}

@misc{krasnov_7_2020,
	title = {7 really good reasons not to use {TypeScript}},
	url = {https://everyday.codes/javascript/7-really-good-reasons-not-to-use-typescript/},
	abstract = {Everyone loves TypeScript. There are a lot of good reasons to use TypeScript, but I am going to give you 7 really good reasons not to.},
	language = {en},
	urldate = {2022-05-19},
	journal = {everyday.codes},
	author = {Krasnov, Michael},
	month = feb,
	year = {2020},
}

@misc{on_why_nodate,
	title = {Why {I} no longer use {TypeScript} with {React} and why you shouldn’t either {\textbar} {HackerNoon}},
	url = {https://hackernoon.com/why-i-no-longer-use-typescript-with-react-and-why-you-shouldnt-either-e744d27452b4},
	language = {en},
	urldate = {2022-05-19},
	author = {on, Dominik Tarnowski},
}

@misc{noauthor_migrating_nodate,
	title = {Migrating a 200,000 {LOC} codebase to {Typescript} in a day and a half},
	url = {https://www.mentimeter.com/blog/product-and-tech/migrating-a-200000-loc-codebase-to-typescript-in-a-day-and-a-half},
	abstract = {How we successfully migrated a 200,000 lines of code codebase to Typescript in a day and a half from Javascript to Flow.},
	language = {en-US},
	urldate = {2022-05-19},
	journal = {Mentimeter},
}

@misc{noauthor_great_nodate,
	title = {The {Great} {CoffeeScript} to {Typescript} {Migration} of 2017 - {Dropbox}},
	url = {https://dropbox.tech/frontend/the-great-coffeescript-to-typescript-migration-of-2017},
	urldate = {2022-05-18},
}

@article{brown_position_2021,
	title = {Position {Paper}: {Goals} of the {Luau} {Type} {System}},
	shorttitle = {Position {Paper}},
	url = {http://arxiv.org/abs/2109.11397},
	abstract = {Luau is the scripting language that powers user-generated experiences on the Roblox platform. It is a statically-typed language, based on the dynamically-typed Lua language, with type inference. These types are used for providing editor assistance in Roblox Studio, the IDE for authoring Roblox experiences. Due to Roblox's uniquely heterogeneous developer community, Luau must operate in a somewhat different fashion than a traditional statically-typed language. In this paper, we describe some of the goals of the Luau type system, focusing on where the goals differ from those of other type systems.},
	urldate = {2022-05-12},
	journal = {arXiv:2109.11397 [cs]},
	author = {Brown, Lily and Friesen, Andy and Jeffrey, Alan},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.11397},
	keywords = {Computer Science - Programming Languages},
}

@article{hirsch_nexus_2012,
	title = {Nexus {Authorization} {Logic} ({NAL}): {Logical} {Results}},
	shorttitle = {Nexus {Authorization} {Logic} ({NAL})},
	url = {http://arxiv.org/abs/1211.3700},
	abstract = {Nexus Authorization Logic (NAL) [Schneider et al. 2011] is a logic for reasoning about authorization in distributed systems. A revised version of NAL is given here, including revised syntax, a revised proof theory using localized hypotheses, and a new Kripke semantics. The proof theory is proved sound with respect to the semantics, and that proof is formalized in Coq.},
	language = {en},
	urldate = {2022-05-03},
	journal = {arXiv:1211.3700 [cs]},
	author = {Hirsch, Andrew K. and Clarkson, Michael R.},
	month = nov,
	year = {2012},
	note = {arXiv: 1211.3700},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Logic in Computer Science},
}

@book{van_dalen_logic_2013,
	address = {London},
	series = {Universitext},
	title = {Logic and {Structure}},
	isbn = {978-1-4471-4557-8 978-1-4471-4558-5},
	url = {http://link.springer.com/10.1007/978-1-4471-4558-5},
	language = {en},
	urldate = {2022-05-02},
	publisher = {Springer London},
	author = {van Dalen, Dirk},
	year = {2013},
	doi = {10.1007/978-1-4471-4558-5},
}

@inproceedings{kamil_verified_2016,
	address = {New York, NY, USA},
	series = {{PLDI} '16},
	title = {Verified lifting of stencil computations},
	isbn = {978-1-4503-4261-2},
	url = {https://doi.org/10.1145/2908080.2908117},
	doi = {10.1145/2908080.2908117},
	abstract = {This paper demonstrates a novel combination of program synthesis and verification to lift stencil computations from low-level Fortran code to a high-level summary expressed using a predicate language. The technique is sound and mostly automated, and leverages counter-example guided inductive synthesis (CEGIS) to find provably correct translations. Lifting existing code to a high-performance description language has a number of benefits, including maintainability and performance portability. For example, our experiments show that the lifted summaries can enable domain specific compilers to do a better job of parallelization as compared to an off-the-shelf compiler working on the original code, and can even support fully automatic migration to hardware accelerators such as GPUs. We have implemented verified lifting in a system called STNG and have evaluated it using microbenchmarks, mini-apps, and real-world applications. We demonstrate the benefits of verified lifting by first automatically summarizing Fortran source code into a high-level predicate language, and subsequently translating the lifted summaries into Halide, with the translated code achieving median performance speedups of 4.1X and up to 24X for non-trivial stencils as compared to the original implementation.},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Kamil, Shoaib and Cheung, Alvin and Itzhaky, Shachar and Solar-Lezama, Armando},
	month = jun,
	year = {2016},
	keywords = {Domain-specific Languages, Program Synthesis, Verified Lifting},
	pages = {711--726},
}

@inproceedings{pugh_uniform_1991,
	address = {New York, NY, USA},
	series = {{ICS} '91},
	title = {Uniform techniques for loop optimization},
	isbn = {978-0-89791-434-5},
	url = {https://doi.org/10.1145/109025.109108},
	doi = {10.1145/109025.109108},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the 5th international conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Pugh, William},
	month = jun,
	year = {1991},
	pages = {341--352},
}

@inproceedings{phothilimthana_swizzle_2019,
	address = {New York, NY, USA},
	series = {{ASPLOS} '19},
	title = {Swizzle {Inventor}: {Data} {Movement} {Synthesis} for {GPU} {Kernels}},
	isbn = {978-1-4503-6240-5},
	shorttitle = {Swizzle {Inventor}},
	url = {https://doi.org/10.1145/3297858.3304059},
	doi = {10.1145/3297858.3304059},
	abstract = {Utilizing memory and register bandwidth in modern architectures may require swizzles --- non-trivial mappings of data and computations onto hardware resources --- such as shuffles. We develop Swizzle Inventor to help programmers implement swizzle programs, by writing program sketches that omit swizzles and delegating their creation to an automatic synthesizer. Our synthesis algorithm scales to real-world programs, allowing us to invent new GPU kernels for stencil computations, matrix transposition, and a finite field multiplication algorithm (used in cryptographic applications). The synthesized 2D convolution and finite field multiplication kernels are on average 1.5--3.2x and 1.1--1.7x faster, respectively, than expert-optimized CUDA kernels.},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Phothilimthana, Phitchaya Mangpo and Elliott, Archibald Samuel and Wang, An and Jangda, Abhinav and Hagedorn, Bastian and Barthels, Henrik and Kaufman, Samuel J. and Grover, Vinod and Torlak, Emina and Bodik, Rastislav},
	month = apr,
	year = {2019},
	keywords = {GPGPU, program synthesis, swizzling},
	pages = {65--78},
}

@inproceedings{ahmad_vector_2022,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2022},
	title = {Vector instruction selection for digital signal processors using program synthesis},
	isbn = {978-1-4503-9205-1},
	url = {https://doi.org/10.1145/3503222.3507714},
	doi = {10.1145/3503222.3507714},
	abstract = {Instruction selection, whereby input code represented in an intermediate representation is translated into executable instructions from the target platform, is often the most target-dependent component in optimizing compilers. Current approaches include pattern matching, which is brittle and tedious to design, or search-based methods, which are limited by scalability of the search algorithm. In this paper, we propose a new algorithm that first abstracts the target platform instructions into high-level uber-instructions, with each uber-instruction unifying multiple concrete instructions from the target platform. Program synthesis is used to lift input code sequences into semantically equivalent sequences of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared to code generated by a professionally-developed optimizing compiler for the same platform.},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ahmad, Maaz Bin Safeer and Root, Alexander J. and Adams, Andrew and Kamil, Shoaib and Cheung, Alvin},
	month = feb,
	year = {2022},
	keywords = {Instruction selection, compiler optimizations, program synthesis},
	pages = {1004--1016},
}

@misc{noauthor_online_2015,
	title = {Online {User} {Guide} {\textbar} {KOBUKI}},
	url = {http://kobuki.yujinrobot.com/wiki/online-user-guide/},
	language = {en-US},
	urldate = {2022-05-02},
	month = jul,
	year = {2015},
}

@inproceedings{larsen_exploiting_2000,
	address = {New York, NY, USA},
	series = {{PLDI} '00},
	title = {Exploiting superword level parallelism with multimedia instruction sets},
	isbn = {978-1-58113-199-4},
	url = {https://doi.org/10.1145/349299.349320},
	doi = {10.1145/349299.349320},
	abstract = {Increasing focus on multimedia applications has prompted the addition of multimedia extensions to most existing general purpose microprocessors. This added functionality comes primarily with the addition of short SIMD instructions. Unfortunately, access to these instructions is limited to in-line assembly and library calls. Generally, it has been assumed that vector compilers provide the most promising means of exploiting multimedia instructions. Although vectorization technology is well understood, it is inherently complex and fragile. In addition, it is incapable of locating SIMD-style parallelism within a basic block. In this paper we introduce the concept of Superword Level Parallelism (SLP) ,a novel way of viewing parallelism in multimedia and scientific applications. We believe SLPP is fundamentally different from the loop level parallelism exploited by traditional vector processing, and therefore demands a new method of extracting it. We have developed a simple and robust compiler for detecting SLPP that targets basic blocks rather than loop nests. As with techniques designed to extract ILP, ours is able to exploit parallelism both across loop iterations and within basic blocks. The result is an algorithm that provides excellent performance in several application domains. In our experiments, dynamic instruction counts were reduced by 46\%. Speedups ranged from 1.24 to 6.70.},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2000 conference on {Programming} language design and implementation},
	publisher = {Association for Computing Machinery},
	author = {Larsen, Samuel and Amarasinghe, Saman},
	month = may,
	year = {2000},
	pages = {145--156},
}

@inproceedings{vanhattum_vectorization_2021,
	address = {Virtual USA},
	title = {Vectorization for digital signal processors via equality saturation},
	isbn = {978-1-4503-8317-2},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446707},
	doi = {10.1145/3445814.3446707},
	abstract = {Applications targeting digital signal processors (DSPs) benefit from fast implementations of small linear algebra kernels. While existing auto-vectorizing compilers are effective at extracting performance from large kernels, they struggle to invent the complex data movements necessary to optimize small kernels. To get the best performance, DSP engineers must hand-write and tune specialized small kernels for a wide spectrum of applications and architectures. We present Diospyros, a search-based compiler that automatically finds efficient vectorizations and data layouts for small linear algebra kernels. Diospyros combines symbolic evaluation and equality saturation to vectorize computations with irregular structure. We show that a collection of Diospyros-compiled kernels outperform implementations from existing DSP libraries by 3.1× on average, that Diospyros can generate kernels that are competitive with expert-tuned code, and that optimizing these small kernels offers end-to-end speedup for a DSP application.},
	language = {en},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {VanHattum, Alexa and Nigam, Rachit and Lee, Vincent T. and Bornholt, James and Sampson, Adrian},
	month = apr,
	year = {2021},
	pages = {874--886},
}

@article{shi_natural_2022,
	title = {Natural {Language} to {Code} {Translation} with {Execution}},
	url = {http://arxiv.org/abs/2204.11454},
	abstract = {Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results) during training, they are able to generate correct solutions for many problems. However, choosing a single correct program from among a generated set for each problem remains challenging. In this work, we introduce execution result--based minimum Bayes risk decoding (MBR-EXEC) for program selection and show that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks. We select output programs from a generated candidate set by marginalizing over program implementations that share the same semantics. Because exact equivalence is intractable, we execute each program on a small number of test inputs to approximate semantic equivalence. Across datasets, execution or simulated execution significantly outperforms the methods that do not involve program semantics. We find that MBR-EXEC consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation.},
	urldate = {2022-04-30},
	journal = {arXiv:2204.11454 [cs]},
	author = {Shi, Freda and Fried, Daniel and Ghazvininejad, Marjan and Zettlemoyer, Luke and Wang, Sida I.},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.11454},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, To Read},
}

@inproceedings{politz_semantics_2012,
	title = {Semantics and {Types} for {Objects} with {First}-{Class} {Member} {Names}},
	copyright = {All rights reserved},
	booktitle = {Workshop on {Foundations} of {Object}-{Oriented} {Languages} ({FOOL})},
	author = {Politz, Joe Gibbs and Guha, Arjun and Krishnamurthi, Shriram},
	year = {2012},
}

@inproceedings{politz_sweep_2016,
	title = {The {Sweep}: {Essential} {Examples} for {In}-{Flow} {Peer} {Review}},
	copyright = {All rights reserved},
	booktitle = {{ACM} {Technical} {Symposium} on {Computer} {Science} {Education} ({SIGCSE})},
	author = {Politz, Joe Gibbs and Collard, Joseph M. and Guha, Arjun and Fisler, Kathi and Krishnamurthi, Shriram},
	year = {2016},
}

@misc{anderson_technology_2015,
	title = {Technology {Device} {Ownership}: 2015},
	shorttitle = {Technology {Device} {Ownership}},
	url = {https://www.pewresearch.org/internet/2015/10/29/technology-device-ownership-2015/},
	abstract = {Smartphone and tablet ownership continues to rise, as the adoption of other digital devices slows and even declines.},
	language = {en-US},
	urldate = {2022-04-26},
	journal = {Pew Research Center: Internet, Science \& Tech},
	author = {Anderson, Monica},
	month = oct,
	year = {2015},
}

@inproceedings{lehmann_finding_2022,
	title = {Finding the {Dwarf}: recovering precise types from {WebAssembly} binaries},
	abstract = {The increasing popularity of WebAssembly creates a demand for understanding and reverse engineering WebAssembly binaries. Recovering high-level function types is an important part of this process. One method to recover types is dataflow analysis, but it is complex to implement and may require manual heuristics when logical constraints fall short. In contrast, this paper presents SnowWhite, a learning-based approach for recovering precise, high-level parameter and return types for WebAssembly functions. It improves over prior work on learning-based type recovery by representing the types-to-predict in an expressive type language, which can describe a large number of complex types, instead of the fixed, and usually small type vocabulary used previously. Thus, recovery of a single type is no longer a classification task but sequence prediction, for which we build on the success of neural sequence-to-sequence models. We evaluate SnowWhite on a new, large-scale dataset of 6.3 million type samples extracted from 300,905 WebAssembly object files. The results show the type language is expressive, precisely describing 1,225 types instead the 7 to 35 types considered in previous learning-based approaches. Despite this expressiveness, our type recovery has high accuracy, exactly matching 44.5\% (75.2\%) of all parameter types and 57.7\% (80.5\%) of all return types within the top-1 (top-5) predictions.},
	language = {en},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	author = {Lehmann, Daniel and Pradel, Michael},
	year = {2022},
	pages = {16},
}

@article{noauthor_fatal_2015,
	chapter = {Technology},
	title = {Fatal {A400M} crash linked to data-wipe mistake},
	url = {https://www.bbc.com/news/technology-33078767},
	abstract = {A military plane crash was probably caused by computer files being accidentally wiped from three of its engines, according to investigators.},
	language = {en-GB},
	urldate = {2022-04-26},
	journal = {BBC News},
	month = jun,
	year = {2015},
}

@book{rising_above_the_gathering_storm_committee_us_rising_2010,
	address = {Washington, DC},
	title = {Rising above the gathering storm, revisited: rapidly approaching category 5},
	isbn = {978-0-309-16097-1 978-0-309-16098-8},
	shorttitle = {Rising above the gathering storm, revisited},
	language = {en},
	publisher = {National Academies Press},
	editor = {Rising Above the Gathering Storm Committee (U.S.) and National Academy of Sciences (U.S.) and National Academy of Engineering and Institute of Medicine (U.S.)},
	year = {2010},
	keywords = {Competition, Economic conditions Forecasting, Globalization, Technological innovations, United States},
}

@book{noauthor_rising_2010,
	title = {Rising above the gathering storm, revisited: rapidly approaching {Category} 5},
	publisher = {National Academy of Sciences, National Academy of Engineering, and Institute of Medicine},
	year = {2010},
}

@unpublished{herbert_language-based_2019,
	title = {A {Language}-based {Serverless} {Function} {Accelerator}},
	copyright = {All rights reserved},
	url = {https://arxiv.org/abs/1911.02178},
	author = {Herbert, Emily and Guha, Arjun},
	year = {2019},
}

@unpublished{hammond_automatic_2019,
	title = {Automatic {Failure} {Recovery} for {End}-{User} {Programs} on {Service} {Mobile} {Robots}},
	copyright = {All rights reserved},
	url = {https://arxiv.org/abs/1909.02778},
	author = {Hammond, Jenna Claire and Biswas, Joydeep and Guha, Arjun},
	year = {2019},
}

@techreport{saftoiu_runtime_2010,
	title = {Runtime {Type}-{Discovery} for {JavaScript}},
	copyright = {All rights reserved},
	url = {ftp://ftp.cs.brown.edu/pub/techreports/10/cs10-05.pdf},
	number = {CS-10-05},
	institution = {Brown University},
	author = {Saftoiu, Claudiu and Guha, Arjun and Krishnamurthi, Shriram},
	year = {2010},
}

@inproceedings{voellmy_maple_2013,
	title = {Maple: {Simplifying} {SDN} {Programming} {Using} {Algorithmic} {Policies}},
	booktitle = {{ACM} {SIGCOMM} {Conference}},
	author = {Voellmy, Andreas and Wang, Junchang and Yang, Y. Richard and Ford, Bryan and Hudak, Paul},
	year = {2013},
}

@inproceedings{singh_automated_2013,
	title = {Automated {Feedback} {Generation} for {Introductory} {Programming} {Assignments}},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} ({PLDI})},
	author = {Singh, Rishabh and Gulwani, Sumit and Solar-Lezama, Armando},
	year = {2013},
}

@inproceedings{solar-lezama_combinatorial_2006,
	title = {Combinatorial {Sketching} for {Finite} {Programs}},
	booktitle = {International {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems} ({ASPLOS})},
	author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Saraswat, Vijay and Seshia, Sanjit A.},
	year = {2006},
}

@misc{webteam_verifying_nodate,
	title = {Verifying {Puppet}: {Checking} {Syntax} and {Writing} {Automated} {Tests}},
	shorttitle = {Verifying {Puppet}},
	url = {https://puppet.com/blog/verifying-puppet-checking-syntax-and-writing-automated-tests/},
	abstract = {One of the issues that crops up when working with Puppet is ensuring that your manifests do what you expect. Errors are bound to happen. A missed brace can make a manifest not compile, or forgetting to include a module or set a variable may mean that running Puppet on the host fails to enforce […]},
	language = {en},
	urldate = {2022-04-26},
	author = {Webteam, Puppet},
}

@misc{puppet_inc_verifying_2012,
	title = {Verifying {Puppet}: {Checking} {Syntax} and {Writing} {Automated} {Tests}},
	url = {https://puppet.com/blog/verifying-puppet-checking-syntax-and-writing-automated-tests/},
	urldate = {2022-04-04},
	author = {Puppet, Inc.},
	month = jan,
	year = {2012},
}

@inproceedings{pouryousef_towards_2020,
	title = {Towards {Logically} {Centralized} {Interdomain} {Routing}},
	abstract = {In this paper, we present the design and implementation of CIRCA, a logically centralized architecture and system for interdomain routing that enables operators to ofﬂoad BGP-style route computation to the cloud while preserving the conﬁdentiality of proprietary information. To this end, our work presents the ﬁrst provably safe, live, and fully distributed convergence detection algorithm for decentralized policy routing and, somewhat surprisingly, shows that long MRAI timers can likely be completely eliminated while signiﬁcantly improving convergence delays with logical centralization. Our experiments with a Quagga-based CIRCA prototype and the Internet’s AS topologies suggest that CIRCA can improve interdomain routing convergence delays and transient route inconsistencies by over an order of magnitude and offers nontrivial incremental deployability beneﬁts with modest changes to widely deployed routing infrastructure.},
	language = {en},
	author = {Pouryousef, Shahrooz and Gao, Lixin and Venkataramani, Arun},
	year = {2020},
	pages = {20},
}

@inproceedings{shieh_netquery_2011,
	title = {{NetQuery}: {A} {Knowledge} {Plane} for {Reasoning} about {Network} {Properties}},
	abstract = {This paper presents the design and implementation of NetQuery, a knowledge plane for federated networks such as the Internet. In such networks, not all administrative domains will generate information that an application can trust and many administrative domains may have restrictive policies on disclosing network information. Thus, both the trustworthiness and accessibility of network information pose obstacles to effective reasoning. NetQuery employs trustworthy computing techniques to facilitate reasoning about the trustworthiness of information contained in the knowledge plane while preserving conﬁdentiality guarantees for operator data. By characterizing information disclosure between operators, NetQuery enables remote veriﬁcation of advertised claims and contractual stipulations; this enables new applications because network guarantees can span administrative boundaries. We have implemented NetQuery, built several NetQuery-enabled devices, and deployed applications for cloud datacenters, enterprise networks, and the Internet. Simulations, testbed experiments, and a deployment on a departmental network indicate NetQuery can support hundreds of thousands of operations per second and can thus scale to large ISPs.},
	language = {en},
	author = {Shieh, Alan and Sirer, Emin Gün and Schneider, Fred B},
	year = {2011},
	pages = {12},
}

@inproceedings{gupta_authorizing_2016,
	title = {Authorizing {Network} {Control} at {Software} {Defined} {Internet} {Exchange} {Points}},
	booktitle = {Symposium on {SDN} {Research} ({SOSR})},
	author = {Gupta, Arpit and Feamster, Nick and Vanbever, Laurent},
	year = {2016},
}

@misc{noauthor_aws_2016,
	title = {{AWS} {Step} {Functions}},
	url = {https://aws.amazon.com/step-functions/},
	year = {2016},
}

@misc{schurmann_adaway_2016,
	title = {{AdAway}},
	url = {https://www.adaway.org},
	author = {Schürmann, D. and Moŝenkovs, D.},
	year = {2016},
}

@misc{williams_subject_nodate,
	title = {Subject {Guides}: {Zotero}: {Getting} {Started}},
	copyright = {Copyright Northeastern University 2022},
	shorttitle = {Subject {Guides}},
	url = {https://subjectguides.lib.neu.edu/Zotero/gettingstarted},
	abstract = {A guide to getting started with the Zotero citation management program. Get help creating an account and getting references into your RefWorks library},
	language = {en},
	urldate = {2022-04-24},
	author = {Williams, Brooke},
}

@misc{pandi_probabilistic_2021,
	title = {Probabilistic {Type} {Inference} by {Optimising} {Logical} and {Natural} {Constraints}},
	url = {https://arxiv.org/abs/2004.00348v3},
	author = {Pandi, Irene Vlassi and Barr, Earl T. and Gordon, Andrew D. and Sutton, Charles},
	year = {2021},
}

@inproceedings{jensen_type_2009,
	title = {Type {Analysis} for {JavaScript}},
	booktitle = {sas},
	author = {Jensen, Simon Holm and Møller, Anders and Thiemann, Peter},
	year = {2009},
}

@inproceedings{diekmann_dont_2020,
	title = {Don't {Panic}! {Better}, {Fewer}, {Syntax} {Errors} for {LR} {Parsers}},
	booktitle = {ecoop},
	author = {Diekmann, Lukas and Tratt, Laurence},
	year = {2020},
}

@inproceedings{bloom_thorn_2009,
	title = {Thorn: {Robust}, {Concurrent}, {Extensible} {Scripting} on the {JVM}},
	booktitle = {oopsla},
	author = {Bloom, Bard and Field, John and Nystrom, Nathaniel and Östlund, Johan and Richards, Gregor and Strniša, Rok and Vitek, Jan and Wrigstad, Tobias},
	year = {2009},
}

@misc{noauthor_firecracker_2021,
	title = {Firecracker {MicroVM}},
	url = {https://firecracker-microvm.github.io/},
	year = {2021},
}

@inproceedings{ren_just--time_2016,
	title = {Just-in-{Time} {Static} {Type} {Checking} for {Dynamic} {Languages}},
	booktitle = {pldi},
	author = {Ren, Brianna M. and Foster, Jeffrey S.},
	year = {2016},
}

@misc{noauthor_computer_2021,
	title = {The {Computer} {Language} {Benchmarks} {Game}},
	url = {https://benchmarksgame-team.pages.debian.net/benchmarksgame/index.html},
	year = {2021},
}

@article{flanagan_componential_1999,
	title = {Componential {Set}-{Based} {Analysis}},
	volume = {21},
	number = {2},
	journal = {toplas},
	author = {Flanagan, Cormac and Felleisen, Matthias},
	year = {1999},
}

@inproceedings{rastogi_safe_2015,
	title = {Safe and {Efficient} {Gradual} {Typing} for {TypeScript}},
	booktitle = {popl},
	author = {Rastogi, Aseem and Swamy, Nikhil and Fournet, Cédric and Bierman, Gavin and Vekris, Panagiotis},
	year = {2015},
}

@inproceedings{bjorner_z_2015,
	title = {ν{Z}: {An} {Optimizing} {SMT} {Solver}},
	booktitle = {tacas},
	author = {Bjørner, Nikolaj and Phan, Anh-Dung and Fleckenstein, Lars},
	year = {2015},
}

@inproceedings{jeremy_g_siek_monotonic_2015,
	title = {Monotonic {References} for {Efficient} {Gradual} {Typing}},
	booktitle = {esop},
	author = {{Jeremy G. Siek} and Vitousek, Michael M. and Cimini, Matteo and Tobin-Hochstadt, Sam and Garcia, Ronald},
	year = {2015},
}

@article{chen_extending_2014,
	title = {Extending {Type} {Inference} to {Variational} {Programs}},
	volume = {36},
	number = {1},
	journal = {toplas},
	author = {Chen, Sheng and Erwig, Martin and Walkingshaw, Eric},
	year = {2014},
}

@article{matthews_operational_2009,
	title = {Operational {Semantics} for {Multi}-{Language} {Programs}},
	volume = {31},
	number = {3},
	journal = {toplas},
	author = {Matthews, Jacob and Findler, Robert Bruce},
	year = {2009},
}

@article{barrett_abstract_2007,
	title = {An {Abstract} {Decision} {Procedure} for a {Theory} of {Inductive} {Data} {Types}},
	volume = {3},
	number = {1–2},
	journal = {Journal on Satisfiability, Boolean Modeling and Computation},
	author = {Barrett, Clark and Shikanian, Iger and Tinelli, Cesare},
	year = {2007},
	pages = {21--46},
}

@misc{noauthor_cloudflare_2020,
	title = {{CloudFlare} {Workers} {Security} {Model}},
	url = {https://developers.cloudflare.com/workers/learning/security-model},
	year = {2020},
}

@misc{noauthor_kubernetes_2020,
	title = {Kubernetes {Github} {Repository}},
	url = {https://github.com/kubernetes/kubernetes},
	year = {2020},
}

@misc{noauthor_kubeless_2020,
	title = {Kubeless {Website}},
	url = {https://kubeless.io/},
	year = {2020},
}

@article{milner_theory_1978,
	title = {A theory of type polymorphism in programming},
	volume = {17},
	number = {3},
	journal = {Journal of Computer and System Sciences},
	author = {Milner, Robin},
	year = {1978},
	pages = {348--375},
}

@misc{noauthor_fastly_2020,
	title = {Fastly {Compute}{\textbackslash}@{Edge}},
	url = {https://docs.fastly.com/products/compute-at-edge},
	year = {2020},
}

@misc{noauthor_fission_2020,
	title = {Fission {Github} {Repository}},
	url = {https://github.com/fission/fission},
	year = {2020},
}

@misc{noauthor_notitle_2020,
	url = {https://docs.fastly.com/products/compute-at-edge},
	year = {2020},
}

@misc{noauthor_azure_2019,
	title = {Azure {Functions} on {Kubernetes} with {KEDA}},
	url = {https://docs.microsoft.com/en-us/azure/azure-functions/functions-kubernetes-keda},
	year = {2019},
}

@inproceedings{shillaker_faasm_2020,
	title = {Faasm: lightweight isolation for efficient stateful serverless computing},
	booktitle = {atc},
	author = {Shillaker, Simon and Pietzuch, Peter},
	year = {2020},
}

@misc{noauthor_fn_2020,
	title = {Fn {Project} {Website}},
	url = {https://fnproject.io/},
	year = {2020},
}

@inproceedings{fuerst_faascache_2021,
	title = {{FaasCache}: {Keeping} {Serverless} {Computing} {Alive} with {Greedy}-{Dual} {Caching}},
	booktitle = {asplos},
	author = {Fuerst, Alexander and Sharma, Prateek},
	year = {2021},
}

@inproceedings{yang_dynamic_2014,
	title = {Dynamic space limits for {Haskell}},
	booktitle = {pldi},
	author = {Yang, Edward Z and Mazières, David},
	year = {2014},
}

@inproceedings{terei_safe_2012,
	title = {Safe haskell},
	booktitle = {haskell},
	author = {Terei, David and Marlow, Simon and Peyton Jones, Simon and Mazières, David},
	year = {2012},
}

@article{kernighan_timing_1998,
	title = {Timing trials, or the trials of timing: experiments with scripting and user-interface languages},
	volume = {28},
	number = {8},
	journal = {Software: Practice and Experience},
	author = {Kernighan, Brian W. and Wyk, Christopher J. Van},
	year = {1998},
	pages = {819--843},
}

@inproceedings{siek_refined_2015,
	title = {Refined {Criteria} for {Gradual} {Typing}},
	booktitle = {snapl},
	author = {Siek, Jeremy and Vitousek, Michael and Cimini, Matteo and Boyland, John},
	year = {2015},
}

@misc{dubey_recognition_2005,
	title = {Recognition, {Mining} and {Synthesis} {Moves} {Computers} to the {Era} of {Tera}},
	author = {Dubey, P.},
	year = {2005},
	note = {Publication Title: Technology{\textbackslash}@Intel Magazine},
}

@book{gabriel_performance_1985,
	address = {USA},
	title = {Performance and {Evaluation} of {LISP} {Systems}},
	isbn = {0-262-07093-6},
	publisher = {Massachusetts Institute of Technology},
	author = {Gabriel, Richard P.},
	year = {1985},
}

@book{graham_ansi_1995,
	title = {{ANSI} {Common} {Lisp}},
	publisher = {Prentice Hall},
	author = {Graham, Paul},
	year = {1995},
}

@inproceedings{flanagan_catching_1996,
	title = {Catching {Bugs} in the {Web} of {Program} {Invariants}},
	booktitle = {pldi},
	author = {Flanagan, Cormac and Flatt, Matthew and Krishnamurthi, Shriram and Weirich, Stephanie and Felleisen, Matthias},
	year = {1996},
}

@inproceedings{madsen_semantics_2020,
	title = {A {Semantics} for the {Essence} of {React}},
	booktitle = {oopsla},
	author = {Madsen, Magnus and Lhoták, Ondrej and Tip, Frank},
	year = {2020},
}

@inproceedings{pavlinovic_finding_2014,
	title = {Finding {Minimum} {Type} {Error} {Sources}},
	booktitle = {oopsla},
	author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas},
	year = {2014},
}

@inproceedings{feldthaus_checking_2014,
	title = {Checking {Correctness} of {TypeScript} {Interfaces} for {JavaScript} {Libraries}},
	booktitle = {oopsla},
	author = {Feldthaus, Asger and Møller, Anders},
	year = {2014},
}

@phdthesis{greenberg_manifest_2013,
	type = {{PhD} {Thesis}},
	title = {Manifest {Contracts}},
	school = {University of Pennsylvania},
	author = {Greenberg, Michael},
	year = {2013},
}

@inproceedings{vitousek_big_2017,
	title = {Big {Types} in {Little} {Runtime}: {Open}-{World} {Soundness} and {Collaborative} {Blame} for {Gradual} {Type} {Systems}},
	booktitle = {popl},
	author = {Vitousek, Michael M. and Swords, Cameron and Siek, Jeremy G.},
	year = {2017},
}

@article{herman_space-efficient_2011,
	title = {Space-efficient gradual typing},
	volume = {23},
	number = {2},
	journal = {hosc},
	author = {Herman, David and Tomb, Aaron and Flanagan, Cormac},
	year = {2011},
	pages = {167--189},
}

@inproceedings{vitousek_design_2014,
	title = {Design and {Evaluation} of {Gradual} {Typing} for {Python}},
	booktitle = {dls},
	author = {Vitousek, Michael M. and Siek, Jeremy G. and Baker, Jim},
	year = {2014},
}

@article{greenman_spectrum_2018,
	title = {A {Spectrum} of {Type} {Soundness} and {Performance}},
	volume = {2},
	number = {ICFP},
	author = {Greenman, Ben and Felleisen, Matthias},
	year = {2018},
}

@inproceedings{greenman_cost_2018,
	title = {On the {Cost} of {Type}-{Tag} {Soundness}},
	booktitle = {pepm},
	author = {Greenman, Ben and Migeed, Zeina},
	year = {2018},
}

@inproceedings{siek_blame_2015,
	title = {Blame and {Coercion}: {Together} {Again} for the {First} {Time}},
	booktitle = {pldi},
	author = {Siek, Jeremy and Thiemann, Peter and Wadler, Philip},
	year = {2015},
}

@inproceedings{swamy_lightweight_2011,
	title = {Lightweight {Monadic} {Programming} in {ML}},
	booktitle = {icfp},
	author = {Swamy, Nikhil and Guts, Nataliya and Leijen, Daan and Hicks, Michael},
	year = {2011},
}

@inproceedings{swamy_theory_2009,
	title = {A {Theory} of {Typed} {Coercions} and {Its} {Applications}},
	booktitle = {icfp},
	author = {Swamy, Nikhil and Hicks, Michael and Bierman, Gavin M.},
	year = {2009},
}

@article{henglein_dynamic_1994,
	title = {Dynamic typing: syntax and proof theory},
	volume = {22},
	number = {3},
	journal = {Science of Computer Programming},
	author = {Henglein, Fritz},
	year = {1994},
	pages = {197--230},
}

@article{campora_casts_2018,
	title = {Casts and {Costs}: {Harmonizing} {Safety} and {Performance} in {Gradual} {Typing}},
	volume = {2},
	number = {ICFP},
	journal = {pacmpl},
	author = {Campora, John Peter and Chen, Sheng and Walkingshaw, Eric},
	year = {2018},
}

@misc{serverless_examples_2020,
	title = {examples},
	url = {https://github.com/serverless/examples},
	publisher = {GitHub},
	author = {{serverless}},
	year = {2020},
	note = {Publication Title: GitHub repository},
}

@misc{noauthor_system_nodate,
	title = {System details and limits},
	url = {https://cloud.ibm.com/docs/openwhisk?topic=openwhisk-limits},
}

@article{huet_zipper_1997,
	title = {The {Zipper}},
	volume = {7},
	number = {5},
	journal = {jfp},
	author = {Huet, Gérard},
	year = {1997},
	pages = {549--554},
}

@misc{shahrad_serverless_2020,
	title = {Serverless in the {Wild}: {Characterizing} and {Optimizing} the {Serverless} {Workload} at a {Large} {Cloud} {Provider}},
	url = {https://arxiv.org/abs/2003.03423},
	author = {Shahrad, Mohammad and Fonseca, Rodrigo and Goiri, Í∼ nigo and Chaudhry, Gohar and Batum, Paul and Cooke, Jason and Laureano, Eduardo and Tresness, Colby and Russinovich, Mark and Bianchini, Ricardo},
	year = {2020},
}

@inproceedings{vandenbroucke_plambdaomegank_2020,
	title = {P\${\textbackslash}lambdaømega\${NK}: {Functional} {Probabilistic} {NetKAT}},
	booktitle = {popl},
	author = {Vandenbroucke, Alexander and Schrijvers, Tom},
	year = {2020},
}

@misc{noauthor_how_nodate,
	title = {How do {I} make my {Lambda} function idempotent to prevent inconsistencies and data loss in my application?},
	url = {https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-idempotent/},
}

@article{goldstein_mbrosi_2020,
	title = {A.{M}.{B}.{R}.{O}.{S}.{I}.{A}: {Providing} {Performant} {Virtual} {Resiliency} for {Distributed} {Applications}},
	volume = {13},
	number = {5},
	journal = {Proceedings of the VLDB Endowment},
	author = {Goldstein, Jonathan and Abdelhamid, Ahmed and Barnett, Mike and Burckhardt, Sebastian and Chandramouli, Badrish and Gehring, Darren and Lebeck, Niel and Meiklejohn, Christopher and Minhas, Umar Farooq and Newton, Ryan and Peshawaria, Rahee Ghosh and Zaccai, Tal and Zhang, Irene},
	year = {2020},
	pages = {588--601},
}

@inproceedings{obetz_formalizing_2020,
	title = {Formalizing {Event}-{Driven} {Behavior} of {Serverless} {Applications}},
	booktitle = {European {Symposium} on {Cloud} {Computing} ({ESOCC})},
	author = {Obetz, Matthew and Das, Anirban and Castiglia, Timothy and Patterson, Stacy and Milanova, Ana},
	year = {2020},
}

@inproceedings{larsen_wnetkat_2016,
	title = {{WNetKAT}: {A} {Weighted} {SDN} {Programming} and {Verification} {Language}},
	booktitle = {International {Conference} on {Principles} of {Distributed} {Systems} ({OPODIS})},
	author = {Larsen, Kim G. and Schmid, Stefan and Xue, Bingtian},
	year = {2016},
}

@inproceedings{webb_blender_2014,
	title = {Blender: {Upgrading} tenant-based data center networking},
	booktitle = {{ACM}/{IEEE} {Symposium} on {Architectures} for {Networking} and {Communications} {Systems}},
	author = {Webb, Kevin C. and Roy, Arjun and Yocum, Kenneth and Snoeren, Alex C.},
	year = {2014},
}

@inproceedings{marcon_predictor_2015,
	title = {Predictor: providing fine-grained management and predictability in multi-tenant datacenter networks},
	booktitle = {{IFIP}/{IEEE} {International} {Symposium} on {Integrated} {Network} {Management} ({IM})},
	author = {Marcon, Daniel S. and Barcellos, Marinho P.},
	year = {2015},
}

@inproceedings{petropoulos_software-defined_2016,
	title = {Software-defined inter-networking: {Enabling} coordinated {QoS} control across the {Internet}},
	booktitle = {International {Conference} on {Telecommunications} ({ICT})},
	author = {Petropoulos, George and Sardis, Fragkiskos and Spirou, Spiros and Mahmoodi, Toktam},
	year = {2016},
}

@inproceedings{gharakheili_third-party_2015,
	title = {Third-party customization of residential {Internet} sharing using {SDN}},
	booktitle = {International {Telecommunication} {Networks} and {Applications} {Conference} ({ITNAC})},
	author = {Gharakheili, Hassan Habibi and Exton, Luke and Sivaraman, Vijay and Matthews, John and Russell, Craig},
	year = {2015},
}

@inproceedings{sonchack_enabling_2016,
	title = {Enabling {Practical} {Software}-defined {Networking} {Security} {Applications} with {OFX}},
	booktitle = {ndss},
	author = {Sonchack, John and Smith, Jonathan M. and Aviv, Adam J. and Keller, Eric},
	year = {2016},
}

@inproceedings{lebrun_software_2018,
	title = {Software resolved networks: {Rethinking} enterprise networks with {IPv6} segment routing},
	booktitle = {sosr},
	author = {Lebrun, David and Jadin, Mathieu and Clad, François and Filsfils, Clarence and Bonaventure, Olivier},
	year = {2018},
}

@article{canini_stn_2014,
	title = {{STN}: {A} robust and distributed {SDN} control plane},
	author = {Canini, Marco and Cicco, Daniele De and Kuznetsov, Petr and Levin, Dan and Schmid, Stefan and Vissicchio, Stefano},
	year = {2014},
}

@inproceedings{kumar_user_2013,
	title = {User control of quality of experience in home networks using {SDN}},
	booktitle = {{IEEE} {International} {Conference} on {Advanced} {Networks} and {Telecommunications} {Systems} ({ANTS})},
	author = {Kumar, Himal and Gharakheili, Hassan Habibi and Sivaraman, Vijay},
	year = {2013},
}

@inproceedings{schulz-zander_opensdwn_2015,
	title = {{OpenSDWN}: {Programmatic} control over home and enterprise {WiFi}},
	booktitle = {sosr},
	author = {Schulz-Zander, Julis and Mayer, {and} Carlos and Ciobotaru, Bogdan and Schmid, Stefan and Feldmann, Anja},
	year = {2015},
}

@misc{amazon_web_services_serverless_2019,
	title = {Serverless {Application} {Repository}},
	url = {https://aws.amazon.com/serverless/serverlessrepo/. Accessed Nov 4 2019},
	author = {{Amazon Web Services}},
	year = {2019},
}

@article{abadi_dynamic_1995,
	title = {Dynamic typing in polymorphic languages},
	volume = {5},
	number = {1},
	journal = {jfp},
	author = {Abadi, Martin and Cardelli, Luca and Pierce, Benjamin C. and Rémy, Didier},
	year = {1995},
	pages = {111--130},
}

@misc{pytorch_contributors_torchscript_2018,
	title = {{TorchScript}},
	url = {https://pytorch.org/docs/master/jit.html. Accessed Nov 2 2019},
	author = {{PyTorch Contributors}},
	year = {2018},
}

@inproceedings{palkar_evaluating_2018,
	title = {Evaluating {End}-to-{End} {Optimization} for {Data} {Analytics} {Applications} in {Weld}},
	booktitle = {International {Conference} on {Very} {Large} {Data} {Bases} ({VLDB})},
	author = {Palkar, Shoumik and Thomas, James and Narayanan, Deepak and Thaker, Pratiksha and Negi, Parimarjan and Palamuttam, Rahul and Shanbhag, Anil and Pirk, Holger and Schwarzkopf, Malte and Amarasinghe, Saman and Madden, Samuel and Zaharia, Matei},
	year = {2018},
}

@inproceedings{oakes_sock_2018,
	title = {{SOCK}: {Rapid} {Task} {Provisioning} with {Serverless}-{Optimized} {Containers}},
	booktitle = {atc},
	author = {Oakes, Edward and Yang, Leon and Zhou, Dennis and Houck, Kevin and Harter, Tyler and Arpaci-Dusseau, Andrea and Arpaci-Dusseau, Remzi},
	year = {2018},
}

@article{wright_practical_1997,
	title = {A {Practical} {Soft} {Type} {System} for {Scheme}},
	volume = {19},
	number = {1},
	journal = {toplas},
	author = {Wright, Andrew K. and Cartwright, Robert},
	year = {1997},
	pages = {87--152},
}

@inproceedings{milano_mixt_2018,
	title = {{MixT}: a language for mixing consistency in geodistributed transactions},
	booktitle = {booktitle},
	author = {Milano, Matthew P. and Myers, Andrew C.},
	year = {2018},
}

@inproceedings{lee_evaluation_2018,
	title = {Evaluation of production serverless computing environments},
	booktitle = {International {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Lee, Hyungro and Satyam, Kumar and Fox, Geoffrey},
	year = {2018},
}

@inproceedings{lam_numba_2015,
	title = {Numba: {A} {LLVM}-based {Python} {JIT} {Compiler}},
	booktitle = {{LLVM} {Compiler} {Infrastructure} in {HPC} ({LLVM})},
	author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
	year = {2015},
}

@inproceedings{wurthinger_practical_2017,
	title = {Practical partial evaluation for high-performance dynamic language runtimes},
	booktitle = {pldi},
	author = {Würthinger, Thomas and Wimmer, Christian and Humer, Christian and Wös, Andreas and Stadler, Lukas and Seaton, Chris and Duboscq, Gilles and Simon, Doug and Grimmer, Matthias},
	year = {2017},
}

@article{bolz_impact_2015,
	title = {The impact of meta-tracing on {VM} design and implementation},
	journal = {The Science of Computer Programming},
	author = {Bolz, Carl Friedrich and Tratt, Laurence},
	month = feb,
	year = {2015},
	pages = {408--421},
}

@phdthesis{ellis_bulldog_1985,
	type = {{PhD} {Thesis}},
	title = {Bulldog: {A} {Compiler} for {VLIW} {Architectures}},
	school = {Yale University},
	author = {Ellis, John R.},
	year = {1985},
}

@inproceedings{gan_open-source_2019,
	title = {An {Open}-{Source} {Benchmark} {Suite} for {Microservices} and {Their} {Hardware}-{Software} {Implications} for {Cloud} and {Edge} {Systems}},
	booktitle = {asplos},
	author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayantara and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and He, Yuan and Delimitrou, Christina},
	year = {2019},
}

@inproceedings{seltzer_dealing_1996,
	title = {Dealing with {Disaster}: {Surviving} {Misbehaved} {Kernel} {Extensions}},
	booktitle = {osdi},
	author = {Seltzer, Margo I. and Endo, Yasuhiro and Small, Christopher and Smith, Keith A.},
	year = {1996},
}

@inproceedings{bershad_extensibility_1995,
	title = {Extensibility, {Safety} and {Performance} in the {SPIN} {Operating} {System}},
	booktitle = {sosp},
	author = {Bershad, Brian and Savage, Stefan and Pardyak, Przemyslaw and Sirer, Emin Gun and Becker, David and Fiuczynski, Marc and Chambers, Craig and Eggers, Susan},
	year = {1995},
}

@inproceedings{hunt_sealing_2007,
	title = {Sealing {OS} {Processes} to {Improve} {Dependability} and {Safety}},
	booktitle = {eurosys},
	author = {Hunt, Galen and Aiken, Mark and Fahndrich, Manuel and Hawblitzel, Chris and Hodson, Orion and Larus, Jim and Levi, Steven and Steensgaard, Bjarne and Tarditi, David and Wobber, Ted},
	year = {2007},
}

@inproceedings{kocher_spectre_2019,
	title = {Spectre attacks: {Exploiting} speculative execution},
	booktitle = {oakland},
	author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
	year = {2019},
}

@misc{noauthor_stack_nodate,
	title = {Stack {Overflow} {Developer} {Survey}},
	url = {https://insights.stackoverflow.com/survey/2019},
}

@misc{ganesan_c_nodate,
	title = {C\# or {Java}? {TypeScript} or {JavaScript}? {Machine} learning based classification of programming languages},
	url = {https://github.blog/2019-07-02-c-or-java-typescript-or-javascript-machine-learning-based-classification-of-programming-languages},
	author = {Ganesan, Kavita},
}

@inproceedings{jung_rustbelt_2018,
	title = {{RustBelt}: {Securing} the {Foundations} of the {Rust} {Programming} {Language}},
	booktitle = {popl},
	author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
	year = {2018},
}

@inproceedings{holzle_optimizing_1994,
	title = {Optimizing {Dynamically}-{Dispatched} {Calls} with {Run}-{Time} {Type} {Feedback}},
	booktitle = {pldi},
	author = {Hölzle, Urs and Ungar, David},
	year = {1994},
}

@inproceedings{barrett_virtual_2017,
	title = {Virtual {Machine} {Warmup} {Blows} {Hot} and {Cold}},
	booktitle = {oopsla},
	author = {Barrett, Edd and Bolz-Tereick, Carl Friedric and Killick, Rebecca and Knight, Vincent and Mount, Sarah and Tratt, Laurence},
	year = {2017},
}

@inproceedings{shahrad_architectural_2019,
	title = {Architectural {Implications} of {Function}-as-a-{Service} {Computing}},
	booktitle = {{EEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Shahrad, Mohammad and Balkind, Jonathan and Wentzlff, David},
	year = {2019},
}

@inproceedings{dean_selective_1995,
	title = {Selective {Specialization} for {Object}-{Oriented} {Languages}},
	booktitle = {pldi},
	author = {Dean, Jeffrey and Chambers, Craig and Grove, David},
	year = {1995},
}

@inproceedings{deutsch_efficient_1983,
	title = {Efficient {Implementation} of the {Smalltalk}-80 {System}},
	booktitle = {popl},
	author = {Deutsch, L. Peter and Schiffman, Allan M.},
	year = {1983},
}

@inproceedings{brown_finding_2017,
	title = {Finding and {Preventing} {Bugs} in {JavaScript} {Bindings}},
	booktitle = {oakland},
	author = {Brown, Fraser and Narayan, Shravan and Wahby, Riad S. and Engler, Dawson and Jhala, Ranjit and Stefan, Deian},
	year = {2017},
}

@misc{team_rustonomicon_2019,
	title = {The {Rustonomicon}: {Lifetimes}},
	url = {https://doc.rust-lang.org/1.9.0/book/lifetimes.html},
	author = {team, The Rust},
	year = {2019},
}

@misc{team_rustonomicon_2019-1,
	title = {The {Rustonomicon}: {References} and {Borrowing}},
	url = {https://doc.rust-lang.org/1.9.0/book/references-and-borrowing.html},
	author = {team, The Rust},
	year = {2019},
}

@misc{team_rustonomicon_2019-2,
	title = {The {Rustonomicon}: {Ownership}},
	url = {https://doc.rust-lang.org/1.9.0/book/ownership.html},
	author = {team, The Rust},
	year = {2019},
}

@misc{team_rustonomicon_2019-3,
	title = {The {Rustonomicon}: {Aliasing}},
	url = {https://doc.rust-lang.org/nomicon/aliasing.html},
	author = {team, The Rust},
	year = {2019},
}

@misc{team_rustonomicon_2019-4,
	title = {The {Rustonomicon}: {References}},
	url = {https://doc.rust-lang.org/nomicon/references.html},
	author = {team, The Rust},
	year = {2019},
}

@inproceedings{serrano_javascript_2018,
	title = {{JavaScript} {AOT} {Compilation}},
	booktitle = {dls},
	author = {Serrano, Manuel},
	year = {2018},
}

@article{gan_leveraging_2019,
	title = {Leveraging {Deep} {Learning} to {Improve} {Performance} {Predictability} in {Cloud} {Microservices} with {Seer}},
	volume = {53},
	number = {1},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Gan, Yu and Zhang, Yanqi and Hu, Kelvin and Cheng, Dailun and He, Yuan and Pancholi, Meghna and Delimitrou, Christina},
	year = {2019},
	note = {Publisher: ACM},
	pages = {34--39},
}

@inproceedings{kannan_grandslam_2019,
	title = {{GrandSLAm}: {Guaranteeing} {SLAs} for jobs in microservices execution frameworks},
	booktitle = {eurosys},
	author = {Kannan, Ram Srivatsa and Subramanian, Lavanya and Raju, Ashwin and Ahn, Jeongseob and Mars, Jason and Tang, Lingjia},
	year = {2019},
}

@inproceedings{boucher_putting_2018,
	title = {Putting the “{Micro}” back in microservices},
	booktitle = {atc},
	author = {Boucher, Sol and Kalia, Anuj and Andersen, David G and Kaminsky, Michael},
	year = {2018},
}

@book{jones_garbage_1996,
	title = {Garbage {Collection}},
	publisher = {John Wiley and Sons},
	author = {Jones, Richard},
	year = {1996},
}

@article{taibi_definition_2018,
	title = {On the definition of microservice bad smells},
	volume = {35},
	number = {3},
	journal = {IEEE software},
	author = {Taibi, Davide and Lenarduzzi, Valentina},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {56--62},
}

@inproceedings{gan_seer_2019,
	title = {Seer: {Leveraging} {Big} {Data} to {Navigate} the {Complexity} of {Performance} {Debugging} in {Cloud} {Microservices}},
	booktitle = {Proceedings of the {Twenty} {Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems} ({ASPLOS})},
	author = {Gan, Yu and Zhang, Yanqi and Hu, Kelvin and He, Yuan and Pancholi, Meghna and Cheng, Dailun and Delimitrou, Christina},
	year = {2019},
}

@inproceedings{elgamal_costless_2018,
	title = {Costless: {Optimizing} cost of serverless computing through function fusion and placement},
	booktitle = {2018 {IEEE}/{ACM} {Symposium} on {Edge} {Computing} ({SEC})},
	author = {Elgamal, Tarek},
	year = {2018},
}

@inproceedings{bebenita_spur_2010,
	title = {{SPUR}: {A} {Trace}-based {JIT} {Compiler} for {CIL}},
	booktitle = {oopsla},
	author = {Bebenita, Michael and Brandner, Florian and Fahndrich, Manuel and Logozzo, Francesco and Schulte, Wolfram and Tillmann, Nikolai and Venter, Herman},
	year = {2010},
}

@inproceedings{gal_trace-based_2009,
	title = {Trace-based {Just}-in-time {Type} {Specialization} for {Dynamic} {Languages}},
	booktitle = {pldi},
	author = {Gal, Andreas and Eich, Brendan and Shaver, Mike and Anderson, David and Mandelin, David and Haghighat, Mohammad R. and Kaplan, Blake and Hoare, Graydon and Zbarsky, Boris and Orendorff, Jason and Ruderman, Jesse and Smith, Edwin W. and Reitmaier, Rick and Bebenita, Michael and Chang, Mason and Franz, Michael},
	year = {2009},
}

@misc{noauthor_behavior_2019,
	title = {Behavior {Not} {Considered} {Unsafe}},
	url = {https://doc.rust-lang.org/reference/behavior-not-considered-unsafe.html. Accessed Nov 3 2019},
	year = {2019},
}

@techreport{bernstein_orleans_2014,
	title = {Orleans: {Distributed} {Virtual} {Actors} for {Programmability} and {Scalability}},
	url = {https://www.microsoft.com/en-us/research/publication/orleans-distributed-virtual-actors-for-programmability-and-scalability/},
	author = {Bernstein, Phil and Bykov, Sergey and Geller, Alan and Thelin, Jorgen},
	year = {2014},
}

@misc{foundation_python_2018,
	title = {Python object serialization},
	url = {https://docs.python.org/3.7/library/pickle.html},
	author = {Foundation, Python Software},
	year = {2018},
}

@inproceedings{miller_instant_2013,
	title = {Instant {Pickles}: {Generating} {Object}-oriented {Pickler} {Combinators} for {Fast} and {Extensible} {Serialization}},
	booktitle = {oopsla},
	author = {Miller, Heather and Haller, Philipp and Burmako, Eugene and Odersky, Martin},
	year = {2013},
}

@inproceedings{walker_static_2006,
	title = {Static {Typing} for a {Faulty} {Lambda} {Calculus}},
	booktitle = {Proceedings of the {Eleventh} {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	author = {Walker, David and Mackey, Lester and Ligatti, Jay and Reis, George A. and August, David I.},
	year = {2006},
}

@inproceedings{orkqvist_resource_2016,
	title = {Resource management of replicated service systems provisioned in the cloud},
	booktitle = {Network {Operations} and {Management} {Symposium} ({NOMS})},
	author = {orkqvist, Mathias Bj{\textbackslash}" and Birke, Robert and Binder, Walter},
	year = {2016},
}

@inproceedings{gan_architectural_2018,
	title = {The {Architectural} {Implications} of {Cloud} {Microservices}},
	booktitle = {Computer {Architecture} {Letters} ({CAL})},
	author = {Gan, Yu and Delimitrou, Christina},
	year = {2018},
}

@misc{fouladi_laptop_2019,
	title = {From {Laptop} to {Lambda}: {Outsourcing} {Everyday} {Jobs} to {Thousands} of {Transient} {Functional} {Containers}},
	author = {Fouladi, Sadjad and Romero, {and} Francisco and Iter, Dan and Li, Qian and Chatterjee, Shuvo and Kozyrakis, Christos and Zaharia, Matei and Winstein, Keith},
	year = {2019},
	note = {Publication Title: atc},
}

@inproceedings{ramalingam_fault_2013,
	title = {Fault {Tolerance} via {Idempotence}},
	booktitle = {popl},
	author = {Ramalingam, Ganesan and Vaswani, Kapil},
	year = {2013},
}

@inproceedings{bracevac_cpl_2016,
	title = {{CPL}: {A} {Core} {Language} for {Cloud} {Computing}},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Modularity}},
	author = {Bračevac, Oliver and Erdweg, Sebastian and Salvaneschi, Guido and Mezini, Mira},
	year = {2016},
}

@misc{inc_java_2018,
	title = {Java {Object} {Serialization} {Specification}},
	url = {https://docs.oracle.com/javase/10/docs/specs/serialization/serial-arch.html},
	author = {Inc, Oracle},
	year = {2018},
}

@inproceedings{shen_x-containers_2019,
	title = {X-{Containers}: {Breaking} {Down} {Barriers} to {Improve} {Performance} and {Isolation} of {Cloud}-{Native} {Containers}},
	booktitle = {{ASPLOS}},
	author = {Shen, Zhiming and Sun, Zhen and Sela, Gur-Eyal and Bagdasaryan, Eugene and Delimitrou, Christina and Renesse, Van Robbert and Weatherspoon, Hakin},
	year = {2019},
}

@misc{noauthor_pulumi_2018,
	title = {Pulumi. {Cloud} {Native} {Infrastructure} as {Code}},
	url = {https://www.pulumi.com/. Accessed Oct 12 2019},
	year = {2018},
}

@inproceedings{gomes_verifying_2017,
	title = {Verifying {Strong} {Eventual} {Consistency} in {Distributed} {Systems}},
	booktitle = {oopsla},
	author = {Gomes, Victor B. F. and Kleppmann, Martin and Mulligan, Dominic P. and Beresford, Alastair R.},
	year = {2017},
}

@inproceedings{chand_formal_2016,
	title = {Formal {Verification} of {Multi}-{Paxos} for {Distributed} {Consensus}},
	booktitle = {fm},
	author = {Chand, Saksham and Liu, Yanhong A. and Stoller, Scott D.},
	year = {2016},
}

@inproceedings{sergey_programming_2017,
	title = {Programming and {Proving} with {Distributed} {Protocols}},
	booktitle = {popl},
	author = {Sergey, Ilya and Wilcox, James R. and Tatlock, Zachary},
	year = {2017},
}

@inproceedings{weerawarana_bringing_2018,
	title = {Bringing {Middleware} to {Everyday} {Programmers} with {Ballerina}},
	booktitle = {Business {Process} {Management}},
	author = {Weerawarana, Sanjiva and Ekanayake, Chathura and Perera, Srinath and Leymann, Frank},
	year = {2018},
}

@inproceedings{chajed_verifying_2018,
	title = {Verifying concurrent software using movers in {CSPEC}},
	booktitle = {osdi},
	author = {Chajed, Tej and Kaashoek, Frans and Lampson, Butler and Zeldovich, Nickolai},
	year = {2018},
}

@inproceedings{hawblitzel_ironfleet_2015,
	title = {{IronFleet}: {Proving} {Practical} {Distributed} {Systems} {Correct}},
	booktitle = {sosp},
	author = {Hawblitzel, Chris and Howell, Jon and Kapritsos, Manos and Lorch, Jacob R. and Parno, Bryan and Roberts, Michael L. and Setty, Srinath and Zill, Brian},
	year = {2015},
}

@inproceedings{fonseca_empirical_2017,
	title = {An {Empirical} {Study} on the {Correctness} of {Formally} {Verified} {Distributed} {Systems}},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Computer} {Systems}},
	author = {Fonseca, Pedro and Zhang, Kaiyuan and Wang, Xi and Krishnamurthy, Arvind},
	year = {2017},
}

@inproceedings{panda_verification_2017,
	title = {Verification in the {Age} of {Microservices}},
	booktitle = {Workshop on {Hot} {Topics} in {Operating} {Systems}},
	author = {Panda, Aurojit and Sagiv, Mooly and Shenker, Scott},
	year = {2017},
}

@inproceedings{woos_planning_2016,
	title = {Planning for {Change} in a {Formal} {Verification} of the {Raft} {Consensus} {Protocol}},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Certified} {Programs} and {Proofs}},
	author = {Woos, Doug and Wilcox, James R. and Anton, Steve and Tatlock, Zachary and Ernst, Michael D. and Anderson, Thomas},
	year = {2016},
}

@inproceedings{dragoi_psync_2016,
	title = {{PSync}: {A} {Partially} {Synchronous} {Language} for {Fault}-tolerant {Distributed} {Algorithms}},
	booktitle = {popl},
	author = {Drăgoi, Cezara and Henzinger, Thomas A. and Zufferey, Damien},
	year = {2016},
}

@inproceedings{desai_compositional_2018,
	title = {Compositional {Programming} and {Testing} of {Dynamic} {Distributed} {Systems}},
	booktitle = {oopsla},
	author = {Desai, Ankush and Phanishayee, Amar and Qadeer, Shaz and Seshia, Sanjit A.},
	year = {2018},
}

@misc{noauthor_openwhisk_2018,
	title = {{OpenWhisk} {Actions}},
	url = {https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md. Accessed Oct 12 2019},
	year = {2018},
}

@misc{noauthor_cloud_2018,
	title = {Cloud {Functions} {Execution} {Environment}},
	url = {https://cloud.google.com/functions/docs/concepts/exec. Accessed Oct 12 2019},
	year = {2018},
}

@inproceedings{bakst_verifying_2017,
	title = {Verifying {Distributed} {Programs} via {Canonical} {Sequentialization}},
	booktitle = {oopsla},
	author = {Bakst, Alexander and Gleissenthall, Klaus v. and K, Rami Gökhan and Jhala, Ranjit},
	year = {2017},
}

@misc{noauthor_choose_2018,
	title = {Choose between {Azure} services that deliver messages},
	url = {https://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services. Accessed Oct 12 2019},
	year = {2018},
}

@inproceedings{baudart_protecting_2018,
	title = {Protecting {Chatbots} from {Toxic} {Content}},
	booktitle = {{ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	author = {Baudart, Guillaume and Dolby, Julian and Duesterwald, Evelyn and Hirzel, Martin and Shinnar, Avraham},
	year = {2018},
}

@inproceedings{ao_sprocket_2018,
	title = {Sprocket: {A} {Serverless} {Video} {Processing} {Framework}},
	booktitle = {socc},
	author = {Ao, Lixiang and Izhikevich, Liz and Voelker, Geoffrey M. and Porter, George},
	year = {2018},
}

@misc{noauthor_aws_2018,
	title = {{AWS} {Lambda} {Developer} {Guide}: {Invoke}},
	url = {https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html. Accessed Oct 12 2019},
	year = {2018},
}

@inproceedings{klimovic_pocket_2018,
	title = {Pocket: {Elastic} {Ephemeral} {Storage} for {Serverless} {Analytics}},
	booktitle = {osdi},
	author = {Klimovic, Ana and Wang, Yawen and Stuedi, Patrick and Trivedi, Animesh and Pfefferle, Jonas and Kozyrakis, Christos},
	year = {2018},
}

@inproceedings{aske_supporting_2018,
	title = {Supporting {Multi}-{Provider} {Serverless} {Computing} on the {Edge}},
	booktitle = {International {Conference} on {Parallel} {Processing} ({ICPP})},
	author = {Aske, Austin and Zhao, Xinghui},
	year = {2018},
}

@inproceedings{singhvi_granular_2017,
	title = {Granular {Computing} and {Network} {Intensive} {Applications}: {Friends} or {Foes}?},
	booktitle = {hotnets},
	author = {Singhvi, Arjun and Banerjee, Sujata and Harchol, Yotam and Akella, Aditya and Peek, Mark and Rydin, Pontus},
	year = {2017},
}

@inproceedings{sampe_data-driven_2017,
	title = {Data-driven {Serverless} {Functions} for {Object} {Storage}},
	booktitle = {middleware},
	author = {Sampé, Josep and Sánchez-Artigas, Marc and García-López, Pedro and París, Gerard},
	year = {2017},
}

@inproceedings{felleisen_control_1986,
	title = {Control {Operators}, the {SECD}-{Machine}, and the λ-{Calculus}},
	booktitle = {Proceedings of the {IFIP} {TC} 2/{WG} 2.2 {Working} {Conference} on {Formal} {Description} of {Programming} {Concepts}},
	author = {Felleisen, Matthias and Friedman, Daniel P.},
	year = {1986},
}

@inproceedings{raychev_predicting_2015,
	title = {Predicting {Program} {Properties} from “{Big} {Code}”},
	booktitle = {popl},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	year = {2015},
}

@article{foster_combinators_2007,
	title = {Combinators for {Bidirectional} {Tree} {Transformations}: {A} {Linguistic} {Approach} to the {View}-update {Problem}},
	volume = {29},
	number = {3},
	journal = {toplas},
	author = {Foster, J. Nathan and Greenwald, Michael B. and Moore, Jonathan T. and Pierce, Benjamin C. and Schmitt, Alan},
	month = may,
	year = {2007},
}

@inproceedings{hong_go_2018,
	title = {Go {Serverless}: {Securing} {Cloud} via {Serverless} {Design} {Patterns}},
	booktitle = {hotcloud},
	author = {Hong, Sanghyun and Srivastava, Abhinav and Shambrook, William and Dumitras, Tudor},
	year = {2018},
}

@inproceedings{paterson_new_2001,
	title = {A {New} {Notation} for {Arrows}},
	booktitle = {icfp},
	author = {Paterson, Ross},
	year = {2001},
}

@article{alpernas_secure_2018,
	title = {Secure {Serverless} {Computing} {Using} {Dynamic} {Information} {Flow} {Control}},
	volume = {2},
	number = {OOPSLA},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Alpernas, Kalev and Flanagan, Cormac and Fouladi, Sadjad and Ryzhyk, Leonid and Sagiv, Mooly and Schmitz, Thomas and Winstein, Keith},
	month = oct,
	year = {2018},
}

@misc{rabbah_composing_2017,
	title = {Composing {Functions} into {Applications} the {Serverless} {Way}},
	url = {https://medium.com/openwhisk/composing-functions-into-applications-70d3200d0fac. Accessed Oct 12 2019},
	author = {Rabbah, Rodric},
	year = {2017},
}

@misc{rabbah_serverless_2018,
	title = {A {Serverless} {Composition} of {Functions}},
	url = {https://medium.com/openwhisk/a-serverless-composition-of-functions-59b6743d3835. Accessed Oct 12 2019},
	author = {Rabbah, Rodric},
	year = {2018},
}

@inproceedings{wang_peeking_2018,
	title = {Peeking {Behind} the {Curtains} of {Serverless} {Platforms}},
	booktitle = {atc},
	author = {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
	year = {2018},
}

@inproceedings{pina_rubah_2014,
	title = {Rubah: {DSU} for {Java} on a stock {JVM}},
	booktitle = {oopsla},
	author = {Pina, Luís and Veiga, Luís and Hicks, Michael},
	year = {2014},
}

@misc{noauthor_amazon_2014,
	title = {Amazon {Web} {Services} {Lambda}},
	url = {https://aws.amazon.com/lambda/. Accessed Oct 12 2019},
	year = {2014},
}

@inproceedings{arnold_ksplice_2009,
	title = {Ksplice: {Automatic} {Rebootless} {Kernel} {Updates}},
	booktitle = {eurosys},
	author = {Arnold, Jeff and Kaashoek, M. Frans},
	year = {2009},
}

@inproceedings{hayden_kitsune_2012,
	title = {Kitsune: {Efficient}, {General}-purpose {Dynamic} {Software} {Updating} for {C}},
	booktitle = {oopsla},
	author = {Hayden, Christopher M. and Smith, Edward K. and Denchev, Michail and Hicks, Michael and Foster, Jeffrey S.},
	year = {2012},
}

@inproceedings{ajmani_modular_2006,
	title = {Modular {Software} {Upgrades} for {Distributed} {Systems}},
	booktitle = {ecoop},
	author = {Ajmani, Sameer and Liskov, Barbara and Shrira, Liuba},
	year = {2006},
}

@article{kiselyov_delimited_2012,
	title = {Delimited control in {OCaml}, abstractly and concretely},
	volume = {435},
	journal = {Theoretical Computer Science},
	author = {Kiselyov, Oleg},
	year = {2012},
	pages = {56--76},
}

@inproceedings{jensen_stateless_2015,
	title = {Stateless {Model} {Checking} of {Event}-{Driven} {Applications}},
	booktitle = {oopsla},
	author = {Jensen, Casper S. and Møller, Anders and Raychev, Veselin and Dimitrov, Dimitar and Vechev, Martin},
	year = {2015},
}

@inproceedings{akkus_sand_2018,
	title = {{SAND}: {Towards} {High}-{Performance} {Serverless} {Computing}},
	booktitle = {atc},
	author = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
	year = {2018},
}

@inproceedings{waye_whip_2017,
	title = {Whip: {Higher}-{Order} {Contracts} for {Modern} {Services}},
	booktitle = {icfp},
	author = {Waye, Lucas and Chong, Stephen and Dimoulas, Christos},
	year = {2017},
}

@inproceedings{findler_contracts_2002,
	title = {Contracts for {Higher}-{Order} {Functions}},
	booktitle = {icfp},
	author = {Findler, Robert Bruce and Felleisen, Matthias},
	year = {2002},
}

@misc{dolan_jq_2018,
	title = {jq: a lightweight and flexible command-line {JSON} processor},
	url = {https://stedolan.github.io/jq/. Accessed Oct 12 2019},
	author = {Dolan, Stephen},
	year = {2018},
}

@inproceedings{jonas_occupy_2017,
	title = {Occupy the {Cloud}: {Distributed} {Computing} for the 99\%},
	booktitle = {Symposium on {Cloud} {Computing}},
	author = {Jonas, Eric and Pu, Qifan and Venkataraman, Shivaram and Stoica, Ion and Recht, Benjamin},
	year = {2017},
}

@inproceedings{fouladi_encoding_2017,
	title = {Encoding, {Fast} and {Slow}: {Low}-{Latency} {Video} {Processing} {Using} {Thousands} of {Tiny} {Threads}},
	booktitle = {nsdi},
	author = {Fouladi, Sadjad and Wahby, Riad S. and Shacklett, Brennan and Balasubramaniam, Karthikeyan Vasuki and Zeng, William and Bhalerao, Rahul and Sivaraman, Anirudh and Porter, George and Winstein, Keith},
	year = {2017},
}

@misc{noauthor_microsoft_2018,
	title = {Microsoft {Azure} {Functions}},
	url = {https://azure.microsoft.com/en-us/services/functions/. Accessed Oct 12 2019},
	year = {2018},
}

@misc{noauthor_google_2018,
	title = {Google {Cloud} {Functions}},
	url = {https://cloud.google.com/functions/. Accessed Oct 12 2019},
	year = {2018},
}

@misc{ellis_openfaas_2018,
	title = {{OpenFaaS}},
	url = {https://www.openfaas.com. Accessed Oct 12 2019},
	author = {Ellis, Alex},
	year = {2018},
}

@misc{noauthor_apache_2018,
	title = {Apache {OpenWhisk}},
	url = {https://openwhisk.apache.org. Accessed Oct 12 2019},
	year = {2018},
}

@inproceedings{baldini_serverless_2017,
	title = {The {Serverless} {Trilemma}: {Function} {Composition} for {Serverless} {Computing}},
	booktitle = {{ACM} {SIGPLAN} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software} ({Onward}!)},
	author = {Baldini, Ioana and Cheng, Perry and Fink, Stephen J. and Mitchell, Nick and Muthusamy, Vinod and Rabbah, Rodric and Suter, Philippe and Tardieu, Olivier},
	year = {2017},
}

@inproceedings{luu_making_2016,
	title = {Making {Smart} {Contracts} {Smarter}},
	booktitle = {ccs},
	author = {Luu, Loi and Chu, Duc-Hiep and Olickel, Hrishi and Saxena, Prateek and Hobor, Aquinas},
	year = {2016},
}

@inproceedings{kiselyov_embedded_2009,
	title = {Embedded {Probabilistic} {Programming}},
	booktitle = {Domain-{Specific} {Languages}},
	author = {Kiselyov, Oleg and Shan, Chung-chieh},
	year = {2009},
}

@inproceedings{wilcox_verdi_2015,
	title = {Verdi: {A} framework for implementing and formally verifying distributed systems},
	booktitle = {pldi},
	author = {Wilcox, James R. and Woos, Doug and Panchekha, Pavel and Tatlock, Zachary and Wang, Xi and Ernst, Michael D. and Anderson, Thomas},
	year = {2015},
}

@inproceedings{hendrickson_serverless_2016,
	title = {Serverless computation with {OpenLambda}},
	booktitle = {{USENIX} {Workshop} on {Hot} {Topics} in {Cloud} {Computing} ({HotCloud})},
	author = {Hendrickson, Scott and Sturdevant, Stephen and Harter, Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
	year = {2016},
}

@article{cejtin_higher-order_1995,
	title = {Higher-order {Distributed} {Objects}},
	volume = {17},
	number = {5},
	journal = {toplas},
	author = {Cejtin, Henry and Jagannathan, Suresh and Kelsey, Richard},
	month = sep,
	year = {1995},
	pages = {704--739},
}

@misc{nadkarni_quantifying_2017,
	title = {Quantifying {Datacenter} {Inefficiency}: {Making} the {Case} for {Composable} {Infrastructure}},
	url = {https://www.hpe.com/us/en/resources/integrated-systems/case-for-composable.html},
	author = {Nadkarni, Ashish},
	year = {2017},
}

@misc{noauthor_bool_2018,
	title = {Bool: {Comprehensive} computer science platform for universities, community colleges, and individual learners},
	url = {https://www.bool.com/. Accessed Oct 12 2019},
	year = {2018},
}

@inproceedings{nelson_exodus_2015,
	title = {Exodus: {Toward} {Automatic} {Migration} of {Enterprise} {Network} {Configurations} to {SDNs}},
	booktitle = {sosr},
	author = {Nelson, Tim and Ferguson, Andrew D. and Yu, Da and Fonseca, Rodrigo and Krishnamurthi, Shriram},
	year = {2015},
}

@misc{conway_cloud_2017,
	title = {Cloud {Native} {Technologies} {Are} {Scaling} {Production} {Applications}},
	url = {https://www.cncf.io/blog/2017/12/06/cloud-native-technologies-scaling-production-applications/},
	publisher = {Cloud Native Computing Foundation},
	author = {Conway, Sarah},
	year = {2017},
}

@article{hughes_generalising_2000,
	title = {Generalising {Monads} to {Arrows}},
	volume = {37},
	number = {1–3},
	journal = {Science of Computer Programming},
	author = {Hughes, John},
	month = may,
	year = {2000},
	pages = {67--111},
}

@inproceedings{nelson_static_2015,
	title = {Static {Differential} {Program} {Analysis} for {Software}-{Defined} {Networks}},
	booktitle = {International {Symposium} on {Formal} {Methods}},
	author = {Nelson, Tim and Ferguson, Andrew D. and Krishnamurthi, Shriram},
	year = {2015},
}

@article{santos_javert_2018,
	title = {{JaVerT}: {JavaScript} verification toolchain},
	volume = {2},
	number = {POPL},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Santos, José Fragoso and Maksimović, Petar and Naudžiūnien{\textbackslash}.e, Daiva and Wood, Thomas and Gardner, Philippa},
	year = {2018},
	pages = {50:1--50:33},
}

@mastersthesis{schuster_reification_2012,
	title = {Reification of {Execution} {State} in {JavaScript}},
	school = {University of Potsdam Germany},
	author = {Schuster, Christopher},
	year = {2012},
}

@inproceedings{zhu_measuring_2018,
	title = {Measuring and {Disrupting} {Anti}-{Adblockers} {Using} {Differential} {Execution} {Analysis}},
	booktitle = {ndss},
	author = {Zhu, Shitong and Hu, Xunchao and Qian, Zhiyun and Shafiq, Zubair and Yin, Heng},
	year = {2018},
}

@misc{noauthor_quasar_2017,
	title = {Quasar: {Lightweight} {Threads} and {Actors} for the {JVM}},
	url = {http://blog.paralleluniverse.co/2014/02/06/fibers-threads-strands/. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_kotlin_2017,
	title = {Kotlin {Coroutines}},
	url = {https://github.com/Kotlin/kotlin-coroutines},
	year = {2017},
}

@article{hernandez_cala_2014,
	title = {{CALA}: {An} unsupervised {URL}-based web page classification system},
	volume = {57},
	journal = {Journal of Knowledge Based Systems},
	author = {Hernández, Inma and Rivero, Carlos R. and Ruiz, David and Corchuelo, Rafael},
	year = {2014},
	pages = {168--180},
}

@article{szczepanski_automated_2013,
	title = {An {Automated} {Framework} with {Application} to {Study} {URL} {Based} {Online} {Advertisements} {Detection}},
	volume = {9},
	number = {1},
	journal = {Journal of Applied Mathematics, Statistics and Informatics (JAMSI)},
	author = {Szczepański, Piotr L. and Wiśniewski, Adrian and Gerszberg, Tomasz},
	year = {2013},
	pages = {47--60},
}

@inproceedings{shih_using_2004,
	title = {Using {URLs} and {Table} {Layout} for {Web} {Classification} {Tasks}},
	booktitle = {www},
	author = {Shih, Lawrence Kai and Karger, David R.},
	year = {2004},
}

@inproceedings{zhang_lightweight_2017,
	title = {A {Lightweight} {Online} {Advertising} {Classification} {System} using {Lexical}-based {Features}},
	booktitle = {secrypt},
	author = {Zhang, Xichen and Lashkari, Arash Habibi and Ghorbani, Ali A.},
	year = {2017},
}

@misc{keil_privoxy_2015,
	title = {Privoxy},
	url = {http://www.privoxy.org},
	author = {Keil, F. and Schmidt, D. and Burgiss, H. and Rian, L. and Rosenfeld, {and} R.},
	year = {2015},
}

@inproceedings{orr_approach_2012,
	title = {An {Approach} for {Identifying} {JavaScript}-loaded {Advertisements} through {Static} {Program} {Analysis}},
	booktitle = {wpes},
	author = {Orr, Caitlin R. and Chauhan, Arun and Gupta, Minaxi and Frisz, Christopher J. and Dunn, Christopher W.},
	year = {2012},
}

@inproceedings{bhagavatula_leveraging_2014,
	title = {Leveraging {Machine} {Learning} to {Improve} {Unwanted} {Resource} {Filtering}},
	booktitle = {aisec},
	author = {Bhagavatula, Sruti and Dunn, Christopher and Kanich, Chris and Gupta, Minaxi and Ziebart, Brian},
	year = {2014},
}

@article{ikram_towards_2017,
	title = {Towards {Seamless} {Tracking}-{Gree} {Web}: {Improved} {Detection} of {Trackers} via {One}-{Class} {Learning}},
	volume = {2017},
	number = {1},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Ikram, Muhammad and Asghar, Hassan Jameel and Kaafar, Mohamed Ali and Mahanti, Anirban and Krishnamurthy, Balachandar},
	year = {2017},
	pages = {79--99},
}

@misc{mvps_blocking_2015,
	title = {Blocking unwanted connections with a hosts file},
	url = {http://winhelp2002.mvps.org/hosts.htm},
	author = {{MVPS}},
	year = {2015},
}

@inproceedings{merzdovnik_block_2017,
	title = {Block {Me} {If} {You} {Can}: {A} {Large}-{Scale} {Study} of {Tracker}-{Blocking} {Tools}},
	booktitle = {eurosp},
	author = {Merzdovnik, Georg and Huber, Markus and Buhov, Damjan and Nikiforakis, Nick},
	year = {2017},
}

@inproceedings{jang_empirical_2010,
	title = {An {Empirical} {Study} of {Privacy}-{Violoating} {Information} {Flows} in {JavaScript} web {Applications}},
	booktitle = {ccs},
	author = {Jang, Dongseok and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
	year = {2010},
}

@misc{mughees_first_2016,
	title = {A {First} {Look} at {Ad}-block {Detection} - {A} {New} {Arms} {Race} on the {Web}},
	author = {Mughees, Muhammad Haris and Qian, Zhiyun and Shafiq, Zubair and Dash, Karishma and Hui, Pan},
	year = {2016},
}

@misc{storey_future_2017,
	title = {The {Future} of {Ad} {Blocking}: {An} {Analytical} {Framework} and {New} {Techniques}},
	url = {https://arxiv.org/pdf/1705.08568.pdf},
	author = {Storey, Grant and Reisman, Dillon and Mayer, Jonathan and Narayanan, Arvind},
	year = {2017},
}

@misc{nithyanand_adblocking_2016,
	title = {Adblocking and {Counter}-{Blocking}: {A} {Slice} of the {Arms} {Race}},
	author = {Nithyanand, Rishab and Khattak, Sheharbano and Javed, Mobin and Vallina-Rodriguez, Narseo and Falahrastegar, Marjan and Powles, Julia E. and Cristofaro, Emiliano De and Haddadi, Hamed and Murdoch, Steven J.},
	year = {2016},
}

@misc{noauthor_codehs_2017,
	title = {{CodeHS}},
	url = {https://codehs.com. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_pypy_2017,
	title = {{PyPy} {Benchmarks}},
	url = {https://bitbucket.org/pypy/benchmarks. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_facebook_2017,
	title = {Facebook comment by {CodeHS} cofounder documenting number of users},
	url = {https://www.facebook.com/groups/CSEdForum/permalink/1865647727066981/?comment_id=1866187627012991&comment_tracking=%7B%22tn%22%3A%22R%22%7D},
	year = {2017},
}

@misc{noauthor_webassembly_2017,
	title = {{WebAssembly}: {Features} to {Add} after the {MVP}},
	url = {https://github.com/WebAssembly/design/blob/71c97d/FutureFeatures.md. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_larceny_2009,
	title = {Larceny {Benchmarks}},
	url = {http://www.larcenists.org/benchmarks2009all.html. Accessed Oct 12 2019},
	year = {2009},
}

@misc{noauthor_ton80_2017,
	title = {Ton80},
	url = {https://github.com/dart-lang/ton80. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_octane_2017,
	title = {Octane},
	url = {https://developers.google.com/octane. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_why_2017,
	title = {Why is my browser freezing when {I} submit an exercise?},
	url = {https://help.codecademy.com/hc/en-us/articles/220803187. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_computer_2017,
	title = {The {Computer} {Language} {Benchmarks} {Game}},
	url = {https://benchmarksgame.alioth.debian.org. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_kraken_2013,
	title = {Kraken},
	url = {https://wiki.mozilla.org/Kraken. Accessed Oct 12 2019},
	year = {2013},
}

@misc{noauthor_operf_2017,
	title = {{OPerf} {Micro}},
	url = {https://www.typerex.org/operf-micro.html. Accessed Oct 12 2019},
	year = {2017},
}

@inproceedings{yoo_wescheme_2011,
	title = {{WeScheme}: {The} {Browser} is {Your} {Programming} {Environment}},
	booktitle = {iticse},
	author = {Yoo, Danny and Schanzer, Emmanuel and Krishnamurthi, Shriram and Fisler, Kathi},
	year = {2011},
}

@inproceedings{canou_scaling_2017,
	title = {Scaling up {Functional} {Programming} {Education}: {Under} the {Hood} of the {OCaml} {MOOC}},
	booktitle = {icfp},
	author = {Canou, Benjamin and Cosmo, Roberto Di and Henry, Grégoire},
	year = {2017},
}

@inproceedings{flanagan_essence_1993,
	title = {The {Essence} of {Compiling} with {Continuations}},
	booktitle = {pldi},
	author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
	year = {1993},
}

@misc{noauthor_jetstream_2017,
	title = {{JetStream} 1.1},
	url = {http://browserbench.org/JetStream/. Accessed Oct 12 2019},
	year = {2017},
}

@misc{guzdial_cs_2017,
	title = {{CS} {Principles}: {Big} {Ideas} in {Programming}},
	url = {http://interactivepython.org/runestone/static/StudentCSP/index.html. Accessed Oct 12 2019},
	author = {Guzdial, Mark and Ericson, Barbara},
	year = {2017},
}

@misc{miller_problem_2016,
	title = {Problem {Solving} with {Algorithms} and {Data} {Structures} using {Python}},
	url = {http://interactivepython.org/runestone/static/pythonds/index.html. Accessed Oct 12 2019},
	author = {Miller, Brad and Ranum, David},
	year = {2016},
}

@inproceedings{li_case_2015,
	title = {A {Case} for {Semantics}-{Altering} {Transformations}},
	booktitle = {{ACM} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software} ({Onward}!)},
	author = {Li, Junsong and Pombrio, Justin and Politz, Joe Gibbs and Krishnamurthi, Shriram},
	year = {2015},
}

@misc{noauthor_tynker_2017,
	title = {Tynker},
	url = {https://www.tynker.com. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_trinket_2017,
	title = {Trinket},
	url = {https://trinket.io. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_pythonroom_2017,
	title = {{PythonRoom}},
	url = {https://pythonroom.com. Accessed Oct 12 2019},
	year = {2017},
}

@inproceedings{tang_environment_2014,
	title = {An {Environment} for {Learning} {Interactive} {Programming}},
	booktitle = {sigcse},
	author = {Tang, Terry and Rixner, Scott and Warren, Joe},
	year = {2014},
}

@misc{miller_how_2017,
	title = {How to {Think} {Like} a {Computer} {Scientist}: {Interactive} {Edition}},
	url = {http://interactivepython.org/runestone/static/thinkcspy/index.html. Accessed Oct 12 2019},
	author = {Miller, Brad and Ranum, David},
	year = {2017},
}

@misc{severance_programming_2017,
	title = {Programming for {Everybody}},
	url = {https://www.coursera.org/learn/python. Accessed Oct 12 2019},
	author = {Severance, Charles},
	year = {2017},
}

@misc{severance_python_2017,
	title = {Python {Data} {Structures}},
	url = {https://www.coursera.org/learn/python-data. Accessed Oct 12 2019},
	author = {Severance, Charles},
	year = {2017},
}

@misc{noauthor_skulpt_2017,
	title = {Skulpt},
	url = {http://www.skulpt.org. Accessed Oct 12 2019},
	year = {2017},
}

@inproceedings{ganz_trampolined_1999,
	title = {Trampolined {Style}},
	booktitle = {icfp},
	author = {Ganz, Steven E. and Friedman, Daniel P. and Wand, Mitchell},
	year = {1999},
}

@article{sitaram_control_1990,
	title = {Control {Delimiters} and {Their} {Hierarchies}},
	volume = {3},
	number = {1},
	journal = {LISP and Symbolic Computation},
	author = {Sitaram, Dorai and Felleisen, Matthias},
	month = may,
	year = {1990},
	pages = {67--99},
}

@inproceedings{mickens_pivot_2014,
	title = {Pivot: {Fast}, {Synchronous} {Mashup} {Isolation} {Using} {Generator} {Chains}},
	booktitle = {oakland},
	author = {Mickens, James},
	year = {2014},
}

@inproceedings{palacz_lively_2008,
	title = {The {Lively} {Kernel} {Application} {Framework}},
	booktitle = {International {Conference} on {Scalable} {Vector} {Graphics}},
	author = {Palacz, Krzysztof},
	year = {2008},
}

@misc{noauthor_khan_2017,
	title = {Khan {Academy}: {Computer} programming},
	url = {https://www.khanacademy.org/computing/computer-programming. Accessed Oct 12 2019},
	year = {2017},
}

@misc{pozo_scimark_2004,
	title = {{SciMark} 2.0},
	url = {http://math.nist.gov/scimark2/. Accessed Oct 12 2019},
	author = {Pozo, Roldan and Miller, Bruce},
	year = {2004},
}

@misc{noauthor_jsweet_nodate,
	title = {{JSweet}},
	url = {http://www.jsweet.org. Accessed Oct 12 2019},
}

@inproceedings{guo_codechella_2015,
	title = {Codechella: {Multi}-user program visualizations for real-time tutoring and collaborative learning},
	booktitle = {{IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author = {Guo, Philip J. and White, Jeffery and Zanelatto, Renan},
	year = {2015},
}

@misc{noauthor_fix_2015,
	title = {fix for goroutine blocking bookkeeping},
	url = {https://github.com/gopherjs/gopherjs/issues/209. Accessed Oct 12 2019},
	year = {2015},
}

@misc{noauthor_confusing_2015,
	title = {Confusing behavior with exceptions inside goroutines},
	url = {https://github.com/gopherjs/gopherjs/issues/225. Accessed Oct 12 2019},
	year = {2015},
}

@misc{noauthor_miscompilation_2016,
	title = {Miscompilation of functions w/ defer statements},
	url = {https://github.com/gopherjs/gopherjs/issues/493. Accessed Oct 12 2019},
	year = {2016},
}

@misc{noauthor_deferring_2016,
	title = {Deferring runtime.{Gosched}() causes error},
	url = {https://github.com/gopherjs/gopherjs/issues/426. Accessed Oct 12 2019},
	year = {2016},
}

@misc{noauthor_tight_2017,
	title = {Tight loop in goroutine never yields},
	url = {https://github.com/gopherjs/gopherjs/issues/698. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_execlimit_2017,
	title = {{execLimit} not triggering on large xrange loops},
	url = {https://github.com/skulpt/skulpt/issues/711. Accessed Oct 12 2019},
	year = {2017},
}

@misc{lerner_eachloop_2017,
	title = {{eachLoop} was simply broken: it did not restore the stack properly},
	url = {https://github.com/brownplt/pyret-lang/commit/812d1c. Accessed Oct 12 2019},
	author = {Lerner, Benjamin S.},
	year = {2017},
}

@misc{noauthor_pyjs_nodate,
	title = {{PyJS}},
	url = {http://pyjs.org},
}

@misc{noauthor_defer_2016,
	title = {defer + go routine/channel + blocking = possible crash},
	url = {https://github.com/gopherjs/gopherjs/issues/381. Accessed Oct 12 2019},
	year = {2016},
}

@misc{noauthor_programs_2017,
	title = {Programs can incorrectly terminate with {yieldLimit} set to suspend at regular intervals},
	url = {https://github.com/skulpt/skulpt/issues/723. Accessed Oct 12 2019},
	year = {2017},
}

@misc{lerner_fix_2017,
	title = {Fix broken {eachLoop}},
	url = {https://github.com/brownplt/pyret-lang/commit/b7f9c9. Accessed Oct 12 2019},
	author = {Lerner, Benjamin S.},
	year = {2017},
}

@misc{politz_too_2014,
	title = {Too much recursion},
	url = {https://github.com/brownplt/pyret-lang/issues/213. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2014},
}

@misc{politz_caja_2015,
	title = {Caja interferes with stack management/events during big-bang},
	url = {https://github.com/brownplt/pyret-lang/issues/512. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2015},
}

@misc{iba_too_2015,
	title = {too much recursion in 2048 example code},
	url = {https://github.com/brownplt/pyret-lang/issues/555. Accessed Oct 12 2019},
	author = {Iba, Wayne},
	year = {2015},
}

@misc{politz_stopping_2014,
	title = {Stopping (indefinitely) stopped programs},
	url = {https://github.com/brownplt/pyret-lang/issues/163. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2014},
}

@misc{politz_stack_2015,
	title = {Stack management fails on shallow, but long-lasting, recursion},
	url = {https://github.com/brownplt/pyret-lang/issues/596. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2015},
}

@misc{politz_more_2017,
	title = {more fixing of {eachLoop}},
	url = {https://github.com/brownplt/pyret-lang/commit/844454. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2017},
}

@misc{politz_make_2014,
	title = {Make module loading stack safe},
	url = {https://github.com/brownplt/pyret-lang/issues/145. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2014},
}

@misc{politz_printing_2014,
	title = {Printing large values is not stack-safe},
	url = {https://github.com/brownplt/pyret-lang/issues/146. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs},
	year = {2014},
}

@misc{krishnamurthi_impossible_2016,
	title = {impossible(?) to kill infinite loop w/ reactor},
	url = {https://github.com/brownplt/pyret-lang/issues/839. Accessed Oct 12 2019},
	author = {Krishnamurthi, Shriram},
	year = {2016},
}

@misc{krishnamurthi_calling_2017,
	title = {calling plotting functions inside a reactor makes program unstoppable},
	url = {https://github.com/brownplt/pyret-lang/issues/1089. Accessed Oct 12 2019},
	author = {Krishnamurthi, Shriram},
	year = {2017},
}

@misc{lerner_infinite_2015,
	title = {An infinite loop hangs big-bang},
	url = {https://github.com/brownplt/pyret-lang/issues/508. Accessed Oct 12 2019},
	author = {Lerner, Benjamin S.},
	year = {2015},
}

@misc{lerner_bignums_2017,
	title = {Bignums considered harmful},
	url = {https://github.com/brownplt/pyret-lang/issues/1118. Accessed Oct 12 2019},
	author = {Lerner, Benjamin S.},
	year = {2017},
}

@misc{nigam_unsafe_2017,
	title = {Unsafe calls in runtime need to be wrapped with {safeCall}},
	url = {https://github.com/brownplt/pyret-lang/issues/1251. Accessed Oct 12 2019},
	author = {Nigam, Rachit},
	year = {2017},
}

@inproceedings{zhang_which_2014,
	title = {Which {Configuration} {Option} {Should} {I} {Change}?},
	booktitle = {icse},
	author = {Zhang, Sai and Ernst, Michael D.},
	year = {2014},
}

@misc{appcelerator_titanium_nodate,
	title = {Titanium},
	url = {https://github.com/appcelerator/titanium_mobile. Accessed Aug 30 2017.},
	author = {{Appcelerator}},
}

@misc{facebook_react_nodate,
	title = {React {Native}},
	url = {https://facebook.github.io/react-native. Accessed Jul 5 2017.},
	author = {Facebook, Inc.},
}

@misc{lerner_responsiveness_2015,
	title = {Responsiveness for rendering huge data at the {REPL}},
	url = {https://github.com/brownplt/code.pyret.org/issues/37. Accessed Oct 12 2019},
	author = {Lerner, Benjamin S.},
	year = {2015},
}

@inproceedings{attariyan_automating_2010,
	title = {Automating configuration troubleshooting with dynamic information flow analysis},
	booktitle = {osdi},
	author = {Attariyan, Mona and Flinn, Jason},
	year = {2010},
}

@inproceedings{su_autobash_2007,
	title = {{AutoBash}: {Improving} configuration management with operating system causality analysis},
	booktitle = {sosp},
	author = {Su, Ya-Yunn and Attariyan, Mona and Flinn, Jason},
	year = {2007},
}

@inproceedings{hieb_representing_1990,
	title = {Representing {Control} in the {Presence} of {First}-class {Continuations}},
	booktitle = {pldi},
	author = {Hieb, R. and Dybvig, R. Kent and Bruggeman, Carl},
	year = {1990},
}

@inproceedings{bruggeman_representing_1996,
	title = {Representing {Control} in the {Presence} of {One}-shot {Continuations}},
	booktitle = {pldi},
	author = {Bruggeman, Carl and Waddell, Oscar and Dybvig, Kent R.},
	year = {1996},
}

@misc{barati_jsc_nodate,
	title = {{JSC} {Heart} {ES6}},
	url = {https://webkit.org/blog/7536/jsc-loves-es6/. Accessed Oct 12 2019},
	author = {Barati, Saam and Suzuki, Yusuke and Pizlo, Filip},
}

@inproceedings{baxter_web-based_2017,
	title = {Web-based {Debugging} for {Free}},
	copyright = {All rights reserved},
	booktitle = {New {England} {Programming} {Langauges} {Symposium} ({NEPLS})},
	author = {Baxter, Sam and Nigam, Rachit and Guha, Arjun and Politz, Joe},
	year = {2017},
}

@article{tarditi_no_1992,
	title = {No {Assembly} {Required}: {Compiling} {Standard} {ML} to {C}},
	volume = {1},
	number = {2},
	journal = {ACM Letters on Programming Languages and Systems (LOPLAS)},
	author = {Tarditi, David and Lee, Peter and Acharya, Anurag},
	month = jun,
	year = {1992},
	pages = {161--177},
}

@article{baker_cons_1995,
	title = {{CONS} {Should} {Not} {CONS} {Its} {Arguments}, {Part} {II}: {Cheney} on the {M}.{T}.{A}.},
	volume = {30},
	number = {9},
	journal = {ACM SIGPLAN Notices},
	author = {Baker, Henry G.},
	month = sep,
	year = {1995},
	pages = {17--20},
}

@article{cook_proving_2011,
	title = {Proving {Program} {Termination}},
	volume = {54},
	number = {5},
	journal = {cacm},
	author = {Cook, Byron and Podelski, Andreas and Rybalchenko, Andrey},
	month = may,
	year = {2011},
	pages = {88--98},
}

@misc{noauthor_quotas_nodate,
	title = {Quotas for {Google} {Services}},
	url = {https://developers.google.com/apps-script/guides/services/quotas. Accessed Oct 12 2019.},
}

@misc{noauthor_gopherjs_2017,
	title = {{GopherJS}},
	url = {https://github.com/gopherjs/gopherjs. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_massachusetts_nodate,
	title = {Massachusetts {Green} {High} {Performance} {Computing} {Center} ({MGHPCC})},
	url = {https://www.nghpcc.org. Accessed Oct 12 2019.},
}

@misc{noauthor_holyoke_nodate,
	title = {Holyoke {Codes}},
	url = {https://holyokecodes.org. Accessed Oct 12 2019.},
}

@article{vitter_faster_1984,
	title = {Faster {Methods} for {Random} {Sampling}},
	volume = {27},
	number = {7},
	journal = {cacm},
	author = {Vitter, Jeffrey Scott},
	month = jul,
	year = {1984},
	pages = {703--718},
}

@article{lamport_how_1979,
	title = {How to {Make} a {Multiprocessor} {Computer} {That} {Correctly} {Executes} {Multiprocess} {Programs}},
	volume = {28},
	number = {9},
	journal = {IEEE Transactionson Computing},
	author = {Lamport, Leslie},
	month = sep,
	year = {1979},
}

@inproceedings{loitsch_exceptional_2007,
	title = {Exceptional {Continuations} in {JavaScript}},
	booktitle = {scheme},
	author = {Loitsch, Florian},
	year = {2007},
}

@article{herlihy_linearizability_1990,
	title = {Linearizability: {A} {Correctness} {Condition} for {Concurrent} {Objects}},
	volume = {12},
	number = {3},
	journal = {toplas},
	author = {Herlihy, Maurice P. and Wing, Jeannette M.},
	month = jul,
	year = {1990},
	pages = {463--492},
}

@misc{noauthor_continuation_nodate,
	title = {Continuation {Barriers}},
	url = {https://docs.racket-lang.org/reference/eval-model.html. Accessed Oct 12 2019},
}

@inproceedings{sitaram_handling_1993,
	title = {Handling {Control}},
	booktitle = {pldi},
	author = {Sitaram, Dorai},
	year = {1993},
}

@article{krishnamurthi_implementation_2007,
	title = {Implementation and {Use} of the {PLT} {Scheme} {Web} {Server}},
	volume = {20},
	number = {4},
	journal = {hosc},
	author = {Krishnamurthi, Shriram and Hopkins, Peter Walton and McCarthy, Jay and Graunke, Paul T. and Pettyjohn, Greg and Felleisen, Matthias},
	month = dec,
	year = {2007},
	pages = {431--460},
}

@inproceedings{ritchie_c3_2016,
	title = {C3: {Lightweight} {Incrementalized} {MCMC} for {Probabilistic} {Programs} using {Continuations} and {Callsite} {Caching}},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Ritchie, Daniel and Stuhlmüller, Andreas and Goodman, Noah},
	year = {2016},
}

@inproceedings{haynes_continuations_1984,
	title = {Continuations and {Coroutines}},
	booktitle = {lfp},
	author = {Haynes, Christopher T. and Friedman, Daniel P. and Wand, Mitchell},
	year = {1984},
}

@techreport{felleisen_transliterating_1985,
	title = {Transliterating {Prolog} {Into} {Scheme}},
	number = {TR182},
	institution = {Indiana University School of Informatics and Computing},
	author = {Felleisen, Matthias},
	year = {1985},
}

@misc{noauthor_vocareum_2017,
	title = {Vocareum},
	url = {https://www.vocareum.com. Accessed Oct 12 2019},
	year = {2017},
}

@misc{codecademy_learn_2017,
	title = {Learn {JavaScript}},
	url = {https://www.codecademy.com/learn/learn-javascript. Accessed Oct 12 2019},
	author = {{Codecademy}},
	year = {2017},
}

@inproceedings{ducasse_seasidemultiple_2006,
	title = {Seaside—{A} {Multiple} {Control} {Flow} {Web} {Application} {Framework}},
	booktitle = {European {Smalltalk} {User} {Group}—{Research} {Track}},
	author = {Ducasse, Stéphane and Lienhardb, Adrian and Rengglib, Lukas},
	year = {2006},
}

@misc{noauthor_codio_nodate,
	title = {Codio {Tkinter}},
	url = {https://codio.com/docs/content/courses/tkinter/. Accessed Oct 12 2019},
}

@misc{noauthor_codio_2017,
	title = {Codio},
	url = {https://codio.com. Accessed Oct 12 2019},
	year = {2017},
}

@misc{herman_asmjs_2014,
	title = {Asm.js},
	url = {http://asmjs.org/. Accessed Oct 12 2019},
	author = {Herman, David and Wagner, Luke and Zakai, Alon},
	year = {2014},
}

@misc{treehouse_beginning_nodate,
	title = {Beginning {JavaScript}},
	url = {https://teamtreehouse.com/tracks/beginning-javascript. Accessed Oct 12 2019},
	author = {{TreeHouse}},
}

@misc{codeschool_learn_nodate,
	title = {Learn {JavaScript} {Online}},
	url = {https://www.codeschool.com/learn/javascript. Accessed Oct 12 2019},
	author = {{CodeSchool}},
}

@misc{noauthor_teams_2017,
	title = {Teams on {CodePen}},
	url = {https://codepen.io/pro/teams/. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_replit_nodate,
	title = {{ReplIt} {JavaScript}},
	url = {https://repl.it/languages/javascript. Accessed Oct 12 2019},
}

@misc{masad_debugjs_nodate,
	title = {debug.js},
	url = {https://github.com/amasad/debugjs.com. Accessed Oct 12 2019},
	author = {Masad, Amjad},
}

@misc{long_unwinder_nodate,
	title = {Unwinder},
	url = {https://github.com/jlongster/unwinder. Accessed Oct 12 2019},
	author = {Long, James},
}

@inproceedings{powers_browsix_2017,
	title = {Browsix: {Bridging} the {Gap} {Between} {Unix} and the {Browser}},
	booktitle = {asplos},
	author = {Powers, Bobby and Vilk, John and Berger, Emery D.},
	year = {2017},
}

@misc{noauthor_infinite_nodate,
	title = {Infinite {Loop} {Buster}},
	url = {https://github.com/CodePen/InfiniteLoopBuster. Accessed Oct 12 2019},
}

@misc{wright_jwacs_2017,
	title = {{JWACS}},
	url = {http://chumsley.org/jwacs/index.html. Accessed Oct 12 2019},
	author = {Wright, James},
	year = {2017},
}

@inproceedings{douceur_leveraging_2008,
	title = {Leveraging {Legacy} {Code} to {Deploy} {Desktop} {Applications} on the {Web}},
	booktitle = {osdi},
	author = {Douceur, John R. and Elson, Jeremy and Howell, Jon and Lorch, Jacob R.},
	year = {2008},
}

@inproceedings{yee_native_2009,
	title = {Native {Client}: {A} {Sandbox} for {Portable}, {Untrusted} x86 {Native} {Code}},
	booktitle = {oakland},
	author = {Yee, Bennet and Sehr, David and Dardyk, Greg and Chen, Brad and Muth, Robert and Ormandy, Tavis and Okasaka, Shiki and Narula, Neha and Fullagar, Nicholas},
	year = {2009},
}

@misc{mozilla_inc_sharedarraybuffer_2017,
	title = {{SharedArrayBuffer}},
	url = {https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer. Accessed Oct 12 2019},
	author = {{Mozilla, Inc}},
	year = {2017},
}

@inproceedings{haas_bringing_2017,
	title = {Bringing the {Web} {Up} to {Speed} with {WebAssembly}},
	booktitle = {pldi},
	author = {Haas, Andreas and Rossberg, Andreas and Schuff, Derek L. and Titzer, Ben L. and Holman, Michael and Gohman, Dan and Wagner, Luke and Zakai, Alon and Bastien, J. F.},
	year = {2017},
}

@inproceedings{vilk_doppio_2014,
	title = {Doppio: {Breaking} the {Browser} {Language} {Barrier}},
	booktitle = {pldi},
	author = {Vilk, John and Berger, Emery D.},
	year = {2014},
}

@misc{max_how_nodate,
	title = {How {A} {Pocket}-{Sized} {Tablet} is {Changing} {Medical} {Education}},
	url = {http://ymm.yale.edu/spring2014/features/feature/187482/. Accessed Oct 12 2019},
	author = {Max, Jill},
}

@misc{noauthor_top_nodate,
	title = {Top {Pick}: {Wind} {Technicians} {Choose} {Intel} {Technology}-based {Tablets} for {Mobile} {Field} {Work}},
	url = {https://itpeernetwork.intel.com/top-pick-wind-technicians-choose-intel-technology-based-tablets-for-mobile-field-work/. Accessed Oct 12 2019},
}

@misc{w3c_web_2015,
	title = {Web {Workers}},
	url = {https://www.w3.org/TR/workers/. Accessed Oct 12 2019},
	author = {{W3C}},
	year = {2015},
}

@misc{mix_narrative_2013,
	title = {Narrative {JavaScript}},
	url = {https://sourceforge.net/projects/narrativejs. Accessed Oct 12 2019},
	author = {Mix, Neil and Grisby, Dan},
	year = {2013},
}

@misc{patino_ecmascript_nodate,
	title = {{ECMAScript} {Spec} {Proposal} for {Realms} {API}},
	url = {https://github.com/tc39/proposal-realms. Accessed Oct 12 2019},
	author = {Patino, Caridy and Herman, David and Miller, Mark},
}

@inproceedings{thivierge_efficient_2012,
	title = {Efficient {Compilation} of {Tail} {Calls} and {Continuations} to {JavaScript}},
	booktitle = {Workshop on {Scheme} and {Functional} {Programming}},
	author = {Thivierge, Eric and Feeley, Marc},
	year = {2012},
}

@misc{lunden_plangrid_nodate,
	title = {{PlanGrid} {Builds} {A} {New} {Market} {For} {The} {iPad}: {The} {Construction} {Industry}},
	url = {https://techcrunch.com/2012/03/02/plangrid-builds-a-new-market-for-the-ipad-the-construction-industry/. Accessed Oct 12 2019},
	author = {Lunden, Ingrid},
}

@inproceedings{appel_separate_1994,
	title = {Separate {Compilation} for {Standard} {ML}},
	booktitle = {pldi},
	author = {Appel, Andrew W. and MacQueen, David B.},
	year = {1994},
}

@misc{vasquez_improving_nodate,
	title = {Improving {Infinite} {Loop} {Detection}},
	url = {https://codepen.io/quezo/post/improving-infinite-loop-detection. Accessed Oct 12 2019},
	author = {Vasquez, Alex},
}

@misc{noauthor_internet_nodate,
	title = {The {Internet} {Arcade}},
	url = {https://archive.org/details/internetarcade. Accessed Oct 12 2019},
}

@misc{schanzer_bootstrap_nodate,
	title = {Bootstrap: {Data} {Science}},
	url = {http://www.bootstrapworld.org/materials/spring2018/courses/data-science/english/},
	author = {Schanzer, Emmanuel and Dooman, Sam and Krishnamurthi, Shriram and Politz, Joe and Lerner, Ben},
}

@misc{elementary_school_nodate,
	title = {School and {District} {Profiles}: {Holyoke}},
	url = {http://profiles.doe.mass.edu/general/general.aspx?topNavID=1&leftNavId=100&orgcode=01370000&orgtypecode=5. Accessed Oct 12 2019},
	author = {Elementary, Massachusetts Department of and Education, Secondary},
}

@inproceedings{pettyjohn_continuations_2005,
	title = {Continuations from generalized stack inspection},
	booktitle = {icfp},
	author = {Pettyjohn, Greg and Clements, John and Marshall, Joe and Krishnamurthi, Shriram and Felleisen, Matthias},
	year = {2005},
}

@misc{noauthor_list_2017,
	title = {List of {Languages} that {Compile} to {JS}},
	url = {https://github.com/jashkenas/coffeescript/wiki/list-of-languages-that-compile-to-js. Accessed Oct 12 2019},
	year = {2017},
}

@misc{noauthor_cs_nodate,
	title = {{CS} {For} {All}},
	url = {https://obamawhitehouse.archives.gov/blog/2016/01/30/computer-science-all. Accessed Oct 12 2019},
}

@article{fox_state_2015,
	title = {State to take over {Holyoke} {Schools}},
	journal = {The Boston Globe},
	author = {Fox, Jeremy C.},
	month = apr,
	year = {2015},
	pages = {B2},
}

@misc{superhighway_2016_nodate,
	title = {2016 {State} of the {States}},
	url = {http://stateofthestates.educationsuperhighway.org/. Accessed Oct 12 2019},
	author = {SuperHighway, Education},
}

@inproceedings{burckhardt_its_2013,
	title = {It's {Alive}! {Continuous} {Feedback} in {UI} {Programming}},
	booktitle = {pldi},
	author = {Burckhardt, Sebastian and Fahndrich, Manuel and Halleux, Peli de and McDirmid, Sean and Moskal, Michal and Tillmann, Nokolai and Kato, Jun},
	year = {2013},
}

@misc{perez_online_2015,
	title = {Online {Learning} {Service} {Pluralsight} {Acquires} {Code} {School} {For} \$36 {Million}},
	url = {https://techcrunch.com/2015/01/26/online-learning-service-pluralsight-acquires-code-school-for-36-million/. Accessed Oct 12 2019},
	author = {Perez, Sarah},
	year = {2015},
}

@misc{lunden_codecademy_2016,
	title = {Codecademy, the free online coding school, raises another \${30M} led by {Naspers}},
	url = {https://techcrunch.com/2016/07/12/codecademy-the-free-online-coding-school-raises-another-30m-led-by-naspers/. Accessed Oct 12 2019},
	author = {Lunden, Ingrid},
	year = {2016},
}

@misc{loizos_zach_2016,
	title = {Zach {Sims} {Of} {Codecademy} {On} {Running} {A} {Company} {That} ({Still}) {Doesn}'t {Charge} {Users}},
	url = {https://techcrunch.com/2016/01/25/zach-sims-of-codecademy-on-running-a-company-that-still-doesnt-charge-users/. Accessed Oct 12 2019},
	author = {Loizos, Connie},
	year = {2016},
}

@misc{university_shoppers_nodate,
	title = {Shopper's {Guide}},
	url = {https://computerservices.temple.edu/shoppers-guide. Accessed Oct 12 2019},
	author = {University, Temple},
}

@misc{noauthor_concordia_nodate,
	title = {Concordia {College} {Computer} {Buyer}'s {Guide}},
	url = {https://www.concordiacollege.edu/directories/offices-services/information-technology/solution-center/resources/computer-buyers-guide/. Accessed Oct 12 2019},
}

@misc{empson_treehouse_2013,
	title = {Treehouse {Lands} \${7M} {From} {Kaplan}, {Social}+{Capital} {To} {Help} {You} {Learn} {To} {Code}},
	url = {https://techcrunch.com/2013/04/09/treehouse-lands-7m-from-kaplan-socialcapital-to-help-you-learn-to-code/. Accessed Oct 12 2019},
	author = {Empson, Rip},
	year = {2013},
}

@misc{university_recommended_nodate,
	title = {Recommended {Laptops} for 2017–2018},
	url = {http://ccit.clemson.edu/support/current-students/laptops/recommended-laptops/. Accessed Oct 12 2019},
	author = {University, Clemson},
}

@misc{noauthor_faqs_nodate,
	title = {{FAQs} about {Technology} at {Boston} {College}},
	url = {http://www.bc.edu/offices/itwelcome/laptop/faq.html. Accessed Oct 12 2019},
}

@inproceedings{gulwani_strisynth_2015,
	title = {{StriSynth}: synthesis for live programming},
	booktitle = {icse},
	author = {Gulwani, Sumit and Mayer, Mikaël and Niksic, Filip and Piskac, Ruzica},
	year = {2015},
}

@inproceedings{nguyen_semfix_2013,
	title = {{SemFix}: {Program} repair via semantic analysis},
	booktitle = {icse},
	author = {Nguyen, Hoang Duong Thien and Qi, Dawei and Roychoudhury, Abhik and Satish Chandra, Satish},
	year = {2013},
}

@misc{noauthor_bridgewater_nodate,
	title = {Bridgewater {College} {Computer} {Purchasing} {Guide}},
	url = {https://helpdesk.bridgewater.edu/index.php?/Knowledgebase/Article/View/176/. Accessed Oct 12 2019},
}

@misc{politz_pyret_2017,
	title = {Pyret},
	url = {https://www.pyret.org/. Accessed Oct 12 2019},
	author = {Politz, Joe Gibbs and Lerner, Benjamin S. and Krishnamurthi, Shriram},
	year = {2017},
}

@misc{turner_will_nodate,
	title = {Will {Linux} distros run on {Windows} 10 {S}?},
	url = {https://blogs.msdn.microsoft.com/commandline/2017/05/18/will-linux-distros-run-on-windows-10-s/. Accessed Jul 5 2017},
	author = {Turner, Rich},
}

@misc{ghani_localstorage_2012,
	title = {{localStorage} / {SQLDatabase} no longer persistent after {iOS} 5.01 {Update}},
	url = {https://issues.apache.org/jira/browse/CB-330},
	author = {Ghani, Amirudin Bin Mohamed},
	year = {2012},
}

@inproceedings{santolucito_probabilistic_2016,
	title = {Probabilistic {Automated} {Language} {Learning} for {Configuration} {Files}},
	booktitle = {cav},
	author = {Santolucito, Mark and Zhai, Ennan and Piskac, Ruzica},
	year = {2016},
}

@inproceedings{xu_early_2016,
	title = {Early detection of configuration errors to reduce failure damage},
	booktitle = {osdi},
	author = {Xu, Tianyin and Jin, Xinxin and Huang, Peng and Zhou, Yuanyuan and Lu, Shan and Jin, Long and Pasupathy, Shankar},
	year = {2016},
}

@misc{welch_chef_2016,
	title = {Chef {Automate} {Now} {Available} as {Fully} {Managed} {Service} on {AWS}},
	url = {https://blog.chef.io/2016/12/01/chef-automate-now-available-fully-managed-service-aws/},
	author = {Welch, Lucas},
	year = {2016},
}

@inproceedings{mechtaev_angelix_2016,
	title = {Angelix: {Scalable} {Multiline} {Program} {Patch} {Synthesis} via {Symbolic} {Analysis}},
	booktitle = {icse},
	author = {Mechtaev, Sergey and Yi, Jooyong and Roychoudhury, Abhik},
	year = {2016},
}

@misc{hatch_salt_2011,
	title = {Salt},
	url = {https://saltstack.com},
	author = {Hatch, Thomas S.},
	year = {2011},
}

@inproceedings{hempel_semi-automated_2016,
	title = {Semi-{Automated} {SVG} {Programming} via {Direct} {Manipulation}},
	booktitle = {uist},
	author = {Hempel, Brian and Chugh, Ravi},
	year = {2016},
}

@inproceedings{zovi_apple_2011,
	title = {Apple {iOS} 4 {Security} {Evaluation}},
	booktitle = {Black {Hat} {USA}},
	author = {Zovi, Dino A. Dai},
	year = {2011},
}

@misc{weins_new_2017,
	title = {New {DevOps} {Trends}: 2017 {State} of the {Cloud} {Survey}},
	url = {http://www.rightscale.com/blog/cloud-industry-insights/new-devops-trends-2017-state-cloud-survey},
	author = {Weins, Kim},
	year = {2017},
}

@inproceedings{zheng_z3-str_2013,
	title = {Z3-str: {A} {Z3}-based string solver for web application analysis},
	booktitle = {esecfse},
	author = {Zheng, Yunhui and Zhang, Xiangyu and Ganesh, Vijay},
	year = {2013},
}

@misc{welch_chef_2016-1,
	title = {Chef {Appoints} {Company}’s {First} {Chief} {Marketing} {Officer} to {Drive} {Continued} {Growth}},
	url = {https://blog.chef.io/2016/11/10/chef-appoints-companys-first-chief-marketing-officer},
	author = {Welch, Lucas},
	year = {2016},
}

@inproceedings{mechtaev_directfix_2015,
	title = {{DirectFix}: {Looking} for {Simple} {Program} {Repairs}},
	booktitle = {icse},
	author = {Mechtaev, Sergey and Yi, Jooyong and Roychoudhury, Abhik},
	year = {2015},
}

@misc{guzdial_apple_nodate,
	title = {Apple removes {Scratch} from {iPad}/{iPhone}/{iTouch}},
	url = {https://computinged.wordpress.com/2010/04/15/apple-removes-scratch-from-ipadiphoneitouch},
	author = {Guzdial, Mark},
}

@misc{apple_ios_nodate,
	title = {{iOS} {Developer} {Program} {License} {Agreement}},
	url = {https://developer.apple.com/programs/terms/ios/standard/ios_program_standard_agreement_20140909.pdf},
	author = {Apple, Inc.},
}

@misc{guzdial_five_2016,
	title = {Five {Principles} for {Programming} {Languages} for {Learners}},
	url = {https://cacm.acm.org/blogs/blog-cacm/203554-five-principles-for-programming-languages-for-learners},
	author = {Guzdial, Mark},
	year = {2016},
}

@misc{noauthor_asus_nodate,
	title = {{ASUS} {C202SA}-{YS02} {ChromeBook}},
	url = {http://store.asus.com/us/item/201604AM060000090},
}

@article{singer_amid_2017,
	title = {Amid {Stiff} {Competition}, {Apple} {Devices} {Lose} {Luster} in {American} {Schools}},
	journal = {The New York Times},
	author = {Singer, Natasha},
	year = {2017},
	pages = {B2},
}

@inproceedings{gordon_lcf_2000,
	title = {From {LCF} to {HOL}: a short history},
	booktitle = {Proof, {Language}, and {Interaction}: {Essays} in {Honour} of {Robin} {Milner}},
	author = {Gordon, Michael J.},
	year = {2000},
}

@inproceedings{macqueen_luca_2014,
	title = {Luca {Cardelli} and the early evolution of {ML}},
	booktitle = {Essays for the {Luca} {Cardelli} {Fest}},
	author = {MacQueen, David},
	year = {2014},
}

@inproceedings{pombrio_hygienic_2015,
	title = {Hygienic {Resugaring} of {Compositional} {Desugaring}},
	booktitle = {icfp},
	author = {Pombrio, Justin and Krishnamurthi, Shriram},
	year = {2015},
}

@inproceedings{pombrio_resugaring_2014,
	title = {Resugaring: {Lifting} {Evaluation} {Sequences} {Through} {Syntactic} {Sugar}},
	booktitle = {pldi},
	author = {Pombrio, Justin and Krishnamurthi, Shriram},
	year = {2014},
}

@book{jr_common_1990,
	title = {Common {Lisp}: {The} {Language}},
	publisher = {Digital Press},
	author = {Jr, Guy L. Steele},
	year = {1990},
}

@misc{noauthor_history_nodate,
	title = {A {History} of {OCaml}},
	url = {https://ocaml.org/learn/history.html},
}

@inproceedings{hudak_history_2007,
	title = {A {History} of {Haskell}: {Being} {Lazy} with {Class}},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {History} of {Programming} {Languages}},
	author = {Hudak, Paul and Hughes, John and Jones, Simon Peyton and Wadler, Philip},
	year = {2007},
}

@inproceedings{clements_modeling_2001,
	title = {Modeling an {Algebraic} {Stepper}},
	booktitle = {esop},
	author = {Clements, John and Flatt, Matthew and Felleisen, Matthias},
	year = {2001},
}

@article{landin_next_1966,
	title = {The next 700 programming languages},
	volume = {9},
	number = {3},
	journal = {cacm},
	author = {Landin, Peter J.},
	year = {1966},
	pages = {157--166},
}

@misc{facebook_notitle_nodate,
	url = {https://facebook.github.io/react/},
	author = {{Facebook}},
}

@article{porter_practical_2014,
	title = {Practical {Fine}-{Grained} {Information} {Flow} {Control} {Using} {Laminar}},
	volume = {37},
	number = {1},
	journal = {toplas},
	author = {Porter, Donald E. and Bond, Michael D. and Roy, Indrajit and McKinley, Kathryn S. and Witchel, Emmett},
	year = {2014},
	pages = {4:1--4:51},
}

@book{harbison_c_1987,
	title = {C: {A} {Reference} {Manual}},
	publisher = {Prentice-Hall},
	author = {Harbison, Samuel P. and Jr, Guy L. Steele},
	year = {1987},
}

@misc{noauthor_babylon_2017,
	title = {Babylon: {A} {JavaScript} parser used in {Babel}},
	url = {https://github.com/babel/babel/tree/master/packages/babylon},
	year = {2017},
}

@inproceedings{ibrahim_remote_2009,
	title = {Remote {Batch} {Invocation} for {Compositional} {Object} {Services}},
	booktitle = {ecoop},
	author = {Ibrahim, Ali and Tilevich, Yang Jiao an Eli and Cook, William R.},
	year = {2009},
}

@article{amza_treadmarks_1996,
	title = {{TreadMarks}: {Shared} {Memory} {Computing} on {Networks} of {Workstations}},
	volume = {29},
	number = {2},
	journal = {Computer},
	author = {Amza, Cristiana and Cox, Alan L. and Dwarkadas, Sandhya and Keleher, Pete and Lu, Honghui and Rajamony, Ramakrishnan and Yu, Weimin and Zwaenepoel, Willy},
	month = feb,
	year = {1996},
	pages = {18--28},
}

@inproceedings{vikram_ripley_2009,
	title = {Ripley: {Automatically} {Securing} {Web} 2.0 {Applications} {Through} {Replicated} {Execution}},
	booktitle = {ccs},
	author = {Vikram, K. and Prateek, Abhishek and Livshits, Benjamin},
	year = {2009},
}

@inproceedings{austin_faceted_2013,
	title = {Faceted {Execution} of {Policy}-{Agnostic} {Programs}},
	booktitle = {plas},
	author = {Austin, Thomas H. and Yang, Jean and Flanagan, Cormac and Solar-Lezama, Armando},
	year = {2013},
}

@inproceedings{cook_remote_2011,
	title = {Remote {Batch} {Invocation} for {SQL} {Databases}},
	booktitle = {dbpl},
	author = {Cook, William R. and Wiedermann, Ben},
	year = {2011},
}

@article{birrell_implementing_1984,
	title = {Implementing {Remote} {Procedure} {Calls}},
	volume = {2},
	number = {1},
	journal = {tocs},
	author = {Birrell, Andrew D. and Nelson, Bruce Jay},
	month = feb,
	year = {1984},
	pages = {39--59},
}

@misc{noauthor_microsoft_nodate,
	title = {Microsoft {Live} {Labs} {Volta}},
	url = {http://web.archive.org/web/20080908193828/http://livelabs.com/volta},
}

@inproceedings{hedin_jsflow_2014,
	title = {{JSFlow}: {Tracking} {Information} {Flow} in {JavaScript} and its {APIs}},
	booktitle = {{ACM} {Symposium} on {Applied} {Computing} ({SAC})},
	author = {Hedin, Daniel and Birgisson, Arnar and Bello, Luciano and Sabelfeld, Andrei},
	year = {2014},
}

@inproceedings{yoo_whalesong_2013,
	title = {Whalesong: {Running} {Racket} in the {Browser}},
	booktitle = {dls},
	author = {Yoo, Danny and Krishnamurthi, Shriram},
	year = {2013},
}

@inproceedings{liu_fabric_2009,
	title = {Fabric: {A} {Platform} for {Secure} {Distributed} {Computation} and {Storage}},
	booktitle = {sosp},
	author = {Liu, Jed and George, Michael D. and Vikram, K. and Qi, Xin and Waye, Lucas and Myers, Andrew C.},
	year = {2009},
}

@misc{lehtosalo_mypy_nodate,
	title = {mypy},
	url = {http://mypy-lang.org},
	author = {Lehtosalo, Jukka},
}

@misc{noauthor_rspec-puppet_nodate,
	title = {{RSpec}-{Puppet}},
	url = {http://rspec-puppet.com/tutorial/},
}

@inproceedings{meijer_linq_2006,
	title = {{LINQ}: {Reconciling} {Object}, {Relations} and {XML} in the .{NET} {Framework}},
	booktitle = {sigmod},
	author = {Meijer, Erik and Beckman, Brian and Bierman, Gavin},
	year = {2006},
}

@misc{noauthor_module_nodate,
	title = {Module {Smoke} {Testing}},
	url = {https://docs.puppet.com/guides/tests_smoke.html},
}

@article{quinlan_induction_1986,
	title = {Induction of {Decision} {Trees}},
	volume = {1},
	number = {1},
	journal = {Machine Learning},
	author = {Quinlan, J. Ross},
	year = {1986},
	pages = {81--106},
}

@inproceedings{bodin_trusted_2014,
	title = {A {Trusted} {Mechanised} {JavaScript} {Specification}},
	booktitle = {popl},
	author = {Bodin, Martin and Chargueraud, Arthur and Filaretti, Daniele and Gardner, Philippa and Maffeis, Sergio and Naudziuniene, Daiva and Schmitt, Alan and Smith, Gareth},
	year = {2014},
}

@misc{andrew_c_myers_jif_nodate,
	title = {{JIF} 3.5.0: {Java} {Information} {Flow}},
	url = {https://www.cs.cornell.edu/jif},
	author = {Andrew C. Myers, Owen Arden, Jed Liu, Tom Magrino, K. Vikram, Lantian Zheng, Lantian Zheng, Steve Zdancewic, Stephen Chong,  and Nystrom, Nate},
}

@inproceedings{corcoran_cross-tier_2009,
	title = {Cross-tier, {Label}-based {Security} {Enforcement} for {Web} {Applications}},
	booktitle = {sigmod},
	author = {Corcoran, Brian J. and Swamy, Nikhil and Hicks, Michael},
	year = {2009},
}

@inproceedings{park_kjs_2015,
	title = {{KJS}: {A} {Complete} {Formal} {Semantics} of {JavaScript}},
	booktitle = {pldi},
	author = {Park, Daejun and Stefănescu, Andrei and Roşu, Grigore},
	year = {2015},
}

@inproceedings{czaplicki_asynchronous_2013,
	title = {Asynchronous {Functional} {Reactive} {Programming} for {GUIs}},
	booktitle = {pldi},
	author = {Czaplicki, Evan and Chong, Stephen},
	year = {2013},
}

@inproceedings{osera_dependent_2012,
	title = {Dependent {Interoperability}},
	booktitle = {plpv},
	author = {Osera, Peter-Michael and Sjöberg, Vilhelm and Zdancewic, Steve},
	year = {2012},
}

@inproceedings{matthews_operational_2007,
	title = {Operational {Semantics} for {Multi}-{Language} {Programs}},
	booktitle = {popl},
	author = {Matthews, Jacob and Findler, Robert Bruce},
	year = {2007},
}

@inproceedings{austin_multiple_2012,
	title = {Multiple {Facets} for {Dynamic} {Information} {Flow} {Control}},
	booktitle = {popl},
	author = {Austin, Thomas H. and Flanagan, Cormac},
	year = {2012},
}

@inproceedings{ahmed_verified_2015,
	title = {Verified {Compilers} for a {Multi}-{Language} {World}},
	booktitle = {snapl},
	author = {Ahmed, Amal},
	year = {2015},
}

@inproceedings{kashyap_jsai_2014,
	title = {{JSAI}: {A} {Static} {Analysis} {Platform} for {JavaScript}},
	booktitle = {fse},
	author = {Kashyap, Vineeth and Dewey, Kyle and Kuefner, Ethan A. and Wagner, John and Gibbons, Kevin and Sarracino, John and Wiedermann, Ben and Hardekopf, Ben},
	year = {2014},
}

@inproceedings{chong_secure_2007,
	title = {Secure {Web} {Applications} via {Automatic} {Partitioning}},
	booktitle = {sosp},
	author = {Chong, Stephen and Liu, Jed and Myers, Andrew C. and Qi, Xin and Vikram, K. and Zheng, Lantian and Zheng, Xin},
	year = {2007},
}

@inproceedings{schoepe_lets_2016,
	title = {Let’s {Face} {It}: {Faceted} {Values} for {Taint} {Tracking}},
	booktitle = {esorics},
	author = {Schoepe, Daniel and Balliu, Musard and Piessens, Frank and Sabelfeld, Andrei},
	year = {2016},
}

@inproceedings{akhawe_towards_2010,
	title = {Towards a {Formal} {Foundation} of {Web} {Security}},
	booktitle = {csf},
	author = {Akhawe, Devdatta and Barth, Adam and Lam, Peifung E. and Mitchell, John C. and Song, Dawn},
	year = {2010},
}

@misc{noauthor_bounty_nodate,
	title = {Bounty {Hunters}: {The} {Honor} {Roll}},
	url = {https://technet.microsoft.com/en-us/security/dn469163.aspx},
}

@misc{noauthor_facebook_nodate,
	title = {Facebook {Bug} {Bounty}: \$5 {Million} {Paid} in 5 {Years}},
	url = {https://www.facebook.com/notes/facebook-bug-bounty/facebook-bug-bounty-5-million-paid-in-5-years/1419385021409053/},
}

@misc{noauthor_cve-2016-6316_nodate,
	title = {{CVE}-2016-6316: {XSS} vulnerability in {Action} {View} in {Ruby} on {Rails}},
	url = {https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-6316},
}

@inproceedings{serrano_hop_2006,
	title = {Hop, a {Language} for {Programming} the {Web} 2.0},
	booktitle = {dls},
	author = {Serrano, Manuel and Gallesio, Erick and Loitsch, Florian},
	year = {2006},
}

@inproceedings{cooper_links_2006,
	title = {Links: {Web} programming without tiers},
	booktitle = {Formal {Methods} of {Components} and {Objects}},
	author = {Cooper, Ezra and Lindley, Sam and Wadler, Philip and Yallop, Jeremy},
	year = {2006},
}

@misc{noauthor_google_nodate,
	title = {Google {Security} {Rewards}–2015 {Year} in {Review}},
	url = {https://security.googleblog.com/2016/01/google-security-rewards-2015-year-in.html},
}

@misc{noauthor_wordpress_nodate,
	title = {{WordPress} 4.6.1 {Security} and {Maintenance} {Release}},
	url = {https://wordpress.org/news/2016/09/wordpress-4-6-1-security-and-maintenance-release/},
}

@inproceedings{chlipala_urweb_2015,
	title = {Ur/{Web}: {A} {Simple} {Model} for {Programming} the {Web}},
	booktitle = {popl},
	author = {Chlipala, Adam},
	year = {2015},
}

@inproceedings{an_static_2009,
	title = {Static {Typing} for {Ruby} on {Rails}},
	booktitle = {ase},
	author = {An, Jong-hoon David and Chaudhuri, Avik and Foster, Jeffrey S.},
	year = {2009},
}

@inproceedings{taly_automated_2011,
	title = {Automated {Analysis} of {Security}-critical {JavaScript} {APIs}},
	booktitle = {oakland},
	author = {Taly, Ankur and Erlingsson, Úlfar and Miller, Mark S. and Mitchell, John C. and Nagra, Jasvir},
	year = {2011},
}

@inproceedings{thiemann_type_2005,
	title = {A {Type} {Safe} {DOM} {API}},
	booktitle = {International {Workshop} on {Database} {Programming} {Languages}},
	author = {Thiemann, Peter},
	year = {2005},
}

@inproceedings{guarnieri_gatekeeper_2009,
	title = {{GateKeeper}: {Mostly} {Static} {Enforcement} of {Security} and {Reliability} {Policies} for {JavaScript} {Code}},
	booktitle = {security},
	author = {Guarnieri, Salvatore and Livshits, Benjamin},
	year = {2009},
}

@inproceedings{chugh_staged_2009,
	title = {Staged {Information} {Flow} for {JavaScript}},
	booktitle = {pldi},
	author = {Chugh, Ravi and Meister, Jeffrey A. and Jhala, Ranjit and Lerner, Sorin},
	year = {2009},
}

@inproceedings{chugh_nested_2012,
	title = {Nested {Refinements} for {Dynamic} {Languages}},
	booktitle = {popl},
	author = {Chugh, Ravi and Rondon, Patrick M. and Jhala, Ranjit},
	year = {2012},
}

@inproceedings{maffeis_isolating_2009,
	title = {Isolating {JavaScript} with {Filters}, {Rewriting}, and {Wrappers}},
	booktitle = {esorics},
	author = {Maffeis, Sergio and Mitchell, John C. and Taly, Ankur},
	year = {2009},
}

@inproceedings{richards_analysis_2010,
	title = {An {Analysis} of the {Dynamic} {Behavior} of {JavaScript} {Programs}},
	booktitle = {pldi},
	author = {Richards, Gregor and Lebresne, Sylvain and Burg, Brian and Vitek, Jan},
	year = {2010},
}

@inproceedings{heidegger_recency_2009,
	title = {Recency {Types} for {Dynamically}-{Typed}, {Object}-{Based} {Languages}: {Strong} {Updates} for {JavaScript}},
	booktitle = {fool},
	author = {Heidegger, Phillip and Thiemann, Peter},
	year = {2009},
}

@misc{overflow_auto_nodate,
	title = {Auto yes to the license agreement on {\textbackslash}textttsudo apt-get -y install oracle-java7-intaller},
	url = {http://stackoverflow.com/questions/19275856},
	author = {Overflow, Stack},
}

@misc{noauthor_textttpuppetlabs-openstack_nodate,
	title = {{\textbackslash}textttpuppetlabs-openstack module},
	url = {https://forge.puppet.com/puppetlabs/openstack},
}

@misc{noauthor_openstack_nodate,
	title = {{OpenStack} {Installation} {Guide} for {Ubuntu}},
	url = {http://docs.openstack.org/liberty/install-guide-ubuntu/},
}

@inproceedings{thiemann_towards_2005,
	title = {Towards a type system for analyzing {JavaScript} programs},
	booktitle = {esop},
	author = {Thiemann, Peter},
	year = {2005},
}

@inproceedings{maffeis_operational_2008,
	title = {An {Operational} {Semantics} for {JavaScript}},
	booktitle = {Asian {Symposium} on {Programming} {Languages} and {Systems}},
	author = {Maffeis, Sergio and Mitchell, John C. and Taly, Ankur},
	year = {2008},
}

@misc{menn_us_nodate,
	title = {U.{S}. {Election} agency breached by hackers after {November} vote},
	url = {http://www.reuters.com/article/us-election-hack-commission-idUSKBN1442VC},
	author = {Menn, Joseph},
}

@misc{w3c_xml_nodate,
	title = {{XML} {Path} {Language} ({XPath})},
	url = {https://www.w3.org/TR/xpath/},
	author = {{W3C}},
}

@article{ernst_daikon_2007,
	title = {The {Daikon} system for dynamic detection of likely invariants},
	volume = {69},
	number = {1–3},
	journal = {Science of Computer Programming},
	author = {Ernst, Michael d and Perkins, Jeff H. and Guo, Philip J. and McCamant, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
	year = {2007},
	pages = {35--45},
}

@inproceedings{wagner_intrusion_2001,
	title = {Intrusion {Detection} via {Static} {Analysis}},
	booktitle = {oakland},
	author = {Wagner, David and Dean, Drew},
	year = {2001},
}

@misc{turnbull_puppet_nodate,
	title = {Puppet, {Chef}, {Deterministic} {Ordering} and the much maligned {DSL}},
	url = {http://kartar.net/2010/01/puppet-chef-deterministic-ordering-and-the-much-maligned-dsl/},
	author = {Turnbull, James},
}

@misc{mozilla_inc_bug_nodate,
	title = {Bug 691654: {Fix} non-determinism in {Puppet} {Manifests}},
	url = {https://bugzilla.mozilla.org/show_bug.cgi?id=691654},
	author = {{Mozilla, Inc.}},
}

@inproceedings{lee_learning_1997,
	title = {Learning {Patterns} from {Unix} {Process} {Execution} {Traces} for {Intrusion} {Detection}},
	booktitle = {{AAAI} {Workshop}: {AI} {Approaches} to {Fraud} {Detection} and {Risk} {Management}},
	author = {Lee, Wenke and Stolfo, Salvatore J. and Chan, Phil},
	year = {1997},
}

@misc{noauthor_puppet_nodate,
	title = {Puppet {Resource} {Type}: {Exec}},
	url = {https://docs.puppet.com/puppet/latest/reference/types/exec.html},
}

@misc{labs_puppet_nodate,
	title = {Puppet {Features}: {Idempotency}},
	url = {http://docs.puppetlabs.com/guides/introduction.html#idempotency},
	author = {Labs, Puppet},
}

@inproceedings{lee_data_1998,
	title = {Data {Mining} {Approaches} for {Intrusion} {Detection}},
	booktitle = {oakland},
	author = {Lee, Wenke and Stolfo, Salvatore J.},
	year = {1998},
}

@inproceedings{forrest_sense_1996,
	title = {A {Sense} of {Self} for {Unix} {Processes}},
	booktitle = {oakland},
	author = {Forrest, Stephanie and Hofmeyr, Steven A. and Somayaji, Anil and Longstaff, Thomas A.},
	year = {1996},
}

@inproceedings{chugh_programmatic_2016,
	title = {Programmatic and {Direct} {Manipulation}, {Together} at {Last}},
	booktitle = {pldi},
	author = {Chugh, Ravi and Hempel, Brian and Spradlin, Mitchell and Albers, Jacob},
	year = {2016},
}

@misc{noauthor_augeas_nodate,
	title = {Augeas},
	url = {http://augeas.net},
}

@misc{shamow_inside_nodate,
	title = {Inside {Puppet}: {About} {Determinism}.},
	url = {http://puppetlabs.com/blog/inside-puppet-about-determinism},
	author = {Shamow, Eric},
}

@misc{noauthor_frenetic_2016,
	title = {The {Frenetic} {Network} {Controller}},
	url = {https://www.frenetic-lang.org},
	year = {2016},
}

@misc{noauthor_pyretics_2015,
	title = {Pyretic's {NetKAT} {Backend}},
	url = {https://github.com/frenetic-lang/pyretic/blob/master/pyretic/core/netkat.py#L49},
	year = {2015},
}

@inproceedings{beckett_temporal_2016,
	title = {Temporal {NetKAT}},
	booktitle = {pldi},
	author = {Beckett, Ryan and Greenberg, Michael and Walker, David},
	year = {2016},
}

@inproceedings{arashloo_snap_2016,
	title = {{SNAP}: {Stateful} {Network}-wide {Abstractions} for {Packet} {Processing}},
	booktitle = {sigcomm},
	author = {Arashloo, Mina Tahmasbi and Koral, Yaron and Greenberg, Michael and Rexford, Jennifer and Walker, David},
	year = {2016},
}

@inproceedings{wang_spn_2016,
	title = {{SPN} {OS}: {Managing} network services with virtual network objects},
	booktitle = {{IEEE} {Conference} on {Network} {Function} {Virtualization} and {Software} {Defined} {Networking} ({NFV}-{SDN})},
	author = {Wang, Xi and Chen, Cong and Palacharla, Paparao and Sekiya, Motoyoshi and Smolka, Steffen and Foster, Nate},
	year = {2016},
}

@inproceedings{rezvani_anomaly-free_2016,
	title = {Anomaly-{Free} {Policy} {Composition} in {Software}-{Defined} {Networks}},
	booktitle = {{IFIP} {Networking}},
	author = {Rezvani, Mohsen and Ignjatovic, Aleksandar and Pagnucco, Maurice and Jha, Sanjay},
	year = {2016},
}

@inproceedings{gabbrielli_no_2019,
	title = {No {More}, {No} {Less} - {A} {Formal} {Model} for {Serverless} {Computing}},
	booktitle = {coordination},
	author = {Gabbrielli, Maurizio and Giallorenzo, Saverio and Lanese, Ivan and Montesi, Fabrizio and Peressotti, Marco and Zingaro, Stefano Pio},
	year = {2019},
	pages = {148--157},
}

@inproceedings{narayana_compiling_2016,
	title = {Compiling {Path} {Queries}},
	booktitle = {nsdi},
	author = {Narayana, Srinivas and Tahmasbi, Mina and Rexford, Jennifer and Walker, David},
	year = {2016},
}

@misc{puppet_puppet_2016,
	title = {Puppet 4.8 {Reference} {Manual}: {Resource} {Type} {Reference}},
	url = {https://docs.puppet.com/puppet/4.8/reference/type.html},
	author = {Puppet, Inc.},
	year = {2016},
}

@inproceedings{fu_puppet_2017,
	title = {μ{Puppet}: {A} {Declarative} {Subset} of the {Puppet} {Configuration} {Language}},
	booktitle = {ecoop},
	author = {Fu, Weili and Perera, Roly and Cheney, James and Anderson, Paul},
	year = {2017},
}

@inproceedings{hanappi_asserting_2016,
	title = {Asserting {Reliable} {Convergence} for {Configuration} {Management} {Scripts}},
	booktitle = {oopsla},
	author = {Hanappi, Oliver and Hummer, Waldemar and Dustdar, Schahram},
	year = {2016},
}

@inproceedings{mcclurg_event-driven_2016,
	title = {Event-{Driven} {Network} {Programming}},
	booktitle = {pldi},
	author = {McClurg, Jedidiah and Hojjat, Hossein and Foster, Nate and Černý, Pavol},
	year = {2016},
}

@inproceedings{chen_felix_2016,
	title = {Felix: {Implementing} {Traffic} {Measurement} on {End} {Hosts} {Using} {Program} {Analysis}},
	booktitle = {sosr},
	author = {Chen, Haoxian and Foster, Nate and Silverman, Jake and Zhang, Michael Whittaker Brandon and Zhang, Rene},
	year = {2016},
}

@inproceedings{mazurak_abash_2007,
	title = {Abash: {Finding} {Bugs} in {Bash} {Scripts}},
	booktitle = {plas},
	author = {Mazurak, Karl and Zdancewic, Steve},
	year = {2007},
}

@inproceedings{moore_shill_2014,
	title = {Shill: {A} {Secure} {Shell} {Scripting} {Language}},
	booktitle = {osdi},
	author = {Moore, Scott and Dimoulas, Christos and King, Dan and Chong, Stephen},
	year = {2014},
}

@misc{puppet_inc_puppet_2005,
	title = {Puppet},
	url = {https://www.puppet.com},
	author = {{Puppet, Inc.}},
	year = {2005},
}

@misc{red_hat_inc_ansible_2012,
	title = {Ansible},
	url = {https://www.ansible.com},
	author = {{Red Hat, Inc.}},
	year = {2012},
}

@misc{google_inc_google_2016,
	title = {Google {App} {Engine} {Incident} 16008},
	url = {https://status.cloud.google.com/incident/appengine/16008},
	author = {{Google Inc.}},
	year = {2016},
}

@misc{puppet_inc_puppet_2016,
	title = {Puppet {Adds} {Two} {New} {Executives} to {Accelerate} its {Rapid} {Growth}},
	url = {https://puppet.com/company/press-room/releases/puppet-adds-two-new-executives-accelerate-its-rapid-growth},
	author = {{Puppet, Inc.}},
	year = {2016},
}

@misc{chef_chef_2009,
	title = {Chef, {Inc}.},
	url = {https://www.chef.io},
	author = {{Chef}},
	year = {2009},
}

@misc{bbc_news_fatal_2015,
	title = {Fatal {A400M} crash linked to data-wipe mistake},
	url = {http://www.bbc.com/news/technology-33078767},
	author = {{BBC News}},
	year = {2015},
}

@misc{the_skype_team_skype_2015,
	title = {Skype {Outage}: {An} {Update}, and an {Apology}},
	url = {https://blogs.skype.com/stories/2015/09/22/skype-outage-an-update-and-an-apology/},
	author = {{The Skype Team}},
	year = {2015},
}

@misc{facebook_engineering_team_more_2010,
	title = {More {Details} on {Today}'s {Outage}},
	url = {https://www.facebook.com/notes/facebook-engineering/more-details-on-todays-outage/431441338919/},
	author = {{Facebook Engineering Team}},
	year = {2010},
}

@misc{the_wall_street_journal_nyse_2015,
	title = {{NYSE} {Says} {Wednesday} {Outage} {Caused} by {Software} {Update}},
	url = {http://www.wsj.com/articles/stocks-trade-on-nyse-at-open-1436450975},
	author = {{The Wall Street Journal}},
	year = {2015},
}

@inproceedings{tang_holistic_2015,
	title = {Holistic {Configuration} {Management} at {Facebook}},
	booktitle = {sosp},
	author = {Tang, Chunqiang and Kooburat, Thawan and Venkatachalam, Pradeep and Chandler, Akshay and Wen, Zhe and Narayanan, Aravind and Dowell, Patrick and Karl, Robert},
	year = {2015},
}

@article{anderson_formal_2016,
	title = {A {Formal} {Semantics} for the {SmartFrog} {Configuration} {Language}},
	volume = {24},
	number = {2},
	journal = {Journal of Network and Systems Management},
	author = {Anderson, Paul and Herry, Herry},
	year = {2016},
	pages = {309--345},
}

@inproceedings{sherman_acms_2005,
	title = {{ACMS}: {The} {Akamai} {Configuration} {Management} {System}},
	booktitle = {nsdi},
	author = {Sherman, Alex and Lisiecki, Philip A. and Berkheimer, Andy and Wein, Joel},
	year = {2005},
}

@inproceedings{delaet_survey_2010,
	title = {A survey of system configuration tools},
	booktitle = {lisa},
	author = {Delaet, Thomas and Joosen, Wouter and Vanbrabant, Bart},
	year = {2010},
}

@misc{ubuntu_details_nodate,
	title = {Details of package {\textbackslash}textttgolang-go in trusty. {Retrieved} {Apr} 15, 2016 from http://packages.ubuntu.com/trusty/devel/golang-go},
	author = {{Ubuntu}},
}

@inproceedings{huang_confvalley_2015,
	title = {{ConfValley}: {A} {Systematic} {Configuation} {Validation} {Framework} for {Cloud} {Services}},
	booktitle = {eurosys},
	author = {Huang, Peng and Bolosky, William J. and Abhishek Singh, Yuanyuan Zhou},
	year = {2015},
}

@inproceedings{andreasen_determinacy_2014,
	title = {Determinacy in {Static} {Analysis} for {jQuery}},
	booktitle = {oopsla},
	author = {Andreasen, Esben and Møller, Anders},
	year = {2014},
}

@inproceedings{monsanto_composing_2013,
	title = {Composing {Software}-{Defined} {Networks}},
	booktitle = {nsdi},
	author = {Monsanto, Christopher and Reich, Joshua and Foster, Nate and Rexford, Jennifer and Walker, David},
	year = {2013},
}

@inproceedings{monsanto_compiler_2012,
	title = {A {Compiler} and {Run}-time {System} for {Network} {Programming} {Languages}},
	booktitle = {popl},
	author = {Monsanto, Christopher and Foster, Nate and Harrison, Rob and Walker, David},
	year = {2012},
}

@inproceedings{voellmy_nettle_2011,
	title = {Nettle: {Functional} {Reactive} {Programming} of {OpenFlow} {Networks}},
	booktitle = {padl},
	author = {Voellmy, Andreas and Hudak, Paul},
	year = {2011},
}

@inproceedings{fischer_engage_2012,
	title = {Engage: {A} {Deployment} {Management} {System}},
	booktitle = {pldi},
	author = {Fischer, Jeffery and Majumdar, Rupak and Esmaeilsabzali, Shahram},
	year = {2012},
}

@inproceedings{foster_frenetic_2011,
	title = {Frenetic: {A} {Network} {Programming} {Language}},
	booktitle = {icfp},
	author = {Foster, Nate and Harrison, Rob and Freedman, Michael J. and Monsanto, Christopher and Rexford, Jennifer and Story, Alec and Walker, David},
	year = {2011},
}

@misc{types_httpdocspuppetlabscomreferenceslatesttypehtml_nodate,
	title = {http://docs.puppetlabs.com/references/latest/type.html},
	author = {Types, Puppet Resource},
}

@inproceedings{hagemark_site_1989,
	title = {Site: {A} {Language} and {System} for {Configuring} {Many} {Computers} as {One} {Computing} {Site}},
	booktitle = {lisa},
	author = {Hagemark, Bent and Zadeck, Kenneth},
	year = {1989},
}

@inproceedings{nelson_tierless_2014,
	title = {Tierless {Programming} and {Reasoning} for {Software}-{Defined} {Networks}},
	booktitle = {nsdi},
	author = {Nelson, Tim and Ferguson, Andrew D. and Scheer, Michael J. G. and Krishnamurthi, Shriram},
	year = {2014},
}

@misc{customers_httpwwwchefiocustomers_nodate,
	title = {http://www.chef.io/customers},
	author = {Customers, Chef},
}

@misc{customers_httppuppetlabscomaboutcustomers_nodate,
	title = {http://puppetlabs.com/about/customers},
	author = {Customers, Puppet},
}

@misc{release_httppuppetlabscomaboutpress-releasespuppetconf-2014-it-automation-event-year-opens-record-attendance_nodate,
	title = {http://puppetlabs.com/about/press-releases/puppetconf-2014-it-automation-event-year-opens-record-attendance},
	author = {Release, PuppetConf 2014 Press},
}

@inproceedings{hummer_testing_2013,
	title = {Testing {Idempotence} and {Convergence} for {Infrastructure} as {Code}},
	booktitle = {middleware},
	author = {Hummer, Waldemar and Rosenberg, Florian and Oliveira, Fábio and Eilam, Tamar},
	year = {2013},
}

@inproceedings{tucker_programming_2001,
	title = {Programming {Languages} for {Software} {Configuration}},
	booktitle = {scm},
	author = {Tucker, David B. and Krishnamurthi, Shriram},
	year = {2001},
}

@article{dolstra_nixos_2010,
	title = {{NixOS}: {A} {Purely} {Functional} {Linux} {Distribution}},
	volume = {20},
	number = {5–6},
	journal = {jfp},
	author = {Dolstra, Eelco and Löh, Andreas and Pierron, Nicholas},
	year = {2010},
	pages = {577--615},
}

@inproceedings{anderson_towards_1994,
	title = {Towards a {High}-{Level} {Machine} {Configuration} {System}},
	booktitle = {lisa},
	author = {Anderson, Paul},
	year = {1994},
}

@article{kozen_kleene_1997,
	title = {Kleene {Algebra} with {Tests}},
	volume = {19},
	number = {3},
	journal = {toplas},
	author = {Kozen, Dexter},
	year = {1997},
	pages = {427--443},
}

@inproceedings{lerner_combining_2013,
	title = {Combining {Form} and {Function}: {Static} {Types} for {jQuery} {Programs}},
	booktitle = {ecoop},
	author = {Lerner, Benjamin S. and Elberty, Liam and Li, Jincheng and Krishnamurthi, Shriram},
	year = {2013},
}

@inproceedings{kang_formal_2012,
	title = {Formal specification of a {JavaScript} module system},
	booktitle = {oopsla},
	author = {Kang, Seonghoon and Ryu, Sukyoung},
	year = {2012},
}

@inproceedings{madsen_static_2015,
	title = {Static {Analysis} of {Event}-{Driven} {Node}.js {JavaScript} {Applications}},
	booktitle = {oopsla},
	author = {Madsen, Magnus and Tip, Frank and Lhoták, Ondrej},
	year = {2015},
}

@phdthesis{felleisen_calculi_1987,
	type = {{PhD} {Thesis}},
	title = {The {Calculi} of {Lambda}-ν-{CS} {Conversion}: {A} {Syntactic} {Theory} of {Control} and {State} in {Imperative} {Higher}-order {Programming} {Languages}},
	school = {Indiana University},
	author = {Felleisen, Matthias},
	year = {1987},
}

@inproceedings{fournet_fully_2013,
	title = {Fully {Abstract} {Compilation} to {JavaScript}},
	booktitle = {popl},
	author = {Fournet, Cedric and Swamy, Nikhil and Chen, Juan and Dagand, Pierre-Evariste and Strub, Pierre-Yves and Livshits, Benjamin},
	year = {2013},
}

@inproceedings{tucker_opium_2007,
	title = {{OPIUM}: {Optimal} {Package} {Install}/{Uninstall} {Manager}},
	booktitle = {icse},
	author = {Tucker, Chris and Shuffleton, David and Jhala, Ranjit and Lerner, Sorin},
	year = {2007},
}

@inproceedings{richards_concrete_2015,
	title = {Concrete {Types} for {TypeScript}},
	booktitle = {ecoop},
	author = {Richards, Gregor and Nardelli, Francesco Zappa and Vitek, Jan},
	year = {2015},
}

@mastersthesis{zalewski_javascript_2014,
	title = {{JavaScript} with {Blame}},
	school = {University of Edinburgh},
	author = {Zalewski, Jakub},
	year = {2014},
}

@inproceedings{arkoudas_verifying_2004,
	title = {Verifying a file system implementation},
	booktitle = {icfem},
	author = {Arkoudas, Konstantine and Zee, Karen and Kuncak, Viktor and Rinard, Martin},
	year = {2004},
}

@inproceedings{moura_z3_2008,
	title = {Z3: {An} {Efficient} {SMT} {Solver}},
	booktitle = {tacas},
	author = {Moura, Leonardo De and Bjørner, Nikolaj},
	year = {2008},
}

@inproceedings{robert_l_bocchino_type_2009,
	title = {A {Type} and {Effect} {System} for {Deterministic} {Parallel} {Java}},
	booktitle = {oopsla},
	author = {Robert L. Bocchino, Jr. and Adve, Vikram S. and Dig, Danny and Adve, Sarita V. and Heumann, Stephen and Komuravelli, Rakesh and Overbey, Jeffrey and Simmons, Patrick and Sung, Hyojin},
	year = {2009},
}

@inproceedings{vechev_automatic_2010,
	title = {Automatic {Verification} of {Determinism} for {Structured} {Parallel} {Programs}},
	booktitle = {sas},
	author = {Vechev, Martin and Yahav, Eran and Raman, Raghavan and Sarkar, Vivek},
	year = {2010},
}

@inproceedings{gardner_local_2014,
	title = {Local {Reasoning} about {POSIX} {File} {Systems}},
	booktitle = {esop},
	author = {Gardner, Philippa and Ntzik, Gian and Wright, Adam},
	year = {2014},
}

@article{morgan_specification_1984,
	title = {Specification of the {UNIX} {Filing} {System}},
	volume = {10},
	number = {2},
	journal = {tse},
	author = {Morgan, Carroll and Sufrin, Bernard},
	year = {1984},
	pages = {128--142},
}

@inproceedings{ridge_sibylfs_2015,
	title = {{SibylFS}: formal specification and oracle-based testing for {POSIX} and real-world file systems},
	booktitle = {sosp},
	author = {Ridge, Tom and Sheets, David and Tuerk, Thomas and Madhavapeddy, Anil and Giugliano, Andrea and Sewell, Peter},
	year = {2015},
}

@inproceedings{sadowski_singletrack_2009,
	title = {{SingleTrack}: {A} dynamic determinism checker for multithreaded programs},
	booktitle = {esop},
	author = {Sadowski, Caitlin and Freund, Stephen N. and Flanagan, Cormac},
	year = {2009},
}

@inproceedings{burnim_asserting_2009,
	title = {Asserting and {Checking} {Determinism} for {Multithreaded} {Programs}},
	booktitle = {esecfse},
	author = {Burnim, Jacob and Sen, Koushik},
	year = {2009},
}

@inproceedings{kawaguchi_deterministic_2012,
	title = {Deterministic {Parallelism} via {Liquid} {Effects}},
	booktitle = {pldi},
	author = {Kawaguchi, Ming and Rondon, Patrick and Bakst, Alexander and Jhala, Ranjit},
	year = {2012},
}

@inproceedings{christakis_formalizing_2014,
	title = {Formalizing and {Verifying} a {Modern} {Build} {Language}},
	booktitle = {fm},
	author = {Christakis, Maria and Leino, K. Rustan M. and Schulte, Wolfram},
	year = {2014},
}

@inproceedings{mccarthy_towards_1962,
	title = {Towards a {Mathematical} {Science} of {Computation}},
	booktitle = {{IFIP} {Congress}},
	author = {McCarthy, John},
	year = {1962},
}

@inproceedings{bohannon_boomerang_2008,
	title = {Boomerang: {Resourceful} {Lenses} for {String} {Data}},
	booktitle = {popl},
	author = {Bohannon, Aaron and Foster, J. Nathan and Pierce, Benjamin C. and Pilkiewicz, Alexandre and Schmitt, Alan},
	year = {2008},
}

@inproceedings{hinrichs_practical_2009,
	title = {Practical {Declarative} {Network} {Management}},
	booktitle = {wren},
	author = {Hinrichs, Tim and Gude, Natasha and Casado, Martin and Mitchell, John and Shenker, Scott},
	year = {2009},
}

@misc{university_httpwwwcscornelleducoursescs4110_nodate,
	title = {http://www.cs.cornell.edu/{Courses}/cs4110},
	author = {University, CS4110: Cornell},
}

@misc{investigations_emphonline_2014,
	title = {{\textbackslash}{emphOnline} {Advertising} and {Hidden} {Hazards} to {Consumer} {Security} and {Data} {Privacy}. {Available} at http://www.hsgac.senate.gov/download/?id={3B38A382}-{8E10}-4527-{904C}-{24F37A0D6220}},
	author = {Investigations, U. S. Senate Subcommittee on},
	year = {2014},
}

@misc{university_httpwwwcsbrowneducoursescs173_nodate,
	title = {http://www.cs.brown.edu/courses/cs173/},
	author = {University, CS173: Brown},
}

@book{chlila_certified_2013,
	title = {Certified {Programming} with {Dependent} {Types}},
	publisher = {MIT Press},
	author = {Chli`la, Adam},
	year = {2013},
}
