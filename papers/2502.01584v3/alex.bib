@misc{mirzadeh2024pre,
  title = {{{GSM-Symbolic}}: {{Understanding}} the {{Limitations}} of {{Mathematical Reasoning}} in {{Large Language Models}}},
  shorttitle = {{{GSM-Symbolic}}},
  author = {Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  year = {2024},
  month = oct,
  number = {arXiv:2410.05229},
  eprint = {2410.05229},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.05229},
  url = {http://arxiv.org/abs/2410.05229},
  urldate = {2024-10-13},
  abstract = {Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65\%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
}

@inproceedings{hendrycks_math2021,
  title = {Measuring {{Mathematical Problem Solving With}} the {{MATH Dataset}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems Track}} on {{Datasets}} and {{Benchmarks}} 1, {{NeurIPS Datasets}} and {{Benchmarks}} 2021, {{December}} 2021, Virtual},
  author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  editor = {Vanschoren, Joaquin and Yeung, Sai-Kit},
  year = {2021},
  url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html},
  urldate = {2024-12-10},
  annotation = {\_rekey: math},
  file = {/home/abgruszecki/zotero-pdf/hendrycks2021.pdf}
}

@techreport{zhong2024,
  title = {Achieving {$>$}97\% on {{GSM8K}}: {{Deeply Understanding}} the {{Problems Makes LLMs Better Solvers}} for {{Math Word Problems}}},
  shorttitle = {Achieving {$>$}97\% on {{GSM8K}}},
  author = {Zhong, Qihuang and Wang, Kang and Xu, Ziyang and Liu, Juhua and Ding, Liang and Du, Bo},
  year = {2024},
  month = oct,
  number = {arXiv:2404.14963},
  eprint = {2404.14963},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2404.14963},
  url = {http://arxiv.org/abs/2404.14963},
  urldate = {2025-02-03},
  abstract = {Chain-of-Thought (CoT) prompting has enhanced the performance of Large Language Models (LLMs) across various reasoning tasks. However, CoT still falls short in dealing with complex math word problems, as it usually suffers from three pitfalls: semantic misunderstanding errors, calculation errors, and step-missing errors. Prior studies involve addressing the calculation errors and step-missing errors, but neglect the semantic misunderstanding errors, which is the major factor limiting the reasoning performance of LLMs. To this end, we propose a simple-yet-effective method, namely Deeply Understanding the Problems (DUP), to improve the LLMs' math problem-solving ability by addressing semantic misunderstanding errors. The core of our method is to encourage the LLMs to deeply understand the problems and extract the key problem-solving information used for better reasoning. Extensive experiments on 10 diverse reasoning benchmarks show that our DUP method consistently outperforms the other counterparts by a large margin. More encouragingly, DUP achieves a new SOTA result on the GSM8K benchmark, with an accuracy of 97.1\% under the zero-shot setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/abgruszecki/zotero-pdf/zhong2024.pdf;/home/abgruszecki/Zotero/storage/547VYBIT/2404.html}
}

@techreport{lei2024,
  title = {{{MACM}}: {{Utilizing}} a {{Multi-Agent System}} for {{Condition Mining}} in {{Solving Complex Mathematical Problems}}},
  shorttitle = {{{MACM}}},
  author = {Lei, Bin and Zhang, Yi and Zuo, Shan and Payani, Ali and Ding, Caiwen},
  year = {2024},
  month = jul,
  number = {arXiv:2404.04735},
  eprint = {2404.04735},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2404.04735},
  url = {http://arxiv.org/abs/2404.04735},
  urldate = {2025-02-03},
  abstract = {Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in {\textbackslash}textbf\{advanced mathematical problems requiring complex, multi-step logical reasoning\}. To enhance their inferential capabilities, current research has delved into {\textbackslash}textit\{prompting engineering\}, exemplified by methodologies such as the Tree of Thought and Graph of Thought. Nonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability. In response to these limitations, this paper introduces the {\textbackslash}textit\{Multi-Agent System for conditional Mining\} ({\textbackslash}textbf\{MACM\}) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from \${\textbackslash}mathbf\{54.68{\textbackslash}\%\} {\textbackslash}text\{ to \} {\textbackslash}mathbf\{76.73{\textbackslash}\%\}\$. The code is available in {\textbackslash}url\{https://github.com/bin123apple/MACM\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  file = {/home/abgruszecki/zotero-pdf/lei2024.pdf;/home/abgruszecki/Zotero/storage/5U6AIIBB/2404.html}
}

@inproceedings{gulati2024,
  title = {Putnam-{{AXIOM}}: {{A Functional}} and {{Static Benchmark}} for {{Measuring Higher Level Mathematical Reasoning}}},
  shorttitle = {Putnam-{{AXIOM}}},
  booktitle = {The 4th {{Workshop}} on {{Mathematical Reasoning}} and {{AI}} at {{NeurIPS}}'24},
  author = {Gulati, Aryan and Miranda, Brando and Chen, Eric and Xia, Emily and Fronsdal, Kai and Dumont, Bruno de Moraes and Koyejo, Sanmi},
  year = {2024},
  month = oct,
  url = {https://openreview.net/forum?id=YXnwlZe0yf\&noteId=yrsGpHd0Sf},
  urldate = {2025-01-02},
  abstract = {As large language models (LLMs) continue to advance, many existing benchmarks designed to evaluate their reasoning capabilities are becoming saturated. Therefore, we present the Putnam-AXIOM Original benchmark consisting of 236 mathematical problems from the William Lowell Putnam Mathematical Competition, along with detailed step-by-step solutions. To preserve the Putnam-AXIOM benchmark's validity and mitigate potential data contamination, we created the Putnam-AXIOM Variation benchmark with functional variations of 52 problems. By programmatically altering problem elements like variables and constants, we can generate unlimited novel, equally challenging problems not found online. We see that almost all models have significantly lower accuracy in the variations than the original problems. Our results reveal that OpenAI's o1-preview, the best performing model, achieves merely 41.95{\textbackslash}\% accuracy on the Putnam-AXIOM Original but experiences around a 30{\textbackslash}\% reduction in accuracy on the variations' dataset when compared to corresponding original problems.},
  langid = {english},
  file = {/home/abgruszecki/zotero-pdf/gulati2024.pdf}
}

@misc{jiang_tokbias2024pre,
  title = {A {{Peek}} into {{Token Bias}}: {{Large Language Models Are Not Yet Genuine Reasoners}}},
  shorttitle = {A {{Peek}} into {{Token Bias}}},
  author = {Jiang, Bowen and Xie, Yangxinyu and Hao, Zhuoqun and Wang, Xiaomeng and Mallick, Tanwi and Su, Weijie J. and Taylor, Camillo J. and Roth, Dan},
  year = {2024},
  eprint = {2406.11050},
  doi = {10.48550/ARXIV.2406.11050},
  url = {https://doi.org/10.48550/arXiv.2406.11050},
  urldate = {2024-10-13},
  archiveprefix = {arXiv},
  annotation = {\_rekey: tokbias},
  file = {/home/abgruszecki/zotero-pdf/jiang_tokbias2024pre.pdf}
}

@techreport{perez2022,
  title = {Discovering {{Language Model Behaviors}} with {{Model-Written Evaluations}}},
  author = {Perez, Ethan and Ringer, Sam and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  year = {2022},
  month = {dec},
  number = {arXiv:2212.09251},
  eprint = {2212.09251},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2212.09251},
  url = {http://arxiv.org/abs/2212.09251},
  urldate = {2024-10-04},
  abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/abgruszecki/zotero-pdf/perez2022.pdf;/home/abgruszecki/Zotero/storage/7R3MUFQG/2212.html},
}

@misc{zhang2023pre,
  title = {How {{Language Model Hallucinations Can Snowball}}},
  author = {Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A.},
  year = {2023},
  month = {may},
  number = {arXiv:2305.13534},
  eprint = {2305.13534},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13534},
  url = {http://arxiv.org/abs/2305.13534},
  urldate = {2024-06-16},
  abstract = {A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67\% and 87\% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/abgruszecki/zotero-pdf/zhang2023pre.pdf;/home/abgruszecki/Zotero/storage/JVWF2HHP/2305.html},
}


@misc{qwq32b,
    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},
    url = {https://qwenlm.github.io/blog/qwq-32b/},
    author = {Qwen},
    month = {March},
    year = {2025}
}

@article{qwen2.5,
      title={Qwen2.5 Technical Report}, 
      author={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      journal={arXiv preprint arXiv:2412.15115},
      year={2024}
}

@misc{gpt-4-5-system-card,
	title = {{OpenAI} GPT-4.5 {System} {Card}},
	url = {https://openai.com/index/gpt-4-5-system-card/},
	language = {en-US},
	urldate = {2025-02-27},
	author = {OpenAI},
	year = {2025},
}

@misc{wang2025thoughtsplaceunderthinkingo1like,
      title={Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs}, 
      author={Yue Wang and Qiuzhi Liu and Jiahao Xu and Tian Liang and Xingyu Chen and Zhiwei He and Linfeng Song and Dian Yu and Juntao Li and Zhuosheng Zhang and Rui Wang and Zhaopeng Tu and Haitao Mi and Dong Yu},
      year={2025},
      eprint={2501.18585},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.18585}, 
}