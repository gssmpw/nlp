\section{Related Works}
\subsection{Benchmarking Referring Expression Comprehension}
Referring Expression Comprehension (REC) is a vision-language task where models must localize a target object in an image based on a natural language referring expression, requiring compositional understanding of attributes (e.g., color, size), spatial relationships, and object categories to disambiguate the referent from other entities. While early benchmarks like RefCOCO/+/g~\cite{mao2016generation,yu2016modeling,Nagaraja2016modeling} established foundational evaluation protocols, subsequent analyses revealed critical limitations. For instance, \cite{cirik-etal-2018-visual} and \cite{akula-etal-2020-words} demonstrated that models could exploit dataset biases rather than genuinely parse linguistic structure: \cite{akula-etal-2020-words} found that up to 83.7\% of RefCOCOg test instances could be solved via keyword matching or visual feature dominance, bypassing compositional reasoning. To address this, \cite{akula-etal-2020-words} introduced Ref-Adv, a dataset where perturbed expressions refer to alternate objects, forcing models to engage with linguistic structure. Subsequent efforts have prioritized compositional reasoning. CLEVR-Ref+~\cite{liu2019clevr} is a synthetic dataset emphasizing relationships, attributes, and linguistic logic. 
Cops-Ref~\cite{chen2020cops} and Ref-Reasoning~\cite{yang2020graph} use GQA scene graphs~\cite{hudson2019gqa} and rule-based methods to create large-scale compositional referring expression comprehension datasets in real-world scenarios. Cops-Ref additionally introduces distracting images based on attributes, relationships, and target names. Recent benchmarks like GITM-MR~\cite{Wu_2023_ICCV_GITM-MR} explores mismatched relationship in the REC task. ARPGrounding~\cite{zeng2024investigating} evaluates the model's compositional reasoning ability by constructing referring expression pairs that target distinct objects of the same category within a single image, differentiated solely through their attributes or relational contexts. RefEgo~\cite{kurita2023refego} and OmniLabel~\cite{schulter2023omnilabel} consider out-of-distribution scenarios where referred targets do not exist in the image.

Building on these advances, we propose a new REC benchmark that more comprehensively assesses the compositional reasoning abilities of multimodal models.
Our dataset incorporates controlled difficulty levels, compelling MLLMs to reason across object categories, attributes, and multi-hop relationships. Crucially, we also introduce negative text and image samples to evaluate model resilience against misalignments and hallucinations, providing a more thorough assessment of their true visual grounding capabilities. By simultaneously addressing compositional reasoning and incorporating negative samples, FineCops-Ref establishes a more realistic and challenging benchmark for advancing REC research in the MLLM era.

% Despite advancements in multimodal learning,
% current multimodal models, including advanced MLLMs like GPT-4V, exhibit poor compositional reasoning, often treating language as a bag of words without considering word order, attributes, or relationships between objects~\cite{suhr-etal-2019-corpus,ma2023crepe,diwan-etal-2022-winoground,tong2024eyes,yuksekgonul2022and,coco_cf,Kamath2024TheHP}. To evaluate these models of compositional reasoning, benchmarks often involve constructing hard negative captions to test models' capabilities, such as distinguishing between "a mug in some grass" and "some grass in a mug"~\cite{parcalabescu-etal-2022-valse,thrush2022winoground,ma2023crepe,hsieh_sugarcrepe_2023}. 
% Some benchmarks focus on negative images~\cite{ray_cola_2023,yarom_what_2023,zhang-etal-2024-countercurate,NEURIPS2023_coco_counter}, while others primarily focus on spatial relationships~\cite{zhang-etal-2024-countercurate,liu-etal-2023-visual,yang2019spatialsense,chen2024spatialvlm}.
% %\cite{hsieh_sugarcrepe_2023} found that previous benchmarks have language biases and that a simple grammar model can distinguish negative captions. 

\subsection{Specialist REC Models}
% The REC methods can generally be divided into two categories based on whether or not it uses LLMs: specialist models and MLLMs.
Early Specialist Models~\cite{hu2017modeling, zhang2018grounding, zhuang2018parallel, yu2018mattnet, liu2019improving, yang2019dynamic, hong2019learning, liu2019learning,Wang2018NeighbourhoodWR} typically employed pre-trained detectors to generate proposals and locate targets through text-to-region matching. Subsequent approaches transitioned to ViT-based architectures, typically adopting one-stage designs~\cite{zhou2021real, deng2021transvg, zhu2022seqtr, TransVG++2023}.
Recent advancements~\cite{kamath2021mdetr, yan2023universal, liu2023grounding, dMDETR23} have shifted towards DETR-based~\cite{carion2020end} frameworks, utilizing encoder-decoder architectures for multimodal fusion and generating target bounding boxes through object queries. Specifically, MDETR~\cite{kamath2021mdetr} pioneered the integration of free-form text with DETR, developing an end-to-end modulated detector capable of identifying objects in images based on raw text queries. UNINEXT~\cite{yan2023universal} implemented Deformable DETR~\cite{zhu2020deformable}, proposing a universal instance perception model. Grounding-DINO~\cite{liu2023grounding}, building upon the advanced DINO framework~\cite{zhang2022dino}, enhanced DINO by incorporating vision-language modality fusion at multiple stages. \cite{Zhao_2024_CVPR} explored the utilization of hard negative samples during training.
More recent models, such as OneRef~\cite{xiao2024oneref} and SimVG~\cite{dai2024simvg} have streamlined the architecture by eliminating complex integration designs and redundant parameters through the implementation of modality-shared feature spaces, employing a unified multimodal encoder. Specialist REC models, typically lightweight and specifically trained for vision tasks, excel in efficient and accurate low-level perception. However, they generally lack broad contextual reasoning capabilities, which limits their performance on complex REC tasks.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{image/pipeline.pdf}
  \caption{The data construction pipeline of FineCops-Ref. Given an image, we first generate paths based on its scene graph. Then, we fill paths into templates and obtain the positive referring expression through LLM rewriting. Meanwhile, we utilize LLM to generate negative expressions, and based on this, we employ diffusion model to create fine-grained editing negative images.}
  \label{pipeline}
\end{figure*}

\subsection{Multimodal Large Language Models for REC}
Unlike Specialist REC models, which usually treat REC as a bounding box regression task, MLLMs often formulate the bounding box prediction as a text generation problem, outputing bounding box coordinates in an autoregressive manner.
Recent MLLMs~\cite{chen2023shikra, you2024ferret, li-etal-2024-groundinggpt, wei2023lenna, wang2023cogvlm, qi2024cogcom, chen2024far, yang2023set} have leveraged the powerful capabilities of LLMs through zero-shot or instruction tuning methods to address complex REC tasks. These MLLMs also known as grounding MLLMs~\cite{xiao2024visualgroundingsurvey}.
%These methods integrate projected visual features with text tokens into LLM, which then directly outputs the bounding box.
Shikra~\cite{chen2023shikra} is an early model to support region-level input and understanding. 
% Ferret~\cite{you2024ferret} offers more flexible referring by implementing a hybrid representation scheme. 
GroundingGPT~\cite{li-etal-2024-groundinggpt} employs a coarse-to-fine training strategy to enhance modelâ€™s semantic awareness. CogVLM~\cite{wang2023cogvlm} incorporated a visual expert module in each transformer layer to enable dual fusion between vision and language features. CogCoM~\cite{qi2024cogcom} introduced Chain of Manipulations, encouraging MLLMs to generate responses through evidential visual reasoning. Regarding visual modules, some methods, such as GLaMM~\cite{Rasheed_2024_CVPR} and LLaVA-Grounding~\cite{zhang2023llava-g}, integrate additional visual components, while others, including Groma~\cite{ma2024groma}, VisualCoT~\cite{shao2024visualcot}, Ferret~\cite{you2024ferret}, and CoVLM~\cite{li2024covlm}, extract regional features as supplementary inputs. Recently, generalist MLLMs, such as Qwen2-VL~\cite{wang2024qwen2} and InternVL 2.5~\cite{chen2024expanding}, have demonstrated significant advancements in grounding capabilities while maintaining strong reasoning performance, achieved through enhanced model architectures, carefully curated datasets, and optimized training strategies.

% However, MLLMs employ LLMs with a vast number of parameters and FLOPs, often exceeding specialist models by tens to hundreds of times, resulting in substantial computational costs. Additionally, MLLMs project visual tokens into the input space of LLMs, increasing the input length. This results in computational complexity growing quadratically with the combined number of textual and visual tokens, also significantly amplifying the computational burden. Despite these computational limitations, MLLMs offer superior generalization and reasoning abilities compared to specialist models, making them suitable for complex REC tasks and scenarios requiring broader contextual understanding.

MLLMs demonstrate superior generalization and reasoning capabilities compared to Specialist Models, making them particularly well-suited for complex REC tasks demanding deeper contextual understanding. However, MLLMs inherit significant computational burdens from two key aspects. First, they rely on LLMs which possess substantially more parameters than Specialist Models. Second, the attention mechanisms used by MLLMs exhibit quadratic scaling of computational complexity. This scaling is further exacerbated by the increased input length resulting from the integration of numerous visual tokens (e.g., image patches) with textual tokens.

% MLLMs directly input the projected visual features into the LLM.
% Recent methods aim to enhance grounding capabilities in MLLMs through dataset construction with coordinate information and additional visual modules. Common methods for datasets include transforming traditional visual datasets into an instruction-following format using templates~\cite{li-etal-2024-groundinggpt,pramanick2023jack,wang_visionllm_2023}, correlating object coordinates with existing captions~\cite{peng2024grounding,qi2024cogcom}, and using LLMs to generate grounded question-answer pairs based on images, object coordinates, and captions~\cite{you2024ferret, wang2024the}. 
% % The All-Seeing Project~\cite{wang2024the} has recently introduced a new dataset (AS-1B) using a scalable data engine that incorporates human feedback and efficient models in the loop.

% In terms of visual modules, some methods integrate additional visual components, such as GLaMM~\cite{Rasheed_2024_CVPR} and LLaVA-Grounding~\cite{zhang2023llava-g}, while others extract regional features as additional inputs~\cite{ma2024groma,shao2024visualcot,you2024ferret,li2024covlm}.