@inproceedings{Kamath2024TheHP,
  title={The Hard Positive Truth About Vision-Language Compositionality},
  author={Amita Kamath and Cheng-Yu Hsieh and Kai-Wei Chang and Ranjay Krishna},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{NEURIPS2023_coco_counter,
 author = {Le, Tiep and LAL, VASUDEV and Howard, Phillip},
 booktitle = {NeurIPS},
 title = {COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs},
 year = {2023}
}

@InProceedings{Nagaraja2016modeling,
author="Nagaraja, Varun K.
and Morariu, Vlad I.
and Davis, Larry S.",
title="Modeling Context Between Objects for Referring Expression Understanding",
booktitle="ECCV",
year="2016",
}

@InProceedings{Rasheed_2024_CVPR,
    author    = {Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M. and Xing, Eric and Yang, Ming-Hsuan and Khan, Fahad S.},
    title     = {GLaMM: Pixel Grounding Large Multimodal Model},
    booktitle = {CVPR},
    year      = {2024},
}

@ARTICLE{TransVG++2023,
  author={Deng, Jiajun and Yang, Zhengyuan and Liu, Daqing and Chen, Tianlang and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang and Ouyang, Wanli},
  journal={TPAMI}, 
  title={TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer}, 
  year={2023},
}

@inproceedings{Wang2018NeighbourhoodWR,
  title={Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks},
  author={Peng Wang and Qi Wu and Jiewei Cao and Chunhua Shen and Lianli Gao and Anton van den Hengel},
  booktitle={CVPR},
  year={2018}
}

@InProceedings{Wu_2023_ICCV_GITM-MR,
    author    = {Wu, Yu and Wei, Yana and Wang, Haozhe and Liu, Yongfei and Yang, Sibei and He, Xuming},
    title     = {Grounded Image Text Matching with Mismatched Relation Reasoning},
    booktitle = {ICCV},
    year      = {2023},
}

@InProceedings{Zhao_2024_CVPR,
    author    = {Zhao, Shiyu and Zhao, Long and G, Vijay Kumar B and Suh, Yumin and Metaxas, Dimitris N. and Chandraker, Manmohan and Schulter, Samuel},
    title     = {Generating Enhanced Negatives for Training Language-Based Object Detectors},
    booktitle = {CVPR},
    year      = {2024},
}

@inproceedings{akula-etal-2020-words,
	title = "Words Aren{'}t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions",
	author = "Akula, Arjun  and
	  Gella, Spandana  and
	  Al-Onaizan, Yaser  and
	  Zhu, Song-Chun  and
	  Reddy, Siva",
	booktitle = {ACL},
	year = "2020",
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{chen2020cops,
  title={Cops-ref: A new dataset and task on compositional referring expression comprehension},
  author={Chen, Zhenfang and Wang, Peng and Ma, Lin and Wong, Kwan-Yee K and Wu, Qi},
  booktitle={CVPR},
  year={2020}
}

@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={Science China Information Sciences},
  year={2024}
}

@article{chen2024spatialvlm,
  title = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
  author = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  journal = {arXiv preprint arXiv:2401.12168},
  year = {2024},
}

@inproceedings{cirik-etal-2018-visual,
	title = "Visual Referring Expression Recognition: What Do Systems Actually Learn?",
	author = "Cirik, Volkan  and
	  Morency, Louis-Philippe  and
	  Berg-Kirkpatrick, Taylor",
	booktitle = {NAACL},
	year = "2018",
}

@InProceedings{coco_cf,
author="Lai, Chengen
and Song, Shengli
and Yan, Sitong
and Hu, Guangneng",
title="Improving Vision and Language Concepts Understanding with Multimodal Counterfactual Samples",
booktitle="ECCV",
year="2025",
}

@ARTICLE{dMDETR23,
  author={Shi, Fengyuan and Gao, Ruopeng and Huang, Weilin and Wang, Limin},
  journal={TPAMI}, 
  title={Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding}, 
  year={2024},
}

@inproceedings{dai2024simvg,
 author = {Dai, Ming and Yang, Lingfeng and Xu, Yihao and Feng, Zhenhua and Yang, Wankou},
 booktitle = {NeurIPS},
 title = {SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion},
 year = {2024}
}

@inproceedings{deng2021transvg,
  title={Transvg: End-to-end visual grounding with transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV},
  year={2021}
}

@article{hong2019learning,
  title={Learning to compose and reason with language tree structures for visual grounding},
  author={Hong, Richang and Liu, Daqing and Mo, Xiaoyu and He, Xiangnan and Zhang, Hanwang},
  journal={TPAMI}, 
  year={2019},
}

@inproceedings{hsieh_sugarcrepe_2023,
	title = {{SugarCrepe}: {Fixing} {Hackable} {Benchmarks} for {Vision}-{Language} {Compositionality}},
	booktitle = {NeurIPS},
	author = {Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},
	year = {2023},
}

@inproceedings{hu2017modeling,
  title={Modeling relationships in referential expressions with compositional modular networks},
  author={Hu, Ronghang and Rohrbach, Marcus and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{kamath2021mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{kurita2023refego,
  title={Refego: Referring expression comprehension dataset from first-person perception of ego4d},
  author={Kurita, Shuhei and Katsura, Naoki and Onami, Eri},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{li-etal-2024-groundinggpt,
	title = "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model",
	author = "Li, Zhaowei  and
	  Xu, Qi  and
	  Zhang, Dong  and
	  Song, Hang  and
	  Cai, YiQing  and
	  Qi, Qi  and
	  Zhou, Ran  and
	  Pan, Junting  and
	  Li, Zefeng  and
	  Tu, Vu  and
	  Huang, Zhida  and
	  Wang, Tao",
	booktitle = {ACL},
	year = "2024",
}

@inproceedings{liu2019clevr,
  title={Clevr-ref+: Diagnosing visual reasoning with referring expressions},
  author={Liu, Runtao and Liu, Chenxi and Bai, Yutong and Yuille, Alan L},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{liu2019improving,
  title={Improving referring expression grounding with cross-modal attention-guided erasing},
  author={Liu, Xihui and Wang, Zihao and Shao, Jing and Wang, Xiaogang and Li, Hongsheng},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{liu2019learning,
  title={Learning to assemble neural module tree networks for visual grounding},
  author={Liu, Daqing and Zhang, Hanwang and Wu, Feng and Zha, Zheng-Jun},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{ma2023crepe,
  title={Crepe: Can vision-language foundation models reason compositionally?},
  author={Ma, Zixian and Hong, Jerry and Gul, Mustafa Omer and Gandhi, Mona and Gao, Irena and Krishna, Ranjay},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{ma2024groma,
  title={Groma: Localized visual tokenization for grounding multimodal large language models},
  author={Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  year={2016}
}

@article{pramanick2023jack,
  title={Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model},
  author={Pramanick, Shraman and Han, Guangxing and Hou, Rui and Nag, Sayan and Lim, Ser-Nam and Ballas, Nicolas and Wang, Qifan and Chellappa, Rama and Almahairi, Amjad},
  journal={arXiv preprint arXiv:2312.12423},
  year={2023}
}

@article{qi2024cogcom,
  title={CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations},
  author={Qi, Ji and Ding, Ming and Wang, Weihan and Bai, Yushi and Lv, Qingsong and Hong, Wenyi and Xu, Bin and Hou, Lei and Li, Juanzi and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2402.04236},
  year={2024}
}

@inproceedings{ray_cola_2023,
	title = {Cola: {A} {Benchmark} for {Compositional} {Text}-to-image {Retrieval}},
	booktitle = {NeurIPS},
	author = {Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan and Krishna, Ranjay and Saenko, Kate},
	year = {2023},
}

@inproceedings{schulter2023omnilabel,
  title={Omnilabel: A challenging benchmark for language-based object detection},
  author={Schulter, Samuel and Suh, Yumin and Dafnis, Konstantinos M and Zhang, Zhixing and Zhao, Shiyu and Metaxas, Dimitris and others},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{shao2024visualcot,
 author = {Shao, Hao and Qian, Shengju and Xiao, Han and Song, Guanglu and ZONG, ZHUOFAN and Wang, Letian and Liu, Yu and Li, Hongsheng},
 booktitle = {NeurIPS},
 title = {Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning},
 year = {2024}
}

@inproceedings{thrush2022winoground,
  title={Winoground: Probing vision and language models for visio-linguistic compositionality},
  author={Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle={CVPR},
  year={2022}
}

@InProceedings{tong2024eyes,
    author    = {Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
    title     = {Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs},
    booktitle = {CVPR},
    year      = {2024},
}

@inproceedings{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  booktitle = {NeurIPS},
  year={2024}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{wang_visionllm_2023,
	title = {{VisionLLM}: {Large} {Language} {Model} is also an {Open}-{Ended} {Decoder} for {Vision}-{Centric} {Tasks}},
	booktitle = {NeurIPS},
	author = {Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and Dai, Jifeng},
	year = {2023},
}

@article{wei2023lenna,
  title={Lenna: Language enhanced reasoning detection assistant},
  author={Wei, Fei and Zhang, Xinyu and Zhang, Ailing and Zhang, Bo and Chu, Xiangxiang},
  journal={arXiv preprint arXiv:2312.02433},
  year={2023}
}

@article{xiao2024visualgroundingsurvey,
      title={Towards Visual Grounding: A Survey}, 
      author={Linhui Xiao and Xiaoshan Yang and Xiangyuan Lan and Yaowei Wang and Changsheng Xu},
      year={2024},
      journal={arXiv preprint arXiv:2412.20206},
}

@inproceedings{yan2023universal,
  title={Universal instance perception as object discovery and retrieval},
  author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{yang2019dynamic,
  title={Dynamic graph attention for referring expression comprehension},
  author={Yang, Sibei and Li, Guanbin and Yu, Yizhou},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{yang2019spatialsense,
  title={Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition},
  author={Yang, Kaiyu and Russakovsky, Olga and Deng, Jia},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{yang2020graph,
  title={Graph-structured referring expression reasoning in the wild},
  author={Yang, Sibei and Li, Guanbin and Yu, Yizhou},
  booktitle={CVPR},
  year={2020}
}

@article{yang2023set,
  title={Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v},
  author={Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.11441},
  year={2023}
}

@inproceedings{yarom_what_2023,
	title = {What {You} {See} is {What} {You} {Read}? {Improving} {Text}-{Image} {Alignment} {Evaluation}},
	booktitle = {NeurIPS},
	author = {Yarom, Michal and Bitton, Yonatan and Changpinyo, Soravit and Aharoni, Roee and Herzig, Jonathan and Lang, Oran and Ofek, Eran and Szpektor, Idan},
	year = {2023},
}

@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{yu2018mattnet,
  title={Mattnet: Modular attention network for referring expression comprehension},
  author={Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{yuksekgonul2022and,
  title={When and why vision-language models behave like bags-of-words, and what to do about it?},
  author={Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},
  booktitle={The Eleventh ICLR},
  year={2022}
}

@inproceedings{zeng2024investigating,
  title={Investigating Compositional Challenges in Vision-Language Models for Visual Grounding},
  author={Zeng, Yunan and Huang, Yan and Zhang, Jinjin and Jie, Zequn and Chai, Zhenhua and Wang, Liang},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhang2018grounding,
  title={Grounding referring expressions in images by variational context},
  author={Zhang, Hanwang and Niu, Yulei and Chang, Shih-Fu},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{zhang2023llava-g,
  title={Llava-grounding: Grounded visual chat with large multimodal models},
  author={Zhang, Hao and Li, Hongyang and Li, Feng and Ren, Tianhe and Zou, Xueyan and Liu, Shilong and Huang, Shijia and Gao, Jianfeng and Leizhang and Li, Chunyuan and others},
  booktitle={ECCV},
  year={2024},
}

@article{zhou2021real,
  title={A real-time global inference network for one-stage referring expression comprehension},
  author={Zhou, Yiyi and Ji, Rongrong and Luo, Gen and Sun, Xiaoshuai and Su, Jinsong and Ding, Xinghao and Lin, Chia-Wen and Tian, Qi},
  journal={TNNLS},
  year={2021},
}

@inproceedings{zhu2022seqtr,
  title={Seqtr: A simple yet universal network for visual grounding},
  author={Zhu, Chaoyang and Zhou, Yiyi and Shen, Yunhang and Luo, Gen and Pan, Xingjia and Lin, Mingbao and Chen, Chao and Cao, Liujuan and Sun, Xiaoshuai and Ji, Rongrong},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{zhuang2018parallel,
  title={Parallel attention: A unified framework for visual object discovery through dialogs and queries},
  author={Zhuang, Bohan and Wu, Qi and Shen, Chunhua and Reid, Ian and Van Den Hengel, Anton},
  booktitle={CVPR},
  year={2018}
}

