\section{Related Works}
\subsection{Benchmarking Referring Expression Comprehension}
Referring Expression Comprehension (REC) is a vision-language task where models must localize a target object in an image based on a natural language referring expression, requiring compositional understanding of attributes (e.g., color, size), spatial relationships, and object categories to disambiguate the referent from other entities. While early benchmarks like RefCOCO/+/g____ established foundational evaluation protocols, subsequent analyses revealed critical limitations. For instance, ____ and ____ demonstrated that models could exploit dataset biases rather than genuinely parse linguistic structure: ____ found that up to 83.7\% of RefCOCOg test instances could be solved via keyword matching or visual feature dominance, bypassing compositional reasoning. To address this, ____ introduced Ref-Adv, a dataset where perturbed expressions refer to alternate objects, forcing models to engage with linguistic structure. Subsequent efforts have prioritized compositional reasoning. CLEVR-Ref+____ is a synthetic dataset emphasizing relationships, attributes, and linguistic logic. 
Cops-Ref____ and Ref-Reasoning____ use GQA scene graphs____ and rule-based methods to create large-scale compositional referring expression comprehension datasets in real-world scenarios. Cops-Ref additionally introduces distracting images based on attributes, relationships, and target names. Recent benchmarks like GITM-MR____ explores mismatched relationship in the REC task. ARPGrounding____ evaluates the model's compositional reasoning ability by constructing referring expression pairs that target distinct objects of the same category within a single image, differentiated solely through their attributes or relational contexts. RefEgo____ and OmniLabel____ consider out-of-distribution scenarios where referred targets do not exist in the image.

Building on these advances, we propose a new REC benchmark that more comprehensively assesses the compositional reasoning abilities of multimodal models.
Our dataset incorporates controlled difficulty levels, compelling MLLMs to reason across object categories, attributes, and multi-hop relationships. Crucially, we also introduce negative text and image samples to evaluate model resilience against misalignments and hallucinations, providing a more thorough assessment of their true visual grounding capabilities. By simultaneously addressing compositional reasoning and incorporating negative samples, FineCops-Ref establishes a more realistic and challenging benchmark for advancing REC research in the MLLM era.

% Despite advancements in multimodal learning,
% current multimodal models, including advanced MLLMs like GPT-4V, exhibit poor compositional reasoning, often treating language as a bag of words without considering word order, attributes, or relationships between objects____. To evaluate these models of compositional reasoning, benchmarks often involve constructing hard negative captions to test models' capabilities, such as distinguishing between "a mug in some grass" and "some grass in a mug"____. 
% Some benchmarks focus on negative images____, while others primarily focus on spatial relationships____.
% %____ found that previous benchmarks have language biases and that a simple grammar model can distinguish negative captions. 

\subsection{Specialist REC Models}
% The REC methods can generally be divided into two categories based on whether or not it uses LLMs: specialist models and MLLMs.
Early Specialist Models____ typically employed pre-trained detectors to generate proposals and locate targets through text-to-region matching. Subsequent approaches transitioned to ViT-based architectures, typically adopting one-stage designs____.
Recent advancements____ have shifted towards DETR-based____ frameworks, utilizing encoder-decoder architectures for multimodal fusion and generating target bounding boxes through object queries. Specifically, MDETR____ pioneered the integration of free-form text with DETR, developing an end-to-end modulated detector capable of identifying objects in images based on raw text queries. UNINEXT____ implemented Deformable DETR____, proposing a universal instance perception model. Grounding-DINO____, building upon the advanced DINO framework____, enhanced DINO by incorporating vision-language modality fusion at multiple stages. ____ explored the utilization of hard negative samples during training.
More recent models, such as OneRef____ and SimVG____ have streamlined the architecture by eliminating complex integration designs and redundant parameters through the implementation of modality-shared feature spaces, employing a unified multimodal encoder. Specialist REC models, typically lightweight and specifically trained for vision tasks, excel in efficient and accurate low-level perception. However, they generally lack broad contextual reasoning capabilities, which limits their performance on complex REC tasks.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{image/pipeline.pdf}
  \caption{The data construction pipeline of FineCops-Ref. Given an image, we first generate paths based on its scene graph. Then, we fill paths into templates and obtain the positive referring expression through LLM rewriting. Meanwhile, we utilize LLM to generate negative expressions, and based on this, we employ diffusion model to create fine-grained editing negative images.}
  \label{pipeline}
\end{figure*}

\subsection{Multimodal Large Language Models for REC}
Unlike Specialist REC models, which usually treat REC as a bounding box regression task, MLLMs often formulate the bounding box prediction as a text generation problem, outputing bounding box coordinates in an autoregressive manner.
Recent MLLMs____ have leveraged the powerful capabilities of LLMs through zero-shot or instruction tuning methods to address complex REC tasks. These MLLMs also known as grounding MLLMs____.
%These methods integrate projected visual features with text tokens into LLM, which then directly outputs the bounding box.
Shikra____ is an early model to support region-level input and understanding. 
% Ferret____ offers more flexible referring by implementing a hybrid representation scheme. 
GroundingGPT____ employs a coarse-to-fine training strategy to enhance modelâ€™s semantic awareness. CogVLM____ incorporated a visual expert module in each transformer layer to enable dual fusion between vision and language features. CogCoM____ introduced Chain of Manipulations, encouraging MLLMs to generate responses through evidential visual reasoning. Regarding visual modules, some methods, such as GLaMM____ and LLaVA-Grounding____, integrate additional visual components, while others, including Groma____, VisualCoT____, Ferret____, and CoVLM____, extract regional features as supplementary inputs. Recently, generalist MLLMs, such as Qwen2-VL____ and InternVL 2.5____, have demonstrated significant advancements in grounding capabilities while maintaining strong reasoning performance, achieved through enhanced model architectures, carefully curated datasets, and optimized training strategies.

% However, MLLMs employ LLMs with a vast number of parameters and FLOPs, often exceeding specialist models by tens to hundreds of times, resulting in substantial computational costs. Additionally, MLLMs project visual tokens into the input space of LLMs, increasing the input length. This results in computational complexity growing quadratically with the combined number of textual and visual tokens, also significantly amplifying the computational burden. Despite these computational limitations, MLLMs offer superior generalization and reasoning abilities compared to specialist models, making them suitable for complex REC tasks and scenarios requiring broader contextual understanding.

MLLMs demonstrate superior generalization and reasoning capabilities compared to Specialist Models, making them particularly well-suited for complex REC tasks demanding deeper contextual understanding. However, MLLMs inherit significant computational burdens from two key aspects. First, they rely on LLMs which possess substantially more parameters than Specialist Models. Second, the attention mechanisms used by MLLMs exhibit quadratic scaling of computational complexity. This scaling is further exacerbated by the increased input length resulting from the integration of numerous visual tokens (e.g., image patches) with textual tokens.

% MLLMs directly input the projected visual features into the LLM.
% Recent methods aim to enhance grounding capabilities in MLLMs through dataset construction with coordinate information and additional visual modules. Common methods for datasets include transforming traditional visual datasets into an instruction-following format using templates____, correlating object coordinates with existing captions____, and using LLMs to generate grounded question-answer pairs based on images, object coordinates, and captions____. 
% % The All-Seeing Project____ has recently introduced a new dataset (AS-1B) using a scalable data engine that incorporates human feedback and efficient models in the loop.

% In terms of visual modules, some methods integrate additional visual components, such as GLaMM____ and LLaVA-Grounding____, while others extract regional features as additional inputs____.