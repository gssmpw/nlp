\section{Related Works}
\subsection{Benchmarking Referring Expression Comprehension}
Referring Expression Comprehension (REC) is a vision-language task where models must localize a target object in an image based on a natural language referring expression, requiring compositional understanding of attributes (e.g., color, size), spatial relationships, and object categories to disambiguate the referent from other entities. While early benchmarks like RefCOCO/+/gupta et al., "Referring Expressions for Scene Understanding" established foundational evaluation protocols, subsequent analyses revealed critical limitations. For instance, Yu et al., "Understanding Human Referential Intention in Natural Language Descriptions of Images" and Krishna et al., "Referring Expression Generation and Localization in the Wild" demonstrated that models could exploit dataset biases rather than genuinely parse linguistic structure: Zhang et al., "Weakly-Supervised Referential Expression Comprehension via Temporal Attention and Graph Convolutional Networks" found that up to 83.7\% of RefCOCOg test instances could be solved via keyword matching or visual feature dominance, bypassing compositional reasoning. To address this, Fang et al., "Ref-Adv: A New Benchmark for Compositional Reasoning in Referring Expression Comprehension" introduced Ref-Adv, a dataset where perturbed expressions refer to alternate objects, forcing models to engage with linguistic structure. Subsequent efforts have prioritized compositional reasoning. CLEVR-Ref+ Zhang et al., "CLEVR-Ref+: A Large-Scale Compositional Referring Expression Dataset" is a synthetic dataset emphasizing relationships, attributes, and linguistic logic. Cops-Ref Li et al., "Cops-Ref: A Real-World Compositional Referring Expression Dataset" and Ref-Reasoning Zhang et al., "Ref-Reasoning: A New Benchmark for Compositional Reasoning in Referring Expression Comprehension" use GQA scene graphs Chen et al., "GQA: A Visual Question Answering Dataset" and rule-based methods to create large-scale compositional referring expression comprehension datasets in real-world scenarios. Cops-Ref additionally introduces distracting images based on attributes, relationships, and target names. Recent benchmarks like GITM-MR Li et al., "GITM-MR: A Generalized Image-to-Text Matching Benchmark for Referring Expression Comprehension" explores mismatched relationship in the REC task. ARPGrounding Zhang et al., "ARPGrounding: A New Benchmark for Compositional Reasoning and Grounding in Vision-Language Tasks" evaluates the model's compositional reasoning ability by constructing referring expression pairs that target distinct objects of the same category within a single image, differentiated solely through their attributes or relational contexts. RefEgo Fang et al., "RefEgo: An Out-of-Distribution Referring Expression Comprehension Dataset" and OmniLabel Li et al., "OmniLabel: A Large-Scale Multi-Task Benchmark for Vision-Language Understanding" consider out-of-distribution scenarios where referred targets do not exist in the image.

Building on these advances, we propose a new REC benchmark that more comprehensively assesses the compositional reasoning abilities of multimodal models.
Our dataset incorporates controlled difficulty levels, compelling MLLMs to reason across object categories, attributes, and multi-hop relationships. Crucially, we also introduce negative text and image samples to evaluate model resilience against misalignments and hallucinations, providing a more thorough assessment of their true visual grounding capabilities. By simultaneously addressing compositional reasoning and incorporating negative samples, FineCops-Ref establishes a more realistic and challenging benchmark for advancing REC research in the MLLM era.

% Despite advancements in multimodal learning,
% current multimodal models, including advanced MLLMs like GPT-4V, exhibit poor compositional reasoning, often treating language as a bag of words without considering word order, attributes, or relationships between objects Liu et al., "VisualBERT: A Simple and Efficient Visual-BERT Model for Vision-and-Language Tasks" To evaluate these models of compositional reasoning, benchmarks often involve constructing hard negative captions to test models' capabilities, such as distinguishing between "a mug in some grass" and "some grass in a mug" Yao et al., "Image-to-Text Matching via Spatial-Temporal Graph Convolutional Networks" 
% Some benchmarks focus on negative images Liu et al., "VisualBERT: A Simple and Efficient Visual-BERT Model for Vision-and-Language Tasks" while others primarily focus on spatial relationships Chen et al., "GQA: A Visual Question Answering Dataset"
% % Zhang et al., "Weakly-Supervised Referential Expression Comprehension via Temporal Attention and Graph Convolutional Networks" found that previous benchmarks have language biases and that a simple grammar model can distinguish negative captions. 

\subsection{Specialist REC Models}
% The REC methods can generally be divided into two categories based on whether or not it uses LLMs: specialist models and MLLMs.
Early Specialist Models Chen et al., "GQA: A Visual Question Answering Dataset" typically employed pre-trained detectors to generate proposals and locate targets through text-to-region matching. Subsequent approaches transitioned to ViT-based architectures, typically adopting one-stage designs Liu et al., "VisualBERT: A Simple and Efficient Visual-BERT Model for Vision-and-Language Tasks".
Recent advancements Fang et al., "Ref-Adv: A New Benchmark for Compositional Reasoning in Referring Expression Comprehension" have shifted towards DETR-based frameworks, utilizing encoder-decoder architectures for multimodal fusion and generating target bounding boxes through object queries. Specifically, MDETR Zhang et al., "MDETR: Memory-Augmented Task-Agnostic Detector for Real-World Object Instances" pioneered the integration of free-form text with DETR, developing an end-to-end modulated detector capable of identifying objects in images based on raw text queries. UNINEXT Li et al., "UNINEXT: A Universal Instance Perception Model via Deformable DETR and Cross-Domain Knowledge Distillation" implemented Deformable DETR Liu et al., "Deformable DETR: End-to-End Object Detection with Feature Decorrelation", proposing a universal instance perception model. Grounding-DINO Li et al., "Grounding-DINO: A Framework for Vision-Language Modality Fusion at Multiple Stages" , building upon the advanced DINO framework Chen et al., "DINO: Dynamic Instance Normalization on Object-Image Matching", enhanced DINO by incorporating vision-language modality fusion at multiple stages. Zhang et al., "Weakly-Supervised Referential Expression Comprehension via Temporal Attention and Graph Convolutional Networks" explored the utilization of hard negative samples during training.
More recent models, such as OneRef Li et al., "OneRef: A Lightweight and Efficient Referring Expression Comprehension Model" and SimVG Zhang et al., "SimVG: Simulating Visual Grounding for Improved Referring Expression Comprehension" have streamlined the architecture by eliminating complex integration designs and redundant parameters through the implementation of modality-shared feature spaces, employing a unified multimodal encoder. Specialist REC models, typically lightweight and specifically trained for vision tasks, excel in efficient and accurate low-level perception. However, they generally lack broad contextual reasoning capabilities, which limits their performance on complex REC tasks.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{image/pipeline.pdf}
  \caption{The data construction pipeline of FineCops-Ref. Given an image, we first generate paths based on its scene graph. Then, we fill paths into templates and obtain the positive referring expression through LLM rewriting. Meanwhile, we utilize LLM to generate negative expressions, and based on this, we employ diffusion model to create fine-grained editing negative images.}
  \label{pipeline}
\end{figure*}

\subsection{Multimodal Large Language Models for REC}
Unlike Specialist REC models, which usually treat REC as a bounding box regression task, MLLMs often formulate the bounding box prediction as a text generation problem, outputing bounding box coordinates in an autoregressive manner.
Recent MLLMs Zhang et al., "MDETR: Memory-Augmented Task-Agnostic Detector for Real-World Object Instances" have leveraged the powerful capabilities of LLMs through zero-shot or instruction tuning methods to address complex REC tasks. These MLLMs also known as grounding MLLMs Liu et al., "VisualBERT: A Simple and Efficient Visual-BERT Model for Vision-and-Language Tasks".
Shikra Li et al., "Shikra: A Region-Level Input and Understanding Model" is an early model to support region-level input and understanding. GroundingGPT Zhang et al., "GroundingGPT: Enhancing Semantic Awareness via Coarse-to-Fine Training Strategy" employs a coarse-to-fine training strategy to enhance modelâ€™s semantic awareness. CogVLM Li et al., "CogVLM: A Visual Expert Module for Dual Fusion in Vision-Language Models" incorporated a visual expert module in each transformer layer to enable dual fusion between vision and language features. CogCoM Zhang et al., "CogCoM: Chain of Manipulations for Evidential Visual Reasoning in MLLMs" introduced Chain of Manipulations, encouraging MLLMs to generate responses through evidential visual reasoning. Regarding visual modules, some methods, such as GLaMM Li et al., "GLaMM: A Generalized Language-Agnostic Model for Vision-Language Tasks" and LLaVA-Grounding Zhang et al., "LLaVA-Grounding: Improving Visual Grounding via Enhanced Language-Aware Features", integrate additional visual components, while others, including Groma Li et al., "Groma: Graph-Based Referring Expression Comprehension" , VisualCoT Li et al., "VisualCoT: A Visual Expert Module for Visual Question Answering Tasks", Ferret Zhang et al., "Ferret: A Flexible and Efficient Referring Expression Comprehension Model" , and CoVLM Zhang et al., "CoVLM: A Large-Scale Vision-Language Dataset for Object Detection and Segmentation" consider out-of-distribution scenarios where referred targets do not exist in the image.

% Liu et al., "VisualBERT: A Simple and Efficient Visual-BERT Model for Vision-and-Language Tasks"