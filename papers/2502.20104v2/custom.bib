@inproceedings{wu2024dettoolchain,
  title={Dettoolchain: A new prompting paradigm to unleash detection ability of mllm},
  author={Wu, Yixuan and Wang, Yizhou and Tang, Shixiang and Wu, Wenhao and He, Tong and Ouyang, Wanli and Torr, Philip and Wu, Jian},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{cirik-etal-2018-visual,
	title = "Visual Referring Expression Recognition: What Do Systems Actually Learn?",
	author = "Cirik, Volkan  and
	  Morency, Louis-Philippe  and
	  Berg-Kirkpatrick, Taylor",
	booktitle = {NAACL},
	year = "2018",
}

@inproceedings{akula-etal-2020-words,
	title = "Words Aren{'}t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions",
	author = "Akula, Arjun  and
	  Gella, Spandana  and
	  Al-Onaizan, Yaser  and
	  Zhu, Song-Chun  and
	  Reddy, Siva",
	booktitle = {ACL},
	year = "2020",
}

@inproceedings{zhang2018grounding,
  title={Grounding referring expressions in images by variational context},
  author={Zhang, Hanwang and Niu, Yulei and Chang, Shih-Fu},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{dai2024simvg,
 author = {Dai, Ming and Yang, Lingfeng and Xu, Yihao and Feng, Zhenhua and Yang, Wankou},
 booktitle = {NeurIPS},
 title = {SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion},
 year = {2024}
}

@inproceedings{cheng2024yolo,
  title={Yolo-world: Real-time open-vocabulary object detection},
  author={Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{jeong2024proxydet,
  title={ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open-Vocabulary Object Detection},
  author={Jeong, Joonhyun and Park, Geondo and Yoo, Jayeon and Jung, Hyungsik and Kim, Heesu},
  booktitle={AAAI},
  year={2024}
}

@article{wang2024ov,
  title={OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion},
  author={Wang, Hao and Ren, Pengzhen and Jie, Zequn and Dong, Xiao and Feng, Chengjian and Qian, Yinlong and Ma, Lin and Jiang, Dongmei and Wang, Yaowei and Lan, Xiangyuan and others},
  journal={arXiv preprint arXiv:2407.07844},
  year={2024}
}

@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={EMNLP},
  year={2014}
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{
xiao2024oneref,
title={OneRef:  Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling},
author={Linhui Xiao and Xiaoshan Yang and Fang Peng and Yaowei Wang and Changsheng Xu},
booktitle={NeurIPS},
year={2024},
}

@ARTICLE{dMDETR23,
  author={Shi, Fengyuan and Gao, Ruopeng and Huang, Weilin and Wang, Limin},
  journal={TPAMI}, 
  title={Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding}, 
  year={2024},
}


@ARTICLE{TransVG++2023,
  author={Deng, Jiajun and Yang, Zhengyuan and Liu, Daqing and Chen, Tianlang and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang and Ouyang, Wanli},
  journal={TPAMI}, 
  title={TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer}, 
  year={2023},
}

@inproceedings{deng2021transvg,
  title={Transvg: End-to-end visual grounding with transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV},
  year={2021}
}

@article{zhou2021real,
  title={A real-time global inference network for one-stage referring expression comprehension},
  author={Zhou, Yiyi and Ji, Rongrong and Luo, Gen and Sun, Xiaoshuai and Su, Jinsong and Ding, Xinghao and Lin, Chia-Wen and Tian, Qi},
  journal={TNNLS},
  year={2021},
}

@inproceedings{zhu2022seqtr,
  title={Seqtr: A simple yet universal network for visual grounding},
  author={Zhu, Chaoyang and Zhou, Yiyi and Shen, Yunhang and Luo, Gen and Pan, Xingjia and Lin, Mingbao and Chen, Chao and Cao, Liujuan and Sun, Xiaoshuai and Ji, Rongrong},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{
zhu2020deformable,
title={Deformable detr: Deformable Transformers for End-to-End Object Detection},
author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
booktitle={ICLR},
year={2021}
}

@inproceedings{
zhang2022dino,
title={{DINO}: {DETR} with Improved DeNoising Anchor Boxes for End-to-End Object Detection},
author={Hao Zhang and Feng Li and Shilong Liu and Lei Zhang and Hang Su and Jun Zhu and Lionel Ni and Heung-Yeung Shum},
booktitle={ICLR},
year={2023}
}

@inproceedings{hu2017modeling,
  title={Modeling relationships in referential expressions with compositional modular networks},
  author={Hu, Ronghang and Rohrbach, Marcus and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{liu2019improving,
  title={Improving referring expression grounding with cross-modal attention-guided erasing},
  author={Liu, Xihui and Wang, Zihao and Shao, Jing and Wang, Xiaogang and Li, Hongsheng},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{zhuang2018parallel,
  title={Parallel attention: A unified framework for visual object discovery through dialogs and queries},
  author={Zhuang, Bohan and Wu, Qi and Shen, Chunhua and Reid, Ian and Van Den Hengel, Anton},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{yu2018mattnet,
  title={Mattnet: Modular attention network for referring expression comprehension},
  author={Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{yang2019dynamic,
  title={Dynamic graph attention for referring expression comprehension},
  author={Yang, Sibei and Li, Guanbin and Yu, Yizhou},
  booktitle={ICCV},
  year={2019}
}

@article{hong2019learning,
  title={Learning to compose and reason with language tree structures for visual grounding},
  author={Hong, Richang and Liu, Daqing and Mo, Xiaoyu and He, Xiangnan and Zhang, Hanwang},
  journal={TPAMI}, 
  year={2019},
}

@inproceedings{liu2019learning,
  title={Learning to assemble neural module tree networks for visual grounding},
  author={Liu, Daqing and Zhang, Hanwang and Wu, Feng and Zha, Zheng-Jun},
  booktitle={ICCV},
  year={2019}
}

@InProceedings{coco_cf,
author="Lai, Chengen
and Song, Shengli
and Yan, Sitong
and Hu, Guangneng",
title="Improving Vision and Language Concepts Understanding with Multimodal Counterfactual Samples",
booktitle="ECCV",
year="2025",
}

@inproceedings{zeng2024investigating,
  title={Investigating Compositional Challenges in Vision-Language Models for Visual Grounding},
  author={Zeng, Yunan and Huang, Yan and Zhang, Jinjin and Jie, Zequn and Chai, Zhenhua and Wang, Liang},
  booktitle={CVPR},
  year={2024}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{xiao2024visualgroundingsurvey,
      title={Towards Visual Grounding: A Survey}, 
      author={Linhui Xiao and Xiaoshan Yang and Xiangyuan Lan and Yaowei Wang and Changsheng Xu},
      year={2024},
      journal={arXiv preprint arXiv:2412.20206},
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{Kamath2024TheHP,
  title={The Hard Positive Truth About Vision-Language Compositionality},
  author={Amita Kamath and Cheng-Yu Hsieh and Kai-Wei Chang and Ranjay Krishna},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{
you2024ferret,
title={Ferret: Refer and Ground Anything Anywhere at Any Granularity},
author={Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang},
booktitle={ICLR},
year={2024}
}

@inproceedings{li-etal-2024-groundinggpt,
	title = "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model",
	author = "Li, Zhaowei  and
	  Xu, Qi  and
	  Zhang, Dong  and
	  Song, Hang  and
	  Cai, YiQing  and
	  Qi, Qi  and
	  Zhou, Ran  and
	  Pan, Junting  and
	  Li, Zefeng  and
	  Tu, Vu  and
	  Huang, Zhida  and
	  Wang, Tao",
	booktitle = {ACL},
	year = "2024",
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@InProceedings{Wu_2023_ICCV_GITM-MR,
    author    = {Wu, Yu and Wei, Yana and Wang, Haozhe and Liu, Yongfei and Yang, Sibei and He, Xuming},
    title     = {Grounded Image Text Matching with Mismatched Relation Reasoning},
    booktitle = {ICCV},
    year      = {2023},
}

@article{yildirim2023inst,
  title={Inst-inpaint: Instructing to remove objects with diffusion models},
  author={Yildirim, Ahmet Burak and Baday, Vedat and Erdem, Erkut and Erdem, Aykut and Dundar, Aysegul},
  journal={arXiv preprint arXiv:2304.03246},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{
peng2024grounding,
title={Grounding Multimodal Large Language Models to the World},
author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei},
booktitle={The Twelfth ICLR},
year={2024}
}

@inproceedings{zhuang2023task,
  title={A task is worth one word: Learning with task prompts for high-quality versatile image inpainting},
  author={Zhuang, Junhao and Zeng, Yanhong and Liu, Wenran and Yuan, Chun and Chen, Kai},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{wang2023dire,
  title={Dire for diffusion-generated image detection},
  author={Wang, Zhendong and Bao, Jianmin and Zhou, Wengang and Wang, Weilun and Hu, Hezhen and Chen, Hong and Li, Houqiang},
  booktitle={ICCV},
  year={2023}
}

@article{
oquab2023dinov2,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal={TMLR},
year={2024},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021},
}

@inproceedings{lin2023visualgptscore,
author = {Lin, Zhiqiu and Chen, Xinyue and Pathak, Deepak and Zhang, Pengchuan and Ramanan, Deva},
title = {Revisiting the role of language priors in vision-language models},
year = {2024},
booktitle = {ICML},
}

@inproceedings{
li2024covlm,
title={Co{VLM}: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding},
author={Junyan Li and Delin Chen and Yining Hong and Zhenfang Chen and Peihao Chen and Yikang Shen and Chuang Gan},
booktitle={ICLR},
year={2024},
}

@article{pramanick2023jack,
  title={Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model},
  author={Pramanick, Shraman and Han, Guangxing and Hou, Rui and Nag, Sayan and Lim, Ser-Nam and Ballas, Nicolas and Wang, Qifan and Chellappa, Rama and Almahairi, Amjad},
  journal={arXiv preprint arXiv:2312.12423},
  year={2023}
}

@InProceedings{tong2024eyes,
    author    = {Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
    title     = {Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs},
    booktitle = {CVPR},
    year      = {2024},
}

@inproceedings{liu2019clevr,
  title={Clevr-ref+: Diagnosing visual reasoning with referring expressions},
  author={Liu, Runtao and Liu, Chenxi and Bai, Yutong and Yuille, Alan L},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{shao2024visualcot,
 author = {Shao, Hao and Qian, Shengju and Xiao, Han and Song, Guanglu and ZONG, ZHUOFAN and Wang, Letian and Liu, Yu and Li, Hongsheng},
 booktitle = {NeurIPS},
 title = {Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning},
 year = {2024}
}

@inproceedings{ma2024groma,
  title={Groma: Localized visual tokenization for grounding multimodal large language models},
  author={Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{kurita2023refego,
  title={Refego: Referring expression comprehension dataset from first-person perception of ego4d},
  author={Kurita, Shuhei and Katsura, Naoki and Onami, Eri},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{mitchell2023detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  booktitle={ICML},
  year={2023},
}

@inproceedings{ma2023crepe,
  title={Crepe: Can vision-language foundation models reason compositionally?},
  author={Ma, Zixian and Hong, Jerry and Gul, Mustafa Omer and Gandhi, Mona and Gao, Irena and Krishna, Ranjay},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{schulter2023omnilabel,
  title={Omnilabel: A challenging benchmark for language-based object detection},
  author={Schulter, Samuel and Suh, Yumin and Dafnis, Konstantinos M and Zhang, Zhixing and Zhao, Shiyu and Metaxas, Dimitris and others},
  booktitle={ICCV},
  year={2023}
}

@InProceedings{Zhao_2024_CVPR,
    author    = {Zhao, Shiyu and Zhao, Long and G, Vijay Kumar B and Suh, Yumin and Metaxas, Dimitris N. and Chandraker, Manmohan and Schulter, Samuel},
    title     = {Generating Enhanced Negatives for Training Language-Based Object Detectors},
    booktitle = {CVPR},
    year      = {2024},
}

@inproceedings{
vaze2022openset,
title={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
author={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
booktitle={ICLR},
year={2022},
}

@inproceedings{thrush2022winoground,
  title={Winoground: Probing vision and language models for visio-linguistic compositionality},
  author={Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zerroug2022benchmark,
  title={A benchmark for compositional visual reasoning},
  author={Zerroug, Aimen and Vaishnav, Mohit and Colin, Julien and Musslick, Sebastian and Serre, Thomas},
  journal={NeurIPS},
  year={2022}
}

}
@inproceedings{wang_visionllm_2023,
	title = {{VisionLLM}: {Large} {Language} {Model} is also an {Open}-{Ended} {Decoder} for {Vision}-{Centric} {Tasks}},
	booktitle = {NeurIPS},
	author = {Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and Dai, Jifeng},
	year = {2023},
}

@inproceedings{ray_cola_2023,
	title = {Cola: {A} {Benchmark} for {Compositional} {Text}-to-image {Retrieval}},
	booktitle = {NeurIPS},
	author = {Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan and Krishna, Ranjay and Saenko, Kate},
	year = {2023},
}


@inproceedings{yuksekgonul2022and,
  title={When and why vision-language models behave like bags-of-words, and what to do about it?},
  author={Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},
  booktitle={The Eleventh ICLR},
  year={2022}
}


@inproceedings{hsieh_sugarcrepe_2023,
	title = {{SugarCrepe}: {Fixing} {Hackable} {Benchmarks} for {Vision}-{Language} {Compositionality}},
	booktitle = {NeurIPS},
	author = {Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},
	year = {2023},
}

@inproceedings{yarom_what_2023,
	title = {What {You} {See} is {What} {You} {Read}? {Improving} {Text}-{Image} {Alignment} {Evaluation}},
	booktitle = {NeurIPS},
	author = {Yarom, Michal and Bitton, Yonatan and Changpinyo, Soravit and Aharoni, Roee and Herzig, Jonathan and Lang, Oran and Ofek, Eran and Szpektor, Idan},
	year = {2023},
}

@article{zhang2024countercurate,
  title={CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples},
  author={Zhang, Jianrui and Cai, Mu and Xie, Tengyang and Lee, Yong Jae},
  journal={Findings of the Association for Computational Linguistics: ACL 2024},
  year={2024}
}

@inproceedings{NEURIPS2023_coco_counter,
 author = {Le, Tiep and LAL, VASUDEV and Howard, Phillip},
 booktitle = {NeurIPS},
 title = {COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs},
 year = {2023}
}


@article{chen2024spatialvlm,
  title = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
  author = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  journal = {arXiv preprint arXiv:2401.12168},
  year = {2024},
}

@inproceedings{yang2019spatialsense,
  title={Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition},
  author={Yang, Kaiyu and Russakovsky, Olga and Deng, Jia},
  booktitle={ICCV},
  year={2019}
}

@article{zhai2023halle-switch,
  title={Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption},
  author={Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun},
  journal={arXiv preprint arXiv:2310.01779},
  year={2023}
}

@article{miyai2024unsolvable,
  title={Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models},
  author={Miyai, Atsuyuki and Yang, Jingkang and Zhang, Jingyang and Ming, Yifei and Yu, Qing and Irie, Go and Li, Yixuan and Li, Hai and Liu, Ziwei and Aizawa, Kiyoharu},
  journal={arXiv preprint arXiv:2403.20331},
  year={2024}
}

@inproceedings{chen2020cops,
  title={Cops-ref: A new dataset and task on compositional referring expression comprehension},
  author={Chen, Zhenfang and Wang, Peng and Ma, Lin and Wong, Kwan-Yee K and Wu, Qi},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{yang2020graph,
  title={Graph-structured referring expression reasoning in the wild},
  author={Yang, Sibei and Li, Guanbin and Yu, Yizhou},
  booktitle={CVPR},
  year={2020}
}

@article{yang2023set,
  title={Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v},
  author={Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.11441},
  year={2023}
}


@inproceedings{kamath2021mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV},
  year={2021}
}

@article{zhao2024open,
  title={An open and comprehensive pipeline for unified object grounding and detection},
  author={Zhao, Xiangyu and Chen, Yicheng and Xu, Shilin and Li, Xiangtai and Wang, Xinjiang and Li, Yining and Huang, Haian},
  journal={arXiv preprint arXiv:2401.02361},
  year={2024}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={ICLR},
year={2022},
}

@inproceedings{
wang2024the,
title={The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World},
author={Weiyun Wang and Min Shi and Qingyun Li and Wenhai Wang and Zhenhang Huang and Linjie Xing and Zhe Chen and Hao Li and Xizhou Zhu and Zhiguo Cao and Yushi Chen and Tong Lu and Jifeng Dai and Yu Qiao},
booktitle={The Twelfth ICLR},
year={2024},
}


@inproceedings{yan2023universal,
  title={Universal instance perception as object discovery and retrieval},
  author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan},
  booktitle={CVPR},
  year={2023}
}

@article{wei2023lenna,
  title={Lenna: Language enhanced reasoning detection assistant},
  author={Wei, Fei and Zhang, Xinyu and Zhang, Ailing and Zhang, Bo and Chu, Xiangxiang},
  journal={arXiv preprint arXiv:2312.02433},
  year={2023}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={Science China Information Sciences},
  year={2024}
}

@inproceedings{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  booktitle = {NeurIPS},
  year={2024}
}

@article{qi2024cogcom,
  title={CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations},
  author={Qi, Ji and Ding, Ming and Wang, Weihan and Bai, Yushi and Lv, Qingsong and Hong, Wenyi and Xu, Bin and Hou, Lei and Li, Juanzi and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2402.04236},
  year={2024}
}

@inproceedings{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={ECCV},
  year={2024},
}

@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={ECCV},
  year={2016},
}

@InProceedings{Nagaraja2016modeling,
author="Nagaraja, Varun K.
and Morariu, Vlad I.
and Davis, Larry S.",
title="Modeling Context Between Objects for Referring Expression Understanding",
booktitle="ECCV",
year="2016",
}

@InProceedings{Rasheed_2024_CVPR,
    author    = {Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M. and Xing, Eric and Yang, Ming-Hsuan and Khan, Fahad S.},
    title     = {GLaMM: Pixel Grounding Large Multimodal Model},
    booktitle = {CVPR},
    year      = {2024},
}

@inproceedings{zhang2023llava-g,
  title={Llava-grounding: Grounded visual chat with large multimodal models},
  author={Zhang, Hao and Li, Hongyang and Li, Feng and Ren, Tianhe and Zou, Xueyan and Liu, Shilong and Huang, Shijia and Gao, Jianfeng and Leizhang and Li, Chunyuan and others},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{
Trainingonthetest,
title={Training on the Test Task Confounds Evaluation and Emergence},
author={Ricardo Dominguez-Olmedo and Florian E. Dorner and Moritz Hardt},
booktitle={The Thirteenth ICLR},
year={2025},
}

@inproceedings{liu2024finecops,
  title={FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension},
  author={Junzhuo Liu and Xuzheng Yang and Weiwei Li and Peng Wang},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{Wang2018NeighbourhoodWR,
  title={Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks},
  author={Peng Wang and Qi Wu and Jiewei Cao and Chunhua Shen and Lianli Gao and Anton van den Hengel},
  booktitle={CVPR},
  year={2018}
}