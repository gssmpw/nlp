[
  {
    "index": 0,
    "papers": [
      {
        "key": "mao2016generation",
        "author": "Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin",
        "title": "Generation and comprehension of unambiguous object descriptions"
      },
      {
        "key": "yu2016modeling",
        "author": "Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L",
        "title": "Modeling context in referring expressions"
      },
      {
        "key": "Nagaraja2016modeling",
        "author": "Nagaraja, Varun K.\nand Morariu, Vlad I.\nand Davis, Larry S.",
        "title": "Modeling Context Between Objects for Referring Expression Understanding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cirik-etal-2018-visual",
        "author": "Cirik, Volkan  and\nMorency, Louis-Philippe  and\nBerg-Kirkpatrick, Taylor",
        "title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "akula-etal-2020-words",
        "author": "Akula, Arjun  and\nGella, Spandana  and\nAl-Onaizan, Yaser  and\nZhu, Song-Chun  and\nReddy, Siva",
        "title": "Words Aren{'}t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "akula-etal-2020-words",
        "author": "Akula, Arjun  and\nGella, Spandana  and\nAl-Onaizan, Yaser  and\nZhu, Song-Chun  and\nReddy, Siva",
        "title": "Words Aren{'}t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "akula-etal-2020-words",
        "author": "Akula, Arjun  and\nGella, Spandana  and\nAl-Onaizan, Yaser  and\nZhu, Song-Chun  and\nReddy, Siva",
        "title": "Words Aren{'}t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2019clevr",
        "author": "Liu, Runtao and Liu, Chenxi and Bai, Yutong and Yuille, Alan L",
        "title": "Clevr-ref+: Diagnosing visual reasoning with referring expressions"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2020cops",
        "author": "Chen, Zhenfang and Wang, Peng and Ma, Lin and Wong, Kwan-Yee K and Wu, Qi",
        "title": "Cops-ref: A new dataset and task on compositional referring expression comprehension"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yang2020graph",
        "author": "Yang, Sibei and Li, Guanbin and Yu, Yizhou",
        "title": "Graph-structured referring expression reasoning in the wild"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hudson2019gqa",
        "author": "Hudson, Drew A and Manning, Christopher D",
        "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Wu_2023_ICCV_GITM-MR",
        "author": "Wu, Yu and Wei, Yana and Wang, Haozhe and Liu, Yongfei and Yang, Sibei and He, Xuming",
        "title": "Grounded Image Text Matching with Mismatched Relation Reasoning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zeng2024investigating",
        "author": "Zeng, Yunan and Huang, Yan and Zhang, Jinjin and Jie, Zequn and Chai, Zhenhua and Wang, Liang",
        "title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "kurita2023refego",
        "author": "Kurita, Shuhei and Katsura, Naoki and Onami, Eri",
        "title": "Refego: Referring expression comprehension dataset from first-person perception of ego4d"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "schulter2023omnilabel",
        "author": "Schulter, Samuel and Suh, Yumin and Dafnis, Konstantinos M and Zhang, Zhixing and Zhao, Shiyu and Metaxas, Dimitris and others",
        "title": "Omnilabel: A challenging benchmark for language-based object detection"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "suhr-etal-2019-corpus",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "ma2023crepe",
        "author": "Ma, Zixian and Hong, Jerry and Gul, Mustafa Omer and Gandhi, Mona and Gao, Irena and Krishna, Ranjay",
        "title": "Crepe: Can vision-language foundation models reason compositionally?"
      },
      {
        "key": "diwan-etal-2022-winoground",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "tong2024eyes",
        "author": "Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining",
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs"
      },
      {
        "key": "yuksekgonul2022and",
        "author": "Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James",
        "title": "When and why vision-language models behave like bags-of-words, and what to do about it?"
      },
      {
        "key": "coco_cf",
        "author": "Lai, Chengen\nand Song, Shengli\nand Yan, Sitong\nand Hu, Guangneng",
        "title": "Improving Vision and\u00a0Language Concepts Understanding with\u00a0Multimodal Counterfactual Samples"
      },
      {
        "key": "Kamath2024TheHP",
        "author": "Amita Kamath and Cheng-Yu Hsieh and Kai-Wei Chang and Ranjay Krishna",
        "title": "The Hard Positive Truth About Vision-Language Compositionality"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "parcalabescu-etal-2022-valse",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "thrush2022winoground",
        "author": "Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace",
        "title": "Winoground: Probing vision and language models for visio-linguistic compositionality"
      },
      {
        "key": "ma2023crepe",
        "author": "Ma, Zixian and Hong, Jerry and Gul, Mustafa Omer and Gandhi, Mona and Gao, Irena and Krishna, Ranjay",
        "title": "Crepe: Can vision-language foundation models reason compositionally?"
      },
      {
        "key": "hsieh_sugarcrepe_2023",
        "author": "Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay",
        "title": "{SugarCrepe}: {Fixing} {Hackable} {Benchmarks} for {Vision}-{Language} {Compositionality}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ray_cola_2023",
        "author": "Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan and Krishna, Ranjay and Saenko, Kate",
        "title": "Cola: {A} {Benchmark} for {Compositional} {Text}-to-image {Retrieval}"
      },
      {
        "key": "yarom_what_2023",
        "author": "Yarom, Michal and Bitton, Yonatan and Changpinyo, Soravit and Aharoni, Roee and Herzig, Jonathan and Lang, Oran and Ofek, Eran and Szpektor, Idan",
        "title": "What {You} {See} is {What} {You} {Read}? {Improving} {Text}-{Image} {Alignment} {Evaluation}"
      },
      {
        "key": "zhang-etal-2024-countercurate",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "NEURIPS2023_coco_counter",
        "author": "Le, Tiep and LAL, VASUDEV and Howard, Phillip",
        "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhang-etal-2024-countercurate",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "liu-etal-2023-visual",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "yang2019spatialsense",
        "author": "Yang, Kaiyu and Russakovsky, Olga and Deng, Jia",
        "title": "Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition"
      },
      {
        "key": "chen2024spatialvlm",
        "author": "Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei",
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "hsieh_sugarcrepe_2023",
        "author": "Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay",
        "title": "{SugarCrepe}: {Fixing} {Hackable} {Benchmarks} for {Vision}-{Language} {Compositionality}"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "hu2017modeling",
        "author": "Hu, Ronghang and Rohrbach, Marcus and Andreas, Jacob and Darrell, Trevor and Saenko, Kate",
        "title": "Modeling relationships in referential expressions with compositional modular networks"
      },
      {
        "key": "zhang2018grounding",
        "author": "Zhang, Hanwang and Niu, Yulei and Chang, Shih-Fu",
        "title": "Grounding referring expressions in images by variational context"
      },
      {
        "key": "zhuang2018parallel",
        "author": "Zhuang, Bohan and Wu, Qi and Shen, Chunhua and Reid, Ian and Van Den Hengel, Anton",
        "title": "Parallel attention: A unified framework for visual object discovery through dialogs and queries"
      },
      {
        "key": "yu2018mattnet",
        "author": "Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L",
        "title": "Mattnet: Modular attention network for referring expression comprehension"
      },
      {
        "key": "liu2019improving",
        "author": "Liu, Xihui and Wang, Zihao and Shao, Jing and Wang, Xiaogang and Li, Hongsheng",
        "title": "Improving referring expression grounding with cross-modal attention-guided erasing"
      },
      {
        "key": "yang2019dynamic",
        "author": "Yang, Sibei and Li, Guanbin and Yu, Yizhou",
        "title": "Dynamic graph attention for referring expression comprehension"
      },
      {
        "key": "hong2019learning",
        "author": "Hong, Richang and Liu, Daqing and Mo, Xiaoyu and He, Xiangnan and Zhang, Hanwang",
        "title": "Learning to compose and reason with language tree structures for visual grounding"
      },
      {
        "key": "liu2019learning",
        "author": "Liu, Daqing and Zhang, Hanwang and Wu, Feng and Zha, Zheng-Jun",
        "title": "Learning to assemble neural module tree networks for visual grounding"
      },
      {
        "key": "Wang2018NeighbourhoodWR",
        "author": "Peng Wang and Qi Wu and Jiewei Cao and Chunhua Shen and Lianli Gao and Anton van den Hengel",
        "title": "Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhou2021real",
        "author": "Zhou, Yiyi and Ji, Rongrong and Luo, Gen and Sun, Xiaoshuai and Su, Jinsong and Ding, Xinghao and Lin, Chia-Wen and Tian, Qi",
        "title": "A real-time global inference network for one-stage referring expression comprehension"
      },
      {
        "key": "deng2021transvg",
        "author": "Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang",
        "title": "Transvg: End-to-end visual grounding with transformers"
      },
      {
        "key": "zhu2022seqtr",
        "author": "Zhu, Chaoyang and Zhou, Yiyi and Shen, Yunhang and Luo, Gen and Pan, Xingjia and Lin, Mingbao and Chen, Chao and Cao, Liujuan and Sun, Xiaoshuai and Ji, Rongrong",
        "title": "Seqtr: A simple yet universal network for visual grounding"
      },
      {
        "key": "TransVG++2023",
        "author": "Deng, Jiajun and Yang, Zhengyuan and Liu, Daqing and Chen, Tianlang and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang and Ouyang, Wanli",
        "title": "TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "kamath2021mdetr",
        "author": "Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas",
        "title": "Mdetr-modulated detection for end-to-end multi-modal understanding"
      },
      {
        "key": "yan2023universal",
        "author": "Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan",
        "title": "Universal instance perception as object discovery and retrieval"
      },
      {
        "key": "liu2023grounding",
        "author": "Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others",
        "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection"
      },
      {
        "key": "dMDETR23",
        "author": "Shi, Fengyuan and Gao, Ruopeng and Huang, Weilin and Wang, Limin",
        "title": "Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "carion2020end",
        "author": "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
        "title": "End-to-end object detection with transformers"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "kamath2021mdetr",
        "author": "Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas",
        "title": "Mdetr-modulated detection for end-to-end multi-modal understanding"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "yan2023universal",
        "author": "Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan",
        "title": "Universal instance perception as object discovery and retrieval"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "zhu2020deformable",
        "author": "Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai",
        "title": "Deformable detr: Deformable Transformers for End-to-End Object Detection"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "liu2023grounding",
        "author": "Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others",
        "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "zhang2022dino",
        "author": "Hao Zhang and Feng Li and Shilong Liu and Lei Zhang and Hang Su and Jun Zhu and Lionel Ni and Heung-Yeung Shum",
        "title": "{DINO}: {DETR} with Improved DeNoising Anchor Boxes for End-to-End Object Detection"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "Zhao_2024_CVPR",
        "author": "Zhao, Shiyu and Zhao, Long and G, Vijay Kumar B and Suh, Yumin and Metaxas, Dimitris N. and Chandraker, Manmohan and Schulter, Samuel",
        "title": "Generating Enhanced Negatives for Training Language-Based Object Detectors"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "xiao2024oneref",
        "author": "Linhui Xiao and Xiaoshan Yang and Fang Peng and Yaowei Wang and Changsheng Xu",
        "title": "OneRef:  Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "dai2024simvg",
        "author": "Dai, Ming and Yang, Lingfeng and Xu, Yihao and Feng, Zhenhua and Yang, Wankou",
        "title": "SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "chen2023shikra",
        "author": "Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui",
        "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"
      },
      {
        "key": "you2024ferret",
        "author": "Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang",
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity"
      },
      {
        "key": "li-etal-2024-groundinggpt",
        "author": "Li, Zhaowei  and\nXu, Qi  and\nZhang, Dong  and\nSong, Hang  and\nCai, YiQing  and\nQi, Qi  and\nZhou, Ran  and\nPan, Junting  and\nLi, Zefeng  and\nTu, Vu  and\nHuang, Zhida  and\nWang, Tao",
        "title": "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model"
      },
      {
        "key": "wei2023lenna",
        "author": "Wei, Fei and Zhang, Xinyu and Zhang, Ailing and Zhang, Bo and Chu, Xiangxiang",
        "title": "Lenna: Language enhanced reasoning detection assistant"
      },
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      },
      {
        "key": "qi2024cogcom",
        "author": "Qi, Ji and Ding, Ming and Wang, Weihan and Bai, Yushi and Lv, Qingsong and Hong, Wenyi and Xu, Bin and Hou, Lei and Li, Juanzi and Dong, Yuxiao and others",
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"
      },
      {
        "key": "chen2024far",
        "author": "Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",
        "title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites"
      },
      {
        "key": "yang2023set",
        "author": "Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng",
        "title": "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "xiao2024visualgroundingsurvey",
        "author": "Linhui Xiao and Xiaoshan Yang and Xiangyuan Lan and Yaowei Wang and Changsheng Xu",
        "title": "Towards Visual Grounding: A Survey"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "chen2023shikra",
        "author": "Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui",
        "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "you2024ferret",
        "author": "Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang",
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "li-etal-2024-groundinggpt",
        "author": "Li, Zhaowei  and\nXu, Qi  and\nZhang, Dong  and\nSong, Hang  and\nCai, YiQing  and\nQi, Qi  and\nZhou, Ran  and\nPan, Junting  and\nLi, Zefeng  and\nTu, Vu  and\nHuang, Zhida  and\nWang, Tao",
        "title": "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "qi2024cogcom",
        "author": "Qi, Ji and Ding, Ming and Wang, Weihan and Bai, Yushi and Lv, Qingsong and Hong, Wenyi and Xu, Bin and Hou, Lei and Li, Juanzi and Dong, Yuxiao and others",
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "Rasheed_2024_CVPR",
        "author": "Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M. and Xing, Eric and Yang, Ming-Hsuan and Khan, Fahad S.",
        "title": "GLaMM: Pixel Grounding Large Multimodal Model"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "zhang2023llava-g",
        "author": "Zhang, Hao and Li, Hongyang and Li, Feng and Ren, Tianhe and Zou, Xueyan and Liu, Shilong and Huang, Shijia and Gao, Jianfeng and Leizhang and Li, Chunyuan and others",
        "title": "Llava-grounding: Grounded visual chat with large multimodal models"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "ma2024groma",
        "author": "Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan",
        "title": "Groma: Localized visual tokenization for grounding multimodal large language models"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "shao2024visualcot",
        "author": "Shao, Hao and Qian, Shengju and Xiao, Han and Song, Guanglu and ZONG, ZHUOFAN and Wang, Letian and Liu, Yu and Li, Hongsheng",
        "title": "Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "you2024ferret",
        "author": "Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang",
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "li2024covlm",
        "author": "Junyan Li and Delin Chen and Yining Hong and Zhenfang Chen and Peihao Chen and Yikang Shen and Chuang Gan",
        "title": "Co{VLM}: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "chen2024expanding",
        "author": "Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others",
        "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "li-etal-2024-groundinggpt",
        "author": "Li, Zhaowei  and\nXu, Qi  and\nZhang, Dong  and\nSong, Hang  and\nCai, YiQing  and\nQi, Qi  and\nZhou, Ran  and\nPan, Junting  and\nLi, Zefeng  and\nTu, Vu  and\nHuang, Zhida  and\nWang, Tao",
        "title": "{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model"
      },
      {
        "key": "pramanick2023jack",
        "author": "Pramanick, Shraman and Han, Guangxing and Hou, Rui and Nag, Sayan and Lim, Ser-Nam and Ballas, Nicolas and Wang, Qifan and Chellappa, Rama and Almahairi, Amjad",
        "title": "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"
      },
      {
        "key": "wang_visionllm_2023",
        "author": "Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and Dai, Jifeng",
        "title": "{VisionLLM}: {Large} {Language} {Model} is also an {Open}-{Ended} {Decoder} for {Vision}-{Centric} {Tasks}"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "peng2024grounding",
        "author": "Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei",
        "title": "Grounding Multimodal Large Language Models to the World"
      },
      {
        "key": "qi2024cogcom",
        "author": "Qi, Ji and Ding, Ming and Wang, Weihan and Bai, Yushi and Lv, Qingsong and Hong, Wenyi and Xu, Bin and Hou, Lei and Li, Juanzi and Dong, Yuxiao and others",
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "you2024ferret",
        "author": "Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang",
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity"
      },
      {
        "key": "wang2024the",
        "author": "Weiyun Wang and Min Shi and Qingyun Li and Wenhai Wang and Zhenhang Huang and Linjie Xing and Zhe Chen and Hao Li and Xizhou Zhu and Zhiguo Cao and Yushi Chen and Tong Lu and Jifeng Dai and Yu Qiao",
        "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "wang2024the",
        "author": "Weiyun Wang and Min Shi and Qingyun Li and Wenhai Wang and Zhenhang Huang and Linjie Xing and Zhe Chen and Hao Li and Xizhou Zhu and Zhiguo Cao and Yushi Chen and Tong Lu and Jifeng Dai and Yu Qiao",
        "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "Rasheed_2024_CVPR",
        "author": "Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M. and Xing, Eric and Yang, Ming-Hsuan and Khan, Fahad S.",
        "title": "GLaMM: Pixel Grounding Large Multimodal Model"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "zhang2023llava-g",
        "author": "Zhang, Hao and Li, Hongyang and Li, Feng and Ren, Tianhe and Zou, Xueyan and Liu, Shilong and Huang, Shijia and Gao, Jianfeng and Leizhang and Li, Chunyuan and others",
        "title": "Llava-grounding: Grounded visual chat with large multimodal models"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "ma2024groma",
        "author": "Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan",
        "title": "Groma: Localized visual tokenization for grounding multimodal large language models"
      },
      {
        "key": "shao2024visualcot",
        "author": "Shao, Hao and Qian, Shengju and Xiao, Han and Song, Guanglu and ZONG, ZHUOFAN and Wang, Letian and Liu, Yu and Li, Hongsheng",
        "title": "Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning"
      },
      {
        "key": "you2024ferret",
        "author": "Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang",
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity"
      },
      {
        "key": "li2024covlm",
        "author": "Junyan Li and Delin Chen and Yining Hong and Zhenfang Chen and Peihao Chen and Yikang Shen and Chuang Gan",
        "title": "Co{VLM}: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"
      }
    ]
  }
]