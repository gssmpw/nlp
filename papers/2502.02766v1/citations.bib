@article{choudhary2020comprehensive,
  title={A comprehensive survey on model compression and acceleration},
  author={Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
  journal={Artificial Intelligence Review},
  volume={53},
  pages={5113--5155},
  year={2020},
  publisher={Springer}
}

@article{huh2021low,
  title={The low-rank simplicity bias in deep networks},
  author={Huh, Minyoung and Mobahi, Hossein and Zhang, Richard and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
  journal={arXiv preprint arXiv:2103.10427},
  year={2021}
}

@article{denil2013predicting,
  title={Predicting parameters in deep learning},
  author={Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{rabanser2017introduction,
  title={Introduction to tensor decompositions and their applications in machine learning},
  author={Rabanser, Stephan and Shchur, Oleksandr and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:1711.10781},
  year={2017}
}

@inproceedings{vandecappelle2017nonlinear,
  title={Nonlinear least squares updating of the canonical polyadic decomposition},
  author={Vandecappelle, Michiel and Vervliet, Nico and De Lathauwer, Lieven},
  booktitle={2017 25th European Signal Processing Conference (EUSIPCO)},
  pages={663--667},
  year={2017},
  organization={IEEE}
}

@article{liu2023tensor,
  title={Tensor Decomposition for Model Reduction in Neural Networks: A Review},
  author={Liu, Xingyi and Parhi, Keshab K},
  journal={arXiv preprint arXiv:2304.13539},
  year={2023}
}

@article{kolda2009tensor,
  title={Tensor decompositions and applications},
  author={Kolda, Tamara G and Bader, Brett W},
  journal={SIAM review},
  volume={51},
  number={3},
  pages={455--500},
  year={2009},
  publisher={SIAM}
}

@article{de2008tensor,
  title={Tensor rank and the ill-posedness of the best low-rank approximation problem},
  author={De Silva, Vin and Lim, Lek-Heng},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={30},
  number={3},
  pages={1084--1127},
  year={2008},
  publisher={SIAM}
}

@article{nie2023low,
  title={Low rank tensor decompositions and approximations},
  author={Nie, Jiawang and Wang, Li and Zheng, Zequn},
  journal={Journal of the Operations Research Society of China},
  pages={1--27},
  year={2023},
  publisher={Springer}
}

@inproceedings{idelbayev2020low,
  title={Low-rank compression of neural nets: Learning the rank of each layer},
  author={Idelbayev, Yerlan and Carreira-Perpin{\'a}n, Miguel A},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8049--8059},
  year={2020}
}

@inproceedings{li2018constrained,
  title={Constrained optimization based low-rank approximation of deep neural networks},
  author={Li, Chong and Shi, CJ},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={732--747},
  year={2018}
}

@article{tai2015convolutional,
  title={Convolutional neural networks with low-rank regularization},
  author={Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and others},
  journal={arXiv preprint arXiv:1511.06067},
  year={2015}
}

@article{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{xiao2023haloc,
  title={HALOC: Hardware-Aware Automatic Low-Rank Compression for Compact Neural Networks},
  author={Xiao, Jinqi and Zhang, Chengming and Gong, Yu and Yin, Miao and Sui, Yang and Xiang, Lizhi and Tao, Dingwen and Yuan, Bo},
  journal={arXiv preprint arXiv:2301.09422},
  year={2023}
}

@inproceedings{price2023improved,
  title={Improved Projection Learning for Lower Dimensional Feature Maps},
  author={Price, Ilan and Tanner, Jared},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{ahmed2023speeding,
  title={Speeding up resnet architecture with layers targeted low rank decomposition},
  author={Ahmed, Walid and Hajimolahoseini, Habib and Wen, Austin and Liu, Yang},
  journal={arXiv preprint arXiv:2309.12412},
  year={2023}
}

@article{lebedev2014speeding,
  title={Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
  author={Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1412.6553},
  year={2014}
}

@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}

@article{zhang2015accelerating,
  title={Accelerating very deep convolutional networks for classification and detection},
  author={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={10},
  pages={1943--1955},
  year={2015},
  publisher={IEEE}
}

@inproceedings{papadimitriou2021data,
  title={Data-driven low-rank neural network compression},
  author={Papadimitriou, Dimitris and Jain, Swayambhoo},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)},
  pages={3547--3551},
  year={2021},
  organization={IEEE}
}

@article{davenport2016overview,
  title={An overview of low-rank matrix recovery from incomplete observations},
  author={Davenport, Mark A and Romberg, Justin},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={10},
  number={4},
  pages={608--622},
  year={2016},
  publisher={IEEE}
}

@article{deng2020model,
  title={Model compression and hardware acceleration for neural networks: A comprehensive survey},
  author={Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal={Proceedings of the IEEE},
  volume={108},
  number={4},
  pages={485--532},
  year={2020},
  publisher={IEEE}
}

@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@article{neill2020overview,
  title={An overview of neural network compression},
  author={Neill, James O'},
  journal={arXiv preprint arXiv:2006.03669},
  year={2020}
}

@article{kamalakara2022exploring,
  title={Exploring low rank training of deep neural networks},
  author={Kamalakara, Siddhartha Rao and Locatelli, Acyr and Venkitesh, Bharat and Ba, Jimmy and Gal, Yarin and Gomez, Aidan N},
  journal={arXiv preprint arXiv:2209.13569},
  year={2022}
}

@inproceedings{sainath2013low,
  title={Low-rank matrix factorization for deep neural network training with high-dimensional output targets},
  author={Sainath, Tara N and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6655--6659},
  year={2013},
  organization={IEEE}
}

@article{horvath2023maestro,
  title={Maestro: Uncovering Low-Rank Structures via Trainable Decomposition},
  author={Horvath, Samuel and Laskaridis, Stefanos and Rajput, Shashank and Wang, Hongyi},
  journal={arXiv preprint arXiv:2308.14929},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{kozyrskiy2020cnn,
  title={Cnn acceleration by low-rank approximation with quantized factors},
  author={Kozyrskiy, Nikolay and Phan, Anh-Huy},
  journal={arXiv preprint arXiv:2006.08878},
  year={2020}
}

@article{fineloraprune,
  title={LORAPRUNE: PRUNING MEETS LOW-RANK PARAME-TER-EFFICIENT FINE-TUNING},
  author={FINE-TUNING, TER-EFFICIENT}
}

@article{hawkins2021low,
  title={Low-Rank+ Sparse Tensor Compression for Neural Networks},
  author={Hawkins, Cole and Yang, Haichuan and Li, Meng and Lai, Liangzhen and Chandra, Vikas},
  journal={arXiv preprint arXiv:2111.01697},
  year={2021}
}

@inproceedings{yu2017compressing,
  title={On compressing deep models by low rank and sparse decomposition},
  author={Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7370--7379},
  year={2017}
}

@article{kim2015compression,
  title={Compression of deep convolutional neural networks for fast and low power mobile applications},
  author={Kim, Yong-Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
  journal={arXiv preprint arXiv:1511.06530},
  year={2015}
}

@article{maison2023compression,
  title={Compression of Recurrent Neural Networks using Matrix Factorization},
  author={Maison, Lucas and Bourboux, H{\'e}lion du Mas des and Courtat, Thomas},
  journal={arXiv preprint arXiv:2310.12688},
  year={2023}
}

@article{kholiavchenko1803iterative,
  title={Iterative Low-Rank Approximation for CNN Compression. arXiv 2018},
  author={Kholiavchenko, Maksym},
  journal={arXiv preprint arXiv:1803.08995},
  year={1803}
}

@inproceedings{astrid2018rank,
  title={Rank selection of CP-decomposed convolutional layers with variational Bayesian matrix factorization},
  author={Astrid, Marcella and Lee, Seung-Ik and Seo, Beom-Su},
  booktitle={2018 20th International Conference on Advanced Communication Technology (ICACT)},
  pages={347--350},
  year={2018},
  organization={IEEE}
}

@article{liang2023homodistil,
  title={Homodistil: Homotopic task-agnostic distillation of pre-trained transformers},
  author={Liang, Chen and Jiang, Haoming and Li, Zheng and Tang, Xianfeng and Yin, Bin and Zhao, Tuo},
  journal={arXiv preprint arXiv:2302.09632},
  year={2023}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{zhang2023post,
  title={Post-training quantization for neural networks with provable guarantees},
  author={Zhang, Jinjie and Zhou, Yixuan and Saab, Rayan},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={2},
  pages={373--399},
  year={2023},
  publisher={SIAM}
}

@article{zhang2023spfq,
  title={SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization},
  author={Zhang, Jinjie and Saab, Rayan},
  journal={arXiv preprint arXiv:2309.10975},
  year={2023}
}

@article{gillis2011low,
  title={Low-rank matrix approximation with weights or missing data is NP-hard},
  author={Gillis, Nicolas and Glineur, Fran{\c{c}}ois},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={32},
  number={4},
  pages={1149--1165},
  year={2011},
  publisher={SIAM}
}

@article{chen2021drone,
  title={Drone: Data-aware low-rank compression for large nlp models},
  author={Chen, Patrick and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29321--29334},
  year={2021}
}

@inproceedings{noach2020compressing,
  title={Compressing pre-trained language models by matrix decomposition},
  author={Noach, Matan Ben and Goldberg, Yoav},
  booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  pages={884--889},
  year={2020}
}

@inproceedings{xu2023survey,
  title={A survey on model compression and acceleration for pretrained language models},
  author={Xu, Canwen and McAuley, Julian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={10566--10575},
  year={2023}
}

@article{zhu2023survey,
  title={A survey on model compression for large language models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}

@article{eckart1936approximation,
  title={The approximation of one matrix by another of lower rank},
  author={Eckart, Carl and Young, Gale},
  journal={Psychometrika},
  volume={1},
  number={3},
  pages={211--218},
  year={1936},
  publisher={Springer}
}

@techreport{stewart1998perturbation,
  title={Perturbation theory for the singular value decomposition},
  author={Stewart, Gilbert W},
  year={1998}
}

@article{takane2001constrained,
  title={Constrained principal component analysis: a comprehensive theory},
  author={Takane, Yoshio and Hunter, Michael A},
  journal={Applicable Algebra in Engineering, Communication and Computing},
  volume={12},
  pages={391--419},
  year={2001},
  publisher={Springer}
}

@article{takane2007regularized,
  title={Regularized linear and kernel redundancy analysis},
  author={Takane, Yoshio and Hwang, Heungsun},
  journal={Computational Statistics \& Data Analysis},
  volume={52},
  number={1},
  pages={394--405},
  year={2007},
  publisher={Elsevier}
}

@article{takane2008regularized,
  title={Regularized partial and/or constrained redundancy analysis},
  author={Takane, Yoshio and Jung, Sunho},
  journal={Psychometrika},
  volume={73},
  pages={671--690},
  year={2008},
  publisher={Springer}
}

@article{tropp2023randomized,
  title={Randomized algorithms for low-rank matrix approximation: Design, analysis, and applications},
  author={Tropp, Joel A and Webber, Robert J},
  journal={arXiv preprint arXiv:2306.12418},
  year={2023}
}

@article{chen2022randomly,
  title={Randomly pivoted Cholesky: Practical approximation of a kernel matrix with few entry evaluations},
  author={Chen, Yifan and Epperly, Ethan N and Tropp, Joel A and Webber, Robert J},
  journal={arXiv preprint arXiv:2207.06503},
  year={2022}
}

@article{maalouf2019fast,
  title={Fast and accurate least-mean-squares solvers},
  author={Maalouf, Alaa and Jubran, Ibrahim and Feldman, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{maalouf2022fast,
  title={Fast and accurate least-mean-squares solvers for high dimensional data},
  author={Maalouf, Alaa and Jubran, Ibrahim and Feldman, Dan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={12},
  pages={9977--9994},
  year={2022},
  publisher={IEEE}
}

@book{ten1993least,
  title={Least squares optimization in multivariate analysis},
  author={Ten Berge, Jos MF},
  year={1993},
  publisher={DSWO Press, Leiden University Leiden}
}

@article{zangrando2024neural,
  title={Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias},
  author={Zangrando, Emanuele and Deidda, Piero and Brugiapaglia, Simone and Guglielmi, Nicola and Tudisco, Francesco},
  journal={arXiv preprint arXiv:2402.03991},
  year={2024}
}

@article{tomasi2006comparison,
  title={A comparison of algorithms for fitting the PARAFAC model},
  author={Tomasi, Giorgio and Bro, Rasmus},
  journal={Computational Statistics \& Data Analysis},
  volume={50},
  number={7},
  pages={1700--1734},
  year={2006},
  publisher={Elsevier}
}

@article{shim2017svd,
  title={SVD-softmax: Fast softmax approximation on large vocabulary neural networks},
  author={Shim, Kyuhong and Lee, Minjae and Choi, Iksoo and Boo, Yoonho and Sung, Wonyong},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chen2021scatterbrain,
  title={Scatterbrain: Unifying sparse and low-rank attention},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17413--17426},
  year={2021}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{Tay2020EfficientTA,
  title={Efficient Transformers: A Survey},
  author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
  journal={ACM Computing Surveys},
  year={2020},
  volume={55},
  pages={1 - 28},
  url={https://api.semanticscholar.org/CorpusID:221702858}
}

@inproceedings{alberti2023sumformer,
  title={Sumformer: Universal Approximation for Efficient Transformers},
  author={Alberti, Silas and Dern, Niclas and Thesing, Laura and Kutyniok, Gitta},
  booktitle={Topological, Algebraic and Geometric Learning Workshops 2023},
  pages={72--86},
  year={2023},
  organization={PMLR}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{chitty2023survey,
  title={A survey of techniques for optimizing transformer inference},
  author={Chitty-Venkata, Krishna Teja and Mittal, Sparsh and Emani, Murali and Vishwanath, Venkatram and Somani, Arun K},
  journal={Journal of Systems Architecture},
  pages={102990},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{phan2020stable,
  title={Stable low-rank tensor decomposition for compression of convolutional neural network},
  author={Phan, Anh-Huy and Sobolev, Konstantin and Sozykin, Konstantin and Ermilov, Dmitry and Gusak, Julia and Tichavsk{\`y}, Petr and Glukhov, Valeriy and Oseledets, Ivan and Cichocki, Andrzej},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXIX 16},
  pages={522--539},
  year={2020},
  organization={Springer}
}

@article{alvarez2017compression,
  title={Compression-aware training of deep networks},
  author={Alvarez, Jose M and Salzmann, Mathieu},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{wang2016accelerating,
  title={Accelerating convolutional neural networks for mobile applications},
  author={Wang, Peisong and Cheng, Jian},
  booktitle={Proceedings of the 24th ACM international conference on Multimedia},
  pages={541--545},
  year={2016}
}

@article{hsu2022language,
  title={Language model compression with weighted low-rank factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:2207.00112},
  year={2022}
}


@InProceedings{pmlr-v201-timor23a,
  title = 	 {Implicit Regularization Towards Rank Minimization in ReLU Networks},
  author =       {Timor, Nadav and Vardi, Gal and Shamir, Ohad},
  booktitle = 	 {Proceedings of The 34th International Conference on Algorithmic Learning Theory},
  pages = 	 {1429--1459},
  year = 	 {2023}
}

@article{tanner2016low,
  title={Low rank matrix completion by alternating steepest descent methods},
  author={Tanner, Jared and Wei, Ke},
  journal={Applied and Computational Harmonic Analysis},
  volume={40},
  number={2},
  pages={417--429},
  year={2016},
  publisher={Elsevier}
}

@article{borzadaran2011log,
  title={Log-concavity property for some well-known distributions},
  author={Borzadaran, GR Mohtashami and Borzadaran, HA Mohtashami},
  journal={Surveys in Mathematics and its Applications},
  volume={6},
  pages={203--219},
  year={2011}
}

@misc{ludoux1991probability,
  title={Probability in Banach spaces: Isoperimetry and processes},
  author={Ludoux, M and Talagrand, M},
  year={1991},
  publisher={Springer-Verlag, Berlin}
}

@article{davenport20141,
  title={1-bit matrix completion},
  author={Davenport, Mark A and Plan, Yaniv and Van Den Berg, Ewout and Wootters, Mary},
  journal={Information and Inference: A Journal of the IMA},
  volume={3},
  number={3},
  pages={189--223},
  year={2014},
  publisher={OUP}
}

@article{seginer2000expected,
  title={The expected norm of random matrices},
  author={Seginer, Yoav},
  journal={Combinatorics, Probability and Computing},
  volume={9},
  number={2},
  pages={149--166},
  year={2000},
  publisher={Cambridge University Press}
}

@article{chou2024gradient,
  title={Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank},
  author={Chou, Hung-Hsu and Gieshoff, Carsten and Maly, Johannes and Rauhut, Holger},
  journal={Applied and Computational Harmonic Analysis},
  volume={68},
  pages={101595},
  year={2024},
  publisher={Elsevier}
}

@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@article{seleznova2024neural,
  title={Neural (tangent kernel) collapse},
  author={Seleznova, Mariia and Weitzner, Dana and Giryes, Raja and Kutyniok, Gitta and Chou, Hung-Hsu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kwon2024efficient,
  title={Efficient Low-Dimensional Compression of Overparameterized Models},
  author={Kwon, Soo Min and Zhang, Zekai and Song, Dogyoon and Balzano, Laura and Qu, Qing},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1009--1017},
  year={2024},
  organization={PMLR}
}

@inproceedings{yaras2023invariant,
  title={Invariant Low-Dimensional Subspaces in Gradient Descent for Learning Deep Matrix Factorizations},
  author={Yaras, Can and Wang, Peng and Hu, Wei and Zhu, Zhihui and Balzano, Laura and Qu, Qing},
  booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
  year={2023}
}

@article{wang2023understanding,
  title={Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination},
  author={Wang, Peng and Li, Xiao and Yaras, Can and Zhu, Zhihui and Balzano, Laura and Hu, Wei and Qu, Qing},
  journal={arXiv preprint arXiv:2311.02960},
  year={2023}
}

@article{li2022implicit,
  title={Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent},
  author={Li, Zhiyuan and Wang, Tianhao and Lee, Jason D and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34626--34640},
  year={2022}
}

@article{li2021happens,
  title={What Happens after SGD Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2110.06914},
  year={2021}
}

@article{sheen2024implicit,
  title={Implicit Regularization of Gradient Flow on One-Layer Softmax Attention},
  author={Sheen, Heejune and Chen, Siyu and Wang, Tianhao and Zhou, Harrison H},
  journal={arXiv preprint arXiv:2403.08699},
  year={2024}
}

@article{vershynin2020high,
  title={High-dimensional probability},
  author={Vershynin, Roman},
  journal={University of California, Irvine},
  volume={10},
  pages={11},
  year={2020}
}

@article{nguyen2019low,
  title={Low-rank matrix completion: A contemporary survey},
  author={Nguyen, Luong Trung and Kim, Junhan and Shim, Byonghyo},
  journal={IEEE Access},
  volume={7},
  pages={94215--94237},
  year={2019},
  publisher={IEEE}
}

@article{recht2010guaranteed,
  title={Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization},
  author={Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A},
  journal={SIAM review},
  volume={52},
  number={3},
  pages={471--501},
  year={2010},
  publisher={SIAM}
}

@article{ma2011fixed,
  title={Fixed point and Bregman iterative methods for matrix rank minimization},
  author={Ma, Shiqian and Goldfarb, Donald and Chen, Lifeng},
  journal={Mathematical Programming},
  volume={128},
  number={1},
  pages={321--353},
  year={2011},
  publisher={Springer}
}

@article{combettes2011proximal,
  title={Proximal splitting methods in signal processing},
  author={Combettes, Patrick L and Pesquet, Jean-Christophe},
  journal={Fixed-point algorithms for inverse problems in science and engineering},
  pages={185--212},
  year={2011},
  publisher={Springer}
}

@inproceedings{yokota2015fast,
  title={A fast automatic low-rank determination algorithm for noisy matrix completion},
  author={Yokota, Tatsuya and Cichocki, Andrzej},
  booktitle={2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  pages={43--46},
  year={2015},
  organization={IEEE}
}

@article{lee2010admira,
  title={Admira: Atomic decomposition for minimum rank approximation},
  author={Lee, Kiryung and Bresler, Yoram},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={9},
  pages={4402--4416},
  year={2010},
  publisher={IEEE}
}

@article{wang2015orthogonal,
  title={Orthogonal rank-one matrix pursuit for low rank matrix completion},
  author={Wang, Zheng and Lai, Ming-Jun and Lu, Zhaosong and Fan, Wei and Davulcu, Hasan and Ye, Jieping},
  journal={SIAM Journal on Scientific Computing},
  volume={37},
  number={1},
  pages={A488--A514},
  year={2015},
  publisher={SIAM}
}

@inproceedings{wang2014rank,
  title={Rank-one matrix pursuit for matrix completion},
  author={Wang, Zheng and Lai, Ming-Jun and Lu, Zhaosong and Fan, Wei and Davulcu, Hasan and Ye, Jieping},
  booktitle={International Conference on Machine Learning},
  pages={91--99},
  year={2014},
  organization={PMLR}
}

@article{lin2010augmented,
  title={The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices},
  author={Lin, Zhouchen and Chen, Minming and Ma, Yi},
  journal={arXiv preprint arXiv:1009.5055},
  year={2010}
}

@article{tao2011recovering,
  title={Recovering low-rank and sparse components of matrices from incomplete and noisy observations},
  author={Tao, Min and Yuan, Xiaoming},
  journal={SIAM Journal on Optimization},
  volume={21},
  number={1},
  pages={57--81},
  year={2011},
  publisher={SIAM}
}

@article{lin2011linearized,
  title={Linearized alternating direction method with adaptive penalty for low-rank representation},
  author={Lin, Zhouchen and Liu, Risheng and Su, Zhixun},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{gao2024low,
  title={Low-rank matrix recovery problem minimizing a new ratio of two norms approximating the rank function then using an ADMM-type solver with applications},
  author={Gao, Kaixin and Huang, Zheng-Hai and Guo, Lulu},
  journal={Journal of Computational and Applied Mathematics},
  volume={438},
  pages={115564},
  year={2024},
  publisher={Elsevier}
}

@article{fornasier2011low,
  title={Low-rank matrix recovery via iteratively reweighted least squares minimization},
  author={Fornasier, Massimo and Rauhut, Holger and Ward, Rachel},
  journal={SIAM Journal on Optimization},
  volume={21},
  number={4},
  pages={1614--1640},
  year={2011},
  publisher={SIAM}
}

@article{mohan2012iterative,
  title={Iterative reweighted algorithms for matrix rank minimization},
  author={Mohan, Karthik and Fazel, Maryam},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={3441--3473},
  year={2012},
  publisher={JMLR. org}
}

@article{bertsimas2023optimal,
  title={Optimal low-rank matrix completion: Semidefinite relaxations and eigenvector disjunctions},
  author={Bertsimas, Dimitris and Cory-Wright, Ryan and Lo, Sean and Pauphilet, Jean},
  journal={arXiv preprint arXiv:2305.12292},
  year={2023}
}

@article{cai2010singular,
  title={A singular value thresholding algorithm for matrix completion},
  author={Cai, Jian-Feng and Cand{\`e}s, Emmanuel J and Shen, Zuowei},
  journal={SIAM Journal on optimization},
  volume={20},
  number={4},
  pages={1956--1982},
  year={2010},
  publisher={SIAM}
}

@article{jain2010guaranteed,
  title={Guaranteed rank minimization via singular value projection},
  author={Jain, Prateek and Meka, Raghu and Dhillon, Inderjit},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}

@article{tanner2013normalized,
  title={Normalized iterative hard thresholding for matrix completion},
  author={Tanner, Jared and Wei, Ke},
  journal={SIAM Journal on Scientific Computing},
  volume={35},
  number={5},
  pages={S104--S125},
  year={2013},
  publisher={SIAM}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013}
}

@article{hastie2015matrix,
  title={Matrix completion and low-rank SVD via fast alternating least squares},
  author={Hastie, Trevor and Mazumder, Rahul and Lee, Jason D and Zadeh, Reza},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={3367--3402},
  year={2015},
  publisher={JMLR. org}
}

@article{recht2013parallel,
  title={Parallel stochastic gradient algorithms for large-scale matrix completion},
  author={Recht, Benjamin and R{\'e}, Christopher},
  journal={Mathematical Programming Computation},
  volume={5},
  number={2},
  pages={201--226},
  year={2013},
  publisher={Springer}
}

@article{wen2012solving,
  title={Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm},
  author={Wen, Zaiwen and Yin, Wotao and Zhang, Yin},
  journal={Mathematical Programming Computation},
  volume={4},
  number={4},
  pages={333--361},
  year={2012},
  publisher={Springer}
}

@article{fan2019factor,
  title={Factor group-sparse regularization for efficient low-rank matrix recovery},
  author={Fan, Jicong and Ding, Lijun and Chen, Yudong and Udell, Madeleine},
  journal={Advances in neural information processing Systems},
  volume={32},
  year={2019}
}

@article{chi2019nonconvex,
  title={Nonconvex optimization meets low-rank matrix factorization: An overview},
  author={Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={20},
  pages={5239--5269},
  year={2019},
  publisher={IEEE}
}

@article{charisopoulos2021low,
  title={Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence},
  author={Charisopoulos, Vasileios and Chen, Yudong and Davis, Damek and D{\'\i}az, Mateo and Ding, Lijun and Drusvyatskiy, Dmitriy},
  journal={Foundations of Computational Mathematics},
  volume={21},
  number={6},
  pages={1505--1593},
  year={2021},
  publisher={Springer}
}

@article{li2020provable,
  title={Provable accelerated gradient method for nonconvex low rank optimization},
  author={Li, Huan and Lin, Zhouchen},
  journal={Machine Learning},
  volume={109},
  pages={103--134},
  year={2020},
  publisher={Springer}
}

@inproceedings{srebro2003weighted,
  title={Weighted low-rank approximations},
  author={Srebro, Nathan and Jaakkola, Tommi},
  booktitle={Proceedings of the 20th international conference on machine learning (ICML-03)},
  pages={720--727},
  year={2003}
}

@article{rey2013weighted,
  title={On weighted low-rank approximation},
  author={Rey, William},
  journal={arXiv preprint arXiv:1302.0360},
  year={2013}
}

@article{manton2003geometry,
  title={The geometry of weighted low-rank approximations},
  author={Manton, Jonathan H and Mahony, Robert and Hua, Yingbo},
  journal={IEEE Transactions on Signal Processing},
  volume={51},
  number={2},
  pages={500--514},
  year={2003},
  publisher={IEEE}
}

@article{tuzhilina2021weighted,
  title={Weighted low rank matrix approximation and acceleration},
  author={Tuzhilina, Elena and Hastie, Trevor},
  journal={arXiv preprint arXiv:2109.11057},
  year={2021}
}

@article{dutta2021adaptive,
  title={An adaptive rank continuation algorithm for general weighted low-rank recovery},
  author={Dutta, Aritra and Liang, Jingwei and Li, Xin},
  journal={arXiv preprint arXiv:2101.00749},
  year={2021}
}

@article{song2023efficient,
  title={Efficient alternating minimization with applications to weighted low rank approximation},
  author={Song, Zhao and Ye, Mingquan and Yin, Junze and Zhang, Lichen},
  journal={arXiv preprint arXiv:2306.04169},
  year={2023}
}

@inproceedings{li2016recovery,
  title={Recovery guarantee of weighted low-rank approximation via alternating minimization},
  author={Li, Yuanzhi and Liang, Yingyu and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  pages={2358--2367},
  year={2016},
  organization={PMLR}
}

@article{tropp2017practical,
  title={Practical sketching algorithms for low-rank matrix approximation},
  author={Tropp, Joel A and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={38},
  number={4},
  pages={1454--1485},
  year={2017},
  publisher={SIAM}
}

@article{ban2019regularized,
  title={Regularized weighted low rank approximation},
  author={Ban, Frank and Woodruff, David and Zhang, Richard},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{razenshteyn2016weighted,
  title={Weighted low rank approximations with provable guarantees},
  author={Razenshteyn, Ilya and Song, Zhao and Woodruff, David P},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={250--263},
  year={2016}
}

@phdthesis{fazel2002matrix,
  title={Matrix rank minimization with applications},
  author={Fazel, Maryam},
  year={2002},
  school={PhD thesis, Stanford University}
}

@article{donoho2006compressed,
  title={Compressed sensing},
  author={Donoho, David L},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={IEEE}
}

@article{candes2011tight,
  title={Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements},
  author={Candes, Emmanuel J and Plan, Yaniv},
  journal={IEEE Transactions on Information Theory},
  volume={57},
  number={4},
  pages={2342--2359},
  year={2011},
  publisher={IEEE}
}

@inproceedings{mohan2010new,
  title={New restricted isometry results for noisy low-rank recovery},
  author={Mohan, Karthik and Fazel, Maryam},
  booktitle={2010 IEEE International Symposium on Information Theory},
  pages={1573--1577},
  year={2010},
  organization={IEEE}
}

@article{negahban2012restricted,
  title={Restricted strong convexity and weighted matrix completion: Optimal bounds with noise},
  author={Negahban, Sahand and Wainwright, Martin J},
  journal={The Journal of Machine Learning Research},
  volume={13},
  pages={1665--1697},
  year={2012},
  publisher={JMLR. org}
}

@article{tanner2023compressed,
  title={Compressed sensing of low-rank plus sparse matrices},
  author={Tanner, Jared and Vary, Simon},
  journal={Applied and Computational Harmonic Analysis},
  volume={64},
  pages={254--293},
  year={2023},
  publisher={Elsevier}
}

@article{candes2012exact,
  title={Exact matrix completion via convex optimization},
  author={Candes, Emmanuel and Recht, Benjamin},
  journal={Communications of the ACM},
  volume={55},
  number={6},
  pages={111--119},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@article{candes2010matrix,
  title={Matrix completion with noise},
  author={Candes, Emmanuel J and Plan, Yaniv},
  journal={Proceedings of the IEEE},
  volume={98},
  number={6},
  pages={925--936},
  year={2010},
  publisher={IEEE}
}

@article{candes2010power,
  title={The power of convex relaxation: Near-optimal matrix completion},
  author={Cand{\`e}s, Emmanuel J and Tao, Terence},
  journal={IEEE transactions on information theory},
  volume={56},
  number={5},
  pages={2053--2080},
  year={2010},
  publisher={IEEE}
}

@article{recht2011simpler,
  title={A simpler approach to matrix completion.},
  author={Recht, Benjamin},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={12},
  year={2011}
}

@article{gross2011recovering,
  title={Recovering low-rank matrices from few coefficients in any basis},
  author={Gross, David},
  journal={IEEE Transactions on Information Theory},
  volume={57},
  number={3},
  pages={1548--1566},
  year={2011},
  publisher={IEEE}
}

@article{chen2015completing,
  title={Completing any low-rank matrix, provably},
  author={Chen, Yudong and Bhojanapalli, Srinadh and Sanghavi, Sujay and Ward, Rachel},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={2999--3034},
  year={2015},
  publisher={JMLR. org}
}

@inproceedings{fazel2008compressed,
  title={Compressed sensing and robust recovery of low rank matrices},
  author={Fazel, Maryam and Cand{\`e}s, Emmanuel and Recht, Ben and Parrilo, Pablo},
  booktitle={2008 42nd Asilomar Conference on Signals, Systems and Computers},
  pages={1043--1047},
  year={2008},
  organization={IEEE}
}

@inproceedings{zuk2015low,
  title={Low-rank matrix recovery from row-and-column affine measurements},
  author={Zuk, Or and Wagner, Avishai},
  booktitle={International Conference on Machine Learning},
  pages={2012--2020},
  year={2015},
  organization={PMLR}
}

@inproceedings{sarlos2006improved,
  title={Improved approximation algorithms for large matrices via random projections},
  author={Sarlos, Tamas},
  booktitle={2006 47th annual IEEE symposium on foundations of computer science (FOCS'06)},
  pages={143--152},
  year={2006},
  organization={IEEE}
}

@article{cai2015rop,
  title={ROP: Matrix recovery via rank-one projections},
  author={Cai, T Tony and Zhang, Anru},
  year={2015}
}

@article{woolfe2008fast,
  title={A fast randomized algorithm for the approximation of matrices},
  author={Woolfe, Franco and Liberty, Edo and Rokhlin, Vladimir and Tygert, Mark},
  journal={Applied and Computational Harmonic Analysis},
  volume={25},
  number={3},
  pages={335--366},
  year={2008},
  publisher={Elsevier}
}

@article{zymnis2009compressed,
  title={Compressed sensing with quantized measurements},
  author={Zymnis, Argyrios and Boyd, Stephen and Candes, Emmanuel},
  journal={IEEE Signal Processing Letters},
  volume={17},
  number={2},
  pages={149--152},
  year={2009},
  publisher={IEEE}
}

@article{jacques2013robust,
  title={Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors},
  author={Jacques, Laurent and Laska, Jason N and Boufounos, Petros T and Baraniuk, Richard G},
  journal={IEEE transactions on information theory},
  volume={59},
  number={4},
  pages={2082--2102},
  year={2013},
  publisher={IEEE}
}

@article{blumensath2013compressed,
  title={Compressed sensing with nonlinear observations and related nonlinear optimization problems},
  author={Blumensath, Thomas},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={6},
  pages={3466--3474},
  year={2013},
  publisher={IEEE}
}

@article{genzel2016high,
  title={High-dimensional estimation of structured signals from non-linear observations with general convex loss functions},
  author={Genzel, Martin},
  journal={IEEE Transactions on Information Theory},
  volume={63},
  number={3},
  pages={1601--1619},
  year={2016},
  publisher={IEEE}
}

@article{plan2016generalized,
  title={The generalized lasso with non-linear observations},
  author={Plan, Yaniv and Vershynin, Roman},
  journal={IEEE Transactions on information theory},
  volume={62},
  number={3},
  pages={1528--1537},
  year={2016},
  publisher={IEEE}
}

@article{plan2017high,
  title={High-dimensional estimation with geometric constraints},
  author={Plan, Yaniv and Vershynin, Roman and Yudovina, Elena},
  journal={Information and Inference: A Journal of the IMA},
  volume={6},
  number={1},
  pages={1--40},
  year={2017},
  publisher={Oxford University Press}
}

@article{thrampoulidis2015lasso,
  title={Lasso with non-linear measurements is equivalent to one with linear measurements},
  author={Thrampoulidis, Christos and Abbasi, Ehsan and Hassibi, Babak},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{goldstein2018structured,
  title={Structured signal recovery from non-linear and heavy-tailed measurements},
  author={Goldstein, Larry and Minsker, Stanislav and Wei, Xiaohan},
  journal={IEEE Transactions on Information Theory},
  volume={64},
  number={8},
  pages={5513--5530},
  year={2018},
  publisher={IEEE}
}

@article{choi2023matrix,
  title={Matrix Completion When Missing Is Not at Random and Its Applications in Causal Panel Data Models},
  author={Choi, Jungjun and Yuan, Ming},
  journal={arXiv preprint arXiv:2308.02364},
  year={2023}
}

@article{xia2021statistical,
  title={Statistical inferences of linear forms for noisy matrix completion},
  author={Xia, Dong and Yuan, Ming},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={83},
  number={1},
  pages={58--77},
  year={2021},
  publisher={Oxford University Press}
}

@article{athey2021matrix,
  title={Matrix completion methods for causal panel data models},
  author={Athey, Susan and Bayati, Mohsen and Doudchenko, Nikolay and Imbens, Guido and Khosravi, Khashayar},
  journal={Journal of the American Statistical Association},
  volume={116},
  number={536},
  pages={1716--1730},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{chen2020noisy,
  title={Noisy matrix completion: Understanding statistical guarantees for convex relaxation via nonconvex optimization},
  author={Chen, Yuxin and Chi, Yuejie and Fan, Jianqing and Ma, Cong and Yan, Yuling},
  journal={SIAM journal on optimization},
  volume={30},
  number={4},
  pages={3098--3121},
  year={2020},
  publisher={SIAM}
}

@article{chen2019inference,
  title={Inference and uncertainty quantification for noisy matrix completion},
  author={Chen, Yuxin and Fan, Jianqing and Ma, Cong and Yan, Yuling},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={46},
  pages={22931--22937},
  year={2019},
  publisher={National Acad Sciences}
}
@article{koltchinskii2011nuclear,
  title={Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion},
  author={Koltchinskii, Vladimir and Lounici, Karim and Tsybakov, Alexandre B},
  year={2011}
}

@book{xu2022recovering,
  title={On Recovering the Best Rank-r Approximation from Few Entries},
  author={Xu, Shun},
  year={2022},
  publisher={Columbia University}
}

@article{ai2014one,
  title={One-bit compressed sensing with non-Gaussian measurements},
  author={Ai, Albert and Lapanowski, Alex and Plan, Yaniv and Vershynin, Roman},
  journal={Linear Algebra and its Applications},
  volume={441},
  pages={222--239},
  year={2014},
  publisher={Elsevier}
}

@article{srebro2004maximum,
  title={Maximum-margin matrix factorization},
  author={Srebro, Nathan and Rennie, Jason and Jaakkola, Tommi},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}

@article{srebro2004generalization,
  title={Generalization error bounds for collaborative prediction with low-rank matrices},
  author={Srebro, Nathan and Alon, Noga and Jaakkola, Tommi},
  journal={Advances In Neural Information Processing Systems},
  volume={17},
  year={2004}
}

@article{rohde2011estimation,
  title={Estimation of high-dimensional low-rank matrices},
  author={Rohde, Angelika and Tsybakov, Alexandre B},
  year={2011}
}

@article{keshavan2009matrix,
  title={Matrix completion from noisy entries},
  author={Keshavan, Raghunandan and Montanari, Andrea and Oh, Sewoong},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@article{keshavan2010matrix,
  title={Matrix completion from a few entries},
  author={Keshavan, Raghunandan H and Montanari, Andrea and Oh, Sewoong},
  journal={IEEE transactions on information theory},
  volume={56},
  number={6},
  pages={2980--2998},
  year={2010},
  publisher={IEEE}
}

@article{sun2016guaranteed,
  title={Guaranteed matrix completion via non-convex factorization},
  author={Sun, Ruoyu and Luo, Zhi-Quan},
  journal={IEEE Transactions on Information Theory},
  volume={62},
  number={11},
  pages={6535--6579},
  year={2016},
  publisher={IEEE}
}

@article{auddy2023perturbation,
  title={Perturbation bounds for (nearly) orthogonally decomposable tensors with statistical applications},
  author={Auddy, Arnab and Yuan, Ming},
  journal={Information and Inference: A Journal of the IMA},
  volume={12},
  number={2},
  pages={1044--1072},
  year={2023},
  publisher={Oxford University Press}
}

@article{raskutti2019convex,
  title={Convex regularization for high-dimensional multiresponse tensor regression},
  author={Raskutti, Garvesh and Yuan, Ming and Chen, Han},
  year={2019}
}

@article{wang2018sparse,
  title={Sparse recovery: from vectors to tensors},
  author={Wang, Yao and Meng, Deyu and Yuan, Ming},
  journal={National Science Review},
  volume={5},
  number={5},
  pages={756--767},
  year={2018},
  publisher={Oxford University Press}
}

@article{xia2021statistically,
  title={Statistically optimal and computationally efficient low rank tensor completion from noisy entries},
  author={Xia, Dong and Yuan, Ming and Zhang, Cun-Hui},
  year={2021}
}

@article{yuan2016tensor,
  title={On tensor completion via nuclear norm minimization},
  author={Yuan, Ming and Zhang, Cun-Hui},
  journal={Foundations of Computational Mathematics},
  volume={16},
  number={4},
  pages={1031--1068},
  year={2016},
  publisher={Springer}
}

@article{pan2020low,
  title={Low-rank and sparse enhanced Tucker decomposition for tensor completion},
  author={Pan, Chenjian and Ling, Chen and He, Hongjin and Qi, Liqun and Xu, Yanwei},
  journal={arXiv preprint arXiv:2010.00359},
  year={2020}
}

@article{xia2022inference,
  title={Inference for low-rank tensorsâ€”no need to debias},
  author={Xia, Dong and Zhang, Anru R and Zhou, Yuchen},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={1220--1245},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{yuan2017incoherent,
  title={Incoherent tensor norms and their applications in higher order tensor completion},
  author={Yuan, Ming and Zhang, Cun-Hui},
  journal={IEEE Transactions on Information Theory},
  volume={63},
  number={10},
  pages={6753--6766},
  year={2017},
  publisher={IEEE}
}

@article{wang2024transformed,
  title={Transformed low-rank parameterization can help robust generalization for tensor neural networks},
  author={Wang, Andong and Li, Chao and Bai, Mingyuan and Jin, Zhong and Zhou, Guoxu and Zhao, Qibin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{lin2013network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1312.4400},
  year={2013}
}

@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{figurnov2016perforatedcnns,
  title={Perforatedcnns: Acceleration through elimination of redundant convolutions},
  author={Figurnov, Mikhail and Ibraimova, Aizhan and Vetrov, Dmitry P and Kohli, Pushmeet},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1389--1397},
  year={2017}
}

@inproceedings{yu2023compressing,
  title={Compressing transformers: features are low-rank, but weights are not!},
  author={Yu, Hao and Wu, Jianxin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={11007--11015},
  year={2023}
}

@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{touvron2022deit,
  title={Deit iii: Revenge of the vit},
  author={Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  booktitle={European conference on computer vision},
  pages={516--533},
  year={2022},
  organization={Springer}
}

@inproceedings{timor2023implicit,
  title={Implicit regularization towards rank minimization in relu networks},
  author={Timor, Nadav and Vardi, Gal and Shamir, Ohad},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1429--1459},
  year={2023},
  organization={PMLR}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{ding2024flat,
  title={Flat minima generalize for low-rank matrix recovery},
  author={Ding, Lijun and Drusvyatskiy, Dmitriy and Fazel, Maryam and Harchaoui, Zaid},
  journal={Information and Inference: A Journal of the IMA},
  volume={13},
  number={2},
  pages={iaae009},
  year={2024},
  publisher={Oxford University Press}
}

@article{ghadermarzy2018learning,
  title={Learning tensors from partial binary measurements},
  author={Ghadermarzy, Navid and Plan, Yaniv and Yilmaz, Ozgur},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={1},
  pages={29--40},
  year={2018},
  publisher={IEEE}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@inproceedings{vanhoucke2011improving,
  title={Improving the speed of neural networks on CPUs},
  author={Vanhoucke, Vincent and Senior, Andrew and Mao, Mark Z and others},
  booktitle={Proc. deep learning and unsupervised feature learning NIPS workshop},
  volume={1},
  number={2011},
  pages={4},
  year={2011}
}

@article{luo2022understanding,
  title={Understanding diffusion models: A unified perspective},
  author={Luo, Calvin},
  journal={arXiv preprint arXiv:2208.11970},
  year={2022}
}

@article{liu2024spinquant,
  title={SpinQuant--LLM quantization with learned rotations},
  author={Liu, Zechun and Zhao, Changsheng and Fedorov, Igor and Soran, Bilge and Choudhary, Dhruv and Krishnamoorthi, Raghuraman and Chandra, Vikas and Tian, Yuandong and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2405.16406},
  year={2024}
}

@article{frantar2022optimal,
  title={Optimal brain compression: A framework for accurate post-training quantization and pruning},
  author={Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4475--4488},
  year={2022}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@article{jiang2023algorithmic,
  title={Algorithmic regularization in model-free overparametrized asymmetric matrix factorization},
  author={Jiang, Liwei and Chen, Yudong and Ding, Lijun},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={3},
  pages={723--744},
  year={2023},
  publisher={SIAM}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}