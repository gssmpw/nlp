\section{Related Work}
\label{sec:rel work}
%Beyond the context of neural network approximation, there is a substantial body of work in the compressed sensing literature on general low-rank matrix approximation and recovery problems, including \cite{fazel2002matrix, fazel2008compressed, jain2010guaranteed}, to name a few. 
%
%Given a matrix $\mathbf{X}_0 \in \mathbb{R}^{m \times n}$ of rank $r$, the standard low rank matrix recovery problem (LRMR) is to recover $\mathbf{X}_0$ from observing $y=\mathcal{A}(\mathbf{X}_0)+z$, where $z$ represents noise. $\mathcal{A}:\mathbb{R}^{m \times n}\rightarrow\mathbb{R}^L$ is a linear measurement operator satisfying restricted isometry property (RIP) and often acts on $\mathbf{X}_0$ by taking standard inner products against $L$ given matrices $A_1,......, A_L \in \mathbb{R}^{m \times n}$ \cite{davenport2016overview}. A special case is when these matrices $A_i$'s are chosen to be elementary matrices and the problem becomes a low rank matrix completion (LRMC) where we observe a subset of the entries \cite{nguyen2019low} indexed by $\Omega \subset N$, i.e. we want to (approximately) recover $\mathbf{X}_0$ from $P_{\Omega}(\mathbf{X}_0)$ plus possible noise where $P_{\Omega}$ is the associated projection operator.
%
%One can formulate the problem into a Frobenius norm minimization problem: $\underset{X}{\text{min}} \|\mathcal{A}(X)-y\|_2^2$ or $\underset{X}{\text{min}} \|P_{\Omega}(X)-P_{\Omega}(\mathbf{X}_0)\|_2^2$ subject to $rank(X)\leq r$ when $r$ is known. Notably, this problem with low rank constraint is in general NP-hard  \cite{gillis2011low}. Another elementary formulation, especially in the case when the rank information is unavailable, is based on nuclear norm heuristic which solves: $\underset{X}{\text{min}} \|X\|_* \ \ \ \text{subject to} \  \mathcal{A}(X)\approx y$ for LRMR, or $\underset{X}{\text{min}} \|X\|_* \ \ \ \text{subject to} \  P_{\Omega}(X)\approx P_{\Omega}(\mathbf{X}_0)$ for LRMC.
%
%In many applications, however, the observation operator may be non-linear or does not satisfy the RIP. Even in the linear case, classical assumptions on the observed entries $\Omega$ being random could fail as in practice the entries we can observe may follow a particular pattern. Observation models different from the classical ones include affine measurements \cite{sarlos2006improved, fazel2008compressed, woolfe2008fast, zuk2015low, cai2015rop} and nonlinear measurement especially quantized linear measurements \cite{zymnis2009compressed, jacques2013robust, blumensath2013compressed, ai2014one, davenport20141, genzel2016high, goldstein2018structured}. In some cases, nonlinear measurement can be transformed into linear measurement with noise via generalized Lasso \cite{plan2016generalized, plan2017high, thrampoulidis2015lasso}. \cite{papadimitriou2021data} approximates the ReLU function $\rho$ by the linear projection $P_{\Omega}$ where $\Omega$ is defined to be the positive entries of the observation $\rho(\mathbf{X}_0)$.
%
%Algorithms for LRMR/LRMC abound \cite{ma2011fixed, combettes2011proximal, cai2010singular, jain2010guaranteed, tanner2013normalized, wen2012solving, jain2013low, recht2013parallel, hastie2015matrix, tanner2016low, sun2016guaranteed}. Generally speaking, when the rank is unknown the problem is usually formulated as a nuclear norm minimization and when the rank is given, Frobenius norm minimization is used more often. They can be viewed as dual formulations of the same problem. In both cases the constraint can be merged into the objection function as a Lagrange multiplier with some relaxation.
%Frobenius norm minimization for LRMC is formally equivalent to a low rank matrix approximation problem. In fact, we can rewrite $P_{\Omega}(\mathbf{X}_0)$ as $B \odot \mathbf{X}_0$ where $B$ is a Boolean matrix. A more general problem when $B$ is replaced by an arbitrary positive-valued matrix $W$ is called weighted low rank matrix approximation and we want to minimize $\|W\odot\mathbf{X}_0 - W\odot X\|_F^2$ when $X$ subject to rank constraint \cite{manton2003geometry, rey2013weighted, tuzhilina2021weighted}.
%Algorithms with theoretical guarantee are largely based on alternating minimization \cite{li2016recovery, dutta2021adaptive, song2023efficient} or sketching \cite{razenshteyn2016weighted, tropp2017practical, ban2019regularized}.
%Among all these works, our proof of the non-linear recovery theorem adapts techniques from \cite{davenport20141}, which addresses 1-bit, or ``sign" observations of linear measurements.