\selectlanguage{english}
\chapter[Improving Data Transfer]{Improving Data Transfer}
\label{ch:data-transfer}


In this chapter we will introduce a novel approach to annotation projection that is based on large pre-trained text-to-text language models and state-of-the-art Machine Translation technology. Our algorithm, named T-Projection, significantly outperforms previous methods of annotation projection by a wide margin. Thanks to this new approach, we have achieved the best results to date for zero-shot cross-lingual transfer between English and eight different low-resource African languages.

\section{Motivation and contributions}

In the previous chapter we demonstrated that model-based zero-shot cross-lingual transfer outperforms data-based approaches. However, model-based zero-shot transfer requires a pre-trained model with high proficiency in both the language and domain of application. Such models are not available for every language and domain, which is the case for the eight different low-resource African languages that will be the subject of study in this chapter. Therefore, data-transfer methods still hold value. The inferior performance of data-transfer methods compared with model-transfer in the previous chapter can be attributed to two main factors. First, Machine Translation of English gold-labeled data often produces a signal which, due to inherent differences in language usage, differs from the signal received when using gold-labeled data in the target language. While both model-transfer and data-transfer approaches are sensitive to this issue, it has a greater impact on the data-transfer approach. This phenomenon can be mitigated by improving the generalization capabilities of sequence labeling models. We will focus on improving generalization in the following chapters. The second factor involves the mis-alignments produced by current annotation projection methods, which use word alignment algorithms. In this chapter, we will focus on this second issue. 


The majority of previous published work on annotation projection explore the application of different word-alignments. However, as demonstrated in Chapter \ref{ch:model-vs-data}, word alignments often produce partial, incorrect or missing annotation projections. This is because word alignments are computed on a word-by-word basis by leveraging word co-occurrences or similarity between vector representations.  It should be noted that this method does not take into consideration the labeled spans or categories to be projected. 

In this chapter we present \textsc{T-Projection}, a novel approach to automatically project sequence labelling across languages. We split the annotation projection into two steps. First, we use mT5 (\cite{mt5}) text-to-text model to generate a set of projection candidates in the target sentence for each labeled category in the source sentence. This step exploits the labeled categories and the cross-lingual capabilities of large pre-trained multilingual language models. Second, we rank the candidates based on the probability of being generated as a translation of the source spans. We use the M2M100
(\cite{JMLR:v22:20-1307}) and NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) state-of-the-art MT models to compute the translation probabilities
(\cite{DBLP:journals/corr/abs-2204-13692}).

The main contributions of this chapter are the following: 
\begin{itemize}
\item We have developed a new annotation projection method, T-Projection. We compare the label projections generated by various systems with manually projected annotations on three different tasks, Opinion Target Extraction (OTE), Named Entity Recognition (NER) and Argument Mining (AM), and five different target languages (French, German, Italian, Russian and Spanish. On average, T-Projection improves the current state-of-the-art annotation projection methods by more than 8 points in F1 score.
\item We performed a real-world NER task evaluation involving eight low-resource African languages. In this downstream evaluation, T-Projection outperforms other annotation projection methods by 3.6 points in F1 score. 
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{Capitulos/5_DataTransfer/diagramas/Tprojection.pdf}
\caption{T-Projection two-step method to project sequence
labeling annotations across languages.}
\label{fig:Tprojection}
\end{figure} 


\section{T-Projection}
\label{sc4:Tprojection}
T-Projection assumes that we have a set of source sentences with sequences of words labeled with a category. Additionally, there is a parallel version of these sentences in a target language, though these translations are not labeled. T-Projection addresses the challenge of transferring the labels from the source sentences to the target sentences.


T-Projection implements two main steps, which are illustrated in Figure \ref{fig:Tprojection}. First, a set of projection candidates in the target sentence are generated for each labeled sequence in the source sentence. Second, each projection candidate is ranked using a Machine Translation model. More specifically, candidates are scored based on the probability of being generated as a translation of the source-labeled sequences. 

While the \emph{candidate generation} step exploits the labeled spans and their categories in the source sentence as well as the zero-shot cross-lingual capabilities of large pre-trained multilingual language models, the \emph{candidate selection} step
applies state-of-the-art MT technology to find those 
projection candidates that constitute the best translation for each source labeled span. These two steps are described in detail in the following two subsections. 

\subsection{Candidate Generation}\label{sc5:candidate-generation}

\begin{figure}[t]
\centering
\includegraphics[width=0.99\textwidth]{Capitulos/5_DataTransfer/diagramas/CandiateGen.pdf}
\caption{Illustration of the candidate generation step. For each label, we generate a set of probable candidates.}
\label{fig:CandiateGen}
\end{figure} 

When projecting labeled sequences from a source dataset into its parallel target dataset, we expect both the source and the target to contain the same number of sequences, each labeled with the same category. For example, consider the English source sentence \textit{``<Person>Obama</Person> went to <Location>New York</Location>''} and its parallel, unlabeled Spanish target sentence \textit{``Obama fue a Nueva York''} We would expect the target sentence also to identify the same two entities (person and location). We propose a projection candidate generation step based on this premise. 

We finetune the text-to-text mT5 \cite{mt5} model using a HTML-tag-style prompt template
(\cite{huang-etal-2022-multilingual-generative}). As illustrated by Figure \ref{fig:CandiateGen}, we build the inputs for the model by concatenating the unlabeled sentence followed by a list of tags (\textit{``<Category>None</Category''}) with the category of each labeled span that we expect to find in the sentence, and the value ``None'' If two or more spans share the same category, we append the tag as many times as there are expected spans in that category.

Unlike the approach of \citet{huang-etal-2022-multilingual-generative}, we do not encode the tags for each category as special tokens. Instead, we verbalize the categories (i.e., PER $\,\to\,$ Person) and use the token representations already existing in the model. We anticipate that thanks to the language modeling pretraining, T5 will have a good semantic representation of sequence labeling types such as Person, Location, Claim, etc.

As Figure \ref{fig:CandiateGen} illustrates, we fine-tune mT5 with the labeled source dataset. We train the model to replace the token \emph{None} with the sequence of words in the input sentence that corresponds to that category.

At inference, we label the target sentences, which are parallel translations of the source sentences. As previously explained, we expect to identify the same number of labeled spans and categories as in the source sentence. Therefore, we use the labels from the corresponding source sentence to construct the prompts. In other words, our goal is to label parallel translations of the sentences used for training. We leverage the zero-shot cross-lingual capabilities of mT5 to project the labels from the source to the target sentence. The output tokens are generated in an autoregressive manner. We employ beam search decoding with 100 beams to generate 100 candidates for each input tag. The decision to generate 100 candidates was informed by a preliminary analysis of the performance of the candidate generation stage. Experiments in Section \ref{sec5:Howmany} demonstrate that this number was an overestimation, and generating 10 to 25 candidates is optimal.

\section{Candidate Selection}\label{sec5:candidate-selection}

\begin{figure}[t]
\centering
\includegraphics[width=0.60\textwidth]{Capitulos/5_DataTransfer/diagramas/CandiateSelection.pdf}
\caption{Candidate selection: candidates are scored based on the probability of being generated as a translation of the source labeled sequences.}
\label{fig:CandiateSelection}
\end{figure}

In the previous step, we generated up to 100 candidate projections for each labeled span in the source sentence. In the candidate selection step, our goal is to identify the best projection candidate in the target sentence for each labeled span in the source sentence. As depicted in Figure \ref{fig:CandiateSelection}, all generated candidates are first grouped by category. For instance, if the previous step produced multiple spans with the same category (e.g., two \textit{locations} in a sentence), all such candidates are included in a single set. Additionally, candidates that are not subsequences of the input sentence are filtered out.

For each labeled span in the source sentence, we rank all the projection candidates that share the same category as the source span using their translation probabilities (also known as translation equivalence). These probabilities are obtained by applying the pretrained M2M100 (\cite{JMLR:v22:20-1307}) or NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) MT models and the \textit{NMTScore} library\footnote{\url{https://github.com/ZurichNLP/nmtscore}} (\cite{DBLP:journals/corr/abs-2204-13692}). Given the source span $\mathbf{A}$ and the candidate $\mathbf{B}$, the translation probability is computed as follows (\cite{DBLP:journals/corr/abs-2204-13692}):

\begin{center}
$p_{\theta_a}(A \mid B):=\left[\prod_{i=0}^{|A|} p_{\theta_a}\left(A^i \mid B, A^{<i}\right)\right]^{\frac{1}{|A|}}$
\end{center}

\noindent The translation probability is normalized:

\begin{center}
$sim(A \mid B) = \frac{p_{\theta_a}(A \mid B)}{p_{\theta_a}(A \mid A)}$
\end{center}

Since translation probability can vary depending on the translation direction, the scores are symmetrized by calculating the scores for both translation directions and averaging them:

\begin{center}
$sim(A, B)=\frac{1}{2} sim(A \mid B)+\frac{1}{2} sim(B \mid A)$
\end{center}

Finally, for each labeled span in the source sentence, we select the candidate in the target sentence with the highest translation probability. Once a candidate has been chosen, that candidate and any others that overlap with it are removed from the set of possible candidates. This prevents the assignment of the same candidate in the target sentence to multiple spans in the source sentence.


\section{Experimental Setup}\label{sec5:Methodology}

To evaluate our method we perform both intrinsic and extrinsic evaluations.



\paragraph{Intrinsic Evaluation:} We selected several datasets that have been manually projected from English into various target languages. These manual annotations serve as the gold standard for evaluating and comparing T-Projection against previous state-of-the-art label projection models. Results are reported using the F1-score, a standard metric for sequence labeling (\cite{DBLP:conf/conll/Sang02}). The intrinsic evaluation focuses on measuring the annotation projection accuracy of the models, isolated from other factors such as the quality of the translation models or any other steps in the pipeline.

\paragraph{Extrinsic evaluation:} In this evaluation we assess the capability of T-Projection to automatically generate training data for sequence labeling tasks, NER in this particular case. The process begins by utilizing the Machine Translation system NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) to translate data from English into 8 low-resource African languages. We then project the labels from English onto the respective target languages. The automatically generated datasets are then employed to train NER models, which are evaluated using a relatively small manually annotated test set. The same procedure is performed with other state-of-the-art label projection models. The comparison of the results obtained is reported in terms of F1-score.



\subsection{Datasets}\label{sec5:datasets}
The datasets used correspond to three sequence labeling tasks which are illustrated by Figure \ref{fig:TasksChap5}. The number of examples and labels are listed in Table \ref{tab5:DatasetLen}.


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Capitulos/5_DataTransfer/diagramas/Tasks.pdf}
    \caption{Sequence labeling tasks in our experiments}
    \label{fig:TasksChap5}
\end{figure}

\input{Capitulos/5_DataTransfer/tables/datasets}

\paragraph{Opinion Target Extraction (OTE)} Given a review, the task of Opinion Target Extraction (OTE) is to identify the linguistic expressions that refer to the reviewed entity. For instance, in the sentence \textit{Serves really good sushi}, the word \textit{sushi} is the opinion target because it is the entity being discussed. We utilize the English SemEval 2016 Aspect Based Sentiment Analysis (ABSA) datasets (\cite{pontiki-etal-2016-semeval}). This dataset contains user reviews from the Restaurant domain. Additionally, for evaluation purposes, we employ parallel datasets in Spanish, French, and Russian, which were generated through Machine Translation and manual label projection in Chapter \ref{ch:model-vs-data}. 

\paragraph{Named Entity Recognition (NER)} The NER task involves detecting named entities and classifying them according to predefined categories. We use a parallel NER dataset in English, Spanish, German, and Italian (\cite{agerri-etal-2018-building}), based on Europarl data (transcriptions of discussions from the European Parliament) (\cite{DBLP:conf/mtsummit/Koehn05}). 
These transcriptions are parallel in multiple languages and were annotated following the CoNLL 2003 guidelines (\cite{DBLP:conf/conll/SangM03}). For the extrinsic evaluation, we use MasakhaNER 2.0 (\cite{adelani-etal-2022-masakhaner}), a human-annotated NER dataset for 20 African languages.




\paragraph{Argument Mining (AM)} The AbstRCT English dataset includes annotations for two types of argument components, Claims and Premises, in medical and scientific texts collected from the MEDLINE database (\cite{DBLP:conf/ecai/0002CV20}). A \textit{Claim} is a concluding statement made by the author about the study's outcome, such as an assertion of a diagnosis or a treatment in the medical domain. A \textit{Premise} is an observation or measurement (ground truth) that supports or challenges another argument component, usually a claim. Premises are considered observed facts and are credible without further evidence. For evaluation, we used the Spanish parallel counterpart, generated following an adapted version of the method described in Chapter \ref{ch:model-vs-data}. The labeled sequences in the AM task consist of very long spans of words, frequently encompassing full sentences. We use the Neoplasm split.




\subsection{Baselines}

\begin{wrapfigure}{r}{0.46\textwidth}
    \vspace{-0.95cm}
    \centering
    \includegraphics[width=\linewidth]{Capitulos/4_ModelvsData/diagramas/Projection.pdf}
    \caption{Illustration of the translation and annotation projection task using word-alignments.}
    \label{fig:projection_with_alignments}
    \vspace{-0.5cm}
\end{wrapfigure}


We use the same alignment systems and methodology described in Chapter \ref{ch:model-vs-data} as a baseline. This approach is illustrated in Figure \ref{fig:projection_with_alignments}. We compare T-Projection with two statistical systems, \textbf{Giza++} (\cite{och-ney-2003-systematic-giza}) and \textbf{FastAlign} (\cite{dyer-etal-2013-simple-fastalign}). These systems are widely used in the field and require very small computational resources. Additionally, we evaluate two current state-of-the-art Transformer-based word-alignment systems, \textbf{SimAlign} (\cite{jalili-sabet-etal-2020-simalign}) and \textbf{AWESOME} (\cite{dou-neubig-2021-word}), which leverage pre-trained multilingual language models to generate alignments. As recommended by the authors, we use multilingual BERT (mBERT) (\cite{devlin-etal-2019-bert}) as the backbone. We tested different models as backbones but observed no improvement in performance. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/5_DataTransfer/diagramas/Translate_with_markers.pdf}
    \caption{Illustration of the translation with markers approach. Markers are introduced around the labeled sequences. The sentence and the labeled spans are translated together.}
    \label{fig:projection_with_markers}
\end{figure}

We also experiment with \textbf{EasyProject} (\cite{chen-etal-2023-frustratingly}), a system that jointly performs translation and projection by inserting special markers around the labeled spans in the source sentence as depicted in Figure \ref{fig:projection_with_markers}. Additionally, we evaluate \textbf{CODEC} (\cite{DBLP:journals/corr/abs-2402-03131}), a subsequent work that divides the projection process into two distinct steps. In this improved approach, the training data in the high-resource language is first translated without markers. During a second decoding phase, the markers are integrated with the constraint that the translation must align with the initial, marker-free output. This two-step process ensures that the final translated sentence with markers remains consistent with what the model would have produced without them, thereby preserving the translation quality. As both these methods generate their own translations they are therefore not suitable for the intrinsic evaluation which is why we only used them for the extrinsic evaluation. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\linewidth]{Capitulos/5_DataTransfer/diagramas/TranslateMatch.pdf}
    \caption{Illustration of the span translation annotation projection approach. The source labels are translated independently, and these translated spans are then matched with their counterparts in the target sentence.}
    \label{fig:projection_translate_match}
\end{figure}


In addition to the previous methods, we also implement two additional baselines inspired by previous works. In the first baseline, inspired by \citet{Li2021CrossLingualNE}, we use the \textbf{XLM-RoBERTa} model (\cite{conneau-etal-2020-unsupervised}) with 3 billion parameters (matching the parameter count of the mT5 model used in T-Projection) and add a token classification layer (linear layer) on top of each token representation. We train the model on the source labeled dataset and use it to predict entities in the translated target sentences.
The second baseline adopts a \textbf{span translation} approach inspired by \cite{DBLP:conf/emnlp/JainPL19} and \citet{DBLP:journals/corr/abs-2211-09394}. In this approach, we translate the labeled spans in the source sentence using the pretrained M2M100 model with 12 billion parameters and then match them with the corresponding spans in the target sentence. For example, as depicted in Figure \ref{fig:projection_translate_match} given the labeled source sentence ``\textit{<Person> Bruce Willis </person> was born in <Location> West Germany </Location>}'' and the target sentence ``\textit{Bruce Willis nació en Alemania Occidental}'', we translate the span \textit{West Germany} into the target language, resulting in \textit{Alemania Occidental}, which is then matched in the target sentence. We employ beam search to generate 100 possible translations and select the most probable one that matches the target sentence.


\subsection{Models Setup}

We use the 3 billion parameters pretrained mT5
(\cite{mt5}) for the \emph{candidate generation} step while \emph{candidates are selected} using the M2M100 12 billion parameter
Machine Translation model (\cite{JMLR:v22:20-1307}). In the case of MasakhaNER, since not all languages are included in M2M100, we resorted to NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) 3 billion parameter model instead, which was also used by the EasyProject method (\cite{chen-etal-2023-frustratingly}). Both MT models demonstrate comparable performance. 

We train the HuggingFace's (\cite{DBLP:journals/corr/abs-1910-03771})  implementation of mT5  \footnote{\url{https://huggingface.co/google/mt5-xl}} (3 billion parameter model) in the candidate generation step using the following hyper-parameters: Batch size of 8, 0.0001 learning rate, 256 tokens sequence length, cosine scheduler with 500 warn up steps and no weight decay. We use AdaFactor (\cite{DBLP:conf/icml/ShazeerS18}) optimizer. We train the model for 10 epochs in the OTE task, and 4 epochs for the NER and AM tasks. 
In the candidate selection step, we also use HuggingFace's implementation of M2M100, and we use m2m100-12B-last-ckpt \footnote{\url{https://huggingface.co/facebook/m2m100-12B-last-ckpt}} checkpoint of M2M100 released by the authors. We use the direct-translation function of the NMTscore library to compute the translation probabilities. 
For MasakhaNER2.0 we use the training script and evaluation script developed by the authors \footnote{\url{https://github.com/masakhane-io/masakhane-ner/blob/main/MasakhaNER2.0/scripts/mdeberta.sh}} and the same hyper-parameter setup than \citet{chen-etal-2023-frustratingly}. 


\section{Intrinsic Evaluation} \label{sec5:IntrinsicEvaluation}


In this section, we present a set of experiments to evaluate T-Projection in comparison to current state-of-the-art approaches for annotation projection. The intrinsic evaluation focuses on measuring the annotation projection accuracy of the models, isolated from other factors such as the quality of the translation models or any other steps in the pipeline. We also separately analyze the performance of the candidate generation and candidate selection steps.

For the OTE task, we train T-Projection and XLM-RoBERTa using the full English ABSA 2016 dataset. Additionally, we train the four word-alignment systems (excluding SimAlign, which is an unsupervised method) using the English data along with the respective translations as parallel corpora. We augment the parallel data with 50,000 random parallel sentences from ParaCrawl v8 (\cite{espla-etal-2019-paracrawl}). Models are evaluated based on the manual label projections described in Chapter \ref{ch:model-vs-data}.

As the Europarl-based NER dataset (\cite{agerri-etal-2018-building}) provides only test data for each language, T-Projection and XLM-RoBERTa are trained using the full English CoNLL 2003 dataset (\cite{DBLP:conf/conll/SangM03}) together with the labeled English Europarl test data. The word alignment models, in turn, are trained with the parallel sentences from the Europarl-based NER data plus 50,000 parallel sentences extracted from Europarl v8 (\cite{DBLP:conf/mtsummit/Koehn05}). We evaluate the models based on the manual annotations provided by \citet{agerri-etal-2018-building}.

For Argument Mining, we use the full Neoplasm data from the AbstRCT dataset to train T-Projection and XLM-RoBERTa, adding its Spanish translation as parallel corpora for the word alignment systems. As this is a medical text corpus, the parallel corpora are complemented with 50,000 parallel sentences from the WMT19 Biomedical Translation Task (\cite{bawden-etal-2019-findings}). We evaluate the models based on the manually projected labels by \citet{DBLP:journals/corr/abs-2301-10527}.

\subsection{Annotation Projection Quality}
\input{Capitulos/5_DataTransfer/tables/IntrinsicEvaluation}


Table \ref{tab5:IntrinsicResults} presents the results of the automatically projected datasets generated by each projection method, compared to the human-projected versions of those datasets. Systems using word alignments achieve consistently good results, particularly those utilizing language models such as SimAlign and AWESOME. Specifically, AWESOME performs well in OTE and NER but poorly in AM. Manual inspection reveals that AWESOME struggles to align articles and prepositions within long sequences. The statistical-based models, Giza++ and FastAlign, achieve competitive performance considering their very low computational resource requirements (they do not require a GPU) compared to the Transformer-based approaches.

XLM-RoBERTa-xl demonstrates strong zero-shot cross-lingual performance. However, the quality of the generated datasets is lower than those produced by the word-alignment systems. The results of the Span Translation approach are disappointing, particularly for the long sequences in the AM task. Translating the labeled spans independently often results in translations that cannot be located in the target sentence.


Our T-Projection method achieves the best results for every task and language. In OTE, it outperforms all other methods by more than 2 points in F1 score averaged across the three languages. This indicates that T-Projection robustly projects labeled spans into machine-translated data. The NER evaluation is slightly different because the parallel data was translated by human experts. In this context, T-Projection significantly improves AWESOME's results by 4.7 points, marking a substantial improvement in the quality of the generated datasets. Despite the word alignment systems being trained with Europarl domain-specific data and most of the training data for T-Projection coming from the CoNLL-2003 dataset (news domain) plus a few annotated sentences (699) from Europarl, T-Projection still achieves the best results in NER label projection. This suggests that our system is effective even in out-of-domain settings. 

Furthermore, T-Projection achieves the highest overall scores in Argument Mining, demonstrating its exceptional ability to project long sequences. T-Projection outperforms the second-best model by 9.4 points in F1 score, with a by 96.0 points in F1-scores, indicating near-perfect projection of all examples in the dataset.

When considering the average performance across the three tasks and five languages, T-Projection improves the F1 score by 8.6 points compared to the second-best system, SimAlign. These results represent a significant advancement over all previous annotation projection approaches. To the best of our knowledge, these are by a wide margin the best annotation projection results published for sequence labeling.


\subsection{The Role of the Candidates} \label{sec5:RoleCandidates}

\input{Capitulos/5_DataTransfer/tables/RoleCandidates}

We perform a set of experiments to measure the relevance and performance of the \emph{candidate generation} and \emph{candidate selection} steps. First, we replace mT5 as the candidate generation model with an n-gram-based approach. We extract all the n-grams with sizes ranging from 1 to the sentence length (e.g., \textit{``Serve'', ``really'', ``good'', ``sushi'', ``Serves really'' ... ``Serves really good sushi''}) and consider them as candidates. Then we rank the candidates using the translation probabilities obtained by the M2M100 model. As shown in Table \ref{tab5:CandidateResults}, the n-gram-based approach's performance is significantly lower than T-Projection in all tasks and languages. This indicates that the mT5 model is crucial for generating relevant candidates. The n-gram approach generates a large number of very similar candidates, which makes it difficult for the M2M100 model to select the correct one. These experiments demonstrate the importance of the mT5 model in generating relevant candidates.

We also replace the \emph{candidate selection} method with the \emph{most probable candidate}. That is, we select the most probable candidate generated by mT5 for each labeled span in the source sentence, thus we do not consider the translation probabilities, as only one candidate is generated for each labeled span. This approach achieves competitive results with the word alignment systems in Table \ref{tab5:IntrinsicResults}, but it is outperformed by T-Projection by an average of 9.2 points in F1-Score.

This ablation study demonstrates that although each step of T-Projection in isolation is able to achieve competitive results on its own, it is the combination of both steps that allows T-Projection to achieve very high performance in the intrinsic evaluation.

Finally, we define an upper bound for the \emph{candidate selection} step. This upper bound is defined by always selecting the correct candidate among the generated candidates. If the correct candidate is not generated by mT5, we select the most probable candidate. This upper bound achieves an average F1 score of 98.0. This result confirms that with a very high probability, the correct candidate is among the candidates generated by mT5.

\subsection{How many candidates are necessary?} \label{sec5:Howmany}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.80\linewidth]{Capitulos/5_DataTransfer/tables/CandidateNo.pdf}
    \caption{F1 score when generating a different number of candidates.}
    \label{fig5:CandidateNo}
\end{figure}

Generating candidates is computationally expensive. The number of FLOPs, memory usage, and inference time increase linearly with the number of candidates. Thus, generating 20 candidates is twice as expensive as generating 10 candidates. Additionally, we must consider the extra cost of computing similarity scores for each candidate. Therefore, we performed an experiment to determine the optimal number of candidates to generate. As shown in Figure \ref{fig5:CandidateNo}, the performance of T-Projection increases with the number of candidates. However, the performance improvement diminishes as the number of candidates grows. For OTE and NER, the improvement is negligible after generating 25 candidates. For AM, generating more than 10 candidates is, in fact, counterproductive. While the reported results in the paper were obtained by generating 100 candidates, a decision informed by preliminary studies using the upper bound described in Section \ref{sec5:RoleCandidates}, the results in Figure \ref{fig5:CandidateNo} suggest that generating 10 to 25 candidates is optimal.


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.80\linewidth]{Capitulos/5_DataTransfer/tables/CandidateNoUperbound.pdf}
    \caption{Number of times the correct candidate is among the top-k candidates generated by mT5.}
    \label{fig5:CandidateNoUpperbound}
\end{figure}

Generating a larger number of candidates is not beneficial for the performance of T-Projection. To further understand why this happens, we computed the F1 score of T-Projection following the upper bound described in Section \ref{sec5:RoleCandidates} for different number of candidates. This is, we compute how many times the correct candidate is among the top-k candidates generated by mT5. As shown in Figure \ref{fig5:CandidateNoUpperbound}, as we increase the number of candidates, the probability of the correct candidate being among the top-k candidates also increases. However, the improvement is not linear. It becomes less significant after generating 25 candidates. The performance improvement from going from 25 to 100 candidates is only 1 point in F1 score for OTE and NER and less than 0.5 points for AM. Interestingly, for AM, the best candidate is among the top-25 candidates 99\% of the time, which explains the great performance of T-Projection in this task. However, if the best candidate is found with a very high probability among the top-25 candidates, what are the candidates beyond the top-25? By examining the candidates generated by mT5 in the NER tasks, we found that only a few of the generated candidates are valid. The remaining ones are hallucinated spans that do not exist in the sentence and are therefore filtered out. These hallucinated spans are usually variations of the correct candidate. We found that there are fewer than 20 valid candidates per sentence, with an average of 5.95 valid candidates per sentence for NER. Thus, generating more than 25 candidates is not beneficial, but it also does not introduce noise that severely hinders the performance of T-Projection, as the extra candidates are usually hallucinated spans that are filtered out in the candidate selection step, rather than different n-grams from the target sentence.


\subsection{Model size and performance}

\input{Capitulos/5_DataTransfer/tables/modelSize} 

We analyze the performance of T-Projection using an mT5 model and a translation system with different numbers of parameters. Additionally, we evaluate T-Projection with various Machine Translation models, namely, NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) and PRISM (\cite{DBLP:conf/emnlp/ThompsonP20}). Table \ref{tab5:ModelSize} demonstrates that the Machine Translation system and its parameter count do not significantly impact T-Projection's performance.

However, the size of the mT5 model has a substantial impact on the system's final performance. While switching from a 3B to a 738M parameter mT5 model yields competitive results for OTE and NER, this is not the case for AM. The overall trend indicates that decreasing the number of parameters results in decreased performance. In summary, to achieve competitive performance across all tasks, T-Projection requires an mT5 model with 3B parameters, although a 738M parameter model remains competitive for OTE and NER.

\section{Extrinsic Evaluation}

\input{Capitulos/5_DataTransfer/tables/ExtrinsicEvaluation}

In this section, we evaluate T-Projection in a real-world low-resource scenario, namely, Named Entity Recognition for African languages. We compare the results obtained by training on NER datasets automatically generated by T-Projection with those automatically projected using three state-of-the-art label projection systems: AWESOME (the second-best NER system in Table \ref{tab5:IntrinsicResults}), EasyProject, and CODEC. We use the exact same settings as \citet{chen-etal-2023-frustratingly} and \cite{DBLP:journals/corr/abs-2402-03131}. For each target language in MasakhaNER2.0, we first translate the English CoNLL dataset using the NLLB-200 3 billion parameter model. Next, we project the English labels into the target language. It should be noted that EasyProject performs both of these processes in a single step. Subsequently, we train an mDebertaV3 (\cite{DBLP:conf/iclr/HeLGC21/deberta}) model using the automatically generated datasets for each target language. Finally, this model is evaluated on the gold MasakhaNER2.0 test data. We only evaluate the eight languages in MasakhaNER2.0 supported by mT5. We focus on named entities referring to Person, Location, and Organization. We also evaluate the zero-shot model-transfer approach presented in Chapter \ref{ch:model-vs-data}. That is, we train the mDebertaV3 model with the original English CoNLL data and evaluate it on the MasakhaNER2.0 test sets.


Table~\ref{tab5:MasakhaNER2} presents the results of the evaluated models on the gold MasakhaNER 2.0 test sets. For T-Projection, we present the results of training with the automatically generated data for the target language only, and also by adding the original English CoNLL data concatenated with the automatically generated data for each target language. Regarding other systems, we only show the former results, as it was the only metric reported by previous work. In order to train and evaluate the NER models, we apply the same hyperparameter settings and code as the authors of EasyProject.

\subsection{T-Projection vs other annotation projection systems}

The results show that T-Projection achieves superior performance for seven out of the eight languages compared to other annotation projection systems. Our model demonstrates a more pronounced performance difference in agglutinative languages such as Igbo and Shona. As outlined in Section \ref{sec5:IntrinsicEvaluation}, our model produces superior alignments compared to AWESOME. Furthermore, we found that EasyProject, which utilizes markers for simultaneous translation and projection, introduces translation artifacts that hinder the performance of the downstream model. These artifacts are particularly noticeable in agglutinative languages, as EasyProject tends to separate words. For instance, in the case of Shona, consider the English sentence \textit{``[Germany]'s representative to the [European Union]'s veterinary committee [Werner Zwingmann]''}. Our system produces the Shona sentence \textit{``Mumiriri [weGermany] kukomiti yemhuka [yeEuropean Union] [Werner Zwingmann]''}, while EasyProject produces \textit{``Mumiriri we [Germany] ku [European Union] komiti yezvokurapa mhuka [Werner Zwingmann]''}. When training mDeberta-v3 with T-Projection's generated data, which preserves the agglutinated words, we achieve better results compared to EasyProject which introduces artifacts by separating agglutinated words during translation and projection. CODEC improves over EasyProject, the two step approach, in which first the translation is performed and then the markers are added, helps to preserve the translation quality. However, the performance of CODEC is still lower than T-Projection for every language except for Zulu. 

\subsection{T-Projection vs Model-transfer}

Table \ref{tab5:MasakhaNER2} presents the results of training the multilingual model mDebertaV3 with the original English CoNLL data and conducting zero-shot evaluation on the MasakhaNER2.0 test sets. The findings indicate that T-Projection outperforms this baseline in six out of eight languages, with an average improvement of 15.6 F1 points. This contrasts with the results from Chapter \ref{ch:model-vs-data}, where model-based transfer learning surpassed data-based approaches. The discrepancy can be attributed to two factors. 
First, the data generated by T-Projection is of higher quality than the data generated by the word alignment systems used in Chapter \ref{ch:model-vs-data}. On average, T-Projection is 8.6 F1 score points better than the best word-alignment system in intrinsic evaluation and 3.8 F1 score points superior in extrinsic evaluation.

Second, the zero-shot evaluation in Chapter \ref{ch:model-vs-data} was conducted from English to other high-resource languages, whereas in this chapter, we perform cross-lingual evaluation into African low-resource languages. These low-resource languages have significantly different morphology and syntax compared to English, and multilingual models typically have lower proficiency in these languages. As demonstrated in Chapter \ref{ch:model-vs-data}, model-based transfer performance requires a multilingual model with high proficiency in both the source and target languages.

These results demonstrate that data-based transfer approaches, such as T\-Projection, can be highly effective for performing Natural Language Processing tasks in low-resource languages, especially in the absence of a high-proficiency multilingual model.

In contrast to previous work, our experiments revealed that concatenating English and translated data did not yield better results, likely due to the superior quality of the data generated by T-Projection. To the best of our knowledge, these are the best zero-shot results achieved for MasakhaNER2.0, highlighting the significant benefits of T-Projection for NLP tasks in low-resource languages.

\section{Conclusions}

In this section we have introduced T-Projection, a novel method for projecting labeled sequences across languages. T-Projection leverages the zero-shot cross-lingual capabilities of large pretrained multilingual language models to generate candidates for each labeled span in the source sentence. These candidates are then ranked using a Machine Translation model to select the best projection candidate for each labeled span in the target sentence.

We have demonstrated that T-Projection outperforms current state-of-the-art label projection systems in both intrinsic and extrinsic evaluations. In the intrinsic evaluation, T-Projection achieves the best results for every task and language, improving the F1 score by 8.6 points compared to the second-best system. In the extrinsic evaluation, T-Projection achieves superior performance in seven out of the eight languages in the MasakhaNER2.0 dataset. These results underscore the effectiveness of T-Projection in generating high-quality training data for sequence labeling tasks in low-resource languages.

Moreover, T-Projection surpasses model-based cross-lingual transfer in the extrinsic evaluation, specifically for named entity recognition in African low-resource languages. While model-based transfer learning outperformed data-based approaches in Chapter \ref{ch:model-vs-data}, it is less effective for cross-lingual transfer from English into the African languages tested in this chapter. In this scenario, T-Projection generates high-quality training data that significantly improves the performance of the downstream model. This demonstrates the potential of data-based transfer approaches for NLP tasks in low-resource languages.
