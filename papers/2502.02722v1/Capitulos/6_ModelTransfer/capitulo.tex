\selectlanguage{english}
\chapter[Improving Model Transfer]{Improving Model Transfer}
\label{ch:model-transfer}

In this chapter we will focus on zero-shot model transfer. In Chapter \ref{ch:model-vs-data} we demonstrated that model transfer could be an effective and efficient approach for cross-lingual transfer when using a high-capacity model on the target language. Therefore, for this approach to be effective, it is crucial to use the most powerful models available. Currently, these models are the text-to-text Large Language Models (LLMs). However, using LLMs for zero-shot cross-lingual sequence labeling is not straightforward. In this chapter we will introduce a constrained decoding algorithm that effectively addresses this issue. A comprehensive empirical evaluation across multiple tasks and languages demonstrates that, when our method is applied to an LLM, it helps not only to improve over the unconstrained beam search baseline but also to outperform the zero-shot cross-lingual capabilities of encoder-only models, especially for languages that significantly differ from English.

\section{Motivation and contributions}
\label{sc6:intro}

In Chapter \ref{ch:model-vs-data}, we demonstrated that the performance of zero-shot cross-lingual transfer can be significantly enhanced by using a high-capacity model for the target language. However, we only employed encoder-only models such as XLM-RoBERTa-large (\cite{conneau-etal-2020-unsupervised}), which has 561 million parameters and was trained on approximately 295 billion tokens. However, as mentioned in Chapter \ref{sc:deep-learning-sota}, the most powerful models currently available are text-to-text Large Language Models (LLMs) like T5 (\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5}), LLaMA (\cite{llama3modelcard}), and GPT-4 (\cite{openai2024gpt4technicalreport}). These models have demonstrated superior capabilities in a wide range of NLP tasks, including the ability to solve tasks for which they were not explicitly trained. Consequently, efforts to scale NLP models have primarily focused on text generation models. As shown in Table \ref{tab:model-size}, the latest generation of LLMs have significantly more parameters and were trained on much larger datasets compared to XLM-RoBERTa.


\input{Capitulos/6_ModelTransfer/tablas/model_size.tex}

LLM models have already been proven effective for Information Extraction and sequence labeling tasks in monolingual evaluations in English (\cite{DBLP:journals/corr/abs-2305-15444, sainz2024gollie}). However, their performance still lags behind encoder-only models in multilingual sequence labeling (\cite{DBLP:conf/semeval/FetahuKCRM23}). For low-resource languages, such as African languages, \cite{DBLP:conf/africanlp/OjoO23} demonstrated that most text-to-text LLMs for named entity recognition do not perform well at all when evaluated in a zero-shot setting. Their results are reproduced in Table \ref{tab6:howgoodafrican}. The table shows that the performance of LLMs is significantly lower than that of XLM-RoBERTa-large. This is a surprising result, given that many of the LLMs have been trained on much larger multilingual datasets and have significantly more parameters than XLM-RoBERTa-large. Thus, the question arises: why do LLMs perform poorly in zero-shot cross-lingual sequence labeling tasks?

\input{Capitulos/6_ModelTransfer/tablas/how_good_are_llm_african.tex}


In this chapter, we investigate the performance of LLMs in zero-shot cross-lingual sequence labeling tasks. Our contributions are as follows:

\paragraph{We identify the challenges faced by text-to-text models for zero-shot sequence labeling:} In this setting we must first establish a text-based input and output representation for the specific task. However, current text-to-text models are tailored for generating free-form text. As our experiments demonstrate, models fail to strictly adhere to the output structure. Moreover, as demonstrated by our experiments, text-to-text models often produce outputs mixing the source language and the target language, which compromises their performance. These issues are illustrated in Figure \ref{fig6:constrained_unconstrained}, where the incorrect output mixes English and Basque (Turkiako-Turkish) and incorrectly segments the organization entity ``Realean''.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/6_ModelTransfer/diagramas/ConstrainedUnconstrainedDecoding.pdf}
    \caption{Comparison between a valid (top green) and invalid (bottom red) output structure to represent a Named Entity Recognition task. English translation: (They) played in Real and in the Turkish national team.}
    \label{fig6:constrained_unconstrained}
\end{figure}

\paragraph{We propose a constrained decoding algorithm for text-to-text models:} We introduce a constrained decoding algorithm that enforces the output structure of the target task. Our method can be seamlessly integrated with any text-to-text model without any significant increase in the decoding cost. Although constrained generation has been previously explored in a monolingual setting (\cite{liu-etal-2022-autoregressive}), we adapt and extend this approach for zero-shot cross-lingual IE. Our new decoding algorithm is evaluated on three popular IE tasks for 25 languages of varied morphological characteristics. Empirical results indicate that our method, when applied to an LLM such as mT0-XL (\cite{DBLP:conf/acl/MuennighoffWSRB23}), not only surpasses the unconstrained beam search baseline but also outperforms the zero-shot cross-lingual performance of encoder-only models. Our method is especially successful for languages that significantly differ from English.

To the best of our knowledge, our new technique achieves the best zero-shot model-based cross-lingual transfer results to date.


\section{Related Work}
In this section, we focus first on related work concerning large language models (LLMs) for sequence labeling. In the second part of the related work, we review prior research on constrained decoding.

\subsection{LLMs for sequence labeling}

The introduction of models like T5 (\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5}) and GPT (\cite{radford2019language}) revolutionized NLP by adopting a text-to-text approach, enabling models to handle a wide array of tasks with a single training objective. Consequently, all NLP tasks can be framed as text-to-text tasks, where the input is a description of the task or a prompt, and the output is the desired result (\cite{chung-flan-instruction-models}). Scaling these models in both the amount of training data and the number of parameters, has led to the development of state-of-the-art models, such as GPT-4 (\cite{openai2024gpt4technicalreport}), LLaMA (\cite{llama3modelcard}), and Mistral (\cite{jiang2023mistral7b}). These models achieve state-of-the-art results on a broad range of NLP tasks (\cite{DBLP:journals/csur/MinRSVNSAHR24}), including those they were not explicitly trained for (\cite{radford2019language}).

In the field of Information Extraction (IE), the text-to-text approach has also been explored. \cite{DBLP:conf/acl/0001LDXLHSW22} introduced a unified text-to-structure generation model capable of handling various IE tasks universally. \cite{DBLP:conf/aaai/Lou0DJLH0023} proposed converting IE tasks into a semantic matching problem, allowing their method to generalize to new domains and label ontologies not encountered during training. \cite{DBLP:journals/corr/abs-2304-08085} framed IE tasks as natural language descriptive instructions and trained a large language model (LLM) across a diverse range of IE tasks. In evaluations involving tasks with unseen label ontologies, their model outperformed other instruction-tuning methods. More recently, \cite{DBLP:conf/acl/BlevinsGZ23} and \cite{sainz2024gollie} proposed using complex instructions that include annotation guidelines, similar to the ones used by human annotators, to enhance the performance of LLMs in sequence labeling tasks. This approach has proven effective in achieving strong performance in classifying unseen categories in sequence labeling tasks in English.

While success has been achieved in labeling unseen categories in English, the supervised performance of LLMs, when training data is available, is still not superior to that of smaller encoder-only models (\cite{sainz2024gollie}). Additionally, recent shared tasks (\citep{DBLP:conf/semeval/FetahuKCRM23}) have shown that for languages other than English, encoder-only language models such as XLM-RoBERTa (\cite{conneau-etal-2020-unsupervised}) and mDEBERTA (\cite{DBLP:conf/iclr/HeLGC21/deberta}) remain the most effective models.




\subsection{Constrained decoding}

The formulation of information extraction tasks in a constrained text-to-text format has been previously explored (\cite{DBLP:conf/nips/VinyalsKKPSH15,DBLP:conf/acl/XiaoDG16,DBLP:conf/naacl/DyerKBS16}). However, it was with the emergence of large-scale text-to-text language models, that this approach garnered significant attention within the community. \citet{DBLP:conf/emnlp/LesterPHCB20} propose a Named Entity Recognition system that uses Viterbi decoding (\cite{viterbi}) with heuristically determined transition probabilities that prohibit illegal transitions. This achieves similar performance to the conditional random field (CRF) models (\cite{DBLP:conf/icml/LaffertyMP01}), but it is more computationally efficient. \citet{genre} and \citet{mgenre} propose a sequence-to-sequence system for Multilingual Entity Linking, which can generate entity names from left to right, token by token, in an autoregressive manner, conditioned by the context. To ensure that only valid entity identifiers are generated, they employ a prefix tree to enable constrained beam search. 

Closer to our work, which focuses on constraining large language models (LLMs) to adhere to a pre-defined output structure, \citet{lu-etal-2021-text2event} presents a constrained decoding algorithm that ensures the model adheres to a specified output structure during inference. Similarly, \citet{zheng-etal-2023-grammar} and \citet{DBLP:journals/corr/abs-2302-02275} propose constrained decoding algorithms that enhance semantic parsing. Instead of constraining the generation of output text, \citet{cui-etal-2021-template} perform Named Entity Recognition (NER) by computing the probability of a text span filling predefined structures. Rather than flattening the structured output into a sequence, \citet{liu-etal-2022-autoregressive} model the output as sequences of actions. These actions are predicted in an autoregressive manner using LLMs, and executing the actions generates the structured output. Their approach improves upon previous methods in NER, end-to-end relation extraction, and co-reference resolution. \cite{DBLP:conf/emnlp/GengJP023} demonstrate that grammar-constrained decoding (GDC) can significantly enhance the performance of large language models (LMs) across a variety of structured NLP tasks, such as information extraction, entity disambiguation, and constituency parsing, by ensuring outputs adhere to a given structure. GCD-enhanced LMs outperform both unconstrained LMs and task-specific finetuned models, particularly in scenarios with limited training data.

Although previous research has demonstrated the effectiveness of constrained decoding for information extraction,  most of it has focused on monolingual settings. Thus, \citet{DBLP:conf/acl/GuoR21} propose an algorithm that employs constrained decoding of text-to-text LLMs for zero-shot NER in low-resource languages. First, they translate labeled data in a word-by-word manner using a dictionary. Then, they construct target language text from the source-language named entities using a pretrained language model. They utilize constrained decoding to ensure the presence of entities in the generated text. This data-transfer method was later surpassed by model-based cross-lingual transfer methods as we demonstrate in Chapter \ref{ch:model-vs-data}.

To project labels across languages in sequence labeling tasks, \citet{DBLP:journals/corr/abs-2402-03131} introduce markers to the input text to represent the labeled sequences. They then translate the text into the target language, achieving both translation and annotation projection. To prevent translation artifacts caused by the markers, they propose a constrained decoding algorithm that ensures the output of the translation when markers are introduced, remains consistent with translations without markers. Although this method is effective, it is a data-based approach that requires training a new model on the projected data. 

\section{Approach}\label{sec6:Approach}

In this section, we describe our representation of a Sequence Labelling task by applying our new Constrained text-to-text approach. Our algorithm can be used for both encoder-decoder (\cite{DBLP:conf/nips/VaswaniSPUJGKP17}) and decoder-only (\cite{DBLP:conf/iclr/LiuSPGSKS18}) architectures, as well as any other auto-regressive architecture. 

\subsection{Input-Output Representation}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{Capitulos/6_ModelTransfer/diagramas/ConstrainedDecoding.pdf}
    \caption{Text-to-Text representation of the Sequence Labeling task. Given an input sentence, the model must generate the same sentence annotated with html-style tags.}
    \label{fig6:constrained}
\end{figure}

The model is prompted with a sentence to label. The expected output is the same sentence annotated with HTML-style tags. An example is provided in Figure \ref{fig6:constrained}. The HTML tags for each task are added as special tokens to the model's vocabulary. Previous research (\cite{DBLP:conf/emnlp/0001NCHYS22}) found that different structures do not greatly impact the performance of the model so we use HTML-style tags because the format is easy for humans to read. Furthermore, LLMs, which have been trained on vast amounts of data from the Internet, are already familiar with this format, and implementing a constrained grammar for this structure is quite straightforward. In any case, our method can be adapted to any other task representation.
For encoder-decoder models, the unlabeled sentence is given as input into the encoder block, while the decoder block generates the labeled output. For encoder-only models, we use the token $\,\to\,$ during training as a separator between the unlabeled and labeled sentence. In the case of instruction-tuned models, instead of the separator, we use corresponding the chat-template to represent the unlabeled sentence as the user input and the labeled sentence as the chatbot response. We also experimented with generating only the labeled spans as output (i.e., \textit{<Person> Obama </Person> <Location> New York </Location>}), but we obtained worse results.


\subsection{Constrained decoding}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{Capitulos/6_ModelTransfer/diagramas/FiniteStateAutomata.pdf}
    \caption{Our Constrained Decoding Algorithm is defined as a Finite State Automaton.}
    \label{fig6:automaton}
\end{figure}

The constrained decoding algorithm ensure that the output sequence contains the same words as the input sequence. This \textbf{prevents hallucinations}, which are very common when a model is trained in one language and then used to label sentences in another language. It also ensures that the output sequence is a valid HTML annotation, with no unclosed tags, empty tags, or other errors. This \textbf{prevents the generation of unparseable outputs}. We implement our constrained decoding algorithm using the Finite State Automaton described in Figure \ref{fig6:automaton}. At each stage, the model can generate only a set of valid tokens. This set includes copying the next word from the input (if the word is split by the tokenizer into multiple tokens, all of them are copied to prevent the splitting of words). It can also open an HTML tag, but only if no tag remains open, or close it, but only if we have already opened a tag and copied at least a word. The generation process ends when all the words in the input have been copied into the output and no tag remains open.

Given a sequence \((x_1, x_2, \dots, x_{t-1})\) that has been generated thus far and a set \(S_t\) of valid next tokens at step \(t\), the next token \(x_t\) is selected as:
\[ x_t = \arg \max_{x \in S_t} P(x|x_1, x_2, \ldots, x_{t-1}) \]
where \(P(x|x_1, x_2, \dots, x_{t-1})\) represents the conditional probability of token \(x\) given the prior tokens. Any token not in \(S_t\) is given a probability of zero, ensuring that the generated sequence adheres to the constraints. The probability for each token \(x_i \in S_t\) is computed using the softmax function applied to the model predictions:

\[
P(x_i|x_1, x_2, \ldots, x_{t-1}) = \frac{e^{x_i}}{\sum_j e^{x_j}}
\]

The probability of the generated sequence up to step \(T\) is computed as:

\[
P(x_{1:T}|\textrm{<bos>}) = \prod_{t=1}^{T} P(x_t|x_1, x_2, \ldots, x_{t-1})
\]

While most previous constrained decoding algorithms are limited to greedy decoding, we implement a \textbf{constrained beam search} approach. We keep track of the top \(k\) most probable sentences at each step \(t\), ensuring a broader exploration of the solution space and yielding higher-quality output sequences that adhere to the given constraints. Our constrained beam search approach adds very little overhead compared to the standard beam search decoding strategy. At each step, our only additional computation is to filter out invalid tokens from the beam. It's important to note that our constrained beam search decoding algorithm merely eliminates invalid sequences from the search space. Consequently, the constrained beam search will always yield an output that is at least as good as, if not superior to, unconstrained beam search.


\section{Experimental Setup}

The datasets used address three information extraction tasks which are illustrated by Figure \ref{fig6:tasks}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{Capitulos/6_ModelTransfer/diagramas/Tasks.pdf}
    \caption{Information Extraction Tasks in our experiments}
    \label{fig6:tasks}
\end{figure}

\paragraph{Named Entity Recognition (NER):} This task consists of detecting
named entities and classifying them according to some pre-defined categories. We evaluate the models on MasakhaNER 2.0 (\cite{adelani-etal-2022-masakhaner}), a manually annotated NER dataset for 20 African languages. We train the models with the CoNLL03 (\cite{DBLP:conf/conll/SangM03}) English training split. We focus on named entities referring to Person, Location and Organization.

\paragraph{Opinion Target Extraction (OTE):} Given a review, the task is to detect the linguistic expression used to refer to the reviewed entity. We use the
English SemEval 2016 Aspect Based Sentiment Analysis (ABSA) datasets
(\cite{pontiki-etal-2016-semeval}). The English training split is used for fine-tuning; results are reported on the Spanish, French, Dutch, Russian and Turkish test sets.

\paragraph{Event Extraction (EE):} It consists of detecting and classifying event mentions according to some pre-defined class-inventory. We use the English ACE05 (\cite{ACE}) training split for training and the Chinese test split for evaluation. We also perform the Entity Mention Extraction task separately as an additional indicator of performance. 

\subsection{Language Models and baselines}

\paragraph{Baselines:} We assess the performance of our grammar-constrained beam search algorithm (\textbf{Cons}) against the unconstrained decoding baseline (\textbf{Base}). After fine-tuning, we test the same checkpoint using both constrained and unconstrained decoding. Additionally, our method is compared to popular encoder-only models, which currently set the benchmark for zero-shot cross-lingual transfer and have been widely adopted by the community. Thus, we evaluate mDeBERTa-v3 (\cite{DBLP:conf/iclr/HeLGC21/deberta}), an 86-million-parameter model, and GLOT500 (\cite{DBLP:conf/acl/ImaniLKSSKMSMYS23}), a 125-million-parameter model. Although we also experimented with XLM-RoBERTa (\cite{conneau-etal-2020-unsupervised}) models of various sizes, they consistently lagged behind mDeBERTa-v3 in performance. For MasakhaNER, we additionally compared with afro-xlmr-large (\cite{alabi-etal-2022-adapting}), a 355-million-parameter encoder-only model fine-tuned on African languages.

\paragraph{Text-to-text Models:} We experiment with three different encoder-decoder models: mT0-XL (\cite{DBLP:conf/acl/MuennighoffWSRB23}) 3.7 Billion parameter model. mT0-XL is an mT5 (\cite{mt5}) pretrained multilingual language model fine-tuned in the cross-lingual task mixture xP3. We also experimented with mT5 itself and Aya-101 (\cite{aya101}) an encoder-decoder model trained with instruction data in 101 languages. 

We also test multiple instruction tuned decoder-only models: Qwen2~(\cite{yang2024qwen2technicalreport}), gemma~(\cite{gemmateam2024gemmaopenmodelsbased}), LlaMA-3~(\cite{llama3modelcard}), Aya-23\\(\cite{aya23}) and Yi 1.5~(\cite{ai2024yiopenfoundationmodels}). These models have been trained on a wide range of tasks and languages, and have demonstrated strong multilingual capabilities.

\subsection{Training Setup}

All models were trained exclusively with English-labeled data and subsequently evaluated in the target languages. For the encoder-only models, we added a token classification layer (linear layer) on top of each token representation and trained them using the Cross-Entropy loss. The text-to-text models, were trained using the standard Next Token Prediction (NTP) loss. 
We finetune all the parameters of mT0 and mT5 using the Adafactor (\cite{DBLP:conf/icml/ShazeerS18}) optimizer. For the other text-to-text models, we found that the full-finetuning approach produces suboptimal results.
Therefore we use Low-Rank Adaptation (LoRA) (\cite{DBLP:journals/corr/abs-2106-09685}) to adapt the models to the target task. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into linear layers of the Transformer architecture.  We applied the LoRA to all linear Transformer block layers as recommended by \cite{qlora}. We use the AdamW  optimizer (\cite{DBLP:journals/corr/abs-1711-05101}). Preliminary experiments showed that LoRA produces a better performance than the full-finetuning approach for these models.  This has already been reported by previous research (\cite{sainz2024gollie}). We hypothesize that the large number of parameters in these models makes them prone to overfitting when finetuning all the hyperparameters on small datasets. For mT0 and mT5, we use a beam size of 4, while for larger models, we use a beam size of 1 as the computational cost of larger beams was prohibitive for us. In any case, we found that increasing the beam size did not significantly improve the performance of the models.

For both, encoder and text-to-text models we use the Huggingface open-source library (Apache-2.0 License) (\cite{DBLP:journals/corr/abs-1910-03771}).


\input{Capitulos/6_ModelTransfer/tablas/hparams.tex}

We evaluate the models at the end of several epochs on the validation set and select the best checkpoint based on the F1 score. The full training hyperparameters are provided in Table \ref{tab6:hparams}. These hyperparameters were chosen based on a hyperparameter search on the validation set. 

\subsection{Evaluation Metrics}

We evaluate the models using the standard F1-score metric for Sequence Labeling tasks (\cite{DBLP:conf/conll/SangM03}). For the text-to-text models, the output of the model is converted into an IOB2 format by splitting the output into words by whitespace. All the models are evaluated using the seqeval library (\cite{seqeval}).

\section{Experiments}
\label{sc6:experiments}

\subsection{Named Entity Recognition}
\label{sc6:ner}

In this section we will present and discuss the experiments in the Named Entity Recognition, Opinion Target Extraction and Event Extraction tasks. 

\input{Capitulos/6_ModelTransfer/tablas/ner.tex}

Table \ref{tab6:NER} presents the performance of our method compared to the baselines in the NER task. All models show comparable performance in English. However, when assessing zero-shot cross-lingual transfer, significant performance differences emerge.

In the zero-shot cross-lingual transfer setting, constrained decoding consistently outperforms unconstrained decoding. For some languages, such as Bambara, Ghomálá, and Éwé, both methods yield similar results. In contrast, other languages, including Shona, isiXhosa, and Zulu, exhibit marked performance improvements. These Southern Bantu languages have unique linguistic features: they capitalize proper names following the noun class prefix (e.g., kweZambia) and display highly inflected morphology (\cite{adelani-etal-2022-masakhaner}). These attributes challenge the cross-lingual transfer abilities of English fine-tuned NER models. Consequently, all baseline models, including the encoder-only variants, perform suboptimally in these languages and are clearly outperformed by our constrained decoding approach.


As demonstrated in Section \ref{sc6:ablation}, text-to-text models struggle with agglutinative languages, frequently mislabeling entities by arbitrarily splitting them into sub-words. Our constrained decoding corrects this by ensuring that the output sentence retains the original words from the input sentence. Overall, constrained decoding excels in the zero-shot cross-lingual setting for languages with highly inflected agglutinative morphology. Although the performance gap is less pronounced for language isolates like Bambara, Éwé, Fon, and Twi, it remains significant.


\input{Capitulos/6_ModelTransfer/tablas/ner_mini.tex}


Models exhibit varying performance across languages. For instance, aya-101 achieves the best performance for Éwé, Luganda, Setswana, Twi, and Wolof, while mT0 is superior for languages such as Fon, Kinyarwanda, chiShona, isiXhosa, and Zulu. We attribute this to the different training data used by the models. Nonetheless, we observe that mT5-xl and mT0-xl, combined with our constrained decoding algorithm, outperform encoder-only models by more than 5 points in F1 score on average. This represents a significant improvement over the previous state-of-the-art for zero-shot cross-lingual transfer in NER tasks.



In Table \ref{tab6:ner_mini} we present the average performance of different models on the MasakhaNER dataset. Qwen2, Gemma, and Aya-23 achieve suboptimal results compared to the other text-to-text models. This is likely due to these models being trained on a smaller number of high-resource languages, rendering them less proficient in African languages. However, the results demonstrate that constrained decoding is effective in improving the performance of all text-to-text models in zero-shot cross-lingual transfer.

\subsection{Opinion Target Extraction}
\label{sc6:ote}

\input{Capitulos/6_ModelTransfer/tablas/ote.tex}

In the NER task, we experimented with cross-lingual transfer approaches using a set of low-resource African languages that significantly differ from English. For the Opinion Target Extraction task, we evaluated cross-lingual transfer performance into languages from the Indo-European language family. Due to the high computational cost of the text-to-text models, we only evaluated the best model from the previous task, mT0-XL.

As shown in Table \ref{tab6:OTE}, excluding Turkish (an agglutinative language), the performance decline in the target languages compared to English is less pronounced, suggesting a more seamless transfer. Even in this context, our constrained generation algorithm significantly surpasses the unconstrained generation. Finally, while mT0-XL and mDeBERTa-v3 show comparable performance, our approach demonstrates slightly higher average performance across the board.


\subsection{Event Extraction}
\label{sc6:ee}

\input{Capitulos/6_ModelTransfer/tablas/ee.tex}

For the Event Extraction task we aim to perform zero-shot cross-lingual transfer from English to Chinese. This task is particularly challenging due to the vast linguistic and cultural differences between the two languages, including script type, syntax, semantics, and the use of tones in Chinese. As reported in Table \ref{tab6:EE}, both GLOT500 and mDEBERTa struggle with the transfer from English to Chinese, whereas mT0-XL achieves much better results. Consistent with previous evaluations, our constrained generation approach improves over the unconstrained generation method by approximately 3 points in F1 score.

\subsection{Model Transfer vs Data Transfer}
\label{sc6:transfer}

In this chapter we focus on improving the zero-shot model-transfer approach. However, constrained decoding can also be used in conjunction with data transfer. In this section we compare the performance of the constrained decoding algorithm when used in both zero-shot model-based transfer and data transfer settings using the MasakhaNER NER dataset. To this end, we use the automatically generated NER datasets for eight African languages from Chapter \ref{ch:data-transfer}. These datasets were generated by translating the CoNLL2003 (\cite{DBLP:conf/conll/SangM03}) English dataset into the target languages using NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) and then projecting the labels using T-Projection. Using the same settings as for the zero-shot approach, we evaluate the performance of the constrained decoding algorithm when fine-tuning the models on the generated datasets. The results are presented in Table \ref{tab6:modelvsdata}. ``Zero'' refers to the models trained with English CoNLL 2003 data and evaluated in the target languages, while "Data" refers to the models fine-tuned on the translated CoNLL 2003 datasets. Both settings use the constrained decoding algorithm.

\input{Capitulos/6_ModelTransfer/tablas/modelvsdata.tex}

The results show that the zero-shot cross-lingual transfer performance when using text-to-text models such as mT5-xl or mT0-xl is significantly better than the zero-shot performance of mDeBERTa-v3, as we already demonstrated in Section \ref{sc6:ner}. However, mDeBERTa-v3 shows very competitive results in the data-transfer setting. For text-to-text models, the performance of the zero-shot and data-transfer approaches varies across languages. In languages where the model is proficient, such as Hausa or Igbo for mT5-xl, the zero-shot approach outperforms the data-transfer approach. However, in languages where the model is less proficient, such as isiXhosa or Zulu, the data-transfer approach is superior. In the case of aya-101 and LLama-3, which we reported to be less proficient in African languages in Section \ref{sc6:ner}, the data-transfer approach results in a significant performance improvement. In fact, aya-101 outperforms all other models in the data-transfer setting. This suggests that the constrained decoding algorithm can be used in conjunction with data-transfer methods to further improve the performance of models in low-resource languages. 

Similar to the insights from Chapter \ref{ch:model-vs-data}, the results suggest that when a model is proficient in both the source and target languages, model-based transfer is superior to data-based transfer. Thanks to the methodology developed in this chapter, we can now successfully leverage the power of text-to-text LLMs in a zero-shot setting to achieve superior zero-shot cross-lingual transfer results. However, when the model is less proficient in the target language, data-based transfer can be a better option. Data transfer also has the advantage of allowing the use of more efficient models. The results demonstrate that while mDeBERTa-v3 is not competitive in the zero-shot setting, it achieves similar results to the best-performing text-to-text models in the data-transfer setting, despite having fewer parameters and requiring less computational resources.

\section{Ablation Study}
\label{sc6:ablation}

In this section we aim to better understand why and in which scenarios constrained decoding performs better than unconstrained decoding. To achieve this, we identify the types of mistakes that unconstrained decoding makes which are subsequently fixed by constrained decoding. These errors can be grouped into three categories: inconsistent HTML markups, word hallucinations, and word splittings.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{Capitulos/6_ModelTransfer/diagramas/Hallucination.pdf}
    \caption{Percentage of hallucinated words compared to the performance delta between unconstrained and unconstrained beam search in MasakhaNER using mT0-XL.}
    \label{fig6:Hallucination}
\end{figure}

\paragraph{Inconsistent HTML markups:} The model occasionally generates HTML markup that cannot be parsed correctly, such as when a tag is opened but never closed. We found that this occurs in less than 1\% of the annotated sentences. Consequently, it has a negligible effect on the overall performance of the model.

\paragraph{Word hallucinations:} The model sometimes includes in the output a word that was not present in the input. This occurs because unconstrained generation often produces output that mixes English and the target language. For instance, given the sentence \textit{``Kaliforni sullã sẽn togse''}, mT0-XL, when using unconstrained decoding, produces \textit{``<Location> California </Location> sullã sẽn togse''}. In this instance, the model has translated \textit{``Kaliforni''} to \textit{``California''}. Furthermore, inadvertent translation is not the only cause of hallucinations in the output. Perhaps due to a limited understanding of the target language, the model often introduces typos (e.g., \textit{``okudlula''} incorrectly becomes \textit{``okudludlule''}). Interestingly, it even mixes African languages. For instance, given a Zulu sentence as input containing the word \textit{``Musawenkosi''} (God Bless You), the model outputs the very similar Chichewa word \textit{``Mumawenkosi''} (You are welcome).

\paragraph{Word Splittings:} They refer to instances where the model either divides a word into multiple subwords or, conversely, combines several words into a single one. This occurs because the model has been trained in English and, when tested on agglutinative languages, it attempts to mimic English morphology by arbitrarily splitting words. For instance, the sequence \textit{``<Location> waseThekwini </Location> <Person> uShauwn Mkhize </Person>''} becomes \textit{``wase <Location> Thekwini </Location> u <Person> Shauwn Mkhize </Person>''}. This behavior is interesting, as lemmatization is a component of many downstream Information Extraction applications. One could argue that this is the desired behavior. However, although accidental lemmatization was performed correctly in this particular example, this is not usually the case. For instance, in Basque (whose results are not reported here for brevity, although the models were tested in this language), as illustrated in Figure \ref{fig6:constrained_unconstrained}, the model incorrectly splits the term \textit{``Realean''} into \textit{``Reale''} and \textit{``an''}. However, \textit{``Reale''} does not represent the correct lemma, which would correspond to \textit{``Reala''}, the name of a football team. Therefore, the model seems to be arbitrarily splitting words to mimic English morphology.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{Capitulos/6_ModelTransfer/diagramas/Hallucination_Params.pdf}
    \caption{Average percentage of mistakes generared by Unconstrained Beam search in MasakhaNER using mT0 models of different sizes}
    \label{fig6:HallucinationvsParams}
\end{figure}


We calculate the percentage of sentences containing some of these errors for each language in the NER task when using mT0-XL with unconstrained generation. The results are depicted in Figure \ref{fig6:HallucinationvsParams}. Additionally, we compared the overall percentage of sentences containing any error with the performance difference between constrained and unconstrained generation. The larger the delta, the greater the performance improvement with constrained generation.

Figure \ref{fig6:HallucinationvsParams} indicates that word splitting and hallucinations correlate with the performance delta, suggesting that addressing these issues is key to the superiority of the constrained generation algorithm. It also underscores that unconstrained generation produces a substantial proportion of sentences with errors. In cases like chiShona and isiXhosa (discussed in Section \ref{sc6:ner}), this could affect over 50\% of the output sentences. It should be noted that word splitting has a more pronounced effect on the performance delta than hallucinations. This can be attributed to the standard sequence evaluation method used for these tasks. 

To convert the model's output into IOB2 encoding, we derive annotations such as \textit{"B-LOC O O O"} for the example \textit{``<Location> California </Location> sullã sẽn togse''}. This annotation remains accurate even if the model translates the entity into English. However, when the model splits or merges words, the IOB2 labeling is disrupted, rendering the sentence incorrect in the evaluation. Thus, although the evaluation method may gloss over hallucination errors, it is important to note that models generate a significant number of hallucinations when producing unconstrained predictions, potentially impacting the ultimate efficacy and applicability of IE systems.



We also evaluated the total number of mistakes generated by unconstrained beam search in the NER task with mT0 models of varying sizes. As illustrated in Figure \ref{fig6:HallucinationvsParams}, word splitting and inconsistent HTML markups remain consistent across models with different parameter sizes. However, the frequency of hallucinations decreases as the model size increases. This might be because models with more parameters have a more refined representation of individual languages and therefore mix languages less frequently.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{Capitulos/6_ModelTransfer/diagramas/F1_Score_by_Model_Parameters.pdf}
    \caption{Average F1 score in MasakhaNER compared to the mT0 model size}
    \label{fig6:F1vsParams}
\end{figure}

Additionally, we assess the average F1 score in the NER task for mT0 models ranging from 300 million to 3.7 billion parameters. The results, presented in Figure \ref{fig6:F1vsParams}, show that as the mT0 model's parameter count increases, the F1 score improves, although we observe diminishing returns beyond 1.2 billion parameters. While our experiments utilize the 3.7 billion parameter mT0-XL, constrained generation surpasses both GLOT500 (a 125 million parameter model) and afro-xlmr-large (355 million parameters) when using an mT0 model with only 580 million parameters. This indicates that the superiority of our method over encoder-only models is not solely due to leveraging a larger model. Notably, with constrained generation, the 580 million parameter mT0 model achieves performance comparable to the 1.2 billion parameter model when the latter employs unconstrained generation. Therefore, constrained generation is also considerably more computationally efficient than its unconstrained counterpart.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{Capitulos/6_ModelTransfer/diagramas/Beams_F1.pdf}
    \caption{Average F1 score of mT0-XL in a subset of MasakhaNER compared to the number of beams used for decoding.}
    \label{fig6:BeamsF1}
\end{figure}

Finally, we evaluate the performance of mT0-XL using a varying number of beams. We assess the same checkpoint with beam search ranging from 1 to 8 beams. For these experiments, we utilize a subset of MasakhaNER2, which includes the following languages: Bambara, Ghomálá, Éwé, Fon, Hausa, Igbo, Kinyarwanda, Luganda, and Mossi. As illustrated in Figure \ref{fig6:BeamsF1}, increasing the number of beams has a negligible effect on performance. Considering that the computational cost and GPU memory requirements increase linearly with the number of beams, in this scenario, using a single beam (greedy decoding) offers the best performance-to-cost ratio. This is because the model is highly confident in its top prediction during each step of the decoding process, and introducing additional beams does not significantly diversify or improve the generated outputs.

\section{Conclusion}

In this Chapter we introduce a Constrained Beam Search Algorithm that can be seamlessly incorporated into any text-to-text LLM. We demonstrate that, compared to Unconstrained Beam Search, our algorithm significantly improves zero-shot cross-lingual performance across a broad range of IE tasks and languages. Through an extensive ablation study, we show that constrained generation effectively mitigates issues such as word-splitting and language mixing, which lead to typos and unintentional translations, errors commonly observed when applying text-to-text models to these tasks. Our approach allows the text-to-text mT0 language model to outperform encoder-only models, which had previously set the state-of-the-art standard for zero-shot cross-lingual IE. To the best of our knowledge, we present the best zero-shot cross-lingual results up to date. 

The method developed in this chapter enables model-based cross-lingual transfer for sequence labelling tasks with text-to-text models. This is a significant step forward in the field of zero-shot cross-lingual transfer, as it allows for the use of more powerful models that can handle a wide range of tasks. Considering the prevailing focus on text-to-text LLMs in current research, and the infrequent training of new encoder-only models, we believe that this represents significant progress in this research area. 