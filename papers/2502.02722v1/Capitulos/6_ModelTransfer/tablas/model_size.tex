\begin{table}[htb]
    \centering
    \small
    \adjustbox{max width=\textwidth}{
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
     & XLM-RoBERTa  & XLM-RoBERTa-xxl    & mT5  & Llama2  & Gemma2  & LLama3  \\ 
     & \cite{conneau-etal-2020-unsupervised} & \cite{DBLP:journals/corr/abs-2105-00572} & \cite{mt5} & \cite{DBLP:journals/corr/abs-2307-09288} & \cite{DBLP:journals/corr/abs-2403-08295-gemma} & \cite{llama3modelcard} \\ \midrule
    Parameters  & 560M & 10.7B  & 11.3B & 70B & 27B & 405B \\
    Train Tokens & 296B & 296B & 1T & 2T & 8T & 17T \\ \bottomrule
    \end{tabular}
    }
    \caption{Size and training data of some relevant open source models.}
    \label{tab:model-size}
    \end{table}