\selectlanguage{english}
\chapter[Introduction]{Introduction}
\label{ch:instroduction}

This thesis is framed within the area of Natural Language Processing (NLP). Natural Language Processing is a multidisciplinary research field within Artificial Intelligence (AI), Computer Science, and Linguistics. NLP involves a wide range of tasks, including, Natural Language Understanding, Machine Translation, Information Extraction, and Text Generation, among others. The main goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable for humans. The Ixa group, within the HiTZ center at the University of the Basque Country, is one of the leading research teams working in NLP. Since its foundation more than 30 years ago, the Ixa group has been a pioneer in developing NLP tools for many different applications, with a special focus on creating language tools for the Basque language. Moreover, Ixa has been involved in many European and international research projects, significantly contributing to languages beyond Basque.

The primary objective of this thesis is to develop cross-lingual transfer learning solutions to address the resource constraints faced by many languages, tasks, and domains. Cross-lingual transfer learning is a research area focused on creating models for low-resource languages by leveraging knowledge from high-resource languages. Specifically, this thesis explores cross-lingual transfer learning for Sequence Labeling tasks, such as Named Entity Recognition, Opinion Target Extraction, and Argument Mining. We propose novel methods for knowledge transfer from high-resource to low-resource languages through translation and annotation projection, as well as multilingual NLP models. Thus, our goal is to develop publicly available models that achieve state-of-the-art performance in low-resource languages and to make these models accessible to the research community. This thesis work was aligned with the objectives of the projects DeepReading\footnote{\url{https://ixa2.si.ehu.eus/deepreading/}}, DeepKnowledge \footnote{\url{http://ixa.si.ehu.es/node/13582}} and Andidote\footnote{\url{https://univ-cotedazur.eu/antidote}}.


\section{Motivation}
\label{motivation}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/1_Introduccion/diagramas/LLMs.pdf}
    \caption{Modern LLMs, which support text, image, and other multimodal representations, have achieved outstanding performance in a wide range of NLP tasks. They have been applied in many real-world applications.}
    \label{fig:llms}
\end{figure}


Neural networks have become an indispensable resource in Natural Language Processing (NLP). Driven by the success of the Transformer architecture (\cite{DBLP:conf/nips/VaswaniSPUJGKP17}), they have demonstrated outstanding performance in various challenging NLP tasks (\cite{DBLP:journals/csur/MinRSVNSAHR24}), such as General Language Understanding (\cite{DBLP:conf/iclr/WangSMHLB19}), Question Answering (\cite{DBLP:conf/acl/RajpurkarJL18}), Text Generation (\cite{brown2020language}), Dialogue (\cite{DBLP:journals/corr/abs-2201-08239}), and Conditional Image Generation (\cite{DBLP:conf/cvpr/RombachBLEO22}), among others. Scaling up these models in terms of parameter count and training data (\cite{chung-flan-instruction-models}) has led to the development of current state-of-the-art NLP systems. Large Language Models (LLMs) such as GPT-4 (\cite{openai2024gpt4technicalreport}) and LLaMA-3 (\cite{llama3modelcard}), trained on hundreds of terabytes of text data and billions of parameters, have proven capable of generating human-like text and have been applied in a wide range of applications, such as the ones depicted in Figure \ref{fig:llms}. These cutting-edge NLP systems hold the potential to bring significant societal changes (\cite{DBLP:journals/corr/abs-2108-07258}).



Despite the remarkable progress in NLP, many challenges remain. LLMs require vast amounts of data and computational resources to achieve optimal performance (\cite{DBLP:journals/corr/abs-2203-15556}). In addition to English, only a handful of Western European languages (principally German, French, and Spanish) and even fewer non-Indo-European languages (primarily Chinese, Japanese, and Arabic) dominate the field (\cite{joshi-etal-2020-state}). While speakers of these languages benefit from the latest innovations in Language Technology—such as quick and accurate access to information using smart assistants, online translation services, interaction with machines using natural language, or speeding-up their work with automatic summarization tools, coding assistants, or image generation tools—speakers of low-resource languages are being left behind (\cite{blasi-etal-2022-systematic}).

Models consistently perform better on high-resource languages, especially English (\cite{etxaniz-etal-2024-multilingual}), while their performance on low-resource languages is significantly lower (\cite{DBLP:journals/corr/abs-2311-07978, DBLP:conf/africanlp/OjoO23}). This disparity is due to the fact that the quality and quantity of the data directly impact the performance of the models (\cite{DBLP:conf/aaai/Liu0YDJCMF21}). For the large majority of the approximately more than 7,000 languages spoken worldwide, this data is scarce or non-existent (\cite{joshi-etal-2020-state}). Therefore, obtaining optimal results would require manually generating annotated data for each application domain and language. Given the rapidly increasing number of tasks and domains to which NLP is applied, this is an unfeasible task in terms of monetary cost and human effort.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{Capitulos/1_Introduccion/diagramas/NER.pdf}
    \caption{Illustration of the Named Entity Recognition (NER) sequence labelling task. The goal is to identify and classify named entities in running text.}
    \label{fig1:Ner}
\end{figure}

The primary objective of this thesis is to develop cross-lingual transfer learning solutions to address the resource constraints faced by many languages, tasks, and domains. \emph{Cross-lingual transfer learning} is a research area focused on creating models for low-resource languages by leveraging knowledge from high-resource languages (\cite{DBLP:conf/nips/ConneauL19}). Cross-lingual transfer learning uses the data and models available in high-resource languages (typically English) to solve tasks in low-resource languages where these resources are scarce or non-existent.

This thesis explores cross-lingual transfer learning for sequence labeling tasks. \emph{Sequence labeling} is the task of assigning a label to each token in a given input sequence (\cite{DBLP:conf/icml/LaffertyMP01}). Figure \ref{fig1:Ner} illustrates the Named Entity Recognition (NER) sequence labeling task, where the goal is to identify and classify named entities in a text. Sequence labeling tasks are essential for many NLP applications, such as Information Extraction, Question Answering, and Sentiment Analysis, among others. By applying cross-lingual transfer learning techniques, such as translation and annotation projection, alongside multilingual NLP models, we aim to leverage resources from high-resource languages to perform sequence labeling in low-resource languages. Our final goal is to develop publicly available models that achieve state-of-the-art performance in low-resource languages. 


\section{Goals and research lines}
\label{goals}

The main goal of this thesis is to develop state-of-the-art cross-lingual transfer learning methods for sequence labeling tasks. We aim to apply these methods to real-world problems where the lack of resources is a significant issue. Additionally, we intend to provide the research community with a set of tools, as well as generate freely available data and models that can be used in the future. The research lines of this thesis are as follows:

\begin{itemize}
    \item \textbf{RL1: Develop better data-based cross-lingual transfer learning methods for sequence labeling tasks.} Data-transfer methods focus on transferring knowledge from high-resource to low-resource languages through translation and annotation projection. At the start of this thesis, most data-based approaches relied on statistical word alignment methods and sub-optimal Machine Translation models. Our goal was to develop improved data-based methods that leverage the latest advances in Machine Translation and NLP models. We also aim to explore the use of multilingual NLP models for data transfer, which have shown promising results in other NLP tasks.
    
    \item \textbf{RL2: Develop better model-based cross-lingual transfer learning methods for sequence labeling tasks.} Model-transfer methods are based on transferring knowledge from high-resource to low-resource languages through pre-trained models. A multilingual NLP model is fine-tuned on data from high-resource languages and then directly applied to low-resource languages. At the start of this thesis, this approach was offering good results in many NLP tasks using encoder-only models. Our objective is to develop improved model-based methods by leveraging the multilingual capabilities of state-of-the-art text-to-text pre-trained models.
    
    \item \textbf{RL3: Real-world application of cross-lingual transfer learning methods.} We aim to apply the developed methods to real-world problems where the lack of resources is a significant issue. By doing so, we aim to better understand which scenarios are best suited for different techniques in cross-lingual transfer learning. Additionally, we develop open-source tools, datasets, and models to support the research community in replicating our experiments and extending our work. These resources are intended to facilitate advancements in NLP for low-resource languages and enable their application across diverse tasks, languages, and domains.

\end{itemize}
\clearpage
\section{Structure of the thesis}

This thesis is structured as a series of interconnected papers, each building on the previous one. The chapters are organized as follows:

In Chapter \ref{ch:related-work}, we present the background of the thesis, review the state-of-the-art in cross-lingual transfer learning for sequence labeling tasks, and introduce the main concepts and techniques used in this research.

Chapter \ref{ch:model-vs-data} focuses on the effectiveness of model-based and data-based cross-lingual transfer learning methods for sequence labeling tasks. We identify the advantages and shortcomings of each method, as well as the challenges faced by current techniques for cross-lingual zero-resource sequence labeling. These insights provide a foundation for the subsequent chapters.

In Chapter \ref{ch:data-transfer}, we introduce a novel data-based method for cross-lingual transfer learning in zero-resource settings. We propose T-Projection, a method that achieves state-of-the-art performance on annotation projection tasks.

Chapter \ref{ch:model-transfer} presents a constrained decoding algorithm that improves the performance of the model-based cross-lingual transfer learning approach. We demonstrate that the constrained decoding algorithm successfully leverages text-to-text models for sequence labeling tasks in low-resource languages achieving state-of-the-art results. 

Chapter \ref{ch:medicalmt5} offers a case study on the application of cross-lingual transfer learning to the medical domain. We show that the methods developed in this thesis can be successfully applied to real-world problems where resource scarcity is a significant issue. By applying both data-based and model-based methods, we develop a comprehensive multilingual pre-training, fine-tuning, and evaluation framework for the medical domain, culminating in the first open-source text-to-text multilingual model for the medical domain.

Finally, Chapter \ref{ch:final-chapter} summarizes the conclusions of the thesis, discusses the main contributions and limitations of the work, and proposes future research directions.


\clearpage


\section{List of scientific contributions}

In this section, we present the scientific contributions developed throughout this thesis. This section is divided into three parts. First, we present the publications that are included in this manuscript. Next, we provide a list of publications closely related to the thesis topic but not included in this manuscript. Finally, we list publications from other lines of research that are outside the scope of this thesis. All papers are listed in chronological order.
\clearpage
\subsection{Contributions included in the thesis}
\label{publi}

These three publications are included in this thesis manuscript, as they present the main contributions of the thesis. Their content will be explained in the following chapters.

\begin{part_of_the_thesis}{Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings.}
    \begin{smallbox}
     Presented in Chapter \ref{ch:model-vs-data}.
    \end{smallbox}
    \underline{Iker García-Ferrero}, Rodrigo Agerri, and German Rigau. \\
    Findings of the Association for Computational Linguistics: EMNLP 2022. \\
    \textit{\href{https://doi.org/10.18653/v1/2022.findings-emnlp.478}{https://doi.org/10.18653/v1/2022.findings-emnlp.478}}
    \end{part_of_the_thesis}

\begin{part_of_the_thesis}{T-projection: High quality annotation projection for sequence labeling tasks}
        \begin{smallbox}
         Presented in Chapter \ref{ch:data-transfer}.
        \end{smallbox}
        \underline{Iker García-Ferrero}, Rodrigo Agerri, and German Rigau. \\
        Findings of the Association for Computational Linguistics: EMNLP 2023 \\
        \textit{\href{https://doi.org/10.18653/v1/2023.findings-emnlp.1015}{https://doi.org/10.18653/v1/2023.findings-emnlp.1015}}
        \end{part_of_the_thesis}

\begin{part_of_the_thesis}{Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain}
    \begin{smallbox}
        Presented in Chapter \ref{ch:medicalmt5}.
    \end{smallbox}
    \underline{Iker García-Ferrero}, Rodrigo Agerri, Aitziber Atutxa Salazar, Elena Cabrio, Iker de la Iglesia, Alberto Lavelli, Bernardo Magnini, Benjamin Molinet, Johana Ramirez-Romero, German Rigau, Jose Maria Villa-Gonzalez, Serena Villata, Andrea Zaninello. \\
    LREC-COLING 2024 \\
    \textit{\href{https://aclanthology.org/2024.lrec-main.974}{https://aclanthology.org/2024.lrec-main.974}}
    \end{part_of_the_thesis}


\clearpage

\subsection{Closely Related Contributions}
These contributions are not included in this manuscript, as they are not directly related to the cross-lingual transfer paradigm. However, they explore complementary topics aligned with the thesis’s main research direction, sharing the objective of advancing Information Extraction systems.

\begin{not_part_of_the_thesis}{Benchmarking meta-embeddings: What works and what does not}
    \underline{Iker García-Ferrero}, Rodrigo Agerri, and German Rigau. \\
    Findings of the Association for Computational Linguistics: EMNLP 2021 \\
    \textit{\href{https://doi.org/10.18653/v1/2021.findings-emnlp.333}{https://doi.org/10.18653/v1/2021.findings-emnlp.333}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{	
    IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases}
    \underline{Iker García-Ferrero}, Jon Ander Campos, Oscar Sainz, Ander Salaberria, and Dan Roth. \\
    Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023) \\
    \textit{\href{https://doi.org/10.18653/v1/2023.semeval-1.186}{https://doi.org/10.18653/v1/2023.semeval-1.186}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{	
    \scalerel*{\includegraphics{Capitulos/1_Introduccion/logos/GoLLIE.png}}{\textrm{\textbigcircle}} GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction}
    Oscar Sainz, \underline{Iker García-Ferrero}, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, Eneko Agirre \\
    The Twelfth International Conference on Learning Representations, 2024 \\
    \textit{\href{https://openreview.net/forum?id=Y3wpuxd7u9}{https://openreview.net/forum?id=Y3wpuxd7u9}}
\end{not_part_of_the_thesis}

\clearpage
 
\subsection{Contributions that are not part of the Thesis}
These contributions result from collaborations with other members of the IXA group and the research community. They focus on evaluating language models, particularly regarding Bias and Data Contamination. Although not directly aligned with the main goals of this thesis, their insights have direct implications for the work conducted in the thesis. 

\begin{not_part_of_the_thesis}{Itzulpen Automatikoko Sistemen Analisia: Genero Alborapenaren Kasua.}
    Ander Salaberria, Jon Ander Campos, \underline{Iker García-Ferrero}, Joseba Fernandez de Landa \\
    In Proceedings of the IV. Ikergazte (2021). Nazioarteko ikerketa euskaraz. Kongresuko artikulu bilduma. Ingeniaritza eta Arkitektura. \\
    \textit{\href{http://ixa.si.ehu.es/node/13328}{http://ixa.si.ehu.es/node/13328}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{Twitterreko Euskal Komunitatearen Eduki Azterketa Pandemia Garaian.}
    Joseba Fernandez de Landa, \underline{Iker García-Ferrero}, Ander Salaberria, Jon Ander Campos \\
    In Proceedings of the IV. Ikergazte (2021). Nazioarteko ikerketa euskaraz. Kongresuko artikulu bilduma. Ingeniaritza eta Arkitektura. \\
    \textit{\href{http://ixa.si.ehu.es/node/13327}{http://ixa.si.ehu.es/node/13327}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{	
    This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models}
    \underline{Iker García-Ferrero}, Begoña Altuna, Javier Álvez, Itziar Gonzalez-Dios, German Rigau. \\
    Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing \\
    \textit{\href{https://doi.org/10.18653/v1/2023.emnlp-main.531}{https://doi.org/10.18653/v1/2023.emnlp-main.531}}
\end{not_part_of_the_thesis}

\clearpage

\begin{not_part_of_the_thesis}{	
    NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark}
    Oscar Sainz, Jon Ander Campos, \underline{Iker García-Ferrero}, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre \\
    Findings of the Association for Computational Linguistics: EMNLP 2023 \\
    \textit{\href{https://doi.org/10.18653/v1/2023.findings-emnlp.722}{https://doi.org/10.18653/v1/2023.findings-emnlp.722}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{	
    Uncovering Social Changes of the Basque Speaking Twitter Community During COVID-19 Pandemic}
    Joseba Fernandez de Landa, \underline{Iker García-Ferrero}, Ander Salaberria, Jon Ander Campos\\
    Proceedings of the 3rd Annual Meeting of the Special Interest Group on Under-resourced Languages @ LREC-COLING 2024 \\
    \textit{\href{https://aclanthology.org/2024.sigul-1.44}{https://aclanthology.org/2024.sigul-1.44}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{	
    \scalerel*{\includegraphics{Capitulos/1_Introduccion/logos/noticia.png}}{\textrm{\textbigcircle}}NoticIA: A Clickbait Article Summarization Dataset in Spanish}
    \underline{Iker García-Ferrero}, Begoña Altuna \\
    Journal Procesamiento del Lenguaje Natural, 2024 \\
    \textit{\href{http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6610}{http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6610}}
\end{not_part_of_the_thesis}

\begin{not_part_of_the_thesis}{	
    Data Contamination Report from the 2024 CONDA Shared Task}
    Oscar Sainz, \underline{Iker García-Ferrero}, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang \\
    Proceedings of The 1st Workshop on Data Contamination (CONDA) @ ACL 2024  \\
    \textit{\href{https://aclanthology.org/2024.conda-1.4/}{https://aclanthology.org/2024.conda-1.4/}}
\end{not_part_of_the_thesis}

\clearpage

\section{List of open-source resources}

As mentioned above, this thesis emphasizes reproducibility and the development of tools and resources freely available for the research community. We have developed open-source software, datasets, and models that other researchers can use to replicate our experiments and build upon our work. The following is a list of open-source resources developed during the years of the thesis:

\subsection{Open source software}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.05\textwidth]{Capitulos/1_Introduccion/logos/metavec.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    MetaVec
    \end{adjustbox}
    }
    A monolingual and cross-lingual meta-embedding generation and evaluation framework. \\
    \textit{\href{https://github.com/ikergarcia1996/MetaVec}{https://github.com/ikergarcia1996/MetaVec}}
\end{resources}


\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.04\textwidth]{Capitulos/1_Introduccion/logos/CENER.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    Context-enriched multilingual named entity recognition \\ using knowledge bases.
    \end{minipage}
    \end{adjustbox}
    }
    A NER frameworks that (1) identifies possible entity candidates by analyzing the input sentence structure, (2) links the candidate to an existing updated knowledge base if possible, and (3) performs the fine-grained classification using the input sentence plus the retrieved information from the KB about the entity. \\
    \textit{\href{https://github.com/ikergarcia1996/Context-enriched-NER}{https://github.com/ikergarcia1996/Context-enriched-NER}}
\end{resources}

\begin{resources}
    {
    \includegraphics[height=0.8cm]{Capitulos/1_Introduccion/logos/easylabel.png}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:model-vs-data}
    \end{resourcessmall}
    Easy Label Projection is a library that allows to project labels from one dataset into another easily. You can automatically generate datasets for languages for which you do not have any labelled data using mGiza, FastAlign, SimALign or AWESOME. \\
    \textit{\href{https://github.com/ikergarcia1996/Easy-Label-Projection}{https://github.com/ikergarcia1996/Easy-Label-Projection}}
\end{resources}

\begin{resources}
    {
    \includegraphics[height=0.6cm]{Capitulos/1_Introduccion/logos/easytrans.png}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:model-vs-data}
    \end{resourcessmall}
    Easy-Translate is a script for translating large text files with a SINGLE COMMAND. Easy-Translate is designed to be as easy as possible for beginners and as seamless customizable and as possible for advanced users. \\
    \textit{\href{https://github.com/ikergarcia1996/Easy-Translate}{https://github.com/ikergarcia1996/Easy-Translate}}
\end{resources}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.06\textwidth]{Capitulos/1_Introduccion/logos/tprojection.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    T-Projection
    \end{minipage}
    \end{adjustbox}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:data-transfer}
    \end{resourcessmall}
    T-Projection is a method to perform high-quality Annotation Projection of Sequence Labeling datasets. The code is built on top of HuggingFace's Transformers and HuggingFace's Accelerate library. \\
    \textit{\href{https://github.com/ikergarcia1996/T-Projection}{https://github.com/ikergarcia1996/T-Projection}}
\end{resources}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.08\textwidth]{Capitulos/1_Introduccion/logos/tinad.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    TINAD Framework
    \end{minipage}
    \end{adjustbox}
    }
    A LLM finetuning and LLM evaluation library for the "This Is NOT a dataset" (TINAD) dataset.  \\
    \textit{\href{https://github.com/hitz-zentroa/This-is-not-a-Dataset}{https://github.com/hitz-zentroa/This-is-not-a-Dataset}}
\end{resources}


\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.04\textwidth]{Capitulos/1_Introduccion/logos/GoLLIE.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    GoLLIE Framework
    \end{minipage}
    \end{adjustbox}
    }
    The framework to finetune and evaluate GoLLIE-style models. Allows to implement any IE task by defining a set of categories and guidelines. Co-developed with Oscar Sainz. \\
    \textit{\href{https://github.com/hitz-zentroa/GoLLIE}{https://github.com/hitz-zentroa/GoLLIE}}
\end{resources}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.04\textwidth]{Capitulos/1_Introduccion/logos/noticia.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    NoticIA Framework
    \end{minipage}
    \end{adjustbox}
    }
    A LLM finetuning and LLM evaluation library for the NoticIA dataset.  \\
    \textit{\href{https://github.com/ikergarcia1996/NoticIA}{https://github.com/ikergarcia1996/NoticIA}}
\end{resources}

\subsection{Open source datasets}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.08\textwidth]{Capitulos/1_Introduccion/logos/tinad.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    This is NOT a Dataset
    \end{minipage}
    \end{adjustbox}
    }
    A large semi-automatically generated dataset of ~400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms that we use to evaluate LLMs.  \\
    \textit{\href{https://huggingface.co/datasets/HiTZ/This-is-not-a-dataset}{https://huggingface.co/datasets/HiTZ/This-is-not-a-dataset}}
\end{resources}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.08\textwidth]{Capitulos/1_Introduccion/logos/antidote.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    Multilingual Medical Corpus
    \end{minipage}
    \end{adjustbox}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:medicalmt5}
    \end{resourcessmall}
    A Multilingual-Medical-Corpus a 3 billion word multilingual corpus for training LLMs adapted to the medical domain. Multilingual-Medical-Corpus includes four languages, namely, English, Spanish, French, and Italian.  \\
    \textit{\href{https://huggingface.co/datasets/HiTZ/Multilingual-Medical-Corpus}{https://huggingface.co/datasets/HiTZ/Multilingual-Medical-Corpus}}
\end{resources}


\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.08\textwidth]{Capitulos/1_Introduccion/logos/antidote.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    Multilingual AbstRCT
    \end{minipage}
    \end{adjustbox}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:medicalmt5}
    \end{resourcessmall}
    We translate the AbstRCT English Argument Mining Dataset dataset to generate parallel French, Italian and Spanish versions using the NLLB200 3B parameter model and projected using word alignment tools. The projections have been manually corrected.  \\
    \textit{\href{https://huggingface.co/datasets/HiTZ/multilingual-abstrct}{https://huggingface.co/datasets/HiTZ/multilingual-abstrct}}
\end{resources}
\clearpage
\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.08\textwidth]{Capitulos/1_Introduccion/logos/antidote.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    Multilingual BioASQ-6B
    \end{minipage}
    \end{adjustbox}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:medicalmt5}
    \end{resourcessmall}
    We translate the BioASQ-6B English Question Answering dataset to generate parallel French, Italian and Spanish versions using the NLLB200 3B parameter model.  \\
    \textit{\href{https://huggingface.co/datasets/HiTZ/Multilingual-BioASQ-6B}{https://huggingface.co/datasets/HiTZ/Multilingual-BioASQ-6B}}
\end{resources}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.04\textwidth]{Capitulos/1_Introduccion/logos/noticia.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    NoticIA
    \end{minipage}
    \end{adjustbox}
    }
    A dataset consisting of 850 Spanish news articles featuring prominent clickbait headlines, each paired with high-quality, single-sentence generative summarizations written by humans.  \\
    \textit{\href{https://huggingface.co/datasets/Iker/NoticIA}{https://huggingface.co/datasets/Iker/NoticIA}}
\end{resources}


\subsection{Open source models}


\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.08\textwidth]{Capitulos/1_Introduccion/logos/antidote.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    medical mT5
    \end{minipage}
    \end{adjustbox}
    }
    \begin{resourcessmall}
    Developed in Chapter \ref{ch:medicalmt5}
    \end{resourcessmall}
    The first open-source text-to-text multilingual model for the medical domain. Medical mT5 is an encoder-decoder model developed by continuing the training of publicly available mT5 checkpoints on medical domain data for English, Spanish, French, and Italian.  \\
    \textit{\href{https://huggingface.co/HiTZ/Medical-mT5-xl}{https://huggingface.co/HiTZ/Medical-mT5-xl}}
\end{resources}
\clearpage
\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.04\textwidth]{Capitulos/1_Introduccion/logos/GoLLIE.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    GoLLIE
    \end{minipage}
    \end{adjustbox}
    }
    A Large Language Model trained to follow annotation guidelines. GoLLIE outperforms previous approaches on zero-shot Information Extraction and allows the user to perform inferences with annotation schemas defined on the fly. Unlike previous approaches, GoLLIE can follow detailed definitions and not only rely on the knowledge already encoded in the LLM. \\
    \textit{\href{https://huggingface.co/HiTZ/GoLLIE-34B}{https://huggingface.co/HiTZ/GoLLIE-34B}}
\end{resources}

\begin{resources}
    {
    \begin{adjustbox}{valign=c}
    \includegraphics[width=0.05\textwidth]{Capitulos/1_Introduccion/logos/clickbait.png}
    \end{adjustbox}
    \begin{adjustbox}{valign=c}
    \begin{minipage}{\textwidth}
    ClickbaitFighter
    \end{minipage}
    \end{adjustbox}
    }
    A model finetuned with the NoticIA Dataset. This model can generate summaries of clickbait headlines. \\
    \textit{\href{https://huggingface.co/Iker/ClickbaitFighter-10B}{https://huggingface.co/Iker/ClickbaitFighter-10B}}
\end{resources}