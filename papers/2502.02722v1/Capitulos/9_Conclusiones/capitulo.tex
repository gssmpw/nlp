\selectlanguage{english}
\chapter[Conclusion and future work]{Conclusion and future work}
\label{ch:final-chapter}

In this thesis, we have developed novel cross-lingual transfer learning methods aimed at addressing the resource constraints of low-resource languages. By leveraging both data-based and model-based approaches, we have demonstrated the potential to significantly improve performance on sequence labeling tasks across diverse languages and domains. Our proposed methods, including T-Projection and constrained decoding algorithms, achieve state-of-the-art results, highlighting the effectiveness of modern Machine Translation and multilingual models in facilitating knowledge transfer. The real-world application to the medical domain further underscores the practical impact of our research. By contributing open-source tools, datasets, and models, this work not only bridges the gap between high-resource and low-resource languages but also sets the stage for future advancements in multilingual NLP. The main contributions of this thesis are summarized as follows:

\begin{itemize}
    \item \textbf{We improved data-based cross-lingual transfer} approaches by developing a novel annotation projection method, namely \textbf{T-Projection} (\cite{garcia-ferrero-etal-2023-projection}) in Chapter \ref{ch:data-transfer}. It leverages state-of-the-art text-to-text multilingual models and Machine Translation systems to project annotations from high-resource to low-resource languages. T-Projection significantly outperforms previous annotation projection methods by a wide margin. This method allows us to \textbf{automatically generate high-quality labeled data for low-resource languages}.
    
    \item \textbf{We enhanced the model-based cross-lingual transfer approach} by, in Chapter \ref{fig:model_transfer} proposing a \textbf{constrained decoding algorithm} that enables model-based \textbf{cross-lingual transfer for sequence labeling tasks with text-to-text models}. This algorithm allows for the use of more powerful models that demonstrate superior zero-shot model-based transfer capabilities. Given the prevailing focus on text-to-text large language models (LLMs) in current research, and the infrequent training of new encoder-only models, this represents significant progress in the field.
    
    \item We expanded \textbf{NLP research in the medical domain for more languages} by developing a multilingual text-to-text open-source model for the medical domain, called \textbf{Medical mT5} (\cite{garcia-ferrero-etal-2024-medmt5}), presented in Chapter \ref{ch:medicalmt5}. By applying the model-based and data-based cross-lingual transfer learning methods developed in this thesis, we have implemented a \textbf{multilingual pre-training, fine-tuning, and evaluation framework for the medical domain}. Medical mT5 demonstrates the importance of the technology and knowledge developed in this thesis, resulting in the \textbf{first multilingual text-to-text medical model} at the time it was created.
    
    \item We conducted a \textbf{comprehensive evaluation of different cross-lingual transfer learning methods across a wide range of tasks, languages, and domains} (\cite{garcia-ferrero-etal-2022-model}), contributing to a better understanding of the situations in which each method is most effective. We demonstrated that \textbf{both our proposed data-based and model-based methods are effective in different scenarios}, and that they can be combined to achieve even better results. We also showed that the proposed methods are robust across different languages and domains and can be easily adapted to new tasks and languages.
    
    \item We released a large collection of \textbf{open-source software, datasets, and models} to facilitate the development of multilingual NLP research. By making our work \textbf{easily accessible} to other researchers, we enable them to \textbf{replicate} our experiments and \textbf{build upon our work}. We expect that the insights and methods developed in this thesis will be applicable to a wide range of NLP tasks, languages, and domains, thus contributing to the advancement of NLP in low-resource languages.
     
\end{itemize}



In terms of \textbf{publications}, this thesis contains 3 papers published in international conferences: 2 at EMNLP and 1 at LREC-COLING. Additionally, we published 3 closely related papers that were not included in this manuscript, 1 at ICLR, 1 at EMLP and 1 at an ACL Workshop. Finally, we submited other peer-reviewed papers, including 1 EMNLP, 2 at Ikergazte, 1 at ACL Workshops, and 1 at SEPLN. Between these papers, the paper \textit{Twitterreko Euskal Komunitatearen Eduki Azterketa Pandemia Garaian} was awarded the Most Relevant Research for the Development of the Basque Country \textbf{award at IkerGazte} 2021. Additionally, the \textit{NoticIA: A Clickbait Article Summarization Dataset in Spanish} project was selected as the \textbf{winner of the \#Somos600M 2024 Hackathon} by the SomosNLP community. This thesis has created interest in the community, as evidenced by the more than  \textbf{250 citations} of the papers published during this PhD, more than  \textbf{56,000 downloads} of the open-source models and datasets released on the Hugging Face Hub and more than  \textbf{550 stars} on the GitHub repositories. It also demonstrated by the  \textbf{talks} given at the OntarioTech University (graduate NLP course), the SEPLN symposium, and the Universitat de Barcelona (AI4HF consortium meeting).  
Finally, outside of the scope of the thesis, we have also contributed to the fair and unbiased evaluation of models by organizing The 1st Workshop on Data Contamination (CONDA) at ACL 2024 (\href{https://conda-workshop.github.io/}{https://conda-workshop.github.io/}, \cite{sainz-etal-2024-data}).

\section{Future work}

With the introduction of Large Language Models (LLMs), which have been pre-trained on massive amounts of text, instructions and further improved with reinforcement learning such as GPT4 (\cite{openai2024gpt4technicalreport}) or LLama-3 (\cite{llama3modelcard}), the field of NLP is evolving rapidly. The focus has shifted from training task-specific models to training multi-task models that can handle a wide range of tasks given a task description or prompt. Multi-task models have proven to outperform tasks specific models (\cite{DBLP:journals/corr/abs-2108-03265}) and can even perform well on tasks that they have not been trained on (\cite{brown2020language}). 

While these models are revolutionizing the field, they require a large amount of data, in terms of unstructured text and, more importantly, they require high-quality instruction-tuning data to achieve the capability of performing tasks given a prompt. This data includes a huge variety of tasks formulated as text-to-text instructions, such as dialogues, examples or summarization, code writing, translation, text generation, etc. Building large-scale instruction-tuning datasets is a very expensive and time-consuming process, therefore for now it has been limited to a very few companies with the monetary resources to do so. 

In this context, the next step in the field of cross-lingual transfer learning is to develop instruction-tuning datasets for low-resource languages. This requires the development of both, data-transfer and model-transfer methods, as well as overcoming other challenges such as adapting LLMs to the culture of the community in which they are to be used. Some of the future work that we plan to explore are:

\begin{itemize}
    \item Exploring the use of Machine Translation to generate instruction-tuning data for low-resource languages based on the already existing instruction-tuning datasets in high-resource languages. However, current sentence-level Machine Translation systems such as M2M100 (\cite{aharoni-etal-2019-massively}) or NLLB200 (\cite{DBLP:journals/corr/abs-2207-04672}) are not able to handle long contexts, which is a requirement for instruction-tuning data, which can be very long. In addition, translating complex structures that mix code, mathematical formulas, and other elements with natural language remains a challenge for current Machine Translation systems. While LLMs have shown great proficiency at document-level translation (\cite{xu2023paradigm,tower_llm_2024}) they are still only proficient at translating between high-resource languages (\cite{DBLP:journals/corr/abs-2311-07978,DBLP:conf/africanlp/OjoO23}). Developing a new generation of long-context Machine Translation systems, able to handle complex structures, for a wide range of languages can enable the generation of instruction-tuning data for low-resource languages.
    \item Synthetic data generation using LLMs (\cite{OpenHermes}) has shown promising results (\cite{zou2023representationengineeringtopdownapproach,DBLP:journals/corr/abs-2404-07503}). This process involves using an already pre-trained LLM and a set of prompts to generate instruction-tuning data for a wide range of tasks. This synthetic data can then be used to instruction-tune a new model that is superior to the original LLM. However, current methods are still limited to English. Model-based cross-lingual transfer can be used to generate synthetic data for low-resource languages. This means that a model pre-trained with unstructured text from many languages and instruction-tuned in only a few high-resource languages may be able to generate synthetic data for all the languages it has been pre-trained on. Similar to the model-based cross-lingual transfer experiments in this thesis, the model can be fine-tuned on a few examples of the target language and then used to generate synthetic data. This synthetic data can be used to train a model for the target language. A better understanding of the model-based cross-lingual transfer methods with LLMs and synthetic data generation methods can enable the development of instruction-tuning datasets for low-resource languages. 
    \item Building LLMs that can receive text in a language and produce plausible output text in that language is only the first step in developing LLMs for low-resource languages. For example, in the case of the Basque language, the latest generation of LLMs can process text in Basque. However, the models fail to answer questions pertinent to Basque culture, while they correctly answer questions about global culture (\cite{etxaniz2024bertaqalanguagemodelsknow}). For an LLM to be useful for a community, it must not only be able to process text in that language and produce plausible output text in the same language, but it must also encode knowledge about the community's culture. As shown in the experiments in Chapter \ref{ch:model-vs-data}, model-based and data-based cross-lingual transfer methods are not enough to overcome this issue. In fact, training the model with English-translated data can exacerbate the problem. Therefore, future research in cross-lingual transfer should not only focus on developing LLMs that can process text in a wide range of languages but also on finding methods to efficiently teach the models about the culture of the community in which the LLM is to be used.

\end{itemize}