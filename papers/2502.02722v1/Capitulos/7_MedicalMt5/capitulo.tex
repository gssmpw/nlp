\selectlanguage{english}
\chapter[Medical MT5: Cross-Lingual Transfer for Domain-Spacific Task]{Medical MT5: Cross-Lingual Transfer for Domain-Spacific Task}
\label{ch:medicalmt5}

In this chapter we will introduce Medical mT5, an open-source multilingual text-to-text large language model for the medical domain. We will leverage all the data-transfer and model-transfer techniques developed in the previous chapters. We will build a multilingual pre-training, fine-tuning, and evaluation framework for the medical domain. Medical mT5 demonstrates the importance of the technology and knowledge developed in this thesis, resulting in the first multilingual text-to-text medical model when it was created.

\section{Motivation and Contributions}


As it is the case for many application domains, there is an increasing interest in applying Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques to assist medical experts in their everyday activities. With this aim in mind, a number of language models have been trained or adapted to the medical domain. These include encoder-only models such as  SciBERT (\cite{beltagy2019scibert}), BioBERT (\cite{DBLP:journals/bioinformatics/LeeYKKKSK20}) or PubmedBERT (\cite{DBLP:journals/health/GuTCLULNGP22}). These models have obtained state-of-the-art results in discriminative tasks, with the advent of text-to-text and text-generation models, a new generation of language models has been developed. These models are typically much larger and have a much broader scope than the encoder-only models. Examples of these models include SciFive (\cite{DBLP:journals/corr/abs-2106-03598}), BioGPT (\cite{10.1093/bib/bbac409})
Med-PaLM (\cite{singhal-palm}), PMC-LLaMA (\cite{wu2023pmcllama}) or ClinicalGPT (\cite{Wang2023ClinicalGPTLL}).

However, the development of all the aforementioned text-to-text LLMs has been focused on a single language, usually English. As a consequence, there is a lack of high-quality multilingual data for pre-training models, a lack of models themselves, and a lack of high-quality multilingual evaluation benchmarks for the medical domain. Although there have been efforts to generate evaluation data in languages other than English (\cite{Wang2023ClinicalGPTLL,carrino-etal-2022-pretrained}), they have consisted largely of monolingual approaches.

To address these issues, we have compiled, to the best of our knowledge, the largest multilingual corpus for training LLMs adapted to the medical domain. Our corpus includes 3B words in four languages, namely, English, Spanish, French, and Italian. While relatively small when compared to existing English datasets (\cite{wu2023pmcllama}), it allowed us to build Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we have built a multilingual evaluation framework for the medical domain that can be used to evaluate the performance of any future multilingual model in the medical domain. 

Medical mT5 has been built on the work presented in previous chapters. We use the data transfer approach to reduce the cost of annotating new multilingual evaluation benchmarks for the medical domain. We also use constrained decoding to achieve high-quality zero-shot model-based cross-lingual transfer. Medical mT5 demonstrates the benefits of the techniques developed during this thesis and their application in real-world scenarios where data is scarce.

Medical mT5 outperforms similarly-sized text-to-text models for the Spanish, French, and Italian benchmarks while being competitive in English to current state-of-the-art text-to-text (\cite{mt5,chung-flan-instruction-models}) and encoder-only models (\cite{DBLP:journals/bioinformatics/LeeYKKKSK20,DBLP:conf/iclr/HeLGC21/deberta}). The results show that continuing pre-training of a multilingual text-to-text model such as mT5 allows to successfully adapt it to the medical domain, even when the amount of domain-specific data is relatively modest (ranging between 1B words for English and Spanish to 150M in Italian). Summarizing, the contributions of this chapter are:



\begin{itemize}
    \item The collection of the largest publicly available in-domain medical multilingual corpus for Spanish, French, and Italian languages. Together with the already existing English data, we release a corpus of 3 billion tokens\footnote{\url{https://hf.co/datasets/HiTZ/Multilingual-Medical-Corpus}}.
    \item We use the data-transfer approaches developed in previous chapters to build two new datasets for Spanish, French, and Italian on Argument Mining\footnote{\url{https://hf.co/datasets/HiTZ/multilingual-abstrct}} and generative Question-Answering\footnote{\url{https://hf.co/datasets/HiTZ/Multilingual-BioASQ-6B}} tasks, generated by taking their original English versions as a starting point.
    \item the public release of two Medical mT5 versions: a 770M\footnote{\url{https://hf.co/HiTZ/Medical-mT5-large}} and 3B\footnote{\url{https://hf.co/HiTZ/Medical-mT5-xl}} parameter text-to-text open-source models which obtain state-of-the-art results in multilingual sequence labeling for the medical domain, most notably in multi-task and zero-shot cross-lingual settings.
\end{itemize}


Other benefits of our Medical mT5 models include the comparatively low hardware requirements needed for both fine-tuning on downstream tasks (the large 770M version easily fits in a 24GB GPU) and for inference (a 12GB GPU should be enough). As an example, a LLaMA 7B model (\cite{wu2023pmcllama}) requires at least 4 80GB A100 GPUs. This makes our models more accessible to the research community and to small and medium-sized companies.

\section{Related Work}



As it has been the case in most application domains, Large Language Models (LLMs) have facilitated significant improvements in the state-of-the-art for medical NLP tasks (\cite{singhal-palm,wu2023pmcllama,mayer2021enhancing}). The most popular approaches use models pre-trained on medical corpora such as SciBERT (\cite{beltagy2019scibert}), BioBERT (\cite{DBLP:journals/bioinformatics/LeeYKKKSK20}), PubmedBERT (\cite{DBLP:journals/health/GuTCLULNGP22}), BSC-BIO (\cite{carrino-etal-2022-pretrained}), or BioLinkBERT (\cite{DBLP:conf/acl/YasunagaLL22}).

While the previous encoder-only models focused on discriminative tasks, the emergence of generative models such as LLaMa (\cite{touvron2023llama}), PaLM (\cite{singhal-palm}), and GPT-3 (\cite{brown2020language}) has generated significant interest in adapting such LLMs to the medical domain. These models include, but are not limited to, SciFive (\cite{DBLP:journals/corr/abs-2106-03598}), an English T5 encoder-decoder model adapted to the scientific domain, and decoder models such as BioGPT (\cite{10.1093/bib/bbac409}), Med-PaLM (\cite{singhal-palm}), PMC-LLaMA (\cite{wu2023pmcllama}), and ClinicalGPT (\cite{Wang2023ClinicalGPTLL}). 

Additionally, a range of abstractive question-answering tasks has been proposed as evaluation benchmarks, on which the larger models (\cite{wu2023pmcllama,singhal-palm,Wang2023ClinicalGPTLL}) achieve the best results. While interesting, both these LLMs and benchmarks have been developed with a focus on a single language, usually English. 

Furthermore, these LLMs require hardware that is simply not affordable for the majority of end-users and researchers. To address these issues, we propose Medical mT5, a multilingual text-to-text model adapted to the medical domain which, despite its relatively modest size and low running costs, obtains competitive results, notably in multi-task and zero-shot cross-lingual settings.


\section{Compiling a Multilingual Corpus for the Medical Domain}\label{sec7:corpus}

\input{Capitulos/7_MedicalMt5/tables/corpus.tex}

Obtaining good quality medical corpora is usually difficult due to the sensitive nature of the data. This is even more challenging for non-English languages, as the availability of data for other languages is in general more restricted. Despite these issues, we have successfully gathered and curated a diverse collection of public relevant corpora of medical texts in English, French, Italian and Spanish to generate the Medical mT5 model. The data sources are summarized in Table \ref{tab7:merged-data}.



\subsection{English}

As listed in table \ref{tab7:merged-data}, we collected around 1B words from three sources related to the medical domain: (i) \textbf{ClinicalTrials} is a set of documents of clinical studies from all over the world; (ii) \textbf{EMEA} is an English-Spanish parallel corpus with documents provided by the European Medicines Agency (\cite{TIEDEMANN12.463}) and, (iii) \textbf{PubMed}, which contains data from various sources such as MEDLINE, life science journals and online books, provides the bulk of the English data.


\subsection{Spanish}

Apart from \textbf{EMEA} and \textbf{PubMed}, which we also used for Spanish, the biggest portion of the data came from the \textbf{Medical Crawler}, a biomedical corpus compiled by \citet{carrino-etal-2022-pretrained}. Additionally, we also included \textbf{SPACC}, \textbf{UFAL} and \textbf{WikiMed}, a corpus built ad-hoc from Wikipedia entries. Table \ref{tab7:merged-data} provides the details of the collected data, which amounts to $\approx$1B words.

\subsection{French}

A total of 7,192,779 sentences and 670,972,717 words were compiled using the data sources listed in Table~\ref{tab7:merged-data}. \textbf{Science Direct} offers a collection of scientific and medical publications. We filtered relevant articles with the keyword ``MÃ©decine'', and the obtained XML documents were parsed to extract the \texttt{<dc:description>} tag.
As for Spanish, we took advantage of \textbf{Wikipedia} and \textbf{PubMed} as a source of medical knowledge. PubMed data was extracted using the \texttt{Bio.Entrez} package\footnote{\url{https://biopython.org/docs/1.75/api/Bio.Entrez.html}}. For wikipedia we obtain HTML formatted data from the category ``Category:MÃ©decine''. The \textbf{EDP French/English Parallel Medical Corpus}~(\cite{DBLP:conf/wmt/Jimeno-YepesNNV17}) provides bilingual content from journals that address domains such as dentistry and life sciences. From this source, we downloaded the dataset labeled ``EDP French corpus, text format''. Finally, \textbf{Google Patents} is a comprehensive repository of patent data from around the world. Google Patents data were retrieved by filtering using the IPC code and abstract language. A final French language verification step was undertaken by applying the \texttt{langdetect} package (version 1.0.9).

\subsection{Italian}

The crawling and pre-processing of the Italian split of the corpus followed the methodology described by \citet{carrino-etal-2022-pretrained}. First, we compiled a list of 504 medical terms, which we use as seeds to scrape the Italian split of the \textbf{MC4 Common Crawl Corpus} by only selecting the pages which contained at least one of the keywords in their URL domain. To create the list, we extracted 600 keyword terms related to medicine from the \textit{Dizionario analogico della Lingua Italiana} (Zanichelli). We excluded some sectors and discarded terms that may lead to ambiguous queries (e.g., actions, which contained mainly verbs, proverbs, general terms like ``assistente'', etc.). We normalized rare variants (``bacteriologia'' to ``batteriologia'') and stemmed all terms without lemmatizing, as most terms are already lemmatized in the dictionary; we performed univerbation of multiword units (e.g., ``esamedelleurine'', ``follow-up''), and removed the duplicates. This resulted in a corpus of 67 million tokens, which we joined with other sources of text such as \textbf{Medical dissertations}, \textbf{Drug use instructions}, \textbf{PubMed abstracts}, etc. as detailed in Table \ref{tab7:merged-data}, resulting in a $\approx$145M word corpus. 

\newcommand{\customsectiontitle}{
  \includegraphics[width=0.7cm]{Capitulos/7_MedicalMt5/logos/logo-antidote.png} Medical mT5
}
\section[Medical mT5]{\customsectiontitle}
\label{sec7:medicalmt5}

Multilingual T5 (mT5) (\cite{mt5}) is an extension of the original T5 (\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5}) framework, which is optimized for multilingual tasks. The T5 model is grounded in the Transformer encoder-decoder architecture (\cite{DBLP:conf/nips/VaswaniSPUJGKP17}). With its decoder block, T5 is capable of generating sequences of tokens in an auto-regressive fashion. T5 was designed to convert every NLP problem into a text-to-text task, and mT5 extends this strategy to a multitude of languages, leveraging a shared vocabulary for diverse scripts. mT5 was trained using mC4, a 1 Trillion token Common Crawl-based dataset covering 101 languages. The pre-training is based on a masked language modeling ``span-corruption'' objective, where consecutive spans of input tokens are replaced with a mask and the model is trained to reconstruct the masked-out tokens.

\subsection{Pre-training Medical mT5}

\input{Capitulos/7_MedicalMt5/tables/pretraining_hparams.tex}

Medical mT5 is built upon the same architecture as mT5 (\cite{mt5}). We release two diffent models: Medical-mT5-large (738M parameters) and Medical-mT5-xl (3 billion parameters). Both models were initialized using the pre-trained weights of their corresponding mT5 checkpoints and continued their pre-training using the 3B word medical domain dataset described in Section \ref{sec7:corpus} (with x2 oversampling for the Italian split). To prevent over-fitting, we run the training for only one epoch, as preliminary experiments showed that performance degraded with more epochs. We adhered to the self-supervised parameter settings described in \citet{mt5} and detailed in Table \ref{tab7:PreTraining}. It should be noted that Medical-mT5-large was trained with a sequence length of 1024 tokens whereas Medical-mT5-xl was limited to a sequence length of 480 tokens due to GPU memory limitations. Medical mT5 was trained using the Flax implementation of mT5 in the Hugging Face Transformers library (\cite{wolf-etal-2020-huggingface-transformers}). All experiments were conducted on our private servers, employing 4xA100 80GB GPUs. We made calculations for a carbon footprint estimation based on a 400W consumption per GPU and a carbon intensity of 0.171 kg/kWh\footnote{Sourced from \url{https://app.electricitymaps.com/map}}. 


\section[Generating New Multilingual Benchmarks]{Generating New Multilingual Benchmarks: Real-World Application of Data Transfer}\label{sec7:new-benchmarks}


There is a lack of multilingual evaluation benchmarks for the medical domain. The only available benchmark in English, Spanish, French, and Italian is the relatively small e3C (\cite{e3c}). While medical domain evaluation datasets are scarce for Spanish, French and Italian, many datasets exist for English. Therefore, this is a good opportunity to apply the data transfer techniques developed in previous chapters to generate data for other languages. We focused on two different types of tasks: (i) a sequence labeling task, \textbf{Argument Mining}, which involves detecting and classifying the argument component spans and their relations, and (ii) \textbf{Abstractive Question Answering}, where the model is expected to generate an answer in response to an input question. In both cases we used existing labeled English data as a starting point.

\subsection{Argument Mining}

The AbstRCT dataset is composed by English medical and scientific texts collected from the MEDLINE database and manually annotated with two types of argument components: Claims and Premises (\cite{mayer2021enhancing}). An example of the task is illustrated in Figure \ref{fig7:abstrct_example}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Capitulos/7_MedicalMt5/diagramas/arg.pdf}
    \caption{Example of an annotated abstract from the AbstRCT dataset.}
    \label{fig7:abstrct_example}
\end{figure}

A `claim'  is a concluding statement made by the author about the outcome of the study. In the medical domain, it may be an assertion of a diagnosis or a treatment. A `premise' corresponds to an observation or measurement in the study (ground truth), which supports or attacks another argument component, usually a claim. It is important that they are observed facts and, therefore, credible without further evidence.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{Capitulos/7_MedicalMt5/diagramas/dataconstruction.pdf}
  \caption{Data construction process for generating the Spanish, French and Italian versions of the AbstRCT dataset.}
  \label{fig7:dataconstruction}
\end{figure}


We generated French and Italian parallel versions of the dataset using the same method as for Spanish in \cite{yegingbergenova-cross}. First, the English dataset is translated into the target language using the machine translation model NLLB200-3.3B (\cite{DBLP:journals/corr/abs-2207-04672}). Then, the labels in the source language are transferred to the target language using AWESOME align (\cite{dou-neubig-2021-word}) and the annotation projection algorithm developed in Chapter \ref{ch:model-vs-data}. Finally, to ensure the quality of the generated dataset, the projections are manually reviewed by an expert in the target language. Thanks to this process, the manual annotation labor is significantly reduced compared to annotating the data from scratch. This process is illustrated in Figure \ref{fig7:dataconstruction}.

The AbstRCT dataset is divided into three splits, neoplasm, glaucoma and mixed. Following previous work, we fine-tune the models with the first one and then evaluate the in-domain performance on the neoplasm test split and the cross-domain performance on the glaucoma and mixed splits. Previous works using the AbstRCT datasets have employed different definitions of the $F_1$ score metric, such as token-level $F_1$ (\cite{mayer2021enhancing,yegingbergenova-cross}). However, in this paper, we report results using the standard sequence level $F_1$ score (\cite{DBLP:conf/conll/SangM03}), a much more strict metric, which explains the lower results for all the models.

\subsection{Question Answering}\label{sec:QA_explained}

We use the BioASQ-6B English Question Answering dataset (\cite{bioasq}) to generate parallel French, Italian and Spanish versions. Given a biomedical question and a set of snippets of text with relevant information about the question, the model must generate the \textit{ideal} answer. This task is similar to the Retrieval Augmented Generation (RAG) task (\cite{NEURIPS2020_6b493230}), where the model must generate an answer given a context or set of contexts. A set of ideal gold answers are provided to assess the performance of the models. We machine-translated the questions and ideal answers into French, Italian and Spanish using the NLLB200 3B parameter model (\cite{DBLP:journals/corr/abs-2207-04672}). In this case, as this is not a sequence labeling task, no annotation projection is needed. Nevertheless, the quality of a small set of translations was manually reviewed to ensure the quality of the generated data. 

\section{Experimental Setup}

In this section we describe the datasets and evaluation tasks used to measure the performance of Medical mT5. We also provide the details of the training and evaluation process, and baseline models used for comparison.

\subsection{Datasets}

\input{Capitulos/7_MedicalMt5/tables/tasks.tex}

The list of tasks used for evaluation is listed in Table \ref{tab7:tasks}. The \textbf{Sequence labeling tasks} include medical NER, detecting and classifying named entities according to some pre-defined categories, and Argument Mining, described in Section \ref{sec7:new-benchmarks}. Performance for every sequence labeling task is evaluated using standard sequence level $F_1$ score (\cite{DBLP:conf/conll/SangM03}). 
We also evaluate the performance of Medical mT5 on the \textbf{Generative Question Answering task} using the BioASQ dataset, described in Section \ref{sec:QA_explained}. 

\subsection{Conversion to Text-to-Text Format}

Medical mT5 is a text-to-text model. This means that, given a text input, it learns to generate a text as output. Therefore, every evaluation task must be converted into a text-to-text format (\cite{mt5}). In our experiments the output text is always generated using beam search with 4 beams. 


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{Capitulos/7_MedicalMt5/diagramas/MedT5-NER.pdf}
  \caption{Text-to-Text representation of the Sequence Labeling task. Given an input sentence, the model is expected to generate the same sentence annotated with html-style tags.}
  \label{fig7:SL}
\end{figure}


To address sequence labeling tasks, we use the same approach presented in Chaper \ref{ch:model-transfer}. As illustrated in Figure \ref{fig7:SL}, Text-to-text models such as Medical mT5 are prompted with the sentence to label. The expected output is the same sentence annotated with HTML-style tags. The HTML tags for each task are added as special tokens to the model vocabulary. We use constrained decoding to ensure that the output contains the same words as the input and a valid HTML annotation. The constrined decoding algorithm is the one presented in Chapter \ref{ch:model-transfer}.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{Capitulos/7_MedicalMt5/diagramas/MedT5-BioASQ.pdf}
  \caption{Text-to-Text representation of the BioASQ task. Given a question and a set of relevant snippets, the model generates an answer.}
  \label{fig:BioASQ}
\end{figure}

With respect to the BioASQ \textbf{Abstractive Question Answering task}, the input prompt contains the question and a context. As shown in Figure \ref{fig:BioASQ}, the context is generated by concatenating all the provided possible snippets. The expected output should be the generated answer to the question, which is then compared to the gold ideal answer. 

\subsection{Baselines}

As we have developed Medical mT5 by continuing the training of mT5 checkpoints, our primary point of comparison should be mT5 (\cite{mt5}). Thus, our first objective
is to assess whether training the model on our multilingual medical-domain
corpus enhances its performance for tasks specific to this domain. Furthermore, we also benchmark our model against SciFive (Pubmed+PMC) a T5-based 738M parameter model
(\cite{DBLP:journals/corr/abs-2106-03598}) trained exclusively on a corpus of 78B words containing scientific and medical English data. Additionally, we compare the performance of Medical mT5 with Flan-T5 (\cite{chung-flan-instruction-models}), which also adopts the T5 architecture but has been finetuned on a huge instruction-following dataset for almost 2K tasks. Flan-T5 achieves state-of-the-art performance in numerous benchmarks, including some from the medical domain (\cite{singhal-palm}). We tested all three types of text-to-text models under identical settings and hyperparameters.

We also measure Medical mT5 with the performance of encoder-only models in sequence labeling tasks. We report results with mDeBERTaV3 (\cite{DBLP:conf/iclr/HeLGC21/deberta}) which is widely used for sequence labeling and excels in multilingual tasks (\cite{adelani-etal-2022-masakhaner,Agerri2022LessonsLF}). Although we also tested XLM-RoBERTa (\cite{conneau-etal-2020-unsupervised}) and GLOT500 (\cite{DBLP:conf/acl/ImaniLKSSKMSMYS23}), their results were worse than those obtained by mDeBERTaV3. Finally, we also compare with BioBERT v1.1 (\cite{DBLP:journals/bioinformatics/LeeYKKKSK20}), which has been pretrained on a large English-only biomedical dataset. We do not evaluate the performance of encoder-only models in the question-answering task, as their architecture is not designed for text generation.

\subsection{Hyperparameters settings}

For sequence labeling, when using encoder-decoder models, we use a learning rate of $1 \times 10^{-4}$, a batch size of 8, and a maximum sequence length of 256 tokens. We use the Adafactor optimizer (\cite{DBLP:conf/icml/ShazeerS18}) with cosine learning rate decay to 0 and 500 warmup steps The number of epochs varies depending on the task. For the E3C dataset, which is very small, we use 100 epochs. For the other datasets, we use 45 epochs. When training the model in a multi-task setting, we use 12 epochs. For the question-answering task, we use 15 epochs. We use a beam size of 4 for all the tasks and no sampling. Models are trained using bfloat16 precision.

For encoder-only models, we use a batch size of 32, a learning rate of $5 \times 10^{-5}$, and 40 epochs. We use the AdamW optimizer (\cite{DBLP:journals/corr/abs-1711-05101}) with a cosine learning rate scheduler that decays the learning rate to 0. We use a maximum sequence length of 256 tokens. Encoder-only models are trained using fp16 precision.

For all models, we evaluate the model during training on the validation set periodically and select the model at the epoch with the highest performance on the validation set.

\section{Experimental Results}
In this section we present the evaluation results of Medical mT5 on Sequence Labeling and Question Answering tasks.
\subsection{Sequence labeling Tasks}
In this section we report on the performance of Medical mT5 and of the baselines in the
\textbf{sequence labeling tasks} across different settings.

\input{Capitulos/7_MedicalMt5/tables/SingleTask.tex}
\paragraph{Single Task Monolingual Supervised Results:} The results when fine-tuning and evaluating the models for each dataset and language are shown in Table \ref{tab7:SingleTask}.  The first observation is that Medical-mT5-large significantly outperforms both mT5-large and mT5-XL, demonstrating the benefits of further training these models with our multilingual medical domain corpus.

When comparing Medical mT5 with FlanT5 and SciFive, the latter models are systematically superior on English. This was anticipated since both have been pre-trained with a much larger amount of English-only data specific to the medical domain. With respect to encoder-only models, they achieve in general worse results than text-to-text models across all tasks and languages (except for the DIANN dataset). It is also noteworthy that FlanT5-XL exhibits robust performance across all datasets and languages, even though it was fine-tuned with English-only data not specific to the medical domain. Nonetheless, Medical-mT5-large obtains in general better results for French, Spanish and Italian while being much smaller in size (738M parameters vs 3B parameters), showing the impact of training Medical mT5 with domain-specific data for those languages.

\input{Capitulos/7_MedicalMt5/tables/multitask.tex}

\paragraph{Multi-Task Supervised Results:} Text-to-text models have
demonstrated improved performance when trained in multi-task settings
(\cite{chung-flan-instruction-models}). Following this, we also experimented with fine-tuning them across all the sequence labeling tasks simultaneously. To inform the model about which labels should be classified for each input example, we add the list of predefined labels from the corresponding dataset to the beginning of the input sentence. For instance, the input depicted in Figure \ref{fig7:SL} is adjusted to \textit{``<Disease> Patient with dilated cardiomyopathy''}. A comparison of the Single Task and Multi-Task settings is presented in Table \ref{tab7:MultiTask}. It can be seen that in this setting Medical mT5 achieves the best overall results for Spanish, French and Italian. On average, Medical-mT5-xl also obtains the best performance, slightly improving over the results of FlanT5-XL and Medical-mT5-large.

\input{Capitulos/7_MedicalMt5/tables/zeroshot.tex}

\paragraph{Zero-shot Cross-Lingual Transfer Results:} Manually annotated medical domain datasets for languages other than English are scarce. Therefore, developing models that can successfully generate predictions for languages different to those used for fine-tuning is crucial. We evaluate this ability to perform zero-shot cross-lingual transfer by fine-tuning Medical mT5 and the baselines on the English AbsRCT Neoplasm dataset, and then evaluating them on the Neoplasm, Glaucoma, and Mixed datasets for Spanish, French, and Italian. The results are presented in Table \ref{tab7:ZeroShot}. Results show that Medical mT5 outperforms any other model. Moreover, Medical-mT5-xl achieves significantly better results than Medical-mT5-large. 

To summarize, Medical mT5 stands out for its superior performance in the evaluation for Spanish, French, and Italian languages, especially for the multitask and the zero-shot transfer settings. These capabilities can help mitigate the scarcity of manually annotated medical data for other target languages. In contrast, SciFive and FlanT5, having been trained on extensive English-only datasets, emerge as the top choices when the primary focus is on English-only tasks.

Finally, despite Medical-mT5-xl being larger than Medical-mT5-large (3B vs 738M), its performance is worse in the single-task evaluation setting. This behaviour is not observed in the multi-task and zero-shot experiments, leading us to hypothesize that the larger Medical-mT5-xl model is more prone to overfit in the single-task supervised setting.



\subsection{Abstractive Question Answering}


In this section we explore the text generation capabilities of Medical mT5 and other baseline text-to-text models on the BioASQ question answering dataset described in Section
\ref{sec:QA_explained}. Previous work typically evaluates the performance 
on this task using the ROUGE score (\cite{bioasq}) to
compare the gold standard answer with the answer generated by the model.
However, we find this metric inadequate for medical domain tasks as it does not
address crucial aspects of the generation such as factuality, potential harm, and bias
(\cite{singhal-palm}). Consequently, we enlisted medical
professionals to analyze the answers produced by the models.

\input{Capitulos/7_MedicalMt5/tables/examplesBioASQ.tex}

During annotation, medical doctors were displayed the question, the ideal gold answers and
the answers generated by each model. If required, they could also inspect
the snippets that provide context to answer each of the questions. We narrowed the
evaluation to Medical-mT5-large, mT5-large, FlanT5-large and SciFive. The evaluation
was conducted by medical doctors proficient/native speakers of English, French and Spanish. For each question, doctors were asked to rank the
answers generated by the models as the best, second-best, third-best, and worst
answer.

Two Spanish medical doctors proficient or native in English and Spanish analyzed 50 English examples and 252 Spanish. For the French language, 3 French clinicians analyzed 186 answers, of which 47 were done by 2 doctors to calculate IAA (Cohen's Kappa Score: 0.28 and Average Spearman's Rank Correlation: 0.48), which indicates a low level of agreement. This exercise provided interesting insights with respect to the performance of the models in text generation tasks in the medical domain. First, medical doctors could not in general establish significant differences between the
answers generated by each of the models; predictions were far too similar, and all tended to
fail on the same questions. As an example, Table \ref{tab7:BioASQ_example} shows the answers to two different questions. As it can be observed, the answers generated by each model are very similar, and the doctors ended up ranking them primarily based on style. 

The final result of the manual analysis is that all the models were chosen a similar number of times as the best. 
We believe that this demonstrates the difficulty of performing and obtaining meaningful evaluation results for this kind of tasks on this specific domain. This is supported by the low IAA agreement obtained in the French annotation. This issue has also emerged in prior research and was partially addressed by employing a very large number of experts and asking them to respond with a yes/no to a set of predefined potential issues in the model output (\cite{singhal-palm}). Still, the variance on the answers provided by the experts was significant.

However, there could be other underlying reasons for this behaviour. First, perhaps the T5 architecture is not ideally suited for text generation as formulated in the BioASQ task, as these models are trained on a masking reconstruction objective rather than on direct text generation tasks. Consequently, the knowledge acquired during pre-training might not generalize well when the models are subsequently trained for text generation purposes. Second, perhaps using much larger models such as MedPaLM (\cite{singhal-palm}) may generate better answer generation, but models of 540B parameters are currently unusable for the large majority of the NLP research labs, including ours. Nonetheless, it should be stressed that research on appropriate evaluation metrics for these tasks is still a difficult challenge which requires further investigation. 

In any case, our results demonstrate the potential of a text-to-text model such as Medical mT5 for multilingual sequence labeling in the medical domain, establishing new state-of-the-art results in the multi-task and zero-shot cross-lingual settings.

\section{Conclusion}

In this chapter, we have presented Medical mT5, the first open-source multilingual text-to-text LLM for the medical domain. Its development has required the compilation of a new 3B word corpus in English, French, Italian and Spanish specific to the medical domain. Furthermore, motivated by the lack of multilingual benchmarks, we have generated evaluation benchmarks for French, Italian and Spanish for Argument Mining and Abstractive Question Answering. 

A comprehensive experimentation on sequence labeling tasks shows that Medical mT5 outperforms strong text-to-text baselines of similarly-sized models in the multi-task and zero-shot cross-lingual evaluation settings. This is particularly interesting as these settings fully exploit the multilingual nature of a text-to-text model such as Medical mT5.

Furthermore, our experiments on Abstractive Question Answering show the inherent difficulty of evaluating generative tasks for this specific domain, where complex issues such as truthfulness and veracity are difficult to capture by automatic metrics. Manual evaluation is not ideal either, as medical doctors were not able to clearly distinguish between the quality of the answers generated by the different models. In line with previous work (\cite{singhal-palm}), we hope our research will bring further attention to this problem and encourage further research on evaluation methods.

Medical mT5 has been built on the work presented in previous chapters. We use the data transfer approach to develop new multilingual evaluation benchmarks for the medical domain. We also use constrained decoding to achieve high-quality zero-shot model-based cross-lingual transfer. Medical mT5 demonstrates the benefits of the techniques developed during this thesis and their application in real-world scenarios where data is scarce.

Regarding the languages chosen for this chapter, acquiring medical domain data is extremely challenging, even for languages such as the ones included. Furthermore, the choice of languages was also influenced by the availability of native medical doctors to do the manual evaluation for Abstractive Question Answering. In any case, we hope that our research will encourage more researchers to join our effort and gather data for their respective languages, thereby creating larger, multilingual medical domain datasets encompassing more languages in the future. 