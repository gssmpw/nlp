\selectlanguage{english}
\chapter[Data transfer vs Model transfer]{Data transfer vs Model transfer}
\label{ch:model-vs-data}

In this chapter we will conduct an in-depth study of the two main techniques used so far for cross-lingual zero-shot Sequence Labeling, focusing on either data or model transfer. We will apply state-of-the-art Machine Translation models, word alignments, and language models to assess the performance of various cross-lingual sequence labeling approaches. We will identify the advantages and shortcomings of each method, as well as the challenges faced by current techniques for cross-lingual zero-shot sequence labeling. These insights will serve as a foundation for our work in subsequent chapters.

\section{Motivation and contributions}
\label{sc4:intro}


The performance of supervised machine-learning methods for Natural Language Processing, including advanced 
deep-neural models (\cite{lample-etal-2016-neural,akbik-etal-2018-contextual,devlin-etal-2019-bert,conneau-etal-2020-unsupervised}),
heavily depends on the availability of manually annotated training data. 
In addition, supervised models show a significant decrease in
performance when evaluated in out-of-domain settings
(\cite{DBLP:conf/aaai/Liu0YDJCMF21}). This means that obtaining optimal results
would require to manually generate annotated data for each application
domain and language, an unfeasible task in terms of monetary cost and human
effort. 

zero-shot cross-lingual transfer approaches aim to apply the resources from a high-resource source language to low-resource target languages. In this chapter, we perform an in-depth study of the two main techniques employed so far for cross-lingual zero-shot sequence labeling, based either on data or model transfer. We implement both approaches using the latest advancements in machine translation, word aligners, and multilingual language models. We focus on two sequence labeling tasks, namely, Named Entity Recognition (NER) and Opinion Target Extraction (OTE). To this end, we present a data-based cross-lingual transfer approach consisting of translating gold-labeled data between English and seven other languages using state-of-the-art Machine Translation systems. Sequence labeling annotations are then automatically projected for every language pair using word alignments. We then compare the performance obtained for each of the target languages against the performance of the zero-shot cross-lingual method, consisting of fine-tuning the multilingual language models on the English gold data and generating the predictions in the required target languages.

The main contributions of this chapter are the following: 
\begin{itemize}
\item We empirically establish the required conditions for each of these two approaches, data-transfer and zero-shot model-based cross-lingual transfer, to outperform the other. In this sense, our experiments show that contrary to what previous research suggested (\cite{fei-etal-2020-cross,Li2021CrossLingualNE}), the zero-shot transfer approach is the most effective method when using high-capacity multilingual language models like XLM-R large. However, data transfer approaches remain valuable for models with weaker downstream cross-lingual performance. As there is no universally available multilingual pretrained model for every language and domain, data-based methods retain their relevance.
\item Our evaluation proves that despite high-quality machine translations and annotation projections, in the data transfer approach, we have identified issues like many-to-one translations or misalignments, which seem to account for the lower performance of data transfer methods compared to the model-based approach.
\item We perform an extensive error analysis which shows that using English gold-labeled data often produces a signal which, due to inherent differences in language usage, differs from the signal received when using gold-labeled data in the target language. This cultural misalignment problem hinders the performance of the zero-shot cross-lingual methods. The issue is more pronounced with low-capacity models that have lower generalization capabilities, whereas higher-capacity models are more successful at labeling words unseen in the training data that share a similar meaning with those seen during training.

\end{itemize}




\section{Methodology}
In this section, we describe our implementation of data-based and model-based transfer methods for Cross-Lingual Sequence Labeling. Our experiments follow a one-to-one framework, using English as the source language. Each model is evaluated in a single target language. We assume a scenario in which we have English gold-labeled train and development data. Furthermore, we also assume that the only gold-labeled data available for the target language is the evaluation set. 

\subsection{Data transfer}

In the data-transfer paradigm, described earlier in Chapter \ref{sc:transfer-methods}, the information extraction model always performs inference in the language it was trained in. This language could be either the source language, typically English, or the target language. We propose to use Machine Translation and annotation projection. We have implemented two distinct data-transfer strategies: Translate-Train and Translate-Test.

\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Capitulos/4_ModelvsData/diagramas/TranslateTrain.pdf}
  \caption{Translate-Train approach: We automatically generate data for the target language by translating the gold-labelled English data. We use this data to train an NLP system in the target language. At inference, the model can be used to label inputs in the target language.}
  \label{fig:Translate_train}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Capitulos/4_ModelvsData/diagramas/TranslateTest.pdf}
  \caption{Translate-Test approach: We train a model with the gold labelled data in the source language. During the inference, the inputs are first translated into the source language. The outputs of the model are projected into the original target language sentence.}
  
  \label{fig:Translate_test}
\end{subfigure}
\caption{Illustration of the two data transfer approaches we have implemented. They are differentiated by the direction in which we translate the data. In both cases, English is the source language and Spanish is the target language.}
\label{fig:Translate_train_vs_translate_test}
\end{figure}

\subsubsection{Translate-Train}
In the Translate-Train approach, our goal is to generate data for the target language automatically, as depicted in Figure \ref{fig:Translate_train}. The process begins with translating English gold-labeled data into the target language using state-of-the-art Machine Translation models. Subsequently, we project the gold labels from the original English sentences onto the translated ones. This results in an automatically generated dataset in the target language. We then utilize this dataset to train an NLP model specifically for the target language. During inference, sentences in the target language are fed directly into the model for annotation. In this approach, the model is trained using ``silver'' data, the quality of which depends on the Machine Translation model's accuracy and the accuracy of the annotation projection algorithm. The data-transfer occurs during the model's training phase. Once trained, the model is capable of directly annotating sentences in the target language without requiring any additional steps

\subsubsection{Translate-Test}
The Translate-Test approach, depicted in Figure \ref{fig:Translate_test}, involves training an NLP model using English gold-labeled data. During the inference step, we first translate the input sentences in the target language into English. The model then generates annotations for these translated sentences. Subsequently, we project back these annotations onto the original sentences in the target language. Unlike the Translate-Train approach, where the model is trained with '``silver'' data, in the Translate-Test, the model is trained with gold data. However, during inference, the input must be first translated into English, and the model's output annotations must be projected back onto the original target language sentences. Therefore, as with Translate-Train, the quality of the predictions in Translate-Test also depends on the Machine Translation model's quality and the accuracy of the annotation projection algorithm.




\subsubsection{Annotation Projection Algorithm}


\begin{figure}[htb]
    \centering
    \includegraphics[width=8cm]{Capitulos/4_ModelvsData/diagramas/Projection.pdf}
    \caption{Illustration of the translation and annotation projection method for the Named Entity Recognition task.}
    \label{fig:annotation_projection}
\end{figure}


In both the Translate-Train and Translate-Test approaches, we project labels from the source language into the target language using automatic word alignments. Word alignments map words in a source language to their corresponding translations in a target language. Consider a source language sentence $x=\left\langle x_1,...,x_n  \right\rangle$ of length $n$, and its translation $y=\left\langle y_1,...,y_m \right\rangle$ in the target language with length $m$. We employ a word aligner to identify pairs $A=\left\{\left\langle x_{i}, y_{j}\right\rangle: x_{i} \in \mathbf{x}, y_{j} \in \mathbf{y}\right\}$, where each pair $\left\langle x_i,y_j\right\rangle$ indicates $y_j$ is the translation of $x_i$. For a sequence $s =\left\langle x_a,...,x_b  \right\rangle \in \mathbf{x}$ labeled with category $C$, we assign the same category $C$ to the sequence $t =\left\langle y_c,...,y_d  \right\rangle \in \mathbf{y}$, if every word $y_j$ in $t$ is aligned with at least one word $x_i$ in $s$: $\{ \forall y_j \in t \, \exists x_i \in  s : ( \left\langle x_i,y_j \right\rangle \in A )\}$. Essentially, if a word in the source sentence labeled with a category is aligned with a word in the target sentence, we label the target word with the same category. This method is illustrated in Figure \ref{fig:projection_errors}.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Capitulos/4_ModelvsData/diagramas/ProjectionErrors.pdf}
    \caption{Illustration of the split annotation and annotation collision errors when projecting a sentence.}
    \label{fig:projection_errors}
\end{figure}

When projecting annotations, we encounter two primary challenges: \textit{Split Annotations} and \textit{Annotation Collisions}. Split Annotations occur when a labeled sequence in the source sentence is divided into multiple sequences in the target sentence, often due to missed alignments by the word alignment algorithm. In such cases, we merge the target sequences if they are separated by only one word. If multiple sequences remain, we retain the longest sequence and discard the others. Annotation Collisions arise when a word in the target sentence aligns with two different labeled sequences in the source sentence, typically resulting from incorrect alignments. This causes a word, or sequence of words, to have overlapping label categories in the target sentence. If the conflicting sequences belong to the same category, we merge them into a single label in the target sentence. If they are of different categories, we choose the longer target sequence and discard the other one. Additionally, if a punctuation symbol in the target sentence aligns with a labeled word in the source sentence, we disregard this alignment. These scenarios are illustrated in Figure \ref{fig:annotation_projection}. For example, in the projection of ``Royal Swedish Academy of Science'' to ``Real Academia Sueca de Ciencias'', the word ``of'' is incorrectly aligned, leading to a projection gap. Since this gap involves only one word, we merge the sequences, labeling ``Real Academia Sueca de Ciencias'' as a single organizational entity. In the case of ``Marie'' and ``Pierre Curie''', incorrect alignment of ``Curie'' with both names results in overlapping person spans in the target sentence. As they are of the same category (Person), we merge them into a single Person label. Although this approach might not be perfect, it helps minimize errors caused by incorrect alignments.




\subsection{Model transfer}

Previously described in Chapter \ref{sc:transfer-methods}, the model transfer approach leverages multilingual language models \cite{devlin-etal-2019-bert, conneau-etal-2020-unsupervised}. These models are pre-trained on extensive unlabeled text corpora in multiple languages. As depicted in Figure \ref{fig:model_transfer}, we fine-tune a multilingual language model with the gold-labeled data available in English. Once the training is complete, the model is capable of directly labeling text in any of the languages included in its pre-training phase. We will refer to this approach to as \textit{zero-shot} cross-lingual sequence labeling.

\begin{figure}[htb]
    \centering
    \includegraphics[width=8cm]{Capitulos/4_ModelvsData/diagramas/ModelTransfer.pdf}
    \caption{Illustration of model transfer approach. A multilingual model is trained in the source language (English). The model is then used to label sentences in the target language (Spanish).}
    \label{fig:model_transfer}
\end{figure}


\section{Experimental Setup}
\label{sc4:experimental-setup}


In this section, we will describe the experimental framework for this Chapter. 

\subsection{Datasets}
\label{sec:chap4_datasets}

We conducted experiments in two sequence labeling tasks, namely, Opinion Target Extraction and Named Entity Recognition. The tasks are illustrated in Figure \ref{fig:chapter4_tasks}. 
We list the number of examples in each dataset in table \ref{tab:Chapter4_dataset_size}. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{Capitulos/4_ModelvsData/diagramas/Tasks.pdf}
    \caption{Illustration of the sequence labeling tasks used in the experiments in this chapter.}
    \label{fig:chapter4_tasks}
    \vspace{-0.5cm}
\end{figure}

\paragraph{Opinion Target Extraction (OTE):} Given a review, the task is to detect the linguistic expression used to refer to the reviewed entity. For instance, in the sentence \textit{Serves really good sushi}, the word \textit{sushi} is the opinion target because it is the entity being discussed. We use the English SemEval 2016 Aspect Based Sentiment Analysys (ABSA) dataset (\cite{pontiki-etal-2016-semeval}). This dataset comprises user reviews from the restaurant domain. We experiment with the English, Spanish, Dutch, French, Russian and Turkish datasets from the restaurant domain. 

\paragraph{Named Entity Recognition (NER):} Given a text, the task is to detect named entities and classify them according to some pre-defined categories. For Spanish and Dutch we use the CoNLL-2002 datasets (\cite{DBLP:conf/conll/Sang02}). For English and German we use the CoNLL-2003 datasets (\cite{DBLP:conf/conll/SangM03}) and for Italian, we use the Evalita 2009 data (\cite{speranza2009named}). We map the Geo-Political Entities from Evalita 2009 to {\it location} labels to make them compatible with the CoNLL data. This dataset contains labeled sentences from news articles.

\input{Capitulos/4_ModelvsData/tables/DatasetSize}

\subsection{Machine Translation}

We tested different open-source and commercial Machine Translation systems. Including DeepL \footnote{\url{https://www.deepl.com/es/translator}}, OpusMT (\cite{tiedemann-thottingal-2020-opus}), M2M100 (1.2B, \cite{JMLR:v22:20-1307}) and mBART (mbart-large-50, \cite{liu-etal-2020-multilingual-denoising,DBLP:journals/corr/abs-2008-00401}). A qualitative analysis performed during the projection of the OTE labels established that DeepL produced more fluent translations. Thus we decided to use DeepL (web version during the second half of 2021) to perform the Machine Translation for our data-based cross-lingual transfer experiments. Turkish was not supported by DeepL at the time, so we used M2M100 1.2B instead. Experiments that compare the different Machine Translation models are discussed in Section \ref{chap4:sec_transaltionbenchmark}.


\subsection{Word Alignments}

To compute word alignments, we use the AWESOME system (\cite{dou-neubig-2021-word}). AWESOME leverages multilingual pre-trained Language Models and fine-tunes them on parallel corpora. Unsupervised training objectives over the parallel corpus improve the alignment quality of the models. The authors claim that AWESOME works better with multilingual-BERT \cite{devlin-etal-2019-bert} as the backbone, so we follow their advice. We also experiment with GIZA++ (\cite{och-ney-2003-systematic-giza}) and FastAlign (\cite{dyer-etal-2013-simple-fastalign}), which are models based on statistical machine translation. Additionally, we explore SimAlign (\cite{jalili-sabet-etal-2020-simalign}), which, similar to AWESOME, leverages multilingual pre-trained models, although in a fully unsupervised manner. All the systems are extensively described in Chapter \ref{ch:2_word_alignments}. As demonstrated in Section \ref{chap4:sec_alignmentbenchmark}, AWESOME produced the highest F1 scores when comparing the model projections to manually annotated projections.

\subsection{Sequence labeling Models}

We use three state-of-the-art multilingual pre-trained language models for sequence labeling: multilingual BERT (mBERT) (\cite{devlin-etal-2019-bert})
and XLM-RoBERTa (XLM-R) base and large (\cite{conneau-etal-2020-unsupervised}). 

The primary distinctions among these models lie in their parameter count and the size of their pretraining corpora. mBERT has 110 million parameters and was trained on Wikipedias\footnote{\url{https://www.wikipedia.org/}} of 104 languages. In contrast, XLM-RoBERTa-base contains 250 million parameters, and XLM-RoBERTa-large has 560 million parameters. Both versions of XLM-RoBERTa were trained using a corpus of 340 billion tokens extracted from 100 languages sourced from CommonCrawl\footnote{\url{https://commoncrawl.org/}}. This corpus, significantly larger as noted in \citet{conneau-etal-2020-unsupervised} encompasses a far broader range of data compared to that used for mBERT, covering various diverse domains. We categorize models with a lower parameter count and those pre-trained on smaller datasets, such as mBERT, as low-capacity language models. Conversely, we classify larger models trained on more extensive corpora, like XLM-RoBERTa-large, as high-capacity language models (\cite{aharoni-etal-2019-massively}). 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth, keepaspectratio]{Capitulos/4_ModelvsData/diagramas/TokenClassification.pdf}
    \caption{Implementation of the sequence labeling model. An encoder-based Language Model is fed the input sequence. The output representations are used by a token classification linear layer to predict the labels.}
    \label{fig:chapter4_tokenclassification}
\end{figure}

As depicted in Figure \ref{fig:chapter4_tokenclassification}, for both language models, we add a token classification layer (a linear layer) on top of each token's representation. This layer computes the probability distribution of the labels for each token. If a word is split into multiple sub-tokens, we use the representation of the first sub-token as input for the classifier. During training, we fine-tune the complete parameters of the model along with the token classification layer. We utilize the sequence labeling implementation from the Hugging Face open-source library (\cite{wolf-etal-2020-huggingface-transformers}). As listed in Section \ref{sec:chap4_datasets}, due to the significant difference in the total number of training examples between the OTE and NER datasets, we employ slightly different hyperparameter settings for each task. For OTE we use a batch size of $32$, $5e-5$ learning rate, and we train the model for $10$ epochs and $128$ maximum sequence length. Since only a train and test splits are available for the OTE task, we use the train set as both, train and development data. For NER we use a batch size of $32$, $2e-5$ learning rate, and we train the model for $4$ epochs and 256 maximum sequence length. For both tasks, we use the ``BILOU'' encoding scheme (\cite{ratinov-roth-2009-design}). 

We report the F1 score, which is the standard metric in sequence labeling tasks. F1 scores and standard deviation scores are reported by averaging the results of five runs with different random seeds. 

\section{Experimental Results}
\label{sec:chap4_experiments}
In this section we compare the \textsc{Translate-Train} and \textsc{Translate-Test} data-transfer approaches with the \textsc{zero-shot} model-transfer approach. As an upper bound, we also train the language models on gold-labeled data in the target languages. This upper bound, which we refer to as \textsc{gold}, is intended to assess the performance of the cross-lingual transfer approaches with respect to an ideal setting.

\subsection{Opinion Target Extraction}
Opinion Target Extraction (OTE) results are reported in Table \ref{tab:chap4_OTE_f1score}. When using mBERT, a low-capacity model, the zero-shot model transfer approach obtains better results for Spanish and French. However, for Dutch, Russian, and Turkish, the data-transfer approaches are superior. However, the overall picture changes as we increase the model capacity. When using XLM-RoBERTa (XLM-R) base, the zero-shot baseline is much closer to the gold upper bound than that of mBERT. This demonstrates that XLM-R has better multilingual transfer learning capabilities for these tasks. In fact, the zero-shot transfer outperforms the Translate-Train and Translate-Test for every language except Turkish. The XLM-R base results on gold-labeled data are also substantially better than those of mBERT, which once again demonstrates the better proficiency of XLM-R in these tasks. To summarise, XLM-R large offers the best cross-lingual transfer performance, as the zero-shot transfer is clearly superior for every language, including Turkish.

A common trend for all three models in the OTE benchmarks is that the Translate-Train approach consistently performs better than the Translate-Test approach. As expected, all the approaches achieve a performance significantly lower than the gold upper bound.


\input{Capitulos/4_ModelvsData/tables/OTE}


\subsection{Named Entity Recognition}

The NER results presented in Table \ref{tab:chap4_zerovsSota} show a number of different patterns. First, the zero-shot approach using mBERT outperforms the data-based cross-lingual transfer methods for the majority of languages. Second, the Translate-Test approach is consistently superior to the Translate-Train approach. Third, the mBERT performance on gold data is similar to that of XLM-R base. Finally, the data transfer approaches achieve the best performance when using XLM-R base for German and Italian.

The only, result that remains consistent from the OTE tasks is that the zero-shot approach using XLM-R large achieves the best results for all languages.

\input{Capitulos/4_ModelvsData/tables/NER}

We also compare our data-transfer and model-transfer implementations with previous research that leverages parallel data and/or annotation projection methods on the NER CoNLL 2002-2003 data. The results are listed in Table \ref{tab:chap4_zerovsSota}. XLM-R large in a zero-shot setting not only outperforms our data-transfer implementation, but it is also superior to previous data-transfer approaches. The only exception is the result obtained by \cite{Li2021CrossLingualNE} for German. 

Our data transfer approaches, although not achieving the best results for every language, are competitive with previously proposed methods. It is important to note that while we only leverage translations of the NER data, previous research uses other resources such as the automatic annotation of large parallel corpora.

\input{Capitulos/4_ModelvsData/tables/NER_SOTA}

\subsection{Discussion}


Previous research has demonstrated (\cite{pires-etal-2019-multilingual, wu-dredze-2020-languages}) that mBERT's performance in cross-lingual transfer learning greatly differs depending on whether the source and target languages are topologically similar or not, with the former being the scenario in which the model performs best. Secondly, the monolingual performance of mBERT, as well as its cross-lingual transfer performance, is much better for high-resource languages than for low-resource languages. This is consistent with our results for mBERT in the NER and OTE tasks. The zero-shot transfer works best for French and Spanish, which are somewhat topologically similar to English. However, the performance of the zero-shot approach is worse than the Translate-Train and Translate-Test approaches for Russian and Turkish. 


%\begin{wrapfigure}{r}{0.5\textwidth}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\linewidth]{Capitulos/4_ModelvsData/tables/datasetsize_partial.pdf}
    \caption{Amount of data in GiB (log-scale) for the languages we use in our experiments in Wiki-100 corpus used for training mBERT and the CC-100 used for XLM-R. The full figure can be found in \cite{conneau-etal-2020-unsupervised}}
    \label{fig:chapter4_pretraining_size}
%\end{wrapfigure}
\end{figure}


While mBERT was trained using Wikipedia data for 104 languages (\cite{devlin-etal-2019-bert}), XLM-R (both base and large) has been trained using CommonCrawl (\cite{conneau-etal-2020-unsupervised}), a much larger multilingual corpus with a variety of texts extracted from the web. As illustrated in Figure \ref{fig:chapter4_pretraining_size}, XLM-R was trained with orders of magnitude more data for Russian and Turkish compared to mBERT, thus acquiring higher proficiency in those languages. This results in the zero-shot approach using XLM-R achieving better performance in Russian and Turkish than the data-transfer approaches. The CommonCrawl dataset also contains a more diverse variety of texts sampled from the Web, perhaps including texts of similar domains to those in the OTE datasets. This may account for the large differences in OTE performance between XLM-R and mBERT. In this sense, the similar performance between mBERT and XLM-R base for NER may be partially attributed to the fact that the CoNLL and Evalita datasets consist of news stories in which most of the labeled entities may appear in the Wikipedia texts used to pre-train mBERT.

Our results suggest that the performance of model-transfer and data-transfer approaches relies on the model's proficiency in the target language and data domain.  If a high-capacity model, such as XLM-R large, with strong proficiency in the target language and domain is available, the zero-shot cross-lingual transfer method achieves the best results. However, for languages and domains where the model lacks sufficient proficiency, data-based cross-lingual transfer (Translate-Train and Translate-Test) approaches remain mainly useful. While XLM-R large in a zero-shot cross-lingual setting achieves the best results for every task and language in the experiments performed in this section, in the next chapters we will demonstrate that XLM-R large is not proficient in every language and domain, for example, in African languages (\cite{adelani-etal-2022-masakhaner}). Thus, developing both better model-transfer and data-transfer approaches is of great relevance.


\section{Error Analysis}
The experiments described in Section \ref{sec:chap4_experiments} showed that when using XLM-R large, the zero-shot approach outperforms the Translate-Train and Translate-Test approaches. The effectiveness of the Translate-Train and Translate-Test approaches depends on the quality of the Machine Translation and annotation projection models. In this section, we will assess the performance of both Machine Translation and annotation projection to better understand if the data-transfer methods are hindered by the performance of any of these steps. We will also conduct an error analysis to gain a deeper understanding of the shortcomings of Translate-Train and Translate-Test in comparison to the zero-shot cross-lingual transfer.

\subsection{Downstream evaluation of Machine Translation Models}
\label{chap4:sec_transaltionbenchmark}

To assess the impact of the Machine Translation system employed, we translated the English OTE gold-labeled data using four distinct translation systems. In all experiments, we utilized AWESOME as the word aligner for annotation projection. We fine-tuned XLM-R large with each set of generated training data and assessed its performance against the gold-labeled test set for each target language. Based on the results provided in Table \ref{tab:chap4_translators}, we can conclude that there are no significant differences in the final F1 scores when employing different translation systems. For each language, a different model demonstrates the best performance, although all systems exhibit similar average performance across all languages. The exception is Turkish,  a language in which MarianMT underperforms.

\input{Capitulos/4_ModelvsData/tables/TranslationBenchmark}

\subsection{Evaluating the Projection Method}
\label{chap4:sec_alignmentbenchmark}

The performance of the data transfer method relies on the quality of the annotation projections. If the annotation projection algorithm does not produce highly accurate projections, it will generate noisy data that would hinder both the Translate-Train and Translate-Test approaches. In this section, we will evaluate the performance of various word alignment systems and compute the performance of the Translate-Test approach using gold annotation projections. To achieve this, we enlisted human annotators who manually projected labels from the English OTE gold-labeled data onto automatic translations in Spanish, French, and Russian. The machine translations were generated using DeepL for Spanish, French, and Russian, and M2M100 for Turkish. The annotators are NLP PhD candidates with either native or proficient skills in both English and the target language. For this experiment, we developed an application to assist during the annotation process. The annotator views the sentence in English, with a highlighted target, and must select the corresponding target in a translated sentence. The complete guidelines and the application code provided to the annotators are available in GitHub \footnote{\url{https://github.com/ikergarcia1996/Annotation-Projection-App}}.

\input{Capitulos/4_ModelvsData/tables/AlignmentBenchmark}




First, we compare the projection of annotations automatically generated by different word alignment methods with those provided by human annotators. Table \ref{tab:chap4_alignmentsbenchmark} shows that language model-based methods (SimAlign and AWESOME) outperform statistically based methods (GIZA++ and FastAlign) by a wide margin. AWESOME consistently outperforms SimAlign for every language. It is worth mentioning that SimAlign uses a fully unsupervised algorithm, while AWESOME requires fine-tuning a Language Model; therefore, the cost in compute resources is significantly higher for AWESOME.

\input{Capitulos/4_ModelvsData/tables/ManualvsAuto}

The performance of the AWESOME system confirms that it is possible to generate high-quality annotations that closely resemble those generated by human experts. However, the model does make some mistakes. To understand the performance implications of the mistakes produced by AWESOME, we compare the Translate-Train approach when using data projected with AWESOME and when using data manually projected by human experts. To achieve this, we fine-tuned an XLM-R large model on both datasets. Table \ref{tab:chap4_ManualvsAutomaticProjection} shows that when training with manually projected data, the results are slightly better, except for Turkish, which again acts as an outlier. Nevertheless, the results when training with manually projected data are still inferior to the zero-shot model-transfer approach. From the results, we can conclude that the projection mistakes produced by AWESOME do not have a significant impact on the performance of the data-based cross-lingual transfer approach, although there is still room for improvement. More importantly, the data-transfer approach is not inferior to the model-based transfer approach due to the errors produced by the annotation projection step.




\subsection{Categorization of mistakes}
We have evaluated the quality of the Machine Translation models and the annotation projection algorithms. We found both to be of high quality and not responsible for the data-transfer method performing worse than the zero-shot transfer approach. In this section we will categorize the errors produced by each approach and compare them. Table \ref{tab:chap4_Mistakes} displays the most frequent false negatives and positives for which there is a significant discrepancy in frequency between methods.


There are a few challenging words that all the systems struggle with. For example, as previously reported by \cite{AGERRI201985}, the words ``comida'' (food) and ``restaurante'' (restaurant) are highly ambiguous in the ABSA task. Both appear labeled as target and unlabeled frequently. As expected, models struggle with these words. In addition, we have identified four main sources of errors:
\input{Capitulos/4_ModelvsData/tables/WordCount}

\input{Capitulos/4_ModelvsData/tables/ModelMistakes}
\paragraph{Many-to-one translation:} Multiple words can share the same sense in a given context and be used interchangeably. This is the case with opinion targets in the ABSA task, such as ``trato'', ``atención'', and ``servicio'' in Spanish. In the context of restaurant reviews, they all convey the same meaning as the English word ``service.'' There are 160 sentences in the English gold-labeled data containing the word ``service''; in 153 of them, ``service'' is labeled as a target. Machine Translation systems, such as DeepL in our experiments, systematically translate it as ``servicio.'' However, as shown in Table \ref{tab:chap4_Wordcount}, in the Spanish gold-labeled data, ``service'' is also commonly referred to as ``trato'' or ``atención'', not just ``servicio.'' Therefore, the training dataset translated and projected from English into Spanish, as demonstrated in previous sections, encompasses high-quality translations and annotation projections. Still, this dataset is not a good reflection of reviews written by native Spanish speakers. As shown in Table \ref{tab:chap4_Wordcount}, the translated dataset does not contain any occurrences of ``trato'' and ``atención'', which often appear in the gold-labeled Spanish test data. In fact, both the zero-shot and the data-based cross-lingual transfer approaches fail to correctly label these words as demonstrated in Table \ref{tab:chap4_Mistakes}. Interestingly, the zero-shot approach using XLM-R large correctly classifies ``trato'' (only failing to label 1 of the 19 occurrences). 

In the case of the Translate-Train approach, as we use a model of increased capacity, the number of false negatives decreases. This shows that the issue is more pronounced with low-capacity models that have lower generalization capabilities, while higher-capacity models are more successful at labeling words unseen in the training data that share a similar meaning with those in the training data. 

The Translate-Test approach does not overcome this issue, as when translating data from the target to the source language, the Machine Translation model does not systematically translate ``trato'' and ``atención'' as ``service.'' Instead, these words are usually translated as ``treatment'' and ``attention'', respectively. In this case, the Machine Translation system fails to understand the context in which the word is used, opting for literal translations. ``Treatment'' and ``attention'' are not commonly used in the restaurant review domain in the same context as ``service.'' In fact, there is no occurrence of the word ``treatment'' in the English gold-labeled data and there are only two occurrences of the word ``attention'', which contrasts with the 153 occurrences of ``service.'' Once again, the model struggles with labeling these words.



There are more examples of many-to-one translations, such as the word ``place'', which in Spanish can be most frequently translated as ``lugar'' or ``sitio.'' However, DeepL almost always translates it as ``lugar'', resulting in ``sitio'' being absent in the automatically generated training data, despite being more frequent than ``lugar'' in the gold-labeled data. In this particular case, the Translate-Test approach is not subject to the problem as both ``lugar'' and ``sitio'' are translated into ``place'' in the Spanish-to-English translation direction.

\paragraph{Cultural alignment:} There is a group of words related to Spanish Government names which are not commonly used in the same contexts in English, constituting a significant portion of the false negatives in the NER datasets listed in Table \ref{tab:chap4_Mistakes}. For example, the word ``Economía'' refers to the ``Ministry of Economy'' or ``Ministerio de Economía'' in Spanish. ``Junta'' denotes a ``local government'' administering a specific region in Spain. ``Plan'' is often used to denote specific ``projects founded by the goverment''. While these words frequently appear in the Spanish data as part of named entities, this is not the case in the English data, where it is more customary to use terms like ``Treasury Department'' (or variations thereof), correctly translated into Spanish by DeepL as ``Departamento del Tesoro''. Thus, during fine-tuning on the translated data, the model does not receive any signal to learn that ``Economy'' may be part of an organization entity. For other terms like ``Plan'' and ``Junta'', the English gold-labelled data lacks references, as these entities are specific to Spanish administration. While the data transfer approach misses many of these named entities, the zero-shot approach is more successful. Training the model with translated data or performing inference with data from other languages translated into English seems to complicate the transfer between different domains. In any case, there is still a significant margin for improvement. The gold standard, trained with gold-labelled data in Spanish, correctly labelled all these entities.


\paragraph{Errors induced by incorrect or missing alignments:} We found that for NER, articles and prepositions (i.e. ``de'', ``la'') are among the words with higher false positive rate for the Translate-Test and Translate-Train approaches. Examining the annotation projection reveals that word aligners struggle to correctly align articles in complex multi-word named entities specially when a one-to-many or many-to-one alignment is required. For example, the word aligners we tested failed to correctly align ``of'' with ``de la'' in the following example: ``Consejo General de la Arquitectura Técnica de España'' (General Council of Technical Architecture of Spain).

\paragraph{Errors induced by annotation inconsistencies:} Finally, another issue is the differences across the original gold-labelled annotations. ``Gobierno'' (Government) and ``Estado'' (State) are labelled as organizations in the Spanish gold-labelled data, but they are not considered to be entities in the English gold-labelled data. The opposite occurs with demonym words. They are labelled as miscellaneous entities in the English data but in Spanish they are not annotated. Cross-lingual models are likely to fail labeling these cases.

Summarizing, we observe that using English gold-labeled data often produces a signal which, due to inherent differences in language usage, differs from the signal received when using gold-labelled data in the target language. This accounts for the substantial performance disparity between all the cross-lingual sequence labeling approaches and the gold standard trained with gold-labelled data in the target language. The zero-shot cross-lingual transfer approach, when employing high-capacity language models, achieves the best transfer performance across languages. This appears to be due to this method being less impacted by issues related to many-to-one translations and cultural alignment. These issues together with miss-alignments seems to be the most common reason for the larger number of false positive and negatives of the data-based cross-lingual transfer method with respect to the zero-shot technique.


\section{Conclusions}

In this chapter we have performed an in-depth and comprehensive evaluation of model-based and data-based zero-shot cross-lingual sequence labeling on two different tasks. A detailed error analysis demonstrates that cross-lingual transfer is hindered by the differences in the cultural behaviour of the source and target language in use. The issue is more pronounced with low-capacity models that have lower generalization capabilities, whereas higher-capacity models are more successful at labeling words unseen in the training data that share a similar meaning with those in the training data. This suggests that developing models with enhanced generalization capabilities could bridge this cultural gap.

Our findings indicate that the zero-shot transfer approach is the most effective method when using high-capacity multilingual language models like XLM-R large. However, data-based cross-lingual transfer approaches remain valuable for models with weaker downstream cross-lingual performance. As there is no universally available multilingual pretrained model for every language and domain, data-based methods retain their relevance. Despite the availability of high-quality machine translations and annotation projections, we have identified issues like many-to-one translations or misalignments, which seem to account for the lower performance of data-based cross-lingual transfer methods compared to the model-based approach.



Our results establish that there is still room for improving the cross-lingual performance of zero-shot sequence labeling. In the following chapters, we will focus on two areas. First, we aim to develop a more effective annotation projection method that enables data-based approaches to achieve comparable or superior performance to the model-based approach. Second, we will work on improving the generalization capabilities of sequence labeling models to address the cultural alignment and many-to-one translation issues identified in this study.

