\selectlanguage{english}


\chapter[Related Work]{Related Work}
\label{ch:related-work}

In this chapter, we will present the state-of-the-art in Natural Language Processing (NLP) and cross-lingual transfer. We will begin with a brief overview of recent advancements in NLP and multilingual language models. Next, we will discuss the different approaches for cross-lingual transfer learning. More specifically, we will present the two main paradigms for cross-lingual transfer: data-based transfer and model-based transfer. For each paradigm, we will describe the various methods proposed in previous research. These methods will serve as the baselines for the experiments presented in the following chapters.


\section{NLP and Deep Learning: Scaling compute and data}
\label{sc:deep-learning-sota}


In the last few years, the Natural Language Processing paradigm has switched from building pipelines of different processing modules (\cite{DBLP:conf/lrec/AgerriBR14}) into end-to-end neural networks trained with vast amounts of text data (\cite{DBLP:journals/corr/abs-2111-01243}).

\begin{figure}[ht]
    \centering
    \includegraphics[height=0.65\linewidth]{Capitulos/2_EstadoDelArte/diagramas/WordEmbeddings.pdf}
    \caption{Illustration of multilingual embeddings, where two languages are mapped into a shared vector space. Words with similar meanings are placed close together.}
    \label{fig:chap3_multilingual_embeddings}
\end{figure}


The first step in this direction was the introduction of word embeddings, such as Word2Vec (\cite{DBLP:journals/corr/abs-1301-3781}), GloVe (\cite{pennington-etal-2014-glove}), and FastText (\cite{DBLP:journals/tacl/BojanowskiGJM17}). These embeddings are trained on large corpora of text and capture the semantic and syntactic properties of words. Word embeddings are used as input features for neural networks that perform a wide range of NLP tasks. Multilinguality was added to this paradigm with the introduction of multilingual word embeddings (\cite{DBLP:journals/jair/RuderVS19}). As depicted in Figure \ref{fig:chap3_multilingual_embeddings}, multilingual word embeddings are trained on text from multiple languages and map words from different languages into a shared vector space (\cite{DBLP:conf/icml/GouwsBC15,DBLP:conf/naacl/LuongPM15}). Alternatively, multilingual word embeddings can be trained on monolingual data and then projected into a shared space using a bilingual dictionary (\cite{zhang-etal-2016-ten,artetxe-etal-2016-learning,DBLP:conf/iclr/SmithTHH17}). This shared space allows for the transfer of knowledge across languages, enabling the training of models in one language and applying them to another. However, the cross-lingual transfer capabilities of word embeddings-based systems are limited and often perform poorly on low-resource languages (\cite{conneau-etal-2018-xnli}).

The introduction of the Transformer architecture (\cite{DBLP:conf/nips/VaswaniSPUJGKP17}) marked a significant shift in the NLP field. Transformers have achieved state-of-the-art performance on a wide range of NLP tasks, such as Machine Translation (\cite{DBLP:conf/nips/VaswaniSPUJGKP17}), Text Classification (\cite{devlin-etal-2019-bert}), General Language Understanding (\cite{DBLP:conf/iclr/WangSMHLB19}), Question Answering (\cite{DBLP:conf/acl/RajpurkarJL18}), Text Generation (\cite{brown2020language}) or Dialogue (\cite{DBLP:journals/corr/abs-2201-08239}) among many others (\cite{DBLP:journals/corr/abs-2111-01243}).  The success of the Transformer architecture has led to the development of a broad range of Transformer-based language models.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{Capitulos/2_EstadoDelArte/diagramas/Bert.pdf}
    \caption{Representation of the BERT architecture. During training, BERT learns to predict missing words in a sentence based on the contextual representations produced by the model.}
    \label{fig:chap3_bert}
\end{figure}

The first prominent Transformer-based language model was BERT (\cite{devlin-etal-2019-bert}). BERT is trained on large corpora of text data and learns to predict missing words in a sentence, producing contextual embeddings that capture the meaning of words in context. This process is illustrated in Figure \ref{fig:chap3_bert}. Similar to word embeddings, BERT embeddings can be used as input features for a wide range of NLP tasks. BERT has been extended to support multiple languages with mBERT (\cite{devlin-etal-2019-bert}), which is trained on text from over 100 languages. mBERT achieved state-of-the-art performance in multiple languages and demonstrated strong performance when trained in English and applied to other languages (\cite{pires-etal-2019-multilingual,artetxe-schwenk-2019-massively}). The success of mBERT has led to the development of other multilingual models, such as XLM-RoBERTa (\cite{conneau-etal-2020-unsupervised}) and DeBERTa (\cite{DBLP:conf/iclr/HeLGC21/deberta}). These models have increasingly larger sizes and are trained on progressively larger corpora of text data.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{Capitulos/2_EstadoDelArte/diagramas/text_to_text.pdf}
    \caption{Representation of the text-to-text framework in T5. Every task is framed as a text input and the model is trained to generate the desired output as text. Figure reproduced from \cite{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5}.}
    \label{fig:chap3_textotext}
\end{figure}


The introduction of T5 (\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5}) and GPT (\cite{radford2019language,brown2020language}) shifted the focus in NLP from learning word representations to a text-to-text approach. T5 is designed to map input text to output text, enabling it to handle a wide array of NLP tasks. Consequently, all NLP tasks are framed as text-to-text tasks, where the input is a description of the task or a prompt, and the output is the desired result, as illustrated in Figure \ref{fig:chap3_textotext}. Unlike previous NLP models, which were fine-tuned for specific tasks, T5 can be trained on a broad spectrum of tasks with a single training objective (\cite{chung-flan-instruction-models}). 

On a similar research line, \cite{radford2019language} introduced the GPT line of models, demonstrating that Large Language Models (LLMs) trained on extensive internet data can perform, given a natural language task description, tasks such as Question Answering, Machine Translation, and Summarization without explicit supervision. This finding led to the emergence of instruction tuning, also known as multitask fine-tuning, as the leading method for achieving generalization in large models to solve diverse NLP tasks. In this approach, models are first trained on vast amounts of unlabeled data and subsequently fine-tuned on a diverse collection of tasks (\citep{DBLP:conf/emnlp/WangMAKMNADASPK22,chung-flan-instruction-models}) framed as text-to-text problems.

Subsequent research has demonstrated that increasing the parameter count of language models (\citep{brown2020language}), coupled with improvements in the size and quality of the instruction tuning dataset, results in enhanced generalization capabilities. Consequently, models have been increasingly scaled up (\cite{chung-flan-instruction-models}) in both the number of parameters and the amount of training data. This scaling has led to the development of state-of-the-art Large Language Models (LLMs) such as GPT-4 (\cite{openai2024gpt4technicalreport}), LLaMA (\cite{DBLP:journals/corr/abs-2407-21783}), and Mistral (\cite{jiang2023mistral7b}). These models, which have billions of parameters, are trained on hundreds of terabytes of text data. They are also trained on a large number of diverse tasks and instructions, enabling them to perform a wide range of NLP tasks.

In addition to achieving state-of-the-art performance on various NLP tasks, these models can also solve tasks for which they were not explicitly trained (\cite{radford2019language,lieber2021jurassic,DBLP:journals/corr/abs-2201-11990,rae2022scalinglanguagemodelsmethods,DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23}). Since these models are trained on a substantial portion of internet data, they are inevitably multilingual and can be applied to a wide range of languages and tasks.

While the latest generation of NLP models has made a huge step forward in terms of performance, they require a huge amount of data and computational resources to train. This has led to a growing gap (\cite{blasi-etal-2022-systematic}) between high-resource languages, such as English, and low-resource languages, for which there is very little data available (\cite{joshi-etal-2020-state}). A notable example is African languages for which both Open Source (\cite{DBLP:journals/corr/abs-2311-07978}) and Comercial (\cite{DBLP:conf/africanlp/OjoO23}) NLP models produce lower performance for African languages. This has led to the development of \emph{cross-lingual transfer} methods, which aim to leverage the knowledge learned from high-resource languages to improve or enable NLP tasks in low-resource languages. In the following sections, we will present the different approaches for cross-lingual transfer.

\section{Cross-Lingual Transfer Methods}
\label{sc:transfer-methods}

In this section, we will present the different approaches for cross-lingual transfer. More specifically, we will present the data-transfer and model-transfer approaches that will constitute the baselines in the following chapters. 

Cross-lingual transfer in Natural Language Processing (NLP) is a method in which knowledge learned from one language (typically a high-resource language with abundant data and resources) is applied to improve or enable NLP tasks in another language (often a low-resource language with limited or no data). This process can be achieved through various techniques, including translating datasets (\cite{Ehrmann}) (data-transfer) or using multilingual models (\cite{devlin-etal-2019-bert,conneau-etal-2020-unsupervised}) that understand multiple languages (model-transfer). The goal is to overcome the scarcity of annotated data in many languages, thus facilitating multilinguality in NLP applications. 

\subsection{Data-based transfer}



Data transfer leverages parallel data and/or Machine Translation to bridge the gap between languages in cross-lingual NLP tasks. Data transfer methods make the assumption of annotation preservation across translations. In the data transfer paradigm, the NLP model is trained and performs inference in the same language. There are two main approaches for data transfer: Translate-Train and Translate-Test. 

\subsubsection{Translate-Train} 


\begin{wrapfigure}{r}{0.55\linewidth}
\vspace{-0.481cm}
    \centering
    \includegraphics[width=\linewidth]{Capitulos/2_EstadoDelArte/diagramas/TranslateTrain.pdf}
    \caption{Illustration of the Translate-Train cross-lingual transfer approach: Given gold data in the source language, this method utilizes translation and annotation projection to create silver-standard training data in the target language.}
    \label{fig:chap3_translatetrain}
    \vspace{-0.5cm}
\end{wrapfigure} 

The Translate-Train approach, illustrated in Figure \ref{fig:chap3_translatetrain},  aims to automatically generate annotated data in languages where such data is scarce by leveraging annotated datasets from a high-resource source language. This method begins with a dataset that is fully annotated in a well-resourced source language, which is then translated into the target language (\citet{DBLP:conf/emnlp/JainPL19,fei-etal-2020-cross}). Following translation, the annotations from the source language are projected onto the translated text, resulting in a new, silver-standard annotated dataset in the target language. This newly created dataset can then be used to train NLP models directly on tasks in the target language, making this approach particularly valuable when original annotated data in the target language is not available. High-quality Machine Translation systems are essential for this approach to succeed. An alternative strategy involves automatically annotating the English version of a multi-parallel corpus and then projecting these annotations to all other languages in the corpus (\citet{Ehrmann}). In situations where neither Machine Translation systems nor parallel data are available, \citet{DBLP:conf/acl/GuoR21} translate labeled data on a word-by-word basis using a dictionary and then constructing target-language text from the source-language annotations using a constrained pre-trained language model trained with unlabeled data in the target language. In any case, this approach results in a silver-standard annotated dataset in the target language. The quality of this automatically generated dataset depends on the quality of the translation or parallel data and the effectiveness of the annotation projection algorithm.



 \subsubsection{Translate-Test}
 
\begin{figure}[ht]
    \centering
    \includegraphics[width=10cm]{Capitulos/2_EstadoDelArte/diagramas/TranslateTest.pdf}
    \caption{Illustration of the Translate-Test cross-lingual transfer approach: A model is trained using gold data in the source language. During inference, inputs in the target language are first translated into the source language, after which predictions are made and then projected back into the target language.}
    \label{fig:chap3_translatetest}
\end{figure}

Instead of building models for the target language, the translate-test approach aims to take advantage of the ability of the models to produce better results for high-resource languages such as English (\cite{etxaniz-etal-2024-multilingual}). The Translate-Test approach is illustrated in Figure \ref{fig:chap3_translatetest}. In this method (\cite{shah2010synergy,10.1007/978-3-540-45175-4_13,tebbifakhr-etal-2020-machine}), the model is trained in a source language with abundant resources (gold standard data). At inference, the inputs in the target language are first translated into the source language. The model, which is trained on the source language data, performs its inference on these translated inputs. Subsequently, the predictions made by the model are translated or projected back into the target language. This approach enables the deployment of NLP models across languages without retraining the model with annotated data in the target language. However, as with the Translate-Train approach, the performance of the Translate-Test approach relies heavily on the quality of the translation of the input data and the projection of the output predictions into the target language. 

\subsubsection{Annotation Projection}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{Capitulos/2_EstadoDelArte/diagramas/TaskExamples.pdf}
    \caption{Illustration of data transfer for different NLP tasks. Each task requires a different method to transfer the labels from the source into the target language.}
    \label{fig:chap3_taskexamples}
\end{figure}



In both the Translate-Train and Translate-Test approaches, it is necessary to project the labels from the source language into the target language or vice versa. As depicted in Table \ref{fig:chap3_taskexamples}, the annotation projection method required varies significantly across different NLP tasks. For instance, in Text Classification, annotation projection is straightforward, as the translated sentence will be labeled with the same category as the source sentence. For text generation tasks, such as Abstractive Question Answering, the label can be translated along with the input sentence. The most challenging tasks are those involving Sequence Labeling. For sequence labeling tasks, which involve span-level annotations, it is necessary to identify the sequence of words in the translated sentence that corresponds to the labeled spans in the source text. The majority of previous work published in this research area explores the application of word-alignments (\cite{Ehrmann}). 


\subsubsection{Annotation projection using word alignments}
\label{ch:2_word_alignments}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{Capitulos/2_EstadoDelArte/diagramas/WordAlignmets_raw.pdf}
    \caption{Illustration of word alignments represented as a bidirectional graph.}
    \label{fig:chap3_wordalignmentsraw}
\end{figure}

Word alignments refer to the process of matching words in a sentence in the source language to their corresponding translations in a target language. As illustrated in Figure \ref{fig:chap3_wordalignmentsraw}, word alignments are represented as a bidirectional graph between words in a parallel sentence. 



Most word-alignment algorithms are based on statistical Machine Translation systems. Giza++ (\cite{och-ney-2003-systematic-giza}) is based on the IBM Models. The IBM alignment models (\cite{brown-etal-1993-mathematics}) use statistical methods to learn the probability of translation between words in a source language and their counterparts in a target language, based on a given corpus of aligned texts. Building upon this research, FastAlign (\cite{dyer-etal-2013-simple}) introduces a log-linear reparameterization of IBM Model 2, which achieves an accuracy comparable to GIZA++ but with improved computational efficiency, thereby enabling faster inference throughput. On a similar line of research, Efmaral and Eflomal (\cite{Ostling2016efmaral}) extend the IBM models with a Bayesian model with Markov Chain Monte Carlo (MCMC) inference for improved accuracy and computational efficiency. 


As neural networks started to outperform previous statistical approaches for most NLP tasks (\cite{DBLP:journals/csur/MinRSVNSAHR24}), a new line of research emerged aiming to generate word alignments using neural networks. This line of research employs multilingual language models that have been pretrained using data from both the source and the target languages. In this line of research, \cite{DBLP:journals/pbml/PeterNN17} built an attention-based neural network in which the attention probabilities are trained to closely align with those obtained from statistical MT toolkits. Building on this work, \cite{DBLP:journals/corr/abs-1901-11359} proposed a method that adds an extra layer of attention on top of the Transformer architecture and directly optimizes its activations towards a given target word.
\cite{garg-etal-2019-jointly} trained a Transformer model to produce both accurate translations and alignments, jointly performing the Machine Translation and Word Alignment tasks. The model outputs the translation during inference while alignments are extracted from the attention probabilities. This approach achieves competitive results compared with GIZA++ without sacrificing translation accuracy. 
However, the model only achieves better performance than GIZA++ when existing word alignments are provided for fine-tuning. 
\cite{DBLP:conf/acl/ZenkelWD20} combines both previous approaches and extends them with a loss function that encourages contiguity in the alignment matrix and a symmetrization algorithm that jointly optimizes the alignment matrix within two models trained in opposite directions. This approach outperforms the alignments generated by GIZA++ and other statistical Word lignment systems. 



\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Capitulos/2_EstadoDelArte/diagramas/WordAlingmentXMLR.pdf}
    \caption{Illustration of word alignments by fine-tuning language models. The query is on the left ``West Germany'' and the translated sentence on the right. The model predicts that ``Alemania Occidental'' is the Spanish translation of ``West Germany''.}
    \label{fig:chap3_wordalignmentsXLMR}
\end{figure}

Instead of extracting alignments from the attention layer of a multilingual language model, \cite{Li2021CrossLingualNE} optimizes a Transformer encoder directly to generate word alignments in its output. As illustrated in Figure \ref{fig:chap3_wordalignmentsXLMR}, the task is formulated as a token classification problem. The alignment model is constructed by concatenating an English text span, representing a labeled sequence, with a sentence in the target language. The model is fine-tuned to predict which tokens in the target sentence correspond to the source text span. They automatically generate a silver fine-tuning dataset using Wikipedia texts in the target language. Anchor text in hyperlinks indicates the location of named entities. The anchored text is machine-translated into the source language. 

\begin{figure}
    \centering
    \includegraphics[width=8cm]{Capitulos/2_EstadoDelArte/diagramas/cosine_similarity_heatmap.pdf}
    \caption{Illustration of the cosine similarity between token embedding representations using Multilingual BERT.}
    \label{fig:chap3_simalign}
\end{figure}

Previous methods that employ neural networks to compute alignments require fine-tuning data, which is very scarce and nonexistent for most language pairs.  SimAlign (\cite{jalili-sabet-etal-2020-simalign}) leverages the contextual embeddings from state-of-the-art multilingual language models such as mBERT (\cite{devlin-etal-2019-bert}). As depicted in Figure \ref{fig:chap3_simalign}, SimAlign identifies alignments between words in parallel sentences based on the similarity of their contextual embeddings. This method allows for more accurate alignments that reflect the contextual use of words within specific sentences. Unlike statistically based word aligners that rely on statistical correlations in large corpora, SimAlign benefits from the deep linguistic and semantic understanding embedded in pre-trained language models. SimAlign offers improvements in alignment quality, especially for languages with complex morphological structures or less parallel data available for training. SimAlign is an unsupervised method, requiring no training data to compute word alignments. Still, SimAlign achieves better alignment accuracy than previous statistical and neural word alignment models. For those language pairs in which parallel data is available, AWESOME (\cite{dou-neubig-2021-word}) improves on this idea by fine-tuning multilingual pre-trained language models on unlabeled parallel text. The main idea is that unsupervised training objectives over the parallel corpus improve the alignment quality of the models.

\begin{figure}
    \centering
    \includegraphics[width=8cm]{Capitulos/2_EstadoDelArte/diagramas/WordAlignmets.pdf}
    \caption{Illustration of annotation projection using word-alignments}
    \label{fig:chap3_wordalignments}
\end{figure}

The word alignments generated by the previously described methods are used for annotation projection. Words in the target sentence are labeled with the same category as the aligned words in the source sentence. This process is depicted in Figure \ref{fig:chap3_wordalignments}. 
Traditionally, most research in this area has focused on automatically annotating the English version of a multi-parallel corpus and then projecting these annotations to all other languages using statistical word alignments, as shown in the works of  \citet{yarowsky-etal-2001-inducing,hwa2005bootstrapping,Ehrmann} and \cite{fu-etal-2011-generating}. \citet{wang-manning-2014-cross} introduces a refinement to this approach by projecting model expectations instead of direct labels, enabling the transfer of model uncertainty across languages and potentially reducing the risk of error propagation. Nevertheless, inaccuracies in word alignment computation remain a significant issue, often resulting in incorrect annotation projections and the generation of noisy data.

To address this problem, \citet{ni-etal-2017-weakly} propose a heuristic scheme for selecting high-quality projection-labeled data from the noisy dataset. This scheme also includes projecting word embeddings from the target language into the source language, allowing the application of the source-language sequence labeling system to the target language without the need for re-training.  \citet{agerri-etal-2018-building} automatically annotate parallel data for multiple source languages and project the labeled data to a single target language. This method demonstrates that leveraging multiple sources can significantly enhance the quality of the projections. 

Instead of relying on automatics labels for the source part of a parallel corpus, \citet{tiedemann-etal-2014-treebank,fei-etal-2020-cross} use Machine Translation to automatically translate the sentences of a gold-labeled dataset to the target languages. The translated data is subsequently annotated by projecting the gold labels from the source dataset onto it. \citet{tiedemann-etal-2014-treebank} make use of GIZA++ for word alignments, whereas \citet{fei-etal-2020-cross} utilize the word alignment probabilities calculated with FastAlign, alongside the part-of-speech (POS) tag distributions of the source and target words, to enhance the precision of annotation projection.

In contrast to the methods previous mentioned,  \cite{shah2010synergy} implement a Translate-Test strategy in which input sentences are first translated into the source language using Google Translate. These sentences are then annotated by the model, and the annotations are projected onto the target sentences using alignments computed by GIZA++.


\subsubsection{Other annotation projection methods}

With the recent advancements in supervised machine translation, a new line of research has emerged which aims to replace word alignments in favour of directly using Machine Translation models.  

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/2_EstadoDelArte/diagramas/TranslateMatch.pdf}
    \caption{Illustration of annotation projection using Machine Translation. Individually labeled sequences are translated separately from the rest of the sentence. The translations of these sequences are then matched with the translations produced by translating the entire sentence.}
    \label{fig:chap3_translate_match}
\end{figure}

\citet{DBLP:conf/emnlp/JainPL19} introduce a "translate-match" methodology, which is illustrated in Figure~\ref{fig:chap3_translate_match}. In this approach the complete sentence, including labeled spans or entities, is first translated into the target language. Simultaneously, the labeled spans are translated independently of the full sentence. These individually translated spans are then matched with the corresponding spans in the translated sentence. However, this matching process does not guarantee that labeled spans will retain consistent translations when isolated from the sentence context, so the authors generate multiple translation candidates for each span and select the best match based on orthographic and phonetic similarities.

\begin{figure}[hbp]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/2_EstadoDelArte/diagramas/TranslateReplace.pdf}
    \caption{Illustration of annotation projection using Machine Translation and placeholders. Labeled sequences are replaced by a placeholder. The sentence with placeholders and the labeled sequences are translated independently. After translation, the placeholders are replaced with the corresponding labeled sequence translation.}
    \label{fig:chap3_translate_replace}
\end{figure}

To improve the matching process \cite{zhou-etal-2022-conner} propose to replace the labeled sequences in the source sentence with a placeholder. The sentence with placeholders and the labeled sequences are translated independently. Finally, the placeholders in the translated sentence are replaced with the corresponding labeled sequence translation. This process is illustrated in Figure \ref{fig:chap3_translate_replace}. They found that the translation model preserves the placeholders. Although this technique effectively addresses the matching challenge, it may introduce translation artifacts. These artifacts arise because the translation model does not process the entire context of the source sentence, potentially diminishing the overall quality of the translation.


\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/2_EstadoDelArte/diagramas/TranslateMarkers.pdf}
    \caption{Illustration of the mark-then-translate approach. Markers are introduced around the labeled sequences. The sentence and the labeled spans are translated together.}
    \label{fig:chap3_translate_markers}
\end{figure}

Instead of translating the labeled sequences independently from the source sentence, \cite{daza-frank-2019-translate} and latter \cite{chen-etal-2023-frustratingly} introduce markers in the source sentence as depicted in Figure \ref{fig:chap3_translate_markers}. This mark-then-translate approach allows the model to jointly translate the source sentence and the labels. However, their studies reveal that the inclusion of markers can degrade the quality of the translation. Additionally, they encounter low projection rates, meaning that markers are frequently omitted in the translated output. To address this, they fine-tune the translation model using a synthetic dataset with named entity recognition annotations. Post-fine-tuning, the model not only preserves translation quality but it also surpasses the accuracy of word-alignment models in annotation projection tasks.



In a subsequent work \cite{DBLP:journals/corr/abs-2402-03131} enhances this method by implementing a constrained decoding algorithm, which ensures that the introduction of markers does not compromise the quality of the translation. In this improved approach, the training data in the high-resource language is first translated without markers. A second decoding phase then integrates the markers, with the constraint that the translation must align with the initial, marker-free output. This two-step process guarantees that the final translated sentence with markers remains consistent with what the model would have produced without them, thus preserving the translation quality.



Similar to the previous method, \cite{parekh2024contextual} also follows a two-step approach: first, translating the sentence without markers, and then performing the label translation. However, instead of using a Machine Translation system, they utilize an instruction-tuned large language model (Llama-2 \cite{DBLP:journals/corr/abs-2307-09288}) to perform the task in a few-shot setting with a few randomly selected examples. Although their method proves to be effective, most low-resource languages lack a high-quality instruction-tuned model, which limits the applicability of this approach.


\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Capitulos/2_EstadoDelArte/diagramas/Xie2018.png}
    \caption{Illustration of bilingual dictionary generation. Monolingual embeddings are projected into a shared space in which a bilingual dictionary is computed by k-nearest-neighbor. Figure reproduced from \cite{xie-etal-2018-neural}.}
    \label{fig:chap3_xie2018}
\end{figure}


All the methods previously described presuppose the availability of a high-quality Machine Translation model and a sizeable parallel corpus containing both the source and target languages. However, this is not the case for all language pairs. For instance, such resources are nonexistent for translations between English and some very low-resource African languages. Taking this into account, an alternative research direction aims to facilitate data transfer between a high-resource language and a very low-resource language using only minimal resources, specifically a bilingual dictionary and unlabeled text in the target language.
\cite{xie-etal-2018-neural} first learns monolingual word embeddings (\cite{DBLP:journals/corr/abs-1301-3781,pennington-etal-2014-glove,DBLP:journals/tacl/BojanowskiGJM17}) for the source and the target language. As depicted in Figure \ref{fig:chap3_xie2018}, both embeddings are mapped into a bilingual vector space using a cross-lingual vector projection (\cite{zhang-etal-2016-ten,artetxe-etal-2016-learning,DBLP:conf/iclr/SmithTHH17}). A word translation dictionary is then computed by k-nearest-neighbor. The source sentence is translated word-by-word using this dictionary and the source label is copied for each corresponding word in the target sentence. Finally, a NER model is trained using the translated data. Building on this approach, \cite{DBLP:conf/acl/GuoR21} aim to refine the low-quality output that results from literal word-by-word translations. They employ a target language model and constrained beam search to produce text in the target language that exhibits a more natural and contextually appropriate word order. The constraints are designed to ensure the presence of entities in the generated text.


\subsection{Model-based transfer}


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Capitulos/2_EstadoDelArte/diagramas/Zeroshot.pdf}
    \caption{Illustration of the model-based coss-lingual transfer approach. A pre-trained multilingual model is finetuned with data in the source language and then applied without modification to label text in the target language.}
    \label{fig:chap3_zero}
\end{figure}

The model transfer approach involves leveraging multilingual models trained in high-resource languages to perform tasks in low-resource languages. In contrast, the data transfer paradigm focuses on manipulating the data to fit a monolingual model. Model transfer exploits the shared representation of languages in a pre-trained multilingual model. Thus, these models can be fine-tuned for a specific task in the source language and then applied without any modification to label text in any of the multiple languages the model supports. This approach is illustrated in Figure \ref{fig:chap3_zero}.


Some of the first attempts at model-based cross-lingual transfer (\cite{tackstrom-etal-2012-cross,kozhevnikov-titov-2014-cross,bharadwaj-etal-2016-phonologically,chaudhary-etal-2018-adapting}) leveraged the structural similarities between languages to facilitate tasks in languages with limited or non-existent training data. However, model-based transfer began to make significant progress (\cite{artetxe-schwenk-2019-massively,pires-etal-2019-multilingual}) following the introduction of Transformer-based (\cite{DBLP:conf/nips/VaswaniSPUJGKP17}) multilingual language models such as BERT (\cite{devlin-etal-2019-bert}) or XLM-RoBERTa (\cite{conneau-etal-2020-unsupervised}). These models were pre-trained using language modelling objectives on extensive datasets comprising over 100 languages. During this pre-training phase, the models acquired a shared representation for all included languages. Subsequently, these models can be fine-tuned on specific tasks with data from a source language and then applied to label data in any of the supported languages directly.

