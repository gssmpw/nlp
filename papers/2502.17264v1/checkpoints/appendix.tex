\section{Proofs and Remarks from \Cref{sec:algorithm}}
In this section, we present the omitted proofs from \Cref{sec:algorithm} and provide some further discussion about the coverage and the computational complexity of Kandinsky conformal prediction.

\subsection{Proofs from \Cref{sec:algorithm}}
\label{sec: algorithm_app}
\begin{customthm}{\ref*{thm:jointcondcov}}[Restated]
    Let $\alpha$ and $\delta$ be parameters in $(0,1)$, and $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over a bounded basis $\Phi : \calX \times \calY \to \RR^d$. Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. Then, there exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$, given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, satisfies

\begin{align*}
&\left|\EE_{(X_{n+1},Y_{n+1})\sim \calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}}\qth{w(X_{n+1},Y_{n+1})\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}}\right| \leq \\
&{} 
\|w\|_{\infty} \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} },
\end{align*}
for every $w \in \calW$.
\begin{comment}
    Let $\alpha, \delta$ be two parameters in $(0,1)$ and let $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over the basis $\Phi : \calX \times \calY \to \RR^d$, where $\Phi$ is bounded. 
    % such that for all $w \in \calW$, $\sup_{x\in \calX, y \in \calY} |w(x,y)| \leq B$. 
    Let $\{\varepsilon_i\}_{i\in[n+1]}$ be independent real-valued random variables, independent of $\{(X_i,Y_i)\}_{i\in[n+1]}$.
    % , and $\varepsilon$ denotes a generic random variable distributed identically to $\varepsilon_i$. 
    For a randomized score function $\tS(x,y,\varepsilon)$, consider the following regression:
    \begin{align*}
        \hat{q} \in \arg \min_{w \in \cal W} &\frac{1}{n} \sum_{i=1}^n \ell_\alpha \left(w(X_i,Y_i), \tS\left(X_i,Y_i, \varepsilon_i\right)\right). 
    \end{align*}
    The prediction set, potentially randomized by $\varepsilon_{n+1}$, is given by
    \begin{align*}
        \calC(X_{n+1}) = \left\{y: \tS\left(X_{n+1},y, 
        \varepsilon_{n+1}\right) \leq \hat{q}(X_{n+1},y)\right\}.
    \end{align*}
    Assume $\{(X_i,Y_i, \varepsilon_i)\}_{i\in [n+1]}$ are independent and identically distributed random variables drawn from the distribution $\calD$, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. There exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i, \varepsilon_i)\}_{i\in [n]}$, the prediction set $\calC$ satisfies
\begin{align*}
&\left|\EE_{(X_{n+1},Y_{n+1}, \varepsilon_{n+1})\sim \calD}\qth{w(X_{n+1},Y_{n+1})\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}}\right| \leq \\
&{} 
\|w\|_{\infty} \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} },
\end{align*}
for every $w \in \calW$.
\end{comment}
\end{customthm}
\begin{proof}
Denote $\tS\left(X_i,Y_i, \varepsilon_i\right)$ by $\tS_i$. Then $\{(X_i,Y_i, \tS_i)\}_{i\in [n+1]}$ are independent and identically distributed. 

Consider the function $L(\eta) = \frac{1}{n} \sum_{i=1}^n \ell_\alpha \left(\hat q(X_i,Y_i)+\eta w(X_i,Y_i), \tS_i\right)$, $\eta \in \RR$, which is convex because $\ell_\alpha(\cdot, \tS_i)$ is convex for each $i$.

For any $w \in \calW$, since $\hat q + \eta w \in \calW$, the subgradients of $L(\eta)$ at $\eta=0$ satisfy
\begin{align*}
    0 \in ~& \partial L(\eta) \big|_{\eta = 0} = 
     \left\{ \frac{1}{n} \sum_{i=1}^n w(X_i,Y_i) \gamma_i ~\bigg|~ \gamma_i = 
    \begin{cases} 
    \alpha - \11\{\tS_i > \hat{q}(X_i,Y_i)\}, & \hat{q}(X_i,Y_i) \neq \tS_i, \\
    \alpha_i \in [\alpha - 1, \alpha], & \hat{q}(X_i,Y_i) = \widetilde{S}_i 
    \end{cases} \right\}.
    % & \sth{ \frac{1}{n} \sum_{\hat q(X_i,Y_i)\neq\tS_i} w(X_i,Y_i)\pth{\alpha-\11\sth{\tS_i > \hat q(X_i,Y_i)}} +
    % \frac{1}{n} \sum_{\hat q(X_i,Y_i)=\tS_i} \alpha_i w(X_i,Y_i) \mid \alpha_i \in [\alpha-1,\alpha], i\in[n] }.
\end{align*}
Therefore, there exist $\alpha_i^\star \in [\alpha-1,\alpha]$, $i\in[n]$ such that
\begin{align*}
    \frac{1}{n} \sum_{\hat q(X_i,Y_i)\neq\tS_i} w(X_i,Y_i)\pth{\alpha-\11\sth{\tS_i > \hat q(X_i,Y_i)}} +
    \frac{1}{n} \sum_{\hat q(X_i,Y_i)=\tS_i} \alpha_i^\star w(X_i,Y_i) = 0.
\end{align*}
This implies
\begin{align}
    \label{eq:thm_jointcondcov_app1}
    \left| \frac{1}{n}\sum_{i=1}^n w(X_i,Y_i)\pth{\alpha-\11\sth{\tS_i > \hat q(X_i,Y_i)}} \right| &= 
    \left| \frac{1}{n} \sum_{\hat q(X_i.Y_i)=\tS_i} w(X_i,Y_i) \pth{\alpha - \alpha_i^\star}\right|  \\
    \label{eq:thm_jointcondcov_app2}
    &\leq \frac{1}{n} \sum_{i=1}^n \11 \sth{\hat q(X_i.Y_i)=\tS_i} |w(X_i,Y_i) |.
\end{align}
For the left hand side of \Cref{eq:thm_jointcondcov_app1}, we consider the function classes
\begin{align*}
    \calH_1 &= \sth{h_1(x,y,s) = \11\sth{ q(x,y) - s<0} \Bigm|  q\in\calW} = \sth{h_1(x,y,s) = \11\sth{ \Phi(x,y)^T\beta - s<0} \Bigm|  \beta\in\RR^d}, \\
    \calH_2 &= \sth{h_2(x,y,s) = w(x,y)(\alpha - h_1(x,y,s)) \Bigm| h_1 \in \calH_1}. \\
\end{align*}
Under the mapping $(x,y,s) \mapsto 
\begin{bmatrix}
    \Phi(x,y) \\
    -s
\end{bmatrix} \in \RR^{d+1}$,
$\calH_1$ is the subset of all homogeneous halfspaces in $\RR_{d+1}$, with the normal vector $\begin{bmatrix}
    \beta \\
    1
\end{bmatrix}$. Therefore, the VC-dimension of $\calH_1$ is at most $d+1$. 

For samples $D = \{(X_i,Y_i, \tS_i)\}_{i\in [n]}$, the empirical Rademacher complexity of a function class $\calH$ restricted to $D$ is defined as
\begin{align*}
    \calR(\calH \circ D) =\frac{1}{n} \EE_{\bsigma}\qth{\sup_{h\in\calH} \sum_{i=1}^n \sigma_i h(X_i,Y_i,\tS_i)},
\end{align*}
where random variables in $\bsigma$ are independent and identically distributed according to $\PP[\sigma_i=1] = \PP[\sigma_i=-1] = \frac{1}{2}$.

\citet{WW19}~(Example 5.24) gives an upper bound of the Rademacher complexity for VC classes. There exists an absolute constant $C'$, such that
\begin{align*}
    \calR(\calH_1 \circ D) \leq \frac{1}{n} \EE_{\bsigma}\qth{\sup_{h\in\calH_1} \left|\sum_{i=1}^n \sigma_i h(X_i,Y_i,\tS_i)\right| } \leq C' \sqrt {\frac{d+1}{n}} \leq C' \sqrt {\frac{2d}{n}}.
\end{align*}
Next, we compute the empirical Rademacher complexity of $\calH_2$ restricted to $D$. For any $h, h' \in \calH_1$, 
\begin{align*}
    \left| w(x,y)(\alpha-h(x,y,s)) - w(x,y)(\alpha-h'(x,y,s)) \right| \leq \|w\|_\infty \left|h(x,y,s) - h'(x,y,s) \right|.
\end{align*}
According to the contraction lemma (Lemma~26.9 in \citet{SL14}), 
\begin{align*}
    \calR(\calH_2 \circ D) \leq \|w\|_\infty \calR(\calH_1\circ D) \leq C'\|w\|_\infty \sqrt {\frac{2d}{n}}.
\end{align*}
Therefore, we have a universal generalization bound for the functions in $\calH_2$. The functions in $\calH_2$ are bounded by $\max\sth{\alpha, 1-\alpha} \|w\|_\infty$. According to Theorem~26.5 in \citet{SL14}, with probability at least $1-\delta$ over the randomness in $D = \{(X_i,Y_i, \tS_i)\}_{i\in [n]}$, for all $h\in\calH_2$,
\begin{align*}
    \EE\qth{h(X_{n+1}, Y_{n+1}, \tS_{n+1})} - \frac{1}{n} \sum_{i=1}^n h(X_i, Y_i, \tS_i) &\leq  2\EE_D\qth{\calR(\calH_2\circ D)} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(2/\delta)}{n}} \\
    &\leq 2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(2/\delta)}{n}}.
\end{align*}
Symmetrically, we can replace $h$ with $-h$ in the class $\calH_2$. Since $\calR(\calH_2\circ D) = \calR(-\calH_2\circ D)$, with probability at least $1-\delta$, for all $h\in\calH_2$,
\begin{align*}
       \frac{1}{n} \sum_{i=1}^n h(X_i, Y_i, \tS_i) - \EE\qth{h(X_{n+1}, Y_{n+1}, \tS_{n+1})}
    \leq 2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(2/\delta)}{n}}.
\end{align*}
Taking the union bound, with probability at least $1-\delta$ over the randomness in $D$, for all $h\in\calH_2$,
\begin{align*}
    \left|\EE\qth{h(X_{n+1}, Y_{n+1}, \tS_{n+1})} - \frac{1}{n} \sum_{i=1}^n h(X_i, Y_i, \tS_i) \right| \leq 2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(4/\delta)}{n}}.
\end{align*}
By expanding functions in $\calH_2$ as $h(x,y,s) = w(x,y)\pth{\alpha - \11\sth{q(x,y)-s < 0}}$ where $q\in\calW$, with probability at least $1-\delta$ over the randomness in $D$, for all $q\in\calW$, 
\begin{align*}
    &\left| \EE \qth{w(X_{n+1},Y_{n+1}) \pth{\11\sth{\tS_{n+1}\leq q(X_{n+1},Y_{n+1}) } - (1-\alpha) }} - \frac{1}{n} \sum_{i=1}^n w(X_i,Y_i) \pth{\alpha - \11\sth{q(X_i,Y_i)- \tS_i < 0}} \right| = \\ 
    &\left| \EE \qth{w(X_{n+1},Y_{n+1}) \pth{\alpha - \11\sth{q(X_{n+1},Y_{n+1}) - \tS_{n+1} < 0}  }} - \frac{1}{n} \sum_{i=1}^n w(X_i,Y_i) \pth{\alpha - \11\sth{q(X_i,Y_i)- \tS_i < 0}} \right| \leq \\
    &2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(4/\delta)}{n}}.
\end{align*}
In particular, this inequality holds for $\hat q$. By plugging in \Cref{eq:thm_jointcondcov_app1} and \Cref{eq:thm_jointcondcov_app2},
\begin{align}
\label{eq:thm_jointcondcov_app3}
\begin{split}
     &\left| \EE_{(X_{n+1},Y_{n+1}) \sim\calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}} \qth{w(X_{n+1},Y_{n+1}) \pth{\11\sth{\tS_{n+1} \leq 
    \hat q(X_{n+1},Y_{n+1}) } - (1-\alpha) }} \right| \leq \\
    &\left| \frac{1}{n} \sum_{i=1}^n w(X_i,Y_i) \pth{\alpha - \11\sth{\hat q(X_i,Y_i)- \tS_i < 0}} \right| + 2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(4/\delta)}{n}} \leq \\
    &\frac{1}{n} \sum_{i=1}^n \11 \sth{\hat q(X_i.Y_i)=\tS_i} |w(X_i,Y_i) | + 2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(4/\delta)}{n}} \leq \\
    &\frac{1}{n}\|w\|_\infty \sum_{i=1}^n \11 \sth{\hat q(X_i.Y_i)=\tS_i} + 2C'\|w\|_\infty \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha} \|w\|_\infty  \sqrt{\frac{2 \ln(4/\delta)}{n}}.
\end{split}
\end{align}
Theorem~3 in \citet{GCC2023} provides an upper bound for a quantity similar to $\sum_{i=1}^n \11 \sth{\hat q(X_i.Y_i)=\tS_i}$, under a different assumption that the distribution of $\tS \mid X$ is continuous. We will follow a similar proof technique, but we assume that the distribution of $\tS \mid \Phi$ is continuous.

There exists $\hbeta \in \RR^d$ such that $\hat q = \Phi(\cdot)^T \hbeta $.
\begin{align*}
    &\PP\qth{ \sum_{i=1}^n \11 \sth{\hat q(X_i.Y_i)=\tS_i}  \geq d+1} = \\
    &\EE\qth{\PP\qth{ \sum_{i=1}^n \11 \sth{\Phi(X_i,Y_i)^T\hbeta=\tS_i}  \geq d+1 \Biggm| \Phi(X_1,Y_1),...,\Phi(X_n,Y_n) }} = \\
    &\EE\qth{\PP\qth{ \exists~1 \leq j_1 \leq ... \leq j_{d+1} \leq n, \forall~1\leq k \leq d+1, \Phi(X_{j_k},Y_{j_k})^T\hbeta=\tS_{j_k} \Biggm| \Phi(X_1,Y_1),...,\Phi(X_n,Y_n) }} \leq \\
    &\EE\qth{ \sum_{1 \leq j_1 < ... < j_{d+1} \leq n} \PP\qth{ \exists~\beta\in\RR^d, \forall~1\leq k \leq d+1, \Phi(X_{j_k},Y_{j_k})^T\beta=\tS_{j_k} \Biggm| \Phi(X_1,Y_1),...,\Phi(X_n,Y_n) } } \leq \\
    &\EE\left[ \sum_{1 \leq j_1 < ... < j_{d+1} \leq n}  \PP\Bigg[ \pth{\tS_{j_1}, ..., \tS_{j_{d+1}} } \in \operatorname{RowSpace}\pth{\qth{\Phi(X_{j_1},Y_{j_1}),...,\Phi(X_{j_{d+1}},Y_{j_{d+1}})}} \right.\\
    &\left. \hspace{4.5in} \Biggm| \Phi(X_1,Y_1),...,\Phi(X_n,Y_n) \Bigg] \right] = \\
    &0.
\end{align*}
The last equation holds because the distribution of $\pth{\tS_{j_1}, ..., \tS_{j_{d+1}} } \Bigm| \Phi(X_1,Y_1),...,\Phi(X_n,Y_n)$ is continuous, and $\operatorname{RowSpace}\pth{\qth{\Phi(X_{j_1},Y_{j_1}),...,\Phi(X_{j_{d+1}},Y_{j_{d+1}})}}$ is at most $d-$dimensional while $\pth{\tS_{j_1}, ..., \tS_{j_{d+1}} }$ is a $(d+1)-$dimensional vector. Therefore, with probability 1,
\begin{align*}
    \sum_{i=1}^n \11 \sth{\hat q(X_i.Y_i)=\tS_i} \leq d.
\end{align*}
Plugging the inequality into \Cref{eq:thm_jointcondcov_app3}, with probability at least $1-\delta$ over the randomness in $D$,
\begin{align*}
    &\left| \EE_{(X_{n+1},Y_{n+1}) \sim\calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}} \qth{w(X_{n+1},Y_{n+1}) \pth{\11\sth{\tS_{n+1} \leq 
    \hat q(X_{n+1},Y_{n+1}) } - (1-\alpha) }} \right| \leq \\
    &\|w\|_\infty \pth{\frac{d}{n} + 2C' \sqrt {\frac{2d}{n}} + \max\sth{\alpha, 1-\alpha}  \sqrt{\frac{2 \ln(4/\delta)}{n}}}.
\end{align*}
\end{proof}

\begin{customcor}{\ref*{cor: expected-cov}}[Restated]
Let $\alpha, \delta, \calW$ be specified as in \Cref{thm:jointcondcov}. Assume that $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. Then, there exists an absolute constant $C$ such that the (randomized) prediction set $\calC$, given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, satisfies
\begin{align*}
\EE_{D,E}  \left[ \sup_{w\in\calW} \EE_{(X_{n+1},Y_{n+1}) \sim \calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}}\qth{\frac{w(X_{n+1},Y_{n+1})}{\|w\|_{\infty}}\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}}\right] \leq 
C \sqrt{\frac{d}{n}} + \frac{d}{n},
\end{align*}

where $D$ is the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and $E$ is the corresponding noise $\{\varepsilon_i\}_{i\in [n]}$.
\begin{comment}
Let $\alpha,\calW, \calC$ be as specified in \Cref{thm:jointcondcov}. Assume $\{(X_i,Y_i, \varepsilon_i)\}_{i\in [n+1]}$ are independent and identically distributed random variables drawn from the distribution $\calD$, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. The calibration dataset $D = \{(X_i,Y_i, \varepsilon_i)\}_{i\in [n]}$ follows the distribution $D\sim\calD^n$. There exists an absolute constant $C$ such that the prediction set $\calC$ satisfies
\begin{align*}
\EE_{D\sim\calD^n}  \left[ \sup_{w\in\calW} \EE_{(X_{n+1},Y_{n+1}, \varepsilon_{n+1})\sim \calD}\qth{\frac{w(X_{n+1},Y_{n+1})}{\|w\|_{\infty}}\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}}\right] \leq 
C \sqrt{\frac{d}{n}} + \frac{d}{n}.
\end{align*}
\end{comment}
\end{customcor}
\begin{proof}
According to \Cref{thm:jointcondcov}, there exists an absolute constant $C'$ such that, with probability at least $1-\delta$ over the randomness in $D$ and $E$, 
\begin{align*}
    &\sup_{w\in\calW} \EE_{(X_{n+1},Y_{n+1}) \sim \calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}}\qth{\frac{w(X_{n+1},Y_{n+1})}{\|w\|_{\infty}}\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}} \leq \\
    &\sup_{w\in\calW} \left|\EE_{(X_{n+1},Y_{n+1}) \sim \calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}}\qth{\frac{w(X_{n+1},Y_{n+1})}{\|w\|_{\infty}}\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}} \right| \leq \\
    & C' \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}}.
\end{align*}
Let the random variable $M_{D,E} = \sup_{w\in\calW} \EE_{(X_{n+1},Y_{n+1}) \sim \calD, \varepsilon_{n+1} \sim \calD_{\text{rn}}}\qth{\frac{w(X_{n+1},Y_{n+1})}{\|w\|_{\infty}}\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}}$. We have 
\begin{align*}
    \PP_{D,E}\left[ M_{D,E} >  C' \sqrt{\frac{d}{n}} + \frac{d}{n} + \max\{\alpha, 1 - \alpha\} \sqrt{\frac{2 \ln(4/\delta)}{n}}  \right] \leq \delta.
\end{align*}
By inverting the bound, for $t>C' \sqrt{\frac{d}{n}} + \frac{d}{n}$,
\begin{align*}
    \PP_{D,E}\qth{M_{D,E} > t} \leq  4 \exp\left[-\frac{n \big(t - C' \sqrt{\frac{d}{n}} - \frac{d}{n}   \big)^2}{2 \max\{\alpha, 1 - \alpha\}^2}\right].
\end{align*}
Since $M_{D,E} \geq 0$, 
\begin{align*}
    \EE_{D,E}\qth{M_{D,E}} &= \int_{t=0}^{+\infty} \PP_{D,E}\qth{M_{D,E}>t} \mathrm{d}t \\
    &= \int_{t=0}^{C' \sqrt{\frac{d}{n}} + \frac{d}{n}} \PP_{D,E}[M_{D,E} > t] \, \mathrm{d}t + \int_{t=C' \sqrt{\frac{d}{n}} + \frac{d}{n}}^{+\infty} \PP_{D\sim\calD^n}\qth{M_D > t} \, \mathrm{d}t \\
    &\leq C' \sqrt{\frac{d}{n}} + \frac{d}{n} + \int_{t=C' \sqrt{\frac{d}{n}} + \frac{d}{n}}^{+\infty} \PP_{D\sim\calD^n}\qth{M_D > t}\, \mathrm{d}t \\
    &\leq  C' \sqrt{\frac{d}{n}} + \frac{d}{n} + \int_{t=C' \sqrt{\frac{d}{n}} + \frac{d}{n}}^{+\infty} 4 \exp\left[-\frac{n \big(t - C' \sqrt{\frac{d}{n}} - \frac{d}{n}   \big)^2}{2 \max\{\alpha, 1 - \alpha\}^2}\right] \, \mathrm{d}t \\
    &= C' \sqrt{\frac{d}{n}} + \frac{d}{n} + \max\{\alpha, 1 - \alpha\} \sqrt{\frac{8\pi}{n}} \\
    &\leq \pth{C' +\sqrt{8\pi} }  \sqrt{\frac{d}{n}} + \frac{d}{n}.
\end{align*}
\end{proof}


\begin{customcor}{\ref*{cor: fract-cov}}[Restated]
Let $\alpha, \delta \in (0,1)$, $\phi(X,Y)$ be a sufficient statistic for $\tS$, such that $\tS$ is conditionally independent of $X,Y$ given $\phi$, and
    % $
    %     \calW = \{\sum_{G \in \calG} \beta_G \PP[Z \in G \mid \phi(X,Y)=\phi(x,y)]: \beta_G \in \RR, \forall~G\in\calG\}
    % $
    $
        \calW = \{\sum_{G \in \calG} \beta_G \Phi_G: \beta_G \in \RR, \forall~G\in\calG\}
    $
    . Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. There exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$ given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set} satisfies, for all $G \in \calG$,
    \begin{align*}
    &\left|\PP\qth{Y_{n+1} \in\calC(X_{n+1}) \mid Z_{n+1}\in G } -(1-\alpha)\right| \leq \\
    &\frac{1}{\PP[Z\in G]} 
     \pth{C\sqrt{\frac{|\calG|}{n}}+\frac{|\calG|}{n}+\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
     \end{align*}
\end{customcor}
\begin{proof}
Since $\tS$ is conditionally independent of $X,Y$ given $\phi$, more formally $\tS \perp\!\!\!\perp X,Y \mid \phi(X,Y)$, and $\varepsilon \perp\!\!\!\perp X,Y$, we have $\tS, \varepsilon \perp\!\!\!\perp X,Y \mid \phi(X,Y)$. Therefore $\tS \perp\!\!\!\perp X,Y \mid \phi(X,Y), \varepsilon$. This implies for any $x_1,y_1,x_2,y_2$ such that $\phi(x_1,y_1) = \phi(x_2,y_2)$, we have $\tS(x_1,y_1,\varepsilon) = \tS(x_2,y_2,\varepsilon)$. Therefore, $\tS(X,Y,\varepsilon)$ is measurable w.r.t. $\phi(X,Y),\varepsilon$. From the definition of $\calW$, any $w(X,Y)$ with $w\in\calW$ is also measurable w.r.t. $\phi(X,Y),\varepsilon$.

According to \Cref{thm:jointcondcov}, there exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness in $\{(X_i,Y_i)\}_{i\in [n]}$ and $\{\varepsilon_i\}_{i \in [n]}$, for every $w \in \calW$, 
\begin{align*}
&\left|\EE_{(X_{n+1},Y_{n+1}) \varepsilon_{n+1}}\qth{w(X_{n+1},Y_{n+1})\pth{\11\sth{ \tS(X_{n+1},Y_{n+1},\varepsilon_{n+1}) \leq \hat q (X_{n+1},Y_{n+1}) }- (1-\alpha)}}\right| = \\
&\left|\EE_{(X_{n+1},Y_{n+1}) \varepsilon_{n+1}}\qth{w(X_{n+1},Y_{n+1})\pth{\11\sth{Y_{n+1} \in \calC(X_{n+1})}- (1-\alpha)}}\right| \leq \\
&{} 
\|w\|_{\infty} \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} } = \\
&\|w\|_{\infty} \pth{C \sqrt{\frac{|\calG|}{n}} + \frac{|\calG|}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
\end{align*}
For any $G \in \calG$, by taking $w(x,y) = \frac{1}{\PP[Z\in G]}\PP[Z \in G \mid \phi(X,Y)=\phi(x,y)]$, we have $\|w\|_{\infty}  \leq \frac{1}{\PP[Z\in G]}$. Since $\tS(X_{n+1},Y_{n+1},\varepsilon_{n+1})$, $\hat q (X_{n+1},Y_{n+1})$ are measurable w.r.t. $\phi(X_{n+1},Y_{n+1}), \varepsilon_{n+1}$, by Bayes formula,
\begin{align*} 
    &\EE_{(X_{n+1},Y_{n+1}), \varepsilon_{n+1}}\qth{w(X_{n+1},Y_{n+1})\pth{\11\sth{ \tS(X_{n+1},Y_{n+1},\varepsilon_{n+1}) \leq \hat q (X_{n+1},Y_{n+1}) }- (1-\alpha)}} = \\
    &\EE_{(X_{n+1},Y_{n+1}), \varepsilon_{n+1}}\qth{ \frac{\PP[Z_{n+1} \in G \mid \phi(X_{n+1},Y_{n+1})]}{\PP[Z_{n+1}\in G]} \pth{\11\sth{ \tS(X_{n+1},Y_{n+1},\varepsilon_{n+1}) \leq \hat q (X_{n+1},Y_{n+1}) }- (1-\alpha)}} = \\
     &\EE_{\phi(X_{n+1},Y_{n+1}), \varepsilon_{n+1}}\Big[ \frac{\PP[Z_{n+1} \in G \mid \phi(X_{n+1},Y_{n+1}), \varepsilon_{n+1}]}{\PP[Z_{n+1}\in G \mid \varepsilon_{n+1}]} \cdot  \\
    &\hspace{0.3\linewidth}  
    \pth{\11\sth{ \tS(X_{n+1},Y_{n+1},\varepsilon_{n+1}) \leq \hat q (X_{n+1},Y_{n+1}) }- (1-\alpha)} \Big] = \\
    &\EE_{(X_{n+1},Y_{n+1}), \varepsilon_{n+1}}\qth{  \11\sth{ \tS(X_{n+1},Y_{n+1},\varepsilon_{n+1}) \leq \hat q (X_{n+1},Y_{n+1}) }- (1-\alpha)
    \Bigm| Z_{n+1} \in G} = \\
    & \PP_{(X_{n+1},Y_{n+1}), \varepsilon_{n+1}} \qth{Y_{n+1} \in \calC(X_{n+1}) \mid Z_{n+1}\in G } - (1-\alpha).
\end{align*}
Combining the two equations above, the proof is complete.
\end{proof}
\begin{customcor}{\ref*{cor: group-cond}}[Restated]
     Let parameters $\alpha,\delta \in (0,1)$, and $\calW = \{\sum_{G \in \calG} \beta_G \11\{(x,y) \in G\}: \beta_G \in \RR, \forall G \in \calG \}$. Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. There exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$, given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set} satisfies, for all $G \in \calG$,
    \begin{align*}
    &\left| \PP[Y_{n+1}\in \calC(X_{n+1}) \mid (X_{n+1},Y_{n+1})\in G] -(1-\alpha)\right| \leq \\
    &{} 
    \frac{1}{\PP[(X_{n+1}, Y_{n+1}) \in G]}\pth{C \sqrt{\frac{|\calG|}{n}} + \frac{|\calG|}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
    \end{align*}
\end{customcor}
\begin{proof}
\Cref{cor: group-cond} follows directly from \Cref{cor: fract-cov}.
\end{proof}

\begin{customcor}{\ref*{cor: distr-shift}}[Restated]
     Let $\alpha$ and $\delta$ be parameters in $(0,1)$, and $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over a bounded basis $\Phi : \calX \times \calY \to \RR^d$. Assume that the data $\{(X_i,Y_i)\}_{i\in [n]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. Then, there exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$, given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, satisfies
    \begin{align*}
        &|\PP_{(X_{n+1},Y_{n+1}) \sim \calD_{\text{T}}, \varepsilon_{n+1}} [Y_{n+1} \in \calC(X_{n+1})] - (1-\alpha)|\leq \\
        &B \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} } ,
    \end{align*}
     where $(X_{n+1},Y_{n+1})$ are drawn independently from a distribution $\calD_{\text{T}}$, for every distribution $\calD_{\text{T}}$ such that $\frac{d\PP_{\calD_{T}}}{d\PP_{\calD}} \in \calW$ and $ \left|\frac{d\PP_{\calD_{T}}}{d\PP_{\calD}}(x,y) \right|\leq B$ for any $x \in \calX$ and $y \in \calY$.  
\end{customcor}
\begin{proof}
 This corollary follows from \Cref{thm:jointcondcov} for any weight function $w \in \calW$ of the form $w(x,y) = \frac{d\PP_{\calD_{T}}}{d\PP_{\calD}}(x,y)$ with $\|w\|_{\infty} \leq B$.
\end{proof}
\subsection{Comparison with Previous Results}
\label{sec: comparison}
For weight functions defined only on the covariates, Kandinsky conformal prediction obtains the same type of guarantees studied in \citet{JNRR2023, GCC2023, ACDR24}. \citet{JNRR2023} and \citet{ACDR24} essentially implement the same quantile regression method as that described in Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set} for the corresponding class of weight functions and for deterministic scores. Typically, for covariate-based weight functions the distribution of $S(X,Y) \mid \Phi(X)$ is continuous and, hence, it is common to set $\tS(x,y,\varepsilon) = S(x,y)$. \citet{JNRR2023} focus on the group-conditional case, where the groups are subsets of $\calX$, and their analysis of high-probability coverage is less optimal compared to the result in \Cref{cor: group-cond}. Additionally, our analysis of the expected weighted coverage deviation in \Cref{cor: expected-cov} provides a tighter upper bound compared to \cite{ACDR24}. Lastly, \citet{GCC2023} implement a different quantile regression method that we discuss further in \Cref{sec:testtimeqr}.


\subsection{Computational Details}
\label{sec: computation}
In \Cref{alg:pred_set}, the prediction set $\calC$ is defined as the subset of all labels in $\calY$ where the score is below the value determined by the quantile function $\hat q$, that varies with $y$. This makes calculating the prediction set from the quantile function $\hat q$ complex for large finite sets of labels, and even more so for continuous label domains. This is a problem that can also be encountered in full conformal prediction. For several applications described in this section and for certain chosen score functions, there exist oracles that, given the quantile function $\hat q$ and the test point $\calX_{n+1}$, return the prediction set $\calC$.



\section{Proofs from  \Cref{sec:testtimeqr}}
In this section, we provide the proof of \Cref{thm:tt-cond}.

\label{sec:tt_app}
\begin{customthm}{\ref*{thm:tt-cond}}[Restated]
       Let $\alpha$ be a parameter in $(0,1)$, and let $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over a basis $\Phi : \calX \times \calY \to \RR^d$. Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. Then, for any $w \in \calW$, the prediction set given by \Cref{alg:tt_quantile_reg} satisfies 
    \begin{align*}
       |\EE_{D,E} [ \wcovdev(\calC, \alpha,w)]| \leq \frac{d}{n+1} \EE_{D_{+}}\left[ \max_{i \in [n+1]}|w(X_i,Y_i)|\right],
    \end{align*}
    where $D$ is the calibration dataset $\{(X_i,Y_i)\}_{i\in[n]}$, $E$ is the corresponding noise $\{\varepsilon_i\}_{i \in [n]}$ and $D_{+}$ is the full dataset $\{(X_i,Y_i)\}_{i\in[n+1]}$.
\end{customthm}
\begin{proof}
   This proof follows the techniques developed in \cite{GCC2023}. For simplicity, $\tS\left(X_i,Y_i, \varepsilon_i\right)$ is denoted by $\tS_i$. 
    Let $\hat{q}_{Y_{n+1}}$ be the quantile function \Cref{alg:tt_quantile_reg} computes for the true label $Y_{n+1}$. For a fixed $w \in \calW$ our objective can be reformulated as
    \begin{align*}
        \EE_{D,E}[\wcovdev(\calC, \alpha, w)] &= \EE\left[ w(X_{n+1},Y_{n+1})\left( \11\left\{ Y_{n+1} \in \calC(X_{n+1})\right\}-(1-\alpha)\right)\right]\\
        & =\EE \left[ w(X_{n+1},Y_{n+1})\left( \alpha - \11\left\{\tS_{n+1} > \hat{q}_{Y_{n+1}}(X_{n+1},Y_{n+1})\right\}\right)\right],
    \end{align*}
    where the expectations on the right hand side are taken over the randomness of $\{(X_i,Y_i)\}_{i\in [n+1]}$ and $\{\varepsilon_i\}_{i\in [n+1]}$.
    
    Since the data $\{(X_i,Y_i)\}_{i \in [n+1]}$ are \mbox{i.i.d.}, the random noise components $\{\varepsilon_i\}_{i \in [n+1]}$ are also \mbox{i.i.d.}, and based on \Cref{alg:tt_quantile_reg}, $\hat{q}_{Y_{n+1}}$ is invariant to permutations of $\{(X_i,Y_i)\}_{i \in [n+1]}$, the triples in \[\{(w(X_i,Y_i), \hat{q}_{Y_{n+}1}(X_i,Y_i), \tS_i,\varepsilon_i))\}_{i \in [n+1]}\] are exchangeable. Hence, we have that
    \begin{align*}
     \EE_{D,E}[\wcovdev(\calC, \alpha, w)] &=\EE\left[ w(X_{n+1},Y_{n+1})\left( \alpha - \11\left\{\tS_{n+1}> \hat{q}_{Y_{n+1}}(X_{n+1},Y_{n+1})\right\}\right)\right] \\
     &=\EE \left[\frac{1}{n+1}\sum_{i \in [n+1]} w(X_i,Y_i)\left( \alpha - \11\left\{\tS_i > \hat{q}_{Y_{n+1}}(X_i,Y_i)\right\}\right)\right].
    \end{align*}
    
    Since $\hat{q}_{Y_{n+1}}$ is a minimizer of the convex optimization problem defined in \Cref{alg:tt_quantile_reg}, we have that for a fixed $w \in \calW$, fixed datapoints $\{(X_i, Y_i)\}_{i \in [n+1]}$, and fixed noise $\{\varepsilon_i\}_{i \in [n+1]}$
    \begin{align*}
         0 \in \partial_{\eta} \left( \frac{1}{n+1} \sum_{i \in [n+1]} \ell_\alpha(\hat{q}_{Y_{n+1}}(X_i,Y_i) + \eta w(X_i,Y_i), \tS_i\right) \Bigg|_{\eta = 0}.
    \end{align*}
    Computing this subrgradient, we obtain that 
    \begin{align*}
        &\partial_{\eta} \left( \frac{1}{n+1} \sum_{i \in [n+1]} \ell_\alpha(\hat{q}_{Y_{n+1}}(X_i,Y_i) + \eta w(X_i,Y_i), \tS_i\right) =\\
        & \Bigg\{\frac{1}{n+1} \Big( \sum_{i \in [n+1]} w(X_i,Y_i) \left(\alpha - \11\left\{\tS_i > \hat{q}_{Y_{n+1}}(X_i,Y_i)\right\}\right)\11\{\tS_i \neq \hat{q}_{Y_{n+1}}(X_i, Y_i)\}+ \\
        &\sum_{i \in [n+1]} v_i w(X_i,Y_i)\11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\Big)\Bigg| v_i \in [\alpha-1,\alpha]\Bigg\}.
    \end{align*}
    Let $v^*_i$ be one of the values in $ [\alpha-1, \alpha]$ that set the subgradient to zero. Then, we have that 
    \begin{align*}
        &\frac{1}{n+1}\sum_{i \in [n+1]}w(X_i,Y_i) \left(\alpha - \11\{ \tS_i > \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\right) = \\
        &\frac{1}{n+1} \sum_{i \in [n]} (\alpha -v^*_i)w(X_i,Y_i)\11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}.
    \end{align*}

    Going back to our previous computation where we have only fixed $w \in \calW$, we apply the equality above to obtain that 
    \begin{align*}
         \EE_{D,E}[\wcovdev(\calC, \alpha,w)] &= 
          \EE \left[\frac{1}{n+1}\sum_{i \in [n+1]} w(X_i,Y_i)\left( \alpha - \11\left\{\tS_i > \hat{q}_{Y_{n+1}}(X_i,Y_i)\right\}\right)\right] \\
          &= \EE\left[ \frac{1}{n+1} \sum_{i \in [n+1] } (\alpha -v^*_i)w(X_i,Y_i) \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\right].
    \end{align*}
    
   % Since $g \in \calG$ is non-negative and $ s_i^* \leq \alpha$, we have that for all $\{(x_i,y_i)\}_{i \in [n+1]}$
    %\[
    %\frac{1}{n+1} \sum_{i \in [n+1]} (\alpha - s_i^*) g(x_i,y_i) \11\{s(x_i,y_i) = \hat{g}(x_i,y_i)\} \geq 0.
    %\]
    %Therefore, we have shown that for any $g \in \calG$
    %\[
    %\EE\left[ g(x_{n+1},y_{n+1})\left( \11\left\{ y_{n+1} \in \calT(x_{n+1})\right\}-(1-\alpha)\right)\right] \geq 0.
    %\]
    
    Now, we want to provide an upper bound for our objective. Recall that from the theorem formulation $D_{+} = \{(X_i,Y_i)\}_{i\in [n+1]}$ and let $E_{+} = \{\varepsilon_i\}_{i \in [n+1]}$. Since $v_i^* \geq \alpha - 1$, we get that 
    \begin{align*}
    |\EE_{D,E}[\wcovdev(\calC, \alpha,w)]| &=
     \left|\EE \left[ \frac{1}{n+1} \sum_{i \in [n+1] } (\alpha -v^*_i)w(X_i,Y_i) \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\right]\right| \\
     &\leq \EE \left[ \frac{1}{n+1} \sum_{i \in [n+1] }|w(X_i,Y_i)| \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\right] \\
     &\leq \EE \left[ \left(\max_{j \in [n+1]}|w(X_j,Y_j)|\right)\frac{1}{n+1} \sum_{i \in [n+1] } \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\right]\\
     &=\EE_{D_{+}}\left[\EE_{E_{+}} \left[ \left(\max_{j \in [n+1]}|w(X_j,Y_j)|\right)\frac{1}{n+1} \sum_{i \in [n+1] } \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\Bigg|\{\Phi(X_i, Y_i)\}_{i \in [n+1]}\right] \right]\\
     &=\EE_{D_{+}}\left[\left(\max_{j \in [n+1]}|w(X_j,Y_j)|\right)\frac{1}{n+1}\EE_{E_{+}} \left[  \sum_{i \in [n+1] } \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\}\Bigg|\{\Phi(X_i, Y_i)\}_{i \in [n+1]}\right] \right]
    \end{align*}
    
     We will now bound the inner expectation of the above expression by showing that conditioning on $\{\Phi(X_i,Y_i)\}_{i \in [n+1]}$
     \[
     \PP \left[ \sum_{i \in [n+1] } \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\} > d \Bigg| \{\Phi(X_i,Y_i)\}_{i \in [n+1]}\right] =0.
     \] 
    In more detail, we can upper bound this probability as follows
    \begin{align*}
        &\PP \left[ \sum_{i \in [n+1] } \11\{\tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)\} > d \Bigg| \{\Phi(X_i,Y_i)\}_{i \in [n+1]}\right] =\\
        & \PP \left[ \exists 1 \leq j_1 < \ldots < j_{d+1} \leq n+1 \text{ s.t. } \forall i \in [d+1], \tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i)  | \{\Phi(X_i,Y_i)\}_{i \in [n+1]} \right] \leq\\
        & \sum_{1 \leq j_1 < \ldots < j_{d+1} \leq n+1} \PP \left[\forall i \in [d+1], \tS_i = \hat{q}_{Y_{n+1}}(X_i,Y_i) | \{\Phi(X_i,Y_i)\}_{i \in [n+1]} \right]\leq\\
        & \sum_{1 \leq j_1 < \ldots < j_{d+1} \leq n+1} \PP \left[\exists w \in \calW  \text{ s.t. }\forall i \in [d+1], \tS_i = w(X_i,Y_i)  | \{\Phi(X_i,Y_i)\}_{i \in [n+1]} \right] =\\
        & \sum_{1 \leq j_1 < \ldots < j_{d+1} \leq n+1} \PP \left[\exists \beta \in \RR^d \text{ s.t. } \forall i \in [d+1], \tS_i= \sum_{k \in [d]}\beta_k\Phi_k(X_i,Y_i) \Bigg| \{\Phi(X_i,Y_i)\}_{i \in [n+1]}\right] =\\
        &  \sum_{1 \leq j_1 < \ldots < j_{d+1} \leq n+1} \PP [ (\tS_{j_1}, \ldots, \tS_{j_{d+1}}) \in \text{RowSpace}([\Phi(X_{j_1},Y_{j_1})|\ldots|\Phi(X_{j_{d+1}}, Y_{j_{d+1}})] )| \{\Phi(X_i,Y_i)\}_{i \in [n+1]}].
    \end{align*}
    We notice that $R\left(\left[\Phi(X_{j_1},Y_{j_1})|\ldots|\Phi(X_{j_{d+1}}, Y_{j_{d+1}})\right]\right)$ is a $d$-dimensional subspace of $\RR^{d+1}$. Since for fixed $\{\Phi(X_i,Y_i)\}_{i \in [n+1]}$ the scores $\tS_{j_1}, \ldots, \tS_{j_{d+1}}$ are independent and continuously distributed, we have that for all $1 \leq j_1 < \ldots < j_{d+1} \leq n+1$
   \[
   \PP \left[ \left(\tS_{j_1}, \ldots, \tS_{j_{d+1}}\right) \in R\left(\left[\Phi(X_{j_1},Y_{j_1})|\ldots|\Phi(X_{j_{d+1}}, Y_{j_{d+1}})\right] \right)| \{\Phi(X_i,Y_i)\}_{i \in [n+1]}\right] = 0.
   \]

     Combining the inequalities of the steps above, we have proven that for all $w \in \calW$
     \[
     |\EE_{D,E} \left[ \wcovdev(\calC, \alpha, w)\right] |\leq \frac{d}{n+1} \EE_{D_{+}}\left[ \max_{i \in [n+1]}|w(X_i,Y_i)|\right].
     \]
\end{proof}


\section{Additional Experimental Details}
We use Histogram-based Gradient Boosting Tree through the implementation of scikit-learn~\citep{Pedregos11}. Specifically, we use the HistGradientBoostingClassifier to train the basis weight functions for Kandinsky conformal prediction. We use HistGradientBoostingRegressor to train the base model for ACSIncome. We apply default hyperparameters suggested by scikit-learn except that we set max\_iter to 250.

\subsection{ACSIncome}
\label{subsec:app_acs}
We preprocess the dataset following \citet{LW0N23}. We additionally apply logarithmic transformation of labels with base 10 and scale the label to $[0,1]$ by min-max scaling.

We train the base Gradient Boosting Tree regressor on 31,000 samples with 10,000 from each state. The calibration set contains 4,000 samples per state and the test set contains 2,000 samples per state. Given a number of groups $|\calG|$, we select samples from states with the $|\calG|$ smallest indices. We train the basis of Kandinsky's weight function class on the training set with selected states, but the base predictor is not retrained. We also filter the calibration and test set for selected states. We repeat the experiments 100 times by reshuffling the calibration and test set, but the training set is fixed such that Kandinsky's weight function class is also fixed for a given number of selected states.

We take the deterministic score function of Conformalized Quantile Regression (CQR). Given base predictors $f_{\alpha/2}(x)$ and $f_{1-\alpha/2}(x)$ for the $\alpha/2$ and $1-\alpha/2$ quantile, respectively,
\begin{align*}
    S(x,y) = \max \sth{ y-f_{1-\alpha/2}(x),  f_{\alpha/2}(x) - y}, \quad \tS = S. 
\end{align*}

\subsection{CivilComments}
\label{subsec:app_civil}
Following \citet{Koh21}, we split the dataset into 269,038 training samples and 178,962 samples for calibration and test. We finetune a DistilBERT-base-uncased model with a classification head on the training set, following the configurations of \citet{Koh21}. We randomly redistribute samples between the calibration and test sets to vary the calibration sample sizes. Since the groups are overlapping, we estimate the ratio between the group average sample size and the overall sample number of 178,962. Then we downsample the dataset accordingly to approximately reach a prescribed group average sample size in the calibration set. We repeat the redistribution procedure 100 times, but the training set is fixed such that the DistilBert model is trained only once. However, we train the weight functions of Kandinsky conformal prediction on the calibration set, since we are considering the setup where training samples are only accessible to algorithms by the base predictor. Therefore, the weight function class is retrained for each calibration set. Since group sample sizes can be different between runs, in \Cref{fig:main_e,fig:main_f}, annotated group sample sizes are estimated by the mean of actual group sample sizes over 100 runs.  

We take the randomized score function of Adaptive Prediction Sets (APS). Given the base classifier $f(x)$ that outputs the probability vector for all classes, we sort their probabilities in decreasing order.
\begin{align*}
    f_{(1)}(x) \geq f_{(2)}(x) \geq ... \geq f_{|\calY|}(x).
\end{align*}
We use $f_y(x)$ to represent the component of $f(x)$ for the class $y$ and $k_x(y)$ to represent the order of $f_y(x)$ such that $f_{(k_x(y))}(x)=f_y(x)$. The non-conformity score is given by
\begin{align*}
    \tS(x,y, \varepsilon) = \sum_{k=1}^{k_x(y)-1} f_{(k)}(x) + \varepsilon f_y(x), \quad \varepsilon\sim \text{Uniform}[0,1].
\end{align*}
