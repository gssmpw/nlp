\section{Introduction}


\begin{comment}
Core idea: group conditional conformal prediction. 


Structure:
\begin{enumerate}
    \item motivation: conformal prediction, uncertainty quantification, important ..., conditioning, not enough for conditional and class-conditional, concrete example (Konstantina)
    
    \item contribution: core contribution is 1) general overlapping group conditional conformal prediction (formulation); 2) reconciling statistical and computational efficiency. split conformal prediction, what we do, it is more computationally efficient; full conformal prediction, the other way; 3) implications for distribution shift; 
    (Jiayun)

    \item 3 bullet points: 1) split 2) full (comparing to previous papers) 3) distributiuon shifts 4) experiments (Jiayun)

    \item related work (Konstantina)
\end{enumerate}
\end{comment}

% Conformal prediction \cite{AlgorithmicLearning2005, lei2014distribution, papadopoulos2002inductive} is a framework to generate prediction sets for any given predictor $f: \cal X \to \cal Y$ for any arbitrary distribution of samples $\cal D$ over $\cal X \times \cal Y$. Its objective is to use the predictor $f$ and samples from $\cal D$ to construct a prediction set function $\calC$ that, given a feature vector $X$, outputs a subset of $\cal Y$ that includes the true label $Y$ with probability at least $1-\alpha$, for $\alpha \in [0,1]$. Ideally, the characteristics of the set $\calC(X)$, such as the size of the set, convey the uncertainty of $f$ in the prediction $f(X)$. 


Conformal prediction \citep{AlgorithmicLearning2005, LW14, papadopoulos2002inductive} is a framework for constructing prediction sets with formal coverage guarantees. Given a predictor $f: \cal X \to \cal Y$ and samples from a distribution $\cal D$, the goal is to build a prediction set function $\calC$ that, for a given covariate vector $X$, outputs a subset of $\cal Y$ guaranteed to include the true label $Y$ with a target probability. Ideally, the size of $\calC(X)$ reflects the uncertainty in $f(X)$.


Specifically, using a trained predictor $f$, a calibration dataset $\{(X_i, Y_i)\}_{i \in [n]}$, and the covariate vector of a test example $X_{n+1}$, a common objective is to construct $\calC(X_{n+1})$ such that with high probability over the draws of the calibration dataset and any internal randomness of $\calC$, the following \emph{marginal coverage guarantee} holds:
% More concretely, we are given an already trained predictor $f$, a calibration data set $\{(X_i, Y_i)\}_{i\in [n]}$, and a test sample $(X_{n+1}, Y_{n+1})$. Given the covariate vector of the test sample, $X_{n+1}$, we want to predict a set $\calC(X_{n+1})$ such that, with high probability over the randomness of the calibration data $\{(X_i,Y_i)\}_{i \in [n]}$ that are drawn independently from $\calD$ and any potential internal randomness of $\calC$, 
\begin{equation}
\label{eq:marginal_guarantee}
\mathbb P_{(X_{n+1},Y_{n+1}) \sim \cal D} \left[ Y_{n+1} \in \calC(X_{n+1}) \right] = 1-\alpha,
\end{equation}
for some $\alpha$ in $(0,1)$.\footnote{ In~\citep{V12} this type of coverage is called training conditional validity for conformal prediction. Another type of guarantee that is common in the conformal prediction literature is $\mathbb \PP \left[ Y_{n+1} \in \calC(X_{n+1}) \right] = 1-\alpha$, where the probability is taken over all $n+1$ points and the internal randomness of $\calC$ (e.g. see \citet{GCC2023}).}
% The training conditional version is stronger and can rule out several trivial solutions. For example, a prediction set $\cal C$ that ignores the input, and outputs $\calY$ with probability $1-\alpha$ or the empty set otherwise satisfies the expected coverage guarantee, but not the high probability version.}
This guarantee ensures coverage “on average” over test samples $(X_{n+1}, Y_{n+1})$ drawn from $\cal D$.
 % This is a \textit{marginal coverage guarantee}, meaning that the specified coverage holds ``on average'' over test samples $(X,Y)$ drawn from $\cal D$. 
However, a limitation of marginal coverage is that the prediction sets might undercover certain values of $(X_{n+1}, Y_{n+1})$ while overcovering others. A potential solution is to require $(1-\alpha)$-coverage conditioning on the value of $(X_{n+1}, Y_{n+1})$. Unfortunately, we generally cannot obtain non-trivial prediction sets that satisfy such pointwise coverage~\citep{V12, LW14, BCRT21}.
 % A similar result holds for conditioning on continuously distributed $Y_{n+1}$.

However, various types of conditional coverage guarantees can still be achieved. For instance, Mondrian conformal prediction \citep{VLNG03} ensures the $(1-\alpha)$-coverage guarantees over a disjoint set of subgroups $\cal G$ defined over both the covariate $X_{n+1}$ and the label $Y_{n+1}$. The method is named after Piet Mondrian, as its partitioning the space $\cal X \times \cal Y$ into a finite set of non-overlapping groups $\calG$ mirrors Mondrian’s iconic compositions of disjoint rectangles. 
\iffalse 
Within each group, the coverage guarantee holds: for every group \( G \in \calG \),
% Mondrian conformal prediction, introduced in \cite{VLNG03}, is such a guarantee, that we condition on discrete events that depend on both the features and the label of the sample. In Mondrian conformal prediction, the space $\cal X \times \cal Y$ is partitioned into a finite number of (non-overlapping) groups $\calG$ and for every group $G \in \calG$ 
\begin{equation}
    \label{eq:groupcond_guarantee}
    \mathbb P_{(X_{n+1}, Y_{n+1}) \sim \cal D} \left[ Y_{n+1} \in \calC(X_{n+1}) \mid (X_{n+1},Y_{n+1}) \in G\right] = 1-\alpha.
\end{equation}
\fi
Class-conditional conformal prediction (for classification) \citep{LBLJ15, DABJT23} can be viewed as a special case of Mondrian conformal prediction, where each label defines a distinct group.

Yet, the disjoint group assumption in Mondrian conformal prediction has notable limitations, as real-world subpopulations of interest often overlap. This is particularly relevant in algorithmic fairness, where protected subgroups--typically defined by combinations of demographic attributes such as race and gender---naturally intersect \citep{kearns18a,HKRR18}. An alternative line of research addresses this challenge by developing conditional coverage guarantees for overlapping groups \citep{JNRR2023, GCC2023}. However, both of their results restrict the group functions to depend solely on the covariate vector $X_{n+1}$.




In this paper, we expand the scope of conditional coverage guarantees by achieving the best of both worlds from these two lines of prior work. Building on \citet{GCC2023, JNRR2023}, we provide coverage guarantees for overlapping groups while, in the spirit of Mondrian conformal prediction \citep{VLNG03}, allowing grouping functions to depend jointly on both the covariates $X_{n+1}$  and the label  $Y_{n+1}$. More generally, our guarantees hold for fractional groups, where membership is defined as a probabilistic function over  $X_{n+1}$ and  $Y_{n+1}$. This added flexibility enables the modeling of subgroups based on protected attributes that are not explicitly observed but can be inferred through covariates and labels~\citep{RBSC20}. We name our method \emph{Kandinsky Conformal Prediction}, drawing inspiration from Wassily Kandinsky’s compositions of overlapping geometric forms, as we provide conditional coverage guarantees for groups that are flexible, overlapping, and probabilistic.

Similar to \citet{GCC2023}, our conditional guarantee can be leveraged to obtain valid coverage guarantees under distribution shift, provided that the density ratio between the test and calibration distributions is captured by one of the group functions we consider. While \citet{GCC2023} is limited to handling covariate shift—since their group functions depend solely on  $X$ —our approach extends to a broader class of distribution shifts affecting both  $X$  and $Y$. 


% \citet{GCC2023} further extends these guarantees to fractional groups, defined by probabilistic group membership functions.

\iffalse
This group-conditional coverage can also be formulated as a weighted conformal prediction objective. For this, we define a class of weights $\calW$ where for every $G \in \calG$ there is a function $w_G(X,Y) = \11\{(X,Y) \in G\}$. Let $\wcovdev(\calC, \alpha, w;\calD)$ be the weighted coverage deviation of prediction set $\calC$, \mbox{i.e.}
\begin{align*}
    &\wcovdev(\calC, \alpha, w) \coloneqq \\
    &\quad\EE\left[w(X_{n+1},Y_{n+1})(\11\{Y_{n+1} \in \calC(X_{n+1})\}-(1-\alpha))\right],
\end{align*}
where the expectation is taken over the randomness of the test point $(X_{n+1},Y_{n+1})$ and the prediction set $\calC$.
Then, the objective is for every $w \in \cal W$ to achieve
\begin{equation}
\label{eq:weighted_guarantee}
\wcovdev(\calC, \alpha, w)= 0.
\end{equation}

We can achieve coverage guarantees stronger than Mondrian conformal prediction by satisfying \Cref{eq:weighted_guarantee} for more general classes of weights $\calW$. For example, this formulation can address potentially overlapping groups and fractional group membership.  


 
Group-conditional coverage has been studied for covariate-based groups~\citep{GCC2023, JNRR2023}. Specifically, \cite{JNRR2023} perform quantile regression to obtain group-conditional coverage for groups that, unlike Mondrian conformal prediction, can overlap. \cite{GCC2023} obtain the weighted coverage guarantee for more general classes of weight functions of the covariates, which can also handle fractional group membership functions and covariate distribution shift.

%\Jeffreycomment{What's done: upenn, stanford (conditioning on covariates), mondrian (depending on both x and y) but only with disjoint groups. What's missing: 1. overlapping groups (multigroup fairness), clean examples; 2. conditioning on event jointly decided by x and y. Beyond: Asian male of low incomes (protective attribute).}


In this work, we provide a framework that achieves the conditional coverage in \Cref{eq:weighted_guarantee} for a more general class of weight functions that depend on both the covariates and the class. This formulation addresses conditioning on group membership functions that are determined by both the covariates and the class, as well as potentially overlapping groups and fractional group membership functions.


Overlapping groups naturally arise when considering multiple group partitions of the population. For instance, we can define demographic groups based on attributes such as race, gender, or age when we are concerned about the fairness of the conformal predictor. 
On the other hand, conditioning on groups that are not fully determined by $x$ and $y$ leads to fractional group membership. For example, we can desire consistent coverage across demographic groups for fairness concerns, but the conditioned events may sometimes be protected attributes that are inaccessible for decision-making by legislative restrictions~\citep{RBSC20}. Finally, conditioning on events jointly determined by $x$ and $y$ allows us to extend these concepts to groups that also depend on the class of the sample. 






The conditional guarantee in Equation~\ref{eq:weighted_guarantee} also implies valid coverage when the prediction set is sampled from a subpopulation of the training distribution, causing \emph{distribution shift} between the training and test distributions. Taking $g$ as the Radon–Nikodym derivative between the test and training distributions, Equation~\ref{eq:cond_guarantee} is equivalent to the marginal coverage for samples under the test distribution $\calD_T$.
\begin{align*}
	\PP_{(x,y) \sim \calD_{T}} \left[ \11\{y \in \calC(x)\} \right]= 1-\alpha.
\end{align*}
The distributions between $\calD$ and $\calD_T$ can be different for both features (covariates) and the label because we define the group function $g$ on both $x$ and $y$, which extends the results of \citet{GCC2023} for covariate distribution shift. Furthermore, we can guarantee marginal coverage for multiple test distributions by incorporating all relevant Radon–Nikodym derivatives into the group set $\calG$.
\fi



Our algorithm is computationally efficient, which relies on solving a linear quantile regression over the vector space spanned by a set of group functions $\calG$. Similar to \citet{JNRR2023}, it preserves the key advantage of requiring only a single quantile regression model that can be applied to all test examples, rather than needing to solve a separate regression for each instance as in \citet{GCC2023}. We show that our method achieves a high-probability conditional coverage error rate of 
$\mathcal O(n_{\calG}^{-1/2})$, where $n_{\calG}$ is the number of samples per group on average, matching the minimax-optimal rate \citep{ACDR24}.
In the special case where group functions only depend on the covariates,  our bound significantly improves the $\tilde{ \mathcal  O}(n_{\calG}^{-1/4})$ coverage error bound in \citet{RO22, JNRR2023} in its dependence on $n_{\calG}$, and sharpen the dependence on $|\calG|$  when compared to the $\mathcal O(n_{\calG}^{-1/2}(1 + |\calG|^{1/2}n_{\calG}^{-1/2}))$ bound in \citet{ACDR24}.


% Technically, we approach the general group conditional guarantee by vanilla linear quantile regression over the vector space spanned by the group functions in $G$, which is an efficient post-processing procedure for any conformal prediction technique. We show that the conditional coverage in Equation~\ref{eq:weighted_guarantee} is valid for finite samples of size $n$, and the training conditional coverage converges to the desired level with the rate of $\mathcal O(\sqrt{|G|\cdot\log n/n})$, which is almost the minimax optimal rate $\mathcal O(\sqrt{|G|/n})$ for covariate-based groups~\citep{ACDR24}. This significantly improves the $\mathcal O((|G|\cdot \log n/n)^{-1/4})$ coverage error bound conditioned on covariate-based groups with binary memberships, established for regularized quantile regression~\citep{RO22, JNRR2023}. Meanwhile, we provide more general group conditional coverage for the simpler vanilla quantile regression.



To complement our main algorithm, which ensures training-conditional validity, we also provide a \emph{test-time inference} algorithm that obtains expected coverage that also takes randomness over the draws on calibration data. By extending \citet{GCC2023}’s approach to accommodate group functions defined over both covariates and labels, we obtain an expected coverage error bound of $\mathcal O(n_{\calG}^{-1})$. However, this result comes at the cost of computational efficiency, as the algorithm needs to perform test-time inference that solves a separate quantile regression for every new test example.

% . The symmetry of the algorithm over all samples leads to an exchangeable distribution of the quantile estimates. We provide an exact, distribution-free lower bound for the classic non-training-conditional coverage of the algorithm, by taking expectation of Equation~\ref{eq:weighted_guarantee} over the randomness of the training data. The algorithm sacrifices computational efficiency because the regression procedure must be executed for each new test sample.

Our contribution can be summarized as follows.
\begin{enumerate}[leftmargin=*]
    % \item We propose and study the formulation of the most general group conditional coverage guarantee for conformal prediction, considering overlapping groups and fractional group membership. Our formulation captures groups determined by covariates, labels, the joint of features and labels, or groups not fully determined by features and labels. The conditional guarantee implies valid coverage under general subpopulation distribution shifts.


   \item  We study the most general formulation of group-conditional coverage guarantees in conformal prediction, handling overlapping groups and fractional group memberships that are defined jointly by covariates and labels. This conditional guarantee can be used to obtain valid coverage under general subpopulation distribution shifts.
    
\item We propose Kandinsky Conformal Prediction, a method that is both computationally efficient and statistically optimal. Computationally, it requires solving a single quantile regression over the vector space spanned by group functions. Statistically, it attains the minimax-optimal group-conditional coverage error with finite samples.

   \item We evaluate our algorithm on real-world tasks, including income prediction and toxic comment detection. Our results show that Kandinsky Conformal Prediction consistently achieves the best-calibrated conditional coverage while scaling effectively with the number of groups.

   % Our analysis examines conditional coverage for geographical groups with fractional memberships and overlapping demographic groups that depend jointly on covariates and labels. Thea
\end{enumerate}

\begin{comment}
Content:
\begin{enumerate}
    \item Algorithms for shift captured by $g(x,y)$,  an extension of the algorithm by UPenn 
    \item Results when $g(x,y)$ has finite dimension
    \item Applications of this type of distribution shift:
    \begin{itemize}
        \item group conditional guarantees
        \item concept shift
        \item class conditional guarantees
        \item combine the above
        \item groups defined on both x and y
    \end{itemize}
    \item Experiments
\end{enumerate}



Discussions:
\begin{enumerate}
    \item A naive approach: conformal prediction separately for each individual group. This naive approach works for non-overlapping groups and has the same coverage bound as ours.
    \item Where does stanford algorithm have an advantage over upenn? Distribution free? No: Exact coverage.
    \item Does our complete conformal prediction algorithm have training-conditional coverage? Without assumptions on the score functions, the (original) full conformal prediction may not have training-conditional coverage. Theorem: For any distribution, there exists a score function such that with probability at least $\alpha$ over sampling, $y$ is not in the conformal prediction interval.
\end{enumerate}

\end{comment}

\subsection{Related Work}
% The literature on conformal prediction is extensive, encompassing various types of coverage guarantees, methods, and applications. There exist several textbooks and surveys that cover the topic in depth \cite{AlgorithmicLearning2005, SV08, BHV14, ABB24}. One of the most well-established conformal prediction methods is split conformal prediction \cite{PPVG02, LGRTW18}, which achieves a marginal coverage guarantee. Various conformal prediction methods, including split conformal prediction and the algorithms presented in this paper, provide guarantees independent of the choice of score function. In parallel, there is a more application-focused body of work that explores different non-conformity scores tailored to specific prediction tasks, including regression \cite{LeiRW2013, RPC19, IzbickiSS20} and classification \cite{SLW19, ABMJ20, RSC20}.


The literature on conformal prediction is vast, covering a range of coverage guarantees, methods, and applications. Several textbooks and surveys provide in-depth discussions \citep{AlgorithmicLearning2005, SV08, BHV14, ABB24}. A classical method is split conformal prediction \citep{PPVG02, LGRTW18}, which ensures marginal coverage. A major advantage of conformal prediction is that the coverage guarantees hold regardless of the choice of non-conformity score functions. Meanwhile, another line of research explores specialized non-conformity scores for specific tasks, such as regression \citep{LeiRW2013, RPC19, IzbickiSS20} and classification \citep{SLW19, ABMJ20, RSC20}.


Our work is closely related to the expanding body of research in conformal prediction that establishes coverage guarantees conditioned on events involving the test example $(X_{n+1}, Y_{n+1})$. A fundamental class of such events corresponds to disjoint subgroups. For instance, Mondrian conformal prediction \citep{VLNG03} partitions the space  $\mathcal{X} \times \mathcal{Y}$ into disjoint regions, encompassing class-conditional \citep{LBLJ15, DABJT23} and sensitive-covariate-conditional \citep{RBSC20} conformal prediction as special cases. 

We introduce Kandinsky Conformal Prediction as an extension of the Mondrian approach, providing coverage guarantees for overlapping group structures. Prior work on overlapping group-conditional coverage primarily considers groups defined solely by covariates  $X_{n+1}$. \citet{BCRT21} addresses this by assigning the most conservative prediction set to points in multiple groups. Our algorithm builds on the quantile regression techniques of \citet{JNRR2023} and \citet{GCC2023}, which compute per-example thresholds on non-conformity scores. Additionally, \citet{ACDR24} establishes a lower bound on group-conditional coverage error rates. Since we generalize to group functions that depend on both covariates and labels, their bound applies to our setting and shows that our error rate is minimax-optimal.


% \citet{JNRR2023} use quantile regression on calibration scores to construct prediction sets with high-probability coverage guarantees. \citet{GCC2023} reformulate conditional coverage as weighted conformal prediction, enabling target coverage for overlapping groups and distribution shifts. \cite{ACDR24} further analyzes the expected coverage rate in \citet{JNRR2023}, proving lower bounds for weight function classes with bounded VC dimension.
 % For example, \cite{BCRT21} presents a method for overlapping groups that operates on each group separately and assigns the most conservative prediction set to poaints that belong to multiple groups. 
% \citet{JNRR2023} perform quantile regression on the calibration data scores to construct prediction sets and show that their method admits a high-probability coverage bound. \citet{GCC2023} reformulate the conditional coverage objective as a weighted conformal prediction. This allows them to obtain the target coverage for both overlapping groups and classes of distributions shifts, by performing quantile regression that also takes into account the score of the test point. \cite{ACDR24} analyzes the expected coverage rate of the quantile regression in \citet{JNRR2023} and proves a lower bound for the expected coverage for weight function classes with bounded VC dimension. 



% The most closely related works to ours are those that provide coverage guarantees conditioned on events dependent on the covariates or the class of the test point.Class-conditional conformal prediction is studied in \cite{LBLJ15}, while  \cite{DABJT23} proposes a class-conditional conformal prediction method that integrates a clustering component for classification tasks with many classes.
% Concurrently, there is a significant line of work on conformal prediction conditioned on groups defined by the test point covariates. \cite{RBSC20} addresses the importance of group conditional conformal prediction from a fairness perspective and provides a method for disjoint groups. 

% For example, \cite{BCRT21} presents a method for overlapping groups that operates on each group separately and assigns the most conservative prediction set to points that belong to multiple groups. 
% \citet{JNRR2023} perform quantile regression on the calibration data scores to construct prediction sets and show that their method admits a high-probability coverage bound. \citet{GCC2023} reformulate the conditional coverage objective as a weighted conformal prediction. This allows them to obtain the target coverage for both overlapping groups and classes of distributions shifts, by performing quantile regression that also takes into account the score of the test point. \cite{ACDR24} analyzes the expected coverage rate of the quantile regression in \citet{JNRR2023} and proves a lower bound for the expected coverage for weight function classes with bounded VC dimension. 

% Separately, Mondrian conformal prediction \cite{VLNG03} ensures group conditional coverage for disjoint groups that depend on both $X_{n+1}$ and $Y_{n+1}$. Our proposed framework advances this line of work by providing conditional guarantees for more flexible group definitions. These can depend on both the covariates and the class of the test point, and can even be overlapping or probabilistic. Additionally, our analysis yields tighter bounds compared to prior work, offering significant improvements for certain applications.


\iffalse 
There are also alternative approaches, beyond coverage guarantees for predefined groups, to overcome the impossibility results for covariate conditional coverage. One such approach is localized conformal prediction \citep{G23,HB24}. Another approach is to modify the conformal prediction pipeline before computing the scores of the calibration set, as seen in \cite{GAO24, XIE24}. Furthermore, \cite{ZHG24} considers the coverage of latent subgroups by modeling the data distribution as a mixture of clusters.
\fi


\begin{comment}
\paragraph{Localized Conformal Prediction}\Jeffreycomment{What about approximate conditional coverage? These papers do not depend on predefined subgroups and pursue approximate $(|X)$ coverage.}
\Jeffreyedit{Localized Conformal Prediction}:  \\
\Jeffreyedit{These papers directly optimize the score function for some measures of conditional coverage } \\
\Jeffreyedit{This paper is conditioned on latent subgroups, modeling the distribution as a mixture of clusters \cite{ZHG24}.}
\end{comment}

Thematically, our work is related to a long line of work in multi-group fairness \citep{kearns18a,HKRR18, KNRW19}. In particular, our algorithm can be seen as predicting a target quantile conditioned on covariates and labels while satisfying the \emph{multiaccuracy} criterion \citep{KGZ19, RO22}.

% An important component of our algorithm is the computation of quantiles that are accurate conditioned on certain events. In the group-conditional case, this is the quantile equivalent to computing multiaccurate means\cite{KGZ19, HKRR18}. This approach fits within the broader literature on multi-group fairness, which provides group-conditional guarantees for overlapping demographic groups across various machine learning and statistical tasks \cite{kearns18a, KKGKR22}.

Finally, our work advances the study of conformal prediction under distribution shifts between calibration and test data. Existing research largely focuses on specific cases, such as covariate shift \citep{TBCR19, QDT23, YKT24, PNI24} or label shift \citep{PR21}, where they consider a single target distribution. By introducing a more general class of group functions that depend jointly on covariates and labels, we establish coverage guarantees that remain valid under broader distribution shifts over $\cal X \times \cal Y$, including multiple potential target distributions.

% Weighted variants of conformal prediction are commonly used to handle distribution shifts between the calibration and the test data. Various works have developed methods a single covariate shift \cite{TBCR19, QDT23,YKT24,PNI24} or a single label Finally, the work of \citet{PR21} addresses conformal prediction a single label shift. Our framework can handle a class of distribution shifts, where the distribution is allowed to change for both the covariates and the class.