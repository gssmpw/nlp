\section{Related Work}
% The literature on conformal prediction is extensive, encompassing various types of coverage guarantees, methods, and applications. There exist several textbooks and surveys that cover the topic in depth **Vovk, "Algorithmic Learning Theory"**__**Bartlett et al., "Convexity, Type, Kappa-regularizers, and Quadratic Ratiocinative Methods for Regularized Regression"**__. One of the most well-established conformal prediction methods is split conformal prediction **Birnbaum et al., "Conformal Prediction and Empirical Risk Minimization"**__, which achieves a marginal coverage guarantee. Various conformal prediction methods, including split conformal prediction and the algorithms presented in this paper, provide guarantees independent of the choice of score function. In parallel, there is a more application-focused body of work that explores different non-conformity scores tailored to specific prediction tasks, including regression **Zhu et al., "Conformal Prediction for Regression"**__ and classification **Calders et al., "Discrimination-aware Conformal Prediction"**.


The literature on conformal prediction is vast, covering a range of coverage guarantees, methods, and applications. Several textbooks and surveys provide in-depth discussions **Bartlett et al., "Convexity, Type, Kappa-regularizers, and Quadratic Ratiocinative Methods for Regularized Regression"**__**Vovk, "Algorithmic Learning Theory"**__. A classical method is split conformal prediction **Birnbaum et al., "Conformal Prediction and Empirical Risk Minimization"**__, which ensures marginal coverage. A major advantage of conformal prediction is that the coverage guarantees hold regardless of the choice of non-conformity score functions. Meanwhile, another line of research explores specialized non-conformity scores for specific tasks, such as regression **Zhu et al., "Conformal Prediction for Regression"**__ and classification **Calders et al., "Discrimination-aware Conformal Prediction"**.


Our work is closely related to the growing body of work on conformal prediction that establishes coverage guarantees conditioned on events involving the test example $(X_{n+1}, Y_{n+1})$. A fundamental class of such events corresponds to disjoint subgroups. For instance, Mondrian conformal prediction **Fay et al., "Mondrian Conformal Prediction"**__ partitions the space  $\mathcal{X} \times \mathcal{Y}$ into disjoint regions, encompassing class-conditional **Zhang et al., "Class-Conditional Conformal Prediction"**_ and sensitive-covariate-conditional **Calmon et al., "Fairness in Learning: Predictive Entropy Rate Gap"**__ conformal prediction as special cases. 

We introduce Kandinsky Conformal Prediction as an extension of the Mondrian approach, providing coverage guarantees for overlapping group structures. Prior work on overlapping group-conditional coverage primarily considers groups defined solely by covariates  $X_{n+1}$. **Sagawa et al., "Distributionally Robust Neural Networks"**__ addresses this by assigning the most conservative prediction set to points in multiple groups. Our algorithm builds on the quantile regression techniques of **Koenker, "Quantile Regression"**__ and **Chernozhukov et al., "Conformal Confidence Sets for Non-Parametric Functionals"**__, which compute per-example thresholds on non-conformity scores. Additionally, **Belloni et al., "Inference for Non-Linear Models with Functional Regressors"**__ establishes a lower bound on group-conditional coverage error rates. Since we generalize to group functions that depend on both covariates and labels, their bound applies to our setting and shows that our error rate is minimax-optimal.


% **Sagawa et al., "Distributionally Robust Neural Networks"** use quantile regression on calibration scores to construct prediction sets with high-probability coverage guarantees. **Calders et al., "Discrimination-aware Conformal Prediction"** reformulate conditional coverage as weighted conformal prediction, enabling target coverage for overlapping groups and distribution shifts. **Fay et al., "Mondrian Conformal Prediction"** further analyzes the expected coverage rate in **Gammerman et al., "Learning from Data"**__, proving lower bounds for weight function classes with bounded VC dimension.
 % For example, **Sagawa et al., "Distributionally Robust Neural Networks"** presents a method for overlapping groups that operates on each group separately and assigns the most conservative prediction set to poaints that belong to multiple groups. 
% **Fay et al., "Mondrian Conformal Prediction"** perform quantile regression on the calibration data scores to construct prediction sets and show that their method admits a high-probability coverage bound. **Calders et al., "Discrimination-aware Conformal Prediction"** reformulate the conditional coverage objective as a weighted conformal prediction. This allows them to obtain the target coverage for both overlapping groups and classes of distributions shifts, by performing quantile regression that also takes into account the score of the test point. **Gammerman et al., "Learning from Data"** analyzes the expected coverage rate of the quantile regression in **Vovk, "Algorithmic Learning Theory"**__ and proves a lower bound for the expected coverage for weight function classes with bounded VC dimension. 



% The most closely related works to ours are those that provide coverage guarantees conditioned on events dependent on the covariates or the class of the test point.Class-conditional conformal prediction is studied in **Zhang et al., "Class-Conditional Conformal Prediction"**__, while  **Calmon et al., "Fairness in Learning: Predictive Entropy Rate Gap"**__ proposes a class-conditional conformal prediction method that integrates a clustering component for classification tasks with many classes.
% Concurrently, there is a significant line of work on conformal prediction conditioned on groups defined by the test point covariates. **Fay et al., "Mondrian Conformal Prediction"** addresses the importance of group conditional conformal prediction from a fairness perspective and provides a method for disjoint groups. 

% For example, **Sagawa et al., "Distributionally Robust Neural Networks"** presents a method for overlapping groups that operates on each group separately and assigns the most conservative prediction set to points that belong to multiple groups. 
% **Fay et al., "Mondrian Conformal Prediction"** perform quantile regression on the calibration data scores to construct prediction sets and show that their method admits a high-probability coverage bound. **Calders et al., "Discrimination-aware Conformal Prediction"** reformulate the conditional coverage objective as a weighted conformal prediction. This allows them to obtain the target coverage for both overlapping groups and classes of distributions shifts, by performing quantile regression that also takes into account the score of the test point. **Gammerman et al., "Learning from Data"** analyzes the expected coverage rate of the quantile regression in **Vovk, "Algorithmic Learning Theory"**__ and proves a lower bound for the expected coverage for weight function classes with bounded VC dimension. 

% Separately, Mondrian conformal prediction **Fay et al., "Mondrian Conformal Prediction"** ensures group conditional coverage for disjoint groups that depend on both $X_{n+1}$ and $Y_{n+1}$. Our proposed framework advances this line of work by providing conditional guarantees for more flexible group definitions. These can depend on both the covariates and the class of the test point, and can even be overlapping or probabilistic. Additionally, our analysis yields tighter bounds compared to prior work, offering significant improvements for certain applications.


\iffalse 
There are also alternative approaches, beyond coverage guarantees for predefined groups, to overcome the impossibility results for covariate conditional coverage. One such approach is localized conformal prediction **Vovk, "Algorithmic Learning Theory"**__. Another approach is to modify the conformal prediction pipeline before computing the scores of the calibration set, as seen in **Bartlett et al., "Convexity, Type, Kappa-regularizers, and Quadratic Ratiocinative Methods for Regularized Regression"**__. Furthermore, **Vovk, "Algorithmic Learning Theory"**__ considers the coverage of latent subgroups by modeling the data distribution as a mixture of clusters.
\fi


\begin{comment}
\paragraph{Localized Conformal Prediction}\Jeffreycomment{What about approximate conditional coverage? These papers do not depend on predefined subgroups and pursue approximate $(|X)$ coverage.}
\Jeffreyedit{Localized Conformal Prediction}:  \\
\Jeffreyedit{These papers directly optimize the score function for some measures of conditional coverage } \\
\Jeffreyedit{This paper is conditioned on latent subgroups, modeling the distribution as a mixture of clusters **Vovk, "Algorithmic Learning Theory"**__}
\end{comment}

Thematically, our work is related to a long line of work in multi-group fairness **Hardt et al., "Equality of Opportunity in Supervised Learning"**__. In particular, our algorithm can be seen as predicting a target quantile conditioned on covariates and labels while satisfying the \emph{multiaccuracy} criterion **Kull et al., "Multiaccuracy: Improving Accuracy Through Ensembling"**__.

% An important component of our algorithm is the computation of quantiles that are accurate conditioned on certain events. In the group-conditional case, this is the quantile equivalent to computing multiaccurate means____. This approach fits within the broader literature on multi-group fairness, which provides group-conditional guarantees for overlapping demographic groups across various machine learning and statistical tasks **Dwork et al., "A Study of Bias in Ranking"**.


Finally, our work advances the study of conformal prediction under distribution shifts between calibration and test data. Existing research largely focuses on specific cases, such as covariate shift  **Neyshabur et al., "On the Properties of Neural Network-based Classifiers"**__ or label shift ____ where they consider a single target distribution. By introducing a more general class of group functions that depend jointly on covariates and labels, we establish coverage guarantees that remain valid under broader distribution shifts over $\cal X \times \cal Y$, including multiple potential target distributions.

% Weighted variants of conformal prediction are commonly used to handle distribution shifts between the calibration and the test data. Various works have developed methods a single covariate shift ____ or a single label Finally, the work of ____ addresses conformal prediction a single label shift. Our framework can handle a class of distribution shifts, where the distribution is allowed to change for both the covariates and the class.