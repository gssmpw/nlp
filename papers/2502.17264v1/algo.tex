\section{Kandinsky Conformal Prediction}
\begin{comment}
Content:
\begin{enumerate}
    \item Algorithms for shift captured by $g(x,y)$,  an extension of the algorithm by UPenn 
    \item Results when $g(x,y)$ has finite dimension
    \item Applications of this type of distribution shift:
    \begin{itemize}
        \item group conditional guarantees
        \item concept shift
        \item class conditional guarantees
        \item combine the above
        \item groups defined on both x and y
    \end{itemize}
\end{enumerate}
\end{comment}

\label{sec:algorithm}
In this section, we describe the components of the \emph{Kandinsky conformal prediction} framework. We will first present our method by extending the conditional conformal prediction algorithm by \citet{JNRR2023} to work with classes of finite-dimensional weight functions of the form $\calW = \{\Phi(\cdot)^T \beta : \beta \in \RR^d\}$, where $\Phi:\calX \times \calY \to \RR$ is a basis that is defined according to the desired coverage guarantee. After presenting our main result, we will demonstrate how this conformal prediction method can be tailored to different applications. The detailed proofs of our results from this section are in \Cref{sec: algorithm_app}.

 Given a coverage parameter $\alpha \in (0,1)$, a predictor $f:\calX \to \calY$, and a calibration dataset of $n$ examples $D = \{(X_i,Y_i)\}_{i\in [n]}$, drawn independently from a distribution $\calD$, the objective of {Kandinsky conformal prediction} is to construct a (possibly randomized) prediction set function $\calC(\cdot)$ such that, with high probability over the calibration dataset $D$ and the randomness of the method that constructs $\calC$, for every $w \in \cal W$
\begin{equation*}
\wcovdev(\calC, \alpha, w) \approx 0.
\end{equation*}
% Starting with the basics of conformal prediction, 
Given the predictor $f$, we use a non-conformity score function $S:\calX \times \calY \to \RR$ to measure how close the prediction $f(x)$ is to the label $y$ for any data point $(x,y) \in \calX \times \calY$. As is common in conformal prediction, our method ensures the desirable coverage for any score function $S$.

Our method consists of two steps: one is performed during training, and the other at test time. Similar to \citet{JNRR2023}, the first step is a quantile regression on the scores of the $n$ data points in the calibration dataset, $\{S(X_i,Y_i)\}_{i\in [n]}$. However, in our more general framework, we allow for the use of a randomized score function to break potential ties in quantile regression, while keeping the assumptions about distribution $\calD$ minimal. We implement this through a randomized score function $\tS:\calX\times\calY\times \RR$ that takes as input the covariates, labels and some random noise $\varepsilon \in \RR$. Therefore, we work with the set of modified scores $\{\tS(X_i,Y_i, \varepsilon_i)\}_{i\in [n]}$, where for every point $i$, $\varepsilon_i$ is drawn independently from a distribution $\calD_{\text{rn}}$. In \Cref{alg:quantile_reg}, we compute a $(1-\alpha)$-``quantile function'' $\hat q$. This $\hat q$ is a weight function from $\calW$ that minimizes the average pinball loss of the $n$ (randomized) scores with parameter $\alpha$.
\begin{comment}
That is,
\begin{align}
    \hat{q} \in \arg \min_{w \in \cal W} &\frac{1}{n} \sum_{i=1}^n \ell_\alpha (w(X_i,Y_i), \tS(S(X_i,Y_i), \varepsilon_i)). 
    \label{eq:quantile_reg}
\end{align}
\end{comment}
\begin{algorithm}
\caption{Quantile Regression of Kandinsky CP}
\label{alg:quantile_reg}
\textbf{Input:} $\{(X_i,Y_i)\}_{i \in [n]}$, $\calW$, and $\tS$
 % \textbf{Output:} Quantile function $\hat q$
\begin{algorithmic}[1]
    
    \STATE For all $i \in [n]$ draw $\varepsilon_i$ independently from $\calD_{\text{rn}}$.
    \STATE Find
    $$\hat{q} \in \arg \min_{w \in \cal W} \frac{1}{n} \sum_{i=1}^n \ell_\alpha (w(X_i,Y_i), \tS(X_i,Y_i, \varepsilon_i)).$$
\end{algorithmic}
 \textbf{Return:} Quantile function $\hat q$
\end{algorithm}

At test time, we run the second step that constructs the prediction set for a given point with covariates $X_{n+1}$. For a fixed estimated quantile function $\hat q$, the prediction set produced by \Cref{alg:pred_set} includes all labels $y \in \calY$ whose (randomized) score (with noise $\varepsilon_{n+1}$ drawn from $\calD_{\text{rn}}$) is at most $\hat{ q}(X_{n+1},y)$. For more details about the computation of \Cref{alg:pred_set} see \Cref{sec: computation}.
\begin{comment}
\begin{align}
    \calC(X_{n+1}) \coloneqq \{y: \tS(S(X_{n+1},y), 
        \varepsilon_{n+1}) \leq \hat{q}(X_{n+1},y)\}.
    \label{eq:pred_set}
\end{align}
\end{comment}
\begin{algorithm}
\caption{Prediction Set Function of Kandinsky CP}
\label{alg:pred_set}
\textbf{Input:} $X_{n+1}$, $\hat{q}$, and $\tS$
% \textbf{Output:} A subset of $\calY$
    \begin{algorithmic}[1]
    \STATE Draw $\varepsilon_{n+1}$ independently from $\calD_{\text{rn}}$.
    \STATE Set
    $$
        \calC(X_{n+1}) = \{y: \tS(X_{n+1},y, 
        \varepsilon_{n+1}) \leq \hat{q}(X_{n+1},y)\}.$$
   
\end{algorithmic}
\textbf{Return:} $\calC(X_{n+1})$
\end{algorithm}

Our main result is stated in \Cref{thm:jointcondcov}, where we prove that, under the assumption that the distribution of the randomized score $\tS$ conditioned on the value of the basis $\Phi$ is continuous, the weighted coverage deviation of $\calC$ converges to zero with high probability at a rate of $O(\sqrt{d/n}+d/n)$, where $d$ is the dimension of $\Phi$. When the distribution of $S(X,Y)\mid\Phi(X,Y)$ is continuous, this theorem holds for the original score function $S$, since setting $\tS(x,y,\varepsilon) = S(x,y)$ satisfies the assumptions.


\begin{theorem}
    Let parameters $\alpha,\delta \in (0,1)$, and $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over a bounded basis $\Phi : \calX \times \calY \to \RR^d$. Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. There exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$ given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set} satisfies, for every $w \in \calW$,
\ifarxiv
\begin{align*}
\left|\wcovdev(\calC, \alpha, w)\right| \leq 
 \|w\|_{\infty} \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
\end{align*}
\else
\begin{align*}
&\left|\wcovdev(\calC, \alpha, w)\right| \leq \\
& \|w\|_{\infty} \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
\end{align*}
\fi

\label{thm:jointcondcov}
\end{theorem}
As a corollary of \Cref{thm:jointcondcov}, we derive a bound on the expected coverage deviation of $\calC$, with the expectation taken over the randomness of the calibration dataset and the noise in the randomized scores. 
\begin{corollary}
Let $\alpha, \delta, \calW$ be specified as in \Cref{thm:jointcondcov} and consider the same assumptions on $\{(X_i, Y_i)\}_{i\in [n+1]}$, $\tilde S$, and $\Phi$  in \Cref{thm:jointcondcov}. 
\iffalse
 Assume that $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous.
 \fi
 Then, there exists an absolute constant $C$ such that the (randomized) prediction set $\calC$, given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, satisfies
\begin{align*}
\EE_{D,E}  \left[ \sup_{w\in\calW} \frac{\wcovdev(\calC, \alpha,w)}{\|w\|_{\infty}}\right] \leq 
C \sqrt{\frac{d}{n}} + \frac{d}{n},
\end{align*}
where $D$ is the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and $E$ is the corresponding noise $\{\varepsilon_i\}_{i\in [n]}$.
\label{cor: expected-cov}
\end{corollary}

\paragraph{Weight Functions Defined on the Covariates}

\citet{ACDR24} show that when the covariate-based weight class $\calW$ contains a class of binary functions with VC dimension $d$, equal to the dimension of the basis $\Phi$, then the convergence rate of the expected coverage deviation is at least $O(\sqrt{d \alpha(1-\alpha)/n})$. This means that for $n > d$, the rate in \Cref{cor: expected-cov} is asymptotically optimal. In \Cref{sec: comparison} we compare our upper bounds with prior work.

\subsection{Applications}
Kandinsky conformal prediction is a general framework that can be specialized to obtain several types of weighted or conditional conformal prediction. The choice of the appropriate basis $\Phi$ and, consequently, the weight function class $\calW$ depends on the desired coverage guarantee. In this subsection, we explore how Kandinsky conformal prediction can be applied to several scenarios, including group-conditional conformal prediction, Mondrian conformal prediction, group-conditional conformal prediction with fractional group membership, and conformal prediction under distribution shift.
% and weighted conformal prediction when the weight functions are defined on the covariates.  

\paragraph{Group-Conditional Conformal Prediction} Suppose we want to achieve the target coverage $(1-\alpha)$ for every group of points $G \subset \calX \times \calY$ within a finite set of potentially overlapping groups $\calG$. Let $\pi_G(\calC)$ be the probability that the true label is included in the prediction set $\calC$, conditioned on the datapoint $(X_{n+1},Y_{n+1})$ belonging to some group $G$. Formally, we use the notation
\begin{align*}
    &\pi_{G}(\calC) \coloneqq \PP[Y_{n+1}\in \calC(X_{n+1}) \mid (X_{n+1},Y_{n+1})\in G].
\end{align*}
We want to ensure that for every $G \in \calG$, $\pi_G(\calC) = 1-\alpha$. Here we can define $\Phi$ to be a $|\calG|$-dimensional vector that has an entry $\11\{(x,y) \in G\}$ for every $G \in \calG$. \Cref{cor: group-cond} provides the rate at which $\pi_G(\calC)$ converges to $(1-\alpha)$, when $\calC$ is constructed by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}. For conciseness, we define $\pi_G \coloneqq \PP[(X_{n+1},Y_{n+1}) \in G]$.


\begin{corollary}
\label{cor: group-cond}
     Let parameters $\alpha,\delta \in (0,1)$, and $\calW = \{\sum_{G \in \calG} \beta_G \11\{(x,y) \in G\}: \beta_G \in \RR, \forall G \in \calG \}$. Under the same assumptions on $\{(X_i, Y_i)\}_{i\in [n+1]}$, $\tilde S$, and $\Phi$  in \Cref{thm:jointcondcov},
% Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. 
there exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$ given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set} satisfies, for all $G \in \calG$,
     \ifarxiv
     \begin{align*}
    \left|\EE_{\varepsilon_{n+1}}[\pi_{G} (\calC)] -(1-\alpha)\right| \leq 
    \frac{1}{\pi_G}\pth{C \sqrt{\frac{|\calG|}{n}} + \frac{|\calG|}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
    \end{align*}
     \else
    \begin{align*}
    &\left|\EE_{\varepsilon_{n+1}}[\pi_{G} (\calC)] -(1-\alpha)\right| \leq \\
    &{} 
    \frac{1}{\pi_G}\pth{C \sqrt{\frac{|\calG|}{n}} + \frac{|\calG|}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
    \end{align*}
    \fi
\end{corollary}

\paragraph{Mondrian Conformal Prediction}
A special case of group-conditional conformal prediction is Mondrian conformal prediction, where groups in $\calG$ are disjoint and form a partition of the domain $\calX \times \calY$. Since each $(X_i, Y_i)$ in the calibration dataset belongs to exactly one group in $\calG$, we can verify that \Cref{alg:quantile_reg} computes a value $\beta_G \in \RR$ for every group $G$ independently. By \Cref{lem: quant-reg}, for every $G \in \calG$, if $\beta_G$ is the largest value that minimized the pinball loss, then it is a $(1-\alpha)$-quantile of the scores of the calibration datapoints that belong to $G$. As a result, for a test point $X_{n+1}$, we construct the prediction set $\calC(X_{n+1})$ by including every $y$ where the score $\tS(X_{n+1},y,\varepsilon)$ is at most the quantile $\beta_G$ corresponding to the group $G$ that contains $(X_{n+1},y)$. This method is the same as the one presented in \cite{VLNG03}, with the difference that in that method, the quantiles are slightly adjusted to achieve the expected coverage guarantee through exchangeability. 

\paragraph{Fractional Group Membership}
In some cases, we are concerned with the conditional coverage in groups defined by unobserved attributes. Let $\calZ$ be the domain of these unobserved attributes, and let $\calD'$, defined over $\calX\times \calY \times \calZ $, be the joint distribution of the covariates, the label, and the unobserved attributes. In this setting, $\calG$ is a set of groups defined as subsets of $\calZ$. Let 
\begin{align*}
    &\pi^Z_{G}(\calC) \coloneqq \PP [ Y_{n+1} \in \calC(X_{n+1}) \mid Z_{n+1} \in G].
\end{align*}
Then, we want to ensure that $\pi^Z_G(\calC) = 1-\alpha$ for every $G \in \calG$. To achieve this, we set the basis of the weight function class as the probability of $Z\in G$ given a statistic $\phi(X,Y)$. For every $G \in \calG$,
% To achieve this, we set the weight function class to be linear functions of the form $\sum_{G \in \calG} \beta_G \Phi_G(x,y)$ over a basis $\Phi: \calX \times \calY \to \RR^{|\calG|}$, where for every group $G \in \calG$
\begin{align}
\label{eq:Phi_XY}
    \Phi_G(x,y) = \PP[Z \in G \mid \phi(X,Y)=\phi(x,y)].
\end{align}
The simplest example of the statistic $\phi$ is $\phi_{\text{XY}}(X,Y) = (X,Y)$. In practice, if a pretrained $\Phi$ is not available, we may use the protected attributes $Z$ in calibration data to estimate the probabilities in $\Phi$. In some cases, we may only have indirect access to the covariates through the pretrained predictor $f(X)$. Then, we can construct the statistic as $\phi_{\text{FY}}(X,Y) = (f(X), Y)$. In general, \Cref{cor: fract-cov} shows that our algorithm ensures group conditional coverage if $\phi$ is a sufficient statistic for the score function, which is satisfied by both $\phi_{\text{XY}},\phi_{\text{FY}}$.
% Since most score functions are determined by the predictor's output, the label, and independent random noise, $\phi_{\text{FY}}(X,Y) = (f(X), Y)$ is also a sufficient statistic for the score.

 For our theoretical result, we assume that we already know these conditional probabilities, $\Phi_G(x,y)$, for all $G \in \calG$, and that the calibration and the test data %$\{(X_i,Y_i)\}_{i \in [n+1]}$ 
 do not include unobserved attributes. \Cref{cor: fract-cov} shows that by constructing 
 %the prediction set function 
 $\calC$ according to Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, the coverage converges to $(1-\alpha)$ at an $O(\sqrt{|\calG|/n}+|\calG|/n)$ rate. For brevity, we define $\pi_G^Z \coloneqq \PP[Z \in G]$. 
 
\begin{corollary}
\label{cor: fract-cov}
Let $\alpha, \delta \in (0,1)$, $\phi(X,Y)$ be a sufficient statistic for $\tS$, such that $\tS$ is conditionally independent of $X,Y$ given $\phi$, and
    % $
    %     \calW = \{\sum_{G \in \calG} \beta_G \PP[Z \in G \mid \phi(X,Y)=\phi(x,y)]: \beta_G \in \RR, \forall~G\in\calG\}
    % $
    $
        \calW = \{\sum_{G \in \calG} \beta_G \Phi_G: \beta_G \in \RR, \forall~G\in\calG\}.
    $
    % Assume that $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. 
    Under the same assumptions on $\{(X_i, Y_i)\}_{i\in [n+1]}$, $\tilde S$, and $\Phi$  in \Cref{thm:jointcondcov}, there exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$ given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set} satisfies, for all $G \in \calG$,
    \ifarxiv
    \begin{align*}
    \left|\EE_{\varepsilon_{n+1}}[\pi^Z_{G}(\calC)] -(1-\alpha)\right| \leq 
    \frac{1}{\pi_G^Z} 
     \pth{C\sqrt{\frac{|\calG|}{n}}+\frac{|\calG|}{n}+\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
    \end{align*} 
    \else
    \begin{align*}
    &\left|\EE_{\varepsilon_{n+1}}[\pi^Z_{G}(\calC)] -(1-\alpha)\right| \leq \\
    &\frac{1}{\pi_G^Z} 
     \pth{C\sqrt{\frac{|\calG|}{n}}+\frac{|\calG|}{n}+\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} }.
    \end{align*}     
    \fi
\end{corollary}
 


\paragraph{Distribution Shifts}

Kandinsky conformal prediction can also ensure the target coverage under a class of distribution shifts, where the joint distribution over both the covariates and the labels can change. In this context, we assume that the calibration dataset $\{(X_i,Y_i)\}_{i\in[n]}$ is sampled \mbox{i.i.d.} from a source distribution $\calD$, while the test example $(X_{n+1}, Y_{n+1})$ may be drawn independently from another distribution $\calD_{T}$. The objective is to construct a prediction set that satisfies
\begin{align*}
    \PP_{(X_{n+1}, Y_{n+1})\sim \calD_T} [Y_{n+1} \in \calC(X_{n+1})] = 1-\alpha,
\end{align*}
for a set of test distributions $\calD_T$. Given a weight function class $\calW$, \Cref{cor: distr-shift} states that we can achieve the desired coverage for all distribution shifts $\calD_{T}$ with its Radon-Nikodym derivative $\frac{d\PP_{\calD_{T}}}{d\PP_{\calD}}(x,y)$ contained in $\calW$. 

% relating the distribution of the test point $\calD_{T}$ to the distribution of the training points $\calD$
% This derivative is denoted by $\frac{d\PP_{\calD_{T}}}{d\PP_{\calD}}(x,y)$, for any point $(x,y) \in \calX\times\calY$.


\begin{corollary}
\label{cor: distr-shift}
     Let $\alpha$ and $\delta$ be parameters in $(0,1)$, and $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over a bounded basis $\Phi : \calX \times \calY \to \RR^d$. 
     % Assume that the data $\{(X_i,Y_i)\}_{i\in [n]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. 
     Under the same assumptions on $\{(X_i, Y_i)\}_{i\in [n+1]}$, $\tilde S$, and $\Phi$  in \Cref{thm:jointcondcov}, there exists an absolute constant $C$ such that, with probability at least $1-\delta$ over the randomness of the calibration dataset $\{(X_i,Y_i)\}_{i\in [n]}$ and the noise $\{\varepsilon_i\}_{i \in [n]}$, the (randomized) prediction set $\calC$, given by Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, satisfies
     \ifarxiv
     \begin{align*}
        \left|\PP_{(X_{n+1}, Y_{n+1})\sim \calD_T, \varepsilon_{n+1}} [Y_{n+1} \in \calC(X_{n+1})] - (1-\alpha)\right|\leq 
        B \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} } ,
     \end{align*}     
     \else
    \begin{align*}
        &|\PP_{(X_{n+1}, Y_{n+1})\sim \calD_T, \varepsilon_{n+1}} [Y_{n+1} \in \calC(X_{n+1})] - (1-\alpha)|\leq \\
        &B \pth{C \sqrt{\frac{d}{n}} + \frac{d}{n} +\max \{\alpha, 1-\alpha\}\sqrt{\frac{2\ln(4/\delta)}{n}} } ,
    \end{align*}
    \fi
     where $(X_{n+1},Y_{n+1})$ are drawn independently from a distribution $\calD_{\text{T}}$, for every distribution $\calD_{\text{T}}$ such that $\frac{d\PP_{\calD_{T}}}{d\PP_{\calD}} \in \calW$ and $ \left|\frac{d\PP_{\calD_{T}}}{d\PP_{\calD}}(x,y) \right|\leq B$ for any $x \in \calX$ and $y \in \calY$.  
\end{corollary}

% \paragraph{Weight Functions Defined on the Covariates}

% \citet{ACDR24} show that when the covariate-based weight class $\calW$ contains a class of binary functions with VC dimension $d$, equal to the dimension of the basis $\Phi$, then the convergence rate of the expected coverage deviation is at least $O(\sqrt{d \alpha(1-\alpha)/n})$. This means that for $n > d$, the rate in \Cref{cor: expected-cov} is asymptotically optimal. In \Cref{sec: comparison} we compare our upper bounds with prior work.


\section{Test-time Quantile Regression}
\label{sec:testtimeqr}

In this section, we explore an alternative quantile regression-based algorithm that extends the method proposed in \cite{GCC2023} to weight function classes over $\calX \times \calY$. This method, detailed in \Cref{alg:tt_quantile_reg}, is more complicated than Algorithms \ref{alg:quantile_reg}, \ref{alg:pred_set} and performs both steps during test time.

Specifically, given the covariates of a test point $X_{n+1}$, \Cref{alg:tt_quantile_reg} computes a separate quantile function $\hat{q}_y$ for each label $y \in \calY$ by minimizing the average pinball loss over the scores of both $\{(X_{i},Y_{i})\}_{i \in [n]}$ and $(X_{n+1},y)$. Since $X_{n+1}$ is required for the quantile regression, this step is performed at test time. The prediction set $\calC$ produced by \Cref{alg:tt_quantile_reg} includes all labels $y \in \calY$ where the score is at most the label specific quantile, evaluated at $(X_{n+1},y)$.

\begin{algorithm}
\caption{Test-time Quantile Regression}
\label{alg:tt_quantile_reg}
\textbf{Input:} $X_{n+1}$, $\{(X_i,Y_i)\}_{i \in [n]}$, $\calW$, and $\tS$\\

% \textbf{Output:} A subset of $\calY$
\begin{algorithmic}[1]
    \STATE For all $i \in [n+1]$, draw $\varepsilon_i$ independently from $\calD_{\text{rn}}$
    \STATE Set
    $$
        \calC(X_{n+1}) = \{y:\tS(X_{n+1},y, \varepsilon_{n+1}) \leq \hat{q}_y(X_{n+1},y)\},
    $$
     where
     \ifarxiv
     \begin{align*}
        \hat{q}_y \in \arg \min_{w \in \calW} \frac{1}{n+1} \sum_{i=1}^n \ell_\alpha (w(X_i,Y_i),\tS(X_i,Y_i, \varepsilon_i)) 
    + \frac{1}{n+1} \ell_\alpha (w(X_{n+1},y),\tS(X_{n+1},y, \varepsilon_{n+1}))).
    \end{align*}
     \else
    \begin{align*}
        \hat{q}_y \in &\arg \min_{w \in \calW} \frac{1}{n+1} \sum_{i=1}^n \ell_\alpha (w(X_i,Y_i),\tS(X_i,Y_i, \varepsilon_i)) \\
    + &\frac{1}{n+1} \ell_\alpha (w(X_{n+1},y),\tS(X_{n+1},y, \varepsilon_{n+1}))).
    \end{align*}
    \fi
\end{algorithmic}
\textbf{Return:} $\calC(X_{n+1})$
\end{algorithm}

\begin{comment}
\begin{align}
    \hat{q}_y \coloneqq \arg \min_{w \in \calW} &\frac{1}{n+1} \sum_{i=1}^n \ell_\alpha (w(X_i,Y_i),\tS(S(X_i,Y_i), \varepsilon_i)) \\
    + &\frac{1}{n+1} \ell_\alpha (w(X_{n+1},y),\tS(S(X_{n+1},y), \varepsilon_{n+1}))
\end{align}
\begin{align}
    \calC_{\text{t}}(X_{n+1}) \coloneqq \{y:\tS(S(X_{n+1},y), \varepsilon_{n+1}) \leq \hat{q}_y(X_{n+1},y)\}
    \label{eq:test_time_pred_set}
\end{align}
\end{comment}

% In \Cref{thm:tt-cond}, we show that 
For the weight functions $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$, where $\Phi : \calX \times \calY \to \RR^d$, under the same assumptions about the data and the distribution of $\tS\mid\Phi$ in \Cref{thm:jointcondcov}, the expected weighted coverage deviation is bounded by $O(d/n)$.
%The factor $\EE[ \max_{i \in [n+1]}|w(X_i,Y_i)|]$ can potentially be much larger than the factor $\|w\|_\infty$ in the analysis of Algorithms \ref{alg:pred_set} and \ref{alg:quantile_reg}.

\begin{theorem}
    Let $\alpha$ be a parameter in $(0,1)$, and let $\calW = \{\Phi(\cdot)^T \beta :\beta \in \RR^d\}$ denote a class of linear weight functions over a basis $\Phi : \calX \times \calY \to \RR^d$. Assume that the data $\{(X_i,Y_i)\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD$, $\{\varepsilon_i\}_{i\in [n+1]}$ are drawn \mbox{i.i.d.} from a distribution $\calD_{\text{rn}}$, independently from the dataset, and the distribution of $\tS\left(X,Y, \varepsilon\right) \mid \Phi(X,Y)$ is continuous. Then, for any $w \in \calW$, the prediction set given by \Cref{alg:tt_quantile_reg} satisfies 
    \begin{align*}
       |\EE_{D,E} [ \wcovdev(\calC, \alpha,w)]| \leq \frac{d}{n+1} \EE_{D_{+}}\left[ \max_{i \in [n+1]}|w(X_i,Y_i)|\right],
    \end{align*}
    where $D$ is the calibration dataset $\{(X_i,Y_i)\}_{i\in[n]}$, $E$ is the corresponding noise $\{\varepsilon_i\}_{i \in [n]}$ and $D_{+}$ is the full dataset $\{(X_i,Y_i)\}_{i\in[n+1]}$.
   \label{thm:tt-cond}
\end{theorem}


% \Cref{alg:tt_quantile_reg} is considerably more computationally expensive than running Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}. This is because it performs quantile regression multiple times for each new test point. Let $T_{\text{qr}}$ be the time it takes to run quantile regression. In a naive implementation, the time complexity of computing the prediction set for a new point scales with $|\calY| \cdot T_{\text{qr}}$. Even when the score function and the weight function class have nice properties that allow us to avoid enumerating over $\calY$, computing the prediction set still requires $O(T_{\text{qr}})$ time for each test point.

\Cref{alg:tt_quantile_reg} is significantly more computationally expensive than Algorithms \ref{alg:quantile_reg} and \ref{alg:pred_set}, as it performs multiple quantile regressions per test point. Let  $T_{\text{qr}}$  denote the time required for a single quantile regression. In a naive implementation, computing the prediction set scales as  $O(|\mathcal{Y}| \cdot T_{\text{qr}})$. Even if for certain score and weight functions the construction of the prediction set avoids enumerating $\mathcal{Y}$, the complexity remains  $O(T_{\text{qr}})$ per test point.