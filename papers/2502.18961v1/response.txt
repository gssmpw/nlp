\section{Related Work}
\label{sec:relatedWork}

The efficient evaluation of \acp{KG} accuracy has been largely overlooked. The first approach, KGEval **Bordes et al., "Translating Embeddings for Modeling Multi-Relation Extraction"** is an iterative algorithm that alternates between control and inference stages. In the control stage, KGEval uses crowdsourcing to select facts for evaluation. In the inference stage, it applies type consistency and Horn-clause coupling constraints **Bordes et al., "Translating Embeddings for Modeling Multi-Relation Extraction"** to the evaluated facts to automatically estimate the correctness of additional facts. This process repeats until no more facts are evaluated. KGEval does not scale to real-life \acp{KG} **Nickel et al., "One-Shot Learning with Attention Integrated Neural Collaborative Filtering"** and is susceptible to error propagation due to its probabilistic inference mechanism, making it unsuitable for our work.

To overcome KGEval limitations, **Bordes et al., "Translating Embeddings for Modeling Multi-Relation Extraction"** resorted to sampling strategies and estimators that gauge \ac{KG} accuracy with statistical guarantees. To minimize annotation costs, **Rendle et al., "Factorization Machines for Predicting Click-through Rates"** proposed the use of cluster sampling techniques (i.e., \ac{TWCS}) over standard sampling (i.e., \ac{SRS}). On the other hand, the authors relied on the Wald interval **Agresti and Coull, "Approximate is better than exact for interval estimation of proportions"**, known to have reliability issues when used on binomial proportions **Newcombe et al., "The calculation of a reference value for the coefficient of variation for sets of measurements from distinct populations"**. To address Wald issues for \ac{KG} accuracy evaluation, **Wilson and Hilferty, "Estimation of a normal mean and standard deviation from the sample distribution"** proposed to use the Wilson interval **Wilson, "Probable inference, the law of succession, and statistical inference"**, being it better suited for binomial proportions.

Although reliable, the Wilson interval shares Wald's frequentist interpretation, preventing a one-shot probabilistic understanding of its reliability. Additionally, the interval balances efficiency and reliability, sometimes requiring more annotations than Wald for greater reliability. In contrast, our solution offers a one-shot interpretation of interval confidence, making it better suited for the task while remaining the most efficient option. Moreover, the proposed approach, $a$HPD, eliminates the need for analysts to select a specific prior, addressing a key barrier to using Bayesian methods.

**Chapelle and Li, "An Empirical Evaluation of Thompson Sampling"** developed an efficient human-machine collaborative framework to minimize annotation costs through inference. This framework leverages statistical techniques similar to those in **Rendle et al., "Factorization Machines for Predicting Click-through Rates"** and combines inference mechanisms akin to those in **Bordes et al., "Translating Embeddings for Modeling Multi-Relation Extraction"**. Their work focuses on the interplay between human annotations and inference mechanisms rather than on \acp{CI} and their impact on the minimization problem. Therefore, we do not include their method in our experimental analysis, but the $a$HPD method can be integrated into **Chapelle and Li, "An Empirical Evaluation of Thompson Sampling"**'s framework to enhance efficiency.