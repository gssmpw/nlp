

\prAtEndRestateii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndii}]\phantomsection\label{proof:prAtEndii}For simplicity, we consider a special case where, for each ad $\ad _i$, the rewards $r_{ij}$ are identical, i.e., $r_{ij} = r_i$, for all associated slots $j \in \slots _i$. We first show that the expected reward is non-monotone. It is easy to see that assigning ads sequentially by the order of the slots increases the expected reward. However, assigning a new ad with a zero reward to an earlier slot decreases the expected reward, as it reduces the probability of subsequent ads of being seen. \par We continue to show that the expected-reward function is non-submodular. For any feasible subset $C \subseteq D \subseteq \E $, the marginal gain $g((i,j) \mid C) = f(C+(i,j)) - f(C)$ of adding an edge $(i, j)$ into a set of edges~$C$ is \begin {align*} g((i, j) \mid C) = \rw _i (1-\q )^{j + \nb (j)} - \q \sum _{(i',j') \in C: j' > j} \rw _{i'} (1-\q )^{j' + \nb (j')}. \end {align*} Compared with $g((i ,j) \mid D)$, the first term is clearly non-increasing, but the second term may increase. For example, we have $g((i ,j) \mid C) < g((i ,j) \mid D)$ by letting $D \setminus C$ be ads with zero rewards placed after slot $j$ \emph {and} before other subsequent items. On the other hand, we also have $g((i ,j) \mid C) \ge g((i ,j) \mid D)$ when slot $j$ is ranked after every occupied slot in $D$.\end{proof}

\prAtEndRestateiii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndiii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofiii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndiii}]\phantomsection\label{proof:prAtEndiii}The proof is similar to the one by \citet {ieong2014advertising} for finely targeted ads, i.e., $|\slots _i| = 1$, for all ads $\ad _i$. The key is to notice that by processing slots backwards, a decision at slot $j$ cannot affect any slot that has not yet been processed, i.e., slots in positions $j'=1,\dots ,j-1$. That is, the user attention for a slot $j'$ does not depend on ads placed later (in slots $j,\dots ,\nV $); additionally, every ad can be re-used as there is no matching constraint. Thus, solving optimally the sequence of sub-problems on slots $j,\dots ,\nV $ with decreasing $j=\nV ,\dots ,1$, yields an optimal solution to \streamadsr . \par The sub-problem for the final slot (i.e., $j=\nV $) is trivial, and \alggback assigns to it the ad with the highest expected reward, if available. Moving backwards to the next slot $j$, \alggback assigns an ad with the highest reward to the slot $j$ only if it improves the total reward, that clearly results in an optimal assignment for this new sub-problem. The proof immediately follows by the above invariant over the backward processing of the slots.\end{proof}

\prAtEndRestateiv*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndiv}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofiv{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndiv}]\phantomsection\label{proof:prAtEndiv}At each iteration, $R_j$ remains unchanged if no re-assignment occurs. Hence, consider when an ad $\ad _i$ is re-assigned from slot $\tilde {j}$ to slot $\tilde {j}'$, and let $\tilde {R}_j$ be the revenue after such a re-assignment. First note that our statement does not regard $R_{\tilde {j}'}$, because the sub-problem \streamads -$\tilde {j}'$ is completed after the re-assignment. Clearly, $\tilde {R}_{j} = R_{j}$ for any $j \ge \tilde {j}$. Now let $j < \tilde {j}$. We prove by induction that $\tilde {R}_{j} \le R_{j}$. Recall that $\tau _{j} = \rw _{e_{j}} - \q R_{j}$, and by design of the \alggbackproxy algorithm it holds $\tau _{j} > 0$. \par First, as a base case, when $j = \tilde {j} - 1$, we have \[ \tilde {R}_{j} = (1-\q ) R_{\tilde {j}} \le (1-\q ) (R_{\tilde {j}} + \tau _{\tilde {j}}) = R_{j}. \] In the inductive step, for $j< \tilde {j} -1 $, we have \begin {align*} \tilde {R}_{j} &= (1-\q ) (\tilde {R}_{j+1} + \indicator [e_{j+1} \in \M - (i,\tilde {j})] \tilde {\tau }_{j+1}) \\ &\le (1-\q ) (R_{j+1} + \indicator [e_{j+1} \in \M ] \tau _{j+1}) = R_{j}, \end {align*} where $\tilde {\tau }_{j} = \rw _{e_{j}} - \q \tilde {R}_{j}$. The inequality follows since $\tilde {R}_{j+1} \le R_{j+1}$ holds regardless of $e_{j+1}$ being in $\M $ or not. This completes the proof.\end{proof}

\prAtEndRestatev*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndv}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofv{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndv}]\phantomsection\label{proof:prAtEndv}The marginal gain $g$ of re-assigning ad $\ad _i$ from slot $\tilde {j}$ to slot $j$ is a sum of two terms. The first term is the loss of removing edge $\tilde {e} = (i,\tilde {j})$, and the second term is the marginal reward of adding the new edge $(i,j)$. By \cref {eq:decomp}, we have that \begin {align*} R_j - \tilde {R}_j &= \sum _{j'=j+1}^{\nV } (1-\q )^{j'-j} \left ( \indicator [e_{j'} \in \M ] \tau _{j'} - \indicator [e_{j'} \in \M - \tilde {e}] \tilde {\tau }_{j'} \right ) \\ &= \tau _{i} (1-\q )^{\tilde {j}-j} + \sum _{j'=j+1}^{\tilde {j}-1} (1-\q )^{j'-j} \left ( \indicator [e_{j'} \in \M ] (\tau _{j'} - \tilde {\tau }_{j'}) \right ) \\ &\le \tau _{i} (1-\q )^{\tilde {j}-j} , \end {align*} where $\tilde {R}_j$ is the reward after the removal, and $\tilde {\tau }_{j} = \rw _{e_{j}} - \q \tilde {R}_{j}$. The last two steps follow from \cref {lemma:R}. The claim follows, \begin {align*} g &= \tilde {R}_j - R_j + \rw _{ij} -\q \tilde {R}_j = (1-\q ) (\tilde {R}_j - R_j) + \rw _{ij} -\q R_j \\ &\ge \rw _{ij} -\q R_j - \tau _{i} (1-\q )^{\tilde {j}-j+1} \ge \rw _{ij} -\q R_j - \tau _{i} (1-\q )^{\tilde {j}-j} \end {align*}\end{proof}

\prAtEndRestatevi*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndvi}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofvi{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndvi}]\phantomsection\label{proof:prAtEndvi}We prove the claim by induction on slots $j\in [\nV ]$ following the same backward ordering (i.e., $j=\nV ,\dots , 1,0$) adopted by \cref {alg:greedy-backproxy}. Let $\ALG _j$ be the solution of \cref {alg:greedy-backproxy} before performing the $j$-th iteration (i.e., having only processed the slots in positions $\nV ,\dots , j+1$)\footnote {for $j=\nV $ there are no such processed slots, while if $j=0$ then $\ALG _j$ corresponds to the output of~\cref {alg:greedy-backproxy}.}, and $\OPT _j$ be the optimal solution to \streamads (i.e., \OPT ) ignoring the first $j$ slots. Let their objective values for the sub-problem \streamadsj be $R_j := f_j(\ALG _j)$ and $R_j^* := f_j(\OPT _j)$, respectively. And also let the marginal revenue in $R$ be $g_j = R_{j-1} / (1-\q ) - R_j$ at the $j$-th slot, and similarly in $R^*$, $g^*_j = R^*_{j-1} / (1-\q ) - R_j^*$. We then write $\Gamma _i := \tau _i (1-\q )^{\sigma (i)-j}$, for each ad $a_i$ matched in $\ALG _j$. \par Let $\tilde {j}$ be smallest $j$ such that it holds $R_{\tilde {j}} \ge R_{\tilde {j}}^*$. Note that $\tilde {j}$ exists, as $R_\nV = R_\nV ^* = 0$. If $\tilde {j} = 0$, the statement trivially follows. Otherwise, we assume the following hypothesis: for every $j < \tilde {j}$, we can charge the marginal revenue $g^*_j$ of $\OPT _j$ to both $g_j$ and $\{ \Gamma _i \}$ in $\ALG _j$, while maintaining the invariant that every $\Gamma _i$ (corresponding to ad $\ad _i$) in $\ALG _j$ is used at most once among all iterations. This immediately implies \begin {align*} 2 f_j(\ALG _j) &= \sum _{j' > j} g_{j'} (1-\q )^{j'-j} + \sum _{e=(i,j') \in \ALG _j} \Gamma _{i} \\ &\ge \sum _{j' > j} g^*_{j'} (1-\q )^{j'-j} = f_j(\OPT _j) \end {align*} by the decomposition in \cref {eq:decomp}. \par For $j = \tilde {j}$, since $R_j \ge R_j^*$, it is sufficient to consider only the marginal gains $\{ g_j \}$, as it holds $R_j \ge R_j^*$. Now, for the next smaller $j$ in an inductive step, we have the following cases. \par \textbf {Case 1}. $\OPT _{j-1} = \OPT _{j}$, that is, \OPT does not include any new ad for its $j$-th slot. If our \ALG also does not select any item for the $j$-th slot, then the inductive step clearly holds. \par Otherwise, notice that~\cref {alg:greedy-backproxy} (re-)assigns an ad only if $g_{LB} > 0$ by \cref {lemma:LB}. Hence, the overall revenue (i.e., $R_{j-1}/(1-\q )$) only increases, and therefore our hypothesis holds also for this case. \par \textbf {Case 2}. $\OPT _{j-1} = \OPT _{j} + e^*$, where $e^* = (i^*, j)$, that is the optimal solution assigns ad $i^*$ to the $j$-th slot. \par \textbf {Case 2.1}. If our \ALG (re-)assigns ad $i$ to slot $j$, i.e., matching the edge $e=(i, j)$, then by the greedy criterion (\cref {eq:greedy-proxy}), we have \[ \rw _e - \Gamma _i \ge \rw _{e^*} - \Gamma _{i^*} . \] Therefore, we can use both $\Gamma _{i^*}$ and $g_j$ to charge for $\rw _{e^*}$. That is, \begin {align*} g_j + \Gamma _{i^*} &\ge \rw _e - \Gamma _{i} - q R_j + \Gamma _{i^*} \ge \rw _{e^*} - q R_j^* = g^*_j, \end {align*} where the first inequality follows by \cref {lemma:LB}, and the second follows by the greedy rule and the fact that $R_j < R_j^*$ (as $j < \tilde {j}$). Note that if ad $\ad _{i^*}$ was not matched in $\ALG _j$ then $\Gamma _{i^*} = 0$, or otherwise, we increase the number of charges on $\Gamma _{i^*}$ by one. \par \textbf {Case 2.2}. $\ALG _{j-1} = \ALG _{j}$. The greedy choice and its inequalities from Case 2.1 still apply, but fail to produce a positive lower bound. That is, $g_{LB} = \rw _e - \Gamma _{i} - q R_j \le 0$ for each $e=(i,j)$. Therefore, it is sufficient to only pay $\Gamma _{i^*}$ for this case.~\ilie {Question: is this the same $i^*$ of $e^*$ or is this the $i^*$ on which $g_{LB}$ is evaluated?}\guangyi {The former.}\ilie {Maybe stupid question, can it be that $\Gamma _{i^*}=0$ and for all other iterations, including the one for this case, $g_{LB}<0$?}\guangyi {Do you mean $\Gamma _{i^*}=0$ for all iterations? I think it doesn't matter and we can pay $\Gamma _{i^*}=0$ even if it is zero. In any case we have $\rw _e - \Gamma _{i} - q R_j + \Gamma _{i^*} \ge \rw _{e^*} - q R_j^*$ and we can get $g^*_j$ covered.} \par In Case 2, we use each $\Gamma _i$ at most once because \OPT contains at most one edge incident to ad $\ad _i$, given the matching constraint. Furthermore, $\tau _i$ is non-decreasing after re-assigning either ad $\ad _i$ (by design of \alggbackproxy ), or other ads $\ad _{i'}$ (by \cref {lemma:R}), so the payments in prior iterations remain valid, completing the proof.\end{proof}

\prAtEndRestatevii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndvii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofvii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndvii}]\phantomsection\label{proof:prAtEndvii}Fix $\q =0$, and then \streamads is reduced to a maximum weighted matching problem (\mwm ). It is well known that a greedy algorithm cannot do better than 2-approximation for \mwm . Concretely, let $\nV =2$. Create two ads $\ad _1, \ad _2$ with slots $\slots _1 = \{1,2\}$ and $\slots _2 = \{2\}$, respectively. Set rewards $\rw _{11} = \rw _{22} = 1$ and $\rw _{12} = 1+\epsilon $. Thus, a backwards-greedy algorithm yields a revenue of $1+\epsilon $ by assigning $\ad _1$ to the 2-nd slot, while the optimum assignment yields 2. The ratio approaches 2 for an arbitrary small $\epsilon $.\end{proof}

\prAtEndRestateviii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndviii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofviii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof of \pratendRef{thm:prAtEndviii}]\phantomsection\label{proof:prAtEndviii}The proof is similar to \cref {thm:greedy-streamads}, except that we need a different inequality for the Case 2 therein. Though \cref {alg:greedy-back} does not use the values $\tau _i$, we use such values here only for the analysis, and assume that \cref {alg:greedy-back} updates the values $\tau _i$ as from \cref {thm:greedy-streamads}. Recall that $\Gamma _i := \tau _i (1-\q )^{\sigma (i)-j}$. \par Suppose that at slot $j$, $\OPT _{j-1} = \OPT _{j} + e^*$, where $e^* = (i^*, j)$. If our \ALG (re-)assigns edge $e=(i, j)$, then by the greedy criterion, \begin {align*} g_{i} &\ge g_{i^*} \\ \rw _{ij} -\q R_j - \kappa _{ij} &\ge \rw _{i^*j} -\q R_j - \kappa _{i^*j}, \end {align*} where $g_i$ denotes the marginal reward of (re-)assigning ad $\ad _i$, and $\kappa _{ij} := \rw _{ij} -\q R_j - g_{i}$. By \cref {lemma:LB}, we have for any $i$, \[ g_i \ge \rw _{ij} -\q R_j - \Gamma _i \quad \implies \quad \Gamma _i \ge \kappa _{ij}. \] Therefore, we can use both $\Gamma _{i^*}$ and $g_j$ to charge for $\rw _{i^*j}$. That is, \begin {align*} g_j + \Gamma _{i^*} &= \rw _{ij} -\q R_j - \kappa _{ij} + \Gamma _{i^*} \\ &\ge \rw _{i^*j} -\q R_j - \kappa _{i^*j} + \Gamma _{i^*} \\ &\ge \rw _{i^*j} -\q R_j^* - \kappa _{i^*j} + \Gamma _{i^*} \\ &\ge \rw _{i^*j} -\q R_j^* = g^*_j, \end {align*} where the inequalities follow by the greedy rule, the fact that $R_j < R_j^*$, and \cref {lemma:LB}, respectively. \par The claim follows by charging every $g^*_j$ to $g_j$ and $\{ \Gamma _i \}$, and noting that every $\Gamma _i$ is used at most once among all iterations. We omit the details for the other cases, as they follow from \cref {thm:greedy-streamads}.\end{proof}
