The \streamads problem was introduced by \citet{ieong2014advertising},
who also devised a \PTAS algorithm.
However, their \PTAS relies on exhaustive enumeration of sub-sequences of slots, and flow computations, which is impractical. %
In this section, we study the structural properties of the \streamads problem
aiming to design a practical algorithm with provable quality~guarantees.

Our first step is to view %
the \streamads problem
as a task of optimizing a specific set function over a bipartite matching.
However, as shown in \cref{prop:nonsubm}, 
this specific set function is neither monotone nor submodular.
Therefore, the problem cannot be approximated 
by existing methods for submodular maximization~\citep{buchbinder2018submodular}.

We then present an example showing that two simple and intuitive heuristics may perform arbitrarily bad.
The first heuristic is a standard greedy strategy that prioritizes placing ads in the \emph{top slots}, 
i.e., the slots appearing at the beginning of the content feed.
The second heuristic is to address the problem leveraging the %
maximum-weight matching (\mwm) method. 
The failure of such approaches, and the problem instance that causes the
two heuristics to perform badly inspire the design of our novel algorithms. 
In the next section	(\cref{sec:algs}) we 
propose
a novel backwards greedy strategy 
that carefully accounts for the placement of ads in \emph{bottom slots}, i.e., the slots appearing at the \emph{end} of the content feed.


\begin{figure}[t]
	\centering
	\input{fig-badcase}
	\caption{Representation of \cref{exmaple:top}, where %
		a natural online greedy algorithm and 
		maximum weighted matching (\mwm)
		perform poorly ($\ad$ and $s$ represent ads and slots).}\label{fig:badcase}
\end{figure}


\begin{propositionE}\label{prop:nonsubm}
	The expected-reward function $f: 2^\E \to \reals$ in \cref{eq:obj} for the \streamads problem is 
	neither monotone nor submodular.
\end{propositionE}
\begin{proofE}
	For simplicity, we consider a special case where,
	for each ad $\ad_i$, the rewards $r_{ij}$ are identical, i.e., $r_{ij} = r_i$, 
	for all associated slots $j \in \slots_i$.
	We first show that the expected reward is non-monotone. 
	It is easy to see that assigning ads sequentially by the order of the slots increases the expected reward.
	However, assigning a new ad with a zero reward to an earlier slot decreases the expected reward, as it reduces the probability of subsequent ads of being seen.
	
	We continue to show that the expected-reward function is non-submodular. 
	For any feasible subset $C \subseteq D \subseteq \E$, 
	the marginal gain $g((i,j) \mid C) = f(C+(i,j)) - f(C)$ 
	of adding an edge $(i, j)$ into a set of edges~$C$ is
	\begin{align*}
		g((i, j) \mid C) 
		= \rw_i (1-\q)^{j + \nb(j)} - \q \sum_{(i',j') \in C: j' > j} \rw_{i'} (1-\q)^{j' + \nb(j')}.
	\end{align*}
	Compared with $g((i ,j) \mid D)$, 
	the first term is clearly non-increasing,
	but the second term may increase.
	For example, we have $g((i ,j) \mid C) < g((i ,j) \mid D)$ 
	by letting $D \setminus C$ be ads with zero rewards placed after slot $j$ \emph{and} before other subsequent items.
	On the other hand, 
	we also have $g((i ,j) \mid C) \ge g((i ,j) \mid D)$ 
	when slot $j$ is ranked after every occupied slot in $D$.
\end{proofE}


Due to the exponentially-decaying attention in the model, 
a reasonable strategy is to prioritize the top slots.
Thus, a logical choice is to employ a greedy algorithm that processes slots in a sequentially increasing order 
and repeatedly matches the ad with the highest reward to the processed slot.
However, as we show below, 
such a greedy algorithm has an unbounded approximation ratio, even for the easier \streamadsr problem.


\begin{example}[Being myopic in top slots]\label{exmaple:top}
	See \cref{fig:badcase} for an illustration.
	For each slot $j = 1, \ldots, \nV-1$, we create a dedicated ad $\ad_j$ with reward~1.
	For the final slot $j=\nV$, we create an ad $\ad_j$ with a large reward~$C$.
	The greedy algorithm assigns each ad in its corresponding slot, and it results 
	in a total expected reward of 
	\begin{equation*}
		\sum_{j=1}^{\nV-1} (1-\q)^{2j-1} +  (1-\q)^{2\nV-1} C
		 \approx \tfrac{(1-\q)}{1-(1-\q)^2} + (1-\q)^{2\nV-1} C.
	\end{equation*}
	On the other hand, assigning only the last ad gives reward $(1-\q)^{\nV-1} C$.
	For certain values of the parameters 
	the approximation ratio can be arbitrarily bad.
	For example, when $\q=1/2$ and $C=2^{2\nV-1}$,
	the approximation ratio is about $2^\nV / 2$. 
\end{example}

The instance in \cref{exmaple:top} is also hard for another intuitive algorithm based on maximum-weight matching (\mwm).
This algorithm finds a \mwm for the bipartite graph between ads and slots with appropriately-defined edge weights.
That is, every edge $(i,j)$ connecting ad $\ad_i$ and slot $j$ has a position-biased weight of $\rw_{ij} (1-q)^{j}$.
Unfortunately, the \mwm algorithm fails to capture the decaying-attention effect of the model.
It is easy to see that, on the instance from \cref{exmaple:top}, the \mwm algorithm selects all available edges, like the afore\-mentioned greedy~algorithm.

By a careful inspection of the bad instance in \cref{exmaple:top}, 
it is clear that to obtain solutions with high expected reward, 
we cannot only focus on the top slots, 
or
ignore the decaying-attention effect of the model.
However, it is difficult to take care of both ends of the slot sequence. %
We show in the next section, that both issues can be handled properly by %
first considering 
bottom~slots, through our novel algorithms.

%
%
