The \streamads problem is more challenging due to the matching constraint.
A first idea to address such a problem would be to leverage the~\alggback algorithm, 
and decompose the reward of a matching 
into a sum of marginal gains, %
one term for each slot.
Then, 
to provide approximation guarantees, we need to 
connect such marginal rewards to those of an optimal solution for \streamads. %
However, such analysis quickly becomes challenging, 
as a single re-assignment (in \cref{alg:bwd:re-assign}) may affect the marginal gain over multiple slots due to the decaying-attention~effect.

To avoid such issues, we relate the total revenue to a lower bound of the marginal gains in the above decomposition,
that we use to develop a novel greedy algorithm.
%
This results in a 2-approximation 
non-oblivious backwards-greedy algorithm 
(\alggbackproxy in \cref{alg:greedy-backproxy}) 
for the \streamads problem, note that
this approximation ratio is tight for any greedy algorithm.
The \alggbackproxy algorithm is called ``\emph{non-oblivious}''~\citep{khanna1998syntactic} 
since
it does not select the next ad
with respect to the objective function $f$ of \cref{eq:obj}.


\smallskip
\noindent
\emph{The \alggbackproxy algorithm.}
The \alggbackproxy algorithm is introduced in \cref{alg:greedy-backproxy}.
Similar to the \alggback algorithm,
it processes the slots in a reverse order, starting from the final slot.
The key difference is that,
at every slot $j=\nV,\dots,1$, it seeks to (re-)assign an ad that maximizes a \emph{lower bound} of the marginal gain, 
which is
\begin{align}
	\arg\max_{i \in \ads_j} \; \left\{\rw_{ij} - \q \, f_j(\M) - \tau_i (1-\q)^{\sigma(i)-j}\right\}, \label{eq:greedy-proxy}
\end{align}
where 
$\tau_i$ is defined below,
$\ads_j = \{ i: j \in \slots_i \}$, and
$\sigma(i)=j$ %
if $\ad_i$ is new to the matching \M, 
otherwise $\sigma(i)$ corresponds to the slot previously selected for $\ad_i$.
We prove shortly (in \cref{lemma:LB}) that \cref{eq:greedy-proxy} is a lower bound 
to the marginal reward obtained by assigning an ad at slot~$j$. 

The term $\tau_i$ represents an \emph{estimate} of the total prior reward provided by ad $\ad_i$.
At the beginning, $\tau_i$ is initialized to be 0 for all $i$.
Every time an ad $\ad_i$ is (re-)assigned to the $j$-th slot,
we update its value according to the following rule:
\begin{align}
	\tau_i = \rw_{ij} - \q \, f_j(\M). \label{eq:tau}
\end{align}
It is easy to see that, 
the first time an ad $\ad_i$ is assigned to the slot $j$,
$\tau_i$ represents its actual marginal gain.
However, afterwards, if the ad $\ad_i$ is re-assigned to a different slot $j'<j$,
$\tau_i$ deviates from its marginal gain 
as it does not consider the variation over
$f_{j'}(\M)$, 
caused by the withdrawal of $\ad_i$ from slot $j$.
During the execution of \alggbackproxy, 
it is important to maintain each $\tau_i, i\in[n]$ 
up-to-update when re-assignments occur.
We write $\tau_{j} = \rw_{e_{j}} - \q \, f_j(\M)$ 
when it is more convenient to use the \emph{slot} index $j$,
where $e_j$ denotes an edge that is assigned to the slot $j$.

The \alggbackproxy algorithm preforms an ad (re-)assignment if it results in a positive lower bound as from \cref{eq:greedy-proxy}.
A matching $\M$ is returned after processing all slots. 
We show in \cref{thm:greedy-streamads} that it holds $2 f(\M) \ge f(\M^*)$, where $\M^*$ is the matching achieving the optimal solution for $f$, i.e., \alggbackproxy is a 2-approximation algorithm.


\smallskip
\noindent
\emph{Decomposition.}
We now introduce the novel decomposition of the reward of a matching \M.
Let $R_j := f_j(\M)$ be the reward of a solution \M for the \streamadsj sub-problem (from \cref{eq:obj-j}).
We have
\begin{align}
	R_j 
	&= (1-\q) \left( R_{j+1} + \indicator[e_{j+1} \in \M] (\rw_{e_{j+1}} - \q R_{j+1}) \right) \label{eq:recursion}\\
	&= \sum_{j'=j+1}^{\nV}  (1-\q)^{j'-j} \indicator[e_{j'} \in \M] (\rw_{e_{j'}} - \q R_{j'}) \label{eq:decomp} \\
	&= \sum_{e=(i,j') \in \M: j' > j}  (1-\q)^{j'-j} (\rw_{e} - \q R_{j'}) \nonumber
	,
\end{align}
where $\indicator[e_j \in \M]$ is a 0--1 indicator function taking value 1 if the edge $e_j$, incident to slot $j$, is in the matching \M.
The first equality (\cref{eq:recursion}) expresses $R_j$ as a sum of $R_{j+1}$, and the marginal gain obtained by allocating slot $j+1$ with edge $e_{j+1}$.
The second equality (\cref{eq:decomp}) recursively expands the term $R_{j+1}$, while groups the other terms into a summation. %
The last equality follows 
a simple double-counting argument. %
In summary, $R_j$ is a cumulative sum of marginal gains, computed backwards, of edges in \M,
when there are \emph{no} re-assignments.
Notice the similarity between the components in the decomposition in \cref{eq:decomp}
and the values $\tau_i$ in \cref{eq:tau} (recall that $R_j = f_j(M)$).

We next characterize the behaviour of $R_j$ when a re-assignments occurs in the backwards-greedy algorithm,
and connect such results to the greedy criterion in \cref{eq:greedy-proxy}.

\begin{lemmaE}\label{lemma:R}
	During the execution of the main loop of~\cref{alg:greedy-backproxy},
	for any fixed $j \in [\nV]$,
	the value $R_j$ is non-increasing 
	since the completion of the sub-problem \streamadsj.
	\end{lemmaE}
\begin{proofE}
	At each iteration, $R_j$ remains unchanged if no re-assignment occurs.
	Hence, consider when an ad $\ad_i$ 
	is re-assigned from slot $\tilde{j}$ to slot $\tilde{j}'$, and
	let $\tilde{R}_j$ be the revenue after such a re-assignment.
	First note that our statement does not regard $R_{\tilde{j}'}$, 
	because the sub-problem \streamads-$\tilde{j}'$ is completed after the re-assignment.
	Clearly, $\tilde{R}_{j} = R_{j}$ for any $j \ge \tilde{j}$.
	Now let $j < \tilde{j}$. 
	We prove by induction that $\tilde{R}_{j} \le R_{j}$.
	Recall that $\tau_{j} = \rw_{e_{j}} - \q R_{j}$,
	and by design of the \alggbackproxy algorithm it holds $\tau_{j} > 0$. 
	
	First, as a base case, when $j = \tilde{j} - 1$, we have
	\[
	\tilde{R}_{j}  = (1-\q) R_{\tilde{j}} 
	\le (1-\q) (R_{\tilde{j}} + \tau_{\tilde{j}})
	= R_{j}.
	\]
	In the inductive step, for $j< \tilde{j} -1 $, we have
	\begin{align*}
		\tilde{R}_{j}  &= (1-\q) (\tilde{R}_{j+1} + \indicator[e_{j+1} \in \M - (i,\tilde{j})] \tilde{\tau}_{j+1}) \\
		&\le (1-\q) (R_{j+1} + \indicator[e_{j+1} \in \M] \tau_{j+1})
		= R_{j},
	\end{align*}
	where $\tilde{\tau}_{j} = \rw_{e_{j}} - \q \tilde{R}_{j}$.
	The inequality follows since $\tilde{R}_{j+1} \le R_{j+1}$ holds regardless of $e_{j+1}$ being in $\M$ or not.
	This completes the proof.
\end{proofE}


\smallskip
\noindent
\emph{Approximation guarantees.}
Next, we explain the novel lower bound presented in \cref{eq:greedy-proxy}.
When re-assigning an ad, the exact marginal gain in reward 
heavily depends on the allocation of all other slots already allocated,
due to the decaying attention,
making the analysis particularly challenging.
Therefore, instead of considering the actual marginal reward, %
\alggbackproxy seeks a greedy choice that
maximizes the non-oblivious lower bound, 
which simplifies our analysis.
We first prove that \cref{eq:greedy-proxy} (evaluated by~\alggbackproxy in \cref{step:LB}) 
is a lower bound to the actual marginal reward,
provided that every $\tau_i$ in \cref{eq:tau} is maintained up-to-update.

\begin{lemmaE}\label{lemma:LB}
	Denote by $g$ the marginal gain in reward of re-assigning ad $\ad_i$ from slot $\tilde{j}$ to slot $j$ with $\tilde{j} > j$.
	Then,
	\[
		g \ge \rw_{ij} -\q R_j - \tau_{i} (1-\q)^{\tilde{j}-j}.
	\]
\end{lemmaE}
\begin{proofE}[no link to proof]
	The marginal gain $g$ of re-assigning ad $\ad_i$ from slot $\tilde{j}$ to slot $j$ is a sum of two terms.
	The first term is the loss of removing edge $\tilde{e} = (i,\tilde{j})$, and
	the second term is the marginal reward of adding the new edge $(i,j)$.
	By \cref{eq:decomp}, we have that 
	\begin{align*}
		R_j - \tilde{R}_j 
		&= \sum_{j'=j+1}^{\nV}  (1-\q)^{j'-j} \left( 
			\indicator[e_{j'} \in \M] \tau_{j'}
			- \indicator[e_{j'} \in \M - \tilde{e}] \tilde{\tau}_{j'}
		\right) \\
		&= \tau_{i} (1-\q)^{\tilde{j}-j} +
		\sum_{j'=j+1}^{\tilde{j}-1}  (1-\q)^{j'-j} \left( 
		\indicator[e_{j'} \in \M] (\tau_{j'} - \tilde{\tau}_{j'})
		\right) \\
		&\le \tau_{i} (1-\q)^{\tilde{j}-j} 
		,
	\end{align*}
	where $\tilde{R}_j$ is the reward after the removal, and 
	$\tilde{\tau}_{j} = \rw_{e_{j}} - \q \tilde{R}_{j}$.
	The last two steps follow from \cref{lemma:R}.
	The claim follows,
	\begin{align*}
		g &= \tilde{R}_j - R_j + \rw_{ij} -\q \tilde{R}_j 
		= (1-\q) (\tilde{R}_j - R_j) + \rw_{ij} -\q R_j \\
		&\ge \rw_{ij} -\q R_j - \tau_{i} (1-\q)^{\tilde{j}-j+1}
		\ge \rw_{ij} -\q R_j - \tau_{i} (1-\q)^{\tilde{j}-j}
	\end{align*}
\end{proofE}


Finally, we are ready to show the approximation ratio for \cref{alg:greedy-backproxy}.


\begin{theoremE}\label{thm:greedy-streamads}
	\cref{alg:greedy-backproxy} returns a 2-approximation for the \streamads problem.
\end{theoremE}
\begin{proofE}[no link to proof]
	We prove the claim by induction on slots $j\in[\nV]$ following the same backward ordering (i.e., $j=\nV,\dots, 1,0$) adopted by \cref{alg:greedy-backproxy}. 
	Let $\ALG_j$ be the 
	solution of \cref{alg:greedy-backproxy} before
	performing the $j$-th iteration
	(i.e., having only processed the slots in positions $\nV,\dots, j+1$)\footnote{for $j=\nV$ there are no such processed slots, while if $j=0$ then $\ALG_j$ corresponds to the output of~\cref{alg:greedy-backproxy}.}, and 
	$\OPT_j$ be the optimal solution to \streamads (i.e., \OPT) ignoring the first $j$ slots.
	Let their objective values for the sub-problem \streamadsj be $R_j := f_j(\ALG_j)$ and $R_j^* := f_j(\OPT_j)$, respectively.
	And also let the marginal revenue in $R$ be $g_j = R_{j-1} / (1-\q) - R_j$ at the $j$-th slot, and similarly in $R^*$, $g^*_j = R^*_{j-1} / (1-\q) - R_j^*$.
	We then write $\Gamma_i := \tau_i (1-\q)^{\sigma(i)-j}$,
	for each ad $a_i$ matched in $\ALG_j$.
	
	Let $\tilde{j}$ be smallest $j$ such that it holds
	$R_{\tilde{j}} \ge R_{\tilde{j}}^*$.
	Note that $\tilde{j}$ exists, as $R_\nV = R_\nV^* = 0$.
	If $\tilde{j} = 0$, 
	the statement trivially follows.
	Otherwise, we assume the following hypothesis:
	for every $j < \tilde{j}$,
	we can charge the marginal revenue 
	$g^*_j$ of $\OPT_j$ to both $g_j$ and $\{ \Gamma_i \}$ in $\ALG_j$,
	while maintaining the invariant that
	every $\Gamma_i$ (corresponding to ad $\ad_i$) in $\ALG_j$ is used at most once among all iterations.
	This immediately implies
	\begin{align*}
		2 f_j(\ALG_j) &=  \sum_{j' > j} g_{j'} (1-\q)^{j'-j} + \sum_{e=(i,j') \in \ALG_j}  \Gamma_{i} \\
		&\ge \sum_{j' > j} g^*_{j'} (1-\q)^{j'-j}
		= f_j(\OPT_j)
	\end{align*}
	by the decomposition in \cref{eq:decomp}.
	
	For $j = \tilde{j}$, 
	since $R_j \ge R_j^*$,
	it is sufficient to consider only the marginal gains
	$\{ g_j \}$, as it holds $R_j \ge R_j^*$.
	Now, for the next smaller $j$ in an inductive step, we have the following cases.
	
	\textbf{Case 1}. $\OPT_{j-1} = \OPT_{j}$, that is, \OPT does not include any new ad for its $j$-th slot.
	If our \ALG also does not select any item for the $j$-th slot, then the inductive step clearly holds.
	
	Otherwise, notice that~\cref{alg:greedy-backproxy} (re-)assigns an ad only if $g_{LB} > 0$ by \cref{lemma:LB}.
	Hence, the overall revenue (i.e., $R_{j-1}/(1-\q)$) only increases, 
	and therefore our hypothesis holds also for this case.
	
	\textbf{Case 2}. $\OPT_{j-1} = \OPT_{j} + e^*$, where $e^* = (i^*, j)$, that is the optimal solution assigns ad $i^*$ to the $j$-th slot.
	
	\textbf{Case 2.1}. If our \ALG (re-)assigns ad $i$ to slot $j$, i.e.,  matching the edge $e=(i, j)$, then by the greedy criterion (\cref{eq:greedy-proxy}), we have
	\[
	\rw_e - \Gamma_i \ge \rw_{e^*} - \Gamma_{i^*} .
	\]
	Therefore, we can use both $\Gamma_{i^*}$ and $g_j$ to charge for $\rw_{e^*}$. 
	That is,
	\begin{align*}
		g_j + \Gamma_{i^*} 
		&\ge \rw_e - \Gamma_{i} - q R_j + \Gamma_{i^*} %
		\ge \rw_{e^*} - q R_j^* = g^*_j,
	\end{align*}
	where the first inequality follows by \cref{lemma:LB}, and
	the second follows by the greedy rule and the fact that $R_j < R_j^*$ (as $j < \tilde{j}$).
	Note that if ad $\ad_{i^*}$ was not matched in $\ALG_j$ then $\Gamma_{i^*} = 0$,
	or otherwise, we increase the number of charges on $\Gamma_{i^*}$ by one. 
	
	\textbf{Case 2.2}. $\ALG_{j-1} = \ALG_{j}$. 
	The greedy choice and its inequalities from Case 2.1 still apply, but fail to produce a positive lower bound. 
	That is, 
	$g_{LB} = \rw_e - \Gamma_{i} - q R_j \le 0$ for each $e=(i,j)$.
	Therefore, it is sufficient to only pay $\Gamma_{i^*}$ for this case.~%
	
	In Case 2, we use each $\Gamma_i$ at most once because \OPT contains at most one edge incident to ad $\ad_i$, given the matching constraint.
	Furthermore, $\tau_i$ is non-decreasing after re-assigning 
	either ad $\ad_i$ (by design of \alggbackproxy),
	or other ads $\ad_{i'}$ (by \cref{lemma:R}),
	so the payments in prior iterations remain valid,
	completing the proof.
\end{proofE}

Note that the 2-approximation guarantee is tight for both \cref{alg:greedy-back} and \cref{alg:greedy-backproxy}, and 
this barrier exists also for the special case where $\q = 0$, that is, a \mwm instance.

\begin{propositionE}
	\cref{alg:greedy-back} and \cref{alg:greedy-backproxy} cannot do better than 2-approximation.
\end{propositionE}
\begin{proofE}
	Fix $\q=0$, and then \streamads is reduced to a maximum weighted matching problem (\mwm).
	It is well known that a greedy algorithm cannot do better than 2-approximation for \mwm.
	Concretely, let $\nV=2$.
	Create two ads $\ad_1, \ad_2$ with slots $\slots_1 = \{1,2\}$ and $\slots_2 = \{2\}$, respectively.
	Set rewards $\rw_{11} = \rw_{22} = 1$ and $\rw_{12} = 1+\epsilon$.
	Thus, a backwards-greedy algorithm yields a revenue of $1+\epsilon$ by assigning $\ad_1$ to the 2-nd slot,
	while the optimum assignment yields 2.
	The ratio approaches 2 for an arbitrary small $\epsilon$.
\end{proofE}


The time complexity for the \alggbackproxy algorithm is 
$\bigO(|\E| + \nV |\M|)$ 
where $|\M| = \min \{ \nV, \nads \}$.
The second term is due to the fact that we may need to compute $f_j(\M), j\in[\nV]$ if a re-assignment occurs.
