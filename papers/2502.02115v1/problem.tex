In this section, we first present the necessary preliminaries, and then formally define the problems 
studied in this paper.

\smallskip
\noindent
\emph{Preliminaries.}
A graph is \emph{bipartite} if its vertices can be partitioned into two disjoint parts, and
edges connect only vertices from different parts. 
Given an undirected graph, a \emph{matching} is a set of edges so that each vertex appears in at most one edge of the set.
For a weighted graph, a \emph{maximum-weight matching} (\mwm) is a matching in which the sum of its edge weights is maximized.

A set function $f: 2^\E \to \mathbb{R}$ %
assigns a value to every subset of a given set $\E$.
A set function $f$ is called \emph{monotonically non-decreasing} if $f(C) \le f(D)$,
for all $C \subseteq D \subseteq \E$.
Additionally, $f$ is called \emph{submodular} if $f(C + e) - f(C) \ge f(D + e) - f(D)$, 
for all $C \subseteq D \subseteq \E$ and element $e \in \E$.
Throughout this paper, we use the shorthands $C + e$ for $C \cup \{e\}$ and $C - e$ for $C \setminus \{e\}$.

An algorithm \ALG is an \emph{$\alpha$-approximation algorithm} for a maximization problem,  
if for any instance \instance of the problem, 
the solution $\ALG(\instance)$ returned by the algorithm
has an objective value that is no smaller than $1/\alpha$ 
times the value of the optimal solution, denoted with $\OPT(\instance)$~\citep{williamson2011design}.
That is, let $f$ be the objective function of the problem,
then it holds that $\alpha\, f(\ALG(\instance)) \ge f(\OPT(\instance))$, for all problem instances~\instance. 
A \emph{polynomial-time approximation scheme} (\PTAS) 
is an $(1+\varepsilon)$-approximation algorithm, for any given $\varepsilon > 0$,
with running time polynomial in the input size, but possibly exponential in~$1/\varepsilon$.


\smallskip
\noindent
\emph{Problem definition.}
We are given a sequence of \nV items (e.g., videos), 
and we assume that there is 
one available \emph{slot} for an ad placement after each item.
Suppose also that we are given \nads ads \ads. 
To improve the efficacy of the ads,
an ad $\ad_i$ can only be placed after a subset of relevant items $\slots_i \subseteq \V$.
A reward $\rw_{ij} \ge 0$ is then %
obtained if ad $\ad_i$ is shown to the user after the $j$-th item, with $j \in \slots_i$.
Throughout the paper, we fix $i$ (resp.~$j$) to be the index of an ad (resp.~a slot).

To model the decaying attention of the user, 
our model considers %
that a user decides to quit browsing (i.e., terminates their session) 
with probability $\q$ after observing every item or ad.
Our goal is to decide the allocation of ads to the available slots 
to maximize the expected reward over the specified model. 
We use the terms reward and revenue interchangeably.
For brevity, we may drop the adjective ``expected'' if it is clear from the context.
More formally, the ad-placement problem is defined as~follows.

\begin{problem}[\streamadsr]\label{problem:adsr}
	We are given a sequence of \nV items~\V with one available slot after each item, 
	a set of \nads ads $\ads = \{ \ad_i \}$ with associated slots $\{ \slots_i \}$,  
	rewards $\{ \rw_{ij} \}$ for $j \in \slots_i$, and a quitting probability  $\q\in[0,1)$.
	The goal is to find a mapping 
	$\M \subseteq \E := \bigcup_{i \in [\nads]} \left(\{ i \} \times \slots_i\right)$ 
	such that every slot can admit at most one ad, i.e.,
	$|\{ i : (i, j) \in \M \}| \le 1$ for all~$j$, 
	and $\M$ maximizes the expected reward
	\begin{align}
		f(\M) := \sum_{e=(i,j) \in \M} \rw_{e} (1-q)^{j + \nb(j)}, \label{eq:obj}
	\end{align}
	where $\nb(j)$ is the number of slots before slot $j$ containing an ad,
	i.e.,
	$\nb(j) = |\{ j' < j : (i, j') \in \M \text{ for some } i \}|$.
\end{problem}

The \streamadsr problem explicitly disallows consecutive ads, 
which helps to avoid ad fatigue
and viewer zapping~\citep{shi2023much}.
\streamadsr also benefits from state-of-the-art recommenders that can be used to obtain high-quality rankings for the content items, as it is a common practice to design  ad-allocation strategies
as a post-processing operation~\citep{yan2020ads,li2024deep}. 
In addition, state-of-the-art machine learning models can also be used to obtain high-quality predictions for the expected rewards $r_e$ from~\cref{eq:obj} over user sessions, e.g., from historical data. %

The \streamadsr problem allows an ad to be displayed multiple times.
However, there are scenarios where displaying an ad multiple times is undesirable.
To prevent such over-exposure of ads,
it is possible to preprocess the slots $\slots_i$ of each ad $\ad_i$
and set a limit on the number of slots $|\slots_i|$.
However, such an approach is limited and not always feasible.
To provide a rigorous model for such cases, 
we introduce the following problem variant.

\begin{problem}[\streamads]\label{problem:ads}
	Given the same input as in the \streamadsr\ problem,
	find a \emph{matching} $\M \subseteq \E := \bigcup_{i \in [\nads]} \left(\{ i \} \times \slots_i\right)$ 
	that maximizes the expected reward $f(\M)$ from~\cref{eq:obj}. %
\end{problem}

Note that the \streamads problem is significantly more general than the previous \streamadsr problem, 
as an ad can be 
displayed multiple times also in \streamads, by simply generating multiple copies of such an ad.
Besides, the \streamads problem also generalizes the classic maximum-weight matching problem (\mwm),
obtained from \streamads by setting the value $\q=0$.
\citet{ieong2014advertising} also prove that there is no \emph{online} algorithm with a constant competitive ratio for \streamads. 
Hence we focus on the offline settings. 

Finally, in \cref{sec:exp:ablation}
we also discuss how to adapt an algorithm for \streamads 
to %
enforce a limit on the total number of ads to be displayed,
which can be useful, for example, to avoid ad fatigue. %

