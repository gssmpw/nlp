We provide the first comprehensive empirical study 
on the \streamads problem. 
We do not consider the \streamadsr problem, 
as it is a special case of the \streamads problem, and 
significantly less challenging given that it can be solved optimally by our~\alggback algorithm. %

%\newpage
\noindent
Our evaluation investigates the following key questions.
	
(1) How do the algorithms perform 
	by fixing the bipartite graph structure,
	and varying the weights of the rewards? (\cref{sec:exp:bip})	

(2) What is the impact of the problem parameters, such the quitting probability $\q$, the number of ads $\nads$, and slots $\nV$? (\cref{sec:exp:ablation})

(3) How do the algorithms perform for the task of native advertising in content feeds in two realistic scenarios? (\cref{sec:exp:ad})

Our source code is made public for reproducibility.{\code}



We now describe the datasets and baselines, while details on the environment 
are presented in \cref{app:exp}.
Note that, to enhance robustness, all results report averages over three independent runs.

\smallskip
\noindent
\emph{Datasets.}
To the best of our knowledge, 
high-quality public real datasets for native advertising are scarce, and existing work mostly uses proprietary data~\cite{yan2020ads,carrion2021blending,liao2022cross}.
Hence, we 
explored %
two distinct types of datasets for our evaluation.
The first type considers random weighted bipartite graphs.
Such data is very general, %
and provides a comprehensive benchmark for the various algorithms considered.
The second type of data is obtained by simulating a scenario of native advertising based on real anonymized ad data; more details are discussed in \cref{sec:exp:ad}.


\smallskip
\noindent
\emph{Algorithms and baselines.}
We evaluate the performance of our algorithms:
the proposed greedy algorithms \alggback (\cref{alg:greedy-back}) and \alggbackproxy (\cref{alg:greedy-backproxy}), and the practical global greedy algorithm \alggglobal.
Baseline algorithms consist of:
two online greedy algorithms \alggforward and \alggonline,
the flow-based algorithm {\algflow} and its augmented variant \algflowg, and
the matching-based algorithm \algmwm.
We set the threshold of \alggonline to be the best reward of an ad allocation to the first slot.
We refer the reader to \cref{sec:algs:others} for a detailed description of the above baselines.



\subsection{Experiments on synthesized bipartite graphs}\label{sec:exp:bip}

\begin{figure}[t]
	\centering
	\subcaptionbox{Symmetric random weighting \label{fig:exp:bip:sym}}[0.49\columnwidth]{
		{\includegraphics{pics/method-R-0.pdf}}
	}
	\hfill
	\subcaptionbox{Finely targeted weighting \label{fig:exp:bip:target}}[0.49\columnwidth]{
		{\includegraphics{pics/method-R-1.pdf}}
	}
	
	\subcaptionbox{Asymmetric random weighting (heavy tops) \label{fig:exp:bip:asym-top}}[0.49\columnwidth]{
		{\includegraphics{pics/method-R-2.pdf}}
	}
	\hfill
	\subcaptionbox{Asymmetric random weighting (heavy bottoms) \label{fig:exp:bip:asym-bottom}}[0.49\columnwidth]{
		{\includegraphics{pics/method-R-3.pdf}}
	}
	\caption{Comparisons on synthesized bipartite graphs with different weighting schemes. Error bars indicate the standard deviation.}
	\label{fig:exp:bip}
\end{figure}


In this setting, we first generate a fixed complete bipartite graph over
$\nads=100$ ads, and
$\nV=1000$ slots.
We evaluate the various algorithms
when the input instance has the following three different 
weighting schemes for the rewards over the edges of the graph:
1) symmetric random weighting,
2) asymmetric random weighting, and
3) finely targeted weighting.
Each setting is described in detail below.
We also fix $\q = 0.1$. %


\smallskip
\noindent
\emph{Symmetric random weighting.}
Each edge of the complete bipartite graph has its weight drawn uniformly at random from 1 to 10.

\smallskip
\noindent
\emph{Asymmetric random weighting.}
The random weighting scheme above
has symmetric edge weights for different slot positions,
which is not common in practice.
We break such symmetry and introduce dependencies with slot positions, by the following two methods.

In the first method, edges connecting a top slot have a larger reward.
More specifically, the reward $\rw_{ij}$ for assigning ad $\ad_i$ to slot $j$ is %
$\rw_{ij} = w \!\cdot\! (\nV - j) / \nV$, with $w$ a random real number in $[1,10]$, i.e., $\rw_{ij}$ likely decreases over increasing slot positions.
In the second method, edges connecting a bottom slot have a larger reward, that is, 
$\rw_{ij} = w \!\cdot\! j / \nV$.

\smallskip
\noindent
\emph{Finely targeted weighting.}
In practice, an ad may be highly relevant to just a few items.
To simulate this scenario,
for each ad $\ad_i$ we select a random slot $j$ and set $r_{ij}=10$, while setting $r_{ij'}=1$ for all other slots $j'\neq j$.

\smallskip
\noindent
\emph{Discussion.}
Results are reported in \cref{fig:exp:bip}.
We first note that the \alggonline\ algorithm has the worst performance, yielding zero reward on most instances. %
This is likely caused by the fact that its performances heavily depend on the threshold \Cthr, 
a parameter that is hard to optimize online.
In the current settings, a lower value  of \Cthr seems to lead to better solutions. %
The na\"{i}ve \alggforward algorithm, as expected, does not output good solutions if there are highly rewarding assignments for bottom slots.
In contrast, the \algmwm algorithm often outputs a solution with expected reward close to the best observed one, despite not accounting for decaying attention.
The 4-approximation algorithm \algflow achieves significantly lower expected rewards compared to the highest reward over all~algorithms. %

Our backwards greedy algorithms \alggback and \alggbackproxy,  \alggglobal, and \algflowg,
consistently outperform all other methods and achieve the highest expected reward over all settings,
with the global greedy algorithm \alggglobal providing slightly better solutions. %

\subsection{Ablation study}\label{sec:exp:ablation}

\begin{figure}[t]
	\centering
	
	\includegraphics{pics/graph-legend-ncol4.pdf}
	
	\subcaptionbox{Scaling the number of ads \label{fig:exp:ablation:ads}}[0.49\columnwidth]{
		{\includegraphics{pics/n1-runtime.pdf}}
	}
	\hfill
	\subcaptionbox{Scaling the number of items \label{fig:exp:ablation:items}}[0.49\columnwidth]{
		{\includegraphics{pics/n2-runtime.pdf}}
	}
	
	\subcaptionbox{Reward by varying \q \label{fig:exp:ablation:q-R}}[0.49\columnwidth]{
		{\includegraphics{pics/q-R.pdf}}
	}
	\hfill
	\subcaptionbox{Reward by varying $k$ \label{fig:exp:ablation:k-R}}[0.49\columnwidth]{
		{\includegraphics{pics/k-R.pdf}}
	}
	\caption{Effects of parameters $\nads, \nV, \q, k$.}
	\label{fig:exp:ablation}
\end{figure}

In this section, we investigate the effect of the various parameters, 
that may affect the performance of the algorithms.
We study 
the scalability with respect to the size of the bipartite graph, sensitivity to the decaying factor \q, and
to an additional cardinality constraint on the total number of ads to be displayed. %
We use the symmetric random weighting introduced previously for the edge weights.

\smallskip
\noindent
\emph{Scalability.}
We fixed $\q = 0.1$.
To test the scalability with respect to the input size, 
we start with
$\nads=100$ and
$\nV=1000$, and
vary the number of ads \nads and the number of videos \nV separately. %
The results are shown in \cref{fig:exp:ablation:ads} and \cref{fig:exp:ablation:items}, respectively.
We set a time limit of one hour for each run.
\algflow and \algmwm clearly 
have the largest running time, %
as they solve expensive optimization sub-problems.
Then, %
\alggglobal has also high running time, especially when $\nads$, the number of ads, grows, 
and is less sensitive to the number $\nV$ of slots due to the lazy evaluation of the rewards,
a technique 
we introduced in \cref{sec:algs:others}.
Considering our backwards-greedy algorithms \alggbackproxy and \alggback,
\alggbackproxy is significantly faster than \alggback,
since it uses a lower bound %
of the true marginal reward, achieving remarkable speedups. %
As expected, the two online algorithms are the fastest, at the expense of significantly lower rewarding solutions.

\smallskip
\noindent
\emph{Effect of \q.}
We fix the size of the complete bipartite graph, of ads and slots, to be
$\nads=100$ and 
$\nV=1000$, 
and we vary the parameter \q.
The result is shown in \cref{fig:exp:ablation:q-R}.
Clearly the expected reward drops as \q increases, as users are more likely to quit browsing early in the session.
We also note that the \algflow algorithm, cannot output a nonzero solution when $\q>0.5$; more details are on the original paper~\citep{ieong2014advertising}, making it not practical for general applications.


\smallskip
\noindent
\emph{Effect of size limit on ads.}
Given an integer $k$, we can adapt the algorithms to produce a matching of size \emph{at most} $k$ as follows.
We terminate the greedy \alggglobal and online algorithms after $k$  ad allocations.
We set the cardinality constraint of the \algflow algorithm to be exactly~$k$.
While, for all the other algorithms, we iteratively remove one ad at a time whose removal minimizes the loss in the expected reward, if more than $k$ slots are matched in their solution.

We fix $\nads=100$, $\nV=1000$ and $\q = 0.1$.
The result are in \cref{fig:exp:ablation:k-R}.
Overall, most algorithms obtain similar performance.
Moreover, as $k$ exceeds 20, their revenue reaches a plateau, and further ads bring unnoticeable benefit, in accordance with the value of $q$. %


\subsection{Simulated native advertising}\label{sec:exp:ad}

\begin{table}
	\caption{Datasets based on real advertisement. We report: $\nads$ number of ads to place, $\nV$ available slots, $|\E|$ number of edges, the range of the rewards and the value of $\q$ used in the experiments.}
	\label{tab:stats}
	\resizebox{0.99\columnwidth}{!}{
		\begin{tabular}{lcccrc}
			\toprule
			Dataset & $\nads$ & $\nV$ & $|\E|$ & $r_e$ ([$\min$ - $\max$]) & $\q$ \\
			\midrule
			YouTube & 120 & 14 999 & 1 799 880 & 2.9$\cdot 10^{-5}$ - 3.92$\cdot 10^5$& 0.1\\
			Criteo & 14 400 & 1 440 & 144 000 & 8.4$\cdot 10^0$ - 1.5$\cdot 10^3$& 0.1\\
			\bottomrule
		\end{tabular}
		}
\end{table}


\begin{figure}[t]
	\centering
	\subcaptionbox{Youtube dataset \label{fig:exp:real:yt}}[0.49\columnwidth]{
		{\includegraphics{pics/method-R-realData-yt.pdf}}
	}
	\hfill
	\subcaptionbox{Criteo dataset\label{fig:exp:real:criteo}}[0.49\columnwidth]{
		{\includegraphics{pics/method-R-realData-criteo}}
	}
	\caption{Comparisons on simulated native advertising using real data.}
	\label{fig:exp:real}
\end{figure}


\begin{figure}[t]
	\centering
	
	\includegraphics{pics/graph-legend-ncol4}
	
	\subcaptionbox{YouTube dataset \label{fig:exp:cumulYT}}[0.49\columnwidth]{
		{\includegraphics{pics/cum-realData-yt}}
	}
	\hfill
	\subcaptionbox{Criteo dataset \label{fig:exp:cumulCrit}}[0.49\columnwidth]{
		{\includegraphics{pics/cum-realData-criteo}}
	}
	\caption{Distributions of the selected slot index. 
		}
	\label{fig:exp:cumulativeCurves}
\end{figure}

