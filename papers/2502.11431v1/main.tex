% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{adjustbox}
\usepackage{amsfonts}
\usepackage{arydshln}

\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{colortbl}
\usepackage{float}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{fontawesome5}
\definecolor{my_green}{RGB}{51,102,0}
\definecolor{my_red}{RGB}{204, 0, 0}
\renewcommand{\checkmark}{\textcolor{my_green}{\ding{51}}} % 
\newcommand{\crossmark}{\textcolor{my_red}{\ding{55}}} % 


\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{makecell}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


\usepackage{graphicx}

\usepackage{dblfloatfix}

% \title{Visualized Information Retrieval With Universal Screenshot Embeddings} 
% \title{An Information Need Is Worth One Single Screenshot}  
% \title{An Information Need Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval}  
% \faIcon{camera-retro}
% \title{An Information Need Is Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval}  

% \title{Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval}  

\title{Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval}  



% definition of visualized information retrieval, i.e., Vis-IR (with one figure for it)
% the value of visualized information retrieval
% related works: text retrieval, multimodal retrieval, preliminary screenshot retrieval
% three-fold values in this work:
% - the curation of massive captioned screenshot data and instruction-tuning data
% - the first large-scale benchmark for visualized information retrieval, covering various task forms and application domains, and comprehensive evaluation of the existing multi-modal retrievers' performance on it
% - the training and public releasing of a family of screenshot embedding models, covering variety of sizes and model architectures, huge advantage over existing methods 

% data curation:
% captioned data
% - data sources: (domain: news, product, papers ...; types: vis-text, figures, tables)
% - data volume 
% - data curation methods: (caption prompt)  
% instructioned data (?) 


% benchmark (the first large-scale benchmark for Vis-IR problem)
% comprehensiveness: 
% - task forms
% - application domains 
% practicability
% - scale of corpus 
% - control of hardness 
% comprehensive performance benchmarking 
% - different mm-ir models and architectures (clip, mllm) 

% model training 
% a family of UniSE models with two architectures 
% training methods


\author{Ze Liu$^{1,2*}$, \ \ \ Zhengyang Liang$^{1,3*}$, \ \ \ Junjie Zhou$^{1,3*}$, \ \ \, Zheng Liu$^{1,4*}$\thanks{Core contributors, with Zheng Liu as the project lead.}, \ \ \  Defu Lian$^{2}$ \\
$^1$ BAAI, \ \ \ \ $^2$ USTC,  \ \ \ \ $^3$ BUPT,  \ \ \ \ $^4$ HKPU \\
\texttt{zhengliu1026@gmail.com} 
} 




\begin{document}
\maketitle 

\begin{abstract}
With the popularity of multimodal techniques, it receives growing interests to acquire useful information in visual forms. In this work, we formally define an emerging IR paradigm called \textit{Visualized Information Retrieval}, or \textbf{Vis-IR}, where multimodal information, such as texts, images, tables and charts, is jointly represented by a unified visual format called \textbf{Screenshots}, for various retrieval applications. We further make three key contributions for Vis-IR. First, we create \textbf{VIRA} (Vis-IR Aggregation), a large-scale dataset comprising a vast collection of screenshots from diverse sources, carefully curated into captioned and question-answer formats. Second, we develop \textbf{UniSE} (Universal Screenshot Embeddings), a family of retrieval models that enable screenshots to query or be queried across arbitrary data modalities. Finally, we construct \textbf{MVRB} (Massive Visualized IR Benchmark), a comprehensive benchmark covering a variety of task forms and application scenarios. Through extensive evaluations on MVRB, we highlight the deficiency from existing multimodal retrievers and the substantial improvements made by UniSE. Our work will be shared with the community, laying a solid foundation for this emerging field. 
\end{abstract} 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/screenshot.pdf}
    \vspace{-0.5cm}
    \caption{A use case of Vis-IR. Users take a screenshot of their interested news by circling and selection, and search for relevant news reports associated with ``Nvidia'' based on a query conditioned on the screenshot.} 
    \vspace{-0.5cm}
    \label{fig:example}
\end{figure}  


\section{Introduction} 
Information retrieval has experienced tremendous progresses in recent years, driven by breakthroughs in foundation models. The growing capacity of language models has enabled precise and generalized text retrieval \cite{bge,wang2022text,e5-mistral}, while the development in vision-language models has extended information retrieval to a broader range of data modalities \cite{mbeir-wei2023uniir,magiclens,vista}. With the recent popularity of multimodal techniques \cite{blip2-li2023blip,llava-liu2023visual,team2023gemini}, there is growing interest in \textit{uniformly representing multimodal data in visual forms and leveraging these visual representations for diverse information retrieval tasks} (shown in Figure \ref{fig:example}) \cite{faysse2024colpali,ma2024unifying,zhang2024gme}. For example, researchers can circle and select a part of a paper, which may include texts, figures, equations, and figures, and search for relevant literature to address a specific question about the selected content. Similarly, people take a screenshot of an advertisement on their cellphones and retrieve descriptions of the corresponding product\footnote{\scriptsize https://blog.google/products/search/google-circle-to-search-android/}.
These innovative paradigms will greatly enhance the flexibility of information retrieval in real-world scenarios, leading to a unified paradigm for search engines. In this paper, we formally define these problems as \textit{Visualized Information Retrieval}, or \textbf{Vis-IR} for brevity. We also define the visual representation for a mixture of multimodal data as a \textbf{screenshot}, which is treated as a unified entity in the retrieval process. 

% Although common multimodal retrievers can be applied to Vis-IR in a zero-shot manner,
Despite that common multimodal retrievers can be applied for Vis-IR in a zero-shot manner, and there are preliminary works focused on improving representations of specific multimodal data, like screenshots of Wiki webpages \cite{ma2024unifying}, several fundamental challenges still persist for this emerging problem. Particularly, there are no tailored models which can offer unified, high-quality support for various Vis-IR applications. Besides, there is a lack of comprehensive benchmarks to evaluate a retriever's performance on Vis-IR tasks. Finally, no specialized datasets are curated and published for training competent Vis-IR models. 

To address the above challenges, we present the following three resources, which are critical to the technical advancement of Vis-IR. 

First, we create a large-scale dataset, namely \textbf{VIRA} (Vis-IR, Aggregation), which comprises a vast collection of 13 million screenshot data from diverse sources, like {webpages} from news websites, shopping platforms, and Wikipedia, homepages from GitHub, research papers from ArXiv, general PDF documents, and various forms of charts. Each screenshot is assigned with a \textit{fine-grained caption} by either extracting from meta data or annotating with sophisticated OCR tools. We further produce two types of \textit{question-answer data} for the well-captioned screenshots: 1) q2s tuples, which comprises a screenshot and a query about the screenshot, 2) sq2s triples, which contain source screenshots, conditioned queries about the source, and target screenshots. Altogether, over 20 million data samples are included by VIRA. 

Second, we develop a family of embedding models, called \textbf{UniSE} (Universal Screenshot Embeddings), built on top of our created dataset. These models enable screenshots to either serve as queries or be queried by various data formats. For example, using a screenshot to search for an image or a document, or retrieving a screenshot based on a textual question. We adopt two alternative model architectures for UniSE: one based on CLIP \cite{clip-radford2021learning}, which is more efficient, the other one leveraging MLLMs \cite{llava-liu2023visual}, which is more expressive. This allows users to flexibly choose the model that best suits their own needs. 

Third, we construct \textbf{MVRB} (Massive Visualized IR Benchmark) to evaluate multimodal retrievers' performance on general Vis-IR tasks. The benchmark includes various task types, such as screenshot-based multimodal retrieval (screenshot to anything, anything to screenshot) and screenshot-conditioned retrieval (e.g., searching for documents using queries conditioned on screenshots). It also covers a variety of important domains, including news, products, papers, and charts. We conduct a comprehensive experimental study using MVRB, which highlights significant deficiencies in existing methods and uncovers key factors influencing the development of Vis-IR retrievers. The results also demonstrate the effectiveness of UniSE in both in-domain and out-of-domain applications. 
 
To summarize, we define the Vis-IR problem, which holds promising prospects. We present three key resources: a large-scale dataset, powerful models, and a comprehensive benchmark, which are foundational to Vis-IR and will be shared with the public to facilitate technical advancement. 

% \section{Related Works}
% % In this section, we overview relevant literature on neural document retrieval and multimodal retrieval. 

% % Neural document retrieval: 1. progresses driven by pre-trained languages and large language models, 2. breakthroughs in general-purpose text retrievers and multilingual text retrieval, 3. extensive applications in reality. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/screenshot_data.pdf}
    \vspace{-0.3cm}
    \caption{Creation process of VIRA dataset, including 1) comprehensive screenshot collection from various sources, 2) fine-grained screenshot captioning, 3) similar screenshots mining, 4) q2s annotation, and 5) sq2s annotation.} 
    \vspace{-0.3cm}
    \label{fig:data}
\end{figure*}  

% \section{Related Works}
% In this section, we overview relevant literature on neural document retrieval and multimodal retrieval. 

% Neural document retrieval: 1. progresses driven by pre-trained languages and large language models, 2. breakthroughs in general-purpose text retrievers and multilingual text retrieval, 3. extensive applications in reality. 

% \paragraph{Neural Document Retrieval.}
% Neural document retrieval is crucial for a wide range of applications, like search engines, open-domain question answering, and retrieval-augmented generation~\cite{dpr,lewis2020retrieval,yu2024visrag}. Traditionally, document retrieval has predominantly been text-driven, with significant advancements propelled by pre-trained language and large language models. These models have facilitated the development of effective document retrieval systems~\cite{DBLP:conf/emnlp/Ni0LDAMZLHCY22,wang2022text,bge,llama2vec}. However, text-based retrieval methods require prior document parsing and are incapable of handling unstructured information within documents, such as complex layouts, charts, and visual elements~\cite{ma2024unifying}. This limitation results in information loss and highlights the need for new retrieval methods that go beyond text. 

% % Multimodal retrieval: 1. multimodel retrieval is made possible by the advancement of multimodal representation learning. VLM: CLIP, ALIGN, SigLIP, and multimodal LLMs: Llava, Gemini, 2. multimodal retrieval by making direct use of CLIP or using fine-tuned MLLMs, 3. growing interest in developing general-purpose multimodal retrievers, progresses made by VISTA, MagicLens, E5-V, MM-Ret, UniIR, VLM2Vec, driven by either leveraging powerful model backbones, or creating extensive training data.  

% \paragraph{Multimodal Retrieval.}
% The development of vision-language models (VLMs)~\cite{clip-radford2021learning, align2021, llava-liu2023visual} has significantly advanced the capabilities of general-purpose multimodal retrievers, enabling them to handle image, text, and composed image-text data~\cite{mbeir-wei2023uniir,vista,jiang2024vlm2vec,nv-mm-embed2024}. However, when applied to multimodal documents, these methods often process text and images in an interleaved manner, requiring additional preprocessing steps for content extraction and potentially leading to information loss. Recent approaches propose using document screenshots as input for unified document representation~\cite{ma2024unifying, faysse2024colpali, zhang2024gme}. By fine-tuning pre-trained VLMs with specific screenshot data, these methods demonstrate greater effectiveness than traditional text-based methods, as they preserve all information within the document. Despite these advancements, existing research still lacks a diverse range of document data and comprehensive evaluation tasks necessary for building and evaluating unified general-purpose visualized information retrieval systems.
% \clearpage 

\section{Dataset: VIRA} 
In this section, we present the VIRA (Vis-IR Aggregation) dataset, which offers diverse and large-scale training data for developing Vis-IR models. While creating the dataset, we emphasize two main factors: 1. \textbf{comprehensiveness} of screenshots, 2. \textbf{utility} of annotation. The creation's outline is introduced as follows. Due to the space limitation, the entire details are provided in Appendix~\ref{appendix:details-VIRA}. 


% As a primary compotennt of this work, we create the VIRA (Vis-IR Aggregation) dataset, which offers diverse and large-scale training data for developing Vis-IR models. While creating the dataset, we emphasize two main factors: 1. comprehensiveness of screenshots, 2. quality of annotation. To this end, we introduce the following workflow\footnote{Given the complexity of VIRA dataset and space limitations, we outline only the main framework here, with detailed specifications provided in the Appendix~\ref{appendix:details-VIRA}.}. 

\subsection{Screenshot collection}
We perform massive collection of screenshot data spanning \textbf{seven categories}, which include \textit{News}: from popular websites like BBC, CNN, and Fox, \textit{Products}: from Amazon, \textit{Research Papers}: from ArXiv, \textit{General Documents}: from PDF Association, \textit{Charts}: from ArXiv, \textit{Common Knowledge}: from Wikipedia; \textit{Project Homepages}: from GitHub. The data is evenly distributed across these categories, leading to \textbf{13 million screenshots} in total. The collected data encompasses a rich variety of formats, showing diverse combinations of text, figures, and structures presented in various layouts. These features substantially enhance the generalization for models trained on corresponding data. 

\subsection{Screenshot Annotation}
We perform two types of annotations for the collected data. First, we assign each screenshot with a fine-grained \textbf{caption}, which paraphrases its detailed semantic. The caption is produced by in alternative ways. For those associated with complete meta data, e.g., project homepage from Github, we apply proper data extraction pipelines to acquire the captions. While for other free-form screenshots, we obtain captions based on OCR methods. Second, we create \textbf{question-answering} annotations for the well-captioned screenshots, which closely align with typical retrieval tasks. Specifically, we consider the following forms of question-answering data. 1. \textbf{q2s tuples}, denoted as (q, s). For a screenshot $s$, we prompt a large language model (LLM) to generate a question $q$ based on the screenshot's caption. For example, given a screenshot with the title “\textit{Nvidia posts record cap-loss due to DeepSeek}”, a question like “\textit{What’s the impact of DeepSeek on Nvidia?}” would be generated. 2. \textbf{sq2s triplets}, denoted as (s1, q, s2). Following MegaPairs methodology~\cite{zhou2024megapairs}, for a pair of relevant screenshots (s1, s2) mined from the corpus, we prompt the LLM to analyze their relationship based on their captions and generate a relational question. For example, if s1 is a breaking news screenshot and s2 is a subsequent news report, a question like “\textit{What’s the follow-up news to it?}” is generated. 
Finally, we create \textbf{13 millions} screenshot-caption samples and \textbf{7 millions} question-answering samples from the above operations. 

We supplement the above data with \textbf{hard negatives}, which are crucial for training retrieval models. For each $q2s$ tuple, we introduce hard negatives based on either textual or visual similarity (using off-the-shelf embedding models). For a $q2s$ tuple, a screenshot $s'$ is selected if it meets any of these conditions: \textit{1}) $s'$ has a similar caption with \textit{s}, \textit{2}) $s'$ is visually similar with \textit{s}, \textit{3}) $s'$ is textually similar with \textit{q}. While for a $s_1q$2$s_2$ triplet, we use $s'$ as a hard negative, if it enjoys a strong textual or visual similarity with the retrieval target $s_2$. 


% \clearpage
\section{Model: UniSE} 
We develop retrieval models based on VIRA dataset called Universal Screenshot Embeddings (UniSE), providing unified supports for general Vis-IR tasks. 

\subsection{Embedding}
We adopt two structures for UniSE, allowing users to make flexible selection based on their own needs. 

\paragraph{UniSE-CLIP.}
The first type adopts the \textit{CLIP architecture} \cite{clip-radford2021learning}, which is more time-efficient. In this form, the screenshot $s$ and text input $t$ are encoded by the visual and textual encoder of CLIP, respectively: $\boldsymbol{e}_s \leftarrow \phi_v(s)$, $\boldsymbol{e}_t \leftarrow \phi_t(t)$. For a composed input of a screenshot $s$ and conditioned textual query $q$, the joint representation is computed by linear combining the screenshot and query's embedding: $\boldsymbol{e}_{s,q} \leftarrow \boldsymbol{e}_s + \boldsymbol{e}_q$. Specifically, we utilize OpenAI's CLIP-Large\footnote{https://huggingface.co/openai/clip-vit-large-patch14}, leveraging both its architecture and pre-trained weights as the foundation for UniSE-CLIP.


\paragraph{UniSE-MLLM.}
The second type is back-ended by \textit{MLLMs (multimodal large language models)} \cite{llava-liu2023visual,wang2024qwen2}, which is more expressive, especially in representing composed inputs. Without losing generality, a composed query is presented by the following template: 
\begin{equation*}
    \text{[Task]: \$task}, \text{[Query]: \$s-tok, \$q-tok, [EOS]}
\end{equation*}
In this place, \$task indicates the task type, \$s-tok stands for the visual tokens from screenshot, while \$q-tok represents the textual tokens from query. The output embedding from the special token [EOS] is used to represent the composed input. For UniSE-MLLM, we adopt Qwen2-VL-2B\footnote{https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct} and initialize it with its pre-trained parameters.


\subsection{Training}
We propose a two-stage training workflow based on the composition of VIRA dataset. First, we perform pre-training using screenshots and their captions, denoted as $\{(s_i,c_i)\}_{N}$. In this stage, the model aims to capture the fine-grained semantic about the screenshot by learning to discriminate its relevant caption from irrelevant ones. Thus, we employ a bidirectional contrastive learning loss, ensuring that the model can correctly match screenshots to their captions and captions to their screenshots:
\begin{equation}
\label{eq-loss1-1}
    \mathcal{L}_{s_1} = \mathcal{L}_{con}(\boldsymbol{e}_s, \boldsymbol{e}_c) + \mathcal{L}_{con}(\boldsymbol{e}_c, \boldsymbol{e}_s)
\end{equation}
where \(\boldsymbol{e}_s\) and \(\boldsymbol{e}_c\) represent the embeddings of screenshots and captions, respectively. The function \(\mathcal{L}_{con}(\boldsymbol{u}, \boldsymbol{v})\) is formulated as:
\begin{equation}
\label{eq-loss1-2}
\mathcal{L}_{con}(\boldsymbol{u}, \boldsymbol{v}) = -\frac{1}{\left |\mathcal{B}\right |} \sum_{i\in \mathcal{B}}^{} \log\frac{\exp(\boldsymbol{u}^{T}_{i}\boldsymbol{v}_{i}/\tau )}{\sum_{j\in \mathcal{B}}^{}\exp(\boldsymbol{u}^{T}_{i}\boldsymbol{v}_{j}/\tau ) }    
\end{equation}
where $\mathcal{B}$ represents the set of in-batch query samples, and $\tau$ is the temperature parameter that controls the strength of penalties on negative samples. 

We continue to fine-tune the pre-trained model based on the question-answering data, denoted as $\{(q_i,s_i)\}_{N}$ ($q_i$ can either be a textual query for a q2s tuple, or a combination of screenshot and its conditioned query for a sq2s triplet.) With this operation, the model is strengthened in handling retrieval-related tasks. In this stage, the following loss function is minimized: 
\begin{equation}
\mathcal{L}_{s_2} = \mathcal{L}_{con}(\boldsymbol{e}_q, \boldsymbol{e}_s)
\label{eq:loss_s2}
\end{equation}
where \(\boldsymbol{e}_q\) and \(\boldsymbol{e}_s\) represent the embeddings of queries and screenshots, respectively. Additional training details and the hyper-parameter settings are provided in~\Cref{appendix:training-details}.


% we continue to train retrieval models on top of vira, which provide unified support for general vis-ir tasks 
% we consider two alternative model strcutures
% one adopts CLIP architecture, which is more time-efficient. We directly use its visual encoder to generate a screenshot embedding and leverage its text encoder to process a textual input
% we linearly combine the text and visual embeddings to represent the screenshot and its conditioned textual query 
% the other one uses mllm architecture, which is more expressive in representing the mixed input of both screenshot and text (e.g., conditional query, search intent)
% use following template: <task> \$task <query> \$screenshot, \$query [eos] 
% use the output embedding from [eos] to represent the input data 

% propose a two-stage training workflow 
% first stage: pre-training 
% learn to capture fine-grained semantic of screenshot
% thus, perform contrastive learning, where a screenshot can discriminate its caption by embedding similarity: 
% loss function here 
% second stage: retrieval-oriented fine-tuning
% learn to generate discriminative embeddings for general retrieval tasks 
% make use of the question-answering data along with the hard negatives 
% loss function here  

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/screenshot_bench-new.pdf}
    \vspace{-0.3cm}
    \caption{MVRB benchmark. There are four task categories: screenshot retrieval, composed screenshot retrieval, screenshot question answering, and open-vocab classification. Each category covers multiple concrete task scenarios.}  
    \vspace{-0.5cm}
    \label{fig:benchmark}
\end{figure*}  

% \clearpage
% \twocolumn[{
%   \centering
%   \includegraphics[width=\textwidth]{fig/benchmark_main.pdf}
%   \captionof{figure}{A use case of Vis-IR. Users take a screenshot of their interested news by circling and selection, and search for relevant news reports associated with ``Nvidia'' based on a query conditioned on the screenshot.}
%   \label{fig:benchmark}
%   \vspace{1em} % 根据需要调整图和正文的间距
% }]


\section{Benchmark: MVRB} 
We create the Massive Vis-IR Benchmark (MVRB), which comprehensively evaluate a retriever's capability in handling general Vis-IR tasks. The outline of MVRB and its creation process is presented below, with more details provided in ~\Cref{appendix:detail-of-MVRB}.

% We propose massive visualized benchmark(MVRB) for this emerging field. Following the previous benchmarks on embedding~\cite{jiang2024vlm2vec, mteb}, 
\subsection{Evaluation Tasks} 
Following the practice of popular retrieval benchmarks, such as MTEB \cite{muennighoff2022mteb} and MMEB \cite{jiang2024vlm2vec}, we introduce four task categories, each encompassing multiple tasks relevant to critical application scenarios. 

\subsubsection{Screenshot Retrieval}
Screenshot Retrieval (SR) consists of evaluation samples, each comprising a textual query $q$ and its relevant screenshot $s$: ($q$, $s$). The retrieval model needs to precisely retrieve the relevant screenshot for a testing query from a given corpus $S$. Each evaluation sample is created in two steps: 1) sample a screenshot $s$, 2) prompt the LLM to generate a search query based on the caption of screenshot. We consider seven tasks under this category, including product retrieval, paper retrieval, repo retrieval, news retrieval, chart retrieval, document retrieval, and slide retrieval. 



\subsubsection{Composed Screenshot Retrieval}
Composed Screenshot Retrieval (CSR) is made up of sq2s triplets. Given a screenshot $s1$ and a query $q$ conditioned on $s1$, the retrieval model needs to retrieve the relevant screenshot $s2$ from the corpus $S$. 
We define four tasks for this category, including product discovery, news-to-Wiki, knowledge relation, and Wiki-to-product.
All tasks in this category are created by human annotators. For each task, annotators are instructed to identify relevant screenshot pairs and write queries to retrieve \(s_2\) based on \(s_1\). Further details on task definitions and the annotation process are provided in \Cref{appendix:CSR-details}.

%with detailed 
% Each evaluation sample is created in three steps: 1) mine a pair of similar screenshots $s1$ and $s2$ from database, 2) prompt the MLLM to generate the relationship between $s1$ and $s2$, 3) prompt the LLM to generate a conditioned query for $s1$ that makes $s2$ as the retrieval target. 
% We create four tasks for this category, including product discovery, news-to-Wiki, knowledge relation, Wiki-to-product. 

\subsubsection{Screenshot Question Answering} 
Screenshot Question Answering (SQA) comprises sq2a triplets. Given a screenshot $s$ and a question $q$ conditioned on $s$, the retrieval model needs to retrieve the correct answer $a$ from a candidate corpus $A$. Each evaluation sample is created in three steps: 1) sample a screenshot $s$, 2) prompt the MLLM to generate a question $q$, 3) prompt the MLLM to generate the answer $a$ for $q$ based on $s$. The following tasks are included in this category: product-QA, news-QA, Wiki-QA, paper-QA, repo-QA. 

\subsubsection{Open-Vocab Classification}
Open-Vocab Classification (OVC) is performed using evaluation samples of screenshots and their textual class labels. Given a screenshot $s$ and the label class $C$, the retrieval model needs to discriminate the correct label $c$ from $C$ based on the embedding similarity. We include the following tasks in this category: product classification, news-topic classification, academic-field classification, knowledge classification. For each task, we employ human labelers to create the label class and assign each screenshot with its correct label. 

% quality control 
% two-stage workflow 
% machine examination and human verification 
% machine examination employs an quality-control committee consisting of multiple mllms, perform automatic examinization based on three principles: 1. clarify, 2. reasonability, 3. correctness
% for each evaluation sample 
% the mllm is prompted to confirm: 1, 2, 3 
% If a condition is determined false by any of mllms, then the corresponding sample is removed 
% If a sample passes the automatic assessment, it is manually checked by the human labors based on the same principles 
% It is added to the test set if it passes human verification 

% corpus contruction
\subsection{Optimizations}  
We conduct the following operations to optimize the quality and usability of the benchmark. 

We first introduce \textbf{quality-control} while creating the evaluation samples. Our quality-control framework comprises two main operations: \textbf{automatic assessment} and \textbf{human verification}. We employ multiple MLLMs to constitute a quality-control committee, which make assessment based on three criteria. 1) Clarify, whether the query conveys a concrete information need, 2) Reasonability, whether the query is appropriate in practice, 3) Correctness, whether the query can be addressed by the retrieval target (a screenshot or an answer).  Each evaluation sample is independently reviewed by the MLLMs, and if it fails to meet any criterion, it is removed. The remaining samples are then verified by human labelers using the same principles. An evaluation sample is successfully created only if it passes both quality-control stages. 

We then \textbf{construct the corpus} for each retrieval task. On the one hand, a corpus needs to maintain sufficient difficulty to effectively distinguish the capabilities of different retrievers. On the other hand, its scale should be properly controlled to prevent excessive evaluation costs. Considering these factors, the retrieval corpus is created in two steps.  First, a moderate number of retrieval candidates (around 5,000) are randomly sampled for each task\footnote{This allows users to complete their evaluation in a few hours using one A800-80G GPU, e.g., 3.5h for UniSE-MLLM.}. Second, a group of hard negative samples are introduced for each sample. For (composed) screenshot retrieval, hard negatives are selected based on their similarity to the query. For screenshot question answering, hard negatives are generated by modifying the correct answer using an LLM (see Appendix \ref{appendix:detail-of-MVRB} for details). The hard negatives also undergo quality-control before being added to the corpus. 

% With the above operations, the evaluation of UniSE-MLLM on MVRB can be completed in 5 hours using one A800 (80G) GPU. 

% \clearpage



\begin{table*}[t]
\footnotesize
\resizebox{\textwidth}{!}
{
\begin{tabular}{@{}cccccccc}
\toprule
\multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Models}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Backbone}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{\#params}}} & \multicolumn{4}{c|}{\textbf{Per Meta-Task Score}}                                                        & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Overall}}} \\ \cmidrule(lr){4-7}
\multicolumn{1}{l|}{}                                 & \multicolumn{1}{c}{}                                   & \multicolumn{1}{c|}{}                                   & \multicolumn{1}{c}{SR} & \multicolumn{1}{c}{CSR} & \multicolumn{1}{c}{SQA} & \multicolumn{1}{c|}{OVC}    & \multicolumn{1}{c}{}                              \\ \midrule
\rowcolor{gray!10} 
\multicolumn{1}{l|}{\texttt{number of datasets}}    &  -- &\multicolumn{1}{c|}{--}       & \texttt{7}             & \texttt{4}   & \texttt{5}   & \multicolumn{1}{c|}{\texttt{4}} &\texttt{20}\\\midrule
\multicolumn{8}{c}{\bf{OCR + Text Retrievers}}                                                                                                                                                                                                                                                                                             \\ \midrule
\multicolumn{1}{l|}{BM25 \cite{bm25-robertson2009probabilistic}}                             &            --              & \multicolumn{1}{c|}{--}                                   & 40.71                & 28.51                 & 38.46                  & \multicolumn{1}{c|}{6.23} & 30.81                                                                            \\
\multicolumn{1}{l|}{DPR \cite{dpr}}                              &         BERT-Base                                               & \multicolumn{1}{c|}{109M}                                   & 24.98                 & 18.73                  & 27.35                  & \multicolumn{1}{c|}{22.08} & 23.74                                            \\
\multicolumn{1}{l|}{BGE \cite{bge}}                              &  BERT-Base                                                       & \multicolumn{1}{c|}{109M}                                   & 45.67                 & 36.73                 & 36.29                  & \multicolumn{1}{c|}{47.91} & 41.99                                            \\
\multicolumn{1}{l|}{E5-Mistral \cite{e5-mistral}}                              &  Mistral-7B                                                       & \multicolumn{1}{c|}{7.11B}                                   & 38.68                 & \underline{51.44}                 & 43.00                  & \multicolumn{1}{c|}{45.08} & 45.51                                            \\ \midrule
\multicolumn{8}{c}{\bf{General Multimodal Retrievers}}                                                                                                                                                                                                                                                                                            \\ \midrule
\multicolumn{1}{l|}{VISTA \cite{vista}}                            &   BERT-Base                                                    & \multicolumn{1}{c|}{196M}                                   &    5.21                    &        11.29                 &   25.78                      &   \multicolumn{1}{c|}{16.61}       &          13.85                                         \\
\multicolumn{1}{l|}{Uni-IR \cite{mbeir-wei2023uniir}}                           &   CLIP-Large                                                     & \multicolumn{1}{c|}{428M}                                  & 12.35                 & 35.92                  & 29.68                  & \multicolumn{1}{c|}{20.06} & 19.63                                            \\

\multicolumn{1}{l|}{CLIP \cite{clip-radford2021learning}}                             &                       CLIP-Large                                 & \multicolumn{1}{c|}{428M}                                    & 18.89                 & 25.39                  & 23.90                  & \multicolumn{1}{c|}{30.40} & 23.75                                            \\
\multicolumn{1}{l|}{SigLIP \cite{siglip2023}}                           &                                     SOViT-400m                   & \multicolumn{1}{c|}{878M}                                    & 38.33                 & 34.48                  & 19.60                  & \multicolumn{1}{c|}{40.64} & 33.34                                           \\
% \multicolumn{1}{l|}{MMRet}                            &                                                        & \multicolumn{1}{l|}{}                                   & 0.1138                 & 0.1595                  & 0.1682                  & \multicolumn{1}{l|}{0.2116} & 0.1633                                            \\
\multicolumn{1}{l|}{E5-V \cite{e5-v2024}}                             &  LLaVA-1.6                                                     & \multicolumn{1}{c|}{8.35B}                                    & 34.11                & 26.59                & 5.23                  & \multicolumn{1}{c|}{32.85} & 25.13                                            \\
\multicolumn{1}{l|}{VLM2Vec \cite{jiang2024vlm2vec}}                          &  Phi-3.5-V                                                      & \multicolumn{1}{c|}{4.15B}                                   & 15.93                 & 48.05                 & \textbf{49.42}                  & \multicolumn{1}{c|}{23.24} & 32.19                                           \\
\multicolumn{1}{l|}{MM-Embed \cite{nv-mm-embed2024}}                         &                          LLaVA-1.6                              & \multicolumn{1}{c|}{7.57B }                                   & 25.86                 & 40.93                  & 42.83                  & \multicolumn{1}{c|}{32.67} & 34.48                                \\  \midrule
\multicolumn{8}{c}{\bf{Screenshot Document Retrievers}}                                                                                                                                                                                                                                                                                           \\ \midrule
\multicolumn{1}{l|}{ColPali \cite{faysse2024colpali}}                          &                                  Paligemma              & \multicolumn{1}{c|}{2.92B}                                   & \underline{61.73}                 & 35.00                  & 35.32                  & \multicolumn{1}{c|}{31.04} & 43.64                                            \\ 
\multicolumn{1}{l|}{DSE \cite{ma2024unifying}}                              &                                  Phi-3-V                      & \multicolumn{1}{c|}{4.15B}                                   &61.54                  & 37.78                 &         39.24         & \multicolumn{1}{c|}{31.51} & 45.21                                            \\
\multicolumn{1}{l|}{GME \cite{zhang2024gme}}                              &                                            Qwen2-VL            & \multicolumn{1}{c|}{2.21B}                                  & {61.62}                 & 37.68                  & 37.78                  & \multicolumn{1}{c|}{\underline{47.98}} & \underline{48.14}                                            \\ 
% \midrule
% \multicolumn{8}{c}{\textbf{UniSE}}                                                                                    \\ \midrule
\multicolumn{1}{l|}{\textbf{UniSE-CLIP (ours)}}                       &                                                 CLIP-Large       & \multicolumn{1}{c|}{428M}                                   &      35.95 &            43.38              &               28.13           & \multicolumn{1}{c|}{40.62}       &     36.41                                             \\
\multicolumn{1}{l|}{\textbf{UniSE-MLLM (ours)}}             &                                                Qwen2-VL        & \multicolumn{1}{c|}{2.21B}           & \textbf{69.63}                 & \textbf{54.49}                & \underline{43.20}                  & \multicolumn{1}{c|}{\textbf{48.26}} & \textbf{55.72}                                            \\ \bottomrule
\end{tabular}
}
\vspace{-8pt}
\caption{Overall performance on MVRB (measure by Recall@1). The aggregation result for each task category and the average score is reported for each method. Top scores are marked in \textbf{bold} while runner-ups are \underline{underlined}.} 
\vspace{-15pt}
\label{tab:main results on MVRB}
\end{table*}


% overall analysis 
% unise (mllm) achieves the leading position in all tasks, outperforming baselines by big advantages 
% unise (clip) achieves the best performance at its scale, comparable with some much larger baselines, offering strong-cost effectiveness 

% mllm-based architecture 
% fine-tuning with screenshot-related data 
% existing methods' limitations: 
% - the overall performance is low 
% - severely limited in certain attributes  
% text-based method is insufficient 
% general multimodal retrievers are unable to handle vis-ir 


\section{Experiments} 
Our experiments focus on three main perspectives. \textbf{1}. A comprehensive study for existing methods' abilities in Vis-IR. \textbf{2}. The value brought by VIRA dataset. \textbf{3}. The improvement achieved by UniSE. 
%1. Baselines
%2. Main Results
%3. Ablation

% \subsection{Existing Retrievers' Performance}

\subsection{Baselines}
% We conducted a comprehensive evaluation of various retrievers on the MVRB. The current retrievers fall into three main categories:  

Our baseline methods fall into three main categories. (1) \textit{OCR + Text Retrievers}, which leverage advanced OCR tools\footnote{https://github.com/PaddlePaddle/PaddleOCR.} to extract text from screenshots, followed by textual retrieval. (2) \textit{General Multimodal Retrievers}, which are vision-language embedding models fine-tuned on diverse multimodal retrieval tasks, such as image retrieval, composed image retrieval, and visual question answering. In our experiments, these models process screenshots in a zero-shot manner. (3) \textit{Screenshot Document Retrievers}. These models are built upon MLLMs architectures and fine-tuned by screenshot-based document retrieval data. Although these models are trained on datasets involving screenshots, their effectiveness is severely constrained by limited data and task diversity, making them insufficient for general Vis-IR tasks. 
We adopt multiple popular methods for each category to ensure a thorough comparison. Detailed specifications about the baseline methods are provided in Appendix~\ref{appendix-baselines}. 

% These baselines utilize advanced OCR techniques\footnote{PaddleOCR (https://github.com/PaddlePaddle/PaddleOCR) is employed in our experiments.} to extract text from screenshot images, followed by text-based retrieval. 
% (2) \textit{General Multimodal Retrievers}: These retrievers were originally designed for processing and retrieving information from natural images. To evaluate their performance on visualized document images, we feed documents as screenshot images in a zero-shot manner, as most of these models lack specific optimizations for handling visualized document images. 
% (3) \textit{Visualized Document Retrievers}: These models build upon the architectures of general multimodal retrievers but are explicitly optimized for the task of visualized document retrieval. By training on datasets composed of screenshot images, they are better equipped to address the distinct characteristics and complexities of visualized documents.
% For each category, we evaluated multiple models to ensure a thorough comparison. Detailed descriptions of the baseline models and their implementation specifics are provided in Appendix~\ref{appendix-baselines}.

\subsection{Main Results}
The overall performance of baseline retrievers and our UniSE models are presented in Table~\ref{tab:main results on MVRB}. We report the aggregation result for each task category, leaving the detailed performance for each task presented in Appendix~\ref{appendix-detailed results}. We have identified the following key observations from this table: 


\noindent\textbf{(1) Our UniSE models achieve the leading performance across all task categories on MVRB.} Specifically, UniSE-MLLM surpasses the previous best screenshot document retriever, GME~\cite{zhang2024gme}, by 7.6\% in average score. Additionally, the UniSE-CLIP model, with only 428 million parameters, outperforms all baselines of no greater sizes while delivering comparable performance to MM-Embed~\cite{nv-mm-embed2024}, the strongest general multimodal retriever, which has 7.57 billion parameters. The superiority of UniSE models verifies the effectiveness of our VIRA dataset. 

\noindent\textbf{(2) The Vis-IR paradigm offers significant advantages over traditional text-based retrieval.}. As shown in~\Cref{tab:main results on MVRB}, even with a complex retrieval workflow powered by advanced OCR tools and powerful text retrievers~\cite{bge}, traditional methods still fall significantly behind UniSE. This suboptimal performance is likely due to the loss of crucial layout and visual semantics, even though OCR can accurately extract textual information. This highlights the great potential of Vis-IR as a unified approach for document retrieval.   


% Modern OCR can accurately extract textual information, but traditional text-based retrieval methods still suffer from losing layout and visual elements, leading to suboptimal performance. As shown in~\Cref{tab:main results on MVRB}, even the best text retrievers~\cite{bge}, while improved, still fall short compared to most Vis-IR methods. This highlights the greater potential of Vis-IR as a technical approach for document retrieval. 



% \noindent\textbf{(3) Existing general multimodal retrievers have substantial room for improvement in Vis-IR.} Although these retrievers can be applied to Vis-IR in a zero-shot manner, most of them underperform compared to \textit{OCR + BGE}. Even models that incorporate visualized documents during training (e.g., vlm2vec with DocVQA) still lag significantly behind contemporary models like ColPali~\cite{faysse2024colpali}. 

\noindent\textbf{(3) Zero-shot application of general multimodal retrievers results in suboptimal performance for Vis-IR}. While these methods can be directly applied to screenshot data, most of them perform worse than other baselines. For instance, the average score of MM-Embed and MLV2Vec falls significantly behind that of GME and DSE, despite using similar MLLM backbones and being fine-tuned on massive datasets. This highlights a key distinction between screenshots and common multimodal data, as the latter lacks a rich combination of data elements presented in visual forms. 

\noindent\textbf{(4) Existing screenshot document retrievers cannot comprehensively address Vis-IR tasks.} Although baselines like ColPali, DSE, and GME achieve substantial improvements in average score, they still underperform other methods in CSR and SQA tasks. Additionally, these methods exhibit inconsistent performance across different domains, as shown in Table \ref{tap:result-of-domains}. In contrast, UniSE-MLLM maintains the leading performance across both task categories and application domains, which demonstrates its well-rounded Vis-IR capabilities. 

% zero-shot application of general multimodal retrieval leads to sub-optimal performance in vis-ir 

% screenshot document retrievers cannot comprehensively support vis-ir tasks 

% \noindent\textbf{(3) Existing general multimodal retrievers can adapt to Vis-IR tasks without additional design.} Although general multimodal retrievers can be applied to Vis-IR tasks in a zero-shot manner, most of them underperform compared to \textit{OCR + BGE}. However, with sufficient and diverse screenshot data, they can easily adapt to Vis-IR tasks. For example, GME~\cite{zhang2024gme} was trained with approximately 1 million visual documents included in their training data, resulting in a significant performance boost compared to VLM2Vec~\cite{jiang2024vlm2vec}, which used the same backbone. Similarly, our UniSE-CLIP and UniSE-MLLM, which also used the same backbones as CLIP~\cite{clip-radford2021learning} and VLM2Vec respectively, showed notable improvements. This demonstrates the effectiveness of our large-scale and diverse VIRA dataset for Vis-IR tasks.



\begin{table}[t]
\centering
\begin{adjustbox}{width=0.475\textwidth}
\begin{tabular}{@{}l|cccccc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multicolumn{6}{c}{\textbf{Domains}} \\ \cmidrule(l){2-7} 
 & Prod. & News & Wiki & Paper & Repo & Others \\ \midrule
\rowcolor{gray!10} 
\multicolumn{1}{l|}{\texttt{\#Datasets}}  & \texttt{4}    & \texttt{3}   & \texttt{3}   & {\texttt{3}} &\texttt{2} &\texttt{5}\\\midrule
BM25 & 28.65 & 28.23 & 21.36 & 39.72 & 45.67 & 34.65 \\
DPR & 17.28 & 33.64 & 25.96 & 21.47 & 33.08 & 27.07 \\
BGE & 38.08 & 48.44 & 34.77 & 41.11 & 51.81 & 47.66 \\ \midrule
VISTA & 14.49 & 19.76 & 17.48 & 11.53 & 20.39 & 6.39 \\
Uni-IR & 19.34 & 24.26 & 23.62 & 8.87 & 18.54 & 21.56 \\
CLIP & 25.96 & 35.57 & 27.70 & 12.14 & 21.75 & 20.27 \\
SIGLIP & \underline{41.46} & 38.73 & 30.19 & 21.09 & 36.10 & 31.76 \\
E5-V & 18.09 & 28.28 & 20.81 & 21.32 & 21.11 & 35.37 \\
VLM2Vec & 28.41 & 38.56 & \underline{39.60} & 20.74 & 37.97 & 31.50 \\
MM-Embed & 31.72 & 40.33 & 36.84 & 23.52 & 42.29 & 35.22 \\ \midrule
ColPali & 36.24 & 41.83 & 26.38 & 47.96 & 55.28 & \underline{53.76} \\
DSE & 37.05 & 48.60 & 28.52 & 46.83 & 60.47 & 52.23 \\
GME & 34.73 & \underline{56.53} & 37.17 & \underline{50.08} & \underline{62.64} & 53.46 \\
UniSE-CLIP & 37.92 & 43.37 & 36.10 & 26.45 & 38.28 & 36.44 \\
Uni-MLLM & \textbf{49.33} & \textbf{64.30} & \textbf{44.50} & \textbf{54.12} & \textbf{70.17} & \textbf{57.61} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-5pt}
\caption{Average performance (SR, CSR, SQA, OVC) on different domains. 
Recall@1 is the evaluation metric.} 
\vspace{-10pt}
\label{tap:result-of-domains}
\end{table}

\subsection{Data Ablation} 
We perform extensive ablation studies to analyze the value from VIRA dataset.  First, we examine the impact from captions and question-answering data, which are used for pre-training and fine-tuning, respectively. Next, we look into the roles of different question-answering data, including the s2q tuples and sq2s triplets. Finally, we assess the performance gain from using hard negatives. 

\subsubsection{Two Annotations}
\label{subsubsection-training-data} 
We first explore the impact from the two types of annotations within our VIRA dataset, as shown in Table \ref{tab:data-ablation}. We compare three approaches: the first one relies solely on screenshot captions (first row), the second one leverages only question-answering data (second-row), the last one uses both captions and question-answering data (last row). To ensure a fair comparison, these methods are fine-tuned for the same number of training steps. 


We derive the following key observations from the experiment results. First, UniSE demonstrates strong Vis-IR capabilities solely with pre-training on screenshot-caption data, already surpassing ColPaLI~\cite{faysse2024colpali} by 2.5\% in overall score. While the alignment between screenshots and captions differs significantly from downstream tasks, it enables the model to capture fine-grained screenshot semantics, laying a solid foundation for further training. Second, question-answering data alone yields even stronger performance, surpassing the caption-only method by 7.8\%. Third, the combined use of both captions and question-answering data provides additional improvements, achieving performance gains of 9.6\% and 1.8\% over methods using only a single type of annotation data. 

\subsubsection{Composite QA Data} 
We further investigate the benefits of using composite question-answering data, specifically q2s tuples and sq2q triplets, as shown in Table \ref{tab:data-ablation}. To assess their individual contributions, we conduct two ablation studies: one using only q2s tuples (3rd row, w/o sq2q) and the other using only sq2q triplets (4th row, w/o q2s). The results indicate that both q2s tuples and sq2q triplets significantly enhance UniSE’s overall performance, improving the average score by 6.8\% and 8.6\%, respectively, compared to the screenshot-caption pre-trained model. Besides, sq2q triplets provide an extra 1.8\% gain over q2s tuples alone. Finally, the effects of both data types are complementary, as their joint usage leads to the highest performance (6th row, use all). 

\subsubsection{Domain Diversity} 
We examine the detailed impact of using diverse data from multiple domains. To this end, we introduce an ablation approach that includes question-answering data solely from Wikipedia while retaining diverse caption data (5th row, w/o. diversity). The experimental results indicate a clear advantage of using diverse training data, as the default method (6th row, use all) outperforms the ablation approach by 2.2\% in overall performance. This underscores the value of domain diversity in providing uniform support across various Vis-IR tasks. 

% use any of the hn improves 
% use both hns leads to the best 
% although hard negatives' quality can be further improved with more expensive methods
% our data presents quick and effective value in improving the retrieval quality

\subsubsection{Hard Negatives}
\label{subsubsection-impact-hn}
Finally, we explore the impact of incorporating hard negatives into the training data. In our experiment, we use three ablation methods, as shown in Table \ref{tab:impact-hard-negative}: 1) without hard negatives (1st row), 2) with only sq2s hard negatives (2nd row), and 3) with only s2q hard negatives (3rd row). Compared to the method without hard negatives, introducing either type of hard negatives leads to improved overall performance. Furthermore, combining both types of hard negatives results in the best performance. While the quality of hard negatives could be further improved with more costly processing, the current results offer valuable support for enhancing Vis-IR models developed from the VIRA dataset.


% \clearpage

% \subsubsection{Question Answering Data}
% \label{subsubsection-instruction-data-details}
% We examine the detailed impact introduced by question-answering data from two perspectives: 1. Benefits from jointly using q2s and sq2s data, 2.  

% We explore the influence of various components within instruction data of VIRA on the performance of UniSE-MLLM. Specifically, we examine the effects of \textit{q2s} tuples, \textit{sq2s} triplets, and domain diversity. To ensure a fair comparison, we pre-train the model with 1M caption data and fine-tune it with a fixed total dataset size of 1M across different instruction data combinations.

% Table~\ref{tab:data-ablation} presents the overall score of UniSE-MLLM fine-tuned within the different instruction data combinations.  We summarize our key findings as follows:  

% \paragraph{Benefits of Instruction Data.}   Both \textit{q2s} tuple and \textit{sq2s} triplet instruction data effectively enhances model performance compared to pre-training only on caption data, with improvements of 6.8\% and 8.6\% in the overall score, respectively. Moreover, \textit{sq2s} instruction data appears to be particularly advantageous, offering an additional 1.8\% improvement over the use of \textit{q2s} alone, suggesting that \textit{sq2s} triplets provide richer contextual information. Furthermore, combining both \textit{q2s} and \textit{sq2s} yields even greater performance enhancements, contributing further improvements of 2.2\% and 0.4\%.

% \paragraph{Importance of Domain Diversity.} {The diversity of data domains is important for enhancing the model's performance.}  The model fine-tuned on multi-domain instruction data achieves an 1.63\% improvement in overall score compared to the model fine-tuned solely on Wikipedia instruction data, which demonstrates the value of domain diversity present in our VIRA dataset. 

\begin{table}[t]
\small
\centering
\begin{adjustbox}{width=0.475\textwidth}
\begin{tabular}{@{}c|cc|c|c@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Caption}}} & \multicolumn{3}{c|}{\textbf{Instructions}}                                                              & \multirow{2}{*}{\textbf{Overall Score}} 
\\ \cmidrule(lr){2-4}
\multicolumn{1}{c|}{}                                  & \text{\textbf{q2s}}         & \text{\textbf{sq2s}}        & \multicolumn{1}{c|}{\textbf{Diversity}} &                                         
\\ 
\midrule
\checkmark                          & \multicolumn{1}{c}{\crossmark} & \multicolumn{1}{c|}{\crossmark} & --                  & {46.14}                    

\\
\crossmark                                  & \checkmark                     & \checkmark                     & \checkmark     &    53.95                                     \\ \midrule
\checkmark                                       & \checkmark                     & \crossmark                     & \checkmark     &      52.94                                   \\
\checkmark                                     & \crossmark                     & \checkmark                     & \checkmark     &      \underline{54.72}                                   \\
\checkmark                                          & \multicolumn{1}{c}{\checkmark} & \multicolumn{1}{c|}{\checkmark} & \crossmark    & {53.53}                    \\ \midrule
\checkmark                                        & \checkmark                     & \checkmark                     & \checkmark     &  \textbf{55.72}                                        \\ \bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-5pt}
\caption{UniSE-MLLM's performance from different data, including 1. caption-only, 2. question-answer only, i.e., q2s and sq2s, 3. w/o. sq2s, 4. w/o. q2s, 5. w/o. diversity, using Wiki as the only source, 6. using all. }
% \vspace{-7pt}
\label{tab:data-ablation}
\end{table}


% \subsubsection{Data Scalability}
% \label{subsubsection-data-scale}
% We evaluate the performance trend of UniSE-MLLM by training it on different sizes of subsets from the VIRA dataset to verify its scalability. As shown in, the performance of UniSE-MLLM consistently improves with the increasing size of training data. This upward trend highlights the effectiveness and scalability of VIRA.



% \begin{table}[htb]
% \centering
% \begin{adjustbox}{width=0.475\textwidth}
% \begin{tabular}{@{}cc|cccc|c@{}}
% \toprule
% \multicolumn{2}{c|}{\textbf{Stage}}                   & \multicolumn{4}{c|}{\textbf{Per Meta-Task Score}} & \multirow{2}{*}{\textbf{Overall}} \\ \cmidrule(r){1-6}
% \textit{\textbf{S1}}      & \textit{\textbf{S2}}      & SR         & CSR        & SQA        & OVC        &                                   \\ \midrule
% \crossmark & \crossmark &       0.04     &    2.18        &     0.16       &         1.75   &     0.93                              \\
% \checkmark & \crossmark &       52.96     &     36.73       &      46.65      &       50.11     &   47.56                                \\
% \crossmark & \checkmark &       63.18     &    50.42        &        45.22    &     52.26        &     53.95                             \\
% \checkmark & \checkmark &      65.21      &   55.54         &      41.94      &       53.71     &   55.16                                \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Performance comparison of UniSE-MLLM within different training stage. \textit{\textbf{S1}}: Pretraining with caption data;~ \textit{\textbf{S2}}: Fine-tuning with instruction data.}
% % \textit{\textbf{q2s}}: ; \textit{\textbf{sq2s}}: ; \textbf{Diversity}: ; \textbf{Caption Pretrain}: 
% \label{tab:different-training-stage}
% \end{table}


% \begin{table}[htb]
% \centering
% \begin{adjustbox}{width=0.475\textwidth}
% \begin{tabular}{cc|c|cccc|c}
% \toprule
% \textit{\textbf{q2s}} & \textit{\textbf{sq2s}} & \textbf{D} & SR & CSR & SQA & OVC & \multicolumn{1}{c}{\textbf{Overall}} \\ \midrule
% \crossmark             & \crossmark              & --            &    &     &     &     &                                       \\
% \checkmark              &     \crossmark         &         \checkmark   &    &     &     &     &                                       \\
% \crossmark             &        \checkmark       & \checkmark           &    &     &     &     &                                       \\
% \checkmark             & \checkmark              & \crossmark           &    &     &     &     &                                       \\
% \checkmark             & \checkmark              & \checkmark           &    &     &     &     &                                       \\\bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Performance comparison of UniSE-MLLM using different data combinations at 1M scale. We investigate the impacts of \textbf{caption} pretraining, instruction data (\textbf{\textit{q2s}} tuple and \textbf{\textit{sq2s}} triplet), and \textbf{diversity} of data domains (multi-domain vs. Wikipedia only).}
% % \textit{\textbf{q2s}}: ; \textit{\textbf{sq2s}}: ; \textbf{Diversity}: ; \textbf{Caption Pretrain}: 
% \label{tab:different-data-composition}
% \end{table}


% \begin{table}[htb]
% \centering
% \begin{adjustbox}{width=0.475\textwidth}
% \begin{tabular}{@{}cc|c|cccc|c@{}}
% \toprule
% \multicolumn{1}{l}{\multirow{2}{*}{\textit{\textbf{q2s}}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textit{\textbf{sq2s}}}} & \multirow{2}{*}{\textit{\textbf{D}}} & \multicolumn{4}{c|}{\textbf{Per Meta-Task Score}} & \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Overall}}} \\ \cmidrule(lr){4-7}
% \multicolumn{1}{l}{}                                       & \multicolumn{1}{l|}{}                                        &                                      & SR         & CSR        & SQA        & OVC        & \multicolumn{1}{l}{}                         \\ \cmidrule(r){1-8}
% \crossmark             & \crossmark              & --       &     49.58     &     36.73       &      46.65      &       50.11     &   46.02                                                \\
% \checkmark              &     \crossmark         &         \checkmark   &  65.90   &  38.70   &  50.92   &   47.03  &     52.94                    \\
% \crossmark             &        \checkmark       & \checkmark           & 58.91   &  57.79   &   43.40  &    58.47 &            54.72                \\
% \checkmark             & \checkmark              & \crossmark           &  58.86  &  51.17   &  48.55   &   50.28  &            53.53                           \\
% \checkmark & \checkmark &  \checkmark &    65.21      &   55.54         &      41.94      &       53.71     &   55.16    \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table}

% \begin{table}[htb]
% \centering
% \begin{adjustbox}{width=0.475\textwidth}
% \begin{tabular}{c|cc|c|c@{}}
% \toprule
% \multirow{2}{*}{\textbf{Caption}} &\multicolumn{2}{c|}{\textbf{Instructions}}     & \multirow{2}{*}{\textbf{Diversity}}  & \multirow{2}{*}{\textbf{Overall Score}} \\ \cmidrule(r){2-3}
%  &\textit{\textbf{q2s}}     & \textit{\textbf{sq2s}}    &                                     &                                          \\ \midrule
% \crossmark & \checkmark & \checkmark & \checkmark      &  53.95  \\ \midrule
% \checkmark & \crossmark & \checkmark  & \checkmark       &         \underline{54.72}                \\ 
% \checkmark & \checkmark & \crossmark & \checkmark   &        52.94                           \\ \midrule
% \checkmark & \checkmark & \checkmark  & \crossmark      &              50.97                        \\ \midrule
% \checkmark & \checkmark & \checkmark & \checkmark     &            \textbf{55.16}     \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Performance comparison of UniSE-MLLM using different data combinations at 1M scale. We investigate the impacts of \textbf{caption} pretraining, instruction data (\textbf{\textit{q2s}} tuple and \textbf{\textit{sq2s}} triplet), and \textbf{diversity} of data domains (multi-domain vs. Wikipedia only).}
% % \textit{\textbf{q2s}}: ; \textit{\textbf{sq2s}}: ; \textbf{Diversity}: ; \textbf{Caption Pretrain}: 
% \label{tab:different-data-composition-1}
% \end{table}


\begin{table}[t]
\small
\centering
\begin{adjustbox}{width=0.475\textwidth}
\begin{tabular}{cc|cc@{}}
\toprule
\multicolumn{2}{c|}{\textbf{Hard Negatives}}                   & \multicolumn{2}{c}{\textbf{Overall Score}}  \\ \cmidrule{1-4}
\centering \text{\textbf{q2s}}     &   \centering  \text{\textbf{sq2s}}  &\text{\textbf{UniSE-MLLM}} &\textbf{UniSE-CLIP}   \\ \midrule
\crossmark & \crossmark        &        54.26 & 34.99 \\ 
\crossmark & \checkmark               &  \underline{54.68} &   \underline{36.19}                             \\
\checkmark & \crossmark     &      54.50                   &   36.08 \\
\checkmark & \checkmark       &   \textbf{55.72}        &  \textbf{36.41} \\ \bottomrule
\end{tabular}
\end{adjustbox}
% \vspace{-5pt}
\caption{Impact from different hard negative configurations. \text{\textbf{q2s}}: incorporate hard negatives for \text{q2s} tuples; \text{\textbf{sq2s}}: incorporate hard negatives for \text{sq2s} triplets.} 
\vspace{-5pt} 
\label{tab:impact-hard-negative} 
\end{table} 

% \vspace{-5pt}
\section{Related Work}
% \vspace{-3pt} 
\paragraph{Neural Document Retrieval.}
Document retrieval is crucial for a wide range of problems, like search engines, open-domain question answering, and retrieval-augmented generation~\cite{dpr,lewis2020retrieval,yuan2023vile}. Traditionally, document retrieval primarily rely on text-based methods, with significant advancements propelled by pre-trained language and large language models. These models have facilitated the development of effective document retrieval systems~\cite{DBLP:conf/emnlp/Ni0LDAMZLHCY22,wang2022text,bge,llama2vec}. However, text-based retrieval methods require prior document parsing and are incapable of handling unstructured information within documents, such as complex layouts, charts, and visual elements~\cite{ma2024unifying}. This limitation results in information loss and highlights the need for new retrieval methods that go beyond text. 

% \paragraph{Document Retrieval.}
% Document retrieval is crucial for a wide range of applications, like search engines, open-domain question answering, and retrieval-augmented generation~\cite{dpr,lewis2020retrieval,yu2024visrag}. Traditional approaches primarily rely on text-based methods, with significant progress driven by pre-trained language and large language models~\cite{DBLP:conf/emnlp/Ni0LDAMZLHCY22,wang2022text,bge,llama2vec}. While effective, these methods are inherently limited in their ability to process visual elements such as charts and images within documents. The emergence of vision-language models (VLMs)~\cite{clip-radford2021learning, align2021, llava-liu2023visual} has expanded the scope of retrieval systems by enabling multimodal data processing~\cite{mbeir-wei2023uniir,vista,jiang2024vlm2vec,nv-mm-embed2024}. However, these methods assume that multimodal documents are well-preprocessed into interleaved image-text sequences, which is unsuitable for the diverse and unstructured nature of many real-world documents. Recent studies~\cite{ma2024unifying, faysse2024colpali, zhang2024gme} have explored the use of document screenshots as unified input representations. This approach eliminates the need for additional content extraction, preserves all information within the document, and shows potential for effectively handling the complexity of real-world documents.

\paragraph{Multimodal Retrieval.} The development of vision-language models (VLMs)~\cite{clip-radford2021learning, align2021, llava-liu2023visual} has significantly advanced the capabilities of general-purpose multimodal retrievers~\cite{mbeir-wei2023uniir,vista,jiang2024vlm2vec,nv-mm-embed2024}, enabling them to perform massive multimodal retrieval tasks, such as text-to-image retrieval~\cite{mscoco-chen2015microsoft}, composed image retrieval~\cite{fashioniq-wu2021fashion,circo}, and visual grounding~\cite{zhu2016visual7w}, etc. However, these methods assume that multimodal documents are well-preprocessed into interleaved image-text sequences, which is unsuitable for the diverse and unstructured nature of many real-world documents. 

Recent studies~\cite{ma2024unifying, faysse2024colpali, zhang2024gme} have explored the use of document screenshots as unified input representations, i.e., visualized information retrieval (Vis-IR). This approach eliminates the need for additional content extraction, preserves all information within the document, and shows potential for effectively handling the complexity of real-world documents. 
Despite these advancements, existing Vis-IR datasets remain limited to specific domains (e.g., Wiki-SS~\cite{ma2024unifying}) and primarily evaluate performance by adapting existing document visual question answering benchmarks to text-to-screenshot retrieval tasks~\cite{slidevqa, mathew2021docvqa, mathew2022infographicvqa, zhu2022towards, li2024multimodal}. This narrow scope restricts the generalizability and applicability of current methods. 
Therefore, there is a clear need for diverse datasets and comprehensive benchmarks to advance this field and enable more robust and scalable solutions.

% \paragraph{Neural Document Retrieval.}
% Neural document retrieval is crucial for a wide range of applications, like search engines, open-domain question answering, and retrieval-augmented generation~\cite{dpr,lewis2020retrieval,yu2024visrag}. Traditionally, document retrieval has predominantly been text-driven, with significant advancements propelled by pre-trained language and large language models. These models have facilitated the development of effective document retrieval systems~\cite{DBLP:conf/emnlp/Ni0LDAMZLHCY22,wang2022text,bge,llama2vec}. However, text-based retrieval methods require prior document parsing and are incapable of handling unstructured information within documents, such as complex layouts, charts, and visual elements~\cite{ma2024unifying}. This limitation results in information loss and highlights the need for new retrieval methods that go beyond text. 



% % Multimodal retrieval: 1. multimodel retrieval is made possible by the advancement of multimodal representation learning. VLM: CLIP, ALIGN, SigLIP, and multimodal LLMs: Llava, Gemini, 2. multimodal retrieval by making direct use of CLIP or using fine-tuned MLLMs, 3. growing interest in developing general-purpose multimodal retrievers, progresses made by VISTA, MagicLens, E5-V, MM-Ret, UniIR, VLM2Vec, driven by either leveraging powerful model backbones, or creating extensive training data.  

% \paragraph{Multimodal Retrieval.}
% The development of vision-language models (VLMs)~\cite{clip-radford2021learning, align2021, llava-liu2023visual} has significantly advanced the capabilities of general-purpose multimodal retrievers, enabling them to handle image, text, and composed image-text data~\cite{mbeir-wei2023uniir,vista,jiang2024vlm2vec,nv-mm-embed2024}. However, when applied to multimodal documents, these methods often process text and images in an interleaved manner, requiring additional preprocessing steps for content extraction and potentially leading to information loss. Recent approaches propose using document screenshots as input for unified document representation~\cite{ma2024unifying, faysse2024colpali, zhang2024gme}. By fine-tuning pre-trained VLMs with specific screenshot data, these methods demonstrate greater effectiveness than traditional text-based methods, as they preserve all information within the document. Despite these advancements, existing research still lacks a diverse range of document data and comprehensive evaluation tasks necessary for building and evaluating unified general-purpose visualized information retrieval systems.





% \paragraph{Visualized Document Retrieval Dataset.}
% There are several visualized document retrieval datasets have been developed by filtering and reorganizing original document-based VQA datasets~\cite{tanaka2023slidevqa, mathew2021docvqa, mathew2022infographicvqa, zhu2022towards, li2024multimodal}. Additionally, Wiki-SS is a dataset with 49,095 training examples, constructed using questions from the NQ~\cite{kwiatkowski2019natural} dataset and manually scraped Wikipedia pages~\cite{ma2024unifying}. Other efforts have focused on building such datasets by scraping PDFs and utilizing VLMs for annotation~\cite{faysse2024colpali, yu2024visrag}. Despite these efforts, large-scale, highly diverse visual document retrieval datasets remain scarce, highlighting a gap in this emerging research field.



 
\vspace{5pt} 
\section{Conclusion} 
In this work, we introduce Vis-IR, a novel paradigm that enables multimodal search in a highly unified manner. Our work makes three fundamental contributions to this emerging field: (1) the creation of VIRA, a large-scale, diverse, and well-annotated dataset for developing Vis-IR models; (2) the development of UniSE, a family of embedding models designed for general Vis-IR applications; and (3) the construction of MVRB, a comprehensive benchmark for evaluating Vis-IR performance. Our experiments uncover the limitations of existing methods in performing Vis-IR tasks and demonstrate the significant improvements brought by VIRA and UniSE. To drive further advancements, we will publicly release all resources and continue expanding in key directions, such as increasing data diversity and incorporating multilingual annotations. 

% formulate the vis-ir problem
% uniformly represent different data forms with screenshots
% thus providing a unified paradgim for search system 
% we made three contributions, including 
% VIRA, a large-scale, diverse, and well-annotated dataset for developing vis-ir models
% UniSE, a family of well-trained embedding models for general vis-ir tasks 
% mvrb, a comprehensive benchmark to evaluate retrieval models's vis-ir abilities 
% we perform comprehensive experiement based on our benchmark 
% whose result highlights the limitations from existing methods, and the improvements made by vira and unise 
% in the future, keep on expansion, such as increasing the diversity of source data, and inclusion of other languages 


\clearpage 

% \section*{Limitations} 
% While this work makes substantial progress over existing methods, several limitations remain to be addressed in the future. First, incorporating multilingual annotations would enhance the development and application of Vis-IR across different cultures. Second, the diversity of data sources can be further improved. We plan to expand the dataset with more critical sources and explore the use of synthetic data. Finally, we aim to train the model on both screenshot and general multimodal data, enabling it to serve broader applications. 

% While this work makes substantial progresses over existing methods, there remain several limitations which need to be addressed in the future. First, the annotation is desired to be multilingual, which will enable the development and application of Vis-IR in different cultures. Second, the diversity of data source can be further improved. We'll continue to expand the dataset from more crucial sources, and consider to leverage synthetic data. Finally, we'll train the model based on both screenshot and general multimodal data, enabling it to fit for broader applications. 

\bibliography{custom}



\appendix
\section*{Appendix}
\section{Details of VIRA Dataset Construction}
\label{appendix:details-VIRA}

\subsection{Data Collection}
\label{appedix:VIRA-data-collection}
% 数据来源与采集方法
% 1.数据集的来源（公开数据集、自行采集、爬取数据等）
% 2.数据采集的具体过程、使用的工具或平台、采集时间范围
% 3.过滤的策略
% \paragraph{Data Source and Collection Process.} 
% And the other is from publicly available open-source datasets.

% 补充每个子类别的数据规模
We collect massive screenshots spanning seven categories. 
Based on the acquisition method, these data primarily fall into two groups: those crawled by us and those curated from publicly available open-source datasets. 
For the crawled data, we use the automated platform \texttt{Playwright}\footnote{https://playwright.dev/python/} to scrape target websites and capture screenshots of the main content pages. To ensure the completeness of the captured images, we perform a verification process using the built-in function of \texttt{Playwright}  and discard any incompletely rendered images. Our entire crawling process adheres to the \texttt{robots.txt} regulations of the respective websites. For the other curated, we have carefully curated the files to ensure they can be converted into complete screenshots, containing both visual and textual content.

\paragraph{Self-Crawled Datasets.} The screenshots obtained through crawling fall into four categories: News, Products, Research Papers, and Project Homepages. The details of the source and processing steps for each category are as follows: 
\begin{itemize}
    \item  \textbf{News}: We scrap news webpages from five mainstream news websites, including BBC, CNN, Fox, CGTN, and Global Times. For each news page, we use \texttt{Playwright} to capture full-page scrolling screenshots and extract text content from the corresponding HTML.
    % The total number of crawled news screenshots is approximately 1.85M.
    
    \item \textbf{Products}:  We primarily focus on product detail pages from the Amazon e-commerce website. Utilizing \texttt{Playwright}, we capture screenshots of these pages, which typically feature product images, titles, prices, categories, and other relevant details of the product. Additionally, we extract textual information from the HTML of each product webpage. 
    % In total, we've captured approximately 1.24M product screenshots.
    
    \item \textbf{Research Papers}: We collect research papers from arXiv using the arXiv API\footnote{https://info.arxiv.org/help/api/basics.html} , spanning from January 2018 to November 2024. For each paper, we retain only the most recent version. The original format of these papers is PDF. We convert each page of every paper into an image and extract the text from each page using the \texttt{PyMuPDF}\footnote{https://pypi.org/project/PyMuPDF/} library.
    % This process results in approximately 2.39M page screenshots.
    
    \item \textbf{Project Homepage}: We collect project homepages from GitHub repositories. Initially, we filter out repositories that contain a README.md file. Using \texttt{Playwright}, we capture screenshots of the README region from the homepages of selected repositories and extract the relevant text from corresponding HTML.  Since some README files contain minimal information (e.g., only the repository name), we discard screenshots with a height of less than 300 pixels. 
    % The total number of project homepage screenshots is approximately 2.38M.
\end{itemize}

\paragraph{Curated Datasets.} We curate open-source datasets to obtain screenshots for three additional categories: General Documents, Charts, and Common Knowledge. The details of the source and collection process for each category are as follows: 
\begin{itemize}
    \item \textbf{General Documents}: We utilize the publicly available PDFA\footnote{https://hf-mirror.com/datasets/pixparse/pdfa-eng-wds} dataset, a document dataset filtered from SafeDocs. To construct our dataset, we first filter out documents in PDF format and convert each page into an image using \texttt{PyMuPDF}. The source dataset provides OCR data for each document, which we process by stitching together the extracted text from top-left to bottom-right, forming a coherent caption corresponding to each image. 
    % The total number of general document screenshot is 1.82M.
    
    \item \textbf{Charts}: We utilize the publicly accessible dataset ArxivCap~\cite{li2024multimodal}, which comprises image-caption pairs derived from 572,000 Arxiv papers.
    % spanning multiple scientific disciplines. 
    Each entry contains an image, like a table or figure, and its caption, initially provided as separate text-image pairs rather than a cohesive screenshot. Therefore, we render each image alongside its caption into a single, unified screenshot. 
    % This process yields a total of 2.01M chart screenshots.
    
    \item \textbf{Common Knowledge}: We used the publicly available dataset Wiki-SS-Corpus~\cite{ma2024unifying}. This dataset is derived from screenshots of Wikipedia entry webpages and includes caption data for the first 500 words of each page. 
    % The total number of Wikipedia screenshots in the dataset is 1.27M.
    
\end{itemize}

After collecting the data, we apply a filtering process to ensure content quality and appropriateness. Specifically, we perform keyword matching on the caption text to filter out NSFW content. Additionally, we remove low-quality entries based on aspect ratio and caption length, discarding screenshots with an aspect ratio exceeding 9 and captions with fewer than 100 characters.

\subsection{Data Annotation}
\label{appendix:data-annotation}
% 我们将训练数据分成两大类，分别是Caption Datasets和instruction tuning datasets，caption data中主要是图像与图像中的文本组成。对于instruction tuning data，我们合成两种数据，screenshot retrieval数据和composed screenshot retrieval数据。
% 关于合成数据的pipeline如下：
% 1. Screenshot retrieval data, 我们首先使用LLM (Qwen2.5-72B-Instruction)，输入caption数据，产生一个对应于caption的问题，其中question做为retrieval的query text，源数据中的图像做为target image。对于不同的domain，我们设置了不同的prompt，产生这部分数据的用时大约为2078 GPU hours (A800)。随后我们分Domain，利用文本嵌入模型 (BGE)对所有的caption进行编码做为corpus，将question和caption分别进行编码得到question embedding和caption embedding。我们利用question embedding检索corpus得到top-15的candidates，并移除了top-1。利用caption embedding检索了corpus得到top-10，并移除了top-3。最后合并这两部分的candidates，并从其中随机采样8个，做为每条训练数据的难负样例。
% 2. composed screenshot retrieval data，我们首先分domain，利用文本嵌入模型(BGE)编码caption得到corpus。随后caption data中的每个text条目编码做为query来检索corpus，得到top-20个相似的caption，我们从其中随机采样10个相似的caption，与原始的query caption构成了10个similar text pair。随机选取1个pair，这个pair中，一条caption对应的image做为query，另一条caption对应的image做为target。随后将这个similar text pair输入到LLM(Qwen-2.5-72B-Instruction)中，让LLM总结pair的异同，生成符合现实场景的question。最终得到qs2s数据。产生这部分数据的用时大约为1002 GPU hours (A800)。对于每条训练数据，在检索过程中，剩余的9个采样得到的数据做为难负样例。
The annotation of VIRA data is categorized into two main types: caption and question-answering.  Caption is the text within the collected screenshots, which is annotated and generated during the screenshot collection process, as detailed in Appendix~\ref{appedix:VIRA-data-collection}. As for the question-answering, we have developed two types of question-answering annotations: q2s tuples and sq2s triplets. The detailed process of creating these question-answering annotations is as follows:

\paragraph{q2s tuples.} A q2s tuple consists of a question \textit{q} and a corresponding screenshot \textit{s} that can be used to answer \textit{q}. Given that we have access to caption for each screenshot, we leverage a large language model (LLM) to generate question-answer pairs closely related to the screenshot. Our approach is similar to the methodology used in constructing Docmatix~\cite{laurenccon2024building}. To enhance annotation quality, we design domain-specific prompts tailored to different categories of screenshots, with the corresponding prompts illustrated in Figure~\ref{fig:prompt-q2s}. For the whole annotation process, we employ the open-source model Qwen2.5-72B-Instruction~\cite{qwen2.5}, which requires approximately 2,078 A800 GPU hours in total.


To enhance the effectiveness of q2s tuples, we augment each tuple with hard negatives. This augmentation process leverages both the text embedding model BGE~\cite{bge} and the visual embedding model EVA-CLIP~\cite{evaclip-sun2023eva}. Specifically, we first encode all captions to construct a corpus embedding and separately encode both the question and the caption of the target screenshot to obtain their respective embeddings. Based on these embeddings, we retrieve the top 15 and top 10 candidates from the corpus for the question and the target screenshot, respectively. To mitigate false negatives, we exclude the top-1 candidate for the question and the top-3 candidates for the target screenshot.

For screenshots in the domains of of News, Products, and Common Knowledge—where natural images frequently appear—we further employ EVA-CLIP to encode the target screenshot and retrieve the top-10 candidates from the corpus, excluding the top-2 to reduce potential false negatives. Finally, we merge all remaining candidate sets and randomly sample 8 candidates to serve as hard negatives for the q2s tuple.


\paragraph{sq2s triplets.} An sq2s triplet consists of two relevant screenshots—a query screenshot and a target screenshot—along with a conditional query.  The target screenshot is used to respond the query conditioned on the query screenshot. To construct an sq2s triplet, we first mine a pair of relevant screenshots from the corpus. Following a similar approach used for augmenting q2s tuples with hard negatives, we employ BGE and EVA-CLIP to retrieve the top-10 candidates and randomly select one as the relevant screenshot for each given screenshot.
For each pair of relevant screenshots, we prompt LLM to analyze their relationship based on their captions and generate a relational query. To enhance annotation quality, we design domain-specific prompts tailored to different categories of screenshot pairs, with the corresponding prompts are illustrated in Figure~\ref{fig:prompt-sq2s} . We utilize the open-source model Qwen2.5-72B-Instruction, and the entire annotation process requires approximately 1002 A800 GPU hours.

Additionally, we augment each sq2s triplet with hard negatives. The conditional query, query screenshot, and target screenshot are used to retrieve candidates from the corpus, following the same methodology applied in augmenting q2s tuples with hard negatives. From the retrieved candidates, 8 candidates are randomly sampled to serve as hard negatives for each sq2s triplet.
% We divide the training data into two main categories: Caption Datasets and Instruction Tuning Datasets. The Caption Data consists of images and the text within those images. For the Instruction Tuning Data, we create two types of data: Screenshot Retrieval Data and Composed Screenshot Retrieval Data.
% The pipeline for generating synthetic data is as follows:

% \textbf{Screenshot Retrieval Data}: First, we use a Large Language Model (LLM) (Qwen2.5-72B-Instruction) to generate a corresponding question from the caption data. The question serves as the retrieval query text, and the image from the source data acts as the target image. Different prompts are set for different domains to generate this data. The process takes approximately 2078 GPU hours (A800). Afterward, we split the data by domain and use a text embedding model (BGE) to encode all captions as the corpus. We then encode both the question and caption to obtain their respective question and caption embeddings. We use the question embedding to retrieve the top 15 candidates from the corpus, excluding the top 1, and use the caption embedding to retrieve the top 10 candidates, excluding the top 3. The two sets of candidates are then merged, and 8 are randomly sampled to serve as difficult negative samples for each training example.
    
% \textbf{Composed Screenshot Retrieval Data}: We first split by domain and use the text embedding model (BGE) to encode captions into a corpus. Then, for each text entry in the caption data, we encode it as a query to retrieve the top 20 most similar captions from the corpus. We randomly sample 10 similar captions and pair them with the original query caption to form 10 similar text pairs. We then randomly select one pair, where one caption's image serves as the query, and the other caption's image serves as the target. This similar text pair is input into the LLM (Qwen-2.5-72B-Instruction), which summarizes the similarities and differences between the pair and generates a question that aligns with real-world scenarios. This results in qs2s data. The generation of this data takes approximately 1002 GPU hours (A800). During the retrieval process for each training example, the remaining 9 sampled data points are used as difficult negative samples.


\subsection{Statistics of VIRA}
The VIRA is an extensive dataset, comprising a total of 20 million data entries. Of these, approximately 12.96 million are caption data, and around 7.11 million are question-answering data, which includes 5.97 million q2s tuples and 1.14 million sq2s triplets. A detailed breakdown of the data for each type in different domains is provided in Table~\ref{tab:statistic-VIRA}.

\begin{table}[htb]
\centering
\begin{tabular}{ccc}
\toprule
Type & Domain & Number \\
\midrule
\multirow{7}{*}{Caption} & News &  1.85M \\ \cmidrule{2-3}
& Products & 1.24M \\ \cmidrule{2-3}
& Research Papers & 2.39M\\ \cmidrule{2-3}
& Project Homepage & 2.38M \\ \cmidrule{2-3}
& General Documents & 1.82M \\ \cmidrule{2-3}
& Charts & 2.01M\\ \cmidrule{2-3}
& Knowledge & 1.27M\\ 
\hline \hline
\multirow{7}{*}{s2q} & News & 1.24M \\ \cmidrule{2-3}
& Products & 787.5K \\ \cmidrule{2-3}
& Research Papers & 997.4K\\ \cmidrule{2-3}
& Project Homepage & 1.0M \\ \cmidrule{2-3}
& General Documents & 1.09M\\ \cmidrule{2-3}
% & Charts & \\ \cmidrule{2-3}
& Knowledge & 850.2K\\ 
\cmidrule{1-3}
\multirow{8}{*}{qs2s} 
& News & 248.0K \\ \cmidrule{2-3}
& Products & 496.1K\\ \cmidrule{2-3}
& Research Papers & 89.8K\\ \cmidrule{2-3}
& Project Homepage & 69.7K\\ \cmidrule{2-3}
& General Documents & 87.5K\\ \cmidrule{2-3}
% & Charts & \\ \cmidrule{2-3}
& Knowledge & 155.4K\\
\bottomrule
\end{tabular}
\caption{Detailed data counts for each domain in VIRA}
\label{tab:statistic-VIRA}
\end{table}
% 1.提供数据集的统计信息、特征分布等，并进行初步分析
% 2.数据每部分的比例或数量

\section{More Details of UniSE Models}
\subsection{Preprocessing Strategies for Screenshot Images}  
For the UniSE-CLIP model, all input screenshot images are resized to \(224 \times 224\) during both training and evaluating, following the default settings of the original CLIP model. In contrast, the UniSE-MLLM model employs a \textbf{smart resize} strategy for both training and evaluation, which preserves the original aspect ratio of the screenshot images while maximizing their resolution to retain visual quality. Specifically, the maximum image size is set as \(M \times 28 \times 28\), where \(M\) denotes the maximum number of image tokens. For an image with height \(H\) and width \(W\) exceeding this limit, its dimensions are adjusted to \(H' = \lfloor \frac{H}{\beta} \rfloor \times 28\) and \(W' = \lfloor \frac{W}{\beta} \rfloor \times 28\), where \(\beta = \sqrt{ \frac{W \times H}{M \times 28 \times 28}}\). In our UniSE-MLLM, \(M\) is set to 2500.
 


\subsection{Training Details}
\label{appendix:training-details}
% 各训练阶段使用的数据
% 超参数设置，steps，computation costs


For both UniSE-CLIP and UniSE-MLLM models, training is conducted in two stages: pre-training and instruction fine-tuning. In the pre-training stage, both models are trained on VIRA's screenshot-caption subset, which contains approximately 13 million screenshot-caption pairs. In the instruction fine-tuning stage, the pre-trained models are further refined using VIRA's question-answering subset, comprising approximately 6 million items. For all models and training stages, the initial learning rate is set to \(5 \times 10^{-6}\), with a linear decay schedule applied during training. Other training details are shown below.

\paragraph{UniSE-CLIP Model.}  
In the pre-training stage, the UniSE-CLIP model is trained with a batch size of 8192 for a single epoch. During the instruction fine-tuning stage, the model uses a batch size of 4096 for one epoch, where each query is paired with one positive screenshot and one hard negative sample. All model parameters are updated during both stages of training.

\paragraph{UniSE-MLLM Model.}  
In the pre-training stage, the UniSE-MLLM model is trained with a batch size of 2048 for one epoch. During the instruction fine-tuning stage, the model uses a batch size of 1024 for one epoch, with each query paired with one hard negative sample. Unlike UniSE-CLIP, UniSE-MLLM employs LoRA~\cite{lora} to fine-tune the language model component of Qwen, while all other layers remain frozen throughout the training process. The LoRA rank is set to 32.

\section{Details of MVRB Benchmark Creation}
\label{appendix:detail-of-MVRB}

% 我们通过机器标注，人工标注以及过滤并重组其他公开数据集的方式，完成我们的benchmark构建。我们的benchmark的任务示例位于表格~\ref{tab:figure:MVRB-example-SR-CSR} 和 表格~\ref{figure:MVRB-example-SQA-OVC}中。我们的benchmark的具体条目位于Table~\ref{tab:benchmark-data-statistics}中，以下给出我们详细的构建细节：
%Screenshot Retrieval:  这类任务包括机器标注和通过过滤并重组其他公开数据集的方式完成构建。 机器标注的任务包括product search, paper search, repo search和 News Search。对于这些任务，我们利用搜集到的数据的caption和 screenshot，分别通过 LLM 和 MLLM 生成 question-answer pairs，并将question作为query，对应的prompt分别提供在 Figure 1 和 Figure 2中。为了保证问题的质量，我们采用了机器评估和人工验证的方式来进行质量控制。首先，我们利用三个MLLM(LLaVA1.6-34B, Molmo-72B, Llama3.2-VL-90B)从三个维度对任务中的每条数据进行评估：清晰性，whether the query conveys a concrete information need, 比如过滤掉一辆跑车这样的query,以确保生成的query适合于检索任务 ；合理性，whether the query is appropriate in practice，比如过滤掉一匹会游泳的马，保证生成的数据符合真实场景；正确性，whether the query can be addressed by the retrieval screenshot, 比如过滤该论文的引用数量是多少这类 无法仅仅通过 screenshot进行回答的问题，对应的prompt提供在 Figure 3中。Each evaluation sample is independently reviewed by the MLLMs, and if it fails to meet any criterion, it is removed. The remaining samples are then verified by human labelers using the same principles. An evaluation sample is successfully created only if it passes both quality-control stages.
%通过 过滤并重组其他公开数据集的方式完成构建的任务包括 chart-search, document-search, slide-search。 我们从chart-vqa, doc-vqa, slide-vqa中选取了它们的test集(加引用)，并将其中的question 作为 query，对应的image作为 target screenshot. 对于这些数据，我们去除了同一个query对应多个candidate的情况，比如SlideVQA中一个question对应的images可能有多张，并且我们去除了context-dependent的question，比如"章节3.4.5的内容是什么？",以 此来保证benchmark中的每个query适用于检索target screenshot。
%为了有效提升任务的难度，使其具有区分性，我们通过 query 和 target screenhot 来获取难负例，其方法类似于 Appendix~\{appedix:data-annotation} 中为 q2s tuples 和 sq2s triplets补充难负例的方式。同时，为了不引入伪负例，我们也通过MLLM进行筛选，保证不会有能够响应这个query的其他screenshot被加入集合。 对应的prompt位于图4中。

% Composite Screenshot Retrieval：
% 我们采用人工标注的方式完成这类任务的构建, 包括product discovery, news-to-wiki, knowledge relation 和 Wiki-to-product这四个任务。 这类任务难度大，具有相当实际的现实意义。我们让标注人员从相应的平台上获取query screenshot，并进行头脑风暴，生成对应的符合现实场景的高质量query，并获取对应的target screenshot。为了提高任务的难度，使其具有更大的区分性，我们还同时令每名标注人员为每个screenshot同时找8~10张相似的screenshots作为难负例。下面给出每个CSR任务具体的构建流程:
% Product discovery任务，我们模拟了用户实际购买商品时的情景，即对于一个给定的商品，用户会由于不同的需求，比如价格、颜色、品牌、配件等，想要获取相关的另一件商品。我们令标注人员在Amazon平台上，根据预定义好的商品类别集合，去搜索相关商品并获取截图，再根据截图中的信息，思考并生成查询，并截取满足查询的另一件商品的Screeshot。
% News-to-Wiki任务，我们模拟了用户浏览实际新闻时的场景，当用户在浏览新闻时，往往想知道具体的人物、时间、地点、起因等细节，这些可以通过wikipedia中的条目获得解答。我们令标注人员浏览主流新闻BBC、Fox、CNN等，而后根据其中的内容，思考并生成问题，并在Wikipedia上进行搜索和截取能够回答问题的词条的screenshot。
% Knowledge relation任务，我们模拟了用户浏览Wikipedia时的场景，即对于一个词条，用户会想知道与它相关的另一个事物的细节，这可以通过另一个wiki 词条获得解答。我们令标注人员在wikipedia平台上，根据预定义好的词条类别集合，去搜索相关词条并获取截图，再根据截图中的信息，思考并生成查询，并截取满足查询的另一个词条的Screeshot。
%wiki-to-product， 用户在浏览wikipedia中的商品词条时，会想要购买对应的商品。我们令标注人员根据实现给出的商品词条，同时在Amazon和wikipedia上平台上截取对应的Screenshot。


% Screenshot Question Answering:
% 这类任务通过机器标注完成构建，包括product-QA，news-QA，Wiki-QA，paper-QA和repo-QA。对于这些任务，我们利用搜集到screenshot，通过MLLM 生成 question-answer pairs，对应的prompt提供在 Figure 5 中。 对于这些任务，我们也分别通过构建Screenshot Retrieval任务时 机器评估和人工验证的方式，评价清晰性，合理性，以及正确性三个维度，以进行质量控制。为了有效提升任务的难度，使其具有区分性，我们通过LLM对于answer进行改写，生成了多个与正确答案类似、容易混淆的错误答案来提高任务难度。对应的prompt 位于 Figure 7中。

% Open-Vocab Classification:
% 对于这类任务，我们首先构建一个合理的、彼此正交的类别corpus。而后通过人工标注的方式完成对于screenshot 打上类别标签。以下是每个任务的分类taxonomy的建立细节：
% Product Classification:从Amazon网站的商品页面获取对应商品的类别，所得到的商品类别是如下的层级类别 “Arts, Crafts & Sewing - Knitting & Crochet - Crochet Hooks”，构建单词树对类别进行清洗，保证类别之间彼此正交
% News Topics:从CNN、BBC、Fox获取新闻分类类别，人工整合，形成一个对于彼此正交、尽可能覆盖实际场景的新闻类别集合
% Academic Fields:从Arxiv主页（https://arxiv.org/）获取预定义好的论文类别集合
% Knowledge Classification:从 wikipedia 类别页面 （https://en.wikipedia.org/wiki/Wikipedia:Contents/Categories）获取其定义好的类别，并进行人工整合，形成一个对于彼此正交、尽可能覆盖实际场景的维基百科词条类别集合

We construct our benchmark through machine annotation, human annotation, and filtering and reorganizing other publicly available datasets. Examples of tasks in our benchmark are presented in Figure~\ref{figure:MVRB-example-SR-CSR} and Figure~\ref{figure:MVRB-example-SQA-OVC}. The specific number of queries and the corpus for each subtask of our benchmark are listed in Table~\ref{tab:benchmark-data-statistics}. Below, we provide detailed information on our construction process for each meta task.

\subsection{Screenshot Retrieval}  
Screenshot Retrieval (SR) tasks are constructed through machine annotation and filtering/restructuring of publicly available datasets. Tasks constructed by machine annotation include \textit{product retrieval}, \textit{paper retrieval}, \textit{repo retrieval}, and \textit{news retrieval}. For these tasks, we use captions of screenshots to generate question-answer pairs using LLMs with the questions serving as queries. 
% The corresponding prompts are provided in Figure~\ref{}. 
To ensure quality, we implement a two-stage quality control process involving \textbf{automatic assessment} and \textbf{human verification}. First, each sample is assessed by three MLLMs (LLaVA1.6-34B\footnote{https://huggingface.co/liuhaotian/llava-v1.6-34b}, Molmo-72B\footnote{https://huggingface.co/allenai/Molmo-72B-0924}, Llama-3.2-90B-Vision-Instruct\footnote{https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct}) across three dimensions: 1) \textit{Clarity} : whether the query expresses a concrete information need, e.g., filtering out vague queries like "\textit{a sports car}", 2) \textit{Reasonableness}: whether the query is realistic, e.g., filtering out "\textit{a horse that can swim}"), and 3) \textit{Correctness}: whether the query can be answered by the retrieved screenshot, e.g., filtering out "\textit{what is the citation count of this paper}" since it cannot be answered solely from the screenshot, with the corresponding prompts detailed in Figure~\ref{fig:prompt-vlm-experts}. Each evaluation sample is independently reviewed by the MLLMs, and any sample failing any criterion is discarded. The remaining samples are further verified by human labelers using the same principles. An evaluation sample is successfully created only if it passes both stages of quality control.

Tasks derived from filtering and restructuring publicly available datasets include \textit{chart-retrieval}, \textit{document-retrieval}, and \textit{slide-retrieval}. We selected test sets from ChartVQA~\cite{chartvqa}, DocVQA~\cite{docvqa}, and SlideVQA~\cite{slidevqa}, using their questions as queries and corresponding images as target screenshots. To refine the evaluation dataset, we remove cases where a single query matches multiple images, such as in SlideVQA. We also filter out context-dependent questions, like "\textit{What is the content of Section 3.4.5?}", ensuring that each query is tailored for retrieving a single screenshot.

To increase task difficulty, we introduce hard negatives based on the query and target screenshot, following a method similar to the hard negatives augmentation approach for q2s tuples and sq2s triplets (Appendix~\ref{appendix:data-annotation}). To prevent false negatives, we use MLLM filtering to ensure no relevant screenshots are mistakenly included in the hard negatives, with the corresponding prompt shown in Figure~\ref{fig:prompt-vlm-false-negatives}.

\subsection{Composed Screenshot Retrieval}  
\label{appendix:CSR-details}
We construct these tasks through human annotation, encompassing four subtasks: \textit{product discovery}, \textit{news-to-Wiki}, \textit{knowledge relation}, and \textit{Wiki-to-product}. These tasks are not only challenging but also hold significant real-world relevance. Annotators are guided to gather query screenshots from appropriate platforms, generate realistic and high-quality queries, and acquire the matching target screenshots. To enhance both the complexity and the discriminative aspect of the task, each annotator also identifies 8$\sim$10 similar screenshots as difficult negatives for each original screenshot. Below, we outline the construction process for each specific CSR task:
\begin{itemize}
    \item \textbf{Product Discovery}: This task simulates a real-world shopping scenario where users search for a related product based on specific needs such as price, color, brand, or accessories. Annotators browse Amazon, search for products from a predefined category set, capture screenshots, and then generate queries based on the information in the screenshots. They then retrieve another product screenshot that satisfies the query.  

    \item \textbf{News-to-Wiki}: This task mirrors a scenario where users read news articles and seek additional details on people, events, locations, or causes, available in Wikipedia entries. Annotators explore news platforms like BBC, Fox, and CNN, develop queries from the content, and find relevant Wikipedia entries, capturing screenshots that answer the query.

    \item \textbf{Knowledge Relation}: This task simulates a scenario where users browsing Wikipedia are interested in exploring related topics. When looking at one entry, they often seek additional information, which is available in another Wikipedia article. Annotators search Wikipedia, capture relevant screenshots, formulate queries based on the screenshot content, and find another entry screenshot that fulfills the query. 

    \item  \textbf{Wiki-to-Product}: This task captures the scenario where users reading Wikipedia articles about products may wish to purchase them.  Annotators are given Wikipedia entries related to products and retrieve corresponding screenshots from Amazon and Wikipedia to create query-target pairs.
\end{itemize}

\subsection{Screenshot Question Answering}
Screenshot Question Answering (SQA) tasks are constructed through machine annotation and include \textit{product-QA}, \textit{news-QA}, \textit{Wiki-QA}, \textit{paper-QA}, and \textit{repo-QA}. For these tasks, we utilize collected screenshots and generate question-answer pairs using MLLMs, with the corresponding prompts provided in Figure~\ref{fig:prompt-vlm-sqa}. To ensure quality of each evaluation example, we follow the same \textbf{automatic assessment} and \textbf{human verification} process used in creating SR tasks, assessing the evaluation example based on clarity, reasonableness, and correctness. To further increase task difficulty, we leverage large language models (LLMs) to generate hard negative answers—plausible yet incorrect answers that closely resemble the correct one—thereby making the task more challenging.

\subsection{Open-Vocab Classification} 
For Open-Vocab Classification (OVC) tasks, we start by constructing a mutually orthogonal and well-organized category corpus. Labels are then manually annotated on screenshots. The details of the taxonomy creation for each task are listed as follows:
\begin{itemize}
    \item \textbf{Product Classification}: Categories are sourced from Amazon product pages and use hierarchies like \textit{"Arts, Crafts \& Sewing - Knitting \& Crochet - Crochet Hooks"}. A word tree is used to refine these categories to ensure exclusivity.

    \item \textbf{News Topics}: Categories are collected from CNN, BBC, and Fox News and are manually consolidated to form a mutually exclusive and comprehensive set that covers a wide range of real-world scenarios.

    \item \textbf{Academic Fields}:  Predefined categories are obtained from the Arxiv homepage\footnote{https://arxiv.org/}.

    \item \textbf{Knowledge Classification}: Categories are defined based on the Wikipedia category page\footnote{https://en.wikipedia.org/wiki/Wikipedia:Contents/Categories} and are manually combined to create an exclusive and comprehensive classification set for Wikipedia entries.
\end{itemize}
    





% MVRB benchmark的构建主要使用四种种方式，机器标注、人工标注、整理已有的数据标签和引入已有的数据集。以下为构建详情：
% 机器标注，我们利用收集到的数据，通过LLM (Qwen2.5-72B-Instruction) 和 MLLM (InternVL2.5-78B)，构造了对应的query或target。我们采用了质量验证来保证生成数据的质量。具体来说，我们采用了多机器专家加上人工抽查。对于一个已经标注好了的任务，我们首先用多个VLM模型从三个维度对任务中的每个数据条目进行验证。(清晰性，保证生成的数据描述不宽泛，比如过滤掉一辆黑色的跑车这样的数据；合理性，保证生成的数据是否符合真实场景，比如过滤掉一匹会游泳的马这类数据；正确性，保证生成的数据只依赖于原始数据图像或文本，比如过滤该论文的引用数量是多少这类问题。)只有当3个VLM专家都通过该数据条目才视为数据质量合格，否则认为该数据质量不合格。如果某任务中的所有数据经过验证后的整体合格率为90%以下，则舍弃这部分数据，重新进行标注；如果达到90%的合格率，则通过人工抽样10%的数据进行三个维度的验证，若人工抽查没有问题，最终采用这部分生成的数据，否则重新进行标注。对于SQA类任务，在使用LLM和VLM构造QA对的时，同时生成了多个与正确答案类似、容易混淆的错误答案来提高任务难度；对于SR类任务，我们首先使用文本嵌入模型(BGE)对target image对应的caption进行编码作为corpus，接着我们用query tex在corpus中进行检索得到相似度较高的caption，将得到的caption与原始的query text一同输入进LLM进行验证，避免引入false negative。
% 人工标注，对于composed screenshot retrieval类任务，我们采用了人工标注流程。具体的，标注人员针对不同的domain，截取该domain中带有富文本信息的图像做为query image，针对该图像构建具有现实意义的query text，接着截取能够回答该query text和query image的target image。对于每个测试样例，我们还选取了2-5个与target image相似但错误的images做为难负样例，提升任务难度。
% 整理，对于OVC中的product和academic fields，我们整理了这些数据已有的数据标签来作为分类标签。具体来说，对于product categorization任务，我们爬取了amazon网站的商品详情页面做为query image，同时从HTML获取到了该商品对应的类别。对于所有的商品类别，我们构建了单词树进行了清洗，保证每个类别之间彼此正交，用这些类别来作为target text。对于academic categorization任务，我们从arxiv中爬取了论文，并从HTML中获取到该论文的分类信息。我们将每个论文的首页做为query image，将这些类别作为target text。
% 引入，对于SR中的Doc、Slide，我们从Doc-VQA、SlideVQA中选取了它们的test集。具体来说，Doc-VQA和SlideVQA中的数据格式为，一张image和一个对这张image提问的question，以及一个对应这个question的正确answer。我们使用了Doc-VQA和SlideVQA中的test集的question作为query text，将对应的image作为target image，不再使用原数据集中的answer部分。接着，对于这些数据，我们去除了同一个query对应多个candidate的情况，比如SlideVQA中一个问题对应的images可能有多张，并且我们去除了提问范围较为宽泛的question，比如章节3.4.5的内容是什么？以此来保证benchmark中的每个query只对应唯一的candidate。

% The construction of the MVRB benchmark mainly uses four methods: machine labeling, manual labeling, organizing existing data labels, and introducing existing datasets. The following are the details of the construction:

% \textbf{Machine labeling}: We used the collected data and constructed corresponding queries or targets using LLM (Qwen2.5-72B-Instruction) and MLLM (InternVL2.5-78B). We employed quality validation to ensure the quality of the generated data. Specifically, we used multiple machine experts along with manual spot-checking. For a task that has been labeled, we first verified each data entry in the task using multiple VLM models from three dimensions: clarity (ensuring that the generated data description is not too broad, such as filtering out data like "a black sports car"); rationality (ensuring that the generated data conforms to real-world scenarios, such as filtering out data like "a horse that can swim"); correctness (ensuring that the generated data only relies on the original image or text, such as filtering out questions like "what is the citation count of this paper"). Only when all three VLM experts validate a data entry is the data considered to have met the quality standards. Otherwise, the data is deemed unqualified. If the overall qualification rate of all the data in a task is below 90\% after verification, the data will be discarded, and re-labeling will be performed. If it reaches 90\%, a 10\% random sample of the data will undergo verification across the three dimensions. If there are no issues with the manual spot check, the generated data is accepted; otherwise, re-labeling will be performed. For SQA tasks, when constructing QA pairs using LLM and VLM, multiple incorrect answers that are similar to the correct answer and easily confused with it are generated to increase task difficulty. For SR tasks, we first use a text embedding model (BGE) to encode the caption of the target image as a corpus. Then, we use the query text to search for similar captions in the corpus. The retrieved caption is then input together with the original query text into LLM for validation to avoid introducing false negatives.

% \textbf{Manual labeling}: For composed screenshot retrieval tasks, we adopted a manual labeling process. Specifically, the annotators select images with rich textual information from different domains as query images, construct query texts that are meaningful within those domains for these images, and then select target images that can answer the query text and query image. For each test sample, we also select 3-5 similar but incorrect images as difficult negative samples to increase task difficulty.

% \textbf{Organizing}: For product and academic fields in OVC, we organized existing data labels to serve as classification labels. Specifically, for product categorization tasks, we crawled product detail pages from Amazon as query images and extracted the corresponding categories from the HTML. For all product categories, we built a word tree and cleaned it to ensure that each category was orthogonal to the others, using these categories as target texts. For academic categorization tasks, we crawled papers from arXiv and extracted the category information from the HTML. We used the first page of each paper as the query image and these categories as the target text.

% \textbf{Introducing}: For SR tasks involving Docs and Slides, we selected the test sets from Doc-VQA and SlideVQA. Specifically, the data format in Doc-VQA and SlideVQA consists of an image, a question about the image, and a correct answer corresponding to the question. We used the questions from the test sets of Doc-VQA and SlideVQA as query texts and the corresponding images as target images, without using the answer parts from the original datasets. Then, for these data, we removed cases where a single query corresponded to multiple candidates, such as when multiple images in SlideVQA correspond to a single question, and we removed questions with too broad a scope, such as "What is the content of section 3.4.5?" This was done to ensure that each query in the benchmark corresponds to a unique candidate.




% \subsection{Data Collection and Composition}

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccc}
\toprule
Task & SubTask & \#Query & \#Corpus \\
\midrule
\multirow{7}{*}{SR} 
& Product Retrieval& 496 & 5436 \\ \cmidrule{2-4}
& Paper Retrieval& 503 & 5021 \\ \cmidrule{2-4}
& Repo Retrieval&510 & 5024 \\ \cmidrule{2-4}
& News Retrieval& 491 & 5401 \\ \cmidrule{2-4}
& Chart Retrieval& 200 & 5000 \\ \cmidrule{2-4}
& Doc Retrieval& 200 & 5000 \\ \cmidrule{2-4}
& Slide Retrieval& 200 & 5000 \\ 
\cmidrule{1-4}
\multirow{4}{*}{CSR} 
& Product Discovery& 107 & 1012 \\ \cmidrule{2-4}
& News-to-Wiki & 101 & 1010 \\ \cmidrule{2-4}
& Knowledge Relation & 100 & 1011 \\ \cmidrule{2-4}
& Wiki-to-product & 100 & 969 \\ 
\cmidrule{1-4}
\multirow{5}{*}{SQA} 
& Product-QA & 498 & 2988 \\ \cmidrule{2-4}
& News-QA & 500 & 3000 \\ \cmidrule{2-4}
& Wiki-QA & 500 & 3000 \\ \cmidrule{2-4}
& Academic-QA & 481 & 2886 \\ \cmidrule{2-4}
& Repo-QA & 489 & 2934 \\
\cmidrule{1-4}
\multirow{4}{*}{OVC} 
& Product Classification & 500 & 5000 \\ \cmidrule{2-4}
& News Toptics & 584 & 74 \\ \cmidrule{2-4}
& Academic Fields & 500 & 142 \\ \cmidrule{2-4}
& Knowledge Classification & 480 & 46 \\
\bottomrule
\end{tabular}}
\caption{The specific number of queries and the corpus for each subtask}
\label{tab:benchmark-data-statistics}
\end{table}

\section{Baselines and Evaluation Setup}
\label{appendix-baselines}
We select retrieval models that are widely adopted in practice and academic research, which can be divided into three main categories: OCR + Text Retrievers, General Multimodal Retrievers, and Visualized Document Retrievers. In the OCR + Text Retrievers category, we used Paddle OCR to extract text from the image part and performed text concatenation in the order from the top-left to the bottom-right. For evaluation metrics, we used the Recall metric and recorded the Recall@1 for each task.

\subsection{OCR + Text Retrievers}
\label{appendix:baseline-ocr+text}
\noindent\textbf{BM25}~\cite{bm25-robertson2009probabilistic} is a classical information retrieval algorithm improved from TF-IDF, which optimizes relevance scoring through document length normalization (parameter $b$) and term frequency saturation mechanisms (parameter $k_1$). 
% Its formula is: $Score(Q,d) = \sum_i^n IDF(q_i)\cdot \frac{f_i\cdot(k_1+1)}{f_i+k_1\cdot(1-b+b\cdot\frac{dl}{avgdl})}$
% where $IDF$ calculates inverse document frequency, $dl$ denotes document length, and $avgdl$ is the average document length. 
For evaluation, we use the code of \href{https://github.com/dorianbrown/rank_bm25}{dorianbrown/rank\_bm25}.

\noindent\textbf{DPR}~\cite{dpr} employs dual BERT encoders for dense retrieval, mapping queries and documents into a shared vector space via contrastive learning. Trained with an ``in-batch negatives'' strategy for efficiency. For evaluation, we use the weight of \href{https://huggingface.co/facebook/dpr-question_encoder-single-nq-base}{facebook/dpr-question\_encoder-single-nq-base}.

\noindent\textbf{BGE}~\cite{bge} is a widely used BERT-based text embedding model. It is trained on a large text corpus and optimized using contrastive learning. For evaluation, we utilize the base version of BGE with weights from \href{https://huggingface.co/BAAI/bge-base-en-v1.5}{BAAI/bge-base-en-v1.5}.

\noindent\textbf{E5-Mistral}~\cite{e5-mistral} is a large language model-based embedding model derived from the Mistral-7B architecture. It uses the last hidden state of the [EOS] token as the embedding for the input text. For evaluation, we utilize the weights from \href{https://huggingface.co/intfloat/e5-mistral-7b-instruct}{intfloat/e5-mistral-7b-instruct}.

\subsection{General Multimodal Retrievers}
\label{appendix:baseline-general}
\noindent\textbf{VISTA}~\cite{vista} is a universal multimodal retriever based on general text embedding models, such as pre-trained BERT models~\cite{bert-devlin2018bert,bge}. It processes input images by converting them into sequences of patches, which are then fed into the pre-trained text embedding model alongside text tokens in an interleaved manner. Although VISTA was trained on natural images, we evaluated its performance on the MVRB by directly inputting document screenshot images.

\noindent\textbf{Uni-IR}~\cite{mbeir-wei2023uniir} is a single retriever to accomplish any multimodal retrieval task. UniIR can follow the instructions to take a heterogeneous query to retrival from a heterogeneous candidate pool with millions of candidates in diverse modalities. We use the weight of \href{https://huggingface.co/TIGER-Lab/UniIR}{TIGER-Lab/UniIR}.

\noindent\textbf{CLIP}~\cite{clip-radford2021learning} is a multimodal contrastive learning model comprising a ViT image encoder and Transformer text encoder. Pretrained on 400 million image-text pairs, CLIP maps cross-modal content into a shared vector space, enabling zero-shot image classification and cross-modal retrieval. It processes an image as 14*14 patches and outputs the 1+196 (ClS token and 196 patch tokens) sequence to represent the image. We use \href{https://huggingface.co/openai/clip-vit-large-patch14}{openai/clip-vit-large-patch14} for evaluation.

\noindent\textbf{SigLIP}~\cite{siglip2023} is a multimodal model based on SoViT-400m~\cite{sovit400m} architecture with a better contrastive loss fuction. The sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. This allows further scaling up the batch size, while also performing better at smaller batch sizes. It pre-trained on WebLi~\cite{webli} at resolution 384*384. We use the weight of \href{https://huggingface.co/google/siglip-so400m-patch14-384}{google/siglip-so400m-patch14-384}.

\noindent\textbf{E5-V}~\cite{e5-v2024} is a unified multimodal embedding framework that maps text/image inputs into a shared semantic space via prompt mechanisms. Innovatively trained with text-only pairwise data through single-modality contrastive learning.  We use the weight of \href{https://huggingface.co/royokong/e5-v}{royokong/e5-v} for evaluation.

\noindent\textbf{VLM2Vec}~\cite{jiang2024vlm2vec} builds upon the multimodal large language model Phi-3.5-V~\cite{phi3}. For any given input, it leverages the last hidden state of the [EOS] token to generate embeddings. VLM2Vec was trained on 20 diverse multimodal datasets, encompassing a range of tasks such as classification, visual question answering, retrieval, and visual grounding. Although the primary focus of its training corpus is on natural images, it also includes several document question answering datasets, such as DocVQA~\cite{mathew2021docvqa}. This extensive training enables VLM2Vec to exhibit strong performance in the Screenshot Question Answering (SQA) tasks within our MVRB benchmark, outperforming other models, including our own UniSE, which are zero-shot for these tasks. Despite its competitive performance in SQA tasks due to this specialized training, VLM2Vec is classified as a general-purpose retriever in our taxonomy because of its limited exposure to document data, both in terms of quantity and the narrow focus on question answering tasks. For evaluation, we use the weight of \href{https://huggingface.co/TIGER-Lab/VLM2Vec-Full}{TIGER-Lab/VLM2Vec-Full}.

\noindent\textbf{MM-Embed}~\cite{nv-mm-embed2024} is a multimodal dual-encoder model addressing vision-text bias through modality-aware negative sample mining. It supports hybrid-modality queries (e.g., text+image combinations) and enhances retrieval precision in complex scenarios via large language model reranking. We use the weight of \href{https://huggingface.co/nvidia/MM-Embed}{nvidia/MM-Embed}.

\subsection{Screenshot Document Retrievers}
\noindent\textbf{DSE}~\cite{ma2024unifying} is a bi-encoder model designed to encode document screenshots into dense vectors for document retrieval. It trained on Wiki pages and documents. We use the weight of \href{https://huggingface.co/Tevatron/dse-phi3-docmatix-v2}{Tevatron/dse-phi3-docmatix-v2}. 

\noindent\textbf{ColPali}~\cite{faysse2024colpali} is an advanced document retrieval model designed to leverage vision-language models (VLMs) to generate high-quality contextual embeddings from document page images. The model is based on PaliGemma-3B~\cite{beyer2024paligemma} and employs the ColBERT~\cite{khattab2020colbert} strategy to create multi-vector representations of document page, thereby enhancing the accuracy and efficiency of retrieval. We use the weight of \href{https://huggingface.co/vidore/colpali-v1.3-hf}{vidore/colpali-v1.3-hf} for evaluation.

\noindent\textbf{GME}~\cite{zhang2024gme} is based on the Qwen2-VL model, and utilizes a contrastive learning approach to integrate diverse data into a unified semantic space. The GME has been trained on the visual document dataset Docmatix~\cite{laurenccon2024building}, which makes it have a great power to handle the visual document. For evaluation, we use the weight of \href{https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct}{Alibaba-NLP/gme-Qwen2-VL-2B-Instruct}.

\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/prompt_q2s.pdf}
    \vspace{-0.3cm}
    \caption{The prompt used for q2s annotation. This prompt is designed for the news domain. For other domains, the word in blue can be substituted with the appropriate term for that domain.} 
    \vspace{-0.3cm}
    \label{fig:prompt-q2s}
\end{figure*}  

\vspace{0.5cm}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/prompt_sq2s.pdf}
    \vspace{-0.3cm}
    \caption{The prompt used for sq2s annotation. This prompt is intended for the paper domain. For other domains, the word in blue can be substituted with the appropriate term for that domain.} 
    \vspace{-0.3cm}
    \label{fig:prompt-sq2s}
\end{figure*}  

\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/prompt_vlm_experts.pdf}
    \vspace{-0.3cm}
    \caption{The prompt used for qulatiy control. We employ MLLMs to evaluate the clarity, plausibility, and validity of the evaluation examples.} 
    \vspace{-0.3cm}
    \label{fig:prompt-vlm-experts}
\end{figure*}  



\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/prompt_vlm_false-negatives.pdf}
    \vspace{-0.3cm}
    \caption{The prompt used for filtering out false negatives.} 
    \vspace{-0.3cm}
    \label{fig:prompt-vlm-false-negatives}
\end{figure*}  


\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/prompt_vqa_task.pdf}
    \vspace{-0.3cm}
    \caption{The prompt used for creating screenshot question answering task.} 
    \vspace{-0.3cm}
    \label{fig:prompt-vlm-sqa}
\end{figure*}  

\newcounter{maintable} 
\renewcommand{\thetable}{\arabic{maintable} (\Alph{table})} 


\setcounter{maintable}{7} % 增加主表格计数器
\setcounter{table}{0} % 设置子表格编号为 A
\begin{table*}[htp]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
& \textbf{BM25} & \textbf{DPR} & \textbf{BGE} & \textbf{E5-Mistral} & \textbf{CLIP} & \textbf{SIGLIP} & \textbf{VISTA} & \textbf{Uni-IR} \\
\midrule
\rowcolor{gray!30} \textbf{SR} & & & & & & & &\\
Product          & 59.27 & 29.44 & 62.70 & 69.75 & 34.88 & 55.24 & 10.48  & 24.19 \\
Paper               & 70.38 & 23.66 & 50.89 & 69.18 & 6.96 & 26.24 & 0.60 & 1.19 \\
Repo       & 44.71 & 30.78 & 53.92 & 60.20 & 15.69 & 47.65 & 3.14 & 7.84 \\
News              & 38.08 & 26.48 & 43.18 & 47.45 & 24.24 & 33.20 & 4.28 & 11.20 \\
Chart              & 9.00 & 8.50 & 21.50 & 33.00 & 4.50 & 24.00 & 0.50 & 4.50 \\
Document   & 19.00 & 19.00 & 30.50 & 29.00 & 7.50 & 21.00 & 2.50 & 6.00 \\
Slide  & 44.50 & 37.00 & 57.00 & 51.50 & 38.50  & 61.00  & 15.00 & 31.50 \\
\textit{Avg} & 40.71 & 24.98 & 45.67 & 51.44 & 18.89 & 38.33 & 5.21 & 12.35 \\
\midrule

\rowcolor{gray!30} \textbf{CSR} & & & & & & & & \\
Knowledge Relation               & 19.00  & 15.00 & 18.00 & 23.00 & 18.00 & 30.00 & 6.00 & 23.00 \\
News2Wiki              & 20.79 & 10.89 & 17.82 & 31.68 & 16.83 & 21.78 & 6.93 & 18.81 \\
Product Discovery               & 25.23 & 28.04 & 27.10 & 33.64 & 32.71  & 55.14 & 25.23 & 29.91 \\
Wiki2Product      & 49.00 & 21.00 & 84.00 & 92.00 & 34.00 & 31.00 & 7.00 & 47.00 \\
\textit{Avg}      & 28.51  & 18.73 & 36.73 & 45.08 & 25.39  & 34.48 & 11.29 & 29.68 \\
\midrule

\rowcolor{gray!30} \textbf{SQA} & & & & & & & & \\
Repo-QA              & 46.63 & 35.38 & 49.69 & 33.33 & 27.81 & 24.54 & 37.63 & 29.24 \\
News-QA                 & 38.40 & 35.40 & 38.80 & 57.60  & 31.80 & 19.80 & 24.00 & 22.20 \\
Product-QA & 22.49 & 10.04 & 26.31 & 27.71 & 15.26 & 16.06 & 15.46 & 12.85 \\
Paper-QA      & 46.78 & 30.15 & 32.85 & 52.18 & 22.45 & 21.62 & 31.60 & 21.62 \\
Wiki-QA          & 38.00 & 25.80 & 33.80 & 44.20 & 22.20 & 16.00 & 20.20 & 21.20 \\
\textit{Avg} & 38.46 & 27.25 & 36.29 & 43.00 & 23.90 & 19.60 & 25.78 & 21.42 \\
\midrule

\rowcolor{gray!30} \textbf{OVC} & & & & & & & & \\
Product Classification              & 7.6  & 1.60 & 36.20 & 41.00 & 21.00 & 39.40 & 6.80 & 10.40 \\
News  Topics            & 8.22  & 39.04  & 63.35 & 59.42 & 50.68 & 63.18 & 30.99 & 39.38 \\
Knowledge Classification               & 7.08  & 37.08 & 52.50 & 42.08 & 42.91  & 44.58 & 26.25 & 26.67 \\
Academic Fileds      & 2.00 & 10.60 & 39.60 & 12.20 & 7.00 & 15.40 & 2.40 & 3.80 \\
\textit{Avg}      & 6.23  & 22.08 & 47.91 & 38.68 & 30.40  & 40.64 & 16.61 & 20.06 \\
\midrule

\rowcolor{gray!30} \textbf{Overall Score} & & & & &  & & & \\
All                  & 30.81 & 23.74 & 41.99 & 45.51  & 23.75 & 33.34 & 13.85 & 19.63 \\
\bottomrule
\end{tabular}
}
\caption{Detailed performance results for each task of different retrievers on MVRB (Recall@1).}
\label{tab:main_exp_per_task}
\end{table*}

\setcounter{table}{1}
\begin{table*}[htp]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
& \textbf{VLM2Vec} & \textbf{E5-V} & \textbf{MM-Embed} & \textbf{DSE} & \textbf{Colpali} & \textbf{GME} & \textbf{UniSE-CLIP} & \textbf{UniSE-MLLM} \\
\midrule
\rowcolor{gray!30} \textbf{SR} & & & & & & & &\\
Product          & 23.59 & 33.46 & 28.83 & 71.17 & 72.50 & 57.80 & 53.02 & 75.40 \\
Paper              & 7.75 & 39.36 & 18.09 & 78.02 & 80.31 & 74.55 & 34.19 & 90.26 \\
Repo         & 17.65 & 36.08 & 30.59 & 74.01 & 68.23 & 76.47 & 40.98 & 83.33 \\
News              & 17.52 & 26.88 & 24.03 & 50.61 & 41.10 & 57.00 & 39.92 & 75.96 \\
Chart               & 11.50 & 23.00 & 11.00 & 29.00 & 24.00 & 39.00 & 11.00 & 36.50 \\
Document  & 12.00 & 26.00 & 18.50 & 47.50 & 59.50 & 51.00 & 17.00 & 50.50 \\
Slide & 21.50 & 54.00 & 50.00 & 80.50  & 86.50  & 75.50 & 55.50 & 75.50 \\
\textit{Avg} & 15.93 & 34.11 & 25.86 & 61.54 & 61.73 & 61.62 & 35.95 & 69.64 \\
\midrule

\rowcolor{gray!30} \textbf{CSR} & & & & & & & & \\
Knowledge Relation             & 47.00  & 18.00  & 26.00  & 22.92  & 16.00  & 20.00 & 29.00 &  41.00 \\
News2Wiki            & 49.50  & 37.62  & 41.58  & 21.88  & 16.80    & 19.80 & 27.72 & 44.55 \\
Product Discovery             & 32.71  & 34.58  & 41.12 & 24.04  & 25.20    & 28.90 & 45.79 & 51.40 \\
Wiki2Product      & 63.00 & 30.00 & 55.00 & 82.29  & 82.00   & 82.00 & 71.00 & 81.00\\
\textit{Avg}    & 48.05 & 30.05 & 40.93 & 37.78  & 35.00   & 37.68 & 43.38 & 54.49 \\
\midrule

\rowcolor{gray!30} \textbf{SQA} & & & & & & & & \\
Repo-QA            & 58.28 & 36.81 & 53.99  & 46.93 & 42.33  & 48.80 & 35.58 & 57.00 \\
News-QA               & 51.40 & 36.20 & 50.20  & 40.52 & 40.40  & 43.00 & 33.00 & 47.60 \\
Product-QA     & 36.55 & 19.68 & 30.12 & 22.98 & 17.67  & 18.80 & 19.68 & 25.10 \\
Paper-QA     & 43.78 & 34.10 & 38.46 & 46.67 & 46.98  & 45.30 & 25.36 & 46.70 \\
Wiki-QA        & 57.00 & 30.20 & 41.40  & 39.11 & 29.20  & 33.00 & 27.00 & 39.60 \\
\textit{Avg}& 49.42 & 31.40 & 42.83 & 39.24 & 35.32  & 37.78 & 28.13 & 43.20 \\
\midrule

\rowcolor{gray!30} \textbf{OVC} & & & & & & & & \\
Product Classification         & 20.80 & 14.20 & 26.80 & 30.00 & 29.60 & 33.40 & 33.20 & 45.40 \\
News Topics & 46.75 & 52.57 & 46.75 & 56.68 & 44.00 & 69.60 & 57.19 & 69.34 \\
Knowledge Classification   & 14.79 & 45.42 & 43.13 & 23.54 & 33.95 & 58.50 & 52.29 & 52.91 \\
Academic Fileds & 10.60 & 19.20 & 14.00 & 15.80 & 16.60 & 30.40 & 19.80 & 25.40 \\
\textit{Avg} & 23.24 & 32.85 & 32.67 & 31.51 & 31.04 & 47.98 & 40.62 & 48.26 \\
\midrule

\rowcolor{gray!30} \textbf{Overall Score} & & & & &  & & & \\
All                 & 32.19 & 32.37  & 34.48 & 48.14 & 43.64  & 13.3 & 34.99 & 55.72 \\
\bottomrule
\end{tabular}
}
\caption{Detailed performance results for each task of different retrievers on MVRB (Recall@1).}
\label{tab:main_exp_per_task2}
\end{table*}



\section{Detailed Results for Each Task}
\label{appendix-detailed results}
The detailed results of different retrievers for each task  on MVRB are presented in Table~\ref{tab:main_exp_per_task} and Table~\ref{tab:main_exp_per_task2}.

\begin{figure*}[h]
\centering
    \scriptsize
    \begin{center}
    % \resizebox{1.0\linewidth}{!}
    \tabcolsep 2pt
    \begin{tabular}{m{1.8cm}m{1.8cm}m{3.5cm}m{2.1cm}>{\columncolor{lightgray}}m{3.5cm}}
            \toprule
             \textbf{Task} & \textbf{Subtask} & \textbf{Query Text} & \textbf{Query Image} & \textbf{Target} \\
            \midrule
            \multirow{13}{*}{\makecell[c]{Screenshot\\ Retrieval}}
                  & Product Retrieval
                  & What are the dimensions of the Baosha Women's Small Sling Crossbody bag?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr1.png}\\
            \cmidrule{2-5}
                  & News Retrieval
                  & What are the four foundational codes that explain Trump's tumultuous path through his life, according to the judge's ruling in the New York fraud case?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr2.png}\\
            \cmidrule{2-5}
                  & Paper Retrieval
                  & What is the 'mass twins' scenario in the context of compact stars?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr3.jpg}\\
            \cmidrule{2-5}
                  & Chart Retrieval
                  & What percentage of the prize pool did eSports team devils.one receive?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr4.png}\\
            \cmidrule{2-5}
                  & \makecell[l]{Document\\ Retrieval}
                  & Who is the staff representative mentioned in the document?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr5.png}\\
            \cmidrule{2-5}
                  & Slide Retrieval
                  & How many more skyscrapers does Tokyo have than London according to the slide?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr6.png}\\
            \cmidrule{2-5}
                  & Repo Retrieval
                  & What are the prerequisites for successfully completing the Knowledge Mining Solution Accelerator?
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sr7.jpg}\\
            \midrule
            \multirow{15}{*}{\makecell[c]{Composite\\Screenshot\\ Retrieval}}
                  & \makecell[l]{Product\\ Discovery}
                  & I need a larger size of the same brand.
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr1.png}
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr2.png}\\
            \cmidrule{2-5}
                  & \makecell[l]{Knowledge\\ Relation}
                  & Which temple is the Buddha statue in the image located?
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr3.png}
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr4.png}\\
            \cmidrule{2-5}
                  & News-to-Wiki
                  & I need information about the political party of this man in this event.
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr5.png}
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr6.png}\\
            \cmidrule{2-5}
                  & Wiki-to-Product
                  & Search the most relevant product page.
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr7.png}
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/csr8.png}\\
            \bottomrule
    \end{tabular}
    \caption{Examples of tasks in Screenshot Retrieval and Composite Screenshot Retrieval in MVRB.}
    \label{figure:MVRB-example-SR-CSR}
    \end{center}
\end{figure*}

\begin{figure*}[h]
\centering
    \scriptsize
    \begin{center}
    % \resizebox{1.0\linewidth}{!}
    \tabcolsep 2pt
    \begin{tabular}{m{1.8cm}m{1.8cm}m{3.5cm}m{2.1cm}>{\columncolor{lightgray}}m{3.5cm}}
            \toprule
             \textbf{Task} & \textbf{Subtask} & \textbf{Query Text} & \textbf{Query Image} & \textbf{Target} \\
            \midrule
            \multirow{15}{*}{\makecell[c]{Screenshot\\ Question \\ Answering}}
                  & Product-QA
                  & What type of chain is used in the S925 Circle of Life Cremation Memorial Necklace?
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sqa1.png}
                  & Cable.\\
            \cmidrule{2-5}
                  & News-QA
                  & What did Jeffrey Garten accidentally send to Ina Garten's publicist?
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sqa2.png}
                  & A text that said, "You're gonna be delicious tonight."\\
            \cmidrule{2-5}
                  & Wiki-QA
                  & What was the initial inspiration for the tone of the Sailor Moon soundtrack?
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sqa3.png}
                  & The initial inspiration for the tone of the Sailor Moon soundtrack was the Charlie's Angels TV series.\\
            \cmidrule{2-5}
                  & Paper-QA
                  & What is the key feature of the optimal contingent debt contract according to Proposition 5?
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sqa4.png}
                  & A contingent debt contract with at most one non-defaultable face value.\\
            \cmidrule{2-5}
                  & Repo-QA
                  & What does the Mastodon-hashtag-collector API method /hashtag/:hashtag return?
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/sqa5.png}
                  & It returns the collected messages for a specific hashtag.\\
            \midrule
            \multirow{13}{*}{\makecell[c]{Open-Vocab\\ Classification}}
                  & \makecell[l]{Product\\ Classification}
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/ovc1.png}
                  & Clothing, Shoes \& Jewelry - Women - Uniforms, Work \& Safety - Clothing - Medical - Scrub Tops\\
            \cmidrule{2-5}
                  & News Topics
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/ovc2.png}
                  & Sports, Football\\
            \cmidrule{2-5}
                  & Arxiv Fileds
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/ovc3.png}
                  & Artificial Intelligence\\
            \cmidrule{2-5}
                  & \makecell[l]{Knowledge\\ Classification}
                  & --
                  & \includegraphics[width=15mm,height=15mm]{fig/benchmark_examples/ovc4.png}
                  & Society and social sciences, Government and Politics\\
            \bottomrule
    \end{tabular}
    \caption{Examples of tasks in Screenshot Question Answering and Open-Vocab Classification in MVRB.}
    \label{figure:MVRB-example-SQA-OVC}
    \end{center}
\end{figure*}


% \section{Case Study}


\end{document}
