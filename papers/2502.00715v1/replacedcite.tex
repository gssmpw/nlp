\section{RELATED WORK}
\begin{comment}
\begin{table*}[ht]
\centering
\caption{Comparison of related works in terms of the simulation platform.}
\label{tab:relatedworkstable}
\resizebox{\textwidth}{!}{%
%\begin{tabular}{|l|l|l|l|l|}
\begin{tabular}{|l|p{4cm}|p{4cm}|p{4cm}|p{4cm}|}
\hline
\hline
\textbf{Ref.} & \textbf{Implementation Strategy} & \textbf{Platform/Simulation Tools} & \textbf{Evaluation Metrics} & \textbf{Dataset/Scenario} \\ \hline
____ %M. Tsampazi et al. 
& DRL-based xApps, actor-critic, hierarchical decision-making  & OpenRAN Gym, srsRAN & Latency, Throughput, Package transmission & 7 BS, 42 UEs (SDR-based) \\ \hline
____ %V. Marojevic et al. 
& DRL-based xApps, actor-critic & OpenAI Gym, mobile-env Simulator ____  & Throughput, Normalized cumulative reward & Heterogeneous networks: 5 base stations and 10 moving UEs \\ \hline
____ %D. Anand et al. 
& Multi-Classification and Offloading Scheme (MLMCOS) & NS-3 & VMAF, R-Factor, RUM Speed Index & Heterogeneous network environment (HetNet) \\ \hline
____ %L. Bonati et al. 
& OpenRAN Gym & Colosseum & Latency, Spectrum efficiency, Scalability & 128 pairs of generic compute servers and SDR \\ \hline
\end{tabular}%
}
\vspace{-5pt}
\end{table*}
\end{comment}

Various simulation platforms and frameworks have been utilized in recent works to evaluate the effectiveness of AI/ML-based optimization strategies within O-RAN architectures. These platforms, ranging from OpenRAN Gym to NS-3 and custom simulators, offer unique features tailored to specific research objectives such as resource allocation, interference management, and network slicing. Each study highlights distinct implementation strategies, performance metrics, and scenarios.
____ presents an O-RAN architecture using Deep RL (DRL)-based xApps for optimizing eMBB, URLLC, and mMTC slices. Tested on the Colosseum wireless network emulator via the OpenRAN Gym framework, these xApps apply Proximal Policy Optimization (PPO) for real-time resource management. The study evaluates multiple configurations, revealing trade-offs in action spaces and reward functions, showcasing scalability and adaptive performance across varied network conditions.
Building upon this, ____ investigates PPO for real-time resource management within O-RAN but adds a comparative analysis of Advantage Actor-Critic (A2C). Both models optimize resource allocation through the E2 interface in communication with the near-real-time RIC, but the study demonstrates PPO's faster convergence and superior rewards over A2C. Tested in a simulated RAN environment under dynamic conditions, the research highlights PPO's superior efficiency for resource management.
In a shift toward interference management, ____ introduces a machine learning-based xApp for mitigating co-tier interference in a Heterogeneous Network (HetNet) environment, with a focus on improving QoE for services like video and VoIP. The xApp employs a Multi-Classification and Offloading Scheme (MLMCOS), utilizing models such as Random Forest and CNN to classify users based on interference levels and offload them to femtocells. Tested using NS-3 ____, this study emphasizes interference management within HetNet environments, offering a more focused solution for dense networks compared to the previous studies on resource allocation.
Further expanding on AI-driven optimization, ____ highlights Colosseum's role as an AI/ML-based digital twin platform for O-RAN development. Through its OpenRAN Gym framework, the platform supports real-time deployment and testing of algorithms such as network slicing, scheduling, and spectrum sharing. Integrated with SDRs and real-world RF conditions, Colosseum allows for scalable, high-fidelity testing of AI/ML xApps, enabling continuous interaction between the digital twin and physical network layers. This infrastructure bridges the gap between simulation and real-world deployment, offering detailed insights into the scalability and robustness of AI/ML solutions under complex conditions.

%These efforts reveal two primary gaps: (1) a lack of full-stack integration at scale---existing systems often handle only a small number of UEs or rely on constrained simulations---and (2) limited real-time RL-based slicing, as most frameworks depend on offline training or partial control loops.

These efforts reveal two primary gaps including 
%\begin{enumerate}[left=0pt, noitemsep]
i) \textbf{Full-stack integration at scale:}  Existing systems often handle only a small number of UEs or operate within constrained simulation setups (____, ____, ____), and ii)
 \textbf{Real-time RL-based slicing:} Many frameworks rely on offline training or partial control loops, deferring real-time decision-making and adaptability (____, ____,____). Unlike many frameworks that rely on offline training, which cannot capture real-time environmental feedback, our work emphasizes online training. In this approach, the RL agent interacts directly with the srsRAN simulator during training, receiving immediate rewards based on its resource allocation decisions. This ensures dynamic adaptation to network traffic and performance, overcoming the limitations of offline methods that fail to reflect real-world conditions. Our framework thus enables responsive and optimized physical resource block allocation in 5G O-RAN architectures.
 %\fa{I feel like we did not emphasize enough on this gap when describing the related work. Either here, or in the last paragraph of the contribution subsection when we talk about PPO with online training, we should descrbe the advantages of an online trained PPO on the ability to adjusts the policy dynamically based on current observations and changes in network conditions. We should also discuss the its high demands for computational resources compared to an offline case and mention this as a bottleneck for our work. could be discussed in the conclusion. }
 %\fa{pls add references for each gap}
%\end{enumerate}
To address the aforementioned gaps, we introduce a full-stack O-RAN solution that unifies the OSC near-RT RIC with srsRAN ____ for real-time slicing:

\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{End-to-end Integration on OSC RIC and srsRAN:} 
    We develop a near-RT xApp that directly controls srsRAN’s gNB through the E2 interface, using E2AP messages to manage PRB allocations for up to 12 UEs.

    \item \textbf{Fully Online Closed-Loop Training:} 
    Our RL agent receives immediate feedback (downlink throughput, slice QoS) from srsRAN and updates its policy in real time. This ensures a genuine closed-loop approach, where each action on the RAN triggers immediate learning and fine-tuning in the xApp.

    \item \textbf{GNU Radio Channel Emulation:}
    We incorporate Free-Space Path Loss (FSPL), Additive White Gaussian Noise (AWGN), single-tap multipath fading, and Doppler shifts to approximate urban mobility.
    %within srsRAN’s ZeroMQ-based environment.

    \item \textbf{Practical Insights for Scalability:}
    We highlight operational constraints such as ZeroMQ saturation (limiting concurrency), partial uplink slicing, and attach-sequencing workarounds. These insights inform future O-RAN development for larger-scale deployments.
\end{itemize}

This paper presents an \emph{online}, RL-driven resource allocation strategy for managing multiple slices. 

The system model and E2-based control mechanisms are introduced in Section~\ref{sec:system}, followed by a detailed discussion of the RL methodology and its constraints in Section~\ref{sec:method}. Section~\ref{sec:scenarios} explores the simulation scenarios and channel conditions used to evaluate the framework. The performance of the system under varying mobility and traffic conditions is analyzed in Section~\ref{sec:results}. Finally, Section~\ref{sec:conclusion} summarizes the findings and outlines potential directions for future research in O-RAN.

\ifthenelse{\boolean{longversion}}{
%