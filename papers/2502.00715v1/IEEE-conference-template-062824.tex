\documentclass[conference]{IEEEtran}
%\usepackage{multirow}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{ifthen}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\newboolean{longversion}
\setboolean{longversion}{false} % TOGGLE SWITCH
%\setlength{\textfloatsep}{100pt}  % Space between floats and text
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\fa}[1]{\textcolor{magenta}{Fatemeh: #1}}
\newcommand{\ts}[1]{\textcolor{cyan}{Tolunay: #1}}
\newcommand{\rb}[1]{\textcolor{red}{Ryan: #1}}
\newcommand{\bd}[1]{\textcolor{green}{Basir: #1}}
\usepackage{enumitem}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{multirow} 
\renewcommand{\baselinestretch}{0.85}
\begin{document}

\title{REAL: Reinforcement Learning-Enabled xApps for  Experimental Closed-Loop Optimization in O-RAN with OSC RIC and srsRAN
\thanks{This material is based upon work supported by the National Science Foundation under Grant Numbers  CNS-2318726, and CNS-2232048.}
}

\author{
	\IEEEauthorblockN{
	Ryan Barker, 
        Alireza Ebrahimi Dorcheh,
        Tolunay Seyfi,
        Fatemeh Afghah}

    \IEEEauthorblockA{Holcombe Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA \\
        Emails: \{rcbarke, alireze, tseyfi, fafghah\}@clemson.edu}

}

\maketitle

%\rb{Long version: (1) ORAN Architecture overview, (2) Expanded srsRAN limitations, (3) Complementary Detail}

\begin{abstract}
Open Radio Access Network (O-RAN) offers an open, programmable architecture for next-generation wireless networks, enabling advanced control through AI-based applications on the near-Real-Time RAN Intelligent Controller (near-RT RIC). However, fully integrated, real-time demonstrations of closed-loop optimization in O-RAN remain scarce. In this paper, we present a complete framework that combines the O-RAN Software Community RIC (OSC RIC) with srsRAN for near-real-time network slicing using Reinforcement Learning (RL). Our system orchestrates resources across diverse slice types (eMBB, URLLC, mMTC) for up to 12 UEs. 
We incorporate GNU Radio blocks for channel modeling, including Free-Space Path Loss (FSPL), single-tap multipath, AWGN, and Doppler effects, to emulate an urban mobility scenario. Experimental results show that our RL-based xApps dynamically adapt resource allocation and maintain QoS under varying traffic demands, highlighting both the feasibility and challenges of end-to-end AI-driven optimization in a lightweight O-RAN testbed. Our findings establish a baseline for real-time RL-based slicing in a disaggregated 5G framework and underscore the need for further enhancements to support fully simulated PHY digital twins without reliance on commercial software.
\end{abstract}

\begin{IEEEkeywords}
O-RAN, %near-RT RIC, Open5GS, 
srsRAN, 
Reinforcement Learning, Network Slicing, Resource Allocation %, 5G SA, GNU Radio, E2, xApps
\end{IEEEkeywords}

\section{Introduction and Related Work}
\label{sec:intro}

Fifth-generation (5G) cellular networks introduce a range of capabilities, including enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communications (URLLC), and massive Machine-Type Communications (mMTC) \cite{AIin5G}. These services pose diverse Quality of Service (QoS) requirements, demanding a more flexible Radio Access Network (RAN) that can dynamically allocate resources in near real-time. The Open Radio Access Network (O-RAN) framework meets this need by disaggregating traditional, vendor-locked architectures into interoperable components. Central to this vision is the near-Real-Time RAN Intelligent Controller (near-RT RIC), an AI-driven platform that executes fine-grained optimization of RAN functions over the E2 interface \cite{ORAN, ORAN_Tommaso}.
A distinguishing feature of O-RAN is the ability to host third-party applications, known as xApps, on the near-RT RIC. These xApps can employ machine learning or control-theoretic strategies to orchestrate diverse slice types, ranging from high-throughput eMBB to latency-sensitive URLLC \cite{AIinORAN}. Among learning-based approaches, \emph{Reinforcement Learning} (RL) is well-suited for adaptive, real-time control: it can iteratively refine its policy based on a reward signal reflecting slice performance (e.g., throughput, latency, or reliability). 

In recent years, several platforms have explored the integration of RL and AI in O-RAN, leveraging testbeds to address resource allocation and network optimization challenges. X5G introduces a modular 5G O-RAN platform with NVIDIA ARC for GPU-accelerated PHY layer tasks and OpenAirInterface (OAI) for higher layers \cite{villa2023x5g}. It supports real-time control via the near-RT RIC, evaluating performance using up to eight concurrent Commercial Off-the-Shelf (COTS) UEs. ORANSlice, on the other hand, emphasizes multi-slice resource allocation with compliance to 3GPP and O-RAN standards, leveraging xApps for dynamic physical resource block (PRB) allocation and scheduling policies \cite{cheng2024oranslice}. ColO-RAN offers a large-scale framework for ML-based xApp development, integrating RL-based control strategies and large-scale data collection using the Colosseum wireless network emulator \cite{coloran}. 
The PandORA framework introduces an automated approach to designing and evaluating DRL-based xApps for Open RAN, leveraging the Colosseum wireless network emulator for large-scale testing under diverse traffic and channel conditions \cite{tsampazi2024pandora}. 
%This work benchmarks multiple xApps that integrate Deep Reinforcement Learning (DRL) agents with varying architectures, reward designs, action spaces, and decision-making timescales.
%demonstrating that fine-tuning control timers and optimizing reward structures can significantly improve network performance. \bd{Let's put way less emphasize on their work.}

%Despite these advancements, existing solutions have notable limitations. X5G lacks a real-time closed-loop system for dynamic optimization via RL-based xApps and demonstrates limited scalability. ORANSlice supports only two UEs in testing and does not incorporate RL for closed-loop adaptability under complex network conditions. ColO-RAN primarily relies on offline training, limiting its ability to respond to real-time changes such as shifting UE positions or traffic demands. These gaps highlight the need for novel frameworks that combine real-time RL control with scalability and comprehensive testing to address the evolving challenges of 5G and beyond.


\section{RELATED WORK}
\begin{comment}
\begin{table*}[ht]
\centering
\caption{Comparison of related works in terms of the simulation platform.}
\label{tab:relatedworkstable}
\resizebox{\textwidth}{!}{%
%\begin{tabular}{|l|l|l|l|l|}
\begin{tabular}{|l|p{4cm}|p{4cm}|p{4cm}|p{4cm}|}
\hline
\hline
\textbf{Ref.} & \textbf{Implementation Strategy} & \textbf{Platform/Simulation Tools} & \textbf{Evaluation Metrics} & \textbf{Dataset/Scenario} \\ \hline
\cite{tsampazi2023} %M. Tsampazi et al. 
& DRL-based xApps, actor-critic, hierarchical decision-making  & OpenRAN Gym, srsRAN & Latency, Throughput, Package transmission & 7 BS, 42 UEs (SDR-based) \\ \hline
\cite{marojevic2022actor} %V. Marojevic et al. 
& DRL-based xApps, actor-critic & OpenAI Gym, mobile-env Simulator \cite{mobile-env}  & Throughput, Normalized cumulative reward & Heterogeneous networks: 5 base stations and 10 moving UEs \\ \hline
\cite{Anand2023xApp} %D. Anand et al. 
& Multi-Classification and Offloading Scheme (MLMCOS) & NS-3 & VMAF, R-Factor, RUM Speed Index & Heterogeneous network environment (HetNet) \\ \hline
\cite{Colosseum2023} %L. Bonati et al. 
& OpenRAN Gym & Colosseum & Latency, Spectrum efficiency, Scalability & 128 pairs of generic compute servers and SDR \\ \hline
\end{tabular}%
}
\vspace{-5pt}
\end{table*}
\end{comment}

Various simulation platforms and frameworks have been utilized in recent works to evaluate the effectiveness of AI/ML-based optimization strategies within O-RAN architectures. These platforms, ranging from OpenRAN Gym to NS-3 and custom simulators, offer unique features tailored to specific research objectives such as resource allocation, interference management, and network slicing. Each study highlights distinct implementation strategies, performance metrics, and scenarios.
\cite{tsampazi2023} presents an O-RAN architecture using Deep RL (DRL)-based xApps for optimizing eMBB, URLLC, and mMTC slices. Tested on the Colosseum wireless network emulator via the OpenRAN Gym framework, these xApps apply Proximal Policy Optimization (PPO) for real-time resource management. The study evaluates multiple configurations, revealing trade-offs in action spaces and reward functions, showcasing scalability and adaptive performance across varied network conditions.
Building upon this, \cite{marojevic2022actor} investigates PPO for real-time resource management within O-RAN but adds a comparative analysis of Advantage Actor-Critic (A2C). Both models optimize resource allocation through the E2 interface in communication with the near-real-time RIC, but the study demonstrates PPO's faster convergence and superior rewards over A2C. Tested in a simulated RAN environment under dynamic conditions, the research highlights PPO's superior efficiency for resource management.
In a shift toward interference management, \cite{Anand2023xApp} introduces a machine learning-based xApp for mitigating co-tier interference in a Heterogeneous Network (HetNet) environment, with a focus on improving QoE for services like video and VoIP. The xApp employs a Multi-Classification and Offloading Scheme (MLMCOS), utilizing models such as Random Forest and CNN to classify users based on interference levels and offload them to femtocells. Tested using NS-3 \cite{ns3}, this study emphasizes interference management within HetNet environments, offering a more focused solution for dense networks compared to the previous studies on resource allocation.
Further expanding on AI-driven optimization, \cite{Colosseum2023} highlights Colosseum's role as an AI/ML-based digital twin platform for O-RAN development. Through its OpenRAN Gym framework, the platform supports real-time deployment and testing of algorithms such as network slicing, scheduling, and spectrum sharing. Integrated with SDRs and real-world RF conditions, Colosseum allows for scalable, high-fidelity testing of AI/ML xApps, enabling continuous interaction between the digital twin and physical network layers. This infrastructure bridges the gap between simulation and real-world deployment, offering detailed insights into the scalability and robustness of AI/ML solutions under complex conditions.

%These efforts reveal two primary gaps: (1) a lack of full-stack integration at scale---existing systems often handle only a small number of UEs or rely on constrained simulations---and (2) limited real-time RL-based slicing, as most frameworks depend on offline training or partial control loops.

These efforts reveal two primary gaps including 
%\begin{enumerate}[left=0pt, noitemsep]
i) \textbf{Full-stack integration at scale:}  Existing systems often handle only a small number of UEs or operate within constrained simulation setups (\cite{marojevic2022actor}, \cite{Colosseum2023}, \cite{Anand2023xApp}), and ii)
 \textbf{Real-time RL-based slicing:} Many frameworks rely on offline training or partial control loops, deferring real-time decision-making and adaptability (\cite{tsampazi2024pandora}, \cite{tsampazi2023},\cite{coloran}). Unlike many frameworks that rely on offline training, which cannot capture real-time environmental feedback, our work emphasizes online training. In this approach, the RL agent interacts directly with the srsRAN simulator during training, receiving immediate rewards based on its resource allocation decisions. This ensures dynamic adaptation to network traffic and performance, overcoming the limitations of offline methods that fail to reflect real-world conditions. Our framework thus enables responsive and optimized physical resource block allocation in 5G O-RAN architectures.
 %\fa{I feel like we did not emphasize enough on this gap when describing the related work. Either here, or in the last paragraph of the contribution subsection when we talk about PPO with online training, we should descrbe the advantages of an online trained PPO on the ability to adjusts the policy dynamically based on current observations and changes in network conditions. We should also discuss the its high demands for computational resources compared to an offline case and mention this as a bottleneck for our work. could be discussed in the conclusion. }
 %\fa{pls add references for each gap}
%\end{enumerate}
To address the aforementioned gaps, we introduce a full-stack O-RAN solution that unifies the OSC near-RT RIC with srsRAN \cite{srsran} for real-time slicing:

\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{End-to-end Integration on OSC RIC and srsRAN:} 
    We develop a near-RT xApp that directly controls srsRAN’s gNB through the E2 interface, using E2AP messages to manage PRB allocations for up to 12 UEs.

    \item \textbf{Fully Online Closed-Loop Training:} 
    Our RL agent receives immediate feedback (downlink throughput, slice QoS) from srsRAN and updates its policy in real time. This ensures a genuine closed-loop approach, where each action on the RAN triggers immediate learning and fine-tuning in the xApp.

    \item \textbf{GNU Radio Channel Emulation:}
    We incorporate Free-Space Path Loss (FSPL), Additive White Gaussian Noise (AWGN), single-tap multipath fading, and Doppler shifts to approximate urban mobility.
    %within srsRAN’s ZeroMQ-based environment.

    \item \textbf{Practical Insights for Scalability:}
    We highlight operational constraints such as ZeroMQ saturation (limiting concurrency), partial uplink slicing, and attach-sequencing workarounds. These insights inform future O-RAN development for larger-scale deployments.
\end{itemize}

This paper presents an \emph{online}, RL-driven resource allocation strategy for managing multiple slices. 

The system model and E2-based control mechanisms are introduced in Section~\ref{sec:system}, followed by a detailed discussion of the RL methodology and its constraints in Section~\ref{sec:method}. Section~\ref{sec:scenarios} explores the simulation scenarios and channel conditions used to evaluate the framework. The performance of the system under varying mobility and traffic conditions is analyzed in Section~\ref{sec:results}. Finally, Section~\ref{sec:conclusion} summarizes the findings and outlines potential directions for future research in O-RAN.

\ifthenelse{\boolean{longversion}}{
% \section{O-RAN Architecture and E2 Interface}
% \label{sec:oran-arch}

% The Open Radio Access Network (O-RAN) initiative aims to redefine the traditional, monolithic RAN by disaggregating its hardware and software components, introducing open interfaces, and enabling AI-driven control loops. This section provides a deeper look into the O-RAN architectural framework, highlighting how the near-Real-Time RAN Intelligent Controller (near-RT RIC) communicates with the RAN via the E2 interface. We also discuss the broader 5G core ecosystem—particularly the role of the User Plane Function (UPF)—and emerging hardware platforms such as smart NICs and Data Processing Units (DPUs) that can further accelerate O-RAN deployments.

% \subsection{Disaggregated RAN Components}
% The O-RAN Alliance specifies a disaggregated architecture in which the Radio Unit (RU), Distributed Unit (DU), and Centralized Unit (CU) are split to allow multi-vendor interoperability. This approach fosters:
% \begin{itemize}
%     \item \textbf{Openness and Modularity:} Each component is accessible through open interfaces, avoiding vendor lock-in.
%     \item \textbf{Programmability:} Standardized APIs and service models enable the near-RT RIC to implement advanced control logic, including machine learning.
%     \item \textbf{Scalability:} Operators can scale individual RU, DU, or CU elements to handle dynamic traffic demands.
% \end{itemize}

% In our work, we use \emph{srsRAN} to emulate the CU/DU for a 5G Standalone (SA) deployment, while a simulated RU handles the physical layer. The near-RT RIC implements the intelligence—our \emph{xApp(s)}—that leverages real-time key performance indicators (KPIs) data and applies slice-specific actions over the E2 interface.

% \subsection{E2 Interface for Near-RT Control}
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{E2.png}
%     \caption{E2 Interface, enabling near real time communication, resource management, and optimization between the core network and gNodeB. \rb{Rendered with mermaid, content is thorough but need to make this larger.}}
%     \label{fig:e2}
% \end{figure}

% A central novelty of O-RAN is the \emph{E2 interface}, which connects the near-RT RIC to the underlying RAN nodes (e.g., gNBs in 5G) as shown in Fig.~\ref{fig:e2}. Through E2 Application Protocol (E2AP) messages and function-specific service models (E2SM), the near-RT RIC can:
% \begin{itemize}
%     \item \textbf{Collect KPIs and Telemetry:} E2 allows near-real-time reporting of throughput, latency, PRB usage, and other physical- or MAC-layer metrics.
%     \item \textbf{Issue Control Actions:} The RIC can dynamically modify scheduling parameters, allocate or revoke PRBs, or even command handovers.
%     \item \textbf{Maintain Policy and Slicing:} By sending slice-level or UE-level policies, the RIC orchestrates traffic flows across URLLC, eMBB, and mMTC slices.
% \end{itemize}

% In our platform, the \emph{O-RAN Software Community RIC (OSC RIC)} terminates E2 messages via its \emph{E2Term} component. A specialized \emph{E2 Service Node (E2SN)} inside srsRAN translates E2 instructions into local RAN actions. Although srsRAN’s E2SN remains experimental—particularly for advanced uplink slicing or detailed PHY metrics—this interface still provides the essential hooks for our Reinforcement Learning (RL) xApp to adapt resource allocation in near real-time.

% \subsection{UPF Integration and the 5G Core Ecosystem}
% While the near-RT RIC focuses on optimizing RAN parameters, the broader 5G core network also plays a critical role in end-to-end connectivity. In 5G Non-Standalone (NSA) or Standalone (SA) modes, the \emph{User Plane Function (UPF)} is pivotal for forwarding user data between the RAN and external networks (e.g., the internet or private clouds). This decoupling of the user plane (UPF) from control-plane entities (AMF, SMF) allows:

% \begin{itemize}
%     \item \textbf{Scalable User Traffic Handling:} The UPF can be independently scaled or replicated to manage high data rates.
%     \item \textbf{Local Breakout and Edge Computing:} Operators can place UPFs closer to RAN sites to reduce latency or integrate them with Multi-access Edge Computing (MEC) platforms.
%     \item \textbf{Policy Enforcement:} The SMF (Session Management Function) configures the UPF to enforce QoS or traffic steering decisions, complementing near-RT RIC orchestration in the RAN.
% \end{itemize}

% Although our testbed primarily highlights the RIC--RAN control loop via E2, the UPF’s behavior can significantly impact end-to-end QoS. For instance, high-volume eMBB flows may induce congestion if the UPF is not provisioned or routed efficiently.

% \subsection{Smart NICs and DPUs for O-RAN Acceleration}
% One promising approach to further enhance O-RAN performance is offloading select data-plane tasks to \emph{Smart NICs} or \emph{Data Processing Units (DPUs)}:
% \begin{itemize}
%     \item \textbf{Smart NICs:} Specialized network interfaces that can parse traffic, handle encryption, or perform Quality of Service (QoS) classification without burdening the CPU. By accelerating packet processing, Smart NICs can reduce the latency between the RAN and UPF.
%     \item \textbf{DPUs:} More advanced than Smart NICs, DPUs often include embedded CPUs or programmable pipelines designed for high-throughput packet processing (e.g., programmable match-action tables, advanced telemetry). A DPU can offload parts of the UPF or even host the entire UPF function inline, alleviating bottlenecks in the user plane.
% \end{itemize}

% Such hardware acceleration aligns well with O-RAN’s open interfaces: the RIC can focus on high-level control (via E2), while Smart NICs or DPUs handle micro-level optimizations in the data plane. In future expansions of our testbed, we plan to investigate synergy between an RL-based near-RT xApp and a DPU-backed UPF, aiming for ultra-low-latency flows in URLLC scenarios and high-throughput offloading for eMBB.

% \subsection{Key Takeaways for Our System Model}
% The next section (\S\ref{sec:system}) describes how we combine OSC RIC, srsRAN, and GNU Radio to demonstrate a near-real-time RL loop. The O-RAN architectural concepts introduced here—disaggregated RAN components, E2-based near-RT control, and integration with a 5G core—form the foundation of our solution. While certain advanced features (e.g., mature UL slicing, DPU-based UPF) remain future work in our testbed, they represent promising directions for fully realizing the potential of open, AI-driven 5G and beyond.
}{}
\section{System Model}
\label{sec:system}

\subsection{Architecture Overview}
In this work, we integrate the O-RAN Software Community RIC (OSC RIC) with srsRAN’s 5G stack to form a complete end-to-end testbed capable of near-real-time radio resource control. Figure~\ref{fig:arch} illustrates the overall architecture, highlighting the following key components:

\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{Open5GS Core:} Provides both \emph{4G (EPC)} and \emph{5G (5GC)} in a single software suite, enabling seamless UE attachment and data exchange with the RAN.
    \item \textbf{Near-RT RIC:} Hosts \emph{xApps} responsible for coordinating slicing decisions and sending resource-allocation commands via the E2 interface.
    \item \textbf{srsRAN gNB:} Emulates the \emph{Centralized Unit (CU)} and \emph{Distributed Unit (DU)} functionality; an \emph{E2 Service Node (E2SN)} bridges the OSC RIC’s E2Term and the gNB’s internal APIs.
    \item \textbf{E2 Interface:} Enables real-time orchestration of downlink PRB quotas, which can be assigned to multiple slices (URLLC, eMBB, mMTC).
    \item \textbf{GNU Radio Blocks (Channel Model):} Inserted between \emph{srsUE} and \emph{srsGNB} via ZeroMQ streams to emulate path loss, fading, noise, and Doppler effects.
\end{itemize}

In our testbed, \textbf{Open5GS Core} provides the following core network components as Dockerized services using a shared configuration:
\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{EPC (Evolved Packet Core):} Allows 4G operation, with the \emph{MME (Mobility Management Entity)} handling control-plane signaling and the \emph{SGW (Serving Gateway)} routing user-plane data.
    \item \textbf{EPC 5GC (5G Core):} Enables 5G standalone functionality, where the \emph{AMF (Access and Mobility Management Function)} and \emph{SMF (Session Management Function)} manage UE registration and session establishment, and the \emph{UPF (User Plane Function)} forwards user traffic to external networks.
\end{itemize}

A dedicated subscriber database 
%in \emph{.CSV} format 
contains each UE’s \emph{IMSI}, \emph{encryption keys}, \emph{operator code}, \emph{QoS parameters}, and IP assignment details—referenced by the MME or AMF to authenticate users on network attach. Although our overall testbed supports both \emph{4G} and \emph{5G}, it is specifically deployed in \textbf{5G SA mode} to align with the \emph{3GPP 7.2 architecture} for O-RAN. In a 4G EPC configuration, the MME collaborates with the SGW and PGW (or UPF in a hybrid mode); meanwhile, for 5G SA, the AMF and SMF manage slicing parameters and session establishment, while the UPF routes user-plane traffic externally. \textbf{Network slicing} is enabled by assigning specific \emph{Slice/Service Types (SST)} and \emph{Slice Differentiators (SD)} in the AMF configuration.
%, ensuring that any UE registering with the correct SD and SST is placed into the desired slice (e.g., URLLC, eMBB, or mMTC). Any mismatch results in registration failure, reinforcing proper slice isolation.

\begin{figure}[!t]
    \centering
\includegraphics[width=1\columnwidth, ]{ORAN_Actor_Critic.jpg}
%\vspace{-10pt}
    \caption{%\fa{this figure does not serve the purpose of the proposed work. We want something to show the online training aspect, the online training aspect. We can either have another fig to show the learning steps/curve on integrate it with this fig. We could also add a pseudo code if space allows. } 
    Overall system architecture. The near-RT RIC controls the srsRAN gNB via the E2 interface, while GNU Radio injects channel impairments. %\fa{we need to define the terms UPF, others in the text. and discuss how implemented/integrated in the model. }
    }
    \label{fig:arch}
\end{figure}

%\begin{figure}[!t]
%    \centering
%\includegraphics[width=1\columnwidth, ]{actor_critic.jpg}
%\vspace{-10pt}
%    \caption{Reinforcement Learning Agent architecture.}
%    \label{fig:arch}
%\end{figure}


%\rb{Revise architecture to include gnuradio, use higher quality, easy to read figures (recommend draw.io).}

\subsection{Channel Assumptions and Modeling}
To ensure realistic network conditions, we incorporate a simplified channel model reflecting an \emph{urban mobility scenario}, accounting for various physical effects:

\begin{enumerate}[left=0pt, noitemsep]
    \item \textbf{Free-Space Path Loss (FSPL):} Signal attenuation is modeled based on the standard free-space path loss formula, which depends on the transmitter--receiver distance, the carrier frequency, and the speed of light. This captures the general propagation loss over distance, typical in open environments.
    
    \item \textbf{Single-Tap Fading:} We introduce a primary path with a complex gain of \([0.85 + 0.25j]\), representing moderate multipath conditions typical of low-rise urban environments. This fading factor is dimensionless, emphasizing the influence of reflections and scattering on signal integrity.
    
    \item \textbf{Additive White Gaussian Noise (AWGN):} Background noise is characterized by a power spectral density proportional to the system temperature, bandwidth, and Boltzmann constant. In practice, the noise voltage is capped below 0.01 in GNU Radio to preserve the stability of the srsRAN implementation.
    
    \item \textbf{Doppler Effects:} For mobile user equipment (UE), such as those supporting Ultra-Reliable Low-Latency Communications (URLLC) at moderate speeds (e.g., 40 km/h), the Doppler shift is computed based on the UE's velocity relative to the carrier frequency. This introduces time-varying phase offsets, presenting challenges for synchronization in dynamic urban scenarios.
\end{enumerate}
% \fa{we don't need to define the AWGN or FSPL formula. At least, have them in line with text not an equation  }

The channel modeling is implemented using GNU Radio, which communicates with srsUE and srsGNB via ZeroMQ streams for seamless data exchange. 
%This approach allows the RL agent’s actions—and srsRAN’s internal schedulers—to adapt in near real-time to the evolving channel state, simulating practical mobility and noise conditions without requiring external hardware. 
In Section~\ref{sec:method}, we elaborate on how these channel characteristics feed into our RL algorithm’s observation space.

%\begin{figure}[!t]
%    \centering
    %\includegraphics[width=0.9\columnwidth]{gnuradio.png}
    %\caption{Sample gnuradio flowgraph, illustrating multi-UE channel modeling and modulation.\ts{The reviewers will not understand this Figure since it is not readable as well lacks in explanation. We can add a conceptual diagram that visualizes the purpose of GNUradio.} \rb{Great idea, let's do it.}}
    %\label{fig:gnuradio}
%\end{figure}

\section{Methodology}
\label{sec:method}

\subsection{Reinforcement Learning Framework for RAN Slicing Optimization}
%We employ Proximal Policy Optimization (PPO) within an actor-critic framework to address the sequential decision-making challenges of RAN slicing. The actor network outputs action vectors \(\mathbf{a}_t\), representing PRB allocations for each slice–UE pair at timestep \(t\), balancing resource provisioning and over-allocation. The critic network estimates the value function \(V^\pi(\mathbf{s}_t)\) %\fa{define the notations for state and policy}, guiding policy updates by evaluating the impact of actions on slice QoS.

We employ Proximal Policy Optimization (PPO) which is an actor-critic framework to address the decision-making challenges of RAN slicing. Here, \(s_t\) represents the state at timestep \(t\), which encapsulates the current UE requirements and status. The actor network outputs action vectors \(\mathbf{a}_t\), representing PRB allocations for each slice at timestep \(t\), balancing resource provisioning and over-allocation. The critic network estimates the value function \(V^\pi(\mathbf{s}_t)\), where \(\pi\) denotes the policy, a mapping from states \(s_t\) to actions \(a_t\), which the agent learns to optimize the long-term reward.

\subsubsection{State and Action Spaces}
The state vector comprises
of UE type, Bit-rate or Buffer size (occupancy) and pathloss, relying primarily on E2 metrics provided by srsRAN. The action space defines PRB allocations at the slice level (eMBB, URLLC, mMTC), rather than per UE. PRBs are first distributed evenly among UEs within each slice, with any remaining PRBs allocated to UEs experiencing higher path loss. The total PRB usage remains constrained by the subframe limit, ensuring compliance with srsRAN’s channel bandwidth.


\subsubsection{Slice-Specific Reward Functions}
\label{subsec:rewards}

We design a reward function $R_t$ that combines the individual rewards $r_{\text{URLLC}, t}$, $r_{\text{eMBB}, t}$, and $r_{\text{mMTC}, t}$ at time $t$. Each slice $s \in \{\text{URLLC, eMBB, mMTC}\}$ has a distinct QoS priority and thus a specialized sub-reward. The final reward is a weighted sum:
\begin{align}
R_t \;=\; \alpha_{\text{URLLC}} \,r_{\text{URLLC},t}
\;+\;\alpha_{\text{eMBB}}\,r_{\text{eMBB},t}
\;+\;\alpha_{\text{mMTC}}\,r_{\text{mMTC},t},
\end{align}
where $\alpha_{\text{URLLC}}, \alpha_{\text{eMBB}}, \alpha_{\text{mMTC}}$ are slice importance weights. Below, we define each sub-reward:

\paragraph{URLLC Reward ($r_{\text{URLLC}}$).}
URLLC focuses on ultra-low latency and reliability. We approximate latency by monitoring buffer occupancy or per-packet delay. We define a negative penalty if the slice’s average delay exceeds $t_{\text{target}}$:
\begin{align}
r_{\text{URLLC}, t} \;=\;
\max\!\Bigl(-1,\,\min\!\Bigl(0,\,
\dfrac{t_{\text{target}} - t_{\text{avg}}}{t_{\text{target}}}\Bigr)\Bigr),
\end{align}
where $t_{\text{avg}}$ is the measured or estimated delay (e.g., from buffer timestamps), and $t_{\text{target}}$ is the URLLC delay threshold. Lower delay yields higher (less negative) reward.

\paragraph{eMBB Reward ($r_{\text{eMBB}}$).}
eMBB aims for high data throughput. We track the slice’s average bitrate $b_{\text{avg}}$ over the last measurement window and compare it against a target $b_{\text{target}}$:
\begin{align}
r_{\text{eMBB}, t} \;=\;
\max\!\Bigl(-1,\,\min\!\Bigl(0,\,
\dfrac{b_{\text{avg}} - b_{\text{target}}}{b_{\text{target}}}\Bigr)\Bigr).
\end{align}
If $b_{\text{avg}}$ meets $b_{\text{target}}$, the sub-reward approaches 0. Otherwise, it becomes increasingly negative, with a floor at $-1$.

\paragraph{mMTC Reward ($r_{\text{mMTC}}$).}
For mMTC, we focus on the number of successfully received packets—especially if the devices are mostly downlink or rely on sporadic transmissions. Let $b_{\text{received}}$ be the total bytes (or packets) received and $b_{\text{expected}}$ the desired or generated amount over the window:
\begin{align}
r_{\text{mMTC}, t} \;=\;
\max\!\Bigl(-1,\,\min\!\Bigl(0,\,
\dfrac{b_{\text{received}} - b_{\text{expected}}}{b_{\text{expected}}}\Bigr)\Bigr).
\end{align}
When $b_{\text{received}} \approx b_{\text{expected}}$, the slice gets a near-zero penalty. If $b_{\text{received}}$ is too low, it is penalized more harshly.

\paragraph{Clipping and Weights.}
We constrain each slice sub-reward to $[-1,\,0]$ to avoid exploding gradients and to unify the magnitude of slice-specific penalties. The weighting factors $\alpha_{\text{URLLC}}, \alpha_{\text{eMBB}}, \alpha_{\text{mMTC}}$ allow fine-tuning of slice priority and configurable fairness. For example, if URLLC must absolutely not be violated, one might set $\alpha_{\text{URLLC}} \gg \alpha_{\text{eMBB}}$.

This formulation ensures each slice’s QoS goals (latency, throughput, or packet reception) are explicitly represented within a single scalar reward $R_t$. During training, the RL agent learns how to adjust PRB allocations across slices to maximize $R_t$ over time, striking a balance among slice priorities and the channel constraints.

\subsubsection{Training Procedure}
The training procedure is as follows: the RL agent sets resource allocation while the xApp applies PRB assignments. During traffic simulation, KPI measurements are gathered every 500 ms. After simulation, the xApp averages these KPIs and saves them in a JSON file. The RL agent then computes the reward using the KPIs and stores the state, action, and reward in replay memory to update its policy. Table \ref{tab:task_spec} lists the parameters for each slice.

%The training procedure follows these steps: First, the RL agent determines the resource allocation, and the xApp applies the PRB assignments. Traffic simulation then begins, during which the xApp collects KPI measurements every 500 ms. Once the simulation concludes, the xApp computes the average KPIs and stores them in a JSON file. The RL agent then processes these KPIs to compute the reward, which, along with the corresponding state and action, is stored in the replay memory. This stored experience is subsequently used to update the RL agent, ensuring adaptive learning based on network dynamics. 

%Table \ref{tab:task_spec} shows the parameters used for each slice in our training.

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{c|c|c}
        \hline
        \textbf{Service} & \textbf{Parameter} & \textbf{Values} \\
        \hline
        \multirow{4}{*}{URLLC} & Gen. Freq. (Hz) & 2 \\
                               & Gen. Bytes (B) & Min: $10^5$, Max: $3\times10^5$ \\
                               & Latency (ms) & 500 \\
                               & $\alpha_{\text{URLLC}}$ & 1 \\
        \hline
        \multirow{2}{*}{eMBB} & Bit Rate & Min: $2\times10^5$, Max: $4\times10^5$ \\
                              & $\alpha_{\text{eMBB}}$ & 1 \\
        \hline
        \multirow{3}{*}{mMTC} & Gen. Freq. (Hz) & 4 \\
                              & Gen. Bytes (B) & Min: $25\times10^3$, Max: $60\times10^3$ \\
                              & $\alpha_{\text{mMTC}}$ & 1 \\
        \hline
    \end{tabular}
    \caption{Task Specification for PRB Allocation}
    \label{tab:task_spec}
\end{table}


\subsection{Resource Management Constraints: Scalability Limitations and Open-Source Commitment}
%\bd{32-core, 64-threads} \rb{24/32 is reflective of Ryan's machine, we should use Basir's specs here as that is where we trained and evaluated the final model. Does he have a different CPU model?} \rb{We confirmed that both machines are the same CPU, which is 24/32. Intel's architecture here: https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html}

While our RL approach is designed to be generic and adaptable to diverse network conditions, 
the limited computational resources on one PC would propose a limitation.
%the current srsRAN-based implementation exhibits inherent limitations that affect both scalability and performance. 
We run our environment via virtualization on a 24-core, 32-thread Intel(R) Core(TM) i9-14900K CPU. In this setup, ZeroMQ, being CPU-bounded and operating as a lightweight TCP/IP-based point-to-point transport layer, manages inter-process communication between srsGNB and each simulated srsUE. Although GNU Radio connects these individual message streams, the use of ZeroMQ in this design would propose a communicational limit. While it supports N-to-N socket instances, increasing the number of simultaneous endpoints 
%on a single message stream 
causes network strain, buffer overflow, and eventual saturation.
Extensive testing shows that an unsaturated channel in this environment supports a total of 28 Mbps across all UEs. However, when more than three UEs transmit simultaneously, the channel saturates, leading to substantial performance degradation and system instability. 
%This architecture cannot substitute for proper lower-PHY or O-RU protocols designed for dense network scenarios. 
To mitigate this constraint and enable simulations with up to 12 UEs, we employ a batching strategy that groups UEs in sets of three, rotating these groups over time intervals as described in Section~\ref{subsec:scenarios-limits}. It is worth noting that more CPU cores would likely improve performance.

In contrast, 
%commercial versions of srsRAN and 
proprietary solutions like \textit{Amarisoft}\cite{amarisoft} exhibit superior scalability, supporting significantly more UEs without similar bottlenecks, but their closed-source nature restricts reproducible research efforts. Our work underscores the importance of open-source frameworks to foster collaborative O-RAN development. Additionally, as of now, the current stable version of srsRAN supports only down-link slicing
%, constraining exploration of URLLC scenarios requiring robust uplink performance. 
We address these challenges by leveraging the O-RAN E2 interface in a closed-loop framework with our resource allocation xApp, enabling dynamic PRB allocation and near real-time KPI monitoring directly over E2. Although throughput measurements are sampled every 500 ms,
%—reducing the granularity of performance insights—
this setup provides timely feedback for the RL agent to update its policy, adapt resource allocations, and evaluate the impact of state changes. 
%Moreover, srsRAN offers limited KPI visibility. Critical metrics such as SINR and \textit{Hybrid Automatic Repeat Request} (HARQ) statistics are either absent or inaccessible. Consequently, 
the RL agent relies on higher-level indicators like throughput and slice occupancy, restricting its ability to optimize network performance effectively. 
From the KPI perspective, the RL agent relies on throughput readings.
Despite the constraints, our methodology demonstrates the feasibility of real-time RL-based slicing with srsRAN. 
%while highlighting areas for improvement, including enhanced channel data, finer scheduling granularity, and robust uplink support.

\section{Simulation Scenarios}
\label{sec:scenarios}

\subsection{ Traffic and Deployment Configuration}

We design our simulation scenarios around three primary slices—{URLLC}, {eMBB}, and {mMTC}—each reflecting distinct downlink-centric 5G services with corresponding QoS demands. The {URLLC} slice (\(4\) UEs) is modeled as autonomous driving systems requiring ultra-reliable, low-latency transmissions at \(40\,\mathrm{km/h}\), emulated by large, non-frequent downlink packets. The {eMBB} slice (\(4\) UEs) comprises two mobile UEs representing smartphone users streaming high-bandwidth content on the move, and two stationary UEs simulating fixed wireless access subscribers consuming large data flows (e.g., 4K video). Finally, the {mMTC} slice (\(4\) UEs) mimics low-throughput IoT endpoints such as digital billboards, where small, high frequency downlink updates reflect srsRAN’s limited uplink support.

In total, the deployment accommodates up to \(12\) UEs, though only three transmit actively at any given time to avoid ZeroMQ saturation. 
These active UEs rotate in groups, maintaining stable throughput while capturing dynamic traffic patterns. 
A realistic channel model—including single-tap path loss, AWGN, Doppler effects, and random distances (\(d \in [0.5\text{\,km}, 2\text{\,km}]\))—emulates a mobile urban environment. This setup provides an efficient testbed for evaluating RL-based resource allocation strategies under diverse, downlink-centric QoS requirements.

%Our simulation scenarios encompass three primary slices, each representing distinct 5G services and QoS requirements. \textbf{URLLC} focuses on ultra-reliable, low-latency applications with UEs moving at \(40\,\mathrm{km/h}\), generating small, frequent packets to emulate mission-critical traffic. \textbf{eMBB} targets high-throughput use cases like video streaming or file transfers, with \(50\%\) of UEs mobile at \(40\,\mathrm{km/h}\) and \(50\%\) stationary, capturing both fixed and mobile user patterns. \textbf{mMTC} represents IoT devices with low-throughput, sporadic downlink traffic, emphasizing passive data consumption due to uplink limitations in srsRAN.
%The deployment involves up to 12 UEs distributed across the three slices: 4 URLLC, 4 eMBB (split evenly between mobile and stationary), and 4 mMTC. Active transmissions are restricted to three UEs at a time to prevent ZMQ overload, with groups rotating every 10–30 seconds to ensure stable throughput. All UEs experience a realistic channel model incorporating single-tap path loss, AWGN, Doppler effects, and randomized distances (\(d \in [0.5\text{\,km}, 2\text{\,km}]\)), simulating a mobile urban environment. This efficient configuration supports the evaluation of RL strategies under dynamic traffic and channel conditions.

\subsection{Overcoming Scalability Constraints in Open Source Deployments}
\label{subsec:scenarios-limits}

 \ifthenelse{\boolean{longversion}}{
% Although \texttt{srsRAN} provides a robust open-source framework for 5G prototyping—especially for short-range, SDR-based experiments—our fully software-driven setup (using \texttt{srsGNB}, \texttt{srsUE}, and \texttt{ZeroMQ}, with no physical SDR) pushes these open-source components well beyond their typical lab-scale usage. This approach aligns with the \texttt{srsProject}’s official O-RAN tutorials, yet it deviates from the hardware-focused design that underpins much of \texttt{srsRAN}’s architecture. Consequently, we expose several limitations in \texttt{srsUE}’s \emph{PHY layer}, the \texttt{srsGNB} E2 interface, and the ZeroMQ transport, which together make a purely software-based digital twin challenging for realistic O-RAN research.

% Commercial variants of \texttt{srsRAN}—and third-party enterprise RU emulators—address some of these scalability problems, but they do not provide the open-source, fully simulated PHY layer needed for large-scale experimentation. Likewise, even \texttt{srsRAN}’s commercial offerings do not include a comprehensive PHY-level digital twin, which is why many researchers turn to \texttt{Amarisoft}, \texttt{Open Air Interface}, or other third party proprietary solutions. Our objective, however, is to keep this work as open and accessible as possible for the broader research community. Below, we document the key issues that arise when \texttt{srsUE} is used as a purely software-based UE emulator, so that others attempting a similar architecture understand the challenges and potential next steps.
% 
}
Since \texttt{srsRAN}, originally designed for hardware-SDR use, one may face few issues while scaling up the number of simulated UEs in a purely software-based digital twin.
%can face scalability and realism constraints when deployed in a purely software-based digital twin—particularly without an O-RU or enterprise-grade RU emulator. While commercial variants and third-party emulators mitigate some issues, none provide a fully open-source simulated PHY. 
We aim to keep our solution accessible for the research community and thus outline the following key challenges in replicating our efforts:

\begin{enumerate}[label=\textbf{(\arabic*)}, leftmargin=1.5em]
  \item \textbf{PRACH Attach Delays} \\
  Simultaneous attachments overwhelm \texttt{ZeroMQ} buffers, causing crashes. To mitigate this, each \texttt{srsUE} adds a random PRACH offset, staggering attach attempts.

  \item \textbf{Downlink-Only Slicing} \\
  The current stable version of \texttt{srsUE} as of writing this paper only supports downlink slicing. While adequate for downlink use cases (e.g., video streaming), this excludes the stringent uplink demands of URLLC and mMTC.

  \item \textbf{Batching Transmitting UEs} \\
  Activating more than three \texttt{srsUE} instances concurrently saturates ZeroMQ and results in a channel throughput degradation. 
  To address this, only three UEs transmit simultaneously.
\end{enumerate}

  % \ifthenelse{\boolean{longversion}}{%
  %   \item \textbf{Limited Bandwidth \& Duplexing} \\
  %   The \texttt{srsUE} code supports only certain FDD bandwidths (5/10/15/20\,MHz), excluding mid-band TDD deployments (e.g., 3.5\,GHz) that are dominant in modern 5G. Its fixed 15 kHz subcarrier spacing (SCS) further constrains throughput and per-UE PRB allocation. Wider channels or advanced numerologies (e.g., 40--100\,MHz, 30--60kHz SCS) remain out of scope without a more robust UE (e.g., \texttt{AmarisoftUE}) or hardware SDR solutions, limiting software-only scenarios to narrowband, short-range tests. 

  %   \item \textbf{Path Loss Constraints.} \\
  %   During our RL training experiments, \texttt{srsUE} became unstable when path loss values in \texttt{GNU~Radio} exceeded 10--20\,dB, with most UEs failing to attach or disconnecting quickly. Empirically, UEs operated most reliably around 10\,dB---corresponding to distances of just a few centimeters (e.g., $\approx4$\,cm at 10\,dB in free space at 1.805\,GHz). At 20\,dB ($\approx13$\,cm), \texttt{srsUE} connected erratically and soon dropped, while at or above 50\,dB ($\approx4.2$\,m), all devices failed to attach. These results highlight how the default PHY-layer logic in the \texttt{srsUE}–\texttt{srsGNB} ZeroMQ pipeline expects extremely short-range conditions. Larger path-loss values, even on the order of a few meters, require extensive customization or external amplification to maintain stable connectivity.

  %   \item \textbf{Power Scaling Issue} \\
  %   In further testing, we probed the I/Q sample power at the gNB Tx and UE Rx ports. Despite raising nominal transmit or receive gains in \texttt{srsGNB}/\texttt{srsUE} to extreme levels (e.g., 105\,dB, 1000\,dB), measured magnitudes remained around 0.0 and 0.0093 in linear scale ($\approx -20.3$\,dB). Periodic spikes near 127\,dB appear linked to buffer saturations rather than real power changes. Consequently, adjusting these software-configured gains has no meaningful effect on the amplitude of transmitted waveforms, undermining attempts to emulate realistic link budgets or path-loss margins in a software-only environment. Practical workarounds include scaling samples manually (e.g., via \texttt{Multiply Const} blocks in \texttt{GNU~Radio}) or leveraging hardware SDR pipelines where transmit/receive gains do shape the I/Q signal.

  %   \item \textbf{Fragile Multi-UE Scalability} \\
  %   While multiple \texttt{srsUE} instances can attach to one \texttt{srsGNB}, purely software-driven concurrency remains unstable with more than three concurrent flows. \texttt{ZeroMQ} can saturate and drop frames, leading to gNB crashes. Our batching approach avoids immediate crashes but diminishes realism for large-scale O-RAN tests.

  %   \item \textbf{Rudimentary E2 Interface \& ASN1C Gaps} \\
  %   The E2 Service Node (E2SN) in \texttt{srsGNB} implements only a partial subset of E2SM procedures. Its \texttt{ASN1C} code lacks support for power control, HARQ reporting, or channel estimation. Consequently, our RL agents must rely on coarse KPI metrics, limiting the granularity of resource-management strategies in a digital twin environment.

  %   \item \textbf{Inconsistent UE Identification \& Helper Scripts} \\
  %   By default, \texttt{srsUE} assigns local indices in the order of successful PRACH attach, independent from the core network ID. If multiple UEs attach nearly simultaneously, collisions reorder or drop them. We mitigate this via a custom script that sends brief traffic bursts in a known sequence, matching E2SM IDs to the core IDs. Although this unifies labeling across logs, it remains a manual workaround to keep software-only multi-UE identification consistent.
    
  %   \item \textbf{Sensitivity to Complex Channels \& ZeroMQ Overheads} \\
  %   Although \texttt{srsUE} and \texttt{srsGNB} handle modest AWGN or single-tap fading, multi-tap or high-Doppler channels often desynchronize the PHY. Even minor timing drift ($\epsilon \neq 1.0$) at higher sample rates (e.g., 11.52\,MSPS) can break connectivity altogether. Moreover, \texttt{ZeroMQ} inflates CPU load and buffering delays, undermining near-real-time operation under heavier or more complex traffic. To remain within practical limits:    
  %   \begin{itemize}
  %     \item \textbf{Taps}: [0.85+0.25j, 0.4-0.1j, 0.2+0.1j] (reduced to mitigate QoS degradation).
  %     \item \textbf{Noise Voltage}: 0.01
  %     \item \textbf{Frequency Offset}: 0.00005 (96\,m/s, constant velocity), often lowered to 0.00000584 ($\approx 40$\,km/h).
  %     \item \textbf{Epsilon}: 1.0
  %   \end{itemize}
    
  %   \item \textbf{Alternative Transport Protocols for Higher Bitrates} \\
  %   While \texttt{ZeroMQ} remains a convenient default in \texttt{srsUE}–\texttt{srsGNB} loops, it often saturates under multi-UE loads, causing latency spikes or outright crashes. For advanced scenarios demanding higher throughput and real-time concurrency, alternative protocols (e.g., \texttt{gRPC}, shared-memory pipelines, or other low-overhead messaging) should be considered. These could better sustain simultaneous flows and large data bursts, mitigating ZeroMQ bottlenecks.
    
  %   \item \textbf{Lack of Robust Uplink Slicing} \\
  %   Although \texttt{srsGNB} contains partial UL PRB-assignment code, it is incomplete and often introduces multi-second delays or system failures when paired with \texttt{srsUE}. URLLC or IoT traffic thus cannot be orchestrated in real time under the current implementation, undermining the viability of truly dynamic uplink slicing in software-only emulation.

  %   \end{enumerate}
  %   \noindent
  %   \textbf{Summary.} In essence, the \texttt{srsGNB} and \texttt{srsUE} components can provide a promising basis for lightweight 5G experimentation—particularly when \emph{hardware SDRs} or external UEs are used for short-range tests. However, because our project emphasizes a purely software-based workflow (i.e., the \texttt{srsUE}–\texttt{ZeroMQ}–\texttt{srsGNB} pipeline) without SDR acceleration, many of the internal constraints (e.g., narrow bandwidth, minimal path-loss tolerance, limited concurrency) become far more pronounced. As these limitations illustrate, the current \texttt{srsUE}/\texttt{srsGNB} loop is not yet adequate for fully realistic, large-scale, and time-critical O-RAN scenarios—especially those requiring robust UL slicing, multi-UE concurrency, or advanced slice coordination. Nevertheless, the openness and modifiability of the \texttt{srsRAN} environment remain valuable for prototyping AI-driven RAN features in controlled settings. Researchers pursuing mid- to large-scale 5G/6G testbeds with complex topologies and stronger uplink demands will likely need complementary solutions (e.g., \texttt{OpenAirInterface}, hardware-based testbeds) or significant custom extensions to achieve higher fidelity and near-real-time performance.
  % }

\subsection{Evaluation Metrics and Logging}
\label{subsec:scenarios-eval}
% \bd{Do we need to say we have pretraining phase?} \rb{Let's add the pretraining phase justification here}
Performance is evaluated using per-UE E2 throughput at a \(500\,\mathrm{ms}\) sampling rate.
%, URLLC latency via packet timestamps or buffer logs, and PPO loss values to track RL convergence. 
Training the agent consists of two phase.
The first phase is the pre-training phase which utilizes the channel throughput we accomplished during our experiments with different number if PRBs in the simulator, rather than simulating the traffic. We use this pre-training phase so that the RL agent has a good starting weights for online learning inside the simulator.
%For training, a pre-training phase leverages a slice experiment-based regression to predict expected throughput for each PRB assignment under ideal channel conditions without simulating the traffic using the . 
This model is then integrated into the simulator for the second training phase, enabling the agent to learn performance implications under realistic constraints such as FSPL, AWGN, fading, and frequency drift. 
%\bd{How about adding this to the conclusion.}Future work could explore multi-tap fading, robust uplink slicing, and scaling beyond 12 UEs for advanced RL applications in O-RAN.

Algorithm \ref{PPO_PRB_Allocation} outlines the procedural steps for PPO-based PRB allocation. The training process begins with the initialization of the PPO agent, where policy and value function parameters are defined. During each training iteration, new tasks are generated for all UEs, and the corresponding state vector is constructed and provided to the PPO agent. Based on the observed state, the PPO agent selects an action that determines the PRB allocation for each slice. The xApp subsequently applies the allocation through RAN Controller (RC) messages and UEs start generating traffic. The KPIs are continuously monitored and logged to evaluate the system's performance. The observed KPIs are used to compute a reward, which is stored along with the state-action pair in the experience buffer. When an update step is reached, the algorithm computes the advantage estimate using Generalized Advantage Estimation (GAE), optimizes the policy via clipped surrogate loss, and updates the value function through regression on discounted returns. The training process continues iteratively until the convergence criteria are met.

\begin{algorithm}
\caption{PPO-based PRB Allocation}
\label{PPO_PRB_Allocation}
\begin{algorithmic}[1]  % Adds line numbers for better readability

    \State \textbf{Initialize:} PPO agent with policy parameters $\theta$ and value function parameters $\phi$

    \While{Training is not complete}
        \State Generate new tasks for all UEs
        \State Construct the state vector $\mathbf{s}_t$
        \State Provide state $\mathbf{s}_t$ to the PPO agent
        \State PPO agent selects action $a_t \sim \pi_{\theta}(a_t | \mathbf{s}_t)$
        \State Allocate PRBs to UEs based on action $a_t$
        \State xApp applies PRB allocation via RC messages
        \State UEs generate traffic
        \State xApp monitors KPIs and logs data in a JSON file
        \State Compute reward $r_t$ based on observed KPIs
        \State Store transition $(\mathbf{s}_t, a_t, r_t)$ in experience buffer
        
        \If {Update step is reached}
            \State Compute advantage estimate $\hat{A}_t$ using Generalized Advantage Estimation (GAE)
            \State Optimize policy $\pi_{\theta}$ using the clipped surrogate loss function
            \State Update value function $V_{\phi}$ 
            \State Perform gradient ascent on $\theta$ and $\phi$
        \EndIf
    \EndWhile

\end{algorithmic}
\end{algorithm}

One of the key advantages of using online training for PPO in this context is its ability to dynamically adapt to changing network conditions. Unlike offline training, which relies on precollected data, online training enables the agent to continuously refine its policy based on the interaction it has with the environment. This adaptability is crucial in O-RAN environments, where traffic patterns and resource demands fluctuate dynamically. However, this approach comes with a significant computational cost. %Continuous training and inference steps require substantial processing power, making real-time execution challenging, especially on resource-constrained edge devices. This high computational demand represents a bottleneck in our work.


\section{Quality of Service Analysis}
\label{sec:results}

In this section, we evaluate the performance of our RL-based slicing framework under the simulation scenarios described in Section~\ref{sec:scenarios}. Our analysis focuses on the overall QoS compliance and the stability of the proposed framework.

We compare the performance of our PPO-based resource allocation approach, detailed in Section~\ref{sec:method}, against DQN and four baseline methods. To assess resource allocation efficiency, we utilize the cumulative distribution function (CDF) to analyze key performance metrics. Specifically, we examine the latency distribution of URLLC traffic to determine compliance with stringent delay requirements. Additionally, for eMBB services, we evaluate the difference between the achieved downlink bitrate and the requested bitrate. For mMTC, we assess the discrepancy between the number of received bytes and the actual transmitted bytes.

Furthermore, we analyze the impact of network congestion and resource contention on service quality, highlighting the ability of each strategy to meet dynamic service demands. Finally, we evaluate overall QoS compliance by measuring adherence to throughput and latency targets across network slices. This includes examining the trade-offs between latency-sensitive URLLC traffic and the bandwidth-intensive requirements of eMBB and mMTC services, providing insights into the effectiveness of resource allocation under varying network conditions.
%In other words it shows the URLLC buffer latency, bitrate distribution for eMBB and transmitted bytes for the mMTC users.
%on the bitrate distribution for mMTC and eMBB slices, the latency characteristics of URLLC traffic, and the overall QoS compliance and stability of the framework. 
%the cumulative distribution function (CDF) of achieved downlink bitrates for mMTC and eMBB slices \bd{minus the requested bitrate only for the eMBB. For the mMTC we measure the number of transmitted bytes}, highlighting how effectively each strategy meets service demands under dynamic conditions. Similarly, the latency distribution of URLLC traffic is analyzed to assess compliance with stringent delay requirements. This includes examining the effects of network congestion and resource contention. Finally, we assess overall QoS compliance by monitoring adherence to throughput and latency targets across slices. This includes evaluating trade-offs between latency-sensitive URLLC traffic and bandwidth-intensive eMBB and mMTC flows, providing insights into how well resources are balanced under varying conditions.

For benchmarking, we compare the RL-based methods with four baseline resource allocation strategies:

\begin{itemize}[left=0pt, noitemsep]
    \item \textbf{Equal Allocation Baseline:} This method distributes the total number of PRBs equally among all active users using integer division. This approach provides a straightforward and fair division of resources but does not adapt dynamically to traffic demands or channel conditions.
    
    \item \textbf{Proportional Allocation Baseline:} This method assigns PRBs in proportion to user demand, which is determined differently based on the traffic type. For URLLC and mMTC users, demand is computed as the product of generation frequency and packet size, assuming a fixed generation period. PRBs are allocated according to each user's share of the total demand, with any leftover PRBs assigned to users with the largest fractional remainder. This approach ensures a more adaptive distribution of resources based on instantaneous demand.
    
    \item \textbf{Pre-allocated Proportional Baseline:} This method ensures that each user receives at least one PRB before distributing the remaining resources based on proportional demand. This method prevents extreme resource starvation by guaranteeing a minimum allocation while still adapting to demand variations.
    
    \item \textbf{3GPP-Based Proportional Fair Frequency-Domain Packet Scheduling (3GPP-PF) \cite{5062197}:} This method follows the proportional fair scheduling principle standardized in 3GPP, balancing user fairness and spectral efficiency by prioritizing users based on their channel conditions and past resource allocations. The proportional fair scheduler dynamically adjusts allocations to mitigate resource starvation and enhance overall spectral efficiency.
\end{itemize}

%\begin{itemize}[left=0pt, noitemsep]
%    \item \textbf{Equal Allocation Baseline:} This method distributes the total number of PRBs equally among all active users. The allocation follows a basic integer division, and any remaining PRBs are assigned based on a predefined priority scheme, prioritizing URLLC users first, followed by eMBB and then mMTC. This approach provides a straightforward and fair division of resources but does not adapt dynamically to traffic demands or channel conditions.

%    \item \textbf{Proportional Allocation Baseline:} This method assigns PRBs in proportion to user demand, which is determined differently based on the traffic type. For URLLC and mMTC users, demand is computed as the product of generation frequency and packet size, assuming a fixed generation period. For eMBB users, demand is determined based on the required bitrate. PRBs are allocated according to each user's share of the total demand, with any leftover PRBs assigned to users with the largest fractional remainder. This approach ensures a more adaptive distribution of resources based on instantaneous demand.

%    \item \textbf{Pre-allocated Proportional Baseline:} This method ensures that each user receives at least one PRB before distributing the remaining resources based on proportional demand. After the initial allocation, the remaining PRBs are assigned using the same proportional logic as the previous baseline. This method prevents extreme resource starvation by guaranteeing a minimum allocation while still adapting to demand variations.

%    \item \textbf{3GPP-Based Proportional Fair Frequency-Domain Packet Scheduling (3GPP-PF) \cite{5062197}:} This method follows the proportional fair scheduling principle standardized in 3GPP, balancing user fairness and spectral efficiency by prioritizing users based on their channel conditions and past resource allocations. It assigns PRBs to maximize the long-term throughput while ensuring that users receive a fair share of the resources over time. The proportional fair scheduler dynamically adjusts allocations to mitigate resource starvation and enhance overall spectral efficiency.
%\end{itemize}

\begin{figure*}[!t]
    \centering
    %---- First subfigure ----
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{thumbnail_urlcc_cdf.png}
        \caption{CDF of URLLC Latency.}
        \label{fig:urlcc_cdf}
    \end{subfigure}
    \hfill 
    %---- Second subfigure ----
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{thumbnail_embb_cdf.png}
        \caption{CDF of eMBB $\Delta$ Bitrate.}
        \label{fig:embb_cdf}
    \end{subfigure}
    \hfill
    %---- Third subfigure ----
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\linewidth]{thumbnail_mmtc_cdf.png}
        \caption{CDF of mMTC $\Delta$ Payload (Log Scale).}
        \label{fig:mmtc_cdf}
    \end{subfigure}
    \caption{Comparison of network slice CDFs.}
    \label{fig:three_cdfs}
\end{figure*}

The evaluation of the CDF plots, shown in Figures \ref{fig:urlcc_cdf}, 
\ref{fig:embb_cdf} and ~\ref{fig:mmtc_cdf} highlights the comparative performance of the RL-based approaches (PPO and DQN) against the baseline resource allocation methods.
In Figure~\ref{fig:mmtc_cdf}, which depicts the bitrate distribution for mMTC slices, PPO demonstrates superior performance by achieving higher throughput levels for a larger proportion of users compared to DQN and the baselines. Similarly, in Figure~\ref{fig:embb_cdf}, which illustrates the throughput distribution for eMBB slices, PPO consistently outperforms other methods, indicating its ability to allocate resources adaptively to meet the higher bandwidth demands of eMBB traffic.
For URLLC traffic, as shown in Figure~\ref{fig:urlcc_cdf}, PPO achieves lower latency for a greater percentage of packets, ensuring compliance with stringent delay requirements. Specifically, the latency CDF indicates that PPO prioritizes low-latency traffic more effectively by dynamically allocating PRBs to meet the strict timing needs of URLLC packets, even under fluctuating network conditions and resource contention with other traffic types. This is particularly evident in the steepness of the latency curve for PPO, which reflects a concentrated distribution of packets with delays well below the critical thresholds. Compared to DQN and the baselines, PPO exhibits a more robust adaptation to channel variability, maintaining consistent performance across different scenarios. While the pre-allocated proportional baseline provides a reasonable minimum latency for many packets due to its fairness-driven approach, its inability to dynamically adjust to traffic demands results in a heavier tail, where a non-negligible percentage of packets experience higher delays. The 3GPP-PF approach, while effective in maintaining fairness, is limited by its reliance on past allocations, leading to occasional inefficiencies in handling dynamic traffic loads. Overall, PPO's latency optimization demonstrates its capability to fulfill the ultra-reliable and low-latency requirements critical for URLLC applications, ensuring that stringent QoS objectives are met.

% First Figure: mMTC CDF
%\begin{figure}[!t]
%    \centering
%    \includegraphics[width=0.85\columnwidth]{thumbnail_mmtc_cdf.png}
%    \caption{CDF of mMTC Bytes Difference (Log Scale).}
%    \label{fig:mmtc_cdf}
%\end{figure}

% Second Figure: eMBB CDF
%\begin{figure}[!t]
%    \centering
%    \includegraphics[width=0.85\columnwidth]{thumbnail_embb_cdf.png}
%    \caption{CDF of eMBB Bitrate Difference.}
%    \label{fig:embb_cdf}
%\end{figure}

% Third Figure: URLLC CDF
%\begin{figure}[!t]
%    \centering
%    \includegraphics[width=0.85\columnwidth]{thumbnail_urlcc_cdf.png}
%    \caption{CDF of URLLC Latency.}
%    \label{fig:urlcc_cdf}
%\end{figure}

\section{Conclusion}
\label{sec:conclusion}
%\fa{the conclusion should be more detailed and impressive.}
In this paper, we presented a framework integrating the OSC near-RT RIC with srsRAN for real-time slicing and resource management in O-RAN. Using PPO in an RL-based xApp, we demonstrated adaptive resource allocation for URLLC, eMBB, and mMTC slices under realistic channel conditions modeled with GNU Radio. Our results show improved QoS compliance, including enhanced throughput, reduced latency, and stable resource distribution.

Despite these achievements, several challenges remain. One key limitation is the high computational demand of PPO, particularly in online training scenarios where policies must dynamically adapt to real-time network variations. This contrasts with offline training methods that rely on pre-collected datasets, reducing computational overhead but limiting adaptability.
Additionally the constraints imposed by not using SDRs particularly in handling increased numbers of UEs due to ZeroMQ.
%increasing the number of simulated UEs
%Additionally, srsRAN imposes constraints on scalability, particularly in handling increased numbers of UEs due to its ZeroMQ-based message passing system. These bottlenecks restrict the full potential of our RL-based slicing framework, necessitating further improvements in both hardware optimization and efficient scheduling mechanisms. 
Moreover, our work underscores the importance of balancing URLLC's stringent latency requirements with eMBB's high throughput demands. The ability of PPO to dynamically reallocate resources based on evolving network conditions proves advantageous, but additional refinements in state-space representation and reward design could enhance performance further. Future research should explore hybrid approaches combining online and offline RL training to mitigate computational burdens while preserving adaptability. Additionally, integrating multi-agent RL techniques could improve decision-making scalability in dense O-RAN deployments.

Overall, this work establishes a foundation for real-time RL-based slicing in O-RAN while identifying areas for future enhancement. Addressing these limitations will be crucial in achieving robust, scalable, and intelligent resource allocation strategies for next-generation wireless networks.



\ifthenelse{\boolean{longversion}}{
% Looking ahead, there are several promising directions to expand this research:
% \begin{itemize}
%     \item \textbf{Mid-band TDD and Massive MIMO:} Extending to n77/n78 frequencies and testing advanced hardware with MIMO capabilities would more closely approximate modern 5G setups and reveal new scaling behaviors.
%     \item \textbf{Robust UL Slicing:} A stable uplink PRB control mechanism in srsRAN would enable fully bidirectional URLLC and mMTC studies, offering deeper insight into real-world IoT scenarios.
%     \item \textbf{Multi-Tap or Clustered Fading Models:} Incorporating more complex channel conditions (e.g., multi-path, Rayleigh/Rician) could stress the RL agent’s adaptability beyond single-tap assumptions.
%     \item \textbf{Scalability and Large-UE Testing:} Investigating performance with dozens or even hundreds of UEs would reveal how well the RL xApp manages resources in dense traffic environments and whether ZeroMQ can be replaced or optimized.
% \end{itemize}
}{}


%\nocite{*}
%\bibliographystyle{IEEEtran}
%\bibliography{References}
\input{References.tex}

\end{document}