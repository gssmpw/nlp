@article{chatkbqa,
 author = {Luo, Haoran and Tang, Zichen and Peng, Shiyao and Guo, Yikai and Zhang, Wentai and Ma, Chenghao and Dong, Guanting and Song, Meina and Lin, Wei and others},
 journal = {ArXiv preprint},
 title = {Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models},
 url = {https://arxiv.org/abs/2310.08975},
 volume = {abs/2310.08975},
 year = {2023}
}

@article{decaf,
 author = {Yu, Donghan and Zhang, Sheng and Ng, Patrick and Zhu, Henghui and Li, Alexander Hanbo and Wang, Jun and Hu, Yiqun and Wang, William and Wang, Zhiguo and Xiang, Bing},
 journal = {ArXiv preprint},
 title = {Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases},
 url = {https://arxiv.org/abs/2210.00063},
 volume = {abs/2210.00063},
 year = {2022}
}

@article{demonstrate-search-predict,
 author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
 journal = {ArXiv preprint},
 title = {Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
 url = {https://arxiv.org/abs/2212.14024},
 volume = {abs/2212.14024},
 year = {2022}
}

@inproceedings{g-retriever,
 author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {132876--132907},
 publisher = {Curran Associates, Inc.},
 title = {G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/efaf1c9726648c8ba363a5c927440529-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@article{gnn-rag,
 author = {Mavromatis, Costas and Karypis, George},
 journal = {ArXiv preprint},
 title = {GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning},
 url = {https://arxiv.org/abs/2405.20139},
 volume = {abs/2405.20139},
 year = {2024}
}

@article{graphCoT,
 author = {Jin, Bowen and Xie, Chulin and Zhang, Jiawei and Roy, Kashob Kumar and Zhang, Yu and Wang, Suhang and Meng, Yu and Han, Jiawei},
 journal = {ArXiv preprint},
 title = {Graph chain-of-thought: Augmenting large language models by reasoning on graphs},
 url = {https://arxiv.org/abs/2404.07103},
 volume = {abs/2404.07103},
 year = {2024}
}

@article{graphllm,
 author = {Chai, Ziwei and Zhang, Tianjie and Wu, Liang and Han, Kaiqiao and Hu, Xiaohai and Huang, Xuanwen and Yang, Yang},
 journal = {ArXiv preprint},
 title = {Graphllm: Boosting graph reasoning ability of large language model},
 url = {https://arxiv.org/abs/2310.05845},
 volume = {abs/2310.05845},
 year = {2023}
}

@inproceedings{kb-binder,
    title = "Few-shot In-context Learning on Knowledge Base Question Answering",
    author = "Li, Tianle  and
      Ma, Xueguang  and
      Zhuang, Alex  and
      Gu, Yu  and
      Su, Yu  and
      Chen, Wenhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.385/",
    doi = "10.18653/v1/2023.acl-long.385",
    pages = "6966--6980",
    abstract = "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at \url{https://github.com/ltl3A87/KB-BINDER}."
}

@inproceedings{kg-gpt,
    title = "{KG}-{GPT}: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
    author = "Kim, Jiho  and
      Kwon, Yeonsu  and
      Jo, Yohan  and
      Choi, Edward",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.631/",
    doi = "10.18653/v1/2023.findings-emnlp.631",
    pages = "9410--9421",
    abstract = "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs."
}

@article{li2024structrag,
 author = {Li, Zhuoqun and Chen, Xuanang and Yu, Haiyang and Lin, Hongyu and Lu, Yaojie and Tang, Qiaoyu and Huang, Fei and Han, Xianpei and Sun, Le and Li, Yongbin},
 journal = {ArXiv preprint},
 title = {StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization},
 url = {https://arxiv.org/abs/2410.08815},
 volume = {abs/2410.08815},
 year = {2024}
}

@inproceedings{mindmap,
    title = "{M}ind{M}ap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
    author = "Wen, Yilin  and
      Wang, Zifeng  and
      Sun, Jimeng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.558/",
    doi = "10.18653/v1/2024.acl-long.558",
    pages = "10370--10388",
    abstract = "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named MindMap, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question {\&} answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference."
}

@article{peng2024graph,
 author = {Peng, Boci and Zhu, Yun and Liu, Yongchao and Bo, Xiaohe and Shi, Haizhou and Hong, Chuntao and Zhang, Yan and Tang, Siliang},
 journal = {ArXiv preprint},
 title = {Graph retrieval-augmented generation: A survey},
 url = {https://arxiv.org/abs/2408.08921},
 volume = {abs/2408.08921},
 year = {2024}
}

@inproceedings{rag,
 author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
 year = {2020}
}

@inproceedings{skill_llm_structured_knowledge_infusion,
 address = {Seattle, United States},
 author = {Moiseev, Fedor  and
Dong, Zhe  and
Alfonseca, Enrique  and
Jaggi, Martin},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2022.naacl-main.113},
 pages = {1581--1588},
 publisher = {Association for Computational Linguistics},
 title = {{SKILL}: Structured Knowledge Infusion for Large Language Models},
 url = {https://aclanthology.org/2022.naacl-main.113},
 year = {2022}
}

@article{structLM,
 author = {Zhuang, Alex and Zhang, Ge and Zheng, Tianyu and Du, Xinrun and Wang, Junjie and Ren, Weiming and Huang, Stephen W and Fu, Jie and Yue, Xiang and Chen, Wenhu},
 journal = {ArXiv preprint},
 title = {StructLM: Towards Building Generalist Models for Structured Knowledge Grounding},
 url = {https://arxiv.org/abs/2402.16671},
 volume = {abs/2402.16671},
 year = {2024}
}

@inproceedings{tree-of-traversals,
    title = "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
    author = "Markowitz, Elan  and
      Ramakrishna, Anil  and
      Dhamala, Jwala  and
      Mehrabi, Ninareh  and
      Peris, Charith  and
      Gupta, Rahul  and
      Chang, Kai-Wei  and
      Galstyan, Aram",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.665/",
    doi = "10.18653/v1/2024.acl-long.665",
    pages = "12302--12319",
    abstract = "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at https://github.com/amazon-science/tree-of-traversals"
}

@article{villalobos2024will,
 author = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
 journal = {ArXiv preprint},
 title = {Will we run out of data? Limits of LLM scaling based on human-generated data},
 url = {https://arxiv.org/abs/2211.04325},
 volume = {abs/2211.04325},
 year = {2022}
}

