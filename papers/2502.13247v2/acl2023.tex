% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% My packages

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{xcolor}
\usepackage{array}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bbold}
\usepackage{colortbl} % For coloring table cells
\usepackage{arydshln}
\usepackage[symbol]{footmisc}
\usepackage{tcolorbox}

\definecolor{lightergreen}{RGB}{26, 153, 0}
\definecolor{lighteryellow}{RGB}{255, 198, 26}
\definecolor{lighterred}{RGB}{204, 0, 0}

%\setlength{\dashlinedash}{1.5pt} % Short dash length to resemble dots
% \setlength{\dashlinegap}{1.5pt} 

\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{lipsum}
\tcbuselibrary{breakable}

\setlength{\dashlinedash}{1.5pt} % Short dash length to resemble dots
\setlength{\dashlinegap}{1.5pt} 


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Grounding LLM Reasoning with Knowledge Graphs}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Alfonso Amayuelas$^{1}$\footnotemark  , Joy Sain$^2$, Simerjot Kaur$^2$, Charese Smiley$^2$, \\
$^1$University of California, Santa Barbara \\
$^2$JP Morgan Chase \\
\texttt{amayuelas@ucsb.edu}  \\ 
\texttt{\{joy.sain, simerjot.kaur, charese.h.smiley\}@jpmchase.com}
}

% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:

% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{
%     Author Name
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
Knowledge Graphs (KGs) are valuable tools for representing relationships between entities in a structured format. Traditionally, these knowledge bases are queried to extract specific information. However, question-answering (QA) over such KGs poses a challenge due to the intrinsic complexity of natural language compared to the structured format and the size of these graphs. Despite these challenges, the structured nature of KGs can provide a solid foundation for grounding the outputs of Large Language Models (LLMs), offering organizations increased reliability and control. 
%This is particularly relevant for organizations that maintain proprietary knowledge bases in specialized domains, where using such data to train LLMs is constrained by legal or economic reasons.  

Recent advancements in LLMs have introduced reasoning methods at inference time to improve their performance and maximize their capabilities. In this work, we propose integrating these reasoning strategies with KGs to anchor every step or "thought" of the reasoning chains in KG data. Specifically, we evaluate both agentic and automated search methods across several reasoning strategies, including Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT), using GRBench, a benchmark dataset for graph reasoning with domain-specific graphs. Our experiments demonstrate that this approach consistently outperforms baseline models, highlighting the benefits of grounding LLM reasoning processes in structured KG data.
\end{abstract}
\vspace{-4pt}

 \footnotetext{$^*$Work done during an internship at JP-Morgan Chase}

\section{Introduction}
\vspace{-2pt}

% Abstract + Introduction + 1 Figure should take the 1st 2 pages

Large Language Models (LLMs) have shown remarkable versatility in answering questions posed in natural language. This is mainly due to their ability to generate text, their broad internal knowledge, and their capacity to access external information \citep{toolqa, rag}. However, a significant area for improvement is their tendency to produce information that, while plausible-sounding, is often unverifiable and lacks traceable origins and sources \citep{hallucination_survey}. 

% The LLM content generation process is based on internal parameters, making it challenging to link their outputs to external sources \citep{llm_internal_knowledge, llm_kb}. The need for more transparency in how these models generate content further complicates efforts to ensure the reliability and accountability of their responses. Consequently, this impacts their suitability for industrial applications \citep{llm_explainability_survey}.

The LLM generation process heavily relies on their internal parameters, making it difficult to link their outputs to external sources \citep{llm_internal_knowledge, llm_kb}. This lack of transparency in content generation poses challenges in ensuring the reliability and accountability of their responses. Thus, this limitation impacts their suitability for industrial applications \citep{llm_explainability_survey}.

These shortcomings become more relevant in applied settings, where LLMs are used for critical operations within organizations. This highlights the importance of integrating LLM generation processes with domain-specific knowledge. Since LLMs are typically trained on general datasets, fine-tuning them for each new domain can be labor-intensive. This challenge is even more pronounced for companies with proprietary internal data due to privacy, legal, and resource challenges. Hence, it is crucial to develop processes that effectively link LLMs with external knowledge bases. 

When connecting LLMs with external knowledge, techniques such as Retrieval-Augmented Generation (RAG) \citep{rag} or generating SQL queries to interact with traditional databases \citep{llm_eval_sql} are often employed. These methods are designed to enable LLMs to interact with external knowledge sources. However, they typically assume that knowledge is well represented in discrete units, such as documents or tables. This approach fails to capture the complex relationships between concepts that may extend beyond one particular document. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/general.png}
    \caption{Methods for Question-Answering in KGs, explaining the general framework described in Section \ref{Section:Method}. \textbf{Left}: \textit{Agent}. LLM decides to take one of the predefined actions to connect with the graph. \textbf{Right:} \textit{Automatic Graph Exploration}. Entities are extracted in each reasoning step, triggering a search for each identified entity.} 
    \label{fig:general}
    \vspace{-12pt}
\end{figure*}



KGs emerge as a solution for more effectively representing complex knowledge. A KG is an organized representation of real-world entities and their relationships. This interconnected nature of concepts is reflected both in written content and in the patterns of their relationships. However, employing them to guide the reasoning processes of LLMs is a novel approach that has only recently started to be explored \citep{rog}.


In this work, we aim to answer questions using domain-specific KGs (\textit{academic, literature, healthcare, etc.}) from GRBench dataset \cite{graphCoT}. Each sample in GRBench contains a manually crafted question and its corresponding answer, which can be obtained by retrieving relevant information from the graph as context. We present various reasoning methods that connect every \textit{thought} or \textit{step} with the KG. These methods include Chain-of-Thought (\textit{CoT} --  \citet{cot}), Tree-of-Thought (\textit{ToT} -- \citet{tot}) and Graph-of-Thought (\textit{GoT} -- \citet{got}). The strategies are shown in Figure \ref{fig:reasoning-strategies}. Our results demonstrate significant improvements in generating accurate answers from the graph, achieving state-of-the-art performance on GRBench. 


%This task is commonly known as Knowledge-Graph Question Answering (KGQA)
% \textcolor{red}{SK: IS THIS NEEDED? Its a repeat: Using structured knowledge, particularly Knowledge Graphs, enhances traceability and provides guarantees for organizations that use LLMs in their processes. It also allows organizations to leverage their own data better. This work introduces methods that advance reasoning capabilities on knowledge graphs and emphasizes the importance of connecting each step of the reasoning process with the KG to guide the model more effectively in the search. It has the potential to be extended to other real-world environments beyond knowledge bases, such as databases and the physical world.}

Our contributions can be summarized as follows:
\begin{itemize}[nosep]
    \item We present a versatile framework that integrates reasoning strategies with KG search during inference. This approach is designed to easily incorporate additional strategies as needed in the future.
    
    \item Our results achieve state-of-the-art performance on the GRBench benchmark, demonstrating an average performance improvement of at least 26.5\% over the CoT method.
    
    \item We conduct an analysis of various aspects of our methods, including the impact of different model sizes.
\end{itemize}


\section{Related Work}
%TODO: Either add 1 short paragraph or extend 1 of them

LLMs require large amounts of data and resources for training \cite{villalobos2024will}. The need to leverage these models with external data after they are trained has driven the popularity of RAG methods, which incorporate external data \citep{rag, demonstrate-search-predict}. Recent advances have combined RAG with structured knowledge such as ontologies, enhancing LLM reasoning capabilities \cite{li2024structrag}. Our work introduces a framework that employs advanced reasoning strategies, grounding LLM outputs in domain-specific KGs for improved performance in specific domains.  This approach facilitates more targeted and iterative interactions with knowledge graphs, distinguishing it from traditional RAG methods.

\paragraph{Structured Knowledge} Structured knowledge, such as Databases or KGs, provides organizations with reliable sources of information that can be more easily maintained and automatically updated. KGs, in particular, offer an adaptable knowledge model that captures complex relationships between interconnected concepts. Some research has focused on developing models that can interact with multiple types of structured knowledge: StructLM  \citep{structLM}, and examining the impact of incorporating structured knowledge into the pretraining of LLMs  \citep{skill_llm_structured_knowledge_infusion}. 


\paragraph{Integrating KGs with LLMs} \citep{peng2024graph} The integration of KGs with LLMs has emerged as a promising approach to enhance AI systems' reasoning capabilities and reliability. In general, we distinguish four primary methods for enabling LLMs to interact with graphs: (1) Learning graph representations \citep{let_graph_do_thinking, graphllm}, however these latent representations currently fall short of text-based methods on Knowledge Graph Question Answering (KGQA) tasks. (2) Using Graph Neural Network (GNN) retrievers to extract relevant entities and provide text-based input to the model \citep{g-retriever, gnn-rag}. (3) Generating code, such as SPARQL, to retrieve data from the graph \citep{kb-binder}. Finally, (4) Methods that allow step-by-step interaction with KG \citep{rog, decaf, chatkbqa}. These last methods currently perform best on KGQA tasks.

% \paragraph{LLM Reasoning with Graphs} There is a general interest in leveraging knowledge graphs as a data structure to improve the reasoning capabilities of LLMs. Knowledge Graphs can also be used to understand the reasoning capabilities of LLMs \cite{wang2024understanding}. In most cases, they use the last form of interaction presented in the previous paragraph. In which the LLMs are allowed to interact with the graphs. Some examples can be found in \citet{think-on-graph, mindmap, rog, kg-gpt}. More recently, pointing in the same direction as our work, some works explore the connection of the traditional LLM reasoning strategies with KGs \citet{graphCoT, tree-of-traversals}.

\vspace{-2pt} 
\paragraph{LLM Reasoning with Graphs} There is a growing interest in leveraging KGs to enhance the reasoning capabilities of LLMs. KGs not only serve as a structured data source but also provide a framework for understanding and improving the reasoning processes of LLMs \cite{wang2024understanding}. This integration enables models to generate more coherent and contextually relevant responses while allowing the tracing and verification of the reasoning steps. The most effective methods typically involve a step-by-step interaction between LLMs and graphs, as discussed in the previous paragraph. Notable examples of this approach include the works of \citet{think-on-graph, mindmap, rog, kg-gpt}. Recent research, including our own, has begun to explore the integration of traditional LLM reasoning strategies with KGs, as shown in studies by \citet{graphCoT, tree-of-traversals}.

\vspace{-2pt}
\section{Background}
\label{Section:Preliminaries}

In this section, we formalize the prerequisite knowledge relevant to this paper. We use $p_\theta$ to denote a pre-trained language model with parameters $\theta$. And letters $x,y,z$ to refer to a language sequence. $x=(x_1, x_2, ..., x_n)$, where each is $x_i$ is a such that $p_\theta(x) = \prod_{i=1}^n p_\theta(x_i | x_{1...i-1})$.

\paragraph{Knowledge Graphs \textnormal{(KGs)}} A KG is a heterogeneous directed graph that contains factual knowledge to model structured information. In a KG, nodes represent entities, events, or concepts, while edges represent the connection and types of relations between them.\\
Formally, a KG is represented as $\mathcal{G}$, defined by a set of triples $\mathcal{G} = \{ (h,r,t)  \mid h,t \in \mathcal{E}, r \in \mathcal{R} \} $, where $\mathcal{E}$, $\mathcal{R}$ denote the set of entities and relations, respectively. 

\paragraph{Knowledge Graph Question-Answering \textnormal{(KGQA)}} It is a common reasoning task that leverages KGs. Given a natural language question, $q$, and an associated KG, $\mathcal{G}$, the goal is to develop a method that retrieves the correct answer, $a$, based on the knowledge extracted from the KG: $a = f(q, \mathcal{G})$.

\paragraph{Step-by-step Reasoning with LLMs} To improve the reasoning capabilities of LLMs at inference time, a common approach is to generate intermediate reasoning steps. The key idea is the introduction of intermediate steps, $Z_{p_\theta} = z_1, ... , z_n$, to add inference sources to bridge the $q$ and $a$. This approach enables models to break down complex, multi-step problems into smaller, manageable intermediate steps. By doing so, additional computational resources can be allocated to problems that necessitate more reasoning steps.

% To improve the reasoning capabilities of LLMs at inference time, a common approach is to generate intermediate reasoning steps, as demonstrated by the Chain of Thought (CoT) method \citet{cot}. The key idea is the introduction of the chain of \textit{thoughts}, $\mathcal{R}_{p_\theta} = z_1, ... , z_n$, to add inference sources to bridge the $q$ and $a$. The CoT approach simulates a step-by-step thought process to arrive at an answer, which is samples as $y \sim p_\theta^{\text{CoT}}(a | q, z_1, ... z_n)$ This method enables models to break down complex, multi-step problems into smaller, manageable intermediate steps. By doing so, additional computational resources can be allocated to problems that necessitate more reasoning steps. Furthermore, CoT provides a transparent window into the model's behavior, offering insights into how it might have arrived at a particular answer. \textcolor{red}{REDUCE}

% To improve the reasoning capabilities of LLMs at inference, a common approach is to generate intermediate reasoning steps, as in the case of Chain of Thought (CoT), \citet{cot}. CoT mimics a step-by-step thought process for arriving at the answer. It allows allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems
% that require more reasoning steps. And it provides an interpretable window into the behavior of the model,
% suggesting how it might have arrived at a particular answer. Formally, the model, $\mathcal{M}$, generates a sequence of logical reasoning steps $R_\mathcal{M}(q) = S_1, S_2, ..., S_n$, where each step is a function of the input and all the previous steps, i.e., $S_i = \mathcal{M}(q, S_1, ... S_{i-1})$. This process continues until a stopping criterion is met and the final answer is derived from the complete reasoning process as $a = g(R_{\mathcal{M}}(q))$.


\section{Method}
\label{Section:Method}


\begin{figure*}[tp]
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cot.jpg}  % White box simulating a white image
        % \caption{\centering Chain of Thought (CoT)}
        \caption{\centering Chain of Thought (CoT)}
        \label{fig:graph-cot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/tot.jpg} % White box simulating a white image
        % \caption{\centering Tree of Thought \\ (ToT)}
        \caption{Tree of Thought\\ (ToT)}
        \label{fig:graph-tot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.42\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/got.jpg} % White box simulating a white image
        % \caption{\centering Graph of Thought \\ (GoT)}
        \caption{Graph of Thought \\ (GoT)}
        \label{fig:graph-got}
    \end{subfigure}
    \caption{Reasoning Strategies: This figure illustrates different LLM reasoning strategies to navigate the potential answer space: CoT, ToT, GoT. Each strategy consists of  "thoughts" connected to the Knowledge Graph (KG) through search methods, as detailed in Section \ref{Section:LM-KG-Interaction}: Agent or Graph Exploration.}
    \label{fig:reasoning-strategies}
\end{figure*}

Integrating knowledge from KGs with LLMs is challenging due to the complexity and high dimensionality inherent in both language and graph structures. Nodes and edges in KGs can represent a wide variety of elements, including entities, objects, and concepts, which adds to the task's intricacy.

This work aims to show how conditioning the model at each step of the LLM reasoning process can improve domain-specific question answering based on graphs. Results from the Graph-CoT in GRBench support this approach \citep{graphCoT}. 

Our method involves generating a reasoning chain composed of $n$ steps, with each step linked to the KG. The generated text at every step is used to extract relevant information from the KG. Using the terminology introduced in Section \ref{Section:Preliminaries}, every reasoning step, $z_i$, is now accompanied by a graph search, $\mathcal{T} (z_i, \mathcal{G}) = \mathcal{G}'_i$ which retrieves information from relevant entities in various forms. 

We can divide the method into two main modules: (1) the implemented reasoning strategies, and (2) the search methods used by the LLM within the graph. The flexibility to incorporate new reasoning strategies and alternative search methods is particularly noteworthy.


\subsection{Reasoning Strategies}

\subsubsection{Chain of Thoughts \textnormal{(CoT)}} 

CoT \cite{cot} is a well-known reasoning method that involves generating a sequence of logical steps, where each step builds upon the previous ones, ultimately leading to a conclusion. Formally, it generates a sequence of reasoning steps \(Z_{p_\theta} (q) = \{z_1, z_2, \ldots, z_n\}\), where each step \(z_i\) is sampled sequentially given the input query \(q\), all previous steps and and graph info from all steps , $\mathcal{G}'$, as \(z_i \sim p_{\theta}^{\text{CoT}} (z_i | q, \mathcal{G}', z_{1 \ldots i-1})\). The final answer \(a\) is derived from this reasoning process given all the generated thoughts $a \sim  p_{\theta}^{\text{CoT}} (a | q, \mathcal{G}', z_{1 \ldots n})$. In practice, it is sample as a continuous language sequence.Figure \ref{fig:graph-cot} represents this method, where each step is linked to the KG.

% \paragraph{Tree of Thoughts \textnormal{(ToT) \cite{tot}}} It extends the Chain of Thought by considering multiple possible paths at each step, forming a tree structure. This is shown in Figure \ref{fig:graph-tot}. This method allows the model to navigate the problem space, exploring multiple reasoning paths, while being influenced by the results from the KG retrieval. It requires the branching factor, $b_t$, as an input parameter. At every step, the model generates $b_t$ simultaneous steps. A state evaluator, $V$, is then presented with all the steps at the current level and selects $b_t$ steps to proceed to the next step.


\subsubsection{Tree of Thoughts \textnormal{(ToT)}} 

ToT \cite{tot} extends the CoT by exploring multiple possible paths at each step, forming a tree structure, as shown in Figure \ref{fig:graph-tot}. This approach frames the problem as a search over a tree, where each node represents a state $s_j = [q, \mathcal{G}', z_1, ... z_i]$, indicating a partial solution with input $q$, the sequence of thoughts so far, $z_1, ..., z_i$, and the retrieved graph info, $\mathcal{G}'$. This method enables the model to navigate the problem space by exploring various reasoning paths while incorporating results from KG retrieval. It requires the branching factor $k$ as an input parameter. At each step, the model generates $k$ simultaneous steps. A state evaluator $V$ then reviews all steps at the current level and selects $t$ steps to continue.

% \begin{enumerate}
    % \item 
\paragraph{Thought Generator $G(p_{\theta}, s, k)$}  Given a tree state, $s_j=[q, \mathcal{G}', z_1, ..., z_i]$, it produces $k$ candidate steps. Each \textit{thought} or step, $z_i$, is a distinct unit generated by the LLM. These thoughts independently interact with the graph, as detailed later in Section \ref{Section:LM-KG-Interaction}. Various methods can be used to generate diverse thoughts; in our case, we utilize independent sampling:
    \begin{itemize}
        \item Sampling. This method generates up to $k$ i.i.d thoughts from a step-by-step CoT process. 
        \begin{align}
            z^{j} \sim p_{\theta}^\text{CoT}(z_{i+1} \mid s) &= \nonumber \\
            p_{\theta}^\text{CoT}(z_{i+1} \mid q, \mathcal{G}', z_1, ..., z_i) (j = 1 ... k )
        \end{align}
        % $$z^{j} \sim p_{\theta}^\text{CoT}(z_{i+1} \mid s) = p_{\theta}^\text{CoT}(z_{i+1} \mid q, z_{1, ... i}) (j = 1 ... k )$$.
    \end{itemize}
    \paragraph{State Evaluator $V(p_{\theta}, S, t)$} The state evaluator assesses reasoning steps at each tree depth, scoring their likelihood of leading to the correct solution. It prioritizes the most promising steps, focusing exploration on paths most likely to reach the correct solution: $\text{Top}_t \{ V(p_\theta,s_i | s_i ) \in S) \}$
    \begin{enumerate}
        \item Selection: It chooses $t$ states that will move to the next level, while discontinuing the others. The LLM is prompted to select between all of them at certain level.
        
        \item Score: The vote $V(p_{\theta}, S) (s) = \mathbb{p}[s=s^*]$, where a good state $s^* \sim p_{\theta}^{\text{score}(s^* \mid S)}$ is selected by the LLM when prompted to estimate the probability of the current state solving the given input question. The top $t$ states are then selected based on these scores.
    \end{enumerate}
    \paragraph{Search Algorithm} The Search Algorithm defines the method for exploring the tree. It involves systematically visiting each node in the tree exactly once. Common tree traversal algorithms include Depth-First Search (DFS) and Breadth-First Search (BFS)\cite{design_analysis_algorithms}. We choose to implement BFS as it progresses through the tree level by level, allowing State Evaluator to make informed decisions based on available information.
    
    \begin{itemize}
        \item BFS: This algorithm explores nodes level by level, visiting all nodes at a given depth before moving to the next level, starting from the root. In this context, BFS efficiently explores each level until a stopping criterion is reached, either the maximum depth (\texttt{max\_depth}) or the model's solution selection mechanism.
    \end{itemize}
% \end{enumerate}
        
%TODO: Probably will have to write 2 different parameters: (1) branching factor, (2) #selection
\subsubsection{Graph of Thoughts \textnormal{(GoT)}}

GoT \cite{got} is an advanced reasoning framework that represents thought processes as interconnected nodes within a graph, where each node corresponds to a reasoning step or concept, as shown in Figure \ref{fig:graph-got}. This approach models the reasoning process as a directed graph $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of edges. The key difference with the previously presented ToT is the aggregation transformation, which creates a single node by merging two separate chains. GoT builds upon the foundational elements of ToT: a Thought Generator $G(p_{\theta}, s,k)$, a State Evaluator $V(p_{\theta}, S)$, and a Search Algorithm. It requires one additional components 


% \begin{enumerate}
\paragraph{Aggregation Transformation $A(z^{j-1}_{i-1}, z^{j+1}_{i-1})$} This transformation aggregates arbitrary thoughts into new ones, combining and reinforcing their advantages while mitigating their disadvantages. Formally, this function creates a new thought, $z^{j}_{i+1}$, by merging two different branches from the previous level: $z_{i+1} = A(z^{j-1}_{i-1}, z^{j+1}_{i-1})$. The graph model represents it by adding outgoing edges from two initial thoughts to the newly merged thought. This process enhances the robustness of the reasoning chain by integrating thoughts from different branches.

    %TODO: Rewrite formulation
    % \item Refining Transformation. A refining function of a current thought represents a self-loop in the graph. This self-loop is equivalent to the self-feedback mechanisms that allow models to auto-correct some discrepancies in their reasoning process \citep{auto-correct-llm}. Resource constraints meant we skipped this step in our experiments.
% \end{enumerate}

A refinement transformation can be added to establish a self-loop, similar to self-feedback mechanisms. However, initial tests showed only marginal improvements in our scenario, while significantly increasing computational overhead. As a result, we opted not to implement this transformation.

\subsection{LLM and KG Interaction Methods}
\label{Section:LM-KG-Interaction}

At each step, the thought process interacts with the provided KG. This retrieves new information and conditions the model for subsequent steps. We present two methods to achieve this interaction and illustrate them in Appendix \ref{App:pipelines}:

\subsubsection{Agent} This approach follows the agent-based methodology, initially described in ReACT \citep{react}. The LLM selects from a set of actions based on the current thought, it selects one of the predefined actions to connect with the graph. Each step in the reasoning chain consists of an interleaved sequence: \textit{thought} $\rightarrow$ \textit{action} $\rightarrow$ \textit{retrieved data}. This method implements four actions as described in GraphCoT \citep{graphCoT}:  
    % \begin{enumerate}
    \paragraph{\texttt{RetrieveNode}(Text)} Identifies the related node in the graph using semantic search.
    
    \paragraph{\texttt{NodeFeature}(NodeID, FeatureName)} Retrieves textual information for a specific node from the graph. 
    
    \paragraph{\texttt{NeighborCheck}(NodeID, EdgeType)} Retrieves neighbors' information for a specific node. 
    
    \paragraph{\texttt{NodeDegree}(NodeID, EdgeType)} Returns the degree (\#neighbors) for a given node and edge type
    % \end{enumerate}
    
\subsubsection{Automatic Graph Exploration}
\label{Section:Automatic_Graph_Exploration}

This method automatically searches the graph based on the generated text from the previous step, similar to the process described in \cite{think-on-graph}. The sequence of actions in each step is:  \textit{thought} $\rightarrow$ \textit{entity-recognition} $\rightarrow$ \textit{graph-exploration} $\rightarrow$ \textit{retrieved data}. Since the dataset lack anchor entities, entities are extracted from the query. The graph exploration follows a Search + Prune approach, exploring the graph based on unvisited mentioned entities at every step. This process is applied first to relations, and then to entities, and it is repeated for the \texttt{max\_depth} exploration parameters. Details are provided in Algorithm \ref{alg:graph_exploration_algorithm}.
    % \begin{enumerate}
    
\paragraph{Discovery} The model is presented with the anchor entity $e^{D-1}_{n}$, and will searches for all relation types associated with that entity. It can also be given a head entity and relation to search for all corresponding tail entities $(h^{D-1}_{n}, r^D_n, ?)$.

\paragraph{Prune} From the edges or entities retrieved during discovery phase, model is prompted to select only those that are relevant to answering the question.
    % \end{enumerate}

\section{Experiments}
\label{Section:Experiments}

\paragraph{Benchmark}
We use the GRBrench dataset \cite{graphCoT} to evaluate our methods. This dataset is specifically designed to evaluate how effectively LLMs can interact with domain-specific graphs to solve the given problem. It includes several graphs spanning various general domains. For our evaluation, we selected 7 graphs across multiple domains, excluding those with excessively high RAM requirements that exceed our available resources.

\vspace{-0.1pt}
\paragraph{Baselines}
The proposed methods, \textit{Agent} and \textit{Automatic Graph Exploration}, applied to CoT, ToT, and GoT, are compared against the following baseline methods: (1) \textbf{Zero-Shot}: Directly querying the model to answer the question without additional context. (2) \textbf{Text RAG} \cite{rag_sruvey}: Text-retrieval method that uses the text representation of nodes as input for the query, with the retrieved data serving as context for the model. (3) \textbf{Graph RAG}: Includes node neighbors (1-hop) for additional context beyond Text RAG. (4) \textbf{Graph CoT (Agent)}: Implements Graph CoT \cite{graphCoT} as an agent for CoT reasoning, utilizing the actions described in Section \ref{Section:LM-KG-Interaction}. 

\vspace{-0.1pt}
\paragraph{Experimental methods}
We implement the methods described in Section \ref{Section:Method}, extending (1) \textbf{Agent} and (2) \textbf{Automatic Graph Exploration} with various reasoning strategies during inference: (1) \textbf{CoT}, (2) \textbf{ToT}, and (3) \textbf{GoT}. For the latter two, we include their State Evaluation methods at each step: (1) \textit{Selection} and (2) \textit{Score}. In the results presented in Table \ref{Table:Results}, we set $n=10$ steps for all methods. ToT and GoT use a branching factor and Selection of $k = t = 3$. Our experiments use only open-access Llama 3.1 (Instruct) \cite{llama3models} as the backend models, which enhances reproducibility and allows for unlimited free calls. Specifically, we employ the 8B, 70B, and 405B versions, using the FP8 variant for the 405B model.

\vspace{-0.1pt}
\paragraph{Evaluation} We use both rule-based and model-based metrics to evaluate the models, following the GRBench paper \cite{graphCoT}. For the rule-based metric, we use Rouge-L (R-L) \citep{rouge_metric}, which measures the longest sequence of words appearing in the same order in both the generated text and the ground truth answer. For the model-based metric, we prompt GPT-4o to assess if the model's output matches the ground truth answer. GPT4Score is the percentage of answers that GPT-4o identifies as correct.

\paragraph{Implementation Details}
The experiments are run on NVIDIA TITAN RTX or NVIDIA A100 using Python 3.8. The models are deployed with vLLM \cite{vllm}, a memory-efficient library for LLM inference and serving. For the baseline, Mpnet-v2 is used as the retriever, and FAISS \cite{faiss} is employed for indexing.


\section{Results}
\label{Section:Labels}
The main results from both the baseline and experimental methods, evaluated using R-L, are presented in Table \ref{Table:Results}. For brevity, additional results using GPT4Score can be found in Appendix \ref{App:results-gpt4score}. We highlight three key insights from the findings: (1) The agentic method generally outperformed automatic graph exploration, indicating that targeted interactions with the KG enhance answer accuracy. (2) The ToT strategy demonstrated superior performance by effectively exploring multiple reasoning paths. (3) Although GoT strategy showed potential, it did not significantly outperform ToT, suggesting a need for further refinement in merging divergent reasoning results.

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{llllccccccc}
\toprule
& \textbf{Method} & & \multicolumn{1}{c}{\textbf{Model}} & \textbf{Healthcare} & \textbf{Goodreads} & \textbf{Biology} & \textbf{Chemistry} & \textbf{Materials Science} & \textbf{Medicine} & \textbf{Physics} \\
                  \hline
\multirow{9}{*}{\rotatebox{90}{\shortstack{Baselines}}} & \multicolumn{2}{c}{}                  & Llama 3.1 8B-Ins                   & 7.32                                & 6.18             & 10.68            & 11.69              & 8.95                      & 8.77            & 11.52            \\
& \multicolumn{2}{l}{Base}                  & Llama 3.1 70B-Ins                   & 9.74                                 & 9.79               & 11.49            & 12.58              & 10.40                       & 12.21             & 12.61            \\
& \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins                   & 8.66                               & 12.49              & 10.52           & 13.51              & 11.73                     & 11.82            & 11.63            \\

\cdashline{2-11}
& \multicolumn{2}{l}{}                  & Llama 3.1 8B-Ins                   & 8.24                               & 14.69              & 12.43            & 11.42              & 9.46                      & 10.75             & 11.29            \\
                  & \multicolumn{2}{l}{Text-RAG}                  & Llama 3.1 70B-Ins                   & 10.32                                & 18.81              & 11.87            & 16.35             & 12.25                      & 12.77             & 12.54            \\
                  & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins           & 11.61                                & 16.23              & 16.11           & 13.82              & 14.23                      & 15.16           & 16.32            \\ \cdashline{2-11}
                  & \multicolumn{2}{l}{}                  & Llama 3.1 8B-Ins                   & 12.94                               & 22.30             & 30.72               & 34.46              & 30.20                       & 25.81             & 33.49            \\
                  & \multicolumn{2}{l}{Graph-RAG}                  & Llama 3.1 70B-Ins                   & 17.95                                & 25.36              & 38.88               & 40.90              & 41.09                      & 31.43             & 39.75            \\
                  & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins         & 16.12                                & 23.13             & 37.57               & 42.58              & 37.74                       & 33.34             & 40.98 \\
                  \hline \hline
 \multirow{6}{*}{\rotatebox{90}{\shortstack{Graph\\CoT}}} & \multicolumn{2}{c}{}                  & Llama 3.1 8B-Ins                   & 16.83                                & 30.91               & 20.15            & 18.43               & 26.29                      & 14.95            & 21.41            \\
 & \multicolumn{2}{l}{Agent}                  & Llama 3.1 70B-Ins                   & 33.48                               & 40.98              & 50.00            & 51.53              & 49.6                      & 48.27            & 44.35           \\ 
 & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins                  & 28.41                              & 36.56              & 41.35            & 48.36             & 47.81                     & 42.54            & 35.24           \\ \cdashline{2-11}
                  & \multicolumn{2}{l}{ \multirow{3}{*}{\shortstack{Graph\\Explore}}}                  & Llama 3.1 8B-Ins                   & 25.58                                & 32.34              & 36.65            & 35.33              & 31.06                      & 31.05                & 35.96            \\
                & \multicolumn{2}{l}{}                  & Llama 3.1 70B-Ins                   & 29.41                                & 29.60            & 44.63          & 49.49             & 39.23                     & 38.87             & 45.52            \\
                & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins                   & 28.45                               & 43.06            & 36.93       & 38.71              & 47.49                      & 55.66                & 32.73            \\
                  \hline \hline
                  
\multirow{12}{*}{\rotatebox{90}{\shortstack{Graph\\ToT}}} & \multirow{6}{*}{Agent} &    \multirow{3}{*}{Score}          & Llama 3.1 8B-Ins                  & 28.91                                & 52.25              & 43.81            & 44.18               & 43.49                      & 36.07             & 39.56            \\
& &              & Llama 3.1 70B-Ins                  & 38.51                                & 51.58              & 64.44            & 61.13               & 55.19                      & 63.00             & 55.33            \\
& &              & Llama 3.1 405B-Ins                  & \textbf{47.51 }                               & 50.73              & \textbf{70.34}           & 64.9               & 49.02                      & \textbf{65.40}             & 44.63            \\ \cdashline{3-11}

    &                   &        \multirow{3}{*}{Select}           & Llama 3.1 8B-Ins                   & 28.67                                 & 50.59               & 42.33            & 37.07              & 40.81                      & 33.17             & 36.50            \\
    &                   &                  & Llama 3.1 70B-Ins                   & 40.26                                 & \textbf{52.59}               & 64.53            & 66.84              & 61.42                      & 61.21             & 55.89            \\

                  &                   &                   & Llama 3.1 405B-Ins                   & 46.90                                 & 51.68               & 70.27            & \textbf{67.95}              & \textbf{63.74 }                     & 64.23             & \textbf{59.56}        \\  \cdashline{2-11}
                  & \multirow{6}{*}{\shortstack{Graph\\Explore}} &      \multirow{3}{*}{Score}             & Llama 3.1 8B-Ins                   & 24.49                                & 36.80              & 35.81            & 36.41              & 34.28                      & 34.49             & 37.69            \\ 
                  &                   &                   & Llama 3.1 70B-Ins                   & 32.79                                & 38.19              & 53.83            & 58.25              & 48.55                      & 52.18                & 48.07           \\
                  &                   &                   & Llama 3.1 405B-Ins                   & 33.90                                & 42.68              & 46.87            & 57.43              & 50.46                      & 55.56                & 48.73           \\ \cdashline{3-11}

                  &                   &     \multirow{3}{*}{Select}              & Llama 3.1 8B-Ins                   & 25.04                                & 37.8              & 36.34            & 38.5              & 32.44                      & 33.31                & 34.85           \\
                  &                   &                   & Llama 3.1 70B-Ins                   & 33.40                                & 39.13              & 54.78            & 58.53              & 47.19   & 51.13                & 47.51           \\
                  &                   &                   & Llama 3.1 405B-Ins                   & 33.82                                & 43.63              & 44.47            & 59.06              & 48.52                      & 55.62                & 46.07           \\
\bottomrule
\end{tabular}
}
\caption{Rouge-L (R-L) performance results on GRBench \cite{graphCoT}, comparing standard LLMs, Text-RAG, Graph-RAG, Graph-CoT, and Graph-ToT. Experiments are described in Section \ref{Section:Experiments}, using LLama 3.1 - Instruct backbone models with sizes 8B, 70B, and 405B-FP8.}
\vspace{-2pt}
\label{Table:Results}
\end{table*}

\paragraph{Agent vs Graph Search} In our experimental results, the agentic method outperformed the graph exploration approach across most datasets and reasoning strategies. The agent-based method, which involves the LLM selecting specific actions to interact with the KG, consistently improves performance as the number of reasoning steps increases, as shown in the Analysis Section \ref{Section:analysis}. This suggests that while graph exploration can quickly provide relevant information, the agentic method's iterative and targeted interactions with the KG yield more accurate and comprehensive answers over longer sequence of steps.

\paragraph{Tree of Thought (ToT)}
The ToT reasoning strategy showed superior performance across its various interaction methods and state evaluators, as summarized in Table \ref{Table:Results}. ToT achieved performance improvements of 54.74\% in agent performance and 11.74\% in exploration mode compared to the CoT version. However, this improvement comes with the trade-off of increased inference time, highlighting the effectiveness of inference-time strategies. In the ToT approach, a state evaluator is used to select the most promising branch for the next step. We also compared the two State Evaluation methods: Selection and Score.

\paragraph{Graph of Thought} 
The results for the GoT strategy are summarized in Table \ref{Table:Results_GoT}. Due to the additional computational time required, we report results for two of the datasets. The GoT strategy did not outperform the ToT approach. Our initial hypothesis was that LLMs could effectively integrate divergent results from different branches. However, in practice, the models struggled to merge these results effectively. Specifically, in the case of Graph-Search, the models often failed to effectively combine different triples found in separate branches. This finding presents an interesting area for future research, potentially leading to the development of more sophisticated reasoning strategies and improved methods for merging results from different branches.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\centering
\resizebox{.407\textwidth}{!}{
\begin{tabular}{cclcc}
\toprule
\multicolumn{2}{c}{\textbf{Method}}                        & \multicolumn{1}{c}{\textbf{Model}}             & \textbf{Healthcare} & \textbf{Biology} \\
\hline
\multirow{6}{*}{\rotatebox{90}{Agent}}  & \multirow{3}{*}{\rotatebox{90}{Score}}  & Llama 3.1 8B-Ins   & 29.11           &    33.25                                                     \\
                        &                         & Llama 3.1 70B-Ins  &   30.88         &    56.64                                                     \\
                        &                         & Llama 3.1 405B-Ins &    43.53        &   48.1                                                      \\
                        \cdashline{2-5}
                        & \multirow{3}{*}{\rotatebox{90}{Select}} & Llama 3.1 8B-Ins   &  29.05          & 40.37                                                         \\
                        &                         & Llama 3.1 70B-Ins  &  40.74          &     65.59                                                    \\
                        &                         & Llama 3.1 405B-Ins &   \textbf{47.63}        &   \textbf{71.49 }                                                     \\
                        \hline
\multirow{6}{*}{\rotatebox{90}{Graph Explore}} & \multirow{3}{*}{\rotatebox{90}{Score}}  & Llama 3.1 8B-Ins   &   24.96         &      21.72                                                   \\
                        &                         & Llama 3.1 70B-Ins  &   31.24         &     50.70                                                    \\
                        &                         & Llama 3.1 405B-Ins & 35.00        &   39.10                                                      \\
                        \cdashline{2-5}
                        & \multirow{3}{*}{\rotatebox{90}{Select}} & Llama 3.1 8B-Ins   &  25.06          &   21.84                                                      \\
                        &                         & Llama 3.1 70B-Ins  &   36.95         &      52.32                                                   \\
                        &                         & Llama 3.1 405B-Ins &   33.74         &   54.64        
                        \\
                        \bottomrule
\end{tabular}
}
\caption{Graph-GoT results on GRBench using Rouge-L with Llama 3.1 Instruct sizes 8B, 70B, and 405B.}
\label{Table:Results_GoT}
\end{table}

\section{Analysis \& Ablation Studies}
\label{Section:analysis}

We conduct the analysis on the Academic datasets from the benchmark, as they all contain the same number of samples and the questions are generated from similar templates.

\begin{figure}[ht]
\centering
\includegraphics[width=0.82\linewidth]{figures/analysis_steps_academic_rl.png}
\caption{Effect of the number of steps in the LLM-KG Interaction Methods. The Agent requires more steps to obtain the performance of the Graph Exploration, while the Graph Exploration only needs the anchor entities to perform the search within the graph.}
\label{fig:analysis-steps}
\end{figure}

\paragraph{How does the number of steps affect the results?}
Figure \ref{fig:analysis-steps} illustrates the effect of varying the number of steps in the KG interaction methods (Agent, Explore) across all academic datasets. The plots indicate that graph exploration performs better with fewer steps, as it automatically traverses the graph for the identified anchor entities. Conversely, the agentic methods improve as the number of steps increases, eventually achieving better performance. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/search_depth_academic_rl.png}
    \caption{Effect of the Search depth in Graph Exploration interaction method for a fixed steps number. The method can achieve relatively good performance with the anchor entities extracted from the question.}
    \label{fig:search-depth}
    \vspace{-10pt}
\end{figure}

\paragraph{What is the effect of the Search Depth in the Automatic Graph Exploration?}
We analyze the effect of search depth in Figure \ref{fig:search-depth}, which presents the performance results across various depths, with a fixed step size of one. The results demonstrate that the performance of the depth-first search plateaus at a depth of 3, highlighting the relevance of search exploration with respect to the given query.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/tree_width_academic_rl.png}
    \caption{Impact of tree width on Agentic ToT performance. It shows a general trend of performance improvement with increasing tree width.}
    \label{fig:tree-width}
\end{figure}

\paragraph{What is the effect of the tree width in the reasoning strategy (ToT)?}
Based on experimental results across all academic datasets, we observe performance variations among the different methods. To gain further insight, we analyze the effect of tree width on the results, as shown in Figure 5. We notice a slight upward trend in performance as the tree width increases, although the difference is more pronounced between CoT and ToT itself, going from one branch to two. The added computational time and resources likely contribute to this performance enhancement.

\paragraph{What is the influence of the state evaluator?}
Figure \ref{fig:state_evaluators} explores the impact of state evaluators, specifically Score and Select, within the ToT framework. The analysis indicates that, while there is no significant difference between the two methods, the Select evaluator generally yields slightly better results. This trend is especially evident in the context of the Agent's performance, though the advantage is less pronounced in automatic graph exploration. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/state_evaluators.png}
    \caption{Influence of the State Evaluators in ToT. The Select method obtains better results in general over Score method.}
    \label{fig:state_evaluators}
    \vspace{-10pt}
\end{figure}


\section{Conclusion}

% This paper explores the grounding of LLM reasoning with Knowledge Graphs by integrating each reasoning step with a connection or search within the graph. We introduced a comprehensive framework that combines various reasoning strategies, including Chain-of-Thought, Tree-of-Thought,  and Graph-of-Thought, with search methods to combine their reasoning process with the graphs. Our evaluation on the GRBench benchmark demonstrated state-of-the-art results, highlighting the effectiveness of our approach in domain-specific question-answering tasks. We observe the superior performance of the agentic method over graph exploration, the enhanced capabilities of the Tree-of-Thought strategy, and the potential for more sophisticated reasoning algorithms if we correctly integrate merging and feedback from different reasoning paths.

% The integration of KGs with LLMs is crucial for grounding LLMs to specific domains without the need for further training at inference time. This approach not only leverages the structured knowledge within KGs but also harnesses the generative capabilities of LLMs, providing a robust framework for accurate and reliable information retrieval. As demonstrated in our work, the importance of inference-time strategies shows the potential for real-time, adaptive reasoning that can be tailored to various specialized domains. This research paves the way for future advancements in combining structured knowledge with generative models, offering promising applications in fields such as healthcare, academia, and e-commerce.

% This paper presents a framework that integrates Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning capabilities, particularly for domain-specific question-answering tasks

This paper investigates the grounding of LLM reasoning with KGs by integrating each reasoning step with the graph. We introduce a comprehensive framework that merges various reasoning strategies, such as CoT, ToT, and GoT, with graph search methods to leverage the structured knowledge in KGs alongside the generative capabilities of LLMs. Evaluations on the GRBench benchmark demonstrate state-of-the-art results, highlighting the effectiveness of this approach in domain-specific question-answering tasks. Additionally, the framework is flexible enough to incorporate multiple reasoning algorithms.  


Integrating KGs with LLMs is essential for grounding LLMs in specific domains without requiring additional training. This approach enables adaptive reasoning at inference time, tailored to domains like healthcare, academia, and e-commerce. Moreover, this work highlights the potential for future advancements in combining structured knowledge from KGs with LLMs' generative capabilities.


\section{Limitations}

In this work, we demonstrate how LLMs can be used to explore a graph while conditioning the next steps based on the graph's results. We show that the two approaches presented achieve superior results in graph exploration. Integrating KGs with LLMs can provide complex relational knowledge for LLMs to leverage. However, the system's performance will depend on the knowledge encoded in the graph and the models' capabilities.

Extending inference-time reasoning methods for LLMs is significantly constrained by computational resources and the time available to the user. This is a common problem in LLM research. Additionally, this work is constrained by the computational resources needed to load the graph into memory, as it demands a significant amount of available RAM.

While LLMs conditioned on external knowledge can generate outputs based on accessed content, their generated output is not strictly limited to that information. Thus, they may still generate hallucinated content. This work represents a step forward in mitigating such cases by providing the necessary content.

\section{Ethics Statement}

This paper advances the integration of Knowledge Graphs and Large Language Models to enhance their capability for domain-specific question answering. This work can facilitate the development of chatbots that provide responses limited to certain answers, aiding in meeting industry-standard protocols and ensuring adherence to privacy guidelines.

By anchoring the models reasoning processes in KGs, we aim to increase the transparency of LLM outputs. Each reasoning step is grounded in structured, retrievable information, enabling users to trace the origin of a given response. We admit that fully interpretable AI remains a challenge, and transparency remains a primary focus for better relying on such complex models.

\section{Acknowledgements}

This paper was prepared for informational purposes in part by the CDAO group of JPMorgan Chase \& Co and its affiliates ("J.P. Morgan") and is not a product of the Research Department of J.P. Morgan. J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.\\
 2025 JPMorgan Chase \& Co. All rights reserved

% Entries for the entire Anthology, followed by custom entries
\bibliography{bibliography.bib}
\bibliographystyle{acl_natbib}

\onecolumn
\appendix


\section{GRBench Statistics}


Detailed statistics of the graphs in GRBench \cite{graphCoT} are shown in Table \ref{Table:dataset_statistics}. \textbf{Academic Graphs} contain 3 types of nodes: paper, author, venue. \textbf{Literature Graphs} contain 4 types of nodes: book, author, publisher and series. \textbf{Healthcare Graph} contains 11 types of nodes: anatomy, biological process, cellular component, compound, disease, gene, molecular function, pathway, pharmacologic class, side effect, and symptom. Questions are created according to multiple templates labeled as easy, medium, and hard, depending on the number of nodes required to give the answer.

\begin{table}[h]
\centering
\resizebox{0.73\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multirow{2}{*}{\textbf{Domain}}   & \multirow{2}{*}{\textbf{Topic}} & \multicolumn{2}{c}{\textbf{Graph Statistics}} & \multicolumn{2}{c}{\textbf{Data}}    \\ \cline{3-6} 
                          &                        & \textbf{\# Nodes}          & \textbf{\# Edges }        & \textbf{\# Templates} & \textbf{\# Questions} \\
\hline
\multirow{5}{*}{Academic} & Biology                & $\sim$4M          & $\sim$39M        & 14           & 140          \\ \cline{2-6} 
                          & Chemistry              & $\sim$4M          & $\sim$30M        & 14           & 140          \\ \cline{2-6} 
                          & Material Science       & $\sim$3M          & $\sim$22M        & 14           & 140          \\ \cline{2-6} 
                          & Medicine               & $\sim$6M          & $\sim$30M        & 14           & 140          \\ \cline{2-6} 
                          & Physics                & $\sim$2M          & $\sim$33M        & 14           & 140          \\
\hline
Literature                & Goodreads              & $\sim$3M          & $\sim$22M        & 24           & 240          \\
\hline
Healthcare                & Disease                & $\sim$47K         & $\sim$4M         & 27           & 270          \\
\hline
\cellcolor{gray!25} \textbf{SUM}                       & \cellcolor{gray!25} -                      & \cellcolor{gray!25} -                 & \cellcolor{gray!25} -                &  \cellcolor{gray!25} \textbf{121}          & \cellcolor{gray!25} \textbf{1210}       \\
\bottomrule
\end{tabular}
}
\caption{Detailed statistics of the GRBench \cite{graphCoT}.}
\label{Table:dataset_statistics}
\end{table}


\section{LLM <--> KG Interaction Pipelines}
\label{App:pipelines}

Description of the two LLM + KG Interaction Pipelines in their CoT form:
\begin{enumerate}[nosep]
    \item Agent -- Figure \ref{fig:agent_pipeline} 
    \item Automatic Graph Exploration -- Figure \ref{fig:graph_explore_pipeline}
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/agent_pipeline.png}
    \caption{Agent Pipeline: (1) Input Query, (2) Thought Generation (3) Action Selection, (4) Environment Observation from the Knowledge Graph. The process is repeated until termination action is generated or limit reached.}
    \label{fig:agent_pipeline}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/graph_explore_pipeline.png}
    \caption{Automatic Graph Exploration Pipeline: (1) Input Query, (2) Thought Generation, (3) Entity Extraction (from query or thought with LLM), (4) Automatic Graph Search as described in Algorithm \ref{alg:graph_exploration_algorithm} (5) Query LLM for answer or continue}
    \label{fig:graph_explore_pipeline}
\end{figure}

\newpage

Algorithm \ref{alg:graph_exploration_algorithm} presents the pseudocode for the Automatic Graph Exploration described in Section \ref{Section:Automatic_Graph_Exploration}.

\begin{algorithm}
\setstretch{0.95}
\caption{Graph Exploration Algorithm}
\label{alg:graph_exploration_algorithm}
\begin{algorithmic}[1]
\Procedure{GraphExplore}{$LLM, seen\_entities, search\_depth$}
    \State $relevant\_attributes$, $found\_triples \gets 0$  
    \For{$depth$ in $search\_depth$}
        \For{$entity$ in $seen\_entities$}
            \If{$seen\_entities[entity\_id].visited$ == True}
                \State Continue
            \Else
                \State $seen\_entities[entity] \gets$ Visited
            \EndIf
            \State $head\_entity\_name$, $entity\_attributes$, $neighbors \gets$  Graph[$entity$]
            \State $pruned\_neighbors \gets$ prune\_relations($LLM, neighbors$)
            \State $pruned\_neighbors \gets$ prune\_entities($LLM, pruned\_neighbors$)
            \State $found\_triples \gets$ generate\_triples($entity, pruned\_neighbros$)
        \EndFor
        \State $seen\_entities \gets$ Update($seen\_entities$, neighbors)
        \If{End?($LLM, found\_triples, relevant\_attributes$) == True}
            \State break
        \EndIf
    \EndFor
    \State \Return $found\_triples, relevant\_attributes, seen\_entities$
\EndProcedure

\end{algorithmic}

\end{algorithm}


\section{Performance results in plots}

Figures \ref{fig:plot_result_rougeL_healthcare} and \ref{fig:plot_result_gpt4score_healthcare} illustrate the performance results using the Rouge-L and GPT4score metrics, respectively, for the healthcare graph for all methods. The results were run on the LLama 3.1 Instruct models (8B, 70B, and 405B-FP8) and demonstrate the improved performance achieved through more complex reasoning and search strategies during inference.


\begin{figure}[h]
    \centering
    \includegraphics[width=.96\linewidth]{figures/healthcare_rl_all.png}
    \caption{Performance results using the Rouge-L metric on the healthcare graph of GRBench \cite{graphCoT}, comparing all methods with LLama 3.1 Instruct models of various sizes (8B, 70B, 405B-FP8). Experimental details are included in Section \ref{Section:Experiments}.}
    \label{fig:plot_result_rougeL_healthcare}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/healthcare_gpt4score_all.png}
    \caption{Performance results using the GPT4Score metric on the healthcare graph of GRBench \cite{graphCoT}, comparing all methods with LLama 3.1 Instruct models of various sizes (8B, 70B, 405B-FP8). Experimental details are included in Section \ref{Section:Experiments}.}
    \label{fig:plot_result_gpt4score_healthcare}
\end{figure}

\newpage

\section{Results on GPT4Score}
\label{App:results-gpt4score}

In this section, we present the results of the experiments described in Section \ref{Section:Experiments} for all methods, using the GPT4Score metric. This metric calculates the percentage of "correct" answers as judged by GPT-4 when presented with both the correct and the generated answer. The tables in this section present the same data as in Tables \ref{Table:Results} and \ref{Table:Results_GoT}, but evaluated using GPT4Score.

\begin{table}[h]
\centering
\resizebox{.95\textwidth}{!}{
\begin{tabular}{llllccccccc}
\toprule
& \textbf{Method} & & \multicolumn{1}{c}{\textbf{Model}} & \textbf{Healthcare} & \textbf{Goodreads} & \textbf{Biology} & \textbf{Chemistry} & \textbf{Materials Science} & \textbf{Medicine} & \textbf{Physics} \\
                  \hline
\multirow{9}{*}{\rotatebox{90}{\shortstack{Baselines}}} & \multicolumn{2}{l}{}                  & Llama 3.1 8B-Ins                   & 11.85                                & 13.33             & 10.71            & 11.43              & 7.86                      & 7.87            & 9.29            \\
& \multicolumn{2}{l}{Base}                  & Llama 3.1 70B-Ins                   & 12.96                                 & 19.17               & 10.00            & 12.14              & 11.43                       & 11.43             & 12.86            \\
& \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins                   & 15.55                               & 26.67              & 12.86           & 12.14              & 12.14                     & 13.57            & 12.14            \\

\cdashline{2-11}
& \multicolumn{2}{l}{}                  & Llama 3.1 8B-Ins                   & 11.85                               & 21.67              & 12.86            & 10.00              & 10.00                      & 8.57             & 7.86            \\
                  & \multicolumn{2}{l}{Text-RAG}                  & Llama 3.1 70B-Ins                   & 12.22                                & 27.5              & 12.14            & 13.57             & 13.57                      & 13.57             & 12.86            \\
                  & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins           & 12.96                                & 26.67              & 15.00           & 13.57              & 12.86                      & 14.29           & 13.57            \\ \cdashline{2-11}
                  & \multicolumn{2}{l}{}                  & Llama 3.1 8B-Ins                   & 14.81                               & 32.50             & 29.29               & 29.28              & 27.86                       & 25.71             & 29.29            \\
                  & \multicolumn{2}{l}{Graph-RAG}                  & Llama 3.1 70B-Ins                   & 17.04                                & 32.92              & 39.29               & 40.71              & 43.57                      & 34.29             & 40.00    \\
                  & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins         & 18.15                                & 31.67             & 37.14               & 42.86              & 40.00                       & 36.43             & 41.43 \\
                  \hline \hline
 \multirow{6}{*}{\rotatebox{90}{\shortstack{Graph\\CoT}}} & \multicolumn{2}{l}{}                  & Llama 3.1 8B-Ins                   & 18.15                                & 32.5               & 20.71            & 19.28               & 25.00                      & 14.29            & 21.43            \\
 & \multicolumn{2}{l}{Agent}                  & Llama 3.1 70B-Ins                   & 32.59                               & 43.75              & 50.00            & 51.43              & 50.00                      & 48.57            & 46.43           \\ 
 & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins                  & 28.89                              & 48.33              & 38.57            & 38.57             & 47.86                     & 56.43            & 34.29           \\ \cdashline{2-11}
                  & \multicolumn{2}{l}{ \multirow{3}{*}{\shortstack{Graph\\Explore}}}                    & Llama 3.1 8B-Ins                   & 22.22                                & 36.67              & 35.00            & 30.71              & 29.29                      & 29.29                & 32.86            \\
                &  \multicolumn{2}{l}{}              & Llama 3.1 70B-Ins                   & 27.78                                & 32.92            & 45.71          & 49.29             & 40.00                     & 40.00             & 44.29            \\
                & \multicolumn{2}{l}{}                  & Llama 3.1 405B-Ins                   & 28.89                               & 48.33            & 38.57       & 38.57              & 47.86                      & 56.43                & 34.29            \\
                  \hline \hline
                  
\multirow{12}{*}{\rotatebox{90}{\shortstack{Graph\\ToT}}} & \multirow{6}{*}{Agent} &    \multirow{3}{*}{Score}          & Llama 3.1 8B-Ins                  & 30.49                                & 55.14              & 43.33            & 41.67               & 44.05                      & 36.43             & 39.52            \\
& &              & Llama 3.1 70B-Ins                  & 30.49                                & 54.48              & 65.48            & 62.14              & 55.95                      & 63.57             & 56.19            \\
& &              & Llama 3.1 405B-Ins                  & \textbf{45.55}                                & 56.53              & \textbf{71.67}            & 65.71               & 52.62                      & 68.81             & 44.76            \\ \cdashline{3-11}

    &                   &        \multirow{3}{*}{Select}           & Llama 3.1 8B-Ins                   & 30.00                                 & 54.17               & 40.71            & 37.14              & 40.00                      & 32.86             & 36.43            \\
    &                   &                  & Llama 3.1 70B-Ins                   & 39.63                                 & 56.67               & 65.00            & 67.14              & 62.86                      & 60.71             & 55.55            \\

                  &                   &                   & Llama 3.1 405B-Ins                   & 44.07                                 & \textbf{58.75}              & 71.43            & \textbf{69.29}              & \textbf{65.00}                      & \textbf{68.81}             & \textbf{60.00}            \\  \cdashline{2-11}
                  & \multirow{6}{*}{\shortstack{Graph\\Explore}} &      \multirow{3}{*}{Score}             & Llama 3.1 8B-Ins                   & 21.48                                & 41.10              & 32.86            & 31.67              & 31.43                      & 32.14             & 35.24            \\ 
                  &                   &                   & Llama 3.1 70B-Ins                   & 24.94                                & 40.97              & 52.38            & 57.86              & 49.29                      & 54.29                & 47.86           \\
                  &                   &                   & Llama 3.1 405B-Ins                   & 30.86                                & 48.33              & 47.86            & 57.14              & 50.71                      & 56.67                & 47.14           \\ \cdashline{3-11}

                  &                   &     \multirow{3}{*}{Select}              & Llama 3.1 8B-Ins   &      21.85    &     41.67                      &   32.86            & 31.67            & 31.43              & 32.14                & 35.24           \\
                  &                   &                   & Llama 3.1 70B-Ins                   & 30.37                                & 42.08              & 54.29            & 57.14              & 47.86                      & 52.14                & 46.43           \\
                  &                   &                   & Llama 3.1 405B-Ins                   & 31.48                                & 48.75              & 45.00            & 57.86              & 48.86                      & 57.14                & 45.71           \\
\bottomrule
\end{tabular}
}
\caption{GPT4Score performance results on GRBench \cite{graphCoT}, comparing standard LLMs, Text-RAG, Graph-RAG, Graph-CoT, and Graph-ToT. Experiments are described in Section \ref{Section:Experiments}, using LLama 3.1 - Instruct backbone models with sizes 8B, 70B, and 405B.}

\label{Table:Results-GPT4Score}
\end{table}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\centering
\resizebox{.407\textwidth}{!}{
\begin{tabular}{cclcc}
\toprule
\multicolumn{2}{c}{\textbf{Method}}                        & \multicolumn{1}{c}{\textbf{Model}}              & \textbf{Healthcare} & \textbf{Biology} \\
\hline
\multirow{6}{*}{\rotatebox{90}{Agent}}  & \multirow{3}{*}{\rotatebox{90}{Score}}  & Llama 3.1 8B-Ins   & 29.88           &    32.86                                                     \\
                        &                         & Llama 3.1 70B-Ins  &   29.51         &    61.69                                                     \\
                        &                         & Llama 3.1 405B-Ins &    41.81        &   48.33                                                     \\
                        \cdashline{2-5}
                        & \multirow{3}{*}{\rotatebox{90}{Select}} & Llama 3.1 8B-Ins   &  30.00          & 40.71                                                         \\
                        &                         & Llama 3.1 70B-Ins  &  39.63          &     69.83                                                    \\
                        &                         & Llama 3.1 405B-Ins &   \textbf{44.81}        &   \textbf{72.86}                                                      \\
                        \hline
\multirow{6}{*}{\rotatebox{90}{Graph Explore}} & \multirow{3}{*}{\rotatebox{90}{Score}}  & Llama 3.1 8B-Ins   &   22.72         &      21.19                                                   \\
                        &                         & Llama 3.1 70B-Ins  &   24.20         &     48.57                                                    \\
                        &                         & Llama 3.1 405B-Ins & 32.22        &   41.67                                                      \\
                        \cdashline{2-5}
                        & \multirow{3}{*}{\rotatebox{90}{Select}} & Llama 3.1 8B-Ins   &  22.59          &  19.28                                                       \\
                        &                         & Llama 3.1 70B-Ins  &   32.96         &      52.86                                                   \\
                        &                         & Llama 3.1 405B-Ins &   31.48         &   57.86        
                        \\
                        \bottomrule
\end{tabular}
}
\caption{Graph-GoT results on GRBench using GPT4Score with Llama 3.1 Instruct sizes 8B, 70B, and 405B.}
\label{Table:Results_GoT_GP4Score}
\end{table}


\newpage

\section{Prompts Archive}

In this section, we gather the necessary prompts essential for implementing the proposed methodologies.

\begin{tcolorbox}[breakable, colframe=black, title=Agent]
\texttt{
\textbf{Agent Step}: Solve a question answering task with interleaving Thought, Interaction with Graph, Feedback from Graph steps. In Thought step, you can think about what further information is needed, and In Interaction step, you can get feedback from graphs with four functions:\\
(1) RetrieveNode[keyword], which retrieves the related node from the graph according to the corresponding query.\\
(2) NodeFeature[Node, feature], which returns the detailed attribute information of Node regarding the given "feature" key.
(3) NodeDegree[Node, neighbor\_type], which calculates the number of "neighbor\_type" neighbors of the node Node in the graph.\\
(4) NeighbourCheck[Node, neighbor\_type], which lists the "neighbor\_type" neighbours of the node Node in the graph and returns them.\\
You may take as many steps as necessary.\\
Here are some examples:\\
\{examples\}\\
Please answer by providing node main feature (e.g., names) rather than node IDs.\\
Generate the next step.\\
Definition of the graph: \{graph\_definition\}\\
Question: \{question\}\\
\{scratchpad\}}
\end{tcolorbox}

\begin{tcolorbox}[breakable, colframe=black, title=Automatic Graph Exploration]
\texttt{\textbf{Search Thought}: Given the previous thoughts, generate the next thought to answer the provided question.\\
Your end goal is to answer the question step by step. 
For context, you are also provided with some knowledge triples from a knowledge base. \\
Follow the format of the examples to generate the next thought.\\
\\
\{examples\}\\
\\
Graph Definition: \{graph\_definition\}\\
Question: \{question\}\\
Knowledge Triples: \\
    \{triples\}\\
Previous thoughts:\\
    \{thoughts\}\\
Related Entity Attributes:\\
    \{attributes\}\\
Next Thought:\\
\\
\textbf{Search End?}: Your are provided with the an original question, the associated subquestion thoughts and their corresponding knowledge graph triples (head\_entity -> relation -> tail\_entity). 
Your task is to answer whether it's sufficient for you to answer the original question (Yes or No).
You are provided with examples. You should follow the same format as in the examples, writing 'Yes' or 'No' within brackets at the beginning of the answer.\\
\textit{(Examples)\\
Task: 
Question: \{\textit{question}\}\\
Thoughts: \{\textit{thoughts}\}\\
Knowledge Triples: \{\textit{triples}\}\\
Entity Attributes: \{\textit{attributes}\}\\
Answer: \\
\\
\textbf{Entity Extraction}: Given the provided text, extract the relevant entities that may appear in a knowledge base.
Return the answer at the end with brackets {{relevant entities}} as shown in the following examples. If there are several entities, separate them with commas.\\
(\textit{Examples})\\
Task:
Text: \{\textit{text}\}\\
Relevant Entities: \\
\\
\textbf{Prune Relations}: From the given entity and relations, select only the relevant relations to answer the question.
Provide the answer at the end with brackets{{answer}} , as shown in the following example.\\
(\textit{Examples})\\
Question: \{\textit{question}\}\\
Head Entity: \{\textit{entity}\}\\
Relations: \{\textit{relations}\}
Answer:\\
\\
\textbf{Prune entities}: You are provided with a question, a head entity, a relation and tail entity or entities from a knowledge base.
Select the tail entity or entities to answer the question.
Return the tail entity or entities at the end with brackets {{relevant entity or entities}}, as shown in the following examples.\\
(\textit{Examples})\\
Question: \{\textit{question}\}\\
Head Entity: \{\textit{head\_entity}\}\\
Relation: \{\textit{relation}\}\\
Tail Entities: \{\textit{tail\_entities}\}\\
Relevant Entities: \\
\\
\textbf{Search Attributes}: Is any of the attributes relevant to answer the question?
Return the answer at the end with brackets {{answer}} , as shown in the following examples.\\
(\textit{Examples})\\
Question: \{\textit{question}\}\\
Entity: \{\textit{entity}\}\\
Attributes: \{\textit{attributes}\}\\
Relevant Attributes: 
}
}
\end{tcolorbox}

\begin{tcolorbox}[breakable, colframe=black, title=State Evaluators]
\textbf{Selection Vote}: Given a question, you need to select the possible chain of thought that may lead to the correct answer with higher probablity. 
You are provided with several choices with thouhgts and related triples from a knowledge base. Decide which choice is most promising to complete the task.
Analyze each choice in detail, then conclude in the last line:
"The best choice is {{s}}" , where s the integer id of the choice. \\
(\textit{Examples})\\
Question: \{\textit{question}\}\\
Choices: \{\textit{choices}\} (+ attributes/triples)\\
Answer: \\
\\
\textbf{Score Vote}: Generate a score for the given reasoning chain. 
The score represents the probability that the chain will lead to the correct answer.
The chains contain interleaved thoughts and related triples from a knowledge base.
Some chains may not be complete, but you need to judge the steps that are provided.
The score can be any floating number between 0 and 1.\\
(\textit{Examples})\\
Question: \{\textit{question}\}\\
Thought Chain: \{\textit{thoughts}\} (+ attributes/triples)\\
Score: 
\end{tcolorbox}

\begin{tcolorbox}[breakable, colframe=black, title=Graph of Thoughts]
\textbf{Graph of Thought Merge}: Generate the next thought for the merged chain of thoughts.
You are provided with the question, two chains of thoughts, and the corresponding merged chain of thought. 
Identify inconsistencies or errors from the previous chains and provide the next thought for the merged chain.
You should follow the same format as in the examples.\\
(\textit{Examples})\\
Question: \{\textit{question}\}\\
Chain 1: \{\textit{chain\_1}\} (+triples/attributes)\\
Chain 2: \{\textit{chain\_2}\} (+triples/attributes)\\
Merged Chain: \{\textit{merged\_chain}\}\\
Next Thought:
\end{tcolorbox}

\end{document}
