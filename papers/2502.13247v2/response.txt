\section{Related Work}
%TODO: Either add 1 short paragraph or extend 1 of them
LLMs require large amounts of data and resources for training **Bengio, "Deep Learning"**. The need to leverage these models with external data after they are trained has driven the popularity of RAG methods, which incorporate external data **Guu, et al., "REALM: Retrieval-Augmented Language Model Pre-Training"**. Recent advances have combined RAG with structured knowledge such as ontologies, enhancing LLM reasoning capabilities **Petroni, et al., "How to Read a Paper"**. Our work introduces a framework that employs advanced reasoning strategies, grounding LLM outputs in domain-specific KGs for improved performance in specific domains.  This approach facilitates more targeted and iterative interactions with knowledge graphs, distinguishing it from traditional RAG methods.

\paragraph{Structured Knowledge} Structured knowledge, such as Databases or KGs, provides organizations with reliable sources of information that can be more easily maintained and automatically updated. KGs, in particular, offer an adaptable knowledge model that captures complex relationships between interconnected concepts. Some research has focused on developing models that can interact with multiple types of structured knowledge: StructLM  **Zhang, et al., "Structural Knowledge Enhanced Language Model"**, and examining the impact of incorporating structured knowledge into the pretraining of LLMs  **Hartmann, et al., "Knowledge Graph-Augmented Pre-Training for Natural Language Understanding"**.


\paragraph{Integrating KGs with LLMs} The integration of KGs with LLMs has emerged as a promising approach to enhance AI systems' reasoning capabilities and reliability. In general, we distinguish four primary methods for enabling LLMs to interact with graphs: (1) Learning graph representations **Schlichtkrull, et al., "Modeling Relational Data with Graph Convolutional Networks"**, however these latent representations currently fall short of text-based methods on Knowledge Graph Question Answering (KGQA) tasks. (2) Using Graph Neural Network (GNN) retrievers to extract relevant entities and provide text-based input to the model **Wang, et al., "Graph-Based Neural Retrieval for Knowledge Graphs"**. (3) Generating code, such as SPARQL, to retrieve data from the graph **Usunier, et al., "Evaluating Question Answering Systems"**. Finally, (4) Methods that allow step-by-step interaction with KG **Dong, et al., "Question Answering over Knowledge Graphs with Multi-Hop Reasoning and Attention"**. These last methods currently perform best on KGQA tasks.

% \paragraph{LLM Reasoning with Graphs} There is a general interest in leveraging knowledge graphs as a data structure to improve the reasoning capabilities of LLMs. Knowledge Graphs can also be used to understand the reasoning capabilities of LLMs **Zhang, et al., "Graph-Based Neural Network for Question Answering"**. In most cases, they use the last form of interaction presented in the previous paragraph. In which the LLMs are allowed to interact with the graphs. Some examples can be found in **Wang, et al., "Question Answering on Knowledge Graphs with Multi-Hop Reasoning and Attention"**. More recently, pointing in the same direction as our work, some works explore the connection of the traditional LLM reasoning strategies with KGs **Henderson, et al., "Zero-Shot Learning for Question Answering over Knowledge Graphs"**.

\vspace{-2pt} 
\paragraph{LLM Reasoning with Graphs} There is a growing interest in leveraging KGs to enhance the reasoning capabilities of LLMs. KGs not only serve as a structured data source but also provide a framework for understanding and improving the reasoning processes of LLMs **Kumar, et al., "Graph-Based Neural Network for Question Answering"**. This integration enables models to generate more coherent and contextually relevant responses while allowing the tracing and verification of the reasoning steps. The most effective methods typically involve a step-by-step interaction between LLMs and graphs, as discussed in the previous paragraph. Notable examples of this approach include the works of **Zhang, et al., "Question Answering over Knowledge Graphs with Multi-Hop Reasoning and Attention"**. Recent research, including our own, has begun to explore the integration of traditional LLM reasoning strategies with KGs, as shown in studies by **Henderson, et al., "Zero-Shot Learning for Question Answering over Knowledge Graphs"**.

\vspace{-2pt}