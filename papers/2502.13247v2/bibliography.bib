

@inproceedings{toolqa,
author = {Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
title = {ToolQA: a dataset for LLM question answering with external tools},
year = {2023},
publisher = {},
address = {},
abstract = {Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available for the broader scientific community on GitHub https://github.com/night-chen/ToolQA.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2180},
numpages = {27},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{rag,
 author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
 year = {2020}
}

@article{hallucination_survey,
 author = {Tonmoy, SM and Zaman, SM and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
 journal = {ArXiv preprint},
 title = {A comprehensive survey of hallucination mitigation techniques in large language models},
 url = {https://arxiv.org/abs/2401.01313},
 volume = {abs/2401.01313},
 year = {2024}
}

@article{llm_explainability_survey,
 author = {Luo, Haoyan and Specia, Lucia},
 journal = {ArXiv preprint},
 title = {From understanding to utilization: A survey on explainability for large language models},
 url = {https://arxiv.org/abs/2401.12874},
 volume = {abs/2401.12874},
 year = {2024}
}

@article{llm_kb,
 author = {Zheng, Danna and Lapata, Mirella and Pan, Jeff Z},
 journal = {ArXiv preprint},
 title = {Large language models as reliable knowledge bases?},
 url = {https://arxiv.org/abs/2407.13578},
 volume = {abs/2407.13578},
 year = {2024}
}

@article{llm_internal_knowledge,
 author = {Ko, Miyoung and Park, Sue Hyun and Park, Joonsuk and Seo, Minjoon},
 journal = {ArXiv preprint},
 title = {Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning},
 url = {https://arxiv.org/abs/2406.19502},
 volume = {abs/2406.19502},
 year = {2024}
}

@article{llm_eval_sql,
 author = {Rajkumar, Nitarshan and Li, Raymond and Bahdanau, Dzmitry},
 journal = {ArXiv preprint},
 title = {Evaluating the text-to-sql capabilities of large language models},
 url = {https://arxiv.org/abs/2204.00498},
 volume = {abs/2204.00498},
 year = {2022}
}


@inproceedings{
rog,
title={Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
author={Linhao Luo and Yuan-Fang Li and Reza Haf and Shirui Pan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ZGNWW7xZ6Q}
}

@article{graphCoT,
 author = {Jin, Bowen and Xie, Chulin and Zhang, Jiawei and Roy, Kashob Kumar and Zhang, Yu and Wang, Suhang and Meng, Yu and Han, Jiawei},
 journal = {ArXiv preprint},
 title = {Graph chain-of-thought: Augmenting large language models by reasoning on graphs},
 url = {https://arxiv.org/abs/2404.07103},
 volume = {abs/2404.07103},
 year = {2024}
}

@article{cot,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
 journal = {Advances in neural information processing systems},
 pages = {24824--24837},
 title = {Chain-of-thought prompting elicits reasoning in large language models},
 volume = {35},
 year = {2022}
}

@article{tot,
 author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
 journal = {Advances in Neural Information Processing Systems},
 title = {Tree of thoughts: Deliberate problem solving with large language models},
 volume = {36},
 year = {2024}
}

@inproceedings{got,
 author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {16},
 pages = {17682--17690},
 title = {Graph of thoughts: Solving elaborate problems with large language models},
 volume = {38},
 year = {2024}
}

@article{demonstrate-search-predict,
 author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
 journal = {ArXiv preprint},
 title = {Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
 url = {https://arxiv.org/abs/2212.14024},
 volume = {abs/2212.14024},
 year = {2022}
}

@article{structLM,
 author = {Zhuang, Alex and Zhang, Ge and Zheng, Tianyu and Du, Xinrun and Wang, Junjie and Ren, Weiming and Huang, Stephen W and Fu, Jie and Yue, Xiang and Chen, Wenhu},
 journal = {ArXiv preprint},
 title = {StructLM: Towards Building Generalist Models for Structured Knowledge Grounding},
 url = {https://arxiv.org/abs/2402.16671},
 volume = {abs/2402.16671},
 year = {2024}
}

@inproceedings{skill_llm_structured_knowledge_infusion,
 address = {Seattle, United States},
 author = {Moiseev, Fedor  and
Dong, Zhe  and
Alfonseca, Enrique  and
Jaggi, Martin},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2022.naacl-main.113},
 pages = {1581--1588},
 publisher = {Association for Computational Linguistics},
 title = {{SKILL}: Structured Knowledge Infusion for Large Language Models},
 url = {https://aclanthology.org/2022.naacl-main.113},
 year = {2022}
}



@inproceedings{
let_graph_do_thinking,
title={Talk like a Graph: Encoding Graphs for Large Language Models},
author={Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=IuXR1CCrSi}
}

@article{graphllm,
 author = {Chai, Ziwei and Zhang, Tianjie and Wu, Liang and Han, Kaiqiao and Hu, Xiaohai and Huang, Xuanwen and Yang, Yang},
 journal = {ArXiv preprint},
 title = {Graphllm: Boosting graph reasoning ability of large language model},
 url = {https://arxiv.org/abs/2310.05845},
 volume = {abs/2310.05845},
 year = {2023}
}


@inproceedings{kb-binder,
    title = "Few-shot In-context Learning on Knowledge Base Question Answering",
    author = "Li, Tianle  and
      Ma, Xueguang  and
      Zhuang, Alex  and
      Gu, Yu  and
      Su, Yu  and
      Chen, Wenhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.385/",
    doi = "10.18653/v1/2023.acl-long.385",
    pages = "6966--6980",
    abstract = "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at \url{https://github.com/ltl3A87/KB-BINDER}."
}


@inproceedings{g-retriever,
 author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {132876--132907},
 publisher = {Curran Associates, Inc.},
 title = {G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/efaf1c9726648c8ba363a5c927440529-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@article{gnn-rag,
 author = {Mavromatis, Costas and Karypis, George},
 journal = {ArXiv preprint},
 title = {GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning},
 url = {https://arxiv.org/abs/2405.20139},
 volume = {abs/2405.20139},
 year = {2024}
}

@article{decaf,
 author = {Yu, Donghan and Zhang, Sheng and Ng, Patrick and Zhu, Henghui and Li, Alexander Hanbo and Wang, Jun and Hu, Yiqun and Wang, William and Wang, Zhiguo and Xiang, Bing},
 journal = {ArXiv preprint},
 title = {Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases},
 url = {https://arxiv.org/abs/2210.00063},
 volume = {abs/2210.00063},
 year = {2022}
}

@article{chatkbqa,
 author = {Luo, Haoran and Tang, Zichen and Peng, Shiyao and Guo, Yikai and Zhang, Wentai and Ma, Chenghao and Dong, Guanting and Song, Meina and Lin, Wei and others},
 journal = {ArXiv preprint},
 title = {Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models},
 url = {https://arxiv.org/abs/2310.08975},
 volume = {abs/2310.08975},
 year = {2023}
}


@inproceedings{mindmap,
    title = "{M}ind{M}ap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
    author = "Wen, Yilin  and
      Wang, Zifeng  and
      Sun, Jimeng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.558/",
    doi = "10.18653/v1/2024.acl-long.558",
    pages = "10370--10388",
    abstract = "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named MindMap, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question {\&} answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference."
}


@inproceedings{
think-on-graph,
title={Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph},
author={Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Saizhuo Wang and Chen Lin and Yeyun Gong and Lionel Ni and Heung-Yeung Shum and Jian Guo},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=nnVO1PvbTv}
}


@inproceedings{kg-gpt,
    title = "{KG}-{GPT}: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
    author = "Kim, Jiho  and
      Kwon, Yeonsu  and
      Jo, Yohan  and
      Choi, Edward",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.631/",
    doi = "10.18653/v1/2023.findings-emnlp.631",
    pages = "9410--9421",
    abstract = "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs."
}



@inproceedings{tree-of-traversals,
    title = "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
    author = "Markowitz, Elan  and
      Ramakrishna, Anil  and
      Dhamala, Jwala  and
      Mehrabi, Ninareh  and
      Peris, Charith  and
      Gupta, Rahul  and
      Chang, Kai-Wei  and
      Galstyan, Aram",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.665/",
    doi = "10.18653/v1/2024.acl-long.665",
    pages = "12302--12319",
    abstract = "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at https://github.com/amazon-science/tree-of-traversals"
}

@article{auto-correct-llm,
 author = {Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang},
 journal = {ArXiv preprint},
 title = {Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies},
 url = {https://arxiv.org/abs/2308.03188},
 volume = {abs/2308.03188},
 year = {2023}
}


@article{react,
  publtype={informal},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  year={2022},
  cdate={1640995200000},
  journal={CoRR},
  volume={abs/2210.03629},
  url={https://doi.org/10.48550/arXiv.2210.03629},
}

@article{rag_sruvey,
 author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
 journal = {ArXiv preprint},
 title = {Retrieval-augmented generation for large language models: A survey},
 url = {https://arxiv.org/abs/2312.10997},
 volume = {abs/2312.10997},
 year = {2023}
}

@article{faiss,
 author = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
 journal = {IEEE Transactions on Big Data},
 number = {3},
 pages = {535--547},
 publisher = {IEEE},
 title = {Billion-scale similarity search with {GPUs}},
 volume = {7},
 year = {2019}
}

@inproceedings{vllm,
 author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
 booktitle = {Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
 title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
 year = {2023}
}

@book{design_analysis_algorithms,
 author = {Aho, Alfred V and Hopcroft, John E},
 publisher = {Pearson Education India},
 title = {The design and analysis of computer algorithms},
 year = {1974}
}

@article{llama3models,
 author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
 journal = {ArXiv preprint},
 title = {The llama 3 herd of models},
 url = {https://arxiv.org/abs/2407.21783},
 volume = {abs/2407.21783},
 year = {2024}
}

@article{villalobos2024will,
 author = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
 journal = {ArXiv preprint},
 title = {Will we run out of data? Limits of LLM scaling based on human-generated data},
 url = {https://arxiv.org/abs/2211.04325},
 volume = {abs/2211.04325},
 year = {2022}
}

@article{li2024structrag,
 author = {Li, Zhuoqun and Chen, Xuanang and Yu, Haiyang and Lin, Hongyu and Lu, Yaojie and Tang, Qiaoyu and Huang, Fei and Han, Xianpei and Sun, Le and Li, Yongbin},
 journal = {ArXiv preprint},
 title = {StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization},
 url = {https://arxiv.org/abs/2410.08815},
 volume = {abs/2410.08815},
 year = {2024}
}

@article{peng2024graph,
 author = {Peng, Boci and Zhu, Yun and Liu, Yongchao and Bo, Xiaohe and Shi, Haizhou and Hong, Chuntao and Zhang, Yan and Tang, Siliang},
 journal = {ArXiv preprint},
 title = {Graph retrieval-augmented generation: A survey},
 url = {https://arxiv.org/abs/2408.08921},
 volume = {abs/2408.08921},
 year = {2024}
}



@inproceedings{
wang2024understanding,
title={Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation},
author={Xinyi Wang and Alfonso Amayuelas and Kexun Zhang and Liangming Pan and Wenhu Chen and William Yang Wang},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=dZsEOFUDew}
}


@inproceedings{rouge_metric,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}