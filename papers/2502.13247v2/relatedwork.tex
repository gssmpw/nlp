\section{Related Work}
%TODO: Either add 1 short paragraph or extend 1 of them

LLMs require large amounts of data and resources for training \cite{villalobos2024will}. The need to leverage these models with external data after they are trained has driven the popularity of RAG methods, which incorporate external data \citep{rag, demonstrate-search-predict}. Recent advances have combined RAG with structured knowledge such as ontologies, enhancing LLM reasoning capabilities \cite{li2024structrag}. Our work introduces a framework that employs advanced reasoning strategies, grounding LLM outputs in domain-specific KGs for improved performance in specific domains.  This approach facilitates more targeted and iterative interactions with knowledge graphs, distinguishing it from traditional RAG methods.

\paragraph{Structured Knowledge} Structured knowledge, such as Databases or KGs, provides organizations with reliable sources of information that can be more easily maintained and automatically updated. KGs, in particular, offer an adaptable knowledge model that captures complex relationships between interconnected concepts. Some research has focused on developing models that can interact with multiple types of structured knowledge: StructLM  \citep{structLM}, and examining the impact of incorporating structured knowledge into the pretraining of LLMs  \citep{skill_llm_structured_knowledge_infusion}. 


\paragraph{Integrating KGs with LLMs} \citep{peng2024graph} The integration of KGs with LLMs has emerged as a promising approach to enhance AI systems' reasoning capabilities and reliability. In general, we distinguish four primary methods for enabling LLMs to interact with graphs: (1) Learning graph representations \citep{let_graph_do_thinking, graphllm}, however these latent representations currently fall short of text-based methods on Knowledge Graph Question Answering (KGQA) tasks. (2) Using Graph Neural Network (GNN) retrievers to extract relevant entities and provide text-based input to the model \citep{g-retriever, gnn-rag}. (3) Generating code, such as SPARQL, to retrieve data from the graph \citep{kb-binder}. Finally, (4) Methods that allow step-by-step interaction with KG \citep{rog, decaf, chatkbqa}. These last methods currently perform best on KGQA tasks.

% \paragraph{LLM Reasoning with Graphs} There is a general interest in leveraging knowledge graphs as a data structure to improve the reasoning capabilities of LLMs. Knowledge Graphs can also be used to understand the reasoning capabilities of LLMs \cite{wang2024understanding}. In most cases, they use the last form of interaction presented in the previous paragraph. In which the LLMs are allowed to interact with the graphs. Some examples can be found in \citet{think-on-graph, mindmap, rog, kg-gpt}. More recently, pointing in the same direction as our work, some works explore the connection of the traditional LLM reasoning strategies with KGs \citet{graphCoT, tree-of-traversals}.

\vspace{-2pt} 
\paragraph{LLM Reasoning with Graphs} There is a growing interest in leveraging KGs to enhance the reasoning capabilities of LLMs. KGs not only serve as a structured data source but also provide a framework for understanding and improving the reasoning processes of LLMs \cite{wang2024understanding}. This integration enables models to generate more coherent and contextually relevant responses while allowing the tracing and verification of the reasoning steps. The most effective methods typically involve a step-by-step interaction between LLMs and graphs, as discussed in the previous paragraph. Notable examples of this approach include the works of \citet{think-on-graph, mindmap, rog, kg-gpt}. Recent research, including our own, has begun to explore the integration of traditional LLM reasoning strategies with KGs, as shown in studies by \citet{graphCoT, tree-of-traversals}.

\vspace{-2pt}