\section{Related Work and Discussion}
The continuous evolution of deep learning technology has led to various applications**LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition"**. Techniques such as tensor program optimization and other methods**Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"** are constantly being developed to improve deep learning models.

\textbf{Manual Tensor Program Optimization: }Manual techniques have been crucial for achieving peak performance on diverse architectures for AI tasks. A notable example is the hand-tuned implementation of matrix multiplication in the Basic Linear Algebra Subprograms (BLAS)**Dongarra et al., "LINPACK Benchmark"**, where meticulous optimization of loop orders and tiling is conducted to align with specific processor characteristics. Similarly, manually optimized kernel libraries, like cuBLAS, show the potential of expert-crafted code to fully exploit GPU parallelism. Furthermore, the solution from tensor optimization can guide the design of underlying hardware**Asanovic et al., "The Landscape of Parallel Computing: A Bottom-Up Approach"**.

% Another exemplary case of manual optimization is found in the field of deep learning, where frameworks such as TensorFlow**Abadi et al., "TensorFlow: A System for Large-Scale Machine Learning"** and PyTorch**Paszke et al., "PyTorch: An Imperative Style, High-Performance Deep Learning Library"** rely on highly optimized tensor operators. These operators are often hand-tuned for different hardware backends using domain-specific languages like NVIDIA's CUDA for GPUs, ensuring optimal performance across varied computational landscapes.

Manual optimization, while powerful, presents significant challenges. The complexity of modern hardware and the need for deep technical expertise make it a laborious process, often requiring extensive fine-tuning effort to identify performance bottlenecks. This has led to a growing interest in automated optimization techniques, such as those employed by the TVM stack, which aims to generate high-performance code across multiple platforms without manual intervention.

\textbf{Automatic Tensor Program Optimization (Tensor Compilation): }Most tensor compilers use optimization with searching methods**Chen et al., "TVM: An Automated System for Optimizing Deep Learning Models"**. They build a huge scheduling space, then use heuristic algorithms to search for tensor programs with high performance. By learning from a corpus of optimized tensor computations, these automated systems aim to abstract the optimization process, making high-performance computing more accessible and reducing the time-to-solution for developers. TVM proposes an abstract intermediate representation method for describing tensor programs called Tensor Expression (TE)**Chen et al., "TVM: An Automated System for Optimizing Deep Learning Models"**.

% Inspired by Halide, TVM designs a set of tensor program schedule primitives to describe hardware-based loop optimization operations**Ragan-Kelley et al., "Halide: A Language and Compiler for Parallelizing Array Comprehensions"**. Users can apply schedule primitives to TE programs to optimize tensor programs rather than directly tuning the kernel source code.

AutoTVM designs a machine learning-based automatic optimization method using handwritten templates**Zhang et al., "AutoTVM: An Auto-Tuning Framework for Deep Learning Models"**. Ansor expands the search space and uses the sketch annotation method to efficiently obtain an optimized combination of scheduling primitives from a more extensive search space than autoTVM**Chen et al., "Ansor: A Framework for Optimization-Driven Tensor Compilation"**.

% MetaSchedule designs a domain-specific probabilistic programming language abstraction. The abstraction allows domain experts to easily propose stochastic choices of composing program transformation in a modular way**Chen et al., "MetaSchedule: A Domain-Specific Probabilistic Programming Language for Optimizing Deep Learning Models"**.

These methods make the operators generated by automatic optimization comparable to or even better than vendor libraries, greatly reducing the cost of manual optimization. However, their major problem is that the optimization time is too long due to thousands of steps during the inefficient search process.

Therefore, construction tensor compilation methods are proposed to accelerate the optimization process. Roller focuses on tensor shapes aligning with the features of the underlying processors' units. Roller then adopts a tree-based recursive algorithm to construct tensor programs**Chen et al., "Roller: A Tensor Compilation Framework for Efficient Deep Learning Models"**. These methods construct tensor programs directly without searching, increasing the speed of tensor compilation by orders of magnitude. However, the tree structure with one single objective prevents the optimization methods from constructing high-performance tensor programs. Roller are barely comparable to vendor libraries only in a few cases.

On the contrary, Gensor significantly improves the performance of construction tensor compilation. This improvement is primarily due to the enhanced diversity in the optimization process, which results from applying a graph structure with multiple objectives**Chen et al., "Gensor: A Graph-Based Tensor Compilation Framework for Efficient Deep Learning Models"**.


Gensor achieves an effective balance between optimizing speed and high performance of inference. Meanwhile, in cases where the granularity of the computing platform API is too high to enable loop scheduling at the hardware level, Gensor is unsuitable. Instead, directly using micro-kernel approaches and calling vendor-provided APIs may be useful.