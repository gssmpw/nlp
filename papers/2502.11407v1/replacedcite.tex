\section{Related Work and Discussion}
The continuous evolution of deep learning technology has led to various applications________. Techniques such as tensor program optimization and other methods____ are constantly being developed to improve deep learning models.

\textbf{Manual Tensor Program Optimization: }Manual techniques have been crucial for achieving peak performance on diverse architectures for AI tasks. A notable example is the hand-tuned implementation of matrix multiplication in the Basic Linear Algebra Subprograms (BLAS)____, where meticulous optimization of loop orders and tiling is conducted to align with specific processor characteristics. Similarly, manually optimized kernel libraries, like cuBLAS, show the potential of expert-crafted code to fully exploit GPU parallelism. Furthermore, the solution from tensor optimization can guide the design of underlying hardware____.

% Another exemplary case of manual optimization is found in the field of deep learning, where frameworks such as TensorFlow____ and PyTorch____ rely on highly optimized tensor operators. These operators are often hand-tuned for different hardware backends using domain-specific languages like NVIDIA's CUDA for GPUs, ensuring optimal performance across varied computational landscapes.

Manual optimization, while powerful, presents significant challenges. The complexity of modern hardware and the need for deep technical expertise make it a laborious process, often requiring extensive fine-tuning effort to identify performance bottlenecks. This has led to a growing interest in automated optimization techniques, such as those employed by the TVM stack, which aims to generate high-performance code across multiple platforms without manual intervention. 

\textbf{Automatic Tensor Program Optimization (Tensor Compilation): }Most tensor compilers use optimization with searching methods____. They build a huge scheduling space, then use heuristic algorithms to search for tensor programs with high performance. By learning from a corpus of optimized tensor computations, these automated systems aim to abstract the optimization process, making high-performance computing more accessible and reducing the time-to-solution for developers. TVM proposes an abstract intermediate representation method for describing tensor programs called Tensor Expression (TE)____. 
% Inspired by Halide, TVM designs a set of tensor program schedule primitives to describe hardware-based loop optimization operations____. Users can apply schedule primitives to TE programs to optimize tensor programs rather than directly tuning the kernel source code. 
AutoTVM designs a machine learning-based automatic optimization method using handwritten templates____. Ansor expands the search space and uses the sketch annotation method to efficiently obtain an optimized combination of scheduling primitives from a more extensive search space than autoTVM____. 
% MetaSchedule designs a domain-specific probabilistic programming language abstraction. The abstraction allows domain experts to easily propose stochastic choices of composing program transformation in a modular way____. 
These methods make the operators generated by automatic optimization comparable to or even better than vendor libraries, greatly reducing the cost of manual optimization. However, their major problem is that the optimization time is too long due to thousands of steps during the inefficient search process.

Therefore, construction tensor compilation methods are proposed to accelerate the optimization process. Roller focuses on tensor shapes aligning with the features of the underlying processors' units. Roller then adopts a tree-based recursive algorithm to construct tensor programs____. These methods construct tensor programs directly without searching, increasing the speed of tensor compilation by orders of magnitude. However, the tree structure with one single objective prevents the optimization methods from constructing high-performance tensor programs. Roller are barely comparable to vendor libraries only in a few cases.
On the contrary, Gensor significantly improves the performance of construction tensor compilation. This improvement is primarily due to the enhanced diversity in the optimization process, which results from applying a graph structure with multiple objectives.


Gensor achieves an effective balance between optimizing speed and high performance of inference. Meanwhile, in cases where the granularity of the computing platform API is too high to enable loop scheduling at the hardware level, Gensor is unsuitable. Instead, directly using micro-kernel approaches and calling vendor-provided APIs may be useful.