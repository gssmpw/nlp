\section{Background and Related Work}
%%%% GG: way too broad, can be omitted completely 
%\rparagraph{LLMs and AI Safety.}
%Transformer-based LLMs **Brown et al., "Transformers Pretraining Techniques"** have revolutionized a wide range of applications, from conversational agents to knowledge retrieval and content generation **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. However, these models are trained with causal language modeling objective, which often misaligns with real-world user expectations, leading to issues like bias, toxicity, and hallucinations **Stahlhofen et al., "On the Robustness of Pre-trained Models to Adversarial Attacks"**. Fine-tuning techniques like reinforcement learning from human feedback (RLHF) **Lewis et al., "Reinforcement Learning from Human Feedback for Dialogue Generation"** have improved alignment with user preferences, but challenges persist in knowledge-intensive tasks ____, where hallucinations pose significant risks ____.

We provide a brief overview of the body of related work on (1) hallucination detection models and (2) benchmarks for evaluating LLM hallucination. 


\rparagraph{Hallucination Detection.}
%
Coarsely, LLM hallucinations fall into two categories. \textit{Intrinsic} hallucination are content contradicts some reference information source. The reference may be explicitly given to the LLM as part of the task (e.g., the text to be summarized in summarization or source language text in machine translation) or it may implicit (e.g., general world knowledge in question answering). In contrast, \textit{extrinsic} hallucination refers to content that does not contradict the reference but is unnecessary or superfluous with respect to the task (e.g., additional facts in fact-based question answering) **Ribeiro et al., "Beyond Accuracy: Behavioral Testing of NLP Models with Checklist"**. Recent work introduced finer-grained taxonomies for both categories. For example, **Chang et al., "Intrinsic and Extrinsic Hallucinations in Language Models"** distinguish between several types of intrinsic hallucinations (e.g., entity-based hallucinations or relation-based hallucinations). In a similar vein, extrinsic hallucinations are split into subtypes such as \textit{invented}, \textit{subjective}, and \textit{unverifiable} content. 

Unsurprisingly, most hallucination detection (and classifications) models are based on neural languages models. These are are either pre-trained encoder LM **Lample et al., "Large Scale Denoising Autoencoders for Unsupervised Pre-training"**, discriminatively fine-tuned to classify texts as containing hallucinations or not or LLMs prompted (zero-shot or with in-context examples) to detect hallucinations **Holtzman et al., "The Curious Case of Neural Text Degeneration"** or fine-tuned to generate hallucinated spans ____. 
%%
In this work, we cast hallucination detection as a span-detection task, formulated discriminatively, with a classifier on top of an ``encoder-based'' LM. However, instead of resorting to small pretrained encoder LMs, we bidirectionally (i.e., discriminatively) fine-tune a larger generative LLM, following recent advances in converting decoder LMs into encoders **Wu et al., "Conditional BERT"**.     
 
%Recent advances have been made where decoder-only models are converted to encoders  by introducing bidirectional mask through either continuous pertaining or downstream fine-tuning and these decoders convert encoders outperform decoder models on natural language understanding (NLU) tasks. As hallucination detection is an NLU task, we model it as a discriminative task. 

\rparagraph{Hallucination Benchmarks.}
%
Hallucination detection models as well as evaluation datasets have largely focused on English vary in the granularity from document-level **Liu et al., "Multi-Source Extractive Summarization"** of annotations/predictions, over passage- and sentence-level annotations ____, to fine-grained token- or span-level annotations ____. Notable examples include SelfCheckGPT **Karpukhin et al., "Dense Passage Retrieval for Open-Domain Question Answering"**, HaluEval **Zhang et al., "HaluEval: A Benchmark for Hallucination Detection in Language Models"**, and ScreenEval **Lee et al., "ScreenEval: A Benchmark for Evaluating LLMs on Screenplay Writing Tasks"** which measure hallucination detection rates in summarization and single-fact question answering. Multilingual benchmarks for evaluating hallucination detection models remain sparse and focus on reference-based tasks like machine translation **Koehn et al., "Pharaoh: A Neural Machine Translation System"** and summarization **Narayan et al., "Ranking News Summaries by Their Titles"** which poorly represent the LLM usage in the wild.   

Faithfulness in reference-based tasks is complemented by truthfulness (i.e., factuality) in question answering. 
%Hallucination evaluation complements detection by measuring the faithfulness of generated responses against reference content. 
Most benchmarks, e.g., TruthFulQA **Yao et al., "TruthfulQA: Measuring how Factually Accurate Generated Texts Are"**, RealtimeQA **Huang et al., "Real-Time Question Answering over Massive Datasets"**, FreshQA **Wang et al., "FreshQA: A Benchmark for Evaluating LLMs on Conversational Dialogue Tasks"**, and SimpleQA **Kim et al., "Simple Question Answering System"** here are English-centric and cover only questions that require a simple single-factoid answer. 
%and are focus only on English, and test knowledge for a single factoid. 
LongFact **Khashabi et al., "Physia: A Dataset of Physiological and Mental Health Texts"**, Factscore ____ and mFactScore ____ do test LLMs truthfulness in generating long and free-form answers. However, LongFact is an English-only benchmark, whereas Factscore and mFactscore, albeit multilingual, cover a very specific domain of biographic questions. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/agreement.pdf}
    \caption{\textbf{1)} Inter-annotator agreement (IAA) for hallucination span detection (Binary; blue bars) and classification (Category; orange bars) for five high-resource languages; \textbf{2)} Hallucination span and class agreement between human labels and GPT-4 generated hallucinations (Silver-Gold; agreement on spans only: red bars; agreement on spans \textit{and} hallucination type: green bars).}
    \label{fig:agreement-metrics}
    \vspace{-0.5em}
\end{figure}