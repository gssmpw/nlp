[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      },
      {
        "key": "aryabumi2024aya",
        "author": "Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and Dash, Saurabh and Cairuz, David and Lin, Hangyu and Venkitesh, Bharat and Smith, Madeline and Marchisio, Kelly and Ruder, Sebastian and others",
        "title": "Aya 23: Open weight releases to further multilingual progress"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others",
        "title": "Qwen2 technical report"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2023instruction",
        "author": "Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others",
        "title": "Instruction tuning for large language models: A survey"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wei2024long",
        "author": "Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and others",
        "title": "Long-form factuality in large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ji2023ai",
        "author": "Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others",
        "title": "Ai alignment: A comprehensive survey"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ji2023survey",
        "author": "Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale",
        "title": "Survey of hallucination in natural language generation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mishra2024finegrained",
        "author": "Mishra, Abhika and Asai, Akari and Balachandran, Vidhisha and Wang, Yizhong and Neubig, Graham and Tsvetkov, Yulia and Hajishirzi, Hannaneh",
        "title": "Fine-grained Hallucination Detection and Editing for Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhou2021detecting",
        "author": "Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzm{\\'a}n, Francisco and Zettlemoyer, Luke and Ghazvininejad, Marjan",
        "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation"
      },
      {
        "key": "liu-etal-2022-multi",
        "author": "Liu, Zihan and Patwary, Mostofa and Prenger, Ryan and Prabhumoye, Shrimai and Ping, Wei and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "manakul-etal-2023-selfcheckgpt",
        "author": "Manakul, Potsawee and Liusie, Adian and Gales, Mark",
        "title": "{S}elf{C}heck{GPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"
      },
      {
        "key": "yang2023new",
        "author": "Yang, Shiping and Sun, Renliang and Wan, Xiaojun",
        "title": "A new benchmark and reverse validation method for passage-level hallucination detection"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mishra2024finegrained",
        "author": "Mishra, Abhika and Asai, Akari and Balachandran, Vidhisha and Wang, Yizhong and Neubig, Graham and Tsvetkov, Yulia and Hajishirzi, Hannaneh",
        "title": "Fine-grained Hallucination Detection and Editing for Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2023label",
        "author": "Li, Zongxi and Li, Xianming and Liu, Yuzhang and Xie, Haoran and Li, Jing and Wang, Fu-lee and Li, Qing and Zhong, Xiaoqin",
        "title": "Label supervised llama finetuning"
      },
      {
        "key": "Dukic2024LookingRI",
        "author": "Duki{\\'c}, David and {\\v{S}}najder, Jan",
        "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling"
      },
      {
        "key": "behnamghader2024llmvec",
        "author": "BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva",
        "title": "{LLM}2Vec: Large Language Models Are Secretly Powerful Text Encoders"
      },
      {
        "key": "schmidt-etal-2024-self",
        "author": "Schmidt, Fabian David and Borchert, Philipp and Vuli{\\'c}, Ivan and Glava{\\v{s}}, Goran",
        "title": "Self-Distillation for Model Stacking Unlocks Cross-Lingual {NLU} in 200+ Languages"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yang2023new",
        "author": "Yang, Shiping and Sun, Renliang and Wan, Xiaojun",
        "title": "A new benchmark and reverse validation method for passage-level hallucination detection"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhou2021detecting",
        "author": "Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzm{\\'a}n, Francisco and Zettlemoyer, Luke and Ghazvininejad, Marjan",
        "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation"
      },
      {
        "key": "manakul-etal-2023-selfcheckgpt",
        "author": "Manakul, Potsawee and Liusie, Adian and Gales, Mark",
        "title": "{S}elf{C}heck{GPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liu-etal-2022-token",
        "author": "Liu, Tianyu and Zhang, Yizhe and Brockett, Chris and Mao, Yi and Sui, Zhifang and Chen, Weizhu and Dolan, Bill",
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation"
      },
      {
        "key": "mishra2024finegrained",
        "author": "Mishra, Abhika and Asai, Akari and Balachandran, Vidhisha and Wang, Yizhong and Neubig, Graham and Tsvetkov, Yulia and Hajishirzi, Hannaneh",
        "title": "Fine-grained Hallucination Detection and Editing for Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "manakul-etal-2023-selfcheckgpt",
        "author": "Manakul, Potsawee and Liusie, Adian and Gales, Mark",
        "title": "{S}elf{C}heck{GPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2023halueval",
        "author": "Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong",
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "lattimer2023fast",
        "author": "Lattimer, Barrett and Chen, Patrick and Zhang, Xinyuan and Yang, Yi",
        "title": "Fast and Accurate Factual Inconsistency Detection Over Long Documents"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "dale2023halomi",
        "author": "Dale, David and Voita, Elena and Lam, Janice and Hansanti, Prangthip and Ropers, Christophe and Kalbassi, Elahe and Gao, Cynthia and Barrault, Loic and Costa-juss{\\`a}, Marta R.",
        "title": "HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "qiu-etal-2023-detecting",
        "author": "Qiu, Yifu and Ziser, Yftah and Korhonen, Anna and Ponti, Edoardo and Cohen, Shay",
        "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "lin2022truthfulqa",
        "author": "Lin, Stephanie and Hilton, Jacob and Evans, Owain",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "kasai2024realtime",
        "author": "Kasai, Jungo and Sakaguchi, Keisuke and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro and others",
        "title": "REALTIME QA: what's the answer right now?"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "vu2023freshllms",
        "author": "Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and others",
        "title": "Freshllms: Refreshing large language models with search engine augmentation"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "wei2024measuring",
        "author": "Wei, Jason and Nguyen, Karina and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William",
        "title": "Measuring short-form factuality in large language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "wei2024long",
        "author": "Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and others",
        "title": "Long-form factuality in large language models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "min2023factscore",
        "author": "Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh",
        "title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "kim-etal-2024-analysis",
        "author": "Kim, Vu Trong  and\nKrumdick, Michael  and\nReddy, Varshini  and\nDernoncourt, Franck  and\nLai, Viet Dac",
        "title": "An Analysis of Multilingual {FA}ct{S}core"
      }
    ]
  }
]