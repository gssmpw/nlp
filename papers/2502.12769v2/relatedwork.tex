\section{Background and Related Work}
%%%% GG: way too broad, can be omitted completely 
%\rparagraph{LLMs and AI Safety.}
%Transformer-based LLMs \citep{vaswani2017attention} have revolutionized a wide range of applications, from conversational agents to knowledge retrieval and content generation \citep{kojima2022large, dubey2024llama, aryabumi2024aya, yang2024qwen2}. However, these models are trained with causal language modeling objective, which often misaligns with real-world user expectations, leading to issues like bias, toxicity, and hallucinations \citep{zhang2023instruction}. Fine-tuning techniques like reinforcement learning from human feedback (RLHF) \citep{ziegler2019fine, ouyang2022training} have improved alignment with user preferences, but challenges persist in knowledge-intensive tasks \citep{wei2024long}, where hallucinations pose significant risks \citep{ji2023ai}.

We provide a brief overview of the body of related work on (1) hallucination detection models and (2) benchmarks for evaluating LLM hallucination. 


\rparagraph{Hallucination Detection.}
%
Coarsely, LLM hallucinations fall into two categories. \textit{Intrinsic} hallucination are content contradicts some reference information source. The reference may be explicitly given to the LLM as part of the task (e.g., the text to be summarized in summarization or source language text in machine translation) or it may implicit (e.g., general world knowledge in question answering). In contrast, \textit{extrinsic} hallucination refers to content that does not contradict the reference but is unnecessary or superfluous with respect to the task (e.g., additional facts in fact-based question answering) \citep{ji2023survey}. Recent work introduced finer-grained taxonomies for both categories. For example, \citet{mishra2024finegrained} distinguish between several types of intrinsic hallucinations (e.g., entity-based hallucinations or relation-based hallucinations). In a similar vein, extrinsic hallucinations are split into subtypes such as \textit{invented}, \textit{subjective}, and \textit{unverifiable} content. 

Unsurprisingly, most hallucination detection (and classifications) models are based on neural languages models. These are are either pre-trained encoder LM \citep{zhou2021detecting,liu-etal-2022-multi}, discriminatively fine-tuned to classify texts as containing hallucinations or not or LLMs prompted (zero-shot or with in-context examples) to detect hallucinations \citep{manakul-etal-2023-selfcheckgpt, yang2023new} or fine-tuned to generate hallucinated spans \citep{mishra2024finegrained}. 
%%
In this work, we cast hallucination detection as a span-detection task, formulated discriminatively, with a classifier on top of an ``encoder-based'' LM. However, instead of resorting to small pretrained encoder LMs, we bidirectionally (i.e., discriminatively) fine-tune a larger generative LLM, following recent advances in converting decoder LMs into encoders \cite{li2023label, Dukic2024LookingRI, behnamghader2024llmvec, schmidt-etal-2024-self}.     

%Recent advances have been made where decoder-only models are converted to encoders  by introducing bidirectional mask through either continuous pertaining or downstream fine-tuning and these decoders convert encoders outperform decoder models on natural language understanding (NLU) tasks. As hallucination detection is an NLU task, we model it as a discriminative task. 

\rparagraph{Hallucination Benchmarks.}
%
Hallucination detection models as well as evaluation datasets have largely focused on English vary in the granularity from document-level \citep{yang2023new} of annotations/predictions, over passage- and sentence-level annotations \citep{zhou2021detecting, manakul-etal-2023-selfcheckgpt}, to fine-grained token- or span-level annotations \citep{liu-etal-2022-token, mishra2024finegrained}. Notable examples include SelfCheckGPT \citep{manakul-etal-2023-selfcheckgpt}, HaluEval \citep{li2023halueval}, and ScreenEval \citep{lattimer2023fast}, which measure hallucination detection rates in summarization and single-fact question answering. Multilingual benchmarks for evaluating hallucination detection models remain sparse and focus on reference-based tasks like machine translation \citep{dale2023halomi} and summarization \citep{qiu-etal-2023-detecting} which poorly represent the LLM usage in the wild.   

Faithfulness in reference-based tasks is complemented by truthfulness (i.e., factuality) in question answering. 
%Hallucination evaluation complements detection by measuring the faithfulness of generated responses against reference content. 
Most benchmarks, e.g., TruthFulQA \cite{lin2022truthfulqa}, RealtimeQA \cite{kasai2024realtime}, FreshQA \cite{vu2023freshllms}, and SimpleQA \cite{wei2024measuring} here are English-centric and cover only questions that require a simple single-factoid answer. 
%and are focus only on English, and test knowledge for a single factoid. 
LongFact \citep{wei2024long}, Factscore \citep{min2023factscore} and mFactScore \cite{kim-etal-2024-analysis} do test LLMs truthfulness in generating long and free-form answers. However, LongFact is an English-only benchmark, whereas Factscore and mFactscore, albeit multilingual, cover a very specific domain of biographic questions. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/agreement.pdf}
    \caption{\textbf{1)} Inter-annotator agreement (IAA) for hallucination span detection (Binary; blue bars) and classification (Category; orange bars) for five high-resource languages; \textbf{2)} Hallucination span and class agreement between human labels and GPT-4 generated hallucinations (Silver-Gold; agreement on spans only: red bars; agreement on spans \textit{and} hallucination type: green bars).}
    \label{fig:agreement-metrics}
    \vspace{-0.5em}
\end{figure}