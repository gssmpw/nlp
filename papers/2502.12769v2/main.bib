%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
Added by GG
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@inproceedings{ebing2024translate,
  title={To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages},
  author={Ebing, Benedikt and Glava{\v{s}}, Goran},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={5325--5344},
  year={2024}
}

@inproceedings{artetxe2023revisiting,
  title={Revisiting Machine Translation for Cross-lingual Classification},
  author={Artetxe, Mikel and Goswami, Vedanuj and Bhosale, Shruti and Fan, Angela and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={6489--6499},
  year={2023}
}

@inproceedings{trippas2024users,
  title={What do users really ask large language models? an initial log analysis of google bard interactions in the wild},
  author={Trippas, Johanne R and Al Lawati, Sara Fahad Dawood and Mackenzie, Joel and Gallagher, Luke},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2703--2707},
  year={2024}
}

@inproceedings{herrlein2024anhalten,
  title={ANHALTEN: Cross-Lingual Transfer for German Token-Level Reference-Free Hallucination Detection},
  author={Herrlein, Janek and Hung, Chia-Chien and Glava\v{s}, Goran},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)},
  pages={92--100},
  year={2024}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

LLM Citations 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{aryabumi2024aya,
  title={Aya 23: Open weight releases to further multilingual progress},
  author={Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and Dash, Saurabh and Cairuz, David and Lin, Hangyu and Venkitesh, Bharat and Smith, Madeline and Marchisio, Kelly and Ruder, Sebastian and others},
  journal={arXiv preprint arXiv:2405.15032},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{martins2024eurollm,
  title={Eurollm: Multilingual language models for europe},
  author={Martins, Pedro Henrique and Fernandes, Patrick and Alves, Jo{\~a}o and Guerreiro, Nuno M and Rei, Ricardo and Alves, Duarte M and Pombal, Jos{\'e} and Farajian, Amin and Faysse, Manuel and Klimaszewski, Mateusz and others},
  journal={arXiv preprint arXiv:2409.16235},
  year={2024}
}

@article{gemma_2024,
    title={Gemma},
    url={https://www.kaggle.com/m/3301},
    DOI={10.34740/KAGGLE/M/3301},
    publisher={Kaggle},
    author={Gemma Team},
    year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={2019}
}

@inproceedings{Conneau2019UnsupervisedCR,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm{\'a}n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:207880568}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{behnamghader2024llmvec,
    title={{LLM}2Vec: Large Language Models Are Secretly Powerful Text Encoders},
    author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
    booktitle={First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=IW1PR7vEBf}
}

@article{li2023label,
  title={Label supervised llama finetuning},
  author={Li, Zongxi and Li, Xianming and Liu, Yuzhang and Xie, Haoran and Li, Jing and Wang, Fu-lee and Li, Qing and Zhong, Xiaoqin},
  journal={arXiv preprint arXiv:2310.01208},
  year={2023}
}

@inproceedings{Dukic2024LookingRI,
  title={Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling},
  author={Duki{\'c}, David and {\v{S}}najder, Jan},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

@inproceedings{schmidt-etal-2024-self,
    title = "Self-Distillation for Model Stacking Unlocks Cross-Lingual {NLU} in 200+ Languages",
    author = "Schmidt, Fabian David and Borchert, Philipp and Vuli{\'c}, Ivan and Glava{\v{s}}, Goran",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics"
}

@article{costa2022no,
  title={No language left behind: Scaling human-centered machine translation},
  author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Hallucination Investigations and AI Safety

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM}
}

@article{tonmoy2024comprehensive,
  title={A comprehensive survey of hallucination mitigation techniques in large language models},
  author={Tonmoy, SM and Zaman, SM and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
  journal={arXiv preprint arXiv:2401.01313},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

English - Only Hallucination Investigations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{garry2024large,
  title={Large language models (LLMs) and the institutionalization of misinformation},
  author={Garry, Maryanne and Chan, Way Ming and Foster, Jeffrey and Henkel, Linda A},
  journal={Trends in cognitive sciences},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{su2024adapting,
  title={Adapting Fake News Detection to the Era of Large Language Models},
  author={Su, Jinyan and Cardie, Claire and Nakov, Preslav},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={1473--1490},
  year={2024}
}

@inproceedings{pan-etal-2023-risk,
    title = "On the Risk of Misinformation Pollution with Large Language Models",
    author = "Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    year = "2023",
    pages = "1389--1403"
}

@article{banerjee2024llms,
  title={Llms will always hallucinate, and we need to live with this},
  author={Banerjee, Sourav and Agarwal, Ayushi and Singla, Saloni},
  journal={arXiv preprint arXiv:2409.05746},
  year={2024}
}

@inproceedings{chen-etal-2023-beyond,
    title = "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
    author = "Chen, Liang and Deng, Yang and Bian, Yatao and Qin, Zeyu and Wu, Bingzhe and Chua, Tat-Seng and Wong, Kam-Fai",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    pages = "6325--6341"
}

@article{llminterativeagent,
author = {Teubner, Timm and Flath, Christoph and Weinhardt, Christof and Aalst, Wil and Hinz, Oliver},
year = {2023},
title = {Welcome to the Era of ChatGPT et al.: The Prospects of Large Language Models},
journal = {Business \& Information Systems Engineering},
doi = {10.1007/s12599-023-00795-x}
}

@inproceedings{liu-etal-2022-multi,
    title = "Multi-Stage Prompting for Knowledgeable Dialogue Generation",
    author = "Liu, Zihan and Patwary, Mostofa and Prenger, Ryan and Prabhumoye, Shrimai and Ping, Wei and Shoeybi, Mohammad and Catanzaro, Bryan",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    year = "2022",
    pages = "1317--1337"
}

@inproceedings{yu2023generate,
    title={Generate rather than Retrieve: Large Language Models are Strong Context Generators},
    author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2023}
}

@article{xiong2024search,
  title={When search engine services meet large language models: visions and challenges},
  author={Xiong, Haoyi and Bian, Jiang and Li, Yuchen and Li, Xuhong and Du, Mengnan and Wang, Shuaiqiang and Yin, Dawei and Helal, Sumi},
  journal={IEEE Transactions on Services Computing},
  year={2024},
  publisher={IEEE}
}

@inproceedings{obaid-ul-islam-etal-2023-tackling,
    title = "Tackling Hallucinations in Neural Chart Summarization",
    author = "Obaid ul Islam, Saad and {\v{S}}krjanec, Iza and Dusek, Ondrej and Demberg, Vera",
    booktitle = "Proceedings of the 16th International Natural Language Generation Conference",
    year = "2023",
    pages = "414--423"
}


@article{lee2022factuality,
  title={Factuality enhanced language models for open-ended text generation},
  author={Lee, Nayeon and Ping, Wei and Xu, Peng and Patwary, Mostofa and Fung, Pascale N and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34586--34599},
  year={2022}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Multilingual - Hallucination Investigations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@inproceedings{raunak-etal-2021-curious,
    title = "The Curious Case of Hallucinations in Neural Machine Translation",
    author = "Raunak, Vikas and Menezes, Arul and Junczys-Dowmunt, Marcin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics",
    year = "2021",
    pages = "1172--1183"
}

@article{guerreiro2023hallucinations,
  title={Hallucinations in Large Multilingual Translation Models},
  author={Guerreiro, Nuno M and Alves, Duarte M and Waldendorf, Jonas and Haddow, Barry and Birch, Alexandra and Colombo, Pierre and Martins, Andr{\'e} FT},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={1500--1517},
  year={2023}
}

@inproceedings{
shafayat2024multifact,
title={Multi-{FA}ct: Assessing Factuality of Multilingual {LLM}s using {FA}ctScore},
author={Sheikh Shafayat and Eunsu Kim and Juhyun Oh and Alice Oh},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=lkrH6ovzsj}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

English -  Hallucination Detection Benchmark Citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@inproceedings{zhou2021detecting,
  title={Detecting Hallucinated Content in Conditional Neural Sequence Generation},
  author={Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzm{\'a}n, Francisco and Zettlemoyer, Luke and Ghazvininejad, Marjan},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1393--1404},
  year={2021}
}


@inproceedings{liu-etal-2022-token,
    title = "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
    author = "Liu, Tianyu and Zhang, Yizhe and Brockett, Chris and Mao, Yi and Sui, Zhifang and Chen, Weizhu and Dolan, Bill",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
    year = "2022",
    pages = "6723--6737"
}

@inproceedings{mishra2024finegrained,
    title={Fine-grained Hallucination Detection and Editing for Language Models},
    author={Mishra, Abhika and Asai, Akari and Balachandran, Vidhisha and Wang, Yizhong and Neubig, Graham and Tsvetkov, Yulia and Hajishirzi, Hannaneh},
    booktitle={First Conference on Language Modeling},
    year={2024}
}

@inproceedings{manakul-etal-2023-selfcheckgpt,
    title = "{S}elf{C}heck{GPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
    author = "Manakul, Potsawee and Liusie, Adian and Gales, Mark",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    pages = "9004--9017"
}

@article{yang2023new,
  title={A new benchmark and reverse validation method for passage-level hallucination detection},
  author={Yang, Shiping and Sun, Renliang and Wan, Xiaojun},
  journal={arXiv preprint arXiv:2310.06498},
  year={2023}
}

@inproceedings{lattimer2023fast,
  title={Fast and Accurate Factual Inconsistency Detection Over Long Documents},
  author={Lattimer, Barrett and Chen, Patrick and Zhang, Xinyuan and Yang, Yi},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={1691--1703},
  year={2023}
}

@article{min2023factscore,
  title={Factscore: Fine-grained atomic evaluation of factual precision in long form text generation},
  author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2305.14251},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Multilingual -  Hallucination Detection Benchmark Citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@inproceedings{qiu-etal-2023-detecting,
    title = "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
    author = "Qiu, Yifu and Ziser, Yftah and Korhonen, Anna and Ponti, Edoardo and Cohen, Shay",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
    pages = "8914--8932"
}


@inproceedings{dale2023halomi,
    title={HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation},
    author={Dale, David and Voita, Elena and Lam, Janice and Hansanti, Prangthip and Ropers, Christophe and Kalbassi, Elahe and Gao, Cynthia and Barrault, Loic and Costa-juss{\`a}, Marta R.},
    booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
    year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

English -  Hallucination Evaluation Benchmark Citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{pal2023med,
  title={Med-halt: Medical domain hallucination test for large language models},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  journal={arXiv preprint arXiv:2307.15343},
  year={2023}
}

@article{vu2023freshllms,
  title={Freshllms: Refreshing large language models with search engine augmentation},
  author={Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and others},
  journal={arXiv preprint arXiv:2310.03214},
  year={2023}
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    pages = "1906--1919"
}

@article{kasai2024realtime,
  title={REALTIME QA: what's the answer right now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Le Bras, Ronan and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{honovich-etal-2021-q2,
    title = "$Q^{2}$: {E}valuating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering",
    author = "Honovich, Or and Choshen, Leshem and Aharoni, Roee and Neeman, Ella and Szpektor, Idan and Abend, Omri",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    pages = "7856--7870"
}


@inproceedings{li2023halueval,
  title={HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  author={Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={6449--6464},
  year={2023}
}

@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    pages = "452--466"
}

@inproceedings{fabbri-etal-2022-qafacteval,
    title = "{QAF}act{E}val: Improved {QA}-Based Factual Consistency Evaluation for Summarization",
    author = "Fabbri, Alexander and Wu, Chien-Sheng and Liu, Wenhao and Xiong, Caiming",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics",
    year = "2022",
    pages = "2587--2601"
}

@article{wei2024measuring,
  title={Measuring short-form factuality in large language models},
  author={Wei, Jason and Nguyen, Karina and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  journal={arXiv preprint arXiv:2411.04368},
  year={2024}
}

@inproceedings{xu2023critical,
  title={A Critical Evaluation of Evaluations for Long-form Question Answering},
  author={Xu, Fangyuan and Song, Yixiao and Iyyer, Mohit and Choi, Eunsol},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  pages={3225--3245},
  year={2023}
}

@inproceedings{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={3214--3252},
  year={2022}
}

@article{wei2024long,
  title={Long-form factuality in large language models},
  author={Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and others},
  journal={arXiv preprint arXiv:2403.18802},
  year={2024}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Multilingual -  Hallucination Evaluation Benchmark Citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{aharoni2022mface,
  title={mface: Multilingual summarization with factual consistency evaluation},
  author={Aharoni, Roee and Narayan, Shashi and Maynez, Joshua and Herzig, Jonathan and Clark, Elizabeth and Lapata, Mirella},
  journal={arXiv preprint arXiv:2212.10622},
  year={2022}
}

@inproceedings{kim-etal-2024-analysis,
    title = "An Analysis of Multilingual {FA}ct{S}core",
    author = "Kim, Vu Trong  and
      Krumdick, Michael  and
      Reddy, Varshini  and
      Dernoncourt, Franck  and
      Lai, Viet Dac",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.247",
    doi = "10.18653/v1/2024.emnlp-main.247",
    pages = "4309--4333",
    abstract = "FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages of varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate 3 mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.",
}

@inproceedings{clark2023seahorse,
  title={SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation},
  author={Clark, Elizabeth and Rijhwani, Shruti and Gehrmann, Sebastian and Maynez, Joshua and Aharoni, Roee and Nikolaev, Vitaly and Sellam, Thibault and Siddhant, Aditya and Das, Dipanjan and Parikh, Ankur},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9397--9413},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%

MISC

%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{ramshaw-marcus-1995-text,
    title = "Text Chunking using Transformation-Based Learning",
    author = "Ramshaw, Lance  and
      Marcus, Mitch",
    booktitle = "Third Workshop on Very Large Corpora",
    year = "1995",
    url = "https://aclanthology.org/W95-0107",
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{alshahrani-etal-2023-depth,
    title = "{DEPTH}+: An Enhanced Depth Metric for {W}ikipedia Corpora Quality",
    author = "Alshahrani, Saied  and
      Alshahrani, Norah  and
      Matthews, Jeanna",
    editor = "Ovalle, Anaelia  and
      Chang, Kai-Wei  and
      Mehrabi, Ninareh  and
      Pruksachatkun, Yada  and
      Galystan, Aram  and
      Dhamala, Jwala  and
      Verma, Apurv  and
      Cao, Trista  and
      Kumar, Anoop  and
      Gupta, Rahul",
    booktitle = "Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.trustnlp-1.16",
    doi = "10.18653/v1/2023.trustnlp-1.16",
    pages = "175--189",
    abstract = "Wikipedia articles are a common source of training data for Natural Language Processing (NLP) research, especially as a source for corpora in languages other than English. However, research has shown that not all Wikipedia editions are produced organically by native speakers, and there are substantial levels of automation and translation activities in the Wikipedia project that could negatively impact the degree to which they truly represent the language and the culture of native speakers. To encourage transparency in the Wikipedia project, Wikimedia Foundation introduced the depth metric as an indication of the degree of collaboration or how frequently users edit a Wikipedia edition{'}s articles. While a promising start, this depth metric suffers from a few serious problems, like a lack of adequate handling of inflation of edits metric and a lack of full utilization of users-related metrics. In this paper, we propose the DEPTH+ metric, provide its mathematical definitions, and describe how it reflects a better representation of the depth of human collaborativeness. We also quantify the bot activities in Wikipedia and offer a bot-free depth metric after the removal of the bot-created articles and the bot-made edits on the Wikipedia articles.",
}

@inproceedings{su-etal-2024-unlocking,
    title = "Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation",
    author = "Su, Tong  and
      Peng, Xin  and
      Thillainathan, Sarubi  and
      Guzm{\'a}n, David  and
      Ranathunga, Surangika  and
      Lee, En-Shiun",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.263/",
    doi = "10.18653/v1/2024.findings-naacl.263",
    pages = "4217--4225",
    abstract = "Parameter-efficient fine-tuning (PEFT) methods are increasingly vital in adapting large-scale pre-trained language models for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying LRL domains and sizes to evaluate the performance of 8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We showed that 6 PEFT architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of PEFT methods."
}

@inproceedings{dabre-etal-2019-exploiting,
    title = "Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation",
    author = "Dabre, Raj  and
      Fujita, Atsushi  and
      Chu, Chenhui",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1146/",
    doi = "10.18653/v1/D19-1146",
    pages = "1410--1416",
    abstract = "This paper highlights the impressive utility of multi-parallel corpora for transfer learning in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k{--}440k) parallel corpus for English and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 3{--}9 BLEU score gains over a simple one-to-one model."
}

@inproceedings{hu2020xtreme,
  title={Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation},
  author={Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle={International Conference on Machine Learning},
  pages={4411--4421},
  year={2020},
  organization={PMLR}
}

@inproceedings{fang2021filter,
  title={Filter: An enhanced fusion method for cross-lingual language understanding},
  author={Fang, Yuwei and Wang, Shuohang and Gan, Zhe and Sun, Siqi and Liu, Jingjing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12776--12784},
  year={2021}
}

@article{xu2024hallucination,
  title={Hallucination is inevitable: An innate limitation of large language models},
  author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2401.11817},
  year={2024}
}

@article{mahapatra2024impact,
  title={Impact of model size on fine-tuned llm performance in data-to-text generation: A state-of-the-art investigation},
  author={Mahapatra, Joy and Garain, Utpal},
  journal={arXiv preprint arXiv:2407.14088},
  year={2024}
}

@article{lu2024scaling,
  title={Scaling Laws for Fact Memorization of Large Language Models},
  author={Lu, Xingyu and Li, Xiaonan and Cheng, Qinyuan and Ding, Kai and Huang, Xuanjing and Qiu, Xipeng},
  journal={CoRR},
  year={2024}
}

@article{Zhou2024larger_and_more,
  title={Larger and more instructable language models become less reliable},
  author={Zhou, Lexin and Schellaert, Wout and Mart{\'\i}nez-Plumed, Fernando and Moros-Daval, Yael and Ferri, C{\`e}sar and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={Nature},
  pages={1--8},
  year={2024},
  doi={10.1038/s41586-024-07930-y},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{
    chang2024how,
    title={How Do Large Language Models Acquire Factual Knowledge During Pretraining?},
    author={Hoyeon Chang and Jinho Park and Seonghyeon Ye and Sohee Yang and Youngkyung Seo and Du-Seong Chang and Minjoon Seo},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=TYdzj1EvBP}
}

@article{JURECA,
author = {{J\"{u}lich Supercomputing Centre}},
title = {{JURECA: Data Centric and Booster Modules implementing the Modular Supercomputing Architecture at J\"{u}lich Supercomputing Centre}},
journal = {Journal of large-scale research facilities},
number = {A182},
volume = {7},
doi = {10.17815/jlsrf-7-182},
url = {http://dx.doi.org/10.17815/jlsrf-7-182},
year = {2021}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, T},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{nordhoff-hammarstrom-2012-glottolog,
    title = "Glottolog/Langdoc:Increasing the visibility of grey literature for low-density languages",
    author = {Nordhoff, Sebastian  and
      Hammarstr{\"o}m, Harald},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}`12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L12-1427/",
    pages = "3289--3294",
    abstract = "Language resources can be divided into structural resources treating phonology, morphosyntax, semantics etc. and resources treating the social, demographic, ethnic, political context. A third type are meta-resources, like bibliographies, which provide access to the resources of the first two kinds. This poster will present the Glottolog/Langdoc project, a comprehensive bibliography providing web access to 180k bibliographical records to (mainly) low visibility resources from low-density languages. The resources are annotated for macro-area, content language, and document type and are available in XHTML and RDF."
}