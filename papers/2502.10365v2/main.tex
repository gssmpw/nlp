%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{caption}
\usepackage{float}

%\usepackage[font=small,labelfont=bf]{caption}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{AffinityFlow: Guided Flows for Antibody Affinity Maturation}

\begin{document}

\twocolumn[
\icmltitle{AffinityFlow: Guided Flows for Antibody Affinity Maturation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Can (Sam) Chen}{mila}
\icmlauthor{Karla-Luise Herpoldt}{amz}
\icmlauthor{Chenchao Zhao}{amz}
\icmlauthor{Zichen Wang}{amz}
\icmlauthor{Marcus Collins}{amz}
\icmlauthor{Shang Shang}{amz}
\icmlauthor{Ron Benson}{amz}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{amz}{Amazon}
\icmlaffiliation{mila}{MILA - Quebec AI Institute (work done during an Amazon internship.}

\icmlcorrespondingauthor{Can (Sam) Chen}{can.chen@mila.quebec or chencan421@gmail.com}
\icmlcorrespondingauthor{Marcus Collins}{collmr@amazon.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} 
% otherwise use the standard text.



\begin{abstract}
%
Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding affinity. 
%
This paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences.
%
Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure.
%
Building on this, we propose an \textit{alternating optimization} framework that \textbf{(1)} fixes the sequence to guide structure generation toward high binding affinity using a structure-based affinity predictor, then \textbf{(2)} applies inverse folding to create sequence mutations, refined by a sequence-based affinity predictor for post selection.
%
A key challenge is the lack of labeled data for training both predictors.
%
To address this, we develop a \textit{co-teaching} module that incorporates valuable information from noisy biophysical energies into predictor refinement.
%
The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa.
%
Our method, \textit{AffinityFlow}, achieves state-of-the-art performance in affinity maturation experiments.
%
We plan to open-source our code after acceptance.
%
\end{abstract}

% We plan open-source the code upon the acceptance of this paper.
%

\section{Introduction}
%

\begin{figure*}[h]
    \centering
    \vspace{-5pt}
    \includegraphics[width=.87\textwidth]{Figures/alternate.pdf}
    \vspace{-5pt}
\caption{Illustration of \textit{alternating optimization}.}
\vspace{-10pt}
\label{fig:motivation_alternate}
\end{figure*}

Natural antibodies protect organisms by specifically binding to target antigens such as viruses and bacteria with high affinity~\cite{murphy2016janeway}, while therapeutic antibodies bind various targets to inactivate them, recruit immune cells to them, or deliver an attached drug compound~\cite{chiu2020engineeringab}.
% 
\textit{In vivo}, antibodies undergo affinity maturation, where their target-binding capacity is incrementally enhanced through somatic hypermutation and clonal selection~\cite{victora2022germinal}. % 
%
When developing a therapeutic antibody, \textit{in vitro} affinity maturation—through targeted mutation and selection—improves the binding affinity of existing antibodies to target antigens~\cite{tabasinezhad2019trends, chiu2020engineeringab}.



These \textit{in vitro} methods, such as random mutagenesis, are labor-intensive and time-consuming.
%
Recent advancements in deep learning have propelled \textit{in silico} affinity maturation forward. One line of research enhances affinity prediction for mutated antibodies~\cite{shan2022deep, liu2021deep, cai2024pretrainable, lin2024geoab, xiong2017bindprofx}; another investigates mutation strategies. 
%
Specifically, protein language models propose plausible mutations to enhance binding affinities, though they lack specificity for target antigens~\cite{hie2024efficient, ruffolo2021deciphering, shuai2021generative}.
%
Similarly, diffusion models guide the sampling towards high-affinity antibody sequences but require the often unavailable or insufficiently accurate antigen-antibody complex structure~\cite{luo2022antigen, zhou2024antigen}. 
%
Our research aligns more with the second line of mutation strategies.
%
In particular, we focus on enhancing antibody binding affinity through sequence mutations, relying solely on the antigen-antibody sequence.

Recognizing the crucial link between antibody structure and function, it is essential to integrate structure into the sequence mutation process.
%
The recent release of AlphaFlow~\cite{jing2024alphafold} builds a sequence-conditioned generative model of protein structure, which opens pathways for structure-based optimization of antibody sequences.
%
Specifically, AlphaFlow repurposes AlphaFold~\cite{jumper2021highly} in a flow matching framework to generate diverse protein conformations.





This motivates the proposal of an \textit{alternating optimization} framework, as illustrated in Figure~\ref{fig:motivation_alternate}:
%
\textbf{(1)} We fix the sequence to guide noisy structures toward high-affinity clean structures.
%
Rather than re-training the entire AlphaFlow model—a process that is inherently time-consuming—we achieve guided structure generation through predictor guidance~\cite{dhariwal2021diffusion}.
%
Specifically, a trained structure-based affinity predictor is integrated into the AlphaFlow sampling process to direct coordinate denoising.
%
\textbf{(2)} With the high-affinity clean structure, we perform inverse folding to introduce targeted mutations, and use a sequence-based predictor for post selection, which identifies promising mutated sequences for the next iteration.





A significant challenge in training both predictors is the scarcity of labeled data. 
%
To address this, we develop a \textit{co-teaching} module that leverages valuable information from noisy biophysical energies to refine the predictors, as shown in Figure~\ref{fig:motivation_coteach}.
%
For any antigen $i$ and antibodies $j, k, m, n$, we use Rosetta~\cite{alford2017rosetta} to compute the binding free energy $\Delta G$ and then calculate the change in binding free energy $\Delta \Delta G_{ijk} = \Delta G_{ij} - \Delta G_{ik}$ to form pairwise discrete labels.
%
The sequence-based predictor selects pairs with which it concurs, considering them likely to be accurate and informative, and uses these consensus samples to enhance the structure-based predictor.
%
For instance, if the sequence-based predictor predicts $\Delta \Delta \hat{G}_{ijk} > 0$, it selects $\Delta \Delta G_{ijk} > 0$ for training the structure-based predictor.
%
Similarly, the structure-based predictor reciprocates by informing the sequence-based predictor; for example, it selects $\Delta \Delta {G}_{ijm} < 0$ to refine the sequence predictor, as shown in Figure~\ref{fig:motivation_coteach}.
%
Noisy data, such as $\Delta \Delta {G}_{ijn} > 0$, are filtered out.
%
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.72\textwidth]{Figures/co_teaching.pdf}
    \vspace{-15pt}
\caption{Illustration of \textit{co-teaching}.}
\label{fig:motivation_coteach}
\end{figure*}
%
This module effectively integrates biophysical insights into both predictors, enhancing their accuracy.

In summary, we introduce \textit{AffinityFlow}, guided flows for affinity maturation.
%
Our contributions are three-fold:
\begin{itemize}[leftmargin=*]
\item We present an AlphaFlow-based \textit{alternating optimization} framework that guides structure generation towards high binding affinity through predictor guidance, followed by targeted mutations.
\item We propose a \textit{co-teaching} module that utilizes valuable insights from noisy biophysical energies to refine structure- and sequence-based predictors.
\item \textit{AffinityFlow} achieves state-of-the-art performance in affinity maturation experiments.
\end{itemize}

\vspace{-10pt}




\section{Preliminaries}

\subsection{Problem Definition}

%Each antibody is structurally composed of two heavy and two light chains. 
Binding affinity between an antibody (Ab) and its antigen (Ag) is predominantly determined by the complementarity determining regions (CDRs) within these chains~\cite{akbar2022progress}.
%
We model an antibody chain as a sequence of  amino acids, each characterized by a type \( a_i \in \{A, C, D, \ldots, Y\} \).
%
While \textit{AffinityFlow} is applicable to all antibody types, this study specifically focuses on single-domain antibodies (sdAb), which consist only of heavy chains~\cite{wesolowski2009single}. We select sdAb for their high specificity, solubility, stability, and lower toxicity and immunogenicity.

Our goal in affinity maturation is to effectively mutate the CDRs within the context of the entire Ab-Ag sequence complex to improve binding affinity.


%
% A CDR region comprises \(m\) amino acids, spanning indices \(l+1\) to \(l+m\), represented as \(R = \{a_j \mid j = l+1, \ldots, l+m\}\). The objective is to effectively mutate \(R\) in the context of the entire antibody-antigen sequence complex, which is represented as \(C = \{a_i \mid i \in \{1, \ldots, N\} \setminus \{l+1, \ldots, l+m\}\}\). 



\subsection{Flow Matching}
Flow matching is a robust generative modeling framework \cite{lipman2022flow, le2024voicebox}.
%
It is characterized by a conditional probability path $
p_t(\boldsymbol{x} \mid \boldsymbol{x}_1), t \in [0, 1],$
which transitions from a prior distribution $p_0(\boldsymbol{x} \mid \boldsymbol{x}_1) = q(\boldsymbol{x})$
to an approximate Dirac delta function $ 
p_1(\boldsymbol{x} \mid \boldsymbol{x}_1) \approx \delta(\boldsymbol{x} - \boldsymbol{x}_1)$
conditioned on a data point $\boldsymbol{x}_1$ from $p_{\text{data}}$.
%
The evolution is facilitated by a conditional vector field $u_t(\boldsymbol{x} \mid \boldsymbol{x}_1)$.
%
The marginal vector field $v(\boldsymbol{x}, t)$ is modeled through a neural network parameterized by $\boldsymbol{\theta}$:
\begin{equation}
    \label{eq: vc_df}
    \hat{v}(\boldsymbol{x}, t; \theta) \approx v(\boldsymbol{x}, t) = \mathbb{E}_{\boldsymbol{x}_1 \sim p_t(\boldsymbol{x}_1 \mid \boldsymbol{x})}[u_t(\boldsymbol{x} \mid \boldsymbol{x}_1)]
\end{equation}
%
Using this modeled vector field, we can generate samples from the data distribution $p_{\text{data}}(\boldsymbol{x})$ by utilizing the corresponding neural Ordinary Differential Equation (ODE).
 



\subsection{AlphaFlow}
AlphaFold \cite{jumper2021highly} serves as a precise single-state protein structure predictor, and AlphaFlow \cite{jing2024alphafold} repurposes AlphaFold within a flow matching framework to generate diverse protein conformations.
%
Given a protein sequence $\boldsymbol{a}$ of length $N$, the objective is to model the structural ensemble, denoted by $p(\boldsymbol{x} \mid \boldsymbol{a})$, where $\boldsymbol{x} \in \mathbb{R}^{3 \times N}$ represents the protein 3D coordinates.

AlphaFlow defines the conditional probability path by sampling initial noise $\boldsymbol{x_0}$ from $q(\boldsymbol{x_0})$ and linearly interpolating it with the data point $\boldsymbol{x_1}$:
\begin{equation}
    \boldsymbol{x} \mid \boldsymbol{x_1}, t = (1 - t) \cdot \boldsymbol{x_0} + t \cdot \boldsymbol{x_1}, \quad \boldsymbol{x_0} \sim q(\boldsymbol{x_0})
\end{equation}
The vector field is derived as:
\begin{equation}
    u_t(\boldsymbol{x} \mid \boldsymbol{x_1}) = (\boldsymbol{x_1} - \boldsymbol{x})/(1-t)
\end{equation}
Instead of directly parameterizing the marginal vector field as in Eq.~(\ref{eq: vc_df}),  the marginal vector field is parameterized in terms of a neural network ${{\hat{\boldsymbol{x}}_1}}(\boldsymbol{x}, t;\boldsymbol{\theta})$ as:
\begin{equation}
    {\hat v}(\boldsymbol{x}, t; \boldsymbol{\theta}) = ({\hat{\boldsymbol{x}}_1}(\boldsymbol{x}, t;\theta) - \boldsymbol{x})/(1-t)
\end{equation}
This approach allows the reuse of the AlphaFold2 template embedding stack to reconstruct the clean structure $\boldsymbol{x_1}$ from the noisy input $\boldsymbol{x}$, with $t$ serving as an additional time embedding. The model focuses on the 3D coordinates of $\beta$-carbons (or $\alpha$-carbon for glycine), defining the prior distribution $q(\boldsymbol{x})$ over these positions as a harmonic prior \cite{jing2023eigenfold} to ensure that inputs to the neural network remain physically plausible. 
%
For this study, the pre-trained AlphaFlow model, which was trained using FAPE loss, is used directly without any further fine-tuning.
%
Since AlphaFlow is trained solely on single proteins, this study connects the antibody sequence and the antigen sequence into one sequence using a linker of $32$ \textit{GGGGS} repeats~\cite{lin2023evolutionary}.
%
The linked sequence complex is then input into the system.



% AlphaFlow is trained exclusively on single proteins; therefore, we focus our study on nanobodies~\cite{jin2023nanobodies}, which consist only of heavy chains, to simplify our approach. We connect the antibody heavy chain and the antigen sequence into a single protein sequence using a linker composed of $32$ repeats of \textit{GGGGS}.
% %
% The resulting linked complex is input into the system. It should be noted that while our algorithm is capable of handling conventional antibodies by using two linkers to join the antibody heavy chain, antibody light chain, and the antigen chain, this paper focuses on nanobodies for simplicity.


\subsection{Affinity Prediction}

This paper focuses on enhancing binding affinity, determined by the difference in free energy between the bound and unbound states, denoted \(\Delta G\). Consequently, we predict \(\Delta G\) as a measure of binding affinity.
%
Protein properties can be predicted from two views: sequence and structure, leading to two prediction methods: sequence-based~\cite{xu2022peer} and structure-based~\cite{gligorijevic2021structure, chen2023structure}.
%

Leading sequence-based models like ESM-2~\cite{lin2022language}, AntiBERTy~\cite{ruffolo2021deciphering}, and IgLM~\cite{shuai2023iglm} are pre-trained on extensive unlabeled protein sequences. These models extract hidden representations to predict properties such as binding energy, denoted as \(f_{\boldsymbol{\alpha}}(\Delta G | \boldsymbol{a})\), where \(\boldsymbol{\alpha}\) represents the model parameters. We choose ESM-2 as our sequence-based predictor due to its versatility, as the antigen is a general protein rather than an antibody.
%
Specifically, we input the antibody and antigen sequences separately into ESM-2 to obtain embeddings, which are then concatenated and fed into a three-layer MLP for the final prediction.


For structure-based prediction, the GVP model is notable for utilizing features from the 3D graph of proteins to predict properties, denoted as \(f_{\boldsymbol{\beta}}(\Delta G | \boldsymbol{x})\)~\cite{jing2021learning}.
%
Integrating the ESM2 model as a feature extractor within the GVP model further enhances performance~\cite{wang2022lm}. 
%
Thus, we employ the ESM2-GVP model as our structure-based predictor in this study.
%
The linked antibody-antigen complex is processed by the pre-trained ESM2 to generate residue embeddings, from which intersected residues are selected for the GVP model.


It is important to note two aspects: (1) The ESM2-GVP model may not outperform the standalone ESM-2 model due to potential unavailability of ground-truth structures and the challenges in making reliable structure predictions for antibodies and antigens;
%
(2) Given that our AlphaFlow system operates solely on \(C_{\beta}\) coordinates and does not account for side-chains, we do not utilize affinity prediction models that require side chain modeling~\cite{liu2021deep, cai2024pretrainable}.

Related works on generative protein modeling and co-teaching are detailed in Appendix~\ref{appendix: related_work}.


 



% $\Delta \Delta G_{ijk} > 0$

% $\Delta \Delta G_{ijm} \hspace{-2.5pt}< 0$

% $\Delta \Delta G_{ijn} > 0$

% $\cdots\cdots\cdots\cdots$


\section{Method}

In this section, we introduce \textit{AffinityFlow}, designed to enhance the binding affinity of antibodies through targeted sequence mutations.
%
Our method, built on AlphaFlow, employs an \textit{alternating optimization} framework for sequence mutation via predictor guidance, as detailed in Section~\ref{subsec: guided_alphaflow}.
%
To overcome the challenge of limited labeled data, we propose a \textit{co-teaching} module in Section~\ref{subsec: co-teaching}.
%
This module leverages useful knowledge from noisy biophysical energies to improve our predictors.
%





\subsection{Alternating Optimization}
\label{subsec: guided_alphaflow}

%We adopt an \textit{alternating optimization} framework for antibody affinity maturation.
%
Initially, the sequence is fixed while we guide the Ab structure generation to achieve high binding affinity, supplemented with predictor-corrector refinement. Based on the generated structure, we then use inverse folding to introduce targeted mutations into the Ab, with the sequence-based predictor selecting promising sequences for the next iteration.


\paragraph{Guided Structure Generation}
%
While AlphaFlow generates structures unconditionally, we aim to steer structure generation toward improved binding affinity using predictor guidance. Following \textit{Lemma 1} in~\cite{zheng2023guided}, predictor guidance in flow matching is formulated as:
\begin{equation}
    \label{eq: guidance}
    \Tilde{v}(\boldsymbol{x}_t, t, \Delta G; \boldsymbol{\theta}) =  \hat{v}(\boldsymbol{x}_t, t; \boldsymbol{\theta}) + \frac{1-t}{t} \nabla_{\boldsymbol{x}_t} \log p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t).
\end{equation}
where $p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t)$ denotes the target binding energy distribution. The derivation details are in Appendix~\ref{appendix: cls}. Training the predictor at different time steps $t$ is resource-intensive; instead, we approximate $p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t)$ directly from $p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{\hat{x}}_1(\boldsymbol{x}_t), 1)$:
\begin{equation}
p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t) \approx p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{\hat{x}}_1(\boldsymbol{x}_t), 1).
\end{equation}
This approximation, termed \(p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{\hat{x}}_1(\boldsymbol{x}_t))\), is effective when \(t\) is close to 1; therefore, we primarily apply predictor guidance in the later stages of sampling.
%

The desired binding energy distribution is formulated as~\cite{lee2023exploring}:
\begin{equation}
    \label{eq: w_dis}
    p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{\hat{x}}_1(\boldsymbol{x}_t)) = e^{-\gamma \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t))} / Z,
\end{equation}
where $\gamma$ is a scaling factor and $Z$ a normalization constant, with the negative sign indicating a preference for lower binding energy. Integrating this into Eq.(\ref{eq: guidance}) leads to:
\begin{equation}
    \label{eq: pred_guidance}
    \Tilde{v}(\boldsymbol{x}_t, t, \Delta G; \boldsymbol{\theta}) =  \hat{v}(\boldsymbol{x}_t, t; \boldsymbol{\theta}) - \gamma\frac{1-t}{t} \nabla_{\boldsymbol{x}_t} \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t)).
\end{equation}
%
This vector field guides the ODE sampling process towards lower binding energy.
%
During sampling, we target the predictor guidance only to CDR coordinates rather than the full protein to simplify the system and enhance its relevance.





\paragraph{Predictor-Corrector}
%
Given that $\boldsymbol{\hat{x}}_1$ represents the $C_{\beta}$ coordinates of the protein structure, which are subject to energy constraints, we apply Amber relaxation~\cite{lindorff2010improved} to adjust $\boldsymbol{\hat{x}}_1$ at each iteration before initiating guided structure generation. This correction step is essential, as predictor guidance on clashed protein structures is ineffective.
%
This approach aligns with the Predictor-Corrector methods described in~\cite{allgower2012numerical, song2020score}, and we therefore adopt the same terminology.
%
Additional related techniques are discussed in Appendix~\ref{appendix: approximations}.


\paragraph{Sequence Mutation}
%
Using the generated structure as a reference, we employ inverse folding with ProteinMPNN~\cite{dauparas2022robust} to identify potential mutations in the CDR regions.
%
We introduce single-point, double-point, and triple-point mutations, and use the sequence-based predictor to select high-affinity variants for subsequent iterations.
%
Since the generated structure is conditioned on the sequence, we avoid multiple simultaneous mutations to preserve the protein structure and minimize disruptive changes.
%
However, under our \textit{alternating optimization} framework, we can introduce a few mutations per iteration, gradually accumulating enough mutations over successive iterations.


\subsection{Co-teaching}
\label{subsec: co-teaching}
%
A primary challenge is the scarcity of labeled data for training both structure-based and sequence-based affinity predictors. To address this, we enhance the predictors by incorporating insights from noisy biophysical energies.

\paragraph{Complex Generation}
%
To compute biophysical energies, initial protein complexes are required.
%
We extract $A$ sdAb structures and $B$ antigen structures from existing PDB files, and then use the docking tool GeoDock~\cite{chu2023flexible} to generate $AB$ complex structures.
%
Next, we employ Rosetta~\cite{alford2017rosetta} to calculate the binding free energy $\Delta G$ for each complex.

\paragraph{Pairwise Discrete Data}
%
Instead of relying on pointwise continuous samples, which can be highly variable and noisy, we generate robust pairwise discrete data.
%
For the $i$-th antigen, we pair antibody $j$ with antibody $k$ and compute the change in binding free energy as $\Delta \Delta G_{ijk} = \Delta G_{ij} - \Delta G_{ik}$.
%
We assign a pairwise label $Y_{ijk}$ as $1$ if $\Delta \Delta G_{ijk} > 0$, indicating stronger binding by antibody $k$, and $0$ otherwise.
%
This approach provides a more reliable measure than using absolute property values.


\paragraph{Sample Selection}
%
Given the potential noise from unreliable biophysical energy calculations, we implement a reciprocal filtering approach to refine the quality of input for each predictor. Each predictor selects samples that align with its predictions to inform the other. Specifically, the sequence-based predictor \(f_{\boldsymbol{\alpha}}(\Delta G | \boldsymbol{a})\) computes \(\hat{Y}^{a}_{ijk} = (\Delta \Delta \hat{G}_{ijk} > 0)\).
%
If \(\hat{Y}^{a}_{ijk} = Y_{ijk}\), this indicates probable accuracy, prompting us to use this consensus sample for the structure-based predictor.
%
The structure-based predictor undergoes a similar process, creating a cyclical filtering system. This ensures both predictors receive well-vetted, high-quality samples for improved reliability.




\paragraph{Fine-tuning}
%
With the selected samples, we aim to enhance the performance of our predictors. For the sequence-based predictor, we minimize the following loss function:
\begin{equation}
\begin{split}
    \mathcal{L}({\boldsymbol{\alpha}}) = 
    & - \sum_{i,j,k} \bigg[ 
    Y_{ijk} \log p_{\boldsymbol{\alpha}}(Y_{ijk} = 1) \\
    & + (1 - Y_{ijk}) \log (1 - p_{\boldsymbol{\alpha}}(Y_{ijk} = 1)) 
    \bigg],
\end{split}
\end{equation}
where \( p_{\boldsymbol{\alpha}}(Y_{ijk} = 1) = \sigma(\Delta \hat{G}^{a}_{ij} - \Delta \hat{G}^{a}_{ik}) \) and \( \sigma(\cdot) \) is the sigmoid function.
%
The structure-based predictor undergoes a similar fine-tuning process. Through this \textit{co-teaching} module, both predictors exchange valuable biophysical information, significantly improving their effectiveness.


\section{Experiments}

\subsection{Benchmark}

\paragraph{Dataset} 
%
We conduct our experiments using a sdAb subset of the SAbDab dataset~\cite{dunbar2014sabdab}. 
%
Following the protocol of~\cite{luo2022antigen}, we exclude structures with a resolution poorer than $4\text{\AA}$ and antibodies targeting non-protein antigens. 
%
Our study focuses on sdAbs, selecting PDB files of $120$ labeled sdAb-antigen pairs to initially train our predictors using mean squared loss. 
%
From these files, we extract $77$ sdAbs and $54$ antigens, resulting in $4,158$ docked complex structures generated by GeoDock. Rosetta is then used to calculate the $\Delta G$ for these complexes.
%
For maturation testing, we select $60$ sdAb-antigen PDB files, ensuring that each antigen is unique and these antigens and antibodies were not included in the training set.


\paragraph{Evaluation}
Our evaluation considers mutations in CDR-H1, CDR-H2, CDR-H3, and the entire CDR region.
%
Each comparative method generates three mutated sequences per antigen, resulting in a total of $180$ sequence designs.

%
We measure performance using three metrics: functionality, specificity, and rationality, following~\cite{ye2024proteinbench}.
%
\textit{Functionality} is assessed by the Improvement Percentage (\textit{IMP}) as described in~\cite{luo2022antigen}. IMP reflects the proportion of mutated sdAbs with reduced binding energy compared to the original. Structures are predicted using IgFold~\cite{ruffolo2023fast}, docked with GeoDock~\cite{chu2023flexible}, and binding energies are analyzed via Rosetta~\cite{alford2017rosetta}.
%
We report \textit{IMP} instead of absolute values to ensure robustness, where a higher \textit{IMP} indicates better performance.
%
\textit{Specificity} measures the sequence similarity among antibodies targeting different antigens.
%
An effective method should generate distinct antibodies for different antigens, so lower similarity (\textit{Sim}) indicates better specificity.
%
\textit{Rationality} is evaluated using inverse perplexity calculated by AntiBERTy~\cite{ruffolo2021deciphering}.
%
This metric, also referred to as {naturalness} (\textit{Nat}), indicates that higher values of \textit{Nat} generally reflect better rationality.


\subsection{Comparisons with Other Methods}

In this paper, we primarily benchmark our method against language model-based methods, given our focus on sequence design.
%
Since our method incorporates additional biophysical energies for training, we ensure fair comparisons by applying the same trained sequence-based predictor across all methods, unless stated otherwise.
%
Each method generates a pool of candidate designs, and the sequence-based predictor selects the top three for final evaluation.



We consider the following language model-based methods:
%
\begin{enumerate}[leftmargin=*]
  \item \textbf{ESM~\cite{hie2024efficient}}: This method uses a pre-trained language model to identify potential mutations. Mutation consensus among six ESM models is assessed, and all promising sequences are collected over nine rounds.

  \item \textbf{AbLang~\cite{Olsen2022}}: Specifically trained on antibody sequences, the AbLang model includes separate models for heavy and light chains. For our purposes, we utilize the heavy chain model to identify promising mutations across nine rounds.


  \item \textbf{nanoBERT~\cite{hadsund2024nanobert}}: Given our focus on sdAbs, nanoBERT, a model pre-trained on sdAb sequences, is employed. We conduct nine rounds of mutation identification.
\end{enumerate}


Beyond language model-based methods, we include an additional sequence-design baseline:
\begin{enumerate}[leftmargin=*,start=4]
  \item \textbf{dWJS~\cite{frey2023protein}}: handles discrete sequences by learning a smoothed energy function, sampling from the smoothed data manifold, and projecting the sample back to the true data manifold with one-step denoising.
\end{enumerate}


We also evaluate three structure-based methods. Although our approach is sequence-based and does not inherently require structures for design, we use AlphaFold2~\cite{jumper2021highly} to predict the structures needed for these comparisons.
%
The following methods are considered:
\begin{enumerate}[leftmargin=*,start=5]
  \item \textbf{DiffAb~\cite{luo2022antigen}}: Trains a diffusion model on amino acid types, coordinates, and orientations. Antibody optimization is achieved by introducing small perturbations into the existing antibody-antigen complex and subsequently denoising the structure. We generate ten designs per antigen and use our predictor to select the top three for evaluation.

  \item \textbf{AbDPO~\cite{zhou2024antigen}}: Based on DiffAb, this model fine-tunes a pre-trained diffusion model using a residue-level decomposed energy preference to enable a low-energy protein sampling process. The sampling and selection processes are similar to those of DiffAb.

  \item \textbf{GearBind~\cite{cai2024pretrainable}}: Utilizes multi-level geometric message passing and contrastive pretraining to improve predictions of affinity. We employ AbDPO to produce ten designs per antigen, from which GearBind selects the three most promising for assessment.
\end{enumerate}


\subsection{Training Details}
%
We use a linker composed of $32$ \textit{GGGGS} repeats to connect the sdAb and antigen.
%
Our method utilizes the \textit{alternating optimization} framework with three iterations, where each iteration introduces single-point, double-point, and triple-point mutations.
%
This allows for producing $1$ to $9$ mutations in total.
%
We set the AlphaFlow sampling steps $T$ to $3$ per iteration with a schedule of \([1.0, 0.6, 0.3, 0.0]\) and use a default scaling factor $\gamma$ of $5$.
%
We employ ESM2-8M, followed by a hidden-dim-$320$ three-layer MLP, as the sequence-based predictor.
%
For the structure-based predictor, we use a five-layer GVP model, which takes the structure and ESM2-8M residue embeddings as input.
%
For the co-teaching module, we use a batch size of $256$ and a learning rate of $1 \times 10^{-4}$ with the Adam optimizer~\cite{kingma2014adam}.
%
Computational efficiency is detailed in Appendix~\ref{appendix: comp_efficiency}, and hyperparameter sensitivity is addressed in Appendix~\ref{appendix: hyper_sensitivity}.



\subsection{Results and Analysis}

\begin{table*}[htbp]
\caption{Overall performance comparison}
\label{tab:overall}
\centering
\scalebox{0.95}{

% \begin{tabular}{c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c@{\hspace{0pt}}c}
\begin{tabular}{c|ccc|ccc|ccc|ccc}

\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{CDR-H1} & \multicolumn{3}{c|}{CDR-H2} & \multicolumn{3}{c|}{CDR-H3} & \multicolumn{3}{c}{All} \\
\cline{2-13}
& IMP\rotatebox[origin=c]{180}{\textdownarrow} & Sim\rotatebox[origin=c]{0}{\textdownarrow} & Nat\rotatebox[origin=c]{180}{\textdownarrow}  & IMP\rotatebox[origin=c]{180}{\textdownarrow} & Sim\rotatebox[origin=c]{0}{\textdownarrow} & Nat\rotatebox[origin=c]{180}{\textdownarrow}  & IMP\rotatebox[origin=c]{180}{\textdownarrow} & Sim\rotatebox[origin=c]{0}{\textdownarrow} & Nat\rotatebox[origin=c]{180}{\textdownarrow}  & IMP\rotatebox[origin=c]{180}{\textdownarrow} & Sim\rotatebox[origin=c]{0}{\textdownarrow} & Nat\rotatebox[origin=c]{180}{\textdownarrow}  \\
\midrule
ESM & ${85.5\%}$ & $0.559$ & $\textbf{0.347}$ & $72.9\%$ & $0.566$ & $\textbf{0.358}$ & $64.0\%$ & $0.573$ & $\textbf{0.359}$ & $84.1\%$ & $0.562$ & $\textbf{0.361}$ \\
AbLang & $88.0\%$ & $0.536$ & $\underline{0.330}$ & $85.4\%$ & ${0.537}$ & $\underline{0.322}$ & $\underline{88.5\%}$ & $0.542$ & $\underline{0.336}$ & $82.9\%$ & $0.548$ & $\underline{0.349}$ \\
nanoBERT & $84.7\%$ & $\underline{0.534}$ & ${0.322}$ & ${85.9\%}$ & $\underline{0.536}$ & ${0.321}$ & $81.6\%$ & $0.537$ & $0.328$ & ${86.0\%}$ & $0.544$ & $0.341$ \\
\midrule
dWJS & $82.7\%$ & $0.535$ & ${0.319}$ & ${69.4\%}$ & ${0.537}$ & ${0.304}$ & $66.1\%$ & $\underline{0.522}$ & $0.294$ & ${85.6\%}$ & $0.545$ & $0.317$ \\
\midrule
DiffAb & $85.5\%$ & $0.541$ & ${0.317}$ & $86.7\%$ & $0.548$ & $0.318$ & $85.6\%$ & $0.528$ & $0.317$ & $84.4\%$ & $\underline{0.540}$ & ${0.316}$ \\
AbDPO & $\underline{88.3\%}$ & $0.540$ & $0.318$ & $\underline{91.1\%}$ & $0.545$ & ${0.318}$ & ${87.8\%}$ & ${0.525}$ & ${0.319}$ & $\underline{90.0\%}$ & $\underline{0.540}$ & $0.315$ \\
GearBind & $87.7\%$ & $0.543$ & $0.315$ & $87.1\%$ & $0.544$ & $0.317$ & $86.7\%$ & $0.527$ & $0.317$ & $88.9\%$ & $0.541$ & $0.314$ \\
\midrule
{\textit{AffinityFlow}} & $\textbf{88.9\%}$ & $\textbf{0.526}$ & ${0.320}$ & $\textbf{93.3\%}$ & $\textbf{0.528}$ & ${0.321}$ & $\textbf{89.7\%}$ & $\textbf{0.514}$ & ${0.322}$ & $\textbf{91.2\%}$ & $\textbf{0.528}$ & ${0.323}$ \\
\bottomrule
\end{tabular}
}
\end{table*}


In Table~\ref{tab:overall}, we present the experimental results on four settings CDR-H1, CDR-H2, CDR-H3 and all design positions. 
%
Delineating lines are drawn to distinguish between different groups of methods.
%
The best and second-best performance are highlighted in \textbf{bold} and \underline{underlined}, respectively.


We make the following observations:
%
\textbf{(1)} As shown in Table~\ref{tab:overall}, our method consistently achieves the best performance in terms of \textit{IMP} and \textit{Sim}, thereby highlighting its effectiveness.
%
\textbf{(2)} The notable \textit{IMP} is mainly due to our effective predictor guidance, which directs the structure sample generation towards low binding energy.
%
\textbf{(3)} The low \textit{Sim} scores can be attributed to antigen-specific modeling and the diversity introduced by the AlphaFlow sampling process. 
%
Language-based methods like ESM, AbLang, and nanoBERT lack this feature, as they do not incorporate specific antigens into their design processes.
%
Structure-based methods such as DiffAb, AbDPO, and GearBind consider specific antigens, but their simplistic diffusion models are less effective at capturing antigen information compared to our method.
%

\textbf{(4)} The language model-based methods ESM, AbLang, and nanoBERT achieve the highest \textit{Nat} scores, as they are implicitly trained for this metric. Beyond these methods, our approach achieves the best \textit{Nat}. 
%
We attribute this to the realistic structure modeling enabled by AlphaFlow and the reliable inverse folding performed by ProteinMPNN, which together translate structures into natural sequences. 
%
\textbf{(5)} AbDPO, as a robust baseline, often achieves strong performance in \textit{IMP}, likely due to incorporating energy information into its training, allowing for low-energy protein sampling.
%
However, AbDPO requires training a separate diffusion model for each complex, adding complexity.
%
\textbf{(6)} Lastly, the high \textit{IMP} scores for baseline methods can largely be attributed to our trained sequence-based predictor. When using a standard predictor trained only on supervised data, \textit{IMP} scores drop significantly. For example, in the CDR-H3 design setting, \textit{IMP} drops from $64.0\%$ to $22.7\%$ for ESM, from $88.5\%$ to $49.4\%$ for AbLang, from $81.6\%$ to $46.7\%$ for nanoBERT, from $66.1\%$ to $23.9\%$ for dWJS, from $85.6\%$ to $49.4\%$ for DiffAb, from $87.8\%$ to $50.6\%$ for AbDPO, from $86.7\%$ to $50.6\%$ for GearBind, and from $89.7\%$ to $68.3\%$ for our method. 
%
In this context, our method demonstrates a clear advantage over the comparison methods.


\subsection{Ablation Studies}

\begin{table}
\centering
\caption{Ablation Study of AffinityFlow on CDR-H3.}
\resizebox{0.75\linewidth}{!}{
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\begin{tabular}{c|ccc}
\toprule
Methods  & IMP\rotatebox[origin=c]{180}{\textdownarrow} & Sim\rotatebox[origin=c]{0}{\textdownarrow} & Nat\rotatebox[origin=c]{180}{\textdownarrow}  \\

\midrule
\textit{one-iteration} & $73.3\%$  & $\textbf{0.512}$ &$0.319$  \\
\textit{w/o PC} & $\underline{83.3\%}$  & $0.521$ &$0.316$  \\
\textit{w/o AlphaFlow} & $63.3\%$  & $0.528$ &$0.314$  \\
\midrule
\textit{w/o energy} & $66.7\%$  & $0.531$ &$0.322$  \\
\textit{w/o selection} & $76.7\%$  & $0.523$ &$\underline{0.326}$  \\
\midrule
Ours & $\textbf{93.3\%}$  & $\underline{0.514}$ &$\textbf{0.330}$\\
\bottomrule
\end{tabular}}\label{tab:ablation}
\end{table}



We use \textit{AffinityFlow} as the baseline to evaluate the effect of removing specific modules, with results shown in Table~\ref{tab:ablation}. The ablation studies are conducted on CDR-H3, considering $10$ antigens for efficiency.
%
Our focus is primarily on the \textit{IMP} metric, so the discussion centers around this metric.


\paragraph{Alternating Optimization}
This framework alternates between updating the structure with the sequence fixed, and mutating the sequence with the structure fixed. In this study, we perform a single iteration, applying multiple mutations simultaneously, referred to as \textit{one-iteration}. 
%
We also evaluate the impact of the predictor-corrector technique by excluding the Amber relaxation step, denoted  \textit{w/o PC}.
%
As shown in Table~\ref{tab:ablation}, both ablations reduce performance, demonstrating the predictor-corrector and Amber relaxation effectiveness.

We also evaluate the effect of directly removing the AlphaFlow framework. In this variant, we perform gradient optimization on the existing protein structure instead of using predictor guidance. This step is followed by Amber relaxation, after which we use ProteinMPNN to identify potential mutations.
%
This variant is denoted by \textit{w/o AlphaFlow} in Table~\ref{tab:ablation}, which shows that leaving out AlphaFlow leads to the greatest performance drop compared to the other two variants.
%
We attribute this to AlphaFlow's ability to capture the natural fluctuations of proteins, resulting in more realistic structures than those generated through direct gradient ascent alone, and accessing binding conformations that may be different from either a structure determined experimentally or predicted by a model like AlphaFold. Less realistic structures in turn yield less natural mutated sequences, as reflected by the \textit{Nat} score decreasing from $0.330$ to $0.314$.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/RBD-Ag.jpg}
    \caption{Visualizations of model-generated antibody structures bound to the SARS-CoV-2 RBD \textbf{(a)} Relative to a fixed antigen, most model-generated antibodies (green) are predicted to bind with a noticeable rotation in binding pose compared to the WT conformation (blue). \textbf{(b)} Our model suggests several mutations frequently, in particular Ala105Pro may stabilize the CDR loop. \textbf{(c)} The buried Lys99Trp mutation interacts with multiple other aromatic residues across the interface.}
    \label{fig:rbd-ag-structure}
\end{figure*}

\paragraph{Co-teaching}
We evaluate the co-teaching module with two variants:
(1) \textit{w/o energy}: using the trained predictor on limited labeled data only.
(2) \textit{w/o selection}: training on pairwise discrete data without sample selection.
%
As shown in Table~\ref{tab:ablation}, both variants reduce the \textit{IMP} metric, highlighting the effectiveness of the module. Notably, \textit{w/o energy} performs worse than \textit{w/o selection}, demonstrating the value of biophysical energy data.
%
We also observe that a better-trained predictor improves specificity: our method achieves the best \textit{Sim}, while \textit{w/o selection} ranks second. This likely results from the predictor’s role in estimating antigen-specific binding energy, leading to greater specificity.

Additionally, we report the Spearman's rank correlation coefficient $R$ (Spearmanr) on the test set. We isolate $10$ antigens from the total dataset, with each antigen paired with $77$ sdAbs. We calculate Spearman's $R$ for each antigen and present the average across the $10$ antigens.
%
The models without energy data achieve $R$ values of $0.0956$ for the sequence-based predictor and $-0.0043$ for the structure-based predictor, respectively, reflecting the limitations of the $120$ labeled entries.
%
By utilizing biophysical energy data for direct fine-tuning, the sequence-based predictor reaches a coefficient of $0.40$, while the structure-based predictor achieves $0.50$. While not state-of-the-art for antibody binding energy prediction in general, these values demonstrate the effectiveness of our approach when limited data is available.
%
Sample selection further improves performance, with the sequence-based predictor achieving a coefficient of $0.51$ and the structure-based predictor reaching $0.52$. These results highlight the benefits of using biophysical energies and sample selection to enhance prediction accuracy.



\subsection{Case Study}

To further understand how \textit{AffinityFlow} generates mutations to improve binding, we analyze the structures of our proposed mutants and the wild-type of a single-domain antibody (sdAb) known to bind the SARS-CoV-2 receptor-binding domain (RBD)~\cite{yao2021highaffinity}. We generate $30$ mutated structures, with half containing mutations only in the CDR3 loop and half having mutations across all CDRs. We use Rosetta to calculate binding energies (\(\Delta\Delta G\)) and other interface metrics relative to the wild-type structure (PDB ID 7D30).

All computed structures show \(\Delta\Delta G < 0\), suggesting that the designed antibodies bind the antigen more tightly than the native sequence. However, we do not observe any correlation between \(\Delta\Delta G\) and which CDRs are allowed to mutate. We measure other interface metrics (dSASA, shape complementarity) for all 30 structures and compare these values with those computed for native antibody-antigen interfaces in the PDB~\cite{adolf2018rosettaantibodydesign}. The results align well with natural structures, demonstrating that our model preserves the correct shape profile of the binding surface. Interestingly, despite conserving the binding interface shape, most mutants (21/30) dock with a rotated binding pose of approximately 67 degrees (Figure~\ref{fig:rbd-ag-structure}a). This rotation shifts interactions away from CDR1 and toward stronger interactions in CDR2 and CDR3.

Certain mutations occur frequently across all model-proposed antibodies, indicating that the model focuses attention on these residues. Notably, Lys99Gly, Ala105Pro, and Asp109Gly appear often, regardless of whether mutations are restricted to the CDR3 loop or allowed across all positions (Figure~\ref{fig:rbd-ag-structure}b). We believe that the Ala105Pro mutation stabilizes the CDR3 loop into an optimal conformation for this antigen. Using a Random Forest regression on mutation features, we find that the rarer Ala105Leu mutation contributes most to improving \(\Delta\Delta G\), likely by increasing hydrophobicity at the interface and promoting assembly.

Most intriguingly, one model-generated sequence includes a Lys99Trp mutation, an unusual amino acid to insert within an interface. Visual examination reveals that this tryptophan residue is inserted such that it creates $\pi$-$\pi$ interactions with two aromatic residues across the interface as well as providing stabilizing interactions with a tyrosine on the antibody itself (Figure \ref{fig:rbd-ag-structure}c). This mutation is particularly interesting since, in the development of the synthetic single domain antibody which we use as our case study, \cite{Li2020potent} made a single point mutation mutant, MR17m, with a single Lys99Tyr substitution. This mutant shows enhanced neutralization activity to the original MR17, with an IC50 of $0.50$ $\mu g/mL$. To further improve this activity, the authors suggest the same Lys99Trp mutation proposed by our model but do not appear to have tested it.

Our structural analysis of mutant and wild-type antibody structures reveals several key insights into the nature of mutations governing antibody-antigen binding. These results validate our computational approach and also highlight its potential to guide rational design of improved antibodies against SARS-CoV-2 and other pathogens, opening new avenues for therapeutic development.




\section{Conclusion}
%
We present \textit{AffinityFlow} for optimizing antibody sequences, introducing several key innovations. First, we develop an AlphaFlow-based \textit{alternating optimization} framework that leverages predictor guidance to steer structure generation toward high binding affinity, followed by targeted sequence mutations. Second, we propose a \textit{co-teaching} module that integrates insights from noisy biophysical energies to refine both structure- and sequence-based predictors.
%
Our method achieves state-of-the-art performance in affinity maturation experiments across functionality, specificity, and rationality metrics, demonstrating the effectiveness of \textit{AffinityFlow} in advancing antibody sequence design.


\section{Impact Statement}
Antibody affinity maturation aims to enhance the binding affinity of antibodies to their target antigens, which has broad implications in therapeutic development. This research has the potential to significantly improve the efficacy of antibody-based treatments for various diseases, including cancer, autoimmune disorders, and infectious diseases. For instance, optimizing antibodies against emerging pathogens could play a crucial role in mitigating future pandemics and saving millions of lives.
%
While this work offers substantial societal benefits, we acknowledge the potential for dual-use concerns. Advances in antibody affinity maturation could, in principle, be misused to develop harmful applications, such as targeting specific biomolecules for malicious purposes. As researchers, we are committed to raising awareness of these risks and promoting ethical use of these methods.
%
We firmly believe that the potential benefits of this research far outweigh the risks, given its promise to address critical global health challenges. Additionally, we emphasize the importance of community oversight and regulatory frameworks to mitigate misuse.
%
In this study, we have focused solely on advancing machine learning methodologies for antibody optimization, and we do not foresee any immediate ethical concerns associated with this work.


\bibliography{main}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Related Work}
\label{appendix: related_work}



\paragraph{Generative Protein Modeling}
Generative protein modeling primarily includes sequence-based language models and structure-based score generative models. Language models are trained on protein sequence datasets using masked prediction~\cite{rives2019biological} or auto-regressive prediction~\cite{ferruz2022protgpt2}. These models are often fine-tuned for specific domains like antibodies, with examples including AbLang~\cite{Olsen2022}, AntiBERTa~\cite{leem2022deciphering}, IgLM~\cite{shuai2021generative}, and nanoBERT~\cite{hadsund2024nanobert}.
%
Additionally, various sequence optimization strategies have been investigated~\cite{chen2023bidirectional, chan2021deep}.
%
Language models have also been explored for modeling tokenized protein structures~\cite{hayes2024simulating, su2023saprot}.
%


Score-based models, such as diffusion-based and flow matching models, mainly focus on generating protein structures. \textbf{(1)} Diffusion-based models like RFdiffusionAA~\cite{krishna2024generalized} and AlphaFold3~\cite{abramson2024accurate} generate structures through coordinate denoising. RFdiffusionAA has been applied to antibody design~\cite{bennett2024atomically}, but its code is not open-sourced. Chroma~\cite{ingraham2023illuminating} introduces property-specific guidance into diffusion models but does not research antibody design. Similarly, \cite{kulyte2024improving} incorporates force-field guidance but struggles to capture realistic structures due to the simplicity of the diffusion model.
%
\textbf{(2)} Flow matching models have shown greater effectiveness and efficiency compared to diffusion models. Recent studies like AlphaFlow~\cite{jing2024alphafold} and FoldFlow-2~\cite{huguet2024sequence} explore sequence-conditioned flow matching for protein structure generation. In this work, we utilize the AlphaFlow framework for antibody sequence design due to its demonstrated effectiveness.
%
It is worth noting that score-based models have also been applied to model discrete biological sequences~\cite{campbell2024generative, frey2023protein, li2024full, ikram2024antibody} and to broader design tasks~\cite{krishnamoorthy2023diffusion, chen2024robust, yuan2024design}

\paragraph{Co-teaching}
%
Co-teaching~\cite{han2018co, yuan2024importance, chen2024parallel} is a robust technique for addressing label noise by utilizing two collaborative models. Each model identifies small-loss samples from a noisy mini-batch to train the other. Co-teaching is conceptually related to decoupling~\cite{malach2017decoupling} and co-training~\cite{blum1998combining}, as all these approaches involve collaborative learning between two models. 
%
While sample selection techniques are commonly used to identify or reweight clean data from noisy datasets~\cite{ren2018learning, chen2021generalized, chen2022gradient}, in our study, we adapt co-teaching to work with biophysical binding energy data rather than a noisy dataset. Specifically, the sequence-based predictor identifies clean samples for training the structure-based predictor, and vice versa.
%







\section{Predictor Guidance in Flow Matching}
\label{appendix: cls}

According to Lemma1 in~\cite{zheng2023guided}, 
%
\begin{equation}
    {\Tilde v}(\boldsymbol{x}_t, t, \Delta G; \boldsymbol{\theta}) = a_t\boldsymbol{x}_t + b_t\nabla_{\boldsymbol{x}_t} \log p_{\boldsymbol{\beta}}(\boldsymbol{x}_t, t \mid \Delta G)
\end{equation}
%
Based on this, we can derive:
\begin{equation}
\begin{aligned}
    {\Tilde v}(\boldsymbol{x}_t, t, \Delta G; \boldsymbol{\theta}) &= a_t\boldsymbol{x}_t + b_t\nabla_{\boldsymbol{x}_t} \log p_{\boldsymbol{\beta}}(\boldsymbol{x}_t, t) + b_t\nabla_{\boldsymbol{x}_t} \log p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t) \\
    &=  {\Tilde v}(\boldsymbol{x}_t, t; \boldsymbol{\theta}) + b_t\nabla_{\boldsymbol{x}_t} \log p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t)  
\end{aligned}
\end{equation}

In our case, $b_t = \frac{1-t}{t}$, and this leads to:
\begin{equation}
    {\Tilde v}(\boldsymbol{x}_t, t, \Delta G; \boldsymbol{\theta}) =  {\hat v}(\boldsymbol{x}_t, t; \boldsymbol{\theta}) + \frac{1-t}{t} \nabla_{\boldsymbol{x}_t} \log p_{\boldsymbol{\beta}}(\Delta G \mid \boldsymbol{x}_t, t).
\end{equation}
%
This guided flows approach is further explored in \cite{yuan2025paretoflow}, where multiple predictors are employed to steer the sample generation process.


\section{Computation Approximation}
\label{appendix: approximations}

The guided vector field is defined by:
\begin{equation}
    \Tilde{v}(\boldsymbol{x}_t, t, \Delta G; \boldsymbol{\theta}) =  \hat{v}(\boldsymbol{x}_t, t; \boldsymbol{\theta}) - \gamma\frac{1-t}{t} \nabla_{\boldsymbol{x}_t} \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t)).
\end{equation}
%
We compute $\nabla_{\boldsymbol{x}_t} \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t))$ as:
\begin{equation}
    \nabla_{\boldsymbol{x}_t} \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t)) = \frac{\partial \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t))}{\partial \boldsymbol{\hat{x}}_1}\frac{ \partial \boldsymbol{\hat{x}}_1(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t}
\end{equation}
%
As $t$ approaches $1$, \(\boldsymbol{\hat{x}}_1\) closely approximates \(\boldsymbol{x}_t\), allowing for the simplification:
\begin{equation}
    \frac{\partial \boldsymbol{\hat{x}_1}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t} \approx \boldsymbol{I},
\end{equation}
where \(\boldsymbol{I}\) represents the identity matrix. Consequently, we approximate the gradient as:
\begin{equation}
    \nabla_{\boldsymbol{x}_t} \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1(\boldsymbol{x}_t)) \approx \frac{\partial \hat{f}_{\boldsymbol{\beta}}(\boldsymbol{\hat{x}}_1)}{\partial \boldsymbol{\hat{x}}_1}
\end{equation}


\section{Computational Efficiency}
\label{appendix: comp_efficiency}

All experiments are conducted on a g5.24xlarge server equipped with GPUs with $23$GB of memory.
%
One iteration of our alternating optimization framework takes approximately $10$ minutes for a protein of length $500$.
%
Language model-based methods are more computationally efficient, with processing times of $18.3$ seconds for ESM, $13.0$ seconds for Ablang, and $11.4$ seconds for nanoBert per sample. However, our method consistently produces significantly better designs than these methods, as discussed.
%
In applications such as antibody design, the most time-consuming and costly stage is often the evaluation of properties in wet-lab experiments. Thus, the differences in computation time between methods for generating high-performance designs are less significant in practical production settings, where optimization performance is prioritized over computational speed.
%
This is consistent with the discussions in A.7.5 Computational Cost~\cite{chen2023bidirectional}.



\section{Hyperparameter Analysis}
\label{appendix: hyper_sensitivity}


\begin{figure*}
\centering
\begin{minipage}[t]{.48\textwidth}
  \centering
    \includegraphics[width=.95\columnwidth]{Figures/gamma_flow.pdf}
    \captionsetup{width=.90\linewidth}
    %\caption{a}
    \caption{The antibody metrics versus scaling factor $\gamma$, normalized to their values at $\gamma$ \textbf{to} those with $\gamma=5.0$.}
    %\caption{The ratio of the high-scoring design score for different predefined target scores to that of the score $10$.  }
    \label{fig: target}
\end{minipage}%
\begin{minipage}[t]{.48\textwidth}
  \centering
    \includegraphics[width=.95\columnwidth]{Figures/step_flow.pdf}
    \captionsetup{width=.90\linewidth}
    \caption{The three antibody metrics versus scaling factor $T$, normalized to their values at $T=3$.}
    %
    \label{fig: steps}
\end{minipage}
\end{figure*}


This section examines the sensitivity of our method to various hyperparameters—namely, the scaling factor ($\gamma$) and the number of sampling steps ($T$) on CDR-H3 with $10$ antigens.
%
The reported metrics are normalized by dividing by the default hyperparameter result to facilitate comparative analysis.

\noindent \textbf{Scaling Factor ($\gamma$):}
%
The effect of varying $\gamma$ is investigated with values $0.0$, $2.5$, $5.0$, $7.5$, and $10$, and $\gamma=5.0$ as the standard setting. 
%
%Results are normalized against the performance at $\gamma=2$ to highlight variations in efficacy.
%
As indicated in Figure~\ref{fig: target}, the \textit{Sim} and \textit{Nat} metrics are stable across the range of $\gamma$. However, below $\gamma \sim 5.0$ the \textit{IMP} metric drops substantially, presumably because there is insufficient exploration of alternate backbone conformations when $\gamma$ is small. 

%performance of the three metrics remains stable across changes in $\gamma$, and improves from $\gamma=0.0$ to $\gamma=5.0$ for the \textit{IMP} metric, demonstrating the effectiveness of increasing predictor guidance.

\noindent \textbf{Number of Sampling Steps ($T$):}
We analyze the impact of the number of sampling steps $T$ on the effectiveness of our method.
%
The normalized metric is plotted as a function of $T$ in Figure~\ref{fig: steps}. Again the \textit{Sim} and \textit{Nat} metrics are relatively unaffected by the choice of $T$, but \textit{IMP} requires $3$ sampling steps for maximum benefit. Again this suggests that more exploration of the conformational space improves the final design.
%
%This suggests that a single step is insufficient to fully incorporate predictor guidance into the sampling process, requiring multiple steps for optimal results.







\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


