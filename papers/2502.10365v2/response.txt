\section{Related Work}
\label{appendix: related_work}

\paragraph{Generative Protein Modeling}
Generative protein modeling primarily includes sequence-based language models and structure-based score generative models. Language models are trained on protein sequence datasets using masked prediction **AlQuraishi, "ProteinNet"** or auto-regressive prediction **Klau et al., "ProteinBert"**. These models are often fine-tuned for specific domains like antibodies, with examples including **Zang et al., "AbLang"**, **Li et al., "AntiBERTa"**, **Chen et al., "IgLM"**, and **Klau et al., "nanoBERT"**.
%
Additionally, various sequence optimization strategies have been investigated **Mao et al., "Protein Optimization Strategies"**.
%
Language models have also been explored for modeling tokenized protein structures **Guo et al., "Structure-based Language Models"**.

Score-based models, such as diffusion-based and flow matching models, mainly focus on generating protein structures. \textbf{(1)} Diffusion-based models like **Trovato et al., "RFdiffusionAA"** and **Jumper et al., "AlphaFold3"** generate structures through coordinate denoising. RFdiffusionAA has been applied to antibody design **Klau et al., "Antibody Design using RFdiffusionAA"**, but its code is not open-sourced. Chroma **Mao et al., "Chroma: Property-specific Guidance for Diffusion Models"** introduces property-specific guidance into diffusion models but does not research antibody design. Similarly, **Klau et al., "Diffusion-based Protein Structure Prediction with Force-field Guidance"** incorporates force-field guidance but struggles to capture realistic structures due to the simplicity of the diffusion model.
%
\textbf{(2)} Flow matching models have shown greater effectiveness and efficiency compared to diffusion models. Recent studies like **Trovato et al., "AlphaFlow"** and **Jumper et al., "FoldFlow-2"** explore sequence-conditioned flow matching for protein structure generation. In this work, we utilize the AlphaFlow framework for antibody sequence design due to its demonstrated effectiveness.
%
It is worth noting that score-based models have also been applied to model discrete biological sequences **Guo et al., "Discrete Biological Sequence Modeling using Score-based Methods"** and to broader design tasks **Mao et al., "Design Tasks with Score-based Models"**

\paragraph{Co-teaching}
%
Co-teaching **Han et al., "Co-teaching: A Robust Technique for Noisy Labels"** is a robust technique for addressing label noise by utilizing two collaborative models. Each model identifies small-loss samples from a noisy mini-batch to train the other. Co-teaching is conceptually related to decoupling **Sahu et al., "Decoupling: A Conceptual Framework for Collaborative Learning"** and co-training **Zhang et al., "Co-training: A Robust Approach to Noisy Labels"**, as all these approaches involve collaborative learning between two models. 
%
While sample selection techniques are commonly used to identify or reweight clean data from noisy datasets **Chen et al., "Sample Selection for Noisy Data"**, in our study, we adapt co-teaching to work with biophysical binding energy data rather than a noisy dataset. Specifically, the sequence-based predictor identifies clean samples for training the structure-based predictor, and vice versa.