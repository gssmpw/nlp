\section{Related Work}
\label{appendix: related_work}



\paragraph{Generative Protein Modeling}
Generative protein modeling primarily includes sequence-based language models and structure-based score generative models. Language models are trained on protein sequence datasets using masked prediction~\cite{rives2019biological} or auto-regressive prediction~\cite{ferruz2022protgpt2}. These models are often fine-tuned for specific domains like antibodies, with examples including AbLang~\cite{Olsen2022}, AntiBERTa~\cite{leem2022deciphering}, IgLM~\cite{shuai2021generative}, and nanoBERT~\cite{hadsund2024nanobert}.
%
Additionally, various sequence optimization strategies have been investigated~\cite{chen2023bidirectional, chan2021deep}.
%
Language models have also been explored for modeling tokenized protein structures~\cite{hayes2024simulating, su2023saprot}.
%


Score-based models, such as diffusion-based and flow matching models, mainly focus on generating protein structures. \textbf{(1)} Diffusion-based models like RFdiffusionAA~\cite{krishna2024generalized} and AlphaFold3~\cite{abramson2024accurate} generate structures through coordinate denoising. RFdiffusionAA has been applied to antibody design~\cite{bennett2024atomically}, but its code is not open-sourced. Chroma~\cite{ingraham2023illuminating} introduces property-specific guidance into diffusion models but does not research antibody design. Similarly, \cite{kulyte2024improving} incorporates force-field guidance but struggles to capture realistic structures due to the simplicity of the diffusion model.
%
\textbf{(2)} Flow matching models have shown greater effectiveness and efficiency compared to diffusion models. Recent studies like AlphaFlow~\cite{jing2024alphafold} and FoldFlow-2~\cite{huguet2024sequence} explore sequence-conditioned flow matching for protein structure generation. In this work, we utilize the AlphaFlow framework for antibody sequence design due to its demonstrated effectiveness.
%
It is worth noting that score-based models have also been applied to model discrete biological sequences~\cite{campbell2024generative, frey2023protein, li2024full, ikram2024antibody} and to broader design tasks~\cite{krishnamoorthy2023diffusion, chen2024robust, yuan2024design}

\paragraph{Co-teaching}
%
Co-teaching~\cite{han2018co, yuan2024importance, chen2024parallel} is a robust technique for addressing label noise by utilizing two collaborative models. Each model identifies small-loss samples from a noisy mini-batch to train the other. Co-teaching is conceptually related to decoupling~\cite{malach2017decoupling} and co-training~\cite{blum1998combining}, as all these approaches involve collaborative learning between two models. 
%
While sample selection techniques are commonly used to identify or reweight clean data from noisy datasets~\cite{ren2018learning, chen2021generalized, chen2022gradient}, in our study, we adapt co-teaching to work with biophysical binding energy data rather than a noisy dataset. Specifically, the sequence-based predictor identifies clean samples for training the structure-based predictor, and vice versa.
%