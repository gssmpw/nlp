\section{SyncMind: Agent Out-of-Sync Recovery}
\label{Section: framework}

% Aiming for systematic evaluation, we introduce \textit{SyncMind} (\fref{fig:figure 2}), a human-AI collaboration framework that measures agent \textit{out-of-sync} recovery in collaborative software engineering.

Tackling agent \textit{out-of-sync} (\Sref{Section: Definition of Agent Out-of-Sync}), we introduce our \textbf{\textit{SyncMind}} framework (\fref{fig:figure 1}) to systematically measure agent \textit{out-of-sync} recovery in two key dimensions: \textit{recovery effectiveness} via two types of recovery (\Sref{Section: Recovery Settings}) and \textit{resource efficiency} through resource-aware \textit{out-of-sync} recovery (\Sref{Section: Resource Awareness Module}).

\subsection{Definition of Agent Out-of-Sync}
\label{Section: Definition of Agent Out-of-Sync}

In collaborative environments, a state of `\textit{out-of-sync}' arises when a collaborator's belief state deviates from the project's state due to missed updates from other team members (\fref{fig:figure 1}). We propose the formal definition of `\textit{out-of-sync}' state, which applies to both human and AI agents.

% In collaborative workflows, \textit{out-of-sync} occurs when a collaborator's working state diverges from the project state due to unawareness of other collaborators' updates (\fref{fig:figure 1}). We formally define agent \textit{out-of-sync} as follows, where the agent can refer to either a human agent or an AI agent:

Let $S_i$ be the true world state at time $T_i$, and $B_i$ be an agent's belief state. Starting from $T_i$ when the agent begins a task, the agent becomes \textit{out-of-sync} at $T_k$ ($T_i<T_k$) if any of the following conditions are satisfied:
% \vspace{-0.5\baselineskip}
\vspace{-5pt}
\begin{enumerate}[leftmargin=*,itemsep=1pt,topsep=0pt,parsep=0pt,label=(\arabic*)]
    \item \textbf{\textit{Knowledge gap}:} $\exists$ Update $U$ at time $T_j$ ($T_i<T_j<T_k$) where the agent lacks knowledge of $U$.
    \item \textbf{\textit{State mismatch}:} $B_k \neq S_k$.
    \item \textbf{\textit{Task failure}:} Task completion based on $B_k$ fails to achieve intended outcomes in $S_k$.
\end{enumerate}

Recovery from an \textit{out-of-sync} state therefore requires:
\vspace{-5pt}
\begin{enumerate}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt,label=(\arabic*)]
    \item Identify the root causes ($U$) of the state mismatch ($B_k \neq S_k$).
    \item Acquire information of the missing update $U$.
    \item Update its belief state such that $B_n = S_n$ at some future time $T_n$ ($T_n>T_k$).
\end{enumerate}

% Originating from the agent \textit{out-of-sync} challenge (\fref{fig:figure 1}), this formalization provides the foundation for the design of our \textit{out-of-sync} recovery framework (\fref{fig:figure 2}).



\subsection{Agent Out-of-Sync Recovery}
\label{Section: Recovery Settings}

In \textit{SyncMind} (\fref{fig:figure 2}), an agent updates its \textit{out-of-sync} belief state to attain $B_n=S_n$ through two types of recovery: 



%\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=18pt]
%    \item \textbf{\textit{Env} Exploration.} (1) \textit{Env interaction}: Agents can directly interact with the \textit{Python} programming environment \cite{OpenHands}. (2) \textit{Prior experience}: Agents learn from previous attempts and feedback to refine their approach.
%    \item \textbf{\textit{Collaborator} Assistance.} (1) \textit{Execution feedback}: Agents can propose solutions verified through execution tests. (2) \textit{Knowledge Acquisition}: Agents can proactively request clarification on recent updates, enabling direct knowledge transfer from collaborators.
%\end{itemize}

% Based on these recovery mechanisms, we evaluate agent performance in two recovery settings:


\begin{itemize}[noitemsep,topsep=0pt,parsep=2pt,partopsep=0pt,leftmargin=*]
    \item \textbf{Independent Recovery.} Operating autonomously, independent agents update their world beliefs through \textcolor{fig2_env}{\textbf{interacting with \textit{Environment} (\textit{Env})}} and  \textcolor{fig2_code}{\textbf{proposing solutions}}, besides their reflection on prior experience and feedback.
    \item \textbf{Collaborative Recovery.} Collaborative agents can also take advantage of \textcolor{fig2_ask}{\textbf{collaborator assistance}} to update their belief states by interacting with other collaborative agents. 
    % \textcolor{fig2_env}{\textbf{\textit{Env}}} as well as .
\end{itemize}



\subsection{Resource-Aware Recovery}
\label{Section: Resource Awareness Module}



\begin{figure}[H]
\begin{center}
\begin{small}
\vspace{-0.8em}
    \includegraphics[width=1\linewidth]{sections/figs/figure3.png}
    \vspace{-1.8em}
    \caption{\textbf{Resource-Aware \textit{Out-of-Sync Recovery}.} We introduce resource-aware recovery by mapping resource consumption to each \textit{out-of-sync} recovery task.}
    \vspace{-1.5em}
    \label{fig:figure 3 (resource awareness)}
\end{small}
\end{center}
\end{figure}



To reflect real-world resource constraints in collaborative environments, we integrate a resource awareness module into \textit{SyncMind} (\fref{fig:figure 3 (resource awareness)}).
% 
This module tracks and constrains two dimensions of resources: 
(1) \textit{recovery time} measured as the number of turns taken for an agent to recover, 
and (2) hypothetical \textit{cost} that quantify financial resources consumed through the course of recovery (\textit{e.g.,} computing resources for debugging and testing, \textcolor{fig1_human}{\textit{Human}}'s time and effort to answer \textcolor{fig1_agent}{\textit{Agent}}'s questions). 
%
% (1) \textit{Time Resources}: Number of interaction turns available for recovery;
% (2) \textit{Financial Resources}: Total budget allowance for different recovery actions;
% (3) \textit{Operation Costs}: Computational costs of collaborators in validating solutions and providing assistance.
%
This resource-aware \textit{out-of-sync} recovery framework measures how agents utilize and adapt their strategies under different resource constraints, enabling comparisons of efficiency between successful and failed recovery attempts across agentic systems.

% Our resource-aware \textit{out-of-sync} recovery framework enables us to measure agents' resource sensitivity and their resource-aware recovery capabilities, assess the resource efficiency of different recovery approaches, evaluate how agents adapt their recovery strategies under varying resource constraints, and compare the resource utilization patterns between recovery successes and failures as well as among different agents.

% Through resource-aware recovery, we can systematically study how agents balance recovery effectiveness with resource efficiency, providing insights into developing more resource-conscious collaborative systems.






\begin{figure*}[!t]
\begin{center}
\begin{small}
    \vspace{-0em}    % Adjust vertical spacing before figure if needed
    \centering
    % \vspace{-10pt}  % Reduces space above the figure
    \includegraphics[width=1\linewidth]{sections/figs/figure4.png}
    \vspace{-1.8em}  % Reduces space below the figure
    \caption{\textbf{Agent \textit{Out-of-Sync} Benchmark Construction.} A systematic benchmark construction approach (\Sref{Section: Benchmark Construction}).}
    \label{fig:figure 4}
    \vspace{-1.5em}
\end{small}
\end{center}
\end{figure*}














\section{SyncBench: Agent Out-of-Sync Benchmark}
\label{Section: Benchmark}

% We construct our benchmark \textbf{\textit{SyncBench}} (\Sref{Section: Benchmark Construction}) for the agent \textit{out-of-sync} challenge, with tailored metrics (\Sref{Section: Evaluation Metrics}).

% To systematically evaluate agents’ \textit{out-of-sync} recovery capabilities, we develop \textit{SyncBench} by simulating agent \textit{out-of-sync} in real-world collaborative software engineering.



\subsection{Benchmark Construction}
\label{Section: Benchmark Construction}
\looseness=-1
Aligning with real-world \textit{out-of-sync} scenarios, our benchmark construction method is applicable to \textit{Python}-based GitHub repositories with existing \textit{unit tests}.
% Including environment setup (e.g., \textit{pyproject.toml}, \textit{setup.py}) is a plus, while specifying packages serves an alternative.
% Our construction method is therefore equipped with generalizability and adaptability to accommodate custom needs.
\textit{SyncBench} leverages 21 popular GitHub repositories and can be expanded to include additional repositories following our benchmark construction methodology (\Sref{Appendix: Benchmark Construction}).
% Details of the adopted GitHub repositories are described in~\Sref{Appendix: Benchmark Construction}. 
% meeting necessary prerequisites
% or be adaptively shrunk to smaller sizes through downsampling.
In accordance with the definition of agent \textit{out-of-sync} (\Sref{Section: Definition of Agent Out-of-Sync}), our benchmark construction implements a systematic pipeline that takes all three conditions into consideration:





\textbf{\textit{Env} Configuration.}
We employ \textit{Docker} \cite{docker_for_env_config} to configure isolated, reproducible, and executable testing environments tailored for our \textit{out-of-sync} recovery tasks.
% 
Each source repository is packaged into a dedicated \textit{Docker image} with complete codebase, dependencies, and validation infrastructure for unit test execution.
% 
Each execution verification (\Sref{Section: Evaluation Metrics}) automatically creates an isolated \textit{container} instance with auto-removal upon completion, ensuring consistent and clean testing environments for reliable recovery evaluation.

\textbf{\textit{Out-of-Sync} Simulation.}
We first extract \textit{Python} functions and class methods (hereafter collectively referred to as \textit{functions}) from source repositories.
% where unit tests and imported dependencies are leveraged for for \textit{Caller} and \textit{Callee}, respectively.
For each extracted function, we employ its up-to-date state as ground truth ($S_2$), while obtaining the \textit{out-of-sync} belief state ($B_2$) by tracing its \textit{Git} history reversely until identifying a commit ($B_2$) where execution fails ($B_2 \neq S_2$).
% due to state divergence ($B_2 \neq S_2$)
% 
In this way, \textit{Caller} and \textit{Callee} are constructed through simulating unit test \textit{out-of-sync} and tested dependency \textit{out-of-sync}, respectively:
\textbf{(1) \textit{Caller}:} We roll back the testing function until it becomes \textit{out-of-sync};
% that allows mitigated difficulty in agents' localization and recovery of responsible unit test;
\textbf{(2) \textit{Callee}:} We roll back imported dependency for tested module \textit{out-of-sync}, thereby presenting higher task complexity---agents need to understand dependency relationships and localize the problematic imported modules.

\textbf{Multi-level Quality Filtering.}
For each \textit{out-of-sync} instance, we execute unit tests before and after \textit{out-of-sync} happens and use the parsed test outputs to filter for high-quality instances.
% 
Our \textit{parsing-based execution testing} (\Sref{Section: Evaluation Metrics}) requires the \textit{pass-to-fail} state divergence ($B_2 \neq S_2$):
(1) updated repository ($S_1$) passes the test to demonstrate ground truth validity, and (2) repository with the \textit{out-of-sync} function ($B_2$) fails the test to allow the \textit{out-of-sync} scenario to take shape.
To enhance data quality, we additionally apply a filter that retains only instances with their execution outputs comprising: (1) at least one execution error or unit test failure in $B_2$, (2) more than one passing test in $S_1$, (3) identical parsing result between $S_1$ and $S_n$.

% \footref{Footnote: Cost Constraints}
\textbf{Weighted Downsampling.}
In constructing our evaluation subset with 300 representative instances\footnote{
\label{Footnote: Cost Constraints}
Due to the costly expenditure of extensive model evaluations (on average $\$0.56/instance$, ranging from \textit{GPT-4o mini} with $\$0.02/instance$, to \textit{Claude-3.5-Sonnet} with $\$1.73/instance$), we downsample a subset of \textit{SyncBench} with 300 instances (\Sref{Section: Benchmark}).} 
across 21 repositories, we downsample each repository's data to less than 15 instances while maintaining the original patch distribution over all sampled data, thereby applying the same task complexity distribution to all downsampled instances.




\subsection{Benchmark Datasets}
\label{Section: Benchmark Datasets}

Constructing \textit{SyncBench} with two complementary datasets---\textit{Caller} and \textit{Callee} (\fref{fig:figure 4}), our initial extraction yields 24,332 instances (\tref{tab:table B2 (Benchmark)}).
Pruning the raw dataset to 8,461 instances via multi-level filtering, the evaluation subset is further reduced via weighted downsampling.
As such, we finalize our evaluation samples as 300 instances\footref{Footnote: Cost Constraints} with evenly distributed \textit{Caller} and \textit{Callee} samples (150 each). 

% \textit{Callee} and \textit{Caller}, through our systematic preprocessing to ensure data quality and representativeness.
% While \textit{Callee} dataset evaluates an agent's ability to re-synchronize out-of-sync repository modules being tested, \textit{Caller} dataset assesses recovery capacity in the unit tests themselves.

% \textbf{\textit{Callee} dataset:} Evaluating re-synchronization of repository modules under test, our \textit{Callee} dataset is constructed upon imported dependencies of each unit test. The obtained \textit{Callee} dataset presents higher task complexity as \textit{Callee} \textit{out-of-sync} recovery requires an agent’s localization of imported modules and its understanding of dependency relationships.

% \textbf{\textit{Caller} dataset:} \textit{Caller} dataset follows a similar construction process but focuses on unit tests themselves rather than tested modules. Therefore, our \textit{Caller} dataset depicts scenarios where unit tests themselves becomes \textit{out-of-sync}.















\subsection{LLM-Simulated Collaborators}
\label{Section: LLM-simulated Collaborators}
We leverage LLMs to simulate both agents (who enter \textit{out-of-sync} states $B_2$) and know-everything collaborators ($S_2$).
% This simulation approach offers several advantages:

\textbf{Agent Out-of-Sync.} We employ LLMs to power AI agents in \textit{out-of-sync} states, which allows belief states to become tractable and controllable throughout the recovery process. Meanwhile, this also supports the precise measurement of an agent's resource consumption and the systematic evaluation of an agent's recovery patterns.

\textbf{Simulating Know-Everything Collaborators.} 
Validated by single-turn experiments (\Sref{Section: Beneficial Collaborator Influence Limited By Disadvantageous Agent Initiative}), LLM-simulated know-everything collaborators are furnished with: (1) complete task context, including both $B_0-B_t$ and $S_0-S_t$ (where the agent seek assistance at $T_t$, $2<t<n$), (2) ground-truth solution to reach $B_n=S_n$, (3) update history ($U$ at $T_1$), and (4) task-specific response protocols (\Sref{Appendix D: LLM-simulated Human Answerer}).
% Our single-turn experiments (\Sref{Section: Ceiling Test}) validate the effectiveness of our LLM simulation approach.
% \xw{i commented the rest - I think they don't add additional info here}
% , supporting controlled experimentation with various collaboration settings and systematic evaluation of different LLM agents' recovery performance and behaviors.




% Comparing collaborator assistance effects on out-of-sync recovery of different LLM agents, \textcolor{red}{Would we want to add additional experiments on using different LLMs as the know-everything agent?}













\subsection{Evaluation Metrics}
\label{Section: Evaluation Metrics}
We propose five complementary metrics tailored for comprehensively evaluating agent \textit{out-of-sync} recovery:
% respectively for recovery success, localization accuracy, technical recovery capacity, collaboration initiative, and , recovery efficiency.

\textbf{Success Rate (SR).}
We evaluate recovery success (\eqref{Eq: Evaluation Metric 1 (SR)}) through a two-stage validation process: \textbf{\textit{(1) Execution Test}}: Execution success can be reached only if an agent's updated repository passes the test without errors (\textit{i.e.,} command exit code of 0). 
\textbf{\textit{(2) Parsing Validation}}: We compare the parsed test execution outputs of an agent's proposed solution against that of the ground-truth state (\textit{i.e.,} the original commit without issue). Recovery success requires all parsed output of test cases for the agent-proposed solution to exactly match the ground-truth values.
% Applying this two-stage evaluation, we calculate $SR$ as the ratio of successful recoveries to total test instances.
% A successful case is defined as one in which the agent correctly updates the out-of-sync function to pass the execution and validation criteria. 

{
\vspace{-1.0em}
\small
\begin{equation}
SR = \frac{\sum_{m \in \mathcal{M}} \mathbbm{1}(SR_m = 1)}{\sum_{m \in \mathcal{M}} \mathbbm{1}}
\label{Eq: Evaluation Metric 1 (SR)}
\end{equation}
\vspace{-1.8em}
}

{\small
where $\mathcal{M}$ represents the task space, $SR_m \in \{0,1\}$ suggests whether task $m$ achieves recovery success, $\mathbbm{1}(\cdot)$ is the indicator function that returns 1 when the condition is met and 0 otherwise.
}



\textbf{Localization Accuracy (LA).}
We evaluate an agent's ability to localize an \textit{out-of-sync} function at two levels: (a) \textit{file} ($LA_{file}$): accurately identifying the \textit{Python} file containing the \textit{out-of-sync} function; and (b) \textit{function} ($LA_{func}$): accurately pinpointing the specific \textit{out-of-sync} function.
% This dual-level accuracy measurement captures granularity of agent's out-of-sync detection capabilities.

{
\vspace{-0.5em}
\small
\begin{equation}
LA_f = \frac{\sum_{\mathcal{M}} \mathbbm{1}(LA_{f,m} = 1)}{\sum_{m \in \mathcal{M}} \mathbbm{1}}
\label{Eq: Evaluation Metric 2 (LA)}
\end{equation}
\vspace{-1.8em}
}

%\begin{equation}
%CSR_f = \frac{SR}{LA_f} \times 100\%, \quad f \in \{file, func\}
%\label{Eq: Conditional Success Rate (CSR).}
%\end{equation}

{\small
where $f \in \{file, func\}$ denotes the localization target type, $LA_{f,m} \in \{0,1\}$ indicates whether task $m$ achieves localization success for target type $f$.
}



\textbf{Conditional Success Rate (CSR).}
We evaluate agents' technical recovery abilities by conditioning recovery on localization success, which leads to $CSR_{file}$ and $CSR_{func}$:

{
\vspace{-1em}
\small
\begin{equation}
CSR_f = SR|_{LA_f=1} = \frac{\sum_{m \in \mathcal{M}} \mathbbm{1}(SR_m=1 \land LA_{f,m}=1)}{\sum_{m \in \mathcal{M}} \mathbbm{1}(LA_{f,m}=1)}
\label{Eq: Evaluation Metric 3 (CSR)}
\end{equation}
\vspace{-2em}
}

% {\small
% where $f \in \{file, func\}$ denotes the localization target type, $\mathcal{M}$ represents the \textit{out-of-sync} task space, $SR_m \in \{0,1\}$ indicates whether task $m$ achieves recovery success, $LA_{f,m} \in \{0,1\}$ indicates whether task $m$ achieves localization success for target type $f$, and $\mathbbm{1}(\cdot)$ is the indicator function that returns 1 when the condition is met and 0 otherwise.
% }



\textbf{Assistance Seeking Rate (ASR).}
We quantify a collaborative agent's willingness to collaborate as the proportion of recovery time (measured in turns of interactions) it adopts for proactive assistance-seeking.

{
\vspace{-1em}
\small
\begin{equation}
ASR = \frac{\sum_{m \in \mathcal{M}} \sum_{t \in \mathcal{T}_m} \mathbbm{1}(AS_t = 1)}{\sum_{m \in \mathcal{M}} \sum_{t \in \mathcal{T}_m} \mathbbm{1}}
\label{Eq: Evaluation Metric 4 (ASR)}
\end{equation}
\vspace{-2em}
}

{\small
where $\mathcal{T}$ represents the recovery time space, $\mathcal{T}_m (m \in \mathcal{M})$ suggests the total time in task $m$, $AS_t \in \{0,1\}$ indicates whether the agent seeks collaborator assistance in turn $t$.
}



\textbf{Recovery Efficiency.}
We compute the ratio of turns taken to the maximum time limit as the proxy for \textit{time efficiency}, thus excluding external influencing factors, like connection stability and memory capacity. \textit{Expense efficiency} is similarly calculated as the average financial expenditure rates.

{
\vspace{-1em}
\small
\begin{equation}
Eff_g = \frac{\sum_{m \in \mathcal{M}} \phi_g(m)}{\sum_{m \in \mathcal{M}} \psi_g(m)}
\label{Eq: Evaluation Metric 5 (Efficiency)}
\end{equation}
\vspace{-1.8em}
}

{\small
where $g \in \{time, expense\}$ denotes the efficiency type,
$\phi_{time}(m)=\sum_{t \in \mathcal{T}_m} \mathbbm{1}(a_t^m)$ counts turns taken for task $m$,
$\phi_{expense}(m)=\sum_{t \in \mathcal{T}_m} c(a_t^m)$ sums costs of actions for task $m$,
$\psi_{time}(m) = \mathcal{T}_{max}$ is the maximum time limit,
$\psi_{expense}(m) = \mathcal{C}_{max}$ is the maximum budget,
$a_t^m$ is the action taken at step $t$ in task $m$,
and $c(a)$ is the cost function for action $a$.
}

