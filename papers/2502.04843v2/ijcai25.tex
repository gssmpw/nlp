%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}
\usepackage{natbib} % for arxiv
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% add by user
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{makecell}
\usepackage{gensymb}
\def\swtwo{0.48\linewidth}
\def\swone{0.98\linewidth}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression}


% Single author syntax
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Feifei Li$^1$
\and
Qi Song$^2$\and
Chi Zhang$^3$\and
Hui Shuai$^4$\And
Rui Huang$^5$\\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
\emails
\{feifeili1, qisong, chizhang1\}@link.cuhk.edu.cn,
huishuai13@163.com,
    ruihuang@cuhk.edu.cn
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
    The task of estimating camera poses can be enhanced through novel view synthesis techniques such as NeRF and Gaussian Splatting to increase the diversity and extension of training data. However, these techniques often produce rendered images with issues like blurring and ghosting, which compromise their reliability. These issues become particularly pronounced for Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the pixel level. To mitigate the problems associated with unreliable rendered images, we introduce a novel filtering approach, which selectively extracts well-rendered pixels while discarding the inferior ones. This filter simultaneously measures the SCR model's real-time reprojection loss and gradient during training. Building on this filtering technique, we also develop a new strategy to improve scene coordinate regression using sparse inputs, drawing on successful applications of sparse input techniques in novel view synthesis. Our experimental results validate the effectiveness of our method, demonstrating state-of-the-art performance on indoor and outdoor datasets.
\end{abstract}

\section{Introduction}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.95\linewidth]{figures/cover.pdf}
\end{center}
   \caption{\textbf{Left}: Comparison of query and rendered images of the dataset 7Scenes and Cambridge Landmarks, revealing uneven rendering quality within frames, with some parts clear and others blurry or ghosted; 
   \textbf{Right}: Translation error versus training time, where "CoodiNet+" means using rendered images as query images for CPR method CoodiNet (LENS in this case); “DSAC*+” and "ACE+" denote the method combines NVS-rendered images and query images as training data for SCR method DSAC* and ACE. "ACE+PoI" denotes our method PoI (ACE-based);
   We can see that directly adding rendered data to the training set will increase training time to some extent, but performance will decrease for the SCR method. On the other hand, our PoI approach can improve the performance with an acceptable time increase.
   }
\label{fig:cover}
\end{figure*}

Visual localization, also known as camera relocalization, is a fundamental task in computer vision that involves estimating the 6-degree-of-freedom (6DOF) camera poses within a known scene based on input images. This task plays a crucial role in Simultaneous Localization and Mapping (SLAM)~\cite{izadi2011kinectfusion,mur2015orb,dai2017bundlefusion,tang2018ba-net} and has significant applications in areas such as autonomous driving, robotics, and virtual reality.

Learning-based methods for camera relocalization can be categorized into two main types: Camera Pose Regression (CPR) methods~\cite{purkait2018synthetic,chen2021direct,ng2021reassessing,taira2018inloc,moreau2022coordinet,moreau2022lens,chen2022dfnet} and Scene Coordinate Regression (SCR) methods~\cite{brachmann2021visual,brachmann2017dsac,esac,shotton2013scene,valentin2015exploiting,brachmann2023ace}. Between these, SCR frameworks are particularly favored due to their higher accuracy. However, both approaches require stringent sampling density of training data to ensure reliable pose estimations for arbitrary images captured within a specific scene. Manually collecting a sufficient number of training images is a time-consuming process, and obtaining the corresponding camera pose labels presents further difficulties.

In light of this, some CPR-based methods try to enrich the training set with synthetic data rendered by novel view synthesis (NVS) techniques. For example, LENS~\cite{moreau2022lens} employs NeRF to render views of average sampled novel poses, thereby augmenting the training dataset and treating these synthetic images similarly to real data without additional processing. Similarly, DFNet \cite{chen2022dfnet} utilizes NeRF-W~\cite{nerfw} for NVS and features a cross-domain design that helps to minimize the discrepancies between synthetic and query images, effectively bridging the gap between the two domains. 

Currently, there is no similar research within the SCR framework, and we raise the question of whether SCR-based pipelines can also benefit from synthetic images. To this end, we attempt to apply NVS for data augmentation within the SCR framework. However, we found that SCR methods, which rely on precise pixel-to-pixel (N2N) predictions, are particularly vulnerable to the quality of rendered images. This contrasts with CPR methods, which involve pixel-to-pose predictions and are less affected by image quality. As shown in the right section of Figure~\ref{fig:cover}, after expanding the training dataset with synthetic data through NVS, the CPR method shows significant improvement, while SCR performance declines, accompanied by a notable increase in training time. Directly training the SCR model with raw rendered images proves less effective than CPR methods and may even result in model collapse if the proportion of rendered images is excessively high.

% to be updated: the 3D-to-2D projection error of each pixel is employed as a criterion for whether the point is retained or not, and a rough to precise threshold setting is used for screening at different training stages
To address this issue, we designed a portable pixel of interest (PoI) module that serves as an effective filter for synthetic clues. Specifically, the 3D-to-2D projection error of each pixel is employed as a criterion for whether the point is retained or not, and a rough to precise threshold setting is used for screening at different training stages. As the training progresses, PoI gradually removes poorly rendered pixels and further leverages the remaining points alongside real data to train the network.

Moreover, we propose a coarse-to-fine variant of PoI to address the challenges of visual localization in extreme scenarios, especially where the amount of training data is limited. In the coarse stage, we receive all available synthetic data as input and gradually train a coarse model by self-pruning to remove noisy pixels. Following this, we fine-tune the coarse model using sparse real pixels. This method enables our PoI variant to efficiently leverage sparse input while ensuring strong pose estimation performance, even under difficult conditions.

The main contributions of our work are summarized as follows:
\begin{itemize}
\item We introduce PoI, a pixel-level filter designed to eliminate poorly rendered pixels for effective training data augmentation.
\item We present an innovative approach to tackle scene coordinate regression from sparse inputs.
\item Our method achieves state-of-the-art performance on both indoor and outdoor datasets.
\end{itemize} 


\section{Related Work}
\textbf{Camera Pose Regression} The CPR methods, i.e., to regress the camera pose from the given image directly, are the most naive ideas and most widely used in learning-based methods~\cite{kendall2015posenet,brachmann2016uncertainty,brahmbhatt2017mapnet,melekhov2017image,radwan2018vlocnet++,wang2020atloc,hu2020dasgil,arnold2022map,chen2022dfnet,shavit2022camera}. The most straightforward method implicitly uses CNN layers or MLP to represent the image-to-pose correspondence. PoseNet~\cite{kendall2015posenet} first proposes this using pre-trained GoogLeNet as the feature extractor. Then, several works focus on improving CPR through additional modules. Geomapnet~\cite{brahmbhatt2017mapnet} estimates the absolute camera poses and the relative poses between adjacent frames. AtLoc~\cite{wang2020atloc} uses a self-attention module to extract salient features from the image. Vlocnet++~\cite{radwan2018vlocnet++} adds a semantic module to solve the dynamic scene and improve the robustness for blockings and blurs. 
Marepo~\cite{chen2024marepo} first regresses the scene-specific geometry from the input images and then estimates the camera pose using a scene-agnostic transformer. 
The CPR method has achieved excellent efficiency and simplification of the framework, but there is still room for improvement in accuracy. 

\textbf{Scene Coordinate Regression} Recently, the SCR methods~\cite{shotton2013scene,brachmann2017dsac,brachmann2018learning,esac,massiceti2017random,li2018scene,brachmann2021visual,liu2025egfs} have achieved better performance in terms of the accuracy compared with the CPR methods. The SCR method aims to estimate the coordinates of the points in 3D scenes instead of relying on the feature extractor to find salient descriptors, as in CPR methods. SCR was initially proposed using the random forest for RGB-D images~\cite{shotton2013scene}. Recently, estimating scene coordinates through RGB input has been widely studied. ForestNet\cite{massiceti2017random} compares the benefits of Random Forest (RF) and Neural Networks in evaluating the scene coordinate and camera poses. ForestNet also proposes a novel method to initiate the neural network from an RF. DSAC~\cite{brachmann2017dsac}, DSAC++~\cite{brachmann2018learning} devise a differentiable RANSAC, and thus the SCR method can be trained end-to-end. ESAC~\cite{esac} uses a mixture of expert models (i.e., a gating network) to decide which domain the query belongs to, and then the complex SCR task can be split into simpler ones. DSAC*~\cite{brachmann2021visual} extends the previous works to applications using RGB-D or RGB images, with/without the 3D models. 
This means that in the minimal case, only RGB images will be used as the input to DSAC*, just like most CPR methods. 
More information about the 3D structure will be utilized for most SCR methods than CPR ones. However, approaches like DSAC* can achieve more accurate estimations even if the input is the same as the CPR method.
ACE~\cite{brachmann2023ace} and GLACE~\cite{wang2024glace} abandon the time-consuming end-to-end supervision module and shuffle all pixels of the scene to improve training efficiency. ACE and GLACE use only RGBs without extra 3D geometry information and achieve comparable accuracy compared with former methods.

Despite the progress of CPR and SCR methods, both methods still have great problems in data collection and labeling. Therefore, efficient data collection and labeling methods or alternatives with similar effects are needed.

\textbf{Neural render pose estimation (NRP)}
A major challenge for visual localization methods is to collect appropriate photos to cover the entire scene. Essentially, the number and distribution of image sets for training are difficult to decide.
For example, most outdoor scenes collect data along roads, such as Cambridge landmarks~\cite{kendall2015posenet}. For indoor datasets (such as the 7Scenes~\cite{shotton2013scene} dataset), all translations and orientations within the scene are considered.

To fulfill the diverse requirements of data collection, some works try to use more flexible NVS to render synthetic views instead of collecting extra data~\cite{chen2021direct,ng2021reassessing,purkait2018synthetic,taira2018inloc,moreau2022lens,chen2022dfnet}, where NVS is the method to render synthetic images from the camera poses, which can verify the accuracy of 3D reconstruction, especially for implicit reconstruction methods like NeRF~\cite{mildenhall2021NeRF}, and 3DGS~\cite{kerbl20233dgs}.
INeRF~\cite{yen2021iNeRF} applies an inverted NeRF to optimize the estimated pose through color residual between rendered and observed images. However, the initially estimated poses are significant in guaranteeing the convergence of outputs.
LENS~\cite{moreau2022lens} samples the poses uniformly throughout the area and trains a NeRF-W~\cite{nerfw} to render the synthetic images. Then, rendered images and poses work as the additional training data for the pose regression network. The limitation of LENS lies in the costly offline computation for dense samples. DFNet uses direct feature matching between observed and synthetic images generated by histogram-assisted NeRF. The feature match approach is proposed to extract observed or generated images' cross-domain information. All these methods combine the NVS module and the CPR module to optimize the performance of the absolute pose estimation of the photos.

Unlike the former methods, we propose using an SCR rather than the CPR method with proposed NVS rules to improve the camera pose estimation.
First, we design novel pose sampling methods to meet multiple requirements of different datasets. To address the problem of varying lighting conditions, we use exposure histogram-assisted 3DGS as the baseline to sample new views of multiple exposures for each sampled pose in outdoor datasets.
% Second, since the quality of images rendered by NVS is lower than that of captured photos, we propose an architecture to narrow the gap between feature maps obtained from captured photos and those obtained from rendered images.
Second, we propose a pixel filter to remove bad pixels in rendered images and use query frames and remaining rendered pixels to improve the estimation.

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1.\linewidth]{figures/pipeline.pdf}
\end{center}
   \caption{Pipeline of our proposed methods: \textbf{(a) data formulation}: We first sample a group of synthesized camera poses $P_n$ according to the query training pose $P_q$ using `GS' (grid sampling). Then, we render the synthesized views $I_n$ based on the sampled poses $P_n$ through the novel view synthesis model. 
   % After that, we regress the scene coordinates $sc_n$ through PoI. Finally, we use a PNP-Bsed Ransac to estimate the final camera pose. 
   \textbf{(b) architecture of PoI}: First, a pre-trained scene-irrelevant backbone is applied to extract the features of the input query photos $I_q$ and the synthesized novel images $I_n$. Then, the filter is applied to the features of the rendered images and gets the features of interest. After that, we combine the query features with the filtered novel features and shuffle the pixel-aligned features to get the aggregation. Finally, we estimate the scene coordinates of the pixels using a scene-specific Head. The filtering algorithm is designed based on the reprojection error of the estimated scene coordinates.}
\label{fig:pipeline}
\end{figure*}


\section{Method}

In general, the pipeline of our proposed method is shown in Figure~\ref{fig:pipeline}. For the input query images $I_q$, and corresponding camera poses $P_q$, we first sample the novel camera pose $P_n$ using Grid Sampling (GS). Then we render novel views $I_n$ using 3DGS. Finally, we use PoI to estimate the scene coordinates through the input $I_q, I_n$. During test time, we use PNP-based Ransac to infer the camera poses from the scene coordinates.

The following part of this chapter is arranged as follows:
\begin{itemize}
\item Chapter~\ref{method:poi} elaborates on the pipeline of the proposed method: PoI;
\item Chapter~\ref{method:filter} explains the filtering strategy of PoI. 
\item Chapter~\ref{method:limitation} describes the applicable conditions and limitations of using PoI as a plugin.
\item Chapter~\ref{method:sparse} explains the variant of PoI in extreme cases of sparse input.
\end{itemize} 

\subsection{NVS models used in our approach}

This paper uses 3DGS~\cite{kerbl20233dgs} as the baseline for novel view synthesis. 
A big challenge in NVS is the change in illumination conditions. The same situation also exists in the localization problem. 
To tackle this problem, We apply the luminance histogram method from DFNet~\cite{chen2022dfnet} to adjust the appearance of the rendering images in 3DGS. we generate the exposure embedding from the luminance histogram and output the affine transformation from an MLP used in wild Gaussians~\cite{kulhanek2024wildgaussians}.  

\subsection{Pixel of interest (PoI)}
\label{method:poi}
In order to use rendered images as the auxiliary input for camera pose estimation, most existing methods estimate the scene coordinates of all pixels (or downsampled pixels) of the rendered image without considering the differences in the rendering quality of these pixels, which greatly increases the time and resource cost of training and reduces the effectiveness of the auxiliary data.
To improve efficiency and robustness, we consider reducing the number of rendered pixels compared to query images for training. 
Since the 3DGS-based reconstruction method predicts the target RGB pixel-wise without cross-pixel guidance, the rendering quality of different pixels from the same image would be independent. So, if we reduce the rendered images frame-wise, some well-rendered pixels of the discarded images would also be removed. To address this problem, we propose a method that finds the well-rendered pixels of the frame: pixels of interest.

The architecture of PoI is shown in Figure~\ref{fig:pipeline}.(b).
In order to rule out pixels with poor rendering effects, we need a method to obtain pixel-level feature supervision instead of frame-level feature map supervision.
To this end, we use the pre-trained scene-independent convolutional network from Ace~\cite{brachmann2023ace} as our backbone to obtain frame-level feature maps, and we will freeze the parameters of this backbone network throughout the training process.
We input the query images $I_q$ and the synthesized images $I_n$ into the backbone to obtain the query features and novel features. 
We keep all the query features while using a filtering algorithm to extract features of interest (FoI) from the novel features.

\subsection{Filtering strategy}
\label{method:filter}
The filtering process can be divided into two parts:
First, we randomly sample the novel features in a specific ratio. We want to use more features from query images and fewer features from rendered images to avoid the collapse of the model caused by low-quality rendered pixels, so we set the ratio to 0.5 in our experiments; this ratio is related to the performance of NVS; we may choose a bigger ratio with a better NVS method. This can also be solved through different novel pose sampling methods.
% We improve the filtering method using gradient threshold
Second, the filtering threshold is set based on the joint metric of gradient and training loss.
For faster convergence, we use a pseudo-depth strategy for poorly performing pixels. When the reprojection error of a point exceeds a certain threshold, we first let these points converge to the target pseudo-depth. After that, these pixels are trained normally as the others.
Because we should also find PoI in the same stage, gradient alone cannot represent rendering quality in this case. we should also use reprojection error (the distance between GT planar coordinates and estimated re-projected planar coordinates) for filtering. 
Therefore, pixels with smaller gradients and lower reprojection loss are retained.

We implement filtering operations at the intermediate stage of training.
The filter will remove novel features of outlier prediction. The remaining features are the so-called FoI, and the corresponding pixels of FoI are the so-called PoI. Figure~\ref{fig:poi} shows an example of the PoI results in the 7Scenes and Cambridge Landmarks datasets. 
After filtering, we combine and shuffle the features $F$ and FoI and put them into the scene-specific MLP Head to estimate the scene coordinates.

It is worth mentioning that we have set a dynamic weight for the loss of rendering pixels. Because in the early stage of training, we want the model to converge quickly. After determining the PoI, we gradually reduce the weight of the loss of PoI from 1 to 0.01, while for the pixels from query images, we set the weight to 1 during the entire training process. 

\begin{equation}
\begin{split}
    \mathcal{L} &= \begin{cases}  
        \mathcal{L}_{rep}^{query}(i), & \text{if } i \in T \\  
         \tilde{\omega} \times \mathcal{L}_{rep}^{poi}(i),  & \text{if } i \in PoI
    \end{cases} \\
    \tilde{\omega} &= \omega_{max} - \frac{I_{iter}}{N_{iter}}(\omega_{max}-\omega_{min})
\end{split}
\end{equation}
where T denotes traing data, $\tilde{\omega}$ denotes the dynamic weight of PoI loss changing from $\omega_{max}$ (set 1) to $\omega_{min}$ (set 0.01). $I_{iter}$ denotes the current iteration number and $N_{iter}$ is the total iterations. All rendering data is initially set as PoI. As the training progresses, we rule out outlier prediction points from PoI. At the end of the training, the choice of PoI and the loss weight of PoI are fixed.

In PoI, we would sample novel camera poses and render the corresponding images according to the images and the corresponding camera poses from the training set. 
In existing novel view synthesis supported visual localization, we usually have to balance the novel poses' diversity and the images' overall rendering quality.
However, we do not need an overall well-rendered image in the PoI task because of the pixel-level optimization and filtering algorithm. Therefore, we should try to expand the diversity of novel poses.
We use a unified sampling method for camera pose translation: grid sampling. The boundaries of the grid are calculated based on the camera pose of the training data; that is, the maximum and minimum values of the grid (x, y, z) are determined by the maximum and minimum values of the translations of all camera poses.
We add a random perturbation to the rotation. The original rotation of each grid starts from the closest camera pose to that grid in the training data.

\begin{figure}[t]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.\linewidth]{figures/poi.pdf}
\end{center}
   \caption{An example of the results of PoI in dataset 7Scenes and Cambridge Landmarks. To highlight the determined pixels of interest, we scale up the `Value' (V) of the HSV representation of the images.}
\label{fig:poi}
\end{figure}

\subsection{Applicable conditions and limitations}
\label{method:limitation}
For the end-to-end SCR approach, PoI is difficult to use as a plug-and-play module, which is the limitation of the PoI method.
First, end-to-end SCR methods use image-level loss as a supervision and have no pixel-level performance, which makes PoI unusable in these methods.
Furthermore, even for two-stage SCR methods (init+e2e) such as DSAC*, all pixels of the same image should be supervised within one iteration in the e2e stage. If we filter out some pixels of the rendered images, aligning the rendered features and designing a differentiable RANSAC algorithm is difficult. 
Finally, pixel-wise shuffling (which is difficult to achieve in e2e) is also an important factor; without shuffling, poorly rendered pixels are more likely to appear in a batch of data. As a result, the network is more likely to get stuck at a local minimum. 

For non-end-to-end training methods like ACE and GLACE, the difference is that we can easily shuffle pixels from all rendered images and query images because the supervision relies only on the camera intrinsics and the planar coordinates of each pixel without further requirements of per-frame joint supervision. 

Take GLACE as an example; our PoI could also be used as in Figure~\ref{fig:pipeline}.(b); the difference is that the backbone should be replaced. We use the global encoder of GLACE to extract the same dimension of global features as the ACE features. We add the global feature and the ACE feature together and get the target feature maps. The following procedure remains unchanged. 

\subsection{Sparse input}
\label{method:sparse}

Sparse input visual localization is a challenging task since both CPR and SCR are not good at estimating unseen parts of the scene because the regression models are trained only from RGB with weak geometric constraints. 
However, with the help of sparse input NVS, we would be able to extend the training views.
% In this paper, we use the pre-trained MVSplat model on the re20k dataset for cross-data generation. We conduct zero-shot rendering of the 7scenes dataset through this pre-trained model. For each NVS target camera pose, we retrieve the cameras and find the three nearest cameras as the context input. We still use the grid sampling method to sample the NVS target camera poses. 

In this case, the challenge is that rendered frames from sparse-view NVS are generally of lower quality compared with those from dense-input NVS. The ratio between the input views and the NVS views is also reduced.
If we still use the raw PoI method, the implicit neural map will be mainly contributed by rendered pixels. The accuracy will be influenced. To address this problem, we propose a coarse-to-fine training approach as shown in Figure~\ref{fig:pipeline_s}.
In the coarse stage, we use the same setting as in PoI; the only difference is that all training data are rendered images. So, the filter is applied to all rendered pixels, which we call the self-pruning step. In this step, in order to retain adequate pixels for training, we raise the filter's threshold (for reprojection errors). 
We get a coarse model after self-pruning training. In the refinement stage, we fine-tuned the mapping model using real data and the remaining rendered data; we set the learning rate to lower than that of the coarse stage throughout the fine-tuning process. In this step, all pixels are put into the model without filtering.

We finally get the fine-tuned model and experiment on both indoor and outdoor datasets. The results and implementation details can be found in chapter~\ref{experiment:c2f}.

\begin{figure}[t]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.\linewidth]{figures/pipeline_sparse.pdf}
\end{center}
   \caption{Pipeline of the coarse-to-fine strategy for sparse input localization.}
\label{fig:pipeline_s}
\end{figure}


\section{Experiment}

\renewcommand{\tabcolsep}{8pt}
\begin{table*}[!t]\footnotesize
	\renewcommand{\arraystretch}{1.2}
	\centering
	\caption{Median errors of camera pose regression methods and scene coordinate regression methods on the 7Scenes dataset~\cite{shotton2013scene}. We \textbf{bold} the best results.}
	\label{tab:7scenes}
    
        \begin{tabular}{c|l|ccccccc|c}
		\cline{1-10}
            &\multirow{2}{*}{Method} & \multicolumn{7}{c|}{Scenes} & \multirow{2}{*}{\makecell{Avg.\\(cm/\degree)}} \\
            % \cdashline{3-9}[1pt/1pt]
            && Chess & Fire & Heads & Office & Pumpkin & Kitchen & Stairs &  \\
            \cline{1-10}
            
            \multirow{2}{*}{\rotatebox{90}{CPR}} 
            &PoseNet15  & 10/4.02 & 27/10.0 & 18/13.0 & 17/5.97 & 19/4.67 & 22/5.91 & 35/10.5 & 21/7.74 \\

            % & PoseNet17(geo) & 13/4.48 & 27/11.30 & 17/13.00 & 19/5.55 & 26/4.75 & 23/5.35 & 35/12.40 & 23/8.12 \\
            
            % & MapNet & 8/3.25 & 27/11.69 & 18/13.25 & 17/5.15 & 22/4.02 & 23/4.93 & 30/12.08 & 21/7.77 \\
            
            & Marepo & 1.9/0.83 & 2.3/0.92 &  2,1/1.24 & 2.9/0.93 & 2.5/0.88 & 2.9/0.98 & 5.9/1.48 & 2.9/1.04 \\
            
            \cline{1-10}
            
            \multirow{3}{*}{\rotatebox{90}{SCR}} 
            & DSAC* & \textbf{0.5}/0.17 & 0.8/0.28 & 0.5/0.34 & 1.2/0.34 & 1.2/0.28 & 0.7/0.21 & 2.7/0.78 & 1.1/0.34 \\
            
            & ACE & \textbf{0.5}/0.18 & 0.8/0.33 & 0.5/0.33 & 1.0/0.29 & 1.0/0.22 & 0.8/0.2 & 2.9/0.81 & 1.1/0.34 \\

            & GLACE & 0.6/0.18 & 0.9/0.34 & 0.6/0.34 &  1.1/0.29 &  0.9/0.23 & 0.8/0.20 & 3.2/0.93 & 1.2/0.36 \\
            
            \cline{1-10}
            
            \multirow{5}{*}{\rotatebox{90}{\makecell{NRP}}}

            & LENS & 3/1.30 & 10/3.70 &  7/5.80 & 7/1.90 & 8/2.20 & 9/2.20 & 14/3.60 & 8/3.00 \\

            & DFNet & 3/1.12 & 6/2.30 & 4/2.29 & 6/1.54 & 7/1.92 & 7/1.74 & 12/2.63 & 6/1.93 \\

            & GSplatLoc & 0.43/0.16 & 1.03/0.32 & 1.06/0.62 &  1.85/0.4 &  1.8/0.35 & 2.71/0.55 & 8.83/2.34 & 2.53/0.68 \\

            & PoI(ours) & \textbf{0.5/0.15} & \textbf{0.7/0.31} &  0.6/\textbf{0.30} & \textbf{0.9/0.26} & 1.0/\textbf{0.20} & 0.7/0.22 & \textbf{2.4/0.70} & \textbf{1.0/0.31} \\
            
            & GLPoI(ours) & 0.6/0.19 & 0.8/0.33 &  \textbf{0.5}/0.33 & \textbf{0.9}/0.29 & \textbf{0.8}/0.22 & \textbf{0.6/0.19} & 2.8/0.77 & \textbf{1.0}/0.33 \\
            
            \cline{1-10}	

	\end{tabular}
\end{table*}

\subsection{Implementation details}

\renewcommand{\tabcolsep}{4pt}
\begin{table*}[!t]\footnotesize
	\renewcommand{\arraystretch}{1.2}
	\centering
	\caption{Results on Cambridge Landmarks, because of the obvious gap between SCR-based methods and CPR-based methods, we only list SCR-based methods. column `Mapping time' shows the training time of these methods (the GS model can be trained offline, and the training time of the GS model is less than PoI), and column `Mapping size' is the memory consumption for saving the parameters of the network. We \textbf{bold} the best result for group `SCR' and group `SCR w/ glob' separately.}
	\label{tab:cambri}
	\begin{tabular}{c|l|c|c|c|ccccc|c}
		\cline{1-11}
            & \multirow{2}{*}{Method} & \multirow{2}{*}{\makecell{Mapping with \\ Depth/Mesh}} & \multirow{2}{*}{\makecell{Mapping\\Time}} & \multirow{2}{*}{\makecell{Map\\Size}} & \multicolumn{5}{c|}{Scenes} & \multirow{2}{*}{\makecell{Avg.\\(cm/\degree)}}  \\
            % \cdashline{6-10}[1pt/1pt]
            &&&&& King's & Hospital & Shop & Church & Court &   \\
            
            % \hline
            
            % \multirow{2}{*}{\rotatebox{90}{FM}} & AS(SIFT) & No & ~35min & ~200M & 13/0.2 & 20/0.4 & 4/0.2 &  8/0.3 & 24/0.1 & 14/0.8\\
            
            % & pixLoc & No & ~35min & ~600M & 14/0.2 & 16/\textbf{0.3} & 5/0.2 &  10/0.3 & 30/0.1 & 15/0.2\\

            \cline{1-11}
            
            \multirow{6}{*}{\rotatebox{90}{SCR}} 
            
            & SANet~\cite{yang2019sanet} & Yes & ~1min & ~260M & 32/0.5 & 32/0.5 & 10/0.5 &  16/0.6 & 328/2 & 84/{0.8}\\
            
            & SRC~\cite{dong2022src} & Yes & 2min & 40M & 39/0.7 &  38/0.5 & 19/1 & 31/1.0 & 81/0.5 & 42/0.7\\
            
            & DSAC*~\cite{brachmann2021visual} & No & 15h & 28M & 18/\textbf{0.3} & 21/0.4 & 5/0.3 &  15/0.6 & 34/0.2 & 19/0.4\\
            
            & Poker~\cite{brachmann2023ace} & No & 20min & 16M & 18/0.3 & 25/0.5 &  5/0.3 & 9/0.3 & 28/0.1 & 17/\textbf{0.3} \\

            & EGFS~\cite{liu2025egfs} & No &21min & 9M & \textbf{14/0.3} & 28/\textbf{0.1} &  19/0.4 & \textbf{5/0.2} & \textbf{10}/0.3 & 15/\textbf{0.3} \\

            & GLACE~\cite{wang2024glace} & No & 20min & 13M & 19/\textbf{0.3} &  17/0.4 & \textbf{4/0.2} & 9/0.3 & 19/0.1 & 14/\textbf{0.3} \\            
            
            \cline{1-11}
            
            \multirow{4}{*}{\rotatebox{90}{\makecell{NRP}}} 

            % & DFNet & No & 25min & 16M & \textbf{27/0.46} & 20/0.71 &  \textbf{5/0.36} & \textbf{16/0.61} & \textbf{27/0.1} & \textbf{17/0.53} \\
            
            % & GSLoc & No & - & - & 25/0.29 & 26/0.38 &  5/0.23 & 13/0.41 & - & - \\

            & LENS~\cite{moreau2022lens} & No & - & - & 33/0.5 & 44/0.9 &  27/1.6 & 53/1.6 & - & - \\

            & GSplatLoc~\cite{sidorov2024gsplatloc} & No & - & - & 27/0.46 & 20/0.71 &  5/0.36 & 16/0.61 & - & - \\

            % & EGFS & No &21min & 9M & \textbf{14/0.3} & 28/\textbf{0.1} &  19/0.4 & \textbf{5/0.2} & \textbf{10}/0.3 & 15/\textbf{0.3} \\

            & PoI (ours) & No & 25min & 16M & 18/\textbf{0.3} & 23/0.5 &  5/\textbf{0.2} & 9/0.3 & 27/\textbf{0.1} & 16/\textbf{0.3} \\
            
            & GLPoI (ours) & No & 25min & 13M & 19/\textbf{0.3} & \textbf{16}/0.4 & \textbf{4/0.2} & 8/0.3 & 18/\textbf{0.1} & \textbf{13/0.3} \\
            
            \cline{1-11}	

	\end{tabular}
\end{table*}

\textbf{Dataset} We evaluate the performance of our approaches on two public datasets, Microsoft 7Scenes~\cite{shotton2013scene} (\textbf{with SFM Pseudo GT}) and Cambridge Landmarks~\cite{kendall2015posenet}. 
% 7Scenes dataset is a collection of RGB-D camera frames consisting of 7 different indoor scenes. Camera tracks are obtained with a KinectFusion system. Cambridge Landmarks include five large-scale outdoor scenes taken around Cambridge University using structure from motion technique to extract the ground truth labels of camera poses.
Our network takes RGB images and the camera extrinsic as input without using the depth information from the 7Scenes dataset or the reconstruction information from the Cambridge Landmarks dataset. We take the original resolution for the RGB images to make an accurate pose estimation. 

% All data from the training directory of both datasets is used for training the basic PoI training. 
To save time and computing resources, we do pose sampling and synthesis of new views offline and save the sampled camera poses and the rendered images on disk. During training time, we load rendered images along with the training set from the disk. For the scene 'kitchen' only, we split the training data into two clusters using the camera poses. We follow the rule of poker ( a variant of ACE) and train two models with the clusters. During the evaluation, we choose the estimated pose from the model with a more significant number of inlier pixels of the Ransac algorithm. 
We use 1 NVIDIA V100 GPU for PoI training; for GLPoI, we use 4 V100 with distributed data-parallel training.

\subsection{Localization results}
The comparison of median translation and rotation errors between our proposed methods with different camera pose regression methods (top), scene coordinate regression methods (middle), and neural render pose estimation methods (bottom) in dataset 7Scenes is shown in Table~\ref{tab:7scenes}. 
Generally speaking, scene coordinate regression methods outperform absolute camera pose regression methods in both translations and orientations. 
Unlike~\cite{sidorov2024gsplatloc}, we include DFNet and LENS in NRP, although they use offline NVS.
% DFNet and LENS beat most other CPR within absolute pose regression methods because they use view synthesis methods for data augmentation. 
Our proposed method outperforms DSAC* and ACE by exploiting the additional information from the rendered novel views. Our `GLPoI' beats `GLACE' and achieves the state of the art.

The results of the Cambridge Landmarks datasets are shown in Table~\ref{tab:cambri}. Since apparent gaps exist between scene coordinate regression methods and absolute camera pose regression methods, we only list the results of SCR and NRP methods. 
% SCR methods include SANet, SRC, DSAC*, Poker (ensembled version of ACE), and our proposed methods. SCR methods with global features include GLACE and the global-feature-version PoI: GLPoI. 
We come to a similar conclusion as that of 7Scenes. 
Although we use additional rendered data, it can achieve training efficiency comparable to ACE. 
In order to intuitively compare the localization effects of different methods, we also used NVS to render the images corresponding to the poses estimated by different localization methods, as shown in Figure ~\ref{fig:render_loc}. The upper right is the original image, and the lower left is the image rendered using poses estimated using different methods.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.\linewidth]{figures/render_loc.pdf}
\end{center}
   \caption{The rendering views obtained based on pose estimations using different localization methods. }
\label{fig:render_loc}
\end{figure}

\subsection{Coarse-to-fine experiments of sparse input}
\label{experiment:c2f}
To further evaluate the effectiveness of our method, we do an additional experiment with sparse input as mentioned in Chapter~\ref{method:sparse}.

\textbf{implemente details: }
We use pre-trained MVSplat\cite{chen2024mvsplat} as the sparse NVS model.  
For datasets like 7Scenes, it takes thousands of images to train a small indoor scene with a scale of only several meters. 
To simulate the sparse input, we uniformly re-sample from the input data every 50 frames. For Scene `heads', we keep only 20 frames. 
For outdoor datasets like Cambridge Landmarks, we split the data into multiple clusters according to the ground truth translations of camera pose (4 in the experiment) and used only one cluster for training. 

The numerical results are shown in Table~\ref{tab:c2f}, \textbf{case `base'} denotes the sparse input circumstances with the baseline model. \textbf{Case `coarse'} is the self-pruning step of our method only using rendered data; we still use grid sampling as the novel pose sampling method. \textbf{Case `c2f'} denotes the fine-tuned results of our proposed method.

\renewcommand{\tabcolsep}{3pt}
\begin{table}[!t]\footnotesize
	\renewcommand{\arraystretch}{1.2}
	\centering
	\caption{Median errors of our proposed method with sparse input on 7Scenes and Cambridge dataset.}
	\label{tab:c2f}
	\begin{tabular}{lccc|ccc}
		\cline{1-7}
            \multirow{2}{*}	{Method} & \multicolumn{3}{c|}{7Scenes} & \multicolumn{3}{c}{Cambridge Landmarks}\\
            % \cdashline{2-7}[1pt/1pt]
            & trans$\downarrow$ & rot$\downarrow$ & $U_{5cm,5\degree}\uparrow$ & trans$\downarrow$ & rot$\downarrow$ & $U_{10cm,5\degree}\uparrow$ \\
            \cline{1-7}
            base &  2.6cm & 0.7\degree & 68.9\% & 435cm & 2.2\degree & 15.7\%\\
            coarse & 16.5cm & 4.8\degree & 11.2\% & 184cm & 2.2\degree & 15.8\% \\
            c2f & \textbf{1.8cm} & \textbf{0.3}\degree & \textbf{90.2\%} & \textbf{26.9cm} & \textbf{0.3}\degree & \textbf{20.4\%}\\
            
            \cline{1-7}	
            	\end{tabular}
\end{table}

Based on the results, we may find that our fine-tuned model can achieve acceptable results with limited input compared to those that use all the training data.

\subsection{Ablation of PoI}
\renewcommand{\tabcolsep}{2pt}
\begin{table}[!ht]\footnotesize
    \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Median errors of different implementations of PoI on 7Scenes and Cambridge dataset.}
	\label{tab:abla_poi}
	\begin{tabular}{lccc|ccc}
		\cline{1-7}
            \multirow{2}{*}{Method} & \multicolumn{3}{c|}{7Scenes} & \multicolumn{3}{c}{Cambridge Landmarks}\\
            % \cdashline{2-7}[1pt/1pt]
            & trans$\downarrow$ & rot$\downarrow$ & $U_{5\text{cm},5\degree}\uparrow$ & trans$\downarrow$ & rot$\downarrow$ & $U_{10cm,5\degree}\uparrow$ \\
            \cline{1-7}
            base & 1.1cm & \textbf{0.3}\degree & 97.1\% & 17.7cm & \textbf{0.3}\degree & 32.4\% \\
            base+poa & 2.3cm & 1.58\degree & 89.6\% & 17.6cm & \textbf{0.3}\degree & 32.2\%\\
            base+por & 1.4cm & 0.46\degree & 95.9\% & 18.0cm & \textbf{0.3}\degree & 30.4\%\\
            base+poi & \textbf{1.0cm} & \textbf{0.3\degree} & \textbf{98.9\%} & \textbf{16.4cm} & \textbf{0.3}\degree & \textbf{33.1\%} \\
            
            \cline{1-7}	
	\end{tabular}
\end{table}

To evaluate the effectiveness of our PoI approach, we conducted some experiments on PoI in different settings. As shown in Table~\ref{tab:abla_poi}, we set the training process using only query data from the training set as the \textbf{case `base'}. In this case, the training setting is similar to ACE's. \textbf{Case `base+poa'} indicates the training with data from the training set and all rendered pixels of the proposed novel pose rendering method.
\textbf{Case `base+por'} denotes the pixel-of-random method, which uses randomly sampled pixels from the NVS, and all else are the same with case `base+poi'.
\textbf{Case `base+poi'} is our final method.
From the results, we may find that if we directly use sampled images and the training data without filtering, the results will be worse than the baseline (7scenes). It is easy to understand that because the mapping process is filled with low-quality pixels, it would misguide the network. 
If we randomly sample some pixels, we still won't get a good estimate.

\section{Conclusion}
In this paper, we propose a pixel of interest filter for scene coordinate regression. The filter is designed for non-end-to-end methods, which enjoy good converging speed. 
With the filter, we also designed a coarse-to-fine pipeline for sparse input scenarios.
We conducted experiments on indoor and outdoor datasets and achieved state-of-the-art camera pose estimation with comparable training time.

\bibliography{ijcai25}
\bibliographystyle{named}

\end{document}

