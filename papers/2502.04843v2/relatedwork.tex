\section{Related Work}
\textbf{Camera Pose Regression} The CPR methods, i.e., to regress the camera pose from the given image directly, are the most naive ideas and most widely used in learning-based methods~\cite{kendall2015posenet,brachmann2016uncertainty,brahmbhatt2017mapnet,melekhov2017image,radwan2018vlocnet++,wang2020atloc,hu2020dasgil,arnold2022map,chen2022dfnet,shavit2022camera}. The most straightforward method implicitly uses CNN layers or MLP to represent the image-to-pose correspondence. PoseNet~\cite{kendall2015posenet} first proposes this using pre-trained GoogLeNet as the feature extractor. Then, several works focus on improving CPR through additional modules. Geomapnet~\cite{brahmbhatt2017mapnet} estimates the absolute camera poses and the relative poses between adjacent frames. AtLoc~\cite{wang2020atloc} uses a self-attention module to extract salient features from the image. Vlocnet++~\cite{radwan2018vlocnet++} adds a semantic module to solve the dynamic scene and improve the robustness for blockings and blurs. 
Marepo~\cite{chen2024marepo} first regresses the scene-specific geometry from the input images and then estimates the camera pose using a scene-agnostic transformer. 
The CPR method has achieved excellent efficiency and simplification of the framework, but there is still room for improvement in accuracy. 

\textbf{Scene Coordinate Regression} Recently, the SCR methods~\cite{shotton2013scene,brachmann2017dsac,brachmann2018learning,esac,massiceti2017random,li2018scene,brachmann2021visual,liu2025egfs} have achieved better performance in terms of the accuracy compared with the CPR methods. The SCR method aims to estimate the coordinates of the points in 3D scenes instead of relying on the feature extractor to find salient descriptors, as in CPR methods. SCR was initially proposed using the random forest for RGB-D images~\cite{shotton2013scene}. Recently, estimating scene coordinates through RGB input has been widely studied. ForestNet\cite{massiceti2017random} compares the benefits of Random Forest (RF) and Neural Networks in evaluating the scene coordinate and camera poses. ForestNet also proposes a novel method to initiate the neural network from an RF. DSAC~\cite{brachmann2017dsac}, DSAC++~\cite{brachmann2018learning} devise a differentiable RANSAC, and thus the SCR method can be trained end-to-end. ESAC~\cite{esac} uses a mixture of expert models (i.e., a gating network) to decide which domain the query belongs to, and then the complex SCR task can be split into simpler ones. DSAC*~\cite{brachmann2021visual} extends the previous works to applications using RGB-D or RGB images, with/without the 3D models. 
This means that in the minimal case, only RGB images will be used as the input to DSAC*, just like most CPR methods. 
More information about the 3D structure will be utilized for most SCR methods than CPR ones. However, approaches like DSAC* can achieve more accurate estimations even if the input is the same as the CPR method.
ACE~\cite{brachmann2023ace} and GLACE~\cite{wang2024glace} abandon the time-consuming end-to-end supervision module and shuffle all pixels of the scene to improve training efficiency. ACE and GLACE use only RGBs without extra 3D geometry information and achieve comparable accuracy compared with former methods.

Despite the progress of CPR and SCR methods, both methods still have great problems in data collection and labeling. Therefore, efficient data collection and labeling methods or alternatives with similar effects are needed.

\textbf{Neural render pose estimation (NRP)}
A major challenge for visual localization methods is to collect appropriate photos to cover the entire scene. Essentially, the number and distribution of image sets for training are difficult to decide.
For example, most outdoor scenes collect data along roads, such as Cambridge landmarks~\cite{kendall2015posenet}. For indoor datasets (such as the 7Scenes~\cite{shotton2013scene} dataset), all translations and orientations within the scene are considered.

To fulfill the diverse requirements of data collection, some works try to use more flexible NVS to render synthetic views instead of collecting extra data~\cite{chen2021direct,ng2021reassessing,purkait2018synthetic,taira2018inloc,moreau2022lens,chen2022dfnet}, where NVS is the method to render synthetic images from the camera poses, which can verify the accuracy of 3D reconstruction, especially for implicit reconstruction methods like NeRF~\cite{mildenhall2021NeRF}, and 3DGS~\cite{kerbl20233dgs}.
INeRF~\cite{yen2021iNeRF} applies an inverted NeRF to optimize the estimated pose through color residual between rendered and observed images. However, the initially estimated poses are significant in guaranteeing the convergence of outputs.
LENS~\cite{moreau2022lens} samples the poses uniformly throughout the area and trains a NeRF-W~\cite{nerfw} to render the synthetic images. Then, rendered images and poses work as the additional training data for the pose regression network. The limitation of LENS lies in the costly offline computation for dense samples. DFNet uses direct feature matching between observed and synthetic images generated by histogram-assisted NeRF. The feature match approach is proposed to extract observed or generated images' cross-domain information. All these methods combine the NVS module and the CPR module to optimize the performance of the absolute pose estimation of the photos.

Unlike the former methods, we propose using an SCR rather than the CPR method with proposed NVS rules to improve the camera pose estimation.
First, we design novel pose sampling methods to meet multiple requirements of different datasets. To address the problem of varying lighting conditions, we use exposure histogram-assisted 3DGS as the baseline to sample new views of multiple exposures for each sampled pose in outdoor datasets.
% Second, since the quality of images rendered by NVS is lower than that of captured photos, we propose an architecture to narrow the gap between feature maps obtained from captured photos and those obtained from rendered images.
Second, we propose a pixel filter to remove bad pixels in rendered images and use query frames and remaining rendered pixels to improve the estimation.

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1.\linewidth]{figures/pipeline.pdf}
\end{center}
   \caption{Pipeline of our proposed methods: \textbf{(a) data formulation}: We first sample a group of synthesized camera poses $P_n$ according to the query training pose $P_q$ using `GS' (grid sampling). Then, we render the synthesized views $I_n$ based on the sampled poses $P_n$ through the novel view synthesis model. 
   % After that, we regress the scene coordinates $sc_n$ through PoI. Finally, we use a PNP-Bsed Ransac to estimate the final camera pose. 
   \textbf{(b) architecture of PoI}: First, a pre-trained scene-irrelevant backbone is applied to extract the features of the input query photos $I_q$ and the synthesized novel images $I_n$. Then, the filter is applied to the features of the rendered images and gets the features of interest. After that, we combine the query features with the filtered novel features and shuffle the pixel-aligned features to get the aggregation. Finally, we estimate the scene coordinates of the pixels using a scene-specific Head. The filtering algorithm is designed based on the reprojection error of the estimated scene coordinates.}
\label{fig:pipeline}
\end{figure*}