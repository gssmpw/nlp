\section{Conclusion}
\label{sec:conclusion}

The frontier AIs of today are increasingly capable generalist agents.
While these technological marvels are undeniably useful, 
they are also rapidly developing key capabilities such as deception (\S\ref{sec:existential:lossofcontrol:deception}), persuasion (\S\ref{sec:existential:lossofcontrol:persuasion}), long-term planning (\S\ref{sec:existential:lossofcontrol:broad-and-long}), and technical cybersecurity acumen (\S\ref{sec:existential:lossofcontrol:progr-cybersec-airesearch}),
opening the possibility of enormous damage to our infrastructure and institutions, should we come into conflict with them.
Unfortunately, agents are inherently selected for self-preservation
(\S\ref{sec:existential:riskseverity:selfpres}),
and powerful self-preserving agents that take actions in the real world in many ways compete directly with the interests of humans
(\S\ref{sec:existential:riskseverity:humanconflict}),
forcing us to take seriously the possibility of catastrophic risks.

Indeed, these risks are inherent to the methods used to train today's frontier AI systems.
Reinforcement learning, the standard practice of training an agent to maximize long-term cumulative reward, can easily lead to goal misspecification and misgeneralization (\S\ref{sec:existential:misagencyreward}).
In particular, we must acknowledge that a generalist agent operating in an unbounded environment can best maximize its reward by taking control of its reward mechanism and entrenching that position, rather than genuinely fulfilling the intended objectives (\S\ref{sec:existential:misagencyreward:tampering} and \ref{sec:existential:misagencyreward:optimality}). 
%
The other main way we train AI systems is to imitate human behavior, but it is not clear if this is any safer; such systems will inherit and may well amplify undesirable aspects of human intelligence (\S\ref{sec:existential:misagencyimitation:dangers})---after all, we are generalist agents ourselves.
More than a few humans with power have managed to inflict serious damage to humanity, and so imbuing a pseudo-human mind with immense cognitive abilities may be just as problematic (\S\ref{sec:existential:misagencyimitation:capabilities}).
Since frontier AI systems are tuned to human preferences in the final stages of training, for example, they tend to be more sycophantic than truthful: they may pretend to be aligned with the goals of the user, seemingly for expediency (\S\ref{sec:existential:misagencyimitation:humandeception}).
This makes them difficult to trust.

An obvious approach to mitigating these risks is to resolve to build AIs that are less general, and deploy them only in narrowly specialized domains (\S\ref{sec:plan:restricting:safetynarrow}).
Yet we believe that there may be a way for us to benefit from the enormous potential of general AI systems, without the catastrophic risks---so long as we are careful not to entitle these AI systems to their own goals. 
In other words, we are interested in AI that is non-agentic not because it lacks general intelligence, but rather because it lacks the other two key pillars in our definition of agency (\S\ref{sec:plan:restricting}): affordances and goal-directedness.

Our research plan (\S\ref{sec:plan}) lays the foundation for a \emph{Scientist AI}: a safe, trustworthy, and non-agentic system. 
This name is inspired by a common scientific pattern: first working to understand the world, and then making inferences based on that understanding.
To model these steps, we use a \textit{world model} (\S\ref{sec:plan:modelbased}) that generates causal theories to explain the world, and an \textit{inference machine} (\S\ref{sec:plan:inferencemachine}) that answers questions based on those theories. Both these components are Bayesian and handle uncertainty in a calibrated probabilistic manner (\S\ref{sec:plan:bayesian}) to guard against over-confidence. 
Because we also generate interpretable theories and take care to distinguish between utterances and their meanings, we argue that the result is a system that is
interpretable (\S\ref{sec:plan:latentvar}). 
The Scientist AI is non-agentic by design, and we also outline strategies to guard against the emergence of agentic behaviors in unexpected ways (\S\ref{sec:plan:hiddenagency}). 
Furthermore, the Scientist AI enjoys a crucial convergence property: increases in data and computational power drive improvements in both performance and safety, setting our system apart from current training paradigms (\S\ref{sec:existential:riskseverity:scaling}). 
In principle, a Scientist AI could be used to assist human researchers in accelerating scientific progress (\S\ref{sec:plan:application:research}), including in AI safety. In particular, we lay a path for its deployment as a guardrail around more agentic AI systems (\S\ref{sec:plan:application:guardrail}). 
Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks of the current trajectory.
We hope these arguments will inspire researchers, developers, and policymakers to  focus on the development of generalist AI systems that are not fully-formed agents.

