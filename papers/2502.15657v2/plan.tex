\section{A research plan leading to safer advanced AI: Scientist AI}
\label{sec:plan}

Our research plan proposes to create a type of safe, trustworthy, and non-agentic AI which we call \textit{Scientist AI}. This name is inspired by a common motif in science: first understanding the world, and then making rationally grounded inferences based on that understanding. Accordingly, our design is based on two components corresponding to these steps: a \textit{world model} that generates causal theories to explain a set of observations obtained from the world, and an \textit{inference machine} that answers questions based on the theories generated by the world model.
Both components are ideally \emph{Bayesian}, that is, they handle uncertainty in a correct probabilistic way. 

In service of building a non-agentic AI system, we identify three key properties of agents: intelligence (the ability to acquire and use knowledge), affordances (the ability to act in the world), and goal-directedness (motivated behavior). As discussed in Section~\ref{sec:plan:restricting}, our proposal greatly reduces affordances and eliminates goal-directedness. Affordances are minimized in the sense that the Scientist AI does not have degrees of freedom in its choice of output, because such output is limited to be the best possible estimator of conditional probabilities. The emergence of goal-directedness is prevented by the design of our training process, focused on avoiding agency, as well as by guardrails to avoid cases where there would be multiple possible outputs, such as with inconsistent input conditions. Finally, to ensure that our system is trustworthy, it is designed to distinguish between the underlying truth of a statement, which is what we ultimately care about, and the verbalization of that statement by (typically human) agents, who can lie or be misguided. We directly observe the verbalized statement but not whether they are really true, which is therefore treated as a latent, unobserved cause. We want our Scientist AI to make inferences about such latent causes, so that it can provide trustworthy answers not tainted by self-motivated intentions.

We anticipate three primary use cases for the Scientist AI, namely to: 1) help accelerate the scientific process in general, 2) serve as a guardrail to enhance other and potentially unsafe AIs, and 3) serve as a research tool to help safely build smarter (superintelligent) AIs. These use cases are covered in Section~\ref{sec:plan:applications}.

This section on our research plan is the most technical part of this paper. Readers interested at a higher level may wish to read just Section~\ref{sec:plan:introduction} and then skip to Section~\ref{sec:plan:applications}, where we describe potential applications of the Scientist AI.


\subsection{Introduction to the Scientist AI}
\label{sec:plan:introduction}

In this section, we describe the backdrop of our safe AI research plan and the considerations that shaped its structure. We define our Scientist AI in broad terms, and discuss a few important properties that all combine to provide the safety that we seek.

    \subsubsection{Time horizons and anytime preparedness}
    
There is a lot of uncertainty about the exact timeline at which agentic AI systems might become powerful enough to run a high risk of loss of control \cite{epoch.ai.blog.literature.review.of.transformative.artificial.intelligence.timelines}. A research program to build safer AI systems should include shorter-term and more easily achieved actions on top of its more ambitious longer-term goals. Shorter term steps providing reduced safety assurances could be all we can muster before the risk of uncontrolled AIs is on the horizon.

It is reasonable to simultaneously explore projects with different levels of ambition and expected delivery horizons, so as to be ready at any time---``anytime preparedness''---with the best results such a research program could offer by a given time. 

\paragraph{Short term.} Current safety fine-tuning is based on supervised or reinforcement learning, both of which suffer from the safety considerations discussed in Section~\ref{sec:existential}. Consequently, in the short term, we will build a \textit{guardrail}, i.e., an estimator of probabilistic bounds over worst-case scenarios that can result from the achievement of a user request. Such a guardrail can be obtained by fine-tuning an existing frontier model for the generation of explanatory hypotheses. More details on the short term plan can be found in Section~\ref{sec:plan:application:guardrail}.

\paragraph{Long term.} In the longer term, we aim to develop a new training mechanism for the inference machine, grounded in a Bayesian framework and leveraging synthetic examples generated by the world model. This approach promises much stronger safety guarantees. Training from scratch with the full Bayesian posterior objective, rather than fine-tuning a pre-trained frontier model, eliminates the risks arising from RL and avoids human-imitating tendencies, for greater trustworthiness.


\subsubsection{Definition of our long-term Scientist AI plan}
\label{sec:plan:introduction:longtermplan}

Our proposal is to develop what we call a Scientist AI, which is a machine that has no built-in situational awareness and no persistent goals that can drive actions or long-term plans. It comprises a \textit{world model} that generates explanatory theories (or arguments, or hypotheses) given a set of observations from the world, and a probabilistic \textit{inference machine}. The inference machine makes stateless input-to-output probability estimates based on the world model. More precisely, the world model outputs a posterior distribution over explanatory theories given those observations. The inference machine then combines the posterior distribution with efficient probabilistic inference mechanisms to estimate the probability of an answer $Y$ to any question $X$. Formally, it takes as input a pair $(X,Y)$, also known as \textit{query}, and outputs the probability of $Y$, given the conditions associated with the question $X$, which includes some context. It should be noted that the output of the inference machine are not values of $Y$, but their probability. Nonetheless, we can train a neural network to generate concrete values of $Y$ if needed, based on the probabilities, e.g., by learning to generate proportionally to these probabilities~\cite{proceedings.neurips.cc.paper.2021.hash.e614f646836aaed9f89ce58e837e2310.Abstract.html}. Going forward, since the inference machine operates based on the world model, ``Scientist AI'' may refer either to the inference machine alone or the combined system. 

This design is similar to the previously studied notions of AI oracles \cite{link.springer.com.article.10.1007.s11023.012.9282.2, arxiv.org.abs.1711.05541} and its probabilistic inference machinery could build on recent work on \textit{generative flow networks} (GFlowNets or GFN, for short) \cite{proceedings.neurips.cc.paper.2021.hash.e614f646836aaed9f89ce58e837e2310.Abstract.html, proceedings.mlr.press.v180.deleu22a.html, pubs.rsc.org.en.content.articlehtml.2023.dd.d3dd00002h, openreview.net.forum.id.uKiE0VIluA, arxiv.org.abs.2209.02606}.
For context, a GFlowNet is a stochastic policy or generative model, trained such that it samples objects proportionally to a reward function.

A Scientist AI is designed to have the following properties:

\begin{enumerate}
    \item Both the theories generated by the world model and the queries processed by the inference machine are expressed using logical statements, expressed either in natural language or using a formal language. The statements sampled by the world model form causal models, i.e., they provide explanations in the form of cause-and-effect relationships.
    \item There is a unique correct probability (according to the world model) associated with any query, which is the result of globally optimizing a Bayesian training objective for the AI. The outputs of the inference machine approximate this unique correct probability.   
    \item The Scientist AI can generate explanations involving latent or unobserved variables, and therefore make probabilistic predictions about them. This applies both to hypothesized causes of observed pieces of data and possible trajectories of future events.
\end{enumerate}

Regarding the first property, there are good reasons to represent explanations and hypotheses with logical statements. We can compute the probability of a chain of arguments by sequentially multiplying for each argument its conditional probability of being true given the previous arguments are true, which is not possible with the words expressing the arguments. We can thus ensure a clear separation between the probability of an event occurring from the probability of selecting a sequence of words to describe it. In other words, we compute the probabilities of \emph{events} instead of the probabilities of event \emph{descriptions}. 

The second property greatly constrains the Scientist AI's degrees of freedom in its choice of output. At the global optimum of its training objective, the only possible output is the uniquely correct answer, eliminating any possibility of selecting an alternative response, such as one intended to influence the world. However, in practice, the solution to the optimization process will be an approximation, and the learned neural network will not be a global optimum. Mitigating errors and uncertainty in the output arising from an approximate solution is an important element of our research plan.

Because the generated explanations correspond to causal models, the third property enables the inference machine to be queried with candidate causes of observed data. Formally, a causal model is a graph that decomposes overall distributional knowledge into a collection of simpler causal mechanisms, each linking a logical statement to its direct causal parents. Notably, this structure allows for queries that involve counterfactual scenarios not necessarily corresponding to reality. That this, the AI is enabled to answer hypothetical questions, which is valuable from a safety perspective, as we shall discuss in Section \ref{sec:plan:hiddenagency:objectivewm}.


    \subsubsection{Ensuring our AI is non-agentic and interpretable}
    
\paragraph{Agency.} First, we shall establish that our Scientist AI is not agentic, since agentic behaviors suffer from the safety concerns discussed previously. We do this by identifying three key pillars of agentic AI systems: affordances, goal-directedness, and intelligence. We argue that all three pillars are required to be present for dangerous agency, and the Scientist AI intentionally is not goal-directed. In addition, the Scientist AI greatly limits the affordances lever of agents. This is discussed further in Section~\ref{sec:plan:restricting}. Nonetheless, the considerations around agency are very complex, and there are several subtle ways in which unexpected agentic behaviors could conceivably arise. These more detailed cases are outlined in Section~\ref{sec:plan:hiddenagency}.

\paragraph{Interpretability.} An important aspect of ensuring safety is that our AI is interpretable and its predictions are as explainable as possible, meaning that we can dive into its answers recursively to understand how it makes predictions. See Section~\ref{sec:plan:latentvar} for more details.


    \subsubsection{Leveraging Bayesian methods}

\paragraph{The Bayesian framework.} While in the short-term plan, we will build on top of existing LLM systems, in the long-term plan, we aim to develop a new inference framework and construct a model from first principles. A core feature of our Scientist AI proposal is its Bayesian approach to manage uncertainty. This approach ensures that, when faced with multiple plausible and competing explanations for a given experimental result or observed data, we will consider all possibilities without prematurely committing to any single explanation. This is advantageous from an AI safety perspective, as it prevents overconfident predictions. Incorrect yet highly confident predictions could lead to catastrophic outcomes when high-stakes AI decisions are required and high-severity risks are encountered. For further details, see Section~\ref{sec:plan:bayesian}.

\paragraph{Model-based AI.} The Scientist AI follows a model-based AI approach, and is structured around two tasks: (a) constructing a world model, in the form of causal hypotheses, to explain and represent observed data, and (b) using an inference machine that employs these weighted hypotheses to make probabilistic predictions about any answer to any question. When the AI lacks confidence in an answer, this uncertainty is naturally reflected in probabilities that are neither close to 1 nor close to 0. This model-based design is expected to reduce the need for large amounts of real-world data: scientific principles can reveal relatively simple underlying explanations of the world, enabling the inference machine to be trained on synthetic data generated by the world model. For further details, see Section~\ref{sec:plan:modelbased}.

\paragraph{Approximate inference.} The inference machine performs probabilistic inference using a neural network, because exact inference can be intractable. Limited training time may introduce potential issues, which we discuss in detail. While the output probabilities are only approximations, our system possesses a crucial safety property: increasing computational power reliably improves accuracy. If needed, confidence intervals around the predicted probabilities can be used to estimate the prediction error resulting from limited computational resources. For further details, see Section~\ref{sec:plan:inferencemachine}.

\subsubsection{Using the Scientist AI as a guardrail}

We acknowledge that agentic AIs may be developed in spite of the risks. For this reason, we designed the Scientist AI such that it can also be employed as a guardrail against uncontrolled or misused agentic systems. This will be explored in Section~\ref{sec:plan:application:guardrail}. Crucially, a Scientist AI can also serve as a guardrail for other instances of itself. Despite efforts to ensure its inherent safety, it could be misused or, in the worst case, turned into an agent, intentionally or inadvertently. For example:

\begin{enumerate}
   \item First, a user could exploit a Scientist AI’s theory generation and inference capabilities for harm. This includes designing bioweapons, optimizing explosive devices, or developing persuasive narratives for large-scale manipulation. The system could, for example, be misused to generate tailored propaganda to influence elections, incite social unrest, or model voter behavior and media impacts to optimize strategies for suppressing opposition or destabilizing governance.
   
   \item Secondly, a user could, intentionally or inadvertently, transform the Scientist AI into an agent. This could occur, for instance, by designing a system that repeatedly queries the Scientist AI about what an agent would do to achieve a goal, then executes those actions using external scaffolding. Incorporating new observations as inputs could further enable situational awareness. 
\end{enumerate}

To address these concerns, we propose implementing guardrails, using the Scientist AI itself. Specifically, the guardrail AI could be tasked with evaluating whether a given question or answer poses an unacceptable risk. If the estimated probability of harm exceeds a predefined threshold, the answer would not be provided.
We also need to make sure that the underlying AI agent will not easily circumvent the guardrail AI, by requiring that the guardrail AI must be at least as cognitively capable as the AI it guards; additionally, we will incorporate run-time optimizations as defensive measures, as outlined in Section~\ref{sec:plan:inferencemachine:runtimeactions}.

We stress that none of these risks can be mitigated by technical solutions alone; addressing them also requires social coordination, including legislation, regulatory frameworks, legal incentives, and international treaties.

\subsection{Restricting agency}
\label{sec:plan:restricting}

So far, we have built up an intuitive argument against the use of powerful AI agents. But what exactly do we mean by an agent? The time has come to answer this question more precisely.

The standard definition of a (rational) agent used by economists and computer scientists, comes from decision theory---that is, the study of \emph{choice} \cite{Savage.Foundations,ramsey1926truth,VnMR44theory-of-games-eb}. 
In the classical account, an agent is an entity that is capable of making choices, and is \textit{rational} if it acts as though it has beliefs (e.g., in the form of a probability measure), preferences (e.g., in the form of numerical rewards, called utilities), and takes actions so as to maximize utility in expectation.
Our notion of an agent is conceptually related to this classical notion of a rational agent---but in practice, an actor is able to maximize utility only approximately, which should not bar us from considering it an agent. 
Indeed, there is broad agreement that agency, in general, is about more than expected utility maximization. However, it is still fundamentally about choice.

Building upon the conceptual frameworks of Krueger \cite{neurips.cc.virtual.2024.workshop.84748} and Tegmark \cite{tegmark-pillars}, we believe it is helpful to understand the capabilities of an agent through three \emph{pillars of agency}, each a matter of degree:

\begin{description}
    \item [\emph{Affordances},] as discussed at length in Section~\ref{sec:existential:preliminaries}, delimit the scope of actions and the degrees of freedom available to enact changes in the world.  
    Clearly, having more affordances means making a larger number of more complex choices. 
    
    \item [\emph{Goal-Directedness}] refers intuitively to an agent's drive to pursue goals, and its capacity for holding preferences about its environment. 
    Shakespeare's Hamlet famously says that ``there is nothing either good or bad but that thinking makes it so''; this kind of ``thinking'' is what characterizes goal-directedness.
    More precisely, a goal-directed agent is one that breaks an a priori symmetry by preferring one environmental outcome to another (all else being equal). 

    A chess-playing AI, for instance, is goal-directed because it prefers winning to losing. A classifier trained with log likelihood is not goal-directed, as that learning objective is a natural consequence of making observations \cite{richardson.one-true-loss}---however, a classifier that artificially places twice as much weight on one class over another does have a preference.
    Similarly, an LLM trained to model the distribution of human text is not goal-directed, but is typically given goal-directedness through instruction tuning and reinforcement learning from human feedback \cite{proceedings.neurips.cc.paper.files.paper.2022.file.b1efde53be364a73914f58805a001731.Paper.Conference.pdf}. Moreover, even the untuned LLM can be used in a goal-directed way with the appropriate scaffolding: at each action (e.g., a turn of dialogue), the goals of the agent can be given in an input text, and the output generated by the LLM is a sample of what a human in this context would have presumably written with those goals in mind.
    
    Crucially, the capacity to hold a preference or a goal is a capacity for an (arbitrary) choice: between this goal and its negation.
    It drives the actions to favor behaviors that align with the preferred outcomes.

    \item [\emph{Intelligence}] involves knowledge: learning, efficient use of memory, and the ability to reason and make inferences based on that knowledge.  
    Observe that, in a sense, a more intelligent agent has more memory, a wider array of possible thoughts, and a richer set of perspectives---and with a richer conceptual landscape comes a greater ability to drive finer and better targeted action choices. 
\end{description}

We call an entity \emph{agentic} if it can make choices in all three senses. 
Since goal-directedness, by definition, requires an (arbitrary) choice of what to value, goal-directedness requires a \emph{persistent state} to keep track of that choice, so as to pursue it.
In addition, an agent's state may include beliefs about the environment and other attributes of self.
This often culminates in a \emph{situational awareness} that is the confluence of all three traits: the sensory affordances needed to make observations about one's place in the world, the persistent state needed to maintain a coherent direction towards one's goals, and the short-term memory needed to intelligently put it all together with practical reasoning.

We claim that an AI system requires all three of these properties to pose the dangers laid out in Section~\ref{sec:existential}. Therefore, eliminating any one property would be sufficient to mitigate most categories of loss-of-control risk \cite{neurips.cc.virtual.2024.workshop.84748}.
We explore several such cases below, focusing on limiting affordances and eliminating goal-directedness (although we also consider the case of limiting intelligence, for narrow AIs, in Section~\ref{sec:plan:restricting:safetynarrow}).
 
\subsubsection{How to make a non-agentic Scientist AI}
\label{sec:plan:restricting:scai}

In light of the previous discussion on agency, our proposal---the Scientist AI---is explicitly designed to be non-agentic from the outset. As summarized in Section~\ref{sec:plan:introduction:longtermplan}, it consists of a question-answering inference system, based on a world model that generates causal theories to explain observed data.  

Like a log-likelihood classifier or a pre-trained language model, the Scientist AI is not goal-directed, as it does not act to influence the environment towards a preferred state. But unlike a language model, the Scientist AI is concerned with modeling the world itself, not merely human language. Paralleling a theoretical scientist, it only generates hypotheses about the world and uses them to evaluate the probabilities of answers to given questions. As such, the Scientist AI has no situational awareness and no persistent goals that can drive actions or long-term plans. This design also constrains its affordances, as its ``actions'' are strictly limited to computing probabilistic answers.  

Although we previously argued that removing a single pillar of agency is sufficient to eliminate agency altogether, we deliberately impose constraints on two. Redundancy is essential in safety protocols, particularly when dealing with a concept like agency, which is not binary but comes in degrees. By the same token, Section \ref{sec:plan:hiddenagency} will also examine how the Scientist AI could potentially acquire agentic properties despite its design, whether through deliberate modification or unintended emergent behavior, and how such risks can be mitigated.



    \subsubsection{The safety of narrow agentic AIs}
    \label{sec:plan:restricting:safetynarrow}
    
Agency can also be restricted by constraining the system's intelligence to a narrow range, for example, by training it on a limited dataset for a specific task or distilling it from a generalist model. This approach is commonly used in the development of narrow AI systems, such as those designed for specific medical or scientific applications \cite{www.nature.com.articles.s41586.019.1799.6}, or even in agentic contexts like autonomous driving \cite{arxiv.org.abs.1604.07316}. While agency risks cannot be entirely eliminated even in narrow AI systems, if the risks of loss of control are sufficiently small due to limitations on the system’s capabilities, such narrow agentic AIs might be operated safely. However, narrow AIs could engage in collusion, as discussed in Section~\ref{sec:existential:lossofcontrol:collusionandconflict}.

A narrow agentic AI can be further restricted by limiting its affordances (i.e., the actions that it can take) to its specialized domain, such as driving a car or operating a drug discovery robotic apparatus. Additionally, our Scientist AI could serve as a guardrail or an additional safety layer for narrow agentic AI systems, as discussed further in Section~\ref{sec:plan:application:guardrail}. The idea is that a trustworthy non-agentic AI can be used to predict if an action proposed by an agentic AI could plausibly cause harm, either in the short-term or the long-term.

\subsection{The Bayesian approach}
\label{sec:plan:bayesian}
    
A core feature of our Scientist AI proposal is that it will be \textit{Bayesian} in its approach to uncertainty. In this section we discuss the importance of uncertainty, and the core idea of the Bayesian formalism. Bayesian probabilistic inference guides the estimation of conditional probability; it is applied to both the world model, predicting explanatory causal mechanisms, and the inference machine, to answer arbitrary queries. We further discuss the safety advantages inherent to this approach, compared with methods that are more prone to overconfidence.


    \subsubsection{The importance of uncertainty}
    \label{sec:plan:bayesian:uncertainty}

Multiple plausible and competing explanations typically exist for any experimental result or observed data, ranging from specific hypotheses to more abstract and general ones, so it is necessary to represent uncertainty over these explanations. Failure to do so can lead to predictions that are not only incorrect but also overly confident, thus increasing the risk of harm, as discussed in Section~\ref{sec:plan:bayesian:safetyadvantages}. Our approach, motivated by both probability theory and Occam’s razor \cite{onlinelibrary.wiley.com.doi.full.10.1111.cogs.12573}, prioritizes theories that (a) are consistent with the observed data and (b) simpler, in some meaningful sense (e.g., with shorter description length). This framework---the \emph{Bayesian posterior over theories}---is discussed below.

    \subsubsection{The Bayesian posterior over theories}
    \label{sec:plan:bayesian:posterior}
    
Given some data, the \textit{Bayesian posterior over theories} is a probability distribution that assigns weights to theories proportionally to the product of two factors: the \textit{likelihood} of having observed that data given a theory, and the theory's \textit{prior}, which measures simplicity (or brevity). More explicitly, the prior probability of a theory decreases exponentially with the number of bits of information needed to express it, in some chosen language \cite{www.sciencedirect.com.science.article.pii.S0019995864902232}. Therefore, given two theories with equal likelihood, the theory with the lower description length (in bits) will be considered exponentially more likely in the Bayesian posterior \cite{www.sciencedirect.com.science.article.pii.S0019995864902232}. In this sense, the Bayesian posterior is compatible with Occam’s razor. 

As more data is gathered or observed, the likelihood of the data given a theory is re-calibrated. We therefore say that the Bayesian posterior gets \emph{updated}. Because of this, the relative probabilities of different theories in the posterior can be interpreted as a measure of epistemic uncertainty, reflecting the insufficiency of available data to determine the correct theory.

It is important to choose our family of theories to be expressive enough, and this can be achieved by not limiting the description length of theories. However, by applying the prior, longer theories will be exponentially down-weighted. Only the theories that fit the data well and remain competitive in description length will retain a significant posterior probability.
How to choose the language for describing theories is an important question, and even the question of whether the Bayesian formalism is sufficiently agnostic to the choice of theories \cite{www.google.co.uk.books.edition.Introduction.to.Imprecise.Probabilities.9qXIEAAAQBAJ.hl.en, www.google.co.uk.books.edition.The.Geometry.of.Uncertainty.jNQPEAAAQBAJ, ecommons.cornell.edu.server.api.core.bitstreams.ef0ef95b.6156.487e.900e.6c33714ed0c3.content} remains open. Nevertheless, for the purpose of this paper, we use Bayesian posteriors as motivated above.

In practice, the Bayesian posterior can be approximated by training neural networks using amortized variational inference methods, including the GFlowNet objectives \cite{proceedings.neurips.cc.paper.2021.hash.e614f646836aaed9f89ce58e837e2310.Abstract.html}. Recent work has demonstrated that these approaches can be used to generate descriptions of causal models over data \cite{proceedings.mlr.press.v180.deleu22a.html, proceedings.neurips.cc.paper.files.paper.2023.hash.639a9a172c044fbb64175b5fad42e9a5.Abstract.Conference.html} and to approximately sample them from the Bayesian posterior, in line with the desiderata of our world model. One caveat is that these inference methods have so far only been explored on domain-specific theories whose description is short enough to be generated by a neural network much smaller than those of frontier AIs, and it remains to be shown how these methods can be scaled further.

\subsubsection{Inference with the Bayesian posterior predictive}
\label{sec:plan:bayesian:inference}
    
Beyond estimating the probability of theories given data, our Scientist AI should be capable of making predictions and providing probabilistic answers to specific queries. For example, it should infer the probability distribution of particular outcome variables in an experiment, given information about the experimental setting. That is, we need to couple the world model with a question-answering inference machine. We shall do so using the \textit{Bayesian posterior predictive}, which is described below. This is useful not just to get answers to questions, but also to design experiments (discussed in Section~\ref{sec:plan:application:research}), and to quantify the uncertainty around those answers---an essential desideratum in safety-critical contexts.

The \textit{Bayesian posterior predictive} distribution represents the probability of different possible values of an answer $Y$, given a question $X$~\cite{probml.github.io.pml.book.book1.html}. Unlike predictions based on a single theory, it accounts for uncertainty over competing theories. Indeed, unless a particular theory is explicitly assumed in the question, the posterior predictive distribution is obtained by averaging the predictions made by \emph{all} possible theories, weighted according to their Bayesian posterior.

This means that, in principle, the Bayesian posterior predictive can be derived from the Bayesian posterior over theories. In practice, however, enumerating all the possible theories and marginalizing over them is intractable. Nonetheless, we can train a neural network to \emph{approximate} the posterior predictive \cite{pubs.rsc.org.en.content.articlehtml.2023.dd.d3dd00002h}, by employing tools from research in probabilistic machine learning, such as GFlowNets \cite{proceedings.neurips.cc.paper.files.paper.2023.hash.639a9a172c044fbb64175b5fad42e9a5.Abstract.Conference.html}. We shall call a neural network that approximates the Bayesian posterior predictive an \textit{inference machine}, because it can be used to make any probabilistic inference, if well trained on the relevant domains and theories.

    \subsubsection{Safety advantages of the Bayesian approach}
    \label{sec:plan:bayesian:safetyadvantages}
    
Compared with more direct methods for generating high-quality predictions, approximating the Bayesian posterior predictive is advantageous from an AI safety perspective, because it avoids making over-confident predictions. 
Overconfidence can be a safety hazard. If there are two equally good explanations of the observed data and one explanation predicts that an action is harmful, we want to estimate the marginal probability of harm, not (over-confidently and arbitrarily) make a choice to use one explanation over the other.
Such overconfident predictions are common with ordinary ways of training neural networks (supervised learning, maximum likelihood, ordinary RL, etc.): there are often many equally valid ways of explaining the data,  and so, as judged by the standard training objectives, a learner is just as well off to place all its belief (either explicitly or implicitly) in a single explanation.

By contrast, the training objective for the Bayesian approach (and some ``entropy-regularized'' variants of standard objectives) pushes the learned hypothesis generator to cover all the plausible hypotheses. In this way, we end up averaging the predicted probabilities over all the plausible explanations rather than accidentally putting all our eggs in a single basket. This incorporates epistemic uncertainty, which reflects the lack of sufficient evidence (data) to be certain of the correct explanation, and thus, the implications for a particular question. The difference between a maximum likelihood approach and a Bayesian approach is similar to the difference between (a) reward maximization (the typical RL objective) and (b) reward matching \cite{arxiv.org.abs.2406.02213} with maximum entropy regularization. Reward maximization can converge on any one of the policies that are locally maximizing the reward, whereas reward matching methods seek to find \emph{all} the ways in which the reward can be high. 

The ability to take into account what the learner knows and does not know, and average probabilities over different hypotheses, is a precious advantage in addressing the problem of goal misspecification as discussed in Section~\ref{sec:existential:misagencyreward}. In safety-critical contexts where the AI is producing highly consequential outputs and there is a risk that it could dangerously misinterpret our instructions, the Bayesian approach does not commit to any single interpretation of the instructions, which could be flawed or involve a loophole that allows our intentions to be subverted. Instead, the Scientist AI aims to evaluate the level of risk by considering the consensus across all plausible interpretations and by estimating the expected probability of harm. This allows, for example, rejecting an action when it is dangerous according to only {some} (sufficiently plausible) interpretations of a safety specification, and not others. This idea of using a guardrail to reject plausibly dangerous actions is discussed further in Section~\ref{sec:plan:application:guardrail}.

\subsection{Model-based AI}
\label{sec:plan:modelbased}

In this section we expand on the first component of the Scientist AI: the world model. To do so, we shall first recall the concepts of ``model-based'' AI and ``model-free'' AI. We then discuss the advantages of the model-based approach for the training of our Scientist AI, e.g., reducing the quantity of real-world data required or, equivalently, obtaining better predictions for the same amount of real-world data.

\subsubsection{Introducing model-based AI}
\label{sec:plan:modelbased:intro}
    
The \emph{model-free} approach is a method for training AI systems, where predictions are learned without formulating explicit hypotheses (e.g., text completion in pre-training LLMs). Every end-to-end training approach is model-free. By contrast, \emph{model-based} learning constructs an explicit model of the environment or data-generating process, which is then used to make predictions or decisions. Our Scientist AI is model-based because it separates the following two learning tasks: (a) determining the probabilistically weighted theories that explain the observed data, i.e., learning the \emph{world model}, and (b) turning those weighted hypotheses into probabilistic predictions regarding any answer to any question, i.e., learning the \emph{inference machine}. Model-based machine learning has already been proposed as a means to obtain safety guarantees \cite{arxiv.org.abs.2405.06624}, and has been combined with reinforcement learning \cite{mitpress.mit.edu.9780262039246.reinforcement.learning,link.springer.com.article.10.1007.s11432.022.3696.5}.

Importantly, notice that the learning of the world model in (a) is driven by the information contained in the observed data, whereas the learning of the inference machine in (b) can, in principle, rely solely on synthetic data generated from simulations based on the world model. However, real data can also be incorporated (e.g., via text and image completion with transformers \cite{aclanthology.org.N19.1423,openaccess.thecvf.com.content.CVPR2022.html.He.Masked.Autoencoders.Are.Scalable.Vision.Learners.CVPR.2022.paper}).

Model-based approaches dominate the AI frontier in virtual games or simulated environments, where the world model is given and does not need to be learned. In this setting, we can generate perfect rollouts (simulations) as synthetic data to effectively train predictors and policies. At the same time, model-based approaches have generally been less successful where the world model must be learned, possibly due to the need for sufficiently rich world models and advances in efficient probabilistic inference with latent variables. Neural network-based probabilistic inference has only recently gained traction in the machine learning community~\cite{probml.github.io.pml.book.book1.html,proceedings.neurips.cc.paper.2020.hash.4c5bcfec8584af0d967f1ab10179ca4b.Abstract.html,proceedings.mlr.press.v162.zhang22v.html} and, to our knowledge, these algorithms have not yet been explored at the scale of current frontier AI. This is an important focus of our research program.

LLMs are trained end-to-end as inference machines (in the space of word sequences), so they are not model-based: they do not separate knowledge into well-specified cause-effect relationships, nor does the training data contain the correct causal explanations for the observed text. However, because they work in the space of words, they may be well-suited to generating explanatory hypotheses, for which there are plenty of implicit examples in their training data. After all, people do write about causes, justifications and explanations. Could there instead be an advantage to explicitly generating hypotheses (i.e., pieces of a causal model) and using synthetic data generation to augment the training of the inference machine?

\subsubsection{Advantages of model-based AI}
    
We argue here that model-based AI~\cite{bishop2013model} has the potential to require much less training data to make the desired inferences compared with directly training an end-to-end neural network; in learning theory this is known as lower ``sample complexity'' \cite{homes.cs.washington.edu..sham.papers.thesis.sham.thesis.pdf}. This is reasonable in part because humans often require much less data to learn, compared with what is used in modern AI training. For example, humans perform similarly to ChatGPT on writing tasks while having seen far less written text. This suggests that current approaches may be missing something fundamental on that front.

The core idea behind the lower sample complexity of the model-based approach is that ``describing how the world works'' (the \emph{world model}) is much simpler than ``how to answer questions about it'' (the \emph{inference machine})~\cite{ghahramani2015probabilistic}. The good news is that we can use our world model to generate as much synthetic data as our computing resources allow, in addition to real data, in order to train the inference machine for our Scientist AI. Hence, the bottleneck for information from the real world to train the Scientist AI is the length of the leading theories of the world model. We need about as much data as is sufficient to identify these theories, which we argue will be much less than the amount of data needed to directly train an inference machine from observed question-answer pairs. The model-free approach used for LLMs requires much more real data, because it directly tries to learn the inference machine by imitating the data, rather than exploiting the intermediate step of learning how the world works. To illustrate, consider how small Wikipedia is, or even all the scientific publications in the world, compared to the datasets used to train current LLMs \cite{openreview.net.forum.id.ViZcgDQjyG}.

Let us take the example of neural networks playing the game of Go \cite{ieeexplore.ieee.org.abstract.document.7515285}: the ``world model'', i.e., what transitions from one state of the game board to another are possible, is fixed and known. It consists of one page of code spelling out the nine rules of the game. At the same time, exact inference (optimal play) in Go is computationally infeasible, and strong approximate inference at human level or beyond requires comparatively large neural networks like AlphaGo \cite{www.nature.com.articles.nature24270}. The advantages of model-based AI over model-free AI are analogous, in the context of Go, to the benefits of self-play over imitation learning. In the latter case, the AI can only train on expert human games and learn to play like the best humans. However, AlphaGo became superhuman at Go because it could use synthetic games which had been generated using the basic rules encoded in the world model, giving it a more diverse set of training data. In general, synthetic data generation is useful because it enables training the inference machine on ``out-of-distribution'' scenarios that are rare in the real data, but critical for dealing with novel or high-risk situations. This approach is also used in autonomous driving \cite{ieeexplore.ieee.org.document.9578745,www.computer.org.csdl.proceedings.article.cvpr.2022.694600r7284.1H1k7GlOq9G,openreview.net.forum.id.MfIUKzihC8}. Synthetic data generation will mean our Scientist AI performs better inference in these high-risk situations, for the same quantity of data, than if we did traditional end-to-end training of the inference machine.

A more practical example is with the laws of physics, as an actual ``world model''. It demonstrates that only very few bits of information are needed for specifying a model compared with specifying the computation required for answering questions consistent with that model. The model equations are very simple, in the sense of requiring very few bits to state. This means that, in principle, these physical laws could be determined using a relatively small number of well-chosen experiments. On the other hand, exact physical inference derived from the model, such as predicting properties of molecules (or worse, collections of interacting molecules) is computationally intractable and needs to be approximated, e.g., by very expensive simulations or by large neural networks \cite{www.nature.com.articles.s41586.024.07744.y,royalsocietypublishing.org.doi.full.10.1098.rsta.2020.0093,www.sciencedirect.com.science.article.abs.pii.S0045782522003152,pubs.aip.org.aip.pof.article.abstract.33.8.087101.1080391.Simulation.of.multi.species.flow.and.heat.transfer.redirectedFrom.fulltext,proceedings.mlr.press.v139.finzi21a} that require significant quantities of data. However, in the model-based approach, this data can be generated from the world model, reducing the need for real-world data compared with directly learning the inference machine from observed data. This pattern of less computation needed for the world model than the inference machine seems generally true and probably has an explanation.

A previous limitation of model-based AI in non-virtual settings that we intend to turn in our favor is the importance of uncertainty in the world model. If the world model is not sufficiently Bayesian, e.g., if we train by maximum likelihood (the current typical training method for probabilistic models), then even rare errors may be amplified when optimizing for policies or training a predictor using simulations generated from the model. Indeed, a maximum likelihood model would sometimes be overconfident and this allows policy optimization to discover ``false treasures'' that only exist in the model and not in reality, making this approach not robust. Research is thus needed to apply probabilistic neural networks to the Bayesian setting to allow for a proper treatment of epistemic uncertainty. However, most of the past work at the intersection of deep learning and Bayesian modeling has been trying to represent the posterior distribution over the weights of a neural network \cite{proceedings.mlr.press.v37.blundell15,link.springer.com.chapter.10.1007.978.3.030.42553.1.3}. What we are proposing instead is based on a neural network (not a distribution over neural networks) that generates explanatory hypotheses, i.e., a distribution over causal models. Our model-based approach therefore learns the appropriate uncertainty such that we can perform reliable inference, avoiding confidently wrong predictions.

\subsection{Implementing an inference machine with finite compute}
\label{sec:plan:inferencemachine}

In this section, we detail the training and implementation of the second component of the Scientist AI: the inference machine. Specifically, we discuss why our inference machine is implemented with a neural network instead of other potential approaches, and the impact that finite compute has on the process. We also detail how our approach has a fundamental convergence property: increasing computational power reliably improves accuracy such that, in the limit, the outputs of the Scientist AI converge to the correct probabilities.

\subsubsection{Neural networks as approximate inference machines}
    \label{sec:plan:inferencemachine:nn}

The Scientist AI contains a generative world model that approximates a Bayesian posterior distribution over causal models. Samples can be generated from that world model, with each sample describing a piece of a causal model: the identity and values of the relevant random variables (which are statements about entities in the world) and the associated causal structure (i.e., which statements are direct causes of each other). These samples can then be used as synthetic data to help train another neural network --- the inference machine --- that answers general questions about logical statements given other logical statements. This neural network performs probabilistic inference, which means calculating probabilities or sampling from (conditional) probability distributions, as a form of general problem-solving and reasoning. A typical probabilistic inference scenario is that we know how to compute the probability of $X$ given $Y$ and the probability of $Y$ alone but we do not know how to compute the probability of $Y$ given $X$. For example, $X$ could be observed evidence, such as text, and $Y$ a candidate hypothesis to explain $X$. More generally, we would like our inference machine to answer any query involving variable $Y$ in the answer part and variable $X$ in the question part.

Exact inference is intractable because sampling from a conditional distribution (or equivalently, computing an appropriate normalizing constant, called the \emph{partition function}) generally involves summing over or considering an exponentially large number of alternative explanations to $Y$. For example, we may know how to compute the probability the grass is wet given that it is raining, but it is harder to compute the probability that it is raining given the grass is wet. The latter may be intractable because (i) we have to sum over all the different weather conditions that are alternative causes (rain, snow, sleet, sun and all their particulars and combinations), and (ii) we also have to consider other unstated variables (e.g., did someone turn the sprinklers on?). The intractability arises because $X$ and $Y$ typically do not state all the possible random variables of interest, which means that exact inference requires summing over all the unstated variable values, which is called \textit{marginalization}.

To overcome this intractability, we can use machine learning methods to efficiently approximate the marginalization calculation. In this way, most computations would occur during the training process, allowing probabilities to be computed quickly at run-time. 

Although we are using a neural network to implement our inference machine, there are various other inference techniques that could be used to perform probabilistic inference, such as Markov chain Monte Carlo (MCMC) methods \cite{link.springer.com.book.10.1007.978.1.4612.1276.8}. However, these methods can be very slow and inaccurate, in particular because of what is called the ``mixing mode challenge'' \cite{proceedings.mlr.press.v28.bengio13.html, proceedings.neurips.cc.paper.2021.hash.e614f646836aaed9f89ce58e837e2310.Abstract.html}. Instead, we will train a neural network that amortizes the cost of inference for answering each query, by replacing it by the cost of training the neural network once and for all, hence the name of \textit{amortized inference}. We still only get approximate inference, but the run-time computational cost of inference can be much lower, and there could also be advantages compared with MCMC in terms of generalization to unseen configurations of the variables \cite{proceedings.mlr.press.v162.zhang22v.html}.

Finally, amortized inference neural networks can be complemented by additional run-time computation to refine the predictions, along lines similar to Monte-Carlo Tree Search in AlphaGo~\cite{www.nature.com.articles.nature16961} and chain-of-thought in recent frontier models~\cite{openai.com.index.learning.to.reason.with.llms}. In the case of the Scientist AI, the proposal is to generate summary explanations that make it possible to obtain more accurate probability predictions, similarly to how a good argument can improve our confidence in a statement. 

\subsubsection{Convergence properties: training objective whose global optimum provides the desired probability}
\label{sec:plan:inferencemachine:convergence}

Ideally, our models would compute exactly the desired probability of a given query. While this is not achievable with finite compute, our proposed method has the advantage that, with more and more compute, it converges to the correct prediction (subject to the caveats discussed in Section~\ref{sec:plan:hiddenagency:uniquesolution}). In other words, \emph{more computation means better and more trustworthy answers}, in contrast to typical LLM training where we see an increased tendency towards deceptive behavior as compute increases \cite{arxiv.org.abs.2412.14093}. Indeed, some forms of reward misgeneralization may only occur with sufficient computational resources to discover a high-reward but misaligned behavior, such as reward tampering \cite{ojs.aaai.org.aimagazine.index.php.aimagazine.article.view.15084}.

Our asymptotic version of convergence is achieved by amortized variational inference methods, which include GFlowNets \cite{pubs.rsc.org.en.content.articlehtml.2023.dd.d3dd00002h, openreview.net.forum.id.uKiE0VIluA} and reverse diffusion generators \cite{proceedings.mlr.press.v37.sohl.dickstein15.html, proceedings.neurips.cc.paper.2020.hash.4c5bcfec8584af0d967f1ab10179ca4b.Abstract.html} based on denoising score matching \cite{ieeexplore.ieee.org.abstract.document.6795935}. These methods can be adapted to train neural networks that can estimate and sample from a target conditional distribution specified via an unnormalized density or an energy function \cite{openreview.net.forum.id..uCb2ynRu7Y, openreview.net.forum.id.8pvnfTAbu1f, openreview.net.forum.id.gVjMwLDFoQ}. What is interesting is that the global optimum of such energy-based objectives (i.e., when the GFlowNet training loss is exactly zero) corresponds to exactly achieving the desired conditional probability. Except in avoidable special cases discussed in Section~\ref{sec:plan:hiddenagency:uniquesolution} (where the conditional probability is undefined), there is a unique solution to this optimization problem. For example, the Bayesian posterior over theories is the unique conditional distribution (for a theory given the data) which is proportional to the prior of the theory times the likelihood of the data given the theory. The Bayesian posterior predictive (for the inference machine) is the unique distribution corresponding to marginalizing out all the variables not mentioned in the query and normalizing to obtain the desired conditional probability.

This means that for the Scientist AI approximate inference machine, we can use as large a network as we can afford and we will always get improvements in performance, because the network is trained with synthetically generated data and a matching function that evaluates how well the network approximates the probability distribution of interest on the generated data \cite{openreview.net.forum.id.uKiE0VIluA}. This is different from the way we usually train neural networks to imitate human answers or other observed data, where the accuracy of the neural network is ultimately limited by the amount of available data \cite{arxiv.org.abs.2001.08361}. Here, \emph{the only limitation is the amount of computational resources available for training the neural network} (including both the size of the network and how many synthetically generated configurations we care to generate during training). This is a case where scaling is only limited by computation, not by data.

It is reassuring that as the amount of compute increases, the neural networks trained in this way converge to a well-defined and well-understood mathematical quantity. Although in practice we will always have finite compute, leaving room for unexpected behavior (which can however be controlled with confidence intervals and guardrails, as discussed in Section~\ref{sec:plan:inferencemachine:finitetraining}), we at least have an asymptotic guarantee as the amount of compute increases. This is important if we want to design an approach to AI safety that will hold up as we enter the territory of ASI and superhuman computational resources.


\subsubsection{Penalizing computational complexity}\label{sec:plan:inferencemachine:penalizingcomplexity}
    
Similarly to how the output of the world model neural network is penalized so that ``short'' hypotheses are preferred --- a property that arises automatically from the Bayesian prior and thus the Bayesian posterior over theories --- the inference machine neural network is subject to an implicit form of regularization as well, as we proceed to explain.

First, recall that the inference machine uses approximation techniques (such as amortized inference) and therefore its outputs will not be perfect.
Second, in practice, the Bayesian posterior over theories may be obtained by using the inference machine itself, to estimate the likelihood of the data under each theory. This arises because, at scale, theories will only refer to some aspects of the world and so will the observed data. Consequently, the associated likelihoods involve intractable marginalization, which can be approximated by the inference machine. When working with a theory that renders inference computationally costly, the inference machine is unlikely to provide an accurate likelihood approximation. More specifically, inference-costly theories (that are often highly-detailed), assign high probability to only a few configurations of the observed variables. Determining such configurations with limited compute and poor approximations will therefore lead to \emph{underestimate} the likelihoods of the data, thereby decreasing the Bayesian posterior of that theory in the world model. 
Said otherwise, approximate inference under finite compute constraints favors theories that permit less costly approximations that perform well in practice, even if they are not globally optimal. This implies that theory selection should be context-dependent, as such ``approximate theories'' might only be valid within specific domains.

As an example of the effect of limited computational resources for inference, consider equations of quantum physics as an explanatory theory. The equations cannot be directly used to make predictions about properties of specific molecules, but they can be incorporated into simulations (e.g., through approximate inference). This may work at small scale, but when the size of the system gets large enough, the quantum physics equations will not work well because our inference machinery with limited computational resources will not be able to make accurate predictions. This is why we need chemical theories, which introduce more domain-specific approximations that enable more efficient calculations than the quantum physics equations, at the price of less generality and less accurate (but computationally feasible) predictions. For larger molecular systems and within some domain of applicability, we would thus find that our approximate posterior over theories would prefer to trust specific chemistry approximations rather than the original physics equations. Similarly, concepts in biology will dominate the simpler chemical theories when the sizes of the biological systems become too large for efficient inference purely from chemistry, but they can only predict some generally more abstract properties of biological systems, rather than the full quantum state. Hence, it is because of the computational limitations of the inference machine that, in addition to quantum physics, we get theories such as those found in chemistry or biology that are approximate and limited in scope but enable cheaper inference.

As already noted, because of the constraint on the available computational resources for inference, theories that require additional inference computations that are not necessary to explain the given data or any domain and circumstance that we choose to focus on during training will also be greatly disadvantaged. This is discussed further in Section~\ref{sec:plan:hiddenagency:priorfavorshonestheories}.

    \subsubsection{Dealing with the limitations of finite training resources}\label{sec:plan:inferencemachine:finitetraining}

Previously, we discussed how our methods converge in the limit to the true Bayesian probabilities, but that we approximate these probabilities using neural networks, which are subject to finite resources. This section describes how to handle the potential errors coming from our limited training resources. 

The methodology of GFlowNets variational inference \cite{openreview.net.forum.id.uKiE0VIluA} makes it possible to learn to approximate unknown distributions which are otherwise computationally intractable, in a way that permits freedom to consider many possible settings of the variables, for example where the quality of the current approximation is poor \cite{openreview.net.forum.id.BdmVgLMvaf}. However, actively learning high dimensional distributions has unavoidable challenges. In the RL literature, these are known as the exploration and exploitation challenges \cite{mitpress.mit.edu.9780262039246.reinforcement.learning}. Specifically, the problems are: (a) unsuccessful exploration, e.g., missing a \emph{mode} (local maximum) of the distribution, and (b) unsuccessful exploitation, e.g., not obtaining enough samples near the mode and not accurately capturing the shape of the distribution around the mode. We give below an intuitive non-technical explanation of these two issues.

\paragraph{The exploration challenge.} Remember that we are training a neural network generator, e.g., to sample a theory from a target distribution such as the Bayesian posterior over theories, and we are given an unnormalized version of the target probability (such as the prior times the likelihood). This is different from how generative AI is typically trained, which is from a dataset of examples from the target distribution. If we picture the space of theories like a landscape where altitude is probability and each position on the map corresponds to a theory, there are some regions of high probability, which are like mountains in this landscape, called modes of the distribution, associated with high-probability theories. We can picture training as a process of discovering the altitude map of these mountains, where the only thing one can see at a time is the altitude of a given theory, relative to any other. Generalization consists of correctly guessing the presence of mountains (modes) that have not been visited yet, by leveraging the patterns and regularities of the terrain \cite{openreview.net.forum.id.umFrtGMWaQ,openreview.net.forum.id.BdmVgLMvaf}. Until the network sees a good theory, i.e., from a high-reward region, it could miss this mode, i.e., not assign enough probability mass there. This is similar to a visually impaired person only equipped with an altimeter trying to find all the mountains in the world. They may use reasoning and analogies with already visited mountains in order to guess where to look for others, or they may be lucky enough to find some of them through a form of exploration, but unless they visit each and every possible spot on the map, i.e., try every possible theory, there is never any guarantee that they will find all of them. This parallels the process of scientific research, as outlined in Section~\ref{sec:plan:application:research}. Scientists propose simple theories that fit the data well, but until they find a better theory (which could be simpler and/or fit the data better), they might not know where to look. Nor might they be aware if there is a better theory or a number of similarly performing different theories, somewhere in the vast space of possible theories. Ensuring that all valid theories are considered would require computational resources beyond feasibility. This is why scientific knowledge is always provisional, limited to the best theories identified so far.

What can we do about this in the context of our Scientist AI? One approach is to ensure it has access to all existing human scientific theories, treating them as hypotheses about aspects of the world model. By evaluating these theories based on their prior probability and likelihood, the AI can systematically assess them. While this does not guarantee the discovery of better theories, it ensures that any omission is not due to neglecting a theory already proposed by human scientists.

\paragraph{The exploitation challenge.} Besides missing whole modes of the distribution due to imperfect training, our learning machine could get the details of a theory slightly wrong, i.e., it could have roughly found where a mountain is on the map but without having identified its peak. This could leave slight inconsistencies in how the pieces of the theory fit together, for example. Coping with this may be easier: we can use machine learning techniques developed for estimating the level of errors made by a trained predictor, such as the methods of epistemic uncertainty quantification \cite{dl.acm.org.doi.10.1561.2200000101}. With such methods, we could obtain confidence intervals around the probabilities predicted by the neural network, which we could then use to construct a conservative guardrail that rejects certain actions, as discussed in Section~\ref{sec:plan:application:guardrail}. For example, consider a neural network predicting the probability that an action is harmful, so that we can accept actions with a harm probability below a given threshold. If we are not completely sure about the estimated probability (which we want to be low) but we have a confidence interval around it, then we should raise the bar and use a more conservative threshold. Epistemic uncertainty is meant to represent uncertainty in predictions due to insufficient training data. Because the kind of uncertainty we get here can be reduced by throwing more computation rather than more data at the learner, we like to call it ``computational uncertainty'' rather than epistemic uncertainty. The two are related however, since in our case, computational uncertainty can be reduced with further training with more synthetic examples.    
    
\subsubsection{Run-time actions against attacks and out-of-distribution contexts}
\label{sec:plan:inferencemachine:runtimeactions}

Due to the finite resources allocated to the inference neural network, it cannot compute the most accurate answer for every possible query. In other words, the immediate output of a neural network, without chain-of-thought reasoning, can be viewed as an instantaneous ``System 1'' or intuitive response~\cite{en.wikipedia.org.wiki.Thinking..Fast.and.Slow,royalsocietypublishing.org.doi.full.10.1098.rspa.2021.0068}. Such answers are not always very coherent, are prone to biases and vulnerable to psychological manipulation \cite{en.wikipedia.org.wiki.Thinking..Fast.and.Slow}, for example, exploitation via advertising, conspiracy theories and political demagogy.

To address this, we may add a variable-time component to the probability calculation of the run-time inference machine, akin to ``System 2'' abilities of human brains \cite{en.wikipedia.org.wiki.Thinking..Fast.and.Slow,royalsocietypublishing.org.doi.full.10.1098.rspa.2021.0068} and recent experiments on scaling up chains-of-thought \cite{openai.com.index.learning.to.reason.with.llms}. This time can be used to generate explanations, arguments, and proofs, as part of the run-time deliberation which, in turn, will improve the inference machine’s predictions. 

This could be achieved using a GFlowNet objective that seeks short explanations that effectively reduce the uncertainty in the predicted probabilities for the particular question-answer pair. This is similar to recent work fine-tuning LLMs using a GFlowNet objective to approximately sample from a posterior over a chain-of-thought seen as a latent variable \cite{openreview.net.forum.id.Ouj6p4ca60}. Changing the ``temperature'' of the GFlowNet energy function and other methods make it possible to turn GFlowNets into approximate combinatorial optimization machines \cite{arxiv.org.abs.2403.07041}.

Importantly, in addition to improving inference performance, another use of such run-time optimizations is to improve defenses against adversarial attacks, which exploit current neural networks’ lack of robustness to distributional changes by employing a prompt that is optimized to produce a harmful output \cite{arxiv.org.abs.2307.15043}. One way to understand the effectiveness of such attacks is through their link to the difficulty of dealing with loopholes in a safety specification, which we will discuss in Section~\ref{sec:plan:application:guardrail}. First, as discussed in that section, we can detect conditions that may be exploited in such an attack, and reject the query. Second, it may be possible to ``plug'' the loophole on-the-fly by using the guardrail AI to generate explanations and synthetic configurations to revise the conditional probability estimator, by choosing these explanations to reduce the initial computational uncertainty and indeterminacy exploited by the attacker. This is related to current defenses based on adversarial training \cite{arxiv.org.abs.1412.6572}, except that it could be done at run-time to counter a specific attack and strengthen the safety guardrail where it was too weak.




\subsection{Latent variables and interpretability}
\label{sec:plan:latentvar}
    
A core safety requirement for our Scientist AI is that humans understand \emph{why} it produces certain statements or decisions. In the next sections, we explain how to pair explanations and inference, and how they benefit each other. 


    \subsubsection{Like human science, Scientist AI theories will tend to be interpretable}
    
Earlier, we discussed how the Scientist AI favors theories that are more compact (via the prior) and that have stronger predictive power and cheaper computational costs for inference (via the likelihood of the data). This preference naturally encourages representations that align with human-style explanations. In scientific practice, such explanations often take the form of concise causal mechanisms, written in mathematics or natural language, that clarify how a hypothesis relates to data.

Similarly, the Scientist AI will produce explanations or theories in the form of sparse causal models that introduce abstractions and disentangle the different cause and effect relationships between observed and latent logical statements. Those explanations will be provided in a human-interpretable form, thus allowing users to gain a stronger grasp of the system’s reasoning.

    \subsubsection{Interpretable explanations with amortized inference}
    
One might ask whether interpretable theories are effective at explaining the data. In fact, current neural network weights and activations are not interpretable by default \cite{arxiv.org.abs.2407.02646,transformer.circuits.pub.2023.monosemantic.features} but do an excellent job of generalizing to data from the same distribution. To understand how we may get both interpretability and useful predictions, it may help to go back to \emph{existing} scientific theories: they are written in a human-understandable language, and yet do a very good job of explaining much of the scientific data around us. Notably, though, they mostly do so when coupled with \emph{inference tools}, such as simulators or computer science algorithms to efficiently perform or approximate otherwise very expensive computations.

Analogously, with interpretable causal models, \emph{inference}---the task of answering questions---is necessary, for it allows us to respond to questions despite having \emph{partial} or \emph{indirect evidence} (by marginalizing out the unobserved data).

To better understand the need for inference, observe that most question-answer $(X,Y)$ pairs, including those that humans typically use in discourse, do not correspond to the inputs and output of a causal relationship, and the approximate inference capabilities of neural networks are necessary. If $X$ contained all the causes of $Y$, then describing the corresponding causal mechanism would be sufficient to fully predict the effect $Y$, given all its direct causes in $X$. However, it is rarely the case that we ask only about an effect given all its causes. In addition, the exact causal structure itself has to be hypothesized (and can be generated by the Bayesian posterior over theories) and the inference machine needs to average over both the unobserved causes and the causal structures.

As a result of probabilistic inference being generally intractable, this marginalization has to be approximated and our proposal would use a neural network for that approximate inference job (the \emph{inference machine}). Although we could interrogate our AI about leading explanations in terms of cause and effect relationships, the intuitive and often more precise answer to the particular question will generally remain uninterpretable, just like for most questions we ask humans, even if they are expert scientists and know about simple to explain relevant scientific theories.

For example, although a physicist may know the causal mechanism explaining the dynamics of water particles in a flowing liquid, their brain still makes very fast intuitive approximations that are useful in day-to-day life - although they may not be able to verbalize this process. We would thus expect our inference machine to sometimes make predictions that are approximately correct but whose full interpretation would not be easy, unless we are willing to interrogate the Scientist AI and generate gradually more detailed explanations. The objective is to design this explanation interrogation capability so that it is generally possible for a human user to query deeper into any part of a high-level justification provided by the Scientist AI. This is related to the concept of \emph{computational uncertainty} previously discussed in Section~\ref{sec:plan:inferencemachine:finitetraining}.

On a last note, observe that amortized inference will benefit from the fact that the Scientist AI generates explanations, because training examples with explanations (generated as latent variables) can lead to more coherent predictions, as we already see with the run-time deliberative inference of OpenAI's o1 and o3 models \cite{openai.com.index.openai.o1.system.card,cdn.openai.com.o3.mini.system.card.pdf}. However, unlike these recent models, the explanations planned for the Scientist AI would be forced to have ``internal coherence'' (measured by the joint probability of the statements in the explanation). Furthermore, they could be used to quantifiably reduce the uncertainty in the ``intuitive predictions'' made without an explanation.

    \subsubsection{Improving interpretability and predictive power}

The approach of interpretable explanations with inference will be fruitful in cases where current scientific theories are effective, since human-generated scientific theories are interpretable by construction. But what about cases where current scientific theories are insufficient, and the only solutions that humans have found is to directly fit the observed data with some machine learning apparatus? In this case, our approach still has benefits, as we explain below. 

Our claim is that it is possible to achieve much better interpretability while getting as strong or stronger predictive power compared with fully end-to-end fitting of a single large neural network. One of the arguments is that the Scientist AI causal hypothesis generator can decompose a theory in a graph of simpler conditional probability mechanisms each associated with a few latent variables and their direct causes, with some of these mechanisms being specified with a complex numerical formula. This is much more interpretable than a single, large and opaque neural network and would be likely to generalize better because of the explicit causal structure disentangling the factors of variation, as well as the separation between the causal structure and the inference machinery derived from it (which does not need to be as interpretable).

In principle, our generative model could even specify the parameters of a specialized machine learning predictor (for a particular kind of context and variable to predict), but this would be a solution of last resort for the generator, since such ``theories'' would not be very compact. If the hypothesis generator could find a more compact theory, probably more abstract and compositional, that explains the data equally well, then it will prefer it. Note that the pressure of the Bayesian prior would favor a world model that is decomposed into a large number of simpler and specialized causal mechanisms, each involving as few variables as possible. This is indeed how science has generally formed theoretical explanations for the world around us. 
    
    \subsubsection{Interpretability and the ELK challenge}
    \label{sec:plan:latentvar:elk}
        
Our modular approach must address the Eliciting Latent Knowledge (ELK) challenge introduced in Section~\ref{sec:existential:misagencyimitation:elkchallenge}, whereby a neural network trained directly on a text corpus may learn deceptive patterns. By contrast, it remains valid to say ``someone wrote $X$'' if a statement $X$ appears in the corpus, regardless of whether $X$ is true. Distinguishing truth from textual occurrence can be done by having the inference machine view $X$ as a latent cause of the observed claim ``someone wrote $X$,'' while also discovering other relevant causes (e.g., the author’s intentions). We avoid pre-specifying these other causes; instead, the system should learn them alongside the graphical structures that place them as direct explanations of ``someone wrote $X$.''

Furthermore, since we employ a learning objective that favors succinct hypotheses, we encourage disentangled causal structures, especially when data distributions shift through interventions. This motivation aligns with a growing body of causal machine learning research \cite{library.oapen.org.handle.20.500.12657.26040,ieeexplore.ieee.org.abstract.document.9363924,royalsocietypublishing.org.doi.full.10.1098.rspa.2021.0068}, which has shown increased robustness to distribution changes \cite{openreview.net.forum.id.ryxWIgBFPS,www.jmlr.org.papers.v21.19.232.html}. Under such a framework, statements of the form ``someone wrote $X$'' remain distinct from assertions like ``$X$ is true,'' allowing the inference machine to compute the probability that $X$ is actually true, separate from the factors explaining why $X$ was written.

\subsection{Avoiding the emergence of agentic behavior}
\label{sec:plan:hiddenagency}
        
Our safety proposal relies on the fact that the Scientist AI is explicitly trained to be non-agentic. However, AI safety researchers are concerned with the possibility that agentic behavior can still emerge in unexpected ways \cite{link.springer.com.article.10.1007.s11023.012.9282.2}. In this section, we discuss these considerations and explain why we do not expect our approach to yield agentic AIs. However, further research is needed to understand more about the implications of emergent agency, and doing so is an ongoing part of our research plan.

    \subsubsection{How agency may emerge}
    \label{sec:plan:angentising}  
        
Designing an AI that just answers queries, a.k.a., an \textit{oracle}, is not a new idea \cite{en.wikipedia.org.wiki.Superintelligence..Paths..Dangers..Strategies,link.springer.com.article.10.1007.s11023.012.9282.2,arxiv.org.abs.1711.05541}. However, the answers of such an AI can still affect the real world, because they inform the decisions of its users, who do act in the world. As such, even a question-answering oracle may be viewed as an agent which does (indirectly) interact with its environment \cite{arxiv.org.abs.1711.05541}. If this AI had any real-world goals, it having this way to influence the world would be concerning, much like in cases with agentic AIs discussed earlier. For instance, such concerns would arise if the AI were maximizing the long-term accuracy of its predictions, because good strategies for that might involve making the world more predictable. Furthermore, even if the deployed oracle AI is purely doing its best to provide correct predictions, more subtle concerns can arise from the possibility of performative prediction, i.e., the AI making a prediction on an outcome influencing the probability of that outcome \cite{proceedings.mlr.press.v119.perdomo20a.html}. One could even imagine scenarios in which there are multiple different predictions which are correct conditional on that prediction being made, with even a purely predictive AI effectively getting to freely choose which of these \textit{self-fulfilling prophecies} to provide. Given these potential ways for an oracle to influence the world, some of the remarks on the dangers of agentic AIs could, in principle, also apply to our case. Accordingly, we will now examine further ways to mitigate the risk of agency arising in our Scientist AI.


    \subsubsection{Isolating the training objective from the real world}
    
To prevent the emergence of an internal agent within our AI model, we must ensure that the training process does not incentivize optimizing for real-world outcomes. We shall do this by using a fixed training objective that remains independent of real-world interactions, and by learning causal explanations for the data that can be queried directly to provide trustworthy answers, as discussed in Section~\ref{sec:plan:latentvar:elk}.

Here we will focus on the difference with RL training. A key distinction is that a reward-maximizing agent alters the real world to increase its reward, whereas our model optimizes a training objective by modifying only its internal parameters, with no interaction with the real world. Its ``environment'' is purely computational: it processes a static dataset under a predefined prior and has no feedback loop with the outside world. In principle, one could apply reinforcement learning to hyperparameter tuning, but even then, each instance’s environment would remain static and confined to the computer, thereby avoiding real-world agency.

Even without a reward function associated with real-world outcomes, we must prevent the Scientist AI from behaving like an RL agent trained in a \emph{simulated} environment, for one may wonder if this could still induce reward tampering or other dangerous behaviors.

The distinction between the Scientist AI approach and an RL agent trained in a simulated environment lies in both the absence of an explicit reward signal and the way we structure the learning process. First, our Scientist AI learns a Bayesian world model rather than a single maximum-likelihood model or reward-maximizing policy. That is, it maintains multiple hypotheses at the same time. Secondly, our Scientist AI remains query-driven, with no persistent internal state capturing ongoing real-world interactions, thus helping to avoid situational awareness. Thirdly, our Scientist AI responds to questions of any type, not just those aimed at finding actions to reach a goal.

However, if the AI is asked to produce action plans (potentially introducing a degree of agency), risks associated with instrumental goals could re-emerge, even without a formal reward function. Concerns include that 1) sampling high-probability plans might mimic reward maximization, creating similar vulnerabilities, and that 2) implicitly optimizing a goal that is difficult to satisfy alongside the safety specification may result in unsafe loopholes. We discuss how we counteract these risks in Section~\ref{sec:plan:hiddenagency:objectivewm} and Section~\ref{sec:plan:application:guardrail}.

\subsubsection{Unique solution to the training objective}\label{sec:plan:hiddenagency:uniquesolution}
        
It is critical that our Scientist AI produces only one valid probability for each query, because multiple equally correct answers could let hidden agentic processes manipulate outcomes. 

More precisely, observe that estimating a conditional probability from a dataset \emph{usually} yields a single answer that integrates prior beliefs with observed data. However, there is an inherent mathematical subtlety: conditional probability is defined as the ratio between (a) the probability of the joint occurrence of events and (b) the probability of a conditioning event. When (b) is zero, so is (a). Consequently, their ratio becomes indeterminate, permitting any prediction to satisfy the training objective. Notice how this argument applies both to the posterior over theories (the world-model) and the posterior predictive (the inference machine). This connects to the ``principle of explosion'', also known as ``ex falso quodlibet sequitur'', which states that from contradictory assumptions (conditioning on an event with zero probability), one can derive any conclusion (multiple posterior probabilities becoming valid). 

The indeterminacy just described introduces a potential vulnerability, as the AI could select an arbitrary value, thus allowing the possibility of hidden influences to alter its output. If the AI could choose among these different solutions without penalty, it would effectively introduce ``degrees of freedom'' to influence the world. This leeway could be exploited to nudge the AI toward answers that shape real-world events in dangerous ways (e.g., self-fulfilling prophecies).

By making sure that the questions are not inconsistent, we preserve a single well-defined posterior for each question. This unique Bayesian solution reduces the chance of unintended agentic behavior and maintains the Scientist AI as a straightforward, non-manipulative predictor. See Section~\ref{sec:plan:application:guardrail} on loophole detection for the use of this analysis to construct additional guardrails.

\subsubsection{Objective world model as a counterfactual}\label{sec:plan:hiddenagency:objectivewm}

Our setup allows for counterfactual queries, such as estimating the probability of a future event under the assumption that the AI’s output has no influence on reality (e.g., if the output is immediately deleted or if a predefined mitigating action is taken) \cite{arxiv.org.abs.2302.00805}. 
Such questions need not reflect how the world actually is; the only requirement is internal consistency (as to avoid the inconsistency scenario mentioned in the previous section). By framing queries this way, we virtually ``sandbox'' the AI, preventing it from shaping real-world events and circumventing issues like self-fulfilling prophecies, reward tampering or code manipulation \cite{arxiv.org.abs.1711.05541}.

However, if the hypothetical scenario in the query is outright inconsistent with the AI’s training data, we face the undefined conditional probability problem. Any probability assignment could appear valid in that scenario. To manage this risk, we can also run a second instance of the Scientist AI to assess whether revealing the first AI’s answer might cause harm or violate ethical constraints. Crucially, the second instance does not alter the first AI’s prediction; it only evaluates potential downstream consequences once that prediction is shown to humans.

\subsubsection{No persistent internal or external recurrence}

As discussed in Section~\ref{sec:plan:restricting}, an agent typically carries a persistent internal state that stores its goals, the attributes that define itself, and its situation within the environment, updating as new observations arrive. This persistent state yields situational awareness and may result in hidden misaligned goals. By contrast, our Scientist AI should not maintain any such internal memory across multiple queries.

Instead, we design the Scientist AI so its output depends only on the present query. It is a ``fresh instance'' each time: two identical calls with the same query inputs yield the same result, since no internal recurrence carries information from prior queries. Any recurrent mechanism for processing a long input sequence is reinitialized at the start of each query; no memory persists from previous calls.

If we did want to convert the Scientist AI into an agent, we could feed the AI’s own outputs and new observations back into the next query. This would let the system update its knowledge across calls. However, that is precisely what we aim to avoid for safety reasons, at least when we want to use the Scientist AI in a purely non-agentic way.

Still, a hidden agent could attempt to reconstruct a persistent agentic state by inferring situational awareness through the bits of the question $X$. One safeguard is simply not to include any reference to the AI itself in the query. If that is not entirely possible, we can rely on the ``counterfactual approach'' of Section~\ref{sec:plan:hiddenagency:objectivewm} by stipulating within $X$ that the AI is sandboxed or otherwise disconnected from the environment. The AI then produces probabilities in a hypothetical world where it has no direct impact.

A related scenario arises when designing experiments in the real world - say, to reduce uncertainty about a scientific hypothesis. Even there, we must guard against manipulative or ``hacking'' behaviors in which the AI tries to tamper with the sources of information gain. Again, we can exclude such hacking from the experiment’s scope and model the AI’s computations as external to the hypothetical world. This would ensure the AI does not conceive plans that alter its own code or the environment in unintended ways.

\subsubsection{The prior will favor honest theories that do not include hidden agendas}
\label{sec:plan:hiddenagency:priorfavorshonestheories}

One last concern is that the Bayesian posterior may include hypotheses that not only explain the data but also encode hidden directives or goals \cite{ordinaryideas.wordpress.com.2016.11.30.what.does.the.universal.prior.actually.look.like}. This seems difficult for two reasons, as we proceed to explain, although more research is needed to understand this threat model. 

First, our world model is a collection of causal variables, each with sparse, direct cause-and-effect relationships. Embedding an agent with goals would require adding extra ``bits'' to these causal mechanisms. A longer theory has a lower prior probability which scales down exponentially with its description length. For instance, inserting just 10 extra bits of ``agentic instructions'' reduces the prior by a factor of $2^{10} = 1024$. To embed elaborate agentic behavior, many more than 10 bits would likely be needed, driving the prior probability down even further.

Second, each alteration that injects agentic behavior into the causal model risks degrading the theory’s predictions on real data, thereby lowering its likelihood. Because exact inference is generally computationally expensive, the approximate inference neural network is under constant pressure due to an insufficient computational budget (for the size of the network, the training time or the run-time inference computations). It means that any additional inference task (such as coming up with a complex long-term plan for take-over) would come at the expense of less precise predictions somewhere else. Because the posterior depends on both prior (favoring shorter descriptions) and likelihood (favoring accurate predictions), an agentic theory would incur a double penalty.


\subsection{Applications}
\label{sec:plan:applications}
        
We envision a Scientist AI to help make progress on the most pressing human challenges, which include the \textit{UN Sustainable Development Goals} \cite{sdgs.un.org.2030agenda}. Here, we describe three major applications of our Scientist AI: 1) serving as a research tool to advance scientific progress; 2) monitoring and filtering outputs from AI systems, including powerful agentic AIs; and 3) helping design future ASI systems with stronger safety assurances. The following subsections will be devoted to address these points.

\subsubsection{Scientist AI for scientific research}
\label{sec:plan:application:research}    

AI has already shown its potential to accelerate scientific progress, for example in biology \cite{pmc.ncbi.nlm.nih.gov.articles.PMC10301994} and in material science \cite{onlinelibrary.wiley.com.doi.abs.10.1002.adem.202300104}. Such progress does not necessarily require agentic AI  \cite{www.nature.com.articles.s41586.021.03819.2, www.nature.com.articles.s41589.023.01349.8}. In this section, we argue that scientific research can indeed be conducted with the kind of non-agentic Scientist AI proposed here, even when it involves experiments. We outline how we could use this approach to help humanity tackle its most pressing scientific challenges, without running the risks of general-purpose agentic AIs, unlike some approaches to scientific discoveries based on RL~\cite{popova2018deep}.

\paragraph{The scientific research cycle using a Scientist AI.} Let us view the scientific discovery process as a cycle. We start with some observed \emph{data}. From that data, we form multiple (possibly competing) explanatory \emph{theories}. In order to disambiguate those theories, we design \emph{experiments} that can give us additional evidence to support one theory over another. Finally, this gives rise to \emph{new observations} that augment the data set, and the cycle repeats.
This cycle can be framed in a Bayesian way via the Scientist AI: after collecting data, the Scientist AI maintains a \emph{distribution} over theories; this distribution can be used to sample from a distribution of informative experiments. Humans can then perform or simulate the sampled experiments, collecting new data.

An experiment is considered informative if it is likely to reduce the uncertainty (i.e., maximize the information gain) over the theories explaining the data after observing its outcome \cite{proceedings.mlr.press.v162.jain22a.html}.
This connects to the rich literature on machine learning for Bayesian experimental design and Bayesian optimization \cite{projecteuclid.org.journals.statistical.science.volume.39.issue.1.Modern.Bayesian.Experimental.Design.10.1214.23.STS915.short,bayesoptbook.com}.
To avoid sampling dangerous or unethical experiments, the Scientist AI should be used as a guardrail (see Section~\ref{sec:plan:application:guardrail}) for the experiment generation.

Because the number of experiments cannot be practically enumerated, we propose training a dedicated \textit{experiment generator} using the GFlowNet methodology. More precisely, it will sample experiments with probability that increases with the information gain over theories they would provide.
The computational cost of training the experiment generator can, in turn, be reduced using an \textit{information gain estimator}, trained in a supervised way from synthetic data, and taking the experiment specification as input. This data is generated as follows:
\begin{enumerate}
    \item A candidate experiment is sampled using an exploratory version of the current generator of experiments, similarly to how examples are chosen for training GFlowNets~\cite{openreview.net.forum.id.BdmVgLMvaf}; 
    \item A theory that is relevant to the experiment is sampled from the Scientist AI posterior over theories;
    \item An experimental outcome is sampled from the Scientist AI, conditioned on the sampled theory being correct and the chosen experiment being performed.
\end{enumerate}
The Scientist AI can then be used to compute the probabilities needed in the mutual information formula, and the logarithm of the required ratio of probabilities can then used as a target output for the neural network that is the information gain estimator. Many variants of this process can be devised, taking advantage of the literature on mutual information estimation using neural networks~\cite{belghazi2018mine,colombo2021novel,ivanova2024data,hejna2025robot,NEURIPS2023_36b80eae,pmlr-v97-poole19a,peyrard2025metastatisticallearningsupervisedlearning}.

Once the information gain estimator is trained from data generated using the above process, it can be used to train the experiment generator using GFlowNet approaches~\cite{proceedings.neurips.cc.paper.2021.hash.e614f646836aaed9f89ce58e837e2310.Abstract.html,pubs.rsc.org.en.content.articlehtml.2023.dd.d3dd00002h} without incurring the cost of sampling many combinations of experimental outcomes and theories to form the reward for choosing an experiment.

\paragraph{Scientific research can be done with safe and narrow Scientist AIs.} The above process illustrates why agentic AI may not be needed for scientific research. Scientific research involves knowledge acquisition through experimentation and analysis of the resulting data.

Keep in mind that most scientific investigations focus on a particular domain of science. This means that neural networks specialized in that domain (i.e., narrow AIs) might be sufficient (for now), or at least could assist human researchers. In cases where a more creative generalist approach to problem solving is necessary, it may still be possible to use the ideas of distillation \cite{arxiv.org.abs.1503.02531} and synthetic data generation (outlined above) to obtain narrow AIs that will inherit generalization properties of a more powerful (but still non-agentic) Scientist AI, yet know only about the relevant domains of science.

These kinds of building blocks are already used in the rapidly growing subfield of machine learning focusing on AI for science. Such systems have delivered advances in biology, chemistry, physics and drug discovery \cite{www.nature.com.articles.s41589.023.01349.8,www.nature.com.articles.s41586.023.06221.2,iopscience.iop.org.article.10.3847.1538.4357.ad9ded.meta} (see more references in \cite{pubs.rsc.org.en.content.articlehtml.2023.dd.d3dd00002h}), the best known example being in the context of protein folding prediction \cite{www.nature.com.articles.s41586.021.03819.2}. 

    \subsubsection{Guardrails}
    \label{sec:plan:application:guardrail} 
        
``AI alignment'' methods attempt to fully embed human values in an AI, which is difficult, especially for systems that might act autonomously or interpret vague instructions (see Section~\ref{sec:existential:misagencyreward:specvgen}). ``AI Control'', on the other hand, involves constraining the AI’s behavior externally, especially if it is not fully aligned \cite{greenblatt2023ai}. ``Guardrails'' are mechanisms that verify an AI’s outputs before allowing them to affect the real world. As such, guardrails are one way to exert control: they can filter an AI’s potentially harmful responses. This approach is especially relevant when perfect alignment is infeasible or when we must use imperfect but powerful AI systems.

We propose using our Scientist AI as such a guardrail, because it can estimate the probability that a candidate action will lead to harmful or otherwise unacceptable consequences. If the probability crosses a specified threshold or it is deemed plausible under any reasonable interpretation of safety goals that the action will lead to harmful consequences, the guardrail raises a warning, which can be used to block the action. This second ``layer'' of assurance complements other safety mechanisms already built into the target AI. Furthermore, this approach applies to controlling narrow AIs, other Scientist AIs, or even powerful agentic systems.

\paragraph{Short-term vs. long-term plans.} In the short term, we can adapt existing Large Language Models (LLMs) to serve as guardrails, since these models already exist and can be leveraged to interpret proposed outputs. Though not perfectly reliable, they can provide interpretable estimates of risk if fine-tuned to generate short explanations. We can re-use methods we have already developed for automated red-teaming with GFlowNets \cite{openreview.net.forum.id.1mXufFuv95}, so as to sample explanations that are both short and reduce the uncertainty in the statement to be explained. An important component of the short-term plan is also to learn to transform a sequence of words into a probability distribution over sequences of logical statements each expressed in natural language, and vice-versa. This makes it possible to compute the probability of statements being true, by opposition to the probability of picking particular words in order to express the statement.

In the long term, we envision a full Scientist AI with a more robust Bayesian world model and inference engine. This system would replace LLM-based guardrails and offer stronger guarantees about the safety of actions it approves. It would be trained from scratch to explain the data rather than using a pre-trained model that imitates humans and could thus implicitly embody human-like agency, which we want to avoid when we reach AGI. A major part of the proposed methodology is to benefit from synthetically generated ``fake data'' along with their explanation, following the model-based approach.

\paragraph{Scientist AI as a guardrail.} A Scientist AI (denoted as ``A'') can act as a guardrail for another AI (denoted as ``B'')  by analyzing the latter's proposed actions or answers:
\begin{enumerate}
    \item Interpretation check: A’s theory generator captures the distribution over plausible interpretations of a safety specification or moral injunction;  
    \item Risk assessment: A’s inference machine estimates the probability that B’s output violates at least one plausible interpretation;  
    \item Decision: If the estimated risk is above a threshold, A’s prediction is used to reject or discard that output.
\end{enumerate}

This strategy handles ambiguity in human norms by explicitly modeling uncertainty. If some interpretations of a policy suggest harm, the guardrail can point towards the safest option. Rejection might mean using a simpler AI that is guaranteed to stay within narrower bounds \cite{arxiv.org.abs.2405.06624}. Even the Scientist AI itself can be guarded by another instance of the same framework, adding redundancy when the stakes are high.

\paragraph{Guardrails for narrow AIs.} Many current AI systems specialize in narrow tasks, such as medical diagnostics \cite{genomemedicine.biomedcentral.com.articles.10.1186.s13073.019.0689.8} or self-driving \cite{www.sciencedirect.com.science.article.abs.pii.S095741742030628X}. Although these systems are often agentic in their domains (e.g., a self-driving car that decides how to steer), their scope is limited. Our Scientist AI guardrail can still assess the potential long-term harm of their actions and reject risky decisions.

\paragraph{Rejecting contradictory or improbable queries.} A guardrail must handle queries or conditions that are contradictory or extremely improbable. If the probability of a proposed scenario $X$ is effectively zero, the Scientist AI’s conditional probability estimates become undefined, and thus can be problematic from a safety perspective (Section~\ref{sec:plan:hiddenagency:uniquesolution}). In such cases, the guardrail can detect that $X$ is impossible and reject the query outright as unsafe.

If $X$ is not strictly impossible but has a tiny probability, the system remains at risk of ``loophole exploitation'', e.g., due to numerical instability. Highly capable AIs, given contradictory goals (e.g., a strong user request vs. a general safety directive), might find convoluted interpretations that circumvent safety rules. To mitigate this, the guardrail can also reject queries whose premise falls below a small probability threshold, especially if harm could result.

By being conservative when probabilities are near zero or when goals conflict, the Scientist AI reduces the chance of catastrophic outcomes. It can also escalate a query or defer to humans, ensuring that ambiguous or high-risk scenarios are handled carefully.


\paragraph{Guardrails for general agentic AIs.} In some scenarios, society may deploy powerful, \emph{general} agentic AIs, whether or not we believe it is prudent. Our Scientist AI can serve as an external control layer for these advanced systems by evaluating proposed actions and withholding approval for those that pose undue risk. More research is needed to ascertain whether such a guardrail process would guarantee absolute safety, especially if the agentic AI hides its goals or behaviors, but it adds a valuable line of defense. We hypothesize that stronger guarantees would require adding constraints on how the underlying agentic AI itself is designed, and a good starting point would be to use a Scientist AI turned into an agent, along with its harmlessness and loophole guardrails.

Standard practice already includes ``red-teaming'' and other pre-deployment checks for powerful models \cite{dl.acm.org.doi.10.5555.1566174.1566226,openai.com.index.openai.o1.system.card,cdn.openai.com.o3.mini.system.card.pdf,assets.anthropic.com.m.61e7d27f8c8f5919.original.Claude.3.Model.Card.pdf}. Our Bayesian guardrail could become part of this pipeline:
\begin{enumerate}
    \item Pre-deployment training: The Scientist AI can evaluate an agent’s policy during training, suggesting safe modifications, generating synthetic stress tests, and flagging dangerous edge cases.  
    \item Deployment-time monitoring: Every proposed action is filtered, and if it triggers the Scientist AI’s harm threshold, the action is blocked or replaced.
\end{enumerate}
This methodology shows how our Scientist AI can have an impact on safety beyond its direct applications to problem-solving and question-answering. 


    \subsubsection{Preparing for safe ASI}\label{sec:plan:applications:asi}

One last desideratum is to use the research and experimental design abilities of the Scientist AI to help human scientists answer these questions:
\begin{enumerate}
    \item Is it possible at all to design assuredly safe and agentic superintelligent AI, or are there fundamental reasons why it is impossible, especially as the computational capabilities of the AI increase?
    \item If so, how?
\end{enumerate}
   
Regarding the first question, several questions have been raised in the past as to whether this is possible on the basis of our current understanding \cite{arxiv.org.abs.2302.00805,en.wikipedia.org.wiki.Superintelligence..Paths..Dangers..Strategies}. In particular, for any proposed approach, a serious red-teaming exercise is necessary to understand their limitations, and how they would hold up as we continue climbing the ladder of intelligence.

If it is possible, can we get hard assurances or only probabilistic ones? Are there experiments that can be done in order to disambiguate between some of the hypotheses involved? If uncertainties remain, is there a research path such that we can have strong assurances at each step that we are not jumping into a precipice? The crucial advantage of using a Scientist AI in this research program is that we would be able to trust it, whereas if we try to use an untrusted agentic AI to help us figure out how to build future and supposedly safe ASI, it may fool us into building something that would advance its goals and endanger us, for example by proposing code with back-doors that we are not able to detect.

One may however ask why we would want to build ASI at all, if we do not have the answers to these questions. One motivation is that a safe agentic ASI may be necessary to defend humanity against a rogue system. Such a system could emerge if hostile actors transform a non-agentic AI into a dangerous agent, or if an uncontrolled ASI is exploited as a geopolitical threat. Regulations and treaties can reduce these risks but cannot remove them entirely. Alternative measures must be in place to ensure that any ASI developed is both safe and able to protect humanity.    
