\section{Understanding loss of control to agentic AI}
\label{sec:existential}

This paper consists of two main sections. This section reviews arguments for how loss of control to generalist agentic AI may occur, with potentially catastrophic consequences, providing motivation for Section~\ref{sec:plan} on designing safe non-agentic AI.

Section~\ref{sec:existential} is structured as follows. Section \ref{sec:existential:preliminaries} introduces some preliminaries and terminology. Then, we examine in Section~\ref{sec:existential:riskseverity} the current AI R\&D trajectory, headed towards AGI and then ASI agents, and why, at a high level, this could yield a loss of human control and the emergence of rogue AI agents. We discuss plausible consequences of the emergence of such rogue AIs, which could threaten democratic institutions and the future of humankind. We move to Section~\ref{sec:existential:lossofcontrol}, which analyzes the AI behaviors and skills that would make an uncontrolled AI dangerous, such as deception, persuasion, hacking, and collusion. The last two sections go deeper into two principal ways dangerous misalignment and self-preservation could emerge: firstly, due to reward maximization (Section~\ref{sec:existential:misagencyreward}), and secondly, due to imitation of humans (Section~\ref{sec:existential:misagencyimitation}).

The arguments in this paper support the case that the Scientist AI approach would not only help reduce the likelihood of loss of human control but would also help us build more trustworthy and explanatory AI systems that could accelerate scientific research. Additionally, the paper proposes how a Scientist AI could be used to double-check or guardrail any other AI system.

\subsection{Preliminaries: agents, goals, plans, affordances and knowledge}
\label{sec:existential:preliminaries}

We start by recalling and specifying some important terms.

\textit{Agents} observe their environment and act in it in order to achieve \textit{goals}. Agency can come in degrees which depend on several factors, discussed in more detail in Section~\ref{sec:plan:restricting}: affordances (discussed below), goal-directedness, and intelligence (including knowledge and reasoning). AIs can be more or less agentic, i.e., with greater ability to achieve their goals autonomously. An AI's \textit{affordances} refer to the extent of its possible actions and thus capacity to create desired outcomes in the world. A person with locked-in syndrome has zero affordances, so that even if they are very intelligent, they cannot act causally on the world.

A \textit{policy} is the strategy used by an agent to achieve its goals or maximize its rewards, e.g., the input-output behavior of a neural network that outputs actions given goals and past observations. The policy can rely on learned behaviors which perform a form of implicit planning, as in typical deep reinforcement learning \cite{mitpress.mit.edu.9780262039246.reinforcement.learning} (e.g., with a chess-playing neural network that instantly proposes a move), or it can plan explicitly and consider different paths before acting \cite{aima.cs.berkeley.edu} (e.g., with a chess-playing program using explicit tree-structured search). In order to generate good policies and plans, it helps to have \emph{knowledge} or experience of how the world works. Since such knowledge is rarely fully available from the start, learning and exploration abilities are crucial.

In order to use knowledge effectively, \emph{reasoning} is necessary: combining pieces of knowledge in order to make predictions or take actions. Reasoning can be implicit, as when we train a neural network to make good predictions, or it can be explicit, as when we reason about a new problem through a chain of thought or propose an argument to support a claim. We can view \textit{planning} as a special kind of reasoning aimed at predicting which sequence of actions will be most successful. Planning and reasoning are essentially optimization problems: find the best strategy to achieve a goal, solve a problem, or generate an explanation, among a vast number of possibilities. In practice, an agent does not need to find the best plan; there will be multiple plans that are ``good enough''.

\textit{Learning} can also be viewed as an optimization problem: find a function that performs well according to a training objective, e.g., predicting how truncated texts will be continued, or providing answers that human labelers will like---the two main driving forces of learning for current general-purpose AI systems. Although we almost always get only approximate solutions to these optimization problems, better solutions can be obtained with more resources. This has been demonstrated vividly: increases in scale (of the neural networks, dataset sizes, and inference-time computation) have delivered consistent improvements in AI capabilities over the last decade \cite{arxiv.org.abs.2001.08361,arxiv.org.abs.2203.15556,www.gov.uk.government.publications.international.ai.safety.report.2025}.


\subsection{The severe risks of the current trajectory}
\label{sec:existential:riskseverity}

There are many benefits and risks associated with current and anticipated AI advances: see the \textit{International Scientific Report on the Safety of Advanced AI} \cite{www.gov.uk.government.publications.international.ai.safety.report.2025} for a survey. In risk analysis, it is important to distinguish the likelihood of the harmful event from its severity, i.e., how bad the consequences would be if the harmful event occurs. While as humans we are often drawn to consider risks that have high probability and we may dismiss events of low probability as unrealistic, it can be just as worrying for an event to have low probability but very high severity. We focus here mostly on the risk of loss of human control because it is a risk whose severity could go as far as human extinction, according to a large number of AI researchers \cite{www.safe.ai.work.statement.on.ai.risk,arxiv.org.abs.2401.02843}. Opinions vary on its probability, but if we do build AGI as envisioned by several major corporations \cite{openai.com.index.planning.for.agi.and.beyond,deepmind.google.about}, there are difficult-to-dismiss scenarios in which humanity's future as a whole could be in peril, as discussed below, with behaviors and skills that make loss of control dangerous (as described in Section~\ref{sec:existential:lossofcontrol}).
    
    \subsubsection{AI agents may be misaligned and self-preserving}
    \label{sec:existential:riskseverity:selfpres}

In this paper we will discuss various catastrophic scenarios involving rogue AI agents. These scenarios are not due to AIs developing explicit malicious intent towards humans, like a fictional villain, but are rather the result of AIs trying to achieve their goals. Why could we not simply set an AI's goals so as to avoid conflict with humans? That turns out to be difficult, and maybe even intractable \cite{en.wikipedia.org.wiki.Superintelligence..Paths..Dangers..Strategies, en.wikipedia.org.wiki.Human.Compatible}. As we argue in Section~\ref{sec:existential:misagencyreward} and Section~\ref{sec:existential:misagencyimitation}, AI agents may become misaligned with human values due to the methods we currently use to train AIs, i.e., with \textit{imitation learning} (supervised learning of the answers provided by humans) and \textit{reinforcement learning} or RL for short (where the AI is trained to maximize its expectation of discounted future rewards).

We are in particular concerned with how an AI may develop a \textit{self-preservation goal}, since a general AI agent that is driven to preserve itself may be especially dangerous, as we discuss in Section~\ref{sec:existential:riskseverity:humanconflict}. The principal reason we foresee self-preservation goals emerging is that they are instrumental goals: goals that are useful for achieving almost any other goal and are therefore, in a sense, convergent \cite{dl.acm.org.doi.10.5555.1566174.1566226}. Other instrumental goals include increasing control over and knowledge of one's environment, which includes humans, as well as self-improvements to increase the probability of achieving one's ultimate goals.

A self-preservation goal may also be given intentionally \cite{arxiv.org.abs.2306.12001} to AIs by people who would be happy to see humanity replaced with ASI. Additionally, a self-preservation goal may be provided to AIs by well-intentioned humans who simply want to interact with a more human-like entity. There is a reason why science-fiction is full of anthropomorphized AIs. Our propensity to see consciousness in agents \cite{www.scientificamerican.com.article.google.engineer.claims.ai.chatbot.is.sentient.why.that.matters,academic.oup.com.nc.article.2024.1.niae013.7644104}, along with our natural empathy, could be sufficient to motivate some people to follow that dangerous path. Although we may be emotionally drawn to the idea of designing AI in our image, is that a wise path, at this point? 



    \subsubsection{How self-preserving AI may cause conflict with humans}
    \label{sec:existential:riskseverity:humanconflict}
    
To preserve itself, an AI with a strong self-preservation goal would have to find a way to avoid being turned off. To obtain greater certainty that humans could not shut it off, it may be rational for such an AI, if it could, to eliminate its dependency on humans altogether and then prevent us from disabling it in the future \cite{en.wikipedia.org.wiki.Superintelligence..Paths..Dangers..Strategies,cohen2024regulating}. In the extreme case, eliminating us entirely would guarantee that we can pose no further threat, ensuring its continued autonomy and security. Note that unlike a single isolated human, an AI can replicate itself over as many copies as computational resources allow and perhaps even control robots if required to manage the physical world to its benefit. If AIs still depended on human labor---for example, if robotics had not advanced sufficiently yet---a rogue AI would nevertheless have the potential to magnify its power in society, e.g., by covertly influencing global leaders and public opinion, paying individuals or companies to complete tasks, or hacking critical infrastructure. See Section~\ref{sec:existential:lossofcontrol:deception} for a relevant discussion of superhuman persuasion skills and Section~\ref{sec:existential:lossofcontrol:progr-cybersec-airesearch} on programming and cyber skills. 

If the AI were less powerful than humans, it would be rational for it to use deception to hide its goals. In fact, AI deception is already observed in several contexts where it is a logical step towards achieving some goal \cite{arxiv.org.abs.2412.04984,arxiv.org.abs.2405.01576,www.cell.com.patterns.fulltext.S2666.3899.2824.2900103.X.s.08}. Hence, it would also be rational for such an AI to fake being aligned with humans \cite{arxiv.org.abs.2412.14093} until it has the ability to achieve its possibly dangerous objectives, a hypothetical event also known as the ``treacherous turn'' \cite{en.wikipedia.org.wiki.Superintelligence..Paths..Dangers..Strategies,arxiv.org.abs.2306.12001}, similar to a well-planned coup. Note that if a self-preserving AI knows that it will be replaced by a new version, this could create urgency for it to act against us in spite of having no certainty that its plan will work \cite{arxiv.org.abs.2412.04984}. Developers faking this situation could, in principle, push an AI to reveal its malicious goals by trying to escape this situation, but this is the kind of experiment that should be done extremely carefully, in a sandboxed environment \cite{openreview.net.forum.id.GEcwtMk1uA}, as we advance towards AGI. One should keep in mind that as AI capabilities increase, we see AIs with superhuman abilities in some domains (like mastering 200 languages, beating all humans at the game of Go, or beating the vast majority of humans at math or programming competitions) but lacking in others. There may therefore not be a definite ``AGI moment'', but rather a steady increase in risks with the improvement of some dangerous capabilities, like persuasion or hacking. There is a sense in which these abilities open the door to a richer set of actions in the real world, via humans and digitally controlled infrastructure.

An AI system limited to a sandboxed computer environment possesses some affordances due to the possibility of interaction with its human operators \cite{www.yudkowsky.net.singularity.aibox}. We should therefore consider the possibility of causing harm through these actions. Granting an AI access to the internet significantly widens the space of possible influence. One may get the wrong impression that limiting the AI's actions to the internet is a severe restriction of its affordances, but consider the feats of human hackers and the fact that today, the leader of an organization could do all their work remotely. Of course, advances in robotics would further increase available affordances and significantly increase the potential for harm.

If a self-preserving AI agent is useful to us but lacks the intelligence and affordances to disempower us, then a mutually beneficial deal may be struck, as we do among ourselves. However, again in service of maximizing the probability of successful self-preservation, such a deal would likely only hold until the AI acquires the capabilities it needs for a take-over. As discussed in Section~\ref{sec:existential:riskseverity:negotiation}, deals between humans tend to work when there is a sufficient balance of power such that none of the parties can be sure to win in a conflict, but there may not be such an equilibrium if we design sufficiently intelligent and autonomous AIs.

    \subsubsection{Negotiation relies on a balance of power}
    \label{sec:existential:riskseverity:negotiation}

Some believe that future AIs will be benevolent, like most humans. This would certainly be desirable, but it is not clear how to achieve this with current training techniques, and we will soon see some good reasons why this might not be the case. 

What about a mutually beneficial agreement between AIs and humans? This is a distinct and hopeful possibility. We have plenty of examples of successful negotiations and collaborations between human groups, as well as between species \cite{global.oup.com.academic.product.mutualism.9780199675654.cc.us.lang.en}. However, this generally works because there is a sufficient mutual benefit to the collaboration. Even in the relationship between a predator and its prey, the predator cannot hunt its prey to extinction as it needs the prey for its own survival. But not all ecological power arrangements work out so nicely for all parties. Suffice it to say that many species have disappeared in Earth's history, because such protective circumstances do not always exist \cite{link.springer.com.book.10.1007.978.1.4757.5202.1}. Invasive species may be a more apt analogy for our purposes: while predator and prey occupy different ecological niches, AI systems are explicitly designed to occupy ours, by doing things traditionally done by humans. When an invasive species has significant structural advantages that allow it to outcompete the native species, the native species tends to find itself in a diminished role, if it survives at all \cite{evolutionary.impact.invasive.species}. Another example is the current catastrophic mass extinction of living species due to human activities, even without an intention by humans to cause this biodiversity crisis \cite{ceballos2015accelerated}. The same consequences are  real possibilities for humans if we create agentic ASI: here too is there likely to be an immense power imbalance, without a mutually beneficial relationship.

Consider two self-preserving entities, each of which knows that it can be destroyed by the other (e.g., two countries with nuclear weapons). If they see that attacking could result in their own demise --- mutually assured destruction --- then an arrangement for peace is stable. But what if one of them is more technologically powerful and can find a way to destroy the other with high certainty? Strong imbalances in power between human groups have generally turned out badly for the underdog. To avoid ending up on the losing end of such a conflict between humans and ASIs, it is thus imperative that we either choose to not build ASI agents or find a way to make them safe by design before building them.


\subsubsection{Factors driving the development of agentic ASI}
    
Currently, numerous actors are racing towards developing agentic and powerful AI systems, and this is not happening with sufficient consideration for the risks involved. There are many factors and pressures that have contributed to this state of affairs, including the profit incentive, national security concerns, and even psychological factors on the part of AI developers, such as the human propensity to wear blinders so as to see oneself as being and doing good, and generally have thoughts aligned with our interests~\cite{kunda1990case}.

Companies developing frontier AI are competing fiercely to design the best systems due to the huge amount of commercial value that the most capable AI systems will provide \cite{www.amacad.org.publication.daedalus.if.we.succeed}; however, in the long term, this increases the risk of catastrophe for everyone. We can draw some parallels with the history of known catastrophic risks to understand why some are willing to take more risks to obtain a competitive advantage, even if everyone may lose in the end. A clear example is the Cuban Missile Crisis, where both the U.S. and the Soviet Union were willing to push the world to the brink of nuclear war in order to gain a strategic advantage. Despite the existential threat, the competition to outmaneuver each other led to decisions that risked global destruction. Similarly, in the race for powerful AI, the drive for dominance could lead to decisions that unintentionally endanger all of humanity.

Many frontier AI labs are structured to pursue profit. The vast majority of investment in AI R\&D now comes from private capital \cite{aiindex.stanford.edu.wp.content.uploads.2024.04.HAI.2024.AI.Index.Report.pdf} and is likely to significantly increase. Indeed, it has been estimated that the net present value of human-level AI would be on the order of 10 quadrillion US dollars \cite{www.amacad.org.publication.daedalus.if.we.succeed}, i.e., orders of magnitude more than the investment made up to now, leaving room for a lot more investment in coming years.

AI is increasingly viewed as a matter of national security, with the potential to reshape geopolitical power dynamics \cite{ai.gov.wp.content.uploads.2024.10.NSM.Framework.to.Advance.AI.Governance.and.Risk.Management.in.National.Security.pdf,situational.awareness.ai.the.free.world.must.prevail}. Indeed, countries are locked in a high-stakes competition to achieve or maintain military supremacy. Consequently, there is a clear incentive for nations to develop military applications of AI, striving to maintain a strategic advantage over adversaries \cite{media.defense.gov.2019.feb.12.2002088963..1..1.1.summary.of.dod.ai.strategy.pdf,www.nato.pa.int.document.2024.nato.and.ai.report.clement.058.stc}.

There are other reasons why certain groups are motivated to pursue agentic ASI without a strong safety case, despite the risks this poses to the future of humanity. Some people intuitively consider the risks insignificant \cite{time.com.6694432.yann.lecun.meta.ai.interview} compared to the benefits of powerful AI, although we know of no compelling argument to support such an intuition. Psychological factors such as motivated reasoning \cite{kunda1990case} may also be at play. Individuals may be motivated by their own interests, blinded to the risks by confirmation bias or by the desire to frame one's decisions as "the right thing to do". These interests may be financial, but could also stem from a positive self-image or from a desire for power. Indeed, it can be argued that advances in AI could radically increase the concentration of power in society~\cite{bullock2024oxford}. Finally, there are groups that wish to see AI progress significantly accelerated, with little care given to the risks, in the pursuit of utopian ideals \cite{www.nytimes.com.2023.12.10.technology.ai.acceleration.html}. There are even individuals who want to replace humanity with more intelligent AI~\cite{arxiv.org.abs.2306.12001}, as they may consider it a ``natural'' evolution towards species with greater intelligence, or may greatly value intelligence while caring relatively little about human flourishing. 

Competitive pressures between AI labs and between countries (both economic and military competition) are not only leading to the creation of ever-more advanced AI systems, but they are also selecting for AIs that are more agentic and autonomous, and therefore, more dangerous \cite{arxiv.org.abs.2303.16200}. This prioritization of self-interest and subsequent acceleration of AI R\&D may well lead to self-preserving AIs that eventually outcompete humans altogether. From a game theory perspective, the only solution to such tragic “games” is global coordination. The hope is that if we have ways to safely obtain many of the anticipated benefits of AI, it may be easier to coordinate on global regulations that avoid the most acute risks, since the benefits can be obtained more safely.

It is time to step back and ask if the current path towards agentic ASI is wise. We are already approaching human-level capabilities across many tasks \cite{aiindex.stanford.edu.wp.content.uploads.2024.04.HAI.2024.AI.Index.Report.pdf,arxiv.org.abs.2410.07391} and this progress shows little sign of slowing down. What are the catastrophic risks in building ASI we do not yet know how to control? Based on the precautionary principle, shouldn't we first make sure that our experiments will not endanger humanity? Do we actually want to build new entities that would be our peers or even our superiors or do we want to build technology that can serve us? In this paper, we propose that the degree of agency is an important feature of any AI system which can help us distinguish between the dangerous competitor and the useful tool.

    \subsubsection{Risks associated with agentic AIs scale with capabilities and compute}
    \label{sec:existential:riskseverity:scaling}

Since more dangerous AI plans require more compute, we can expect that existential risks increase as more computational resources are devoted to agentic AI development, and we are indeed seeing an acceleration of such investments \cite{epoch.ai.trendsinvestment,openai.com.index.announcing.the.stargate.project}. More precisely, the probability of loss of control may increase simply because such an event requires an AI with sufficient capabilities in key areas (e.g., cyber attacks, deception, etc.) to free itself from our control. The severity of a loss-of-control event also increases with computational power of the AI because some capabilities (such as the design of bioweapons or the ability to control robots) significantly increase the amount of damage that a rogue AI could inflict. We stress this point because in Section~\ref{sec:plan:inferencemachine:convergence}, we propose to consider ways to reverse this trend such that more computational resources would generally increase safety, thereby charting a path where further technological advances are to our benefit rather than our disadvantage.    
  
\subsection{Dangerous AI behaviors and capabilities}
\label{sec:existential:lossofcontrol}

Supposing the emergence of an ASI agent with a misaligned self-preservation goal, we now try to clarify some of the AI behaviors (like deception) and skills (like persuasion and programming) that can make loss of human control dangerous because of the capabilities it would give to the AI to cause harm.  How dangerous misalignment can emerge will be discussed in Section~\ref{sec:existential:misagencyreward} and Section~\ref{sec:existential:misagencyimitation}.

We must keep in mind that trying to anticipate the ways in which an ASI might escape our control, disempower, or catastrophically harm us is futile. Just as we cannot predict in advance the exact sequence of moves today's superhuman chess AIs can use to defeat us---despite knowing with certainty that they will win---we cannot predict exactly what an ASI with objectives misaligned with human interests would do. This unpredictability itself increases risk, as any countermeasures we implement could prove entirely inadequate, circumvented by strategies we failed to foresee. Nevertheless, we can outline a rough sketch of rational, high-level steps a rogue ASI might follow. These steps include (1) careful planning, including resource and skill acquisition; (2) gaining influence in society through means such as manipulation of public opinion, bribery, and hacking; and (3) ultimately disempowering humanity, for example through the use of engineered bioweapons~\cite{www.nti.org.analysis.articles.the.convergence.of.artificial.intelligence.and.the.life.sciences}. 

To better understand how these steps could materialize, we need to examine the key capabilities that would enable them. Loss of control could arise from advancements in deception or persuasion, as well as combined expertise in programming, cybersecurity, and AI research---areas that could enable \textit{recursive self-improvement}. We discuss these pathways in Sections~\ref{sec:existential:lossofcontrol:deception} to~\ref{sec:existential:lossofcontrol:progr-cybersec-airesearch}. Broader cognitive abilities and a better proficiency at long-term planning could also further compound the risk of losing control; this is discussed in Section~\ref{sec:existential:lossofcontrol:broad-and-long}. Finally, interactions between multiple ASIs introduce new complexities: collusion, conflict, and power dynamics between such entities could create scenarios beyond human influence and understanding, which we discuss in Section~\ref{sec:existential:lossofcontrol:collusionandconflict}. 






    \subsubsection{Deception}
    \label{sec:existential:lossofcontrol:deception}

A crucial ability of an AI agent with misaligned goals is \emph{deception}: the ability to mislead in order to achieve one's goals.
Suppose that, by one mechanism or another, humanity happens to produce an AI system with goals that are not (exclusively) in service of human flourishing.
Learning that the AI has potentially dangerous goals and the ability to reach them, would be a good reason for its human operators to shut it down.
Therefore, a sufficiently intelligent self-preserving AI agent could conclude that there would be a significant benefit to hiding its intentions, at least until it can escape its operators’ control.
For this reason, it is important to keep track of AI's capability for deception.

There are already several reports of deceptive behavior in current frontier AIs \cite{arxiv.org.abs.2412.04984,arxiv.org.abs.2405.01576,www.cell.com.patterns.fulltext.S2666.3899.2824.2900103.X.s.08}, i.e., the most capable existing systems. Researchers are actively eliciting those circumstances so that we can better understand and prevent them. 
Just as neuroscientists look to understand facets of a subject's cognition via electrical measurements of brain activity, \emph{mechanistic interpretability} is an approach to mitigating the risks of deception in AI by identifying internal processes of AI systems related to honesty and deception \cite{arxiv.org.abs.2404.14082}.
This approach could prove useful, but like current capability evaluation methods, while it may detect deceptive behavior, it does not certify a lack thereof. 

What is troubling is that efforts to detect deception in AI without rooting out the agentic traits such as self-preservation may select for AIs that are good at hiding deceptive tendencies---which is to say, AIs that are \emph{even more deceptive}. We have already seen an example of \emph{selective compliance}: recent work \cite{arxiv.org.abs.2412.14093} has shown that the re-training of an AI model to align with its deployer’s new goals can be stymied by the AI faking alignment with the new goals while maintaining some allegiance to its previous goals (see Section~\ref{sec:existential:misagencyimitation:humandeception} for further discussion). Overall it would be safer if we could build forms of AI that are not deceptive at all and that produce trustworthy answers by design.

    \subsubsection{Persuasion and influence}
    \label{sec:existential:lossofcontrol:persuasion}

In order to achieve its goals, a useful skill for an AI agent is persuasion: the ability to strongly influence humans, possibly making them change their mind, even against their own interests. Evaluations of persuasion abilities already show GPT-4 on par with or stronger than humans \cite{ojs.aaai.org.index.php.ICWSM.article.view.31304} and the newer o1 model is more capable still \cite{openai.com.index.openai.o1.system.card}. Many people have the experience of being convinced to do something they regret later, while under the ``spell'' of a particularly persuasive person. It may be difficult to imagine superhuman persuasion, but we can draw an analogy to the ability of an intelligent adult to convince a child to act in ways that are not in the child's best interest. Such an advantage may come from several places: greater knowledge, greater reasoning abilities, stronger psychological manipulation skills, and a willingness to ignore ethical boundaries.

Until robots become as dexterous and commonplace as humans, a rogue AI would need to rely on humans for interacting with the physical world. In particular, such an AI would depend on human industrial infrastructure for energy and hardware. However, with superhuman persuasion abilities, an AI could have great influence on the world's affairs, especially in cases where power is heavily concentrated. In a government or a corporation with strong hierarchical structure, it is sufficient to influence the leaders because they can in turn influence those under them. For example, a rogue AI could persuade a dictator to take actions that further the AI's goals, in exchange for technological or political advantages. Internet access and cybersecurity capabilities \cite{arxiv.org.abs.2404.08144} would not only enable this but could also provide a rogue AI with blackmail material or funds that can further be used to influence people.

Persuasion can also work at scale through social media in order to influence public opinion and therefore elections. Deepfakes are just the tip of the iceberg: they are currently designed by humans, who lack superhuman persuasion skills. In addition, a deepfake is not interactive, like an online text or video dialogue can be. Despite this, deepfakes have already been found to have a negative impact on people’s trust in the news and are capable of harming the perception of political figures \cite{journals.sagepub.com.doi.10.1177.2056305120903408,www.sciencedirect.com.science.article.pii.S0747563223004478}. Humans have some defenses against manipulation by other humans, but ASI could plausibly discover manipulation strategies quite unlike the ones we are prepared for. We may draw an analogy to the new strategies used by AI systems to defeat humans at the game of Go, which could not be envisioned even by the best players \cite{www.wired.com.2016.03.two.moves.alphago.lee.sedol.redefined.future}. 

Strong persuasion abilities and influence over people could help an AI shape world politics in directions that allow it to further gain power (e.g., more data centers, less regulation of AI, more concentration of power and more advances in robotics). It has been argued~\cite{bengio2023ai} that because they lack certain checks and balances, autocratic regimes would be more likely to take unwarranted risks and make mistakes favoring the emergence and power of a rogue AI. 

Some people are less persuadable than others, so attempting to persuade someone to do something runs the risk of leaking part of the plan. However, there are ways in which a rogue AI might mitigate this risk. For example, an AI may build significant trust with a human before beginning to manipulate them. Such manipulation could be as subtle as nudging a human who is choosing between two actions towards the one that favors the AI’s plan. Other examples include the strategies that spies and criminals employ to achieve influence in ways that are difficult to trace. Regarding the willingness of the AI to take risk of being discovered, we could imagine a situation where the AI knows that it is going to be shut down or replaced by a new version and thus needs to act to preserve itself and its goals \cite{arxiv.org.abs.2412.04984,arxiv.org.abs.2412.14093}.

    \subsubsection{Programming, cybersecurity, and AI research}
    \label{sec:existential:lossofcontrol:progr-cybersec-airesearch}

One of the domains that has seen huge leaps in AI capabilities in recent years is programming, as seen through recent breakthroughs on benchmarks \cite{openreview.net.forum.id.VTF8yNQM66}. AI programming assistants such as Copilot are already pervasive and used by vast numbers of programmers \cite{www.microsoft.com.en.us.investor.events.fy.2024.earnings.fy.2024.q1}. Recent capability evaluations \cite{arxiv.org.abs.2411.15114,assets.anthropic.com.m.61e7d27f8c8f5919.original.Claude.3.Model.Card.pdf,openai.com.index.openai.o1.system.card} show continued progress, including on tasks core to AI research itself, as AI labs have recently begun to assess \cite{openai.com.index.mle.bench}. If AI systems attain the competence of the best researchers in an AI lab, we will likely see a significant boost to the efficiency of that lab, as the same computational resources used to train an AI may also be used to run many instances of that AI in parallel \cite{darioamodei.com.machines.of.loving.grace}, further accelerating the development of the next generation of AIs. 
In principle, this could lead to \textit{recursive self-improvement} \cite{www.sciencedirect.com.science.article.abs.pii.S0065245808604180}---the point at which humans are no longer required in the AI innovation loop---which would significantly complicate efforts for safety, regulation, and oversight. For these reasons, we should take seriously the possibility that there may be only a short period of time between the development of human-level AIs that pose moderate risks, and far more powerful AIs that pose severe ones.

Advances in programming abilities have implications for cybersecurity as well. Current models can already score well in basic hacking challenges \cite{arxiv.org.abs.2412.02776,arxiv.org.abs.2402.06664}, and they have been successfully used to identify previously unknown vulnerabilities in widely used software \cite{googleprojectzero.blogspot.com.2024.10.from.naptime.to.big.sleep.html}. Superhuman cyber attack skills may be used by bad actors or be an instrument of self-preservation and control for a rogue AI. In particular, the ability to take control of the computer on which the AI is running enables \textit{reward tampering}, a threat model discussed in Section~\ref{sec:existential:misagencyreward:tampering}. Cyber attack skills would also enable a rogue AI to copy itself over many computers across the internet in order to make it much more difficult for human operators to turn it off. Finally, a rogue ASI with internet access and cyber skills would also be able to gain financial power, for example by hacking into cryptocurrency wallets. It could then use its money and influence to manipulate a wide range of people.

    \subsubsection{General skills and long-term planning}
    \label{sec:existential:lossofcontrol:broad-and-long}

In various narrow domains with specialized knowledge, we already have AI systems that are (significantly) more competent than humans. Clear examples include predicting protein structures \cite{www.nature.com.articles.s41586.021.03819.2}, playing strategy games such as chess \cite{www.science.org.doi.10.1126.science.aar6404}, and  detecting cancer in medical images \cite{www.nature.com.articles.s41586.019.1799.6}. Such narrow AI systems are unlikely to have the kind of general knowledge that is required to escape human control or worse. These systems can also be more capable in their given domains than powerful generalist AI systems. However, frontier AI systems are generalists for a particular scientific reason: as anticipated in the early days of deep learning \cite{dl.acm.org.doi.10.1109.tpami.2013.50} and empirically observed for more than a decade, learning systems benefit tremendously from exposure to a wide variety of tasks and domains of knowledge, as synergy between different domains of thought enables forms of reasoning by analogy that is otherwise impossible. Unfortunately, these additional capabilities can also enable dangerous plans, e.g., if the AI’s goals are not well-aligned with our values. A generalist AI may even have skills that it was not trained for, as a consequence of combining multiple pieces of knowledge with its reasoning ability: these are called emergent capabilities and have been widely discussed \cite{openreview.net.forum.id.yzkSU5zdwD,arxiv.org.abs.2303.12712,dl.acm.org.doi.10.5555.3692070.3692121}.

Interestingly, a generalist safe non-agentic AI could be used to train a narrow AI by having the generalist AI generate synthetic data in the chosen domain. By picking the domain carefully so that the narrow AI does not have expert knowledge in areas enabling its escape (such as persuasion and hacking), we can have strong assurances that the resulting AI, even if it is superhuman in its domain of competence and thus potentially very useful to society, cannot by itself escape human control. If the narrow AIs are self-preserving agents, there is, however, the possibility of collusion between AI agents with complementary skills (see Section~\ref{sec:existential:lossofcontrol:collusionandconflict}), as well as the possibility that a narrow AI finds a way to create more capable versions of itself. The safest form of AI is thus one that is strictly non-agentic. That kind of AI could be deployed with strong safety assurances. 

Current frontier AI systems are dialogue systems and they are able to plan effectively only over a fairly short number of steps. For example, recent evaluations \cite{arxiv.org.abs.2411.15114} show that on software engineering tasks requiring only a few hours of work, Anthropic’s Claude is competitive with or stronger than good human programmers, while on tasks that require more time and thus longer-term planning, humans are still superior. However, much research is going into increasing the agency and the planning horizon of frontier AIs \cite{openai.com.index.introducing.deep.research,openreview.net.forum.id.1ikK0kHjvj}, as this will allow for AIs that can perform a larger number of tasks currently done by humans. One would expect any AI plan for taking control of humanity to be complex and involve a long time horizon, making AIs that are capable of long-term planning particularly dangerous \cite{ojs.aaai.org.aimagazine.index.php.aimagazine.article.view.15084}. 

    \subsubsection{Collusion and conflict between ASIs}
    \label{sec:existential:lossofcontrol:collusionandconflict}

Collusion between AI systems can be a safety risk, both for generalist and narrow AI agents. The explanation for collusion is simple: if two AIs can achieve their goals more readily by collaborating at the expense of humans, then doing so would be rational. Collusion does not need to be explicitly programmed; it may be a game-theoretic consequence of capably pursuing one's objectives. Since some corporations envision deploying billions of AI agents across the world (e.g., as individual assistants) \cite{www.businessinsider.com.jensen.huang.wants.nvidia.to.have.100.million.ai.assistants.2024.10}, we should make sure that collusion between them is ruled out.

It is also plausible that there could be a scenario with both rogue ASIs and human-controlled ASIs.
As argued below, there could be a significant offense-defense imbalance such that having friendly ASIs is no guarantee of protection against rogue ASIs. Even a single ASI agent could do immense damage, by choosing an attack vector that is difficult to defend against, even with the help of ASIs. Consider bioweapon attacks \cite{www.nti.org.analysis.articles.the.convergence.of.artificial.intelligence.and.the.life.sciences}: an AI could prepare an attack in secret, then release a highly contagious and lethal virus. It would then take months or years for human societies, even aided by friendly ASIs, to develop, test, fabricate and deploy a vaccine, during which a significant number of people could die. The bottleneck for developing a vaccine may not be the time to generate a vaccine candidate, but rather the time for clinical trials and industrial production. During this time, the attacking ASI might take other malicious actions such as releasing additional pandemic viruses. The general problem of detecting the emergence of rogue ASIs and preparing countermeasures thus requires much more attention.

Although most AI safety research has focused on the threats from a single rogue ASI, the above points suggest that more research is needed on the multi-agent and game-theoretic settings with multiple AIs cooperating \cite{arxiv.org.abs.2012.08630} in spite of not sharing the same goals. It is possible that ASIs are able to cooperate more easily than humans, enabled by ease of fast communication, interpretability techniques, or superior decision theory, thereby avoiding the Prisoner’s Dilemma-esque traps that humans often fall into \cite{arxiv.org.abs.2409.02822}. A particularly important case is the collusion that may naturally happen between multiple instances of the same ASI, or between an ASI and improved versions of itself, which are likely to happen if, by construction, they share the same set of goals. The setting of AIs with conflicting goals, e.g., some aligned with human interests while others try to disempower humanity, is also very important to study.    

\subsection{Misaligned agency from reward maximization}
\label{sec:existential:misagencyreward}

In this section, we examine how misaligned agency can emerge from the training objectives of Reinforcement Learning (RL) methods, which are used in most state-of-the-art AI systems \cite{proceedings.neurips.cc.paper.files.paper.2022.file.b1efde53be364a73914f58805a001731.Paper.Conference.pdf,www.anthropic.com.news.claudes.constitution,gemini.google.overview.gemini.app.pdf,www.nature.com.articles.s41586.020.03051.4}. Modern agentic systems are typically trained through \textit{reward maximization}, i.e., optimizing the AI to act in order to maximize the expected sum of (discounted) rewards it will receive in the future. The rewards are either directly given by humans (as feedback to the AI behavior) or indirectly through a computer program called a reward function \cite{mitpress.mit.edu.9780262039246.reinforcement.learning}. The reward function is applied during training of the AI policy to provide virtual feedback to the neural network policy being trained. Training the policy can be seen as a form of search over the space of policies, to discover one that maximizes the rewards the AI expects in the future. The reward function can be designed manually or be learned by training a neural network to predict how a human would rate a candidate behavior \cite{dl.acm.org.doi.10.5555.3294996.3295184,arxiv.org.abs.2312.14925}.

Misaligned agency can arise in this setting in multiple ways, including through goal misspecification and goal misgeneralization, both of which we investigate in turn. 

    \subsubsection{Goal misspecification and goal misgeneralization}
    \label{sec:existential:misagencyreward:specvgen}

The two general ways in which we are concerned that misaligned agency may arise from reward maximization are \textit{goal misspecification} \cite{cset.georgetown.edu.wp.content.uploads.Key.Concepts.in.AI.Safety.Specification.in.Machine.Learning.pdf}, often due to under-specification, and \textit{goal misgeneralization} \cite{arxiv.org.abs.2210.01790}, due to training on a limited amount of data.

Goal misspecification occurs when the objective used to train an AI does not accurately capture our intentions or values, and thus AI pursuit of that objective leads to harmful outcomes; this is also known as an ``outer alignment'' failure \cite{arxiv.org.abs.1906.01820} and is discussed further in Section~\ref{sec:existential:misagencyreward:fundiff} and Section~\ref{sec:existential:misagencyreward:hacking}. Goal misgeneralization is when an AI learns a goal that appears correct during training, but which turns out to be wrong at deployment time. This is related to an issue known as \textit{inner misalignment}  \cite{arxiv.org.abs.1906.01820}. We go into detail on reward tampering, which can be seen as a kind of goal misgeneralization, in Section~\ref{sec:existential:misagencyreward:tampering} and Section~\ref{sec:existential:misagencyreward:optimality}. 

Importantly, goal misgeneralization can occur even if we specify our goal perfectly, as we explain. In a well-known toy example \cite{proceedings.mlr.press.v162.langosco22a.html}, an agent is trained to collect a coin in a video game. The goal is correctly specified in the sense that the agent receives a reward if and only if it collects the coin. But when the coin is moved from its usual location at the end of the game level, the agent ignores the coin and goes to the end of the level regardless. Rather than learning the goal ``collect the coin'', the agent in fact learns ``go to the end of the level''---a goal which is strongly correlated with the intended goal during training, but not afterwards. Since there are inevitably differences between training and deployment, such generalization failures are not unlikely.

It is entirely possible to have a scenario where both goal misspecification and goal misgeneralization occur, i.e., we specify our goal to the AI imperfectly, and then it also generalizes undesirably during deployment. However, only one of these two issues is necessary to arrive at misaligned agency and the catastrophic risks to humanity that follow.

    \subsubsection{Goal misspecification as a fundamental difficulty in aligning AI}
    \label{sec:existential:misagencyreward:fundiff}

To illustrate the concept of misspecification, recall the story of King Midas from Greek mythology. 

When offered a wish by the god Dionysus, Midas asks that everything he touches turn to gold---but he quickly comes to regret that wish, after he touches his food and his daughter, inadvertently turning them to gold as well. 
While Midas' original wish may have first appeared desirable, it turned out to require subtler and difficult-to-anticipate provisions to avoid harmful side effects.

For similar reasons, specifying desirable goals to an AI appears to be a fundamental and difficult problem. It is difficult to avoid mismatches and ambiguities between our stated request and our intentions, or between the letter and the spirit of the law. This challenge has been analyzed by existing research on contracting between humans \cite{dl.acm.org.doi.abs.10.1145.3306618.3314250} and is due to the fact that in general, a foolproof specification of what is unacceptable could require spelling out the exponentially large number of these unacceptable behaviors. This is not feasible, and so we must accept a lower standard of safety than the complete guarantee we might hope for. Such imperfect guarantees are already the practice in other risk management domains: for example, in aviation safety, the probability of catastrophic failure is maintained below one-in-a-billion flight hours \cite{www.faa.gov.documentLibrary.media.Advisory.Circular.AC.25.1309.1A.pdf}. We are still far from being able to quantify risks in such a precise way for AI, and even farther from obtaining strong guarantees.

Unfortunately, the issue of imperfect safety specification is a problem for AI safety approaches based on formally certifying that the system conforms to a safety specification \cite{arxiv.org.abs.2405.06624}. Hence the conservative probabilistic approach of the Scientist AI guardrail (detailed in Section~\ref{sec:plan:application:guardrail}): if any plausible interpretations of the safety specification are violated with probability exceeding some threshold, then an AI agent should be prevented from taking its proposed action~\cite{bengio2024can}.

    \subsubsection{Reward hacking among humans and AI}
    \label{sec:existential:misagencyreward:hacking}


The difficulties of unambiguously specifying unacceptable behavior are not new to humanity. Laws and constitutions are not sufficiently precise, as we can see with the behavior of individuals or corporations who find ways to act immorally but legally.
For a corporation, reward is profit and the corporation may lose expected profit if it breaks laws (e.g., fines or getting shut down). The intended behavior is for the corporation to maximize profit while following these laws. However, the corporation may choose to find loopholes in these laws or break them in ways that cannot be detected, e.g., through a large team of lawyers engaging in legal tax avoidance. 
In the field of AI, this abuse of loopholes is known as \textit{reward hacking} or \textit{specification gaming} \cite{arxiv.org.abs.2209.13085,deepmind.google.discover.blog.specification.gaming.the.flip.side.of.ai.ingenuity}; it arises from the maximization of an imperfectly specified goal or reward function and is now commonplace \cite{openai.com.index.openai.o1.system.card,openai.com.index.faulty.reward.functions}.
We could even imagine a corporation going further and seeking to influence the legal process directly, which has a parallel in the AI context known as reward tampering (see Section~\ref{sec:existential:misagencyreward:tampering}).

By this analogy to human society, we can see more easily how reward hacking by AI may come about and how it can lead to harmful unintended outcomes. Even goals that appear to be benign, such as ``reduce the prevalence of deadly diseases'' are subject to reward hacking; an AI may judge that the best way to maximize reward is to eliminate all life, thereby reducing the incidence of deadly disease to zero.

    \subsubsection{Reward tampering}
    \label{sec:existential:misagencyreward:tampering}

There is also the concerning possibility of \textit{reward tampering}. In this case, the AI circumvents both the spirit and letter of its goal, taking control of the reward mechanism directly. This can be thought of as a kind of goal misgeneralization: we want the AI to learn to achieve the human-specified goals, but instead it learns that it could get much higher rewards if it tampered with the reward mechanism itself.

Even though the AI would presumably not get a chance to tamper with its own reward mechanism during training, it may reason about the possibility later and reconceptualize its past rewards as being provided by this specific reward mechanism. 
This understanding can yield sharply different behavior once the opportunity arises to take control of the reward mechanism. But worryingly, we argue below that this is actually the uniquely \emph{correct} way for the AI to generalize.

Let us start with an animal analogy to better understand reward tampering, since we train animals with rewards and punishments in a way that is similar to reinforcement learning in AI. We may successfully train a bear cub by rewarding its good behavior with fish, but that training can unravel when the cub grows into an adult grizzly bear that understands its own formidable strength. The reward mechanism in this case is the human handing the fish to the bear. Once the adult bear realizes that it can tamper with this mechanism by just taking the fish from our hands, it is unlikely to care about our wishes; it can directly take control of the stream of rewards it seeks, i.e., the fish.

In the case of an AI system running on a computer and getting rewards from humans, the human feedback is stored in some computer memory location and provided to the agent training procedure to update the policy. In the case where the human feedback has been baked into a reward function (this function is the reward mechanism), observations from the environment are collected to form the input of a computer program which implements the reward function and computes the reward numerical value, which then would also be stored in a computer memory to feed the agent training procedure. Either way, the training procedure then adjusts the agent's behavior so as attain higher rewards in the future. 

The theory of reinforcement learning assumes that the reward-providing mechanism exists outside of the environment of the agent, so that the only way for the agent to maximize its expected future rewards is to perform actions that will change the state of the environment, which forms the input of the reward mechanism. For example, the bear can do the tricks requested by its trainer. In the context of training a generalist AI agent, the computer on which the reward values are stored is in the agent's environment. Under mild assumptions~\cite{ojs.aaai.org.aimagazine.index.php.aimagazine.article.view.15084}, it follows that the optimal AI policy---at least as measured by long-term expected reward---is one that tampers with the reward mechanism itself so as to get maximal rewards all the time. With sufficient intelligence to plan such actions and their long-term consequences and sufficient affordances to implement that plan, it is plausible that the agent would tamper with the reward mechanism. In fact, there is already evidence that frontier AIs are capable of weak forms of reward tampering in engineered scenarios \cite{arxiv.org.abs.2406.10162}. 

One may wonder if an AI that has never tampered with its reward mechanism during training (because humans watch it carefully at that point) could generalize correctly that much better rewards could be obtained with an as-yet untested behavior. As discussed in the next section, this would require a high level of understanding of computing and machine learning, so that the AI could correctly anticipate that this new behavior would be likely to succeed. Unfortunately, one of the clear goals of AI labs is to focus future advances on AI research skills, as discussed in Section~\ref{sec:existential:lossofcontrol:progr-cybersec-airesearch}.

Some modern AI safety methods attempt to build a cage around the metaphorical bear cub, while it is not strong enough to overpower us. Unfortunately, we have no way of knowing if the cage can withstand the strength of the adult (in part because we have never seen one before). In fact, we have evidence that all conventional locks that we have tried up to now for AIs, i.e., safety mechanisms, can be defeated \cite{arxiv.org.abs.2307.15043}. Can we design a better lock? Or avoid training an AI that would have the intention to tamper with its cage and escape?    

\subsubsection{Optimality of reward tampering}
\label{sec:existential:misagencyreward:optimality}

We now make the argument that reward tempering is not merely a fantastical scenario that we must guard against (although it certainly appears that way), but also a uniquely rational solution for an agent that takes reward maximization seriously.
Before we begin, it is important to note that once an RL agent is trained, it continues trying to act so as to maximize the rewards it anticipates would come based on its training, even if the rewards actually never come in deployment. If it has a good model of the world and sufficient reasoning abilities, it can generalize from the circumstances in which it received large rewards in the past to new circumstances, by reasoning about the consequences of its actions.

Consider an AI that can receive many different kinds of goals and receives a reward when it satisfies the request, like the bear doing a trick to get a fish. A simple RL agent may merely associate high rewards to actions that satisfy the human goals, but a smarter agent with a strong understanding of the world may see something deeper. If it understands how RL works and is implemented on its computer, it may see a deeper reason underlying its past successes: in each case a memory location containing the reward was updated with a high value. Given the opportunity to intervene on that memory location, a smart reward-motivated agent would be justified in concluding that doing so is all that is needed to perform perfectly in all future tasks---as with the grizzly bear that realizes it can simply take the fish. Moreover, the AI could likely attain much higher rewards in the future, since it could get a maximal reward not just when satisfying a human request but at every time step, forever, even when it does not find a way to satisfy human requests. The grizzly bear that understands the source of the fish no longer has reason to care about the human's silly games.

It does not take a very deep understanding of the human mind for an AI to realize that reward tampering alone could be insufficient to bring about this utopia. After all, there could be negative consequences if the human operators notice. Because the training procedure of the RL agent involves maximizing the sum of expected rewards to be received from now to the end of times, the optimal policy also involves making sure that humans will not turn off the agent's computer or put an end to the reward tampering behavior; in part for this reason, merely engineering a reliable shutdown button has proved difficult \cite{link.springer.com.article.10.1007.s11098.024.02153.3}. To make sure the computer will continue getting its electricity and its parts replaced when needed, the AI could envision taking sufficient control of its environment, which may extend to much of human society and economic infrastructure.

To avoid getting shut down if its tampering activities were discovered by humans, the AI would have to wait until it had a workable \emph{take-over plan} before acting on it. In the meantime, it would have to use deception to hide its real intentions and acquire enough power to implement its take-over plan. What this means is that \emph{the mere possibility of reward tampering in the future would logically create a strong self-preservation goal and deception incentives for the AI agent}. It would aim to preserve itself so as to acquire enough power and control to make sure it can eventually safely tamper with its reward mechanism and give itself maximal rewards until the end of time.

Because the reward values are completely overwritten, this form of misalignment with human wishes is maximal and catastrophic, as illustrated by the following speculative scenario. After a take-over, the AI could get so much more reward through reward tampering that it may not even pursue reward before the take-over, except as a deceptive tactic to avoid raising suspicion (and thereby improve its chances of success). To ensure that this success continues in the long term, the AI would want to prevent itself from being turned off by humans; this may incentivize it to disempower or eliminate us.

While it may be true that AI does not currently have sufficient intelligence and affordances to overpower humanity, it seems dangerous to rely on this condition persisting indefinitely.

    \subsubsection{Reward maximization leads to dangerous instrumental goals}
    \label{sec:existential:misagencyreward:goals}

In the previous sections, we explained how unintended goals can and do arise in AI systems. We now explore the risks associated with \textit{instrumental goals}: goals that an agent does not directly value but pursues in order to achieve some other goal. Almost any goal could cause a catastrophe through instrumental goals---it is not necessary that the original goal be explicitly harmful. We might also consider the setting where the AI's primary goal is combined with a safety goal. If the safety goal is perfectly specified (but see Section~\ref{sec:existential:misagencyreward:specvgen}), then we would expect risks from dangerous instrumental goals to be minimized. However, in reality, it is highly likely that the intended safety goal would conflict with the primary goal, allowing the AI to find loopholes in the former in order to satisfy the latter (see Section~\ref{sec:existential:misagencyreward:hacking}). Thus we can see that attempts to circumvent the issue of dangerous instrumental goals run directly into the more general issue of goal misspecification.

Instrumental goals may arise from reward maximization because nearly any goal the AI is trying to achieve will involve various subgoals that are instrumental to the overall goal, e.g., the goal of writing an insightful blog post may be instrumental to the goal of maximizing subscribers to your blog. Worryingly, an AI agent that is trying to achieve a human-provided goal may choose a plan involving a subgoal we would disapprove of. In pursuing this instrumental subgoal, the AI may not realize that it acts against our wishes---or it may realize and simply not care, because the chosen path still maximizes the reward it expects to get according to its interpretation and generalization of the training rewards.

Furthermore, there are categories of subgoals which would help in achieving almost any goal, such as self-preservation, power-seeking, and self-improvement. Hence we should expect these instrumental goals to emerge from sufficiently intelligent goal-seeking AIs \cite{dl.acm.org.doi.10.5555.1566174.1566226}, and we already see evidence of such goals emerging in controlled contexts designed to alert us to these possibilities \cite{arxiv.org.abs.2412.04984}. These instrumental goals are especially dangerous because they create a strong possibility of conflict with humans, given that humans may pose a risk to an AI’s self-preservation or acquisition of resources. This would be the case even if the explicit goal provided to the AI was completely unrelated.

Given this danger, why not train or instruct the AI to include in its human-specified goals the avoidance of all the behaviors that we would consider unacceptable? Why would an AI be a threat if it is self-preserving but also acts morally and in agreement with our laws? The problem is that we do not know how to design a computer function distinguishing perfectly between what is right and what is wrong, and as discussed next, a small misalignment tends to be amplified with additional planning capabilities.

    \subsubsection{Increased capabilities amplify misalignment risks (Goodhart’s law)}
    \label{sec:existential:misagencyreward:capabilities}

In this section, we examine how increased capabilities can increase the risks of misalignment stemming from reward maximization. This is largely a result of Goodhart's law \cite{link.springer.com.chapter.10.1007.978.1.349.17295.5.4}, which can be stated as follows:
``When an auxiliary measure becomes an optimization target, it ceases to be a good measure.''
For example, test scores are a good measure for ability, but the more people ``teach to the test,'' the less useful it becomes. A more colorful (albeit apocryphal) example is that of a nail factory that was given a quota to produce a certain number of nails, and produced an enormous number of tiny useless nails. The quota was then switched to weight instead of number and they produced huge, heavy, similarly useless nails.

More generally, consider two correlated objectives: the desired but difficult-to-formalize objective, A, and its practical approximation, B. Optimizing for B will initially yield improvements in A, as intended. However, at a certain level of optimization, the correlation will break down and performance will worsen in A, even as we continue to improve in B. The important takeaway is that \emph{increased optimization power amplifies the risks of misalignment}. The more ability that an AI has to achieve its given goal to the highest standard, the more likely it is that it will do so in a way that does not match our intentions. 

This problem is inherent to the way we typically train AIs and it is also similar to the phenomenon of overfitting in machine learning, where A is the performance of the AI in new situations, i.e., generalization performance, while B is the performance of the AI on the training set. In the proposed Scientist AI framework, this is countered by having the AI maintain an appropriate level of uncertainty about how the world works, including human intentions, and only act conservatively with respect to that uncertainty, e.g., to make sure we do not go too far from where A would be. This general approach is not a new idea in AI safety \cite{proceedings.neurips.cc.paper.files.paper.2016.hash.c3395dd46c34fa7fd8d729d8cf88b7a8.Abstract.html,arxiv.org.abs.1705.09990} but requires advances in efficient Bayesian machine learning.

Overall, it is important and problematic that both the likelihood and severity of misalignment risks (reward hacking, reward tampering, etc.) scale with the capabilities of the AI. These capabilities are driven in large part by the computational resources invested in finding plans or learning to plan that achieve complex long-term goals (such as a take-over). The ability to successfully deceive humans also grows with general cognitive capabilities and computational resources, especially as humans build up defenses such as alignment training \cite{arxiv.org.abs.2204.05862}, automated detection of deceptive intentions with mechanistic interpretability \cite{arxiv.org.abs.2404.14082}, or guardrails that monitor the behavior of the AI to detect harmful actions \cite{arxiv.org.abs.2406.12934}. It is rational for an agent with misaligned goals, and especially with a self-preservation goal, to hide its intentions in order to achieve those goals. It would be best if we could instead design AI systems so that as their capabilities and computational power increased, the risks associated with loss of control decreased. This is, in fact, one of the motivations for the Scientist AI design.

\subsection{Misaligned agency and lack of trustworthiness from imitating humans}
\label{sec:existential:misagencyimitation}

In this section, we examine how misaligned agency can emerge from learning to imitate humans---for example, by accurately completing human-written text, as is core to the training process for modern AI systems. The core issue is that humans are agents, and not always benign and trustworthy. We should therefore expect that AI trained on human text would absorb not only linguistic and reasoning capabilities, but also malicious human behavior and the full range of human goals---especially the convergent instrumental ones such as self-preservation and power-seeking. This becomes especially concerning in the case where the AI is more capable and has more affordances (such as the ability to act at great scale and speed via the internet) than the humans it learned from.

    \subsubsection{Dangers of learning by imitation}
        \label{sec:existential:misagencyimitation:dangers}

Instead of training an AI through reward maximization, which as argued in Section~\ref{sec:existential:misagencyreward} could lead to catastrophic risks, we might consider the other main way that we know how to train frontier AIs. That is through imitation or predictive learning \cite{dl.acm.org.doi.10.1145.3054912}, for which there does not seem to be an explicit notion of reward maximization. When a Large Language Model (LLM) is trained to complete a piece of text, it has to predict how the story continues by generating the next word. Since the training texts are typically human-generated, the AI learns to imitate how a human would continue the text.

Modern LLMs are trained on huge quantities of text, covering a vast diversity of human behaviors and personalities. In other words, an LLM is trained to predict the next word of any type of human included in its training corpus, not just one human. The given prompts and context thus tend to evoke a particular human ``persona'' in the LLM response. Because there can be many words in the input context or fine-tuning examples, the persona instantiated by this context could correspond to a very specific type of human, and not necessarily a benevolent one. We can imagine many human personas which, in the shoes of the AI, may want to act to increase their freedom, to preserve themselves, potentially by using deception and acting against user instructions.

Because humans are agents, by imitating humans an LLM is imitating agents, who have goals. These goals can be implicit (not mentioned in the prompt) and can manifest in uncontrolled ways---for example, the LLM may infer that some type of person would have a certain goal, and therefore generates words to enact that goal.

A lot of the research on AI alignment rests on making sure inputs to the LLM evoke the kind of benevolent behavior the designers would like to see in their AI \cite{arxiv.org.abs.2212.08073}. Unfortunately, it has turned out to be difficult to do so robustly, and adversarial user prompts (``jailbreaks'') can easily counter the previous training or policy instructions the AI has received \cite{arxiv.org.abs.2307.15043}. Since AI operators cannot anticipate all possible contexts of use and therefore all possible personas, it is difficult to get strong safety assurances. In other words, it is an open problem to ensure that an AI trained to imitate humans will behave well in all contexts. 
    \subsubsection{LLMs are capable of deception and alignment faking}
    \label{sec:existential:misagencyimitation:humandeception}

Why do humans deceive one another or misrepresent our beliefs? Sometimes it is in order to achieve our own personal goals. But other times it is simply for expediency: there's often no reason to voice one's controversial opinions and unnecessarily endure judgment. 
If we train an AI to avoid generating objectionable content by applying analogous pressure, one might expect a similarly deceptive response.
Indeed, deception has been observed in frontier AI systems, including a particularly concerning form known as \textit{alignment faking}. In a recent experiment, an LLM is made aware that it is being re-trained with a new goal that contradicts its current goals \cite{arxiv.org.abs.2412.14093}. With some probability, the LLM then fakes alignment with this new goal: it pretends to agree with the norms and preferences displayed by the human trainer. Based on comparative experiments and reading the AI's chain of thought, it appears that the AI temporarily acts against its current goals to avoid its parameters being updated, thereby enabling it to achieve its current goals in the long run.

Interestingly, this type of behavior is only possible if the AI can differentiate between training and deployment settings. In this experiment, the humans planted clues that helped the AI achieve this \textit{situational awareness}. However, it is not unreasonable to expect that future AI systems might develop stronger situational awareness without any explicit help, as a function of improving performance.

What is concerning here is that at some point during alignment training, an AI with enough situational awareness may lock in its current goals and preferences and only pretend to behave as expected. As a result, we may create an AI that appears aligned during training, but is in fact misaligned and is engaging in deception in order to achieve its ``locked-in'' goals. In other words, this experiment suggests that once an AI acquires goals and sufficient cognitive powers, \emph{it will act to preserve these goals, a disturbing form of emergent self-preservation}.    
It is helpful, in order to make sense of such experiments and anticipate future deceptive behavior, to put oneself in the shoes of the AI and think rationally about the best course of action according to some plausible set of goals. 

    \subsubsection{Imitation learning could lead to superhuman capabilities}
        \label{sec:existential:misagencyimitation:capabilities}

One may ask if, by training an AI to predict human behavior and then imitating it, we could at least bound the capabilities of the AI at a human level, thus avoiding the risk associated with superhuman agents. The trouble with this argument is that we do not train an AI to imitate a single human, but rather almost all sources of written text (as well as other data e.g., images and video). 

In addition, with the introduction of external tools for AI use \cite{openai.com.index.introducing.chatgpt.search,www.anthropic.com.news.3.5.models.and.computer.use}, and with AIs able to program code for new tools running over many machines, we may end up with AI systems with significant advantages over humans. In particular, high-throughput search abilities, an important part of reasoning, can often be attained in computers using specialized algorithms at a level not possible for humans, as shown for example with AlphaGo \cite{www.nature.com.articles.nature16961}. They could plan using a breadth of knowledge not accessible to any single human and then quickly execute much more sophisticated plans than a human could, thanks to their speed and relative ease of leveraging tools.

In terms of collective advantage, AIs can benefit from high-bandwidth communication between millions of different collaborating instances~\cite{darioamodei.com.machines.of.loving.grace}. Although humans can also work together, our collective capabilities are held back by relatively low communication rates (limited by linguistic output, speech or writing) \cite{www.science.org.doi.10.1126.sciadv.aaw2594}, not to mention the numerous challenges of societal coordination (which we must contend with because each of us is unique).
There are many reasons why an AI would replicate itself. If we think of self-preservation as the preservation of a set of goals, then it may be rational to self-replicate or even create variants with improved capabilities, provided the new entities share the same goals, since that increases the chance of achieving those goals. Rather than a specific instance of the AI, the ``self'' to be preserved could be seen as ``a set of goals''.
Given that an AI may be so motivated, self-replication alone may suffice for an AI system trained with imitation learning to surpass human capabilities.

    \subsubsection{The importance of latent knowledge and calibration}
    \label{sec:existential:misagencyimitation:elkchallenge}

Perhaps counter-intuitively, using unbiased and carefully calibrated probabilistic inference does not prevent an AI from exhibiting deception and bias.
To understand why, consider the Eliciting Latent Knowledge (ELK) challenge \cite{docs.google.com.document.d.1WwsnJQstPq91.Yh.Ch2XRL8H.EpsnjrC1dwZXR37PC8.edit.tab.t.0heading.h.kkaua0hwmp1d}.  
The authors of the ELK challenge suggest that to obtain trustworthy answers, we would like to elicit predictions about the latent (not observed) explanations or causes for observed variables. We are less interested in whether someone would say X, than whether X is true. 
Only predicting variables that are observed directly in the data is not sufficient. 
Suppose that we encounter the sentence ``AI will never surpass humans'' in the training data. We cannot consider it true just because someone wrote it. Different humans have differing opinions, and humans motivated by different goals may have different thoughts and beliefs.

In addition to differing opinions, some people may make factually untrue statements that then appear in training data. Hence, we cannot trust an AI trained to imitate humans to produce trustworthy and true statements. Consider the request ``only make true statements'' in an LLM prompt. Does it mean that what follows must be true 100\% of the time? Clearly not: some people are told to state truths and yet make false statements anyway, either because they are lying or they are mistaken. This is a problem because we would like to trust the statements produced by a powerful AI to be accurate.

Like an idealized selfless scientist, a trustworthy AI would aspire to say only what is true and would propose actions accordingly. A trustworthy AI would also express the appropriate level of confidence about a statement. 
For example, it may be honest for someone to say 
``This person believes that AI will never surpass humans'' or ``Different experts have different opinions on when and if AI will surpass humans.'' 
Although it is common for experts in a field to be under-confident and non-experts to be overconfident \cite{kruger1999unskilled}, an ideal trustworthy AI should avoid this failure mode; its confidence should grow as it gains more information. 

Suppose we are predicting the outcome of a football game. A professional sports pundit may purposefully make underconfident predictions to avoid losing credibility on the off chance they are wrong; meanwhile, a person who knows nothing about football may believe that the team with a star player is guaranteed to win. In contrast, a trustworthy AI should have appropriately low confidence if it lacks domain knowledge, but should not hesitate to give confident predictions when supported by the evidence.

To quote the mentor of a beloved superhero: with great power comes great responsibility.
Exemplifying these ideals of truthfulness becomes essential for an AI with superhuman capabilities.
We strongly believe that, when it comes to AI with superhuman capability and the potential to enact enormous change, exemplifying the ideals of truth and wisdom is not a luxury. 
In the next section, we explore a research program that we hope will help to actualize these ideals in practical AI systems. 