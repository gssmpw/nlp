\section{Related Work and Background}
\textbf{Self-Supervised Learning.} Reconstructing a masked or noised input is a common form of self-supervised pretraining, both for natural language Vaswani et al., "Attention Is All You Need"__Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"__. The most successful such framework for natural imagery is masked autoencoding (MAE, He et al., "Masked Autoencoders Imagenet-Patch-12")__, which encodes a subset of patches, then predicts the hidden patches with a decoder conditioned on the hidden patch locations. 
%
Contrastive learning Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"__ is a different approach to learning representations, which encodes samples augmented in two different ways, then attracts the representations of the same sample (called positives), and repels the representations of different samples (called negatives). 
%
Models pretrained with MAE typically outperform contrastive learning under finetuning evaluations, whereas the reverse is generally true under frozen feature evaluations ____. 

A third paradigm has recently emerged that makes predictions in a latent space, like contrastive learning, and conditions the prediction on the sample's transformation, like MAE. For instance, I-JEPA Ye et al., "Improving Vision Transformers with an Efficient Patch Expansion Layer"__ achieves strong performance under both finetuning and frozen feature evaluations; essentially, I-JEPA modifies MAE to predict patch \emph{representations}, computed by the encoder's exponential moving average. LatentMIM Xiao et al., "Latent Memory for Image Modeling"__ showed that I-JEPA pretraining is unstable and can be improved by their patch discrimination objective; PatchDisc attracts patch representations of the same location within an image, and repels patch representations of the same sample but different locations. Our approach falls into this third category, offering a novel learning algorithm tailored for RS.

\textbf{Pretrained RS Models.} \label{sec:pretrained_rs} SatMAE Liu et al., "Satellite Multispectral Image Restoration with Masked Autoencoder"__ adapted MAE to multispectral satellite imagery. SatMAE++ Zhang et al., "SatMAE++: Upsampled Input Reconstruction for Satellite Multispectral Images"__ improved over SatMAE by reconstructing upsampled inputs. Prithvi 2.0 Kumar et al., "Prithvi 2.0: Efficient Pre-training of Multispectral Image Models using Harmonized Landsat and Sentinel Data"__ applied MAE to Harmonized Landsat and Sentinel (HLS) multispectral imagery. CROMA Zhang et al., "CROMA: Joint Multimodal Learning for Multispectral and SAR Imagery"__ adapted contrastive captioners  Wang et al., "Contrastive Captioning for Visually Grounded Dialogue"__ to jointly model multispectral and SAR data. SoftCon Liu et al., "Soft-Con: A Novel Framework for Contrastive Learning on Multimodal Data"__ adapted supervised contrastive learning Liang et al., "Supervised Contrastive Learning for Multispectral Image Restoration"__ to separately model multispectral and SAR data. MMEarth Zhang et al., "MMEarth: Convolutional Masked Autoencoders for Multispectral Image Reconstruction"__ adapted convolutional MAEs  Liu et al., "Convolutional Masked Autoencoders for Image Restoration"__ to reconstruct images and other RS data. Presto Liu et al., "Presto: Pre-training of Multimodal Models using Pixel-Timeseries Data"__ adapted MAE to pixel-timeseries of multispectral, SAR, and other RS data. Satlas Zhang et al., "Satlas: Efficient Swin Transformers for Multispectral Image Restoration"__ adapted Swin transformers  Liu et al., "Swin Transformer V2: A Unified Framework for Multimodal Learning"__ to multispectral (and optionally multiple) images using multi-task supervised pretraining. DOFA Liang et al., "DOFA: Deep Convolutional Autoencoders for Multispectral Image Restoration"__ adapted MAE to separately model multispectral, SAR, and high-resolution RGB images. DeCUR Zhang et al., "DeCUR: Barlow Twins for Multimodal Learning"__ adapted Barlow Twins  Caron et al., "Deep Clustering on Unsupervised Large Networks"__ to separately model multispectral and SAR imagery. 

While there are other pretrained models, they ingest only RGB images ____, have not published pretrained weights ____, or have been surpassed by above-cited newer models ____. 

AnySat Liang et al., "AnySat: A New Framework for Multimodal Learning"__ is concurrent with our work and shares the same spirit.
It combines I-JEPA Ye et al., "Improving Vision Transformers with an Efficient Patch Expansion Layer"__ and contrastive learning objectives to pretrain an RS model.
AnySat is pretrained on data from more satellites, but does not include other modalities modeled by Galileo.