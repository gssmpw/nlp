\section{Related Work and Background}
\textbf{Self-Supervised Learning.} Reconstructing a masked or noised input is a common form of self-supervised pretraining, both for natural language \cite{devlin2018bert, radfordimproving, skipgram} and natural imagery \cite{xie2022simmim, he2022masked, vincent2008extracting}. The most successful such framework for natural imagery is masked autoencoding (MAE, \citet{he2022masked}), which encodes a subset of patches, then predicts the hidden patches with a decoder conditioned on the hidden patch locations. 
%
Contrastive learning \cite{le2020contrastive, oord2018representation, chen2020simple, chopra2005learning} is a different approach to learning representations, which encodes samples augmented in two different ways, then attracts the representations of the same sample (called positives), and repels the representations of different samples (called negatives). 
%
Models pretrained with MAE typically outperform contrastive learning under finetuning evaluations, whereas the reverse is generally true under frozen feature evaluations \cite{park2023what, he2022masked, garrido2024learning}. 

A third paradigm has recently emerged that makes predictions in a latent space, like contrastive learning, and conditions the prediction on the sample's transformation, like MAE. For instance, I-JEPA \cite{assran2023self} achieves strong performance under both finetuning and frozen feature evaluations; essentially, I-JEPA modifies MAE to predict patch \emph{representations}, computed by the encoder's exponential moving average. LatentMIM \cite{wei2024towards} showed that I-JEPA pretraining is unstable and can be improved by their patch discrimination objective; PatchDisc attracts patch representations of the same location within an image, and repels patch representations of the same sample but different locations. Our approach falls into this third category, offering a novel learning algorithm tailored for RS.

\textbf{Pretrained RS Models.} \label{sec:pretrained_rs} SatMAE \cite{cong2022satmae} adapted MAE to multispectral satellite imagery. SatMAE++ \cite{noman2024rethinking} improved over SatMAE by reconstructing upsampled inputs. Prithvi 2.0 \cite{szwarcman2024prithvi} applied MAE to Harmonized Landsat and Sentinel (HLS) multispectral imagery. CROMA \cite{fuller2024croma} adapted contrastive captioners \cite{yu2022coca} to jointly model multispectral and SAR data. SoftCon \cite{wang2024multi} adapted supervised contrastive learning \cite{khosla2020supervised, zhang2022use} to separately model multispectral and SAR data. MMEarth \cite{nedungadi2024mmearth} adapted convolutional MAEs \cite{woo2023convnext} to reconstruct images and other RS data. Presto \cite{tseng2023lightweight} adapted MAE to pixel-timeseries of multispectral, SAR, and other RS data. Satlas \cite{bastani2023satlaspretrain} adapted Swin transformers \cite{liu2021swin} to multispectral (and optionally multiple) images using multi-task supervised pretraining. DOFA \cite{xiong2024neural} adapted MAE to separately model multispectral, SAR, and high-resolution RGB images. DeCUR \cite{wang2024decoupling} adapted Barlow Twins \cite{zbontar2021barlow} to separately model multispectral and SAR imagery. 

While there are other pretrained models, they ingest only RGB images \cite{reed2023scale, tang2024cross, mendieta2023towards}, have not published pretrained weights \cite{guo2024skysense, han2024bridging}, or have been surpassed by above-cited newer models \cite{fuller2022satvit, wang2023ssl4eo, ayush2021geography, manas2021seasonal, jean2019tile2vec, astruc2024omnisat, jakubik2023foundation}. 

AnySat \cite{astruc2024anysat} is concurrent with our work and shares the same spirit.
It combines I-JEPA \cite{assran2023self} and contrastive learning objectives to pretrain an RS model.
AnySat is pretrained on data from more satellites, but does not include other modalities modeled by Galileo.