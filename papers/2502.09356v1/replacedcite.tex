\section{Related Work and Background}
\textbf{Self-Supervised Learning.} Reconstructing a masked or noised input is a common form of self-supervised pretraining, both for natural language ____ and natural imagery ____. The most successful such framework for natural imagery is masked autoencoding (MAE, ____), which encodes a subset of patches, then predicts the hidden patches with a decoder conditioned on the hidden patch locations. 
%
Contrastive learning ____ is a different approach to learning representations, which encodes samples augmented in two different ways, then attracts the representations of the same sample (called positives), and repels the representations of different samples (called negatives). 
%
Models pretrained with MAE typically outperform contrastive learning under finetuning evaluations, whereas the reverse is generally true under frozen feature evaluations ____. 

A third paradigm has recently emerged that makes predictions in a latent space, like contrastive learning, and conditions the prediction on the sample's transformation, like MAE. For instance, I-JEPA ____ achieves strong performance under both finetuning and frozen feature evaluations; essentially, I-JEPA modifies MAE to predict patch \emph{representations}, computed by the encoder's exponential moving average. LatentMIM ____ showed that I-JEPA pretraining is unstable and can be improved by their patch discrimination objective; PatchDisc attracts patch representations of the same location within an image, and repels patch representations of the same sample but different locations. Our approach falls into this third category, offering a novel learning algorithm tailored for RS.

\textbf{Pretrained RS Models.} \label{sec:pretrained_rs} SatMAE ____ adapted MAE to multispectral satellite imagery. SatMAE++ ____ improved over SatMAE by reconstructing upsampled inputs. Prithvi 2.0 ____ applied MAE to Harmonized Landsat and Sentinel (HLS) multispectral imagery. CROMA ____ adapted contrastive captioners ____ to jointly model multispectral and SAR data. SoftCon ____ adapted supervised contrastive learning ____ to separately model multispectral and SAR data. MMEarth ____ adapted convolutional MAEs ____ to reconstruct images and other RS data. Presto ____ adapted MAE to pixel-timeseries of multispectral, SAR, and other RS data. Satlas ____ adapted Swin transformers ____ to multispectral (and optionally multiple) images using multi-task supervised pretraining. DOFA ____ adapted MAE to separately model multispectral, SAR, and high-resolution RGB images. DeCUR ____ adapted Barlow Twins ____ to separately model multispectral and SAR imagery. 

While there are other pretrained models, they ingest only RGB images ____, have not published pretrained weights ____, or have been surpassed by above-cited newer models ____. 

AnySat ____ is concurrent with our work and shares the same spirit.
It combines I-JEPA ____ and contrastive learning objectives to pretrain an RS model.
AnySat is pretrained on data from more satellites, but does not include other modalities modeled by Galileo.