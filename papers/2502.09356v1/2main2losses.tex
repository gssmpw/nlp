\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage[table]{xcolor}         % colors
\usepackage{tikz}
\usepackage{fp}
\usepackage{colortbl}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{mathrsfs}
\usepackage{ifthen}
\usetikzlibrary{patterns.meta}
\usepackage{float}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{colortbl}
\usepackage{commath}
\usepackage[group-separator={,},group-minimum-digits=4]{siunitx}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{multirow}
\usepackage{dingbat}

% \definecolor{ourcolor}{HTML}{4DB269}
\definecolor{ourcolor}{HTML}{f542dd}
\definecolor{hannahscolor}{HTML}{f542dd}
\definecolor{secondcolor}{HTML}{4283F5}
\definecolor{thirdcolor}{HTML}{F5B442}
\colorlet{lightour}{ourcolor!20}
\colorlet{lighthannah}{hannahscolor!20}
\colorlet{verylighthannah}{hannahscolor!5}

\newcommand{\circlednum}[2][black]{\strut\raisebox{-1pt}{\tikz{\node[circle,inner sep=0.6pt,fill=#1,font=\scriptsize\bfseries\color{white}]{#2};}}}
\newcommand\SmallCaption[1]{%
  \captionsetup{font=scriptsize}%
  \caption{#1}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Galileo: Learning Global and Local Features in Pretrained Remote Sensing Models}

\begin{document}

\twocolumn[
\icmltitle{Learning Global and Local Features in Pretrained Remote Sensing Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{equal2}{â€ }

\begin{icmlauthorlist}
\icmlauthor{Gabriel Tseng}{equal,mila,mcgill,ai2}
\icmlauthor{Anthony Fuller}{equal,carleton}
\icmlauthor{Marlena Reil}{mila,mcgill}
\icmlauthor{Henry Herzog}{ai2}
\icmlauthor{Patrick Beukema}{ai2}
\icmlauthor{Favyen Bastani}{ai2}
\icmlauthor{James R. Green}{carleton}
\icmlauthor{Evan Shelhamer}{ubc}
\icmlauthor{Hannah Kerner}{equal2,asu}
\icmlauthor{David Rolnick}{equal2,mila,mcgill}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{mila}{Mila -- Quebec AI Institute}
\icmlaffiliation{mcgill}{McGill University}
\icmlaffiliation{asu}{Arizona State University}
\icmlaffiliation{carleton}{Carleton University}
\icmlaffiliation{ai2}{Allen Institute for AI (Ai2)}
\icmlaffiliation{ubc}{University of British Columbia}

\icmlcorrespondingauthor{Gabriel Tseng}{gabriel.tseng@mail.mcgill.ca}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution \icmlEqualSupervision} % otherwise use the standard text.

\begin{abstract}
From crop mapping to flood detection, machine learning in remote sensing has a wide range of societally beneficial applications. The commonalities between remote sensing data in these applications present an opportunity for pretrained machine learning models tailored to remote sensing to reduce the labeled data and effort required to solve individual tasks. However, such models must be: (i) flexible enough to ingest input data of varying sensor modalities and shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to model Earth surface phenomena of varying scales and types. To solve this gap, we present Galileo, a family of pretrained remote sensing models designed to flexibly process multimodal remote sensing data. We also introduce a novel and highly effective self-supervised learning approach to learn both large- and small-scale features, a challenge not addressed by previous models. Our Galileo models obtain state-of-the-art results across diverse remote sensing tasks.
\end{abstract}

\section{Introduction}\label{sec:intro}

Machine learning is increasingly being used to analyze remote sensing (RS) data across a wide variety of societally important problems, such as agricultural analyses for food security \cite{rapidresponse} and flood detection for rapid disaster response \cite{frame2024rapid}. Labels for these applications can be expensive or difficult to acquire \cite{kebede2024assessing}, motivating recent research into pretrained RS models that can produce accurate insights even with few labels. The commonalities across RS data and tasks here present an advantage, but individual tasks nonetheless present a diversity of specific requirements. Specifically, practitioners need ML models for remote sensing to \circlednum{1} flexibly process many different modalities and input shapes (where we define input shape as $[\textrm{height} \times \textrm{width} \times \textrm{timesteps} \times \textrm{modalities}]$), and \circlednum{2} model phenomena which occur at very different scales. 

Different RS applications may require very different inputs, but which inputs are best may not be obvious \emph{a priori}. In addition, data may not always be available, for reasons ranging from the temporary unavailability of a satellite \cite{european2023mission} to cloud cover \cite{coluzzi2018first} to revisit periods for different satellites. This requires our models to \circlednum{1} \textbf{flexibly process different modalities and input shapes}. For example, to support timely predictions when detecting fishing vessels, \citet{beukema2023satellite} leverage a range of modalities ranging from optical to synthetic aperture radar (SAR) to nightlights. Achieving strong performance with these modalities requires different input shapes; for example, models benefited from seeing historical imagery when processing SAR data but not optical data. Similarly, \citet{kruse2023satellite} found that processing RS data \emph{first} as single-timestep imagery, and \emph{then} as pixel-timeseries was optimal when detecting plastic waste.

In addition, \circlednum{2} \textbf{RS phenomena occur at very different scales}. For example, practitioners use machine learning and RS to map kilometer-scale glaciers which persist for decades \cite{baraka2020machine} and detect meter-scale maritime vessels which are extremely transient in time \cite{beukema2023satellite}. Single applications can span a huge range of scales; for example, plastic waste sites can range in size from \num{1000} m$^2$ to \num{40000} m$^2$ \cite{kruse2023satellite}, requiring models to identify phenomena across these scales to accurately detect these sites. Similarly, marine phenomena can range from a few isolated pixels (for marine debris) to large contiguous areas (for sediment-laden water) \cite{kikaki2024detecting}.

\begin{figure*}
\centering
\input{system_diagram}
\caption{\textbf{\emph{SSL for RS}}. \textbf{Top left:} Attracts representations originating from the same sample and repels representations from other samples. \textbf{Top center:} Predicts pixels of hidden patches. \textbf{Top right:} Predicts representations of hidden patches. \textbf{Bottom left:} Attracts representations originating from the same patch and repels representations from other patches. \textbf{Galileo (ours):} Our method simultaneously attracts varied-level representations originating from the same patch and repels elsewhere --- and attracts pixel predictions originating from the same patch and repels elsewhere. This strategy encourages learning global \emph{and} local features.}
\label{fig:system}
\end{figure*}

We address these challenges by proposing a new family of pretrained RS models called ``Galileo'' (\textbf{G}lobal \textbf{a}nd \textbf{L}ocal Flex\textbf{i}b\textbf{l}e \textbf{E}arth \textbf{O}bservation models). We develop a highly multimodal dataset that varies in space and time to train these models, and we modify the ViT architecture \cite{dosovitskiy2021an} to process this dataset. We present a novel self-supervised learning (SSL) algorithm to learn useful representations of small- and large-scale features. Our algorithm combines two training objectives: one makes predictions in (essentially) the pixel space, and the other makes predictions in the latent space (Figure \ref{fig:system}). We show that these dual objectives are complementary and necessary to improve training stability and achieve good performance across all task scales.

We demonstrate the efficacy of our Galileo models via an extensive benchmarking suite, covering many applications, domains, and RS data types. Fair comparison of RS models is challenging \cite{Corley_2024_CVPR}. To achieve this, we run hundreds of experiments across $16$ pretrained RS baselines to robustly compare with existing methods. 

% \input{input_shapes}

\section{Multi-modal \& Multi-scale Self-Supervision}

\input{table_ranks_subset_flexibility}

We aim to pretrain a model that can be applied to a range of RS tasks, even when few labels are available. This requires that we can (i) process many different modalities, (ii) ingest varying input shapes, and (iii) model phenomena that occur at very different scales. 

To train a model with capabilities for multimodal processing and input shape flexibility, we first build a pretraining dataset where each training instance includes data for multiple RS modalities, and captures data across a suitably large spatial and temporal range. During model pretraining, we subsample modalities and input shapes (subsetting space and time) from this dataset, such that the resulting model can process multi-scale and multimodal RS data. 

However, a traditional ViT architecture expects samples to share the same modalities and input shapes. We therefore adapt the ViT architecture to ingest arbitrary subsets of modalities, with varying input shapes.

Most existing SSL algorithms have been developed to be effective on natural imagery rather than RS data. We therefore develop a novel SSL algorithm that learns strong representations of both: (i) large objects and lower-frequency features, and (ii) tiny objects and high-frequency features. We achieve this by integrating two complementary algorithms, each specialized for a different scale of feature extraction.

\subsection{Achieving Flexibility via the Data and Architecture}

\subsubsection{Collecting Diverse Pretraining Data} \label{sec:dataset}

We collect a large, globally sampled pretraining dataset of \num{127155} training instances. Section \ref{app:pretraining_dataset} describes our dataset sampling process. We include a wide range of RS inputs to serve diverse applications. A training instance consists of \num{4} types of data covering \num{9} RS data modalities. We select these modalities based on their uses in past machine learning for remote sensing efforts \cite{van2023worldcereal,beukema2023satellite,poggio2021soilgrids}.

We group the modalities by whether they vary in space, time, both, or neither.
A single instance consists of $24$ monthly timesteps and $96 \times 96$ pixels at a $10$m/pixel resolution.

\textbf{Space-time varying data.} These data consist of imagery acquired by Sentinel-1 \& -2 satellites. For Sentinel-1, we take the VV and VH polarizations; and for Sentinel-2, we take all bands except the B1, B9 and B10 bands. All bands are resampled to a $10$m/pixel resolution. We also include NDVI \cite{tucker1979red} from Sentinel-2 as an input. 

\textbf{Space varying data.} These data consist of elevation and slope captured by the Shuttle Radar Topography Mission \cite{srtm}, which are constant in time; Dynamic World land cover map probabilities \cite{brown2022dynamic}, averaged over time for temporal consistency; and World Cereal agricultural land cover maps \cite{van2023worldcereal}.

\textbf{Time varying data.} These data consist of precipitation and temperature from the ERA5 dataset \cite{hersbach2020era5}; climate water deficit, soil moisture, and actual evapotranspiration from TerraClimate \cite{abatzoglou2018terraclimate}; and VIIRS nighttime lights \cite{elvidge2017viirs}. Although these modalities vary in space as well, their spatial resolution (ERA5 has a spatial resolution of tens of kilometres per pixel) means we treat them as static in space from the perspective of a single instance.

\textbf{Static data.} These data consist of population estimates from the LandScan dataset \cite{dobson2000landscan}, the spatial location of the instance, defined by its central latitude and longitude, Dynamic World classes spatially averaged over the instance, and World Cereal agricultural land cover maps spatially averaged over the instance. We include the averaged Dynamic World and World Cereal inputs in addition to the space-varying inputs. 

\subsubsection{Architecting a ViT for All Inputs}\label{sec:architecture}

We require a model for the wide range of modalities and input shapes in our dataset to handle the variety of RS data inputs (Section \ref{sec:dataset}).
Transformers are general architectures, but still require tokenization strategies to handle the varied input dimensions of space, time, and modality.

We adapt the ViT \cite{dosovitskiy2021an} architecture to ingest channel groups and \textit{variable} input shapes and timesteps, as follows:

\textbf{Patchification and Channel-group projections.} We group all inputs described in Section \ref{sec:dataset} into ``channel-groups'' when projecting the input to the encoder dimension, where a channel-group combines semantically cohesive channels (e.g. the RGB bands from Sentinel-2). We patchify --- i.e., split the input tensor into spatial squares, timesteps, and channel groups --- and project our inputs to the encoder dimension $D$ using the following transformations: (i) Space-time data, $\mathbb{R}^{H \times W \times T \times C} \rightarrow \mathbb{R}^{\frac{H}{P} \cdot \frac{W}{P} \cdot T \cdot G \times D}$, $H$ is the height, $W$ is the width, $P$ is the patch size (in pixels per side), $T$ is the timesteps, $C$ are the channels, $G$ are the channel groups. (ii) Space data, $\mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{\frac{H}{P} \cdot \frac{W}{P} \cdot G \times D}$, (iii) Time data, $\mathbb{R}^{T \times C} \rightarrow \mathbb{R}^{T \cdot G \times D}$, and (iv) Static data, $\mathbb{R}^{C} \rightarrow \mathbb{R}^{G \times D}$.

\textbf{Token Embeddings.} After these linear projections, our encoder creates spatial and temporal sinusoidal position embeddings, learnable channel embeddings, and month embeddings to enable seasonal reasoning; we denote these token position embeddings as $\mathtt{e} \in \mathbb{R}^{L \times D}$, where $L$ is the token sequence length. Our encoder adds these embeddings to the linear projections, previously computed. It concatenates all channel groups along the sequence dimension ---  forming our input sequence, $\mathbf{x} \in \mathbb{R}^{L \times D}$.  Importantly, our approach is robust to the presence and varying lengths of channel group sequences.

\textbf{Flexible input shapes.}

We adopt resizeable patch embeddings \cite{beyer2023flexivit}. This method resizes the weights of the ViT's linear projection layer, so that varying patch sizes $P$ can be projected into the encoder dimension. Since each token can now represent different spatial scales, we dynamically adjust the spatial positional embedding frequencies to match the spatial scale, following \citet{reed2023scale}'s ``Ground Sample Distance Positional Encodings.'': $2$D position embeddings are interpolated by a factor of $\frac{P}{H} = \frac{P}{W}$.

\subsection{Learning global and local features via SSL} \label{sec:method_ssl}

\begin{figure*}
    \centering
    \input{fig1.tex}
    \caption{We train our model to learn both global and local features by alternating between the global (left) and local (right) pretraining tasks. These tasks differ in \circlednum{1} masking strategies and \circlednum{4} target encoding strategies. In this figure, the pretraining data is split into the channel-groups described in Section \ref{sec:architecture}.}
    \label{fig:main}
    % \vspace{-5pt}
\end{figure*}

Objects in RS data can range in size from a single pixel to hundreds or thousands of pixels. A single pretraining task is unlikely to capture this huge range in feature size, which we evidence through illustrative experiments. We introduce a novel algorithm that combines two objectives: one to learn local features and the other to learn global features. 

Our general latent prediction framework is inspired by \citet{assran2023self}, \citet{garrido2024learning}, and \citet{wei2024towards}, and operates as follows: (i) Given a batch of samples, we construct two \emph{different} views of each sample, $\mathbf{x}_1 \in \mathbb{R}^{L_1 \times D}$ and $\mathbf{x}_2 \in \mathbb{R}^{L_2 \times D}$. (ii) Our ``online'' encoder computes patch encodings $\mathbf{z}_1 = \mathbf{E}(\mathbf{x}_1)$, while our ``target'' encoder --- an exponential moving average of the online encoder --- computes target patch encodings $\mathbf{z}_2 = \mathbf{E}_{\text{EMA}}(\mathbf{x}_2)$. (iii) A predictor transformer $\mathbf{P}$ receives the target view's position, time, month, and channel group embeddings $\mathtt{e}_2 \in \mathbb{R}^{L_2 \times D}$ as placeholder queries and predicts patch encodings $\mathbf{p} \in \mathbb{R}^{L_2 \times D}$ by cross-attending to the online patch encodings, i.e., $\mathbf{p}=\mathbf{P}(\mathtt{e}_2, \mathbf{z}_1)$. (iv) The predictions $\mathbf{p}$ and targets $\mathbf{z}_2$ are compared to compute a loss $\mathcal{L}(\mathbf{p}, \mathbf{z}_2)$ that updates the online encoder. 

\subsubsection{Learning Global Features} \label{subsection:global}

Our global algorithm learns coarse-grained, lower-frequency features suited for classification applications. We modify our general latent prediction framework as follows:

\textbf{Target Depth.} Target patch encodings are typically extracted from the target view through the \textit{entire} target encoder. Some RS modalities (e.g., Dynamic World landcover classes) map closely to downstream tasks (e.g., landcover classification). These inputs may not need to be heavily processed (deeply encoded) to be useful targets. To account for the variable processing level of RS inputs, we compute targets by saving token representations after the $\ell^\text{th}$ layer, where $\ell$ varies by modality. We select $\ell$ based on each modality's abstraction level: pseudo-labels use only linear projections (no encoder layers), Sentinel-1 and Sentinel-2 use all encoder layers, and other channels use half the encoder layers. We denote our level-specific target encoder as $\mathbf{E}_{\text{EMA}}^{\ell}$.

\textbf{Loss function.} I-JEPA \cite{assran2023self} and Image World Model \cite{garrido2024learning} compute the mean-squared error (MSE) between predictions and targets. LatentMIM demonstrated superior performance via the ``Patch Discrimination'' (PatchDisc) loss \cite{wei2024towards}, which discriminates between tokens in a sample. To encourage globally discriminative representations, we extend the PatchDisc loss to better discriminate samples in a batch. We define our new loss, called AllDisc, below (note this is equivalent to PatchDisc if the $\sum_{i'}^{B}$ summation is removed):

\resizebox{!}{0.97\height}{$
    \mathcal{L}(\mathbf{u},\mathbf{v}) = \frac{-\tau}{B}\sum_{i}^{B} \frac{1}{L_i}
    \sum_{j}^{L_{i}} \log
    \frac{\text{exp}(\text{sim}(\mathbf{u}_{i,j},\mathbf{v}_{i,j})/\tau)}
    {{\sum_{i'}^{B}}
    \sum_{j'}^{L_{i'}}
    \text{exp}(\text{sim}(\mathbf{u}_{i,j},\mathbf{v}_{i',j'})/\tau)
    }
$}

with the softmax temperature $\tau$, the sample index $i$, the batch size $B$, the token index $j$, the number of tokens in the $i^{th}$ sample $L_{i}$, and the $l_2$ normalized dot product sim$(\mathbf{u},\mathbf{v}) = \mathbf{u}^{\top}\mathbf{v}/\norm{\mathbf{u}}\norm{\mathbf{v}}$. Our global loss is:

\makebox[\columnwidth]{
$\mathcal{L}_{global} = \text{AllDisc}(\mathbf{P}(\mathtt{e}_2, \mathbf{E}(\mathbf{x}_1)),\text{sg}(\mathbf{E}_{\text{EMA}}^{\ell}(\mathbf{x}_2)))$
}

where sg is the stop-gradient operation.

\textbf{Masking Strategy.} Structured or contiguous masking increases the distance between online and target tokens. This reduces the ability for the online encoder and predictor to locally interpolate between online (``visible'') tokens to predict the target token's representation. ``Space masking'' samples masks across space while maintaining consistency across channel groups and time; ``time masking'' does the same across time while maintaining consistency across channel groups and space. We alternate between the two during pretraining (see complete masking details in Appendix \ref{app:global}).

\subsubsection{Learning Local Features} \label{subsection:local}

Our local algorithm learns fine-grained, higher-frequency features suited for segmentation applications. We modify our general latent prediction framework as follows:

\textbf{Target Depth.} All prior latent prediction methods target patch representations, rather than pixels, to encourage the encoder to ignore ``low-level'' image details. While pixel-level details are usually irrelevant in natural imagery, individual pixels can contain essential details in RS. Therefore, we target the lowest representation level: the pixel space. To achieve this with a contrastive objective, we compute targets using the target encoder's linear projection, $\mathbf{E}_{\text{EMA}}^{proj}$, \emph{skipping all transformer blocks}.

\textbf{Loss Function.} MAE \cite{he2022masked} computes the MSE between predicted and hidden pixels. Although this encourages local features, we can do better by \emph{amplifying local details} via the PatchDisc loss. This combination amounts to performing contrastive learning in the pixel space; to our knowledge, ours is the first SSL algorithm to perform contrastive learning in the pixel space.

\makebox[\columnwidth]{
$\mathcal{L}_{local} = \text{PatchDisc}(\mathbf{P}(\mathtt{e}_2, \mathbf{E}(\mathbf{x}_1)), \text{sg}(\mathbf{E}_{\text{EMA}}^{proj}(\mathbf{x}_2)))$
}

\textbf{Masking Strategy.} Unstructured masking distributes the set of online and target tokens across all axes. As a result, target tokens are located close to online tokens more frequently than if contiguous masking were used. This makes local token relationships more useful to solve the pretext task. See complete masking details in Appendix \ref{app:local}.

\subsubsection{Combining Local and Global Objectives} \label{sec:combined}

While we can learn strong local and global representations independently, we aim to learn representations that excel at both. In Section \ref{sec:ablations}, we show that these goals conflict when using a single pretraining task. Better classification hurts segmentation; better segmentation hurts classification. Combining both pretraining objectives resolves this conflict.

This combination has a secondary benefit. Latent prediction frameworks are known to be unstable; they can find trivial solutions that learn useless representations. In Section \ref{sec:ablations}, we show that our dual objective algorithm is far more robust than single objectives. 

Our dual algorithm, Galileo, alternates between global and local objectives during pretraining (Figure \ref{fig:main}):

\makebox[\columnwidth]{
$\mathcal{L}_{Galileo} = \frac{1}{2}(\mathcal{L}_{global} + \mathcal{L}_{local})$
}

Ablation experiments in Section \ref{sec:ablations} show that PatchDisc outperforms AllDisc when combining global and local objectives, so we use PatchDisc for both objectives. 

\input{cosine_similarities}

\textbf{Representation measurements.} We performed an experiment to verify our intuition about global and local algorithms. For all EuroSat training samples, we compute the cosine similarity between the token representations in a sample and then average across all samples, measuring the within-sample representational differences. Similarly, we average all tokens over each sample and then compute the cosine similarities between samples, measuring the between-sample differences. We find our local objective amplifies within-sample features, whereas our global objective amplifies between-sample features (Table \ref{tab:cosines}). We also confirm these intuitions via downstream tasks in Section \ref{sec:ablations}.

\section{Experimental Framework}

\textbf{Pretraining.} We pretrain three model sizes for $500$ epochs using the algorithm described in Section \ref{sec:combined}. Please see the Appendix for complete details.

\textbf{Downstream Tasks.}\label{section:downstream_tasks} We evaluate our model on all Sentinel-2 tasks in GeoBench \cite{lacoste2024geo}.
These cover single-timestep image classification and segmentation in various applications and geographies.
We also test on fine-grained segmentation via the MADOS marine debris dataset \cite{kikaki2024detecting}, Sentinel-1 image segmentation via Sen1Floods11 \cite{bonafilia2020sen1floods11}, image-timeseries segmentation via PASTIS \cite{garnot2021panoptic}, optical pixel-timeseries classification via Breizhcrops \cite{russwurm2019breizhcrops}, and multimodal pixel-timeseries classification via CropHarvest \cite{tseng2021cropharvest}.

\textbf{Comparisons.} \label{section:comparisons} We benchmark our models against all SOTA pretrained RS models (described in Section \ref{sec:pretrained_rs}).
We report results on the full test set for each task.
Feature scaling, image sizes, and hyperparameter selections have significant effects on model performance \cite{Corley_2024_CVPR}.
We therefore rerun evaluations for all baseline models and sweep feature scaling methods and learning rates (where appropriate).
In addition, we resize all images to the pretraining image size.
For the image classification and segmentation tasks, we measure model results across four training set sizes (``partitions''): $100$\%, $20$\%, $5$\%, and $1$\%.
We use a patch size of \num{4} for all models with variable patch sizes.
When applying single-timestep models to the multi-timestep PASTIS dataset, we additionally sweep pooling methods to pool per-timestep encodings.
See Appendix \ref{app:eval_details} for complete details.

\input{table_cls_knn_subset}
\input{table_cls_ft_subset}

\section{Results}

We present model rankings averaged across all tasks and partitions in Table \ref{tab:ranks_subset}. We evaluate Galileo against common RS benchmarks; however, while many pretrained models can \emph{only} process the benchmark modalities, Galileo is trained to process numerous additional modalities which are readily available to practitioners (Table \ref{tab:ranks_subset}, ``Supported Inputs''). This functionality is highly valuable to practitioners despite not being captured by these common benchmarks.

\textbf{Image results.} We compare Galileo to image-specialized models in Tables \ref{tab:knn_subset}, \ref{tab:ft_subset} and \ref{tab:seg_results_subset}; besides Satlas, these models were pretrained on single-timestep imagery, devoting all their capacity to images. Nonetheless, Galileo-Base outranks all such models on image classification and segmentation. Our lightweight models also excel at these tasks, often outperforming much larger models; we anticipate that these Galileo-Nano and Galileo-Tiny models will be highly valuable to many cost-sensitive RS practitioners in research and production. Furthermore, Galileo's variable patch sizes allow for trade-offs between computational cost and model performance; by increasing the patch size, an instance is split up into fewer tokens, reducing the MACs required to obtain an embedding --- we plot this trade-off in Figure \ref{fig:model_performance}.

Besides Galileo, AnySat is the only model that supports single-timestep images and pixel-timeseries. Among these two generalist models, Galileo far exceeds AnySat on standard benchmarks, for example, by $10.8\%$ on EuroSat.

\textbf{Timeseries classification results.} We compare Galileo to generalist AnySat and the pixel-timeseries specialist Presto in Table \ref{tab:ts_results}. We conclude similarly: Galileo outranks the specialist model and far exceeds AnySat.

\input{table_seg_lp_subset}
\input{eurosat_macs}
\input{table_ts}

\subsection{Ablations} \label{sec:ablations}

For all our ablation experiments, we pretrain ViT-Tiny models for $200$ epochs. We select four diverse validation tasks covering segmentation (Sen1Floods11 and MADOS), image classification (EuroSat), and timeseries classification (CropHarvest), using only the validation sets for ablations. 

We begin by ablating our global and local feature learning algorithms in isolation; while the global learning algorithm excels at the classification tasks and the local learning algorithm excels at the segmentation tasks, neither excel at both. We then ablate our combined algorithm, which excels on both the classification and segmentation tasks.

\input{global_ablations}

\textbf{Ablations within our global feature learning algorithm.} We find that replacing our approach by random masking or PatchDisc significantly harms classification. Similarly our AllDisc loss outperforms MSE (used in I-JEPA) by $2.3\%$ on EuroSat and $2.7\%$ on CropHarvest (Tab.~\ref{tab:global_ablation}). Using only linear projections for target processing reduces performance by $2.6\%$ on EuroSat and $2.93\%$ on CropHarvest, confirming the importance of targeting higher-level features for classification.

\input{local_ablations}

\paragraph{Ablations within our local feature learning algorithm.} Space and time masking significantly degrade MADOS performance. Cross-batch negative sampling (AllDisc) slightly harms segmentation (Tab. \ref{tab:local_ablation}). Using more encoder layers for target processing (as in LatentMIM's full $12$-layer approach) fails to learn useful representations, validating our targeting of low-level features through linear projections alone.

\input{combined_ablations}

\textbf{Ablations to our combined algorithm.} Leveraging PatchDisc for both objectives and providing the target encoder with all available tokens outperforms naively combining our prior global and local objectives (Tab.~\ref{tab:combined_ablation}). Not sharing predictor parameters across objectives is optimal. Interestingly, our dual-objective strategy achieves successful training runs more consistently. We believe this is a valuable finding beyond our RS focus, as improving latent prediction SSL is an active research area \cite{wei2024towards, littwin2024enhancingjepasspatialconditioning, mo2024connecting}.

\section{Related Work and Background}

\textbf{Self-Supervised Learning.} Reconstructing a masked or noised input is a common form of self-supervised pretraining, both for natural language \cite{devlin2018bert, radfordimproving, skipgram} and natural imagery \cite{xie2022simmim, he2022masked, vincent2008extracting}. The most successful such framework for natural imagery is masked autoencoding (MAE, \citet{he2022masked}), which encodes a subset of patches, then predicts the hidden patches with a decoder conditioned on the hidden patch locations. 
%
Contrastive learning \cite{le2020contrastive, oord2018representation, chen2020simple, chopra2005learning} is a different approach to learning representations, which encodes samples augmented in two different ways, then attracts the representations of the same sample (called positives), and repels the representations of different samples (called negatives). 
%
Models pretrained with MAE typically outperform contrastive learning under finetuning evaluations, whereas the reverse is generally true under frozen feature evaluations \cite{park2023what, he2022masked, garrido2024learning}. 

A third paradigm has recently emerged that makes predictions in a latent space, like contrastive learning, and conditions the prediction on the sample's transformation, like MAE. For instance, I-JEPA \cite{assran2023self} achieves strong performance under both finetuning and frozen feature evaluations; essentially, I-JEPA modifies MAE to predict patch \emph{representations}, computed by the encoder's exponential moving average. LatentMIM \cite{wei2024towards} showed that I-JEPA pretraining is unstable and can be improved by their patch discrimination objective; PatchDisc attracts patch representations of the same location within an image, and repels patch representations of the same sample but different locations. Our approach falls into this third category, offering a novel learning algorithm tailored for RS.

\textbf{Pretrained RS Models.} \label{sec:pretrained_rs} SatMAE \cite{cong2022satmae} adapted MAE to multispectral satellite imagery. SatMAE++ \cite{noman2024rethinking} improved over SatMAE by reconstructing upsampled inputs. Prithvi 2.0 \cite{szwarcman2024prithvi} applied MAE to Harmonized Landsat and Sentinel (HLS) multispectral imagery. CROMA \cite{fuller2024croma} adapted contrastive captioners \cite{yu2022coca} to jointly model multispectral and SAR data. SoftCon \cite{wang2024multi} adapted supervised contrastive learning \cite{khosla2020supervised, zhang2022use} to separately model multispectral and SAR data. MMEarth \cite{nedungadi2024mmearth} adapted convolutional MAEs \cite{woo2023convnext} to reconstruct images and other RS data. Presto \cite{tseng2023lightweight} adapted MAE to pixel-timeseries of multispectral, SAR, and other RS data. Satlas \cite{bastani2023satlaspretrain} adapted Swin transformers \cite{liu2021swin} to multispectral (and optionally multiple) images using multi-task supervised pretraining. DOFA \cite{xiong2024neural} adapted MAE to separately model multispectral, SAR, and high-resolution RGB images. DeCUR \cite{wang2024decoupling} adapted Barlow Twins \cite{zbontar2021barlow} to separately model multispectral and SAR imagery. 

While there are other pretrained models, they ingest only RGB images \cite{reed2023scale, tang2024cross, mendieta2023towards}, have not published pretrained weights \cite{guo2024skysense, han2024bridging}, or have been surpassed by above-cited newer models \cite{fuller2022satvit, wang2023ssl4eo, ayush2021geography, manas2021seasonal, jean2019tile2vec, astruc2024omnisat, jakubik2023foundation}. 

AnySat \cite{astruc2024anysat} is concurrent with our work and shares the same spirit.
It combines I-JEPA \cite{assran2023self} and contrastive learning objectives to pretrain an RS model.
AnySat is pretrained on data from more satellites, but does not include other modalities modeled by Galileo.

\section{Conclusion}

In this work, we identify two requirements for the application of pretrained models in a wide range of RS contexts: (i) the ability to flexibly process different modalities and input shapes, and (ii) the ability to model RS phenomena which occur at very different scales. To meet these requirements, we present the Galileo family of pretrained RS models. 

We achieve these requirements by innovating on (i) the pretraining dataset used to train the Galileo models, (ii) the model architecture, allowing the model to flexibly ingest a highly multimodal dataset that varies in both space and time, and (iii) the SSL algorithm, to encourage the model to learn phenomena occurring at vastly different scales. 

We run hundreds of evaluations --- including extensive sweeps of baseline pretrained RS models --- to robustly demonstrate Galileo's performance across a wide range of domains, modalities, and task types. We run thorough ablations of our method. We hope the resulting insights will serve the SSL community beyond RS.

The model weights, pretraining code, pretraining data and evaluation code are open sourced at \href{https://github.com/nasaharvest/galileo}{github.com/nasaharvest/galileo}.

\section*{Impact Statement}

Applications of machine learning to RS span a range of societally important applications, from species distribution modelling \cite{teng2024satbird} to disaster management \citep{kansakar2016review}. By providing a set of RS models which can perform well even when few labels are available, we hope to enable RS practitioners to continue exploring and deploying these applications. We take several steps to encourage the adoption of these models, including training the models on publicly available RS data and training a diversity of model sizes so that they can be used in compute-constrained environments.

\citet{tuia2023artificial} highlight that a risk of these models is that they can be used to collect information about populations so that decisions are made without their involvement. We encourage the deployment of Galileo in collaboration with local communities and stakeholders \citep{maui,kshirsagar2021becoming,nakalembe2023considerations}.

\bibliography{example_paper}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Methodology details}

\subsection{The Galileo SSL algorithm}

We adopt a latent prediction framework inspired by \citet{assran2023self}, \citet{garrido2024learning}, and \citet{wei2024towards}, which operates as follows: \circlednum{1} Given a batch of samples, we construct two \emph{different} views of each sample, $\mathbf{x}_1 \in \mathbb{R}^{L_1 \times D}$ and $\mathbf{x}_2 \in \mathbb{R}^{L_2 \times D}$. \circlednum{2} Our ``online'' encoder computes patch encodings $\mathbf{z}_1 = \mathbf{E}(\mathbf{x}_1)$, while our ``target'' encoder --- an exponential moving average of the online encoder --- computes target patch encodings $\mathbf{z}_2 = \mathbf{E}_{\text{EMA}}(\mathbf{x}_2)$. \circlednum{3} A predictor transformer $\mathbf{P}$ receives the target view's position, time, month, and channel group embeddings $\mathtt{e}_2 \in \mathbb{R}^{L_2 \times D}$ as placeholder queries and predicts patch encodings $\mathbf{p} \in \mathbb{R}^{L_2 \times D}$ by cross-attending to the online patch encodings, i.e., $\mathbf{p}=\mathbf{P}(\mathtt{e}_2, \mathbf{z}_1)$. \circlednum{4} The predictions $\mathbf{p}$ and targets $\mathbf{z}_2$ are compared to compute a loss $\mathcal{L}(\mathbf{p}, \mathbf{z}_2)$ that updates the online encoder. 

We adapt this latent prediction framework for learning local and global features. We outline those adaptations below.

\subsubsection{Learning Global Features} \label{app:global}

We design this algorithm to learn abstract, lower-frequency features suited for classification applications. \circlednum{1} View construction involves: \circlednum{a} uniformly sampling the number of channel groups $N \in \{2,3,\ldots,17\}$, \circlednum{b} randomly selecting $N$ channel groups (e.g., RGB, SAR, ERA5), \circlednum{c} repeating steps (a-b) for the target encoder while excluding overlapping channel groups, \circlednum{d} applying either spatial or temporal masking, and \circlednum{e} tokenizing both views to obtain $\mathbf{x}_1$ and $\mathbf{x}_2$. Space masking samples masks across space while maintaining consistency across channel groups and time; time masking does the same across time while maintaining consistency across channel groups and space. \circlednum{2}-\circlednum{3} Following our general framework, we compute $\mathbf{z}_{1}$ and $\mathbf{p}$, and compute targets using varied exit depths from the target encoder, $\mathbf{E}_{\text{EMA}}^{\ell}$. \circlednum{4} We use our AllDisc loss (defined in Section \ref{subsection:global} as:

\makebox[\columnwidth]{
$\mathcal{L}_{global} = \text{AllDisc}(\mathbf{P}(\mathtt{e}_2, \mathbf{E}(\mathbf{x}_1)),\text{sg}(\mathbf{E}_{\text{EMA}}^{\ell}(\mathbf{x}_2)))$
}

\subsubsection{Learning Local Features} \label{app:local}
We design this algorithm to learn fine-grained, higher-frequency features suited for segmentation applications. \circlednum{1} View construction involves: \circlednum{a} tokenizing the entire sample, and \circlednum{b} randomly selecting $5\%$ of tokens for $\mathbf{x}_1$ and $50\%$ for $\mathbf{x}_2$. \circlednum{2}-\circlednum{3} Following our general framework, we compute $\mathbf{z}_{1}$ and $\mathbf{p}$, but compute targets using only the target encoder's linear projection, i.e., $\mathbf{E}_{\text{EMA}}^{proj}$ --- skipping transformer blocks such that the predictor targets low-level features. \circlednum{4} We use LatentMIM's PatchDisc loss, tasking the model to discriminate between patches on the basis of low-level features alone:

\makebox[\columnwidth]{
$\mathcal{L}_{local} = \text{PatchDisc}(\mathbf{P}(\mathtt{e}_2, \mathbf{E}(\mathbf{x}_1)), \text{sg}(\mathbf{E}_{\text{EMA}}^{proj}(\mathbf{x}_2)))$
}

\subsubsection{Combining Local and Global Objectives}

As noted in Section \ref{sec:combined}, our combined method alternates between the local and global objectives during pretraining:

\makebox[\columnwidth]{
$\mathcal{L}_{Galileo} = \frac{1}{2}(\mathcal{L}_{global} + \mathcal{L}_{local})$
}


\section{Pretraining details}

\subsection{A globally sampled pretraining dataset}\label{app:pretraining_dataset}

To construct the Galileo dataset, we split the global WorldCover map \cite{zanaga2022esa} into $1000 \times 1000$ pixels ($10km \times 10km$) tiles. For each tile, we compute two feature sets: \circlednum{1} the number of pixels within each WorldCereal classification class, and \circlednum{2} the latitude and longitude of the tile. We use these features to train a $k$=\num{150000} $k$-means clustering algorithm, and select the tiles closest to the centroid of each cluster. This yields \num{150000} training points, of which 85\% (\num{127155}) are successfully exported using Google Earth Engine \cite{gorelick2017google}. By including both the pixel counts and the latitude and longitudes as features to the $k$-means algorithm, we ensure both the semantic and geographic diversity of the model's training points --- Figure \ref{fig:data} shows a chloropleth map of the exported points.

We use this sampling procedure to construct a rich dataset to pretrain our model. This dataset consists of \num{9} RS inputs, ranging from directly sensed inputs (such as Sentinel-2 optical imagery) to semantically dense maps (such as the Dynamic World landcover maps) --- these are discussed in detail in Section \ref{sec:dataset}. Table \ref{tab:data_ablation} studies the impact of each of these modalities on the model's downstream performance, by pretraining the global-local model (Section \ref{sec:combined}) while omitting a single data product.

\input{data_ablations}

\subsection{Implementation}

\begin{wrapfigure}{l}{0.4\linewidth}
    \includegraphics[width=\linewidth]{pics/exported_training_points_all.png}
    \caption{The number of exported training points per H3 cell \cite{h3} at resolution $=2$ . We sample from the entire globe, aiming for semantic diversity (defined by the WorldCover landcover map classes \cite{zanaga2022esa}) and geographic coverage.}
    \label{fig:data}
\end{wrapfigure}

All models are trained on single H100 GPUs (model sizes and training times are described in Table \ref{tab:size_cost}). We use an effective batch size of $512$, which consists of a minibatches of $128$ instances augmented and repeated $4$ times \cite{hoffer2019augment}. For data augmentations, we randomly apply vertical and horizontal flipping and 90-degree rotations to each instance. When repeating the data, we first randomly select a patch size $P \in [1, 2, 3, 4, 5, 6, 7, 8]$. We then randomly select a (size, timestep) combination $(S, T) \in [(4, 12), (5, 6), (6, 4), (7, 3), (9, 3), (12, 3)]$. We then randomly subset spatially height $H = P \times S$, width $W = P \times S$ and timesteps $T$ from each instance in the batch.

\input{table_size_cost}

We use bfloat16 precision, and the AdamW optimizer with $\beta_1 = 0.9$ and $\beta_2 = 0.999$ with gradient clipping. We warmup our learning rate for 30 epochs to a maximum learning rate before applying a cooldown via a cosine decay schedule. We use exponential moving averaging (EMA) to update our target encoder with a momentum value of 0.996 which linearly increases to 1 throughout pretraining following \citet{assran2022masked}.

For all ablations (Section \ref{sec:ablations}), we pretrain a ViT-Tiny model for $200$ epochs to a maximum learning rate of $2 \times 10^{-3}$ and use a weight decay of $0.02$. For the final Galileo models, we pretrain the models for $500$ epochs and conduct a sweep of $[\textrm{learning rate} \times \textrm{weight decay}]$. For the ViT-Nano and ViT-Tiny architectures, we sweep $\textrm{learning rates} \in [1\times 10^{-3}, 2\times 10^{-3}, 3\times 10^{-3}]$ and $\textrm{weight decays} \in [1\times 10^{-2}, 2\times 10^{-2}, 3\times 10^{-2}]$. For the ViT-Base architecture, we sweep $\textrm{learning rates} \in [1\times 10^{-4}, 3\times 10^{-4}, 1\times 10^{-3}, 2\times 10^{-3}, 3\times 10^{-3}]$ and $\textrm{weight decays} \in [1\times 10^{-2}, 2\times 10^{-2}, 3\times 10^{-2}]$.

\section{Evaluation details} 
\label{app:eval_details}

\subsection{Implementation}
To ensure consistent experimental settings when comparing pretrained models, we rerun all evaluations under identical conditions. For the $k$NN probing, we follow the implementation of \citet{gwilliam2022beyond} --- we use the pretrained models to compute representations of the test data (as values) and training data (as keys) --- we then use the keys to classify the test data. Following \citet{fuller2024croma} and \citet{reed2023scale}, we use $k=20$. When linear probing, we use the pretrained models to compute representations of the training data and use this to train linear probes. We sweep learning rates when training the linear probes ($\{1, 3, 4, 5\} \times 10^{\{-4, -3, -2, -1\}}$) and apply the trained linear probes to the computed representations of the test data. When finetuning, we sweep learning rates when finetuning ($\{1, 3, 6\} \times 10^{\{-5, -4, -3\}}$) and apply the finetuned models to the test data.

\subsection{Evaluation Datasets}

We evaluate our models on the datasets described below. For all GeoBench-modified datasets \cite{lacoste2024geo} - m-Eurosat, m-BigEarthnet, m-So2Sat, m-Brick-Kiln, m-Cashew-Plant and m-SA-Crop-Type, we use the training, validation and test splits shared by GeoBench. In addition, we use the $1\%$, $5\%$ and $20\%$ partitions shared by GeoBench.

\begin{itemize}
\item \textbf{m-EuroSat} \cite{helber2019eurosat}: The full training set consists of \num{2000} images, with \num{1000} images in the validation and test sets. Images are $64 \times 64$ pixels.
\item \textbf{m-BigEarthNet} \cite{sumbul2019bigearthnet}: The full training set consists of \num{20000} images, with \num{1000} images in the test set. Images are $120 \times 120$ pixels.
\item \textbf{m-So2Sat} \cite{zhu2020so2sat}: The full training set consists of \num{19992} images (with \num{986} images in the test set), and images are $32 \times 32$ pixels.
\item \textbf{m-Brick-Kiln} \cite{lee2021scalable}: The full training set consists of \num{15063} images, with \num{999} images in the test set. Images are $64 \times 64$ pixels.
\item \textbf{m-Cashew-Plant} \cite{yin2023mapping}: The full training set consists of \num{1350} images, with \num{50} images in the test set. Images are $256 \times 256$; we subtile them into $64 \times 64$ images.
\item \textbf{m-SA-crop-type} (\href{https://source.coop/repositories/esa/fusion-competition}{link}): The full training set consists of \num{3000} images, with \num{93} images in the test set. Images are $256 \times 256$; we subtile them into $64 \times 64$ images.
\item \textbf{MADOS} \cite{kikaki2024detecting}: The full MADOS dataset consists of \num{2804} $140 \times 140$ images, extracted from \num{174} Sentinel-2 scenes. We use the train/val/test splits from MADOS ($50\%$/$25\%$/$25\%$) --- each split was created as a representative subset of the entire MADOS dataset. In addition, we subtile each image into $80 \times 80$ images.
\item \textbf{PASTIS} \cite{garnot2021panoptic}: The full PASTIS dataset consists of \num{2433} $128 \times 128$ timeseries, with 38-61 timesteps per timeseries. We subtile each timeseries spatially into $64 \times 64$ images. In addition, we compute monthly aggregations of the timeseries. \citet{garnot2021panoptic} share 5 folds of the data; we use folds $\{1, 2, 3\}$ for training, \num{4} for validation and \num{5} for testing. When applying single-timestep models to this dataset, we additionally sweep pooling methods to pool per-timestep encodings (as described in Section \ref{app:eval_details}).
\item \textbf{Breizhcrops} \cite{russwurm2019breizhcrops}: The Breizhcrops dataset consists of pixel-timeseries in \num{4} NUTS-3 regions in Brittany, France. We use \num{2} for training (FRH01, with \num{178613} parcels and FRH02 with \num{140645} parcels). We use FRH03 (\num{166391} parcels) for validation and FRH04 (\num{122614} parcels) for testing. The dataset consists of variable sequence lengths; we compute monthly aggregations of the timeseries.
\item \textbf{CropHarvest} \cite{tseng2021cropharvest}: The CropHarvest dataset consists of \num{3} pixel-timeseries tasks: (i) crop vs. non crop in Togo, with \num{1319} samples in the training set and \num{306} samples in the test set, (ii) maize vs. rest in Kenya with \num{1345} samples in the training set and \num{1942} m$^2$ of densely labelled pixels in the test set, and (iii) coffee vs. rest in Brazil with \num{794} samples in the training set and \num{4.2} km$^{2}$ of densely lablled pixels in the test set.
\end{itemize}

\subsection{Comparing to baseline models}

\citet{Corley_2024_CVPR} found that input-image sizes and feature scaling methods can have significant impacts on the performance of pretrained RS models. We therefore resize all input images to the sizes that the models were pretrained on. In addition, we treat feature scaling methods as an additional hyperparameter, and sweep it in addition to the learning rates (where those are applicable, i.e. for linear probing and finetuning). Finally, the PASTIS dataset consists of multiple timesteps of optical imagery. Since all benchmark models (except AnySat) cannot ingest the full timeseries natively, we use multiple forward passes. We select two methods for combining the outputs of these forward passes - \circlednum{1} a mean of the encodings, and \circlednum{2} a max, following \citet{bastani2023satlaspretrain}.

The reported test results are therefore computed by sweeping the cross product of the following hyperparameters:
$$
[\textrm{Learning Rate}] \times [\textrm{Temporal aggregations}]
$$

We select all hyperparameters using the validation sets in the downstream datasets.

In addition to conducting this sweep, we run the linear probes 5 times and average the results. When running the linear probe, we sweep the learning rate and feature scaling method concurrently for the first run. We select the feature scaling method from this first run, and fix it for all subsequent runs. We then select the best other hyperparameters per run, and aggregate these to obtain our final results.

We run this sweep for all evaluation datasets with the exception of the CropHarvest tasks; these consist of small training sets and no validation sets against which the hyperparameters can be selected. We therefore follow \citet{tseng2023lightweight} in using the same feature scaling methods as was used during pretraining, and using scikit-learn's regression algorithm with default parameters \cite{pedregosa2011scikit} for all models.

\subsubsection{Feature Scaling}
The pretrained models we benchmark against apply either standardization (MMEarth, DOFA, AnySat and Presto) or normalization (all other models) during pretraining. We sweep the following normalization statistics, either via standardization on normalization depending on the pre-training procedure: \circlednum{1} statistics from the downstream datasets, \circlednum{2} SatMAE pretraining statistics, \circlednum{3} SSL4EO \cite{wang2023ssl4eo} statistics, \circlednum{4} Galileo pretraining dataset statistics, \circlednum{5} Presto pretraining dataset statistics. For all of these statistics, we additionally sweep standard deviation multipliers. Prithvi 2.0 statistics only cover a subset of Sentinel-2 bands; we therefore only include those statistics in the sweeps for the Prithvi 2.0 model.

\section{Results} 

\begin{wraptable}{r}{0.5\linewidth}
    % if we have this table in a seperate .tex file, wraptable doens't work.
    \caption{Galileo m-Eurosat classification test performance (\%) as a function of patch size measured via $k$NN for different training set \%s. MACs required to process a single EuroSat instance are also recorded; by selecting the model size and patch size, practitioners can make trade offs between model performance and inference costs.}
    \label{tab:eurosat_per_patch}
    \resizebox{\linewidth}{!}{{\begin{tabular}{lcccccc}
        \toprule
       Arch. & patch size & GMACs & 100 \% & 20 \% & 5\% & 1\% \\
         \toprule
         \multirow{2}*{ViT-Nano} & 8 & 0.25 & 88.7 & 81.9 & 55.0 & 38.5 \\
         & 16 & 0.06 & 85.7 & 79.3 & 56.0 & 41.1 \\
         \midrule
         \multirow{2}*{ViT-Tiny} & 8 & 1.71 & 88.3 & 83.0 & 59.7 & 41.3 \\
         & 16 & 0.43 & 83.6 & 78.4 & 50.1 & 33.8 \\
         \midrule
         \multirow{2}*{ViT-Base} & 8 & 27.20 & 92.6 & 88.3 & 72.4 & 56.9 \\
         & 16 & 6.80 & 88.0 & 82.4 & 58.6 & 48.9 \\
         \bottomrule
    \end{tabular}}}
\end{wraptable}


We include full results for the image classification tasks (Table \ref{tab:knn_full}) and segmentation tasks (Table \ref{tab:seg_results_full}). In addition, full results for the m-Eurosat dataset with varying patch sizes are recorded in Table \ref{tab:eurosat_per_patch} - these values are used in Figure \ref{fig:model_performance}.

We rank the models in Table \ref{tab:ranks}. When ranking the models, we compute the average rank of each model across each dataset and partition.

\include{table_cls_knn}
\include{table_cls_ft}
\include{table_seg_lp}
\include{table_ranks}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
