\section{Related Work}
\subsection{Remote Physiological Measurement}
\label{subsec:physiological_measurement}
%--------------------------------------------

Since the successful extraction of rPPG signals from facial videos by \cite{VerkruysseGreen2008}, plenty of rPPG measurement approaches have been developed. Initially, traditional signal processing methods based on signal decomposition \cite{PohICA2010} were developed to extract rPPG signals from facial videos, recovering fundamental physiological signals through Blind Source Separation (BSS) techniques such as Independent Component Analysis (ICA) \cite{McDuffANMPM2011,PohICA2010} and Principal Component Analysis (PCA) \cite{LewandowskaPCA2011}. However, BSS-based techniques did not take into account the physical and optical properties of the skin. To leverage prior knowledge of the physiological waveform dynamics, several methods were proposed based on signal projection or color space decomposition. For instance, CHROM \cite{DeCHROM2013} eliminated specular reflections in videos by transforming the face video into a linear combination of chrominance-signals. POS \cite{WangPOS2017} proposed a physically grounded demixing approach by defining a plane orthogonal to the skin color space. To further enhance the generalization of POS, Zhang \textit{et al.} proposed APOS \cite{ZhangAPOS2024}, which can adaptively determine the signal plane used in POS, thereby improving the robustness against complex scenarios. However, all these signal processing methods struggled to effectively separate noise from different sources and neglected a significant amount of spatio-temporal and chromatic space information \cite{McDuffRPPGSURVEY2023}. 

In recent years, deep learning (DL) models have gained prominence in rPPG measurement due to their exceptional nonlinear fitting capabilities and spatio-temporal representation capacities. Among these methods, early end-to-end spatio-temporal networks captured physiological signals in face videos by designing and combining 3D convolution modules \cite{liu2020Deeprppg, CINrPPG2023}. CIN-rPPG \cite{CINrPPG2023} proposed utilizing both the channel and spatial interactions to extract video features for accurate rPPG measurement. As the development of the Transformer in computer vision field, some researchers transferred Vision Transformer (ViT) \cite{AlexeyViT2021} or its variants \cite{LiuSwin2021} to the rPPG measurement task for capturing global spatio-temporal context information. Yu \textit{et al.} first proposed PhysFormer \cite{YuPhysFormer2022} and PhysFormer++ \cite{YuPhysformer++2023} to leverage the long-range sequence modeling ability of Transformer architecture for rPPG measurement. Afterward, Dual-TL \cite{Dual-TL2024} used a hybrid spatio-temporal Transformer-based architecture to enhance the modeling capability for long video sequence dependencies. Some other approaches \cite{shao2023tranphys, LiurPPGMAE2023} were inspired by the Masked Autoencoder (MAE) \cite{HeMAE2022} paradigm, which enhanced the quality of the estimated rPPG signals by pre-training Transformer models on large datasets (e.g., VIPL-HR \cite{NiuVIPL2018}) as prior knowledge. Additionally, some researchers proposed to manually craft spatio-temporal map (STMap) to inherit the self-similar prior of facial videos by averaging pixels in different facial regions of interest (ROIs) \cite{NiuRhythm2020,NiuCVD2020}, but such self-similarity was hard to obtain when serious head movement happened \cite{LiurPPGMAE2023}. In general, these methods ignored the effect of visual interference in the real-world, which may produce degraded rPPG signals and harm the fidelity of heart rate measurement.

To alleviate visual interference induced by head movement, Li et al. \cite{LiMotionRobust2023} manually designed Physiological Feature Extraction (PFE) and Temporal Face Alignment (TFA) modules for capturing facial motion and resolution variations. STPhys \cite{CaoSTPhys2024} built a CNN-based spatio-temporal model to enhance video quality and achieve high accurate rPPG measurement in low-light conditions. ConDiff-rPPG \cite{WeiConDiff2024} used the diffusion model to restore the damaged STMap, thereby improving the robustness under heavy head movements and occlusions. Lastly, MotionMatters \cite{MotionMatters2024} proposed to augment the facial videos with existing motion magnification approaches, thereby enhancing the model's robustness against head motion after training. However, these models heavily relied on these modules tailored for the specific type of interference. As a result, they performed poorly when faced with other unexpected visual disturbances. To address these limitations, we propose to decompose the noise-free GT-PPG signals into a latent feature representation. These noise-free PPG features are combined into a codebook that can correct degraded rPPG features, regardless of the type of visual interference that accumulates. With the learned noise-free codebook, we transform rPPG measurement into a code query task to mitigate the visual interference of non-physiological information contained in the video.

% Since the successful extraction of the rPPG signals from the green channel of facial videos by \cite{VerkruysseGreen2008}, plenty of rPPG measurement approaches have been developed. Initially, traditional signal processing methods \cite{PohICA2010,DeCHROM2013,DeI2014} based on color space transformations and signal decomposition have been employed to extract rPPG signals from facial videos. However, these traditional methods may not work well in complex scenarios as they work under strong assumptions. In recent years, the utilization of deep learning (DL) models has gained prominence in rPPG measurement due to their exceptional nonlinear fitting capabilities and spatial-temporal representation capacities. Among these methods, various end-to-end spatial-temporal networks \cite{ChenDeepPhys2018,YuPhysNet2019,YuPhysFormer2022} leveraging convolution and transformer architectures have been devised to directly recover rPPG signals from facial videos. Besides, approaches involving hand-crafted feature maps based on facial regions of interest (ROI) \cite{NiuCVD2020,LuDualGAN2021} have been developed to alleviate the interference from irrelevant backgrounds. Subsequently, rPPG signals are extracted from these hand-crafted feature maps using 2D convolutional neural networks. Previous methods directly extract subtle rPPG signals from facial videos and neglect the significant modality gap between videos and rPPG signals, which tends to cause averaged results that harm the-fidelity of the signals. While PulseGAN \cite{SongPulseGAN2021} utilizes the generation prior of GAN to refine the extracted rPPG signals, it is obvious that GAN training is susceptible to modal collapse. To address these limitations, we treat rPPG measurement as a code query task within a finite codebook to mitigate the uncertainty in cross-modal mapping.

%--------------------------------------------
\subsection{Discrete Representation Learning}
\label{subsec:prior_learning}
%--------------------------------------------

Discrete representation with learned dictionary has demonstrated its superiority in image restoration tasks, such as super-resolution \cite{YangISRVSR2010, TimofteANRFESR2013, AhmedCSCUW2019, GuoTCSVT24} and denoising \cite{EladIDVSROLD2006}, since the fine details and textures could be well-preserved in the dictionary. The concept of leveraging pre-learned prior knowledge to restore degraded images and videos further drove advancements in deep learning-based image restoration \cite{JoPSISRULT2021} and synthesis methods \cite{OordVQVAE2017, RazaviVQVAE22019, EsserVQGAN2021}. VQ-VAE \cite{OordVQVAE2017} was the first to use a highly compressed representation (i.e., codebook) learned by the vector quantized autoencoder model for image synthesis. VQ-GAN \cite{EsserVQGAN2021} further improved the quality of synthesized images while significantly reducing the codebook size by using adversarial loss and perceptual loss. Recently, codebook-based discrete prior representation learning has been exploited in other fields. UniColor \cite{HuangUniColor2022} achieved robust image colorization by disentangling and quantizing chroma representation from a continuous grayscale image with a codebook. CodeFormer \cite{ZhouCodeFormer2022} decomposed high-resolution face images into a codebook with VQ-VAE, then achieved image super-resolution by replacing low-resolution face image features with the decomposed codebook items. Similarly, GSS \cite{ChenGSS2023} constructed a codebook consisting of real semantic masks with a VQ-VAE and queried the codebook items with an additional image encoder to assist semantic segmentation. 

Due to the ability of the discrete prior representation to preserve pure knowledge details, more recent works have attempted to extend discrete representation learning to modalities outside of images. Evonne et al. \cite{NgL2L2022} proposed to disentangle facial actions (expression coefficients and 3D head rotations) into an action unit codebook, which generated realistic facial actions of the listener w.r.t. the facial actions and audio of the speaker. PCT \cite{GengPCT2023} posited that the coordinate vectors of body joints can be equivalently represented by multiple discrete tokens, while these tokens together formed a codebook prior to human poses. This formulation allowed for performing robust human pose estimation by encoding image features to query the learned codebook items. Inspired by the 3D Face Morphable Model (3DMM) \cite{Li3DMM2017}, CodeTalker \cite{XingCodetalker2023} represented general facial expressions with a finite discrete codebook as a prior for facial motions, thereby proposing a temporal autoregressive model for speech-conditioned facial motion synthesis. Taking the recent advancement of discrete representation learning, we first explore to design a adaptive noise-free codebook for PPG features in order to achieve robust rPPG measurement in this paper.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figures/mainframe.pdf }
\caption{Overview of CodePhys. In Stage I, a codebook consisting of noise-free PPG features is learned by reconstructing GT-PPG signals, which is treated as the prior. In Stage II, the spatial-aware encoder (composed of a video feature extractor and a spatio-temporal encoder with an auxiliary prior branch) queries corresponding PPG features with respect to the input video. Subsequently, the decoder network reconstructs the estimated rPPG signal from the queried features. The embedding layer in Stage II is the same as that in Stage I. The DConv, TDC, Conv1D, and LN represent deformable convolution \cite{DaiDeformCNN2017}, temporal difference convolution \cite{YuAuto2020}, 1D convolution, and layer normalization, respectively.}
\label{fig:overview}
\end{figure*}
%--------------------------------------------