\section{Related Work}
\textbf{Affordance Detection.}
Originating from the 2D domain, initial work in affordance detection primarily focused on identifying objects with affordances~\citep{do2018affordancenet}. 
Building on this foundation, later studies~\citep{lu2022phrase} introduced linguistic descriptions to improve detection, but they continued to emphasize object-level affordances, lacking fine-grained analysis. 
Addressing this problem, subsequent research~\citep{chen2023affordance, li2023locate, luo2022learning, nagarajan2019grounded, mi2020intention} has focused on detecting specific affordance parts, establishing a new benchmark for precision in the field.
With the advancement of embodied AI, the scope of affordance learning has expanded into 3D domain.
3D AffordanceNet~\citep{deng20213daffordancenet} introduces the first benchmark dataset for learning affordance from object point clouds.
IAGNet~\citep{yang2023IAGNet} propose a setting for learning 3D affordance parts guided by image queries. 
Recently, some work~\citep{nguyen2023open} also explores the open-vocabulay affordance detection in point clouds.
However, these methods primarily focus on linking object geometric features with fixed affordance labels, overlooking the semantic aspect.
This limitation makes it challenging to understand natural language instructions and hampers the ability to generalize affordance detection to unseen scenarios. 
In contrast, the proposed 3D-ADLLM overcomes the limitations of fixed label sets and enhance the ability to comprehend semantic complex description. 
Specifically, we shift the detection paradigm from label-based semantic segmentation into Instruction Reasoning Affordance Segmentation (IRAS).


\textbf{3D Large Multi-Modal Models.}
3D object-level LMMs~\citep{yu2022pointbert,xue2023ulip,zhou2023uni3d} have successfully bridged the gap between 3D vision and text by leveraging large-scale 3D object datasets like~\citep{deitke2023objaverse,vishwanath2009modelnet}.
ShapeLLM~\citep{qi2024shapellm} further advances the embodied interaction and referring expression grounding through its novel and powerful point encoder.
However, despite these advances, such models still face challenges in interpreting complex spatial relationships within 3D scenes.
For scene-level LMMs, models like Chat-3D~\citep{wang2023chat3d} and LL3DA~\citep{chen2024ll3da} enable interaction with scene objects using pre-selection mechanisms. 
Building on this foundation, Chat-3D v2 ~\citep{huang2023chat3dv2} enhances referencing and grounding accuracy by incorporating object identifiers, while 3D-LLM ~\citep{hong20233dllm} improves scene comprehension by integrating positional embeddings and location tokens.
Unlike previous works that primarily focus on 3D grounding and understanding, our method introduces a specialized token, \texttt{<AFF>}, which enables LLMs to directly detect affordances and generate affordance masks within 3D open-world scene.