[
  {
    "index": 0,
    "papers": [
      {
        "key": "do2018affordancenet",
        "author": "Do, Thanh-Toan and Nguyen, Anh and Reid, Ian",
        "title": "Affordancenet: An end-to-end deep learning approach for object affordance detection"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lu2022phrase",
        "author": "Lu, Liangsheng and Zhai, Wei and Luo, Hongchen and Kang, Yu and Cao, Yang",
        "title": "Phrase-based affordance detection via cyclic bilateral interaction"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2023affordance",
        "author": "Chen, Joya and Gao, Difei and Lin, Kevin Qinghong and Shou, Mike Zheng",
        "title": "Affordance grounding from demonstration video to target image"
      },
      {
        "key": "li2023locate",
        "author": "Li, Gen and Jampani, Varun and Sun, Deqing and Sevilla-Lara, Laura",
        "title": "Locate: Localize and transfer object parts for weakly supervised affordance grounding"
      },
      {
        "key": "luo2022learning",
        "author": "Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng",
        "title": "Learning affordance grounding from exocentric images"
      },
      {
        "key": "nagarajan2019grounded",
        "author": "Nagarajan, Tushar and Feichtenhofer, Christoph and Grauman, Kristen",
        "title": "Grounded human-object interaction hotspots from video"
      },
      {
        "key": "mi2020intention",
        "author": "Mi, Jinpeng and Liang, Hongzhuo and Katsakis, Nikolaos and Tang, Song and Li, Qingdu and Zhang, Changshui and Zhang, Jianwei",
        "title": "Intention-related natural language grounding via object affordance detection and intention semantic extraction"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "deng20213daffordancenet",
        "author": "Deng, Shengheng and Xu, Xun and Wu, Chaozheng and Chen, Ke and Jia, Kui",
        "title": "3d affordancenet: A benchmark for visual object affordance understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yang2023IAGNet",
        "author": "Yang, Yuhang and Zhai, Wei and Luo, Hongchen and Cao, Yang and Luo, Jiebo and Zha, Zheng-Jun",
        "title": "Grounding 3d object affordance from 2d interactions in images"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "nguyen2023open",
        "author": "Nguyen, Toan and Vu, Minh Nhat and Vuong, An and Nguyen, Dzung and Vo, Thieu and Le, Ngan and Nguyen, Anh",
        "title": "Open-vocabulary affordance detection in 3d point clouds"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yu2022pointbert",
        "author": "Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen",
        "title": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling"
      },
      {
        "key": "xue2023ulip",
        "author": "Xue, Le and Gao, Mingfei and Xing, Chen and Mart{\\'\\i}n-Mart{\\'\\i}n, Roberto and Wu, Jiajun and Xiong, Caiming and Xu, Ran and Niebles, Juan Carlos and Savarese, Silvio",
        "title": "Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding"
      },
      {
        "key": "zhou2023uni3d",
        "author": "Zhou, Junsheng and Wang, Jinsheng and Ma, Baorui and Liu, Yu-Shen and Huang, Tiejun and Wang, Xinlong",
        "title": "Uni3d: Exploring unified 3d representation at scale"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "deitke2023objaverse",
        "author": "Deitke, Matt and Schwenk, Dustin and Salvador, Jordi and Weihs, Luca and Michel, Oscar and VanderBilt, Eli and Schmidt, Ludwig and Ehsani, Kiana and Kembhavi, Aniruddha and Farhadi, Ali",
        "title": "Objaverse: A universe of annotated 3d objects"
      },
      {
        "key": "vishwanath2009modelnet",
        "author": "Vishwanath, Kashi Venkatesh and Gupta, Diwaker and Vahdat, Amin and Yocum, Ken",
        "title": "Modelnet: Towards a datacenter emulation environment"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "qi2024shapellm",
        "author": "Qi, Zekun and Dong, Runpei and Zhang, Shaochen and Geng, Haoran and Han, Chunrui and Ge, Zheng and Yi, Li and Ma, Kaisheng",
        "title": "Shapellm: Universal 3d object understanding for embodied interaction"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2023chat3d",
        "author": "Wang, Zehan and Huang, Haifeng and Zhao, Yang and Zhang, Ziang and Zhao, Zhou",
        "title": "Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2024ll3da",
        "author": "Chen, Sijin and Chen, Xin and Zhang, Chi and Li, Mingsheng and Yu, Gang and Fei, Hao and Zhu, Hongyuan and Fan, Jiayuan and Chen, Tao",
        "title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding Reasoning and Planning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "huang2023chat3dv2",
        "author": "Huang, Haifeng and Wang, Zehan and Huang, Rongjie and Liu, Luping and Cheng, Xize and Zhao, Yang and Jin, Tao and Zhao, Zhou",
        "title": "Chat-3d v2: Bridging 3d scene and large language models with object identifiers"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hong20233dllm",
        "author": "Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang",
        "title": "3d-llm: Injecting the 3d world into large language models"
      }
    ]
  }
]