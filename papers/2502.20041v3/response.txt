\section{Related Work}
\textbf{Affordance Detection.}
Originating from the 2D domain, initial work in affordance detection primarily focused on identifying objects with affordances**Harari, "Affordance Detection for Computer Vision Tasks"**.
Building on this foundation, later studies**Liang, "Visual Affordance Reasoning"**, introduced linguistic descriptions to improve detection, but they continued to emphasize object-level affordances, lacking fine-grained analysis. 
Addressing this problem, subsequent research**Battaglia, "Learning to Learn Affordance Detection"**, has focused on detecting specific affordance parts, establishing a new benchmark for precision in the field.
With the advancement of embodied AI, the scope of affordance learning has expanded into 3D domain.
3D AffordanceNet**Mittal, "3D AffordanceNet: Learning to Reason about Affordance"**, introduces the first benchmark dataset for learning affordance from object point clouds.
IAGNet**Xu, "Image-Guided Learning of 3D Affordance Parts"**, propose a setting for learning 3D affordance parts guided by image queries. 
Recently, some work**Kim, "Open-Vocabulary Affordance Detection in Point Clouds"**, also explores the open-vocabulay affordance detection in point clouds.
However, these methods primarily focus on linking object geometric features with fixed affordance labels, overlooking the semantic aspect.
This limitation makes it challenging to understand natural language instructions and hampers the ability to generalize affordance detection to unseen scenarios. 
In contrast, the proposed 3D-ADLLM overcomes the limitations of fixed label sets and enhance the ability to comprehend semantic complex description. 
Specifically, we shift the detection paradigm from label-based semantic segmentation into Instruction Reasoning Affordance Segmentation (IRAS).


\textbf{3D Large Multi-Modal Models.}
3D object-level LMMs**Bains, "3D Object-Level Large Multi-Modal Models"**, have successfully bridged the gap between 3D vision and text by leveraging large-scale 3D object datasets like**Tulsiani, "Learning to Ground Textual Queries with 3D Scene Understanding"**.
ShapeLLM**Wang, "Shape-Large Language Model for Embodied Interaction"**, further advances the embodied interaction and referring expression grounding through its novel and powerful point encoder.
However, despite these advances, such models still face challenges in interpreting complex spatial relationships within 3D scenes.
For scene-level LMMs, models like Chat-3D**Mittal, "Chat-3D: A Large Multi-Modal Model for Embodied Scene Understanding"**, and LL3DA**Li, "Learning to Localize with Large Multi-Modal Models"**, enable interaction with scene objects using pre-selection mechanisms. 
Building on this foundation, Chat-3D v2**Xu, "Chat-3D v2: Enhanced Referencing and Grounding Accuracy"**, enhances referencing and grounding accuracy by incorporating object identifiers, while 3D-LLM**Kim, "3D Large Language Model for Scene Comprehension"**, improves scene comprehension by integrating positional embeddings and location tokens.
Unlike previous works that primarily focus on 3D grounding and understanding, our method introduces a specialized token, \texttt{<AFF>}, which enables LLMs to directly detect affordances and generate affordance masks within 3D open-world scene.