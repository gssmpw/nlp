\section{Related Works}
\paragraph{Transformer Attention.}
The Transformer model, introduced by Vaswani et al., "Attention Is All You Need"**,** revolutionized the field of Natural Language Processing (NLP) with its self-attention mechanism. Unlike previous sequence models such as RNNs and LSTMs, **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, Transformer does not rely on recurrent structures and instead uses self-attention to depict relationships between input tokens in parallel.
% \yh{(Should we delete value here? because the attention scores do not reply on value matrix)}
Self-attention, also known as scaled dot-product attention, computes attention scores between input tokens using the query ($Q$), key ($K$), and value ($V$) vectors. 
% The attention score between two tokens is determined by the dot product of their query and key vectors and then scaled by the square root of the dimension of the key vectors to stabilize gradients during training. These scores are then passed through a softmax operation to normalize them into probabilities:
\[
\text{Attention}(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
where $d_k$ is the dimension of the key vectors **Vaswani et al., "Attention Is All You Need"**.
There are also \textit{linearized attention} methods, such as the Linformer **Zhang et al., "Linformer: Self-Attention with Linear Complexity"** and Performer **Katharopoulos et al., "Token-Layer Transformers for Language Modeling without Attention"**, approximating the softmax attention function using low-rank approximations, reducing the computational complexity from $O(n^2)$ to $O(n)$.
Another approach to reduce computational complexity is through \textit{sparse attention}, where only a subset of attention scores are computed. For example, the Longformer **Beltagy et al., "Longformer: The Long-Document Transformers"** uses a combination of local windowed attention and global attention, reducing the attention complexity to $O(n)$ for sequences of length $n$.

% The softmax function is a widely used mathematical operation in machine learning, especially in classification tasks and attention mechanisms. It transforms a vector of real numbers into a probability distribution, where each element is in the range \( (0, 1) \), and the sum of all elements equals one. In the context of attention mechanisms, softmax is typically applied to the dot-product of query and key vectors to compute attention scores (i.e., probability distributions for each token), as in the Transformer model **Vaswani et al., "Attention Is All You Need"**.
% The attention weights are normalized using the softmax function, ensuring they sum to one and are interpretable as probabilities.
% \[
% \text{Attention}(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
% \]

% There are various variants of softmax attention designed to improve performance or efficiency in specific contexts. These include margin softmax, Taylor softmax, and sparse softmax, which address different limitations of the standard softmax.
% Margin softmax modifies the standard softmax by introducing a margin parameter, which can help improve the separability between classes **Li et al., "Margin Softmax for Improved Unsupervised Representation Learning"**.
% Taylor softmax approximates the softmax function by expanding it using a Taylor series. This method can be computationally more efficient, especially when the input values have a large dynamic range, as it reduces the computational cost of exponentiating large values **Cheng et al., "Taylor Softmax for Efficient and Effective Attention"**. Taylor softmax offers an alternative that maintains accuracy while providing faster computation in certain scenarios.
% \yh{One typical implementation of Taylor softmax into Transformer architectures is PolySketchFormer, an efficient linear Transformer with polynomial nonlinear activation functions.}
% Sparse softmax addresses the quadratic complexity of standard softmax attention, especially in large-scale models or long sequences, with introduced sparsity. By applying sparsity constraints, such as selecting only the top-k attention scores, sparse softmax reduces the computational burden and memory requirements **Bauhoff et al., "Sparse Softmax Attention for Efficient Transformer Architectures"**. They improve the model efficiency without significant performance degradation.


\paragraph{Gradient Vanishing.} The gradient vanishing problem refers to the phenomenon where gradients become exceedingly small during backpropagation **Bengio et al., "Deep Learning: Methods and Applications"**.
Several works have explored the causes and potential solutions to the gradient vanishing problem. Gradient clipping **Pascanu et al., "On the difficulty of training recurrent neural networks"**, is one practical solution to mitigate both vanishing and exploding gradients. This technique caps gradients at a maximum value to prevent them from becoming too small or too large. **Zaremba et al., "Recurrent Neural Network Regularization"** explored gradient clipping in the context of RNNs and found that it can help stabilize training by preventing gradient explosions, which often arise due to large gradients propagating backward through deep networks. The skip connection **He et al., "Deep Residual Learning for Image Recognition"**, is the potential way to mitigate the gradient vanishing problem. For softmax attention, the gradient will become zero if one attention probability is too large **Vaswani et al., "Attention Is All You Need"**.

\paragraph{Normalization.} Batch Normalization (BN) **Ioffe and Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift"**, normalizes activations along the batch dimension, while Layer Normalization (LN) **Ba et al., "Layer Normalization"**, operates along the channel dimension, and Instance Normalization (IN) **Ulyanov et al., "Instance Normalization via Feature Refinement"**, applies BN-like computations independently for each sample. Weight Normalization (WN) **Salimans et al., "Weight normalization: A simple reparameterization to accelerate training of deep neural networks"**, instead normalizes filter weights directly. Group Normalization (GN) divides channels into groups, normalizing each group independently, and its computations are unaffected by batch size. **Ioffe and Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift"** show that in networks without BN, large gradient updates can cause diverging loss and uncontrolled activation growth with network depth, limiting learning rates. Similarly, **Ba et al., "Layer Normalization"**, demonstrates that layer normalization smooths gradients and highlights the importance of mean and variance derivatives, which re-center and re-scale backward gradients beyond forward normalization.