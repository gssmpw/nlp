% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}


\input{math_commands}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{graphicx}

% \usepackage[colorlinks,linkcolor=red, anchorcolor=blue, citecolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{mathrsfs}
% \usepackage{amsmath,amsfonts,graphicx,amssymb,bm,amsthm, bbm}
\usepackage{color}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage{mathrsfs}
\usepackage{threeparttable}
\usepackage{comment}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[noabbrev,capitalise]{cleveref}
\usepackage{footnote}
\usepackage{subcaption}
\usepackage{cleveref} 
\usepackage{makecell}
\usepackage{listings}  % Code
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage[toc,page]{appendix}
\usepackage{float}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{listings} 

\usepackage{pifont}
\usepackage{xspace}
\newcommand{\methodFullName}{Self-Adjusting Softmax\xspace}
\newcommand{\methodShortName}{SA-Softmax\xspace}
\newcommand{\yh}[1]{{\color{blue}#1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Self-Adjust Softmax}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Chuanyang Zheng \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Yihang Gao \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Chuanyang Zheng\textsuperscript{1}},
 \textbf{Yihang Gao\textsuperscript{2}},
 \textbf{ Guoxuan Chen\textsuperscript{3}},
 \textbf{Han Shi\textsuperscript{4}},
\\
 \textbf{ Jing Xiong\textsuperscript{3}},
 \textbf{Xiaozhe Ren\textsuperscript{4}},
 \textbf{Chao Huang\textsuperscript{3}},
 \textbf{Xin Jiang \textsuperscript{4}},
\\
 \textbf{ Zhenguo Li\textsuperscript{4}},
 \textbf{ Yu Li\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
\\
 \textsuperscript{1}The Chinese University of Hong Kong,
 \textsuperscript{2}National University of Singapore\\
 \textsuperscript{3}The University of Hong Kong,
 \textsuperscript{4}Noah's Ark Lab
\\
 \small{
 \textbf{Code:} \href{https://github.com/chuanyang-Zheng/SA-Softmax}{https://github.com/chuanyang-Zheng/SA-Softmax}} \\
   \small{ \textbf{Contact:} \href{cyzheng21@link.cuhk.edu.hk}{cyzheng21@link.cuhk.edu.hk}
 }
}

\begin{document}
\maketitle
\begin{abstract}
The softmax function is crucial in Transformer attention, 
which normalizes each row of the attention scores with summation to one, achieving superior performances over other alternative functions.
However, the softmax function can face a gradient vanishing issue when some elements of the attention scores approach extreme values, such as probabilities close to one or zero.
In this paper, we propose \methodFullName (\methodShortName) to address this issue by modifying $softmax(x)$ to $x \cdot softmax(x)$ and its normalized variant $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$.
We theoretically show that \methodShortName provides enhanced gradient properties compared to the vanilla softmax function.
Moreover, \methodShortName Attention can be seamlessly integrated into existing Transformer models to their attention mechanisms with minor adjustments.
We conducted experiments to evaluate the empirical performance of Transformer models using \methodShortName compared to the vanilla softmax function. 
These experiments, involving models with up to 2.7 billion parameters, are conducted across diverse datasets, language tasks, and positional encoding methods.
\end{abstract}

\section{Introduction}
Transformer-based models \citep{vaswani2017attention} have delivered exceptional performances across widespread applications, including language processing \citep{zhang2020pegasus,guo2021longt5,ainslie2023colt5}, computer vision \citep{alexey2020image,touvron2021training,liu2021swin,chen2024pixartalpha,peebles2023scalable}, quantitative research \citep{zhou2024dont,liu2021finbert,wu2023bloomberggpt}, and scientific machine learning \citep{taylor2022galactica,geneva2022Transformers}. 
A critical component of the Transformer is its attention mechanism, 
which computes the importance and contribution of each token in a sequence for next-token generation.
%which computes the importance of each token in a sequence relative to others. 
Central to this mechanism is the softmax function, a mathematical operation that normalizes attention scores 
token-wise, ensuring a summation of one.
% row-wise to ensure they sum to 1.
This property facilitates probabilistic interpretability and enables a more expressive attention mechanism. For example, \citet{chen2024sepllm,xiao2024infllm} observed that most attention scores are usually concentrated on specific tokens, allowing for more efficient Transformer architectures by discarding tokens with lower accumulative attention scores. As a result, the normalized attention scores produced by softmax provide insights into the mechanism of next-token generation in LLMs. Moreover, compared to other attention functions, softmax exhibits some unique and advantageous properties, which contribute to the superior performance of softmax-based Transformer models~\citep{han2024bridging,deng2023superiority}.
% \yh{!!!Question: Really? softmax should be inefficient compared with linear attention. I think the most advantages of using softmax are its stabilized values (sum to one) and communication between tokens (attention scores involve other tokens). Maybe we need to rewrite this sentence. What about the following: This property facilitates probabilistic interpretability and enables a more expressive attention mechanism. For example, \citet{chen2024sepllm,xiao2024infllm} observed that most attention scores are usually concentrated on specific tokens, allowing for more efficient Transformer architectures by discarding tokens with lower accumulative attention scores. As a result, the normalized attention scores produced by softmax provide insights into the mechanism of next-token generation in LLMs. Moreover, compared to other attention functions, softmax exhibits some unique and advantageous properties, which contribute to the superior performance of softmax-based Transformer models~\citep{han2024bridging,deng2023superiority}.}

% Learning within a neural network is performed via gradient backpropagation, wherein gradients propagate backward through the network layer by layer using the chain rule \citep{lecun2002efficient}. During backpropagation, gradient magnitudes tend to diminish, resulting in a weaker gradient signal reaching the bottom layers and inefficient learning, which is called as vanishing gradient problem \citep{glorot2010understanding,bengio1994learning}.  Residual connections \citep{he2016deep} are used in Transformers so that gradients can bypass the layers via skip connections during backpropagation to improve the gradient magnitude for bottom layers, reinforcing the idea that architectures capable of efficient gradient backpropagation tend to offer better training performance.

% \yh{these sentences are a little bit strange, as relu and sigmoid are not acting as normalization.??? This paragraph may need careful rewriting. Let's do it later.}
One of the primary limitations of softmax lies in its susceptibility to the gradient vanishing problem. When input values to the softmax function become highly polarized, i.e., extreme values that are very large or small, the resulting probabilities can exhibit extreme sparsity. This, in turn, leads to gradients that approach zero, impeding effective learning and optimization during backpropagation. Such issues are particularly pronounced in deep architectures, where the accumulation of small gradients can hinder convergence and degrade model performance \cite{vaswani2017attention,duvvuri2024laser}.
Several variations have been proposed, including ReLU attention \cite{nair2010rectified,chen2020arelu,wortsman2023replacing,shen2023study} or sigmoid attention \cite{ramapuram2024theory}. These alternatives aim to address specific shortcomings of softmax, such as its sensitivity to extreme input values or its restricted output range, which may limit the behavior of the attention mechanism. However, these approaches often fall short of achieving comparable stability, interpretability, or general performance, especially in large-scale models where softmax continues to dominate due to its robustness and simplicity.





% Previous studies have highlighted the challenges of gradient saturation in neural networks and emphasized the importance of designing mechanisms to address these issues without compromising the functional benefits of softmax \cite{goodfellow2016deep, hochreiter1998vanishing}.
% \yh{what do these papers propose to address this issue?}

To address this limitation, we propose a novel modification to the softmax function, introducing \methodFullName (\methodShortName), which enhances gradient propagation while preserving the probabilistic properties and ranking order of traditional softmax. 
% Our method starts with $x \cdot softmax(x)$, intended to enlarge the gradient. This adjustment not only amplifies gradients in scenarios where they would typically diminish but also ensures that the attention distribution remains interpretable and stable. 
% Furthermore, to achieve improved convergence performance, we propose a normalized variant: $ \frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$, which incorporates normalization to stabilize training and improve the overall performance. 
Our approach builds on theoretical insights and empirical observations. First, we show theoretically that modifying the softmax function to $x \cdot softmax(x)$ amplifies gradient magnitudes, addressing gradient saturation under a range of typical conditions. Building on this formulation, we further refine the formulation to $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$, incorporating the normalization while enhancing gradient flow. It also maintains the relative ordering of input values, which serves as a critical property for the effectiveness of attention mechanisms. The proposed modification of the vanilla softmax function ensures compatibility with standard Transformer architectures and facilitates seamless integration into existing frameworks.
\begin{enumerate} 
    \item We propose $x \cdot softmax(x)$ as an alternative to the vanilla softmax in the attention mechanism to improve gradient magnitudes, thereby enhancing backpropagation during training.
    % and prove that final attention score could be negative (as shown in Appendix \ref{appendix: visualization attention}. 
    Additionally, we refine $x \cdot softmax(x)$ to $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$ with normalization, preserving a critical property of softmax while achieving superior performance. 
    
    \item We conduct extensive experiments across various datasets, tasks, and models, comparing the proposed \methodShortName and its variants with the standard $softmax(x)$. Results demonstrate that our approach effectively mitigates gradient vanishing and consistently improves performances across models with different scales. 
    \item We validate the proposed methods on large-scale pre-training datasets with a training length of 2048. Moreover, we also show the effectiveness of the proposed method in downstream tasks.
    \end{enumerate}



\section{Related Works}
\paragraph{Transformer Attention.}
The Transformer model, introduced by Vaswani et al. \cite{vaswani2017attention}, revolutionized the field of Natural Language Processing (NLP) with its self-attention mechanism. Unlike previous sequence models such as RNNs and LSTMs \cite{graves2012long}, Transformer does not rely on recurrent structures and instead uses self-attention to depict relationships between input tokens in parallel.
% \yh{(Should we delete value here? because the attention scores do not reply on value matrix)}
Self-attention, also known as scaled dot-product attention, computes attention scores between input tokens using the query ($Q$), key ($K$), and value ($V$) vectors. 
% The attention score between two tokens is determined by the dot product of their query and key vectors and then scaled by the square root of the dimension of the key vectors to stabilize gradients during training. These scores are then passed through a softmax operation to normalize them into probabilities:
\[
\text{Attention}(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
where $d_k$ is the dimension of the key vectors \cite{vaswani2017attention}.
There are also \textit{linearized attention} methods, such as the Linformer \cite{wang2020linformer} and Performer \cite{choromanski2020rethinking}, approximating the softmax attention function using low-rank approximations, reducing the computational complexity from $O(n^2)$ to $O(n)$.
Another approach to reduce computational complexity is through \textit{sparse attention}, where only a subset of attention scores are computed. For example, the Longformer \cite{beltagy2020longformer} uses a combination of local windowed attention and global attention, reducing the attention complexity to $O(n)$ for sequences of length $n$.

% The softmax function is a widely used mathematical operation in machine learning, especially in classification tasks and attention mechanisms. It transforms a vector of real numbers into a probability distribution, where each element is in the range \( (0, 1) \), and the sum of all elements equals one. In the context of attention mechanisms, softmax is typically applied to the dot-product of query and key vectors to compute attention scores (i.e., probability distributions for each token), as in the Transformer model \cite{vaswani2017attention}.
% The attention weights are normalized using the softmax function, ensuring they sum to one and are interpretable as probabilities.
% \[
% \text{Attention}(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
% \]

% There are various variants of softmax attention designed to improve performance or efficiency in specific contexts. These include margin softmax, Taylor softmax, and sparse softmax, which address different limitations of the standard softmax.
% Margin softmax modifies the standard softmax by introducing a margin parameter, which can help improve the separability between classes. This variant is particularly useful in classification problems where the model needs to better distinguish between closely related classes \cite{liu2017sphereface}.
% Taylor softmax approximates the softmax function by expanding it using a Taylor series. This method can be computationally more efficient, especially when the input values have a large dynamic range, as it reduces the computational cost of exponentiating large values \cite{choromanski2020performer}. Taylor softmax offers an alternative that maintains accuracy while providing faster computation in certain scenarios.
% \yh{One typical implementation of Taylor softmax into Transformer architectures is PolySketchFormer, an efficient linear Transformer with polynomial nonlinear activation functions.}
% Sparse softmax addresses the quadratic complexity of standard softmax attention, especially in large-scale models or long sequences, with introduced sparsity. By applying sparsity constraints, such as selecting only the top-k attention scores, sparse softmax reduces the computational burden and memory requirements \cite{child2019generating}. They improve the model efficiency without significant performance degradation.


\paragraph{Gradient Vanishing.} The gradient vanishing problem refers to the phenomenon where gradients become exceedingly small during backpropagation \cite{lillicrap2020backpropagation}.
Several works have explored the causes and potential solutions to the gradient vanishing problem. Gradient clipping \cite{zhang2019gradient} is one practical solution to mitigate both vanishing and exploding gradients. This technique caps gradients at a maximum value to prevent them from becoming too small or too large. \citet{pascanu2013difficulty} explored gradient clipping in the context of RNNs and found that it can help stabilize training by preventing gradient explosions, which often arise due to large gradients propagating backward through deep networks. The skip connection \cite{he2016deep} is the potential way to mitigate the gradient vanishing problem. For softmax attention, the gradient will become zero if one attention probability is too large \cite{vaswani2017attention}.

\paragraph{Normalization.} Batch Normalization (BN) \cite{ioffe2015batch} normalizes activations along the batch dimension, while Layer Normalization (LN) \cite{ba2016layer} operates along the channel dimension, and Instance Normalization (IN) \cite{huang2017arbitrary} applies BN-like computations independently for each sample. Weight Normalization (WN) \cite{salimans2016weight} instead normalizes filter weights directly. Group Normalization (GN) divides channels into groups, normalizing each group independently, and its computations are unaffected by batch size. \citet{bjorck2018understanding} show that in networks without BN, large gradient updates can cause diverging loss and uncontrolled activation growth with network depth, limiting learning rates. Similarly, \citet{xu2019understanding} demonstrates that layer normalization smooths gradients and highlights the importance of mean and variance derivatives, which re-center and re-scale backward gradients beyond forward normalization.




\section{Method}
% The softmax attention mechanism is a core component of transformer models, allowing them to communicate across different tokens in a sequence according to their relevance. Originally introduced in the context of the transformer architecture by Vaswani et al. \cite{vaswani2017attention}, this mechanism has revolutionized natural language processing by enhancing models' ability to capture long-range dependencies in sequential data. 
% The effectiveness of softmax attention lies in its ability to compute a dynamic weighting of input features, which has been foundational in many transformer-based models \cite{devlin2018bert, brown2020language}. 
% \yh{For example, it has been theoretically and experimentally verified that the associate recall, one of the key advantages of transformer models over RNN-based models, results from the softmax attention mechanism. Additionally, \cite{} emphasized that the attention mechanism plays a pivotal role in enabling the in-context learning capabilities of transformer models.}

% The softmax attention mechanism is a core component of transformer models, allowing them to weigh different elements in a sequence according to their relevance. Originally introduced in the context of the transformer architecture by Vaswani et al. \cite{vaswani2017attention}, this mechanism has revolutionized natural language processing by improving models' ability to capture long-range dependencies in sequential data. The effectiveness of softmax attention lies in its ability to compute a dynamic weighting of input features, which has since been foundational in many transformer-based models \cite{devlin2018bert, brown2020language}. 

% In this section, we explore the mathematical formulation of softmax attention, provide a detailed analysis of its gradient computations, and discuss challenges such as gradient vanishing, along with proposed strategies to mitigate such issues.

\subsection{Softmax Attention Mechanism}
In the attention mechanism, the weight \( \alpha_{ij} \) represents the attention score between token $i$ (the query) and token $j$ (the key). This score quantifies the relative importance of token $j$ to token $i$, among all tokens in the input sequence. It is formulated as

\begin{equation}
\alpha_{ij} = softmax\left(\frac{q_i^T k_j}{\sqrt{d_k}}\right) = \frac{\exp\left(\frac{q_i^T k_j}{\sqrt{d_k}}\right)}{\sum_{j'} \exp\left(\frac{q_i^T k_{j'}}{\sqrt{d_k}}\right)},
\end{equation}
where \( q_i \) and \( k_j \) are the query and key vectors for tokens $i$ and $j$, respectively, and \( d_k \) is a scaling factor based on the dimensionality of the keys \cite{vaswani2017attention}. 
The softmax function ensures that the resulting attention scores \( \alpha_{ij} \) are normalized and can be interpreted as probabilities, summing to one over all tokens $j$ for a given query token $i$.

The final output of the attention mechanism for each query token \( i \) is then calculated as a weighted sum of the values \( v_j \) corresponding to each token $j$ in the sequence, with the weight determined by the attention scores \( \alpha_{ij} \). The output of the attention mechanism for token $i$ is defined as
\begin{equation}
\text{Attention}_{i}(Q, K, V) = \sum_j \alpha_{ij} v_j,
\end{equation}
where \( Q \), \( K \), and \( V \) are matrices representing all queries, keys, and values for a given sequence. This approach allows the model to focus selectively on parts of the sequence that contribute meaningfully to the current query position \cite{bahdanau2014neural}. 

\subsection{Gradient of Softmax Attention}
Training Transformer models involves updating all trainable parameters using their gradients. The backpropagation process, which relies on the chain rule, requires the computation of the derivative of the softmax function with respect to its inputs. However, when the input values to the softmax function become extremely large or small, the function can enter flat regions. This results in vanishing gradients, which can hinder the efficient training of model parameters.


% For training transformer models, it is essential to calculate the gradient of the attention mechanism with respect to its inputs, which enables backpropagation through the model. The softmax functionâ€™s gradient is crucial here, as it defines how changes in the attention weights impact the overall model loss \cite{goodfellow2016deep}.

We denote the pre-softmax attention scores (i.e., the input to the softmax function before normalization) as
\begin{equation}
z_{i,j} = \frac{q_i^T k_j}{\sqrt{d_k}},
\end{equation}
then the derivative of the output attention scores (after passing through the softmax function) with respect to the input \( z_{i,j} \) admits
\begin{equation}
\label{eq_derivatives}
    \begin{split}
        & \frac{\partial \alpha_{ij}}{\partial z_{i,j}} = \alpha_{ij} (1 - \alpha_{ij}),\\
        & \frac{\partial \alpha_{ij}}{\partial z_{i,j^{\prime}}} = -\alpha_{ij} \alpha_{ij^{\prime}}, \quad \text{for } j^{\prime} \neq j.
    \end{split}
\end{equation}
% \yh{!!!This paragraph is a little bit strange... The attention weight depends not only on its own value but also on all other weights should be reflected by Equation (1), not Equation (4).}

This Jacobian matrix structure implies that each attention weight depends not only on its own value but also on the values of all other weights. This property, while beneficial for capturing complex relationships, can also make optimization challenging in some scenarios, as explored in the next section.

\subsection{Gradient Vanishing in Softmax Attention}

One notable issue with softmax attention is the vanishing gradient problem, especially when attention scores become highly peaked. When the softmax output approaches 1 for a specific score and 0 for others, the gradients can become excessively small, slowing down or even halting learning. This is particularly problematic in deeper models where multiple layers of attention are stacked.

The vanishing gradient issue arises from the form of the softmax derivatives. Let us examine the two cases:
Consider the token $i$ in the attention mechanism,
and let $z_{i,j}$ and $\alpha_{i,j}$ represent the attention scores of all tokens relative to token $i$, for $j = 1,2,\ldots,T$. In the extreme case where one of the attention weights dominates, i.e., $\alpha_{i,j^{*}} \approx 1$ and $\alpha_{i,j} \approx 0$, for $j \neq j^{*}$. Then Equation \ref{eq_derivatives} implies that $\frac{\partial \alpha_{i,j}}{\partial z_{i,j^{\prime}}} \approx 0$ for all $j, j^{\prime}=1,2,\ldots, T$.
This result indicates that, under such circumstances, the derivative of the output attention weights with respect to the input pre-softmax attention scores vanishes, leading to gradient vanishing across all tokens.
Moreover, in a milder case where $\alpha_{i,j} \approx 0$ holds for some $j$, we have $\frac{\partial \alpha_{i,j}}{\partial z_{i,j^{\prime}}} \approx 0$ and $\frac{\partial \alpha_{i,j^{\prime}}}{\partial z_{i,j}} \approx 0$ for $j^{\prime} = 1,2,\ldots,T$. This means that the derivative of the softmax function partially vanishes, if input and output correspond to $\alpha_{i,j}$ and $z_{i,j}$ of token $j$, resulting in gradient vanishing for those specific tokens. 




In summary, when the extreme case arises where one attention score dominates while others approach zero, the softmax mechanism suffers from complete gradient vanishing for all tokens, leading to slow training and failure in gradient backpropagation. In the milder case, where some attention scores are close to zero, the derivatives associated with these tokens and their attention scores still vanish, causing suboptimal training performance.


The extreme case, where some attention scores approach zero, frequently occurs in attention mechanisms due to the exponential function's sensitivity to large values. In the following section, we first introduce a modification to the vanilla softmax, called \methodFullName (\methodShortName), which is theoretically guaranteed to enhance and amplify gradient propagation. Additionally, we propose several variants of \methodShortName, designed to further improve the effectiveness and stability of Transformer models by incorporating normalization techniques.



% \paragraph{Gradient for \( \frac{\partial \alpha_{ij}}{\partial z_{i,j}} \):}
% The term \( \alpha_{ij}(1 - \alpha_{ij}) \) is maximized when \( \alpha_{ij} = 0.5 \), resulting in a gradient magnitude of \( 0.25 \). However, as \( \alpha_{ij} \) approaches 0 or 1, the term \( \alpha_{ij}(1 - \alpha_{ij}) \) approaches 0, leading to a vanishing gradient.

% \paragraph{Gradient for \( \frac{\partial \alpha_{ij}}{\partial z_{i,j'}} \):}
% For \( j' \neq j \), the gradient is proportional to \( -\alpha_{ij} \alpha_{ij'} \). When \( \alpha_{ij} \) becomes very large (close to 1), other attention weights \( \alpha_{ij'} \) will necessarily be small (close to 0). Thus, the product \( \alpha_{ij} \alpha_{ij'} \) becomes negligibly small, further reducing the gradient magnitude.


%These observations imply that when one attention score dominates (i.e., \( z_{i,j} \) is much larger than the others), the softmax function produces highly peaked outputs where one weight is close to 1 and all others are close to 0. In this regime, the gradients of the softmax function with respect to its inputs become extremely small, leading to \textit{gradient vanishing}.

% Alternative gradient formulations have been proposed to address this. One approach is to apply a modified scaling factor in the softmax input to reduce peaking. Another promising solution is to use \( x \cdot softmax(x) \), which maintains larger gradients by scaling the softmax output linearly with \( x \):
% \begin{equation}
% \frac{\partial \alpha_{ij}}{\partial z_{i}} = z_i \cdot softmax(z_i)
% \end{equation}
% Another approach is to rescale the input values using a normalized factor such as \( \frac{(x - x_{\text{min}})}{x - x_{\text{max}}} \cdot softmax(x) \), which has shown effectiveness in mitigating gradient vanishing by ensuring input values are balanced within a fixed range \cite{sun2021artificial}.

% The softmax attention mechanism is an essential component of modern transformer architectures. While its use has led to significant advances in machine learning, particularly in NLP, the vanishing gradient problem presents a significant challenge. Advanced formulations, such as rescaling techniques, offer promising avenues to maintain gradient flow and improve learning efficiency in deep models. Future research may further refine these approaches to address gradient challenges in attention-based models.

\subsection{\methodFullName}
To address the issue of potential gradient vanishing of the softmax function, we propose modifying the attention mechanism by scaling the softmax output with its input, called \methodFullName (\methodShortName). Specifically, we redefine the output of attention scores as follows:
\begin{equation}
\beta_{i,j} = z_{i,j} \cdot softmax(z_{i,j}),
\end{equation}
where \( z_{i,j} \) is the pre-softmax attention score of the token $i$ corresponding to the token $j$. This modification introduces an additional scaling term \( z_{i,j} \) to calculate the final attention scores besides the standard softmax function, amplifying the gradient propagation compared to the original formulation.


\paragraph{Gradient Analysis of \methodShortName.}
Let us evaluate the gradient of the modified attention scores \( \beta_{i,j} \) with respect to the input \( z_{i,j^{\prime}} \). Differentiating \( \beta_{i,j} = z_{i,j} \cdot softmax(z_{i,j}) \) with respect to \( z_{i,j^{\prime}} \), we have

\begin{equation}
\label{eq:sa-softmax i=j}
\begin{split}
    \frac{\partial \beta_{i,j}}{\partial z_{i,j}} & = softmax(z_{i,j}) + z_{i,j} \cdot \frac{\partial softmax(z_{i,j})}{\partial z_{i,j}}\\
    & = \alpha_{i,j} + z_{i,j} \cdot \alpha_{i,j} (1- \alpha_{i,j}),
\end{split}
\end{equation}
and
\begin{equation}
\label{eq:sa-softmax i/=j}
    \begin{split}
        \frac{\partial \beta_{i,j}}{\partial z_{i,j^{\prime}}} & = z_{i,j^{\prime}} \cdot \frac{\partial softmax(z_{i,j})}{\partial z_{i,j^{\prime}}}\\
        & = - z_{i,j} \cdot \alpha_{i,j} \alpha_{i,j^{\prime}},
    \end{split}
\end{equation}
with $j^{\prime} \neq j$.
% The gradient of the softmax function itself is given by:
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_i} = \alpha_i (1 - \alpha_i) \quad \text{for } i = j
% \end{equation}
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_{i'}} = -\alpha_i \alpha_{i'} \quad \text{for } i \neq j
% \end{equation}
% Thus, the modified gradient for \( \beta_i \) becomes:
% \begin{equation}
% \frac{\partial \beta_i}{\partial x_i} = \alpha_i + x_i \cdot \alpha_i (1 - \alpha_i)
% \end{equation}
% \begin{equation}
% \frac{\partial \beta_i}{\partial x_{i'}} = -x_i \cdot \alpha_i \alpha_{i'}
% \end{equation}

\paragraph{Implications for Gradient Vanishing.}
According to Equation \ref{eq:sa-softmax i=j}, considering the extreme case where $\alpha_{i,j^{*}} \approx 1$, the gradient is amplified as the first term $\alpha_{i,j^{*}}$ is dominant and governs the gradient. Moreover, for tokens where $\alpha_{i,j} \approx 0$, the gradient is enhanced by the dynamic and self-adjusting scaler $z_{i,j}$, as demonstrated in Equations \ref{eq:sa-softmax i=j} and \ref{eq:sa-softmax i/=j}. Therefore, our method significantly enhances the gradient propagation for tokens with $\alpha_{i,j^{\prime}} \approx 1$ and improves the gradient for tokens satisfying $\alpha_{i,j} \approx 0$ through the dynamic and self-adjusting scalers.
\paragraph{ Comparison with Standard Softmax.}
In the standard softmax attention, the gradient softmax output \( \alpha_{i,j} \) with respect to the input \( z_{i,j} \) tends to vanish when \( \alpha_{i,j} \) approaches 0 or 1. By introducing an additional self-adjusting term in the attention score computation (i.e., modufying $softmax(x)$ to \( x \cdot softmax(x) \)), we allow for a more resilient gradient. As shown in Equations \ref{eq:sa-softmax i=j} and \ref{eq:sa-softmax i/=j}, this approach may not completely eliminate the gradient vanishing problem, it significantly mitigates its effects, especially in cases with long sequences or deep networks, where gradients from softmax attention typically diminish~\cite{vaswani2017attention}.





\subsection{Variants of \methodShortName}
In this section, we further develop some variants of \methodShortName, by utilizing normalization techniques on the self-adjusting term, to further stablize the training process. 

\paragraph{Variant 1: \( (x - x_{\min}) \cdot softmax(x) \)}
% However, $x*softmax(x)$ approach has an inconsistency: since the softmax normalization over \( x_i \) is not linear, larger values of \( x_i \) do not necessarily yield proportionately larger values of \( \beta_i \). As a result, the scaling effect intended to amplify gradients can be unpredictable, particularly when there is a wide range of \( x_i \) values.
A notable potential inconsistency in \methodShortName arises from the negative attention scores, which can lead to unpredictable and difficult-to-interpret behavior in the attention mechanism.

To address this issue, we propose a modified approach that shifts the self-adjusting term by its minimum value along the sequence. Specifically, we reformulate the attention computation as \( (x - x_{\min}) \cdot softmax(x) \), where \( x_{\min} \) represents the minimum value of \( x \) across the sequence. This modification ensures that all attention scores are non-negative, thereby stabilizing the scaling effect across different \( x_i \):
\begin{equation}
\gamma_{i,j} = (z_{i,j} - z_{i,\min}) \cdot softmax(z_{i,j})
\end{equation}
where $z_{i,\min}:=\min\{z_{i,j}: j=1,2,\ldots,T\}$ denotes the minimum values of $z_{i,j}$ along the sequence.
This adjustment enhances the robustness of the attention mechanism by ensuring consistency and stability in the scaling of attention scores.
% This adjustment provides a more stable gradient and mitigates the issue where larger \( x_i \) values did not necessarily produce larger outputs.
% \paragraph{Gradient Analysis of \( (x - x_{\min}) \cdot softmax(x) \)}
% Let us derive the gradient of \( \gamma_i \) with respect to \( x_i \). To make it easy for analysis, we do not consider the backpropagation for $x_{\min}$. Differentiating \( \gamma_i = (x_i - x_{\min}) \cdot softmax(x_i) \) with respect to \( x_i \), we get:
% \begin{equation}
% \frac{\partial \gamma_i}{\partial x_i} = softmax(x_i) + (x_i - x_{\min}) \cdot \frac{\partial softmax(x_i)}{\partial x_i}
% \end{equation}
% The gradient of the softmax function with respect to \( x_i \) is given by:
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_i} = \alpha_i (1 - \alpha_i) \quad \text{for } i = j
% \end{equation}
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_{i'}} = -\alpha_i \alpha_{i'} \quad \text{for } i \neq j
% \end{equation}
% Substituting these, the modified gradient for \( \gamma_i \) becomes:
% \begin{equation}
% \frac{\partial \gamma_i}{\partial x_i} = \alpha_i + (x_i - x_{\min}) \cdot \alpha_i (1 - \alpha_i)
% \end{equation}
% \begin{equation}
% \frac{\partial \gamma_i}{\partial x_{i'}} = -(x_{i'} - x_{\min}) \cdot \alpha_i \alpha_{i'}
% \end{equation}
% \paragraph{Implications for Gradient Stability}
% The term \( (x_i - x_{\min}) \cdot \alpha_i (1 - \alpha_i) \) in the gradient effectively amplifies the gradient more consistently across a range of \( x_i \) values. This formulation ensures that larger \( x_i \) values yield proportionately larger \( \gamma_i \) values, maintaining more stable gradients. As a result, this approach offers a more reliable means of preserving gradient magnitude, thereby facilitating learning in deep networks or models with long sequences.
% \paragraph{Comparison with Standard Softmax and \( x \cdot softmax(x) \)}

% In contrast to the standard softmax, which suffers from gradient vanishing, and \( x \cdot softmax(x) \), which has inconsistent scaling, our proposed \( (x - x_{\min}) \cdot softmax(x) \) formulation provides a balanced solution. By shifting \( x \) by \( x_{\min} \), the proposed approach achieves a consistent scaling effect, promoting stable gradients without altering the relative ranking of attention scores.

\paragraph{Variant 2: \( \frac{x - x_{\min}}{x_{\max}-x_{\min}} \cdot softmax(x) \)}
The first variant introduced a shift in the self-adjusting term to ensure the non-negativity of attention scores. Building on this idea, a more widely used technique is normalization. To further stabilize training, we normalize the self-adjusting term to $ \frac{x - x_{\min}}{x_{\max}-x_{\min}} \in [0,1]$. Therefore, the attention scores are calculated as follows:
\begin{equation}
\delta_{i,j} = \frac{z_{i,j} - z_{i,\min}}{z_{i,\max} - z_{i,\min}} \cdot softmax(z_{i,j}),
\end{equation}
where \( z_{i,\max} = \max\{z_{i,j}:j=1,2,\ldots,T\} \) denotes the maximum value of $z_{i,j}$ along the sequence, and $z_{i,\min}:=\min\{z_{i,j}: j=1,2,\ldots,T\}$ represents the minimum value. This normalization ensures that the self-adjusting term \( \frac{(x - x_{\min})}{x_{\max} - x_{\min}} \) lies within the bounded region \([0, 1]\), resulting in a stable scaling effect across different input distributions.
% \paragraph{Gradient Analysis of \( \frac{(x - x_{\min})}{x_{\max} - x_{\min}} \cdot softmax(x) \)}
% Let us examine the gradient of \( \delta_i \) with respect to \( x_i \). Differentiating \( \delta_i = \frac{(x_i - x_{\min})}{x_{\max} - x_{\min}} \cdot softmax(x_i) \) with respect to \( x_i \), we get:
% \begin{align}
% \frac{\partial \delta_i}{\partial x_i} = \frac{1}{x_{\max} - x_{\min}} \cdot softmax(x_i) \\ +  \frac{(x_i - x_{\min})}{x_{\max} - x_{\min}} \cdot \frac{\partial softmax(x_i)}{\partial x_i}
% \end{align}
% The gradient of softmax with respect to \( x_i \) is given by:
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_i} = \alpha_i (1 - \alpha_i) \quad \text{for } i = j
% \end{equation}
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_{i'}} = -\alpha_i \alpha_{i'} \quad \text{for } i \neq j
% \end{equation}
% Substituting these, we obtain the modified gradient for \( \delta_i \):
% \begin{equation}
% \frac{\partial \delta_i}{\partial x_i} = \frac{\alpha_i}{x_{\max} - x_{\min}} + \frac{(x_i - x_{\min})}{x_{\max} - x_{\min}} \cdot \alpha_i (1 - \alpha_i)
% \end{equation}
% \begin{equation}
% \frac{\partial \delta_i}{\partial x_{i'}} = -\frac{(x_{i'} - x_{\min})}{x_{\max} - x_{\min}} \cdot \alpha_i \alpha_{i'}
% \end{equation}
This formulation provides a stable gradient computation, as the adjusting term, normalized by \( x_{\max} - x_{\min} \), prevents excessively large values and ensures all values falling within a bounded range.

% \paragraph{Implications for Model Optimization}
% By normalizing \( x - x_{\min} \) to \([0, 1]\), we ensure that the scaled softmax values are more consistent across different batches and inputs. This normalization facilitates model optimization by keeping gradients within a controlled range, leading to more stable training and improved gradient flow across network layers.

% \paragraph{Comparison with Previous Methods}
% Compared to \( x \cdot softmax(x) \) and \( (x - x_{\min}) \cdot softmax(x) \), our proposed normalization-based scaling provides additional stability by maintaining a fixed range for the scaled input. This approach reduces the variability in gradient magnitudes and helps address cases where attention scores have a broad range, further improving the model's training dynamics \cite{goodfellow2016deep}.


\paragraph{Variant 3: \( \frac{x - min(x_{\min},0)}{max(0,x_{\max})-min(x_{\min},0)} \cdot softmax(x) \)}
To make it easier for the model optimization, we add a threshold to further normalize the  $ \frac{(x - x_{\min})}{x_{max}-x_{min}}$ $x - x_{\min}$ to $ \frac{x - min(x_{\min},0)}{max(0,x_{\max})-min(x_{\min},0)} \in$ [0,1].
\begin{equation}
\delta_i = \frac{x - min(x_{\min},0)}{max(0,x_{\max})-min(x_{\min},0)} \cdot softmax(x_i)
\end{equation}
where \( x_{\max} = \max(x) \) and \( x_{\min} = \min(x) \). When the $x$ becomes positive and \( x_{\max} - x_{\min} \gg 0 \)  ,  $\frac{x - min(x_{\min},0)}{max(0,x_{\max})-min(x_{\min},0)}$ will close to 1 so that the $\frac{x - min(x_{\min},0)}{max(0,x_{\max})-min(x_{\min},0)} \cdot softmax(x_i)$ degrades to $softma(x)$.
% This normalization ensures that \( \frac{(x - x_{\min})}{x_{\max} - x_{\min}} \) lies within \([0, 1]\), providing a controlled scaling effect across different input distributions.
% \paragraph{Gradient Analysis of \( \frac{(x - x_{\min})}{x_{\max} - x_{\min}} \cdot softmax(x) \)}
% Let us examine the gradient of \( \delta_i \) with respect to \( x_i \). Differentiating \( \delta_i = \frac{(x_i - x_{\min})}{x_{\max} - x_{\min}} \cdot softmax(x_i) \) with respect to \( x_i \), we get:
% \begin{align}
% \frac{\partial \delta_i}{\partial x_i} = \frac{1}{x_{\max} - x_{\min}} \cdot softmax(x_i) \\ +  \frac{(x_i - x_{\min})}{x_{\max} - x_{\min}} \cdot \frac{\partial softmax(x_i)}{\partial x_i}
% \end{align}
% The gradient of softmax with respect to \( x_i \) is given by:
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_i} = \alpha_i (1 - \alpha_i) \quad \text{for } i = j
% \end{equation}
% \begin{equation}
% \frac{\partial softmax(x_i)}{\partial x_{i'}} = -\alpha_i \alpha_{i'} \quad \text{for } i \neq j
% \end{equation}
% Substituting these, we obtain the modified gradient for \( \delta_i \):
% \begin{equation}
% \frac{\partial \delta_i}{\partial x_i} = \frac{\alpha_i}{x_{\max} - x_{\min}} + \frac{(x_i - x_{\min})}{x_{\max} - x_{\min}} \cdot \alpha_i (1 - \alpha_i)
% \end{equation}
% \begin{equation}
% \frac{\partial \delta_i}{\partial x_{i'}} = -\frac{(x_{i'} - x_{\min})}{x_{\max} - x_{\min}} \cdot \alpha_i \alpha_{i'}
% \end{equation}
% This normalized formulation provides a stable gradient, as the division by \( x_{\max} - x_{\min} \) prevents excessively large values and ensures all values fall within a consistent range.

\section{Experiment}
\paragraph{Datasets.} We use the Arxiv and Books3 dataset for our experiments. The training is conducted using a batch size of 512 or 1024 sequences, each with a sequence length from length 128 to length 2048. The models are trained for 50000 iterations. Throughout the training process, we monitor both the training and gradient. We also evaluate our methods in downstream datasets, such as sequence classification and machine translation.

\paragraph{Experiment Setting.} We begin by conducting experiments on the Arxiv and Books datasets, evaluating model performance across training sequence lengths ranging from 128 to 1024 with various positional encodings. Next, we validate our method on models of varying scales, from 125M to 2.7B parameters. Following this, we analyze the performance of different model variants and assess their ability to extrapolate to longer sequence lengths. Subsequently, we further validate the method on downstream tasks, including text classification and machine translation. Lastly, we visualize the gradient behavior across different methods to provide deeper insights into their effectiveness. The experiment setting details are presented in Appendix \ref{appendix: experiment setting}. By default, we use the \( \frac{x - min(x_{\min},0)}{max(0,x_{\max})-min(x_{\min},0)} \cdot softmax(x) \).

% \subsection{Compare with Baseline Performance}
% \begin{table}[htb]
% \caption{The perplexity on Books dataset with training length 512, \textbf{RoPE position encoding}, compared to baselines.}
% \centering
% \resizebox{0.45\textwidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% \textbf{Datasets}&\textbf{Method} & \textbf{128} & \textbf{512} & \textbf{1024}  \\ \midrule
% Arxiv& Baseline &  & 6.70 & 5.93\\ 

% Arxiv& \methodShortName & 14.62 & 6.63& 5.90 \\
% \midrule
% Books& Baseline &89.60 & 38.29 & 30.53 \\
% Books& \methodShortName & 89.36 & 37.57 & 30.29 \\
% \bottomrule
% \end{tabular}
% }
% \label{table:compare_with_baseline_rope}
% \end{table}
% \paragraph{Effectiveness across sequence lengths} The \methodShortName demonstrates notable improvements in perplexity over the baseline method across a range of sequence lengths, as illustrated in Table \ref{table
% }. In the Arxiv dataset, the \methodShortName achieved perplexity scores of 14.62, 6.63, and 5.90 at sequence lengths of 128, 512, and 1024, respectively, showcasing a clear advantage as sequence length increases. Notably, the perplexity score difference between the baseline and \methodShortName decreases as the sequence length grows, indicating that \methodShortName efficiently handles longer input sequences with improved predictive accuracy. For example, the perplexity reduction from the baseline at length 1024 is a significant improvement, with \methodShortName outperforming the baseline by 0.03 in perplexity. This trend is consistent across the Books dataset as well, where \methodShortName outperformed the baseline with scores of 89.36, 37.57, and 30.29 at lengths of 128, 512, and 1024, respectively.
% These results indicate that \methodShortName maintains stability in predictive performance, which is essential for tasks that require maintaining coherence and accuracy over longer inputs.

% \paragraph{Performance across datasets} In addition to performing well across sequence lengths, \methodShortName exhibits strong generalization across different datasets. For both the Arxiv and Books datasets, the \methodShortName model consistently outperformed the baseline in terms of perplexity. This robustness across datasets indicates that \methodShortName effectively captures context and patterns unique to each dataset, contributing to a lower overall perplexity. For instance, on the Books dataset, the baseline perplexity at a sequence length of 512 is 38.29, while \methodShortName achieves a reduced perplexity of 37.57. At a sequence length of 1024, the baseline achieves a perplexity of 30.53, whereas \methodShortName further reduces this to 30.29, highlighting its adaptability to the complexity of the Books dataset.
% For the Arxiv dataset, the perplexity scores follow a similar trend, where the baseline method performs with a perplexity of 6.70 at length 512, compared to \methodShortName's lower score of 6.63. The improvements extend to the 1024 token length, where \methodShortName achieves a perplexity of 5.90, compared to the baseline's 5.93. Although the gains appear modest numerically, these reductions in perplexity signify more precise language modeling in complex academic datasets like Arxiv, where precise contextual understanding and terminology are essential.



\subsection{Compare with Baseline Performance}
\begin{table}[htb]
\caption{The perplexity on Arxiv and Books dataset with different position encodings.}
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
\textbf{Data}&\textbf{PE}&\textbf{\methodShortName} & 128 & 512 & 1024  \\ \midrule
Arxiv&Kerple& \ding{53} &   14.61&6.70 & 5.47\\ 
Arxiv&Kerple& \ding{51} & 14.51 & 6.66& 5.44 \\ 
Arxiv&FIRE& \ding{53} &   14.76&6.67 & 5.43\\ 
Arxiv&FIRE& \ding{51} & 14.46 & 6.59& 5.38 \\ 
Arxiv&RoPE& \ding{53} &   14.86&6.70 & 5.52\\ 
Arxiv&RoPE& \ding{51} & 14.62 & 6.63& 5.49 \\ 
Arxiv&DAPEV2-Kerple& \ding{53} &   14.27&6.63 & 5.26\\ 
Arxiv&DAPEV2-Kerple& \ding{51} & \textbf{14.10} & \textbf{6.36}& \textbf{5.20} \\ 
\midrule
Books&Kerple &\ding{53} &88.88 & 38.46 & 28.65 \\  
Books&Kerple& \ding{51} &87.56 & 37.95 & 28.37\\ 
Books&FIRE &\ding{53} &88.48 & 38.12 & 28.57 \\  
Books&FIRE& \ding{51} &87.98 & 37.34 & 28.00\\ 
Books&RoPE &\ding{53} &89.60 & 38.29 & 30.53 \\  
Books&RoPE& \ding{51} &89.36 & 37.57 & 30.29\\ 
Books&DAPEV2-Kerple &\ding{53} &84.33 & 36.25 & 26.86\\  
Books&DAPEV2-Kerple &\ding{51} &\textbf{83.63}&\textbf{35.93} &\textbf{26.56} \\  
\bottomrule
\end{tabular}
}
\label{table: different_pe}
\end{table} 

\paragraph{The \methodShortName could improve performance improvements across different position encodings.}
The results in Table \ref{table: different_pe} highlight the effectiveness of \methodShortName in improving perplexity for Kerple, FIRE, RoPE and DAPEV2-Kerple. Without \methodShortName (\ding{53}), RoPE achieves perplexities of 89.60, 38.29, and 30.53 for sequence lengths 128, 512, and 1024, respectively. With \methodShortName (\ding{51}), these values drop to 89.36, 37.57, and 30.29, showcasing its contribution. DAPEV2-Kerple exhibits more significant improvements, with perplexities dropping from 84.33, 36.25, and 26.86 to 83.63, 35.93, and 26.56 across the respective sequence lengths when \methodShortName is applied. This demonstrates the universal applicability of \methodShortName to enhance position encoding methods.

\paragraph{DAPEV2-Kerple achieves the best performance, especially with \methodShortName.}
Among all tested configurations, DAPEV2-Kerple combined with \methodShortName yields the lowest perplexity scores, outperforming both the baseline RoPE and RoPE with \methodShortName. For instance, at a sequence length of 1024, DAPEV2-Kerple with \methodShortName achieves a perplexity of 26.56, compared to 30.29 for RoPE with \methodShortName. This superiority is consistent across shorter sequence lengths as well, with DAPEV2-Kerple maintaining its advantage even without \methodShortName. These results confirm that DAPEV2-Kerple is the most effective position encoding method for reducing perplexity in language modeling tasks.

\paragraph{The proposed \methodShortName improves both short and long-sequence modeling.}
The analysis indicates that \methodShortName enhances performance at all sequence lengths, demonstrating its ability to handle both short-range and long-range dependencies effectively. The reductions in perplexity are keeped at longer sequence lengths, particularly for DAPEV2-Kerple (e.g., a drop from 26.86 to 26.56 for length 1024), suggesting that \methodShortName still provides better optimization for long contexts. This capability is critical for modern language models that often deal with extensive input sequences.

\begin{table}[!htbp]
\caption{The perplexity on Arxiv and  Books dataset with training length 2048, evaluate from length 128 to length 2048.}
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{cccccccc}
\toprule
\text{Dataset}&PE&\methodShortName&  128 & 256&512&1024&2048  \\ \midrule
Arxiv &RoPE&\ding{53} &9.14&7.53 & 5.42&5.00&4.95\\ 
Arxiv &RoPE&\ding{51}&9.05 &7.46 &5.38&4.96&4.92\\ 
Arxiv &DAPEV2-Kerple&\ding{53} &8.80&7.22 & 5.16&4.74&4.64\\ 
Arxiv &DAPEV2-Kerple&\ding{51}&8.70 &7.15 &5.13&4.72&4.61\\ \midrule
Books &RoPE&\ding{53} &35.99&31.32 & 25.97&24.32&22.65\\ 
Books &RoPE&\ding{51}&35.71 &31.15 &25.906&24.23&22.63\\ 
Books &DAPEV2-Kerple&\ding{53} &34.13&29.54 & 24.28&22.60&20.85\\ 
Books &DAPEV2-Kerple&\ding{51}&33.60 &29.16 &24.06&22.40&20.71\\ \midrule 
\end{tabular}
}
\label{table:lengh 2048}
\end{table}

\paragraph{The \methodShortName still works well on longer training length.} The results in Table \ref{table:lengh 2048} demonstrate the impact of using \methodShortName (SA-Softmax, indicated by \ding{51}) versus not using it (\ding{53}) on the Arxiv and Books datasets under different positional encodings (RoPE and DAPEV2-Kerple) across evaluation lengths from 128 to 2048, with a training length of 2048. For both datasets and positional encodings, \methodShortName consistently improves performance, as evidenced by lower perplexity values. On the Arxiv dataset, DAPEV2-Kerple with \methodShortName achieves the best results, with perplexity decreasing from 8.70 at length 128 to 4.61 at length 2048, outperforming baseline DAPEV2-Kerple in all cases. For the Books dataset, DAPEV2-Kerple combined with \methodShortName achieves the lowest perplexity. Similarly, RoPE with \methodShortName also achieves better performance than baseline RoPE on Arxiv and Books dataset from evaluation length 128 to length 2048. These results indicate that \methodShortName effectively enhances model performance, and works well on longer training length.


% \begin{table}[htb]
% \caption{The perplexity on Arxiv and  Books dataset with training length 2048, evaluate from length 128 to length 2048.}
% \centering
% \resizebox{0.49\textwidth}{!}{
% \begin{tabular}{cccccccc}
% \toprule
% \text{Dataset}&PE&\methodShortName&  128 & 256&512&1024&2048  \\ \midrule
% Arxiv &RoPE&\ding{53} &xx&xx & xx&xx&xx\\ 
% Arxiv &RoPE&\ding{51}&xx &xx &xx&xx&xx\\ \midrule
% Arxiv &DAPEV2-Kerple&\ding{53} &xx&xx & xx&xx&xx\\ 
% Arxiv &DAPEV2-Kerple&\ding{51}&xx &xx &xx&xx&xx\\ \midrule
% Books &RoPE&\ding{53} &34.13&29.54 & 24.28&22.60&20.85\\ 
% Books &RoPE&\ding{51}&33.60 &29.16 &24.06&22.40&20.71\\ 
% Books &DAPEV2-Kerple&\ding{53} &34.13&29.54 & 24.28&22.60&20.85\\ 
% Books &DAPEV2-Kerple&\ding{51}&33.60 &29.16 &24.06&22.40&20.71\\ 
% \bottomrule
% \end{tabular}
% }
% \label{table:lengh 2048}
% \end{table}




\subsection{The performance on Larger Model Size}
\begin{table}[htb]
\caption{The perplexity on Arxiv and Books dataset with different model sizes, with training length 512.}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\textbf{PE}&\textbf{Dataset}&\textbf{\methodShortName}&125M & 350M & 1.3B & 2.7B  \\ 
\midrule
RoPE&Arxiv& \ding{53}&6.70 & 6.26 & 6.01 &5.93\\
RoPE&Arxiv&\ding{51} &6.63 &6.20 &5.92 & 5.83 \\ \midrule
DAPEV2-Kerple&Arxiv&\ding{53}&6.63 & 6.02 & 5.79 &5.70\\
DAPEV2-Kerple&Arxiv&\ding{51} &6.36 &5.97 &5.74&5.65 \\ \midrule
RoPE&Book & \ding{53} &38.29 &33.81 & 30.94 &29.98 \\
RoPE&Book&   \ding{51} &37.57 & 33.17 &30.24 & 29.15 \\ \midrule
DAPEV2-Kerple&Book&\ding{53} &36.25 &32.20 & 29.32 &28.15 \\
DAPEV2-Kerple&Book& \ding{51} &35.93 & 31.82 &28.91 & 27.75 \\
\bottomrule
\end{tabular}
}
\label{table:large_model}
\end{table}

\paragraph{\methodShortName still enhances performance for larger model sizes.}
Table \ref{table:large_model} demonstrates the effectiveness of \methodShortName across different model sizes, ranging from 125M to 2.7B parameters, on the Books and Arxiv datasets. The results show that, as model size increases, the integration of \methodShortName consistently improves performance compared to the baseline (\ding{53}). For example, with RoPE on the Books dataset, the perplexity at 2.7B parameters decreases from 29.98 to 29.15 when \methodShortName is applied. Similarly, for DAPEV2-Kerple on the same dataset, perplexity improves from 28.15 to 27.75 at the largest model size, highlighting the compatibility of \methodShortName with large-scale models.

% \paragraph{\methodShortName narrows the gap between smaller and larger models.}
% Interestingly, \methodShortName allows smaller models to achieve comparable performance than larger baseline models. For instance, on the Arxiv dataset, RoPE with \methodShortName at 1.3B parameters achieves a perplexity of 5.92, outperforming the baseline DAPEV2-Kerple at 2.7B parameters (5.93). This finding underscores the ability of \methodShortName has the potential to enhance model efficiency by significantly improving perplexity without requiring proportional increases in model size.

\paragraph{\methodShortName delivers consistent improvements across datasets.}
The results are consistent across both the Books and Arxiv datasets, confirming the generalizability of \methodShortName. On the Arxiv dataset, for instance, RoPE with \methodShortName reduces perplexity across all model sizes, from 6.70 to 6.63 at 125M parameters and from 5.93 to 5.83 at 2.7B parameters. Similar trends are observed for DAPEV2-Kerple, where the improvements are slightly less pronounced but still consistent. These findings indicate that \methodShortName is robust and effective across diverse text corpora and model configurations.




\subsection{The Performance of Different Variants}
\begin{table}[htb]
% \vspace{-5pt}
\caption{The perplexity on the Books dataset with training length 512, compared to baselines.}
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\text{Length}&\textbf{Variant}&  RoPE & $DAPEV2-Kerple$  \\ \midrule
128 &$softmax(x)$ &89.60 & 84.33&\\ 
128 &$x*softmax(x)$ &85.98 &82.63 &\\  
128 &$(x-x_{max})*softmax(x)$ &86.10 &\textbf{82.08} &\\ 
128 &$\frac{(x - x_{\min})}{x_{max}-x_{min}} \cdot softmax(x)$ &89.36 &84.21 &\\  
128 &$\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$ &\textbf{89.36} &83.63 &\\ 
\midrule
512 & $softmax(x)$&38.29 &36.25 &\\ 
512 & $x*softmax(x)$&38.07 &35.82 &\\  
512 &$(x-x_{max})*softmax(x)$ &39.47 &\textbf{35.70} &\\ 
512 &$\frac{(x - x_{\min})}{x_{max}-x_{min}} \cdot softmax(x)$ &37.93 &36.21 &\\ 
512 &$\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$ &\textbf{37.57} &35.93 &\\ 
\midrule
1024 &$softmax(x)$ &28.78 & 26.86&\\ 
1024 &$x*softmax(x)$ &28.96 & 26.62 &\\  
1024 &$(x-x_{max})*softmax(x)$ &30.26 & 26.78 &\\  
1024 &$\frac{(x - x_{\min})}{x_{max}-x_{min}} \cdot softmax(x)$ &28.73 & 26.74 &\\  
1024 &$\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$ &\textbf{28.57} &\textbf{26.56} &\\  
\bottomrule
\end{tabular}
}
\label{table:different_variants}
% \vspace{-10pt}
\end{table}

\paragraph{The $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ variant is a robust default choice.}  
Table~\ref{table:different_variants} shows that this variant consistently improves perplexity across all sequence lengths (128, 512, and 1024) and for both RoPE and DAPEV2-Kerple position encodings. For RoPE, this variant achieves the best performance at 128, 512, and 1024 lengths, and for DAPEV2-Kerple, it also achieves better performance than baseline from length 128 to length 1024. This suggests that the variant balances performance improvements across different configurations, making it a reliable choice when specific experimental conditions are not predefined.

\paragraph{Optimal \methodShortName variants depend on the experimental setup and position encoding method.}  
The results reveal that different variants perform best under different conditions. For example, the $(x - x_{\max}) \cdot softmax(x)$ variant achieves the lowest perplexity for DAPEV2-Kerple at lengths 128 (82.08) and 512 (35.70), outperforming all other configurations for these specific setups. Similarly, the standard $x \cdot softmax(x)$ variant shows competitive performance at 1024-length sequences, achieving 26.62 for DAPEV2-Kerple. These variations highlight that while certain formulations may work well across the board, optimal performance often depends on the interaction between the sequence length and the positional encoding technique.


% \subsection{The Performance of Length Extrapolation}
% \begin{table}[htb]
% \caption{The perplexity on Books dataset with DAPEV2-Kerple. Base: $softmax(x)$; V1: $x*softmax(x)$; V2: $(x-x_{max})*softmax(x)$; V3: $\frac{(x - x_{\min})}{x_{max}-x_{min}} \cdot softmax(x)$; V4: $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$  }
% \centering
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{cccccccc}
% \toprule
% \text{Length}&\textbf{Variant}&  128 & 256&512&1024&2048&4096  \\ \midrule
% 128 &Base &84.33 & 78.72&72.46&69.52&66.04&69.58\\ 
% 128 &V1 &83.68 &78.06 &71.86&\textbf{69.01}&65.78&\textbf{69.32}\\  
% 128 &V2 &82.08 &\textbf{76.69} &\textbf{71.77}&72.00&77.22&112.66\\ 
% 128 &V3 &84.21 &78.51 &72.33&69.49&66.02&69.61\\  
% 128 &V4&\textbf{83.63} &77.95 &71.84&69.07&\textbf{65.99}&69.54\\ 
% \midrule
% 512 &Base &47.07 & 42.11&36.25&34.18&31.91&32.42\\ 
% 512 &V1 &\textbf{46.30} &\textbf{41.44} &\textbf{35.75}&\textbf{33.72}&\textbf{31.42}&\textbf{32.19}\\  
% 512 &V2 &46.36 &41.53 &35.79&33.95&32.03&33.84\\ 
% 512&V3 &46.83 &41.84 &36.01&33.97&31.66&32.25\\  
% 512 &V4&46.45&41.60 &35.93&33.89&31.63&32.23\\ 
% \midrule
% 1024 &Base &38.82 & 34.15&28.65&26.86&24.81&25.15\\ 
% 1024 &V1 &38.33 &33.78 &28.39&26.62&24.62&25.06\\  
% 1024 &V2 &46.91 &41.60 &35.26&33.25&1264.94&10775.42\\ 
% 1024 &V3 &38.72 &34.03 &28.52&26.74&24.73&25.18\\  
% 1024 &V4&\textbf{38.26} &\textbf{33.73} &\textbf{28.37}&\textbf{26.56}&\textbf{24.61}&\textbf{24.99}\\ 
% \bottomrule
% \end{tabular}
% }
% \label{table:length_extrapolation}
% \end{table}

% \paragraph{The DAPEV2-Kerple with variants demonstrates improved performance both within and beyond training lengths.}  
% Table~\ref{table:length_extrapolation} compares the perplexity of DAPEV2-Kerple across different sequence lengths, ranging from 128 to 4096, using several variants (Base, V1, V2, V3, V4). The results indicate that the choice of variant significantly influences performance, with V1 and V4 generally achieving the best results across both shorter training lengths (e.g., 128, 512) and extrapolated longer lengths (e.g., 2048, 4096). Notably, V4 provides the most stable improvements across all lengths, achieving the best perplexity scores at lengths 1024 and beyond (e.g., 24.61 at 2048 and 24.99 at 4096).

% \paragraph{Variant V2 excels at shorter lengths but suffers at longer lengths.}  
% Variant V2 achieves exceptional performance at shorter lengths, particularly for 128-length sequences where it outperforms all other variants with a perplexity of 82.08. However, at longer lengths, V2 exhibits numerical instability, as evidenced by the drastic increase in perplexity (e.g., 1264.94 at 2048 and 10775.42 at 4096). This suggests that while V2 is effective for shorter sequences, it lacks the robustness needed for extrapolation.

% \paragraph{Variants V1 and V4 perform consistently well across different sequence lengths.}  
% For sequence lengths within training limits (e.g., 128, 512), both V1 and V4 outperform the Base configuration in most cases. For example, at length 512, V1 achieves the best perplexity (35.75), closely followed by V4 (35.93), compared to Base (36.25). As sequence length increases to extrapolated ranges, these variants continue to show robust performance. At length 4096, V4 achieves the best perplexity (24.99), demonstrating its strong extrapolation capabilities.

% \paragraph{Variant V4 balances within-training and extrapolated performance.}  
% Variant V4 emerges as the most robust choice across all sequence lengths, achieving the best perplexity scores at extrapolated lengths (e.g., 26.56 at 1024, 24.61 at 2048, and 24.99 at 4096). This suggests that V4 is particularly well-suited for tasks requiring both strong within-training performance and reliable extrapolation to longer sequences, making it an ideal variant for DAPEV2-Kerple.


\subsection{Performance on Downstream Tasks}
\begin{table}[htb]
% \vspace{-5pt}
\caption{The performance on downstream tasks, with 125M model size and 300B training tokens.}
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Dataset}&Metrics& Softmax & \methodShortName\\ \midrule
Lambda&ppl$\downarrow$&21.63&\textbf{20.43}\\ 
WikiText&ppl$\downarrow$&27.57&\textbf{27.47}\\ 
ARCEasy&acc$\uparrow$&45.92&\textbf{47.52}\\ 
HellaSwag&acc$\uparrow$&30.34&\textbf{30.42}\\ 
PiQA& acc$\uparrow$&64.64&\textbf{64.69}\\
OpenBookQA&acc$\uparrow$&16.80&\textbf{18.00}\\ 
SciQ&acc$\uparrow$&76.80&\textbf{77.60}\\ 
Winogrande&acc$\uparrow$&51.54&\textbf{51.85}\\ 
\bottomrule
\end{tabular}
}
\label{table: downstream tasks}
\vspace{-5pt}
\end{table}
\paragraph{Pretrain Setting.} We pre-train a 125M model with 300B tokens from the Pile dataset and evaluate the performance on the downstream tasks \cite{black2022gpt}. Following the setting of previous works \cite{black2022gpt}, the training steps are 143000 with training length of 2048 and a global batch size 1024.
\paragraph{The \methodShortName achieve better performance than baseline softmax.} 
% As shown in Table \ref{table: downstream tasks}, \methodShortName demonstrates superior performance compared to the baseline Softmax model. \methodShortName achieves a substantial reduction in perplexity (ppl) on the Lambda dataset \cite{paperno2016lambada}, from 24.17 to 22.34, and an improvement on WikiText \cite{merity17pointer}, where perplexity drops from 29.65 to 29.59.
As shown in Table \ref{table: downstream tasks}, \methodShortName demonstrates superior performance compared to the baseline Softmax model across a wide range of tasks. The improvements are particularly notable in tasks requiring language modeling and reasoning. For instance, on the Lambda dataset \cite{paperno2016lambada}, \methodShortName achieves a significant reduction in perplexity (ppl) from 21.63 to 20.43. 
Similarly, on WikiText \cite{merity17pointer}, \methodShortName reduces perplexity from 27.57 to 27.47.
Additionally, it attains higher accuracy (acc) on several datasets, including ARCEasy \cite{clark2018arc}, HellaSwag \cite{zellers2019hellaswag}, PiQA\cite{clark2018arc}, OpenBookQA\cite{bisk2020piqa}, SciQ \cite{welbl2017crowdsourcing}, and Winogrande \cite{kocijan2019winogrande}. These results underscore the effectiveness of \methodShortName, even with a relatively small model size, when trained on a large-scale corpus. with potential for further improvements through increased model size and training data.


% \paragraph{Pretrain Setting.} We pre-train a 125M model with 170B tokens and evaluate the performance on the downstream tasks\cite{black2022gpt}. Following the setting of previous works \cite{black2022gpt}, the training steps are 82000 with training length of 2000 and a global batch size 1024.
% \paragraph{The \methodShortName achieve better performance than baseline softmax.} As shown in Table \ref{table: downstream tasks}, \methodShortName demonstrates superior performance compared to the baseline Softmax model. \methodShortName achieves a substantial reduction in perplexity (ppl) on the Lambda dataset, from 24.17 to 22.34, and an improvement on WikiText , where perplexity drops from 29.65 to 29.59. Additionally, it attains higher accuracy (acc) on several datasets, including ARCEasy, HellaSwag , OpenBookQA, SciQ , and Winogrande . These results underscore the effectiveness of \methodShortName, even with a relatively small model size, when trained on a large-scale corpus. The findings suggest that \methodShortName is robust and scalable, with potential for further improvements through increased model size and training data.



\section{The Performance on Classification and Translation Taks}
\begin{table}[htb]
\caption{Accuracy achieved on various downstream classification tasks. The \textbf{Improve $\Delta$} column shows the improvement in percentage points when using \methodShortName compared to Softmax.}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Dataset}&Softmax& \methodShortName & $\Delta$ \\ \midrule
AG-News&93.75&95.83&2.08\\ 
DBPedia&99.11&100&0.09\\ 
Yelp-Review&65.00&67.50&2.50\\ 
YahooNews&72.92&73.96&1.04\\ 
AmazomNews&62.50&68.75&6.25\\ 
\bottomrule
\end{tabular}
}
\label{table: classification_task}
\end{table}

\begin{table}[htb]
\caption{Performance comparison on IWSLT2017 machine translation tasks. Bold values indicate the best performance for each pair.}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\textbf{Input}&\methodShortName&en&nl&de&it&ro \\ \midrule
en &\ding{53}&- &25.98 & 22.53 &24.08 & 21.98 \\
en &\ding{51}&- &\textbf{26.25} & \textbf{23.57} &\textbf{24.67} & \textbf{22.21} \\ \midrule
nl &\ding{53}&31.43 &- & 18.57 &15.89 & 14.67 \\
nl &\ding{51}&\textbf{32.10} &- &\textbf{19.21} & \textbf{16.14}  & \textbf{15.04} \\ \midrule
de &\ding{53}&26.83 &18.44 &-&14.55 & \textbf{13.72} \\
de &\ding{51}&\textbf{27.49} & \textbf{18.76} &-& \textbf{14.76}&13.57  \\ \midrule
it &\ding{53}&28.31 &15.50 &15.65 &- &15.77 \\
it &\ding{51}&\textbf{28.55} &\textbf{15.65} &\textbf{15.97} & -&\textbf{16.09} \\ \midrule
ro &\ding{53}&28.75 &15.42 & 15.72&18.27 & -\\
ro &\ding{51}&\textbf{29.21} &\textbf{16.71} &\textbf{16.11} &\textbf{18.54} & -\\ 
\bottomrule
\end{tabular}
}
\label{table: machine_translation}
\end{table}
\paragraph{The Performance on Classification and Translation Taks, the experiment setting is presented in Appendix \ref{appendix: experiment setting}.} The results across both classification and machine translation tasks indicate the consistent effectiveness of \methodShortName over traditional methods such as Softmax. On classification tasks (Table~\ref{table: classification_task}), \methodShortName achieves notable improvements across all datasets, with the highest improvement observed on the AmazonNews dataset (+6.25 percentage points) and significant gains on AG-News (+2.08), Yelp-Review (+2.50), and YahooNews (+1.04). 
On machine translation tasks (Table~\ref{table: machine_translation}), \methodShortName consistently outperforms baseline methods across multiple language pairs. 
% The improvements are particularly pronounced for translations involving lower-resource languages, where the model's ability to generalize plays a critical role. 
For instance, when translating from English (EN) to other languages, \methodShortName achieves the highest accuracy gains, with improvements such as +0.27 (EN to NL) and +1.04 (EN to DE). 
% Similarly, for translations originating from Dutch (NL), \methodShortName improves performance across all target languages, showcasing its adaptability to diverse linguistic structures. 
Across both downstream classification and machine translation tasks, the improvements are consistently observed. 
These results collectively indicate that \methodShortName enhances both the accuracy and generalization capabilities of models, making it a promising alternative to traditional Softmax-based approaches. 

% \subsection{Analyze the Training Loss and Gradient}
% \paragraph{Optimizer Gradient.} As shown in Appendix Appendix \ref{appendix: loss and gradient}, comparing the gradients of $softmax(x)$ and $x \cdot softmax(x)$, the latter shows consistently larger gradients initially due to the multiplicative factor of $x$. A detailed analysis in the methodology section confirms this behavior. In contrast, the normalized variant, $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$, produces gradients similar to or smaller than $softmax(x)$ early on but can grow larger later in training. This is due to normalization, which stabilizes updates but reduces gradient magnitude in the early stages.

% \paragraph{Training Loss Across Steps.} For DAPEV2-Kerple, $x \cdot softmax(x)$ and $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ can both achieve lower loss than baseline $softmax(x)$. However, $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ is better than baseline $softmax(x)$ through the whole training steps, while the $x \cdot softmax(x)$ achieves better performance than bseline at late training step. This may caused by that the $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ is a normalized version so that easier for optimizer. 

\subsection{Visualization of Attention Output}

% \begin{figure}[htb]
% %\begin{figure}
% \centering
% \includegraphics[width=0.23\textwidth]{fig/512/ori/attention_1.pdf}
% \hspace{0in}
% \includegraphics[width=0.23\textwidth]{fig/512/gradient_x/attention_1.pdf}
% \hspace{0in}
% \includegraphics[width=0.23\textwidth]{fig/512/gradient_zero/attention_1.pdf}
% \hspace{0in}

% \caption{
% \small
% \textbf{The visualization of attention output, from left to right: 1) $softmax(x)$; 2) $x*softmax(x)$; 3) $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$.
% }
% }
% \label{fig: attetnion_output}
% \end{figure}
We also visualize the attention probability for different methods in Appendix \ref{appendix: visualization attention}.

\paragraph{$\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ and $softmax(x)$ present similar pattern, compared to $\text{x} \cdot softmax(x)$}. As shown Appendix \ref{appendix: visualization attention}, the $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ range may be larger than baseline $softmax(x)$. Also, the $softmax(x)$ and $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ are more similar, compared to $x*softmax(x)$. The $x*softmax(x)$ may have some special attention patterns, as shown in layer 3 and layer 10. 

\paragraph{Attention scores can be negative, contrary to previous beliefs that attention scores must be positive.}
In prior work, the research community has attempted to replace softmax attention with ReLU attention or Sigmoid attention, operating under the assumption that attention scores should always be positive \cite{nair2010rectified,chen2020arelu,wortsman2023replacing,shen2023study} \cite{ramapuram2024theory} However, in this work, we successfully demonstrate that attention scores can indeed take on negative values. As shown in Appendix \ref{appendix: visualization attention}, we observe that transformers can still be effectively trained even when the attention scores contain negative elements and the sum of each row is not strictly equal to one.



\section{Conclusion}
We propose \methodFullName, a modification designed to improve gradient dynamics and enhance performance in transformers. To demonstrate the effectiveness of \methodShortName, we conduct extensive experiments, including analyses with various positional encodings, training lengths, and model sizes and different variants. Additionally, we evaluate \methodShortName on downstream tasks, where the variant $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ consistently proves to be the most effective across diverse settings. This powerful adjustment significantly enhances transformer scalability and generalization, offering promising potential for a wide range of applications.


\section*{Limitations}

The proposed method needs to find the max and min values first for the normalization. Therefore, there may be additional costs.




% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\nocite{*}

\bibliography{custom}
% \bibliographystyle{acl}

\clearpage
\newpage
\appendix

\section{Model Configuration} 
\label{appendix: experiment setting} 
\paragraph{Pretrain Setting.} All experiments are conducted on 8 GPUs. The 125M and 350M model configurations are the following.
% The layer number is 12, the hidden dimension is 768 and the attention head is 12. The learning rate is 6e-4.

\begin{table}[!ht]
    \centering
    \setlength{\tabcolsep}{2pt}
    \label{model configuration}
    \caption{\textbf{Model Configurations.}}
    \begin{tabular}{c c c c c}
    \toprule
    & & \textbf{125M} & & \textbf{350M} \\ \midrule
    Training sequence length & & $512$ & & $512$\\
    Batch size & & 2 $\times$ 8  & & 2 $\times$ 8 \\
    Number of iterations & & $50$k & & $50$k \\
    Dropout prob. & & $0.0$ & & $0.0$ \\
    Attention dropout prob. & & $0.0$ & & $0.0$ \\
    Attention head && 12 && 16  \\
    Feature dimension && 768 && 1024\\
    Layer number && 12 && 24 \\
    Optimizer & & Adam & & Adam\\
    Optimizer parameter betas & & [0.9, 0.95] && [0.9, 0.95] \\
    Learning rate & & $6\mathrm{e}-4$  & & $3\mathrm{e}-4$ \\
    Precision & & float132 & & float32 \\ 
    \bottomrule
    \end{tabular}
    \label{tab:model_configs}
\end{table}


\paragraph{Experiment Setting for Classification and Translation tasks.}
For the sequence classification tasks presented in Table \ref{table: classification_task}, the feature dimension is set to 128, with 2 attention heads and 6 layers. The datasets are AGNews, DBPedia, Yelp-Review, YahooNews, AmazonNews \cite{zhang2015character}.
In contrast, for the machine translation tasks shown in Table \ref{table: machine_translation}, the feature dimension is increased to 512, with 8 attention heads and 12 layers. The dataset comes from IWSLT2017 datasets \cite{cettolo-etal-2017-overview}.

% \section{Training Loss and Gradient}
% \label{appendix: loss and gradient}
% \begin{figure}[!htbp]
% % \vspace{-5pt}
% %\begin{figure}
% \setlength{\abovecaptionskip}{0.1cm}
% \centering
% \includegraphics[width=0.4\textwidth]{fig/loss.pdf}
% \hspace{0in}
% \includegraphics[width=0.4\textwidth]{fig/gradient.pdf}
% \caption{
% \small
% The loss difference and gradient are different between our methods and baseline. 
% }
% \label{fig: dif_loss}
% % \vspace{-10pt}
% \end{figure}
 

\section{Error Bar}
\begin{table}[htb]
\caption{The perplexity on Books3 dataset with three random seeds.}
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Data}&\textbf{\methodShortName} & Mean & Std  \\ \midrule
Kerple &\ding{53} &38.21 &0.3873 \\  
Kerple& \ding{51} &37.71 &0.3826\\ 
FIRE &\ding{53} &38.00  &0.2211 \\  
FIRE& \ding{51} &37.24 &0.2786\\ 
RoPE &\ding{53} &38.03 &0.2165 \\  
RoPE& \ding{51} &37.48 &0.3287\\ 
DAPEV2-Kerple &\ding{53} &35.92 &0.3821\\  
DAPEV2-Kerple &\ding{51} &35.58 &0.4037 \\  
\bottomrule
\end{tabular}
}
\label{table: sa-softmax}
\end{table} 


\section{Analyze the Training Loss and Gradient}
\label{appendix: loss and gradient}
\begin{figure}[!htbp]
% \vspace{-5pt}
%\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}
\centering
\includegraphics[width=0.4\textwidth]{fig/loss.pdf}
\hspace{0in}
\includegraphics[width=0.4\textwidth]{fig/gradient.pdf}
\caption{
\small
The loss difference and gradient difference between our methods and baseline. 
}
\label{fig: dif_loss}
% \vspace{-10pt}
\end{figure}
\paragraph{Optimizer Gradient.} As shown in Figure \ref{fig: dif_loss}, comparing the gradients of $softmax(x)$ and $x \cdot softmax(x)$, the latter shows larger gradients initially due to the multiplicative factor of $x$. A detailed analysis in the methodology section confirms this behavior. In contrast, the normalized variant, $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$, produces gradients similar to or smaller than $softmax(x)$ early on but can grow larger later in training. This is due to normalization, which stabilizes updates but reduces gradient magnitude in the early stages.

\paragraph{Training Loss Across Steps.} For DAPEV2-Kerple, $x \cdot softmax(x)$ and $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ can both achieve lower loss than baseline $softmax(x)$. However, $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ is better than baseline $softmax(x)$ through the whole training steps, while the $x \cdot softmax(x)$ achieves better performance than bseline at late training step. This may caused by that the $\frac{(x - \min(x_{\min}, 0))}{\max(0, x_{\max}) - \min(x_{\min}, 0)} \cdot softmax(x)$ is a normalized version so that easier for optimizer. 

\section{Risk}
This work focuses on utilizing self-adjust softmax to improve the transformer architecture. This is no specific risk. Also, we use AI assistants for writing.
% \newpage
\onecolumn
\section{Visualization of Attention Score}
\label{appendix: visualization attention}
\begin{figure}[!htbp]
%\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}
\centering
\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_1.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_1.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_1.pdf}
\hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_2.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_2.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_2.pdf}
\hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_3.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_3.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_3.pdf}
\hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_4.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_4.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_4.pdf}
\hspace{0in}

\setlength{\abovecaptionskip}{0.1cm}
\centering
\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_5.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_5.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_5.pdf}
\hspace{0in}




\caption{
\small
\textbf{The visualization of attention output, from left to right: 1) $softmax(x)$; 2) $x*softmax(x)$; 3) $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$.
}
}

\end{figure}


\begin{figure}[!htbp]
%\begin{figure}

% \includegraphics[width=0.32\textwidth]{fig/512/ori/attention_3.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_3.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_3.pdf}
% \hspace{0in}

% \includegraphics[width=0.32\textwidth]{fig/512/ori/attention_4.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_4.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_4.pdf}
% \hspace{0in}

% \setlength{\abovecaptionskip}{0.1cm}
% \centering
% \includegraphics[width=0.32\textwidth]{fig/512/ori/attention_5.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_5.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_5.pdf}
% \hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_6.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_6.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_6.pdf}
\hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_7.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_7.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_7.pdf}
\hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_8.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_8.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_8.pdf}
\hspace{0in}


\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_9.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_9.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_9.pdf}
\hspace{0in}

\setlength{\abovecaptionskip}{0.1cm}
\centering
\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_10.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_10.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_10.pdf}
\hspace{0in}



\caption{
\small
\textbf{The visualization of attention output, from left to right: 1) $softmax(x)$; 2) $x*softmax(x)$; 3) $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$.
}
}

\end{figure}

\clearpage
\begin{figure}[!htbp]
%\begin{figure}

% \includegraphics[width=0.32\textwidth]{fig/512/ori/attention_8.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_8.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_8.pdf}
% \hspace{0in}


% \includegraphics[width=0.32\textwidth]{fig/512/ori/attention_9.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_9.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_9.pdf}
% \hspace{0in}

% \setlength{\abovecaptionskip}{0.1cm}
% \centering
% \includegraphics[width=0.32\textwidth]{fig/512/ori/attention_10.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_10.pdf}
% \hspace{0in}
% \includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_10.pdf}
% \hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_11.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_11.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_11.pdf}
\hspace{0in}

\includegraphics[width=0.32\textwidth]{fig/512/ori/attention_12.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_x/attention_12.pdf}
\hspace{0in}
\includegraphics[width=0.32\textwidth]{fig/512/gradient_zero/attention_12.pdf}
\hspace{0in}



\caption{
\small
\textbf{The visualization of attention output, from left to right: 1) $softmax(x)$; 2) $x*softmax(x)$; 3) $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$.
}
}


\end{figure}



\clearpage
\newpage
\section{Implementation}
\label{appendix: implementation}
In this section, we present the implementation of the proposed \methodShortName module in \texttt{PyTorch} which allows for research purpose \citep{paszke2019pytorch}.

\definecolor{lightgreen}{rgb}{0,0.8,0}
\definecolor{darkgreen}{rgb}{0,0.8.0.2}
\definecolor{backcolour}{rgb}{0.97,0.97,0.94}
\lstset{language=Python,
basicstyle=\footnotesize,
breaklines=true,
backgroundcolor = \color{backcolour},
keywordstyle=\color{blue}\ttfamily,
stringstyle=\color{lightgreen}\ttfamily,
commentstyle=\color{gray}\ttfamily,
xleftmargin=2.5em,xrightmargin=0.5em, aboveskip=1em,
morecomment=[l][\color{darkgreen}]{\#}}

\begin{lstlisting}
import torch
import torch.nn as nn

class SA-Softmax(nn.Module):
  def __init__(self, operation_name):
    """


    Args:
      operation_name: "softmax","v1","v2","v3", or "v4"
    """
    super(SA-Softmax, self).__init__()


    self.operation_name= operation_name:
    

  def forward(self, attention: torch.Tensor, bias: torch.Tensor):
    """
    Args:
      attention: input sequence, which is q^T * k,
         shape [bsz, num_heads, seq_len, seq_len]
      bias: bias matrix, which can be generated by ALiBi, Kerple 
      FIRE or other additive position encodings
         shape [1,num_heads, seq_len, seq_len]

    Returns:
      attention with SA-Softmax,
      shape [bsz, num_heads, seq_len, seq_len]
    """
    attention_probs = softmax(attention_scores, attention_mask)
        
    if self.gradient_name=="v1":
        attention_probs=attention_probs*attention_scores
        attention_probs=torch.tril(attention_probs)
        
       
    elif self.gradient_name=="v2":
        B, H, T, _ = attention_scores.shape
        # Create a mask for the lower triangular part (including diagonal)
        mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device= attention_scores.device))
        # Apply mask to get lower triangular values, replace upper triangle with inf (so it doesn't affect min)
        x_lower_tri = attention_scores.masked_fill(~mask, float('inf'))
        # Get the minimum value along the last dimension
        min_attention_score, _ = x_lower_tri.min(dim=-1,keepdim=True)
        attention_scores=torch.tril(attention_scores)
        attention_probs=attention_probs*(attention_scores-min_attention_score)
        attention_probs=torch.tril(attention_probs)

    elif self.gradient_name=="v3":
        B, H, T, _ = attention_scores.shape

        # Create a mask for the lower triangular part (including diagonal)
        mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device= attention_scores.device))
        # Apply mask to get lower triangular values, replace upper triangle with inf (so it doesn't affect min)
        x_lower_tri = attention_scores.masked_fill(~mask, float('inf'))
        # Get the minimum value along the last dimension
        min_attention_score, _ = x_lower_tri.min(dim=-1,keepdim=True)
        # Apply mask to get lower triangular values, replace upper triangle with inf (so it doesn't affect min)
        x_lower_tri = attention_scores.masked_fill(~mask, float('-inf'))
        max_attention_score, _ = x_lower_tri.max(dim=-1,keepdim=True)

        attention_probs=attention_probs*((attention_scores-min_attention_score)/(max_attention_score-min_attention_score+1e-10))
    elif  self.gradient_name=="v4":
        attention_scores_tril_this=torch.tril(attention_scores)
        min_attention_score=torch.min(attention_scores_tril_this, -1,keepdim=True)[0]
        max_attention_score=torch.max(attention_scores_tril_this, -1,keepdim=True)[0]
        min_attention_score=torch.minimum(min_attention_score, torch.zeros(1, device= attention_scores.device))
        max_attention_score=torch.maximum(max_attention_score,torch.zeros(1, device= attention_scores.device))
        attention_probs=attention_probs*((attention_scores-min_attention_score)/(max_attention_score-min_attention_score+1e-10))
    
`   attention_probs=torch.tril(attention_probs)
    return attention_probs
\end{lstlisting}

\end{document}
