@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{zhang2020pegasus,
  title={{PEGASUS}: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@article{guo2021longt5,
  title={Long{T}5: Efficient text-to-text transformer for long sequences},
  author={Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
  journal={Findings of the Association for Computational Linguistics: NAACL},
  year={2022}
}

@inproceedings{ainslie2023colt5,
  title={Co{LT}5: Faster Long-Range Transformers with Conditional Computation},
  author={Ainslie, Joshua and Lei, Tao and de Jong, Michiel and Ontanon, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and Lee-Thorp, James and Tay, Yi and others},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{alexey2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Alexey, Dosovitskiy},
  journal={arXiv preprint arXiv: 2010.11929},
  year={2020}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}



@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{
chen2024pixartalpha,
title={PixArt-\${\textbackslash}alpha\$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis},
author={Junsong Chen and Jincheng YU and Chongjian GE and Lewei Yao and Enze Xie and Zhongdao Wang and James Kwok and Ping Luo and Huchuan Lu and Zhenguo Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=eAKmQPe3m1}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}


@inproceedings{
zhou2024dont,
title={Don't Trust: Verify -- Grounding {LLM} Quantitative Reasoning with Autoformalization},
author={Jin Peng Zhou and Charles E Staats and Wenda Li and Christian Szegedy and Kilian Q Weinberger and Yuhuai Wu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=V5tdi14ple}
}


@inproceedings{liu2021finbert,
  title={Finbert: A pre-trained financial language representation model for financial text mining},
  author={Liu, Zhuang and Huang, Degen and Huang, Kaiyu and Li, Zhuang and Zhao, Jun},
  booktitle={Proceedings of the Twenty-ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={4513--4519},
  year={2021}
}

@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}


@article{taylor2022galactica,
  title={Galactica: A large language model for science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}


@article{geneva2022transformers,
  title={Transformers for modeling physical systems},
  author={Geneva, Nicholas and Zabaras, Nicholas},
  journal={Neural Networks},
  volume={146},
  pages={272--289},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{
xiao2024efficient,
title={Efficient Streaming Language Models with Attention Sinks},
author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NG7sS51zVF}
}


@article{zhu2024near,
  title={Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention},
  author={Zhu, Qianchao and Duan, Jiangfei and Chen, Chang and Liu, Siran and Li, Xiuhong and Feng, Guanyu and Lv, Xin and Cao, Huanqi and Chuanfu, Xiao and Zhang, Xingcheng and others},
  journal={arXiv preprint arXiv:2406.15486},
  year={2024}
}

@article{xiao2024infllm,
  title={{InfLLM}: Unveiling the Intrinsic Capacity of {LLMs} for Understanding Extremely Long Sequences with Training-Free Memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Han, Song and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.04617},
  year={2024}
}

@article{fountas2024human,
  title={Human-like Episodic Memory for Infinite Context LLMs},
  author={Fountas, Zafeirios and Benfeghoul, Martin A and Oomerjee, Adnan and Christopoulou, Fenia and Lampouras, Gerasimos and Bou-Ammar, Haitham and Wang, Jun},
  journal={arXiv preprint arXiv:2407.09450},
  year={2024}
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}

@article{chen2024sepllm,
  title={Sep{LLM}: Accelerate large language models by compressing one segment into one separator},
  author={Chen, Guoxuan and Shi, Han and Li, Jiawei and Gao, Yihang and Ren, Xiaozhe and Chen, Yimeng and Jiang, Xin and Li, Zhenguo and Liu, Weiyang and Huang, Chao},
  journal={arXiv preprint arXiv:2412.12094},
  year={2024}
}


@inproceedings{
han2024bridging,
title={Bridging the Divide: Reconsidering Softmax and Linear Attention},
author={Dongchen Han and Yifan Pu and Zhuofan Xia and Yizeng Han and Xuran Pan and Xiu Li and Jiwen Lu and Shiji Song and Gao Huang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=RSiGFzQapl}
}

@article{deng2023superiority,
  title={Superiority of softmax: Unveiling the performance edge over linear attention},
  author={Deng, Yichuan and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2310.11685},
  year={2023}
}


@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@inproceedings{cettolo-etal-2017-overview,
    title = "Overview of the {IWSLT} 2017 Evaluation Campaign",
    author = {Cettolo, Mauro  and
      Federico, Marcello  and
      Bentivogli, Luisa  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Sudoh, Katsuhito  and
      Yoshino, Koichiro  and
      Federmann, Christian},
    booktitle = "Proceedings of the 14th International Conference on Spoken Language Translation",
    month = dec # " 14-15",
    year = "2017",
    address = "Tokyo, Japan",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2017.iwslt-1.1",
    pages = "2--14",
}

@article{sun2023survey,
  title={A survey of reasoning with foundation models},
  author={Sun, Jiankai and Zheng, Chuanyang and Xie, Enze and Liu, Zhengying and Chu, Ruihang and Qiu, Jianing and Xu, Jiaqi and Ding, Mingyu and Li, Hongyang and Geng, Mengzhe and others},
  journal={arXiv preprint arXiv:2312.11562},
  year={2023}
}

@inproceedings{zheng2024dape,
  title={Dape: Data-adaptive positional encoding for length extrapolation},
  author={Zheng, Chuanyang and Gao, Yihang and Shi, Han and Huang, Minbin and Li, Jingyao and Xiong, Jing and Ren, Xiaozhe and Ng, Michael and Jiang, Xin and Li, Zhenguo and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{zheng2024dapev2,
  title={DAPE V2: Process Attention Score as Feature Map for Length Extrapolation},
  author={Zheng, Chuanyang and Gao, Yihang and Shi, Han and Xiong, Jing and Sun, Jiankai and Li, Jingyao and Huang, Minbin and Ren, Xiaozhe and Ng, Michael and Jiang, Xin and others},
  journal={arXiv preprint arXiv:2410.04798},
  year={2024}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{chi2022kerple,
  title={Kerple: Kernelized relative positional embedding for length extrapolation},
  author={Chi, Ta-Chung and Fan, Ting-Han and Ramadge, Peter J and Rudnicky, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8386--8399},
  year={2022}
}

@article{li2023functional,
  title={Functional interpolation for relative positions improves long context transformers},
  author={Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  journal={arXiv preprint arXiv:2310.04418},
  year={2023}
}


@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{graves2012long,
  title={Long short-term memory},
  author={Graves, Alex and Graves, Alex},
  journal={Supervised sequence labelling with recurrent neural networks},
  pages={37--45},
  year={2012},
  publisher={Springer}
}

@article{lillicrap2020backpropagation,
  title={Backpropagation and the brain},
  author={Lillicrap, Timothy P and Santoro, Adam and Marris, Luke and Akerman, Colin J and Hinton, Geoffrey},
  journal={Nature Reviews Neuroscience},
  volume={21},
  number={6},
  pages={335--346},
  year={2020},
  publisher={Nature Publishing Group UK London}
}


@article{zhang2019gradient,
  title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1905.11881},
  year={2019}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}




@inproceedings{katharopoulos2020transformers,
  title={Transformers are {RNNs}: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}



@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  note={OpenAI}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@incollection{bridle1990probabilistic,
  title={Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition},
  author={Bridle, John S},
  booktitle={Neurocomputing: Algorithms, architectures and applications},
  pages={227--236},
  year={1990},
  publisher={Springer}
}

@incollection{lecun2002efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--50},
  year={2002},
  publisher={Springer}
}

@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin},
  title = {{JAX}: {A}utograd and {XLA}},
  year = {2018},
  howpublished = {\url{https://github.com/google/jax}},
  note = {Accessed: 2024-09-25}
}

@misc{pax2023github,
  author = {Google, Research},
  title = {Pax: A {JAX}-based neural network training framework},
  year = {2023},
  howpublished = {\url{https://github.com/google/paxml}},
  note = {Accessed: 2024-09-25}
}

@misc{google_tpuv5_2023,
  title = {Google Cloud TPU v5e: Next-Generation AI Hardware for Large-Scale Model Training},
  author = {Google Cloud},
  year = {2023},
  howpublished = {\url{https://cloud.google.com/blog/products/ai-machine-learning/introducing-tpu-v5e}},
  note = {Accessed: 2024-09-25},
}


@inproceedings{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  booktitle={Journal of Machine Learning Research},
  year={2020},
  volume={21},
  number={140},
  pages={1--67},
  url={http://jmlr.org/papers/v21/20-074.html}
}


@inproceedings{choromanski2021performer,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Q and Mohiuddin, Afroz and Kaiser, {\L}ukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@article{dosovitskiy2021vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2021}
}

@inproceedings{gulati2020conformer,
  title={Conformer: Convolution-augmented Transformer for Speech Recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  booktitle={Proc. Interspeech},
  year={2020}
}


@inproceedings{panayotov2015librispeech,
  title={Librispeech: An ASR corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

@article{you2019lamb,
  title={Large batch optimization for deep learning: Training BERT in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jason and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}


@article{loshchilov2017adamw,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@article{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}


@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@software{init2winit2021github,
  author = {Justin M. Gilmer and George E. Dahl and Zachary Nado and Priya Kasimbeg and Sourabh Medapati},
  title = {{init2winit}: a JAX codebase for initialization, optimization, and tuning research},
  url = {http://github.com/google/init2winit},
  version = {0.0.2},
  year = {2023},
}

@article{dahl2023benchmarking,
  title={Benchmarking neural network training algorithms},
  author={Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others},
  journal={arXiv preprint arXiv:2306.07179},
  year={2023}
}

@article{blanchard2021accurately,
  title={Accurately computing the log-sum-exp and softmax functions},
  author={Blanchard, Pierre and Higham, Desmond J and Higham, Nicholas J},
  journal={IMA Journal of Numerical Analysis},
  volume={41},
  number={4},
  pages={2311--2330},
  year={2021},
  publisher={Oxford University Press}
}



@article{dao2024flashattention,
  title={FlashAttention 2: Faster Attention with Better Memory Scheduling},
  author={Dao, Tri and Fu, Daniel and Wang, Xinyang G and others},
  journal={arXiv preprint arXiv:2401.14155},
  year={2024},
  url={https://arxiv.org/abs/2401.14155}
}

@article{roy2021routing,
  title={Efficient Routing Transformers: Dynamic Token Interaction Models for Natural Language Processing},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={arXiv preprint arXiv:2003.05997},
  year={2021},
  url={https://arxiv.org/abs/2003.05997}
}



@article{child2019sparse,
  title={Generating Long Sequences with Sparse Transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019},
  url={https://arxiv.org/abs/1904.10509}
}


@article{blanchard2019accurate,
  title={Accurate computation of the log-sum-exp and softmax functions},
  author={Blanchard, Pierre and Higham, Desmond J and Higham, Nicholas J},
  journal={arXiv preprint arXiv:1909.03469},
  year={2019}
}

@inproceedings{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015},
  url={https://arxiv.org/abs/1409.0473}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}


@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@inproceedings{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019},
  pages={3261--3275},
  publisher={Curran Associates, Inc.},
  dataset={WSC, RTE, COPA, CB}
}

@inproceedings{kocijan2019winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Kocijan, Vid and Chamorro-Perera, Elias and Sileo, Damien and Raiman, Jonathan and Clark, Peter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2020},
  volume={34},
  number={05},
  pages={8732--8740},
  dataset={Winogrande}
}

@inproceedings{levesque2012winograd,
  title={The Winograd Schema Challenge},
  author={Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
  booktitle={Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
  pages={552--561},
  year={2012},
  dataset={Winograd}
}

@inproceedings{pilehvar2019wic,
  title={WiC: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1267--1273},
  year={2019},
  dataset={WiC}
}

@inproceedings{mostafazadeh2016storycloze,
  title={A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},
  author={Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={839--849},
  year={2016},
  dataset={StoryCloze}
}

@article{zhang2018record,
  title={ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
  author={Zhang, Shuailong and Liu, Huanbo and Liu, Shuyan and Wang, Yuwei and Liu, Jiawei and Gao, Zhiyu and Xu, Wei and Xu, Yiming and Sun, Xin and Cui, Lei and others},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018},
  dataset={ReCoRD}
}

@inproceedings{lai2017race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={785--794},
  year={2017},
  dataset={RACE}
}

@inproceedings{bisk2020piqa,
  title={PIQA: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Le Bras, Ronan and Gao, Jianfeng and Choi, Yejin},
  booktitle={Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence},
  year={2020},
  dataset={PIQA}
}

@inproceedings{mihaylov2018openbookqa,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018},
  dataset={OpenBookQA}
}

@article{khashabi2018multiRC,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  journal={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={252--262},
  year={2018},
  dataset={MultiRC}
}



@inproceedings{clark2018boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2019},
  dataset={BoolQ}
}

@article{clark2018arc,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Caroline and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018},
  dataset={ARC (Easy and Challenging)}
}

@article{nie2019anli,
  title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  journal={arXiv preprint arXiv:1910.14599},
  year={2019},
  dataset={ANLI}
}


@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{llama3herd2024,
  title={The Llama 3 Herd of Models},
  author={ Team Meta-AI},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024},
  url={https://arxiv.org/abs/2407.21783}
}

@article{gemini2024,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team Gemini},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024},
  url={https://arxiv.org/abs/2403.05530}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}


@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE Transactions on Neural Networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}


@article{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

@inproceedings{xiong2020layer,
  title={On Layer Normalization in the Transformer Architecture},
  author={Xiong, Ruibin and Yang, Yingquan and He, Di and Zheng, Kai and Zheng, Shuxin and Lan, Yaliang and Wang, Jingdong and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}


@article{loshchilov2016sgdr,
  title={SGDR: Stochastic Gradient Descent with Warm Restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@misc{mlcommons_bert_dataset,
  author       = {MLCommons},
  title        = {BERT Dataset Documentation},
  howpublished = {\url{https://github.com/mlcommons/training/blob/master/language_model/tensorflow/bert/dataset.md}},
  note         = {Accessed: 2024-10-16}
}

@article{duvvuri2024laser,
  title={LASER: Attention with Exponential Transformation},
  author={Duvvuri, Sai Surya and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:2411.03493},
  year={2024}
}

@article{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, R},
  journal={arXiv preprint arXiv:1211.5063},
  year={2013}
}

@article{bjorck2018understanding,
  title={Understanding batch normalization},
  author={Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{xu2019understanding,
  title={Understanding and improving layer normalization},
  author={Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{huang2017arbitrary,
  title={Arbitrary style transfer in real-time with adaptive instance normalization},
  author={Huang, Xun and Belongie, Serge},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1501--1510},
  year={2017}
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{wortsman2023replacing,
  title={Replacing softmax with relu in vision transformers},
  author={Wortsman, Mitchell and Lee, Jaehoon and Gilmer, Justin and Kornblith, Simon},
  journal={arXiv preprint arXiv:2309.08586},
  year={2023}
}

@article{chen2020arelu,
  title={Arelu: Attention-based rectified linear unit},
  author={Chen, Dengsheng and Li, Jun and Xu, Kai},
  journal={arXiv preprint arXiv:2006.13858},
  year={2020}
}

@article{shen2023study,
  title={A study on relu and softmax in transformer},
  author={Shen, Kai and Guo, Junliang and Tan, Xu and Tang, Siliang and Wang, Rui and Bian, Jiang},
  journal={arXiv preprint arXiv:2302.06461},
  year={2023}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@article{ramapuram2024theory,
  title={Theory, analysis, and best practices for sigmoid self-attention},
  author={Ramapuram, Jason and Danieli, Federico and Dhekane, Eeshan and Weers, Floris and Busbridge, Dan and Ablin, Pierre and Likhomanenko, Tatiana and Digani, Jagrit and Gu, Zijin and Shidani, Amitis and others},
  journal={arXiv preprint arXiv:2409.04431},
  year={2024}
}


@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@inproceedings{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1525--1534},
  year={2016}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@inproceedings{liu2021logiqa,
  title={LogiQA: a challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={3622--3628},
  year={2021}
}

@inproceedings{merity17pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}

@inproceedings{welbl2017crowdsourcing,
  title={Crowdsourcing Multiple Choice Science Questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  booktitle={Proceedings of the 3rd Workshop on Noisy User-generated Text},
  pages={94--106},
  year={2017}
}


@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{zheng2023progressive,
  title={Progressive-hint prompting improves reasoning in large language models},
  author={Zheng, Chuanyang and Liu, Zhengying and Xie, Enze and Li, Zhenguo and Li, Yu},
  journal={arXiv preprint arXiv:2304.09797},
  year={2023}
}





@article{gaoalgoformer,
  title={AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures},
  author={Gao, Yihang and Zheng, Chuanyang and Xie, Enze and Shi, Han and Hu, Tianyang and Li, Yu and Ng, Michael and Li, Zhenguo and Liu, Zhaoqiang},
  journal={Transactions on Machine Learning Research},
  year={2025}
}


@article{xiong2025parallelcomp,
  title={ParallelComp: Parallel Long-Context Compressor for Length Extrapolation},
  author={Xiong, Jing and Shen, Jianghan and Zheng, Chuanyang and Wan, Zhongwei and Zhao, Chenyang and Yang, Chiwun and Ye, Fanghua and Yang, Hongxia and Kong, Lingpeng and Wong, Ngai},
  journal={arXiv preprint arXiv:2502.14317},
  year={2025}
}