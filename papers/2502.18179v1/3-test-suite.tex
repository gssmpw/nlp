\section{Test-Suite for IE from Layout-rich Documents with LLMs}
\label{sec:test_suite_IE_layout}

We implement a comprehensive test suite to assess IE tasks from LRDs. The pipeline, depicted in Figure~\ref{fig:pipeline}, systematically transforms raw input into structured output across multiple stages for evaluating the efficacy of design decisions.

\input{figures/pipeline}

\subsection{Data Structuring}
We convert raw documents into machine-readable formats by extracting text and layout features. The process involves two conversions: %
1) Extracting textual content and layout information using OCR data, and 2) creating a markdown representation of PDFs.

\noindent \textbf{Chunking.}
We employ three chunk sizes: (1) max: 4096 tokens, (2) medium: 2048 tokens, and (3) small: 1024 tokens. Documents are segmented into $N$ chunks based on document length and chunk size, preparing them for the prompting phase. Each chunk is formed by accumulating whole words until the token limit. This ensures intact word boundaries and maintaining a sequential, non-overlapping structure. Layout information is preserved by associating each text segment with
normalized and quantized spatial coordinates, retaining the structural context of the document.
\subsection{Model Engagement}
Model engagement consists of constructing input to the LLM comprising at least three components: (1) a task instruction outlining the IE task, (2) the target schema $S$, and (3) the document chunk.
We adhere to best practices from NLP for prompt structure and IE task instruction. The schema $S$ is implemented as a dictionary of key-value pairs, where values specify the format of the corresponding attribute using regex expressions in Listing~\ref{lst:schema}.

\begin{lstlisting}[language=Python, caption={Schema Definition}, label={lst:schema}]
"file_date": r"\d{4}-\d{2}-\d{2}",
"foreign_princ_name": r"[\w\s.,'&-]+",
"registrant_name": r"[\w\s.,'&-]+",
"registration_num": r"\d+",
"signer_name": r"[\w\s'.-]+",
"signer_title": r"[\w\s.,'&-]+",
\end{lstlisting}
We also implement two ICL strategies: (1) Few-Shot and (2) Chain-of-Thought (CoT)
(see Appendix~\ref{appendix:prompt_generation_details} for more details).
For $N$ prompts, the LLM performs \emph{N completions}, with one completion per prompt. The outputs are collected and stored as raw predictions, ready for Output Refinement.

\subsection{Output Refinement}
\label{subsec:post_processing}
We refine the raw outputs from the LLM to ensure alignment with the target schema, addressing challenges related to prediction variability, schema definition differences, and data formatting inconsistencies (e.g., varying date formats). We implement three techniques inspired by related work in data integration~\cite{dong2013big}: \textit{Decoding}, \textit{Schema Mapping}, and \textit{Data Cleaning}. This process results in three sets of predictions: initial predictions, mapped predictions, and cleaned predictions.

\noindent \textbf{Decoding.} The decoding step parses each LLM completion as a JSON object, discarding any that fail to parse. The process then consolidates predictions for each document by reconciling outputs generated across individual pages and chunks. With $N$ completions for $N$ prompts, corresponding to $N$ chunks, the model generates multiple predictions for a single document. Reconciliation ensures a unified document-level output by deduplicating nested predictions and aggregating unique values. If multiple unique values exist for a single entity, they are stored together to preserve variability. The outcome of this step is referred to as the \emph{initial predictions}.

\noindent \textbf{Schema Mapping.} LLMs are expected to return only keys $\{a_1, a_2, ..., a_k\}$ specified in the target schema $S$. 
However, they may occasionally fail to return the keys as expected. E.g., `file date'' is returned instead of ``file\_date''. Such LLM ``overcorrection'' can hinder strict schema conformance.
As a countermeasure, we implement a post-processing step that maps the predicted keys to align with the target schema.
Our mapping step integrates multiple weak-supervision signals, such as \textit{exact matching, partial matching,} and \textit{synonym-based logic}, inspired by the recent techniques for ontology alignment~\cite{furst2023versamatch}.
The outcome of this step is the \emph{mapped predictions}, where entity keys $\{a_1, a_2, ..., a_k\}$ are standardized and fully aligned with the schema $S$.

\noindent \textbf{Data Cleaning.} A common issue concerns the format of the values $T_k$ of predicted key-value pairs $\{(a_1, T_1), (a_2, T_2), ..., (a_k, T_k)\}$. We must standardize formats, such as dates and names, to align with the target schema $S$.
One source of error is LLM hallucinations~\cite{ji2023survey, huang2023survey, xu2024hallucination}, while another problem is that information is often not aligned to a common format inside the source data. For instance, two documents might use two different formats for dates (``April 1992'' vs ``1992-04-01''). Additional issues include capitalization, redundant whitespace, or special characters. We utilize the regex-defined data types in our schema to automatically apply data cleaning functions. 
The outcome of this step is the \emph{cleaned predictions}, representing the final fully normalized outputs.

\noindent \textbf{Evaluation Techniques}.
\label{subsec:evaluation_techniques} 
Evaluating IE for LRDs requires comparing the extracted data against an annotated test dataset. We implement three metrics for this evaluation: \textit{exact match, substring match}, and \textit{fuzzy match}.








\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item \textbf{Exact Match} searches for perfect alignment between predicted and ground truth values. A match is valid only when the values are identical. This strict approach is ideal for extracting specific, unambiguous entities like dates or numerical identifiers.

\item \textbf{Substring Match} checks whether ground truth values are fully contained within the predicted values as complete substrings, without being split or partially matched. It ensures all ground truth values appear in their entirety within predictions, making it effective for tasks such as extracting full names or addresses, where additional contextual details (e.g., titles like Mr. and Mrs.) may be included in the predictions without making the extraction incorrect.

\item  \textbf{Fuzzy Match} uses similarity metrics for approximate matches. A match is valid if the highest similarity ratio exceeds a predefined threshold (default: 0.8). This method is well-suited for scenarios with minor variations caused by OCR errors or formatting discrepancies.
\end{itemize}




















