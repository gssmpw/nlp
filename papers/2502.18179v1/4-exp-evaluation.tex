\section{Experimental Evaluation}
\label{sec:experimental_evaluation}

\subsection{Experimental Design}
\paragraph{Methodology.} The goal of our experimental setup is to study how different parameters in the pipeline, shown in Figure~\ref{fig:pipeline}, affect the overall performance of IE from LRDs using LLMs. To investigate the design dimensions, we start with a baseline configuration and and systematically alter factors at a single dimension at a time, following a one-factor-at-a-time (OFAT) methodology. This approach allows us to isolate and understand the impact of each parameter change on the IE performance.

Our intuition is that aggregating the knowledge gained for each dimension independently, we can achieve a deeper understanding of the design space and possibly identify an effective overall configuration for IE, without the need for a comprehensive factorial exploration. However, we also validate the findings by comparing the results of the OFAT method with those obtained from a brute-force approach, which is based on conducting 432 experiments.

\paragraph{Dataset and LLMs.}
We utilize the Visually Rich Document Understanding (VRDU) dataset~\cite{wang2023vrdu}, which includes two benchmarks. Each benchmark includes training samples of 10, 50, 100, and 200 documents with high-quality OCR for assessing data efficiency, as well as generalization tasks: \textit{Single Template Learning (STL), Unseen Template Learning (UTL)}, and \textit{Mixed Template Learning (MTL)}. For further details on how this dataset is tailored for our experiments and diverse models, please refer to Appendix~\ref{appendix:dataset_details}.

We evaluate GPT-3.5, GPT-4o, and LLaMA3-70B for text-only structured data extraction from LRDs.
Additionally, we compare their results with GPT-4 Vision and LayoutLMv3 to assess the performance gap between multimodal LLMs and domain-specific, fine-tuned models, respectively.

\paragraph{The Baseline Configuration.}
The baseline configuration is outlined in Table~\ref{tab:all_conf},%
where the configuration is selected based on best practices such as in \cite{perot-etal-2024-lmdx}
for the following reasons: (1) \textbf{OCR} reflects real-world scenarios for digitized LRDs. (2) \textbf{Medium chunk size} balances efficiency and context preservation, addressing token limits in LLMs. (3) \textbf{Few-shot prompting} combines pre-trained knowledge with minimal task-specific guidance. (4) Using \textbf{zero examples} provides a clear benchmark for assessing the model's raw performance. (5) \textbf{Initial predictions} are retained to evaluate models' raw output without modifications, ensuring a direct assessment of their capabilities. (6) Finally, \textbf{exact match} provides a stringent measure of correctness, offering a reliable baseline for comparison across configurations.

\begin{table}[h!]
\centering
\footnotesize
\caption{Overall configuration parameters. Baseline configuration is highlighted with \colorbox{blue!10}{light blue}.}
\label{tab:all_conf}
\begin{tabular}{p{3.2cm}p{3.6cm}}
\toprule  
\textbf{Parameter}       & \textbf{Values}                         \\ \midrule
Input Type              & \colorbox{blue!10}{OCR}, Markdown          \\ 
Chunk Size Category     & Small, \colorbox{blue!10}{Medium}, Max     \\ 
Prompt Type             & \colorbox{blue!10}{Few-Shot}, CoT         \\ 
Example Number          & \colorbox{blue!10}{0}, 1, 3, 5             \\ 
Post-processing Strategy & \shortstack[l]{\colorbox{blue!10}{Initial}, Mapped, Cleaned} \\
Evaluation Technique    & \shortstack[l]{\colorbox{blue!10}{Exact}, Substring, Fuzzy} \\
 \bottomrule
\end{tabular}
\end{table}


\subsection{The Input Dimension}

We substitute OCR input with Markdown and as outcomes in both STL and UTL scenarios. The differences in performance between OCR and Markdown are model- and context-dependent, exhibiting no consistent trend favoring one input type over the other, as shown in Table~\ref{tab:input_type}.

\begin{table}[h]
    \centering %
    \scriptsize %
    \caption{Performance results for different LLMs across \textbf{STL} and \textbf{UTL} levels with different input types. Baseline configuration in \colorbox{blue!10}{light blue}.} 
    \label{tab:input_type}  
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l l l l}
        \toprule  
        \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Level}} & \multicolumn{2}{c}{\textbf{Exact Match (F1)}} \\  
        \cmidrule(lr){3-4}
        & & \textbf{OCR} & \textbf{Markdown} \\
        \midrule
        \multirow{2}{*}{GPT-3.5} & STL  & \colorbox{blue!10}{0.650} & 0.647 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.003) }} \\  
                                 & UTL  & \colorbox{blue!10}{0.645} & 0.657 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.012) }} \\ 
        \cmidrule(lr){2-4}
        \multirow{2}{*}{GPT-4o} & STL  & \colorbox{blue!10}{0.670} & 0.633 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.037) }} \\  
                                 & UTL  & \colorbox{blue!10}{0.659} & 0.633 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.026) }} \\  
        \cmidrule(lr){2-4}
        \multirow{2}{*}{LLaMA3} & STL  & \colorbox{blue!10}{0.640} & 0.657 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.017) }} \\ 
                                    & UTL  & \colorbox{blue!10}{0.640} & 0.662 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.022) }} \\ 
    \bottomrule  
    Avg \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm stdev.) }} & & \colorbox{blue!10}{0.650 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }}} & 0.648 \textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.012) }}
    \end{tabular}
\end{table}

OCR input serves as a stable baseline for IE tasks, delivering consistent performance across models. GPT-4o has noticeable performance drops with Markdown input, indicating its reliance on OCR for optimal results. In contrast, Markdown marginally improves performance for LLaMA3-70B at both STL and UTL scenarios, suggesting its potential benefits from the additional structure or semantic cues. GPT-3.5 demonstrates robustness to changes in input type, with only slight fluctuations in performance. On average, OCR marginally outperforms Markdown (0.650 vs. 0.648), but the differences are minor, with standard deviations indicating similar stability. 

\subsection{The Chunk Dimension}

To evaluate the impact of chunk size, we varied it from medium to max and small while keeping all other parameters constant.
Table~\ref{tab:chunk_size} demonstrates how chunk size affects performance across STL and UTL levels.
\begin{table}[h]
    \small
    \caption{Performance results for different LLMs across \textbf{STL} and \textbf{UTL} levels with different chunk size categories. Baseline configuration in \colorbox{blue!10}{light blue}.} 
    \label{tab:chunk_size}
    \resizebox{\linewidth}{!}{
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l c c c c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Level}} & \multicolumn{3}{c}{\textbf{Exact Match (F1)}} \\
    \cmidrule(lr){3-5}
    & & \textbf{Small\textbf{{\color{gray}\fontsize{7}{8.4}\selectfont($\leq 1024$)}}} & \textbf{Medium\textbf{{\color{gray}\fontsize{7}{8.4}\selectfont($\leq 2048$)}}} & \textbf{Max\textbf{{\color{gray}\fontsize{7}{8.4}\selectfont($\leq 4096$)}}} \\
    \midrule
    \multirow{2}{*}{GPT-3.5} & STL & 0.562\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.088) }} & \colorbox{blue!10}{0.650} & 0.645\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.005) }} \\
             & UTL & 0.561\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.084) }} & \colorbox{blue!10}{0.645} & 0.644\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.001) }} \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{GPT-4o} & STL & 0.602\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.068) }} & \colorbox{blue!10}{0.670} & 0.674\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.004) }} \\
             & UTL & 0.600\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.059) }} & \colorbox{blue!10}{0.659} & 0.657\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.002) }} \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{LLaMA3} & STL & 0.615\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.025) }} & \colorbox{blue!10}{0.640} & 0.647\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.007) }} \\
               & UTL & 0.608\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.032) }} & \colorbox{blue!10}{0.640} & 0.644\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.004) }} \\
    \bottomrule
    Avg\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm stdev.) }} & & 0.591\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.023) }} & \colorbox{blue!10}{0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }}} & 0.651\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }}
    \end{tabular}}
\end{table}

Medium and max chunk sizes provide the most consistent and stable results across models, with an average F1 score of 0.650 (±0.011) and 0.651 (±0.011), respectively.
Due to insufficient context, small chunk sizes result in significant performance drops, particularly for GPT-3.5 and GPT-4o.
\emph{These findings suggest that max chunk size is optimal, but medium can be a good option for LLMs with limited context lengths.}.

\subsection{The Prompt Dimension}

Table~\ref{tab:prompt_type_with_examples} presents the impact of prompt type and the number of examples on model performance at STL and UTL levels. Surprisingly, in-context demonstrations do not enhance performance for either few-shot or CoT experiments. For both experiments, the setting with zero examples achieves the highest average performance: few-shot 0.650 (±0.011) and CoT 0.649 (±0.008). Performance consistently declines as the number of examples increase, likely due to noise that impairs generalization. \textit{Overall, there is no significant difference between few-shot and CoT.}

\begin{table}[h]
    \centering %
    \scriptsize %
    \caption{Different LLMs across \textbf{STL} and \textbf{UTL} levels with different prompt types and example numbers. Baseline Configuration is highlighted in \colorbox{blue!10}{light blue}.}
\label{tab:prompt_type_with_examples}  
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{l c c c c c}
    \toprule  
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Level}} & 
    \multicolumn{4}{c}{\textbf{Exact Match (F1)}} \\  
    \cmidrule(lr){3-6}
    & & \textbf{0} & \textbf{1} & \textbf{3} & \textbf{5} \\  
    \midrule
    \rowcolor{black!10!} \multicolumn{6}{c}{\textbf{\textit{few-shot}}} \\
    \multirow{2}{*}{GPT-3.5} & STL & \colorbox{blue!10}{0.650} & 0.586\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.064) }} & 0.593\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.057) }} & 0.548\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.102) }} \\  
                              & UTL & \colorbox{blue!10}{0.645} & 0.566\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.079) }} & 0.564\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.081) }} & 0.541\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.104) }} \\  
    \cmidrule(lr){2-6}
    \multirow{2}{*}{GPT-4o} & STL & \colorbox{blue!10}{0.670} & 0.608\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.062) }} & 0.602\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.068) }} & 0.595\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.075) }} \\  
                              & UTL & \colorbox{blue!10}{0.659} & 0.597\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.062) }} & 0.607\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.052) }} & 0.601\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.058) }} \\  
    \cmidrule(lr){2-6} 
    \multirow{2}{*}{LLaMA3} & STL & \colorbox{blue!10}{0.640} & 0.599\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.041) }} & 0.606\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.034) }} & 0.603\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.037) }} \\  
                                & UTL & \colorbox{blue!10}{0.640} & 0.582\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.058) }} & 0.601\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.039) }} & 0.597\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.043) }} \\
    \midrule
  
    Avg\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm stdev.) }} &  & \colorbox{blue!10}{0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }}} & 0.589\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.014) }}& 0.595\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.016) }}& 0.580\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.028) }} \\

    \rowcolor{black!10!} \multicolumn{6}{c}{\textbf{\textit{CoT}}} \\
    \addlinespace[0.7mm]
    \multirow{2}{*}{GPT-3.5} & STL & 0.653\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.003) }} & 0.602\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.048) }} & 0.544\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.106) }} & 0.533\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.117) }} \\  
                              & UTL & 0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.005) }} & 0.575\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.007) }} & 0.548\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.097) }} & 0.516\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.129) }} \\  
    \cmidrule(lr){2-6}
    \multirow{2}{*}{GPT-4o} & STL & 0.655\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.015) }} & 0.615\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.055) }} & 0.612\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.058) }} & 0.605\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.065) }} \\  
                              & UTL & 0.659\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.614\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.045) }} & 0.611\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.048) }} & 0.607\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.052) }} \\  
    \cmidrule(lr){2-6} 
    \multirow{2}{*}{LLaMA3} & STL & 0.635\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.005) }} & 0.603\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.037) }} & 0.613\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.027) }} & 0.610\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.003) }} \\  
                                & UTL & 0.644\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.004) }} & 0.586\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.054) }} & 0.601\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.039) }} & 0.598\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(-0.042) }} \\  
    \midrule

    
    Avg\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm stdev.) }} &  & 0.649\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.008) }} & 0.599\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.015) }}& 0.588\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.032) }}& 0.578\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.042) }}\\
        \bottomrule

    \end{tabular}
\end{table}

\subsection{Output Refinement}

We examine two output refinement strategies, Schema Mapping and Data Cleaning (see Sec.~\ref{subsec:post_processing}), to evaluate their impact shown in Table~\ref{tab:post_processing}.
\begin{table}[h]
    \small
    \caption{Different LLMs across \textbf{STL} and \textbf{UTL} levels with different post-processing strategies. Baseline configuration is highlighted in \colorbox{blue!10}{light blue}.} 
    \label{tab:post_processing}
    \resizebox{\linewidth}{!}{
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l c c c c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Level}} & \multicolumn{3}{c}{\textbf{Exact Match (F1)}} \\
    \cmidrule(lr){3-5}
    & & \textbf{Initial Pred.} & \textbf{Mapped Pred.} & \textbf{Cleaned Pred.} \\
    \midrule
    \multirow{2}{*}{GPT-3.5} & STL & \colorbox{blue!10}{0.650} & 0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.737\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.087) }} \\
             & UTL & \colorbox{blue!10}{0.645} & 0.645\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.733\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.088) }} \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{GPT-4o} & STL & \colorbox{blue!10}{0.670} & 0.670\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.749\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.079) }} \\
             & UTL & \colorbox{blue!10}{0.659} & 0.659\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.741\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.082) }} \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{LLaMA3} & STL & \colorbox{blue!10}{0.640} & 0.640\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.724\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.084) }} \\
               & UTL & \colorbox{blue!10}{0.640} & 0.640\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(0) }} & 0.725\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.085) }} \\
    \bottomrule
    Avg\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm stdev.) }} & & \colorbox{blue!10}{0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }}} & 0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }} & 0.734\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.009) }}
    \end{tabular}}
\end{table}

\paragraph{Schema Mapping} involves mapping the predicted schema keys to the target schema keys. Our results show no change in F1 scores compared to the initial predictions. This suggests that the models already effectively return the correct attributes, making the mapping step unnecessary.

\paragraph{Data Cleaning} uses the defined data types to perform automatic value cleaning, consistently achieving the highest F1 scores across all models. \emph{This underscores the need for post-processing steps for IE with LLMs to align the extracted data with the target format to handle LLM hallucinations and inconsistent source data formats (see Sec.~\ref{subsec:post_processing}).}

\subsection{Evaluation Techniques}

We explore three evaluation techniques to assess their impact on model performance (see Sec.~\ref{subsec:evaluation_techniques}).
On average, Fuzzy Match achieved the highest F1 score (0.733), outperforming Substring Match (0.676) and Exact Match, as shown in Table~\ref{tab:evaluation_techniques}. We provide a detailed error analysis of fuzzy and substring match accuracy in Appendix~\ref{appendix:eval_tech}, showing that they provide a near-perfect precision when manually checked for semantic equivalence with precision scores of 0.98 and 1.00, respectively.
\textit{This shows Fuzzy Match's ability to balance flexibility and precision. 
}
\begin{table}[h]
    \small
    \caption{ Different LLMs across \textbf{STL} and \textbf{UTL} levels with different evaluation techniques. Baseline configuration in \colorbox{blue!10}{light blue}.}
    \label{tab:evaluation_techniques}
    \resizebox{\linewidth}{!}{
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l c c c c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Level}} & \textbf{Exact Match} & \textbf{Substring Match} & \textbf{Fuzzy Match} \\
    & & \textbf{(F1)} & \textbf{(F1)} & \textbf{(F1)} \\
    \midrule
    \multirow{2}{*}{GPT-3.5} & STL & \colorbox{blue!10}{0.650} & 0.683\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.033) }} & 0.730\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.080) }} \\
             & UTL & \colorbox{blue!10}{0.645} & 0.682\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.037) }} & 0.726\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.081) }} \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{GPT-4o} & STL & \colorbox{blue!10}{0.670} & 0.690\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.020) }} & 0.750\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.080) }} \\
             & UTL & \colorbox{blue!10}{0.659} & 0.678\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.019) }} & 0.744\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.085) }} \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{LLaMA3} & STL & \colorbox{blue!10}{0.640} & 0.661\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.021) }} & 0.727\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.087) }} \\
               & UTL & \colorbox{blue!10}{0.640} & 0.662\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.022) }} & 0.723\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(+0.083) }} \\
    \bottomrule
    Avg\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm stdev.) }} & & \colorbox{blue!10}{0.650\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }}} & 0.676\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.011) }} & 0.733\textbf{{\color{gray}\fontsize{5.5}{8.4}\selectfont(\textpm 0.010) }}
    \end{tabular}}
\end{table}

\subsection{Putting it All Together}

In the preceding sections, we investigated the influence of various parameters on model performance along the IE extraction pipeline, analyzing one factor at a time. Drawing from the underlying 12 experiments, we identified the optimal parameter for each step and each model based on the experimental outcomes (Table~\ref{tab:OFAT_configurations_per_model}). In addition, we conducted an exhaustive full factorial exploration with 432 configurations ($2*3*2*4*3*3=432$, see Table~\ref{tab:all_conf}) to find the best parametrization per LLM (Table~\ref{tab:Brute_force_configurations_per_model}). Lastly, we find the worst configuration on a per LLM and per model basis (Table~\ref{tab:worst_configurations_per_model}).
The performance of these different configurations is depicted in Figure~\ref{fig:configuration_comparison}.
We gain several insights:

\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]

\item \emph{OFAT approximates well the Brute-Force configuration with a fraction ($\sim2.8\%$) of the required computation.} We see in Table~\ref{tab:OFAT_configurations_per_model} and Table~\ref{tab:Brute_force_configurations_per_model} that they match except for 4 parameter choices (\texttt{prompt} and \texttt{example No.} parameters for GPT-4o and LlaMa3). Likewise, their F1 scores are close to each other (Figure~\ref{fig:configuration_comparison}), with OFAT achieving 0.791 and Brute-Force achieving 0.801 overall.


\item \emph{Adapting the IE pipeline to the LLM is necessary to achieve a competitive performance.} 
Overall, the OFAT configuration improves from the baseline F1 of 0.650 to 0.791, a $22\%$ improvement. For Brute-Force, the improvement is $23\%$. In comparison, the worst configuration achieves only 0.45 on average, almost half of the best configuration.

\item \emph{Besides the need for pipeline customization, common patterns exist across LLMs.} First, output refinement and evaluation techniques boost performance across all models. Second, there is a tendency to larger context sizes. Lastly, larger models (GPT-4o, LLaMa3) benefit more from examples, while the CoT pattern generally aids IE.

\end{itemize}

\begin{table}[h]
\centering
\footnotesize
\caption{OFAT configurations on a per-model basis and corresponding performance results.}
\begin{tabular}{lcccc}
\toprule
\textbf{Parameter} & \textbf{GPT-3.5} & \textbf{GPT-4o} & \textbf{LLaMA3} \\
\midrule
Input Type & Markdown & OCR & Markdown  \\
Chunk Size & Medium & Max & Max \\
Prompt & CoT & Few-Shot & Few-Shot &\\
Example No. & 0 & 0 & 0 \\
Output Refin. & Cleaned & Cleaned & Cleaned \\
Evaluation & Fuzzy & Fuzzy & Fuzzy \\
\bottomrule
\end{tabular}
\label{tab:OFAT_configurations_per_model}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\caption{Brute-Force configurations on a per-model basis and corresponding performance results.}
\begin{tabular}{lcccc}
\toprule
\textbf{Parameter} & \textbf{GPT-3.5} & \textbf{GPT-4o} & \textbf{LLaMA3} \\
\midrule
Input Type & Markdown & OCR & Markdown  \\
Chunk Size & Medium & Max & Max \\
Prompt & CoT & CoT & CoT &\\
Example No. & 0 & 5 & 5 \\
Output Refin. & Cleaned & Cleaned & Cleaned \\
Evaluation & Fuzzy & Fuzzy & Fuzzy \\
\bottomrule
\end{tabular}
\label{tab:Brute_force_configurations_per_model}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\caption{Worst performance configurations on a per-model basis and corresponding performance results.}
\begin{tabular}{lcccc}
\toprule
\textbf{Parameter} & \textbf{GPT-3.5} & \textbf{GPT-4o} & \textbf{LLaMA3} \\
\midrule
Input Type & OCR & Markdown & OCR  \\
Chunk Size & Small & Small & Small \\
Prompt & Few-shot & Few-shot & Few-shot \\
Example No. & 5 & 5 & 5 \\
Output Refin. & Initial & Initial & Initial \\
Evaluation & Exact & Exact & Exact \\
\bottomrule
\end{tabular}
\label{tab:worst_configurations_per_model}
\end{table}

\input{figures/configuration_comparison}

Finally, we compare our purely text-based approach to IE to (1) GPT4-vision, a multimodal LLM, and (2) LayoutLMv3, a leading layout-aware model that we fine-tune to our dataset (Table~\ref{tab:model_comparison}).
Despite fine-tuning, LayoutLMv3 does not perform as well as our evaluated LLMs, even on documents that contain the same structure as the fine-tuning data (STL). We also observe that LLMs remain stable across template types. The best-performing model is GPT4-vision, a multi-modal LLM, where we directly provide an image of the PDF for IE. The model benefits from less context loss due to chunking and its ability to jointly use textual and visual features. However, when considering token usage and cost, GPT4-vision requires $\sim2$ times the tokens compared to text-only approaches and $>10$ times the cost under the current OpenAI pricing scheme (Nov. 2024). Therefore, text-only approaches constitute a good trade-off between performance and cost for practical applications.

\begin{table}[h]
    \centering %
    \footnotesize %
    \caption{Cost (per experiment) and performance of LLMs across \textbf{STL} and \textbf{UTL} levels for the OFAT configuration. The tokens for LayoutLMv3 include the fine-tuning. $^*$We run LauoutLMv3 and LlaMa3 locally, without API costs. } 
    \label{tab:model_comparison}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l r l l l}
    \toprule
    \textbf{Models} & \textbf{Tokens} & \textbf{API Cost} & \textbf{STL (F1)} & \textbf{UTL (F1)} \\
    \midrule
    GPT-3.5                          & 322K  & \$0.18  & 0.778  & 0.754  \\ 
    GPT-4o                           & 278K  & \$0.40  & 0.797  & 0.790  \\
    LLaMA3                           & 239K  & \$0$^*$      & 0.843  & 0.820  \\
    GPT-4-vision                     & 585K  & \$4.76   & 0.902  & 0.897  \\
    LayoutLMv3                       & 176.1M  & \$0$^*$      & 0.603  & 0.194  \\
    \bottomrule
    \end{tabular}
    \rmspace
\end{table}



    








