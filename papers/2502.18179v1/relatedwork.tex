\section{Related Work}
\label{sec:related_work}

\paragraph{IE using LLMs.} Transformer-based models like BERT~\cite{devlin2019bert}, GPT~\cite{brown2020language}, and RoBERTa~\cite{liu2019roberta} have advanced NLP with self-attention and large-scale pretraining but struggle with layout-rich documents (LRDs). To address this, layout-aware and multimodal models have emerged. LayoutLM~\cite{xu2020layoutlm} integrates spatial features, with later versions~\cite{xu2021layoutlmv2, xu2022layoutlmv3} and models like GPT-4V~\cite{openai2023gpt4} and Gemini Pro~\cite{anil2023gemini} enhancing document understanding through multimodal learning. LayoutLLM~\cite{fujitake-2024-layoutllm} and structural-aware approaches~\cite{lee2022formnetstructuralencodingsequential} further improve extraction accuracy, while end-to-end models like Donut~\cite{kim2022ocr} bypass OCR for direct document image processing. Additionally, ChatUIE~\cite{xu-etal-2024-chatuie} adopts a chat-based approach for flexible IE, while ReLayout~\cite{jiang-etal-2025-relayout} enhances document understanding through layout-aware pretraining, advancing LLMs for real-world use.

\paragraph{Strategies for IE from LRDs.} Graph-based models like GCNs~\cite{liu-etal-2019-graph} and AliGATr~\cite{nourbakhsh-etal-2024-aligatr} enhance relation extraction by capturing textual-visual relationships. Reading order is critical; Token Path Prediction (TPP)~\cite{zhang-etal-2023-reading} resolves OCR layout ambiguities, while global tagging~\cite{shaojie-etal-2023-document} mitigates text ordering issues for better extraction. For structured data, TabbyPDF~\cite{jain2020tabbypdf} targets table extraction, DocExtractor~\cite{zhong2020docextractor} processes forms, and LMDX~\cite{perot-etal-2024-lmdx} unifies OCR, preprocessing, and postprocessing for document IE. Additionally, XFORMPARSER~\cite{cheng-etal-2025-xformparser} offers a simple yet effective multimodal and multilingual approach for parsing semi-structured forms.

\paragraph{Preprocessing, Chunking, Prompting, Postprocessing, and Evaluation Techniques.} Chain-of-Thought (CoT) prompting~\cite{wei2022cot} enhances reasoning in complex LRD extraction tasks, while diverse prompt-response datasets~\cite{zmigrod-etal-2024-value} improve LLM robustness. Instruction-finetuned LLMs have also demonstrated effectiveness in domain-specific applications, such as clinical and biomedical tasks, where zero-shot and few-shot learning enable adaptive extraction without extensive fine-tuning~\cite{labrak-etal-2024-zero}. Postprocessing techniques, including text normalization, entity resolution~\cite{hwang2021entityresolution}, and majority voting~\cite{wang2022majority}, refine extracted data by correcting OCR and extraction errors for greater accuracy.

Despite advancements, current studies often focus on isolated components rather than evaluating full IE pipelines. Key stages such as OCR, chunking, and postprocessing are frequently assessed independently, leading to an incomplete understanding of their interplay. Our research bridges this gap by systematically evaluating strategies across all pipeline stages to identify optimal configurations for leveraging pre-trained models.