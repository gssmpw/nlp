@article{anil2023gemini,
  title={Gemini Pro: Integrating Multimodal Capabilities for Enhanced Information Extraction},
  author={Anil, Rohan and others},
  journal={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{cheng-etal-2025-xformparser,
    title = "{XF}orm{P}arser: A Simple and Effective Multimodal Multilingual Semi-structured Form Parser",
    author = "Cheng, Xianfu  and
      Zhang, Hang  and
      Yang, Jian  and
      Li, Xiang  and
      Zhou, Weixiao  and
      Liu, Fei  and
      Wu, Kui  and
      Guan, Xiangyuan  and
      Sun, Tao  and
      Wu, Xianjie  and
      Li, Tongliang  and
      Li, Zhoujun",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.41/",
    pages = "606--620",
    abstract = "In the domain of Document AI, parsing semi-structured image form is a crucial Key Information Extraction (KIE) task. The advent of pre-trained multimodal models significantly empowers Document AI frameworks to extract key information from form documents in different formats such as PDF, Word, and images. Nonetheless, form parsing is still encumbered by notable challenges like subpar capabilities in multilingual parsing and diminished recall in industrial contexts in rich text and rich visuals. In this work, we introduce a simple but effective Multimodal and Multilingual semi-structured FORM PARSER (XFormParser), which is anchored on a comprehensive Transformer-based pre-trained language model and innovatively amalgamates semantic entity recognition (SER) and relation extraction (RE) into a unified framework. Combined with Bi-LSTM, the performance of multilingual parsing is significantly improved. Furthermore, we develop InDFormSFT, a pioneering supervised fine-tuning (SFT) industrial dataset that specifically addresses the parsing needs of forms in a variety of industrial contexts. Through rigorous testing on established benchmarks, XFormParser has demonstrated its unparalleled effectiveness and robustness. Compared to existing state-of-the-art (SOTA) models, XFormParser notably achieves up to 1.79{\%} F1 score improvement on RE tasks in language-specific settings. It also exhibits exceptional improvements in cross-task performance in both multilingual and zero-shot settings."
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019}
}

@inproceedings{fujitake-2024-layoutllm,
    title = "{L}ayout{LLM}: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
    author = "Fujitake, Masato",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.892/",
    pages = "10219--10224",
    abstract = "This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks."
}

@article{hwang2021entityresolution,
  title={Entity Resolution in Heterogeneous Data Sources: A Survey},
  author={Hwang, Sunghwan and others},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021}
}

@article{jain2020tabbypdf,
  title={TabbyPDF: Table Structure Recognition in PDF Documents},
  author={Jain, Priya and others},
  journal={arXiv preprint arXiv:2007.12308},
  year={2020}
}

@inproceedings{jiang-etal-2025-relayout,
    title = "{R}e{L}ayout: Towards Real-World Document Understanding via Layout-enhanced Pre-training",
    author = "Jiang, Zhouqiang  and
      Wang, Bowen  and
      Chen, Junhao  and
      Nakashima, Yuta",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.255/",
    pages = "3778--3793",
    abstract = "Recent approaches for visually-rich document understanding (VrDU) uses manually annotated semantic groups, where a semantic group encompasses all semantically relevant but not obviously grouped words. As OCR tools are unable to automatically identify such grouping, we argue that current VrDU approaches are unrealistic. We thus introduce a new variant of the VrDU task, real-world visually-rich document understanding (ReVrDU), that does not allow for using manually annotated semantic groups. We also propose a new method, ReLayout, compliant with the ReVrDU scenario, which learns to capture semantic grouping through arranging words and bringing the representations of words that belong to the potential same semantic group closer together. Our experimental results demonstrate the performance of existing methods is deteriorated with the ReVrDU task, while ReLayout shows superiour performance."
}

@inproceedings{kim2022ocr,
  title={Ocr-free document understanding transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle={European Conference on Computer Vision},
  pages={498--517},
  year={2022},
  organization={Springer}
}

@inproceedings{labrak-etal-2024-zero,
    title = "A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks",
    author = "Labrak, Yanis  and
      Rouvier, Mickael  and
      Dufour, Richard",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.185/",
    pages = "2049--2066",
    abstract = "The recent emergence of Large Language Models (LLMs) has enabled significant advances in the field of Natural Language Processing (NLP). While these new models have demonstrated superior performance on various tasks, their application and potential are still underexplored, both in terms of the diversity of tasks they can handle and their domain of application. In this context, we evaluate four state-of-the-art instruction-tuned LLMs (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on a set of 13 real-world clinical and biomedical NLP tasks in English, including named-entity recognition (NER), question-answering (QA), relation extraction (RE), and more. Our overall results show that these evaluated LLMs approach the performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, particularly excelling in the QA task, even though they have never encountered examples from these tasks before. However, we also observe that the classification and RE tasks fall short of the performance achievable with specifically trained models designed for the medical field, such as PubMedBERT. Finally, we note that no single LLM outperforms all others across all studied tasks, with some models proving more suitable for certain tasks than others."
}

@misc{lee2022formnetstructuralencodingsequential,
      title={FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction}, 
      author={Chen-Yu Lee and Chun-Liang Li and Timothy Dozat and Vincent Perot and Guolong Su and Nan Hua and Joshua Ainslie and Renshen Wang and Yasuhisa Fujii and Tomas Pfister},
      year={2022},
      eprint={2203.08411},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.08411}, 
}

@inproceedings{liu-etal-2019-graph,
    title = "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
    author = "Liu, Xiaojing  and
      Gao, Feiyu  and
      Zhang, Qiong  and
      Zhao, Huasha",
    editor = "Loukina, Anastassia  and
      Morales, Michelle  and
      Kumar, Rohit",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-2005/",
    doi = "10.18653/v1/N19-2005",
    pages = "32--39"
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{nourbakhsh-etal-2024-aligatr,
    title = "{A}li{GAT}r: Graph-based layout generation for form understanding",
    author = "Nourbakhsh, Armineh  and
      Jin, Zhao  and
      Parekh, Siddharth  and
      Shah, Sameena  and
      Rose, Carolyn",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.778/",
    doi = "10.18653/v1/2024.findings-emnlp.778",
    pages = "13309--13328",
    abstract = "Forms constitute a large portion of layout-rich documents that convey information through key-value pairs. Form understanding involves two main tasks, namely, the identification of keys and values (a.k.a Key Information Extraction or KIE) and the association of keys to corresponding values (a.k.a. Relation Extraction or RE). State of the art models for form understanding often rely on training paradigms that yield poorly calibrated output probabilities and low performance on RE. In this paper, we present AliGATr, a graph-based model that uses a generative objective to represent complex grid-like layouts that are often found in forms. Using a grid-based graph topology, our model learns to generate the layout of each page token by token in a data efficient manner. Despite using 30{\%} fewer parameters than the smallest SotA, AliGATr performs on par with or better than SotA models on the KIE and RE tasks against four datasets. We also show that AliGATr`s output probabilities are better calibrated and do not exhibit the over-confident distributions of other SotA models."
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={OpenAI},
  year={2023},
  url={https://www.openai.com/research/gpt-4}
}

@inproceedings{perot-etal-2024-lmdx,
    title = "{LMDX}: Language Model-based Document Information Extraction and Localization",
    author = "Perot, Vincent  and
      Kang, Kai  and
      Luisier, Florian  and
      Su, Guolong  and
      Sun, Xiaoyu  and
      Boppana, Ramya Sree  and
      Wang, Zilong  and
      Wang, Zifeng  and
      Mu, Jiaqi  and
      Zhang, Hao  and
      Lee, Chen-Yu  and
      Hua, Nan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.899/",
    doi = "10.18653/v1/2024.findings-acl.899",
    pages = "15140--15168",
    abstract = "Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art and exhibiting emergent capabilities across various tasks. However, their application in extracting information from visually rich documents, which is at the core of many document processing workflows and involving the extraction of key entities from semi-structured documents, has not yet been successful. The main obstacles to adopting LLMs for this task include the absence of layout encoding within LLMs, which is critical for high quality extraction, and the lack of a grounding mechanism to localize the predicted entities within the document. In this paper, we introduce Language Model-based Document Information EXtraction and Localization (LMDX), a methodology to reframe the document information extraction task for a LLM. LMDX enables extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and localizing the entities within the document. Finally, we apply LMDX to the PaLM 2-S and Gemini Pro LLMs and evaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and showing how LMDX enables the creation of high quality, data-efficient parsers."
}

@inproceedings{shaojie-etal-2023-document,
    title = "Document Information Extraction via Global Tagging",
    author = "Shaojie, He  and
      Tianshu, Wang  and
      Yaojie, Lu  and
      Hongyu, Lin  and
      Xianpei, Han  and
      Yingfei, Sun  and
      Le, Sun",
    editor = "Sun, Maosong  and
      Qin, Bing  and
      Qiu, Xipeng  and
      Jiang, Jing  and
      Han, Xianpei",
    booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2023",
    address = "Harbin, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2023.ccl-1.62/",
    pages = "726--735",
    language = "eng"
}

@article{wang2022majority,
  title={Majority Voting for Improved Consistency in Information Extraction Tasks},
  author={Wang, Lin and others},
  journal={arXiv preprint arXiv:2207.05214},
  year={2022}
}

@article{wei2022cot,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and others},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@inproceedings{xu-etal-2024-chatuie,
    title = "{C}hat{UIE}: Exploring Chat-based Unified Information Extraction Using Large Language Models",
    author = "Xu, Jun  and
      Sun, Mengshu  and
      Zhang, Zhiqiang  and
      Zhou, Jun",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.279/",
    pages = "3146--3152",
    abstract = "Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations. Extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. In this paper, we present ChatUIE, an innovative unified information extraction framework built upon ChatGLM. Simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE can significantly improve the performance of information extraction with a slight decrease in chatting ability."
}

@inproceedings{xu2020layoutlm,
  title={Layoutlm: Pre-training of text and layout for document image understanding},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1192--1200},
  year={2020}
}

@article{xu2021layoutlmv2,
  title={LayoutLMv2: Multi-modal pre-training for visually-rich document understanding},
  author={Xu, Yiheng and Xu, Jingjing and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2012.14740},
  year={2021}
}

@article{xu2022layoutlmv3,
  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2204.08387},
  year={2022}
}

@inproceedings{zhang-etal-2023-reading,
    title = "Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction",
    author = "Zhang, Chong  and
      Guo, Ya  and
      Tu, Yi  and
      Chen, Huan  and
      Tang, Jinyang  and
      Zhu, Huijia  and
      Zhang, Qi  and
      Gui, Tao",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.846/",
    doi = "10.18653/v1/2023.emnlp-main.846",
    pages = "13716--13730",
    abstract = "Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequence-labeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For better evaluation of VrD-NER systems, we also propose two revised benchmark datasets of NER on scanned documents which can reflect real-world scenarios. Experiment results demonstrate the effectiveness of our method, and suggest its potential to be a universal solution to various information extraction tasks on documents."
}

@article{zhong2020docextractor,
  title={DocExtractor: An End-to-End System for Information Extraction from Forms and Receipts},
  author={Zhong, Jian and others},
  journal={arXiv preprint arXiv:2012.04573},
  year={2020}
}

@inproceedings{zmigrod-etal-2024-value,
    title = "{\textquotedblleft}What is the value of {templates}?{\textquotedblright} Rethinking Document Information Extraction Datasets for {LLM}s",
    author = "Zmigrod, Ran  and
      Shetty, Pranav  and
      Sibue, Mathieu  and
      Ma, Zhiqiang  and
      Nourbakhsh, Armineh  and
      Liu, Xiaomo  and
      Veloso, Manuela",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.770/",
    doi = "10.18653/v1/2024.findings-emnlp.770",
    pages = "13162--13185",
    abstract = "The rise of large language models (LLMs) for visually rich document understanding (VRDU) has kindled a need for prompt-response, document-based datasets. As annotating new datasets from scratch is labor-intensive, the existing literature has generated prompt-response datasets from available resources using simple templates. For the case of key information extraction (KIE), one of the most common VRDU tasks, past work has typically employed the template {\textquotedblleft}What is the value for the key?{\textquotedblright}. However, given the variety of questions encountered in the wild, simple and uniform templates are insufficient for creating robust models in research and industrial contexts. In this work, we present K2Q, a diverse collection of five datasets converted from KIE to a prompt-response format using a plethora of bespoke templates. The questions in K2Q can span multiple entities and be extractive or boolean. We empirically compare the performance of seven baseline generative models on K2Q with zero-shot prompting. We further compare three of these models when training on K2Q versus training on simpler templates to motivate the need of our work. We find that creating diverse and intricate KIE questions enhances the performance and robustness of VRDU models. We hope this work encourages future studies on data quality for generative model training."
}

