\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{listings}       
\usepackage{svg}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{xcolor}         
\usepackage{pifont}
\usepackage{caption}        
\usepackage{tcolorbox}      
\usepackage{booktabs} 
\usepackage{multirow} 
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{float}
\usepackage{balance}
\definecolor{myyellow}{RGB}{255,171,64}
\definecolor{mylightyellow}{RGB}{252,229,205}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{mylightyellow},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,                 
    captionpos=b,                    
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    tabsize=2,
    frame=single,
    rulecolor=\color{myyellow},
    keywordstyle=\color{blue},
    stringstyle=\color{black},
    showstringspaces=false           %
}

\definecolor{myblue}{rgb}{0.2, 0.2, 1.0}

\lstset{style=mystyle}

\tcbset{colback=gray!5, colframe=gray!80, fonttitle=\bfseries}

\usepackage[english, status=draft, margin=false, inline=true]{fixme}
\fxusetheme{color}
\FXRegisterAuthor{jf}{ajf}{\color{red}JF}
\FXRegisterAuthor{gs}{ags}{\color{blue}GS}

\lstdefinelanguage{JSON}{
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\bfseries\color{blue},
    stringstyle=\color{brown},
    commentstyle=\color{gray},
    showstringspaces=false,
    morestring=[b]",
    moredelim=[s][\color{blue}]{:}{\ },
    moredelim=[l][\color{magenta}]{,}
}

\title{Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs}

% \author{
% Gaye Colakoglu$^{1, 2}$, 
% G端rkan Solmaz$^{2}$,
% Jonathan F端rst$^{1}$
% \\
% $^{1}$Zurich University of Applied Sciences, Switzerland \\
% $^{2}$NEC Laboratories Europe, Heidelberg, Germany\\
% colgay01@students.zhaw.ch, gurkan.solmaz@neclab.eu, jonathan.fuerst@zhaw.ch\\
% }

\author{
Gaye Colakoglu$^{1,2}$ \quad G端rkan Solmaz$^{2}$ \quad Jonathan F端rst$^{1}$ \\
$^{1}$Zurich University of Applied Sciences, Switzerland \\
$^{2}$NEC Laboratories Europe, Heidelberg, Germany \\
\texttt{colgay01@students.zhaw.ch, gurkan.solmaz@neclab.eu, jonathan.fuerst@zhaw.ch}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\rmspace}{\vspace{-2ex}}

\begin{document}
\maketitle
\begin{abstract}

This paper defines and explores the design space for information extraction (IE) from \textit{layout-rich documents} using large language models (LLMs).
The three core challenges of layout-aware IE with LLMs are \textit{1) data structuring, 2) model engagement, and 3) output refinement}. Our study delves into the sub-problems within these core challenges, such as input representation, chunking, prompting, and selection of LLMs and multimodal models. It examines the outcomes of different design choices through a new \textit{layout-aware IE test suite}, benchmarking against the state-of-art (SoA) model LayoutLMv3. The results show that the configuration from one-factor-at-a-time (OFAT) trial achieves near-optimal results with $14.1$ points F1-score gain from the baseline model, while full factorial exploration yields only a slightly higher $15.1$ points gain at \(\sim36\times \)  greater token usage. We demonstrate that well-configured general-purpose LLMs can match the performance of specialized models, providing a cost-effective alternative. Our test-suite is freely available at \url{https://github.com/gayecolakoglu/LayIE-LLM}.

\end{abstract}

\input{1-introduction}
\input{2-design-space}
\input{3-test-suite}
\input{4-exp-evaluation}
\balance
\input{5-related-work}
\input{6-conclusion}
\clearpage
\input{7-limitations}

\balance
\bibliography{bibliography.bib}

\clearpage
\appendix %

\section{Appendix}

\subsection{Prompt Generation Details}
\label{appendix:prompt_generation_details}
\textit{Document representation} in the example section of the prompt is generated by the LLM, condensing the original OCR data while retaining information relevant to the target schema to reduce token usage and cost. This version integrates only text features. However, in the new example section, both text and layout features are preserved without modification, embedding spatial structure into text sequences to enhance model understanding. This is the only part that differs between the example and new document sections of the prompt. \textit{Task descriptions} provide clear extraction guidelines, specifying what information to retrieve, while \textit{schema representation} defines the expected JSON format to ensure consistency in extracted data. Additionally, CoT prompting includes a \textit{reasoning} component, guiding the model through logical steps to improve accuracy on complex tasks (see Figures~\ref{fig:few-shot-prompt},~\ref{fig:chain-of-thought-prompt}).

\begin{figure}[!htb]
\centering
\begin{tcolorbox}[colframe=myyellow, colback=mylightyellow, sharp corners=south, title=Prompt Template: Few-shot,
boxsep=0.5mm,
left=1mm,
right=1mm,
top=1mm,
bottom=1mm]
\footnotesize

\begin{lstlisting}[frame=none, 
                  basicstyle=\ttfamily, 
                  tabsize=4, 
                  morekeywords={Document, Task, Extraction},
                  stringstyle=\color{myblue}]
        ### Examples ###
(<Document>) 
{DOCUMENT_REPRESENTATION} 
(</Document>) 
(<Task>) 
{TASK_DESCRIPTION} 
{SCHEMA_REPRESENTATION} 
(</Task>) 
(<Extraction>) 
{EXTRACTION} 
(</Extraction>)
        ### New Documents ###
(<Document>) 
{DOCUMENT_REPRESENTATION} 
(</Document>) 
(<Task>) 
{TASK_DESCRIPTION} 
{SCHEMA_REPRESENTATION} 
(</Task>) 
(<Extraction>)
\end{lstlisting}
\end{tcolorbox}
\caption{Few-Shot Prompt Structure with 1-shot Example }
\label{fig:few-shot-prompt}
\end{figure}

\begin{figure}[ht]
\centering
\begin{tcolorbox}[colframe=myyellow, colback=mylightyellow, sharp corners=south, title=Prompt Template: CoT,
boxsep=0.5mm,
left=1mm,
right=1mm,
top=1mm,
bottom=1mm]
\footnotesize
\begin{lstlisting}[frame=none, 
                  basicstyle=\ttfamily, 
                  tabsize=4, 
                  escapeinside={  }, 
                  morekeywords={Document, Task, Extraction, Reasoning},  
                  stringstyle=\color{myblue}]
        ### Examples ###
(<Document>) 
{DOCUMENT_REPRESENTATION} 
(</Document>) 
(<Task>) 
{TASK_DESCRIPTION} 
{SCHEMA_REPRESENTATION} 
(</Task>)
(<Reasoning>) 
{REASONING} 
(</Reasoning>)
(<Extraction>) 
{EXTRACTION} 
(</Extraction>)
        ### New Documents ###
(<Document>) 
{DOCUMENT_REPRESENTATION} 
(</Document>) 
(<Task>) 
{TASK_DESCRIPTION} 
{SCHEMA_REPRESENTATION} 
(</Task>) 
(<Reasoning>) 
{REASONING} 
(</Reasoning>)
(<Extraction>)
\end{lstlisting}
\end{tcolorbox}
\caption{Chain of Thought Prompt Structure with 1-shot Example }
\label{fig:chain-of-thought-prompt}
\end{figure}

\subsection{Tailoring Dataset for our Test-Suite}
\label{appendix:dataset_details}
We used the Registration Form with six entity types for this project, as shown in Figure~\ref{fig:vrdu_registration_entities}.
\begin{figure}[!htb]
\footnotesize
\begin{lstlisting}[frame=single, basicstyle=\ttfamily, tabsize=4]
{
  "file_date": "",
  "foreign_principle_name": "",
  "registrant_name": "",
  "registration_num": "",
  "signer_name": "",
  "signer_title": ""
}
\end{lstlisting}
\caption{VRDU Registration Form Entities.} 
\label{fig:vrdu_registration_entities}
\end{figure}

The VRDU dataset includes predefined few-shot splits consisting of training, testing, and validation sets. These splits contain 10, 50, 100, and 200 training samples, each with three subsets, as shown in Figure~\ref{fig:dataset_splits}. The dataset also includes different levels (Lv1: Single, Lv2: Mixed, and Lv3: Unseen Type) and various template types (Amendment, Dissemination, and Short-Form). 

\input{figures/dataset_splits}

For each template-level combination, we selected the first JSON file ending in 0 with 10 training samples. Since this training data will be utilized for few-shot and Chain-of-Thought (CoT) prompting, only the first five documents were chosen from the training samples of the selected JSON files. Each level type (STL, UTL) includes template types (Amendment, Dissemination, Short-Form), each with 0, 1, 3, or 5 examples. In STL, these categories use the first document for one-example prompts, the first three for three-example prompts, and all five for five-example prompts. The same structure applies to UTL, with examples specific to its categories. This ensures consistency across template-level combinations while varying the number of examples in the prompt. Figure~\ref{fig:train_examples} shows an example of few-shot and CoT examples. This process was repeated for every level, template type, and example count. The example texts were generated using a Large Language Model (LLM), which was instructed to summarize the provided OCR text for the given document while ensuring the inclusion of target schemas and entities.

\begin{figure}[ht]
\footnotesize
\begin{lstlisting}[frame=single, basicstyle=\ttfamily, tabsize=4]
few_shot_examples = {
  "STL": {
    "Amendment": {
      0: [],
      1: [
        {
          "text": "This document is an amendment to the regis...",
          "entities": {
            "file_date": "1982-10-31",
            "foreign_principle_name": "Japan Trade Center...",
            "registrant_name": "PressAid Center",
            "registration_num": "1833",
            "signer_name": "Akira Tsutsumi",
            "signer_title": "Director General"
          }
        }
      ],
      3: [
        {...},
        {...},
        {...}
      ],
      5: [
        {...},
        {...},
        {...},
        {...},
        {...},
      ]
    },
    "Dissemination": {...
\end{lstlisting}
\caption{VRDU Registration Form Entities.} 
\label{fig:train_examples}
\end{figure}

For the test files, to ensure a fair comparison, we selected the first 40 common files from the chosen JSON files within each template type at each level. This means that for Lv1 Amendment and Lv3 Amendment test files, the first common 40 files were selected as test files, and the same strategy was applied for other template types as well. Due to the mixed nature of test files in Lv2, the mixed template type was excluded from this project. 

For the models GPT-3.5, GPT-4o, and LLaMA3-70B, we created these few-shot and CoT examples. However, no few-shot examples were used for GPT-4 Vision; we used the same test dataset as for the other three models with basic instructions. For LayoutLMv3, we used the same training and test datasets as the other models but included the entire validation set (300 samples) instead of selecting only five. The training and test datasets remained unchanged across all models to ensure a fair evaluation and consistency.

\subsection{Success of Evaluation Techniques}
\label{appendix:eval_tech}

We manually reviewed the results of the baseline experiment from GPT-3.5, GPT-4o, and LLaMA3-70B to assess the success of the substring and fuzzy match metrics. The analysis focused on cases where the exact match score was 0 but substring/fuzzy was 1, highlighting predictions that failed strict matching but were successfully handled in other techniques. This created a dataset to test how well substring and fuzzy matching handle difficult cases. In total, we examined 91 key-value pairs for fuzzy match and 37 for substring match. 


\balance

\input{figures/error_rates}

Based on our intubation, we came up with seven different error categories see Figure~\ref{fig:error_rates}. From left to right in Error Types 5 OCR error means errors occur because of OCR; for example, handwritten names' or signatures are sometimes extracted as different names incorrectly (e.g Jim Slattery handwriting extracted as Jim Slatters) GT error represents the case where GT information is not completely true sometimes its missing some words and sometimes it has additional words for example gt value is 'Om Saudi Arabia 1' but it is Kingdom' Saudi Arabia in the document. LLM Halucination represents where LLM come up a prediction that is not exist data in the OCR for example gt value is for file date '1992-04-21' but prediction is '1992-04-24' even though OCR data and document itself does not contain any number 24. Additional info represents that prediction is correct, but there is some additional info' for example, while the signer's name is 'Daniel Manatt'' LLM predicts it as 'Daniel Manatt Todd' where Todd exists in OCR and the document has a notary name as a handwritten signature. Wrong info means gt is exist in the OCR and the document but LLM choose another date as prediction for example while file date '2016-10-08' exist in OCR LLM return 2016-10-31 as file date which belongs registration date in the document. 
\begin{table}
    \centering %
    \scriptsize %
    \caption{Performance results of substring and fuzzy evaluation techniques over exact match based on manually annotated data, considering categorized error types.} 
    \label{tab:eval_performances}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l c c c c}
    \toprule
    \textbf{Data} & \textbf{Evaluation} & \multirow{2}{*}{\textbf{Precision}} & \multirow{2}{*}{\textbf{Recall}} & \multirow{2}{*}{\textbf{F1}} \\
    \textbf{Points} & \textbf{Techniques} & & & \\
    \midrule
    \multirow{2}{*}{37} & Exact Match & 0.000 & 0.000 & 0.000 \\
             & Substring Match & 1.000 &  1.000 &  1.000  \\
    \cmidrule(lr){2-5}
    \multirow{2}{*}{91} & Exact Match & 0.000 & 0.000 & 0.000  \\
             & Fuzzy Match & 0.984 & 1.000 & 0.992  \\
    \bottomrule
    \end{tabular}
\end{table}

Human error is for the errors occurred physically by humans failures for example sometimes documents ar scanned partly/crooked by humans. Finally incomplete predictions represent prediction is include gt but not completely for example sometime gt is 'Japan External Trade Organization Tokyo, Japan' but prediction is missing only last part which is Tokyo, Japan.  Figure~\ref{fig:error_rates} represent all these error types and their rates in both Substring and Fuzzy match categories where exact match is 0 but they labeled as 1.

For our evaluation the success of Fuzzy and Substring we decided to label data points as 0 only in the case where wrong info is 1. For other error categories we accept them and label gt as 1 for those. Table~\ref{tab:eval_performances} represent performance metrics for fuzzy and substring calculated with gt labels explained above with the data points (exact match 0 substring/fuzzy 1) 

\end{document}
