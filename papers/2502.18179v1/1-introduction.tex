\section{Introduction}

Information extraction (IE) entails extracting structured data, such as names, dates, or financial figures, from unstructured documents. 
Within organizations, key information often resides in layout-rich documents (LRDs) such as reports and presentations that combine visual elements (e.g., charts, tables) with textual structure and content~\cite{park2019cord, Wang_2023, zmigrod2024buddiebusinessdocumentdataset}. LRDs challenge traditional natural language processing (NLP) techniques, which are designed for plain texts~\cite{cui2021document, tang2023unifying}.

\input{figures/designspace}

Recent layout-aware models at the intersection of NLP and Computer Vision (CV) address this gap by including visual and structural features to improve information extraction from LRDs (e.g., LayoutLMv1-v3~\cite{xu2020layoutlm, xu2020layoutlmv2, huang2022layoutlmv3}, GraphDoc~\cite{zhang2022multimodal}, DocFormer~\cite{appalaraju2021docformer}).
However, these models require substantial dataset-specific fine-tuning; users must manually annotate a training dataset with bounding boxes and extraction elements for each new document set.

While LLMs show significant potential, there exist open questions regarding about their adoption for IE from LRDs. In which form should the document content be provided to the LLM? Which methods are most effective for in-context learning (ICL) and instruction tuning? Do larger multimodal LLMs like GPT-4o provide advantages over traditional Optical Character Recognition (OCR) and text-based models? How coherent is the LLM output, and what post-processing is needed? How does the performance of open- and closed-sourced LLMs of different scales compare? How does performance compare to fine-tuned SoA layout-aware models?

This paper addresses the questions above, by systematically exploring the design space for IE with LLMs from LRDs. We implement and evaluate a range of preprocessing, chunking, prompting, and post-processing techniques alongside ICL strategies. Furthermore, we experimentally assess the performance of diverse LLMs and LayoutLMv3~\cite{huang2022layoutlmv3}, a fine-tuned layout-aware model.

Our results reveal multiple important insights. First, current, general LLMs can easily compete with SoA fine-tuned models such as LayoutLMv3 with the additional benefit to not require any training data. Second, instead of fine-tuning through data, LLMs require tuning of the IE pipeline to achieve a competitive performance---the gap between a best-practices baseline and the tuned configuration achieved by our lightweight OFAT method is 14.1 points in F1. Third, while purely text-based LLMs achieve a competitive performance through our method, multi-modal LLMs that directly combine textual with visual features still achieve higher performance with the disadvantage of higher costs (token use and API costs) and less transparency.

\noindent Shortly, our contributions are as follows:

\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]

\item Introducing the \textbf{Design Space of IE from LRDs} using LLMs, consisting of the three core challenges: 1) Data structuring, 2) Model engagement, and 3) Output refinement  (Sec.~\ref{sec:design_space_IE}).

\item Development of a \textbf{layout-aware IE test suite} for analyzing effects of OCR- and text-based inputs, chunk sizes, few-shots and CoT prompting, LLM model selection, decoding, entity mapping, data cleaning, and F1-score-based benchmarking using exact, substring, and fuzzy matches
(Sec.~\ref{sec:test_suite_IE_layout}).

\item   Comprehensive evaluation using \textbf{GPT-4o, GPT3.5, and LLaMA3} models, \textbf{GPT4-vision}, and the SoA \textbf{LayoutLMv3} model (Sec.~\ref{sec:experimental_evaluation}).

\item We open-source all our code and experimental results (data accessible in~\cite{wang2023vrdu}). Our test suite can be used and adapted by others to tune their IE pipelines to their datasets and LLMs (\url{https://github.com/gayecolakoglu/LayIE-LLM}).

\end{itemize}


