\section*{Limitations}
\label{sec:limitations}

For the experimental evaluation in the design space, we focused on the Visually Rich Document Understanding (VRDU) dataset~\cite{wang2023vrdu}, which has ground-truth annotations that make it easy for result analysis. We believe incrementally adding new datasets focusing on layout-rich documents with ground truth and making a combined benchmark would help improve the design space itself as we can test different application-specific scenarios. 

The evaluation metrics are planned to be extended with metrics that are popularly used in the context of LLMs such as ROUGE~\cite{lin2004rouge} and BLEU~\cite{papineni2002bleu} scores and their adaptations~\cite{yang2018adaptations}. A fair comparison using cost-related metrics is mostly harder to compute due to their changing depending on the scenario. For instance, in some scenarios where we listed cost as 0 due to having a free-to-use service, it actually comes with a cost. Similarly, the cost of computation can be considered from different perspectives such as the energy usage of a model. 

The test suite is currently limited to the steps that we listed in Fig.~\ref{fig:pipeline}, whereas one could imagine additional steps or factors which affect the performances and may even deliver more satisfactory outcomes. We would like to incorporate additional steps that we learn from the community and incrementally enlarge the design space and extend the testing capabilities for IE from LRDs.

Last, we have only evaluated three LLMs, one multi-modal LLM and one layout-aware, fine-tuned model (LayoutLMv3).
In the future, we want to extend our evaluation to further LLMs and fine-tuned, layout-aware models (e.g., ). Despite that, our results already show a clear trend in which specialized, fine-tuned models struggle to keep up with progress in general-purpose LLMs. E.g., LayoutLMv3, only reaches an F1 score of 0.681 with fine-tuning and on the same document types (STL), substantially less than Llama3, the best text-based LLM with 0.832 and GPT-4-vision, our tested multi-modal LLM with 0.890.
This underlines the importance of our work, in which choices in data structuring, model engagement, and output refinement become critical in achieving the best performance.

\section*{Ethical Considerations}
Large Language Models (LLMs) can contain biases that can have a negative impact on marginalized groups~\cite{gallegos2024bias}. For the task of information extraction, this could have the impact that uncommon names for people and places are auto-corrected by the LLM to their more common form. In our experiments, we have encountered some instances of this and plan to investigate this further.
