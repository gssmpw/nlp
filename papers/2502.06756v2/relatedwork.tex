\section{Related Works}
\subsection{Coarse Masks in Image Segmentation}
%strict standard of pixel-accurate annotations
Coarse masks are common and ubiquitous in the image segmentation task due to their strict standard of pixel-accurate annotations. To relieve human burden, some works adopt the pseudo-labeling paradigm to obtain segmentation masks. These approaches usually leverage incomplete annotations~(\eg, none, point, box, image-level labels or partially fully-labeled data) to obtain segmentation masks, which can be roughly categorized into \textit{unsupervised}~\cite{cho2021picie,ziegler2022leopart,ke2022hsg, hwang2019segsort,van2021maskcontrast,zhou2022maskclip,shinreco,shin2023namedmask}, \textit{weakly-supervised}~\cite{Lin2016ScribbleSupSC,Dai2015BoxSupEB,Papandreou2015WeaklyandSL,Ahn2018PSA,Xie_2022_CLIMS, Wang2020SEAM,xu2022mctformer}, and \textit{semi-supervised}~\cite{wang2022noisyboundary,filipiak2022politetearcher,yang2023unimatch,xu2022pcr}. Although labor-efficient, the quality of pseudo mask is unsatisfactory, which can heavily impair the performance of subsequent segmentation model training. The noisy labels even exist in human annotation~(\eg, MS COCO~\cite{lin2014microsoftcoco}), which is inevitable for achieving pixel-accurate annotations at scale. This paper focuses on enhancing the quality of the coarse mask and consequently contributes to subsequent model training.

\subsection{Mask Refinement Technique}
To overcome the inaccuracy of coarse masks, several mask refinement methods have been explored~\cite{zhang2021refinemask,kirillov2020pointrend,xu2017deepmat,zhang2019canet,yuan2020segfix}. Most existing works are designed for specific networks or tasks and thus lack generality and flexibility. For example, PointRend~\cite{kirillov2020pointrend} and RefineMask~\cite{zhang2021refinemask} are built upon Mask RCNN~\cite{he2017maskrcnn} for instance segmentation, BPR~\cite{tang2021look} propose a model-agnostic post-processing mechanism but mainly focuses on instance segmentation. 
%CascadePSP is designed for ultra high-resolution images, FocalClick is designed for iterative segmentation.
%SPN~\cite{spnet} learns affinity among pixels based on CNN features and performs propagation between similar pixels. SegFix attempts to replace the labels of the boundary pixels with the label of the interior pixels. Recent works 
The dataset-dependant training in a close-world paradigm makes them overfit to specific datasets.
CascadePSP~\cite{cheng2020cascadepsp} and CRM~\cite{shen2022crm} train on a large merged dataset and perform well across different semantic segmentation datasets, but the performance is poor on the complex instance segmentation setting. %However, most of them are trained for a specific segmentation task in a close-world paradigm, making them overfit to specific datasets and lacking generality. 
SegRefiner interprets segmentation refinement as a data generation process but the diffusion step is inefficient for practical use. 
Dense CRF~\cite{CRF} is a training-free post-process approach but it lacks high-level semantic context and usually struggles to work in complex scenarios.
Differently, we aim to design a versatile, generic and efficient post-processing tool across diverse segmentation models, tasks and datasets, which makes it a highly meaningful and valuable tool with broad applications.

 
\subsection{Segment Anything Model}
Segment Anything Model (SAM) has been considered as a milestone vision foundation model for promptable image segmentation. Several works have used this powerful foundation model to benefit downstream vision tasks, including object tracking~\cite{cheng2023segmenttrack, yang2023trackanything}, image editing~\cite{gao2023editanything}, 3D object reconstruction~\cite{shen2023anything3d} and many real-world scenarios~\cite{ma2024medsam, han2023samtransparent, tang2023samcamouflaged}, while the potential of SAM in segmentation refinement task and the effect of different prompt types has been barely explored.












\begin{figure}[tb]%\begin{figure*}
  \centering
  \begin{subfigure}{0.71\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/fig_framework_af.pdf}
    \caption{SAMRefiner}
    \label{fig:framework-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.26\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/fig_framework_b.pdf}
    \caption{IoU Adaption}
    \label{fig:framework-b}
  \end{subfigure}
  \caption{(a) An overview of our proposed framework. SAMRefiner leverages SAM to refine coarse masks by automatically generating prompts from coarse masks, including distance-guided points, context-aware elastic boxes and Gaussian-style masks. We select the best mask from multiple generated masks based on SAM's IoU predictions. (b) An overview of the introduced IoU adaption step, which aims to enhance the IoU prediction ability of SAM on specific datasets. We adopt a LoRA-style adaptor at the last layer of IoU MLP and a ranking loss is used to improve the top-1 accuracy of IoU predictions. This step is self-boosted and requires no additional annotation.}
  \label{fig:framework overview}
  \vspace{-4mm}
\end{figure}