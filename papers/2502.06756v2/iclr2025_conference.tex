
\documentclass{article} % For LaTeX2e
\usepackage[dvipsnames, table]{xcolor}         % colors
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%\usepackage{hyperref}
\usepackage{url}


\def\eg{\emph{e.g}} 
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}} 
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} 
\def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} 
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} 
\def\wolog{w.l.o.g\onedot}
\def\etal{\emph{et al}\onedot}

\usepackage{graphicx}
\usepackage{booktabs}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves 

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}

\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{float,wrapfig}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{float}
% \usepackage{floatrow}

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm
\usepackage{bbding}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{float}
\usepackage{subcaption}
\AtEndPreamble{
    \usepackage[capitalize]{cleveref}
    \crefname{section}{Sec.}{Secs.}
    \Crefname{section}{Section}{Sections}
    \crefname{table}{Tab.}{Tabs.}
    \Crefname{table}{Table}{Tables}
}


\title{SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{\textbf{Yuqi Lin}$^{1,2}$, \textbf{Hengjia Li}$^{1}$, \textbf{Wenqi Shao}$^{2}$, \textbf{Zheng Yang}$^{3,*}$, \textbf{Jun Zhao}$^{3}$,\\
\textbf{Xiaofei He}$^{1,3}$, \textbf{Ping Luo}$^{2,4}$, \textbf{Kaipeng Zhang}$^{2,*}$ \\
$^1$State Key Lab of CAD\&CG, College of Computer Science, Zhejiang University \\
$^2$Shanghai AI Laboratory ~~~   $^3$ FABU Inc. ~~~   $^4$ The University of Hong Kong\\
\texttt{linyq5566@gmail.com~~~ kp\_zhang@foxmail.com} \\
}

\def\thefootnote{*}\footnotetext{Corresponding authors.}
\def\thefootnote{\arabic{footnote}}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle




\begin{abstract}
In this paper, we explore a principal way to enhance the quality of widely pre-existing coarse masks, enabling them to serve as reliable training data for segmentation models to reduce the annotation cost. In contrast to prior refinement techniques that are tailored to specific models or tasks in a close-world manner, we propose SAMRefiner, a universal and efficient approach by adapting SAM to the mask refinement task.
The core technique of our model is the noise-tolerant prompting scheme. Specifically, we introduce a multi-prompt excavation strategy to mine diverse input prompts for SAM~(\ie, distance-guided points, context-aware elastic bounding boxes, and Gaussian-style masks) from initial coarse masks. These prompts can collaborate with each other to mitigate the effect of defects in coarse masks. In particular, considering the difficulty of SAM to handle the multi-object case in semantic segmentation, we introduce a split-then-merge~(STM) pipeline. Additionally, we extend our method to SAMRefiner++ by introducing an additional IoU adaption step to further boost the performance of the generic SAMRefiner on the target dataset. This step is self-boosted and requires no additional annotation. The proposed framework is versatile and can flexibly cooperate with existing segmentation methods. We evaluate our mask framework on a wide range of benchmarks under different settings, demonstrating better accuracy and efficiency. SAMRefiner holds significant potential to expedite the evolution of refinement tools. Our code is available at \href{https://github.com/linyq2117/SAMRefiner}{SAMRefiner}.

\end{abstract}




\section{Introduction}
\label{sec:intro}


Image segmentation aims to assign a label to each pixel in an image such that pixels with the same label share certain characteristics. There are different notations about the group labels, such as semantic categories or instances. In the past few years, although significant progress has been made in image segmentation, %\eg, instance segmentation and semantic segmentation, 
the prevailing approaches rely on fully annotated training images, which are tedious to obtain. To reduce human labor, a labor-efficient alternative is generating segmentation masks by preceding models, especially those designed under incomplete supervisions~(\eg, \textit{unsupervised, weakly supervised or semi-supervised} annotations~\cite{wang2023cutler, wang2022noisyboundary, lin2022clipes}). 
These generated segmentation masks can serve as pseudo labels to train advanced segmentation models or iteratively upgrade existing models~\cite{zhu2021selftraining, yang2022st++}. With the ever-increasing data amount, this pseudo-labeling paradigm showcases great practicality and potential to expand dataset volume for large-scale learning.
However, the initial pseudo masks are usually noisy and lack fine details, particularly in object boundaries or in high-frequency regions~(seeing \cref{fig:1}), hindering them from providing reliable supervision for model training.


\begin{figure}[tb]
  \centering
  \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1.0\linewidth]{images/fig1.pdf}
    \caption{Visualizations of segmentation masks. \textbf{Left:} The initial masks generated by~\cite{lin2022clipes}. \textbf{Mid:} Masks refined by dense CRF~\cite{CRF}. \textbf{Right:} Masks refined by our framework. }
    \label{fig:1}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=0.9\linewidth]{images/fig_lidar.pdf}
    \caption{The performance of our proposed mask refinement framework SAMRefiner on different benchmarks and comparisons with related works.}
    \label{fig:lidar}
  \end{subfigure}
  \caption{Visualization of segmentation masks and performance.}
  \label{fig:short1}
  \vspace{-6mm}
\end{figure}



Several mask refinement techniques have been proposed to improve the mask quality, but they suffer from major drawbacks: \textbf{1) model-dependent:} Some methods develop custom refinement modules tailored to specific networks and train them in an end-to-end fashion~\cite{zhang2021refinemask, ke2022masktransfiner}, making them fail to work on different models. \textbf{2) task-specific:} Another group of techniques
~\cite{chen2022focalclick, cheng2020cascadepsp, shen2022crm} resort to model-agnostic refinement mechanisms but they usually focus on specific task~(\eg, semantic segmentation or instance segmentation).
\textbf{3) category-limited:} Most previous works require training on target datasets with annotated data, limiting them to generalize to unseen categories and granularity.
\textbf{4) time-inefficient:} Recent works~\cite{shen2022crm, SegRefiner} demonstrate better performance but refine one instance at a time, which is inefficient in complex instance segmentation tasks. 


Recently, Segment Anything Model~(SAM)~\cite{kirillov2023sam}, an interactive image segmentation model that segments intended objects by user-provided prompts~(\eg, point, box), has been proposed and achieved significant success in many image segmentation tasks. Some researchers have endeavored to adapt it to various tasks in order to take advantage of SAMâ€™s powerful representation capability to alleviate inadequate training samples. However, most of these studies
focus on predicting masks from scratch and how to adapt SAM for the mask refinement task with pre-existing coarse masks remains an unexplored and challenging problem. We argue that this task is of great value in practical applications due to the widespread pre-existing masks (\eg, masks provided by offline models, inaccurate human annotations, or other forms of pre-processing). Making modifications on them could facilitate the annotation and benefit various downstream tasks.

However, since SAM is prompt-driven, adapting SAM to the refinement task is not trivial because it is difficult to obtain accurate prompts for SAM merely from coarse masks. Applying SAM directly to mask refinement using naive strategies would suffer from distorted prompts caused by noise and result in inferior performance. For example, in \cref{fig:sam-failure}, we adopt the commonly used box prompt~(tight box of coarse mask) and observe that this naive approach fails to obtain satisfactory performance because diverse types of errors~(\eg, false-negative, false-positive) contained in the coarse mask would mislead the prompt extraction. Besides, results of directly taking the coarse mask as prompt are also terrible for SAM~(the \textit{4th} column in \cref{fig:sam-failure}) due to its inherent nature in pre-training.~(More prompt analyses are provided in the \textit{Method Section} and \textit{Appendix}.) \textbf{\textit{Therefore, how to mine noise-tolerant prompts from the coarse mask poses a great challenge.}}





In this paper, we tame SAM for the mask refinement tasks, which have unique characteristics compared to other segmentation tasks for the existence of coarse masks. We propose a universal and efficient framework called SAMRefiner, the core technique of which is the noise-tolerant prompting scheme.  Specifically, to mitigate the effect of defects in coarse masks to prompt generation, we propose a multi-prompt excavation strategy to mine diverse and seemly prompts, including distance-guided points, context-aware elastic bounding boxes~(CEBox), and Gaussian-style masks. These multi-prompts can collaborate with each other to generate high-quality masks and are more robust to noise than the single prompt. To overcome the confusion caused by multi-object cases, we introduce a split-then-merge~(STM) pipeline to make it better suited for semantic segmentation. 
Meanwhile, given that the original SAM lacks dataset-specific priors, resulting in inaccurate IoU branch predictions, we propose SAMRefiner++. This approach incorporates an additional IoU adaptation step to enhance SAM's prediction accuracy on specific datasets by leveraging coarse mask priors. This minimal adaption startegy operates in a self-boosted manner and requires no extra annotations.


\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{images/fig-sam-failure.pdf}
    %\vspace{-6mm}
   \caption{Failure cases of SAM using the tight box of the coarse mask~(red box) and directly using the coarse mask as the prompt. The tight box is sensitive to the false negative~(first row) and false positive~(last row) errors in the coarse mask, which would mislead SAM's predictions. And the separate mask prompt fails to work for SAM. Our proposed multi-prompt excavation strategy is robust to the noise.
   }
   \label{fig:sam-failure}
   \vspace{-4mm}
\end{figure}



We conduct experiments on a wide range of semantic and instance segmentation settings, with pseudo masks generated from incomplete supervisions, existing models, and synthetic data. Experimental results demonstrate the outstanding mask refinement capability of SAMRefiner~(seeing \cref{fig:lidar}). Our approach is a generic post-processing tool and can be incorporated into any image segmentation approach in a self-training fashion with constant performance improvement.

Our contributions are summarized as follows:
\begin{itemize}
	\item \textbf{New Roadmap:} SAMRefiner offers the first solution to address the mask refinement task based on SAM, which is of great value in practical applications.
    \item \textbf{New Method:} We uncover the deficiency of SAM in the mask refinement task and propose an effective and efficient framework to mine noise-tolerant prompts, successfully addressing the challenging universal mask refinement task. 
    \item \textbf{Novel Insights:} While our work is based on SAM, it offers several novel insights and observations like the impact of mask prompt and the IoU adaption strategy.
    \item \textbf{Stronger practicality and performance:} This framework is versatile and can flexibly cooperate with existing segmentation methods under various setting. It significantly enhances the pseudo mask quality~(\eg, over 10\% for WSSIS) while taking less time~(\eg, 5$\times$ faster than CascadePSP).
\end{itemize}













\section{Related Works}

\subsection{Coarse Masks in Image Segmentation}
%strict standard of pixel-accurate annotations
Coarse masks are common and ubiquitous in the image segmentation task due to their strict standard of pixel-accurate annotations. To relieve human burden, some works adopt the pseudo-labeling paradigm to obtain segmentation masks. These approaches usually leverage incomplete annotations~(\eg, none, point, box, image-level labels or partially fully-labeled data) to obtain segmentation masks, which can be roughly categorized into \textit{unsupervised}~\cite{cho2021picie,ziegler2022leopart,ke2022hsg, hwang2019segsort,van2021maskcontrast,zhou2022maskclip,shinreco,shin2023namedmask}, \textit{weakly-supervised}~\cite{Lin2016ScribbleSupSC,Dai2015BoxSupEB,Papandreou2015WeaklyandSL,Ahn2018PSA,Xie_2022_CLIMS, Wang2020SEAM,xu2022mctformer}, and \textit{semi-supervised}~\cite{wang2022noisyboundary,filipiak2022politetearcher,yang2023unimatch,xu2022pcr}. Although labor-efficient, the quality of pseudo mask is unsatisfactory, which can heavily impair the performance of subsequent segmentation model training. The noisy labels even exist in human annotation~(\eg, MS COCO~\cite{lin2014microsoftcoco}), which is inevitable for achieving pixel-accurate annotations at scale. This paper focuses on enhancing the quality of the coarse mask and consequently contributes to subsequent model training.

\subsection{Mask Refinement Technique}
To overcome the inaccuracy of coarse masks, several mask refinement methods have been explored~\cite{zhang2021refinemask,kirillov2020pointrend,xu2017deepmat,zhang2019canet,yuan2020segfix}. Most existing works are designed for specific networks or tasks and thus lack generality and flexibility. For example, PointRend~\cite{kirillov2020pointrend} and RefineMask~\cite{zhang2021refinemask} are built upon Mask RCNN~\cite{he2017maskrcnn} for instance segmentation, BPR~\cite{tang2021look} propose a model-agnostic post-processing mechanism but mainly focuses on instance segmentation. 
%CascadePSP is designed for ultra high-resolution images, FocalClick is designed for iterative segmentation.
%SPN~\cite{spnet} learns affinity among pixels based on CNN features and performs propagation between similar pixels. SegFix attempts to replace the labels of the boundary pixels with the label of the interior pixels. Recent works 
The dataset-dependant training in a close-world paradigm makes them overfit to specific datasets.
CascadePSP~\cite{cheng2020cascadepsp} and CRM~\cite{shen2022crm} train on a large merged dataset and perform well across different semantic segmentation datasets, but the performance is poor on the complex instance segmentation setting. %However, most of them are trained for a specific segmentation task in a close-world paradigm, making them overfit to specific datasets and lacking generality. 
SegRefiner interprets segmentation refinement as a data generation process but the diffusion step is inefficient for practical use. 
Dense CRF~\cite{CRF} is a training-free post-process approach but it lacks high-level semantic context and usually struggles to work in complex scenarios.
Differently, we aim to design a versatile, generic and efficient post-processing tool across diverse segmentation models, tasks and datasets, which makes it a highly meaningful and valuable tool with broad applications.

 
\subsection{Segment Anything Model}
Segment Anything Model (SAM) has been considered as a milestone vision foundation model for promptable image segmentation. Several works have used this powerful foundation model to benefit downstream vision tasks, including object tracking~\cite{cheng2023segmenttrack, yang2023trackanything}, image editing~\cite{gao2023editanything}, 3D object reconstruction~\cite{shen2023anything3d} and many real-world scenarios~\cite{ma2024medsam, han2023samtransparent, tang2023samcamouflaged}, while the potential of SAM in segmentation refinement task and the effect of different prompt types has been barely explored.












\begin{figure}[tb]%\begin{figure*}
  \centering
  \begin{subfigure}{0.71\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/fig_framework_af.pdf}
    \caption{SAMRefiner}
    \label{fig:framework-a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.26\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/fig_framework_b.pdf}
    \caption{IoU Adaption}
    \label{fig:framework-b}
  \end{subfigure}
  \caption{(a) An overview of our proposed framework. SAMRefiner leverages SAM to refine coarse masks by automatically generating prompts from coarse masks, including distance-guided points, context-aware elastic boxes and Gaussian-style masks. We select the best mask from multiple generated masks based on SAM's IoU predictions. (b) An overview of the introduced IoU adaption step, which aims to enhance the IoU prediction ability of SAM on specific datasets. We adopt a LoRA-style adaptor at the last layer of IoU MLP and a ranking loss is used to improve the top-1 accuracy of IoU predictions. This step is self-boosted and requires no additional annotation.}
  \label{fig:framework overview}
  \vspace{-4mm}
\end{figure}








\section{Method}
\label{method}
In this section, we introduce our proposed mask refinement framework, as is shown in \cref{fig:framework overview}. We first review the architecture of SAM and its usage. Then, we introduce multi-prompt excavation strategies to exploit SAM. We further present an efficient adaption variant to enhance the accuracy of IoU predictions in a self-boosted manner.








\subsection{Review of SAM}
We start by introducing the components of SAM, 
which consists of an image encoder, a prompt encoder, and a mask decoder. 
1) The image encoder is based on a standard Vision Transformer (ViT) pre-trained by MAE~\cite{he2022mae}. It generates a $16\times$ downsampled embedding of the input image.
2) The prompt encoder can be either \textit{sparse} (points, boxes, text) or \textit{dense} (masks). For sparse prompts, points and boxes are represented as positional encodings summed with learned embeddings. Text prompts are processed by the text encoder of CLIP~\cite{radford2021clip}. Dense prompts are directly convolved with the image embeddings and summed element-wise.
3) The mask decoder employs prompt-based self-attention and two-way cross-attention. This allows interaction between prompt-to-image and image-to-prompt embeddings, enabling simultaneous updates to the encoded image and prompt features. After two decoder layers, the output mask tokens are processed by a 3-layer MLP and then perform a spatially point-wise product with the upsampled image embedding to get target masks. %An IoU token is leveraged to produce confidence scores for each predicted mask.

SAM is able to produce both a single mask or multiple masks~(\ie, three masks) for each input prompt. The multi-mask mode is designed to address the ambiguity problem and an additional IoU token is adopted to learn the confidence of each mask, which reflects the IoU between each predicted mask and the target object. In \cref{fig:iou-adaption-1}, we empirically find that the multi-mask mode is generally superior to the single-mask mode by simply selecting the mask with the best IoU predictions so we adopt the multi-mask mode in our experiments.



\subsection{Prompt Excavation}
\label{prompt excavation part}

As a promptable segmentation model, the input prompts play a crucial role in SAM because these prompts provide localization guidance of intended objects. To employ SAM for mask refinement, 
we need to mine prompts merely based on the initial coarse masks, which is challenging for the existence of noise and defects. 
Unlike previous works that mostly use one type of prompt~\cite{dai2023samaug}, our prompt excavation strategies aim to mine diverse and seemly prompts~(including points, boxes and masks), making them collaborate with each other to mitigate the effect of defects in coarse masks. Note that SAM fails to work by merely using the mask as an input prompt, and we provide analysis in subsequent parts.





\begin{figure}[tb]
  \centering
  \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=0.9\linewidth]{images/ablation-box-mask.pdf}
    %\vspace{-6mm}
   \caption{Effects of the context-aware elastic box~(top two rows) and mask prompt~(last two rows). %Both of them play a crucial role in mitigating the impact of defects in coarse masks.
   }
   \label{fig:ablation_box_mask}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1.0\linewidth]{images/fig_semantic_problem2.pdf}
    %\vspace{-6mm}
   \caption{Effects of the proposed split-then-merge (STM) strategy.
   }
   \label{fig:ablation_split_then_merge}
  \end{subfigure}
  \caption{Visualizations of our proposed techniques effects. All of them play a crucial role in mitigating the impact of defects in coarse masks.}
  \label{fig:short2}
  \vspace{-4mm}
\end{figure}






\textbf{Points.} The point prompt can provide position information for either foreground or background objects. However, it is difficult to determine the most salient point when using binary coarse masks. To solve this challenge, we leverage a simple but empirically effective object-centric prior: The center of an object tends to be positive and feature-discriminative, while uncertainty is mostly located along boundaries. Based on this criteria, we select the foreground point that has a maximum distance to the nearest background position as the positive prompt. Similarly, the negative prompt should satisfy the following principle: 1) the point is farthest away from the foreground region; 2) the point is within the bounding box of the foreground region.


\textbf{Boxes.} The box prompt shows a more powerful localization ability for the abundant cues it contains. Given a binary mask, it is simple to find the maximum bounding rectangle~(tight box) of foreground regions as the box prompt. 
However, the false-negative pixels in the coarse mask may hinder the quality of the bounding box, resulting in incomplete coverage of the potential object~(\cref{fig:ablation_box_mask}). 

To address this, we propose a context-aware elastic box~(CEBox) to adjust the tight box conditionally. The bounding box can be expanded in four directions according to the surrounding context. Specifically, the input image~$\mathcal{I} \in R^{H \times W \times 3}$ is encoded as feature embedding~$\mathcal{F}_{im} \in R^{h \times w \times c}$ in SAM latent space by the image encoder, %$E_{\theta}$, 
where $(H, W)$, $(h, w)$ denote original image size and embedding size. % $\theta$ denote model parameters.
%\begin{equation}
%  \mathcal{F}_{im} = E_{\theta} (\mathcal{I})
%  \label{eq1}
%\end{equation}
The coarse mask $\mathcal{M}_{coarse} \in R^{H \times W}$ is resized to $\hat{\mathcal{M}} \in R^{h \times w}$ to keep aligned with $\mathcal{F}_{im}$.
We calculate the mean feature embedding of the coarse mask (denoted as query embedding) as follows:
\begin{equation}
  \mathcal{F}_{query} = \frac{1}{|\mathbbm{1}_{\hat{\mathcal{M}}>0}|}\sum \mathbbm{1}_{\hat{\mathcal{M}}>0} (\mathcal{F}_{im})
  \label{eq2}
\end{equation}
where $\mathbbm{1}_{\hat{\mathcal{M}}>0} \in \{0,1\}$ is the indicator function to determine foreground regions, $|\cdot|$ represents the number of elements. We calculate the affinity between $\mathcal{F}_{query}$ and each spatial location in resized image embedding $\hat{\mathcal{F}}_{im} \in R^{H \times W \times c}$ to obtain a similarity map $Sim \in R^{H \times W}$ and binary it by 0.5:
\begin{equation}
  Sim = [\mathcal{F}_{query} \cdot \hat{\mathcal{F}}_{im}]_{>=0.5}
  \label{eq3}
\end{equation}

For each direction in \textit{\{left, right, up, down\}}, we enlarge the tight box $\mathcal{B}$ by 10\% of the corresponding side length and approximate the positive ratio in the enlarged region $Sim_{context}$. A threshold $\lambda$ is used to determine the necessity to expand the current box in this direction. 
To avoid over-enlarge, we limit the maximum expanding pixels each time and run multiple iterations for progressive expansion.









\textbf{Masks.} Most existing works employ point or box as the initial prompt while mask prompt is usually discarded~\cite{dai2023samaug, chen2023samwsss, zhang2023personalize}. The mask fails to serve as the initial input prompt for SAM separately~(seeing qualitative results in \cref{fig:sam-failure} and quantitative results in \cref{tab:prompt_ablation}). This is because the mask prompt merely acts as an auxiliary for point and box in the cascade refinement during SAM pre-training, with the predicted logits of the previous iteration as input to guide the next one. However, we argue that the mask prompt is vital to distinguish foreground and background in the mask refinement task, especially in the case that the box prompt fails to work~(\eg, the oversized box results in falsely detected objects or background in \cref{fig:ablation_box_mask}). Considering the inaccuracy of coarse mask, we leverage a Gaussian-style mask $GM$ based on distance transform used in point prompt: %$G(x, y) = exp(-\frac{(x-x_0)^2}{2\sigma_{x}^2} - \frac{(y-y_0)^2}{2\sigma_{y}^2})$
\begin{equation}
  GM(x, y) = \omega \cdot exp(-\frac{(x-x_0)^2 + (y-y_0)^2}{| \mathbbm{1}_{\mathcal{M}_{coarse}>0} | \cdot \gamma})
  \label{eq4}
\end{equation}
where $GM(x, y)$ represents the mask prompt at location~$(x, y)$, $(x_0,  y_0)$ is the \textit{mask center point} that is farthest to the background regions, $\omega$, $\gamma$ are the factors to control the amplitude and span of the distribution. We provide a detailed analysis of the Gaussian Mask in the Appendix \cref{sec:appendix_gaussian}.




\textbf{Application on Semantic Segmentation.} The semantic segmentation mask is category-wise and there may exist many objects in a semantic mask. 
In \cref{fig:ablation_split_then_merge}, we find that SAM struggles to segment multiple objects with a large span~(either miss-detect or falsely detect) using common prompts. Although the mask prompt can mitigate this problem, it fails when objects of different categories are mingled. We further propose a split-then-merge~(STM) pipeline to solve it. \textbf{1) Split:} we split the mask by finding all connected regions. Note that some regions are noisy and trivial due to the inaccuracy of coarse masks. \textbf{2) Merge:} To form semantically meaningful regions, we iteratively merge the close regions based on the box area variation and mask area occupancy. Two regions will be merged only if the change of box area of~(before and after region merging) is small and the mask area occupancy of the merged box is enough. An elaboration of this strategy is provided in \cref{alg:algo1}.







\subsection{IoU Adaption}
For SAM, the quality of the generated masks is determined by the input prompts, while the selection of the best mask is based on IoU predictions (denoted as $IoU_{pred}$). Traditional SAM leverages an individual token to produce the mask when multiple prompts are given (single-mask mode). However, in \cref{fig:iou-adaption-1}, we empirically observe that selecting the best mask from multiple predictions of SAM based on $IoU_{pred}$ is generally superior to the single-mask mode under all prompt combinations. We give a detailed analysis in the Appendix \cref{sec:appendix_iouadaption}. However, in \cref{fig:iou-adaption-2}, we find that mask selection based on $IoU_{pred}$ still falls short of the upper limit~(select mask by ground-truth IoUs $IoU_{GT}$), which indicates the inaccuracy of SAM's top-1 IoU prediction. This is because the abovementioned SAMRefiner, which is training-free and generalized to most cases, is agnostic to downstream categories. SAM's IoU head is not specifically trained for intended objects, leading to suboptimal IoU predictions.



\begin{figure}
  \centering
  \begin{subfigure}{0.3\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/best_iou_singlevssamiouvsgt.png}
    \caption{Mask quality using different prompt types.}
    \label{fig:iou-adaption-1}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.3\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/best_iou_nosingle.png}
    \caption{The effect of different IoU selection criteria.}
    \label{fig:iou-adaption-2}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.3\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/top1-acc3.png}
    \caption{Top-1 accuracy using different IoU selection criteria.}
    \label{fig:iou-adaption-3}
  \end{subfigure}
  \caption{The effect of different prompt types, mask modes and IoU selection criteria on DAVIS-585.}
  \label{fig:iou-adaption}
  \vspace{-2mm}
\end{figure}



For the mask refinement task, where the GT masks are unavailable, we propose that coarse masks can act as effective priors to guide IoU predictions for domain-specific categories. To verify it, we denote the IoU between SAM's output mask and coarse mask as $IoU_{coarse}$, and compare the top-1 IoU accuracy of $IoU_{coarse}$ against $IoU_{pred}$. Results in \cref{fig:iou-adaption-3} indicate that the top-1 performance based on $IoU_{coarse}$ outperforms $IoU_{pred}$ for simple point and box prompts, which is close to that based on ground truth.
In contrast, the coarse IoU performs poorly in multi-prompt cases. It is likely that the less prompt provides ambiguous guidance for SAM and results in variant masks, enabling the coarse masks to provide effective guidance in selecting the intended one. However, the masks generated by multi-prompts have better quality than the coarse mask and thus may mislead the selection. To pursue better performance, 
we enhance SAM's IoU ranking ability by training under the single prompt case supervised by $IoU_{coarse}$ and expect it to benefit multi-prompt cases.  
\textbf{This process is conducted in a self-boosted manner and requires no extra annotations.}



Specifically, we focus on minimal adaptation of SAM toward better IoU predictions. To preserve the zero-shot transfer capability of SAM, we fix the model parameters of the pre-trained SAM and only add a LoRA-style adaptor~\cite{hu2021lora} in the IoU head, as is shown in~\cref{fig:framework-b}. Considering the inaccuracy of $IoU_{coarse}$, we adopt a ranking-based loss instead of a regression loss. In particular, for each SAM's predicted mask $M_i$ and its predicted IoU $x_i$, we calculate their coarse IoU and denote the index of the mask with the best coarse IoU as $j$. The pairwise ranking loss is computed as:
\begin{equation}
  loss = \sum_{i=1, i\neq j}^{n} \max (0, \, x_i - x_j + m)
  \label{eq5}
\end{equation}
where $n$ is the number of total masks~(3 for SAM), $m$ is the margin to control the minimal difference. This loss encourages the best IoU score $x_j$ to be higher than the remaining ones, thus promoting the accuracy of top-1 prediction. We train the adaptor based on the single prompt and use multi-prompt during inference. %The coarse masks we used are SAMRefiner refined ones. 
Note that despite LoRA's popularity, its optimal placement remains unclear. Previous works empirically place LoRA layers in some specific layers (\eg, backbone), altering existing knowledge to adapt to new domains, which modify the learned knowledge and affect the mask generation. In contrast, our approach inserts the LoRA layer in the IoU head, preserving SAM's full capability to generate high-quality masks while improving mask selection. To our knowledge, this minimal adaptation is underexplored and may provide new insights for the field. 
We denote SAMRefiner with this adaption step as SAMRefiner++, which only focuses on selecting better masks on the target dataset and has no effect on mask generation.

















\section{Experiment}
\subsection{Experimental Setup}
\textbf{Datasets and Implementation Details.} 
For a comprehensive evaluation of the mask refinement performance of SAMRefiner, we conduct experiments on a wide range of benchmarks, including those designed for mask refinement~(DAVIS-585\cite{chen2022focalclick}), instance segmentation~(COCO\cite{lin2014microsoftcoco}), semantic segmentation~(VOC\cite{everingham2010pascal}) under different settings. As a mask refiner, our method keeps the same setting as each baseline for pair comparison. The metrics we used include (boundary) IoU~\cite{Cheng_2021_boundaryiou}, (boundary) mask AP and mIoU. The threshold $\lambda$ and $\mu$ used in the box and mask prompt are set to 0.1 and 0.5 respectively. The factors $\omega, \gamma$ for Gaussian distribution are set to 15 and 4 by default. We adopt pre-trained SAM with the ViT-H image encoder as the segmentation foundation model and more details are provided in the Appendix. 



\begin{table}[t]
 \caption{The quality of refined masks using different prompts and the effect of IoU adaption on DAVIS-585. Results are presented as SAMRefiner / SAMRefiner++}
 \vspace{4mm}
  \centering
  \setlength{\tabcolsep}{6pt}
  \resizebox{0.6\columnwidth}{!}{
  \begin{tabular}{lcccc}
    \toprule
    Prompt Type      &     IoU  & boundary IoU  & Top-1 Acc \\
    \midrule
    Coarse Mask &  81.4 & 71.4 & - \\
    \midrule 
    Point     &    53.7 / 56.4 \qquad & 49.9 / 53.1 & 51.5 / 62.1  \\
    Box  &   68.8 / 70.8 & 61.9 / 63.5 & 36.4 / 56.8 \\
    Mask & 37.3 / 40.4 & 32.6 / 33.5 & 28.4 / 30.8 \\
    \midrule
    Point + Box &   76.7 / 79.1 & 69.0 / 70.9 & 37.1 / 53.7 \\
    Point + Mask &   77.5 / 80.6 & 67.7 / 71.6 & 43.2 / 72.6\\
    Box + Mask &   84.6 / 85.1 & 74.2 / 75.4 & 36.2 / 60.7 \\
    \midrule
    ALL & 86.9 / 87.1 & 75.1 / 75.4 & 44.1 / 63.8 \\
    \bottomrule
  \end{tabular}
  }
  \vspace{-4mm}
  \label{tab:prompt_ablation}
\end{table}


\subsection{Ablation Experiments}
In this section, we conduct detailed ablation studies to analyze the effect of each component in our framework. We mainly experiment on DAVIS-585, as it is specifically designed for mask correction and contains various defects in the mask. We also leverage popular COCO and VOC to evaluate our method for specific scenarios.



\textbf{Effect of different prompts and IoU adaption.} \cref{tab:prompt_ablation} shows the performance of using different prompts for SAMRefiner. The results indicate that our proposed multi-prompt excavation strategy performs better than the single prompt. The mask prompt, which is barely considered in previous works, shows poor performance on its own but can bring an obvious advantage of nearly 20\% IoU for point and box. Besides, we compare the mask selection by SAM's original IoU predictions~(number before /) and our adapted IoU head~(number after /). The IoU adaption step can significantly boost the top-1 accuracy of the best mask selection and further benefit the final IoU performance. We provide more ablation studies in Appendix for the page limit.





\textbf{Effect of different design choices in SAMRefiner.} 
We analyze the impact of different design choices for each prompt and report their relative contribution in \cref{tab:point-cebox-stm}. (1) We compare different strategies to sample the positive point prompt in \cref{tab:ablate_point}, including randomly choosing from the coarse mask, selecting the center of the bounding box, and selecting the point having a maximum distance to the background. Compared to random choice, using the box center shows even worse performance. This is because the bounding box is sensitive to the noise in the mask~(\eg, the distant false positives), resulting in the inaccuracy of the box center. Our distance-guided are more robust to the noise and can obtain 52.5\% IoU with only a positive point. The performance can be further improved to 53.7\% by adding the extra negative point mentioned in \cref{prompt excavation part}. (2) \cref{tab:ablate_cebox} shows the impact of using the tight box and context-aware elastic box~(CEBox). We adopt the coarse masks generated from PointWSSIS~\cite{kim2023devil} on COCO, which usually suffers from incomplete masks. 
The proposed CEBox can produce better boxes with higher $AP^{box}$ and benefit mask generation. (3) In \cref{tab:ablate_stm}, we compare the commonly used mIoU with/without STM. 
Results show that STM can bring remarkable improvement for extremely coarse masks~(\ie, 6.2\% for MaskCLIP) and can constantly promote performance on better initial masks.

\begin{table}[t]
  \caption{Ablation study of our proposed strategies on different cases.}
  \vspace{-2mm}
  \begin{subtable}[h]{0.3\textwidth}
  \centering
  \caption{Point sampling strategy.}
  \vspace{-2mm}
  \scalebox{0.75}{
  \begin{tabular}{lcc}
    \toprule
    Point  & IoU & bIoU  \\
    \midrule
    Random & 37.8 & 38.5         \\
    Box Center & 22.2   & 26.3      \\
    Distance-Guided & 53.7   & 56.4    \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:ablate_point}
  \end{subtable}
  \hfill
  \begin{subtable}[h]{0.32\textwidth}
  \centering
  \caption{Context-aware elastic box.}
  \vspace{-2mm}
  \scalebox{0.8}{
  \begin{tabular}{cccc}
    \toprule
    CEBox  & AP$^{\text{box}}$ & AP$^{\text{mask}}$ & AP$^{\text{boundary}}$  \\
    \midrule
    \XSolidBrush & 36.7 & 37.5    & 25.6     \\
    \Checkmark & 38.2   & 37.8    & 25.9  \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:ablate_cebox}
  \end{subtable}
  \hfill
  \begin{subtable}[h]{0.32\textwidth}
  \centering
  \caption{STM strategy.}
  \vspace{-2mm}
  \scalebox{0.8}{
  \begin{tabular}{ccc}
    \toprule
    STM  & MaskCLIP  & CLIP-ES  \\
    \midrule
    \XSolidBrush & 51.1      &  79.1    \\
    \Checkmark & 57.3       & 79.3 \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:ablate_stm}
  \end{subtable}
  \label{tab:point-cebox-stm}
%\vspace{-4mm}
\end{table}

  




  




















\begin{table*}
  \centering
  \caption{Results of instance segmentation under different supervisions on COCO 2017. The Annotations denote the supervision type, including $\mathcal{U}$(unlabeled), $\mathcal{P}$(point-level label), $\mathcal{F}$(full labeled). Networks represent the final segmentation model trained based on the pseudo masks. We follow the default setting of each baseline method.}
  \resizebox{0.85\columnwidth}{!}{
  \begin{tabular}{lccllll}
    \toprule
    \multirow{2}{*}{Methods} & \multirow{2}{*}{Annotations} & \multirow{2}{*}{Networks} & \multicolumn{2}{c}{{COCO train5K}} & \multicolumn{2}{c}{{COCO val2017}} \\
    \cline{4-5} \cline{6-7}
    & & & \multicolumn{1}{c}{AP$^{\text{mask}}$} & \multicolumn{1}{c}{AP$^{\text{boundary}}$} & \multicolumn{1}{c}{AP$^{\text{mask}}$} & \multicolumn{1}{c}{AP$^{\text{boundary}}$} \\
    \midrule
    \multicolumn{6}{c}{\textbf{Unsupervised}} \\
    \midrule
    CutLER & None & Cascade R-CNN & - & - & 8.8 & 2.8 \\
    \rowcolor{Gray!30} \textit{+SAMRefiner} & None & Cascade R-CNN & - & - & 12.1${\textcolor{green!80!black}{(+3.3)}}$  & 5.0${\textcolor{green!80!black}{(+2.2)}}$  \\
    \midrule
    \multicolumn{6}{c}{\textbf{Semi-supervised}} \\
    \midrule
    NB   & $\mathcal{F}$ 1\% + $\mathcal{U}$ 99\%  & Mask R-CNN    & 4.4 & 1.6 & 6.7  & 2.3 \\
    \rowcolor{Gray!30} \textit{+SAMRefiner}       & $\mathcal{F}$ 1\% + $\mathcal{U}$ 99\%   &  Mask R-CNN     & 6.9${\textcolor{green!80!black}{(+2.5)}}$ & 4.4${\textcolor{green!80!black}{(+2.8)}}$ & 11.8${\textcolor{green!80!black}{(+5.1)}}$   & 6.5${\textcolor{green!80!black}{(+4.2)}}$  \\
    NB   & $\mathcal{F}$ 5\%  + $\mathcal{U}$ 95\%   &  Mask R-CNN   & 18.3 & 8.8 & 24.0 & 12.4\\
    \rowcolor{Gray!30} \textit{+SAMRefiner}       & $\mathcal{F}$ 5\% + $\mathcal{U}$ 95\%   &  Mask R-CNN     & 22.3${\textcolor{green!80!black}{(+4.0)}}$ & 14.4${\textcolor{green!80!black}{(+5.6)}}$ & 27.4${\textcolor{green!80!black}{(+3.4)}}$ & 16.5${\textcolor{green!80!black}{(+4.1)}}$\\
    NB   & $\mathcal{F}$ 10\% + $\mathcal{U}$ 90\%  &  Mask R-CNN   & 23.0 & 11.8 & 28.9  & 16.3 \\
    \rowcolor{Gray!30} \textit{+SAMRefiner}       & $\mathcal{F}$ 10\% + $\mathcal{U}$ 90\%   &  Mask R-CNN    & 26.1${\textcolor{green!80!black}{(+3.1)}}$ & 17.0${\textcolor{green!80!black}{(+5.2)}}$ & 30.5${\textcolor{green!80!black}{(+1.6)}}$  & 18.6${\textcolor{green!80!black}{(+2.3)}}$ \\
    \midrule
    \multicolumn{6}{c}{\textbf{Weakly Semi-supervised}} \\
    \midrule
    PointWSSIS   & $\mathcal{F}$ 1\% + $\mathcal{P}$ 99\% & SOLOv2   & 15.1  & 6.7 & 23.9  & 11.5 \\
    \rowcolor{Gray!30} \textit{+SAMRefiner}       & $\mathcal{F}$ 1\% + $\mathcal{P}$ 99\%   & SOLOv2    & 25.4${\textcolor{green!80!black}{(+10.3)}}$ & 16.3${\textcolor{green!80!black}{(+9.6)}}$ & 30.2${\textcolor{green!80!black}{(+6.3)}}$  & 18.2${\textcolor{green!80!black}{(+6.7)}}$ \\
    PointWSSIS   & $\mathcal{F}$ 5\%  + $\mathcal{P}$ 95\%  & SOLOv2 & 32.3  & 19.7 & 33.4  & 19.6 \\
    \rowcolor{Gray!30} \textit{+SAMRefiner}       & $\mathcal{F}$ 5\% + $\mathcal{P}$ 95\%   & SOLOv2    & 37.7${\textcolor{green!80!black}{(+5.4)}}$ & 25.9${\textcolor{green!80!black}{(+6.2)}}$ & 34.6${\textcolor{green!80!black}{(+1.2)}}$  & 21.6${\textcolor{green!80!black}{(+2.0)}}$ \\
    PointWSSIS   & $\mathcal{F}$ 10\% + $\mathcal{P}$ 90\%  & SOLOv2  & 39.9  & 26.4 & 35.5  & 21.9 \\
    \rowcolor{Gray!30} \textit{+SAMRefiner}       & $\mathcal{F}$ 10\% + $\mathcal{P}$ 90\%  & SOLOv2    & 42.8${\textcolor{green!80!black}{(+2.9)}}$ & 30.2${\textcolor{green!80!black}{(+3.8)}}$ & 36.1${\textcolor{green!80!black}{(+0.6)}}$   & 22.9${\textcolor{green!80!black}{(+1.0)}}$ \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:segmentation performance COCO}
  %\vspace{-4mm}
\end{table*}


\subsection{Application on Incomplete Supervision}



\textbf{Instance Segmentation.} To verify the effectiveness of our framework, we apply it to various typical methods, including unsupervised~(CutLER~\cite{wang2023cutler}), semi-supervised~(NoisyBoundary~\cite{wang2022noisyboundary}) and weakly semi-supervised~(PointWSSIS~\cite{kim2023devil}). Experiments are conducted on COCO following these methods.
We evaluate the mask quality in terms of two aspects: 1) the performance of pseudo masks on the train set and 2) the performance of the final segmentation model trained based on these pseudo masks. The pseudo masks are evaluated on a subset of COCO train set~(train 5K) and the final segmentation model~\cite{cai2018cascade, he2017maskrcnn, wang2020solov2} is evaluated on the validation set. We compare both the commonly used mask AP and boundary AP. Results in \cref{tab:segmentation performance COCO} demonstrate the superiority of our framework. It can constantly boost the quality of pseudo masks in all settings, especially for those label-limited scenarios~(\eg, the improvement for PointWSSIS with 1\% annotations can reach 10.3\%). Besides, the segmentation model can also benefit from refined masks, with a significant improvement under different settings, demonstrating that SAM can offer valuable knowledge and cues to improve these label-limited scenarios.





\textbf{Semantic Segmentation.} 
\cref{tab:segmentation performance voc} shows the improvement of pseudo masks generated by unsupervised semantic segmentation~(MaskCLIP~\cite{zhou2022maskclip}) and weakly supervised semantic segmentation~(BECO~\cite{rong2023beco} and CLIP-ES~\cite{lin2022clipes}). We refine the pseudo masks on the train set and use them to train a DeepLabV2~\cite{chen2017deeplab} model following~\cite{lin2022clipes}. The results show that our method brings obvious performance gains for both the pseudo masks and segmentation models. The average improvement of pseudo masks is more than 5\% and even approaches 10\% for MaskCLIP and CLIP-ES. The superior performance across various datasets and settings demonstrates the generalization and flexibility of our framework.





\subsection{Comparison with State-of-the-art.}
In \cref{tab:sota}, we compare our SAMRefiner with state-of-the-art model-agnostic refinement methods, including dense CRF\cite{CRF}, CascadePSP\cite{cheng2020cascadepsp}, CRM\cite{shen2022crm} and SegRefiner\cite{SegRefiner}. 
We first conduct experiments on previously used DAVIS-585, COCO and VOC. The results prove that 1) CRF shows inferior performance due to the lack of high-level semantic context and unfitness for the binary mask.
2) CascadePSP and CRM show competitive performance on semantic segmentation~(VOC), but the improvement is limited or even worse than coarse masks on instance segmentation~(DAVIS-585 and COCO). It is likely that these methods are trained on a merged dataset consisting of extremely accurate mask annotations, which has a strong relation to VOC and makes them fail to generalize to complex scenarios like COCO. We also explore the use of high-quality datasets on SAM~(\ie, HQ-SAM~\cite{ke2023samhq}) in the Appendix.
3) SegRefiner's performance is not stable across different settings because it lacks the ability to process diverse defects in the coarse masks. 4) SAMRefiner is more generic and can improve performance remarkably on various datasets due to its better robustness to the mask noise.



\begin{minipage}[t]{0.45\textwidth}
%\centering
\captionof{table}{Results of semantic segmentation under different supervisions on PASCAL VOC 2012. The Annotations denote the supervision type, including $\mathcal{U}$(unlabeled), $\mathcal{I}$(image-level label). Results on val set are based on training a DeepLabV2 model.}
\vspace{2mm}
\resizebox{0.99\columnwidth}{!}{
  \begin{tabular}{lcll}
    \toprule
    Methods  & Annotations  & mIoU(train)   & mIoU(val)  \\
    \midrule
    MaskCLIP  & $\mathcal{U}$     & 47.8       & 47.3     \\
    \rowcolor{Gray!30} \textit{+SAMRefiner} & $\mathcal{U}$     & 57.3 ${\textcolor{green!80!black}{(+9.5)}}$      & 53.5${\textcolor{green!80!black}{(+6.2)}}$     \\
    \midrule
    BECO  & $\mathcal{I}$     & 66.3       & 69.5     \\
    \rowcolor{Gray!30} \textit{+SAMRefiner} & $\mathcal{I}$      & 71.8${\textcolor{green!80!black}{(+5.5)}}$       & 70.9  ${\textcolor{green!80!black}{(+1.4)}}$    \\
    CLIP-ES & $\mathcal{I}$      & 70.8       & 70.3     \\
    \rowcolor{Gray!30} \textit{+SAMRefiner}    & $\mathcal{I}$          & 79.3${\textcolor{green!80!black}{(+8.5)}}$   & 74.9${\textcolor{green!80!black}{(+3.6)}}$  \\
    \bottomrule
  \end{tabular}
  }
  %\vspace{-4mm}
  \label{tab:segmentation performance voc}
\end{minipage}
\hspace{4mm}
\begin{minipage}[t]{0.45\textwidth}
%\centering
\captionof{table}{Comparisons with SOTA methods. CM represents Coarse Mask.}
  \resizebox{0.99\columnwidth}{!}{
  \begin{tabular}{lcccccc}
    \toprule
    Source  & CM & CRF & PSP  & CRM   & SR & Ours  \\
    \midrule
    \multicolumn{6}{c}{\textbf{DAVIS-585}} \\
    DAVIS-585 & 81.4 & 81.0     &  81.9   & 82.9  & 80.3    & \textbf{87.1} \\
    \midrule
    \multicolumn{6}{c}{\textbf{COCO}} \\
    NB & 15.2 & 13.9   & 15.9  & 15.1 & 15.8 & \textbf{18.4} \\
    PointWSSIS & 29.1 & 24.4     &  28.9   & 25.6  & 29.7    & \textbf{35.3} \\
    MaskRCNN & 35.2 & 31.5 & 34.6 & 31.7 & 35.4 & \textbf{36.5} \\
    \midrule
    \multicolumn{6}{c}{\textbf{PASCAL VOC}} \\
    MaskCLIP  & 47.8 & 48.2     & 55.3 & 56.8  & \textbf{58.5}   & 57.3 \\
    BECO    & 66.3   & 66.5     & 68.4 & 69.0  & 68.7    & \textbf{71.8} \\
    CLIP-ES & 70.8   & 72.6     & 76.9 & 78.7 & 74.7    & \textbf{79.3} \\
    DeepLabV2 & 76.5 & 77.8  & 81.2 & 81.6 & \textbf{83.1} & 78.8 \\
    \midrule
    Time~(h)  & - & 1.0 & 3.4 & 1.5 & 1.4 & \textbf{0.6} \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:sota}\vspace{6mm}
\end{minipage}




Besides, we compare the total time cost to refine masks for COCO train5K~(with about 5K images and 37K masks). CRF is tested with 16 workers, and others are based on one 3090 GPU. SAMRefiner takes less than half the inference time compared to previous methods because SAM can batch process multiple masks in an image simultaneously, while other methods can only refine one mask each time. The batch processing capability makes SAMRefiner more efficient and competitive in practical use.


In addition, considering the original ground-truth annotations used in the COCO dataset are not accurate, we follow SegRefiner\cite{SegRefiner} to evaluate the predictions of different fully supervised segmentation models on COCO val set using LVIS\cite{gupta2019lvis} annotations. Results in \cref{tab:coco-lvis0} indicate that our method outperforms other works by a large margin and can consistently enhance the mask quality generated by various networks~(\eg, both CNN and Transformer), validating its generality for broad applications.



\begin{table}[t]
  \caption{Performance of refined masks on COCO val set using LVIS annotations.}
  \vspace{-2mm}
  \begin{subtable}[h]{0.35\textwidth}
  \centering
  \caption{Results on MaskRCNN.}
  %\vspace{-2mm}
  \scalebox{0.64}{
  \begin{tabular}{lll}
    \toprule
    Method  & AP$^{\text{mask}}$ &  AP$^{\text{boundary}}$  \\
    \midrule
    MRCNN(RN50) & 39.8 & 27.3        \\
    +SegFix & 40.6 & 29.1        \\
    +BRP  &  41.0  &  30.4   \\
    +SegRefiner & 41.9  &  32.6   \\
    \rowcolor{Gray!30} +SAMRefiner & 45.3${\textcolor{green!80!black}{(+5.5)}}$   & 35.9${\textcolor{green!80!black}{(+8.6)}}$    \\
    \midrule
    MRCNN(RN101) & 41.6 & 29.0        \\
    +SegFix & 42.2 & 30.6        \\
    +BRP  &  42.8  &  32.0   \\
    +SegRefiner & 43.6  &  34.1   \\
    \rowcolor{Gray!30} +SAMRefiner & 46.6${\textcolor{green!80!black}{(+5.0)}}$   & 36.9${\textcolor{green!80!black}{(+7.9)}}$    \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:coco-lvis1}
  \end{subtable}
  \hfill
  \begin{subtable}[h]{0.65\textwidth}
  \centering
  \caption{Results on more segmentation models.}
  %\vspace{-2mm}
  \scalebox{0.65}{
  \begin{tabular}{lll|lll}
    \toprule
    Method  & AP$^{\text{mask}}$ &  AP$^{\text{boundary}}$ & Method  & AP$^{\text{mask}}$ &  AP$^{\text{boundary}}$ \\
    \midrule
    PointRend & 41.5 & 30.6  &   SOLO  &  37.4  &  24.7    \\
    +SegRefiner & 42.8 & 33.7  &   +SegRefiner  &  40.5  &  31.3    \\
    \rowcolor{Gray!30} +SAMRefiner & 45.5${\textcolor{green!80!black}{(+4.0)}}$   & 36.0${\textcolor{green!80!black}{(+5.4)}}$ & +SAMRefiner & 44.1${\textcolor{green!80!black}{(+6.7)}}$   & 34.2${\textcolor{green!80!black}{(+9.5)}}$   \\
    \midrule
    RefineMask  &  41.2  &  30.5 & CondInst & 39.8 & 29.2    \\
    +SegRefiner & 41.9 & 33.0  &   +SegRefiner  &  41.1  &  32.2    \\
    \rowcolor{Gray!30} +SAMRefiner & 44.7${\textcolor{green!80!black}{(+3.5)}}$   & 35.3${\textcolor{green!80!black}{(+4.8)}}$ & +SAMRefiner & 45.2${\textcolor{green!80!black}{(+5.4)}}$   & 35.8${\textcolor{green!80!black}{(+6.6)}}$   \\
    \midrule
    MaskTransifiner & 42.2 & 31.6   & Mask2Former & 46.8 & 37.0     \\
    +SegRefiner & 43.3 & 34.4  &   +SegRefiner  &  47.4  &  38.8    \\
    \rowcolor{Gray!30} +SAMRefiner & 46.3${\textcolor{green!80!black}{(+4.1)}}$   & 36.3${\textcolor{green!80!black}{(+4.7)}}$  & +SAMRefiner & 49.0${\textcolor{green!80!black}{(+2.2)}}$   & 39.0${\textcolor{green!80!black}{(+2.0)}}$ \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:coco-lvis2}
  \end{subtable}
  \label{tab:coco-lvis0}
%\vspace{-4mm}
\end{table}

  




\section{Conclusion}
This paper uncovers the deficiency of SAM in the mask refinement task and proposes a universal and efficient framework called SAMRefiner to adapt SAM for mask refinement. We propose a multi-prompt excavation to generate diverse prompts that are robust to the defects in coarse masks. An optional IoU adaption step is introduced to further boost the performance on the target dataset without additional annotated data. We evaluate SAMRefiner on a wide range of image segmentation benchmarks under different settings, demonstrating its consistent accuracy and efficiency. 





\section{Acknowledgment} 
This work was supported in part by The National Key R\&DProgram of China (NO.2022ZD0160101, NO.2022ZD0160102)























\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}


\newpage
\appendix



%\begin{center}
%\begin{minipage}{0.7\linewidth}
\begin{algorithm}[tb]%[H]
\caption{The Region Merging Strategy} 
\label{alg:algo1} 
\begin{algorithmic}[1] 
\REQUIRE ~~\\ 
Region number $\mathcal{R}$; region label for each mask pixel $\boldsymbol{M}_{label}$; a hyper-parameter $\mu$.
\ENSURE ~~\\ 
Merged regions masks $\mathcal{M}^{stm}$.
\STATE Initialize $\boldsymbol{M}_{merge}=\boldsymbol{M}_{label}$, \, $\mathcal{M}^{stm} = \varnothing$.\\
\FOR{$i = 1$ to $\mathcal{R}$}
\STATE $\mathcal{B}_i \gets$ Extract minimum bounding box in $\boldsymbol{M}_{label}^i$.
\STATE $a_i^{box}, a_i^{mask} \gets$ Compute areas for $\mathcal{B}_i, \boldsymbol{M}_{label}^i$.
\FOR{$j = i+1$ to $\mathcal{R}$}
\STATE $\mathcal{B}_j \gets$ Extract minimum bounding box in $\boldsymbol{M}_{label}^j$.
\STATE $a_j^{box}, a_j^{mask} \gets$ Compute areas for $\mathcal{B}_j, \boldsymbol{M}_{label}^j$.
\STATE $(\bar{\mathcal{B}}, \, \bar{a}^{box}) \gets$ Find merged boxes for $(\mathcal{B}_i, \mathcal{B}_j)$ and compute its area.
\IF{$(a_i^{box}+a_j^{box}) > \mu \cdot \bar{a}^{box}$ and  $(a_i^{mask}+a_j^{mask}) > \mu \cdot \bar{a}^{box}$}
\STATE Merge region $i$ and region $j$.\\
\STATE Update $\boldsymbol{M}_{merge}$.
\ENDIF
\ENDFOR
\ENDFOR
\STATE $\mathcal{G} \gets$ Extract merged region labels from $\boldsymbol{M}_{merge}$.\\
\FOR{$k \in \mathcal{G}$}
\STATE $\mathcal{M}^{stm}$.append($\boldsymbol{M}_{merge}^k$).\\
\ENDFOR
\RETURN $\mathcal{M}^{stm}$. 
\end{algorithmic}
\end{algorithm}
%\end{minipage}
%\end{center}



\section{Additional Details}

\subsection{Datasets Details}

\textbf{DAVIS-585.} DAVIS-585 is proposed in FocalClick~\cite{chen2022focalclick} to evaluate the interactive mask correction task. It consists of 585 samples and generates the flawed initial masks by simulating the defects on ground-truth masks using super-pixels. There are different types of defects, \eg, boundary error, external false positive, and internal true negative, making it a comprehensive benchmark for the mask correction task.

\textbf{MS COCO 2017.} COCO comprises 80 object classes and one background class, with 118,287 training samples and 5,000 validation samples. We perform instance segmentation experiments on COCO following previous works~\cite{wang2022noisyboundary, kim2023devil}. To ensure a fair comparison, we maintain the same split of data subsets~(\eg, 1\%, 5\%, 10\%) as each baseline method. We assess pseudo labels quality by randomly sampling 5,000 images in the train set~(denoted as train5K) that have no intersection with annotated data subsets.

\textbf{PASCAL VOC 2012.} We conduct semantic segmentation experiments on PASCAL VOC 2012 following \cite{lin2022clipes, advcam}. It contains 20 categories and one background category. We evaluate the pseudo mask quality on the train set with 1464 images. An augmented set with 10,582 images~\cite{voc_trainaug} is usually used for training in the WSSS task. 

\subsection{Implementation Details}
\label{sec:implemention_details}
We implement our method with PyTorch~\cite{paszke2019pytorch}. For SAMRefiner, we didn't use multi-scale strategy and images are kept at their original sizes before being processed by SAM. For IoU adaption step, we use SGD optimizer with 0.01 learning rate. The batch size is set to 5 and we only train for 1 epoch. The learning rate is reduced to one-tenth at steps 60 and 100. We use margin ranking loss with the margin as 0.02 and the LoRA rank is set to 4. Note that the IoU adaption step is optional and we only adopt it on DAVIS-585. The time cost reported in the paper is tested on a single 3090 GPU. For instance segmentation, the threshold $\lambda$ is set to 0.1 for the box prompt. For semantic segmentation, $\mu$ used in the STM is set to 0.5. The factors $\omega, \gamma$ for Gaussian distribution are set to 15 and 4 by default.
We present the pseudo-code for the region merging strategy in \cref{alg:algo1}, which is an important component of our split-then-merge~(STM) pipeline for semantic segmentation.



\begin{table}[t]
\caption{Quantitive comparison between automatic grid point prompt and our prompt strategy on DAVIS-585.}
  \centering
  %\resizebox{0.9\columnwidth}{!}{
  \begin{tabular}{lcccc}
    \toprule
    Prompt Type      &     IoU  & boundary IoU  & Time~(minute)\\
    \midrule
    Coarse Mask &  81.4 & 71.4 & - \\
    \midrule 
    Max IoU     &    70.6 & 65.5 & 8.0  \\
    Merge  &   81.9  & 73.1  & 8.0 \\
    \midrule
    Ours &  \textbf{86.9} & \textbf{75.1} & \textbf{1.6} \\
    \bottomrule
  \end{tabular}
  %}
  %\vspace{-4mm}
  \label{tab:grid prompt}
\end{table}


\section{Additional Experiments}



\subsection{Comparison with Automatic Mask Generator}
SAM can produce masks for an entire image by sampling a grid of points over the image as prompts. 
This automatic manner can be used for mask refinement by matching the potential masks to the coarse mask. We validate its performance leveraging two matching criteria: 1) Max IoU: For each coarse mask, we select the SAM-generated segments with the highest IoU as the refined mask. 2) Merging: For each SAM-generated segment, it is viewed as a part of the final refined mask if the overlap area between this segment and coarse mask exceeds a certain percentage~(\eg, 0.5) of this segment area~\cite{chen2023samwsss}. 


\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{images/gird_point_prompt.pdf}
    %\vspace{-6mm}
   \caption{Qualitive comparison with grid point prompts.
   }
   \label{fig:vis_grid_point_prompt}
   %\vspace{-2mm}
\end{figure*}





We compare our prompt excavation strategy with these two automatic grid-style point prompts in~\cref{tab:grid prompt}. We note that the performance drops severely for the Max IoU approach and barely improves for the Merging approach on DAVIS-585. It stems from the inherent drawbacks of this prompt generation manner, which is shown in \cref{fig:vis_grid_point_prompt}. First, the grid point prompts split an image into several fine-grained masks and it is difficult to control the granularity. The best-matched mask selected by Max IoU usually fails to cover the whole object. Second, although the Merging strategy can obtain relatively complete objects, it is susceptible to defects in the coarse mask~(\eg, false positives) and tends to result in over-detected. 
Thirdly, the SAM-generated segments are not exclusive and sometimes an object may be included in multiple masks with different granularity.
It remains challenging to filter them out by the strategies above. In contrast to this bottom-up paradigm, our prompt excavation strategy directly produces diverse prompts for the target object~(top-down paradigm), which is more purposive, accurate and robust to the noise in coarse masks. In addition, these grid-based prompts are inefficient~(\ie, taking 5$\times$ more time than ours) because of the massive prompts and time-consuming post-processing~(\eg, NMS~\cite{neubeck2006NMS}) to filter low-quality and duplicate masks.







\begin{figure}[t]
  \centering
  \begin{subfigure}{0.35\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/ablation_backbone_iter.png}
    \caption{Effects of different backbones and cascaded post-refinement.}
    \label{fig:sub-a}
  \end{subfigure}
  \hspace{6mm}
  \begin{subfigure}{0.35\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/ablation_logitsmask_iter.png}
    \caption{Comparison with SAM predicted mask logits.}
    \label{fig:sub-b}
  \end{subfigure}
  \caption{Ablation study of different backbones and cascaded post-refinement strategies as well as mask logits.}
  \label{fig:sub}
\end{figure}


\subsection{Effects of Different Backbones and Cascaded Post-refinement}
The pre-trained SAM models are available with three backbone sizes and the mask can be iteratively processed by cascaded refinement. We compare the impact of using different backbones and iterations in~\cref{fig:sub-a}. The results show that the largest ViT-H outperforms other backbones at the first iteration, and multiple iterations can further improve the mask quality, especially for the ViT-L backbone. 

Note that the mask prompt of each iteration in the cascaded refinement is from our prompt exvacation strategy~(\ie, Gaussian-style mask). We also compare with some typical practices used in the original SAM, including \textbf{(a)} only using point and box prompt at the first iteration and adding SAM's predicted mask logits~(produced by the previous iteration) at subsequent iterations; \textbf{(b)} using all prompts generated by our method at the first iteration but replacing the Gaussian-style mask with SAM predicted mask logits for subsequent iterations. Results in \cref{fig:sub-b} demonstrate that SAM's mask logits can contribute to point and box~(condition a) in the cascaded refinement but fail to work when our mask prompt is adopted in the initial step~(condition b). This indicates that our Gaussian-style mask can provide more powerful guidance than the mask logits, which not only produce high-quality masks in the initial step but also more advantageous for cascaded refinement.








\subsection{Upgraded Results based on HQ-SAM}
\label{sec:hqsam}
HQ-SAM~\cite{ke2023samhq} is an advanced version of SAM that can enable more accurate segmentation. Our framework can also be applied to this powerful variant, and we conduct experiments on various benchmarks based on it to pursue better performance. In \cref{tab:samhq}, we compare the performance of our framework using SAM and HQ-SAM on DAVIS-585, VOC~(BECO~\cite{rong2023beco} and CLIP-ES~\cite{lin2022clipes}) and COCO~(NB~\cite{wang2022noisyboundary} and WSSIS~\cite{kim2023devil}). Results show that there is a significant improvement on DAVIS-585 and VOC while the performance is fair to SAM on COCO. This is because HQ-SAM enhances original SAM by specifically training on a high-quality dataset with large and salient objects, which aligns well with the characteristics of datasets like DAVIS-585 and VOC. In contrast, COCO has plenty of small objects and may not benefit as much from HQ-SAM. 




\begin{table}[b]
\caption{Comparison between SAM and HQ-SAM. We report IoU / boundary IoU on DAVIS-585, AP / boundary AP on COCO and mIoU on VOC.}
  \centering
  \setlength{\tabcolsep}{6pt}
  \resizebox{0.85\columnwidth}{!}{
  \begin{tabular}{lccccc}
    \toprule
    Model      &  DAVIS-585 & NB & PointWSSIS & BECO  & CLIP-ES \\
    \midrule
    SAM & 87.7 / 78.9 & 18.4 / 11.8 & 35.3 / 24.1 & 71.8 & 79.3  \\
    HQ-SAM & 90.6 / 81.7 & 18.4 / 12.2 & 35.0 / 24.3 & 73.6 & 81.0  \\
    %\midrule
    \bottomrule
  \end{tabular}
  }
  
  %\vspace{-4mm}
  \label{tab:samhq}
\end{table}





\subsection{Applications on Different Tasks}
\textbf{Application on high-resolution images.} We evaluate our SAMRefiner on the BIG dataset, which includes ultra-high resolution images ranging from 2K to 6K. We directly refine the coarse masks generated by DeepLabV3+~\cite{chen2018deeplabv3} based on SAM without dataset-specific finetuning, using IoU and mean Boundary Accuracy (mBA) as metrics following~\cite{cheng2020cascadepsp, shen2022crm}. Results in \cref{tab:big} show that our framework can effectively improve the quality of coarse masks and is superior or comparable to the previous methods by using the powerful HQ-SAM.



\begin{table}[t]
  \caption{Additional experiment results on BIG and relabeled PASCAL VOC datasets. The coarse masks are produced from DeepLabV3+~\cite{chen2018deeplabv3}. }
  \vspace{-2mm}
  \begin{subtable}[h]{1.0\textwidth}
  \centering
  \caption{Results on BIG dataset.}
  \vspace{-2mm}
  \scalebox{0.9}{
  \begin{tabular}{lccccc}
    \toprule
    Method      &  Coarse Mask  & SegFix  & PSP & CRM & Ours \\
    \midrule
    IoU & 89.4 & 90.0  &  92.2  & 91.8  & 93.9 \\
    mBA  & 60.2 & 69.3 & 74.6  &  75.0  &  74.8 \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:big}
  \end{subtable}
  \\
  \\
  \begin{subtable}[h]{1.0\textwidth}
  \centering
  \caption{Results on relabeled VOC.}
  \vspace{-2mm}
  \scalebox{0.9}{
%\begin{minipage}[c]{0.2\linewidth}
%\centering
%\begin{table}
%  \centering
  \begin{tabular}{lccccc}
    \toprule
    Method      &  Coarse Mask  & SegFix  & PSP & CRM & Ours \\
    \midrule
    IoU & 87.1 & 88.0  &  89.0  & 88.3  & 89.6 \\
    mBA  & 61.7 & 66.4 & 72.1  &  72.3  &  71.9 \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:relabeled voc}
  \end{subtable}
  \label{tab:big-voc}
%\vspace{-4mm}
\end{table}



\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{images/fig_cocovalgt.pdf}
    %\vspace{-6mm}
   \caption{Visualizations of COCO annotations and our refined annotations.
   }
   \label{fig:cocovalgt}
   \vspace{-5mm}
\end{figure*}


\textbf{Application on relabeled VOC.}
CascadePSP~\cite{cheng2020cascadepsp} introduces a relabeled VOC dataset with accurate boundary annotations for better evaluation. We follow this setting to validate our framework on this benchmark. Results in \cref{tab:relabeled voc} demonstrate the effectiveness of our method, with better IoU than existing methods. Note that the semantic obscurity may result in the inconsistency between human subjective annotations and SAM predictions, hindering SAM from obtaining better performance.


\begin{table}[b]
\caption{Performance of refined masks on COCO2017 val.}
\setlength{\tabcolsep}{6pt}
  \centering
  \begin{tabular}{lll}
    \toprule
    Data  & AP$^{\text{mask}}$ &  AP$^{\text{boundary}}$  \\
    \midrule
    COCO & 38.3 & 27.3        \\
    \rowcolor{Gray!30} +Ours & 41.5${\textcolor{green!80!black}{(+3.2)}}$   & 33.0${\textcolor{green!80!black}{(+5.7)}}$    \\
    \bottomrule
  \end{tabular}
  \label{tab:coco-lvis}
  \vspace{-4mm}
\end{table}


\textbf{Application on human annotations correction.}
The human-annotated masks can also be coarse due to the strict standard of pixel-accurate annotations. For example, COCO~\cite{lin2014microsoftcoco} masks are annotated in the polygon format, which is inaccurate in the boundary area~(seeing \cref{fig:cocovalgt}). LVIS~\cite{gupta2019lvis} constructs more precise annotations for COCO images. We refine the mask in the COCO val set using our SAMRefiner and evaluate them based on LVIS annotations. 
Results in \cref{tab:coco-lvis} show that our methods can also work for inaccurate human annotations. There is a remarkable increase~(\ie, 3.2\% mask AP and 5.7\% boundary AP) for the mask quality. We provide qualitative comparisons in \cref{fig:cocovalgt}.





\begin{figure}[t]
  \centering
  \begin{subfigure}{0.35\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/sigma_strength_threshold_ablation.png}
    \caption{Analysis of $\omega, \gamma$.}
    \label{fig:more-ablation-a}
  \end{subfigure}
  \hspace{6mm}
  \begin{subfigure}{0.35\linewidth}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{images/ablation_cebox_lambda.png}
    \caption{Analysis of $\lambda$.}
    \label{fig:more-ablation-b}
  \end{subfigure}
  \caption{Ablation study of (a): $\omega, \gamma$ and (b): $\lambda$}
  \label{fig:more-ablation}
\end{figure}


\subsection{Additional Ablation Studies}
\label{sec:more_ablation}
\textbf{Analysis of $\omega, \gamma$ in the mask prompt.} We leverage a Gaussian-style
mask in our prompt excavation strategy, with two factors $\omega, \gamma$ controlling the amplitude and span of the distribution. We perform a sensitive analysis of these two parameters in \cref{fig:more-ablation-a}. 
When $\omega$ is too small (\ie, $\omega=1$), the effect of mask prompts is negligible since the mask inputs of the original SAM are the predicted logits, which are not scaled to 0-1. 
We note that a relatively higher value for $\omega$ can promote mask prompts to benefit mask refinement, and the performance is not sensitive to these higher $\omega$ as well as $\gamma$.



\textbf{Analysis of $\lambda$ in the context-aware elastic box.} We introduce a threshold $\lambda$ in CEBox to determine whether to expand current boxes based on context features, which controls the trade-off of box sizes. We give an analysis of $\lambda$ in \cref{fig:more-ablation-b}. The proposed CEBox has consistent bonuses compared to the baseline~($\lambda=0$) under different thresholds. We set $\lambda=0.1$ in our experiments to avoid over-enlargement.














\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{images/fig_stm1.pdf}
    %\vspace{-6mm}
   \caption{Illustrations of Split-Then-Merge (STM) pipeline. The \textit{Region 3} in red color is small. Please zoom in for better visibility.
   }
   \label{fig:stm1}
   %\vspace{10mm}
\end{figure*}


\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{images/fig_stm2.pdf}
    %\vspace{-6mm}
   \caption{Visual comparisons between STM and baseline (without STM).
   }
   \label{fig:stm2}
   %\vspace{10mm}
\end{figure*}





\section{Technical Details}
%\section{Further elaborations}
\subsection{Further elaborations of STM}
The Split-Then-Merge (STM) pipeline is proposed to solve the multi-object case in the semantic segmentation. In this case, SAM struggles to segment multiple objects with a large distance using common prompts, resulting in either missed detection or false detection (\cref{fig:ablation_split_then_merge}). We propose STM to convert semantic masks with multiple objects into instance masks to ensure better compatibility with SAM. As shown in \cref{fig:stm1}, STM includes two stages: 1) Split: split the mask by finding all connected regions, which tends to be messy and noisy; 2) Merge: iteratively merge the adjacent regions to form semantically meaningful regions based on the box area variation and mask area occupancy (\cref{alg:algo1}). The STM is performed before prompt extraction. Once finished, we can produce prompts based on the merged mask and leverage SAMRefiner for refinement. As shown in \cref{fig:stm2}, STM can effectively mitigate the impact of multiple objects in semantic segmentation, yielding better results than the baseline.







\begin{figure*}[t]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{images/fig_gaussian.pdf}
    %\vspace{-6mm}
   \caption{Visualization of Gaussian-style Masks under different $\gamma$.
   }
   \label{fig:gaussian}
   %\vspace{10mm}
\end{figure*}




\subsection{Further elaborations of Gaussian-style Mask}
\label{sec:appendix_gaussian}
Note that the central point is not the geometry central point of the mask, but the farthest positive point selected by the previous point prompt step. We only apply the Gaussian operation to the foreground region of the mask, and the Gaussian-style mask is a generalized form of the coarse mask. For instance, when the amplitude $\omega$ is set to 1 and the span $\gamma$ is sufficiently large, the Gaussian-style mask is equivalent to the original coarse mask. Visualizations of the Gaussian Mask are presented in \cref{fig:gaussian}.

There are two main reasons for using the Gaussian-style mask: \textbf{1) Compatibility with SAM:} The original SAM doesn't support the binary masks as prompts. This is because the mask prompt merely acts as an auxiliary for point and box in the cascade refinement during SAM pre-training, with the predicted logits of the previous iteration as input to guide the next one. Therefore, the mask input for SAM requires logits with continuous values, while the original coarse mask is discrete-valued (0 and 1). The Gaussian operation can convert the binary mask to continuous, making it compatible with SAM. 
\textbf{2) The object-centric prior:} The center of an object tends to be positive and feature-discriminative, while uncertainty is mostly located along boundaries. The Gaussian-style mask effectively reduces the weights near boundaries. 
As shown in \cref{fig:more-ablation-a}, when $\omega=1$, the performance drops significantly due to the incompatible value space, while the Gaussian transformed mask can consistently outperform the original coarse mask under different $\omega$ and $\gamma$.




\begin{figure*}[h]
  \centering
  \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=0.9\linewidth]{images/fig_book.pdf}
    %\vspace{-6mm}
   \caption{Instance-level feature similarity. 
   }
   \label{fig:book}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    %\fbox{\rule{0pt}{0.5in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=0.9\linewidth]{images/fig_ski.pdf}
    %\vspace{-6mm}
   \caption{Component-level feature similarity.
   }
   \label{fig:ski}
  \end{subfigure}
  \caption{Visualizations of feature similarity between base and other masks.}
  \label{fig:bookski}
  \vspace{-4mm}
\end{figure*}



\subsection{Further elaborations of CEBox}

For SAM, the image features of different instances (even within the same category) exhibit distinct characteristics. This enables SAM to produce fine-grained, component-level segments, making it support a variety of downstream applications. To illustrate this, we analyze feature similarity between different masks in \cref{fig:book}. As shown, the features of different instances, even within the same class, display certain differences. This characteristic allows SAM to distinguish between instances effectively (\eg, adjacent books). Similar conclusions can also be drawn for the part segmentation, as shown in \cref{fig:ski}.
On this basis, we can flexibly adjusting $\lambda$, a threshold to determine the necessity to expand the current box in each direction based on image feature similarity, according to different settings. For instance, a relaxed threshold could be applied for general segmentations, while a stricter threshold may be more suitable for fine-grained segmentations, such as distinguishing different instances or components.




\subsection{Further elaborations of IoU Adaption}
\label{sec:appendix_iouadaption}
Although the original SAM uses an individual token when multiple prompts are provided, we empirically observe that selecting the best mask from the remaining three masks based on the IoU prediction yields better performance than the fourth mask, as shown in \cref{fig:iou-adaption-1}. This is because although the three predictions converge, some details remain different and usually better than the fourth token. We provide visualizations in \cref{fig:4mask} to compare the masks generated by different tokens. Though the improvement may not be remarkable, the advantage of IoU adaptation is that it doesn't require any additional annotated data and only takes advantage of the priors contained in the target dataset.  SAMRefiner++ serves as a complementary enhancement to SAMRefiner when coarse masks on target datasets can provide high-quality guidance and this step is not mandatory.


\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{images/fig_4masks.pdf}
    %\vspace{-6mm}
   \caption{Visualization of masks generated by different tokens in SAM decoder.
   }
   \label{fig:4mask}
   %\vspace{10mm}
\end{figure*}


\subsection{Impact of the distance-guided point sampling strategy}
The distance-guided point sampling strategy outperforms the box-center method as it effectively mitigates the impact of false-positive noise, which often distorts the bounding box and causes the box center to deviate from the actual object, as shown in \cref{fig:boxpoint}.


\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{images/fig_boxpoint.pdf}
    %\vspace{-6mm}
   \caption{Comparison between box center point and distance-guided point.
   }
   \label{fig:boxpoint}
   %\vspace{-4mm}
\end{figure*}


\subsection{Analysis of the quality of coarse masks}
In \cref{fig:coarse}, we provide visualizations of the refined masks based on coarse masks with varying levels of quality. The results show that SAMRefiner works effectively when the coarse masks meet a certain quality standard but may fail when the coarse masks are extremely inaccurate. This is because the mask refinement task becomes an ill-posed problem if the initial mask is too coarse. For example, if the coarse mask only covers a person's head, reconstructing the entire person would be impossible without additional information due to the inherent ambiguity. Fortunately, most real-world coarse masks, such as those generated by model predictions, usually meet a certain quality standard and can be effectively handled by our proposed approach.
























\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.7\linewidth]{images/fig_coarser.pdf}
    %\vspace{-6mm}
   \caption{Visualizations of refined masks based on coarse masks with varying levels of quality.
   }
   \label{fig:coarse}
   %\vspace{10mm}
\end{figure*}



















\section{Discussions and Limitations}
\label{sec:discuss}



\begin{table}[t]
\centering
\caption{Comparison of different mask refinement methods.}
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{|l|p{4cm}|l|p{3cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Method}    & \textbf{Design Principle}                                                     & \textbf{Architecture} & \textbf{Training Data}                                     & \textbf{Advantages}                                                  & \textbf{Drawbacks}                          \\ \hline
\textbf{dense CRF}          & Maximize label agreement \newline between pixels with \newline similar low-level color          & None                  & None                                                      & Training-free, \newline Easy to use                                           & inaccurate                                  \\ \hline
\textbf{CascadePSP}         & Align the feature map \newline with the refinement target \newline in a cascade fashion        & CNN                   & MSRA-10K, \newline DUT-OMRON, \newline  ECSSD, \newline FSS-1000                      & Class-Agnostic, \newline Accurate on \newline semantic segmentation                    & 
 Task-dependent, \newline Inefficient                 \\ \hline
\textbf{CRM}                & Align the feature map \newline with the refinement target \newline continuously                & CNN                   & MSRA-10K, \newline DUT-OMRON, \newline ECSSD, \newline FSS-1000                      & Class-Agnostic, \newline Accurate on \newline semantic segmentation                    & Task-dependent, \newline Inefficient                 \\ \hline
\textbf{SAMRefiner}         & Design noise-tolerance \newline prompts to enable SAM \newline for mask refinement              & Transformer           & SA-1B                                                     & Class-agnostic, \newline Task-agnostic, \newline Accurate, Efficient                   & Objects with \newline intricate architecture         \\ \hline
\end{tabular}}

\label{table:comparison_multiline}
\end{table}


\subsection{Comparison of different mask refinement methods.}

We provide a detailed discussion of the differences between SAMRefiner and related methods (dense CRF, CascadePSP, CRM) in terms of the design principle, architecture, training data, advantages, and drawbacks in \cref{table:comparison_multiline}. Among these methods, dense CRF is a training-free post-process approach based on low-level color characteristics, making it efficient and easy to use. However, it struggles in complex scenarios due to its lack of high-level semantic context. CascadePSP and CRM, on the other hand, focus on aligning the feature map with the refinement target using CNN-based architectures. They are trained on a combined dataset with extremely accurate mask annotations and demonstrate strong performance on semantic segmentation tasks. Nevertheless, their performance on instance segmentation is less competitive, primarily due to the absence of complex cases in their training data and the inherent limitations of CNNs. Additionally, the cascade structure of CascadePSP and the multi-resolution inference required by CRM make them inefficient when handling masks with a large number of objects.

In contrast, SAMRefiner leverages the strengths of SAM by designing noise-tolerant prompts specifically for mask refinement tasks. This approach achieves better accuracy and efficiency compared to existing methods. Nevertheless, it may underperform for objects with intricate structures, a limitation inherited from SAM itself. This issue can be addressed using enhanced variants, such as HQ-SAM,  as the experiments conducted in \cref{sec:hqsam}.






\subsection{The target and relevance of SAMRefiner.} 
SAMRefiner is designed to be a universal framework for correcting coarse pseudo masks generated by various sources. This task is significant due to the wide source of coarse masks in practical scenarios, such as model predictions and even inaccurate human annotations. Our SAMRefiner can be treated as an effective post-processing method to improve data quality and the refined masks can then be used to train advanced models for specific tasks, which we denote as the pseudo-labeling paradigm. Although some recent works attempt to construct large foundation models to achieve open-vocabulary capability and show impressive performance, we argue that the pseudo-labeling paradigm still remains meaningful for certain application scenarios. First, people usually focus on limited semantic categories in specific practical use~(\eg, automatic driving). The open-vocabulary setup is unnecessary and sometimes even detrimental to the performance of focused objects. Second, the foundation models tend to be large and inference-inefficient, which is not suitable for resource-limited and time-sensitive settings. Therefore, it is more efficient to customize specific models for different application scenarios instead of directly using large foundation models. Our framework treats the foundation model as a separate post-processor to improve the data quality of customized models, which is more generic and flexible. It can complement various segmentation methods and has the potential to be complementary to other refinement techniques and foundation models.




\subsection{Application scenarios and limitations of SAMRefiner(++).} 
\textbf{Application scenarios.} In this paper, we propose an effective mask refinement method SAMRefiner and its variant SAMRefiner++. SAMRefiner is a training-free method that refines masks using noise-tolerant prompts. It retains most of the characteristics of the original SAM and inherits its "universal capability." In contrast, SAMRefiner++ refer to the combination of SAMRefiner and IoU Adaption, which require additional training on target datasets. This method is specifically tailored for certain conditions and has strict prerequisites, such as the quality of coarse masks, which is dataset-dependent and may not achieve remarkable results on all datasets. As a result, SAMRefiner++ is not intended to be a universal method. Instead, it offers a potential approach to achieve further improvements without requiring additional annotations. 


\textbf{Limitations.} For the mask refinement task, the final performance is highly affected by the quality of the initial coarse masks. The defects in masks are diverse, making it challenging to design a single effective method that applies to all scenarios. Our prompt excavation strategy proposes diverse prompt types to mitigate the effect of defects in the coarse masks. Although more noise-robust than previous works, it still fails to work when the initial masks are extremely noisy, as is shown in the last few rows in \cref{fig:morevis}. Besides, there may exist semantic obscurity between SAM predictions and our subjectivity~(\eg, whether the category \textit{table} should contain the items on the table in \cref{fig:limits}), which is inevitable due to the lack of downstream data and a potential solution is dataset-specific adaption. Finally, SAM struggles to process multiple objects in semantic segmentation due to the absence of this condition during pre-training. Although our proposed STM can partly mitigate this, sometimes it fails to work~(second row in \cref{fig:limits}). An option to consider is to finetune SAM to make it adapted to this setting.  
 


\begin{figure*}
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.6\linewidth]{images/suppl_limit.pdf}
    %\vspace{-6mm}
   \caption{Failure cases on semantic segmentation.
   }
   \label{fig:limits}
   \vspace{-4mm}
\end{figure*}


\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{images/fig_more_vis.pdf}
    %\vspace{-6mm}
   \caption{More visualizations on VOC. The last three rows show some failure cases.
   }
   \label{fig:morevis}
   %\vspace{25mm}
   \newpage
\end{figure*}





\section{More Qualitative Results}
In \cref{fig:morevis} and \cref{fig:moreviscoco}, we provide more qualitative results of our refined masks and previous works on PASCAL VOC 2012 and COCO 2017. We can observe that our SAMRefiner produces satisfactory segmentation results on boundaries and detailed structures. It is effective in both simple and complex scenes. The failure cases mainly stem from the extreme inaccuracy of coarse masks, resulting in false activation or missed detection.

\begin{figure*}[h]
  \centering
  %\vspace{20mm}
  %\fbox{\rule{0pt}{2in} \rule{0.95\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{images/fig_more_vis_coco.pdf}
    %\vspace{-6mm}
   \caption{More visualizations on COCO. The last two rows show some failure cases.
   }
   \label{fig:moreviscoco}
   %\vspace{10mm}
\end{figure*}













\end{document}
