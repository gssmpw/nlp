\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{url}
\usepackage{multirow}
\usepackage{bm}

%\usepackage[nottoc]{tocbibind}
\newcommand{\red}[1]{\textcolor{black}{#1}}
\usepackage{array}
\newcommand{\bhline}[1]{\noalign{\hrule height #1}}   
\usepackage{booktabs}
\usepackage{caption}
\usepackage{siunitx}
\usepackage{multicol}
\usepackage{arydshln}
\usepackage{byo-twemojis}

\usepackage{subfigure}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\title{Wavelet-based Positional Representation \\for Long Context}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yui Oka, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito\\
NTT Human Informatics Laboratories, NTT Corporation\\
\texttt{yui.oka@ntt.com} \\
%\And
%Ji Q. Ren \& Yevgeny LeNet \\
%Department of Computational Neuroscience \\
%University of the Witwatersrand \\
%Joburg, South Africa \\
%\texttt{\{robot,net\}@wits.ac.za} \\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. 
This is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of positions in longer sequences.
We analyzed conventional position encoding methods for long contexts and found the following characteristics.
(1) When the representation dimension is regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted as a restricted wavelet transform using Haar-like wavelets. 
However, because it uses only a fixed scale parameter, it does not fully exploit the advantages of wavelet transforms, which capture the fine movements of non-stationary signals using multiple scales (window sizes). 
This limitation could explain why RoPE performs poorly in extrapolation.
(2)
Previous research as well as our own analysis indicates that Attention with Linear Biases (ALiBi) functions similarly to windowed attention, using windows of varying sizes.
However, it has limitations in capturing deep dependencies because it restricts the receptive field of the model.
From these insights, we propose a new position representation method that captures multiple scales (i.e., window sizes) by leveraging wavelet transforms without limiting the model's attention field.
Experimental results show that this new method improves the performance of the model in both short and long contexts. 
In particular, our method allows extrapolation of position information without limiting the model's attention field.
\end{abstract}
\section{Introduction}
Several pre-trained large language models based on Transformer architecture \citep{NIPS2017_3f5ee243} have demonstrated robust capabilities in various generative tasks \citep{devlin-etal-2019-bert, 2020t5, NEURIPS2020_1457c0d6, Touvron2023LLaMAOA, jiang2023mistral7b}.
However, limitations on the input sequence length arise due to the computational resource constraints encountered during the pre-training phase. 
Such constraints necessitate a determination of the maximum allowable length of sequences, hereinafter $L_{\rm train}$, prior to the pre-training process, thus hindering the model's performance in processing sequences longer than those encountered during training. 
This weakness is primarily attributed to the positional encoding's ineffectiveness in handling sequences that exceed the length of those encountered during the model's training phase \citep{devlin-etal-2019-bert, press2022train}.

Rotary Position Embedding (RoPE) \citep{su2021roformer} has become a common approach in many language models that handle long contexts, and it employs a rotation matrix to encode positional information and facilitate the processing of long sequences. 
To manage sequences longer than those encountered during training, various scaling strategies \citep{chen2023extendingcontextwindowlarge,bloc97,peng2024yarn,liu2024scaling} have been applied to RoPE, although these often require additional fine-tuning and incur further learning costs in addition to those of pre-training.
In contrast, Attention with Linear Biases (ALiBi) \citep{press2022train} is able to sequence length estimation beyond the limits of pre-training without requiring additional fine-tuning. 
However, ALiBi limits the attention's receptive field \citep{chi-etal-2023-dissecting} in the manner of windowed attention \citep{Beltagy2020Longformer}. 
For this reason, a model using ALiBi may not be able to obtain information that is in a distant dependency relationship.
%This limitation arises from the softmax function in the self-attention mechanism and the linear bias, which reduces the impact of distant tokens.
%\citep{ding2024longropeextendingllmcontext}
\begin{figure*}[t]
\centering
\includegraphics[width=14.0cm]{20241001.png}
\caption{Overview of Wavelet-based Relative Positional Representation
As in RPE \citep{shaw-etal-2018-self}, our method computes a relative positional representation $(p_{m,n})^{T}$ to the query $q_{m}$ and the key $k_{n}$.
Instead of learnable embedding in RPE, the position is computed based on the wavelet function.
Different wavelet functions $\psi_{a,b}$ are used for each dimension of the head $d$. 
Furthermore, the scale parameter $a$ and the shift parameter $b$ change depending on the dimension of the head $d$.
}
\label{intro_w}
\end{figure*}
In this paper, we analyze conventional positional encoding methods for long contexts, and we propose a novel positional representation that permits extrapolation without constraining the attention mechanism's receptive field.
%First, we mathematically show that RoPE performs a process similar to a wavelet transformation—considered the ultimate time-frequency analysis methodology—to capture the dynamic change in a signal over time.
%However, RoPE does not perform a transformation in accordance with the order of positions but rather in accordance with the number of dimensions.
First, we mathematically show that RoPE performs a process similar to a wavelet transformation—considered the gold standard of time-frequency analysis methodology.
We interpreted the position of each token in the sequence as a time point in time-frequency analysis.
However, RoPE does not perform a transformation in accordance with the order of positions but rather in accordance with the number of dimensions, and it does not capture the dynamic change in a signal over time.
Furthermore, the values corresponding to the wavelet scale (i.e., window size) are %restricted, 
constant,
so RoPE does not make good use of the key characteristic of wavelet transforms, which is the ability to analyze signals on multiple scales.
In other words, RoPE may fail to capture the dynamic change in a signal over time, such as what occurs in natural language. 
%This limitation becomes especially salient when considering the longer context than the maximum allowable length of sequences in training.
In this study, we also show that ALiBi provides different window sizes for each head.

Based on these insights, we propose a wavelet transform-based method, using multiple window sizes, to offer a robust and flexible approach to positional encoding.
By performing a wavelet transform along the order of positions and introducing various scale parameters, our method can capture the dynamic changes in a sequence over positions in the manner of the original feature of wavelet transformation, i.e., time-frequency analysis.
Following the methodology of Relative Position Representation (RPE) \citep{shaw-etal-2018-self}, we implement our method with relative ease. 

From our experiments on extrapolation capabilities using the wikitext-103 dataset \citep{merity2017pointer}, the results demonstrate that our method surpasses traditional positional encoding methods in perplexity. 
We also report that our method has lower perplexity than RoPE in experiments with long contexts using the Llama-2 model 
 \citep{touvron2023llama2openfoundation} and the CodeParrot dataset.
%Our paper not only bridges a critical gap in the mathematical understanding of position encoding in the context of natural language processing (NLP) but also sets a new precedent in the application of time-frequency analysis methods for enhancing the interpretability and performance of NLP models. 
%Our analysis further elucidates that our proposed method adeptly represents token positioning without imposing limitations on the receptive field of self-attention mechanisms. 
%本研究では、RoPEが周波数分析手法の一つであるフーリエ変換と類似していることを数学的に説明する。
%RoPEと自己アテンションにより、各トークンの次元成分は周波数成分に変換される。
%しかし、一般にフーリエ変換では信号の時間変化を解析できないことが知られている。
%さらに、フーリエ変換は周期的であり、無限に続く信号に適しているが、多くの自然言語は周期的でない。
%これらの欠点を克服するために、我々は時間周波数解析の手法であるウェーブレット1に基づく位置表現を提案する。
%Shawらの方法を応用し、ウェーブレット変換が外挿に有効であることを示す。
\section{Background}
\subsection{Positional Representation}
Within the Transformer architecture, positional encoding is employed to accurately represent the sequential position of each token. 
Positional encoding can be divided into two main types: absolute position, which expresses the position of a token from the static beginning of the sequence, and relative position, which expresses the position of each token in relation to the other tokens within the sequence.
%Sinusoidal PE \citep{NIPS2017_3f5ee243} used a sinusoidal basis function to compute the absolute position and also added it to the word embedding.
RoPE \citep{su2021roformer}, which adopts a type of absolute position, uses a rotation matrix to compute the position and then multiplies it by the query and key to represent the position.
%In the relative positional representation, which represents the distance from each token to the next, the positional representation is done for queries and keys in multi-head attention, not for word embeddings.
RPE \citep{shaw-etal-2018-self}, based on a type of relative position, uses a learnable embedding that represents the position of distances of up to 16 or 32 tokens by clipping.
Two other variations include T5 Bias \citep{2020t5}, which has an enlarged RPE window size, and Transformer-XL \citep{dai-etal-2019-transformer}, which uses a sine wave for position representation instead of learnable embedding.

Position encoding plays a critical role in enabling models to effectively handle long context sequences, and it allows for extrapolation. 
Relative position is not a position expression that depends on the length of the sequence, so it is effective in extrapolation.
ALiBi \citep{press2022train} is an effective position representation method for extrapolation: It uses the relative position bias of all tokens by adding a linear bias to each head's attention score, rather than using position embedding. 
%However, a drawback arises due to ALiBi's restriction on the self-attention mechanism's receptive field\citep{chi-etal-2023-dissecting}.
However, ALiBi is unable to obtain information in a distant dependency relationship due to its constraints on the self-attention mechanism's receptive field\citep{chi-etal-2023-dissecting}.
%XPOS \citep{sun-etal-2023-length} is  a RoPE approach with exponential decay, and a method to compute attention for each block separated by the maximum length of sequences in training.
On the other hand, absolute position is unsuitable for extrapolation because it expresses the position of all words in the sequence. For this reason, many methods have been proposed for fine-tuning RoPE by interpolating positions in using absolute position \citep{chen2023extendingcontextwindowlarge,bloc97,peng2024yarn}. 
%Additionally, another method performs RoPE position interpolation by optimizing parameters without the cost of fine-tuning \citep{ding2024longropeextendingllmcontext}.
\subsection{Frequency Analysis and Time-frequency Analysis}
Frequency analysis in signal processing involves analyzing the frequency components of a signal to understand its behavior. 
The Fourier transform (FT) \citep{bracewell1986fourier} is a key method for frequency analysis, converting a signal from the time domain to the frequency domain, thus providing a global view of its frequency content. 
However, the FT does not provide any information about \textbf{when} specific frequencies occur.
To address this limitation, time-frequency analysis techniques have been applied. 
The wavelet transform (WT) \citep{doi:10.1137/0515056,192463} offers a more flexible approach by analyzing the signal at multiple scales or resolutions. 
The WT adaptively provides high time resolution for high-frequency components and high frequency resolution for low-frequency components, making it well-suited for analyzing signals with non-stationary or transient features. 
This adaptability allows the wavelet transform to capture both time and frequency information with varying degrees of precision.
\section{RoPE and Wavelet Transform}\label{secrope}
%In this section, we describe how RoPE performs processing in a similar way to wavelet transformation.
\subsection{Preliminary}
\paragraph{Wavelet Transform}
A wavelet is a wave that decays quickly and locally as it approaches zero.
A function $\psi$ defined on a real $\mathbb{R}$ is called a wavelet function if it belongs to the space \red{$L^{2}(R)$} of square integrable functions and satisfies the following conditions:
\begin{equation}
\int_{-\infty}^{\infty}\mid\psi(x)\mid^{2} dx < \infty .
  \label{wave_}
\end{equation}
The wavelet function is defined as follows.
\begin{equation}
\psi_{a,b}(t) = \frac{1}{\sqrt{a}}\psi\Bigl(\frac{t-b}{a}\Bigl) .
  \label{wave_let}
\end{equation}
In this case, $b$ is the shift and $a>0$ is the scale parameter.
The scale parameter $a$ simultaneously changes the range over which the wavelet is localized as well as the wavelet's amplitude.
%Figure \ref{wavelet_transform_image} shows the image of shift and scale of wavelet.
Typical wavelets include the Haar wavelet \citep{Haar1910}, Ricker wavelet \citep{10.1190/1.1445082}, and Morlet wavelet \citep{10.1007/11492429_41}.
%Many wavelets are orthogonal.
\red{Suppose that we sample $T$ values at regular intervals from a continuous signal.}
Wavelet transform (WT) \citep{doi:10.1137/0515056} is the process of transforming a signal $x(t)$ into the frequency domain and time domain by computing the inner product of the wavelet function $\psi_{a,b}(t)$ and signal $x(t)$.
%A wavelet transforms these values \citep{doi:10.1137/0515056} to the frequency domain and time domain by calculating the inner product of the wavelet function $\psi_{a,b}(t)$ and the signal $x(t)$:
\begin{align}
W(a, b) &= \sum^{T-1}_{t=0}\psi_{a,b}(t)x(t) .
%&= \langle x(t), \psi_{a,b}(t)\rangle_{t\in\mathbb{R}}
\label{dis_wavelet}
\end{align}
In some cases, the term "Discrete Wavelet Transform" or "Wavelet Transform" is used to refer to multi-resolution analysis \citep{192463}, but in this paper we follow the original definition.
We can see that the FT only converts to the frequency domain, whereas the WT converts to two domains: scale $a$ and shift $b$.
For example, consider the case of a conversion to two scales and four shifts.
%When $a \in [2, 4]$ and $b \in [0, 1, 2, 3]$, the wavelet transform can be expressed in terms of determinants.
%This wavelet transform matrix is an orthogonal matrix:
When $a \in [2, 4]$ and $b \in [0, 1, 2, 3]$, the wavelet transform can be expressed in terms of determinants as follows:
\begin{align}
\begin{bmatrix}
W(2, 0) \\
W(4, 0) \\
W(2, 1) \\
W(4, 1) \\
\vdots\\
W(4, 3) \\
\end{bmatrix}
=
\begin{bmatrix}
\psi_{2, 0}(0) & \psi_{2, 0}(1) &\psi_{2, 0}(2) & ... & \psi_{2, 0}(T-1)\\
\psi_{4, 0}(0) & \psi_{4, 0}(1) &\psi_{4, 0}(2) & ... & \psi_{4, 0}(T-1)\\
\psi_{2, 1}(0) & \psi_{2, 1}(1) &\psi_{2, 1}(2) & ... & \psi_{2, 1}(T-1)\\
\psi_{4, 1}(0) & \psi_{4, 1}(1) &\psi_{4, 1}(2) & ... & \psi_{4, 1}(T-1)\\
\vdots &\vdots &\vdots &\ddots & \vdots\\
\psi_{4, 3}(0) & \psi_{4, 3}(1) &\psi_{4, 3}(2) & ... & \psi_{4, 3}(T-1)\\
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
x(2) \\
\vdots\\
x(T-1) \\ 
\end{bmatrix}.
\label{wave2}
\end{align}
%\textcolor{red}{This wavelet transform matrix is inherently orthogonal.}
Furthermore, since $\psi_{a,b}(t)=\psi_{a,0}(t-b)$ from Eq.\ref{wave_let}, $\psi$ of the wavelet transform in Eq \ref{wave2}  is expressed as follows.
\begin{align}
\small
\begin{bmatrix}
W(2, 0) \\
W(4, 0) \\
W(2, 1) \\
W(4, 1) \\
\vdots\\
W(4, 3) \\
\end{bmatrix}
=
\begin{bmatrix}
\psi_{2, 0}(0) & \psi_{2, 0}(1) &\psi_{2, 0}(2) & ... & \psi_{2, 0}(T-1)\\
\psi_{4, 0}(0) & \psi_{4, 0}(1) &\psi_{4, 0}(2) & ... & \psi_{4, 0}(T-1)\\
\psi_{2, 0}(-1) & \psi_{2, 0}(0) &\psi_{2, 0}(1) & ... & \psi_{2, 0}(T-2)\\
\psi_{4, 0}(-1) & \psi_{4, 0}(0) &\psi_{4, 0}(1) & ... & \psi_{4, 0}(T-2)\\
\vdots &\vdots &\vdots &\ddots & \vdots\\
\psi_{4,0}(-3) & \psi_{4,0}(-2) &\psi_{4,0}(-1) & ... & \psi_{4,0}(T-3)\\
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
x(2) \\
\vdots\\
x(T-1) \\
\end{bmatrix}
\label{wave3} .
\end{align} 
%At this time, m and theta are constants that are not affected by the variable t of the wavelet function.
%Note that the values of the wavelet matrix outside the local range become 0 due to the characteristics of the scale parameter $a$. 
%The local range of the wavelet changes depending on the wavelet function.
Due to the characteristics of the scale parameter $a$, the values of the wavelet matrix become 0 or approach 0 outside a certain range that depends on the specific wavelet function.
\paragraph{RoPE}
RoPE incorporates positional information directly into the self-attention mechanism by rotating the query and key vectors in complex space.
When divided into even and odd dimensions, the following calculations are performed for the $m$-th query in each sequence. 
In even dimensions, RoPE is expressed as follows.
\begin{align}
\small
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{2} \\
\vdots\\
q^{m}_{d-2}\\
\end{bmatrix}
=
\begin{bmatrix}
\cos m\theta_{1}&-
\sin m\theta_{1}& 0 & 0 & ...&0&0\\
0 & 0 & \cos m\theta_{2}&-\sin m\theta_{2}& ...&0&0\\
\vdots &\vdots &\vdots &\vdots &\ddots &\vdots & \vdots\\
0 & 0 &0 & 0 &...&\cos m\theta_{d/2}&-\sin m\theta_{d/2}\\
\end{bmatrix}
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
\vdots\\
q^{m}_{d-2}\\
q^{m}_{d-1}\\ 
\end{bmatrix}.
\label{rope2_odd}
\end{align}
where $q^{m} \in \mathbb{R}^{1\times d}$ is the $m$-th query when the number of dimensions is $d$ and $\theta_{i} = 10000^{-2(i-1)/d}, i \in [1,2,...,d/2]$.
For RoPE in odd dimensions, see Appendix \ref{rope_aa}.
The same process is also performed for the $n$-th key $k^{n} \in \mathbb{R}^{1\times d}$.
\subsection{Theoretical Analysis}\label{sec32}
First, we show the wavelet transform using the following two Haar-like wavelets \citep{Haar1910}.
\begin{equation}
  \psi (t) =
  \begin{cases}
    \cos f(t) &0\leq t \textless 1 ,\\
    -\sin f(t) &1 \leq t \textless 2,\\
    0 &\mathrm{otherwise}.\\
  \end{cases}
    \psi^{'} (t) =
  \begin{cases}
    \sin f(t) &0\leq t \textless 1 ,\\
    \cos f(t) &1 \leq t \textless 2,\\
    0 &\mathrm{otherwise}.\\
  \end{cases}  \label{rope_wave_haar_define}
  \end{equation}
%where $\theta$ is assumed to be a real constant.
\red{Here, $f: \mathbb{R} \rightarrow \mathbb{R}$ is a function that satisfies
 $ \int_{-\infty}^\infty \psi(t) \, dt = 0 $ and Eq.(\ref{wave_}).}
%such that $0 \leq f(t) < \frac{2}{\pi} + 2k\pi$ for $0 \leq t < 1$, and $\frac{2}{\pi} + 2k\pi \leq f(t) < \pi + 2k\pi$ for $1 \leq t < 2$.}
%These two Haar-like wavelets clearly satisfy the conditions of Eq.(\ref{wave_}).
Assuming that when $x(t) (0 \leq t \leq d-1)$ is a signal with $d$ elements, the wavelet $\psi$ is used and wavelet transform is performed at each scale $a=1$.
We define the shift parameter as $b_j = j - \delta(j) (j=0,2,..,d-2)$.
Here, $\delta(t)$ is a function such that $0\leq t \leq d-1$ and $0\leq \delta(t) < 1$. %Also, $i$ can be expressed in terms of $j$ as $i =\left\lceil \frac{j+1}{2} \right\rceil$.
%and shift $b\in [0,2,...,d-2]$, the following results are obtained.
When the wavelet function is Haar-like wavelet $\psi (t)$ in Eq.(\ref{rope_wave_haar_define}) and $a =1$ and $b \in [b_0, b_2,..,d_{d-2}]$, the wavelet matrix $\psi$ in the wavelet transform $w = \psi x$ can be expressed in terms of determinants as follows.
\begin{align}
\small
\begin{bmatrix}
W(1, b_0) \\
W(1, b_2) \\
\vdots\\
W(1, b_{d-2}) \\
\end{bmatrix}
=
\begin{bmatrix}
\cos \phi_0 & -\sin \phi_1 & 0 & 0 & ... & 0& 0 \\
0 & 0 & \cos \phi_2 & -\sin \phi_3 & ... & 0& 0 \\
\vdots &\vdots &\vdots &\ddots & \vdots& \vdots& \vdots\\
 0 & 0 & 0 & 0 &... & \cos \phi_{d-2} & -\sin \phi_{d-1} \\
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
\vdots\\
x(d-2) \\
x(d-1) \\ 
\end{bmatrix}.
\label{wave_rope}
\end{align}
\red{To simplify the notation in the matrix representation above, we write $\phi_j$ for $j=0,1,\dots,d-1$, where $\phi_j = f(1+\delta(j))$ if $j$ is odd, and $\phi_j=f(\delta(j))$ otherwise.
Let $x$ be the query $q^{m}$, and define $f$ such that $\phi_j=
\phi_{j+1} = m\theta_{\left\lceil \frac{j+1}{2} \right\rceil}$ for $j=0,2,4, \dots, d-2$, where $\theta_{i}=10000^{-2(i-1)/d}$ and $i \in [1,2,...,d/2]$.
Under this definition, the transformation matrix of Eq. (\ref{wave_rope}) becomes identical to that of Eq. (\ref{rope2_odd}) in RoPE. \footnote{\red{The proof of the existence of $f(t)$ that satisfies this condition is provided in Appendix \ref{sec:appendix_proof_f}.}}}
In other words, RoPE can be viewed as a wavelet transform using Haar-like wavelets that change amplitude on a fixed scale.
%Furthermore, the same result as RoPE from Eq (\ref{rope2_e}) can be obtained when using $\psi^{'}$ for wavelet transformation. 
Furthermore, the same result as RoPE in odd dimensions can be obtained when using $\psi^{'}$ for wavelet transformation.
\footnote{Additionally, when $\sin m\theta_{i}=\cos m\theta_{i}$, the Haar wavelet matrix and RoPE are the same when the scale is 2, and the shift is $[2, 4, \dots, d/2]$. 
Refer to Appendix \ref{sec:appendix_haar} for the detailed proof. }
This wavelet transform in RoPE is performed across the number of query head dimensions $d$.
Therefore, RoPE can be considered a wavelet transformation along the head dimension using a wavelet with a fixed scale of 2.\footnote{From previous research\citep{10.5555/3495724.3496356}, we also hypothesized that this could be equivalent to a Fourier transform.
However, this hypothesis does not hold
(refer to Appendix \ref{rope_hypo} for details).}
\section{Window Size Variability in ALiBi}\label{secalibi}
ALiBi has a restricted receptive field and behaves in the manner of windowed attention \citep{chi-etal-2023-dissecting, Beltagy2020Longformer}.
A receptive field refers to the specific region of the input space that significantly influences the model's output, typically representing the area where the most relevant features are captured.
%and show from the attention map that ALiBi has a different window size for each head.
ALiBi is expressed as
\begin{equation}
\mathrm{softmax}(q_{m}K^{T}+slope\cdot[-(m-1),\dots,-2,-1,0]),
  \label{alibi_f}
\end{equation}
where the $slope$ is a head-specific slope fixed before training and $K^{T} \in \mathbb{R}^{m\times d}$ is the first $m$ keys. 
In this section, we analyzed the window size in ALiBi using the attention map.
\begin{figure*}[t]
\centering
\includegraphics[width=12.0cm]{20240919.png}
%Scores scaled by softmax normalization
\caption{Heatmap of scaled attention scores via softmax normalization in ALiBi without non-overlapping inference. The vertical axis represents the query, while the horizontal axis corresponds to the key in the attention map. For clarity, values of 0.001 or more are mapped to black, while values below that are mapped to yellow. The maximum allowable length of sequences is $L_{\rm train}=512$, and the inference length is $1012$. 
}
\label{alibi_attn}
\end{figure*}
\subsection{Insights from Attention Map Analysis}
A heatmap of scaled attention scores obtained through softmax normalization is shown in Figure \ref{alibi_attn}.
The number of heads $N$ is 8, and the slope of ALiBi is $[\frac{1}{2}, \frac{1}{4},\frac{1}{8},\frac{1}{16},\frac{1}{32},\frac{1}{64},\frac{1}{128},\frac{1}{256}]$.
In extrapolation, sequences are often divided, but in this section the sequences are not divided.
The experimental setting was set to the same as that in Section \ref{sec6.1}.
The perplexity results are shown in Table \ref{result1}.

The attention map shows that ALiBi uses multiple window sizes corresponding to relative positions and that the window size increases as the slope decreases.
%In particular, with complex arrays, having various windows like this makes it possible to fully capture subtle changes in the data.
Moreover, previous research \citep{chi-etal-2023-dissecting} shows that constraining the window size (slope) to a single value leads to increased perplexity. 
Consequently, one of the reasons ALiBi is effective, compared to a previous relative position using fixed window sizes in T5 Bias \citep{2020t5}, is its ability to accommodate multiple window sizes. 
ALiBi does not perform calculations like those in Eq. (\ref{dis_wavelet}), so it does not exactly match the wavelet transform.
However, having windows of various sizes is similar to the role of the scale parameter used in wavelet transforms. 
%While this approach is useful for maintaining computational simplicity, it may not fully capture subtle changes in the data, especially in complex sequences. 
\section{Wavelet-based Positional Representation}\label{sec_wave_hyp}
Wavelet transform (WT) is a method of analyzing signals using variable-scale wavelets, and it is possible to adjust the scale of the window. This scalability allows both broad and fine signal features to be efficiently extracted by shifting the wavelet while changing the window size. In particular, this is suitable for investigating non-stationary signals.
For this reason, we believe that the wavelet transform approach is effective for capturing the dynamic fluctuations of signals that change over time, and it is also effective for the fluid nature of natural language, which is not constrained by periodicity. Furthermore, when extrapolating, it is important to be able to respond flexibly to changes in context and information. For this reason, we believe that the wavelet transform is also an effective method for extrapolation.

%Inspired by the fact that RoPE performs like wavelet transforms along dimensions (Section \ref{secrope}), as well as ALiBi providing windows of various sizes (Section \ref{secalibi}), 
When applying wavelet transforms to positional encoding, a key question arises: Which features should be leveraged for handling long-context dependencies? 
%Table \ref{overview_wave_aaa} provides an unified view of conventional positional encoding methods. 
Notably, RoPE shares conceptual similarities with the wavelet transform (Section \ref{secrope}); however, RoPE depends on absolute positional information, which limits its effective context window to the training length ($L_{\rm train}$) and restricts its extrapolation capabilities. 
In contrast, ALiBi offers extrapolation capabilities by using relative position, and it supports varying window sizes (Section \ref{secalibi}). 
However, ALiBi’s linear bias constrains its receptive field, making it insufficient for capturing long-range dependencies. 
According to \citet{press2022train}, conventional relative positional encoding (RPE) methods \citep{shaw-etal-2018-self, 2020t5}, which rely on a fixed window size, are similarly ineffective for extrapolation. 
In conclusion, we adopt relative position with flexible window sizes to handle long-context and extrapolation.

Accordingly, we propose positional representation based on wavelet transform with the following characteristics:
\begin{enumerate}
 \item\textbf{Position-based Transformation}: RoPE predominantly relies on independent transformation based on the 'head' dimensions. 
ALiBi employs multiple windows based on the relative position of the sentence, rather than the dimension of the head, which may contribute to its performance.
Therefore, we apply a wavelet transform based on the relative position of the sentence.
%\item\textbf{Complexity of Wavelets}: 
\item\textbf{Type of Wavelet}: RoPE can be thought of as a wavelet transform using the Haar wavelet, which is the simplest wavelet.
However, Haar wavelets might fall short in capturing the intricacies of natural languages. 
Transitioning toward the use of more sophisticated wavelet functions could enhance our approach to distilling and representing a broader spectrum of features inherent in natural languages. 
 \item\textbf{Diversification of Window Sizes (Scale Parameters)}: From our analysis of ALiBi, we found that having multiple windows is effective for long contexts. The original version of RoPE works with a single fixed scale. To address this limitation, we introduce a variety of scale and shift parameters.
\end{enumerate}
%In this section, we propose a position representation that incorporates these options in order to supports extrapolation.
\subsection{Methodology}
\paragraph{Incorporating Wavelet Transform into PE}
Due to the wavelet shift feature, we adopt relative position representation using ALiBi because it is more suitable than absolute position representation. \footnote{\red{We also considered incorporating wavelet transforms into RoPE, but decided not to do this because it would make the computational cost even higher. A discussion on this is included in Appendix \ref{rope_base_wavelet}.}}
In a transformer model \citep{NIPS2017_3f5ee243}, the self-attention mechanism operates by projecting the input sequence into three distinct representations—queries ($Q$), keys ($K$), and values ($V$)—using learnable weight matrices. 
Self-attention sublayers employ $N$ attention heads.
In self-attention sublayers, $e_{m,n}$ is the attention score for each query, and then the key is calculated.
RPE\citep{shaw-etal-2018-self} expresses position by calculating the inner product of the query and the relative position embedding.
We incorporate the wavelet function into RPE as follows.
\begin{equation}
  e_{m,n}
  = \frac{q_{m}k_{n}^{T}+q_{m}(p_{m,n})^{T}}{\sqrt{d}} ,
  \label{p1}
\end{equation}
where $q_{m} $ is the $m$th query $(q_{m} \in \mathbb{R}^{1\times d}, 1 \leq m\leq L)$ of a sentence of length $L$, $k_{n}$ is the $n$th key $(k_{n}\in \mathbb{R}^{1\times d}, 1 \leq n\leq L)$ for $q_{m}$, and
$d$ is the number of dimensions of each head.
Here, $p_{m,n}$ is the relative position from the $m$-th query to the $n$-th key.
RPE \citep{shaw-etal-2018-self} uses learnable embedding for $p_{m,n} \in \mathbb{R}^{d}$ and a fixed scale by clipping.
However, instead of using learnable embeddings to represent $p_{m,n}$, we use $d$-pattern wavelet functions with multiple scales to calculate the position.
In our method, there is no clipping, and the distance of the position expression is fixed regardless of the length of the sentence.
\paragraph{Wavelet Function}
In conventional wavelets, such as in Eq. (\ref{wave_let}),
the amplitude also varies depending on the scale parameter $a$.
In the proposed method, all amplitudes are the same.
\begin{equation}
\psi_{a,b}(t) = \psi\Bigl(\frac{t-b}{a}\Bigl) .
  \label{wave_let2}
\end{equation}
The variable $t$ is assigned the relative position, which is $t=m-n$. We used the Ricker wavelet \citep{10.1190/1.1445082} as a base wavelet,
which is formulated as follows.
\begin{equation}
\psi(t)=(1-t^2)\exp\Bigl(\frac{-t^2}{2}\Bigl) .
\label{ricker_calc}
\end{equation}
%The value of Ricker Wavelet with fixed amplitude is between 1 and approximately -0.46, and the wave is localized between $-4a \leq t \leq 4a$.
%Figure \ref{proposed_pos2} shows part of the wavelet function used in the proposed method.
%The scale parameter allows the distance of the positional representation to be specified explicitly, and the shift parameter allows a different wavelet-based positional representation in each dimension.
\paragraph{Shift and scale parameters}
We use $s$ distinct patterns for the scale parameter $a$ and $\frac{d}{s}$ patterns for the shift parameter $b$.
\begin{equation}
(a, b) \in \{2^0,2^1,2^2,...2^{s-1}\} \times \{0, 1, 2, 3,..., \frac{d}{s}-1\} .
  \label{p20240917}
\end{equation}
The scale parameter is a power of 2 derived from the principles of the discrete wavelet transform. 
By combining the $\frac{d}{s}$-pattern shift parameters $b$ with the $s$-pattern scale parameters $a$, we generate $d$ distinct wavelets. 
In this way, our method can set the $s$-pattern \textbf{context window size} using the scale parameter $a$ and the $d$-pattern \textbf{context window} using both the scale parameter $a$ and the shift parameter $b$.
For instance, with a head dimension of $d=128$, we use $s=8$ scale variants ($a \in \{2^0,2^1,...,2^7\}$) and 16 shift variants ($b \in \{0,1,2,...,15\}$), resulting in $8 \times 16 = 128$ unique wavelets.
Finally, $p_{m,n}$ is computed as follows.\footnote{Implementation tips for reducing the memory and computational efficiency of the proposed method are included in Appendix \ref{a3}.}
\begin{equation}
  p_{m,n} = \Bigl(1-\Bigl(\frac{m-n-b}{a}\Bigl)^2\Bigl)\exp \Bigl(-\frac{1}{2}\Bigl(\frac{m-n-b}{a}\Bigl)^2\Bigl) .
  %, a=2^{n}, b= d mod N
  \label{p20240916}
\end{equation}
%The scale parameter $a$ was chosen so that the wave localization distance does not exceed the maximum length $L_{\rm train}$.
%For example, when the dimensions is $d=128$ and the number of heads is $N=8$, $a$ and $b$ are as follows：
%\begin{equation}
%  p_{m,n} = \psi\Bigl(\frac{m-n-(d \bmod N)}{2^h}\Bigl)
%  %, a=2^{n}, b= d mod N
%  \label{p20240916}
%\end{equation}
\section{Short-Context Experiment}\label{sec5}
\subsection{Experimental Settings}\label{sec6.1}
First, we conducted a small-scale experiment to compare our approach with various  position encodings.
We used the WikiText-103 dataset \citep{merity2017pointer},
which consists of over 103 million tokens of English Wikipedia articles.
We performed a comparative evaluation using a Transformer-based language model \citep{baevski2018adaptive}.
The dimensionality of the word embedding $d_{model}$ is 1024, the number of heads $N$ is 8, the dimensionality of the heads $d$ is 128, and the number of layers is 16.
The implementation was based on the fairseq \citep{ott2019fairseq}-based code\footnote{\url{https://github.com/ofirpress/attention_with_linear_biases}} provided in a previous work\citep{press2022train}, and all hyperparameters were set to the same values as those in the literature\citep{press2022train}.\footnote{See Appendix \ref{a4} for more details of hyperparameters.}
%Perplexity was used as the evaluation metric.
The maximum allowable lengths of sequences were set to $L_{\rm train}=512$ and $L_{\rm train}=1024$.
\paragraph{Compared Methods}
Although $\theta=10,000$ is usually used for RoPE, it has been found that extending $\theta$ to 500,000 is effective for long contexts \citep{xiong-etal-2024-effective}. Therefore, we compared $\theta=10,000$ with $\theta=500,000$.
In addition to ALiBi and RoPE, the following position representations were also compared:
NoPE \citep {NEURIPS2023_4e85362c}, in which position information is given, and TransXL \citep{dai-etal-2019-transformer}, which is a relative positional representation that uses sine waves.
%\begin{itemize}
%  \item NoPE \citep {NEURIPS2023_4e85362c} - No position information is given.
%  \item Trans-XL \citep{dai-etal-2019-transformer} - Relative positional representation uses sine waves.
%\end{itemize}
\paragraph{Evaluation Metric}
We use perplexity as our evaluation metric.
Following previous research \citep{press2022train}, we evaluated the validation set.
To evaluate sequences longer than $L_{\rm train}$ tokens, it is common to divide the sequence into $L_{\rm train}$-length sub-sequences, evaluate each independently, and report the average score.
However, methods that use relative positions to express a wide range, such as ALiBi, Trans-Xl, and the proposed method, are able to consider a wider range of contexts than $L_{\rm train}$.
For this reason, in this paper, we report not only the perplexity of non-overlapping inference but also the normal perplexity when the sequence is not divided into partial sequences.
Note that when the sequence length is less than $L_{\rm train}$, the scores for the perplexity of non-overlapping inference and the normal perplexity without division into partial sequences are the same.
Of course, when perplexity is considered without division into partial sequences, the performance of RoPE is expected to decrease greatly 
because unknown values are used for RoPE when processing a sequence longer than the length encountered during training.
\begin{table*}
\caption{
Perplexity of validation set in extrapolation experiments using Wikitext-103. Maximum allowable lengths of sequences in pre-training are $L_{\rm train}=512$ and $L_{\rm train}=1024$.
}
    \centering \scriptsize
    \begin{tabular}{lccccccccccc} \toprule
& & \multicolumn{10}{c}{Sequence Length}\\ \cmidrule(lr){3-12}
 & &  \multicolumn{6}{c}{$L_{\rm train}=512$} & \multicolumn{4}{c}{$L_{\rm train}=1024$}\\ \cmidrule(lr){3-8} \cmidrule(lr){9-12}
 & pos & 128 & 256 & 512 & 1012 & 1512 & 2512 & 1024 & 1524 & 3024 & 5024\\ \midrule
\multicolumn{12}{c}{Perplexity in Non-overlapping Inference with $L_{\rm train}$}\\ \midrule
NoPE{\tiny \citep{NEURIPS2023_4e85362c}} & {\tiny -} &  26.38 & 23.23 & 21.53 & 21.52 & 21.53 & 21.53 & 20.81 & 21.52 & 21.49 & 21.45 \\
RoPE {\tiny\citep{su2021roformer}}& {\tiny abs} & 23.82 & 20.98 & 19.39  & 19.35 & 19.39 & 19.38 & 18.42 & 19.51 & 19.52 & 19.48  \\
RoPE {\tiny\citep{xiong-etal-2024-effective}} &{\tiny abs} & 23.81 & 20.95 & 19.35 & 19.32 & 19.35 & 19.33 & 18.50 & 19.53 & 19.54 & 19.50 \\
Trans-XL {\tiny\citep{dai-etal-2019-transformer}}& {\tiny rel}& 24.16 & 21.53 & 19.96 & 19.92 & 19.93 & 19.96 & 18.67 & 19.75 & 19.74 & 19.70 \\
ALiBi{\tiny\citep{press2022train}} & {\tiny rel}& 24.18 & 21.32 &19.69 & 19.64 & 19.69 &19.64 & 18.66 & 19.64 & 19.65 & 19.62 \\\hdashline
Wavelet(Ricker)  & {\tiny rel}&  \textbf{23.64} & \textbf{20.82} & \textbf{19.19} & \textbf{19.15} & \textbf{19.17} & \textbf{19.20} & \textbf{18.26} & \textbf{19.30} & \textbf{19.34} & \textbf{19.26} \\
\midrule
%ALiBi\citep{press2022train} + RoPE\citep{su2021roformer}& 19.29 & 18.36 & 18.12 & 17.89\\ 
%RoPE+ALiBi & 19.29 & 18.36 & 18.12 & 17.89\\
\multicolumn{12}{c}{Perplexity without Non-overlapping Inference}\\ \midrule
NoPE{\tiny \citep{NEURIPS2023_4e85362c}}&{\tiny -} & 26.38 & 23.23 & 21.53 & 21.03 & 21.58 & 48.48 & 20.81 & 20.45 & 22.11 & 59.37 \\
RoPE {\tiny\citep{su2021roformer}} & {\tiny abs}& 23.82 & 20.98 & 19.39 & 23.25 & 44.38 & 93.94 & 18.42 & 18.29 & 33.20 & 122.52 \\
RoPE {\tiny\citep{xiong-etal-2024-effective}} & {\tiny abs}& 23.81 & 20.95 & 19.35 & 23.70 & 40.39 & 77.90 & 18.50 & 18.30 & 29.25 & 83.43\\
Trans-XL{\tiny\citep{dai-etal-2019-transformer}}  & {\tiny rel}& 24.16 & 21.53 & 19.96 & 19.09 & 18.92 & 19.05 & 18.67 & 18.25 & 18.17 & 18.76\\
ALiBi{\tiny\citep{press2022train}}  & {\tiny rel}& 24.18 & 21.32 &19.69 & 18.71 & 18.42 & 18.41 & 18.66 & 18.14 & 17.86 & 17.88\\ \hdashline
Wavelet(Ricker)  & {\tiny rel}& \textbf{23.64} & \textbf{20.82} & \textbf{19.19} &  \textbf{18.23} & \textbf{18.00} & \textbf{17.99} & \textbf{18.26} & \textbf{17.13} & \textbf{17.14} & \textbf{17.44} \\
Haar (Fixed scale) & {\tiny rel}& 24.98 & 22.07 & 20.49 & 51.61 & 116.87 & 299.26 & - & - & - & - \\
Haar  & {\tiny rel} & 23.73 & 20.89 & 19.27 & 18.34 & 18.11 & 18.17 & - & - & - & -\\
Morlet  & {\tiny rel}& 24.15 & 21.28 & 19.65 & 19.02 & 20.46 & 26.56 & - & - & - & -\\
Gaussian  & {\tiny rel}& 23.77 & 20.90 & 19.30 & 18.31 & 18.02 & \underline{17.88} & - & - & - & -\\
%Wavelet $a=2^8$  & & \checkmark & - & - & - & - & - & - & - & 18.29 & 17.21 & 17.30 & 17.66 \\
%ALiBi\citep{press2022train} + WaveRP & \\
%Haar Scaling Bias ($Scale_{max}=256$)   & \\
%Wavelet Scaling Bias ($Scale_{max}=256$)   & 19.92 & 18.98 & 18.79 & 18.87 & 19.12 & 18.73 & 18.62 & 18.68  \\ %1/4, 1/8, ..., 1/512
%Wavelet Scaling Bias ($Scale_{max}=512$)   & 20.11 & 19.19 & 19.00 & 19.20 &  \\  %1/8, 1/16, ..., 1/1024
%Haar Scaling Bias + Haar Wavelet  \\
%Wavelet Scaling Bias + Mexican Wavelet \\
%Wavelet Encoding  & 20.46 & 29.41 & 40.05 & 60.49 & 19.76 & 19.43 & 22.29 & \\
\bottomrule
    \end{tabular}
    \label{result1}
\end{table*}
\subsection{Main Results} \label{sec62}
The experimental results are shown in Table \ref{result1}.
The results of perplexity in inference without overlap show that the proposed method using wavelets achieved the lowest perplexity and was also effective for extrapolation. 
In RoPE, the values used during training are also used in inference without overlap, so the perplexity remains low even when the sequence length exceeds $L_{\rm train}$. 
At the same time, however, perplexity is higher for ALiBi and Trans-XL than for RoPE, which is attributed to the limited context range of the position representation's applicability due to the division of the sequence into sub-sequences.
In contrast, the proposed method maintains low perplexity even in the case of division into sub-sequences, suggesting that the wavelet position representation is highly effective.

On the other hand, perplexity without non-overlapping inference showed the opposite results.
First, since RoPE uses absolute positions, it is necessary to use new values for unknown positions, and thus perplexity increased significantly.
However, in the case of $\theta=500,000$, the increase in perplexity was relatively small.
On the contrary, Trans-XL and ALiBi, which use relative positions, were able to handle longer contexts, and perplexity decreased as the range of position representations expanded.
In the proposed method, perplexity also decreased and the best score was achieved.
Trans-XL uses a position representation based on a periodic sine wave function, but the proposed method, which uses wavelets, could further decrease perplexity.
This result supports our claim (section \ref{sec_wave_hyp}) that an approach like wavelet transformation is more effective than periodic functions in capturing the fluid nature of natural language, which is not constrained by periodicity.
\subsection{Analysis}
\subsubsection{How effective are the other wavelet types?}\label{how_effective_other}
%In the experiments in Section \ref{sec62}, the wavelet function was limited to the Ricker wavelet.
We also conducted experiments to see whether the same effect could be obtained with other wavelets.
The wavelets tested were the Gaussian-based wavelet $\psi(t)=exp(-t^2)$, the Morlet-based wavelet$\psi(t)=exp(-t^2)cos(at) $, and the Haar-based wavelet.
Note that when $\psi(t/a)$ exists in our Morlet wavelet, the frequency of this cosine wavelet is not affected by the scale parameter $a$.
We used the following formula for the Haar wavelet. 
\begin{equation}
    \psi(t) =
  \begin{cases}
    1 &-0.5\leq t \textless 0 ,\\
    -1 &-1 \leq t \textless -0.5,\\
    0 &\mathrm{otherwise}.\\
  \end{cases}
\end{equation}
We kept the shift and scale parameters constant, only changing the wavelet function. 
We also tested the Haar wavelet when set to $a \in \{2^0,2^0,2^0,...2^0\}$. 
Consequently, this restricted Haar wavelet had the same scale parameter setting as the RoPE demonstrated in Section \ref{sec32}.
\footnote{Normally, the wave is localized when $t>0$ in the Haar wavelet, but in the decoder model, only the range $t<0$ is used. 
Therefore, we transformed the Haar wavelet into a form that reflects the original function f(x) across the y-axis.}
The graphs of these wavelet functions are shown in Appendix \ref{a5} (Fig. \ref{wave_other_type}).
Extrapolation experiments were conducted under the same conditions as the experimental setup in Section \ref{sec5}, with $L_{\rm train}=512$ during training.

As shown in Table \ref{result1}, the Ricker-, Haar- and Gaussian-based wavelets had lower perplexity than the Morlet wavelet.
One possibility is that complex wavelets with multiplied cosine waves, such as Morlet wavelets, are not suitable for relative positional representation.
On the other hand, wavelets with all positive values, such as Gaussian-based wavelets, are expected to represent positions within a narrower distance than the window specified by the scale parameter due to softmax normalization.
This suggests that wavelets with a specific range of negative values are suitable, like a Ricker wavelet, for positional representation.
Although the Haar wavelet is simple, it is such a wavelet with negative values within a specific range.
Therefore, it is considered effective, although not as much as a Ricker wavelet.
However, when the scale parameter is restricted ( $a \in \{2^0,...,2^0\}$), as in RoPE, the perplexity increases. 
This demonstrates the importance of having multiple scales, or in this case, window sizes.
We also performed ablation studies for each shift and scale parameter (Appendix \ref{scale_shift_ab}) and for discrete wavelets as well as continuous wavelets (Appendix \ref{wave_type_abration}).
\subsubsection{Can It Handle Tokens with Long-Range Dependencies?}
Figure \ref{proposed_attn} shows the attention map of scaled attention scores obtained through softmax normalization for the proposed method.
The inference length is $L = 1012$ without non-overlapping inference.
The most notable feature of the proposed method is that it is always able to attend to specific tokens.
The words that always receive attention are those that are important in the sentence, such as the special token, the first token, and the subject of the sequence.
On the other hand, ALiBi has a restricted receptive field for attention, making it unable to capture long-distance dependencies.
Similar to the proposed method, RoPE emphasizes important and special words but struggles to capture those that are farther apart. 
Moreover, as the sentence lengthens, it loses the ability to attend to the initial word.
This tendency was also seen in sentences shorter than $L_{\rm train}$.
Accordingly, the proposed method has demonstrated its superiority at capturing long dependencies without restricting the receptive field of attention.
\begin{figure*}[t]
\centering
\includegraphics[width=11cm]{20240917.png}
\caption{Heatmap of scaled attention scores via softmax normalization in 4th head after softmax operation without non-overlapping inference. The vertical axis represents the query, while the horizontal axis corresponds to the key. For clarity, values of 0.001 or more are mapped to black, while values below that are mapped to yellow. The maximum allowable length of sequences in pre-training is $L_{\rm train}=512$ and the inference length is $1012$. See Appendix \ref{a_attn} for other heads.}
\label{proposed_attn}
\end{figure*}
\section{Long Context}\label{sec_long}
\subsection{Experimental Settings}
Next, we conducted a large-scale experiment using a Llama-based model \citep{touvron2023llama2openfoundation}.
We pre-trained the Llama-2-7B\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}} model from scratch.
For pre-training, we used the RedPajama dataset \citep{together2023redpajama},
which selects a 1B-token sample of all samples.
The maximum allowable length of sequences in pre-training was set to $L_{\rm train}=4096$.
For the same reason as given in Section \ref{sec6.1}, we set $\theta=500,000$ for RoPE.
Furthermore, when the scale parameter is $a \in \{2^0,2^1,...,2^7\}$, the range within which the wavelet is localized becomes narrow. Therefore, in our method, we changed the scale parameter to $a \in \{2^2,2^3,...,2^9\}$.
The other parameters are the same as those used for the Llama-2-7B model\citep{touvron2023llama2openfoundation}.
We used CodeParrot \footnote{\url{https://huggingface.co/datasets/codeparrot/codeparrot-clean}} for evaluation, which is good for long-distance testing because it requires an understanding of patterns and contextualization of information over long distances.
\red{
\footnote{See Appendix \ref{a6} for more details of the hyperparameters.}}
\subsection{Main Results}
The experimental results are shown in Table \ref{llama2_result}.
Regardless of whether interpolation or extrapolation was applied, the perplexity of our method was lower than RoPE. 
Therefore, even with large-scale models and long contexts, our method was found to be effective.
Moreover, the results in Section \ref{sec62} show that not dividing the sequence further reduces perplexity. 
Therefore, our method might also be able to further reduce perplexity.
We investigated the use of LongBench\citep{bai-etal-2024-longbench}, with the results given in Appendix \ref{longbench}.

\red{In addition, position interpolation methods \citep{chen2023extendingcontextwindowlarge,bloc97,peng2024yarn,ding2024longropeextendingllmcontext} have been proposed to adapt RoPE for longer contexts. We believe these methods can be integrated into our approach for the following reasons. 
First, the parameter $\theta$ in RoPE corresponds to the scale parameter $a$ in our method, implying compatibility between the two frameworks. 
Both $\theta$ and $a$ refer to the upper limit of the number of positions to be expressed.
Second, the LongRoPE paper \citep{ding2024longropeextendingllmcontext} reveals that performance improves when extrapolation is avoided for the initial positions, which likely aligns with the shift parameter $b$ in our method. 
%Therefore, it is highly likely that existing position interpolation methods can be incorporated into our method.
Thus, it is highly likely that existing position interpolation methods will integrate seamlessly with our approach.}
\begin{table*}[t]
\caption{\label{llama2_result} Perplexity in Non-overlapping Inference with $L_{\rm train}=4096$. }
\begin{center}
\scriptsize
\begin{tabular}{lcccc}
\toprule
% & \multicolumn{5}{c}{CodeParrot}\\\cmidrule(lr){1-5}
& \multicolumn{4}{c}{Sequence Length} \\
 & 4 k & 8 k & 16 k & 32 k \\ \midrule
RoPE {\tiny\citep{xiong-etal-2024-effective}} &   9.45 & 9.33 & 9.12 & 8.90\\
%+YaRN {\tiny\citep{peng2024yarn}} & 4k & 8k &  8.62 & \\
%+YaRN {\tiny\citep{peng2024yarn}} & 4k & 8k &  8.62 & 7.91 & 7.71 & 7.52\\
%+LongRoPE {\tiny\citep{ding2024longropeextendingllmcontext}} & 4k & 8k & 8.63 & \\ 
%+LongRoPE  {\tiny\citep{ding2024longropeextendingllmcontext}} & 4k & 8k & 8.63 & 7.90 & 7.70 & 7.51 \\\hdashline
%Wavelet $a=2^9$ & 4k & 4k &  &\\
Wavelet &  9.00 & 9.01 & 8.83 & 8.60 \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
\section{Conclusion}
In this paper, we demonstrated that RoPE can be interpreted as a wavelet transform, and we introduced a novel positional representation method that leverages the wavelet transform's advantages, effectively capturing positional information across various window sizes. 
Our experimental results demonstrate the proposed method's superior performance in extrapolation tasks when compared to traditional positional representation techniques. 
Importantly, our approach offers the advantage of not constraining the receptive field, which allows more flexible and comprehensive analysis of positions. 
Calculating relative positions is known to require more resources than calculating absolute positions, so we show methods for reducing memory consumption in Appendix \ref{a3}. 
However, the computational overhead of calculating relative positions may still impose a bottleneck, and thus reducing it is an important direction for future work.
%\bibliography{custom}
\nocite{sun-etal-2023-length,10.1063/5.0152833, ngo2023multiresolution, 10204550, li2021learnable, 10.1007/11492429_41}
%なお，引用された文献以外も載せたい場合は，\nocitep{*} とかきます。 特定の文献のみを載せたい場合は \nocitep{Lang:alg,Rudin:RCA} などとします。
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\clearpage
\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Rotary Position Embedding} \label{rope_aa}
RoPE incorporates positional information directly into the self-attention mechanism by rotating the query and key vectors in the complex space.
When divided into even and odd dimensions, the following calculations are performed for the $m$-th query in each sequence. 
In even dimensions, RoPE is expressed as follows.
\begin{align}
\small
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{2} \\
\vdots\\
q^{m}_{d-2}\\
\end{bmatrix}
=
\begin{bmatrix}
\cos m\theta_{1}&-
\sin m\theta_{1}& 0 & 0 & ...&0&0\\
0 & 0 & \cos m\theta_{2}&-\sin m\theta_{2}& ...&0&0\\
\vdots &\vdots &\vdots &\vdots &\ddots &\vdots & \vdots\\
0 & 0 &0 & 0 &...&\cos m\theta_{d/2}&-\sin m\theta_{d/2}\\
\end{bmatrix}
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
\vdots\\
q^{m}_{d-2}\\
q^{m}_{d-1}\\ 
\end{bmatrix}.
\label{rope2_o}
\end{align}
In odds dimensions, RoPE is expressed as follows.
\begin{align}
\small
\begin{bmatrix}
q^{m}_{1} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-1}\\
\end{bmatrix}
=
\begin{bmatrix}
sin\theta_{1}&cos\theta_{1}& 0 & 0 & ...&0&0\\
0 & 0 & sinm\theta_{2}&cosm\theta_{2}& ...&0&0\\
\vdots &\vdots &\vdots &\vdots &\ddots &\vdots & \vdots\\
0 & 0 &0 & 0 &...&sinm\theta_{d/2}&cosm\theta_{d/2}\\
\end{bmatrix}
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
\vdots\\
q^{m}_{d-2}\\
q^{m}_{d-1}\\ 
\end{bmatrix} ,
\label{rope2_e}
\end{align}
where $q^{m} \in \mathbb{R}^{1\times d}$ is the $m$-th query when the number of dimensions is $d$ and $\theta_{i} = 10000^{-2(i-1)/d}, i \in [1,2,...,d/2]$.
%This rotation, determined by $\theta_{i}$, captures both absolute and relative positions, preserving the inner product between vectors. 
The same process is also performed for the $n$-th key $k^{n} \in \mathbb{R}^{1\times d}$.

\subsection{\red{Proof of the Existence of $f(t)$}} 
\label{sec:appendix_proof_f}
%\paragraph{More detailed Explanation}
We prove the existence of $f(t)$ as described in \ref{sec32} such that $\phi_j= \phi_{j+1} = m\theta_{\left\lceil \frac{j+1}{2} \right\rceil}$, where $\theta_{i}=10000^{-2(i-1)/d}$ and $i \in [1,2,...,d/2]$.
Here, we restrict our proof to $\psi (t)$ in Eq.(\ref{rope_wave_haar_define}), but a similar argument can be applied to $\psi^{'} (t)$, following analogous steps to establish its validity.

First, we revisit the definition of $\psi(t)$:
\begin{equation}
    \psi(t) =
  \begin{cases}
    \cos f(t) &0\leq t < 1,\\
    -\sin f(t) &1 \leq t < 2,\\
    0 &\mathrm{otherwise}.\\
  \end{cases}\label{rope_wave２}
\end{equation}
Here, $f: \mathbb{R} \rightarrow \mathbb{R}$ is a monotonous function that satisfies $ \int_{-\infty}^\infty \psi(t) \, dt = 0 $ and Eq.(\ref{wave_}).
Assuming that when $x(t) (0 \leq t \leq d-1)$ is a signal with $d$ elements, the wavelet $\psi$ is used and wavelet transform is performed at each scale $a=1$.
We define the shift parameter as $b_j = j - \delta(j) (j=0,2,..,d-2)$.
Here, $\delta(t)$ is a monotonous function such that $0\leq t \leq d-1$ and $0\leq \delta(t) < 1$. %Also, $i$ can be expressed in terms of $j$ as $i =\left\lceil \frac{j+1}{2} \right\rceil$.
%and shift $b\in [0,2,...,d-2]$, the following results are obtained.
\begin{align}
\scriptsize
\begin{bmatrix}
W(1, b_0) \\
W(1, b_2) \\
\vdots\\
W(1, b_j) \\
\vdots\\
W(1, b_{d-2}) \\
\end{bmatrix}
=
\begin{bmatrix}
\cos \phi_0 & -\sin \phi_1 & 0 & 0 & ... & ... & 0& 0 \\
0 & 0 & \cos \phi_2 & -\sin \phi_3 & ... & ... & 0& 0 \\
\vdots &\vdots &\vdots &\ddots & \vdots& \vdots & \vdots & \vdots\\
0 & 0 & 0  & ... & \cos \phi_{j} & -\sin \phi_{j+1} & ... & 0  \\
\vdots &\vdots &\vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
 0 & 0 & 0 & 0 &... & ... & \cos \phi_{d-2} & -\sin \phi_{d-1} \\
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
\vdots\\
x(j) \\
x(j+1) \\
\vdots\\
x(d-2) \\
x(d-1) \\ 
\end{bmatrix} .
\label{wave_rope2}
\end{align}
To simplify the notation in the matrix representation above, we write $\phi_j$ for $j=0,1,\dots,d-1$, where $\phi_j = f(1+\delta(j))$ if $j$ is odd, and $\phi_j=f(\delta(j))$ otherwise. We let $x$ be the query $q^{m}$.
The function $f(t)$ is defined such that $0 < f(t) \leq 2k\pi$ for $0 \leq t < 1$ and $0 < f(t) \leq 2k\pi$ for $1 \leq t < 2$, where $k$ is the smallest natural number satisfying $m < 2k\pi$.
\paragraph{Do Haar-like wavelets satisfy the necessary conditions of a wavelet?}
Here, $f(t)$ must be a function such that $\psi(t)$ satisfies the conditions of a wavelet.
For Eq. \ref{wave_}, it is evident that it holds for any $f$ satisfying $0 < f(t) \leq 2k\pi$.
Next, we consider the zero-mean property.
As an example, consider $f(t)$ defined as $f(t) = 2k\pi t$ for $0\leq t < 1$ and $ f(t) = 2k\pi(t-1)$ for $1 \leq t < 2$. If we set $\theta = f(t)$, we have:
\begin{equation}
\int_{-\infty}^{\infty} \psi(t) dt = \int_{0}^{2k\pi} \cos\theta d\theta + \int_{0}^{2k\pi} -\sin\theta d\theta = 0 .
\end{equation}
Since this satisfies the zero-mean property, we conclude that there exists an $f(t)$ such that $\psi(t)$ is a wavelet.

Furthermore, we observe that there exists a $\delta(t)$ satisfying $\phi_j (= f(\delta(j))) = \phi_{j+1} (= f(1+\delta(j))) = 2k\pi\delta(t) = m\theta_{\left\lceil \frac{j+1}{2} \right\rceil}$ for $j = 0, 2, \dots, d-2$.
In other words, we can simply choose a function $\delta(j)$ that satisfies $\delta(j) = \frac{m\theta_{\left\lceil \frac{j+1}{2} \right\rceil}}{2k\pi}$ for $j = 0, 2, \dots, d-2$.

\subsection{Haar Wavelet}
\label{sec:appendix_haar}
Here, we explain wavelet transform using the Haar wavelet, which is the simplest wavelet.
The definition of the Haar wavelet is as follows.
\begin{equation}
    \psi(t) =
      \begin{cases}
        1 &0\leq t \textless 1/2, \\
        -1 &1/2 \leq t \textless 1,\\
        0 &otherwise.\\
      \end{cases}\quad
    \phi (t) =
        \begin{cases}
        1 &0\leq t \textless 1,\\
        0 &otherwise.\\
      \end{cases}\label{haar_formula}
\end{equation}
Haar wavelets are defined not only by a wavelet function $\psi$ but also by a scaling function $\phi$.

The method of analyzing signals by performing a discrete wavelet transform using these two functions is called multi-resolution analysis.
When the scale is fixed at 2 and the shift $b \in [0,2,...,d/2]$, the wavelet transform using the wavelet function and scaling function is expressed as follows.
\begin{align}
\scriptsize
\begin{bmatrix}
\psi_{2, 0}(0) & \psi_{2, 0}(1) &\psi_{2, 0}(2) & \psi_{2, 0}(3) & ... & \psi_{2, 0}(T-2) & \psi_{2, 0}(T-1)\\
\phi_{2, 0}(0) & \phi_{2, 0}(1) &\phi_{2, 0}(2) & \phi_{2, 0}(3) & ... & \phi_{2, 0}(T-2) & \phi_{2, 0}(T-1)\\
\psi_{2, 0}(-2) & \psi_{2, 0}(-1) &\psi_{2, 0}(0) & \psi_{2, 0}(1) & ... & \psi_{2, 0}(T-4) & \psi_{2, 0}(T-3)\\
\phi_{2, 0}(-2) & \phi_{2, 0}(-1) &\phi_{2, 0}(0) & \phi_{2, 0}(1) & ... & \phi_{2, 0}(T-4) & \phi_{2, 0}(T-3)\\
\vdots & \vdots &\vdots &\vdots &... & \vdots &\vdots &\\
\psi_{2, 0}(-\frac{d}{2}) & \psi_{2, 0}(-\frac{d}{2}+1) &\psi_{2, 0}(-\frac{d}{2}+2) & \psi_{2, 0}(-\frac{d}{2}+3) & ... & \psi_{2, 0}(0) & \psi_{2, 0}(1)\\
\phi_{2, 0}(-\frac{d}{2}) & \phi_{2, 0}(-\frac{d}{2}+1) &\phi_{2, 0}(-\frac{d}{2}+2) & \phi_{2, 0}(-\frac{d}{2}+3) & ... & \phi_{2, 0}(0) & \phi_{2, 0}(1)\\
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
x(2) \\
\vdots\\
x(T-2) \\
x(T-1) \\
\end{bmatrix}.
\label{wave_haar}
\end{align}

From Eq.(\ref{haar_formula}), $\psi_{2,0}$ and $\phi_{2,0}$ are as follows.
\begin{equation}
    \psi_{2,0} (t)  =
      \begin{cases}
        1/\sqrt{2} &0\leq t \textless 1, \\
        -1/\sqrt{2}&1 \leq t \textless 2,\\
        0 &otherwise.\\
      \end{cases}\quad
    \phi_{2,0} (t) =
        \begin{cases}
        1/\sqrt{2} &0\leq t \textless 2,\\
        0 &otherwise.\\
      \end{cases}\label{haar_formula2}
\end{equation}
Therefore, the Haar wavelet transform is a $2\times 2$ block matrix.
\begin{align}
\scriptsize
\begin{bmatrix}
\psi(2,0) \\
\phi(2,0) \\
\psi(2,2) \\
\phi(2,2) \\
\vdots\\
\psi(2,T-2) \\
\phi(2,T-2) \\
\end{bmatrix}
=
\begin{bmatrix}
1/\sqrt{2} & -1/\sqrt{2} & 0 & 0 & ... & 0 & 0\\
1/\sqrt{2} & 1/\sqrt{2} & 0 & 0 & ... & 0 & 0\\
0 & 0 & 1/\sqrt{2} & -1/\sqrt{2} & ... & 0 & 0 \\
0 & 0 & 1/\sqrt{2} & 1/\sqrt{2} &... & 0 & 0 \\
\vdots & \vdots &\vdots &\vdots &... & \vdots &\vdots &\\
0 & 0 & 0 & 0 & ... & 1/\sqrt{2} & -1/\sqrt{2}\\
0 & 0 & 0 & 0 & ... & 1/\sqrt{2} & 1/\sqrt{2}\\
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
x(2) \\
x(3) \\
\vdots\\
x(T-2) \\
x(T-1) \\
\end{bmatrix}.
\label{wave_haar2}
\end{align}
This matrix is the Haar forward transform using matrix multiplication for a $T$ element signal.
This matches the RoPE matrix with $m\theta=\pi/4$.
\clearpage
\subsection{Isn't RoPE a Fourier transform?}
\label{rope_hypo}
We also hypothesized that this could be equivalent to a Fourier transform.
However, this hypothesis does not hold.
When a signal $x(t)$ that changes over time is Fourier transformed, its spectrum $F(k)$ is obtained.
%This spectrum is a function of frequency, and all information about time is lost.
The process of converting an actual discrete signal $x(t)$ into a spectrum $F(k)$ is as follows.
\begin{align}
F(f) = \sum^{T}_{t=0}x(t)w^{f\cdot t} .
%&=x(0)w^{f\cdot0} + ... + x(T)w^{f\cdot T} 
%&=\langle x(t), w^{k\cdot t}\rangle_{t\in\mathbb{R}}
  \label{dis_fourier1}
\end{align}
The Fourier transform can be expressed as a matrix formula as follows.
\begin{align}
\scriptsize
\begin{bmatrix}
F(0) \\
F(1) \\
F(2) \\
\vdots\\
F(f) \\
\end{bmatrix}
=
\begin{bmatrix}
w^{0\cdot0} & w^{0\cdot1} &w^{0\cdot2} & ... & w^{0\cdot(T-1)}\\
w^{1\cdot0} & w^{1\cdot1} &w^{1\cdot2} & ... & w^{1\cdot(T-1)}\\
w^{2\cdot0} & w^{2\cdot1} &w^{2\cdot2} & ... & w^{2\cdot(T-1)}\\
\vdots &\vdots &\vdots &\ddots & \vdots\\
w^{f\cdot0} & w^{f\cdot1} &w^{f\cdot2} & ... & w^{f\cdot(T-1)}\\ .
\end{bmatrix}
\begin{bmatrix}
x(0) \\
x(1) \\
x(2) \\
\vdots\\
x(T-1) \\ 
\end{bmatrix}.
\label{fourier_rope_for}
\end{align}
Here, $f \in \mathbb{R}$ is the wave number, $T \in \mathbb{R}$ is the number of samples, and $i$ is the imaginary unit. $w = exp(-\frac{2\pi i}{T})$ is called the Twiddle Factor \citep{10.1145/1464291.1464352}, which is a complex number expressed in polar form using Euler's formula $e^{-i\theta}=cos\theta -isin\theta$.
In the complex plane, $w^{f\cdot t}$ represents a point on the unit circle with an argument of the complex number $-\frac{ft2\pi}{T}$.
From this formula, we can see that the Fourier transform calculates the inner product of all signals and sine waves. 
However, in RoPE, the inner product with sine waves is calculated only within each block.

Next, when calculating the attention score with RoPE, does the Fourier transform hold?
Attention scores of the $m$-th query $q^{m}$ and the $n$-th key $k^{n}$ with RoPE are calculated as follows.
\begin{align}
\begin{bmatrix}
R_{m}^{1}(Q_{m}^{1})^{T},
...,
R_{m}^{d/2}(Q_{m}^{d/2})^{T}\\
\end{bmatrix}
\begin{bmatrix}
R_{n}^{1}K_{n}^{1} \\
\vdots\\
R_{n}^{d/2}K_{n}^{d/2}\\
\end{bmatrix}
%&=R_{m}^{1}Q_{m}^{1}R_{n}^{1}K_{n}^{1} + ... + R_{m}^{d/2}Q_{m}^{d/2}R_{n}^{d/2}K_{n}^{d/2} \\
%&=\sum_{i=1}^{d/2} R_{n}^{i}Q_{m}^{i}R_{n}^{i}K_{n}^{i} \\
=\sum_{i=1}^{d/2} (Q_{m}^{i})^{T}R_{n-m}^{i}K_{n}^{i} ,
%&=\langle R^{k\cdot(n-m)}, Q_{m}^{k}K_{n}^{k}\rangle_{k\in\mathbb{R}}
\label{theor}
\end{align}
where $Q^{d/2}_{m}$ is the query divided into every two dimensions, and $R_{m}^{d/2}$ is the rotation matrix. 
\begin{equation*}
Q^{d/2}_{m}
=\\
\begin{bmatrix}
q_{m}^{d-1} \\
q_{m}^{d} \\
\end{bmatrix}
,
K^{d/2}_{n}
=\\
\begin{bmatrix}
k_{n}^{d-1} \\
k_{n}^{d} \\
\end{bmatrix}
,
R_{m}^{d/2}
=\\
\begin{bmatrix}
cosm\theta_{d/2}&-sinm\theta_{d/2}\\
sinm\theta_{d/2}&cosm\theta_{d/2}\\
\end{bmatrix} .
\end{equation*}
Aligning with the Fourier transform, as illustrated in Equation \ref{fourier_rope_for}, requires a process involving the inner product between a frequency tensor of dimensions f×T and a signal tensor of dimensions T×1 (such as the query vector). However, RoPE operates on independent 2×2 blocks, where each block is processed separately. Consequently, RoPE's block-wise operations do not conform to the structure required by the Fourier transform.
Moreover, if we focus solely on the RoPE and key operations in Equation \ref{theor}, they may appear to align with the structure of a Fourier transform. 
However, since the final step involves taking the inner product with the query, the overall operation deviates from the path of becoming a perfect match with the Fourier transform.
Furthermore, the rotation factor represents a rotation in the complex plane, and even if it is expressed as in Eq.(\ref{fourier_rope_for}) using a rotation matrix, it does not completely match a rotation matrix that represents a rotation in the Euclidean plane.

Therefore, RoPE cannot be equated with the Fourier transform. Furthermore, even if it were the same as the Fourier transform, it would be unsuitable for processing non-stationary signals and thus unsuitable for processing natural language, which is a non-stationary flow.

\clearpage
\subsection{\red{Consideration of wavelet transformation based on RoPE}}\label{rope_base_wavelet}
In this paper, we explore the incorporation of wavelet transforms into RoPE (Relative Positional Encoding) following our previous discussion on RPE (Relative Position Encoding). In this regard, integrating wavelet transforms into RoPE presents challenges for controlling computational and memory costs. In Sections 3 and 4, we highlighted the potential effectiveness of employing multiple scales for extrapolation. With this in mind, we present a simplified formula for applying various scales and wavelet transforms to RoPE, which we refer to here as a RoPE-based Wavelet. 
\begin{align}
\small
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
q^{m}_{2} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-1}\\
q^{m}_{d-2}\\
\end{bmatrix}
=
\begin{bmatrix}
\cos m\theta_{1}&
-\sin m\theta_{1}& 0 & 0 & ...&0&0\\
\sin m\theta_{1}&
\cos m\theta_{1}& 0 & 0 & ...&0&0\\
\cos m\theta_{2} & -\sin m\theta_{2} & \cos m\theta_{2}&-\sin m\theta_{2}& ...&0&0\\
\sin m\theta_{2} & \cos m\theta_{2} & \sin m\theta_{2}&\sin m\theta_{2}& ...&0&0\\
\vdots &\vdots &\vdots &\vdots &\ddots &\vdots & \vdots\\
\cos m\theta_{d/2} & -\sin m\theta_{d/2} & \cos m\theta_{d/2}&-\sin m\theta_{d/2} &...&\cos m\theta_{d/2}&-\sin m\theta_{d/2}\\
\sin m\theta_{d/2} & \cos m\theta_{d/2} & \sin m\theta_{d/2}&\sin m\theta_{d/2} &...&\sin m\theta_{d/2}&\cos m\theta_{d/2}\\
\end{bmatrix}
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
q^{m}_{2} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-2}\\
q^{m}_{d-1}\\ 
\end{bmatrix} ,
\label{ropewave_2}
\end{align}
where $q^{m} \in \mathbb{R}^{1\times d}$ is the $m$-th query when the number of dimensions is $d$ and $\theta_{i} = 10000^{-2(i-1)/d}, i \in [1,2,...,d/2]$.
The same process is also performed for the $n$-th key $k^{n} \in \mathbb{R}^{1\times d}$.

Conversely, the method introduced in Section \ref{sec_wave_hyp} is here called RPE-based Wavelet. The key differences between RoPE-based Wavelet and RPE-based Wavelet are as follows: 

\begin{itemize}
    \item  \textbf{Number of Scale Parameters}: In RPE-based Wavelet, the scale parameters can be selected up to the maximum sequence length. However, in RoPE-based Wavelet, the selection is limited to a maximum of \( d \). 
    \item  \textbf{Memory Usage}: RoPE-based Wavelet requires a wavelet matrix that corresponds to the number of absolute positions \( m \). Consequently, the memory usage is significantly higher. Unlike RoPE-based Wavelet, RPE-based Wavelet does not necessitate a wavelet matrix that matches \( m \) values, allowing the use of Tip 2 from Appendix \ref{a3}, which improves memory efficiency. 
    \item \textbf{Absolute and Relative Positions}: When applying wavelet transforms using RoPE-based Wavelet, it is necessary to use absolute positions. In contrast, RPE-based Wavelet can use relative positions, which enhances extrapolation. 
    \item  \textbf{Computational Cost}: Implementing wavelet transforms via RoPE-based Wavelet requires processing both the query and the key, necessitating two calculations. RPE-based Wavelet, as discussed in Section \ref{sec_wave_hyp}, only requires one computation, since it processes only the query. 
\end{itemize}
Additionally, we conducted an experiment with RoPE-based Wavelet. 
\textbf{Unfortunately, we had to halt the learning process because it took over five times longer than anticipated.} Considering the learning costs associated with large-scale language models in recent years, we believe the RoPE-based Wavelet approach is not feasible.
\clearpage
\subsection{Implementation Tips for Wavelet Position Representation} \label{a3}
%\begin{figure*}[t]
%\centering
%\includegraphics[width=14.0cm]{20240917_2.png}
%\caption{Overview of Wavelet-based Relative Positional Representation.
%As in RPE \citep{shaw-etal-2018-self}, our method computes a relative positional representation $p_{m,n}$ to the query $q_{m}$ and the key $k_{}$.
%Instead of learnable embedding in RPE, the position is computed based on the wavelet function.
%Different wavelet functions $\psi_{a,b}$ are used for each dimension of the head $d$. 
%The scale parameter $a$ and the shift parameter $b$ change depending on the dimension of the head $d$.
%}
%\label{intro_w}
%\end{figure*}
%Expressions denoting relative positions necessitate a tensor with $(d, length, length)$, posing significant challenges in terms of computational and memory efficiency during the training of large-scale language models. To enhance both computational and memory efficiency, we introduced the following implementation.
\paragraph{Tip 1}
Similar to RPE\citep{shaw-etal-2018-self}, we used Eq. (\ref{p1}) as
\begin{equation*}
  \alpha_{ij}
  = softmax\Bigl(\frac{q_{i}K^{T}+q_{i}(p_{ij})^{T}}{\sqrt{d_{k}}}\Bigl) .
\end{equation*}
By transforming it in this way, it is possible to reduce the computational complexity to $O(batch\times n\times length^{2} \times d + length^{2}\times d)$,
where $batch$ is the batch size, $n$ is the number of heads, $length$ is the number of tokens, and $d$ is the number of dimensions of each head.
\red{The experiments in Section \ref{sec5} are implemented based on the methodology introduced in this section.}
\paragraph{Tip 2}
When dealing with long contexts of over 4 k with a large model, the memory efficiency of $(d, length, length)$ of the wavelet position becomes a bottleneck. Therefore, we further reduce the memory usage to $(d, length)$ by using \texttt{torch.scatter} to scatter the wavelet position representation to the attention mask.
In the relative position representation in the decoder, only the position information of the token before the current token is required, for example, $0, -1, -2, etc$. Therefore, we pre-compute the information up to $0, -1, -2, ... length$ and reduce the memory usage by using \texttt{torch.scatter} to distribute it.
Specifically, we prepare a $(d, length)$ wavelet tensor and calculate the 2D inner product with the query, which has been transposed to $(length\times batch, d)$.
The tensor after the calculation becomes $(length\times batch, length)$, which is then scattered using \texttt{torch.scatter} so that it becomes a relative position in the attention mask.
This reduces the amount of memory used from $(d, length, length)$ to $(d, length)$, and the calculation can be performed using calculations between 2D tensors.
\red{The experiments in Section \ref{sec_long} are implemented based on the methodology introduced in this section.}
\subsection{Experimental Settings in Short-Context Experiment}\label{a4}
The parameter settings used in the extrapolation experiments were the same as those in the original ALiBi paper.
The dimensionality of the word embedding $d_{model}$ is 1024, the number of heads $N$ is 8, the dimensionality of the heads $d$ is 128, and the number of layers is 16.
The implementation was based on the fairseq \citep{ott2019fairseq}-based code\footnote{\url{https://github.com/ofirpress/attention_with_linear_biases}} provided in a previous work\citep{press2022train}, and all hyperparameters were set to the same values as those in the literature\citep{press2022train}.
The number of training epochs is 205, and the batch size is 9216.
The learning rate was set to 1.0, and the learning process was updated by 1e-7 every 16,000 steps.
\subsection{Experimental Settings in Long-Context Experiment}\label{a6}
The dimensionality of the word embedding $d_{model}$ is 4096, the number of heads $N$ is 32, the dimensionality of the heads $d$ is 128, and the number of layers is 32.
The number of training steps is 30,000, and the batch size is 1.
The learning rate was set to 0.0003.
We used AdamW\citep{loshchilov2018decoupled} as the optimizer, with $(\beta_{1},\beta_{2})=(0.9,0.95)$.
In accordance with previous research \citep{rubin2024retrievalpretrainedtransformerlongrangelanguage, wu2022memorizing, zhang2024soaring4k400kextending}, we then used 100 sampled sequences in the training set for evaluation.
In this experiment, due to the large model size and long sequence length, we report perplexity only for non-overlapping inference using $L_{\rm train}$, since the memory capacity is exceeded.
\clearpage
\subsection{Ricker Wavelet}\label{a5}
Figures \ref{wave_r_1} and \ref{wave_r_2} show the Ricker wavelets with multiple scale $a$. 
\begin{figure}[h]
\centering
\includegraphics[width=12cm]{20241002.png}
\caption{Graph of compared Ricker wavelet functions with $a = [2^0,2^1,2^2,2^3,2^4]$}
\label{wave_r_1}
%\end{figure}
%\begin{figure}[h]
\centering
\includegraphics[width=12cm]{20241002_2.png}
\caption{Graph of compared Ricker wavelet functions with $a = [2^5,2^6,2^7,2^8,2^9]$}
\label{wave_r_2}
\end{figure}
\subsection{Wavelet Type}\label{a5}
Figure \ref{wave_other_type} shows graphs of the wavelets compared in Section \ref{how_effective_other}. 
It can be seen that the simplest is the Haar wavelet, while the most complex is the Morlet wavelet.
\begin{figure}[h]
\centering
\includegraphics[width=13cm]{20240611.png}
\caption{Graph of compared wavelet functions. The case with scale parameter $a=2^4$ and shift parameter $b=0$ is shown.}
\label{wave_other_type}
\end{figure}
\clearpage
\subsection{\red{Example of heat map and text correspondence}}\label{wave_heat_text}
Figure \ref{proposed_attn_aaa} shows the attention map after softmax operation for the proposed method.
First, the notable feature of the proposed method is that it is always able to pay attention to specific tokens.
The words that always receive attention are those that are important in the sentence, such as the '$<$/s$>$' token, the first token, and words that are the subject of the sequence, such as 'he.'
Moreover, as with ALiBi, the proposed method has a different scope of attention for each head.
\begin{figure*}[h]
\centering
\includegraphics[width=14cm]{emnlp202434.drawio1.png}
\caption{Heatmap of attention score $e_{ij}$ after softmax operation for the proposed method. The maximum sequence length  is $L_{max}=512$, and the sequence length at inference is $L=1012$. From left to right, $n=1,2,4$th heads are shown. Scores above 0.01 are mapped in black and the rest in yellow. Words that were always given attention in all heads are shown in red, and words that were frequently given attention only in the $n=2$nd head are shown in blue. Sentences are omitted in the middle because they are long with $1012$ tokens.}
\label{proposed_attn_aaa}
\end{figure*}
\clearpage
\subsection{Can It Handle Tokens with Long-Range Dependencies?}\label{a_attn}
\begin{figure}[h]
\centering
\includegraphics[width=10cm]{20241001_2.png}
\caption{Heatmap of scaled attention scores via softmax normalization in 1-3rd and 5-8th head after softmax operation for ALiBi, RoPE, and our method. For clarity, values of 0.001 or more are mapped to black, while values below that are mapped to yellow. }
\label{attn_all}
\end{figure}
\clearpage
\subsection{\red{Ablation Study of Scale and Shift Parameters}}\label{scale_shift_ab}
In this section, we present the findings from our ablation study focusing on the shift and scale parameters of the Ricker and Gaussian wavelets. As indicated in Table \ref{result1}, both wavelet types demonstrate substantial effectiveness in our method.
 To further evaluate their performance, we explored the contributions of two parameters, i.e., the scale parameter \( a \) and the shift parameter \( b \), while keeping all other settings consistent with those outlined in Section \ref{sec5}. 
 \paragraph{Results}
The results of our experiments are summarized in Table \ref{result1_a10}. 
Both the Ricker and Gaussian wavelets exhibit similar trends regarding the influence of the scale and shift parameters on extrapolation performance. 
Initially, we observed that increasing the scale parameter value \( a \) while holding the shift parameter \( b \) ($\{2^0,2^1,...,2^7\}\times\{0,1,2,...,15\}$, $\{2^1,2^2,...,2^8\}\times\{0,1,2,...,15\}$ and $\{2^2,2^3,...,2^9\}\times\{0,1,2,...,15\}$) constant maintained the performance of extrapolation, albeit with some fluctuations. 
Conversely, when we increased the number of shift parameters while decreasing the number of scale parameters ($\{2^0,2^1,2^2,2^4\}\times\{0,1,2,...,31\}$ and $\{2^0,2^1\}\times\{0,1,2,...,63\}$), there was a noticeable decline in performance. 
These findings underscore the significance of the scale parameters in extrapolation. 
Moreover, we found that increasing the number of scale parameters while decreasing the number of shift parameters led to performance improvements in some instances ($\{2^0,2^1,...,2^{15}\}\times\{0,1,2,...,7\}$ and $\{2^0,2^1,...,2^{31}\}\times\{0,1,2,3\}$). 
However, when the shift parameters were reduced to two or entirely eliminated ($\{2^0,2^1,...,2^{63}\}\times\{0,1\}$ and $\{2^0,2^1,...,2^{127}\}$ ), relying solely on the scale parameters resulted in a deterioration of extrapolation performance. 
Moreover, even when the scale parameter was fixed and only the shift parameter was used ($\{2^7\}\times\{0,1,2,..,127\}$), the extrapolation performance decreased.
This suggests the potential importance of shift parameters as well. 

In conclusion, our analysis highlights the critical roles of both shift and scale parameters in the effectiveness of our wavelet-based method.
\begin{table*}
\caption{Perplexity of validation set in extrapolation experiments using Wikitext-103. Maximum allowable length of sequences in pre-training is $L_{\rm train}=512$.}
    \centering \scriptsize
    \begin{tabular}{lccccccccc} \toprule
& & &  \multicolumn{6}{c}{Sequence Length}\\ \cmidrule(lr){4-10}
 & scale $a$ & shift $b$ & 128 & 256 & 512 & 1012 & 1512 & 2512 \\ \midrule
\multicolumn{10}{c}{Perplexity without Non-overlapping Inference}\\ \midrule
Ricker &  $\{2^0,2^1,...,2^7\}$ & $\{0,1,2,...,15\}$ & \underline{23.64} & \underline{20.82} & \underline{19.19} & 18.23 & 18.00 & 17.99 \\
Ricker &  $\{2^1,2^2...,2^8\}$ & $\{0,1,2,...,15\}$ & 23.77 & 20.89 & 19.25 & 18.23 & 17.97 & 18.02 \\
Ricker &  $\{2^2,2^3...,2^9\}$ & $\{0,1,2,...,15\}$ & 23.92 & 21.03 & 19.40 & 18.41 & 18.14 & 18.07 \\
Ricker &  $\{2^0,2^1,2^2,2^3\}$ & $\{0,1,2,...,31\}$ & 23.96 & 21.13 & 19.55 & 18.87 & 19.40 & 21.73 \\
Ricker &  $\{2^0,2^1\}$ & $\{0,1,2,...,63\}$ & 24.49 & 21.60 & 19.95 & 20.90 & 32.01 & 70.80\\
Ricker &  $\{2^0,2^1...,2^{15}\}$ & $\{0,1,2,...,7\}$ &23.74 & 20.88 & 19.24 & \underline{18.22} & \underline{17.96} & 17.84  \\
Ricker &  $\{2^0,2^1...,2^{31}\}$ & $\{0,1,2,3\}$ & 23.75 & 20.86 & 19.26 & 18.24 & \underline{17.96} & \underline{17.84} \\
Ricker &  $\{2^0,2^1...,2^{63}\}$ & $\{0,1\}$ & 23.75 & 20.88 & 19.30 & 18.31 & 18.04 & 18.02\\
Ricker &  $\{2^0,2^1...,2^{127}\}$ & $\{0\}$ & 23.97 & 21.10 & 19.46 & 18.50 & 18.27 & 18.29 \\
Ricker &  $\{2^7\}$ & $\{0,1,2,...,127\}$ & 24.35 & 21.45 & 19.80 & 20.68 & 20.87 & 21.31 \\
\hdashline
Gaussian &  $\{2^0,2^1,...,2^7\}$ & $\{0,1,2,...,15\}$ & 23.77 & 20.90 & 19.30 & 18.31 & 18.02 & 17.88 \\
Gaussian &  $\{2^1,2^2...,2^8\}$ & $\{0,1,2,...,15\}$ & 23.92 & 21.02 & 19.41 & 18.41 & 18.15 & 18.01 \\
Gaussian &  $\{2^2,2^3...,2^9\}$ & $\{0,1,2,...,15\}$ & 23.98 & 21.09 & 19.46 & 18.43 & 18.13 & 17.93 \\
Gaussian &  $\{2^0,2^1,2^2,2^3\}$ & $\{0,1,2,...,31\}$ & 23.83 & 29.96 & 19.33 & 18.43 & 18.40 & 18.94\\
Gaussian &  $\{2^0,2^1\}$ & $\{0,1,2,...,63\}$ & 24.28 & 21.35 & 19.70 & 18.96 & 19.63 & 23.14\\
Gaussian &  $\{2^0,2^1...,2^{15}\}$ & $\{0,1,2,...,7\}$ & \underline{23.72} & 20.86 & \underline{19.24} & \underline{18.24} & 17.95 & \underline{17.77} \\
Gaussian &  $\{2^0,2^1...,2^{31}\}$ & $\{0,1,2,3\}$ & 23.78 & \underline{20.92} & 19.29 & 18.30 & \underline{18.01} & 17.85 \\
Gaussian &  $\{2^0,2^1...,2^{63}\}$ & $\{0,1\}$ & 23.86 & 20.98 & 19.37 & 18.46 & 18.20 & 18.10 \\
Gaussian &  $\{2^0,2^1...,2^{127}\}$ & $\{0\}$ & 24.21 & 21.31 & 19.68 & 18.71 & 18.45 & 18.45 \\
Gaussian &  $\{2^7\}$ & $\{0,1,2,...,127\}$ & 24.48 & 21.62 & 20.05 & 19.53 & 22.63 & 35.23 \\
\hdashline
Haar &  - & - & 24.98 & 22.07 & 20.49 & 51.61 & 116.87 & 299.26 \\
Haar &  $\{2^0,2^1,...,2^7\}$ & $\{0,1,2,...,15\}$ & 23.73 & 20.89 & 19.27 & 18.34 & 18.11 & 18.17 \\
Morlet  &  $\{2^0,2^1,...,2^7\}$ & $\{0,1,2,...,15\}$ & 24.15 & 21.28 & 19.65 & 19.02 & 20.46 & 26.56 \\
\bottomrule
    \end{tabular}
    \label{result1_a10}
\end{table*}
%https://jp.mathworks.com/help/wavelet/ug/wavelet-families-additional-discussion.html#f8-40111
\clearpage
\subsection{\red{Ablation Study of Wavelet types}}\label{wave_type_abration}
In this section, we also explored a variety of wavelet types beyond those previously discussed. 
In Section \ref{how_effective_other}, our focus was primarily on wavelets that could be computed directly from mathematical formulas. 
However, in this section, we expand our inquiry to include wavelets with varying numbers of vanishing moments as well as discrete wavelet transformations. 
Additionally, drawing from previous research \citep{Wang2020Encoding}, we considered the necessity for a distinct approach when incorporating complex numbers into positional encoding. 
Consequently, our study did not encompass wavelets that incorporate complex numbers. 
\paragraph{Wavelet types}
The specific wavelets under consideration in our investigation are outlined as follows:
\begin{itemize}
    \item Daubechies (\texttt{db}) \citep{daubechies1992ten} - Compactly supported orthonormal wavelets
    \item Symlets (\texttt{sym}) - Wavelets with minimum asymmetry
    \item Coiflets (\texttt{coif}) - The scaling and wavelet functions have the same number of vanishing moments
    \item Meyer (\texttt{dmey}) - Wavelets defined in the frequency domain
    \item Biorthogonal Spline (\texttt{bior}) - Two wavelets are used: one for decomposition, and the other for reconstruction
    \item Reverse biorthogonal Spline (\texttt{rbio})
\end{itemize}
In addition, the graphs of these wavelets are shown in Figures \ref{wave_20241122_1} and \ref{wave_20241122_2}.
As the number of vanishing moments increases, the wave oscillation becomes larger. 
Therefore, we also conducted a survey by vanishing point moment.
The name of a wavelet is derived from the number of vanishing moments. 
For example, \texttt{db6} is a Daubechies wavelet with 6 vanishing moments, and \texttt{sym3} is a Symlet wavelet with 3 vanishing moments. 
In the case of Coiflet wavelets, \texttt{coif3} is a Coiflet wavelet with 6 vanishing moments. 
The names of \texttt{bior} and \texttt{rbio} wavelets are derived from the number of vanishing moments possessed by the decomposition and reconstruction wavelets, respectively. 
For example, \texttt{bior3.5} is Biorthogonal wavelet that has 3 vanishing moments for the decomposition wavelet and 5 vanishing moments for the reconstruction wavelet.
Biorthogonal wavelets and Reverse-Biorthogonal wavelets can calculate the approximate values of decomposition wavelets and reconstruction wavelets, but in this case, we only used the values of decomposition wavelets.
%decomposition and reconstruction
\paragraph{Experimental Settings}
We used Pywavelet \citep{Lee2019} \footnote{\url{https://pywavelets.readthedocs.io/en/latest/index.html}} to calculate the approximate values of these wavelets.
In addition, in this experiment, we calculated the approximate values by specifying 8 levels of 
$\{1,2,...,8\}$ instead of the 8-pattern scale parameters $\{2^0,2^1,...,2^7\}$.
We used the shift parameter $\{0,1,2,...,15\}$.
The other experimental settings are the same as those in Section \ref{sec5}.
\paragraph{Results}
The experimental results are summarized in Table \ref{result_type_biop}. 
Overall, the performance observed was suboptimal for extrapolation. 
However, it is important to note that since the parameters were fixed at levels $\{1,2,...,8\}$, we believe that performance may be enhanced with adjustments to these levels. 
Notably, the rbio1.1 wavelet demonstrated promising extrapolation capabilities, suggesting significant potential for future improvements. 
In contrast, the coif and dmey wavelets exhibited limited performance, even with shorter sequences, indicating their potential unsuitability for position encoding tasks. 
Conversely, while the extrapolation performance ($> 512$) of other wavelets was generally low, their interpolation performance ($\leq 512$) remained consistently stable, highlighting another avenue for enhancement. Furthermore, the performance of the db, bior, and rbio wavelets showed a positive correlation with an increasing number of vanishing points. This finding underscores the importance of vanishing points as a critical factor influencing performance. In conclusion, our analysis indicates that both the shape of the wavelet and the number of vanishing points play significant roles in determining extrapolation performance. Future work should explore these relationships further to identify optimal configurations for improved performance outcomes.

\begin{table*}
\caption{Perplexity without Non-overlapping Inference. We evaluated the validation set in extrapolation experiments using Wikitext-103. The maximum allowable length of sequences in pre-training is $L_{\rm train}=512$.}
    \centering \scriptsize
    \begin{tabular}{lccccccc} \toprule
& & \multicolumn{6}{c}{Sequence Length}\\ \cmidrule(lr){2-8}
Wavelet type &  128 & 256 & 512 & 1012 & 1512 & 2512 \\ \midrule
\multicolumn{8}{c}{Continuous Wavelet Families}\\ \midrule
Ricker &  \textbf{23.64} & \textbf{20.82} & \textbf{19.19} &  \textbf{18.23} & \textbf{18.00} & \textbf{17.99} \\
Gaussian &    23.77 & 20.90 & 19.30 & 18.31 & 18.02 & \underline{17.88} \\
Morlet  & 24.15 & 21.28 & 19.65 & 19.02 & 20.46 & 26.56 \\\midrule
\multicolumn{8}{c}{Discrete Wavelet Families}\\ \midrule
Haar  &   23.73 & 20.89 & 19.27 & 18.34 & 18.11 & 18.17 \\
%Gaussian wavelets (gaus) \\
%Mexican hat wavelet (mexh) \\
%Morlet wavelet (morl) \\
%cgau, shan, fbsp, cmorは複素数が入ってるのでテストしない
%Complex Gaussian (cgau) \\
%Shannon (shan) \\
%Frequency B-Spline (fbsp) \\
%Complex Morlet (cmor)\\
\hdashline
\texttt{db2} &  25.22 & 22.26 & 20.64 & 30.30 & 60.27 & 130.93\\
\texttt{db4} &  25.22 & 22.47 & 21.37 & 41.78 & 51.75 & 56.18\\
\texttt{db8} &  25.19 & 22.48 & 21.58 & 26.90 & 31.55 & 39.75\\
\texttt{db16}  & 25.23 & 22.43 & 21.24 & 21.15 & 22.16 & 46.65\\
\texttt{db32}  & 25.12 & 22.35 & 21.14 & 21.20 & 22.40 & 38.00\\
\texttt{sym2} &  25.11 & 22.21 & 20.68 & 31.25 & 61.00 & 126.32 \\
\texttt{sym4} &  25.27 & 22.56 & 21.98 & 24.70 & 26.81 & 42.81 \\
\texttt{sym8} &  29.27 & 26.13 & 24.63 & 23.97 & 31.47 & 92.36\\
%Symlets (sym16) & 16& & \\
\texttt{coif1}  &  31.24 & 28.00 & 26.24 & 64.62 & 71.06 & 97.60 \\
\texttt{coif2}  &  25.24 & 22.47 & 21.39 & 27.74 & 27.39 & 44.26 \\
\texttt{coif4}  &  49.91 & 45.15 & 42.42 & 41.07 & 56.08 & 110.27  \\
\texttt{coif8}  &  25.15 & 22.39 & 21.26 & 21.31 & 22.26 & 35.73 \\
\texttt{coif16}  &  126.38 & 117.88 & 113.42 & 132.14 & 166.77 & 230.95  \\
\texttt{dmey}  &  30.38 & 27.12 & 25.45 & 25.88 & 46.35 & 131.48 \\
%\texttt{bior1.1}  & 427.75 & 412.16 &407.83 & 411.89 & 419.22 & 435.29 \\
\texttt{bior1.3}  & 26.27 & 23.36 & 23.69 & 23.38 & 30.71 & 88.66 \\
\texttt{bior2.2} &  25.28 & 22.51 & 21.59 & 29.71 & 29.43 & 50.25 \\
\texttt{bior2.6} &  25.29 & 22.70 & 21.60 & 22.15 & 22.71 & 40.61  \\
\texttt{bior3.1} &  26.92 & 24.02 & 22.38 & 59.30 & 113.81 & 205.54 \\
\texttt{bior3.5} &  25.17 & 22.49 & 21.65 & 27.41 & 27.19 & 53.99\\
\texttt{bior3.9} &  25.24 & 22.48 & 21.51 & 21.89 & 23.86 & 50.14 \\
\texttt{bior4.4} &  25.52 & 22.72 & 21.64 & 21.67 & 24.42 & 51.46 \\
\texttt{bior5.5} &  25.21 & 22.55 & 21.72 & 23.43 & 24.68  & 36.30 \\
\texttt{bior6.8} & 25.14 & 22.39 & 21.21 & 21.10 & 22.31 & 46.97\\
\texttt{rbio1.1}  &  24.26 & 21.34 & 19.69 & 18.79 & 18.63 & 18.98 \\
\texttt{rbio1.3}  &  25.28 & 22.50 & 21.39 & 52.06 & 47.78 & 59.94\\
\texttt{rbio2.2}  &  25.92 & 23.08 & 21.98 & 68.57 & 86.12 & 93.90 \\
\texttt{rbio2.6}  &  25.29 & 22.68 & 21.60 & 24.54 & 24.47 & 44.57 \\
\bottomrule
    \end{tabular}
    \label{result_type_biop}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=14cm]{20241122_1.png}
\caption{Graph of compared wavelets with level=10. Pywavelet \citep{Lee2019} was used to calculate wavelets.}
\label{wave_20241122_1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=14cm]{20241122_2.png}
\caption{Graph of compared wavelets with level=10. Pywavelet \citep{Lee2019} was used to calculate wavelets.}
\label{wave_20241122_2}
\end{figure}
\clearpage
\subsection{\red{Evaluation on LongBench}}\label{longbench}
The models pre-trained in Section \ref{sec_long} were evaluated on LongBench \citep{bai-etal-2024-longbench}. 
This evaluation was conducted using a dataset that contained relatively long sentences. 
Furthermore, the multi-document QA task and single-document QA task were evaluated on all datasets. 
Since pre-training was conducted using an English dataset, evaluation was conducted using only the English dataset.

The results are shown in Figure \ref{longbench_result}. 
In some datasets, the performance of the model that adopted RoPE was good (Qasper, MuSiQue, and QMSum). 
In NarrativeQA, the two models attained almost the same score.
However, in the remaining tasks, the proposed method was more effective. 
Note that this is an evaluation of a model that was pre-trained on a small dataset (redpajama-1B). 
As future work, it will be necessary to pre-train the model with a larger dataset and conduct evaluations with other models that are effective for long sentences, such as LongRangeArena \citep{tay2021long}.
\begin{figure}[h]
\centering
\includegraphics[width=12cm]{20241126.png}
\caption{Evaluation results using LongBench\citep{bai-etal-2024-longbench}. We evaluated the model pretrained in Section \ref{sec_long}. The scores of the difference between the model using the proposed method, which uses wavelet-based position representation, and the model using RoPE are shown. The tasks were evaluated using the dataset, which contains relatively long sentences.}
\label{longbench_result}
\end{figure}
\begin{table*}[h]
\caption{\label{longbench_data}Overview of the dataset statistics in LongBench \citep{bai-etal-2024-longbench}. Avg len (average length) is computed using the number of words in the English. }
\begin{center}
\scriptsize
\begin{tabular}{lccc}
\toprule
Dataset & Avg len & Metric & Samples\\
\midrule
NarrativeQA & 18,409 & F1 & 200\\ 
Qasper & 3,619 & F1 & 200\\
MultiFieldQA-en & 4,559 & F1 & 150\\
HotpotQA & 9,151 & F1 & 200 \\
2WikiMQA & 4,887 & F1 & 200 \\
MuSiQue & 11,214 & F1 & 200\\
TriviaQA & 8,209 & F1 & 200\\
SAMSum & 6258 & Rouge-L & 200\\
QMSum & 10614 & Rouge-L & 200\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}


\end{document}

\subsection{Window Size Variability In ALiBi} \label{alibi_aa}
\paragraph{Experimental Settings} 
We performed a comparative evaluation using a Transformer-based language model \citep{baevski2018adaptive} and the WikiText-103 dataset \citep{merity2017pointer}.
The number of heads $N$ is 8, and the slope of ALiBi is $\frac{1}{2}, \frac{1}{4},\frac{1}{8},\frac{1}{16},\frac{1}{32},\frac{1}{64},\frac{1}{128},\frac{1}{256}$.
The maximum allowable length of sequences was set to $L_{\rm train}=512$.
The implementation and all hyperparameters were set to the same values as those in the literature \citep{press2022train}.
%This setting is the same as in Section \ref{sec6.1}. 
For details, see Section \ref{sec6.1} or Appendix \ref{a4}.
In extrapolation, sequences are often divided, but in this section the sequences are not divided.


\subsection{Unified View}
The results of comparing RoPE and our method are shown in Table \ref{overview_wave}. In RoPE, the dimension was transformed to a $d/2$ shift and a unified scale parameter. 
Our method transforms from position to a shift of $d/N$ and a scale of $N$. 
Compared to RoPE, the pattern of the shift parameters in our method is smaller. 
On the contrary, the number of scale parameters increases. 
Furthermore, the wavelet function changes from a simple Haar-like wavelet to a more complex Ricker wavelet.
\begin{table*}[h]
\caption{\label{overview_wave} Unified view of RoPE and proposed method}
\begin{center}\scriptsize
\begin{tabular}{l|c|c}
\hline \hline
Wavelet Transform & Signal $\to$ Shift, Scale & Wavelet \\ \hline
RoPE & $d$ $\to$ $\frac{d}{2}$ Shift, 2 Scale & Haar-like\\
Wavelet & $pos$ $\to$ $\frac{d}{N}$ Shift, $N$ Scale & Ricker\\
\hline \hline
\end{tabular}
\end{center}
\end{table*}


\subsection{Wavelet Position Representation based on the form of RoPE}\label{wave_rope}

\begin{align}
\small
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
q^{m}_{2} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-1}\\
q^{m}_{d-2}\\
\end{bmatrix}
=
\begin{bmatrix}
\cos m\theta_{1}&
-\sin m\theta_{1}& 0 & 0 & ...&0&0\\
\sin m\theta_{1}&
\cos m\theta_{1}& 0 & 0 & ...&0&0\\
\cos m\theta_{1} & -\sin m\theta_{1} & \cos m\theta_{2}&-\sin m\theta_{2}& ...&0&0\\
\sin m\theta_{1} & \cos m\theta_{1} & \sin m\theta_{2}&\sin m\theta_{2}& ...&0&0\\
\vdots &\vdots &\vdots &\vdots &\ddots &\vdots & \vdots\\
\cos m\theta_{1} & -\sin m\theta_{1} & \cos m\theta_{2}&-\sin m\theta_{2} &...&\cos m\theta_{d/2}&-\sin m\theta_{d/2}\\
\sin m\theta_{1} & \cos m\theta_{1} & \sin m\theta_{2}&\sin m\theta_{2} &...&\sin m\theta_{d/2}&\cos m\theta_{d/2}\\
\end{bmatrix}
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
q^{m}_{2} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-2}\\
q^{m}_{d-1}\\ 
\end{bmatrix}.
\label{ropewave_1}
\end{align}

\begin{align}
\small
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
q^{m}_{2} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-1}\\
q^{m}_{d-2}\\
\end{bmatrix}
=
\begin{bmatrix}
\cos m\theta_{1}&
-\sin m\theta_{1}& 0 & 0 & ...&0&0\\
\sin m\theta_{1}&
\cos m\theta_{1}& 0 & 0 & ...&0&0\\
\cos m\theta_{2} & -\sin m\theta_{2} & \cos m\theta_{2}&-\sin m\theta_{2}& ...&0&0\\
\sin m\theta_{2} & \cos m\theta_{2} & \sin m\theta_{2}&\sin m\theta_{2}& ...&0&0\\
\vdots &\vdots &\vdots &\vdots &\ddots &\vdots & \vdots\\
\cos m\theta_{d/2} & -\sin m\theta_{d/2} & \cos m\theta_{d/2}&-\sin m\theta_{d/2} &...&\cos m\theta_{d/2}&-\sin m\theta_{d/2}\\
\sin m\theta_{d/2} & \cos m\theta_{d/2} & \sin m\theta_{d/2}&\sin m\theta_{d/2} &...&\sin m\theta_{d/2}&\cos m\theta_{d/2}\\
\end{bmatrix}
\begin{bmatrix}
q^{m}_{0} \\
q^{m}_{1} \\
q^{m}_{2} \\
q^{m}_{3} \\
\vdots\\
q^{m}_{d-2}\\
q^{m}_{d-1}\\ 
\end{bmatrix}.
\label{ropewave_2}
\end{align}


\begin{table*}[t]
\caption{\label{overview_wave_aaa} Unified view of positional encoding method}
\begin{center}\small
\begin{tabular}{l|c|ccccc}
\hline \hline
PE & pos & Wavelet Transform & Window Size & Receptive Field & Extrapolation\\
\hline
RoPE & abs & Yes, $a=2$ &  $L_{\rm train}$, constant & unlimited \byoTwemoji{head; eyes normal; mouth laughing} & No \byoTwemoji{head; expression thinking; mouth thinking; hands thinking}\\
ALiBi & rel & No &  multiple & limited \byoTwemoji{head; expression thinking; mouth thinking; hands thinking} & Yes \byoTwemoji{head; eyes normal; mouth laughing} \\
RPE & rel & No &  32 or 128 constant & - & No \byoTwemoji{head; expression thinking; mouth thinking; hands thinking}\\ 
Wavelet (Ours) & rel & Yes, $a=[2^0,2^1,...2^{s}]$ &  multiple & unlimited \byoTwemoji{head; eyes normal; mouth laughing} & Yes \byoTwemoji{head; eyes normal; mouth laughing} \\
\hline \hline
\end{tabular}
\end{center}
\end{table*}