\section{Related Works}
Inspired by the success of large-scale model in NLP**Vinyals, "Sequence to Sequence Learning with Neural Networks"**, more and more recent studies have focused on scaling-up recommendation model capacity for better performance**Rendle, "Factorization Machines with Optimized Kernels for Sparsity-aware Recommendation Systems"**. 
One fundamental challenge to employ them in industrial-scale applications is the restricted training and inference budget for serving models. 
To overcome that, multiple prior studies explored the adaption of knowledge distillation (KD)**Kim, "Learning to Distill a Neural Network"**__**Wang, "Distilling Knowledge from Teachers for Compact Model Computation"** in recommendation problems, where a large teacher model continuously supervises a compact student model by co-training and finally only the student model is used for serving**He, "Co-Distillation: A Simple Approach to Improve the Performance of Deep Neural Networks with Fewer Parameters"**. 
One limitation of those studies is that they primarily focused on static environments, while in industrial applications, large-volume data continuously arrive. **Chen, "A Continual Learning Framework for Recommendation Systems"** proposed to handle that by a continual learning framework. 
Another limitation is that those studies follow co-training based distillation. 
It will increase the training cost of the serving model, as well as the risk of model staleness due to iteration delay, implying performance loss when new incoming data has distribution shifting. **Ren, "External Distillation for Recommendation Systems"** proposed to handle that by external distillation where teacher training happens separately from supervising students.

Among existing studies, ours are closest to **Wang, "Improving the Performance of Deep Neural Networks with Fewer Parameters"** and **He, "Co-Distillation: A Simple Approach to Improve the Performance of Deep Neural Networks with Fewer Parameters"**.
Compared to**Chen, "A Continual Learning Framework for Recommendation Systems"**, our study has the following salient differences: (1) Different settings for the teacher model. To amortize building and maintaining resources, the teacher model in ExFM is an FM that uses an aggregation of student models' feature sets. As a result, the teacher itself often has feature gap and dimension mismatch when compared to an individual VM, so ExFM does not generate student models from the teacher model like**Kim, "Learning to Distill a Neural Network"**. (2) Teacher update does not depend on students. **Wang, "Distilling Knowledge from Teachers for Compact Model Computation"** instead couples the teacher modelâ€™s update with students' by replay learning. Such dependency will increase overhead of teacher update and bring in staleness, as illustrated by Figure~\ref{fig:staleness}, implying a risk of huge performance loss. (3) Consideration of data distribution shifting in an industrial streaming setting. We develop Student Adapter to handle that and provide theoretical guarantees. 
Compared to**Ren, "External Distillation for Recommendation Systems"**, our differences and contributions are as follows. 
Its auxiliary distillation looks similar to our AH, but our AH is an isolated task arch that consumes supervision from FM in a dedicated fashion, not simultaneously consuming true labels like in**He, "Co-Distillation: A Simple Approach to Improve the Performance of Deep Neural Networks with Fewer Parameters"**. 
**Chen, "A Continual Learning Framework for Recommendation Systems"** lacks theoretical proof to justify its proposal and does not provide a solution to mitigate the distribution gap between FM and VMs due to data distribution shifting. 
Instead, we develop and prove Student Adapter can achieve the goal. 
Moreover, **Wang, "Distilling Knowledge from Teachers for Compact Model Computation"** does not contain sufficient details and lacks comprehensive benchmark experiments on external datasets.