@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{anil2022factory,
  title={On the factory floor: ML engineering for industrial-scale ads recommendation models},
  author={Anil, Rohan and Gadanho, Sandra and Huang, Da and Jacob, Nijith and Li, Zhuoshu and Lin, Dong and Phillips, Todd and Pop, Cristina and Regan, Kevin and Shamir, Gil I and others},
  journal={arXiv preprint arXiv:2209.05310},
  year={2022}
}

@inproceedings{chen2023unbiased,
  title={Unbiased knowledge distillation for recommendation},
  author={Chen, Gang and Chen, Jiawei and Feng, Fuli and Zhou, Sheng and He, Xiangnan},
  booktitle={WSDM},
  pages={976--984},
  year={2023}
}

@inproceedings{fang2024scaling,
  title={Scaling laws for dense retrieval},
  author={Fang, Yan and Zhan, Jingtao and Ai, Qingyao and Mao, Jiaxin and Su, Weihang and Chen, Jia and Liu, Yiqun},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1339--1349},
  year={2024}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{kang2021item,
  title={Item-side ranking regularized distillation for recommender system},
  author={Kang, SeongKu and Hwang, Junyoung and Kweon, Wonbin and Yu, Hwanjo},
  journal={Information Sciences},
  volume={580},
  pages={15--34},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{kang2023distillation,
  title={Distillation from heterogeneous models for top-k recommendation},
  author={Kang, SeongKu and Kweon, Wonbin and Lee, Dongha and Lian, Jianxun and Xie, Xing and Yu, Hwanjo},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={801--811},
  year={2023}
}

@article{kang2024unbiased,
  title={Unbiased, Effective, and Efficient Distillation from Heterogeneous Models for Recommender Systems},
  author={Kang, SeongKu and Kweon, Wonbin and Lee, Dongha and Lian, Jianxun and Xie, Xing and Yu, Hwanjo},
  journal={ACM Transactions on Recommender Systems},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{khani2024bridging,
  title={Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems},
  author={Khani, Nikhil and Wei, Li and Nath, Aniruddh and Andrews, Shawn and Yang, Shuo and Liu, Yang and Abbo, Pendo and Kula, Maciej and Kahn, Jarrod and Zhao, Zhe and others},
  booktitle={Proceedings of the 18th ACM Conference on Recommender Systems},
  pages={758--761},
  year={2024}
}

@inproceedings{kweon2021bidirectional,
  title={Bidirectional distillation for top-K recommender system},
  author={Kweon, Wonbin and Kang, SeongKu and Yu, Hwanjo},
  booktitle={Proceedings of the Web Conference 2021},
  pages={3861--3871},
  year={2021}
}

@inproceedings{lee2024continual,
  title={Continual Collaborative Distillation for Recommender System},
  author={Lee, Gyuseok and Kang, SeongKu and Kweon, Wonbin and Yu, Hwanjo},
  booktitle={SIGKDD},
  pages={1495--1505},
  year={2024}
}

@inproceedings{liu2022position,
  title={Position awareness modeling with knowledge distillation for CTR prediction},
  author={Liu, Congcong and Li, Yuejiang and Zhu, Jian and Teng, Fei and Zhao, Xiwei and Peng, Changping and Lin, Zhangang and Shao, Jingping},
  booktitle={Proceedings of the 16th ACM Conference on Recommender Systems},
  pages={562--566},
  year={2022}
}

@inproceedings{pan2024ads,
  title={Ads recommendation in a collapsed and entangled world},
  author={Pan, Junwei and Xue, Wei and Wang, Ximei and Yu, Haibin and Liu, Xun and Quan, Shijie and Qiu, Xueming and Liu, Dapeng and Xiao, Lei and Jiang, Jie},
  booktitle={SIGKDD},
  pages={5566--5577},
  year={2024}
}

@inproceedings{shin2023scaling,
  title={Scaling law for recommendation models: Towards general-purpose user representations},
  author={Shin, Kyuyong and Kwak, Hanock and Kim, Su Young and Ramstr{\"o}m, Max Nihl{\'e}n and Jeong, Jisu and Ha, Jung-Woo and Kim, Kyung-Min},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={4},
  pages={4596--4604},
  year={2023}
}

@inproceedings{tang2018ranking,
  title={Ranking distillation: Learning compact ranking models with high performance for recommender system},
  author={Tang, Jiaxi and Wang, Ke},
  booktitle={SIGKDD},
  pages={2289--2298},
  year={2018}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{zhang2024scaling,
  title={Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta},
  author={Zhang, Wei and Li, Dai and Liang, Chen and Zhou, Fang and Zhang, Zhongke and Wang, Xuewei and Li, Ru and Zhou, Yi and Huang, Yaning and Liang, Dong and others},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={47--55},
  year={2024}
}

@inproceedings{zhu2020ensembled,
  title={Ensembled CTR prediction via knowledge distillation},
  author={Zhu, Jieming and Liu, Jinyang and Li, Weiqi and Lai, Jincai and He, Xiuqiang and Chen, Liang and Zheng, Zibin},
  booktitle={CIKM},
  pages={2941--2958},
  year={2020}
}

