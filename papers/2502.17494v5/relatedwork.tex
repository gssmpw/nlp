\section{Related Works}
Inspired by the success of large-scale model in NLP~\cite{achiam2023gpt,touvron2023llama}, more and more recent studies have focused on scaling-up recommendation model capacity for better performance~\cite{zhang2024scaling,zhang2024wukong,fang2024scaling,pan2024ads,shin2023scaling,anil2022factory}. 
One fundamental challenge to employ them in industrial-scale applications is the restricted training and inference budget for serving models. 
To overcome that, multiple prior studies explored the adaption of knowledge distillation (KD)~\cite{hinton2015distilling} in recommendation problems, where a large teacher model continuously supervises a compact student model by co-training and finally only the student model is used for serving~\cite{kang2024unbiased,kang2023distillation,chen2023unbiased,liu2022position,kang2021item,kweon2021bidirectional,zhu2020ensembled,tang2018ranking}. 
One limitation of those studies is that they primarily focused on static environments, while in industrial applications, large-volume data continuously arrive. \cite{lee2024continual} proposed to handle that by a continual learning framework. 
Another limitation is that those studies follow co-training based distillation. 
It will increase the training cost of the serving model, as well as the risk of model staleness due to iteration delay, implying performance loss when new incoming data has distribution shifting. \cite{khani2024bridging} proposed to handle that by external distillation where teacher training happens separately from supervising students.

Among existing studies, ours are closest to \cite{lee2024continual} and \cite{khani2024bridging}.
Compared to~\cite{lee2024continual}, our study has the following salient differences: (1) Different settings for the teacher model. To amortize building and maintaining resources, the teacher model in ExFM is an FM that uses an aggregation of student models' feature sets. As a result, the teacher itself often has feature gap and dimension mismatch when compared to an individual VM, so ExFM does not generate student models from the teacher model like~\cite{lee2024continual}. (2) Teacher update does not depend on students. \cite{lee2024continual} instead couples the teacher modelâ€™s update with students' by replay learning. Such dependency will increase overhead of teacher update and bring in staleness, as illustrated by Figure~\ref{fig:staleness}, implying a risk of huge performance loss. (3) Consideration of data distribution shifting in an industrial streaming setting. We develop Student Adapter to handle that and provide theoretical guarantees. 
Compared to~\cite{khani2024bridging}, our differences and contributions are as follows. 
Its auxiliary distillation looks similar to our AH, but our AH is an isolated task arch that consumes supervision from FM in a dedicated fashion, not simultaneously consuming true labels like in~\cite{khani2024bridging}. 
\cite{khani2024bridging} lacks theoretical proof to justify its proposal and does not provide a solution to mitigate the distribution gap between FM and VMs due to data distribution shifting. 
Instead, we develop and prove Student Adapter can achieve the goal. 
Moreover, \cite{khani2024bridging} does not contain sufficient details and lacks comprehensive benchmark experiments on external datasets.