
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{xurl}

\usepackage{tikz}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usetikzlibrary{tikzmark}

\title{End-to-end Training for Text-to-Image Synthesis using Dual-Text Embeddings}
% \title{Learning Dual Text Embeddings by Synthesising Images Conditioned on Text}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Yeruru Asrar Ahmed \email asrar@cse.iitm.ac.in \\
      \addr Department of Computer Science and Engineering\\
      Indian Institute of Technology Madras
      \AND
      \name Anurag Mittal \email amittal@cse.iitm.ac.in \\
      \addr Department of Computer Science and Engineering\\
      Indian Institute of Technology Madras
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Text-to-Image (T2I) synthesis is a challenging task that requires modeling complex interactions between two modalities (\textit{, i.e.}, text and image). A common framework adopted in recent state-of-the-art approaches to achieving such multimodal interactions is to bootstrap the learning process with pre-trained image-aligned text embeddings trained using contrastive loss. Furthermore, these embeddings are typically trained generically and reused across various synthesis models. In contrast, we explore an approach to learning text embeddings specifically tailored to the T2I synthesis network, trained in an end-to-end fashion. Further, we combine generative and contrastive training and use two embeddings, one optimized to enhance the photo-realism of the generated images, and the other seeking to capture text-to-image alignment.  A comprehensive set of experiments on three text-to-image benchmark datasets (Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate embeddings gives better results than using a shared one and that such an approach performs favourably in comparison with methods that use text representations from a pre-trained text encoder trained using a discriminative approach.  Finally, we demonstrate that such learned embeddings can be used in other contexts as well, such as text-to-image manipulation.

\end{abstract}

% \vspace{-1cm}
\section{Introduction}
\label{sec:intro}
% \vspace{-0.2cm}

Visualizing images for any textual statement is elemental to human understanding of the world. Intelligent systems' ability to generate images from the text for human understanding has a wide range of applications such as information sharing, computer-aided design, text-to-image search, and photo editing. Image synthesis from text is a challenging task due to complex interaction and ambiguous association of the text modality with the image modality. For instance, multiple textual descriptions can describe the same image and vice versa. In addition, finer details of the images may not always be well captured in textual descriptions. In this domain, Generative Adversarial Networks (GANs) \citep{GAN_2014} were the go-to method to generate realistic images. Conditional GANs \citep{condtionalgan,acgan,cgans_projections} allow us to generate real images semantically coherent with text \citep{reed2016generative,tacgan,GAWWN} by conditioning the generation process on global sentence embeddings.

Though GANs have been shown to generate meaningful images, naively generating high-resolution images from text leads to subpar visual results and training instability due to the complex nature of the task. A set of methods attempts to solve this problem by \textit{ advancing the visual generation} part of the model. For instance, StackGAN \citep{stack_gan} employs a \textit{hierarchical stage-wise training} of GANs from a low-resolution to a high resolution and conditions the generator at every stage by images generated from the previous stage generator. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/overall_picture.drawio.pdf}
    % \vspace{-0.5cm}
    \caption{ (a) Text and image are projected into a shared embedding space to enhance mutual information capturing discriminative features (Discriminative Embeddings), (b) Word embeddings are trained by generating images capturing semantic details (Generative Embeddings) and (c) Our Dual Text Embedding approach combining both generative and discriminative embeddings.}
    \label{fig:overall_picture}
    % \vspace{-0.8cm}
\end{figure*}

Another set of methods \citep{AttnGAN,DMGAN,CPGAN} addresses high-quality image generation from a text by improving the compatibility between text and image modalities. It is achieved by semantically aligning the visual features in image sub-regions with the pre-trained word embeddings through attention. These pre-trained embeddings are trained by projecting text and image features into a shared embedding space and maximising mutual information between text and image features using contrastive loss \citep{AttnGAN,clip}. Furthermore, these embeddings are typically trained generically and reused across various synthesis models \citep{AttnGAN,CPGAN,DF_GAN_CVPR,SSA-GAN,DALL-E,Dalle_2,make_a_scene,stable_diffusion}.  This work explores a new direction for learning text representations within the Text-to-Image (T2I) framework through unified end-to-end training, as illustrated in Figure \ref{fig:overall_picture}, to enhance compatibility between image and text modalities. Additionally, we combine generative and contrastive training while utilizing two distinct embeddings: one optimized for enhancing the photorealism of generated images and the other focused on capturing accurate text-to-image alignment as shown in Figure \ref{fig:overall_picture}. We evaluate the proposed approach on three datasets: 1) Oxford-102 \citep{flower_dataset}, 2) Caltech-UCSD Birds 200 (CUB) \citep{CUB_dataset}, and 3) MS-COCO \citep{mscoco}. We use three metrics to assess the generated images: \textit{Inception Score (IS)} \citep{IS_score}, \textit{Fréchet Inception Distance (FID)} \citep{FID_score} for image quality, and \textit{R-precision} \citep{AttnGAN} to measure text-image alignment. Our model reduces the FID score from $14.06$ to $13.67$ on CUB and $40.31$ to $30.07$ on Oxford-102. For MS-COCO, the FID drops from $35.49$ to $25.17$, outperforming AttnGAN \citep{AttnGAN}, which uses embeddings trained generically in discriminative approach using contrastive loss.  Furthermore, we observe that employing separate embeddings gives superior results compared to a shared embedding approach as verified in Section \ref{sec:ablationdualemb}.  Finally, we demonstrate that such learnt dual-text embeddings can be used in other contexts as well, such as for text-to-image manipulation.

The contributions of this paper are summarized as follows:

% \vspace{-0.2cm}
\begin{itemize}
    \item We propose a novel approach for learning text embeddings specifically tailored to the requirements of Text-to-Image synthesis networks, optimized through an end-to-end training paradigm to enhance synthesis performance.
    \item We introduce a dual-embedding framework that integrates generative and contrastive training paradigms, with one embedding optimized for photo-realism and the other for robust text-to-image alignment.
    \item Our approach demonstrates competitive performance compared to methods that utilize text representations derived from pre-trained text encoders optimized using a discriminative training paradigm.
    \item We show the application of such learnt embeddings for additional tasks, such as text-to-image manipulation.
\end{itemize}

% \vspace{-0.8cm}
\section{Related Work}
% \vspace{-0.2cm}
In this section, some of the relevant works in the literature relating to this paper are discussed briefly.

\noindent\textbf{Generative Adversarial Networks:}  In past few years, GANs \citep{GAN_2014} had been the go-to method for generating images and class-specific images \citep{condtionalgan,acgan,cgans_projections} on small datasets such as MNIST \citep{MNIST} and CIFAR \citep{cifar}. However, GAN training is highly unstable when used to generate images on large datasets such as ImageNet \citep{imagenet}. Researchers have explored to fix this training instability by re-framing  GAN loss and regularisation \citep{wgan,I_wgan,lsgan,sn_gan,orthogonal_regularisation} to generate high-resolution images on large datasets \citep{progressive_gan,big_gan}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/Dte_arch.pdf}
    %Model_achitecute.drawio.pdf}
    \vspace{-0.2cm}
    \caption{Overview of DTE-GAN architecture. DTE-GAN consists of three core components: i) a single-stage generator $G$ (Section \ref{sec:generator}), ii) a discriminator $D$ (Section \ref{sec:discriminator}), and iii) a dual text embedding setup (Section \ref{sec:dualtextembed}). In the Figure, $W_G$ = generator-side word embeddings, $S_G$ = generator-side sentence embedding, $W_D$ = discriminator-side word embeddings, $S_D$ = discriminator-side sentence embedding. The model is optimised using two objective functions: 1) adversarial loss, and 2) multi-modal contrastive loss.}
    \label{fig:modelarch}
    % \vspace{-0.9cm}
\end{figure*}

\noindent \textbf{Text-to-Image synthesis:} GANs conditioned on global sentence-level embeddings are known to generate meaningful images at low resolutions \citep{GAWWN,tacgan,reed2016generative}. StackGAN \citep{stack_gan} generates high-resolution images in stage-wise approach, where the generator at each stage is conditioned by the image generated from the previous stage. Unlike StackGAN, HDGAN \citep{hd_gan} trains a single generator and multiple discriminators for each resolution. 
AttnGAN \citep{AttnGAN} uses text embeddings to fine-tune image features and also introduces a multimodal contrastive loss (DAMSM loss) to bridge the gap between generated images and words. DM-GAN \citep{DMGAN} refines words and image features using a memory module. MirrorGAN \citep{MirrorGAN} generates a caption for the generated images that improves the text \textit{vs.} image semantic consistency. SD-GAN \citep{SDGAN} introduces a Siamese structure for the generator that uses Conditional Batch Normalization (CBN) \citep{CBN} to improve the alignment of text-image. CPGAN \citep{CPGAN} learns a memory-attended text encoder by attending to salient features in images for each word and fine-grained discriminator \citep{control_gan}.  DTGAN \citep{DTGAN} applies channel and spatial attention, conditioned on sentence vector to focus on important features for each textual representation. XMC-GAN \citep{XMC-GAN} maximises the mutual information between text and image using intra-modality and inter-modality contrastive losses. DF-GAN \citep{DF_GAN_CVPR} uses deep affine transformed global sentence embedding to condition the geand theator and the matching-aware discriminator. 
In this space of text-to-image semantic alignment-based methods, pre-trained embeddings are an inherent prerequisite. These embeddings are only trained by the discriminative approach. Unlike these methods, our apporach (DTE) attempts to learn text embeddings that capture generative and discriminative properties.  

\noindent \textbf{Generative embedding learning:} Some methods attempt to learn embeddings (text / visual) end-to-end as part of a generator. For example, \citep{NDR,VQGAN} learn discrete embeddings for visual representation and show substantial improvement in the performance of text-to-image synthesis \citep{DALL-E,CogView}. Further better and compact representation are learned to improve the quality of image generation \citep{vq_vae_2,res_quatizer,make_a_scene}. Unlike these works that consider only the generation process while learning embeddings, DTE explores the capture of different perspectives of generation and discrimination process by learning dual text embeddings.

\noindent \textbf{Large Scale Text-to-Image Synthesis:} Denoising Diffusion Probabilistic models \citep{denosing_diffusion} have currently achieved remarkable success in image generation \citep{ddpm,imporved_ddpm,diffusion_beats_GAN} by reversing the \textit{forward Markovian process} with noise removal in multiple steps. Though diffusion-based models are able to generate images with complex and varied interactions for text and generate high-quality images \citep{Dalle_2,imagen,Vq_diffusion}, these approaches require large-scale training and exploit pre-trained discriminative language models like CLIP \citep{clip}. Further, CLIP-based language and image encoders are used as a bootstrapping approach for predicting conditional representations in large-scale GAN-based approaches \citep{galip, scaling_up_gan, lafite}. Unlike CLIP, which is trained to capture text-image alignment only, DTE is proposed to learn text-image alignment and text representation toimprove image realism. 

% \vspace{-0.4cm}
\section{Methodology}
% \vspace{-0.2cm}
In this section, an end-to-end framework called "\textit{Dual Text Embedding GAN}" (DTE-GAN) is formulated for learning text embeddings learn tailored to the T2I synthesis network in an end-to-end manner while capturing different representation for text for improving photo-realism and capturing text-image alignment. In the following sub-sections, the overall architecture is introduced followed by specific details on generator and discriminator architectures respectively; then the final sub-section focuses on learning dual text embeddings.

% \vspace{-0.2cm}
\subsection{Model overview}
\label{sec:overview}

DTE-GAN consists of three core components: 1) a  novel dual text embedding setup,  2) a single-stage generator $G$, and 3) a discriminator $D$. The overall architecture is shown in Figure \ref{fig:modelarch}.

First, the given text $T$ is passed through the novel dual text embedding procedure to encode the text into two types of sentence embeddings, namely: 1) Generator-side sentence embedding $S_G$, and 2) Discriminator-side sentence embedding $S_D$. 
The dual text embedding setup consists of two separate Bi-LSTM \citep{bi-lstm} text encoders (\textit{Generator-side} and \textit{Discriminator-side}) and their own independent word embeddings ($W_G$ \& $W_D$). 
As the name suggests, the \textit{Generator-side} word embeddings $W_G$ and its encoder are intended to be trained from the image-generation process (generator $G$) and its losses, while the \textit{Discriminator-side} word embeddings $W_D$ and its encoder are optimised by the contrastive loss at the discriminator $D$. Such a decoupling between these two parts of embedding adds flexibility in capturing different natures of image generations and discrimination processes. Specifically, the image generation process warrants learning representation for creating an image, while the discrimination process strives to learn features for improving text-image alignment. Further, the separation of these two embeddings allows $W_G$ to learn from noisy gradients of $G$ independently (as $G$'s gradients are initially noisy due to fake image - sentence pairs), while $W_D$ learns from stable gradients of $D$ (real image - sentence pairs).

During training, $W_G$, the generator-side text encoder, and Generator $G$ are trained from gradient signals of the generation process \textit{i.e.,} Adversarial loss of fake images ($I_{fake}$)  and multi-modal contrastive loss between the generated image $I_{fake}$ and the given text $T$. Next, $W_D$ and the discriminator-side text encoder are trained from the gradient signals of the multi-modal contrastive loss between the real image $I_{real}$ and the given text $T$. Further, discriminator $D$ is trained from Adversarial loss ($I_{fake}$, $I_{real}$) and multi-modal contrastive loss between the real image $I_{real}$ and the given text $T$.

% \vspace{-0.45cm}
\subsection{Generator}
\label{sec:generator}
% \vspace{-0.35cm}


As opposed to other methods that use stacks of GANs or hierarchical GAN, a single-stage generator is employed that can generate an image at any resolution owing to its simpler design and easier training procedure. 
The generator $G$ takes three inputs: i) a noise vector $z$ of dimension $d_z$ from a Standard Gaussian Distribution $\mathcal{N}(0,1)$ with the Truncation Trick \citep{big_gan,DF_GAN_CVPR,DTGAN}, ii) the generator-side sentence embedding $S_G$ of dimension $d_{SG}$, and iii) the discriminator-side sentence embedding $S_D$ of dimension $d_{SD}$. Next, the two sentence embeddings ($S_G$, $S_D$) together are passed through the conditioning augmentation \citep{stack_gan} to get the conditional vector $C_t$ which is concatenated with a noise vector $z$ sampled from a Standard Gaussian Distribution $\mathcal{N}(0,1)$ to form the input vector $f_G$ and passed through a fully connected layer with reshape to create a low-resolution spatial feature map. It may be noted that, in this step, $S_D$ is detached from the gradient flow of $G$ to avoid getting gradients from the generation process (refer to ablation studies in Section \ref{sec:ablationdualemb}).

Further, this low-resolution feature map is passed through a set of upsampling blocks (\textit{UpBlock}) followed by a convolution layer that accepts the last high-resolution feature map and outputs the generated image $I_{fake}$ of dimension $3 \times h \times w$ ($h=$ height, $w=$ width). Each \textit{UpBlock} is formulated as a residual layer consisting of a bi-linear upsampling step followed by two convolution blocks (convolutional layer + Conditional Batch normalisation (CBN)\citep{CBN} + LeakyReLU \citep{lrelu}). To increase the stochastic capability of the model, scaled noise is added (similar to StyleGAN \citep{styleGAN}) to input before passing to the convolutional layer. The modulation parameters in Conditional Batch normalisation (CBN) \citep{SDGAN,CBN} $\gamma_c$, and $\beta_c$ are calculated from $f_G$ by means of a linear projection layer. The modulation parameters $\gamma_c$, and $\beta_c$ in CBN are calculated as follows:

\vspace{-0.6cm}
\begin{align}
    \operatorname{BN}(x \mid C_t,z) &=\left(\gamma+\gamma_{c}\right) \cdot \frac{x-\mu(x)}{\sigma(x)}+\left(\beta+\beta_{c}\right)\\%\nonumber\\
    f_G &= \text{Concat}[C_t,z]\\%\nonumber\\
    \gamma_{c} &={FC}_{\gamma}(f_G) \\ %\nonumber\\
    \beta_{c} &={FC}_{\beta}(f_G)
\end{align}


The generator is trained to minimise adversarial loss ($\mathcal{L}_{Adv}^G$), and multi-modal contrastive loss ($\mathcal{L}_{\text{cont}}^{G}$).
$\mathcal{L}_{\text{cont}}^{G}$ is formulated as a loss between the features from the generated image and the discriminator-side sentence embeddings. Mathematically, the objective functions can be written as follows:

\vspace{-0.6cm}
\begin{align}
    \mathcal{L}_{Adv}^{G} &= \mathbb{E}_{\hat{x} \sim p_{G}}[-D(\hat{x})] \\%\nonumber\\
    % \mathcal{L}_{CA} = D_{K L}\left(\mathcal{N}\left(\mu\left(C_{t}\right), \Sigma\left(C_{t}\right)\right) || \mathcal{N}(0, I)\right) \\
    \mathcal{L}_{\text{cont}}^{G}\left(\hat{f}_{v_i}, S_{D_i}\right) &=-\log \frac{\exp \left(Sim\left(\hat{f}_{v_i}, S_{D_i}\right)\right)}{\sum_{j=1}^{N} \exp \left(Sim\left(\hat{f}_{v_i}, S_{D_j}\right)\right)}\\%\nonumber\\
    Sim(f_v, S_D) &=\cos \left(f_v, S_D\right) / \tau 
\end{align}

Here, $Sim(.,.)$ is a score function to calculate the similarity between sentence embeddings and image features, $\cos (u, v)=u^{T} v /\|u\|\|v\|$ is the Cosine Similarity between features and $\tau$ denotes the temperature hyper-parameter, and $\hat{f}_{v}$ represents visual features extracted by the discriminator for the generated image $I_{fake}$. We use conditioning augmentation \citep{stack_gan} to sample the sentence condition from an independent Gaussian Distribution $\mathcal{N}\left(\mu\left(s_{t}\right), \Sigma\left(s_{t}\right)\right)$. The regularisation term from conditioning augmentation ($\mathcal{L}_{CA}$) for combined sentence embeddings($s_t$) is:

\vspace{-0.6cm}
\begin{align}
\mathcal{L}_{CA} = D_{K L}\left(\mathcal{N}\left(\mu\left(s_{t}\right), \Sigma\left(s_{t}\right)\right) \| \mathcal{N}(0, I)\right)
\end{align}

Here $\mu(s_t)$ and $\Sigma(s_t)$ are mean and diagonal covaraince matrices that are computed as functions of the combined sentence embedding. The regualarisation term is KL Divergence between the Conditioning Gaussian and a Standard Gaussian Distribution. The final loss for generator is defined as:

\vspace{-0.6cm}
\begin{equation}
     \mathcal{L}_{G} = \mathcal{L}_{Adv}^{G} +\lambda_1 \mathcal{L}_{CA} + \lambda_2 \mathcal{L}_{\text{cont}}^{G} 
 \end{equation}

 % \vspace{-0.4cm}
 \subsection{Discriminator}
\label{sec:discriminator}
% \vspace{-0.2cm}

The discriminator $D$ is designed to serve two purposes: (1) to be a critic to determine whether the image is real or fake, and (2) to be a feature encoder to extract image features for multi-modal contrastive loss. The given image ($I_{real}$ or $I_{fake}$) is passed through a series of downsampling blocks (\textit{DownBlock}s), until the feature map is of size $8 \times 8$. Next, these $8 \times 8$ dimensional spatial features are passed through two separate branches: one for extracting features for the adversarial loss and the other for computing image features for multi-modal contrastive loss. For the adversarial branch, the input is passed through a DownBlock, ResBlock, and a fully connected layer to predict the logit to represent if the given image is real or fake. The predicted logit is used as input to an Adversarial Hinge loss \citep{sn_gan} $\mathcal{L}_{Adv}^{D}$ as follows:  

\vspace{-0.6cm}
\begin{equation}
\begin{split}
\mathcal{L}_{Adv}^{D}=\mathbb{E}_{x \sim p_{\text {data }}}[\max (0,1-D(x))] & \\  +  \mathbb{E}_{\hat{x} \sim p_{G}}[\max (0,1+D(\hat{x}))]& 
\end{split}
\end{equation}

Here, $x$ and $\hat{x}$ are real ($I_{real}$) and generated ($I_{fake}$) images.

In the multi-modal contrastive loss branch, the features are passed through a DownBlock, ResBlock, and a linear projection layer to output visual features $f_v$. The multi-modal contrastive loss $\mathcal{L}_{\text {cont }}^D$ takes as input the real image features $f_{v_i}$ and sentence embeddings $S_{D_i}$ and calculates the contrastive loss to increase the mutual information in text and image as follows:  

\vspace{-0.6cm}
\begin{equation}
    \mathcal{L}_{\text{cont}}^D\left(f_{v_i}, S_{D_i}\right)=-\log \frac{\exp \left(Sim\left(f_{v_i}, S_{D_i}\right)\right)}{\sum_{j=1}^{N} \exp \left(Sim\left(f_{v_i}, S_{D_j}\right)\right)}
\end{equation}



 $\mathcal{L}_{\text {cont }}^D$ is the contrastive loss between real image - text pairs.  The final objective function for the Discriminator is defined as:

 \vspace{-0.6cm}
 \begin{equation}
     \mathcal{L}_{D} = \mathcal{L}_{Adv}^{D} + \lambda_3 \mathcal{L}_{\text{cont}}^D
 \end{equation}


\subsection{Dual text embedding learning}
\label{sec:dualtextembed}
% \vspace{-0.2cm}

Embeddings can be viewed as memory representation learned by reducing a loss. Multiple embeddings, each learned by optimising on different losses, will capture various memory representations for the same word. The goal of the dual text embedding setup is to learn generator-side word embeddings $W_G$ (along with its encoder) to capture complex representation of words to aid improve the photo-realism of the generated images and discriminator-side word embeddings $W_D$ (along with its encoder) to capture distinctive features for words to align text-image associativity. To achieve this, we make sure that $W_G$ receives only the gradients from image-generation process whereas $W_D$ receives gradients from the contrastive loss. Specifically, we formulate the generator-side embedding loss ($\mathcal{L}_{emb}^G$) and the discriminator-side embedding loss ($\mathcal{L}_{emb}^D$) as follows:

% \vspace{-0.5cm}
 \begin{align}
     \mathcal{L}_{emb}^G &= \mathcal{L}_{G} \\%\nonumber\\
     \mathcal{L}_{emb}^D &= \lambda_3 \mathcal{L}_{\text{cont}}^D
 \end{align}
 % \vspace{-0.4cm}

Here, $\mathcal{L}_{G}$ denotes the loss function for the generator $G$, $\mathcal{L}_{\text{cont}}^D$ denotes the multi-modal contrastive loss between real image ($I_{real}$) features and discriminator-side sentence embedding $S_D$ for the given text T.

  \begin{figure*}[!ht]
    % \vspace{-0.4cm}
    \centering
    \includegraphics[width=1\textwidth]{images/bird_results_DFGAN.drawio.pdf}
    % \includegraphics[width=1\textwidth]{images/coco_results_main.drawio.pdf}
    % \vspace{-0.4cm}
    \caption{Visual comparision of the images generated by DF-GAN \citep{DF_GAN_CVPR} and DTE-GAN on CUB\citep{CUB_dataset} and COCO\citep{coco_dataset} Datasets. }
    \label{fig:bird_results}
\end{figure*}

\begin{figure*}[!ht]
    % \vspace{-1.2cm}
    \centering
    \includegraphics[width=1\textwidth]{images/flower_results_3.drawio.pdf}
    % \vspace{-0.4cm}
    \caption{Illustration of the images generated by HDGAN \citep{hd_gan} and those of  DTE-GAN on Oxford-102 Flower Dataset \citep{flower_dataset}.}
    \label{fig:flower_results}
\end{figure*}


% \vspace{-0.4cm}
\section{Experiments}
\label{sec:Experiments}
% \vspace{-0.2cm}
 
 In this section, datasets and evaluation metrics are introduced for experiments. Futher, proposed DTE is evaluated and compared quantitatively and qualitatively with other methods in the literature. The specific training details and hyper-parameters are mentioned in the supplementary material.

\textbf{Datasets:} DTE-GAN is evaluated on three datasets, namely, 1) Caltech-UCSD birds (CUB) \citep{CUB_dataset}, 2) Oxford-102 flowers \citep{flower_dataset}, and 3) MS COCO \citep{mscoco} datasets. For CUB and Oxford-102 datasets, we have similar setup to StackGAN \citep{stack_gan}. Ten captions are provided for each image in both the datasets. The MS-COCO dataset consists of around 80k training and 40k validation images; and for every image, there are 5 captions provided with the dataset.  

\begin{table*}[!ht]
\centering
\begin{tabular}{lcccccc}
\toprule
%\multirow{2}{*}{\textbf{\makecell[l]{Variant \\ name}}} & 
\multirow{2}{*}{\textbf{\makecell{Method}}} & \multicolumn{3}{c}{\textbf{CUB}} & \multicolumn{3}{c}{ \textbf{COCO}} \\
\cmidrule{2-7}
    &\textbf{IS} $\uparrow$   & \textbf{FID} $\downarrow$   & \textbf{R\%} $\uparrow$   & \textbf{FID} $\downarrow$      & \textbf{R\%} $\uparrow$     & \textbf{NoP} $\downarrow$     \\
\midrule
StackGAN \citep{stack_gan} & $3.70 \pm .04$  & - & - & - & - & -\\
AttnGAN \citep{AttnGAN} & $4.36 \pm .02$  & $23.98$ & $67.82$ & $35.49$ & $83.82$ & $230M$\\
MirrorGAN \citep{MirrorGAN} & $4.56 \pm .17$  & $18.32$ & $57.67$ & $34.71$ & $74.53$ & -\\
 DM-GAN \citep{DMGAN} & $4.75 \pm .07$ & $16.09$ & $72.32$ & $32.64$ & $88.56$ & $46M$\\
 KT-GAN \citep{KT-GAN} & $4.85 \pm .05$ & $17.32$ & - & $30.73$ & - & -\\
 TIME \citep{TIME} & $4.91 \pm .04$ & $\color{blue}14.30$ & $71.57$& $31.14$ & - & $120M$\\
 DAE-GAN \citep{DAE-GAN} & $4.42 \pm .04$ & $15.19$ & $\color{green}85.45$ & $28.12$ & $\color{red}92.61$ & $98M$\\
 CSM-GAN \citep{CSM-GAN} & $4.62 \pm .08$ & $20.18$ & - & $33.48$ & - & -\\
 DR-GAN \citep{DR-GAN} & $4.90 \pm .05$ & $14.96$ & - & $27.80$ & - & $73M$\\
 ALR-GAN \citep{ALR-GAN} & $4.96 \pm .04$ & $15.14$ & $77.54$ & $29.04$ & $69.20$ & $76M$\\
 
 DF-GAN \citep{DF_GAN_CVPR} & $4.86 \pm .04$ & $14.81$ & - & $\color{red}19.32$ & - & $\color{green}19M$\\
 SSA-GAN \citep{SSA-GAN} & $ \color{red} 5.17 \pm .08$ & $15.61$ & $\color{blue}85.4$ & $\color{green}19.37$ & $\color{blue}90.6$ & $\color{blue}26M$\\
\midrule

\textbf{DTE-GAN}& $\color{green}5.12 \pm .04$  & $\color{red} 13.67$ & $\color{red}86.64$& $25.17$ & $\color{green}90.82$ & $\color{red}11M$\\
         % \hline
 \textbf{DTE-GAN+MAGP}& $\color{blue}5.09 \pm .02$  & $\color{green}13.94$ & $81.33$  & $\color{blue}19.69$ & $88.39$ & $\color{red}11M$\\
 
      \bottomrule
\end{tabular}
\caption{Quantitative comparison between DTE-GAN and other models on CUB \citep{CUB_dataset} and COCO \citep{coco_dataset} datasets. "-" indicates values are unreported. The best three results are marked with \textcolor{red}{red}, \textcolor{green}{green}, and \textcolor{blue}{blue}, respectively. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
\label{tab:Bird_COCO_table}
% \vspace{-1.2cm}
\end{table*}

\subsection{Visual comparison}
% \vspace{-0.2cm}
The generated images are visually compared between DF-GAN \citep{DFGAN} and DTE-GAN in the CUB and COCO datasets. In Figure \ref{fig:bird_results}, it is evident that DF-GAN struggles to depict complete bird shapes. The presented model, employing dual text embeddings, minimizes image generation loss, improving shape accuracy and realistic fine-grained features in generated images. Furthermore, DTE-GAN outperforms DF-GAN in pose representation, resulting in more natural-looking images. With regard to semantic consistency between images and text, DTE-GAN captures detailed structures and overall coherence compared to DF-GAN. Despite not using word embeddings for image region attention, DTE-GAN's ability to learn embeddings for both generation and discrimination allows it to generate images with finer details. DTE-GAN produces images that resemble real images due to its learned embeddings encompassing generation and discrimination aspects. For the COCO dataset, DTE-GAN generates images of comparable quality to DF-GAN while using significantly fewer parameters. This efficiency is achieved by learning tailored text embeddings specifically for the synthesis model. In Figure \ref{fig:flower_results}, images for the Oxford-102 dataset are generated and compared with those of HDGAN \citep{hd_gan}. We can observe that our model is able to capture the complex variation of the flowers and generate more realistic images than HDGAN.


\begin{table}[!ht]
    \centering
    \begin{tabular}{lcc}
         \toprule
         {\textbf{Method}}& {\textbf{IS} $\uparrow$} & \textbf{FID} $\downarrow$\\ 
         \midrule
         StackGAN \citep{stack_gan} & $3.20 \pm .01$ & $51.89$  \\
         StackGAN++ \citep{stackgan++} & $3.26 \pm .01$ & $48.68$ \\
         HDGAN \citep{hd_gan} & $3.45 \pm .07$ & $43.17$ \\
         % SegAttnGAN \citep{SegAttnGAN} &$ 3.36 \pm .08$ & - \\
         SSTIS \citep{SSTIS} &$ 3.37 \pm .05$ & - \\
         SS-TiGAN \citep{SSBI} &$ 3.45 \pm .04$ & $40.54$ \\
         DualAttn-GAN \citep{Dual_Attn_GAN} &$ 4.06 \pm .05$ & $\color{blue}40.31$ \\
         RAT-GAN \citep{RATGAN} &$ \color{blue}4.09  $& - \\
         \midrule
         \textbf{DTE-GAN}& $\color{green}4.21 \pm .08$  & $\color{red}30.07$\\
         \textbf{DTE-GAN+MAGP}& $\color{red}4.26 \pm .07$  & $\color{green}31.13$\\
         \bottomrule
    \end{tabular}
    \caption{Quantitative comparison between DTE-GAN and other models on Oxford-102 Dataset. The best three results are marked with \textcolor{red}{red}, \textcolor{green}{green}, and \textcolor{blue}{blue}, respectively. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better."-" indicates values are unreported.}
    \label{tab:Flower_table}
    % \vspace{-1.1cm}
\end{table}

% \vspace{-0.5cm}
\subsection{Quantitative Evaluation}
% \vspace{-0.2cm}

\textbf{Evaluation metrics:} To evaluate the quality of images generated, the following metrics used are: \textit{Inception Score (IS)} \citep{IS_score} calculates the Kullback-Leibler (KL) divergence between a conditional distribution and marginal distribution for class probabilities from Inception-v3 \citep{szegedy2016rethinking} model. Higher IS suggest generated images are from higher diverse classes. \textit{Fr\'echet Inception Distance (FID)} \citep{FID_score} calculates the Fr\'echet Distance between two multivariate Gaussians, which are fit to the global features extracted from the Inception-v3 \citep{szegedy2016rethinking}  model on the synthetic and generated images. Lower the FID suggest generated images closer to real images. \textit{R-precision} (R\%) precision evaluates text-to-image alignment, by assessing whether generated images can be used to retrieve the text. 


The performance of proposed model is compared with that of the lightweight GAN approaches (having similar training setups) for the task of text-to-image synthesis on CUB and COCO datasets in Table \ref{tab:Bird_COCO_table}. From Table \ref{tab:Bird_COCO_table}, on the CUB dataset, we observe that DTE-GAN improves \textit{IS} from $4.91$ to $5.12$, achieves the best \textit{R-precision} of $86.64$ and further decreases \textit{FID} from $14.06$ to $13.67$. We also train our DTE-GAN with the proposed regularisation trick Matching-Aware Gradient Penalty (MAGP) \citep{DF_GAN_CVPR}, which smooths out discriminator function and allows to generate more realistic images. On the CUB dataset, compared to AttnGAN \citep{AttnGAN} that employs contrastive loss-based embeddings, our model decreases \textit{FID} from $23.98$ to $13.67$. Such an improvement illustrates the effectiveness of the end-to-end learned dual embeddings over fixed pre-trained embeddings learned on the same data. On MS-COCO \citep{mscoco} dataset (in Table \ref{tab:Bird_COCO_table}), we achieve similar performance of DF-GAN and SSA-GAN with fewer parameters, underscoring the efficiency of the proposed DTE and its ability to learn embeddings tailored specifically to the synthesis model. DTE-GAN+MAGP achieves similar performance as that of SSA-GAN \citep{SSA-GAN} on COCO dataset as SSA-GAN and DF-GAN \citep{DF_GAN_CVPR} both incorporating MAGP. 

As shown in the Table \ref{tab:Bird_COCO_table}, we have employed a network with significantly fewer parameters by reducing the width of the layers by half compared to SSA-GAN \cite{SSA-GAN} and DF-GAN \cite{DFGAN} in their respective UpBlock and DownBlock. Despite this reduction in complexity, our model achieves comparable results by learning more effective text encoding representations, which improve the performance of Text-to-Image synthesis

Following previous works \citep{DF_GAN_CVPR,DTGAN,SSA-GAN}, we report only \textit{FID} scores, as \textit{IS} scores for the MS-COCO dataset do not reflect the quality of the synthesised images. In comparison to TIME \citep{TIME} which learns embeddings along with the Text-to-Image synthesis model, DTE-GAN achieves significant improvement (0.6 in \textit{FID}, +15\% in R-precision) demonstrating the effectiveness of dual text embeddings. In Table \ref{tab:Flower_table}, on the Oxford-102 dataset, we use \textit{IS} and \textit{FID} scores for evaluation, as R-precision scores are not available in the literature. In this dataset, our model improves \textit{IS} score from $4.06$ to $4.21$ over the state-of-the-art (DualAttn-GAN \citep{Dual_Attn_GAN}, LeicaGAN \citep{Leica_gan}) models and remarkably decreases \textit{FID} from $40.31$ to $30.07$. 

\vspace{-0.3cm}
\subsection{Additional Studies}
\vspace{-0.3cm}

\begin{figure*}[t]
    % \vspace{-1.5cm}
    \centering
    \includegraphics[width=1\textwidth]{images/clip_images.drawio.pdf}
    \includegraphics[width=1\textwidth]{images/bird_clip_images.drawio.pdf}
    \vspace{-0.6cm}
    \caption{Images generated using CLIP, CLIP + Learnable Generator side embeddings (CLIP + $\mathbf{G_{emb}}$) and DTE on CUB and Oxford-102 datasets.}
    \label{fig:DTE_vs_CLIp}
    \vspace{-0.3cm}
\end{figure*}

\begin{table}[!ht]
\centering
\begin{tabular}{ccccc}
\toprule
                 \textbf{Dataset} & \textbf{Embeddings}  & \textbf{IS} $\uparrow$ & \textbf{FID} $\downarrow$ & \textbf{R \%} $\uparrow$\\
\midrule
\multirow{3}{*}{\textbf{CUB}} & \textbf{CLIP}  & $4.53$ & $21.33$ & $74.12$\\
%  \cline{2-5}
                     & \textbf{CLIP+$\mathbf{G_{emb}}$ } & $4.51$ & $19.36$ & $78.35$\\
                     
                     & \textbf{DTE} & $\color{red}5.12$ & $\color{red}13.67$ &  $\color{red}86.64$   \\
\midrule
\multirow{3}{*}{\textbf{Oxford}} & \textbf{\textbf{CLIP}}  &  $3.72$ & $38.36$ & $71.78$   \\
%  \cline{2-5}
                     & \textbf{CLIP+$\mathbf{G_{emb}}$} &$3.81$ & $35.93$ &  $73.87$   \\
                     
                     
                     & \textbf{DTE} & $\color{red}4.21$  & $\color{red}30.07$ & $\color{red}83.19$  \\
\bottomrule
\end{tabular}
\caption{We compare quality of T2I generation of our proposed DTE approach with that of models trained using CLIP \citep{clip} and CLIP+$\mathbf{G_emb}$. The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
\label{tab:dte_vs_clip}
\end{table}

% \vspace{-1.5cm}
\subsubsection{DTE vs CLIP:}
\label{sec:DTE_VS_CLIP}

We compare DTE with pre-trained CLIP embeddings by training a GAN for text-to-image generation. Additionally, we train another GAN model resembling the DTE setup. This model uses learnable Generator-side embeddings and pre-trained CLIP embeddings for the discriminator (referred to as CLIP+ $G_{emb}$). Table \ref{tab:dte_vs_clip} compares image quality using different CUB and Oxford-102 datasets embeddings. CLIP+$\mathbf{G_{emb}}$ improves over just CLIP. In Figure \ref{fig:DTE_vs_CLIp}, CLIP images differ from reality, while CLIP+$\mathbf{G_{emb}}$ matches better text and real images due to learned generative embeddings. DTE's combined approach creates images closer to real images. Furthermore, we demonstrate that this approach can be integrated with pre-trained vision-language models, specifically CLIP+$\mathbf{G_{emb}}$ , where only the generator-side embeddings are learned. This method of focusing solely on generator-side embeddings can be seamlessly incorporated into current diffusion-based Text-to-Image models \citep{stable_diffusion,Dalle_2} within the existing training frameworks and emphasises learning embedding as needed for synthesis model.

\subsubsection{Generalisation ability of DTE:}
%AttnGAN with DTE approach

To assess how well the DTE setup applies to other architectures, we integrate it into AttnGAN \citep{AttnGAN}, now called AttnGAN+DTE. In AttnGAN, pre-trained text embeddings (DAMSM embeddings) that are generic is used training synthesis model. Instead, we train embeddings scratch using DTE. Incorporating DTE into AttnGAN involves modifying its discriminators to include a dual branch (adversarial loss and multi-modal contrastive loss) after reaching $8 \times 8$ feature size. As AttnGAN uses words for alignment in the attention layer, we introduce a word-contrastive loss \citep{AttnGAN,XMC-GAN} in the final discriminator as an additional branch when the feature size is $8 \times 8$, aimed at reducing the semantic gap between words and image features. We combine generator-side and discriminator-side word embeddings to provide word features for the Generator's attention mechanism. The results in Table \ref{tab:attn_dte_table} show that AttnGAN+DTE can train without pre-trained embeddings and even improve the results, proving that DTE can be integrated well with other methods.

\begin{table}[!ht]
    % \vspace{-0.5cm}
    \centering
    % \vspace{-0.1cm}
    \begin{tabular}{lccc}
         \toprule
         {\textbf{Method}}&  \textbf{IS} $\uparrow$ &\textbf{FID} $\downarrow$&\textbf{R\%} \\ 
         \midrule
         AttnGAN & $4.36 \pm .02$  & $23.98$& $67.82$\\
         AttnGAN+DTE &  $4.38 \pm .03$ & $21.45$& $71.39$\\ 
         DTE-GAN & $\color{red}5.12 \pm .04$  & $\color{red}13.67$ & $\color{red}86.64$\\
         \bottomrule
    \end{tabular}
    \caption{Impact of DTE approach on AttnGAN \citep{AttnGAN} on CUB Dataset \citep{CUB_dataset}. The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
    \label{tab:attn_dte_table}
    % \vspace{-2cm}
\end{table}

\begin{figure}[!ht]
    \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.6\textwidth]{images/manipulation_images_birds_5_box.drawio.pdf}
    \vspace{-0.2cm}
    \caption{Examples of manipulated images generated by LightWeight GAN  \citep{lightweight_GAN} using DTE-GAN pre-trained embeddings on CUB dataset. Source images are manipulated by the caption of concept images.}
    \vspace{-0.4cm}
    \label{fig:manipulation_results}
\end{figure}


\begin{table}[!ht]

    % \vspace{-0.5cm}
    \centering
    \begin{tabular}{lcc}
        \midrule
         \textbf{Method}& \textbf{IS} $\uparrow$ & \textbf{FID} $\downarrow$ \\
         \midrule
        %  SISGAN \citep{sisgan} & $2.24$ & - \\
        %  TAGAN \citep{tagan} & $3.32$ & - \\
         MANIGAN  & $8.48$ & $9.75$ \\
         LWGAN  & $8.26$ & $8.02$  \\
         LWGAN w/ DTE-EMB & $\color{red}8.56$ & $\color{red}7.77$ \\
         \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Quantitative comparison of Inception score and FID for manipulated images on CUB dataset. We use Lightweight GAN \citep{lightweight_GAN} which we name as LWGAN with the pre-trained embeddings using DTE-GAN (LWGAN w/ DTE-EMB). The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
    \label{tab:manipulations}
\end{table}

\vspace{-0.2cm}
\subsubsection{Application to Text-to-Image manipulation task:} 
\vspace{-0.2cm}
To demonstrate the versatility of the learned dual embeddings, we apply them to text-to-image manipulation tasks. We train dual text embeddings through DTE-GAN on the CUB dataset for text-to-image synthesis. We then utilise the pre-trained word embeddings ($W_G, W_D$) from this synthesis task for text-to-image manipulation in the Lightweight GAN for Text-to-Image manipulations \citep{lightweight_GAN} task on the same dataset. It is important to note that these pre-trained dual text embeddings remain fixed during training of manipulation network. In Table \ref{tab:manipulations}, the quantitative performance of the model using these pre-trained embeddings is compared with that of other text-to-image manipulation models.  The model improves the Inception score from 8.48 (MANIGAN \citep{manigan}) to 8.56 and reduces the \textit{FID} score from 8.02 \citep{lightweight_GAN} to 7.77 on the CUB dataset. This demonstrates the ability of dual text embeddings to generalise effectively across different tasks. Figure \ref{fig:manipulation_results} illustrates few visual examples of the text-to-image manipulations. 


\begin{table*}[!ht]
\vspace{-0.2cm}
\centering
\begin{tabular}{lcccccccccc}
\toprule
%\multirow{2}{*}{\textbf{\makecell[l]{Variant \\ name}}} & 
\multirow{2}{*}{\textbf{\makecell{Emb.\\ type}}} & \multicolumn{4}{c}{\textbf{ Components}} & \multicolumn{3}{c}{\textbf{CUB}} & \multicolumn{3}{c}{ \textbf{Oxford-102}} \\
\cmidrule{2-11}
    & $\mathcal{L}_G$    & $\mathcal{L}_{cont}^D$ &$S_D \rightarrow G$ &$S_G \rightarrow D$   & \textbf{IS} $\uparrow$    & \textbf{FID} $\downarrow$   & \textbf{R\%} $\uparrow$   & \textbf{IS}  $\uparrow$     & \textbf{FID} $\downarrow$     & \textbf{R\%} $\uparrow$     \\
\midrule
    Shared & \xmark  &\cmark & - & - & $4.54 \pm .04$ & $15.81$ & $85.63$ & $3.52 \pm .06$ & $33.57$ & $81.73$\\  
   Shared &    \cmark   &  \cmark & - & - & $4.27 \pm .06$ & $18.38$ & $82.73$ & $3.32 \pm .05$ & $34.83$ & $76.15$\\
  Dual & \cmark  &\cmark & \xmark & \xmark & $ 4.73\pm .05$ & $14.93$ & $63.79$ & $3.84 \pm .03$ & $32.98$ & $54.97$\\
 Dual & \cmark  &\cmark  & \cmark & \cmark & $4.25 \pm .05$ & $18.01 $ & $69.38$ & $3.28 \pm .04$ & $35.41$ & $70.48$\\
 Dual & \cmark & \cmark & \cmark & \xmark &$\color{red}5.12 \pm .04$ & $\color{red}13.67$ & $\color{red}86.64$ & $\color{red}4.21 \pm .08$  & $\color{red}30.07$ & $\color{red}83.19$ \\ 
      \bottomrule
\end{tabular}
\caption{Quantitative comparison of DTE with its variants. Here, $\mathcal{L}_G$ = generator loss, $\mathcal{L}_{cont}^D$ = multi-modal contrastive loss between real image - text pairs, $S_D \rightarrow G$ = whether the generator has access to the discriminator-side sentence embedding, $S_G \rightarrow D$ = whether the discriminator has access to the generator-side sentence embedding. The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
\label{tab:modelvariants}
% \vspace{-1cm}
\end{table*}


\begin{table*}[!ht]
\vspace{-0.2cm}
\centering
\begin{tabular}{lcccccccc}
\toprule
%\multirow{2}{*}{\textbf{\makecell[l]{Variant \\ name}}} & 
\multirow{2}{*}{\textbf{\makecell{Epoch\\ Count}}} & \multicolumn{2}{c}{\textbf{ Components}} & \multicolumn{3}{c}{\textbf{CUB}} & \multicolumn{3}{c}{ \textbf{Oxford-102}} \\
\cmidrule{2-9}
     &$S_D \rightarrow G$ &$S_G \rightarrow D$   & \textbf{IS} $\uparrow$    & \textbf{FID} $\downarrow$   & \textbf{R\%} $\uparrow$   & \textbf{IS}  $\uparrow$     & \textbf{FID} $\downarrow$     & \textbf{R\%} $\uparrow$     \\
\midrule
 0  & \cmark & \cmark & $4.21 \pm .04$ & $18.33 $ & $69.81$ & $3.34 \pm .05$ & $36.18$ & $71.13$\\
 100  & \cmark & \cmark & $4.59 \pm .06$ & $16.24 $ & $74.46$ & $3.56 \pm .05$ & $33.27$ & $75.68$\\
 300  & \cmark & \cmark & $\color{red}4.82 \pm .04$ & $\color{red}14.96 $ & $\color{red}78.57$ & $\color{red}3.91 \pm .04$ & $\color{red}31.38$ & $\color{red}78.92$\\
 \bottomrule
\end{tabular}
\caption{Quantitative comparison of of ($S_G \rightarrow D$) whether the discriminator has access to the generator-side sentence embedding from the epoch count. $S_D \rightarrow G$ represents generator has access to the discriminator-side sentence embedding. The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
\label{tab:modelvariants_gen_emb}
% \vspace{-1cm}
\end{table*}

\vspace{-0.2cm}
\subsubsection{Importance of dual text embeddings}
\label{sec:ablationdualemb}
% Need to add one more result to it 
\vspace{-0.2cm}

To verify the effectiveness of the proposed Dual Text Embeddings (DTE) setup, different ways of organising the word embeddings between generator $G$ and discriminator $D$ are evaluated on CUB and Oxford-102 datasets and results shown in Table \ref{tab:modelvariants}. Specifically, four variants of organising the embeddings are compared, namely: \textit{i}) A shared word embedding layer between $G$ and $D$ that is trained only with a multi-modal contrastive loss $\mathcal{L}_{cont}^D$ between real image-text pairs as it is trained only to capture distinctive features (Table \ref{tab:modelvariants}, row 1). \textit{ii}) A shared word embedding layer between $G$ and $D$ that is trained using both the generator loss $\mathcal{L}_G$ and real image-text pair contrastive loss $\mathcal{L}_{cont}^D$ to capture distinctive and intricate appearance features in single embeddings (Table \ref{tab:modelvariants}, row 2).  \textit{iii}) A dual embedding setup where $G$ doesn't have access to the discriminator-side sentence embedding $S_D$(Table \ref{tab:modelvariants},row 3). \textit{iv}) A dual embedding setup where both $G$ and $D$ have access to the generator-side sentence embeddings $S_G$ and discriminator-side sentence embeddings $S_D$ (Table \ref{tab:modelvariants}, row 4). 
The shared embedding model trained only with $\mathcal{L}_{cont}^D$ to capture distinctive features (Table \ref{tab:modelvariants}, row 1) achieves similar R-precision scores as those of the proposed DTE-GAN, but there is significant drop in \textit{IS} and \textit{FID} scores suggesting that there is drop in the quality of generated images. 
Next, the shared embedding model trained with both $\mathcal{L}_G$ and $\mathcal{L}_{cont}^D$ performs inferior to the one trained only with $\mathcal{L}_{cont}^D$. It proves that capturing generator-side noisy gradient signals into the same word embeddings degrades the performance. Further, the dual embedding model with independent generator- and discriminator-side embeddings achieves better IS and FID scores compared to those of shared embedding models but has a significant drop in R-precision, suggesting that the images generated are realistic but do not capture text-to-image alignment. Further, allowing the discriminator to have access to the generator-side embeddings significantly drops the performance (Table \ref{tab:modelvariants}, row 4). As the Discriminator-side embeddings are learned with ground-truth real image-text pairs, it is found beneficial to allow Generator to have access (\textit{i.e.,} a sneak peek) to Discriminator-side embeddings (Table \ref{tab:modelvariants}, row 5). Discriminator-side embeddings capture distinct features and Generator-side feature captures intricate details to improve photo-realism; providing both the information to Generator allows to generate more realistic and text-aligned images. On the other hand, Generator-side embeddings are learned using noisy gradients from fake images and allowing the discriminator to access them introduces an adverse effect and decreases the performance (Table \ref{tab:modelvariants}, row 4).

\subsubsection{Generator-side embeddings to Discriminator}
\label{sec:gen_emb_to_disc}

When generator-side embeddings are provided to the discriminator, we observe a significant drop in overall image generation quality, as reported in Table \ref{tab:modelvariants}. To further evaluate this effect, we have conducted an experiment in which generator-side embeddings are supplied to the discriminator after a specified number of training epochs, with the results presented in Table \ref{tab:modelvariants_gen_emb}. Specifically, generator-side $S_G$ embeddings are combined with $S_D$ using summation from the start of training (epoch = 0), after 100 epochs when the generator began producing images with plausible structure, and after 300 epochs when more realistic images were generated. The low or noisy quality of generated images during the initial stages affects the learning of generator-side embeddings; consequently, providing these embeddings to the discriminator negatively impacts the overall image generation quality.



\begin{table*}[!ht]
% \vspace{-1.2cm}
\centering
\begin{tabular}{lcccccccc}
\toprule
%\multirow{2}{*}{\textbf{\makecell[l]{Variant \\ name}}} & 
\multirow{2}{*}{\textbf{\makecell{Epoch\\ Count}}} & \multicolumn{2}{c}{\textbf{ loss}} & \multicolumn{3}{c}{\textbf{CUB}} & \multicolumn{3}{c}{ \textbf{Oxford-102}} \\
\cmidrule{2-9}
     &$\mathcal{L}_{cont}^D$ &$\mathcal{L}_G$   & \textbf{IS} $\uparrow$    & \textbf{FID} $\downarrow$   & \textbf{R\%} $\uparrow$   & \textbf{IS}  $\uparrow$     & \textbf{FID} $\downarrow$     & \textbf{R\%} $\uparrow$     \\
\midrule
 0  & \cmark & \cmark & $4.27 \pm .06$ & $18.38 $ & $82.73$ & $3.32 \pm .05$ & $34.83$ & $76.15$\\
 100  & \cmark & \cmark & $4.38 \pm .04$ & $16.82 $ & $83.72$ & $3.39 \pm .04$ & $34.04$ & $78.69$\\
 300  & \cmark & \cmark & $4.47 \pm .05$ & $16.23 $ & $84.51$ & $3.47 \pm .03$ & $33.96$ & $79.94$\\
 -  & \cmark & \xmark & $\color{red}4.54 \pm .04$ & $\color{red}15.81 $ & $\color{red}85.63$ & $\color{red}3.52 \pm .06$ & $\color{red}33.57$ & $\color{red}81.73$\\
 \bottomrule
\end{tabular}
\caption{Quantitative comparison of shared embeddings trained with contrastive loss ($\mathcal{L}_{cont}^D$) and Generator's loss ($\mathcal{L}_G$) after the epoch count. Single embeddings without $\mathcal{L}_G$ training achieves superior performance. The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
\label{tab:modelvariants_dual_loss}
% \vspace{-1cm}
\end{table*}

\subsubsection{Shared Embeddings with Dual Loss}
\label{sec:dual_loss}

As shown in Table \ref{tab:modelvariants}, shared embeddings (or single embeddings for text) trained with both contrastive loss $\mathcal{L}_{cont}^D$ and the generator's loss $\mathcal{L}_G$ yield subpar results compared to embeddings trained solely with contrastive loss. We hypothesise that this is because, during the early stages of GAN training, the generator produces predominantly noisy images, negatively impacting the learning of embeddings. To investigate the impact of the generator's loss, we trained multiple networks with shared embeddings and applied the generator's loss to these embeddings at different training stages. The results, reported in Table \ref{tab:modelvariants_dual_loss}, indicate that applying both losses from the early stages of training adversely affects image generation quality. Moreover, decoupling the embeddings introduces flexibility, enabling them to capture diverse representations, ultimately improving overall image generation quality.



\subsubsection{Dual Captions for Dual Text Encoders}
\label{sec:dual_captions}

Earlier approaches in Text-to-Image synthesis have employed dual \citep{SDGAN} or multiple captions \citep{RifeGAN} to enhance semantic consistency and facilitate richer feature extraction. To analyse the impact of using different captions in a dual text encoder setup, we have conducted experiments and reported the results in Table \ref{tab:Dual Captions}. Consistent with previous findings, our results show a marginal improvement in performance. This improvement is attributed to the ability of each text encoder to independently extract superior features, thereby reinforcing the overall semantic alignment and image quality.

\begin{table*}[!ht]
% \vspace{-1.2cm}
\centering
\begin{tabular}{lcccccc}
\toprule
%\multirow{2}{*}{\textbf{\makecell[l]{Variant \\ name}}} & 
\multirow{2}{*}{\textbf{\makecell{Embedding\\ Type}}}  & \multicolumn{3}{c}{\textbf{CUB}} & \multicolumn{3}{c}{ \textbf{Oxford-102}} \\
\cmidrule{2-7}
       & \textbf{IS} $\uparrow$    & \textbf{FID} $\downarrow$   & \textbf{R\%} $\uparrow$   & \textbf{IS}  $\uparrow$     & \textbf{FID} $\downarrow$     & \textbf{R\%} $\uparrow$     \\
\midrule
 \textbf{Single}   & $5.12 \pm .04$ & $13.67 $ & $86.64$ & $4.21 \pm .08$ & $30.07$ & $83.19$\\
 \textbf{Dual}   & $\color{red}5.14 \pm .04$ & $\color{red}13.37 $ & $\color{red}87.12$ & $\color{red}4.28 \pm .09$ & $\color{red}29.71$ & $\color{red}83.38$\\
 \bottomrule
\end{tabular}
\caption{Quantitative comparison of shared embeddings trained with single and dual captions. Single embeddings with out $\mathcal{L}_G$ training achieves superior performance. The best results are \textcolor{red}{red}. ‘‘$\uparrow$’’ indicates the higher, the better, while ‘‘$\downarrow$’’ indicates the lower, the better.}
\label{tab:Dual Captions}
% \vspace{-1cm}
\end{table*}


% % \vspace{-0.2cm}
% \section{Limitation and Future Scope}
% \label{sec:issues}

% This work proposes an approach to learning vision-language models by generating images. Due to computational constraints (our model is trained on a single 1080Ti graphics card with 12 GB memory), we focus our approach and conduct experiments on smaller datasets. For future work, we aim to explore learning vision-language models by generating images using diffusion-based \citep{ddpm,diffusion_beats_GAN} models on large-scale openly available datasets \citep{webvideo10m,DiffusionDBLargescalePrompt2022}.

\vspace{-0.1cm}
\section{Conclusion}
\vspace{-0.1cm}
This study introduces a novel approach to text-to-image synthesis by proposing Dual Text Embeddings (DTE), which learns text embeddings tailored explicitly for the synthesis task in an end-to-end manner. Unlike traditional methods that depend on pre-trained, generic embeddings, DTE uses two separate embeddings designed for specific purposes: one to improve the photo-realism of generated images and the other to ensure better alignment between text and images. DTE decouples these objectives with dedicated training techniques, leading to enhanced performance.

Our experiments on three benchmark datasets (Oxford-102, Caltech-UCSD, and MS-COCO) demonstrate that having separate embeddings yields better results than using a shared representation. Furthermore, the DTE framework performs favourably compared to methods relying on pre-trained text embeddings optimized with contrastive loss. Additionally, their versatility highlights the adaptability of learned dual embeddings to other language-based vision tasks, such as text-to-image manipulations.

Future work includes extending this dual embedding framework to other multimodal applications, including image or video captioning and visual question answering, further showcasing the potential of task-specific text embeddings in advancing language-vision interactions.


\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{wgan}
Martin Arjovsky, Soumith Chintala, and Léon Bottou.
\newblock Wasserstein gan, 2017.

\bibitem[Brock et~al.(2017)Brock, Lim, Ritchie, and Weston]{orthogonal_regularisation}
Andrew Brock, Theodore Lim, J.~M. Ritchie, and Nick Weston.
\newblock Neural photo editing with introspective adversarial networks, 2017.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{big_gan}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis, 2019.

\bibitem[Cai et~al.(2019)Cai, Wang, Yu, Li, Xu, Li, and Li]{Dual_Attn_GAN}
Yali Cai, Xiaoru Wang, Zhihong Yu, Fu~Li, Peirong Xu, Yueli Li, and Lixian Li.
\newblock Dualattn-gan: Text to image synthesis with dual attentional generative adversarial network.
\newblock \emph{IEEE Access}, 7:\penalty0 183706--183716, 2019.
\newblock \doi{10.1109/ACCESS.2019.2958864}.

\bibitem[Chen et~al.(2019)Chen, Lučić, Houlsby, and Gelly]{CBN}
Ting Chen, Mario Lučić, Neil Houlsby, and Sylvain Gelly.
\newblock On self-modulation for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1810.01365.pdf}.

\bibitem[Cheng et~al.(2020)Cheng, Wu, Tian, Wang, and Tao]{RifeGAN}
Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, and Dapeng Tao.
\newblock Rifegan: Rich feature generation for text-to-image synthesis from prior knowledge.
\newblock In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  10908--10917, 2020.
\newblock \doi{10.1109/CVPR42600.2020.01092}.

\bibitem[Dash et~al.(2017)Dash, Gamboa, Ahmed, Liwicki, and Afzal]{tacgan}
Ayushman Dash, John Cristian~Borges Gamboa, Sheraz Ahmed, Marcus Liwicki, and Muhammad~Zeshan Afzal.
\newblock Tac-gan - text conditioned auxiliary classifier generative adversarial network, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{diffusion_beats_GAN}
Prafulla Dhariwal and Alexander~Quinn Nichol.
\newblock Diffusion models beat {GAN}s on image synthesis.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=AAWuCvzaVt}.

\bibitem[Ding et~al.(2021)Ding, Yang, Hong, Zheng, Zhou, Yin, Lin, Zou, Shao, Yang, and Tang]{CogView}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang Lin, Xu~Zou, Zhou Shao, Hongxia Yang, and Jie Tang.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan (eds.), \emph{Advances in Neural Information Processing Systems}, pp.\  19822--19835. Curran Associates, Inc., 2021.

\bibitem[Esser et~al.(2020)Esser, Rombach, and Ommer]{VQGAN}
Patrick Esser, Robin Rombach, and Bj{\"{o}}rn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock \emph{CoRR}, abs/2012.09841, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.09841}.

\bibitem[Gafni et~al.(2022)Gafni, Polyak, Ashual, Sheynin, Parikh, and Taigman]{make_a_scene}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.13131}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{GAN_2014}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.~Q. Weinberger (eds.), \emph{Advances in Neural Information Processing Systems}, volume~27, 2014.

\bibitem[Gu et~al.(2022)Gu, Chen, Bao, Wen, Zhang, Chen, Yuan, and Guo]{Vq_diffusion}
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo~Zhang, Dongdong Chen, Lu~Yuan, and Baining Guo.
\newblock Vector quantized diffusion model for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  10696--10706, June 2022.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and Courville]{I_wgan}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C Courville.
\newblock Improved training of wasserstein gans.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{FID_score}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), \emph{Advances in Neural Information Processing Systems}, pp.\  6840--6851. Curran Associates, Inc., 2020.

\bibitem[Kang et~al.(2023)Kang, Zhu, Zhang, Park, Shechtman, Paris, and Park]{scaling_up_gan}
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.
\newblock Scaling up gans for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  10124--10134, June 2023.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and Lehtinen]{progressive_gan}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and variation, 2018.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{styleGAN}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Krizhevsky(2009)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{MNIST}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2022)Lee, Kim, Kim, Cho, and Han]{res_quatizer}
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.
\newblock Autoregressive image generation using residual quantization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  11523--11532, June 2022.

\bibitem[Li et~al.(2019)Li, Qi, Lukasiewicz, and Torr]{control_gan}
Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr.
\newblock Controllable text-to-image generation.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Qi, Lukasiewicz, and Torr]{manigan}
Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip~H.S. Torr.
\newblock Manigan: Text-guided image manipulation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Qi, Torr, and Lukasiewicz]{lightweight_GAN}
Bowen Li, Xiaojuan Qi, Philip Torr, and Thomas Lukasiewicz.
\newblock Lightweight generative adversarial networks for text-guided image manipulation.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin (eds.), \emph{Advances in Neural Information Processing Systems}, pp.\  22020--22031. Curran Associates, Inc., 2020{\natexlab{b}}.

\bibitem[Liang et~al.(2020)Liang, Pei, and Lu]{CPGAN}
Jiadong Liang, Wenjie Pei, and Feng Lu.
\newblock Cpgan: Content-parsing generative adversarial networks for text-to-image synthesis.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), \emph{Computer Vision -- ECCV 2020}, pp.\  491--508, Cham, 2020. Springer International Publishing.
\newblock ISBN 978-3-030-58548-8.

\bibitem[Liao et~al.(2022)Liao, Hu, Yang, and Rosenhahn]{SSA-GAN}
Wentong Liao, Kai Hu, Michael~Ying Yang, and Bodo Rosenhahn.
\newblock Text to image generation with semantic-spatial aware gan.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  18187--18196, June 2022.

\bibitem[Lin et~al.(2014{\natexlab{a}})Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{coco_dataset}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), \emph{Computer Vision -- ECCV 2014}, 2014{\natexlab{a}}.

\bibitem[Lin et~al.(2014{\natexlab{b}})Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{mscoco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), \emph{Computer Vision -- ECCV 2014}, pp.\  740--755, Cham, 2014{\natexlab{b}}. Springer International Publishing.
\newblock ISBN 978-3-319-10602-1.

\bibitem[Liu et~al.(2021)Liu, Song, Zhu, de~Melo, and Elgammal]{TIME}
Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard de~Melo, and Ahmed Elgammal.
\newblock Time: Text and image mutual-translation adversarial networks.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 35\penalty0 (3):\penalty0 2082--2090, May 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/16305}.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{lrelu}
Andrew~L. Maas, Awni~Y. Hannun, and Andrew~Y. Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{in ICML Workshop on Deep Learning for Audio, Speech and Language Processing}, 2013.

\bibitem[Mao et~al.(2017)Mao, Li, Xie, Lau, Wang, and Paul~Smolley]{lsgan}
Xudong Mao, Qing Li, Haoran Xie, Raymond~Y.K. Lau, Zhen Wang, and Stephen Paul~Smolley.
\newblock Least squares generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}, Oct 2017.

\bibitem[Mirza \& Osindero(2014)Mirza and Osindero]{condtionalgan}
Mehdi Mirza and Simon Osindero.
\newblock Conditional generative adversarial nets, 2014.

\bibitem[Miyato \& Koyama(2018)Miyato and Koyama]{cgans_projections}
Takeru Miyato and Masanori Koyama.
\newblock cgans with projection discriminator, 2018.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{sn_gan}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1QRgziT-}.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{imporved_ddpm}
Alex Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.09672}.

\bibitem[Nilsback \& Zisserman(2008)Nilsback and Zisserman]{flower_dataset}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{Indian Conference on Computer Vision, Graphics and Image Processing}, Dec 2008.

\bibitem[Odena et~al.(2017)Odena, Olah, and Shlens]{acgan}
Augustus Odena, Christopher Olah, and Jonathon Shlens.
\newblock Conditional image synthesis with auxiliary classifier {GAN}s.
\newblock In Doina Precup and Yee~Whye Teh (eds.), \emph{Proceedings of the 34th International Conference on Machine Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\  2642--2651. PMLR, 06--11 Aug 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems 32}, pp.\  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Qiao et~al.(2019{\natexlab{a}})Qiao, Zhang, Xu, and Tao]{Leica_gan}
Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.
\newblock Learn, imagine and create: Text-to-image generation from prior knowledge.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2019{\natexlab{a}}.

\bibitem[Qiao et~al.(2019{\natexlab{b}})Qiao, Zhang, Xu, and Tao]{MirrorGAN}
Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.
\newblock Mirrorgan: Learning text-to-image generation by redescription.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2019{\natexlab{b}}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language supervision, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.00020}.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and Sutskever]{DALL-E}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock \emph{CoRR}, abs/2102.12092, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.12092}.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{Dalle_2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.06125}.

\bibitem[Razavi et~al.(2019)Razavi, van~den Oord, and Vinyals]{vq_vae_2}
Ali Razavi, Aaron van~den Oord, and Oriol Vinyals.
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2019.

\bibitem[Reed et~al.(2016{\natexlab{a}})Reed, Akata, Mohan, Tenka, Schiele, and Lee]{GAWWN}
Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee.
\newblock Learning what and where to draw, 2016{\natexlab{a}}.

\bibitem[Reed et~al.(2016{\natexlab{b}})Reed, Akata, Yan, Logeswaran, Schiele, and Lee]{reed2016generative}
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
\newblock Generative adversarial text to image synthesis.
\newblock In Maria~Florina Balcan and Kilian~Q. Weinberger (eds.), \emph{Proceedings of The 33rd International Conference on Machine Learning}, volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\  1060--1069. PMLR, 2016{\natexlab{b}}.

\bibitem[Rombach et~al.(2021)Rombach, Blattmann, Lorenz, Esser, and Ommer]{stable_diffusion}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
\newblock High-resolution image synthesis with latent diffusion models, 2021.

\bibitem[Ruan et~al.(2021)Ruan, Zhang, Zhang, Fan, Tang, Liu, and Chen]{DAE-GAN}
Shulan Ruan, Yong Zhang, Kun Zhang, Yanbo Fan, Fan Tang, Qi~Liu, and Enhong Chen.
\newblock Dae-gan: Dynamic aspect-aware gan for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp.\  13960--13969, October 2021.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Ayan, Mahdavi, Lopes, Salimans, Ho, Fleet, and Norouzi]{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S.~Sara Mahdavi, Rapha~Gontijo Lopes, Tim Salimans, Jonathan Ho, David~J Fleet, and Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language understanding, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.11487}.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford, and Chen]{IS_score}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Proceedings of the 30th International Conference on Neural Information Processing Systems}, NIPS'16, pp.\  2234–2242, Red Hook, NY, USA, 2016. Curran Associates Inc.
\newblock ISBN 9781510838819.

\bibitem[Schuster \& Paliwal(1997)Schuster and Paliwal]{bi-lstm}
M.~Schuster and K.K. Paliwal.
\newblock Bidirectional recurrent neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 45\penalty0 (11):\penalty0 2673--2681, 1997.
\newblock \doi{10.1109/78.650093}.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{denosing_diffusion}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In Francis Bach and David Blei (eds.), \emph{Proceedings of the 32nd International Conference on Machine Learning}, volume~37 of \emph{Proceedings of Machine Learning Research}, pp.\  2256--2265, Lille, France, 07--09 Jul 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/sohl-dickstein15.html}.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  2818--2826, 2016.

\bibitem[Tan et~al.(2021)Tan, Liu, Liu, Yin, and Li]{KT-GAN}
Hongchen Tan, Xiuping Liu, Meng Liu, Baocai Yin, and Xin Li.
\newblock Kt-gan: Knowledge-transfer generative adversarial network for text-to-image synthesis.
\newblock \emph{IEEE Transactions on Image Processing}, 30:\penalty0 1275--1290, 2021.
\newblock \doi{10.1109/TIP.2020.3026728}.

\bibitem[Tan et~al.(2022{\natexlab{a}})Tan, Liu, Yin, and Li]{CSM-GAN}
Hongchen Tan, Xiuping Liu, Baocai Yin, and Xin Li.
\newblock Cross-modal semantic matching generative adversarial networks for text-to-image synthesis.
\newblock \emph{IEEE Transactions on Multimedia}, 24:\penalty0 832--845, 2022{\natexlab{a}}.
\newblock \doi{10.1109/TMM.2021.3060291}.

\bibitem[Tan et~al.(2023{\natexlab{a}})Tan, Liu, Yin, and Li]{DR-GAN}
Hongchen Tan, Xiuping Liu, Baocai Yin, and Xin Li.
\newblock Dr-gan: Distribution regularization for text-to-image generation.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 34\penalty0 (12):\penalty0 10309--10323, 2023{\natexlab{a}}.
\newblock \doi{10.1109/TNNLS.2022.3165573}.

\bibitem[Tan et~al.(2023{\natexlab{b}})Tan, Yin, Wei, Liu, and Li]{ALR-GAN}
Hongchen Tan, Baocai Yin, Kun Wei, Xiuping Liu, and Xin Li.
\newblock Alr-gan: Adaptive layout refinement for text-to-image synthesis.
\newblock \emph{IEEE Transactions on Multimedia}, 25:\penalty0 8620--8631, 2023{\natexlab{b}}.
\newblock \doi{10.1109/TMM.2023.3238554}.

\bibitem[Tan et~al.(2022{\natexlab{b}})Tan, Lee, Neo, and Lim]{SSTIS}
Yong~Xuan Tan, Chin~Poo Lee, Mai Neo, and Kian~Ming Lim.
\newblock Text-to-image synthesis with self-supervised learning.
\newblock \emph{Pattern Recognition Letters}, 157:\penalty0 119--126, 2022{\natexlab{b}}.
\newblock ISSN 0167-8655.
\newblock \doi{https://doi.org/10.1016/j.patrec.2022.04.010}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0167865522001064}.

\bibitem[Tan et~al.(2023{\natexlab{c}})Tan, Lee, Neo, Lim, and Lim]{SSBI}
Yong~Xuan Tan, Chin~Poo Lee, Mai Neo, Kian~Ming Lim, and Jit~Yan Lim.
\newblock Text-to-image synthesis with self-supervised bi-stage generative adversarial network.
\newblock \emph{Pattern Recognition Letters}, 169:\penalty0 43--49, 2023{\natexlab{c}}.
\newblock ISSN 0167-8655.
\newblock \doi{https://doi.org/10.1016/j.patrec.2023.03.023}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0167865523000880}.

\bibitem[Tao et~al.(2020)Tao, Wu, Zhang, and Wang]{DFGAN}
Ming Tao, Songsong Wu, Xiaofeng Zhang, and Cailing Wang.
\newblock Dcfgan: Dynamic convolutional fusion generative adversarial network for text-to-image synthesis.
\newblock pp.\  1250--1254, 11 2020.
\newblock \doi{10.1109/ICIBA50161.2020.9277299}.

\bibitem[Tao et~al.(2022)Tao, Tang, Wu, Jing, Bao, and Xu]{DF_GAN_CVPR}
Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu.
\newblock Df-gan: A simple and effective baseline for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  16515--16525, June 2022.

\bibitem[Tao et~al.(2023)Tao, Bao, Tang, and Xu]{galip}
Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu.
\newblock Galip: Generative adversarial clips for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  14214--14223, 2023.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and kavukcuoglu]{NDR}
Aaron van~den Oord, Oriol Vinyals, and koray kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie, and Perona]{CUB_dataset}
P.~Welinder, S.~Branson, T.~Mita, C.~Wah, F.~Schroff, S.~Belongie, and P.~Perona.
\newblock {Caltech-UCSD Birds 200}.
\newblock Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.

\bibitem[Xu et~al.(2018)Xu, Zhang, Huang, Zhang, Gan, Huang, and He]{AttnGAN}
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
\newblock Attngan: Fine-grained text to image generation with attentional generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2018.

\bibitem[Ye et~al.(2024)Ye, Wang, Tan, and Liu]{RATGAN}
Senmao Ye, Huan Wang, Mingkui Tan, and Fei Liu.
\newblock Recurrent affine transformation for text-to-image synthesis.
\newblock \emph{IEEE Transactions on Multimedia}, 26:\penalty0 462--473, 2024.
\newblock \doi{10.1109/TMM.2023.3266607}.

\bibitem[Yin et~al.(2019)Yin, Liu, Sheng, Yu, Wang, and Shao]{SDGAN}
Guojun Yin, Bin Liu, Lu~Sheng, Nenghai Yu, Xiaogang Wang, and Jing Shao.
\newblock Semantics disentangling for text-to-image generation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2019.

\bibitem[Zhang et~al.(2017{\natexlab{a}})Zhang, Xu, Li, Zhang, Wang, Huang, and Metaxas]{stackgan++}
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas.
\newblock Stackgan++: Realistic image synthesis with stacked generative adversarial networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, PP, 10 2017{\natexlab{a}}.
\newblock \doi{10.1109/TPAMI.2018.2856256}.

\bibitem[Zhang et~al.(2017{\natexlab{b}})Zhang, Xu, Li, Zhang, Wang, Huang, and Metaxas]{stack_gan}
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris~N. Metaxas.
\newblock Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}, Oct 2017{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Koh, Baldridge, Lee, and Yang]{XMC-GAN}
Han Zhang, Jing~Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang.
\newblock Cross-modal contrastive learning for text-to-image generation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  833--842, June 2021.

\bibitem[Zhang \& Schomaker(2020)Zhang and Schomaker]{DTGAN}
Zhenxing Zhang and Lambert Schomaker.
\newblock {DTGAN:} dual attention generative adversarial networks for text-to-image generation.
\newblock \emph{CoRR}, abs/2011.02709, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.02709}.

\bibitem[Zhang et~al.(2018)Zhang, Xie, and Yang]{hd_gan}
Zizhao Zhang, Yuanpu Xie, and Lin Yang.
\newblock Photographic text-to-image synthesis with a hierarchically-nested adversarial network.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2018.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Chen, Li, Tensmeyer, Yu, Gu, Xu, and Sun]{lafite}
Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun.
\newblock Lafite: Towards language-free training for text-to-image generation.
\newblock \emph{arXiv preprint arXiv:2111.13792}, 2021.

\bibitem[Zhu et~al.(2019)Zhu, Pan, Chen, and Yang]{DMGAN}
Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi~Yang.
\newblock Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2019.

\end{thebibliography}


% \input{main.bbl}

\appendix
% \section{Appendix}

\section{DTE-GAN with MA-GP}

Recent methods \citep{DF_GAN_CVPR,DTGAN,SSA-GAN} have significantly improved quality of synthesised images on COCO dataset \citep{mscoco}. Their improved performance may be associated with the Matching-Aware zero-centered Gradient Penalty (MA-GP) term adopted in these methods. MA-GP is applied to Discriminator with real images and its corresponding text to smooth the loss function that allows the Generator to synthesise more realistic images, incorporating MA-GP into DTE-GAN by changing the Discriminator to conditional Discriminator (towards one-way output for gradient penalty). Mismatch pairs are not used as DTE-GAN has a multi-modal contrastive branch for text-image alignment. For conditional prediction, following similar setup of DF-GAN \citep{DF_GAN_CVPR} of replicating sentence features and concatenating with image features to predict logit values for adversarial loss, the modified adversarial loss function of Discriminator for conditional loss with MA-GP is:

\begin{align}
\begin{split}
L^{Adv}_{D}=&-\mathbb{E}_{x \sim \mathbb{P}_{data}}[\max (0,1-D(x, S_D))] \\
&+\mathbb{E}_{\hat{x} \sim \mathbb{P}_{G}}[\max (0,1+D(\hat{x}, S_D))]\\
&+k \mathbb{E}_{x \sim \mathbb{P}_{data}}\left[\left(\left\|\nabla_{x} D(x, S_D)\right\|+\left\|\nabla_{S_D} D(x, S_D)\right\|\right)^{p}\right] \\
\end{split}
\end{align}

Here, \textit{k} and \textit{p} are hyper-parameters (we use the same hyper-parameter values from DF-GAN \citep{DF_GAN_CVPR}). For training discriminator-side word embeddings and their sentence encoder from real image-text pairs, we do not update the weights using the gradient of fake conditional prediction from the adversarial loss. Conditional Adversarial loss for Generator is:
\begin{align}
L^{Adv}_{G}=&-\mathbb{E}_{\hat{x} \sim \mathbb{P}_{G}}[D(\hat{x}, S_D)]    
\end{align}

The final objective function for the Generator and Discriminator is defined as:
 
 \begin{align}
     \mathcal{L}_{G} &= \mathcal{L}_{Adv}^{G} +\lambda_1 \mathcal{L}_{CA} + \lambda_2 \mathcal{L}_{\text{cont}}^{G} \\
     \mathcal{L}_{D} &= \mathcal{L}_{GAN}^{D} + \lambda_3 \mathcal{L}_{\text{cont}}^D 
 \end{align}

\section{Text Encoding Scheme}

We have used a single-stage Bi-LSTM \cite{bi-lstm} for text encoding, following the popular DAMSM \cite{AttnGAN} embeddings commonly employed in lightweight GAN models \cite{AttnGAN,DMGAN,SSA-GAN,DFGAN}. The DAMSM embeddings are trained to learn discriminative features by distinguishing between instances, ensuring a fair comparison focused on design principles rather than simply increasing the number of parameters. Additionally, we have conducted an ablation study by replacing the Bi-LSTM \citep{bi-lstm} with a 4-layer Transformer encoder \cite{transformers} in both the generator and discriminator text encoders, and we have reported the results in Table \ref{tab:bi_lstm vs Transformer}. 

\begin{table}[!ht]
\centering
\begin{tabular}{ccccc}
\toprule
                 \textbf{Dataset} & \textbf{Encodings}  & \textbf{IS} $\uparrow$ & \textbf{FID} $\downarrow$ & \textbf{R \%} $\uparrow$\\
\midrule
\multirow{2}{*}{\textbf{CUB}} & \textbf{Bi-LSTM}  & $5.12$ & $13.67$ & $86.64$\\
%  \cline{2-5}
                     
                     & \textbf{Transformer Encoder} & $\color{red}5.19$ & $\color{red}13.12$ &  $\color{red}87.9$   \\
\midrule
\multirow{2}{*}{\textbf{Oxford}} & \textbf{\textbf{Bi-LSTM}}  &  $4.21$  & $30.07$ & $83.19$  \\
%  \cline{2-5}        
                     
                     & \textbf{Transformer Encoder} & $\color{red}4.27$  & $\color{red}29.61$ & $\color{red}83.94$  \\
\bottomrule
\end{tabular}
\caption{We compare quality of T2I generation using Bi-LSTM and 4 layer Transformer Encoder text encoding scheme and report the results on CUB and Oxford-102 dataset.}
\label{tab:bi_lstm vs Transformer}
\end{table}

 \section{Details of the Proposed Architecture}
\label{sec:intro_arch}

In this section, we elaborate internal architecture details of the DTE-GAN. Proposed model is implemented using Pytorch \citep{NEURIPS2019_9015} framework. DTE-GAN architecture consists of a dual text embedding setup (Section \ref{sec:dualtext}), a single-stage Generator (Section \ref{sec:gen}) and a Discriminator (Section \ref{sec:disc}). 
%DTE-GAN + MA-GP achieves similar performance as SSA-GAN \citep{SSA-GAN} 


\subsection{Dual Text Embeddings}
\label{sec:dualtext}
% \vspace{-0.3cm}

In the Dual Text Embeddings setup, bi-Directional LSTM \citep{bi-lstm} are used as text encoder both generator-side and discriminator-side. For each direction in the LSTM, hidden layer size is set as $128$. The size of word embeddings $W_D$ and $W_G$ is set to $256$. The sentence embeddings $S_G$, $S_D$ are encoded from the output of last hidden state of respective text encoders. For both the text encoders, sentence embedding size is set to $256$.  

\begin{figure}[t]
    \centering
    \includegraphics[scale = 0.40]{images/UpBlock_AAAI.drawio.pdf}
    \caption{UpBlock used in Generator of DTE-GAN.}
    \label{Fig:UpBlock}
\end{figure}

\subsection{Generator}
\label{sec:gen}

Single-stage generator $G$ is used to generate $256\times 256$ resolution images with base channel dimension of $64$. Details of $G$'s architecture are shown in Table \ref{tab:Gen_arch}. The generator $G$ takes noise $z$ along with generator-side sentence embeddings $S_G$ and the discriminator-side sentence embedding $S_D$ and passes them through a set of linear layers followed by a set of upsampling blocks (UpBlocks). UpBlock at each stage is utilised for up sampling spatial features as shown in Figure \ref{Fig:UpBlock}. $S_G$ and $S_D$ are also used to calculate modulation parameters for Conditional Batch Normalisation \citep{SDGAN}. Features are passed through a self modulation convolution and a $1 \times 1$ convolution resulting in generation of final image of dimension $3 \times 256 \times 256$

\begin{table*}[h!]
    \centering
    \begin{tabular}{c}
         \toprule
         \midrule
         \makecell[c]{$z \epsilon \mathbb{R}^{100}$  $\sim$ $\mathcal{N}(0,I)$ , $S_G$ $\epsilon$ $\mathbb{R}^{256}$, $S_D$  $\epsilon$ $\mathbb{R}^{256}$, \\$W_G$  $\epsilon$ $\mathbb{R}^{256}$, $W_D$  $\epsilon$ $\mathbb{R}^{256}$}\\
         \midrule
         Linear(512) $\longrightarrow$ $512$ \\
         \midrule
         Conditional Augmentation(512) $\longrightarrow$ $200$ \\
         \midrule
         Linear(200+100) $\longrightarrow$ $(8*ch) \times 4 \times 4$ \\
         \midrule
         UpBlock $\longrightarrow$ $(8*ch) \times 8 \times 8$  \\
         \midrule
         UpBlock $\longrightarrow$ $(8*ch) \times 16 \times 16$  \\
         \midrule
         UpBlock $\longrightarrow$ $(4*ch) \times 32 \times 32$  \\
         \midrule
         UpBlock $\longrightarrow$ $(2*ch) \times 64 \times 64$  \\
         \midrule
         UpBlock $\longrightarrow$ $(2*ch) \times 128 \times 128$  \\
         \midrule
         UpBlock $\longrightarrow$ $ch \times 256 \times 256$  \\
         \midrule
         Self Modulation Convolution $\longrightarrow$ $ch \times 256 \times 256$  \\
         \midrule
         $1 \times 1$ Convolution $\longrightarrow$ $3 \times 256 \times 256$  \\
         \midrule
         \bottomrule
    \end{tabular}
    \caption{Generator architecture of DTE-GAN. Base channel dimension \it{ch} = $64$.}
    \label{tab:Gen_arch}
\end{table*}




\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.50]{images/DownBlock.drawio.pdf}
    \caption{DownBlock used in Discriminator of DTE-GAN.}
    \label{Fig:DownBlock}
\end{figure}

\subsection{Discriminator}
\label{sec:disc}
% \vspace{-0.3cm}

 Discriminator $D$ is utilised to provide adversarial loss and also act as a feature extractor for multi-modal contrastive loss (as shown in Table \ref{tab:Disc_arch}). Unlike multiple / multi-stage discriminator setup, presented model with a single discriminator, is easy to train and not having a cumbersome training procedure. The discriminator $D$ takes image of dimension $3 \times 256 \times 256$ and passes it through a set of down sampling blocks (DownBlocks- as shown in Figure \ref{Fig:DownBlock}) followed by two branches, one for the adversarial loss and the other multi-modal contrastive loss.

\begin{table*}[h!]
    \centering
    \begin{tabular}{c|c}
         \toprule
         \midrule
         \multicolumn{2}{c}{RGB images $3\times256\times256$, $S_D$ $\epsilon$ $\mathbb{R}^{256}$, $W_D$ $\epsilon$ $\mathbb{R}^{256}$ } \\
         \midrule
         \multicolumn{2}{c}{DownBlock $\longrightarrow$ $ch \times 128 \times 128$} \\
         \midrule
         \multicolumn{2}{c}{DownBlock $\longrightarrow$ $(2*ch) \times 64 \times 64$} \\
         \midrule
         \multicolumn{2}{c}{DownBlock $\longrightarrow$ $(4*ch) \times 32 \times 32$} \\
         \midrule
         \multicolumn{2}{c}{DownBlock $\longrightarrow$ $(4*ch) \times 16 \times 16$} \\
         \midrule
         \multicolumn{2}{c}{DownBlock $\longrightarrow$ $(4*ch) \times 8 \times 8$} \\
         \midrule
         DownBlock $\longrightarrow$ $(8*ch) \times 4 \times 4$ & DownBlock $\longrightarrow$ $(8*ch) \times 4 \times 4$ \\
         \midrule
         ResBlock $\longrightarrow$ $(8*ch) \times 4 \times 4$ & ResBlock $\longrightarrow$ $(8*ch) \times 4 \times 4$ \\
         \midrule
         Linear($(8*ch) \times 4 \times 4$) $\longrightarrow$ $1$ & Linear($(8*ch) \times 4 \times 4$) $\longrightarrow$ $256$ \\
         \midrule
         Adversarial Loss & Multi-Modal Contrastive Loss \\
         \midrule
         \bottomrule

    \end{tabular}
    \caption{Discriminator architecture of DTE-GAN. Base channel dimension \it{ch} = $64$.}
    \label{tab:Disc_arch}
\end{table*}

\subsection{Implementation Details}
Implementation of the models is done using the PyTorch framework \citep{NEURIPS2019_9015} and optimising the network using Adam optimiser \citep{Adam} with the following hyper parameters: $\beta_1 = 0.5$, $\beta_2 = 0.999$, batch size = $24$, learning rate = $0.0002$, $\lambda_1 = 1$, $\lambda_2 = 1$ and $\lambda_3 = 1$. Spectral Normalisation \citep{sn_gan} is used for all convolutions and fully connected layers in generator and discriminator. The model is trained for 600 epochs on CUB and Oxford-102 datasets (takes $\sim$4 days in 2 NVIDIA 1080Ti GPUs) and 120 epochs for COCO dataset (takes $\sim$7 days in 2 NVIDIA 1080Ti GPUs). During inference, we report results with exponential moving average weights, with a decay rate of 0.999. For R-precision, we obtain text features from D-side Bi-LSTM sentence encoder and image features from discriminator network.

\end{document}
