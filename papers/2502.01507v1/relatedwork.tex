\section{Related Work}
% \vspace{-0.2cm}
In this section, some of the relevant works in the literature relating to this paper are discussed briefly.

\noindent\textbf{Generative Adversarial Networks:}  In past few years, GANs \citep{GAN_2014} had been the go-to method for generating images and class-specific images \citep{condtionalgan,acgan,cgans_projections} on small datasets such as MNIST \citep{MNIST} and CIFAR \citep{cifar}. However, GAN training is highly unstable when used to generate images on large datasets such as ImageNet \citep{imagenet}. Researchers have explored to fix this training instability by re-framing  GAN loss and regularisation \citep{wgan,I_wgan,lsgan,sn_gan,orthogonal_regularisation} to generate high-resolution images on large datasets \citep{progressive_gan,big_gan}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/Dte_arch.pdf}
    %Model_achitecute.drawio.pdf}
    \vspace{-0.2cm}
    \caption{Overview of DTE-GAN architecture. DTE-GAN consists of three core components: i) a single-stage generator $G$ (Section \ref{sec:generator}), ii) a discriminator $D$ (Section \ref{sec:discriminator}), and iii) a dual text embedding setup (Section \ref{sec:dualtextembed}). In the Figure, $W_G$ = generator-side word embeddings, $S_G$ = generator-side sentence embedding, $W_D$ = discriminator-side word embeddings, $S_D$ = discriminator-side sentence embedding. The model is optimised using two objective functions: 1) adversarial loss, and 2) multi-modal contrastive loss.}
    \label{fig:modelarch}
    % \vspace{-0.9cm}
\end{figure*}

\noindent \textbf{Text-to-Image synthesis:} GANs conditioned on global sentence-level embeddings are known to generate meaningful images at low resolutions \citep{GAWWN,tacgan,reed2016generative}. StackGAN \citep{stack_gan} generates high-resolution images in stage-wise approach, where the generator at each stage is conditioned by the image generated from the previous stage. Unlike StackGAN, HDGAN \citep{hd_gan} trains a single generator and multiple discriminators for each resolution. 
AttnGAN \citep{AttnGAN} uses text embeddings to fine-tune image features and also introduces a multimodal contrastive loss (DAMSM loss) to bridge the gap between generated images and words. DM-GAN \citep{DMGAN} refines words and image features using a memory module. MirrorGAN \citep{MirrorGAN} generates a caption for the generated images that improves the text \textit{vs.} image semantic consistency. SD-GAN \citep{SDGAN} introduces a Siamese structure for the generator that uses Conditional Batch Normalization (CBN) \citep{CBN} to improve the alignment of text-image. CPGAN \citep{CPGAN} learns a memory-attended text encoder by attending to salient features in images for each word and fine-grained discriminator \citep{control_gan}.  DTGAN \citep{DTGAN} applies channel and spatial attention, conditioned on sentence vector to focus on important features for each textual representation. XMC-GAN \citep{XMC-GAN} maximises the mutual information between text and image using intra-modality and inter-modality contrastive losses. DF-GAN \citep{DF_GAN_CVPR} uses deep affine transformed global sentence embedding to condition the geand theator and the matching-aware discriminator. 
In this space of text-to-image semantic alignment-based methods, pre-trained embeddings are an inherent prerequisite. These embeddings are only trained by the discriminative approach. Unlike these methods, our apporach (DTE) attempts to learn text embeddings that capture generative and discriminative properties.  

\noindent \textbf{Generative embedding learning:} Some methods attempt to learn embeddings (text / visual) end-to-end as part of a generator. For example, \citep{NDR,VQGAN} learn discrete embeddings for visual representation and show substantial improvement in the performance of text-to-image synthesis \citep{DALL-E,CogView}. Further better and compact representation are learned to improve the quality of image generation \citep{vq_vae_2,res_quatizer,make_a_scene}. Unlike these works that consider only the generation process while learning embeddings, DTE explores the capture of different perspectives of generation and discrimination process by learning dual text embeddings.

\noindent \textbf{Large Scale Text-to-Image Synthesis:} Denoising Diffusion Probabilistic models \citep{denosing_diffusion} have currently achieved remarkable success in image generation \citep{ddpm,imporved_ddpm,diffusion_beats_GAN} by reversing the \textit{forward Markovian process} with noise removal in multiple steps. Though diffusion-based models are able to generate images with complex and varied interactions for text and generate high-quality images \citep{Dalle_2,imagen,Vq_diffusion}, these approaches require large-scale training and exploit pre-trained discriminative language models like CLIP \citep{clip}. Further, CLIP-based language and image encoders are used as a bootstrapping approach for predicting conditional representations in large-scale GAN-based approaches \citep{galip, scaling_up_gan, lafite}. Unlike CLIP, which is trained to capture text-image alignment only, DTE is proposed to learn text-image alignment and text representation toimprove image realism. 

% \vspace{-0.4cm}