\section{Introduction}
Large Language Models (LLMs) present an opportunity to transform education by offering ubiquitous access to individualized tutoring~\citep{jurenka2024towards}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/skills_new2.pdf}
    \caption{
    Effective teaching requires various skills which we categorize into expertise, student understanding, and pedagogical ability. \mathtutorbench\ evalutes these according to the tasks shown in the outer ring.
    }
    \label{fig:skills-figure}
\end{figure}
While these models excel at generating correct answers~\cite{wei2022chain,achiam2023gpt}, experienced teachers 
help students think for themselves and do not just give away answers to make learning effortless~\cite{sharma2024towards}.
Teaching involves a combination of skills including subject expertise, the ability to diagnose and correct student mistakes, and the application of sound pedagogical techniques. For example, teachers need to know when to withhold answers from students, use Socratic questioning~\cite{anghileri2006scaffolding}, or how to engage them cognitively in problem solving~\cite{chi2014icap,kapur2016examining}.
Therefore, a crucial element of building LLM tutors is their evaluation; it is critical to understand whether their guidance is helpful to prevent harm, and to guide progress in future model development.

Yet, current evaluation practices do not meet these criteria.
On the one hand, automatic metrics usually evaluate tutoring models by measuring the word overlap between a ground-truth response and a generated response~\cite{beasharedtask2023}, or focus exclusively on question-answering performance~\cite{tutorchat24}. This is fast but arguably fails to capture the intricacies of tutoring.
Although human evaluation might be a way to capture these nuances by defining suitable criteria to capture them~\citep{tack2022ai,mrbench2024}, it is expensive. %
Importantly, it can only create a snapshot of current performance and cannot be used to evaluate or compare to future models. 


In this work, we fill this gap by releasing \mathtutorbench, a collection of datasets and metrics to holistically evaluate dialog tutoring models for math tutoring. Teaching is a complex and multifaceted task that extends beyond subject mastery~\cite{nye2014autotutor,beasharedtask2023,bridge24}.
Therefore, \mathtutorbench\ is divided into three categories: \emph{math expertise} which evaluates the subject-matter expertise of the tutor, \emph{student understanding} which evaluates the tutor's ability to verify, locate and correct student solutions, and \emph{teacher response generation} which evaluates the scaffolding abilities of the tutor.
Math expertise and student understanding are evaluated based on standard metrics, and we propose a novel metric for evaluating teacher response generation.
In particular, we train a small and quick-to-run reward model by contrasting effective and less effective tutor utterances in terms of structured scaffolding guidance with questions and hints instead of giving away the answer~\citep{anghileri2006scaffolding}. 
The reward model is then used to score tutor model generations. 
We show that this metric is reliable by showing that it can distinguish utterances from expert teachers from those stemming from novice teachers~\citep{bridge24} with high accuracy.

We evaluate various open- and closed-weight state-of-the-art LLMs and specialized tutoring models on \mathtutorbench.
Our results show that there is a trade-off between subject expertise and pedagogical abilities that is dependent on the degree of specialization of a tutoring LLM.
Specializing an LLM for pedagogy comes at the cost of solving ability and, conversely, a high solving accuracy often means that the LLM lacks pedagogy.
Still, more specialized tutoring models tend to retain their teaching abilities even further into a dialog with a student, while general models quickly become worse.
With this, our work contributes to accelerating the development of tutoring LLMs by providing a holistic benchmark that can be evaluated quickly and fairly using automatic metrics.
We release our code and data publicly to promote open research on tutoring LLMs.

