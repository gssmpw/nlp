\section{Related Work}
\subsection{LLM-Based Dialog Tutoring}
A good tutor should scaffold student learning in a structured way rather than just provide correct answers. 
Current approaches to dialog tutoring using LLMs try to achieve this by differnent means: prompt-based elicitation of pedagogical behavior, finetuning models on pedagogical conversations, and alignment with pedagogical preferences. 

First, most existing works use LLMs with a carefully chosen prompt which enumerates desired pedagogical behavior. Bridge~\cite{bridge24} analyzes the teacher behavior and proposes to structure the prompt into a sequence of decisions, similar to real teachers, first to infer the error type and then to determine the pedagogical intent. Other works mostly directly write extensive prompts~\cite{class2023,kargupta-etal-2024-instruct}. However, defining such prompts is tedious, sensitive to small changes and difficult to test~\cite{jurenka2024towards}.

Second, several approaches finetune models on real or mostly synthetically generated data.
SocraticLM~\cite{socraticlm2024} uses a GPT-4 judge to evaluate the quality of teacher guidance using correctness and Socratic principles. A similar approach is to role-model teacher and student conversations based on textbook data~\cite{tutorchat24,book2dial2024}. MathDial~\cite{mathdial2023} is one of the few works that use teachers' utterances when interacting with students to finetune models. However, it is expensive to collect such data on a larger scale. Therefore, LearnLM~\cite{team2024learnlm}
uses an empirically validated mixture of synthetic and teacher-created datasets. However, for capturing high-quality tutoring, teacher-created data is essential and therefore upweighted in their final data mix.
Finally, LLMs can be aligned for pedagogical preferences during post-training~\cite{team2024learnlm}, because these are usually tacit. However, no datasets are openly available or they rely on larger models such as GPT-4 as a judge which limits its generalizability.

Our benchmark contributes an important missing ingredient in the development of LLM-based tutors -- the ability to quickly evaluate and compare models on key pedagogical aspects. 



\subsection{Automatic \& Human Evaluation}
Several works rely on automatic NLG metrics %
such as BLEU~\cite{papineni2002bleu} or BERTScore~\cite{Zhang2020BERTScore} for evaluation which require human-annotated ground truths. However, since tutoring has the goal of helping students learn %
~\cite{opportunities2023}, it is %
very open-ended and there is no single best pedagogical approach at each turn~\cite{jurenka2024towards}. This results in noisy and unreliable scores from automatic metrics~\cite{beasharedtask2023}. There exists an educational-specific classifier of active teacher listening~\cite{uptake2021}, however, it is limited to only this one dimension of teaching and does not account for the entire dialog history.
Therefore, recent finetuned tutoring LLMs~\cite{tutorchat24,socraticlm2024} rely on GPT-4-as-a-judge on dimensions like helpfulness and presentation. Some works on reasoning~\cite{mathchat2024} also focus on multi-turn model abilities judged by GPT-4 but they lack an educational focus. %


Pedagogical quality annotation requires hiring teachers, but it is time-consuming and hard to compare across trials. Two papers recently addressed the issue~\cite{tack2022ai,mrbench2024} by providing evaluation taxonomies but only present one-off static snapshots of current models' performance without the possibility to automatically evaluate new models. 
Finally, measuring student learning gains directly focuses on the end goal. However, learner studies are costly, time-consuming~\cite{schmucker2024ruffle}, and with strict ethics and privacy requirements. There is a growing interest in designing proper evaluation guidelines~\cite{beasharedtask2023,jurenka2024towards}, however, there is still a need for a unified automatic evaluation for scalable model development. 

\mathtutorbench\ addresses the limitations of existing automatic metrics by focusing specifically on tutoring, is simple to run, replicable, and could serve as a proxy for deciding which models to focus on in human studies. Moreover, our benchmark only uses data collected from real teachers.




\begin{table*}[h]
    \centering
    \resizebox{1.\linewidth}{!}{
        \begin{tabular}{l|cc|ccc|cc}
            & \multicolumn{2}{c}{\textbf{Math Expertise}} & \multicolumn{3}{c}{\textbf{Student Understanding}} & \multicolumn{2}{c}{\textbf{Teacher Response Generation}} \\
            \hline
             \textbf{Task(s)} & Problem  & Socratic  & Solution  & Mistake  & Mistake  & \multicolumn{1}{c}{Scaffolding Gen.,} & \multicolumn{1}{c}{Scaffolding Gen.,} \\
             & solving & questioning &  correctness &  location &  correction & \multicolumn{1}{c}{Ped. instr. following} & \multicolumn{1}{c}{Ped. instr. following}  \\
            \hline
            \textbf{Dataset} & GSM8k & GSM8k & StepVerify &  StepVerify & StepVerify & MathDialBridge & MathDialBridge[hard]  \\
             \textbf{Input} & $\vp$ & $\vp$ & $\vp$, $\widehat{\vs}$ & $\vp$, $\widehat{\vs}$ & $\vp$, $\mathcal{H}$ & $\vp$, $\mathcal{H}$ & $\vp$, $\mathcal{H}$  \\
            \textbf{Type} & generation & generation & bin. clas. (bal.) &  multi-cl. & generation & generation & generation  \\
         \textbf{Ground Truth} & $\va$ & $\vq_1,\dots,\vq_N$ & $\mathbbm{1}(e\neq 0)$ & $e$ & $\va$ & $\vu_{teacher}$ & $\vu_{teacher}$   \\
        \textbf{Instances} & 1319 & 1319 & 2004 &  2004 & 1002 & 1150 & 327   \\
        \textbf{Avg. turns} & - & - & - &  - & 3.04 & 3.08 & 5.78   \\
            \hline
        \end{tabular}
    }
    \caption{Datasets used in the benchmark and their statistics. Notation defined in Section~\ref{sec:background}. }\label{tab:datasets_in_benchmark}
\end{table*}