\section{Adaptive LayerNorm conditioning}
\label{sec:adaln}

A visualization of the adaptive layer norm is given in Figure \ref{fig:dit_block}. We use the point cloud and language as primary modality in this visualization. In a Diffusion Transformer (DiT) block visualized in Figure \ref{fig:dit_block}, the most significant difference to a vanilla transformer block is scaling and shifting operations conditioned on the image CLS token. The scaling factors $\alpha$, $\gamma$ and the shifting factor $\beta$ are applied to self-attention and feed-forward part of the DiT block. The expression $\text{AdaLN}(z_t^P, z_t^L | z_t^I)$ indicates that image embedding is used as condition and mapped to factors $\alpha$, $\gamma$ and $\beta$, while the point cloud and language embeddings go through the self-attention and feed-forward blocks with additional scaling and shifting operations by these factors.

