\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduce the Fusion of Point Cloud and Visual representation Network, a novel approach that integrates RGB and point cloud features using AdaLN conditioning within a transformer. By fusing features at each residual connection, our method effectively captures complementary information from both modalities. Extensive experiments on the RoboCasa benchmark demonstrate significant performance gains over baselines, highlighting the importance of thoughtful cross-modal integration. These results open new avenues for exploring advanced fusion strategies to further enhance robotic perception and understanding of complex environments.



% In this work, we introduced FPV, a novel approach that combines the complementary strengths of point cloud and RGB modalities for robotic tasks. Point clouds capture detailed geometric structures of the scene, while RGB images provide rich semantic cues. Our architecture employs a lightweight MLP to encode point cloud data and a ResNet model to extract features from multiple views of the RGB data. These representations are then fused through a transformer-based diffusion policy, where point cloud embeddings flow into the backbone and the RGB features condition the AdaLN layers. When evaluated on 24 tasks from the challenging RoboCasa benchmark, our method outperforms single-modality baselines by a margin of approximately 19.4\%. We also investigated alternative fusion strategies, such as simple concatenation, and found that our proposed approach consistently yields stronger results across the majority of tasks. These findings highlight the crucial role of thoughtful cross-modal integration and open new possibilities for exploring more advanced fusion designs that further enhance a robotâ€™s understanding of its environment.