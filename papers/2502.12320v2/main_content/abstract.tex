% \begin{abstract}
% Point cloud representations have demonstrated efficiency in imitation learning, particularly for robotic manipulation tasks, due to their ability to directly encode geometric structures. However, RGB images provide rich texture and semantic information that can be crucial for certain tasks. While existing works attempt to bridge this gap by assigning 2D image features to point clouds, such approaches often lose global contextual information from the original images. In this work, we propose a novel framework that effectively combines the strengths of both point cloud and RGB modalities. We first conduct controlled experiments to highlight the limitations of using either modality alone, showing that neither is universally optimal across all tasks. To address this, we design a model that aligns and balances point cloud and RGB features, enabling robust multimodal learning. Through extensive experiments on the challenging RoboCasa benchmark, our method achieves state-of-the-art performance across all tasks. This work sets a new standard for multimodal imitation learning, showcasing the effectiveness of leveraging both geometric and visual information.
% \end{abstract}
% This work sets a new standard for multimodal imitation learning, showcasing the effectiveness of leveraging both geometric and visual information.

 % Point cloud (PC) representations have demonstrated efficiency in imitation learning, particularly for robotic manipulation tasks, due to their ability to directly encode spatial structures. In contrast, RGB images provide rich texture and semantic information that can be crucial for certain tasks. Existing methods for combining these modalities attempt to combine them by assigning 2D image features to the corresponding points in the PC. However, such approaches often lose global contextual information from the original images. In this work, we propose UniPerceive-IL, a novel imitation learning framework that effectively combines the strengths of both point cloud and RGB modalities. Through extensive experiments on the challenging RoboCasa benchmark, we highlight the limitations of using either modality alone, and we show that our method has state-of-the-art performance across all tasks. 


% \begin{abstract}
% Point clouds efficiently capture geometric structures, making them essential for manipulation tasks in imitation learning. In contrast, RGB images provide rich texture and semantic information that can be crucial for certain tasks. 
% %
% However, existing methods for fusing these modalities often discard valuable information, for instance, \textcolor{red}{by assigning only a subset of RGB features from the original image to the corresponding points in the point cloud.} 
% %
% In this work, we propose a novel imitation learning method, FPV, that effectively combines the strengths of both, point cloud and RGB modalities. Through extensive experiments on the challenging RoboCasa benchmark, we demonstrate the limitations of relying on either modality alone and show that our method achieves state-of-the-art performance across all tasks.
% \end{abstract}

\begin{abstract}
Learning for manipulation requires using policies that have access to rich sensory information such as point clouds or RGB images. 
Point clouds efficiently capture geometric structures, making them essential for manipulation tasks in imitation learning. In contrast, RGB images provide rich texture and semantic information that can be crucial for certain tasks. 
%
Existing approaches for fusing both modalities assign 2D image features to point clouds. However, such approaches often lose global contextual information from the original images.
%
In this work, we propose FPV-Net, a novel imitation learning method that effectively combines the strengths of both point cloud and RGB modalities. Our method conditions the point-cloud encoder on global and local image tokens using adaptive layer norm conditioning, leveraging the beneficial properties of both modalities. 
Through extensive experiments on the challenging RoboCasa benchmark, we demonstrate the limitations of relying on either modality alone and show that our method achieves state-of-the-art performance across all tasks.
\end{abstract}