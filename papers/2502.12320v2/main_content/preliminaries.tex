% \newpage
\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Problem Formulation}

Imitation learning (IL) aims to train an agent to perform tasks by learning from expert demonstrations. Given a dataset of expert trajectories \(\mathcal{D} = \{(\traj_i)\}_{i=1}^{N}\), where each trajectory \(\traj_i\) consists of a sequence of observations and corresponding expert actions

\begin{equation}
    \traj_i = (\obs_1, \act_1, \obs_2, \act_2, \dots, \obs_K, \act_K),
\end{equation}

the goal is to learn a policy \(\pi(\act|\obs): \mathcal{O} \to \mathcal{A}\) that maps observations to actions in a manner that mimics expert behavior.

% In a \textbf{multi-task imitation learning} setting, the agent must generalize across multiple tasks \(\mathcal{T} = \{T_1, T_2, \dots, T_M\}\). Each task \(T_m\) is associated with a distribution over trajectories \(\mathcal{D}_m\), and the objective is to learn a unified policy that can perform well across all tasks:

% \begin{equation}
%     \pi_\theta^* = \arg\min_{\pi_\theta} \sum_{m=1}^{M} \mathbb{E}_{\traj \sim \mathcal{D}_m} \left[ \mathcal{L}(\pi_\theta(o), a) \right],
% \end{equation}

% where \(\mathcal{L}(\cdot, \cdot)\) denotes a loss function measuring the discrepancy between the predicted and expert actions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multi-Modal Imitation Learning}
In a multi-modal imitation learning framework, the agent receives a multi-modal observation at each time step \(k\) consisting of:

\textbf{Language instruction} (\(\mathbf{x}_k^L\)): Provides high-level task semantics and contextual guidance, enabling the agent to generalize across diverse instructions.

\textbf{RGB image} (\(\mathbf{x}_k^I\)): Captures visual scene information, including object appearances, spatial arrangements, and environmental semantics.

\textbf{Point cloud} (\(\mathbf{x}_k^P\)): Offers a structured 3D representation of the environment, encoding geometric and spatial relationships that are crucial for manipulation.

Thus, an observation in the framework is defined as 
\begin{equation}
    \obs = (\mathbf{x}_k^L, \mathbf{x}_k^I, \mathbf{x}_k^P) \in \mathcal{O},
    % = \mathcal{X}_L \times \mathcal{X}_I \times \mathcal{X}_P.
\end{equation}
where $\mathcal{O}$ denotes the observation space.
%
Building on the success of Action Chunking \cite{zhao2023learning} in Imitation Learning, we formulate the objective as predicting a sequence of future actions
\begin{equation}
    \act = (\act_k, \act_{k+1}, \dots, \act_{k+H}) \in \mathcal{A}^H,
\end{equation}
where \(H\) is the prediction horizon, and $\mathcal{A}$ denotes the action space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Score-Based Diffusion Policies}

FPV-Net adopts the continuous-time denoising diffusion model from EDM \cite{karras2022elucidating} to represent the policy. Denoising diffusion models aim to time-reverse a stochastic noising process that transforms the data distribution into Gaussian noise \cite{song2020denoising}, allowing for generating new samples that are distributed according to the data. In FPV-Net, a score-based diffusion model is used for the policy \(\pi(\act|\obs)\). The denoising process is governed by a stochastic differential equation (SDE) given by
\
\begin{equation}
\small
\label{eq: conditional Karras Song SDE}
\begin{split}
\mathrm d \act =  \big( \beta_t \sigma_t - \dot{\sigma}_t  \big) \sigma_t \nabla_\act \log p_t(\act | \obs)  \mathrm dt + \sqrt{2 \beta_t} \sigma_t \mathrm d B_t,
 \end{split}
\end{equation}

where \(\beta_t\) determines how much noise is injected, $B_t$ denotes a standard Wiener process, and \(p_t(\act | \obs)\) is the score function of the diffusion process which moves samples towards regions of high data density. To generate new samples from noise, one trains a neural network to approximate \(\nabla_\act \log p_t(\act | \obs)\) using Score Matching (SM) \cite{6795935}. The SM objective is
\begin{equation}
\label{eq: denoising score matching loss}
\begin{split}
     \mathcal{L}_{D_{\theta}} = 
     \mathbb{E}_{\mathbf{\sigma_t}, \act, \boldsymbol{\epsilon}} \left[  \alpha (\sigma_t) \| D_{\theta}(\act + \boldsymbol{\epsilon}, \obs, \sigma_t)  - \act  \|_2^2 \right],
     \end{split}
\end{equation}
where $D_{\theta}(\act + \boldsymbol{\epsilon}, \obs, \sigma_t)$ is the trainable network. During training, noise is sampled from a predefined distribution and added to an action sequence. The network then predicts the denoised actions and computes the SM loss.
Once training is complete, new action sequences can be generated by starting from random noise and approximating the reverse SDE in discrete steps using a numerical ODE solver. Specifically, one samples an initial action $\act_t \sim \mathcal{N}(0, \sigma_t^2 I)$ from the prior and progressively denoises it. In FPV-Net, this is accomplished via the DDIM-solver \cite{song2020denoising}, which is an ODE solver tailored for diffusion models that can denoise actions in just a few steps. In all experiments, FPV-Net uses 4 denoising steps.



% \textbf{Reverse Process.}
% The reverse process reconstructs the original action sequence from the noisy version, \textbf{while being conditioned on the observation} \( o_t \). This corresponds to solving the reverse-time SDE:

% \begin{equation}
%     dA_t = \left[ f(A_t, t) - g(t)^2 \nabla_{A_t} \log p_t(A_t | o_t) \right] dt + g(t) d\bar{W}_t,
% \end{equation}

% where:
% \begin{itemize}
%     \item \( \bar{W}_t \) is a reverse Wiener process,
%     \item \( \nabla_{A_t} \log p_t(A_t | o_t) \) is the learned \textbf{conditional score function}, estimated via a neural network.
% \end{itemize}

% \textbf{Denoising Model.}
% The diffusion model learns to denoise the action sequence at different noise levels by parameterizing the \textbf{denoising function} \( f_\theta(A_t^{(k)}, o_t, k) \):

% \begin{equation}
%     p_\theta(A_t^{(k-1)} | A_t^{(k)}, o_t) = \mathcal{N}(A_t^{(k-1)}; f_\theta(A_t^{(k)}, o_t, k), \sigma^2 I)
% \end{equation}

% where \( \sigma^2 \) is the variance schedule.

% This formulation ensures that the predicted action sequence is \textbf{explicitly dependent on the observation} \( o_t \), allowing the model to generate behavior that is conditioned on the environment state.

% \textbf{Training Objective.}
% The model is trained using a simplified denoising score matching (DSM) objective
% \begin{equation}
%     \mathcal{L}(\theta) = \mathbb{E}_{o_t, A_t^{(0)}, \epsilon, t} \left[ \lambda(t) \| \epsilon - \epsilon_\theta(A_t^{(t)}, o_t, t) \|^2 \right].
% \end{equation}
% Here:
% \begin{itemize}
%     \item $\epsilon$ represents the Gaussian noise added during diffusion,
%     \item $\epsilon_\theta$ is the network's prediction of the noise,
%     \item $\lambda(t)$ is a weighting function derived from EDM.
% \end{itemize}

% This framework ensures that the policy effectively learns multi-step action sequences while leveraging the diffusion model's generative capabilities.
