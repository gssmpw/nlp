% \newpage
\section{Method}
\label{sec:model}

Fusion of Point Cloud and Visual representation Network (FPV-Net) is a multi-modal transformer-based diffusion policy which leverages point cloud, image and language inputs. In this section, we introduce how we process these different modalities and propose three different fusion methods to combine point cloud features and image features. An overview of our model is shown in Figures \ref{fig:input_components} and \ref{fig:dit_block}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Language Processing}
% \label{sec:language_processing}

% To incorporate language instructions into the policy, we utilize a frozen CLIP model to extract a fixed-dimensional semantic embedding. Given a language instruction \( x_t^L \), we obtain its representation as:

% \begin{equation}
%     z_t^L = \phi_{\text{CLIP}}(x_t^L), \quad z_t^L \in \mathbb{R}^{d_L},
% \end{equation}

% where:
% \begin{itemize}
%     \item \( \phi_{\text{CLIP}}(\cdot) \) denotes the CLIP text encoder,
%     \item \( z_t^L \) is the resulting **language embedding** of dimension \( d_L \),
%     \item \( x_t^L \) is the raw textual instruction.
% \end{itemize}

% This language embedding serves as a **conditioning signal** for the policy, enabling multi-modal alignment between textual instructions and visual or point cloud inputs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Image Processing}
\label{sec:rgb_processing}
To extract meaningful representations from RGB inputs, we utilize a FiLM-ResNet architecture \cite{perez2018film}, which is conditioned on the language instructions. This approach allows the model to modulate feature extraction based on linguistic context, improving the alignment between vision and language modalities.
Most prior works \cite{chi2023diffusion,zhao2023learning,reuss2024multimodal} in imitation learning extract only a global token from ResNet \cite{he2016deep} feature maps, discarding fine-grained local spatial information. However, we argue that both global and local features are critical for capturing fine-grained visual details necessary for action prediction.
To address this, we extract features as follows:


\textbf{Global Token}: We apply global average pooling over the ResNet feature map to obtain a single global representation.

\textbf{Local Tokens}: Instead of discarding spatial features, we flatten the feature map into a sequence of local tokens, preserving important spatial details.

% \begin{itemize}
%     \item \textbf{Global Token}: We apply global average pooling over the ResNet feature map to obtain a single global representation.
%     \item \textbf{Local Tokens}: Instead of discarding spatial features, we flatten the feature map into a sequence of local tokens, preserving important spatial details.
% \end{itemize}
Finally, we concatenate the global token with the local feature tokens, forming a comprehensive visual representation
% \begin{small}
\begin{equation}
\small
    \mathbf{z}^I_t = \text{Concat}(\text{AvgPool}(F_{\text{ResNet}}(\mathbf{I})), \text{Flatten}(F_{\text{ResNet}}(\mathbf{I}))),
\end{equation}
% \end{small}

where $F_{\text{ResNet}}(\mathbf{I})$ denotes the extracted feature map from FiLM-ResNet.
This enriched representation provides the policy with a multi-scale visual understanding, ensuring that both high-level semantics and fine-grained local details contribute to decision-making. The illustration of the image processing can be found in Figure \ref{fig:input_components}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Point Cloud Processing}
\label{sec:pc_processing}

Prior approaches in 3D imitation learning, such as 3D Diffusion Policy (DP3) \cite{ze20243d} and 3D Diffuser Actor (3DA) \cite{ke20243d}, suffer from key limitations. DP3’s max pooling discards local geometric features, while 3DA’s 2D feature lifting loses global contextual information from original images. Moreover, 3DA generates an excessive number of point tokens, leading to higher computational costs.
%
To effectively process a point cloud \( \mathbf{x}_t^P \in \mathbb{R}^{N \times 3} \) consisting of \( N \) points in 3D space, we construct a structured representation as follows:

\textbf{Furthest Point Sampling (FPS)} \cite{eldar1994farthest, qi2017pointnet}: We sample \( M = 256 \) center points, ensuring a coverage of the global geometric structure.

\textbf{k-Nearest Neighbors (KNN)} \cite{qi2017pointnet++}: For each center point, we retrieve its \( K = 32 \) nearest neighbors, forming local point groups that capture fine-grained spatial structures.

% \begin{itemize}
%     \item \textbf{Furthest Point Sampling (FPS)}: We sample \( M = 256 \) center points, ensuring a coverage of the global geometric structure.
%     \item \textbf{k-Nearest Neighbors (KNN)}: For each center point, we retrieve its \( K = 32 \) nearest neighbors, forming local point groups that capture fine-grained spatial structures.
% \end{itemize}

Each local point group is encoded into a latent representation using a point cloud encoder \( \psi_\theta \). The final point cloud embedding is represented as
\begin{equation}
    \mathbf{z}_t^P = \{ \psi_\theta(\textbf{G}_m) \}_{m=1}^{M}, \quad \mathbf{z}_t^P \in \mathbb{R}^{M \times d},
\end{equation}

where $\textbf{G}_m \in \mathbb{R}^{K \times 3}$ represents the $K$-neighbor subset for the $m$-th sampled center, $\psi_\theta(\cdot)$ is the point cloud encoder that extracts a per-group embedding, and $\mathbf{z}_t^P$ consists of $M = 256$ tokens, each of dimension $d$.
%
By structuring the point cloud representation into a tokenized format, our approach preserves both local fine-grained features and global contextual information, ensuring a more expressive representation for 3D imitation learning.
We explore two different point cloud encoding strategies:

\textbf{Lightweight MLP Encoder}: Inspired by 3D Diffusion Policy \cite{ze20243d}, we use a multi-layer perceptron (MLP) followed by a max pooling layer to process each point group independently. This method is computationally efficient and preserves local structures.

\textbf{Pretrained SUGAR Model}: We leverage a pretrained point cloud encoder, SUGAR \cite{Chen_2024_SUGAR}, to extract richer and more informative features, benefiting from knowledge gained in large-scale 3D datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Single-Modality Policy}

% In addition to multi-modal fusion, we also investigate a **single-modality policy**, where the agent learns from either **RGB** or **point cloud** inputs separately. 

% We adopt a \textbf{Transformer-based diffusion policy}, which follows the same **diffusion framework** as our multi-modal model. The architecture details can be found in Figure~\ref{fig:architecture}.

% \subsubsection{Input Modality Variants}
% We consider two configurations:
% \begin{itemize}
%     \item \textbf{RGB-Only Policy}: The policy network processes RGB images as input while disregarding point cloud data.
%     \item \textbf{Point Cloud-Only Policy}: The policy network takes only point cloud inputs without RGB features.
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{figure_and_table/robo_casa/robocasa_new}

% \subsection{Modality Fusion Policy}
\subsection{Fusing Multi-Modal Embeddings}
\label{sec:modality_fusion}

To effectively integrate multi-modal observations, including RGB images, point clouds, and language embeddings, we explore three different fusion strategies for combining image and point cloud features. In the following, other than the image embedding $ \mathbf{z}_t^I$ and the point cloud embedding $\mathbf{z}_t^P$, we use $\mathbf{z}_t^L \in \mathbb{R}^{d_L}$ to denote language embeddings, which are obtained via the frozen CLIP model \cite{radford2021learningtransferablevisualmodels}.

\subsubsection{Concatenation-Based Fusion}
A straightforward approach is to directly concatenate the embeddings of these three modalities and use it as input for the transformer policy. This fused representation $\mathbf{z}_t^{\text{fusion}}$ can be written as

\begin{equation}
    \mathbf{z}_t^{\text{fusion}} = \text{Concat}(\mathbf{z}_t^I, \mathbf{z}_t^P, \mathbf{z}_t^L).
\end{equation}
Although this fusion retains all feature information, it lacks a structured interaction between modalities.

\subsubsection{Adaptive LayerNorm Conditioning}
\label{sec:adaln}
Inspired by the use of Adaptive LayerNorm (AdaLN) layers to condition on classes in DiT models \cite{Peebles2022DiT}, we explore using AdaLN conditioning layers not on language, but on the point cloud or the image modality. In this way, AdaLN conditioning treats one modality as conditioning input and the other modalities as main feature inputs.
The conditioning inputs scale or shift main feature within the attention mechanism

\[
\text{AdaLN}(\mathbf{z} \mid \mathbf{\mathbf{c}}) = \gamma(\mathbf{c}) \odot \frac{\mathbf{z} - \mu(\mathbf{z})}{\sigma(\mathbf{z})} + \beta(\mathbf{c}),
\]

where $\mathbf{z}$ is the main feature, $\mathbf{c}$ is the conditioning input, $\mu(\mathbf{z})$ and $\sigma^2(\mathbf{z})$ are the mean and variance of the main input $\mathbf{z}$, and $\gamma(\mathbf{c})$ and $\beta(\mathbf{c})$ are learnable functions that map the conditioning input to a pair of scale and shift parameters. More details about AdaLN conditioning can be found in Appendix \ref{sec:adaln}.

\paragraph{Image and Language as Main Modality}
In this setup, we select the image embeddings $\mathbf{z}_t^I$ and language embeddings $\mathbf{z}_t^L$ as the primary modality. The AdaLN layers take the point cloud embeddings $\mathbf{z}_t^P$ as conditions to modulate the activation of the primary modality. The fusion is formulated as

\begin{equation}
    \mathbf{z}_t^{\text{fusion}} =  \text{AdaLN}(\mathbf{z}_t^I, \mathbf{z}_t^L | \mathbf{z}_t^P).
\end{equation}

\paragraph{Point Cloud and Language as Main Modality}
Alternatively, we consider using point cloud embedding and language embedding as primary modality and image embedding as conditions

\begin{equation}
    \mathbf{z}_t^{\text{fusion}} =  \text{AdaLN}(\mathbf{z}_t^P, \mathbf{z}_t^L | \mathbf{z}_t^I).
\end{equation}

The observation embedding \(\mathbf{z}_t^{\text{fusion}}\) will then be fed into the transformer-based diffusion policy (Figure \ref{fig:dit_block}).

% Compared to simply concatenating the feature vectors, AdaLN is more suitable for multi-modal representation, where the main modality can be selected based on the task requirements. 
%This design allows for flexible multi-modal representations where the main feature branch can be selected based on task requirements.
%Each fusion method provides a distinct way to integrate vision and point cloud features, with **concatenation** offering simplicity, while **AdaLN conditioning** enables more structured multi-modal interactions.
