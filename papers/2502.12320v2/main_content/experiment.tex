% \newpage
\section{Experiments}
\label{sec:experiments}

We conduct extensive experiments to answer the following questions:
% Our experiments mainly focus on the following questions:

\textbf{Q1)} Is a single modality enough to perform efficiently on challenging environments?

\textbf{Q2)} How does our method compare with state-of-the-art imitation learning policies?

\textbf{Q3)} What kinds of fusion types are most powerful?
% \todo[inline]{Please answer these questions somewhere if you state them here, i.e., reference Q1)-Q3)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{figure_and_table/main_table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Simulations}

\textbf{RoboCasa} \cite{robocasa2024}:
RoboCasa is a large-scale simulation framework designed to train generalist robots in diverse and realistic household environments, with a particular emphasis on complex kitchen tasks. It features 120 meticulously crafted kitchen scenes, over 2,500 high-quality 3D objects across 150 categories, and 100 tasks divided into foundational atomic tasks and intricate composite tasks. Leveraging generative AI tools, RoboCasa achieves unparalleled diversity, realism, and scalability in robotic learning.
This benchmark is characterized by its exceptional difficulty, stemming from the highly diverse scenarios it presents. Each scenario is accompanied by only one demonstration, significantly increasing the challenge for learning algorithms. For instance, in pick-and-place tasks, the object to be manipulated varies across scenarios, with just one demonstration per case. Furthermore, the training and evaluation datasets feature completely distinct scenes, further testing a model’s ability to generalize and adapt robot behaviors to novel scenarios.
With its extensive task set, environmental variability, and high-fidelity simulations, RoboCasa establishes itself as a new standard for evaluating robotic learning methodologies, pushing the boundaries of generalization and adaptability in robot learning.

\textbf{Training and Evaluation}: We train each method for 100 epochs and rollout the models for 50 times for all tasks in RoboCasa. We group similar tasks together as shown in Table \ref{tab:task_groups} and train the models for each of the groups. 

\subsection{Baselines}

\textbf{BC} \cite{robocasa2024}: We inherit the result reported in RoboCasa. RoboCasa uses the BC-Transformer implemented by RoboMimic. The BC policy uses a CLIP model to encode the goal instruction and a ResNet-18 with FilM layers to encode the image-based observations.

\textbf{3D Diffusion Policy (DP3)} \cite{ze20243d}: DP3
extracts point-wise features from single-view points clouds with a MLP-based encoder and forms a compact 3D visual representation. Robot actions are then generated by a convolutional network-based architecture, conditioned on this representation and the current robot states.

\textbf{3D Diffuser Actor (3DA)} \cite{ke20243d}: 3DA is a diffusion-based policy conditioned on 3D scene features and language instructions.
The 3D scene features are extracted and aggregated from single or multi-view images and depth maps. The policy denoises rotation and translation of the robot's end-effector as action.

\subsection{FPV-Net}
\label{sec:proposed_models}

We systematically evaluate how the FPV-Net deals with different modalities while maintaining a consistent architecture and diffusion policy configuration across all experiments. This setup allows us to directly compare the effectiveness of different representations.

\textbf{PC-only}:
We first group the point cloud by selecting 256 centers via Furthest Point Sampling (FPS), then retrieve 32 nearest neighbors using K-Nearest Neighbors (KNN) to form 256 point groups. Each group is passed through a lightweight MLP encoder, obtaining an embedding per group. These embeddings, along with a language embedding from CLIP, a timestep embedding, and the noisy action, are provided to a transformer-based diffusion policy.

\textbf{RGB-only}:
In this model, each camera view is processed by a ResNet-18 model, which is pretrained and then finetuned separately for each view. FiLM layers condition the network on the CLIP-encoded language instruction. The resulting embeddings from all camera views are subsequently given to the same transformer-based diffusion policy employed in the PC-only model.
\input{figure_and_table/ablation/fusion}

\textbf{PC+RGB}:
This variant simply concatenates the point group embeddings from PC-only with the RGB embeddings from RGB-only, and feeds the combined representation into the transformer-based diffusion policy.

\textbf{FPV-MLP}:
% \todo[inline]{What is the c. standing for?}
Here, the point cloud is processed as before, but we additionally exploit local RGB features. Specifically, we use the 8x8 feature map produced by the third ResNet layer for each image. This feature map is flattened and concatenated with the global ResNet embedding, producing 65 tokens per view. Tokens from all views, along with a learnable class token, are passed to a transformer. The output of the class token serves as the condition vector for AdaLN, while the point group embeddings enter the diffusion policy in the usual way. 

\textbf{FPV-SUGAR}:
In this model, we use the point cloud encoder of the pretrained 3D visual representation model SUGAR \cite{Chen_2024_SUGAR}, which also partitions points into 256 groups of 32 via FPS and KNN, but subsequently also employs a 12-layer transformer. We use the model pretrained on multi-object scenes using objects from the Objaverse \cite{objaverse} dataset. To reduce computational cost, we freeze the first 10 layers and finetune only the last 2. The RGB images are processed similarly to FPV-MLP, except that we use the 4x4 feature map from the fourth ResNet layer. Finally, the conditioned transformer-based diffusion policy is applied as before.

\subsection{Main Results}


Table \ref{table:main_results_2d} shows that models utilizing both modalities outperform those using a single modality, which addresses Q1. Simply concatenating point cloud and RGB features leads to a 10\% improvement, illustrating the complementary nature of spatial and semantic information: each modality contributes unique advantages that are not fully captured by the other. Notably, pick-and-place and insertion tasks benefit most from having both modalities, suggesting that both spatial and semantic cues are crucial for manipulating objects unseen during training.
In one particular task the PC-only method performs noticeably better than the other models, namely the \textsc{TurnSinkSpout} task, which requires further investigation.
\input{figure_and_table/ablation/condition_vector}

Our PC-only approach outperforms 3D Diffusion Policy by a margin of 5.25\%, answering Q2. A likely explanation is that the max-pooling step discards spatial information critical to the diffusion policy. By contrast, our approach retains more of the point cloud’s geometric structure. Furthermore, grouping points instead of handling each point separately like DP3 allows our PC-only model to better capture local spatial features.

FPV-MLP and FPV-SUGAR, conditioning on RGB features, offer further gains, yielding an average success rate of around 50\%, higher than the simple concatenation of modalities. This suggests the diffusion policy exploits the rich texture and semantic details from RGB data when using AdaLN for conditioning more effectively than taking these features purely as an additional input. Another possible reason is that the transformer-based diffusion policy can better separate the two modalities, focusing on spatial relations through self-attention over point groups while annotating each group with semantic features via AdaLN conditioning.

3DA exhibits a very low success rate on RoboCasa in our experiments. This may be attributed to our decision to train each model for 100 epochs to ensure a fair comparison. However, as a relatively more complex model, 3DA likely requires a longer training duration to achieve optimal performance.


\subsection{Ablation on different fusion}


We compared the performance of different fusion strategies for integrating point cloud and RGB embeddings within the transformer architecture. Concat. refers to a straightforward concatenation of both embeddings. Cond. on PC denotes using RGB features as the main modality while conditioning on point cloud features through AdaLN conditioning. Conversely, Cond. on RGB treats point cloud features as the primary modality, with RGB features providing the conditioning signal via AdaLN. As shown in Figure \ref{fig:ablation_fusion}, conditioning the RGB-based transformer on point cloud features underperforms compared to simple concatenation. This could be due to compressing the entire point cloud into a single vector, which may discard crucial spatial details, particularly for tasks like \textsc{Coffee}, where precise grasping of a mug is required. In contrast, conditioning on RGB features yields the best performance across most tasks, effectively addressing Q3.

% We explored various strategies for combining the point cloud and RGB embeddings within the transformer. One option is to concatenate both embeddings and provide them jointly to the transformer. Another approach is AdaLN conditioning, as described in Section \ref{sec:adaln}. Figure \ref{fig:ablation_fusion} shows that using point cloud features to condition the RGB-based transformer performs worse than straightforwardly concatenating the two feature sets. This might be because compressing the entire point cloud into a single vector discards the spatial details needed for tasks like \textsc{Coffee}, where a mug must be grasped. On the other hand, conditioning on RGB features achieves the best performance in most tasks, addressing Q3.


\input{figure_and_table/ablation/rgb_features}
\subsection{Ablation on obtaining condition vector}


AdaLN does not directly support sequences as input, so a single token must be extracted to condition on point clouds or RGB features. We compare two methods: (1) a simple max-pooling layer, and (2) a transformer whose learnable class token serves as the global representation. Figure \ref{fig:ablation_cond_vec} indicates that the transformer-based approach consistently outperforms max pooling in all tested tasks.


\subsection{Ablation on RGB features}
In order to identify the influence of global tokens and local tokens from ResNet feature map, we evaluate FPV-Net with different feature granularity: global features versus 4x4 or 8x8 feature maps. The results are presented in Figure \ref{fig:ablation_rgb_features}, which show that by adding local features from ResNet would gain performance significantly on most tasks such as \textsc{Buttons} and \textsc{Drawers}, whereas the \textsc{Doors} task show less sensitivity. This contrast could be due to the smaller size of buttons and drawer handles, which require finer-grained feature maps for accurate manipulation.

% As mentioned in Section~\ref{sec:proposed_models}, FPV uses a transformer to obtain a condition vector from ResNet features across multiple views when conditioning on RGB. We evaluate different feature granularity: global features versus 4x4 or 8x8 feature maps. Figure \ref{fig:ablation_rgb_features} reveals that some tasks, such as \textsc{Buttons} and \textsc{Drawers}, benefit from local semantic information, whereas tasks in \textsc{Doors} show less sensitivity. This contrast likely stems from the smaller size of buttons and drawer handles, which require finer-grained feature maps for accurate manipulation.


\subsection{Ablation on finetuning SUGAR}

Finally, we examine the effect of different finetuning strategies on FPV-SUGAR. Figure \ref{fig:ablation_sugar_ft} compares a fully frozen SUGAR encoder with an encoder in which only the last two layers are finetuned. With the exception of the stove task, finetuning the last two layers improves performance in nearly every scenario, providing a 2\% boost in average success rate. Finetuning even more layers could potentially further increase the performance of the model.

\input{figure_and_table/ablation/sugar_ft}

% \subsection{Ablation on pretrained clip features}