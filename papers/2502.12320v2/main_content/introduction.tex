% \newpage
\section{Introduction}
\label{sec:introduction}

% \textcolor{red}{NEED REFERENCES, CONTRIBUTION}
% \textcolor{red}{Motivation: current methods using point cloud or RGB lose information, for example 3d diffusion policy and 3d diffuser actor}

Imitation Learning (IL) has become a fundamental approach in robotic learning \cite{brohan2022rt, chi2023diffusion, zhao2023learning, black2024pi_0, kim24openvla}, allowing agents to acquire complex behaviors by mimicking expert demonstrations. IL can additionally benefit from contextual information that provides task description, therefore reducing the need for inferring task goal from the demonstrations \cite{ding2019goal}. % Give the example of robot kitchen helper?
A crucial aspect of IL is the choice of the used input representation, as it directly impacts the agent's ability to generalize and make informed decisions. 
RGB images are a common input modality because they offer rich texture and semantic information that can be critical for tasks involving object recognition and contextual reasoning \cite{mandlekar2021matters,reuss2024multimodal,liu2024rdt}.  Additionally, they are easy to obtain and relatively cheap, making them a practical choice in many scenarios.
Another input modality is a point cloud \cite{zhu2024point, ze20243d, ke20243d}, which provides us with geometric information. Point cloud representations have proven highly effective for robotic manipulation due to their ability to directly encode 3D spatial structures. A further modality are language instructions.  They contain relevant task context \cite{stepputtis2020language, li2023vision, reuss2024multimodal}, such as human understandable task descriptions. All these input types provide different benefits and limitations in the learning process, and we should fuse them appropriately to extract all the individual benefits, while offsetting the limitations. Therefore, fusing different modalities is a relevant but challenging problem.

In this paper, we focus on the fusion of RGB images and point clouds while also taking language instructions into account. Despite their complementary nature, integrating these RGB images and point clouds remains a significant challenge in IL. Existing approaches \cite{gervet2023act3d, shridhar2023perceiver, ze20243d} primarily attempt to assign 2D visual features to point clouds, thereby incorporating RGB information into 3D representations. However, such strategies often fail to retain the global contextual information from images, leading to suboptimal performance in tasks that require both precise spatial reasoning and high-level semantic understanding. As a result, neither modality alone—nor naïve fusion techniques—achieves universally strong performance across diverse imitation learning benchmarks. Yet, more recent approaches of combining modalities such as adaptive conditioning in Layer-Norm layers \cite{Peebles2022DiT} has not yet been explored in the imitation learning context, even though it allows a more flexible sensor fusion scheme. 
\input{figure_and_table/architecture}
\input{figure_and_table/dit_block}

To address this limitation, we introduce \textbf{F}usion of \textbf{P}oint Cloud and \textbf{V}isual Representation \textbf{Net}work (FPV-Net), a novel imitation learning method designed to effectively align and balance the strengths of both point cloud and RGB images. Our approach leverages novel conditioning methods for sensor fusion \cite{Peebles2022DiT} and ensures that the geometric precision of point clouds is preserved while leveraging the global semantic richness of RGB inputs, enabling a more robust and generalizable policy learning process.
For the extraction of representations from RGB images, we use a neural network based on the FiLM-ResNet architecture \cite{perez2018film}. This extraction process is conditioned on the language instruction, thus effectively incorporating this modality into our method. Moreover, we make use of both local features and global features, which we show to be critical for the manipulation tasks. 
%TODO describe the key elements of the  the framework
To extract data from point clouds, we apply Furthest Point Sampling \cite{eldar1994farthest} and k-Nearest Neighbors, that are then encoded into learned embeddings.  For fusing the modalities, we explore 3 different approaches, and show that fusing Point Cloud and Language as main modalities while using RGB images as the conditional modality using AdaLN conditioning \cite{Peebles2022DiT} performs best. Figure \ref{fig:input_components} illustrates how FPV-Net extracts features from different modalities.

We evaluate FPV-Net on RoboCasa \citep{robocasa2024}, a challenging benchmark for robotic manipulation. We conduct extensive experiments to analyze the impact of different input modalities. Our results indicate that neither point clouds nor RGB images alone provide optimal performance across all tasks, whereas naïve fusion methods often degrade performance due to poor alignment between modalities.
FPV-Net consistently outperforms state-of-the-art approaches \cite{ke20243d,ze20243d} across all tasks, establishing a new benchmark in multimodal imitation learning.

% To summarize, we identify main contributions of this paper as threefold:

%     \textbf{1)} We conduct systematic experiments on RoboCasa, demonstrating that neither using RGB images nor point clouds alone is sufficient for strong performance. Each modality could perform well on some tasks, but perform poorly on other tasks. 
    
%     \textbf{2)} We introduce FPV, a diffusion-based multi-modal imitation learning method. FPV use point cloud inputs as the main modality and visual inputs as the conditional modality, which are injected into the model by AdaLN conditioning \cite{Peebles2022DiT}. Additionally, we use language instructions for contextual guidance. FPV achieves state-of-the-art performance across the majority of all tasks. To our knowledge, using AdaLN to fuse point cloud and RGB modalities is a novel insight.

%     \textbf{3)} We demonstrate that local RGB features play a crucial role in fine-grained robotic manipulation tasks. By integrating both global and local features, the model's performance is significantly enhanced.
%     %experimented with alternative design choices (ablations) and provided analysis

To summarize, our main contributions are threefold. First, we conduct systematic experiments on RoboCasa, showing that neither RGB images nor point clouds alone are sufficient for strong performance, as each modality excels in some tasks but performs poorly in others. Second, we introduce FPV-Net, a diffusion-based multi-modal imitation learning method that leverages point cloud inputs as the main modality and visual inputs as a conditional modality, integrated via AdaLN conditioning \cite{Peebles2022DiT}, while also incorporating language instructions for contextual guidance. FPV-Net achieves state-of-the-art performance across most tasks, and, to our knowledge, using AdaLN to fuse point cloud and RGB modalities is a novel insight. Third, we demonstrate the critical role of local RGB features in fine-grained robotic manipulation tasks, showing that integrating both global and local features significantly enhances model performance.






% Our work is organized as follows. In \Cref{sec:related_works}, we review related works. \Cref{sec:model} describes the proposed model, explaining how each input modality is processed and the novel fusion method of point cloud and RGB features. In \Cref{sec:experiments}, we provide experimental evaluations in the challenging RoboCasa benchmark with analysis of alternative design choices. 
% \todo[inline]{Maybe give a short outline of the paper here. Ending on a enumeration looks quite ugly IMO}

% We conduct systematic experiments on RoboCasa, demonstrating that neither modality alone is sufficient for strong performance.
% We achieve state-of-the-art results across all tasks, showcasing the effectiveness of our approach in leveraging multimodal perception for IL.
% By bridging the gap between geometric reasoning and semantic understanding, our work sets a new foundation for multimodal learning in imitation-based robotic manipulation.