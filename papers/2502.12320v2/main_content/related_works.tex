% \newpage

\section{Related Works}
\label{sec:related_works}

\textbf{Visual Imitation Learning.}
Recent state-of-the-art imitation learning methods \cite{chi2023diffusion,reuss2024efficient, kim24openvla,liu2024rdt,li2025gr} often use 2D images as state representation due to their rich global information and ease of acquisition from raw sensory inputs. However, 2D images lack explicit 3D information such as precise 3D coordinates and object geometry \cite{zhu2024point}, which are crucial for many robotic manipulation tasks. While using multiple camera views can partially mitigate this drawback, it requires significantly more training data to infer the 3D spatial information effectively \cite{ze20243d}. Moreover, image-based policies struggle with occlusions and viewpoint variations \cite{peri2024point}, making generalization across diverse environments challenging.

\textbf{Imitation Learning with 3D Scene Representation.}
An alternative approach is to leverage 3D scene representations, such as point cloud \cite{zhu2024point, ze20243d, ke20243d}, which provide explicit spatial structure and thus enable better spatial reasoning. However, using point clouds usually requires down-sampling \cite{eldar1994farthest}, leading to loss of fine-grained information from the raw sensory data. 
Recently, several studies \cite{shridhar2023perceiver, gervet2023act3d, ke20243d} have investigated how to effectively incorporate both 2D and 3D representations into imitation learning. For instance, Act3D \cite{gervet2023act3d} generates feature clouds using multi-view RGB images and depth information. 3D Diffuser Actor \cite{ke20243d} lifts ResNet features to 3D using the depth map. Unlike these approaches, FPV-Net introduces a novel 2D-3D fusion strategy by conditioning Transformer policy with 2D images from multiple views while processing tokenized 3D representations, enabling better generalization and spatial reasoning.

\textbf{Multi-modal Sensory Fusion in Imitation Learning.}
Most existing research on multi-modal sensory fusion in imitation learning focuses on combining image observations with language goal conditioning. A common strategy is to treat image and language inputs as separate tokens within a Transformer and train the policy from scratch \cite{reuss2024multimodal, bharadhwaj2024roboagent}. Another line of research leverages large pre-trained Vision-Language Models (VLMs) and fine-tunes them with demonstrations to create Vision-Language-Action (VLA) models \cite{cheang2024gr, kim24openvla, black2024pi_0}. However, these methods predominantly rely on 2D image features, which limits their effectiveness when working with small datasets or tasks requiring detailed spatial reasoning. In the contrary, FPV-Net fuses 2D and 3D observations, enabling more efficient multi-modal learning.



% Diffuser Actor
% ACT3D