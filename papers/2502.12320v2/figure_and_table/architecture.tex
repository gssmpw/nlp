% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figure_and_table/architecture/architecture.pdf}
%     \caption{Overview of our framework: our policy receives language embedding, point patches tokens, and image tokens. Instead of putting them together, we propose to regard images as a condition of the transformer block. We also highlight the importance of both global features and local features from resnet.}
%     \label{fig:main_architecture}
% \end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure_and_table/architecture/input_components.pdf}
    \caption{Processing each input modality to generate corresponding embeddings. \textbf{Top}: A FiLM-ResNet  architecture is used to extract a feature map from the context image. The feature map is processed through average pooling and flattening to obtain global and local feature tokens, which are then concatenated and fed into the transformer along with a learnable CLS token, whose output is used as a condition vector for the diffusion policy (Figure \ref{fig:dit_block}). \textbf{Middle}: The point cloud input is processed by applying FPS to sample points, followed by KNN to group point patches using these FPS points as centers. The resulting patches are passed through a point patches encoder, which can be a lightweight MLP or the pretrained SUGAR model. \textbf{Bottom}: The CLIP model is employed to generate the language embedding for the behavior prompt.} 
    % \vspace{-0.3cm}
    \label{fig:input_components}
    \vspace{-0.5cm}
\end{figure}

