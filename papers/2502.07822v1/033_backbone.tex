The currently popular point-based detectors, such as 3DSSD, IA-SSD, and DBQ-SSD, all use PointNet-style 3D backbones as the point semantic and geometric information extractors, and their advantages in detection accuracy and inference speed have been well demonstrated. In order to ensure lightweightness, our PDM-SSD also adopts this structure of 3D backbone. To highlight the performance gains after solving the problem of discontinuous receptive fields, we do not make too many changes to the backbone, following the design of IA-SSD in general. The specific structure is shown in Fig. \ref{fig2}.

The point cloud is first passed through a vanilla feature augmentation network stacked with MLP to increase the feature dimension. Then, it enters a feature extractor consisting of four repeated SA (Set Abstraction) modules to learn geometric and semantic information, as shown in Eq. \ref{eq4}. The point-wise features outputted at each stage are denoted as $\{F^p_{1}, F^p_{2}, F^p_{3}, F^p_{4}\}$. Specifically, before each stage, we downsample the point cloud to reduce the model's spatial complexity and feature redundancy. Then, we use PointNet for local feature extraction, which can be divided into three steps: 1) Point indexing: using the current stage sampled points as centers, perform vanilla ball query within the range of the sampled points in the previous stage to index the $k$ nearest points to each center within a certain radius $r$. 2) Feature learning: use MLP to learn the features of the points within the ball, further improving the learning depth. 3) Pooling: perform max-pooling operation on the features of the points within the ball in the feature dimension. Due to the unordered nature of point clouds, this operation ensures that even if the order of the point cloud is changed, the pooled features remain unchanged. In each stage, we simultaneously use two sets of combinations of radius and sampling number for multi-scale feature extraction, and aggregate the multi-scale features after the pooling layer. In summary, the point quantities of $F^p_{1}, F^p_{2}, F^p_{3}, F^p_{4}$ decrease while the feature dimensions gradually increase.

It should be noted that in the model, $F^p_{1}, F^p_{2}$ adopt Farthest Point Sampling (FPS) downsampling method, while $F^p_{3}, F^p_{4}$ adopt foreground point downsampling method with semantic embedding, following the approach of IA-SSD. The former ensures the global coverage of sampling points in the presence of a large number of redundant points, reducing global information loss. The latter ensures a high recall rate of foreground points, reducing target information loss. The specific implementation is shown in Fig. \ref{fig2}. We add a network branch $S(\cdot)$ to the SA module in the second and third stages to extract the semantic information of sampling points. Then, the sampling points in the next stage are selected from the points with the highest probability of being foreground points, as shown in the following equation:
\begin{equation}
	sample\_index_{i+1}=topK(S(F_i)) \quad i=2,3
\end{equation}
From this, we obtain a small number of foreground sampling points with their rich local geometric and semantic information in point-wise features.