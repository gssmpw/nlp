In this paper, we focus on single model 3D object detectors based on pure point clouds. Based on the structural types of point clouds input into the 3D object detectors, we provide a brief overview.

\subsection{Grid-based 3D object detectors}
Grid-based 3D object detectors first convert point cloud into discrete grid representations such as voxels, pillars, BEV feature maps, and FV feature maps. Voxels are 3D cubes that contain points inside voxel cells. They can preserve the spatial information of the original point cloud to a great extent while reducing the number of computational units. VoxelNet \cite{zhou_voxelnet_2017} is a pioneering work that uses sparse voxel grids and proposes a novel voxel feature encoding (VFE) layer to extract features from the points inside a voxel cell. Building upon VoxelNet, VoxelNeXt \cite{chen2023voxelnext} utilizes a fully sparse structure for feature extraction and detection heads, partially overcoming the high memory consumption issue of VoxelNet. Currently, most state-of-the-art detectors on public datasets use voxel-based feature extractors, such as DistillBEV \cite{wang2023distillbev} and LidarMultiNet \cite{ye_lidarmultinet_2022}. Pillars can be seen as special voxels with a voxel size of 1 in the vertical direction. PointPillars \cite{lang_pointpillars_2019} is a seminal work that introduces the pillar representation, and then PillarNet \cite{shi_pillarnet_2022} and FastPillars \cite{zhou_fastpillars_2023} add additional encoder networks to enhance feature learning in the model structure. Compared to voxel-based detectors, pillar-based ones have fewer grid cells, making them more similar to 2D image detection and enabling faster inference speed.
The Birdâ€™s-eye view (BEV) feature map is a dense 2D representation typically obtained by projecting voxel, pillar, and raw point cloud data. BEV-based detectors \cite{simon_complex-yolo_2018,noauthor_multi-view_nodate,beltran2018birdnet,zeng2018rt3d,ali2018yolo3d,barrera2020birdnet+} usually require the addition of cell-level features to expand the feature space, such as height and density. The feature learning process for those detectors is similar to 2D detection tasks. It is worth noting that the BEV representation of the scene and perception results greatly simplifies the environmental space in which the objects are located, making it easy to propagate information to downstream tasks such as path planning and control \cite{jia2023driveadapter}. Therefore, BEV-based detectors are currently a hot research direction and are gradually forming a new generation of BEV perception paradigm with multiple tasks and modalities \cite{hu2023planning,li2023delving,li2022bevformer}.
The range image is a dense 2D representation that contains 3D distance information in each pixel. LaserNet \cite{meyer2019lasernet} is a pioneering work that detects 3D objects from range images, utilizing the deep layer aggregation network (DLA-Net) \cite{yu2018deep} to extract multi-scale features. Laserflow \cite{meyer2020laserflow} and RangeRCNN \cite{liang2020rangercnn} further enhance feature learning by adding an encoder-decoder structure. However, when preprocessing point clouds into Bird's Eye View (BEV) and Front View (FV) feature maps, spatial information is lost. This makes it more challenging to identify objects with pillar-like features, such as pedestrians, in the BEV representation. Additionally, in the RV representation, occlusions make it harder to accurately localize objects. In general, grid-based detectors have prominent advantages and disadvantages. The regularized feature representation significantly reduces the design cost of the model, and models that perform well in 2D detection tasks can be easily transferred to 3D tasks. Furthermore, through padding, convolution, and pooling operations, the unoccupied space in the original point cloud is filled with features, making the model more robust in object recognition. However, such models compress the geometric information of the point cloud while regularizing it. To extract deep features, a backbone with a large number of parameters is required, which greatly increases the deployment cost and reduces the inference speed of the model. Moreover, since point clouds are sparsely distributed, most grid cells in 3D space are empty and do not contain any points, leading to high memory consumption in grid-based methods, further increasing the deployment cost of the model.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=.8\textwidth]{AL_pics/workflow.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The overall workflow of PDM-SSD. In the joint training phase, the input LiDAR point clouds are first passed through the embedding network to expand the feature space of the points. Then, a PointNet-style 3D backbone network is utilized to extract features for each point. This 3D backbone network consists of several stages of downsampling modules, local feature aggregation modules, and multi-scale feature aggregation modules. The neck network includes our proposed point dilation mechanism, where points are lifted to the grid level and feature filling is performed for the unoccupied space in the original point cloud using a special mechanism. The grid-wise features are then used to regress the heatmap of the scene, which provides information about the target's position, and the grid features are jointly learned with the fusion detection head to learn the global features of the target. In the auxiliary training phase, we do not utilize the information provided by neck but only compute the prediction loss of heatmap.}
	\label{fig2}
	\vspace{-0.3cm}
\end{figure}

\subsection{Point-based 3D object detectors}
Different from grid-based methods, point-based methods \cite{qi_deep_2019,shi2019pointrcnn,yang_3dssd_2020} directly learn geometry from unstructured point clouds. Specifically, point clouds are first passed through a point-based backbone network, where points are gradually sampled and features are learned by point cloud operators, further generating specific proposals for objects of interest. 3D bounding boxes are then predicted based on the features of these proposals. The implementation of point-based detectors relies on the early work on point-based point cloud classifiers. PointNet \cite{qi2017pointnet} pioneers this class of methods, which consists of two basic structures: a multilayer perceptron (MLP) network for feature learning and max-pooling as a symmetric function. The former can transform features to specified dimensions and learn global features of the point cloud, while the latter can overcome the disorder of point clouds, and its symmetry ensures that the model has the same output regardless of the input order.
Currently, the main differences among point-based detectors lie in the downsampling method and the structure of the local feature extractor. To ensure global coverage of the sampled points, PointRCNN \cite{shi2019pointrcnn} pioneers the use of Furthest Point Sampling (FPS) to progressively downsample the input point cloud and generate 3D proposals. To improve the recall rate of foreground points and reduce information loss of the objects, 3DSSD \cite{yang_3dssd_2020} proposed the Feature Furthest Point Sampling (F-FPS) method, which uses feature distance instead of Euclidean distance to adaptively retain foreground points. Zhang et al. \cite{zhang2022not} proposed an instance-aware sampling method, which adds a branch network to predict the semantic information of points in the sampling process to retain foreground points. Their lightweight object detector, IA-SSD, greatly improves the recall rate of foreground points and achieves state-of-the-art accuracy on the KITTI dataset. However, IA-SSD relies heavily on dataset-specific hyperparameters, and this hard-labeling method leads to feature redundancy. Yang et al. \cite{yang2022dbq} proposed DBQ-SSD to dynamically aggregate local features, further improving the inference speed without sacrificing detection accuracy. Liang et al. \cite{liang2023spsnet} believed that foreground points contribute differently to the detection results and proposed SPSNet to regress the stability of the sampled points, thereby sequentially introducing a stability-based downsampling method. 
In summary, point-based detectors directly take raw point clouds as input without loss of spatial information. The feature learning module has a small number of parameters, making it more suitable for practical applications in terms of inference speed and memory consumption. For example, IA-SSD and DBQ-SSD achieve detection speeds of 83FPS and 162FPS, respectively, on an 2080Ti GPU. However, due to the unordered and irregular nature of point clouds, the difficulty of model design is much greater than that of convolutional and transformer-based models. Additionally, point-based detectors can only extract features from the provided point cloud space. For incomplete and sparse objects, their perception range does not change as the query ball radius increases, resulting in discontinuous receptive fields. It is difficult for them to learn the overall features of the objects. Therefore, point-based detectors are far less robust than grid-based detectors \cite{zhu2023understanding} in detecting diluted objects caused by occlusion and adverse weather conditions.

\subsection{Point-voxel based 3D object detectors}
Point-voxel based approaches combine points and voxels in the 3D object detection process to overcome the limitations of point-based and voxel-based methods. Liu \textit{et al.} \cite{liu2019point} and Tang\textit{et al.} \cite{tang2020searching} attempt to bridge the features of points and voxels by using point-to-voxel and voxel-to-point transformations in the backbone networks. Points provide detailed geometric information, while voxels are computationally efficient. PV-RCNN \cite{shi2023pv}, a two-stage 3D detector, incorporates the first-stage detector from Second \cite{yan2018second} and introduces the RoI-grid pooling operator for second-stage refinement. Qian \textit{et al.} \cite{qian2022badet} propose a lightweight region aggregation refine network (BANet) that constructs a local neighborhood graph to improve box boundary prediction accuracy. Additionally, Shi \textit{et al.} \cite{shi2023pv} propose PV-RCNN++, which utilizes VectorPool aggregation for better aggregating local point features with reduced resource consumption. Compared to pure voxel-based detection approaches, point-voxel based methods integrate features in the 3D backbone, allowing them to benefit from both fine-grained 3D shape and structure information obtained from points, resulting in improved detection accuracy, especially for two-stage detectors. However, this comes at the cost of increased inference time.

To ensure the inference speed of the model, our 3D backbone network retains a point-based design instead of using point-voxel-based approaches that integrate two types of features to gain benefits in the feature learning process. Instead, in the neck structure, we rely on the proposed point dilation mechanism to lift points to a new dimension and combine the detection head to jointly learn features of non-occupied space, thus ensuring the continuity of the model's receptive field. In summary, PDM-SSD provides a new approach to balance the inference speed and detection accuracy of the model.