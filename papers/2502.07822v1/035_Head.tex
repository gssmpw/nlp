\begin{figure}[t]
	\begin{center}
		% \vspace{-0.2cm}
		\includegraphics[width=.7\textwidth]{AL_pics/head.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Joint Learning. In the case of sparsity and extremely incomplete targets, on one hand, the vote points generated by the vote network may deviate from the target center, and on the other hand, the point-wise features have a prediction for the target category lower than the threshold. We use the auxiliary information generated by the neck to improve these issues. First, we supplement the vote point set with the learned heatmap. Then, we concatenate the point-wise features and grid features, and pass them through the channel attention network to form new features for predicting the parameters of the target box.}
	\label{fig7}
	\vspace{-0.3cm}
\end{figure}

We propose a hybrid detection head to simultaneously extract point-wise features and grid features, as shown in Fig. \ref{fig7}. For the point-wise features $F^p_4$, inspired by VoteNet, they are first input into a voting network to move the sampled points towards the center of the objects as much as possible. Then, the context features of the objects are extracted using the moved positions as centers. Finally, these features are used for semantic classification and regression of geometric parameters. This part follows the basic structure of a traditional point-based detection head. However, as described in Section \ref{sec:pf}, due to the limited receptive field, the information of objects in the scene is not fully explored, especially for sparse and locally scattered objects, where their voting centers may be inaccurate or the context features may have a probability of being classified as objects below the certain threshold.

The grid feature, with the support of angle and scale coefficients, not only expands the receptive field of point-wise features but also enhances the connectivity of features from multiple inflated centers, potentially enabling the extraction of holistic features from sparse and incomplete targets. To achieve this goal, we construct a sparse heatmap using annotation information to supervise the learning of grid features, following the design of the dense CenterHead in the training process. Additionally, we find that even treating this part of the network as an auxiliary learning task can significantly improve the performance of the point-based detection head. Therefore, we design the model from the perspectives of both auxiliary learning and joint learning.

\textbf{Auxiliary Learning (AL).} The $F^g_4$ used for heatmap regression is essentially obtained by dilating $F^p_4$ used for prediction. We believe that supervising $F^g_4$ with sparse heatmaps will also increase the receptive field of $F^p_4$, thereby obtaining more accurate detection results. In addition, another advantage of auxiliary learning is that the auxiliary network does not participate in prediction and inference stages of the model, thus preserving the inference speed of the point-based detector completely. We will demonstrate its superiority in the experimental section.

\textbf{Joint Learning (JL).} In joint learning, we will fully utilize grid features and learned scene heatmaps. As shown in Fig. \ref{fig7}, in addition to the votes generated by the vote network from point-wise features, we supplement the centers of the top $K$ maximum values in the predicted heatmap. These two sets of points are aggregated together to learn contextual features through the point-based head. Then, we contact the contextual point-wise features with the grid features of the cells where these points are located in the depth dimension. Finally, after a simple channel attention, the features are used for semantic classification and detection box parameter regression. The above operations can not only supplement the spatial position information of the scene targets provided by the heatmap, but also adaptively fuse point-wise features and grid features, thereby compensating for the inaccuracies caused by the limited receptive field of the point-based detector and the low semantic probabilities of the targets. We will explain this in detail in \ref{sec:pdm}.