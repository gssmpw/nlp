\newif\ifARXIV
\ARXIVtrue
% \ARXIVfalse


\ifARXIV
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}
\else
\documentclass[sigconf,screen,review,anonymous]{acmart}

% ACM boilerplate
\copyrightyear{2026} 
\acmYear{2026} 
\setcopyright{rightsretained} 
\acmConference[ICSE '26]{48th IEEE/ACM International Conference on Software Engineering}{2026}{Rio De Janeiro, Brazil}
\acmDOI{DOI} 
\acmISBN{ISBN}

\fi




\newif\ifDEBUG
% \DEBUGtrue
 \DEBUGfalse

\newif\ifNOBOXES
%\NOBOXEStrue
\NOBOXESfalse

\newif\ifSPACEHACK
% \SPACEHACKtrue
\SPACEHACKfalse


\newif\ifANONYMOUS
% \ANONYMOUStrue
\ANONYMOUSfalse


% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}

% inlined bib file
\usepackage{filecontents}

%-------------------------------------------------------------------------------

\input{misc/typesetting}
\input{data/data}

\begin{document}

%-------------------------------------------------------------------------------

%don't want date printed
% \date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
% \title{\Large \bf TypoSmart: Low False-Positive Detection on ``Sleepy'' Typosquatting Attacks \\for Package Registries
% }

\ifARXIV
\else
\begin{abstract}
% \JD{Remove all claims about content. README == metadata}
Package confusion attacks such as typosquatting
% , often manifesting as typosquatting attacks, 
threaten software supply chains.
Attackers make packages with names that syntactically or semantically resemble legitimate ones, tricking engineers into installing malware.
While prior work has developed defenses against package confusions in some software package registries, notably NPM, PyPI, and RubyGems, gaps remain:
  high false-positive rates;
  generalization to more software package ecosystems;
  and
  insights from real-world deployment.
%\JD{The following remark is suitable for the body but not the abstract. I cleaned up the opening part of this paragraph, so just move the rest to the body or Discussoin.}
%Prior works focused on user-level where they deploy the defense in CI/CD pipeline and the false-positive is acceptable, while our work focused on a registry-level where the FPs are important to maintain the reputation of package contributors and organizations in the community.

% \JD{The next paragraph needs to be reworked. We have a run-on style and the order of presentation does not seem suitable.}
In this work, we introduce \tool, a solution designed to address the challenges posed by package confusion threats. We begin by presenting the first empirical analysis of benign signals derived from prior package confusion data, uncovering their threat patterns, engineering practices, and measurable attributes.
We observed that 13.3\% of real package confusion attacks are initially stealthy, so we take that into consideration and refined the definitions.
Building on state-of-the-art approaches, we extend support from three to six software package registries, and leverage package metadata to distinguish benign packages.
% integrating analysis of package content and metadata. 
% Our system achieves a 73\%–91\% improvement in speed in identifying confusing packages, 
Our approach significantly reduces \FPReducedRate false-positive (from 77\% to 13\%), with acceptable additional overhead to filter out benign packages by analyzing the package metadata.
\tool is in production at our industry partner, whose analysts have already confirmed \NumReviewedThreats packages detected by \tool as real attacks. %, resulting in the removal of \NumRemovedPkgs packages within three months.
% \WJ{We do have updated data. TODO: Get the data.}
We share lessons learned from production and provide insights to researchers.
% \WJ{TODO: Update the numbers}



% This paper presents a typosquatting detection system that integrates an embedding-based name similarity approach, popularity metadata, and fine-grained filtering with package-level metadata. We perform a large-scale analysis across six registries---NPM, PyPI, RubyGems, Maven, Golang, and Hugging Face---unifying prior approaches and introducing novel optimizations to handle ecosystems lacking direct popularity metrics. Our evaluation demonstrates that the system not only improves detection accuracy but also substantially reduces false positives by leveraging metadata verification. Deployed in a production setting, our approach yields actionable alerts for practitioners, and our disclosures to registry maintainers have led to the removal of \TODO{xx} reported malicious packages. By bridging gaps in both methodology and coverage, this work offers a comprehensive, faster, and more accurate solution for detecting typosquatting attacks in modern software and AI model ecosystems. Our work has \TODO{...}

\iffalse
\vspace{0.1cm}
%\textit{``There are only two hard things in computer science: cache invalidation and naming things.'' --- P. Karlton}
\hspace{0.5cm} \textit{``Don't judge a book by its cover'' --- Anon.}
\fi

\end{abstract}
\fi


% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10010405.10010481.10010482.10003259</concept_id>
%        <concept_desc>Applied computing~Supply chain management</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%    <concept>
%        <concept_id>10002978.10002997.10002998</concept_id>
%        <concept_desc>Security and privacy~Malware and its mitigation</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%    <concept>

%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Applied computing~Supply chain management}
% \ccsdesc[500]{Security and privacy~Malware and its mitigation}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Software Package Registries, Software Supply Chain Security, Package Confusion, Typosquatting}
  
\title{\Large \bf \tool: A Low False-Positive System for Detecting Malicious and Stealthy Typosquatting Threats in Package Registries}
\title{\tool: Detecting Active and Stealthy Package Confusion Threats in Software Registries via Integrated Content and Metadata Analysis}
\title{\tool: Detecting Software Package Confusion Attacks via Integrated Metadata Analysis}
\title{\tool: Detecting Package Confusion Attacks at Scale}
\title{\tool: Using Metadata to Detect Package Confusion Attacks Accurately and at Scale}
\title{\tool: Using Metadata to Detect Active and Stealthy Package Confusion Attacks Accurately and at Scale}
% SmartSquat: Detecting Malicious Typosquatting in Large-Scale Ecosystems with Enhanced Embedding Search and Metadata Postprocessing


%for single author (just remove % characters)
% \author{
% {\rm Your N.\ Here}\\
% Your Institution
% \and
% {\rm Second Name}\\
% Second Institution
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
% } % end author

\ifANONYMOUS

\else
 \author{
 {\rm Wenxin Jiang}\\
 Purdue University
 \and
 {\rm Berk Çakar}\\
 Purdue University
 \and
 {\rm Mikola Lysenko}\\
 Socket, Inc.
 \and
 {\rm James C. Davis}\\
 Purdue University
%Name Institution
 } % end author
\fi


\maketitle




\ifARXIV
\begin{abstract}
% \JD{Remove all claims about content. README == metadata}
Package confusion attacks such as typosquatting
% , often manifesting as typosquatting attacks, 
threaten software supply chains.
Attackers make packages with names that syntactically or semantically resemble legitimate ones, tricking engineers into installing malware.
While prior work has developed defenses against package confusions in some software package registries, notably NPM, PyPI, and RubyGems, gaps remain:
  high false-positive rates;
  generalization to more software package ecosystems;
  and
  insights from real-world deployment.
%\JD{The following remark is suitable for the body but not the abstract. I cleaned up the opening part of this paragraph, so just move the rest to the body or Discussoin.}
%Prior works focused on user-level where they deploy the defense in CI/CD pipeline and the false-positive is acceptable, while our work focused on a registry-level where the FPs are important to maintain the reputation of package contributors and organizations in the community.

% \JD{The next paragraph needs to be reworked. We have a run-on style and the order of presentation does not seem suitable.}
In this work, we introduce \tool, a solution designed to address the challenges posed by package confusion threats. We begin by presenting the first empirical analysis of benign signals derived from prior package confusion data, uncovering their threat patterns, engineering practices, and measurable attributes.
We observed that 13.3\% of real package confusion attacks are initially stealthy, so we take that into consideration and refined the definitions.
Building on state-of-the-art approaches, we extend support from three to six software package registries, and leverage package metadata to distinguish benign packages.
% integrating analysis of package content and metadata. 
% Our system achieves a 73\%–91\% improvement in speed in identifying confusing packages, 
Our approach significantly reduces \FPReducedRate false-positive (from 77\% to 13\%), with acceptable additional overhead to filter out benign packages by analyzing the package metadata.
\tool is in production at our industry partner, whose analysts have already confirmed \NumReviewedThreats packages detected by \tool as real attacks. %, resulting in the removal of \NumRemovedPkgs packages within three months.
% \WJ{We do have updated data. TODO: Get the data.}
We share lessons learned from production and provide insights to researchers.
% \WJ{TODO: Update the numbers}



% This paper presents a typosquatting detection system that integrates an embedding-based name similarity approach, popularity metadata, and fine-grained filtering with package-level metadata. We perform a large-scale analysis across six registries---NPM, PyPI, RubyGems, Maven, Golang, and Hugging Face---unifying prior approaches and introducing novel optimizations to handle ecosystems lacking direct popularity metrics. Our evaluation demonstrates that the system not only improves detection accuracy but also substantially reduces false positives by leveraging metadata verification. Deployed in a production setting, our approach yields actionable alerts for practitioners, and our disclosures to registry maintainers have led to the removal of \TODO{xx} reported malicious packages. By bridging gaps in both methodology and coverage, this work offers a comprehensive, faster, and more accurate solution for detecting typosquatting attacks in modern software and AI model ecosystems. Our work has \TODO{...}

\iffalse
\vspace{0.1cm}
%\textit{``There are only two hard things in computer science: cache invalidation and naming things.'' --- P. Karlton}
\hspace{0.5cm} \textit{``Don't judge a book by its cover'' --- Anon.}
\fi

\end{abstract}
\fi

%-------------------------------------------------------------------------------
\vspace{-0.5cm}
\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/threat_model_v3.pdf}
    \caption{
    Threat model illustrating common active and stealth package confusion attacks. These malicious packages can mislead developers into installing them or being included as dependencies in downstream packages. %Prior work failed to differentiate the activity of package confusion threats.
    % \JD{Discussed, please show the simpler version I drew}
    % , while \tool integrates package content and metadata to make this distinction and protect SPRs.
    % \BC{I am working on it}\WJ{We need larger font size.}
    % \JD{No Need For Upper Case Everywhere In The Caption}
    % \JD{Use the terms `active typosquat' and `stealth typosquat'.}
    % \JD{This figure states the attacker can compromise a benign package. That sounds like hijacking which we exclude. Please correct the figure.}
    % \WJ{Remake the figure to show the syntactic and semantic examples better.}
    % \WJ{This figure needs to show active+stealth, and syntactic+semantic.}
    }
\vspace{-0.5cm}

    \label{fig:ThreatModel}
\end{figure}

%-------------------------------------------------------------------------------
% \JD{Review and revise statements about false positives throughput. Is the issue really false positives, or is it lack of ground truth data?}
% \JD{Review use of the terms `lexical' and `syntactic' and `semantic'. Perhaps we should be talking about hierarchy as syntax.}
% \TODO{Remove package content from our main claim}
Software package registries (SPRs) are indispensable in modern development.
They help engineers develop applications by integrating open-source packages, reducing costs and accelerating product cycles~\cite{raymond_cathedral_1999,SotoValero2019EmergenceofSWDiversityinMavenCentral, Wittern2016JSPackageEcosystem_new}.
% \JD{The next sentence does not illustrate criticality at all. Needs statistics/evidence...not `popular'. Popular to whom? Students and hobbyists are not critical. As an example reference, Kelechi's USENIX has a nice reference to an industry study that showed how much open-source software is integrated into commercial code.}
% Popular registries
% such as NPM for JavaScript and PyPI for Python
% illustrate how critical these repositories have become
% ~\cite{gu2023investigatingPackageRelatedSecurityThreatsinSoftwareRegistries, Ladisa2023TaxonomyofAttacksonOSSSC, okafor_sok_2022}. However, this deep reliance on third-party components also heightens risks to the software supply chain~\cite{Ohm2020ReviewofOpenSourceSSCAttacks} \JD{To fix limited references: add 3-5 more papers here in a block.}.
SPRs are used by industry and governments~\cite{18fopensourcepolicy, synopsys2024ossra}, including in AI and safety-critical applications~\cite{Jiang2022PTMReuse, stenbit2003OSSinDOD}.
The resulting software supply chain is an attractive target --- if an adversary can cause an engineer to install a malicious package, the attacker may gain access to downstream users' resources.
One of their tactics is \textit{package confusion attacks}~\cite{kaplan_survey_2021, neupane2023beyondTyposquatting}: creating malicious packages with mis-leading names~\cite{gu2023investigatingPackageRelatedSecurityThreatsinSoftwareRegistries, Ladisa2023TaxonomyofAttacksonOSSSC, okafor_sok_2022}. %: \textit{package confusion attacks}.
\cref{fig:ThreatModel} shows the attack model, covering naming tactics (lexical, syntactic, and semantic confusion) and attack timing (active vs. stealthy).
%two aspects of these  of two kinds of package confusion attacks: \ul{\textit{syntactic}} (also known as \textit{typosquatting} attacks) and \ul{\textit{semantic}}.

\iffalse
An adversary publishes a package that may be confused with a legitimate one.
Through techniques like choosing a similar name, adversaries trick engineers into unintentionally installing a malicious package, leading to data stealing, cryptojacking, or other breaches~\cite{neupane2023beyondTyposquatting}.
Addressing the typosquatting attack problems is critical not just for traditional software packages but also for open-source pre-trained AI models. Recent research shows that the AI model supply chain is equally susceptible to malicious uploads, with typosquatting enabling attackers to spread harmful or backdoored models through platforms like Hugging Face~\cite{Jiang2022PTMReuse}.
% \JD{I feel that we should write something positive about the approaches first, eg ``Researchers have proposed many approaches to address typosquatting, using techniques like X [cite, cite] and Y [cite, cite]. However, three problems remain:''}
\fi

Researchers have proposed various methods to mitigate package confusion attacks.
% , with some specifically targeting typosquatting -- 
% where attackers create deceptive package names to mislead developers. 
Early solutions detected packages with lexically similar names~\cite{vu2020typosquatting, taylor2020spellboundDefendingAgainstPackageTyposquatting,sern2021typoswype}.
State-of-the-art approaches have begun exploring semantic similarity~\cite{neupane2023beyondTyposquatting}.
% \WJ{@Jamie. PTAL the next sentence}
However, as criticized by Ohm \etal~\cite{ohm2023SoKPracticalDetectionofSSCAttacks},
prior work did not consider the false positive rate which is important for industry use~\cite{ohm2023SoKPracticalDetectionofSSCAttacks}.
% (\eg 99.9\% false positive rate in ~\cite{neupane2023beyondTyposquatting}).
We observe two other shortcomings:
  prior work focuses on limited SPRs, notably NPM and PyPI;
  and
  the literature lacks any experience reports from production deployment.

\iffalse
Therefore, a detailed analysis is still lacking to understand the nature of confusing packages that do not contain malware. 
% none of these approaches have analyzed flagged packages that do not contain malware
Moreover, three key gaps remain: (1) high false-positive rates that undermine detection accuracy~\cite{ohm2023SoKPracticalDetectionofSSCAttacks}—with current systems triggering alarms every 200 to 1,000 package installations, failing to meet customer expectations for accurate defense; (2) limited coverage across diverse software package registries, as most studies focus only on NPM, PyPI, and RubyGems while neglecting registries with hierarchical naming structures~\cite{neupane2023beyondTyposquatting}; and (3) limited insights from large-scale SPR deployments on how to scale package confusion detection in production~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}.
\fi

% However, three gaps
% % \JD{not sure we need ``practical'' here} practical
%  remain: (1) high false positive rate~\cite{ohm2023SoKPracticalDetectionofSSCAttacks}, (2) limited coverage across SPRs~\cite{neupane2023beyondTyposquatting}, and (3) limited insights from SPR perspectives.
% Prior work --- triggering alarms approximately every 200 to 1,000 package installations --- falls short of customer expectations, and has limited coverage on only three SPRs~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}.

% \PA{These items sounds more like limitations of existing work, and not necessarily challenges. Do you want to frame it as such? Also, at this point, I don't think it is obvious why we should care about solving these challenges/limitations.}
% \JD{Ultimately I feel that problem 1 and problem 3 are the same problem --- ``Existing techniques do not generalize to all registries. New registries introduce new forms of typosquatting that must be addressed.'' Can we reduce to two problems in this way?}
\iffalse
\begin{enumerate}[leftmargin=*, itemsep=0.1ex]
    % \item \textbf{Lack of Support for Hierarchical Naming.} Registries such as Maven, Golang, and Hugging Face employ hierarchical naming conventions, which have not been thoroughly explored in current research.

    \item \textbf{High False Positive Rate.} Existing detection methods primarily rely on name similarity, resulting in a large number of false alarms when applied to millions of packages. While this may be acceptable for frontend CLI tools during installation, it could have adverse effects if implemented at the package registry level.

    \item \textbf{Limited Coverage Across Registries.} Most large-scale studies focus on NPM, PyPI, and RubyGems, whereas registries like Maven and Golang remain understudied despite their widespread use. Additionally, Hugging Face—today's leading AI model registry with over one million models—has received minimal attention concerning package confusion attacks.
    % \JD{This third problem definitely sounds like a practical problem (``incremental advance''), but the other two seem bigger to me?}
\end{enumerate}
\fi

%
% (1) the need for more scalable package comparison methods\JD{Add a bit of motivation, eg the scale of registries and the need to handle millions of packages. But later you say the detection speed is 0.22 ms/package so that's just about 4 minutes to scan a million-package registry (0.22ms*1millionpkgs/1000 = 220 seconds = 4 minutes). Sounds like there is no problem.}, (2) \JD{Number 2 does not sound like an essential problem. Registry can easily publish popularity and remove this gap so your paper shouldn't depend on this for motivation.} the absence of popularity metrics for certain registries, (3) high false positive rates in the state-of-the-art detection systems, and (4) the lack of data and insights from production deployments. \JD{Not sure about this gap and the next sentence. Essential for what? (This paragraph is about the knowledge gap --- why do we need this data/insight?)}
% A deeper understanding of the challenges and solutions for deploying typosquat detectors in production environments and at scale is essential.
%

% \JD{Next paragraph should be aligned and ordered like the gaps you identify in the preceding paragraph. The deployment experience part should be in the Evaluation paragraph.}
With our industry partner, we developed a novel system to detect package confusion attacks.
To address the problem of high false positive rates, we measured the engineering practices of package confusion attackers, and propose signals, based on package metadata, that distinguish benign packages from genuine (though perhaps stealthy) threats.
To generalize prior work to more SPRs, we analyzed package naming practices and confusion attacks in six SPRs, and identified novel aspects of these SPRs that expand the attack surface for confusion attacks.
To make our approach production-grade, we develop embedding-based name comparisons and a novel nearest-neighbor search algorithm that provides greater efficiency and accuracy.

\iffalse
Our study advances the field by refining the traditional definition of package confusion. 
Rather than merely identifying deceptive package names, we now detect both active and stealth attacks through integrated analysis of package content and metadata. 
We then present \tool, the first scalable deployment of a novel package confusion detection system.
Our approach addresses key limitations by leveraging comprehensive package metadata to reduce false-positive rates, integrating enhanced embedding-based name comparisons to scale on production, and deploying to a production environment.
% Achieving both continuous coverage and high accuracy in production introduces unique complexities.
% For package registries, this necessitates achieving continuous coverage and high accuracy in production while minimizing false-positive rates and therefore address \ul{problem 1}.
% We also extend the state-of-the-art to six ecosystems while significantly reducing the false-positive rate to address \ul{problem 2}.
% We deployed our system across six registries—NPM, PyPI, RubyGems, Maven, Golang, and Hugging Face—encompassing both established package managers and the leading AI model hub.
Overall, our work defines false-positive criteria and provides critical insights, enabling \tool to align detection accuracy with practical, large-scale SPR deployment concerns.
\fi

% To address this, we developed a false-positive verifier that leverages package metadata and large-language models (LLMs) to substantially reduce spurious alerts. Specifically, we employ embedding similarity search to optimize name-similarity computations and implement a fine-grained metadata verification process to further minimize false alarms, ensuring practical and reliable system performance in real-world deployments.
% \WJ{\textbf{Production validation.} Deploying typosquat detectors in real-world CI/CD pipelines raises questions of computational overhead, data completeness, and how best to mitigate false positives in a large volume of alerts—issues rarely explored in depth.}
% Consequently, we developed an optimized pipeline featuring \textbf{quantized embedding models} and \textbf{approximate nearest neighbor (ANN) searches}, boosting efficiency and coverage.
%
%
% % To address these gaps, we deployed a state-of-the-art typosquat detection approach in a production environment and iteratively optimized the system based on real-world usage.
% \JD{The next sentence is a bit boring, you see, which affirms my concern about the lack of real complexity/challenge for popularity data}
% We utilized popularity metadata from \emph{ecosyste.ms} to compensate for missing information in registries like Maven and Golang. Our deployment experience informed further optimizations to enhance system accuracy and efficiency.
% Building upon our deployment experience, we optimized the typosquat detection approach by employing a quantized embedding model and utilizing approximate nearest neighbor (ANN) search, which significantly accelerated the detection process. To reduce the false positive rate, we also leveraged package metadata to verify the legitimacy of each detected package.
% \JD{This paragraph sounds like it's more of your system design and implementation. I was expecting it to be about your findings/results.}

We integrated these techniques in the \tool system.
We evaluated \tool on previous attack datasets as well as several months' of production data.
We report that \tool achieves a false positive rate of 13\%, an improvement of \FPReducedRate over the state of the art.
On a modern server, \tool has average latency of 6.8 seconds per package.
In production, \tool identified \NumReviewedThreats confirmed package confusion attacks in its first three months. %three months\TODO{Update this number and time}.

\iffalse
Additionally, this work presents 4 lessons we learned from production deployment from the perspective of SPRs, offering insights into deployment strategies.

%improved 73\%-91\% neighbor search speeds, and reduced false positives by \FPReducedRate through metadata-driven checks.
% , which incur a latency of \toolFPVerificationLatency.
% The overall system operates at acceptable speeds of \toolNeighborSearchLatency per pair of packages during neighbor searches.
By prioritizing true positives that practitioners find valuable, our methodology improves the relevance and reliability of detection outcomes.
%further bolstering the security of the software supply chain.
Additionally, this work presents 4 lessons we learned from production deployment from the perspective of SPRs, offering insights into optimal deployment strategies.
\tool is being used in production at our industry partner, and contributed to the removal of \NumRemovedPkgs confusing packages in three months\TODO{Update this number and time}.
\fi

To summarize our contributions:
% \WJ{ConfuDB?}
% \JD{This is too many contributions. Make fewer richer contributions, like ``We accomplish X via Y'' instead of ``(1) We accomplish X. (2) Via Y''. Let's aim for three total. I suggest you keep contribution 1 as-is, merge contributions 2 and 4, and merge contributions 3 and 5.}
\begin{itemize}[leftmargin=1.5em, itemsep=0.1ex]
    \item %Drawing from production deployment experience and empirical analysis of package confusion-relevant practices,
    We refine the definition of package confusion attacks to encompass both active and stealthy threats.
    \item We generalize prior analyses to several new SPRs, noting the effect of package naming hierarchies on naming attack tactics.
    \item We propose novel algorithms to improve attack detection accuracy (embedding) and efficiency (nearest neighbor search).
    \item We open-source our production dataset, \texttt{ConfuDB}, which includes \NumConfuDB production data triaged by security analysts.
    \item We share lessons learned from using \tool in production.
    %present \tool, an embedding-based detection system, extending state-of-the-art work to SPRs and substantially reducing the false positive rates.
    %\item Our real-world deployment provides practical lessons for implementing robust supply chain defenses from the perspective of package registries.
\end{itemize}

\noindent
{\ul{\textbf{Significance:}}} 
Package confusion attacks are common yet challenging to detect.
This paper extends current knowledge by introducing a novel detection system, designed through empirical analysis of previous data and insights gained from ongoing deployment.
\tool is used in production by our industry partner. % as part of their security feed services. 
%It helped remove \NumRemovedPkgs confusing packages from Dec. 2024--Feb. 2025.

\iffalse
We are the first to 
    (1) empirically analyze previous package confusion data and refine its definitions;
    (2) present a novel detection system that integrates package content and metadata, expanding analysis from three to six registries;
    (3) share production experience of such system. 
Our findings significantly enhance production detection mechanisms for SPRs and offer valuable insights for future research on securing the software supply chain.
% and facilitating knowledge transfer in production through software engineering approaches.
\WJ{Should we talk about how we combine empirical study in the knowledge transfer process and use production data to improve our design?}
\fi

\iffalse
\begin{enumerate}[leftmargin=1.2em, itemsep=0.1ex]
    \item
    % \textbf{False-Positive Mitigation via Metadata.}
    Drawing from production deployment experience, we refined the definition of typosquatting to include both active and stealth typosquatting attacks.
    \item We developed a novel system aimed for package registries that incorporates package metadata, substantially reducing false positives to align with practitioner needs.
    \item
    % \textbf{Expanded Multi-Registry Analysis.}
    % \PA{I didn't find this in the paper though.}
    We advance the state-of-the-art by extending typosquatting analysis to six software package registries, including Maven, Golang, and Hugging Face --- the first study to address typosquatting in pre-trained model packages --- alongside NPM, PyPI, and RubyGems.
    \item
    We leverage embedding-based package neighbor search and external popularity metrics to achieve faster and more accurate detection at scale, optimizing performance for production environments.
    % % \textbf{Embedding-Based Approach with Production Optimizations.}
    % We employ
    % % quantized embeddings,
    % ANN search and external popularity metrics, enabling faster, more accurate detection at large scale.
    % from \TODO{xx}\% to \TODO{yy}\%.
    \item
    % \textbf{Deployment Experience.}
    Our real-world deployment offers valuable lessons for implementing supply chain defenses from the package registry perspective, covering aspects such as system efficiency, responsible disclosure practices, and directions for future work.
\end{enumerate}
\fi

\iffalse
\section{Background and Related Work}
%-------------------------------------------------------------------------------
\subsection{Software Supply Chain Security}

% Why software supply chain is important

% What impact it can have. Examples of supply chain attacks e.g. xzutil, xxx.

% AI Model supply chain security
\TODO{Models Are Codes: Towards Measuring Malicious Code Poisoning
Attacks on Pre-trained Model Hubs

Typosquat is one of the threat model.}


\subsection{Typosquatting and Package Confusion}

% Origin of typosquat attack
Typosquatting originated from DNS domain names~\cite{koide2023phishreplicantLanguageModelBasedDNSSquattingDetection, moubayed2018DNSTyposquatDomainDetection}

% Taxonomy of detections
Package typosquatting, or package confusion...


% Detection methods
Previous methods have focused on maximizing detection by applying typosquatting detectors across the Cartesian product of popular and less popular packages, aiming to catch all potential typosquats.
The simplest approach utilizes Levenshtein distance (edit distance) measurements~\cite{taylor2020spellbound}. A more advanced method employs \textit{FastText} embeddings to capture semantic similarities and phonetic resemblance~\cite{neupane2023beyondTyposquatting}.

% \subsection{Embedding Models and Efficient Search Approaches}
\fi
\section{Background and Related Work} %-------------------------------------------------------------------------------
% \WJ{TODO: Merge 2.2.3 into each subsections}
% \WJ{Add more gap analysis after each paragraph}
% \WJ{TODO: Add more discussion and describe the limitation of SOTA works}
Here we discuss software package registries and naming conventions (\cref{sec:background-SPR-PkgNaming}), package confusion attacks (\cref{sec:background-Taxonomy}), and defenses (\cref{sec:background-Defenses}).

\subsection{SPRs and Package Naming}
\label{sec:background-SPR-PkgNaming}

Much of modern software development depends on community ecosystems of libraries and frameworks that facilitate software reuse~\cite{Wittern2016JSPackageEcosystem_new, Zimmermann2019SecurityThreatsinNPMEcosystem}.
% , build tools, CI pipelines, and distribution services.
We refer to these reusable software artifacts as \textit{software packages}.
These packages are distributed by \textit{software package registries~(SPRs)}~\cite{SotoValero2019EmergenceofSWDiversityinMavenCentral, schorlemmer2024signing}.
\cref{tab:registry-overview} shows the SPRs for six popular ecosystems comprising over 7 million software packages.
% \JD{Wenxin: I added the next sentence, it may be a little unclear/inconsistent with rest of paper, please review.}
% \WJ{I rewrote the sentence to make it clearer}
Some SPRs restrict package publication to individual accounts, so the publisher's identifier does not appear in the package name. Others allow grouping packages into namespaces or organizations, which are incorporated into the package name.
We call the former ``1-level'' and the latter ``hierarchical''.

%Together, these SPRs contain over 7 million software packages that are used by organizations worldwide.

% \JD{I added the following paragraph, please review for accuracy}
SPRs typically act as infrastructure and leave it to the engineering community to establish norms for interaction. 
One aspect of their ``hands-off'' approach is that SPRs impose almost no constraints on package names --- they define an allowed character set and forbid duplicate names~\cite{NPMPackageName, rubygemsNaming, golangPackageNames, pep423, mavenNamingConventions, jfrogHuggingFaceNaming}.
This flexibility introduces two related challenges for engineers.
First, based on application needs (\eg traditional software vs. machine learning), engineers in different SPRs have opted for different naming conventions, which can enhance readability, maintainability, and collaboration~\cite{butler2010exploring, jiang2023PTMNaming}, but which may also confuse newcomers.
Second, as we detail next, SPRs' permissive naming practices enables adversaries to choose package names to deliberately confuse engineers.

%a broad naming surface that can be exploited by adversaries.
%To improve the adaptability of state-of-the-art package confusion techniques, we developed tailored approaches that extend these techniques to accommodate diverse naming structures.

\iffalse
SPRs accelerate engineering practice but their use exposes applications to many software supply chain security attack vectors~\cite{ohm_backstabbers_2020, okafor_sok_2022, Ladisa2023TaxonomyofAttacksonOSSSC, Zahan2022WeakLinksinNPMSupplyChain, gu2023investigatingPackageRelatedSecurityThreats}.
% \JD{To fix limited references: add 3-5 more papers here in a block.}.
The most common and longstanding supply chain attack vector is called \textit{typosquatting attack}, a specific kind of \textit{package confusion} threat~\cite{neupane2023beyondTyposquatting}.
A recent study indicates that, as of 2024, typosquatting campaigns continue to target developers by exploiting hundreds of popular JavaScript packages, with over 250 typosquatting packages published in total~\cite{lyons2024typosquatting}. Previous research has also demonstrated that a single typosquatting package can result in hundreds of user downloads~\cite{reversinglabs_r77_typosquatting}.
% \JD{Insert here: some statistics about typosquat in practice (eg to show how common it is). This may echo things in the Intro.}
As a more generic attack, a package confusion attack is more of a social engineering attack, which attacks users through command line, transitive dependencies~\cite{neupane2023beyondTyposquatting}, and even through Stack Overflow or Twitter/X posts~\cite{sharma2024SonatypePyPIMalwarePytoileur, NPM2017crossenvMalware} and registry mirrors~\cite{hurley2024MirrorTyposquat}.
Our goal is to detect such attacks.
In preparation, we next describe the current taxonomy of typosquatting attacks (\cref{sec:background-Taxonomy}) and then examine existing defenses (\cref{sec:background-Defenses}).
\fi

\begin{table}[h]
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{
Ecosystems examined in this study, highlighting their primary domains, naming conventions, and SOTA works on package confusion detections.
% popularity metrics.
Registry sizes are as of Jan. 2025.
%The Hugging Face model counts are sourced from \cite{HuggingFaceWeb}, while the other figures are derived from a commercial software supply chain database, as collected in January 2025.
An example 1-level naming pattern is PyPI's \texttt{requests} module.
An example hierarchical name is Hugging Face's \texttt{google-bert/bert-base-uncased}.
\iffalse
Example naming patterns are:
  NPM (\texttt{lodash}),
  or \texttt{@namespace/pkg\_name}),
  PyPI (\texttt{requests}), RubyGems (\texttt{rails}),
  Maven (\texttt{groupId:artifactId}),
  Golang (\texttt{domain/author/repo}),
  and
  Hugging Face (\texttt{author/model}).
  \fi
% \PA{I feel these example names should be in the table. Would reduce the caption length. To do this, I would suggest removing the pop metric (not sure how useful it is), domain and # pkgs (we know these are the top registries for these ecosystems) columns. The domains can be bracketed after the registry name.}
% The numbers of packages are collected in Jan 2025.
% \WJ{Do we need to add the total number of packages in each ecosystem in this table? If so, that will be more than one column.}
% \JD{So I think that the ``Prior Work'' column should only be reporting papers that gave a proper empirical study or system design, rather than whether or not they ``support'' the ecosystem. I think ossgadget supports any ecosystem but it isn't the same as a study that analyzed typosquat in NPM.}
}
\label{tab:registry-overview}
\footnotesize
\centering
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Registry (\# pkgs)} & \textbf{Domain} & \textbf{Name Structure} & \textbf{Prior Work} \\
% \textbf{Pop. metric?}\\
\midrule
PyPI (\NumAllPyPIPkgs)
& Python
& 1-level
& \cite{vu2020typosquatting, neupane2023beyondTyposquatting}
\\
% & Yes \\

RubyGems (\NumAllRubyPkgs)
& Ruby
& 1-level
& \cite{neupane2023beyondTyposquatting}
\\
% & Yes \\

Maven (\NumAllMavenPkgs)
& Java
& Hierarchical
& N/A
\\ 
% & External \\

Golang (\NumAllGolangPkgs)
& Go
& Hierarchical
& N/A
\\
% & External \\

Hugging Face (\NumAllHFPkgs)
& AI Models
& Hierarchical
& \cite{jiang2023PTMNaming}
\\
% & Yes \\

NPM (\NumAllNPMPkgs)
& JavaScript
& Both
& \cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}
\\
% & Yes \\

\bottomrule
\end{tabular}
}

\end{table}

% --- reusable code libraries and frameworks
% Central to this ecosystem is software package managers, commonly known as
% Recently, AI model package registries like Hugging Face have emerged, streamlining AI development and introducing a pre-trained model supply chain as a new dimension of the software supply chain~\cite{Jiang2022PTMReuse, Jiang2022PTMSupplyChain}.
% While SPRs enhance engineering efficiency, they also expand the potential attack surface.

% \subsection{Software Supply Chain Security}
% This paper focuses on one aspect in which the attack surface is expanded: that engineers must identify and encode a package to depend on, and that they make mistakes in this process.
% We start by reviewing concepts in typosquatting ---
%   the existing taxonomy (\cref{X}) and
%   the state-of-the-art detection methods (\cref{Y}).
% %  software supply chain and software registries (\cref{sec:background-SSCandSPR}),
% % describes
% % typosquatting in SPRs (\cref{sec:background-typosquatinSPRs}), and
%   and the state-of-the-art in typosquatting taxonomy detection (\cref{sec:background-TaxonomyAndDetection}).

  %%%

% \JD{The rest of this paragraph is describing a hijack attack, not a typosquat, so it feels out of place. Can you give an example of a famous typosquat. You need to talk about the aspect of the attack surface that is relevant to this paprt.}
% A compromise in any package can cascade through the supply chain of dependent projects~\cite{Ohm2020ReviewofOpenSourceSSCAttacks}.
% Notable incidents, such as the malicious update to the NPM package \texttt{event-stream} (which impacted hundreds of applications) underscore the severe consequences of such vulnerabilities, eroding trust across diverse platforms~\cite{arvanitis2022systematicAnalysisofEventStreamIncident, NPM_event_stream_2018}.


% \PA{AT the end of almost every paragraph, you try to contextualize your work with respect to that paragraph. But this confuses me as to what your key contribution is. I don't think you should talk about your work so much in the background section.}

%\subsection{Software Ecosystems}
%\label{sec:background-SSCandSPR}


% Modern software development relies on a complex supply chain composed of third-party libraries, build tools, continuous integration (CI) pipelines, and distribution services. A key component of this supply chain is \textit{software packages}, which encompass reusable code libraries and frameworks~\cite{Wittern2016JSPackageEcosystem_new, Zimmermann2019SecurityThreatsinNPMEcosystem}.
% Central to this ecosystem is software package managers, commonly known as \textit{package registries}~\cite{SotoValero2019EmergenceofSWDiversityinMavenCentral, schorlemmer2024signing}.
% Recently, registries for AI model packages, such as Hugging Face, have also gained significant traction among practitioners, not only reducing engineering costs in AI software development but also establishing a pre-trained model supply chain as a new type of software supply chain~\cite{Jiang2022PTMReuse, Jiang2022PTMSupplyChain}.



% \JD{Should this topic sentence refer to packages or to ecosystems?}

% \Andreas{Nit: I believe that SolarWinds was distributed directly by the vendor as proprietary software, and not through package registries - so if ``such vulnerabilities'' is meant to refer to malware distributed through software registries, then I don't think it fits.}
% \JD{Seems like the next sentences belong in \$2.1, not here.}

% \Andreas{Table 1 introduces ``Name Structure'' and ``Pop. metric'' without explaining why they're relevant to registries and typosquatting attacks. I suggest that the first paragraph of \cref{sec:registry-model} should appear in this Background section.}
% \PA{Yeah, I agree with Andreas that this next paragraph needs more context. Alternatively, I think you can consider removing the paragraph, and reference figure 1 and your work after discussing existing taxonomy or typosquat detection and their limitations.}
% \WJ{@Jamie this is the structural change I would like to discuss.}

% \MIK{NPM is technically multi level since you can have namespaces}
% \WJ{Added examples for that}

\iffalse
\subsection{Typosquatting in Software Package Registries}
\label{sec:background-typosquatinSPRs}
% Typosquatting attacks are the most common software supply chain attack, ~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, neupane2023beyondTyposquatting}.
% A more detailed definition of typosquatting is provided in \cref{sec:registry-model}.

\textit{Typosquatting}---or \textit{package confusion}---is the most common and long-known supply chain attack vector in the traditional software registries, wherein adversaries register deceptive names closely resembling those of legitimate packages~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, Ladisa2023TaxonomyofAttacksonOSSSC}. Originally observed in domain squatting~\cite{moubayed2018dnstyposquat,koide2023phishreplicant}, it has since spread to software registries such as NPM, PyPI, and Maven~\cite{Ohm2020ReviewofOpenSourceSSCAttacks}.
Even small changes, such as a single-character substitution, omission, or insertion, can lead developers --- or automated systems reliant on imperfect input handling, such as copy-paste errors or integrations with AI agents --- to unintentionally install malicious code.
We provide a more detailed discussion of typosquatting attacks and present a refined definition in \cref{sec:registry-model}.


% \subsection{Typosquatting in AI Model Repositories}

In parallel with traditional software repositories, typosquatting in the AI supply chain exploits the same fundamental dynamic: publishing model names that closely mimic well-known ones to deceive users or automated pipelines into downloading harmful variants~\cite{protectai_huggingface, 2022JiangEmpirical, jiang2023PTMNaming}. As machine learning workflows are increasingly integrated into critical infrastructure --- spanning domains such as financial services and autonomous vehicles~\cite{liu2021finbert, cui2024surveyonMultiLLM4AutoDriving} --- the accidental inclusion of a malicious model can have severe operational and security consequences~\cite{hu2021AISecurity}. AI model registries, such as Hugging Face~\cite{HuggingFaceWeb} and PyTorch Hub~\cite{PytorchHub}, play a central role in these workflows and are indispensable for machine learning practitioners~\cite{Jiang2022PTMReuse, 2022JiangEmpirical}. Prior research underscores that ``models are code'', highlighting how backdoors or malicious payloads embedded in pre-trained AI models can jeopardize user data integrity, compromise system functionality, or produce erroneous results in production systems~\cite{zhao2024modelsareCodes, Jiang2022PTMReuse}.
To address these concerns, we present the first study of typosquatting attacks in the context of the AI supply chain.
\fi

% \subsection{Typosquat: Taxonomy and Detection}
% \label{sec:background-TaxonomyAndDetection}

\subsection{Package Confusion Attacks and Taxonomy}
\label{sec:background-Taxonomy}
% \JD{Merge 2.2 and 2.3 into one subsection with two subsubsections}
%While SPRs streamline software development, their open nature also expands the attack surface for supply chain threats. 
\textit{Package confusion}
% —often manifested as typosquatting—
is an attack in which adversaries seek to persuade engineers to install malicious packages~\cite{neupane2023beyondTyposquatting}.\footnote{Package confusion is the SPR-specific manifestation of the general naming confusion attack, which affects all IT endeavors including DNS (domain names)~\cite{coyle2023chrome,ulikowski2025dnstwist, gabrilovich2002homograph}, , BNS (Blockchain names)~\cite{muzammil2024BlockchainTyposquatting}, and container images (image names)~\cite{liu2022exploringContainerRegistryTyposquatting, datadoghqWhoami}.}
% \JD{Wenxin, see my message}
%\footnote{Package confusion is the SPR-specific manifestation of the general naming confusion attack, which affects all IT endeavors including DNS (domain names)~\cite{coyle2023chrome,ulikowski2025dnstwist, gabrilovich2002homograph}, , BNS (Blockchain names)~\cite{muzammil2024BlockchainTyposquatting} , and cloud image names~\cite{liu2022exploringContainerRegistryTyposquatting, datadoghqWhoami}. Each of these attacks requires defenses tailored to the context.}
Attackers typically choose a package name similar to a legitimate one that meets the engineer's needs~\cite{neupane2023beyondTyposquatting}.
Attacks may be multi-pronged, \eg using advertising campaigns to increase their chances of fooling engineers~\cite{sharma2024SonatypePyPIMalwarePytoileur,he2024FakeStars}.
%\JD{\cite{NPM2017crossenvMalware} does not mention twitter/X advertising? I removed it from the previous sentence for now}
Package confusion attacks are an ongoing concern, with hundreds of packages detected by researchers in Socket~\cite{socketMaliciousNPMPackage}, ReversingLabs~\cite{reversinglabs_r77_typosquatting}, Orca~\cite{orcaDependencyConfusion}, and others~\cite{birsan2021dependencyconfusion}.

%Adversaries mislead users through command-line interactions~\cite{taylor2020spellbound, vu2020typosquatting}, transitive dependency chains~\cite{neupane2023beyondTyposquatting}, and even recommendations on platforms such as Stack Overflow and Twitter/X~\cite{sharma2024SonatypePyPIMalwarePytoileur, NPM2017crossenvMalware}. Attackers can also manipulate registry mirrors to amplify their reach~\cite{hurley2024MirrorTyposquat}. 
%A 2024 study found that typosquatting campaigns continue to target developers, with over 250 deceptive packages registered against popular JavaScript libraries and individual typosquatting packages attracting hundreds of downloads~\cite{}. 

%Beyond simple typographical errors, these attacks exploit social engineering tactics. 
% Notably, the inherent permissiveness of SPR naming policies -- which allows any unclaimed name to be registered—enables typosquatting in both traditional software registries and emerging AI model hubs (\eg Hugging Face)~\cite{protectai_huggingface, jiang2023PTMNaming}. 

\cref{tab:typosquat_taxonomy} summarizes the state-of-the-art taxonomy of package confusion techniques, based on Neupane \etal~\cite{neupane2023beyondTyposquatting}.
%This taxonomy provides illustrative examples of different forms of textual manipulations commonly employed by typosquatting attacks.
The variations in naming structure in different SPRs also affect the nature of package confusion by SPR.
In SPRs with 1-level names, as shown in prior work, confusion attacks look like the top portion of the table.
In hierarchical SPRs (this work), the attacker has a larger naming surface to exploit (bottom of table).
%Additionally, based on production requirements, we extend the taxonomy by incorporating an additional attack vector: \textit{command squatting}.

\iffalse
SPRs accelerate engineering practices but also expose applications to numerous software supply chain security threats~\cite{ohm_backstabbers_2020, okafor_sok_2022, Ladisa2023TaxonomyofAttacksonOSSSC, Zahan2022WeakLinksinNPMSupplyChain, gu2023investigatingPackageRelatedSecurityThreats}.
Among these, typosquatting attacks, a form of package confusion, remain one of the most persistent vectors~\cite{neupane2023beyondTyposquatting}. A 2024 study found that typosquatting campaigns continue to target developers, exploiting hundreds of popular JavaScript packages, with over 250 typosquatting packages published in total~\cite{lyons2024typosquatting}. Prior research has shown that even a single typosquatting package can attract hundreds of downloads~\cite{reversinglabs_r77_typosquatting}, highlighting the real-world impact of these attacks.

More broadly, package confusion attacks leverage social engineering tactics, deceiving users through command-line installations, transitive dependencies~\cite{neupane2023beyondTyposquatting}, misleading recommendations on platforms like Stack Overflow and Twitter/X~\cite{sharma2024SonatypePyPIMalwarePytoileur, NPM2017crossenvMalware}, and even tampered registry mirrors~\cite{hurley2024MirrorTyposquat}. Our goal is to detect and mitigate such attacks.


% \TODO{Reorder or cut}
%\textit{Typosquatting}, also called \textit{package confusion}, is the most common and longstanding supply chain attack vector.
Typosquatting attacks on software packages are enabled by permissive package naming policies.\footnote{We acknowledge that typosquatting plagues all IT endeavors (\eg DNS~\cite{coyle2023chrome,ulikowski2025dnstwist} and Blockchain~\cite{muzammil2024BlockchainTyposquatting}). We focus on software packages.}
In all major SPRs (\eg those of~\cref{tab:registry-overview}), if a name is not already registered, then an attacker can claim it for himself.
To carry out a typosquatting attack, an adversary publishes a software package whose name closely resembles that of a legitimate package.
The attacker's goal is to cause engineers to accidentally rely on their package instead, allowing the attacker to deliver malware to the developers or users of downstream software~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, Ladisa2023TaxonomyofAttacksonOSSSC}.
% We provide a more detailed discussion of typosquatting attacks and present a refined definition in \cref{sec:ThreatDefandThreatModel}.
Typosquatting occurs in the AI software supply chain too ---
%Similarly, with the rise of AI pretrained model hubs, typosquatting in the AI supply chain exploits the same fundamental dynamic:
on the Hugging Face SPR, attackers publish pre-trained models with names mimicking famous models to deceive engineers into using harmful variants~\cite{protectai_huggingface, 2022JiangEmpirical, jiang2023PTMNaming}.
% To address these concerns, we present the first study of typosquatting attacks in the context of the AI supply chain by running our system on Hugging Face model packages.
% \PA{What section contains the study you referenced here?}
% There has been
% Prior work~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting, sern2021typoswype} classifies typosquatting techniques based on specific \textit{textual manipulations}.
\fi


% These strategies leverage common patterns, such as typos, visual ambiguities, or naming confusion, to deceive users.
% While some techniques (\eg homophonic similarity) may appear trivial, they exploit real-world scenarios where developers rely on automated tools or conduct only cursory manual checks during dependency installation, leaving them vulnerable to subtle, malicious alterations.

\iffalse
As shown in~\cref{tab:registry-overview}, SPRs usually use two different kinds of naming structure to identify the software packages they distribute.
This naming structure also affects the SPR's susceptibility to typosquatting attacks.
Single-level naming systems (\eg PyPI, RubyGems) use a straightforward mapping where each package is identified by a single string (\eg \texttt{lodash} or \texttt{requests}), making them easier to manage but also prone to direct typosquatting.
In contrast, hierarchical naming schemes (\eg Maven’s \texttt{groupId:artifactId} or Hugging Face’s \texttt{author/model}) offer greater flexibility for organizing packages but also broaden the attack surface.
By exploiting these complex naming structures, attackers can employ more sophisticated strategies to deceive users.
\fi


{
\renewcommand{\arraystretch}{0.5}
\tiny
\begin{table}[h]
\centering
\caption{
Common package confusion techniques with examples.
The top section presents some of the taxonomy proposed by Neupane \etal~\cite{neupane2023beyondTyposquatting}.
We added four patterns (bottom, cf.~\cref{sec:ProblemState-CaseStudy1-TP}), generalizing the prior taxonomy to attacks in other SPRs.
The examples for those four patterns are confusion threats flagged by \tool.
% to SPRs with hierarchical names.
%We consider \textit{command squatting} out of scope in this work.
}
\label{tab:typosquat_taxonomy}
\footnotesize
\begin{tabular}{p{0.17\textwidth}p{0.26\textwidth}}
\toprule
\textbf{Technique} & \textbf{Example (\textit{basis $\to$ attack})} \\
\midrule
1-step Levenshtein Distance     & \texttt{crypt\textbf{\ul{o}}} $\to$ \texttt{crypt} \\
\\
%Prefix/suffix augmentation & \texttt{dateutil} $\to$ \texttt{\textbf{\ul{python3-}}dateutil} \\
%\\
Sequence reordering      & \texttt{python-nmap} $\to$ \texttt{\textbf{\ul{nmap-python}}} \\
\\
%Delimiter modification & \texttt{active\textbf{\ul{-}}support} $\to$ \texttt{activesupport} \\
%\\
%Grammatical substitution  & \texttt{serialize} $\to$ \texttt{serialize\textbf{\ul{s}}} \\
%\\
Scope confusion  & \texttt{\textbf{\ul{@}}cicada\textbf{\ul{/}}render} $\to$ \texttt{cicada\textbf{\ul{-}}render} \\
\\
Semantic substitution  & \texttt{b\textbf{\ul{z2file}}} $\to$ \texttt{b\textbf{\ul{zip}}} \\
\\
%Asemantic substitution  & \texttt{discord.\textbf{\ul{js}}} $\to$ \texttt{discord.\textbf{\ul{app}}} \\
%\\
%Homophonic similarity  & \texttt{uglify\textbf{\ul{-}}js} $\to$ \texttt{uglifi\textbf{\ul{.}}js} \\
%\\
%Simplification  & \texttt{pwd\textbf{\ul{hash}}} $\to$ \texttt{pwd} \\
%\\
Alternate spelling  & \texttt{colorama} $\to$ \texttt{colo\textbf{\ul{u}}rama} \\
%\\
%Homographic replacement  & \texttt{d\textbf{\ul{j}}ango} $\to$ \texttt{d\textbf{\ul{i}}ango} \\
\midrule
\textbf{Impersonation Squatting}  & \texttt{\textbf{\ul{meta}}-llama/Llama-2-7b-chat-hf} $\to$ \newline\texttt{\textbf{\ul{facebook}}-llama/Llama-2-7b-chat-hf}\\
\\
\textbf{Compound Squatting}  & \texttt{@typescript\textbf{\ul{-}}eslint/eslint\textbf{\ul{-plugin}}} $\to$ 
\newline \texttt{@typescript\textbf{\ul{\_}}eslint\textbf{\ul{er}}/eslint} \\
\\
\textbf{Domain Confusion} (Golang)  & \texttt{\ul{\textbf{github.com}}/prometheus/prometheus} $\to$ 
\newline \texttt{\textbf{\ul{git.luolix.top}}/prometheus/prometheus} \\
\\
\textbf{Command Squatting}  & \texttt{NPM i \ul{\textbf{$--$help}}} $\to$ \texttt{NPM i
 \ul{\textbf{\texttt{help}}}} \\
\bottomrule
\end{tabular}
\end{table}
}


% \MIK{Did we end up implementing any other types of typosquat detection?}

% \cref{tab:registry-overview} summarizes the SPRs for six ecosystems.
% The naming structure of a registry plays a significant role in its susceptibility to attacks. Single-level naming systems (\eg PyPI, RubyGems) use a straightforward mapping where each package is identified by a single string (\eg \texttt{lodash} or \texttt{requests}), making them easier to manage but also prone to direct typosquatting. In contrast, hierarchical naming schemes (\eg Maven’s \texttt{groupId:artifactId} or Hugging Face’s \texttt{author/model}) offer greater flexibility for organizing packages but also broaden the attack surface. By exploiting these complex naming structures, attackers can employ more sophisticated strategies to deceive users.
% These ecosystems reflect contemporary designs, encompassing both single-level naming schemes (\eg NPM, PyPI, RubyGems) and hierarchical naming structures (\eg Maven, Golang, Hugging Face), as detailed in \cref{tab:registry-overview}. Each registry demonstrates unique naming conventions, popularity metrics, and policies, which can be leveraged for various software supply chain attacks, including typosquatting, code injection, package manipulation~\cite{ohm2023SoKPracticalDetectionofSSCAttacks, gokkaya2023SSCReviewofAttacks}.
% While some registries (\eg NPM, PyPI) feature simple name-to-package mappings, others (\eg Maven, Hugging Face) rely on hierarchical coordinates or author-based namespaces. Understanding these structural differences is critical for analyzing how attackers may deploy malicious packages.
% Our work examines the naming conventions of six ecosystems and proposes tailored detection approaches for each.

\subsection{Defenses Against Package Confusion}
\label{sec:background-Defenses}
% \JD{I suggest you use paragraph headings for `considerations for defense' (here you would talk about false positives and any other major issuse), `academic defenses', `production defenses'}
% \JD{This paragraph does not analyze the cited works in a coherent way. I want to know the same information (dimensions of analysis) about each work. You say WHERE Spellbound is implemented and then HOW typomind works. Confusing.}
% \JD{Please add the phrase ``The state of the art technique is'' (or `techniqueS ARE') so the reader knows.}
% This section explores three key aspects of typosquatting defense: considerations for minimizing false positives, academic approaches, and production tools.


\iffalse
\subsubsection{Considerations for Defenses}
% \JD{This is a bad topic sentence. Also I cannot think of a good topic sentence because this paragraph is doing too many things. It does not fit under the heading 2.2.2 ``Typosquat defense'' which should only contain analysis of existing defenses.}
% \PA{This paragraph feels like it is explaining another dimension of typosquat attacks. Do you want to have a separate subsection for typosquat attacks? Then this section just focuses on the academic and production defenses. Alternatively, you can edit this paragraph so it is obvious what the defense considerations are (which is not currently stated)}
Considering typosquatting \ul{attack behavior}, attackers exploit permissive naming policies in registries by mimicking the names of legitimate packages. Current state-of-the-art detection techniques primarily rely on lexical similarity as their main criterion, often leading to high false-positive rates due to the limited information encoded in package names~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}. Beyond simple naming mimicry, attackers can deploy \textit{stealth typosquatting attacks}, where malicious packages not only imitate the names but also replicate the functionality of legitimate packages.
These attacks enable silent malware injection and compromise systems, targeting either maliciously designed placeholder packages or legitimate packages with similar names that might later be compromised, posing significant risks to supply chain security.
% The growing reliance on large language models (LLMs) for code generation has further exacerbated these threats. LLMs often hallucinate package names, generating installation commands for nonexistent or similarly named packages. Recent studies reveal that such hallucinations are prevalent across all tested LLMs~\cite{spracklen2024LLMPackageHallucinations}.
To address these challenges, we propose a refined definition of typosquatting attacks that includes \textit{stealth typosquatting} in \cref{sec:ThreatDefandThreatModel}, explicitly identifying such packages by analyzing the malicious intent behind each package.
\fi


Package confusion attacks can be mitigated at many points in the reuse process, \eg by scanning for malware in registries~\cite{vu_lastpymile_2021} or during dependency installation~\cite{latendresse2022PruductionDependenciesinNPM} or by sandboxing dependencies at runtime~\cite{vasilakis_breakapp_2018,ferreira_containing_2021_new,amusuo2025ztd}.
Such techniques provide good security properties, but are computationally expensive.
Our approach is part of a class of cheaper techniques that leverage the most pertinent attribute of a package confusion attack: the similarity of the name to that of another package.
We describe the literature in this vein.
%Most package confusion defenses focus on detecting threats prior to dependency installation, \eg identifying them at package creation time or raising signals during dependency selection.
%We review 

\subsubsection{Approaches}

In the academic literature, the earliest name-oriented defenses were purely lexical:
Taylor \etal~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting} and Vu \etal~\cite{vu2020typosquatting} 
measured similarity using Levensthein distance~\cite{levenshtein1966binary} to identify PyPI packages with minor textual alterations from one another.
To detect confusion attacks based on name semantics (\eg \code{bz2file} vs. \code{bzip}), Neupane \etal~\cite{neupane2023beyondTyposquatting} applied FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to enable an abstract comparison of package names. % via cosine similarity.

%Some of these techniques have been applied in industry.
%In production environments, industry tools are designed to support engineers at multiple stages of package installation and dependency management. 
%For instance, Socket defends against typosquatting by providing real-time detection during pull requests and a CLI tool that analyzes dependencies before installation~\cite{socket_typosquatting}.
In acknowledgment of package confusion attacks, industry has also offered some defenses.
NPM and PyPI both use lexical distances to flag packages or otherwise reduce user errors~\cite{NPM_threats_mitigations,psf_acceptable_use_policy,zornstein2023stealthy}.
%uses Levenshtein detection to catch lexically similar names~\cite{NPM_threats_mitigations}, and PyPI enforces an impersonation policy complemented by automatic typo-correction mechanisms to reduce user errors~\cite{psf_acceptable_use_policy, zornstein2023stealthy}.
Some other companies also offer relevant security tools.
For example, Stacklok combines Levenshtein distance with repository and author activity metrics to assign a risk score to suspicious packages~\cite{stacklok_typosquatting}.
Microsoft's OSSGadget generates lexical string permutations and verifies the existence of mutated package names on SPRs to detect potential package confusion attacks~\cite{MicrosoftOSSGadget}.
% \JD{need to explain the technique, not the scope...}
%further broadens the scope by offering a CLI tool that scans for typosquatting across multiple ecosystems~\cite{MicrosoftOSSGadget}.
% (also seen in NuGet)



\subsubsection{Knowledge gaps}

Despite these advances, key challenges persist that hinder the accuracy and reliability of current package confusion detection approaches.
The primary concern was articulated by Ohm \etal~\cite{ohm2023SoKPracticalDetectionofSSCAttacks}: these techniques have high false-positive rates.
Second, detailed empirical treatments have so far focused exclusively on package confusion attacks in NPM, PyPI, and RubyGems~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting, vu2020typosquatting}, leaving unknown the confusion attacks in SPRs with hierarchical naming structures.
Finally, although summaries of package confusion defenses in production are reassuring, the lack of detailed production data makes their utility unclear.

%continue to hinder accurate detection, and a detailed analysis of false positives -- packages flagged as suspicious yet lacking malicious content -- remains unknown~\cite{ohm2023SoKPracticalDetectionofSSCAttacks}.

\iffalse
We conducted the first empirical analysis of prior package confusion data and summarized the engineering practices and measurable attributes.
Our novel system also extend the literature to three additional SPRs, \ie Maven, Golang, and Hugging Face.

Although the measures from prior work are valuable, they tend to be reactive and rely on manual triage when a clear malware signal is absent. We integrate package content and metadata in the detection system to partially automate the triage process in production which significantly reduced the false positive rates. 
Additionally, we outline the production requirements needed to transform academic insights into robust industry practices, and share the lessons learned from production experience.
\fi

\iffalse
We describe typosquat defenses
  from academia (\cref{sec:back-defense-academic})
  and
  from industry (\cref{sec:back-defense-industry}),
  and then describe the weaknesses.
  % and knowledge gaps (\cref{sec:back-defense-gaps}).
\fi

%\subsubsection{Academic Defenses}
%\label{sec:back-defense-academic}

% \WJ{When we talk about high false-positive rate of prior work, we need first detail what do we mean by citing \cite{ohm2023SoKPracticalDetectionofSSCAttacks}
% }
% \JD{Wenxin, we need to cite all the works on typosquat detection.}
\iffalse
Although there are many academic works on software supply chain security~\cite{okafor_sok_2022}, there have been relatively few academic works that specifically target the detection of package confusion attacks.\footnote{Package confusion is the SPR-specific manifestation of the general naming confusion attack, which affects all IT endeavors including DNS (domain names)~\cite{coyle2023chrome,ulikowski2025dnstwist, gabrilovich2002homograph}, , BNS (Blockchain names)~\cite{muzammil2024BlockchainTyposquatting}, and container images (image names)~\cite{liu2022exploringContainerRegistryTyposquatting, datadoghqWhoami}.} % Each of these attacks requires defenses tailored to the context.}
%Prior detection techniques have explored multiple areas specifically on syntactic confusion (\ie typosquatting), including DNS domains~\cite{moubayed2018dns, kintis2017LongitudinalStsudyofCombosquattingAbuse}, mobile apps~\cite{hu2020mobileSquatting}, blockchain~\cite{muzammil2024BlockchainTyposquatting}, and container registry~\cite{liu2022exploringContainerRegistryTyposquatting}.
\fi

\iffalse
proposed \texttt{Typomind}, a system that employs 12 heuristic rules for detecting typosquats~\cite{neupane2023beyondTyposquatting}.
Of particular note is their approach to addressing the ``semantic substitution'' class of attacks (\cref{tab:typosquat_taxonomy}).
They used FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to encode each element of a package name into high-dimensional vectors.
By analyzing cosine similarity between these vectors, a wider range of confusion techniques could be detected.
\fi

%to package typosquatting was Taylor \etals \texttt{Spellbound} which was integrated into the installation pipeline to protect NPM users from potential typosquatting attacks~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}.
%Vu \etal applied Levenshtein distance to identify PyPI package typosquatting attacks~\cite{vu2020typosquatting}.
%Their scheme was based on lexical similarity --- they detected typosquats by identifying minor textual alterations in package names.


%Effective defenses against typosquatting attacks in package registries require solutions that balance accuracy with practical deployability. Taylor \etal introduced \texttt{Spellbound}, a lexical similarity based tool designed to detect typosquatting by identifying minor textual alterations in package names~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}. To address more complex patterns of similarity, recent approaches have incorporated embedding-based techniques.
%Notably, Neupane \etal proposed \texttt{Typomind}, a system that employs 12 heuristic rules for detecting package confusion~\cite{neupane2023beyondTyposquatting}.
%Among these, one key rule utilizes FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to encode split naming components of packages as high-dimensional vectors. By analyzing cosine similarity between these components, \texttt{Typomind} effectively identifies near-duplicate or subtly altered names that traditional edit-distance methods or simple lexical approach might fail to detect~\cite{neupane2023beyondTyposquatting}.
% \PA{I feel I would be more interested in if these approaches support or dont support hierarchical naming conventions (not if they investigated it). Also, more specificity on how they struggle with hierarchical names may be helpful.}




%\subsubsection{Industry Defenses}
%\label{sec:back-defense-industry}











% The emergence of ``sleep'' typosquatting attacks --- where packages with similar names and functionality silently inject malware and compromise dependents --- poses a significant threat to the supply chain. These so-called \textit{defensive typosquatting} attacks exploit the allowance of similar package names by registries, creating a broader attack surface.


% \WJ{@Mik, could you check this paragraph and let me know if it looks good and accurate?}
% \TODO{Mik:
% for similar tools, there are a bunch of typosquat tools for urls, but not much for packages.  all of the registries have some internal measures for preventing typosquats, but they are generally not sufficient
% \\
% note that NPM does not actually say how they classify typosquats, but from what i gather in conversations, they just use levenshtein distance and also hold a few popular names internally
% \\
% the main registry is not well staffed and is more reactive than proactive at handling typosquats
% https://docs.NPMjs.com/threats-and-mitigations#by-typosquatting--dependency-confusion}
% \PA{Maybe use the same frontend vs backend to analyze production defenses. Providing tools for users sounds like front-end support. More work is needed to help registries identify, flag or remove these packages.}
% \WJ{@Berk, one sentence each please.}
\iffalse
One class of industry tools supports engineers at package installation time.
% \JD{Fill in some details based on the links, or say ``no details provided''.}
Socket defends against typosquatting by providing real-time detection
% of malicious packages in a software project during pull requests, inspecting introduced dependencies via their GitHub app. Furthermore, their CLI tool can be used as a prefix to \texttt{NPM install}, proactively analyzing dependencies at installation-time~\cite{socket_typosquatting}.
Stacklok's approach is to identify typosquatting by analyzing package names using Levenshtein distance, evaluating repository and author activity metrics, and assigning a risk score through their platforms~\cite{stacklok_typosquatting}.
% GitGuardian...~\cite{gitguardian_typosquatting}.
Microsoft's OSSGadget provides a CLI tool to detect typosquats across multiple ecosystems~\cite{MicrosoftOSSGadget}.
%These solutions often rely on known CVEs or malware scanners.

% \WJ{TODO: In addition, PyPi and NuGet have implemented an automatic typo-correction mechanism to assist users who accidentally type package names with incorrect capitalization. By employing these measures, PyPI and NuGet significantly reduce the chances of users falling victim to deceptive tactics. This approach provides a more secure environment for package distribution and installation. 
% Source: \url{https://checkmarx.com/blog/a-new-stealthier-type-of-typosquatting-attack-spotted-targeting-NPM/}
% }

Some software package registries also seek to detect typosquatting.
\texttt{NPM} uses Levenshtein-based detection to identify and block package names that are deceptively similar to popular packages, thereby preventing typosquats from entering the registry~\cite{NPM_threats_mitigations}.
\texttt{PyPI} has an impersonation policy, which prohibits deceptively similar usernames, organization names, and project names, reducing the risk of typosquatting and related attacks~\cite{psf_acceptable_use_policy}.
In addition, all major SPRs remove malware (including typosquat packages) when they become aware of it~\cite{ferreira2021containing, guo2023empiricalStudyonMaliciousCodeinPyPI, gu2023investigatingPKGRelatedSecurityThreatsinSRs}.
However, some typosquats are subjective; in the absence of a clear malware signal, human analysts remain necessary to triage reports.
%However, these measures are often insufficient and reactive due to resource constraints.
\fi

%These limitations highlight the need for robust techniques that combine advanced detection with effective takedown policies, as demonstrated in our findings from real-world deployments. Furthermore, existing research has largely overlooked typosquatting in hierarchical registries like Maven, Golang, and Hugging Face~\cite{mavenWeb, GolangWeb, HuggingFaceWeb}.
%To bridge this gap, we present a system model specifically designed for SPRs and provide valuable insights into additional underexplored registries.

\iffalse
\subsubsection{Gap Analysis and Contributions}
\label{sec:back-defense-gaps}

%New typosquatting packages continue to stymie existing  defenses~\cite{infosecurity_new_typosquatting_2024, socket_new_typosquatting_2025}.

%In the open-source community, package registries --- being central to all package distribution --- are uniquely positioned to provide critical support in combating these attacks.
\TODO{I need to remove this subsectin and move gaps to previous subsections in background based on the feedback from lab meeting.}
Our work addresses several gaps across the existing knowledge described in~\cref{sec:back-defense-academic,sec:back-defense-industry}.

\begin{enumerate}[left=0.1cm]
\item \textbf{High false positive rate:}
  Existing tools say that only $\sim$0.1\% of their reports were malware~\cite{neupane2023beyondTyposquatting} because they do not have a clear definition of typosquatting false-positive.
  % \JD{Comment here about accuracy and whether the issue is false positives or false negatives or both.}
  What is the real typosquatting false positive and how to reduce the rate remains an open problem.
\item \textbf{Limited registry focus:}
  So far, the major research papers have focused on only 3 SPRs: NPM, PyPI, and RubyGems~\cite{vu2020typosquatting, taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}.
  As indicated by~\cref{tab:registry-overview}, there are many SPRs and these SPRs vary in typosquat-relevant ways --- \eg how names are constructed and whether popularity information is available.
\item \textbf{Limited insight from SPR perspective:}
  Both academic defenses and many industry tools are focused on supporting the individual engineer with engineer-side tooling based on limited local information.
  Such tooling is certainly useful, but we believe it is a stopgap until SPRs or other ecosystem players can deploy a viable typosquat detection system at scale to proactively prevent typosquat creation or automatically take them down more quickly.
  False positives are particularly problematic for such a venture because inaccurate notifications erode trust ecosystem-wide.
  We lack reports from production deployments to understand what challenges arise. % and what lessons can be shared across the industry.
\end{enumerate}

We address these gaps as follows.
First, we conduct a novel analysis of typosquat true and false positives in order to improve accuracy over prior work.
Second, our work expands on prior work to consider 6 registries, which (to the best of our knowledge) represent all major typosquat-relevant variations across all public SPRs.
Third, we have deployed our system in collaboration with an industry partner and share lessons learned.
\fi

% \Andreas{This paragraph refers to ``frontend'' applications and ``backend'' environments. I think they should be defined, especially since a major part of your contribution is a focus on the backend. Perhaps in \$2.1 you can describe the actors / components involved in software installation via registries so that you can now refer to the components the prior work focuses on.}
\iffalse
Considering the \ul{target} of the defense mechanisms, current state-of-the-art techniques are primarily designed for frontend users during the package installation process~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting, MicrosoftOSSGadget}. While these highly sensitive detection systems are effective at identifying potential threats, they often produce substantial false positives during large-scale deployment.
In such systems, inaccurate notifications can harm the reputation of legitimate contributors and organizations, therefore eroding trust within the ecosystem.
False positives are more manageable in \textit{frontend} applications, where they serve to alert end users to potential risks when installing packages. However, in \textit{backend} environments, they pose significant challenges --- systems with a high false-positive rate are unsuitable as registries alarms, or \textit{blocking policy}, which is highly desirable and beneficial for SPRs.
To achieve this goal, we first developed a taxonomy that extends beyond simple naming patterns by incorporating package metadata, enabling a more robust and comprehensive threat detection framework.
We propose a backend solution that emphasizes accuracy, significantly reducing false-positive rates while maintaining operational efficiency.
\fi

\iffalse
However, none of these approaches have investigated typosquatting attacks on SLRs with hierarchical naming conventions, relying solely on package names --- a method that can be prone to inaccuracies and lead to high false-positive rates.
They also claimed that their system is designed to detect potential package confusion and serves solely as a warning mechanism.
In contrast, our approach leverages package metadata and extends state-of-the-art analysis across six ecosystems, achieving a significantly lower false-positive rate. This makes it suitable not only as a warning system but also potentially as a \textit{blocking policy} for SPRs.
\fi










\iffalse

\WJ{Below is old content.}

The typosquatting problem has been a persistent challenge in the software supply chain, prompting extensive research and development of defenses aimed at safeguarding supply chain security~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, vu2020typosquatting}. These defenses encompass a range of strategies, including string-based~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, MicrosoftOSSGadget} and embedding-combined approach~\cite{neupane2023beyondTyposquatting} approaches, alongside other innovative methodologies, such as keyboard typing based ~\cite{sern2021typoswype, le2019smorgaasbordofTypoes}.
Though there are some state-of-the-art solutions deployed in production environments, but specifically used for detecting domain name typosquatting and package typosquatting attack in practice remains unclear~\cite{, }.
\paragraph{String-Based Methods}

A straightforward approach is to measure string similarity between a suspicious package name and known popular targets using edit-distance metrics (\eg Levenshtein distance~\cite{levenshtein1966binary}). For example, \texttt{SpellBound} by Taylor \etal~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting} flags packages that lie within a small edit distance of widely used libraries. While simple and transparent, these methods can yield excessive false positives in repositories where many legitimate packages share overlapping or derivative names.

\paragraph{Embedding-Based Methods}

To capture more nuanced phonetic and semantic resemblances, recent work has turned to embedding-based detection. For instance, Neupane \etal~\cite{neupane2023beyondTyposquatting} propose \texttt{typomind}, which employs FastText embeddings~\cite{bojanowski2017FasttextEmbedding} to represent package names as vectors in a high-dimensional space. By comparing the cosine similarity of these embeddings, \texttt{typomind} uncovers near-duplicate or deceptively modified names that naive edit-distance checks might overlook. However, as with most highly sensitive detectors, large-scale deployment can still produce significant false alarms without further refinement.

Despite these advances, not all registries supply the same level of detail about packages, and some---particularly Maven or certain AI model repositories---offer only limited data regarding package popularity or ownership. Consequently, balancing coverage (high recall) with false-positive suppression remains an open challenge in practical deployments. Addressing these inconsistent signals and curbing superfluous alerts is central to our approach.

\paragraph{Other typosquat defenses}

\paragraph{Production Defense}
Microsoft has published an open-source supply chain defense package called OSSGadget which can  ~\cite{MicrosoftOSSGadget}
\fi

% \PA{Based on the last paragraph, it sounds like suppression of false positives is central to your approach. I think this should be more emphasized in the introduction.}

% \PA{So far, it seems like prior typosquatting works do not properly consider hierarchical package registries or detect your new classes. Is this true? if yes, I think it should be more emphasized in the intro and this comparison to related works.}

\section{Problem Statement}
\label{sec:ProblemStat}

In this section, we give
  our definition of package confusion threats (\cref{sec:ProblemStatement-Definition}),
  our threat model (\cref{sec:SystemandThreatModel}),
  and
  our system requirements (\cref{sec:ProblemState-SysReq}).
% then investigate three key aspects: attacker practices (\cref{sec:PracticeAnalysis}), measurable signals in suspicious packages from prior work (\cref{sec:ProblemState-CaseStudy2-FP}), and system requirements in production (\cref{sec:ProblemState-SysReq}), alongside the overall system and threat model (\cref{sec:SystemandThreatModel}).

% and deployment requirements (\cref{sec:ProblemStatement-SysReq}).
% We then define our system and threat model (\cref{sec:SystemandThreatModel}. By integrating insights from both attacker behaviors and legitimate engineering patterns, we refine typosquatting definitions, establish key requirements for live detection, and enhance real-world applicability (\cref{sec: ProblemState-SysReq}).


\iffalse
Thus, we ask:
\begin{RQList}
    \item \textbf{RQ1} What are the malicious attackers' practices?
    \item \textbf{RQ2} What measurable attributes can be used to assess the intent of suspicious typosquatting packages?
    \item \textbf{RQ3} What system requirements are essential for live typosquatting detection?
\end{RQList}
\fi


\subsection{Refined Definition of Package Confusion}
\label{sec:ProblemStatement-Definition}
% Integrating our findings from RQ1 and RQ2, 
We refine the definition of package confusion threat by emphasizing the manifestation of malicious intent.
Our refined definition is supported by our empirical analysis of real attacks in \cref{sec:PracticeAnalysis}.

\begin{tcolorbox}[title=Definition: Package Confusion Threat]
\textbf{A package confusion threat} is a software package whose name mimics a trusted resource, with the intent of deceiving developers into installing code that is \ul{actively} malicious or \ul{may become so (stealthy)}.\footnotemark
\end{tcolorbox}

\footnotetext{For comparison, Neupane \etal wrote: \textit{``Package confusion attack: a malicious package is created that is designed to be confused with a legitimate target package and downloaded by mistake''}~\cite{neupane2023beyondTyposquatting}. 
This requires the package to be actively malicious.} %, and even includes benign packages with adversarial names, whereas ours explicitly covers both attack types and encompasses mimicry that is either syntactic or semantic.}

\noindent
The mimicry may be \textit{\ul{lexical}} (\ie typosquatting), \textit{\ul{syntactic}}, or \textit{\ul{semantic}}, but the intent is to confuse an engineer.

Because our definition considers threats, not just attacks, the current absence of malware does not imply future innocence.
In contrast, prior work relies on malware detectors and only finds active attacks~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting,neupane2023beyondTyposquatting}.
%In contrast, prior research only considers detected package confusion attacks as true positives when they contain malicious code, while the content and intent behind the false positives remains under explored.
Incorporating stealthy threats into this definition raises the possibility of high false positive rates.
%Without being able to use malware signatures or CVEs to pinpoint active package confusion attacks, our
%inclusion of intent raises the possibility of high false positive rates.
To address this, we analyzed false positives from prior package confusion data (\cref{sec:ProblemState-CaseStudy2-FP}) 
% \JD{I thought we were looking at Neupane which includes semantic?} 
to identify signals that distinguish benign packages from stealthy threats.

%As a result, typosquat attacks are counted as ``true positives'' only if they contain or deliver explicit malicious payloads.
%An attacker could publish a benign fork that adopts a misleadingly similar name, only to introduce malicious code in a later version.
% Hence, \emph{our system flags name-based threats even when immediate harm is absent}.

%\paragraph{Comparisons with definitions in prior works:}
%We distinguish our refined definition with those used by other typosquatting detection research.

% To answer these questions, we first analyze attacker strategies (\cref{sec:PracticeAnalysis}), investigate the intent, and summarize the measurable attributes in suspicious packages (\cref{sec:ProblemState-CaseStudy2-FP}). 


% We first report on several complementary investigations into attacker behaviors and benign engineering behaviors.
% Our findings allow us to refine the definition of typosquatting, extend the state-of-the-art taxonomy, and learn legitimate behaviors that lead to false positives.

%introduces the practices of typosquatting attacks (\cref{sec:PracticeAnalysis}) and refined definition in our work.

\subsection{Threat Model}
\label{sec:SystemandThreatModel}

We describe the context and threats considered in this work.

\textbf{Context Model:}
We focus on software package registries, including SPRs hosting traditional software packages and pre-trained AI models.
The relevant properties are given in~\cref{sec:background-SPR-PkgNaming}.

%These registries allow users to publish and share software artifacts (\ie traditional packages and pretrained models) with other users. %, thereby facilitating software reuse.
%\cref{tab:registry-overview} summarizes relevant aspects of these SPRs. %demonstrate their importance across software development communities.

\textbf{Threat Model:}
%We focuses on attackers who can publish packages to software package registries and use the published package to deliver malicious code to unsuspecting users and applications.
We include some threats and exclude others.

\begin{itemize}[left=0.05cm]
    \item \textit{In-scope:} We consider attacks where packages with deceptively similar names to legitimate packages are published in the software package registry. 
    As our definition permits, the attacker may initially publish non-malicious content (stealthy threat) and later introduce malware (active threat).
    \item \textit{Out-of-scope:} We exclude attacks using non-confusing package names, or that compromise existing legitimate packages.
    These attacks must be mitigated by other measures~\cite{okafor_sok_2022}.
    % We also exclude \texttt{command squatting} from our scope, as this attack targets registry commands, which fall outside the bounds of our defined focus.
\end{itemize}

\noindent
This threat model is stronger than those of existing package confusion detection techniques~\cite{vu2020typosquatting, taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}, because we permit attacks to start with stealthy (non-malicious) package content.

\iffalse
which assume attackers exploit typing errors to mislead users and capture installs, whereas we assess both names and package content.
% \JD{Cite the specific papers here}.
% They only consider typosquat packages with malicious code (\ie active threats, not stealthy ones).
%This threat model also protects against currently benign typosquat packages that can introduce malicious code in the future.
\cref{fig:TPAnalysis} demonstrates stealthy threats in the wild, with malicious code introduced months after the original package is published. %, and after it may have gotten tens or hundreds of downloads.
\fi

\subsection{Requirements}
\label{sec:ProblemState-SysReq}
A detection system must meet both security and operational goals. 
% These requirements ensure robust identification of malicious intent, compatibility with diverse ecosystems, and practical performance for real-world deployment. 
Working with our industry partner, we identified four requirements:

\myparagraph{\ul{Req$_{1}$}: High precision and recall.}
A threat detection system must balance
  precision (low false positive rate) against recall (low false negative rate).
False negatives enable attacks, but false positives lead to alert fatigue~\cite{ahrq_alert_fatigue}. 
%:Existing systems have an unacceptably high false positive rate (\TypomindFPRate, \cref{sec:ProblemState-CaseStudy2-FP}) and their false negative rate is hard to assess due to limited ground truth.
%effectively capturing true positives while maintaining a low false-negative rate
%Additionally, to avoid “alert fatigue” and protect the reputation of benign package contributors, the system must minimize unnecessary alerts, aiming for a false-positive rate of only a few percent. Packages with similar names, such as legitimate forks\cite{wyss2022whatTheForkFindingHiddenCodeClonesinNPM}, should not be flagged unless they exhibit clear signs of malicious intent.

\myparagraph{\ul{Req$_{2}$}: Efficient and Timely Detection.}
The system must scale to large registries --- low-latency checks enabling real-time feedback.
% It must deliver acceptable throughput and latency to enable large-scale scans and real-time classification, all while ensuring cost-effectiveness.

\myparagraph{\ul{Req$_{3}$}: Compatibility Across Ecosystems.}
The system must support many SPRs with their diverse naming schemes (\cref{sec:background-SPR-PkgNaming}). %, including both 1-level and hierarchical names. %, along with comprehensive metadata integration for threat analysis.
% from single-level systems
% (\eg NPM, PyPI) to hierarchical ones (\eg Maven, Hugging Face).
% The system should efficiently leverage internal and external data sources to retrieve popularity metadata.

% \myparagraph{\ul{Req$_{4}$}: Frequent Metadata Updates}
% The system must regularly ingest data from sources like package registries, version histories, and domain checks to maintain up-to-date threat intelligence. This ensures developers are promptly informed about suspicious packages released since the last scan, independent of popularity metrics.

\myparagraph{\ul{Req$_{4}$}: High Recall of Stealth Package Confusion Attacks.}
The system must identify both active and stealth package confusions.


% \paragraph{Req$_{6}$: Manageable False-Positive Load}
% To prevent “alert fatigue,” robust verification methods such as code similarity checks, publisher identity assessments, or metadata validation must help distinguish genuine threats from legitimate forks. A precision rate of 95\% or higher is typically targeted, adjustable based on an organization’s risk tolerance.

% \paragraph{Req$_{6}$: Cost-Effective Performance}
% The system must optimize throughput and latency to enable large-scale scans and real-time classification while keeping costs manageable.
% For instance, it should handle 1 million packages within six hours on a moderately provisioned server and maintain sub-second response times for CI workflows.

\section{Analysis of Confusion Attacks}
\label{sec:EmpiricalAnalysis}

To address the high false positive rates of previous detection systems for package confusion, Ohm \etal recently called for a deeper analysis of the available evidence~\cite{ohm2023SoKPracticalDetectionofSSCAttacks}.
We respond to their call in two ways:
  describing attackers' practices (\cref{sec:PracticeAnalysis}),
  and
  reporting metadata features that distinguish true from false positives (\cref{sec:ProblemState-CaseStudy2-FP}).

\iffalse
have relied on Levenshtein distance~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting} and heuristic checks~\cite{neupane2023beyondTyposquatting} to detect package confusion threats. 
However, these approaches often suffer from high false-positive rates, lack comprehensive content analysis of confusing packages, and support only a limited range of SPRs.
To achieve generalizable, scalable, live detection across SPRs, we must first understand both the underlying practices of attackers (\cref{sec:PracticeAnalysis}) and the engineering approaches behind confusing packages (\cref{sec:ProblemState-CaseStudy2-FP}).
\fi


\subsection{Attackers' Practices}
\label{sec:PracticeAnalysis}

% Although previous research has proposed various methods for detecting package confusion attacks, the issue persists across multiple registries.
We examined first the role of stealth in confusion attacks, and then the attack variations in SPRs beyond NPM, PyPI, and RubyGems.
Our findings informed requirements 1, 3, and 4 (\cref{sec:ProblemState-SysReq}).

%To better understand the engineering practices, we analyzed real attacks and benign false positives, identifying key shortcomings in previous work.

%However, this taxonomy was created based on open-source data collected from only three ecosystems that employ single-level naming structure --- NPM, PyPI, and RubyGems --- and has not been evaluated for adaptability to SPRs with hierarchical naming conventions, such as Maven, Go modules, and Hugging Face.
%Extending this work, our study explores how these manipulations can be adapted to hierarchical naming ecosystems, including emerging AI model registries, shedding light on the evolving threat landscape across diverse platforms.


% \subsubsection{Analysis of Attackers Behavior}
% \subsection{Case Study 1: True-Positive Analysis}
\label{sec:ProblemState-CaseStudy1-TP}

% \JD{This material feels very strange here. I think we should move this and the next subsection to \$3.}

\subsubsection{Stealthy Confusion Attacks}
We analyzed the versions of the 240 
% \JD{the number is 240, right? that's what Figure 2 says} 
confirmed NPM package confusion ``true-positives'', defined as \ul{confusing packages that include malware}. These data were collected and reported by Neupane \etal~\cite{neupane2023beyondTyposquatting}. We analyzed the number of days these malicious packages were available before malware was injected.
%Due to the lack of package content (the SPR removed the packages),
We estimated the injection time based on the last version updated before removal by the SPR.

\Cref{fig:TPAnalysis} illustrates our findings on the release time before malware injection.
Most package confusion attacks injected malware on the first day of release, \eg in the initial version.
However, in 13.3\% of the packages, attackers undertook stealth attacks, deploying malware after days or weeks.
Attackers may use this strategy to grow a userbase before exploitation. %evade detection for extended periods, increasing the likelihood of successful exploitation before the malicious activity is discovered.
% \JD{I added the next sentence, please check it.}
Prior work on package confusion did not account for this behavior. % nor account for it in their system designs.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/days_historgram_updated.pdf}
    \caption{
    Distribution of time until malware, in packages with confusing names.
    13.3\% (32/240) of attacks occur $\geq$5 days after release.
    }
    \label{fig:TPAnalysis}
\end{figure}

% \PA{Do you want to add a sentence or two about how these additional attack patterns were identified. I am wondering if these are all the new patterns we should be concerned about (completeness).}
% \BC{Which risks?}
% \WJ{TODO: How to be systematic.}
\subsubsection{Extending the Taxonomy to New SPRs}
% \WJ{@Jamie Is this paragraph okay as data source?}
% \JD{Feels a bit weird --- notably because typically taxonomies are developed through iterative card sort type methods. Mikola is on the author team and so this paper can be written with the perspective of Socket. I suggest that you say instead ``We analyzed the existing taxonomy and some real attacks and concluded that two-three additional categories would be needed'' --- here the `we' is `Socket'. But to extend a taxonomy you must argue that these don't fit into the existing taxonomy. Right now you are just saying ``because we said so''. Why is impersonation not captured already? Why not compound? I think that `command squat' in particular looks like it is described by the existing taxonomy --- the difference is not in how the text is changed (add/remove a hyphen) but in which thing is similar (another package or another ecosystem construct like the command line installer's options). I think you should just drop command squat, it seems hard to defend taxonomically and your system doesn't support it anyway right?.}
Prior work has examined malicious naming strategies in only a few SPRs.
Our industry partner's analysts have studied package confusion attacks in many other SPRs, including all those in~\cref{tab:registry-overview}.
We discussed Neupane \etals taxonomy with them, and compared it to the real-world attack patterns they have observed.
Through this dialogue, we identified four additional mechanisms for package confusion attacks.
Definitions follow; see~\cref{sec:background-Taxonomy} for examples.
%Our findings indicate that in registries with hierarchical naming conventions (\cref{tab:registry-overview}), additional categories of package confusion emerge that are not fully captured by the current taxonomy. Based on our analysis, we propose the following extensions:
% To get a better understanding of the SOTA typosquatting attacks, we shared the existing taxonomy with a software supply chain security company team and sought their feedback on gaps in the current taxonomy, based on their observations of emerging threats across multiple SPRs.
% Following discussions with their team, we extended the package confusion taxonomy introduced by Neupane \etal~\cite{neupane2023beyondTyposquatting}, adding two new categories specifically targeting hierarchical naming systems (\cref{tab:typosquat_taxonomy}). These extensions help to capture additional attack vectors that exploit the structural complexities of modern SPRs:

\begin{itemize}[leftmargin=*, itemsep=0.1ex]
  \item \textbf{Impersonation Squatting.}
    In hierarchical SPRs, attackers impersonate legitimate maintainers or organizations by registering a similar \texttt{author} or \texttt{groupId}.
  %, then publishing malicious packages with well-known project names.
  %For example, ``\ul{meta}-llama/Llama-2-7b-chat-hf'' vs. ``\ul{facebook}-llama/Llama-2-7b-chat-hf''.
  \item \textbf{Compound Squatting.}
    Making multiple coordinated edits to a hierarchical name, \eg altering both scope and delimiters at once.
    We consider this distinct because the compounded changes complicate conventional lexical comparisons. 
  %For example, \texttt{@typescript-eslint/eslint-plugin} becoming \texttt{@typescript\_eslinter/eslint}. %, merging multiple confusion tactics into a single new name.
  \item \textbf{Domain Confusion.}
    In SPRs where package names include URLs (golang), attackers may register domains resembling those of trusted mirrors or proxies.
  %For example, \texttt{github.com/prometheus/prometheus} vs. \texttt{git.luolix.top/prometheus/prometheus
  \item \textbf{Command Squatting.}
    Registering package names that mimic command-line options in other packages or utilities.
  %For example, NPM package \texttt{help} (\texttt{NPM i help}) impersonate the command line argument for help (\texttt{NPM i --help}).
\end{itemize}

\subsection{Metadata Features of Package Confusion}
\label{sec:ProblemState-CaseStudy2-FP}
% \subsection{Case Study 2: False-Positive Analysis}

% \WJ{I don't think FP data is accurate but couldn't figure out a better name. Maybe suspicious data?}
% \Andreas{This subsection seems out of place - I would have expected it in \$3, not in a ``Requirements'' section.}
% \subsubsection{Preliminary Analysis}

% \TODO{Move this subsection to appendix and only talk about the taxonomy we identified here.}

% \JD{This material feels very strange here.}

% \subsubsection{Motivation.}
%Name similarity alone fails to capture \emph{stealth attacks}, leading
As discussed in~\cref{sec:background-Defenses}, almost all package confusion detectors make use only of the names of packages.
While this allows those systems to avoid costly content analysis, it leads to high false positive rates.
We propose to integrate metadata to distinguish true from false positives.
Although metadata signals can be circumvented, our approach aims to raise the attacker's cost, reflecting the typical cat-and-mouse dynamics in open package ecosystems~\cite{Ladisa2023TaxonomyofAttacksonOSSSC}.
%We therefore performed the first analysis of the causes of false positives. %, seeking to discern metadata 
%They also suffer from high false-positive rates, with over 99.9\% of flagged packages not containing malware~\cite{neupane2023beyondTyposquatting}. 
%\WJ{Is this in the right place?}
%To improve detection accuracy, it is crucial to analyze the characteristics of suspicious packages and identify measurable indicators that can reliably distinguish true attacks from benign false positives.




%These false positives, defined as \ul{possible typosquatting packages without malware injected}, often include legitimate forks, relocated packages, or developer tests that pose no actual threat to the software supply chain (\cref{sec:ProblemStatement-Definition}).

% \subsubsection{Methodology.}
% \PA{What question are you trying to answer here?}

{
\renewcommand{\arraystretch}{0.3}
\begin{table*}[h]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\caption{
Overview of the 13 metadata-based verification rules. Each rule includes a description of its purpose and the specific implementation steps taken to verify flagged packages. The final three rules (R12--R14)
% \JD{The final two rules are R9, R10 , not R8 and R9?}
were added as part of our refinement process based on further observed false-positive patterns after we deployed our system in production.
\WJ{I think the content of this table can be improved.}
% \WJ{Can we move this table into appendix? I think the rules are overlapped with our taxonomy but the implementation details are not.}
% \JD{We do not have an ablation of this, right?}
% \WJ{TODO: Ablation}
% \WJ{Add measurable attributes as the second column?}
% \WJ{Update this based on latest metric name in Fig 5}
% \JD{The numbers in this table are not right (only 12 entries?) and the caption says 13. Also we jump from R8 to R10. Later in the text we also write ``R1--13'' or something, so make sure such references are also up to date.}
}
\label{tab:metadata_rules}
\scriptsize
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{p{0.2\textwidth} p{0.33\textwidth} p{0.44\textwidth}}
\toprule
\textbf{Rule} & \textbf{Description} & \textbf{Implementation} \\
\midrule
\textbf{R1: Obviously unconfusing}
& Determine if a package name conveys an intentional, brand-specific identity (e.g., \texttt{catplotlib}), clearly differentiating it from deceptive imitations.
& Cross-reference flagged names with known legitimate projects and use LLM analysis to assess whether the naming convention is deliberate and unambiguous for developers.\\
\\
\textbf{R2: Distinct Purpose}
& Distinguish packages with different functionalities, even if names are superficially similar (\eg \texttt{lodash-utils} vs. \texttt{lodash}).
& Extract package descriptions and calculate semantic similarity using TF-IDF cosine scores. A score below 0.5 indicates distinct purposes, reducing suspicion of deception. \\
\\
\textbf{R3: Fork Package}
& Detect benign forks sharing near-identical code or metadata with a popular package.
& Compare README files, version histories, and file structures for high overlap. Similarities without malicious edits suggest harmless forks. \\
\\
\textbf{R4: Active Development/Maintained}
& Determine if packages are frequently updated or actively maintained by multiple contributors, which are less likely to have malicious intent.
& Retrieve metadata for the last update, commit history, and version count. Classify packages with recent updates (\eg within 30 days) or more than five versions as legitimate. \\
\\
\textbf{R5: Comprehensive Metadata}
& Identify packages missing critical metadata elements, such as licenses, maintainers, or homepages, which are typical of legitimate projects.
& Check for the presence of licenses, contact details, and repository links. 
% Flag packages missing two or more of these elements as potential typosquats. 
\\
\\
\textbf{R6: Overlapped Maintainers}
& Distinguish legitimate extensions or rebrands by verifying if the flagged package shares maintainers with the legitimate one.
& Match maintainer identifiers (\eg email, GitHub handle) between flagged and legitimate packages. Overlapping maintainers suggest legitimate intent. \\
\\
\textbf{R7: Adversarial Package Name}
& Filter out name pairs with significant length differences, as these often indicate unrelated projects rather than covert mimicry.
& Compare string lengths of flagged and legitimate package names. A difference exceeding 30\% indicates likely unrelated naming.\\
\\
\textbf{R8: Well-known Maintainers}
& Trust packages maintained by reputable and recognized authors/organizations.
& Leverage knowledge in LLM training data to identify if a maintainer is trustworthy in the community.\\
\\
\textbf{R9: Clear Description}
& Verify that each package includes a README or equivalent documentation detailing its purpose, usage, and key features.
& Analyze repository metadata to ensure there is a clear, summarized description of the package; packages lacking such documentation should be flagged for further review. \\
\\
\textbf{R10: Has Malicious Intent} 
& Identify packages that deliberately mimic legitimate ones through near-identical descriptions, or have obviously suspicious content.
& Use LLM to analyze the package name and description to triage whether there is obvious malicious intent in the package. \\

\\
\textbf{R11: Experiment/Test Package} 
& Identify packages that are used for test or experiment purposes only.
& Use LLM to analyze package descriptions and determine whether it is an experiment/test. \\

\\

% If a flagged package is published by a recognized maintainer (\eg \texttt{expressjs}), it is considered legitimate regardless of name similarity.
% For example, \texttt{expressjs/body-parser} is trusted even if a similarly named package like \texttt{body-parser} exists.
% \sout{\textbf{R6: Consistent Quality Metrics}} & \sout{Detect artificially boosted popularity by comparing download \midrule against community engagement metrics (\eg commits, contributors).} & \sout{Examine ratios such as downloads-per-contributor. Discrepancies (\eg high downloads but low activity) indicate potential typosquats.} \\
% \\
\midrule
\textbf{R12: Package Relocation}
& Account for legitimate package relocations, common in hierarchical registries like Maven.
& Parse metadata files (\texttt{pom.xml}) for \texttt{<relocation>} tags or analogous fields. Identify and ignore renamed or migrated projects. \\
\\
\textbf{R13: Organization Allowed List}
& Prevent false positives by excluding packages published by trusted or verified organizations.
& Maintain an allowedlist of approved organizations. If a flagged package is published under an allowed organization (\eg \texttt{@oxc-parser/binding-darwin-arm64}), it should be considered legitimate, comparing to \texttt{binding-darwin-arm64}. \\
\\
\textbf{R14: Domain Proxy/Mirror}
& Account for legitimate proxies or mirrors in the ecosystem, common in hierarchical registries like Golang. 
& Maintain an allowed list of recognized domains that serve as proxies or mirrors. If a given package is published under a valid domain (\eg \texttt{gopkg.in/go-git/go-git}), consider it legitimate when compared to its primary source (\texttt{github.com/go-git/go-git}). \\
\\
\bottomrule
\end{tabular}
}
\end{table*}
}

\subsubsection{Method}
% \BC{I think numbers doesn't hold. I remember we had 665 samples and 99\% confidence level}
%To understand the causes of such package names,
% \JD{randomly sampled? otherwise you don't get to talk about confidence levels}
We randomly sampled \SampledTypomindFPNum packages from the original \TypomindFPNum false positives reported by Neupane \etal~\cite{neupane2023beyondTyposquatting}, which have suspicious package names but are not flagged as malware in their respective security analyses.
This sample size gives a confidence level of \FPPrelimDataConfidenceLevel with a margin of error of \FPPrelimDataMarginError on the resulting distribution of causes.

To identify possible metadata signals in the existing package confusion data,
% false-positives as defined in \cref{sec:ProblemStatement-Definition},
we began by having two researchers --- experts in software supply chains --- analyze 200 of these packages.
They analyzed each package’s metadata (READMEs, maintainers, versions, etc.).
Each analyst independently proposed possible features based on this analysis (codebook) and then they discussed this codebook together to reach agreement on terms and definitions.
To test valididty, they then independently applied this codebook to the 200 packages and measured agreement using Cohen's Kappa~\cite{cohen1960coefficient}.
The initial inter-rater reliability was  \InterRaterAgreement (``substantial''~\cite{mchugh2012interrater}).
The researchers subsequently discussed to resolve discrepancies and refine their analysis.
Through discussion, they reached consensus on measurable attributes that could indicate malicious intent or the possibility of a stealthy attack.
% They also identified false-positive categories and additional categories for true-positives.

\iffalse
Based on the initial analysis, the researchers proposed an automated filter to analyze factors such as the presence of a README file and the similarity of package purposes.
This filter was developed with \texttt{gpt-4o} as a filter.
We provide implementation details including our prompt in Appendix~\ref{Appendix-LLMFilter}.
\fi

Based on the high agreement in this process, one of these researchers analyzed the remaining 426 packages.
% assisted by a tool developed by both researchers. %\footnote{Details of this tool are in Appendix~\ref{Appendix-LLMFilter}.}

\iffalse
assisted by the LLM filter, mapping non-malicious factors to identify potential metadata cues for distinguishing legitimate from malicious naming overlaps.
\fi


\iffalse
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/FP_taxonomy.pdf}
    \caption{Taxonomy of engineering practices derived from our empirical analysis. The study was conducted in three rounds: Round 1 and Round 2 (\cref{sec:ProblemState-CaseStudy2-FP}) focused on initial observations, while Round 3 incorporated insights from production data. Further details are available in \cref{tab:metadata_rules}.
    \TODO{Refine this figure in terms of space and readability.}
    \WJ{Consider merging this with \cref{tab:metadata_rules}?}
    }
    \label{fig:FP-taxonomy}
\end{figure*}
\fi

\subsubsection{Results}
Of the 626 packages sampled, 601 were still accessible.
Among these, we identified
  \FPBenignPkgs benign packages (\FPBenignPkgsPercent)
  and 
  what we believe are \FPStealthyPkgs stealthy attacks (\FPStealthyPkgsPercent).
The progression of our results and the manually labeled dataset are available in our artifact (\cref{sec:OpenScience}).
%  This suggests that the tool by Neupane \etal has an approximately \TypomindFPRate false positive rate\JD{be careful about the rest of this sentence --- is this consistent with the FP definition given later on? we flag things that don't have malware and if analyst agrees then it is treated as a true positive...}, given that only 0.1\% of the flagged packages contain malware~\cite{neupane2023beyondTyposquatting}. 
We identified 11 metadata features of the packages used in confusion attacks, as well as benign signals.
These attributes include factors such as a distinct purpose, adversarial package naming, and the comprehensiveness of available metadata.
% \JD{Check the next sentence please}
See~\cref{tab:metadata_rules} for the features leveraged in \tool.

% /\cref{fig:FP-taxonomy} shows the evolution of taxonomy per round.

% This manually labeled dataset was also used as the evaluation dataset in \cref{sec:Eval}.
% We include the dataset in our artifact (\cref{sec:OpenScience}).

%\cref{fig:FP-taxonomy} presents the taxonomy for false positives and true positives in package pairs with similar names.
%The remaining data was deemed reliable, eliminating the need for further review.

\iffalse
Our analysis highlights the need for improved categorization methods to minimize the false-positive rate and effectively classify stealth typosquatting attacks. To address this, we propose an additional requirement:

The frequency of each category is summarized in Appendix~\ref{Appendix-LLMFilter}.
\fi

\iffalse
\textit{Prior works focus on malicious \ul{content}:}
% \BC{change content => malicious content}
Prior research on studying or detecting typosquatting~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting} typically assume typosquatt packages always contain malicious code.
As a result, attacks are counted as “true positives” only if they contain or deliver explicit malicious payloads.
While this approach pinpoints dangerous packages, it can miss harmful \emph{intent}, such as a name that mimics a popular project but has not yet weaponized its code.
Our preliminary analysis in \cref{sec:ProblemState-CaseStudy1-TP} demonstrate the occurrence of these threats in the popular package registries.
% Additionally, a initially benign package could later be compromised by an attacker.
% \PA{Do you have an example to show that this is important?}

\textit{Our work focuses on malicious \ul{intent}:}
% \BC{change intent => malicious intent}
In this work, we expand the definition of a typosquatt threat to cover any \emph{malicious intent} in the package naming strategy itself --- even if the code is not yet malicious:
% \PA{I'll be interested in seeing how you define and measure malicious intent later on.}



\paragraph{False Positives in This Definition:}
One risk introduced by this refined definition is the potential of high false positives due to the lexical similarity of benign package names.
For instance, programming practices like forking frequently result in similar package names~\cite{hadian2022exploringTrendsandPracticesofForks, jiang2023PTMNaming} and a naive implementation of typosquatting threat detection will flag legitimate forks as threats in the software supply chain.
Therefore, we need a \textit{smarter} approach to detect malicious intent between packages of lexically similar names.


With this in mind, we now
clarify how our system determines ``threat''.
We assess various signals
  ---
  similarity of its name to popular packages,
  unusual publisher activity,
  and potential overlaps with known malicious tactics
  ---
  to determine whether a package is \emph{deliberately} masquerading as a legitimate resource or if users should be alerted to the possibility of a potential compromise.
This multi-factor assessment establishes what we call an \emph{threat threshold}:
  packages exceeding that threshold (\ie strongly suggesting deceptive or harmful motivation) are flagged as typosquatting attempts.

% \PA{This definition of false positive sounds very subjective though. I'll be interested in seeing how sound your operationalization and measurements are.}
A flagged package constitutes a false positive if it \emph{does not meet our threshold for malicious intent}—in other words, if it lacks both demonstrably harmful code and any strong indication that it is poised for future malicious use.
Typical examples include legitimate forks, test packages, or brand extensions that resemble well-known projects but pose no real threat.

\begin{tcolorbox}[title=Definition of A False-Positive in This Work]
\textbf{False Positive:}
% \WJ{We consider "packages that is not evidently designed for malicious intent but could be compromised and deceive users as true-positive (with a lower risk level)."
% How can we  modify the definition here to indicate this?}
A package
% flagged by our system
that is neither clearly malicious nor evidently risky for a stealth attack.
\end{tcolorbox}

\noindent
We acknowledge that this definition of false positives introduces subjectivity. To mitigate this, we detail our data-driven design process in \cref{sec:SystemDesign-Step5}.

\fi

\iffalse

\section{Study Context and Threat Model}
\label{sec:StudyContextandThreatModel}
%-------------------------------------------------------------------------------

% \PA{It is unclear how characterizing SPRs relate to threat model.}
% \WJ{Maybe the section heading should be "Study Context and Threat Model"?}
% \WJ{Should we move the ecosystem context into background? Then in the background we have a new subsection for "package registries and supply chain"}
% \JD{Yes, please do. That will help with some of the issues in \$2.}
% \Andreas{I suggest a sentence here motivating this section. Why is a study needed?}
\BC{Change ``encompassing multiple registries'' with ``encompassing multiple ecosystems'', maybe?}
Although typosquatting attacks have long been a persistent issue, effective defense techniques for mitigating them in production environments within SPRs remain scarce. As an initial effort, we developed a system in collaboration with a software supply chain security company that operates an SPR-like platform encompassing multiple registries.
In this section, we present the context of our study by characterizing SPRs (\cref{sec:registry-model}), outlining attacker capabilities (\cref{sec:threat-actor-capabilities}), and summarizing the threat model in this work (\cref{sec:ThreatModel}).

% \Andreas{After reading this section, I'm left wondering whether ``stealth'' typosquatting is a real problem, or hypothetical. Can you motivate the need for a solution for these attacks in this section? I understand that a key distinguishing feature of your solution vs prior work is finding these stealth packages, so I think it should be motivated.}

% \PA{I think you may want to use a more recognized term here (\eg System Model). Then the section heading can be "System and Threat Model". Also, this subsection feels like background information to me.}
\subsection{Ecosystem Context and Registry Model}
\label{sec:registry-model}





% We consider six major public registries that store code libraries or AI models, each with different policies, naming conventions, and exposure to typosquatting. Single-level naming (\eg NPM) identifies packages with a single string, whereas hierarchical conventions
% % (\eg Maven’s \texttt{groupId:artifactId}, Hugging Face’s \texttt{author/model})
% grant more flexibility but also expand the attack surface. In addition to basic character-based typosquatting, we extended the existing package confusion taxonomy created by Neupane \etal~\cite{neupane2023beyondTyposquatting} with two additional categories. Attackers can exploit \emph{Impersonation Squatting} and \emph{Compund Squatting} within hierarchical naming structures (\cref{tab:typosquat_taxonomy}):

% \PA{Are you the one identifying these typosquatting types? Or are they part of an existing taxonomy? It is not clear to me here.}
% \WJ{Making it clearer here.}
\PA{Do you want to add a sentence or two about how these additional attack patterns were identified. I am wondering if these are all the new patterns we should be concerned about (completeness).}
\BC{Which risks?}
We extend the existing package confusion taxonomy introduced by Neupane \etal~\cite{neupane2023beyondTyposquatting}, adding two new categories specifically targeting hierarchical naming systems (\cref{tab:typosquat_taxonomy}). These extensions help to capture additional attack vectors that exploit the structural complexities of modern SPRs:

\begin{itemize}[leftmargin=*, itemsep=0.5ex]
  \item \textbf{Impersonation Squatting.} Impersonating a legitimate maintainer or organization by registering a deceptively similar \texttt{author} or \texttt{groupId}, then publishing malicious packages with well-known project names. For instance, \texttt{meta-llama/Llama-2-7b-chat-hf} vs. \texttt{facebook-llama/Llama-2-7b-chat-hf}~\cite{protectai_huggingface}.
  \item \textbf{Compound Squatting.} Making multiple coordinated edits to the hierarchical name, such as altering both scope and delimiters at once. For example, \texttt{@typescript-eslint/eslint-plugin} becoming \texttt{@typescript\_eslinter/eslint}, merging multiple confusion tactics into a single new name.
\end{itemize}

Popularity metrics, which indicate the usage or visibility of packages, also vary across ecosystems. Registries like NPM, PyPI, RubyGems, and Hugging Face provide direct popularity indicators such as download counts or star ratings. In contrast, Maven and Golang often require external sources to assess package popularity, making them less transparent. These inconsistencies in visibility make it easier for attackers to exploit less-monitored areas of the ecosystem, where suspicious activities may go unnoticed.



\iffalse
\subsection{Model of Ecosystem Context / Software Package Registries}

\JD{Seems like there should be a table here to summarize the registries of interest. This subsection should open by referring to the table. Do you unify these specific examples into a general statement of an SPR? Right now the section does not, but I think you could open with ``Here's our model of an SPR. It's based on our analysis of X registries, summarized in Table Y.'' The rest of the section would then expand on the dimensions included in the table, possibly grouping by dimension (with per-registry analysis) or by registry (going through each dimension, like you currently do). Not sure which one makes more sense.}

\iffalse
\cref{Fig:TODO} shows the open-source package ecosystems or software package registries.

\paragraph{Maven}

\paragraph{Golang}
\href{https://portswigger.net/daily-swig/suspicious-finds-researcher-discovers-go-typosquatting-package-that-relays-system-information-to-chinese-tech-firm}{Suspicious finds: Researcher discovers Go typosquatting package that relays system information to Chinese tech firm}

\href{https://michenriksen.com/archive/blog/finding-evil-go-packages/}{Finding Evil Go Packages
}
"The novel Dependency Confusion attack vector is luckily not something a Go developer has to worry about since the source is always explicitly specified when importing a package, so when Go fetches the external dependency, it can’t be confused about where to fetch it"

\paragraph{Hugging Face}
\fi

\paragraph{Maven:}
Maven is a widely adopted build automation and dependency management tool for Java projects. It relies on a centralized repository system, with Maven Central being the primary public repository. Packages in Maven are uniquely identified by a combination of \texttt{groupId}, \texttt{artifactId}, and \texttt{version}, forming a coordinate that specifies the exact package. The \texttt{groupId} typically represents the organization or author, while the \texttt{artifactId} denotes the specific project or library name. This hierarchical naming convention facilitates organized package management but also introduces potential vectors for typosquatting attacks, targeting either the \texttt{groupId}, the \texttt{artifactId}, or both.

\paragraph{Golang:}
Golang, commonly known as Go, is an open-source programming language developed by Google. The Go ecosystem uses a decentralized module system where packages are retrieved directly from their version control system (VCS) repositories based on import paths that often resemble URLs. Developers explicitly specify the full path to the package, including the domain, repository owner, and package name. This explicit specification reduces the risk of dependency confusion attacks because Go fetches dependencies from the specified locations without ambiguity. However, it opens up possibilities for typosquatting through malicious packages hosted at deceptive domains or repositories with similar names. Recent reports have highlighted such threats, including the discovery of Go packages that relay system information to unauthorized servers~\cite{portswigger2021suspicious} and methodologies for identifying malicious Go packages~\cite{henriksen2021finding}.

The inherent trust in the specified import paths means that if an attacker controls a domain or repository with a misleading name, they can potentially distribute malicious code to unsuspecting developers. This scenario underscores the importance of verifying the authenticity of package sources in Go.

\paragraph{Hugging Face: }
Hugging Face is a leading platform for open-source pre-trained AI models~\cite{Jiang2022PTMReuse, jiang2024peatmoss}, providing repositories for sharing pre-trained models and datasets. Packages in Hugging Face are identified by a combination of \texttt{author} (or organization) names and \texttt{model} or \texttt{dataset} names, forming a hierarchical structure similar to that of Maven. This setup allows for efficient organization and discovery of resources but also presents opportunities for typosquatting attacks. Malicious actors can create models or datasets with names that are slight variations of popular ones or impersonate well-known authors by using similar author names, thereby tricking users into downloading compromised resources.

The open nature of the platform encourages community contributions but also requires vigilance to ensure that shared models and datasets are trustworthy. As machine learning models are increasingly integrated into applications, the risks associated with malicious or tampered models become more significant.
\fi

\subsection{Refined Definition and Novelty}
\label{sec:ProblemStatement-Definition}
\paragraph{Prior work focusing on \ul{content}:} \BC{change content => malicious content}
Prior research identifies ``package confusion'' by comparing package names to well-known libraries or models~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}. Attacks are commonly counted as “true positives” only if they contain or deliver explicit malicious payloads. While this approach pinpoints dangerous packages, it can miss harmful \emph{intent}, such as a name that mimics a popular project but has not yet weaponized its code. Additionally, a initially benign package could later be compromised by an attacker.
% \PA{Do you have an example to show that this is important?}

\paragraph{Our work focusing on \ul{intent}:} \BC{change intent => malicious intent}
We expand the criteria for what constitutes a true threat by prioritizing \emph{malicious intent} in the naming strategy itself—even if the code is not yet malicious:
% \PA{I'll be interested in seeing how you define and measure malicious intent later on.}

\begin{tcolorbox}[title=Definition: Typosquatting Threat]
\textbf{A typosquatting threat} is the deliberate creation of a package (or AI model) with a name that closely mimics a popular, trusted resource, done specifically to deceive developers into installing code that either is malicious, \ul{or stealth (will become so)}.
\end{tcolorbox}

Under this definition, the absence of malware does not imply innocence.
An attacker could publish a benign fork that adopts a misleadingly similar name, only to introduce malicious code in a later version.
Hence, \emph{our system flags name-based threats even when immediate harm is absent}.

\paragraph{False Positives in This Definition:}
Clearly, this definition may result in many false positives based solely on the lexical similarity of package names. This outcome is an inherent challenge given the nature of open-source software development.
For instance, forking is a common practice in the open-source ecosystem, often resulting in similar package names~\cite{hadian2022exploringTrendsandPracticesofForks, jiang2023PTMNaming}.
\BC{OK, I think we should argue that "forking is benign." If we say that attackers can exploit forks, then they are not FPs anymore.}
While such practices are typically benign, malicious attackers can exploit forks to introduce malware into the ecosystem, either directly or by compromising the authors of existing forks~\cite{cao2022WhatThefork}.

With this in mind, we now
clarify how our system determines ``threat''.
We assess various signals
  ---
  similarity of its name to popular packages,
  unusual publisher activity,
  and potential overlaps with known malicious tactics
  ---
  to determine whether a package is \emph{deliberately} masquerading as a legitimate resource or if users should be alerted to the possibility of a potential compromise.
This multi-factor assessment establishes what we call an \emph{threat threshold}:
  packages exceeding that threshold (\ie strongly suggesting deceptive or harmful motivation) are flagged as typosquatting attempts.

% \PA{This definition of false positive sounds very subjective though. I'll be interested in seeing how sound your operationalization and measurements are.}
A flagged package constitutes a false positive if it \emph{does not meet our threshold for malicious intent}—in other words, if it lacks both demonstrably harmful code and any strong indication that it is poised for future malicious use.
Typical examples include legitimate forks, test packages, or brand extensions that resemble well-known projects but pose no real threat.

\begin{tcolorbox}[title=Definition of A False-Positive in This Work]
\textbf{False Positive:}
% \WJ{We consider "packages that is not evidently designed for malicious intent but could be compromised and deceive users as true-positive (with a lower risk level)."
% How can we  modify the definition here to indicate this?}
A package
% flagged by our system
that is neither clearly malicious nor evidently risky for a stealth attack.
\end{tcolorbox}

\noindent
We acknowledge that this definition of false positives introduces subjectivity. To mitigate this, we detail our data-driven design process in \cref{sec:SystemDesign-Step5}.

\iffalse
\subsection{Observed Attacker Behavior}
% Attacker Tactics and Capabilities}
\label{sec:threat-actor-capabilities}
\WJ{Merge this with \$2.3 and cut this.}
% \WJ{This subsection has overlap with threat model.}
% \Andreas{I \textit{think} that the purpose of this section is to describe how attackers have been observed to use typosquatting attacks. Perhaps rename to ``Categorizing Observed Attacker Abuses of Registries'' or similar? The current name sounds like ``Threat Model'' without saying threat model. This may also be background to frame the problem?}
% \TODO{Add more references}
\cref{fig:ThreatModel} illustrates the workflow of a typical typosquatting attack within the software supply chain. Our threat model assumes that attackers can \textit{publish} new packages or \textit{hijack} existing packages in targeted SPRs by leveraging multiple accounts or creating misleadingly named organizations. To gather insights, we conducted a systematic web search using the keyword ``\textit{typosquatting attack AND software package}''. We analyzed the top 50 results published from 2022 to 2024, and summarized the attackers' behavior.

\PA{Hi Wenxin, do you have counts for these objectives and approaches? I think providing counts will demonstrate more transparency.}
The primary objectives of these attacks can be categorized as follows:

% \Andreas{How do you come up with this list of behaviors? Is it exhaustive? Are these from reports of attacker behaviors?}
\setlist[enumerate]{leftmargin=1.2em, itemsep=0.3ex}
\begin{enumerate}
% [leftmargin=*, label=(\arabic*)]

    \item \textbf{Malware Injection~\cite{lyons2024typosquatting, regeski2024typosquattingTargetsPythonDevelopers}:}
    Attackers exploit subtle alterations to package names to deceive users into downloading \ul{\textit{malicious content}}. In single-level registries, these tactics include minor typographical modifications, such as insertions, deletions, or substitutions. For hierarchical registries, strategies extend to author impersonation and reordering of package names, misleading users into installing harmful or incorrect software.


    \item \textbf{Stealth Typosquatting Threat~\cite{zornstein2023stealthy_typosquatting}.}
% \WJ{Maybe ``sleepy'' is not the right word here since I didn't find a good citation...}
% \JD{I don't think there is a term in the literature. I think ``sleeper'' or ``stealth'' would be fine, but I guess `defensive typosquat' is also present in the literature so why not use that?}
\PA{I am not sure these two citations here best represent the prose. They point to stealthier typosquat attack but the uploaded packages seem to be malicious from the start.}
    Attackers create or compromise packages with \ul{\textit{malicious intent}}, explicitly aiming to deliver harmful payloads.
    A common tactic involves ``stealth'' attacks
    % --- also referred to as \textit{defensive typosquatting}~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting} \JD{I'm confused, why would anyone call this behavior `defensive'? Can you reply to this comment and explain?} ---
    where packages initially appear benign to evade detection but are later updated with harmful content.
    % \JD{The next sentence describes behavior that is out of scope for this project. It is a supply chain attack but not typosquatting. So I think it should be omitted here. Could go in Discussion or Limitations or Threat Model.}
    % Similarly to the \texttt{event-stream} incident~\cite{NPM2018eventstream}, attackers may leverage a similar process to compromise trusted packages by targeting their maintainers, either through credential theft or coercion~\cite{nagra2009surreptitious, kaplan2021surveyonCommonThreatsinNPMPyPI}.
    Unlike direct malware injection, these attacks often involve deploying a package with a name similar to a legitimate one, exploiting the proximity in the namespace to target the original package and its users.



\end{enumerate}

\noindent
The key attacker approaches are as follows:
% \WJ{We are focusing on detecting the two kinds of activities but our work is not directly relevant to the approaches. Should we cut it?}
% \JD{Well, with security the reader always wants to know if the attacker can bypass your defense (They always can, so this question might be asked more usefully as ``how much more it would cost them compared to the absence of the defense''). So knowing the typical techniques seems like it would be helpful for analyzing the security properties you achieve. You would typically do that analysis as part of \$5 or \$6, with subsection titled  ``Security Analysis'' that gives a conceptual/theoretical discussion of how much of the attack surface your system is addressing and how easy it might be to bypass.}
% \Andreas{What is the difference between a ``tactic'' and ``approach''? I think that what you're describing are: attacker goals (deliver malware to victim), approaches to achieve the goal (malware injection, stealth typosquatting), and techniques the attacker employs to actualize the approach (name deception, etc.).}

\begin{enumerate}
    \item \textbf{Package Name Deception.}
    Attackers create package names that closely resemble legitimate ones by introducing minor typographical errors, such as misspellings or character substitutions. This strategy exploits users' typing mistakes to trick them into inadvertently downloading malicious packages instead of the intended legitimate ones.




    \item \textbf{Popularity Manipulation.}
    To enhance the credibility of malicious packages, attackers artificially inflate metrics like download counts, stars, and forks. Techniques such as "download farming" and "citation farming" are used to make these packages appear more trustworthy and widely adopted~\cite{he2024FakeStars}.


    \item \textbf{Exploiting Repository Trust.}
    Attackers leverage the inherent trust in repositories by spoofing legitimate domains or manipulating import paths. For example, in Golang ecosystems, they may register domains similar to trusted ones to deliver malicious code. On platforms like Hugging Face, attackers publish trojaned models under authoritative-sounding author names to deceive users into installing harmful content.



    \item \textbf{Dependency Confusion.}
    Dependency confusion is a supply chain attack where attackers publish malicious packages to public registries using names identical to an organization’s internal or private packages. When build systems are misconfigured to prioritize public sources, they inadvertently download and execute these malicious versions, allowing attackers to inject harmful code into the organization’s software~\cite{birsan2021dependencyconfusion}.
\end{enumerate}

% \paragraph{Constraints on the Attacker.}
% We assume attackers cannot forcibly modify legitimate packages owned by others, nor can they directly falsify official popularity statistics at the registry backend. However, ecosystem policies often lack stringent checks on new package names, enabling a range of creative “look-alike” or “sound-alike” naming attacks.

% \paragraph{Security Implications.}
% A successful typosquat in any of these registries can compromise user data, introduce cryptominers, or sabotage AI-based outcomes—especially troubling when integrated deep into CI/CD pipelines or enterprise ML workflows. Through consistent naming deceptions, adversaries exploit both user oversight and automation-driven dependency fetching, making name-based attacks a sustained problem despite improved authentication and scanning practices.

\fi

\subsection{System and Threat Model}

We describe the system we secure and the threats covered in this work.

\textbf{System Model:}
We focus on software package registries, including registries hosting traditional software packages (\eg NPM) and pretrained AI models (\eg Huggingface).
These registries allow users to publish and share software artifacts (traditional packages and pretrained models) with other users, thereby facilitating software reuse.
The number of packages hosted in these registries, as shown in \cref{tab:registry-overview}, demonstrate their popularity across software development communities.

\textbf{Threat Model:}
Our threat model focuses on attackers who can publish packages to software package registries and use the published package to deliver malicious code to unsuspecting users and applications.
We include some threats and exclude others.
\begin{itemize}
    \item \textit{In-scope:} We consider attacks where packages with deceptively similar names to legitimate packages are published in the software package registry. However, the published package may or may not contain any malicious payload.
    \item \textit{Out-of-scope:} We do not consider attackers that either directly publish malicious packages with new names or compromise existing legitimate packages.
    These attacks are mitigated by the broader software supply chain security measures~\cite{okafor_sok_2022}.
\end{itemize}
This threat model is substantially stronger than those of existing typosquat detection techniques that only considers typosquat packages with malicious code.
This threat model also protects against currently benign typosquat packages that can introduce malicious code in the future.
\cref{fig:TPAnalysis} demonstrates the viability of this threat class as malicious code can be introduced months after the original package is published, and after it may have gotten tens or hundreds of downloads.

\iffalse

\subsection{System and Threat Model}
\label{sec:ThreatModel}
% \subsection{Why Existing Solutions Fall Short}
% \label{sec:ProblemStatement-SOTALimitation}

% \JD{This probably needs to come earlier.}
% \JD{Give two examples, early on --- back in \$3 after you give the threat model. One example should be a typosquat that was malicious from the start (or near its creation, anyway), and the other should be a typosquat *that became malicious later on*. That will make the description of the gap in prior work here more compelling, as well as motivating the problem definition that you provide.}
% \WJ{Is the description of tactics in \cref{sec:threat-actor-capabilities} concret or we need more specific example, like providing a list of packages?
% }

% \PA{Looks like the next couple words are hanging}
Prior research highlights several key shortcomings of SOTA typosquatting detection systems, including high false-positive rates, limited adaptability, and insufficient utilization of contextual information.
and needs a generic approach for
We aim to enhance the infrastructure for SPRs and outline the specific adversarial tactics associated with typosquatting that our system seeks to mitigate as follows:

\paragraph{System Model:} \Andreas{I think "System Model" should define the system that you aim to defend, not the security solution that you implement. What are the properties of the system that you aim to protect?}
We propose a system designed to support software package registries (SPRs) in identifying and mitigating typosquatting threats through backend operations. Existing solutions predominantly focus on frontend interactions, such as command-line interface (CLI) tools for end-users~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}. While these solutions are effective in assisting users at the point of package installation, backend-focused tools --- requiring much higher accuracy alongside acceptable latency and throughput --- tailored for package registry maintainers remain significantly underexplored.

Our system addresses this gap by providing a backend typosquatting detection system that prioritizes accuracy and efficiency while maintaining low false-positive rates. This system is designed to integrate seamlessly into registry workflows, enabling proactive detection of suspicious packages before they reach end-users. By leveraging this backend capability, SPRs can bolster their defenses against typosquatting threats and better protect their ecosystems.




\paragraph{Threat Model:}

Our threat model considers adversaries targeting SPRs to distribute malicious software packages. Specifically, adversaries who attempt to publish malicious packages to the registries and coerce users to install them by employing typosquatting techniques.

Our threat model considers adversaries targeting SPRs through typosquatting attacks, as well as stealth threats in the ecosystems. The attackers register or hijack packages with deceptively similar names to legitimate ones, aiming to exploit user mistakes or trust to introduce malicious code into the software supply chain.
Our threat model in this work is defined as follows:

\begin{itemize} [leftmargin=*, itemsep=0.1ex]
    \item \textit{In scope:} This includes immediate maliciousness from typosquatting attacks where attackers use package name deception to deliver harmful payloads. It also covers deferred maliciousness, where packages appear benign initially but could introduce malicious updates later, as well as limited cases of popularity manipulation to enhance the credibility of malicious packages.
    \item \textit{Out of scope:} The model does not address metadata manipulation, such as altering descriptions or version histories, nor does it cover attacks targeting high-profile maintainers or organizations through phishing or account compromise. Broader supply chain attacks, including dependency confusion and substitution, also fall outside the scope of this work.
\end{itemize}
% \PA{Is the lack of production-focused insight a design or implementation limitation? If design, you should discuss how. If implementation, it may not be interesting to emphasize.}
This threat model supports SPRs from the backend by detecting and mitigating typosquatting threats, including immediate maliciousness through package name deception, deferred maliciousness via delayed updates. Our system aims to (1) raise alarms for both immediate and deferred threats for SPR users and (2) provide SPRs with a focused list of typosquatting threats, enabling efficient monitoring and scanning of suspicious packages. While addressing typosquatting-specific risks, the model excludes broader supply chain attacks like dependency confusion, substitution, and metadata manipulation.

\iffalse

\fi

\subsection{Model of Attacker Capabilities}
% \label{sec:ThreatModel-AttackerCap}

\JD{This section does not define the attacker's capabilities. Need to rewrite. Here are some of the capabilities you should include: (1) Can the attacker publish their own modules? (2) Can the attacker modify existing modules they did not originally publish? (3) Can the attacker manipulate popularity metrics, and if so, to what extent? (this one is very important since it is a sometimes-implicit assumption in prior work that I think is actually a bit questionable...let's write it down and then in the Discussion we can reflect on how plausible the assumption is, it would be nice to be able to cite work on download farms and citation farming etc.) (4) Can the attacker access the source code etc of popular modules?}
Attackers can exploit ecosystem vulnerabilities even with security measures like Two-Factor Authentication (2FA) and Automated Dependency Scanning in place. Common strategies include hiding malicious content within packages or employing "stealth attacks," \JD{are you defining this term? there should be a citation I think} where a benign package is published initially and malicious code is introduced later. These methods are often used for cryptojacking, gaining unauthorized network or shell access, and other forms of exploitation~\cite{Ohm2020ReviewofOpenSourceSSCAttacks, taylor2020spellboundDefendingAgainstPackageTyposquatting}.

One prevalent tactic is typosquatting, where attackers create package or author names resembling popular ones by introducing minor typographical errors. This includes both author name typosquatting (\eg using \texttt{expresss} instead of \texttt{express}) and package name typosquatting (\eg \texttt{lodashs} instead of \texttt{lodash}). These techniques rely on human error and automated scripts to deceive users into installing malicious packages~\cite{neupane2023beyondtyposquatting, moubayed2018dnstyposquat}. Another related tactic is dependency confusion, where attackers publish packages mimicking internal dependencies, exploiting build systems that inadvertently fetch public versions instead of intended private ones~\cite{koide2023phishreplicant}.



\iffalse
%%%%%

\subsection{Existing Threats}

\TODO{Use the theory from Typomind paper~\cite{neupane2023beyondTyposquatting}}

\subsection{Additional Threats}
\TODO{Define additional threats specifically for author name typosquats which happens more on Maven, Golang, and HF because their package name include author names.}
\fi
\fi


\fi

% \section{Problem Statement and Requirements}
% \section{Threat Model and System Requirements}
% \label{sec:ThreatModelandSysRequirements}
% \JD{These opening remarks feel repetitive. Open more directly with ``Our goal is to...''}
% Typosquatting attacks capitalize on naming similarities to deceive developers into installing malicious or soon-to-be malicious packages. Prior work has examined string-based detection~\cite{taylor2020spellbound, neupane2023beyondTyposquatting}, but it typically measures success by detecting packages that \emph{already contain} harmful code, overlooking the central issue of \textbf{``stealth'' typosquats}.

% \WJ{Should we talk about the accuracy and FP rate in separate requirements? Currently they are both in req1 because both are under accuracy requirement.}
% \JD{I think they should stay in the shared requirement, it will seem silly to split them. Everyone knows this requirement.}



\iffalse
We seek four key guarantees that expand on, and differ from, prior typosquatting defenses:

\PA{Do you want to call these System Requirements? The item does not sound like security guarantees.}
\begin{enumerate}
  \item \textbf{G$_{1}$: High Recall of Malicious-Intent Names.}
    The system must capture both actively malicious packages \emph{and} those evidently designed to confuse or mislead, thereby minimizing false negatives.

  \item \textbf{G$_{2}$: Low False-Positive Rate.}
    Although perfect precision is unattainable, we target minimal disruption to developer workflows. In practice, we strive for a false-positive rate under a few percent to avoid overwhelming teams with unnecessary alerts (\TODO{xx}). Many packages have superficially similar names or are legitimate forks~\cite{wyss2022whatTheForkFindingHiddenCodeClonesinNPM}, so we only flag those that signal genuine malicious intent.

  \item \textbf{G$_{3}$: Efficient and Timely Detection.}
    Our approach must handle large registries (\eg  millions of packages) in a timely manner, such as completing nightly full scans or near-real-time checks for newly published packages. Throughput should be sufficient for scanning $\sim$1M entries overnight on a standard server, and latency should be low enough to provide quick feedback in CI/CD pipelines.

  \item \textbf{G$_{4}$: Compatibility Across Ecosystems.}
    The solution should accommodate single-level (\eg NPM, PyPI) and hierarchical (\eg Maven, Hugging Face) naming schemes without assuming uniform metadata or popularity signals. This ensures that partial or external data sources suffice to identify malicious naming trends.
\end{enumerate}

By centering our approach on malicious intent, we go beyond the simpler “contains malicious code” criteria. This reduces the window of exploitation, discourages “stealth attacks”, and addresses the real motive behind suspicious naming.

\subsection{Production Requirements}
\label{sec: ProblemState-Prod}

While the security guarantees outline the system’s detection goals, real-world deployment imposes additional constraints. We enumerate three main requirements:

\paragraph{Req$_{1}$: Frequent Metadata Updates.}
Practitioners need reliable, up-to-date information about newly published packages—even if popularity metrics are incomplete. The system must regularly ingest data from various sources (\eg package registries, version histories, or domain name checks) so that emergent threats are caught early. Critically, the intent is to keep developers informed about suspiciously named packages released \emph{since the last scan}, rather than to incorporate popularity as a direct detection mechanism.

\paragraph{Req$_{2}$: Manageable False-Positive Load.}
High false-positive rates lead to ``alert fatigue'', which often results in developers ignoring warnings~\cite{xx}.
Hence, the system must offer robust verification methods—such as code similarity checks, publisher identity assessment, or metadata validation—to distinguish purposeful name confusion from legitimate forks. The acceptable threshold (\eg 95\% precision) may vary by organization’s tolerance for risk.

\paragraph{Req$_{3}$: Cost-Effective Performance.}
Large ecosystems necessitate scanning millions of packages within a limited time (\eg overnight or in short CI windows). While one could theoretically purchase more compute resources to reduce latency, such an approach is not always financially viable. Thus, the system must balance:
\begin{itemize}
  \item \textbf{Throughput}—how many packages can be processed per hour to handle full ecosystem scans.
  \item \textbf{Latency}—how quickly each new package is classified to block malicious installs in real-time.
\end{itemize}
For instance, a typical target is to process \TODO{1 million packages in under 6 hours on a moderate CPU server}.
% , while also providing sub-second responses to queries in CI environments.
\fi



\iffalse

\JD{The reader wants to know whether, and how, this formulation of problem and requirements is novel/different from prior work. Please be clear about this.}

To protect package users from the threat model we discussed above, we describe the detailed problem statement in this section, including the desired security guarantees (\cref{sec: ProblemState-Security}) and the systems requirements for production (\cref{sec: ProblemState-Prod})

\JD{This part does not clarify what you are changing or why.}
Prior work defined pacakge confusion attack as~\cite{neupane2023beyondTyposquatting}.
In this work, we scope-down the definition:
\begin{tcolorbox}
    A \textbf{typosquatting attack} involves the creation of packages with names that are deliberately similar to those of popular packages, typically with \ul{\textit{malicious intent}}. These packages may either contain active malicious code or be designed to introduce harmful content at a later stage.
    %
    \textit{Incorporating malicious intent into this definition is different from exiting typosquatting defense techniques.}
\end{tcolorbox}
\subsection{Security Guarantees}
\label{sec: ProblemState-Security}

\JD{This subsection currently does not describe security guarantees. What guarantee(s) is your approach providing? (Your guarantee can be probabilistic -- false positive/negative are inevitable, but you should indicate the criterion that Socket has for deployability.}
To protect users from typosquatting attacks, an automated detection mechanism is essential.
A straightforward approach is to identify packages with similar names that have a significant difference in popularity. \JD{Yep, and you should cite the prior work that did so, as well as any challenge like missing popularity info}
\JD{seems to ignore the `malicious intent' part --- that part becomes quite tricky so you should not call it `straightforward'.}

\subsection{Production Requirements}
\label{sec: ProblemState-Prod}
For practical deployment in a production environment, the typosquatting detection system must meet four requirements:
% \JD{``four requirements'' is a lot nicer than ``several key''. If something isn't important, don't mention it in a big way. We assume you are only telling us important things, so don't inflate with adjectives like `key'.}

\paragraph{Req$_{1}$: Regularly Updated Package Metadata:}
An effective detection mechanism must be integrated with a continuously \JD{I don't think `continuous' is possible because computers are discrete? `regular' might be a better term.} updated database that includes the latest package metadata and popularity metrics. This ensures that newly published packages and changes in package popularity are promptly reflected in the detection process.
\JD{I think that including popularity in this paragraph is a mistake --- this is combining the requirements with the solution approach, which is not correct. The requirement exists **only** because users need warnings about the most recent published packages --- that's a ``user'' reason for the requirmeent, as opposed to a ``system'' reason.}

\paragraph{Req$_{2}$: Standardized Support for Popularity Metrics Across Ecosystems:}
Ecosystems like Maven and Golang do not consistently provide direct popularity metrics, such as download counts. To address this challenge, the system must incorporate standardized methods for estimating package popularity across all ecosystems. This may involve aggregating alternative metrics such as repository stars, forks, dependent packages, or leveraging external datasets like \texttt{ecosyste.ms} that provide normalized popularity indicators.
\JD{I think this requirement overlaps with Req$_{1}$, but it's more of a problem than Req$_{1}$. This requirement is solely due to how you designed your specific system, so it's not appropriate to give it as a general requirement for a typosquat technique. You should remove this.}

\paragraph{Req$_{3}$: Minimize False Positives While Maintaining High True Positive Rates:}
In a production environment, particularly within continuous integration (CI) pipelines, a high false-positive rate can be disruptive and lead to alert fatigue \JD{cite}. Many packages may have similar names intentionally or be legitimate forks for customized use cases~\cite{wyss2022whatTheForkFindingHiddenCodeClonesinNPM}. Therefore, the detection system must balance accurately identifying malicious typosquatting packages (true positives) while minimizing the incorrect flagging of legitimate packages (false positives). This requires sophisticated analysis that goes beyond name similarity, potentially incorporating code analysis, publisher reputation, and historical behavior.
\JD{The previous sentence is inappropriate because it is about design, not user requirement. Instead, we want to know ``What false positive rate is acceptable?'' Cite prior work on this point and include Socket's own standard.}

\paragraph{Req$_{4}$: Low Latency and Cost-Effectiveness:}
For integration into CI/CD pipelines and real-time applications, the detection mechanism must operate with low latency to avoid hindering development workflows. Additionally, it should be cost-effective in terms of computational resources and scalability, enabling widespread adoption without imposing significant overhead.
\JD{Not sure about this statement about `widespread adoption'. Presumably it just needs to be cost effective by a single company so they can sell it as a service. This paragraph overall felt a little confused, you are talking about low latency as well as cost effectiveness and I think these are related but not identical. I can buy a bigger computer to get lower latency but this is not cost effective. Maybe the REQ should be `cost effective' and you can talk within it about obtaining low latency without excessive compute resource cost. And again, a specific target from Socket will be helpful: ``throughput or latency of X packages/time unit on a machine with resources Y''. Also note that it's not clear whether what you want is latency or throughput --- which measure is more appropriate (or both?) and why? I presume that latency can be resolved by storing the results in a DB, so then it seems that throughptu is the appropriate measure: ``We want to run an update every night and our budget is machines like Y and so we need to be able to process X packages/second in order to scan all of these registries.''}

By meeting these requirements, the proposed system can effectively detect typosquatting attacks in a production setting, providing robust security guarantees while maintaining usability and efficiency.
\fi

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth, 
    trim=8 12 14 10, 
    clip]{figures/system_overview_v6.pdf}
    \caption{
    Overview of the \tool design with five primary components.
    % each labeled with a blue circled number. 
    The main novelty of \tool is 
      the techniques of Part 4 and 5.
    We also improve on prior work in Part 2.
      %Step 2 (improving on prior semantic analysis~\cite{neupane2023beyondTyposquatting})
      and
    Part 1 (DB) can be constructed using public registry APIs.
    Part 3's approach for popular packages is from prior work, adapted for the broader set of SPRs we handle~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting}.
    %In Step 2, we employed an embedding-similarity-based approach, which differs from conventional methods. Additionally, we observed that incorporating Steps 4 and 5 significantly enhances system accuracy.
    The red texts indicate the evaluation questions (EQs).
    % \WJ{TODO: We need to update the EQs.}
    % \JD{Proposal: (1) add the titles of the steps into the diagram; and (2) In the caption, add a sentence like: Steps 1-2 are also pursued in prior work (CITE). Our approach to Steps 1-2 differ in XYZ, and we find that including Steps 3-5 offer a substantial improvement in F1-score (CREF).}
    % \TODO{Update the figure. Include command sqatting}
    % \TODO{Update the EQ numbers.}
    }
    \label{fig:pipeline}
\end{figure*}
% \vspace{-10mm}

%\newpage 
\section{\tool Design and Implementation}
\label{sec:SystemDesign}

%Existing methods often rely on simplistic criteria, such as the absence of current malware activity or name similarity to legitimate packages, to classify packages as benign or potential typosquats (\cref{sec:background-Defenses}). 
%Existing methods are prone to ambiguity and result in high false-positive rates, failing to consider the intents behind confusing package names (\cref{sec:ProblemStat}).
We introduce \tool, our novel detector for package confusion attacks. %designed to reduce false positives while prioritizing the detection of \textit{malicious intent}. 
\tool is designed to meet the system requirements (\cref{sec:ProblemState-SysReq}). %, prioritizing accuracy (Req$_1$) over latency (Req$_2$).
Addressing the insights from our empirical study (\cref{sec:EmpiricalAnalysis}), \tool integrates both syntactic and semantic name analysis, hierarchical naming checks, and metadata-based verification to enhance threat detection and mitigation.
\tool is intended for \textit{backend} use, in package registries or similar platforms, and so it prioritizes accuracy (Req$_1$) over latency (Req$_2$). % to ensure robust and reliable detection.

\cref{fig:pipeline} shows the five main components of \tool:

\setlist[enumerate]{leftmargin=1.35em, itemsep=0.1ex}
\begin{enumerate}
    \item A metadata database, ensuring real-time awareness of new and evolving packages (\textbf{Req$_{4}$}).
    \item A fine-tuned embedding database to capture domain-specific semantic name similarities 
    % essential for detecting malicious intent 
    (\textbf{Req$_{1,3}$}).
    \item Using popularity metrics to protect high-value targets (\textbf{Req$_{2,3}$}).
    \item Checking for syntactic and semantic confusion strategies (\textbf{Req$_{2}$}).
    % Running \emph{ML-based package confusion searches}, using approximate nearest neighbors (ANN) and quantization to scale efficiently (\textbf{Req$_{2}$}).
    % \JD{There is no longer a Req$_5$. Is the next item up to date?}
    \item Filtering out benign packages using package metadata
    % , mitigating false positives and maintaining developer trust 
    (\textbf{Req$_{1,4}$}).
\end{enumerate}



\iffalse
Our system proactively addresses existing shortcomings by \ul{analyzing suspicious package names, integrating package content and metadata into the detection system, and leveraging LLMs to detect the existence of malicious intent}, thereby flagging harmful naming patterns before they can cause damage.
\fi

% The following sections outline how we operationalize these techniques to meet stringent security guarantees (\cref{sec: ProblemState-Security}), satisfy production requirements (\cref{sec: ProblemState-Prod}), and overcome the limitations of existing approaches (\cref{sec:ProblemStatement-SOTALimitation}).


% \PA{Can I achieve malicious intent detection using prior works by simply removing the check for harmful content?}
% \WJ{The problem is prior work does not have the harmful check part. They ran a malware scanner on the suspicious packages they identified which gave less than 1\% actual malware.}

\noindent
%Each step builds upon the artifacts of the previous ones.
% --- metadata drives embedding creation; embeddings and popularity signals fuel confusion searches; and suspicious results are then verified with richer heuristics.
The first three components provide infrastructure to support our analysis (components 4 and 5).
Next, we detail the rationale and implementation of each part of \tool. % (\cref{sec:SystemDesign-Step1-Metadata} -- \cref{sec:SystemDesign-Step6}).
% as well as the system limitation (\cref{sec:SystemDesign-Limitation}).

\subsection{Part \textcolor{blue}{\textcircled{\small{1}}}: Package Metadata Database}
\label{sec:SystemDesign-Step1-Metadata}

\subsubsection{Rationale}
% Keeping pace with new or updated packages across multiple ecosystems requires regularly refreshed metadata. Without such updates, attackers could publish deceptive names and remain undetected for extended periods. Hence,
SPRs grow rapidly, and package confusion attacks are initiated regularly.
Comprehensive and regularly updated metadata ingestion enable early threat detection.

\subsubsection{Approach}
\tool relies on a database with package names, version histories, commit logs, license info, maintainer records, and other publicly-accessible metadata, all updated on a weekly basis.
Regular updates reduce concerns about stale data and ensure our results are up to date. %pipeline quickly analyzes newly introduced packages. 
We specifically use our industry partner's private database, which consolidates metadata from NPM, PyPI, RubyGems, Maven, Golang, and Hugging Face. 

\iffalse
In this work, we utilize the package metadata dataset provided by Socket\footnote{\url{https://socket.dev/}}.
Socket is a comprehensive platform that aggregates and maintains extensive metadata for a wide variety of software packages across multiple ecosystems, including JavaScript, Python, Ruby, Java, and Go. This dataset offers regularly updated and comprehensive package information, enabling the detection of both existing ecosystem attacks and emerging zero-day vulnerabilities. By leveraging this dataset, we effectively address \ul{\textit{Req$_{1}$}}, which requires access to up-to-date metadata.
\JD{Is this a paid/subscription database? And the popularity stuff should be given in this part as an extension you're making to their database, which makes this step more interesting.}
\fi

\subsection{Part \textcolor{blue}{\textcircled{\small{2}}}: Defining Trusted Resources}
\label{sec:SystemDesign-Step3}

\subsubsection{Rationale}
Confusion attacks mimic trusted resources.
If an attacker chooses a name that mimics an untrusted package, little harm can result.
Untrusted packages can be compared against trusted ones to focus on the threats of highest potential impact.


%, packages with high download counts or strong community engagement allows us to concentrate detection efforts where they are most needed, minimizing overhead and reducing alert fatigue from infrequently used packages.
% However, we do not treat popularity as the sole criterion; packages exhibiting suspicious naming patterns are still flagged under our intent-centric approach, even in the absence of popularity data. This balanced reliance on popularity enables us to prioritize scanning likely targets while maintaining coverage for less popular projects that may pose malicious risks.

\subsubsection{Approach}
% \emph{Direct Metrics.}
% Building on prior work, we prioritize packages based on popularity, treating widely-used ones as more likely to be legitimate, while still scrutinizing all packages for signs of stealth typosquatting attacks to balance resource efficiency and detection accuracy.
Like prior works~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting,neupane2023beyondTyposquatting}, we operationalize trust in terms of popularity.
Software engineers commonly make use of popularity signals as a proxy for trustworthiness~\cite{WhatsinaGithubStar}, and thus attackers choose names similar to popular packages.
Our specific popularity measure depends on the SPR.
Many SPRs --- in our case, NPM, PyPI, RubyGems, and Hugging Face -- offer weekly or monthly download counts.
Our other two supported SPRs, Maven and Golang, do not publish download metrics.
For these, we make use of the \texttt{ecosyste.ms} database, which estimates popularity using indicators like stargazers, forks, and dependent repositories~\cite{ecosystemsWeb}. 
The thresholds were set based on production experience. We chose 
    5,000 weekly downloads for NPM, PyPI, and RubyGems; 
    1,000 for Hugging Face due to its costly usage; 
    and an \texttt{avg\_ranking} score of 10 for Maven and Golang. 
We then adjusted the Golang thresholds to 4 to balance production latency.

% \JD{How are thresholds chosen?}

Adversaries may inflate the popularity statistics of their packages~\cite{he2024FakeStars}, which could cause a malicious (or decoy) package to become trusted.
If our partner's analyst team flags any package as suspicious (\cref{sec:SystemDesign-Step6}), it is removed from the list of trusted packages.

As a secondary definition of trust, we analyzed the command-line interfaces of tooling for each SPR, \eg the ``npm'' and ``mvn'' commands.
These interfaces contain built-in keywords and commands which a careless engineer might confuse or mis-type for package names, \eg mistaking \texttt{npm i help} (install a package named help) for \texttt{npm i $--$help} (help command).
All such keywords are included as trusted resources.

%For packages in the unpopular list, we compare them against all packages in the popular list.


%While the resulting composite \texttt{average\_ranking} metric is not without limitations, it effectively identifies libraries that are frequently referenced or depended upon.
%prompting us to integrate external data sources such as \texttt{ecosyste.ms}, which 

\iffalse
\tool makes use of a list of popular packages.
Confusion attacks can be compared against this list to reduce the number of alerts.
%Our definitions of popularity for the supported SPRs are given next.

%Inspired by prior work~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting},
We define \textit{trusted} packages in terms of popularity.
%package popularity to determine the \textit{trusted} packages that are usually prime attack targets.

We mark the top packages as \textit{trusted} packages. % 

To address the size of each ecosystem and satisfy \textbf{Req$_{3}$}, we made the following design decision: For a given input package, we categorize packages into two groups: a popular list and an unpopular list.

\fi


% \paragraph{Partial Usage.}
% Crucially, any package—\emph{popular or not}—can still trigger alerts if our embedding and metadata checks (\S\ref{sec:SystemDesign-Step2}, \S\ref{sec:SystemDesign-Step5}) detect strong evidence of malicious intent. In other words, popularity helps us \emph{prioritize} suspicious candidates but does not exclude low-profile packages from scrutiny. This dual approach addresses real-world constraints by dedicating more resources to heavily used packages (and thus reducing false positives on obscure ones), while maintaining the flexibility to catch crafty attackers who start with low-visibility repos. As such, Step 3 serves both \textbf{Req$_{2}$} (manageable false positives through targeted scanning) and partially aids \textbf{Req$_{3}$} (cost-effectiveness) by allocating computational effort where it is most impactful.


\subsection{Part \textcolor{blue}{\textcircled{\small{3}}}: Package Name Embeddings}
\label{sec:SystemDesign-Step2}
% \JD{This section makes no remarks about why you did it this way, how neupane did it, relevance to Reqs, etc.}

\subsubsection{Rationale}
Detecting maliciously similar names requires accurately capturing subtle lexical, syntactic, and semantic variations. Traditional Levenshtein edit distance methods often fail to account for domain-specific semantic nuances (\eg \texttt{meta-llama} vs. \texttt{facebook-llama}), while generic embedding models can introduce inaccuracies,
% for short names, 
resulting in higher false-positive or false-negative rates. 
Prior work apply embedding for word-wise semantic similarity check~\cite{neupane2023beyondTyposquatting}, while we would also like to use embedding for the whole name to support efficient neighbor search (\cref{sec:SystemDesign-Step4}).
Robust, fine-tuned embeddings address these shortcomings by providing enhanced semantic sensitivity, reducing erroneous alerts.
Moreover, this embedding approach is also generic and unified to support various naming convention of SPRs.

\subsubsection{Approach}
% \JD{Mention the handling of hierarchical packages here as well.}
% \WJ{It's in the next paragraph}
An \emph{embedding fine-tuned on real package names} enhances semantic sensitivity, enabling the detection of adversarial or suspicious names. This capability is integral to assessing the risk associated with a package and serves as a cornerstone of our \textit{intent-centric} approach.
Following \cite{neupane2023beyondTyposquatting}, we build upon FastText~\cite{bojanowski2017FasttextEmbedding}, starting with the pre-trained model pre-trained on \texttt{cc.en.300.bin}, and \emph{fine-tune} it using all (totally $ \sim $9.1 million) package names extracted from the metadata database in Nov. 2024 (\S\ref{sec:SystemDesign-Step1-Metadata}).
By fine-tuning, our version is expected to better capture domain-specific subwords.
% \JD{can? is expected to? (do we show this?)
% \begin{itemize}[leftmargin=*, itemsep=0.1ex]
%     \item \textbf{Domain-Specific Subwords}: Frequent domain specific terms in package names are better captured.
%     \item \textbf{Hierarchical Structures}: Splitting names in Maven (\texttt{groupId:artifactId}) or Hugging Face (\texttt{author/model}) to create multiple embeddings per pack=age.
% \end{itemize}

% \JD{I assume we ablate this so say that and cref to eval}
% \WJ{To clarify, are we talking about the ablation of hierarchical embeddings? I thought we need ablation for Step 5 but not this step. It is not necessary to have separate ablation for hierarchical embeddings in my opinion because using the whole embedding will introduce high false positive rates (based on our experience but this is also theoretically true). I added a sentence at the end of this paragraph.}
We then use the fine-tuned embedding model to create an embedding vector database utilizing the vector format provided by \texttt{pgvector} due to its efficient vector operations for databases~\cite{pgvector}.
Given a package name, we remove its delimiters, pass the concatenated string to the embedding model, and store the extracted embedding in our database.
We note that for hierarchical names from Golang (\texttt{domain/author/repository}), Maven (\texttt{groupId:artifactId}), and Hugging Face (\texttt{author/model}), we split names to create both author and package identifier embeddings per package.
This approach mitigates high false-positive rates that occur when a single embedding causes similar packages from the same organization or author to be incorrectly classified as confusing in \cref{sec:SystemDesign-Step4}.

The complete embedding database occupies 24~GB, with each embedding vector corresponding to a single package name (or its \texttt{author name} and \texttt{package identifier}). This setup not only facilitates rapid query-based lookups but also supports subsequent steps in package neighbor searching.
Visualization of the extracted embedding vectors is available in our artifact (\cref{sec:OpenScience}).

% \JD{I want this section to either describe the flow of a package step by step, or to say at the end ``integration''``we scan the whole DB  }
% \WJ{Is the previous paragraph not enough?}\WJ{Added a sentence in  the first paragraph in 5.3.2. Not sure what other details we can add here.}

% \subsubsection{Illustrative Example}


% \cref{fig:embeddingSpace} presents a t-SNE visualization of the embedding space for 10,000 NPM package names. This visualization illustrates how the embeddings capture both semantic relationships and syntactic similarities among package names. In the plot, package names that are semantically related or have minor syntactic differences are clustered closely together. For instance, packages that are common targets for typosquatting—those with names differing by a single character or with transposed letters—are located in proximity within the embedding space.

% The clustering behavior observed in \cref{fig:embeddingSpace} indicates that the fine-tuned embeddings effectively encode meaningful patterns in package names. Packages with similar functionalities or those that are potential typosquatting variants of popular packages naturally group together, demonstrating that the embeddings are sensitive to both the lexical composition of the package names and their contextual relevance within the ecosystem. By capturing these nuanced relationships, the embeddings provide a robust foundation for typosquatting detection, enabling similarity searches that effectively retrieve genuinely related or suspiciously similar packages. This capability is crucial for identifying malicious packages that exploit naming similarities to deceive users.

% All extracted embeddings are stored in a PostgreSQL database to facilitate efficient querying and analysis in subsequent steps. The total size of the embeddings amounts to approximately 24GB, reflecting the comprehensive scope of our dataset and the high dimensionality of the embeddings.


\subsection{Part \textcolor{blue}{\textcircled{\small{4}}}: 
% ML-Based 
Package Confusion Search}
\label{sec:SystemDesign-Step4}

Having obtained the appropriate infrastructure (Parts 1--3), 
we must now detect package confusion threats efficiently yet accurately.
In Part 4, we efficiently flag possible threats, and then in Part 5 we filter out likely-benign ones.

\subsubsection{Rationale}
Modern SPRs are enormous, so any detection scheme must be designed with scalability in mind. %for name-based threats is computationally intensive.
Given the set of all packages in an ecosystem, we follow prior work~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting, neupane2023beyondTyposquatting} by comparing the (relatively large) ``long tail'' of untrusted resources to the (relatively small) set of trusted resources.

\iffalse
A scalable strategy is essential to identify suspicious package similarities across millions of entries in near-real-time.
 (\textbf{Req${2}$}). 
Moreover, hierarchical naming conventions (\eg \texttt{groupId:artifactId} in Maven) and ecosystem dynamics add complexity, as attackers may target authorship (\ie \emph{author squatting}) or registry commands  (\ie \emph{command squatting}) rather than solely manipulating package names. Thus, the solution must achieve high recall while maintaining efficiency and minimizing false positives, ensuring developers are not overwhelmed with excessive alerts.
\fi

\subsubsection{Approach}

\paragraph{Trusted Resources Selection}
We compare untrusted resources (\ie unpopular packages) against all names in the popular list.
For packages in the trusted resources list (\ie popular packages), we compare it only with ``more'' trusted packages --- for popularity, we use download rates $\geq$10x higher or ecosyste.ms scores $\geq$2x higher.

% While creating static command lists for each registry would offer a temporary solution, maintaining them would be complex in these decentralized ecosystems. 

% a more scalable and efficient approach involves dynamically constructing and updating these lists through automated policies and tools. Implementing such dynamic systems could greatly mitigate the risk of command squatting while reducing the need for manual maintenance.

\paragraph{Neighbor Search (Package Name Similarity Search)}
We perform a distance search between each resource and the set of trusted resources.
Syntactic similarity is measured using Levenshtein distance with a threshold of 2, while semantic similarity is determined using embedding distance. For semantic similarity, we apply a cosine similarity threshold of 0.93, as suggested in \cref{sec:EQ1}. For hierarchical names, we adjust the similarity thresholds to 0.99 for package identifiers and 0.9 for author names to better detect \texttt{compound squatting} attacks.
% ... \JD{Fill in --- any customization/optimization/etc.? thresholds?}
To efficiently measure semantic similarity, we leverage the \textit{Approximate Nearest Neighbors} method with the \textit{HNSW} index~\cite{malkov2018HNSW} in PostgreSQL. We opted for \textit{HNSW} over \textit{IVFFlat} based on benchmarking results that show faster search speeds and minimal vector perturbation~\cite{tembo_pgvector_2024}. By partitioning the embedding space into multiple clusters, the \textit{HNSW} index limits distance computations to a smaller subset of candidate packages, rather than exhaustively comparing all pairs (\textbf{Req$_{3}$}).

\paragraph{Similarity Sort and Most Similar Neighbor(s)}
% \JD{Add note here about the sorting approach.}
Inspired by Neupane \etal~\cite{neupane2023beyondTyposquatting}, we define a similarity function that ranks neighbors based on Levenshtein distance, n-gram similarity, phonetics, substring matching, and fuzzy ratio. This sorting ensures the system accurately identifies the most likely attack target.
If the previous steps produce any nearby neighbors, we consider these as possible attack targets.
We treat the nearest neighbors as the most likely targets.
The suspicious package and these nearest neighbors are propagated to the benignity check (\cref{sec:SystemDesign-Step5}).
Currently we consider only the two nearest neighbors, balancing accuracy against speed.


\iffalse
\begin{enumerate}[leftmargin=*]
\item \textbf{Index Construction.}
We first run k-means clustering on the embedding vectors (extracted in \S\ref{sec:SystemDesign-Step2}) to produce a predetermined number of centroids. Each package name’s vector is then assigned to its nearest centroid.

\item \textbf{Query Process.}
When a new or updated package is evaluated, we determine the centroid nearest to its embedding. We then compute similarity only against vectors in that cluster, dramatically cutting the computational cost relative to brute-force searches. This design supports real-time lookups and batch scans alike.

\item \textbf{Author-Level Similarity.}
In Maven, Golang, and Hugging Face, package identities often include author or group-level namespaces. We separately embed these author IDs to identify potential \emph{impersonation attacks}, in which malicious packages appear to originate from well-known maintainers. By comparing embedding distances among author names, we can flag suspiciously similar accounts, a tactic that single-level matching might miss entirely.
\end{enumerate}
\fi


\iffalse
\paragraph{Quantization for Performance.}
While ANN indexing accelerates queries, the high dimensionality of fine-tuned FastText vectors can still impose significant computational and storage demands. To address this, we applied the quantization technique to compress model weights by reducing precision, and maintaining high-quality embeddings while reducing model size. We then regenerate and store all package name vectors using the quantized model in PostgreSQL, enhancing both batch and real-time search performance while contributing to \textbf{Req$_{3}$} by lowering compute costs.
\fi
% \paragraph{Tailored Algorithm per ecosystem}
% \WJ{There is something we need to say here, but maybe we should move it to appendix}

% \TODO{HuggingFace challenges:semantic difference vs. typos}
\iffalse
\begin{enumerate}[leftmargin=*]
\item \textbf{Quantizing FastText Models.}
We apply the built-in \texttt{quantization} function in FastText~\cite{joulin2016fasttext}, compressing weight parameters by lowering precision. This preserves the model’s ability to generate high-quality embeddings while shrinking its size.

\item \textbf{Re-Embedding and Storage.}
All package name vectors are regenerated using the quantized model, then stored in PostgreSQL. This further speeds up both batch and real-time searches, contributing to \textbf{Req$_{3}$} (cost-effectiveness) by minimizing compute demands.

\item \textbf{Accuracy-Performance Trade-off.}

In \cref{sec:eval-quantization}, we evaluate how much retrieval precision we sacrifice for these gains. \TODO{Our findings show only a small drop in F1 score, outweighed by faster inference times and reduced memory footprints.} This ensures feasible deployment at scale without requiring specialized hardware or advanced GPU clusters.
\end{enumerate}
\fi

% \subsubsection{Why This Matters.}
% \WJ{TODO: This paragraph can be cut}
% By combining approximate nearest neighbors with quantized embeddings, we achieve strong recall for name-similarity attacks while controlling latency and resource usage --- thereby maintaining a balance between high detection coverage and real-world deployability (\textbf{Req$_{2}$} and \textbf{Req$_{3}$}). In particular:
% \begin{itemize}[leftmargin=*]
%     \item \textbf{Author-level checks} catch hierarchical attacks (\eg \texttt{facebook-llama} impersonating \texttt{meta-llama}).
%     \item \textbf{Quantized embeddings} keep the per-query cost low, even as the registry expands to millions of packages.
%     \item \textbf{IVFFlat indexing} ensures real-time responsiveness for suspicious packages, fostering immediate alerts in CI/CD environments.
% \end{itemize}

% In short, \emph{Step 4} forms the core matching engine of our pipeline, detecting known typosquatting tactics and adapting to structural nuances across ecosystems without saturating computational budgets or developer attention.


\iffalse
\subsection{4: ML-based Package Confusion Search}

\subsubsection{Package Name Similarity Search}

\paragraph{Approximate Nearest Neighbors (ANN) Search}

To efficiently identify packages with similar names, we employ an Approximate Nearest Neighbors (ANN) search using the \texttt{Inverted File Flat (IVFFlat)} index within our PostgreSQL database. The IVFFlat index is a quantization-based indexing method that partitions the dataset into multiple buckets, enabling faster similarity searches by limiting the search space to a subset of relevant clusters~\cite{zhu2023ivfflat}. This approach significantly accelerates vector similarity computations compared to brute-force methods, making it feasible to perform real-time searches over large-scale embedding datasets.

In our implementation, we first cluster the package name embeddings into a predefined number of centroids using k-means clustering. During a query, the search process identifies the nearest centroid to the query embedding and only computes exact distances within that cluster. This reduces the computational overhead while maintaining high recall for nearest neighbor searches.

\paragraph{Detecting Confusing Author Accounts}

In ecosystems like Maven, Golang, and Hugging Face, package names often include author or group identifiers as part of a hierarchical naming structure. Attackers can exploit this by creating typosquatting packages that mimic not only popular package names but also author names. To detect such attacks, we separately analyze the embeddings of author names and package identifiers.

First, we extract embeddings for both the author names (or \texttt{groupId}) and the package identifiers (or \texttt{artifactId}) independently. By doing so, we can perform similarity searches on author names to identify accounts that may be impersonating legitimate authors. This involves computing the similarity between the embeddings of known legitimate authors and those of other authors in the ecosystem.

We compute the similarity between the embedding vector of a legitimate author (\eg 'legit\_author') and all other author embeddings in the database. We then order the results by the highest similarity scores, effectively identifying author accounts that are most similar to the legitimate one. Packages associated with these accounts can then be flagged for further inspection.

By focusing on the author-level embeddings, we can detect typosquatting attacks that might not be apparent when only analyzing package names. This method is crucial for ecosystems where the author or group identifier is a significant component of the package's identity.

\subsubsection{Quantization to Improve Inference Speed}
\JD{Including this as a subsubsection is a bit unusual, usually this is part of a section or subsection called Implementation which is placed at the end of Design\&Impl or as its own section. Placed within this Step, it raises questions like `Is this the only place where you made changes for performance?'' This is not fatal but be thoughtful about where to put this info.}
Due to the high computational cost associated with similarity comparisons over high-dimensional embedding vectors in large datasets, we implement quantization techniques to reduce both storage requirements and inference time.

We apply quantization directly to the pre-trained FastText models using the built-in \texttt{quantization} function provided by FastText~\cite{joulin2016fasttext}. This process compresses the original models by reducing the precision of the weight parameters, effectively decreasing the model size without significantly compromising accuracy. The quantized models retain the ability to generate embeddings for package names but operate with improved efficiency.

By using the quantized FastText models, we regenerate the embeddings for all package names across the six ecosystems. The quantization reduces the computational load during both the embedding generation and the similarity search phases. This optimization is crucial for handling large-scale data and performing real-time similarity searches in a production environment.

The quantized embeddings are then stored in our PostgreSQL database, replacing the original, uncompressed embeddings. This setup allows us to perform similarity searches using the more compact embeddings, significantly improving query performance while maintaining acceptable levels of precision.

We evaluated the performance of the quantized models in Section~\ref{sec
}, where we compare retrieval accuracy and query latency against the original models. The results demonstrate that using quantized FastText models provides a favorable trade-off between speed and accuracy. Specifically, we observe substantial reductions in inference time and storage requirements with minimal impact on the quality of the similarity search results.

By integrating quantization into our system in this manner, we meet the production requirement of low latency and cost-effectiveness (\textit{Req$_{4}$}), ensuring that the typosquatting detection mechanism operates efficiently at scale without the need for additional complex quantization techniques or database modifications.
\fi


% \subsubsection{Structure Design of the Final System}



\subsection{Part \textcolor{blue}{\textcircled{\small{5}}}: Benignity Filter}
\label{sec:SystemDesign-Step5}

% \TODO{Analyze these metrics using data from typomind to specify the threshold for each.}
% \WJ{Reuse the metadata feature from \url{https://dl.acm.org/doi/pdf/10.1145/3691620.3695493}}

\subsubsection{Rationale}
Prior work used purely string-oriented methods, which often misclassify harmless or beneficial packages as confusion attacks.
To mitigate false positives, we developed a metadata-driven benignity check inspired by a recent metadata-aware malware detector~\cite{sun2024MetadataFeaturesforMalwareDetection}.
We seek to filter out legitimate packages based on explainable heuristics, avoiding unnecessary alerts. % , and classify them into different categories (\textbf{Req$_{5}$}). 
% This section details our preliminary analysis, the rules derived from it, and how we evaluate the final false-positive (FP) verifier.

% Although name-based detection (Steps~2--4) successfully flags suspiciously similar packages, purely string-oriented methods often over-trigger on benign forks, relocated projects, or rebranded namespaces. This can overload developers with alerts, undermining trust in the system (\textbf{Req$_{1}$}, which calls for manageable false-positive rates). To mitigate these spurious detections, we introduce a metadata-driven verification procedure that check each flagged package for clear signs of legitimacy (\eg active development, overlapping maintainers) and classify them into different categories (\textbf{Req$_{5}$}).


\subsubsection{Approach}
% \paragraph{Heuristic Rules and Refinements}
Our analysis of false-positive data (\cref{sec:ProblemState-CaseStudy2-FP}) showed many reasons for which legitimate packages may have names resembling other trusted resources.
We iteratively developed 10 rules that utilize metadata features
(\eg version history, maintainers)
to distinguish package confusion attacks from benign packages.
We apply these rules to assess the benignity of fetched neighbors from \cref{sec:SystemDesign-Step4} and reduce the false positive rate.
We added several additional rules during deployment.
% \JD{Indicate that the nearest neighbors are referenced as part of this analysis.}
During implementation, we identify rules that require description analysis (\eg R1, R2, R3) or additional knowledge (\eg R8). To handle these cases, we use an LLM to analyze each package and generate the corresponding rules. We provide the package description and relevant metadata as input, refining the prompt iteratively based on feedback from security analysts. The final prompts are available in our artifact (\cref{sec:OpenScience}).

\cref{tab:metadata_rules} summarizes the goals and implementation details for each rule.
Once our name-based detector flags a package as suspicious, we retrieve its metadata and apply benignity check rules, using a simple weighted sum to calculate a risk score.
We have explored both manually-assigned weights and learning them from regression.
The simple scoring function facilitates feedback from analysts.
%Implementation details including the prompts of our verifier can be found in \cref{sec:OpenScience}.





% \PA{I'll check to see how you evaluated your derived rules and what the results were.}


% \paragraph{Initial Set of Six Rules (R1--R6).} Our first iteration addressed issues like brand extensions (\textbf{R1}), legitimate forks (\textbf{R3}), and code re-organization (\textbf{R4}, \textbf{R5}, \textbf{R6}). Yet some false positives persisted, particularly in hierarchical ecosystems.

% \paragraph{Refinements (R7--R9).} Subsequent manual reviews uncovered common relocation patterns in Maven (\textbf{R7}), packages maintained by the same author accounts (\textbf{R8}), and major length discrepancies (\textbf{R9}). Incorporating these checks further reduced extraneous alerts.

% \paragraph{Verification Pipeline}


% \paragraph{Concurrency and Logging.}
% To process large registries efficiently, we rely on multiprocessing: each worker gathers metadata, executes the rules in order, and logs the results. If the package fails all nine checks, it remains flagged as a true-positive typosquat.

\subsection{Part \textcolor{blue}{\textcircled{\small{6}}}: Alerting in Production}
\label{sec:SystemDesign-Step6}

% \paragraph{Limited Representation}
%To mitigate package confusion attacks, our system issues notifications to relevant stakeholders whenever it detects suspicious packages across different package registries. The goal is to prompt closer scrutiny of packages that exhibit potentially deceptive naming, to determine whether they constitute genuine attacks, and to assess any malicious behavior within them. We envision this alert mechanism as an additional layer of defense, complementing other software-package metrics such as supply chain security, quality, maintenance, vulnerability management, and licensing compliance.


\paragraph{Use in Production}
\tool is integrated into our industry partner's security scanning system for software packages.
They had previously relied primarily on content-based malware scanners, but these are so computationally costly that they cannot be applied at scale.
Their former approach for scalable detection of confusion attacks was a lexical check (Levenshtein distance) which had an unacceptably high false positive rate.
\tool replaced that approach and complements the malware scanners.
A team of analysts triages alerts from \tool and the other scanners to make a threat assessment.
If they deem a package to be malicious, that information is propagated to our partner's customers through their security feeds.
 
% \Andreas{At this point, I don't think SBOMs have been introduced, so it's not clear why they're being analyzed.}
% Currently, our system has generated alerts for \TODO{xxx} in production, revealing \TODO{yy} packages with confirmed malicious content and \TODO{yy} with malicious intent. We expect to continue monitoring ecosystems and issuing timely alerts, thereby contributing to improved security within these package communities.


\paragraph{Experience-Driven Optimizations}
We trialed several versions and parameterizations of \tool over several months to enhance its performance.
We share four examples.
First, to improve accuracy across ecosystems, we tailored the neighbor search query (Part 4).
For instance, lengthy identifiers, \eg Maven's \texttt{artifact\_id} and Golang's \texttt{domain name}, diminish the effectiveness of embedding similarity.
To address this, we opted to compute separate similarity measurements for each component of the package names and apply an edit distance threshold.
If the difference exceeds two-thirds of the original name's length, it is unlikely to confuse users.
% \footnote{This is based on our production experience in real world data.}
Second, within our threat model, we determined that changes to both identifiers in Maven, Golang, and Hugging Face were unlikely to confuse users and no such attacks were observed.
% \JD{``(and no such attacks were observed)''?}
Consequently, we excluded these from our algorithm.
Third, we refined our metadata checks for benignity.
We added rules \texttt{R11}, \texttt{R12}, and \texttt{R13}
% \JD{There is no R11 in the table}
based on observed patterns.
%These new rules were subsequently integrated into the system, along with their corresponding metrics. 
Fourth, for performance, we experimented with quantized versions of the embedding model.
Ultimately we found the latency gain a poor bargain for the resulting accuracy degradation.


\iffalse

\subsection{Evaluation of the False-Positive Verifier}



\iffalse
\paragraph{Rule 1: Intentional Naming} - Packages with intentional, brand-related naming patterns are likely to be legitimate, reducing the chances of false positives.
% \TODO{For example}

\paragraph{Rule 2: Distinct Purpose} - If the flagged package and the legitimate package serve clearly different purposes or functionalities, it is likely a false positive.

\paragraph{Rule 3: Fork Identification} - Packages with identical or highly similar description files and metadata are more likely to be genuine typosquat cases.

\paragraph{Rule 4: Active Development} - Packages that are actively maintained (\eg recent updates, multiple versions, \TODO{multiple maintainers?}) are typically not typosquats, thus reducing false positives.

\paragraph{Rule 5: Comprehensive Metadata} - Packages missing at least two critical metadata elements (\eg license, official webpage) are more likely to be true positives.

\paragraph{Rule 6: Consistent Quality Metrics} - Discrepancies between quality indicators (\eg high download count but low contributor activity) suggest a higher likelihood of a typosquat.


We ran an initial version of this on 20,000 packages from each ecosystems and manually sampled a set of true positives and false positives to evaluate the effectiveness of our FP verifier. After reviewing the preliminary data, we added the following rules:

\paragraph{Rule 7: Relocation of Java Packages} During our analysis, we found a lot of false positives are from the pacakge relocations. Therefore, we use the metadata from \texttt{pom.xml} file on Maven to identify the relocation information and filter those relocated pacakge out. For example, \textit{io.fnproject.fn:runtime} was relocated from \textit{com.fnproject.fn:runtime}.

\paragraph{Rule 8: Overlapping Maintainers} We observed that many false positives occur when packages share the same authors. To address this, we implemented a maintainer check: if two packages have overlapping maintainers, they are likely to be false positives. For example, \texttt{botocore-a-la-carte-machinelearning} and \texttt{botocore-a-la-carte-chatbot} share the same maintainer, indicating a false positive.

\paragraph{Rule 9: Substantially Different Package Name Lengths}


\subsubsection{Implementation}

To implement our rules for false-positive removal, we developed a pipeline that automates metadata verification and comparison across three registries: NPM, PyPI, and Rubygems. The process begins by loading potential typosquats detected by our tool, retrieving metadata for both the flagged and legitimate packages, and applying the six rules to determine if the flagged package should be classified as a false positive.
We describe out detailed implementation below:

\paragraph{Rule 1: Intentional Naming} Packages that exhibit deliberate, brand-related naming patterns are less likely to be typosquats. Our implementation compares the flagged package name with the legitimate one to detect creative variations or intentional play on words. This rule helps filter out cases where a package's naming is intended for branding or marketing purposes.

\paragraph{Rule 2: Distinct Purpose} If the flagged and legitimate packages serve clearly different purposes or functionalities, the flagged package is likely not a typosquat. The implementation extracts and compares package descriptions, analyzing the semantic content. If the descriptions show low similarity (less than 50%), we consider it a distinct purpose, reducing the likelihood of a false positive.

\paragraph{Rule 3: Fork Identification} Packages with identical or highly similar README files, descriptions, or metadata are more likely to be genuine typosquat cases. Our implementation uses text similarity metrics to compare these elements. If there is a high degree of similarity, the flagged package is considered a potential fork, suggesting it may indeed be a typosquat.

\paragraph{Rule 4: Active Development} Packages that are actively maintained (\eg have been recently updated or have multiple versions) are generally not typosquats. Our script retrieves metadata such as the last update date and version count. Packages with updates within the last 30 days or \TODO{more than five versions} are classified as actively developed, indicating they are less likely to be typosquats.

\paragraph{Rule 5: Comprehensive Metadata} Packages missing critical metadata elements (\eg license, official webpage, maintainers) are more likely to be typosquats. Our tool checks for the presence of these key attributes. If several are missing, it raises a flag, marking the package as a potential typosquat, as legitimate packages typically have complete metadata.

\paragraph{Rule 6: Consistent Quality Metrics} Discrepancies between quality indicators, such as high download counts but low contributor activity, suggest a higher likelihood of a typosquat. Our implementation calculates ratios like downloads per version and checks for inconsistencies across metrics. If quality indicators are imbalanced, the package is flagged as a likely typosquat, while consistent metrics suggest legitimacy.

Each of these rules is applied sequentially, and the results are logged to provide transparency in the verification process. We use multiprocessing to handle registries concurrently, allowing efficient processing of large datasets. If a package passes any rule suggesting it is legitimate, it is classified as a false positive; otherwise, it remains flagged as a true positive typosquat. The results are saved for further review and refinement.

\subsubsection{Evaluation of FP verifier}

We conducted an evaluation of our FP verifier on the typosquat dataset from a prior real-world typosquatting attack dataset created by Neupane \etal~\cite{neupane2023beyondTyposquatting}. Among all the 1232 packages in their dataset, our FP verifier was able to classify 1225 true positives and 7 false positives. This achieves an accuracy of 99\% which is substantially effective on classifying true-positives.
\WJ{TODO: False-positive dataset.}
\fi


\iffalse

\subsection{Step 4: Legitimate Package Verification Using Metadata}

\subsubsection{Preliminary Analysis}

\TODO{@Berk Sampling and analysis of FP data}
\WJ{@Jamie Could you take a look at this methodology and let me know if it looks good?}

To refine our typosquatting detector and reduce false alarms, we began by examining a subset of packages flagged as false positives in the work by Neupane \etal~\cite{neupane2023beyondTyposquatting}. Their tool identifies suspicious names by string similarity, without considering actual package content or metadata. As a result, many of the flagged entries potentially include benign forks, relocated packages, or developer experiments --- rather than truly malicious or high-intent typosquats per our definition (\cref{sec:ProblemStatement-Definition}).



\paragraph{Methodology:}
We randomly sampled \SampledTypomindFPNum of these packages from the original \TypomindFPNum false positives, achieving a \FPPrelimDataConfidenceLevel\% confidence level with a \FPPrelimDataMarginError\% margin of error. Two researchers independently inspected each sampled package’s metadata (\eg maintainers, version history), code content, and naming context to assess whether it revealed deceptive or harmful intent. Initial interrater reliability was \TODO{xx} by Cohen’s Kappa~\cite{cohen1960coefficient}, after which the researchers discussed disagreements and reached consensus on all cases.

\paragraph{Results:}
We catalogued four recurring false-positive categories in total, reflecting issues like legitimate package relocation or brand extensions. We also tracked category “saturation” (\cref{fig:xxx}) to ensure our taxonomy comprehensively captured the diverse non-malicious reasons for similar naming. Full details on the sampled dataset and analysis steps appear in \cref{sec: artifact}. This process guided subsequent improvements to our system, striking a better balance between detecting truly malicious typosquats and avoiding false positives (\textit{Req$_{3}$}).


\paragraph{Preliminary Results:}

\JD{The reader may wonder why not learn these rules, why should we use heuristics instead? (I assume it's for explainability?)}
Based on our observations, we initially established six rules to filter out false positives:

\paragraph{Rule 1: Intentional Naming Patterns} Packages with deliberate, brand-related naming patterns are likely to be legitimate, reducing the chances of false positives. For example, a package named \texttt{express-plus} that legitimately extends the \texttt{express} framework may not be a typosquat.

\paragraph{Rule 2: Distinct Purpose or Functionality} If the flagged package and the legitimate package serve clearly different purposes or functionalities, it is likely a false positive. For instance, if \texttt{lodash-utils} provides utility functions unrelated to the original \texttt{lodash} library, it may not be a typosquat.

\paragraph{Rule 3: Fork Identification} Packages with identical or highly similar README files, descriptions, or metadata are more likely to be genuine typosquats. Identifying such similarities helps confirm cases where a package may have been forked with malicious intent.

\paragraph{Rule 4: Active Development} Packages that are actively maintained—evidenced by recent updates or multiple versions—are typically not typosquats, thus reducing false positives. A package with regular commits and a history of releases is less likely to be malicious.

\paragraph{Rule 5: Comprehensive Metadata} Packages missing critical metadata elements such as a license, official webpage, or maintainer information are more likely to be typosquats. Legitimate packages usually provide complete metadata to establish credibility.

\paragraph{Rule 6: Consistent Quality Metrics} Discrepancies between quality indicators—for example, a high download count but low contributor activity—suggest a higher likelihood of a typosquat. Consistent metrics across various indicators are characteristic of legitimate packages.

After applying these rules to our dataset, we reviewed the preliminary results and identified additional patterns that led to false positives.
\JD{Did you refine the original 6?}
Consequently, we added three more rules to further refine our detection:

\paragraph{Rule 7: Package Relocation} We found that many false positives were due to package relocations, especially in Maven. By identifying relocation information in metadata files (\eg the \texttt{pom.xml} file in Maven), we filtered out these cases. For example, \textit{io.fnproject.fn
} was relocated from \textit{com.fnproject.fn
} and should not be considered a typosquat.

\paragraph{Rule 8: Overlapping Maintainers} Many false positives occur when packages share the same maintainers. If two packages have overlapping maintainers, they are likely not typosquats. For instance, \texttt{botocore-a-la-carte-machinelearning} and \texttt{botocore-a-la-carte-chatbot} share the same maintainer, indicating legitimate extensions rather than typosquatting.

\paragraph{Rule 9: Substantially Different Package Name Lengths} Packages with significantly different name lengths compared to the legitimate package are less likely to be typosquats. A package name that is much longer or shorter may serve a different purpose and not be an attempt to impersonate.

\subsubsection{Implementation}

To implement these rules for false-positive removal, we developed a pipeline that automates metadata verification and comparison across the registries. The process begins by loading potential typosquats detected by our tool, retrieving metadata for both the flagged and legitimate packages, and applying the rules to determine if the flagged package should be classified as a false positive. Below, we describe our detailed implementation of each rule:

\paragraph{Rule 1: Intentional Naming Patterns}

Our implementation compares the flagged package name with the legitimate one to detect creative variations or intentional plays on words. We use string similarity metrics and check for the presence of additional terms that indicate purposeful naming (\eg \texttt{-plus}, \texttt{-extra}, \texttt{-utils}). This helps filter out cases where a package's naming is intended for branding or legitimate extension, rather than typosquatting.

\paragraph{Rule 2: Distinct Purpose or Functionality}

We extract and compare the package descriptions, analyzing the semantic content using natural language processing techniques. We calculate the cosine similarity between the TF-IDF vectors of the descriptions. If the similarity score is below a threshold (\eg 0.5), we consider the packages to serve distinct purposes, reducing the likelihood of typosquatting.

\paragraph{Rule 3: Fork Identification}

We compare the README files, code repositories, and other metadata using text and structural similarity metrics. A high degree of similarity suggests that the flagged package may be a fork or a direct copy, which could indicate typosquatting. We also check for identical version histories and file structures.

\paragraph{Rule 4: Active Development}

Our script retrieves metadata such as the last update date, version count, and commit history. Packages that have been updated within the last 30 days or have more than a specified number of versions (\eg five) are classified as actively developed. We also consider the number of contributors and the frequency of updates.

\paragraph{Rule 5: Comprehensive Metadata}

We check for the presence of critical metadata elements such as license information, official webpage URLs, maintainer contact details, and repository links. Packages missing several of these key attributes are flagged as potential typosquats. We consider the absence of at least two critical metadata elements as a strong indicator.

\paragraph{Rule 6: Consistent Quality Metrics}

We calculate quality indicators such as download counts, contributor activity, issue tracker activity, and community engagement. We look for inconsistencies, such as a high download count with minimal contributor activity or lack of community interaction. Such discrepancies may suggest that the package's popularity is artificially inflated.

\paragraph{Rule 7: Package Relocation}

We parse metadata files (\eg the \texttt{pom.xml} file in Maven) to identify packages that have been relocated or renamed. We look for tags or fields indicating relocation, such as \texttt{<relocation>} in Maven. These packages are filtered out, as they are legitimate but may appear similar to previous versions.

\paragraph{Rule 8: Overlapping Maintainers}

We compare the maintainers of the flagged package and the legitimate package by retrieving maintainer information from the package metadata or repository. If there is any overlap in maintainers or contributing authors, the package is likely legitimate. We use unique identifiers like email addresses or usernames for accurate matching.

\paragraph{Rule 9: Substantially Different Package Name Lengths}

We calculate the lengths of the package names and compare them. If the flagged package name is significantly longer or shorter than the legitimate package name (\eg a difference of more than 30%), it may be less likely to be a typosquat. This helps eliminate packages that are unlikely to be mistaken for the legitimate one due to noticeable differences in name length.

Each of these rules is applied sequentially, and the results are logged to provide transparency in the verification process. We use multiprocessing to handle registries concurrently, allowing efficient processing of large datasets. If a package satisfies any rule suggesting it is legitimate, it is classified as a false positive; otherwise, it remains flagged as a true positive typosquat. The results are saved for further review and refinement.

\subsubsection{Evaluation of False-Positive Verifier}

We evaluated our false-positive verifier using two datasets, as shown in \cref{table:verifier_results}: the typosquat dataset from a prior study by Neupane \etal~\cite{neupane2023beyondTyposquatting} and human-evaluated false-positive data provided by Socket. In Neupane’s dataset, which includes 1,232 packages, our verifier correctly classified 1,225 as true positives and identified 7 as false positives, achieving an accuracy of 99\%. This demonstrates the verifier’s effectiveness in distinguishing true typosquats from legitimate packages. For Socket’s dataset of 19,521 packages with similar but legitimate names, the verifier achieved a True Negative Rate (specificity) of 99.33\%, correctly identifying 19,390 non-typosquats while incorrectly flagging 131 as false positives, resulting in a False Positive Rate of 0.67%.


\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Total Packages} & \textbf{True Positives} & \textbf{False Positives} & \textbf{True Negatives} & \textbf{Accuracy} & \textbf{Specificity (TNR)} & \textbf{False Positive Rate} \\ \hline
Neupane \etal~\cite{neupane2023beyondTyposquatting} & 1,232 & 1,225 & 7 & N/A & 99\% & N/A & N/A \\ \hline
Socket & 19,521 & N/A & 131 & 19,390 & N/A & 99.33\% & 0.67\% \\ \hline
\end{tabular}%
}
\caption{Performance evaluation of the false-positive verifier on two datasets.}
\label{table:verifier_results}
\end{table}


By implementing these rules and integrating metadata verification into our detection pipeline, we significantly improved the precision of our typosquatting detector. This approach ensures that legitimate packages are not incorrectly flagged, thereby maintaining trust in the ecosystem and meeting the production requirement of balancing true positives while minimizing false positives (\textit{Req$_{3}$}).
% Total packages processed: 1232
% True Positives: 1225
% False Positives: 7
% Accuracy: 0.99                                    \section{Evaluation}
\label{sec:evaluation}

In this section, we experimentally validate our typosquatting detection system, assessing its accuracy, performance, and ability to discover new threats. We begin by summarizing our experimental setup and datasets (\S\ref{sec:Eval-exp-setup}), then introduce the baselines we compare against (\S\ref{sec:baselines}). Finally, we present our research questions and results (\S\ref{sec:results}), culminating in an analysis of how effectively our approach meets real-world requirements.
\fi
\fi

% suppressing false positives - integrating empirically-derived signals
% hierarchically-aware - new design for identifying name similarity

\iffalse
\subsection{Implementation}

Our system is implemented in Python, with embeddings stored in PostgreSQL.
Quantization and embedding generation rely on a modified FastText pipeline (\cref{sec:SystemDesign-Step2}).
\tool has over 3K \texttt{LOC}, primarily comprising $\sim$1K \texttt{LOC} for embedding creation, $\sim$500 \texttt{LOC} for popularity
check, $\sim$1K \texttt{LOC} for neighbor search, $\sim$300 \texttt{LOC} for metadata verification, and $\sim$100 \texttt{LOC} for alerting.
% \JD{Can you comment on the lines of code, possily broken down by component?}
\fi

\section{Evaluation}
\label{sec:Eval}

% \WJ{TODO: We might also need a benchmark on AI model versions. Let's try o1 and o1 mini if possible. At least the mini models will have much lower latency}
To evaluate \tool, we pose five Evaluation Questions (EQs) to assess its performance at the component level (\textbf{EQ1}–\textbf{EQ3}) and the system level (\textbf{EQ4}–\textbf{EQ5}).
At the component level, we measure the effectiveness of novel mechanisms introduced in \tool.
At the system level, we evaluate its integrated functionality and scalability compared to baseline tools (\cref{sec:baselines}), and ability to detect real-world package confusion threats. 
% \WJ{TODO: Make sure this is included.}
Additionally, we compare our approach to SOTA methods to benchmark its effectiveness. An overview of the evaluation process is illustrated in \cref{fig:pipeline}.




% We frame our experimental evaluation around six core questions, each assessing a different aspect of our system:
% \JD{Justify these measurements: ``We measure the novel components, and then we measure properties of the integrated system. See Figure XXX for an illustration.}

% \JD{Now add subheadings, one for the component-level and one for the integrated system. And 1-2 sentences that introduce each question set.}

\paragraph{Component-level}
We assess how individual components contribute effectively to the overall system.

\begin{itemize}[leftmargin=*, itemsep=0.1ex]
    \item \textbf{EQ1: Performance of Embedding Model.}
    What is the accuracy and efficiency of our
    % fine-tuned/quantized
    embedding model? (\cref{sec:SystemDesign-Step3})

    \item \textbf{EQ2: Neighbor Search Accuracy.}
    How effective is the neighbor search approach?
    (\cref{sec:SystemDesign-Step4})

    \item \textbf{EQ3: Metadata Verification Accuracy.}
    % \JD{FP verifier -> benignity check}
    How much does the benignity filter reduce false positive rates? (\cref{sec:SystemDesign-Step5})
\end{itemize}

\paragraph{System-level}
We examine the performance of the full \tool system and compare to other approaches.
%emphasizing its practical applicability, scalability, and capability to detect novel typosquatting threats.


\begin{itemize}[leftmargin=*, itemsep=0.1ex]
    \item \textbf{EQ4: Discovery.}
    Can \tool identify previously unknown package confusion threats?
    % especially among newly published packages or AI model entries?
    \item \textbf{EQ5: Baseline Comparison.}
    How does \tool perform in terms of accuracy and latency compared to SOTA tools? 
    % (Throughput is critical for full registry scans, while low latency ensures viability for on-demand API queries.)
    % \JD{``What is the throughput and latency of system?''. You should justify why both metrics are of interest --- throughput is useful for whole-registry scans, while latency is useful when offering on-demand query via API. Are both relevant?}

    % \JD{System needs a cool name. Ask GPT. Write the title as ``CODENAME: blah blah blah''}

    % \item \textbf{EQ6: Coverage in Hierarchical Ecosystems.}
    % How effectively does \tool detect complex typosquatting patterns, such as \textit{impersonation squatting} and \textit{compound squatting}, within hierarchical naming ecosystems?

    % \item \textbf{EQ6: Comparison to SOTA  Methods}
    % How does \tool compare to existing state-of-the-art methods in detecting typosquatting threats, in terms of accuracy and efficiency?
    % , and robustness?   \JD{robustness? what is?}
\end{itemize}

All experiments run on a server with 32 CPU cores (Intel Xeon CPU @ 2.80GHz) and 256 GB of RAM.
Notably, the training and fine-tuning of FastText models do not require GPUs.
% \JD{Any GPU? If not, you might note that FastText doesn't need a GPU (if that's the case?)}

% \PA{I'll suggest you rearrange your questions based on importance. In my opinion, the most important question is - does the system as a whole work (EQ4-2, EQ5)? What is the false positivity rate (some variant of EQ4-1)? I also want to see how these metrics compare with those from prior works}

% \PA{EQ1, EQ3 and EQ4-1 can be one EQ on conduction ablasion study - how do the design of each component contribute to the success of the overall system}


\subsection{Baseline and Evaluation Datasets}
\label{sec:Eval-Setup}

\paragraph{State-of-the-Art Baselines}
\label{sec:baselines}

We compare our system to the Levenshtein distance approach~\cite{vu2020typosquatting}, and
Typomind~\cite{neupane2023beyondTyposquatting}. 
We consider OSSGadget~\cite{MicrosoftOSSGadget} out of scope because it only handles lexical confusions and its latency for long package names makes it unsuitable for production. These are all state-of-the-art open-source tools.

% \JD{Need a sentence: These are the state of art or otherwise reasonable choices. (\eg stacklok would be nice but it's not OSS?)}


\iffalse
We compare our system to three representative baselines: Typomind~\cite{neupane2023beyondTyposquatting}, Spellbound~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}, and OSSGadget~\cite{MicrosoftOSSGadget}.
These are the leading academic and industry tools available for comparison.
\fi
%These baselines allow us to gauge whether hierarchical name support, quantization, and metadata-driven false-positive reduction yield tangible improvements over existing approaches.

\paragraph{Evaluation Datasets}
We evaluate using two datasets:

\begin{itemize}[leftmargin=1.5em, itemsep=0.1ex]
    \item \texttt{\textbf{NeupaneDB}}: 
    % all confirmed package confusing attack true positives, while include many suspicious packages that does not include malware.
    \NumNeupaneDB packages from \cite{neupane2023beyondTyposquatting}, including
        \NumTypomindData confirmed real-world package confusion attacks, 
        and   
        \SampledAvailableTypomindFPNum manually labeled data we analyzed in \cref{sec:ProblemState-CaseStudy2-FP}.
\item  \texttt{\textbf{ConfuDB}}: \NumConfuDB packages triaged by security analysts, collected during the development and refinement of \tool (\cref{sec:SystemDesign-Step6}).
\end{itemize}


% These data focus on string-similarity attacks but do not incorporate stealth attack checks.
% For baseline comparisons, we utilize the dataset from the state-of-the-art study published at USENIX'23~\cite{neupane2023beyondTyposquatting}, incorporating their published evaluation results as reference points. To assess the system's performance on previously unknown typosquat threats, we applied our system to a randomly sampled dataset of 5000 packages from each registry.
% \PA{What are the sizes of these datasets?}
% \JD{It would be better to run ourselves than to reuse the data, but given the time constraint you can simply say "We used the same dataset so we copied the evaluation result from USENIX'23"}
% \begin{itemize}[leftmargin=*, itemsep=0.1ex]
%     \item \textbf{Typomind Ground-Truth}~\cite{neupane2023beyondTyposquatting}: Includes \NumTypomindData confirmed typosquats labeled typosquatting packages from NPM, PyPI, and RubyGems. These data focus on string-similarity attacks but do not incorporate malicious intent checks.
%     % \item \textbf{Hugging Face Typosquats}~\cite{protectai_huggingface}: Compiled from public blog posts and suspicious model listings, reflecting hierarchical naming (\texttt{author/model}) and AI-specific threats.
%     % \item \textbf{LLM-Generated Name Hallucinations}~\cite{spracklen2024LLMPackageHallucinations}: \NumHallucinationData synthetic examples created by large language models, testing robustness against unusual or creative manipulations.
%     % \item \textbf{Human-Reviewed False-Positives}: A curated set of \NumProdReviewedTypos flagged packages verified by security analysts from our industry partner. These packages are labeled as truly malicious or high-intent versus benign forks.
% \end{itemize}

% Combining these datasets ensures coverage of both low-level name confusion (\eg edit-distance changes) and higher-level intent-based attacks (\eg \textit{impersonation squatting} in AI model hubs).

% \paragraph{Evaluation Pipeline.}
% All packages are embedded using our fine-tuned (and optionally quantized) FastText model. We then run the similarity search (Step 3 in \S\ref{sec:system_design}) to obtain candidate typosquats for each known “popular” package or author. Finally, we apply the nine-step metadata verification procedure (Step 4) to reduce false positives.



\iffalse
\begin{itemize}[leftmargin=*]
    \item \textbf{Typomind}~\cite{neupane2023beyondTyposquatting}: An embedding-based detector using FastText for name similarity but lacking metadata verification or hierarchical name handling.
    \item \textbf{SpellBound}~\cite{taylor2020spellboundDefendingAgainstPackageTyposquatting}: The most state-of-the-art tool employing edit-distance calculations, identifying packages that fall within a small Levenshtein distance of well-known libraries.
    \item \textbf{OSSGadget}~\cite{MicrosoftOSSGadget}: Similar to SpellBound, it uses edit-distance checks to flag packages within a minimal Levenshtein threshold of popular libraries.
    % \item \textbf{Socket v1}: \WJ{How to present this baseline without mentioning ``Socket''?}
    % \PA{Does providing this comparison produce new insight? Especially as there are publicly available baselines to compare with.}
    % A prior version of our system that partially uses popularity metrics and direct name comparisons but omits advanced embedding or code-level heuristics.
\end{itemize}
\fi



% \subsection{Evaluation Questions}
% \label{sec:eval-questions}
\subsection{EQ1: Perf. of Embedding Model}
\label{sec:EQ1}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ1 Summary:}
Our embedding model demonstrates superior effectiveness and efficiency, ensuring an acceptable overhead for SPR during the preparation of the embedding database.
\end{tcolorbox}
\fi


We measured effectiveness and efficiency of our embedding model.

\subsubsection{Effectiveness}

\paragraph{Method}
We evaluate the effectiveness of embedding-based similarity detection by comparing three approaches:

\begin{enumerate}[leftmargin=*, itemsep=0.2ex]
    \item \textit{Levenshtein-Distance}, calculates the number of single-character edits required to change one package name to another.
    \item \textit{Pre-trained FastText}~\cite{bojanowski2017FasttextEmbedding} (\texttt{cc.en.300.bin}), used in the SOTA work Typomind~\cite{neupane2023beyondTyposquatting}, employs the general-purpose embedding model \texttt{cc.en.300.bin}~\cite{bojanowski2017FasttextEmbedding} to capture semantic relationships.
    \item \textit{Fine-tuned FastText (\textbf{Ours})}, which we have adapted using a corpus specifically composed of package names to better capture domain-specific similarities.
\end{enumerate}

To systematically compare these methods, we construct a balanced test set consisting of both positive and negative pairs, with each category containing 1,239 data points.


\begin{itemize}[leftmargin=*, itemsep=0.2ex]
    \item \textit{Positive Pairs}: 1,239 real package confusions from \texttt{\textbf{NeupaneDB}}~\cite{neupane2023beyondTyposquatting}.

    \item \textit{Negative Pairs}: Created by randomly pairing unrelated package names from registries, ensuring low similarity scores across all tested methods. These pairs are guaranteed not to represent package confusion relationships.
\end{itemize}

For each approach, we applied a predefined similarity threshold to classify package pairs as potential package confusion attacks. The threshold was selected via a grid search to optimize Precision and Recall, ensuring effective identification of true confusions. Pairs with similarity scores above the threshold were classified as positive, while those below were classified as negative. False positives were subsequently filtered using our false-positive verification process in Step \textcolor{blue}{\textcircled{\small{5}}}.
We compute Precision, Recall, and F1 scores for each approach to assess their performance.

% To get the best threshold for each model, we performed a grid search to determine the optimal similarity threshold, ensuring that embedding similarity effectively identifies true typosquats by detecting similar package names. Some false positives, initially misclassified as suspicious packages, are subsequently filtered out by our false-positive verification process in Step \textcolor{blue}{\textcircled{\small{5}}}.

% \begin{itemize}[leftmargin=*, itemsep=0.2ex]
%     \item \textit{Precision}: The proportion of correctly identified package confusions out of all flagged pairs.
%     \item \textit{Recall}: The proportion of actual package confusions that were correctly identified.
%     \item \textit{F1 Score}: The mean of Precision and Recall, providing a balanced performance metric.
% \end{itemize}

%This evaluation highlights the differences in naming patterns captured by each method and demonstrates the advantages of our fine-tuned FastText model in accurately identifying typosquatting attacks while minimizing false positives.
\renewcommand*{\arraystretch}{0.5} \begin{table}[h] 
\tiny 
\centering 
\caption{Similarity effectiveness of using distance and embedding. We apply a grid search which automatically determines the optimal threshold to maximize F1 scores for positive pairs while maintaining relatively high F1 scores for negative pairs.}
\label{tab:EQ1_effectiveness_updated} 

% Quantization slightly improves F1 scores because the false positive rate goes up a bit while the false negative rate goes down more substantially.} 
\resizebox{0.95\linewidth}{!}
{
\footnotesize 
\begin{tabular}{lccccccc} 
\toprule \textbf{Model} & \multicolumn{3}{c}{\textbf{Positive Pairs}} & \multicolumn{3}{c}{\textbf{Negative Pairs}} & \textbf{Overall Score} \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-7} & P & R & F1 & P & R & F1 Score & \\ 
\midrule 
Levenshtein Distance & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
Pretrained & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
\midrule
Fine-Tuned (\textbf{Ours}) & 1.00 & \textbf{0.90} & \textbf{0.95} & 1.00 & 0.96 & 0.98 & \textbf{0.96} \\
% \TODO{Typomind} & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
% Hybrid (\textbf{Ours})  & 1.00 & 0.93 & 0.96 & 1.00 & 1.00 & 1.00 & 0.98 \\

\bottomrule 
\end{tabular} 
}
\end{table}







\iffalse
\renewcommand*{\arraystretch}{0.4}
\begin{table*}[h]
\footnotesize
\centering
\caption{
Performance of embedding models.
We selected the fine-tuned model with a threshold of 0.9 to achieve high F1 scores for positive pairs and relatively high F1 scores for negative pairs.
Quantization slightly improves F1 scores because the false positive rate goes up a bit while the false negative rate goes down more substantially.
}
\label{tab:EQ1_effectiveness_updated}
\scriptsize
\begin{tabular}{lllccccccc}
\toprule
\textbf{Model} & \textbf{Quantization} & \textbf{Threshold} & \multicolumn{3}{c}{\textbf{Positive Pairs}} & \multicolumn{3}{c}{\textbf{Negative Pairs}} & \textbf{Overall Score} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
& & & Precision & Recall & F1 Score & Precision & Recall & F1 Score & \\
\midrule
Edit Distance & N/A & 4 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
\midrule
Pretrained & float32 & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
Pretrained & float16 & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
Pretrained & int8 & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.95 \\
\midrule
Fine-Tuned & float32 & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
Fine-Tuned & float16 & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
Fine-Tuned & int8 & 0.9 & 1.00 & 0.88 & 0.94 & 1.00 & 0.96 & 0.98 & 0.96 \\
\midrule
Hybrid & N/A & 0.5 & 1.00 & 0.91 & 0.95 & 1.00 & 0.91 & 0.95 & 0.95 \\
\bottomrule
\end{tabular}
\end{table*}
\fi

\iffalse
{
\renewcommand*{\arraystretch}{0.4}
\begin{table*}[h]
\footnotesize
\centering
\caption{
Evaluation Metrics (EQ1) for Positive and Negative Pairs Across Various Thresholds.
We selected the fine-tuned model with a threshold of 0.9 to achieve high F1 scores for positive pairs and relatively high F1 scores for negative pairs.
Quantization slightly improves F1 scores because the false positive rate goes up a bit while the false negative rate goes down more substantially.
% \WJ{Is it really necessary to put this table here? Or Figure 5 is enough?}
% \JD{Caption should mention that the quantization slightly improves F1 score and that this is because the false positive rate goes up a little but the false negative rate goes down a lot.}
% \JD{Make sure the tuning curve of 0.8 is also placed in here as a figure. Use the proper name for it I forget :-D}
\JD{Can we omit the rows for 0.6 and 0.8 throughout (and just write ``some thresholds omitted for space'')}
\JD{I find this table analysis a bit hard to follow. Can you expand the caption slightly and also perhaps the prose?}
\JD{DISCUSSION: ``I am thinking maybe we can cut the thresholds and just put the best result from the best threshold for each model''}
}
\label{tab:EQ1_effectiveness}
\scriptsize
\begin{tabular}{lllccccccc}
\toprule
\textbf{Model} & \textbf{Quantization} & \textbf{Threshold} & \multicolumn{3}{c}{\textbf{Positive Pairs}} & \multicolumn{3}{c}{\textbf{Negative Pairs}} & \textbf{Overall Score} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-10}
& & & Precision & Recall & F1 Score & Precision & Recall & F1 Score & \\
\midrule
\multirow{3}{*}{Edit Distance} & \multirow{3}{*}{N/A}
 & 2 & 1.00 & 0.78 & 0.87 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 3 & 1.00 & 0.79 & 0.88 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 4 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
\midrule
\multirow{15}{*}{Pretrained}
 & \multirow{5}{*}{float32}
    & 0.5 & 1.00 & 0.87 & 0.93 & 1.00 & 0.92 & 0.96 & 0.94 \\
 & & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
 & & 0.7 & 1.00 & 0.83 & 0.91 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.8 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 0.9 & 1.00 & 0.66 & 0.80 & 1.00 & 1.00 & 1.00 & 0.90 \\
\cmidrule{2-10}
 & \multirow{5}{*}{float16}
    & 0.5 & 1.00 & 0.87 & 0.93 & 1.00 & 0.92 & 0.96 & 0.94 \\
 & & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.96 \\
 & & 0.7 & 1.00 & 0.83 & 0.91 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.8 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.94 \\
 & & 0.9 & 1.00 & 0.66 & 0.80 & 1.00 & 1.00 & 1.00 & 0.90 \\
\cmidrule{2-10}
 & \multirow{5}{*}{int8}
    & 0.5 & 1.00 & 0.87 & 0.93 & 1.00 & 0.93 & 0.96 & 0.94 \\
 & & 0.6 & 1.00 & 0.85 & 0.92 & 1.00 & 0.98 & 0.99 & 0.95 \\
 & & 0.7 & 1.00 & 0.83 & 0.90 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.8 & 1.00 & 0.80 & 0.89 & 1.00 & 1.00 & 1.00 & 0.95 \\
 & & 0.9 & 1.00 & 0.66 & 0.80 & 1.00 & 1.00 & 1.00 & 0.90 \\
\midrule
\multirow{15}{*}{Fine-Tuned}
 & \multirow{5}{*}{float32}
    & 0.5 & 1.00 & 0.98 & 0.99 & 1.00 & 0.03 & 0.05 & 0.52 \\
 & & 0.6 & 1.00 & 0.97 & 0.98 & 1.00 & 0.05 & 0.10 & 0.54 \\
 & & 0.7 & 1.00 & 0.96 & 0.98 & 1.00 & 0.21 & 0.35 & 0.66 \\
 & & 0.8 & 1.00 & 0.94 & 0.97 & 1.00 & 0.56 & 0.72 & 0.84 \\
 & & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
\cmidrule{2-10}
 & \multirow{5}{*}{float16}
    & 0.5 & 1.00 & 0.98 & 0.99 & 1.00 & 0.03 & 0.05 & 0.52 \\
 & & 0.6 & 1.00 & 0.97 & 0.98 & 1.00 & 0.05 & 0.10 & 0.54 \\
 & & 0.7 & 1.00 & 0.96 & 0.98 & 1.00 & 0.21 & 0.35 & 0.66 \\
 & & 0.8 & 1.00 & 0.94 & 0.97 & 1.00 & 0.56 & 0.72 & 0.84 \\
 & & 0.9 & 1.00 & 0.90 & 0.95 & 1.00 & 0.96 & 0.98 & 0.96 \\
\cmidrule{2-10}
 & \multirow{5}{*}{int8}
    & 0.5 & 1.00 & 0.98 & 0.99 & 1.00 & 0.04 & 0.09 & 0.54 \\
 & & 0.6 & 1.00 & 0.97 & 0.98 & 1.00 & 0.07 & 0.13 & 0.56 \\
 & & 0.7 & 1.00 & 0.96 & 0.98 & 1.00 & 0.23 & 0.38 & 0.68 \\
 & & 0.8 & 1.00 & 0.93 & 0.97 & 1.00 & 0.61 & 0.76 & 0.86 \\
 & & 0.9 & 1.00 & 0.88 & 0.94 & 1.00 & 0.96 & 0.98 & 0.96 \\
\midrule
\multirow{5}{*}{Hybrid} & \multirow{5}{*}{N/A}
 & 0.5 & 1.00 & 0.91 & 0.95 & 1.00 & 0.91 & 0.95 & 0.95 \\
 & & 0.6 & 1.00 & 0.87 & 0.93 & 1.00 & 0.87 & 0.93 & 0.93 \\
 & & 0.7 & 1.00 & 0.82 & 0.90 & 1.00 & 0.82 & 0.90 & 0.90 \\
 & & 0.8 & 1.00 & 0.72 & 0.84 & 1.00 & 0.72 & 0.84 & 0.84 \\
 & & 0.9 & 1.00 & 0.64 & 0.78 & 1.00 & 0.64 & 0.78 & 0.78 \\
\bottomrule
\end{tabular}
\end{table*}
}
\fi


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/finetuned_threshold_accuracy.pdf}
        \caption{}
        \label{fig:threshold_accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/finetuned_roc_curve.pdf}
        \caption{}
        \label{fig:roc_curve}
    \end{subfigure}
    \caption{Performance Metrics of the Fine-Tuned Model: (a) Threshold Accuracy and (b) ROC Curve.}
    \label{fig:EQ1-ThresholdCurves}
\end{figure}
%\vspace{-5mm}



\paragraph{Result}
% \JD{Maybe I'm bad at reading this table, but it seems that all approaches have at least one row with a good F1 score overall and also good results by positive and negative pairs?}
\cref{tab:EQ1_effectiveness_updated} presents the comparison between our fine-tuned model and baseline approaches. The results show that our approach outperforms both the Levenshtein Distance and Pretrained models, achieving the highest F1 score for positive pairs (0.95) while maintaining strong performance on negative pairs (0.98).

\cref{fig:EQ1-ThresholdCurves} illustrates the performance metrics of our approach through a grid search, highlighting the relationship between similarity threshold and accuracy, as well as the ROC curve for the fine-tuned model. The optimal similarity threshold of 0.93, identified through this search, is used in the production system. With an AUC of 0.94, our model demonstrates a balanced trade-off between precision and recall for both positive and negative pairs. 
% These results confirm the effectiveness of our fine-tuning strategy in improving similarity detection.


% \cref{tab:EQ1_effectiveness_updated} presents the results comparing our fine-tuned model with baseline approaches. 
% \cref{fig:EQ1-ThresholdCurves} shows the grid search results for our approach, including accuracy and ROC curves for the fine-tuned model. The best similarity threshold 0.93 is used in the production system. 
% The table shows that the accuracy of the quantized and original versions of our fine-tuned model is nearly identical, indicating minimal loss in performance from quantization. Additionally, the overall F1 score of the fine-tuned model is approximately 1\% higher than the pre-trained model, with a notable 2-3\% improvement in recall for positive pairs.
% This improvement in recall is particularly critical for detecting package confusion attacks, as correctly identifying all positive pairs is essential for comprehensive threat mitigation. At the same time, our approach maintains a relatively high accuracy for negative pairs.


\subsubsection{Efficiency}

\paragraph{Method}
To assess the efficiency of embedding database creation, we evaluated the overhead of embedding database creation step (\cref{sec:SystemDesign-Step2}).
We evaluated the embedding database creation efficiency by measuring the throughput, latency, and overall overhead associated.



\begin{table}[h]
\centering
\caption{Evaluation of embedding model efficiency, \textit{HNSW} indexing overhead, and total embedding database creation overhead. 
}
% The throughput and latency differences between \texttt{float16} and \texttt{int8} are minimal due to similar overall I/O and storage overheads, as well as the efficient handling of embeddings using \texttt{pgvector}. However, some increased latency in \texttt{int8} embedding creation is observed due to additional quantization and processing steps required for integer-based representations.}
% Updated \texttt{float16} and \texttt{int8} quantization formats.}
\resizebox{0.95\linewidth}{!}{%
\scriptsize
\begin{tabular}{l r r r r r r r}
\toprule
\textbf{Ecosystem}  & \textbf{Throughput} & \textbf{Latency} & \textbf{Indexing Time} & \textbf{Total Overhead} \\
& \textbf{(pkgs/s)} & \textbf{(ms)} & \textbf{(s)} & \textbf{(s)}\\
\midrule
NPM & 7705.99 & 0.13 & 7.42 & 650.19
% 642.77 + 7.42
\\
PyPI & 13581.15 & 0.07 & 4.39 & 46.85
% 42.46 + 4.39
\\
RubyGems & 8694.68 & 0.12 & 4.07 & 28.18
% 24.11 + 4.07
\\
Maven & 4887.19 & 0.20 & 4.91 & 134.47
% 129.56 + 4.91
\\
Golang & 8490.36 & 0.12 & 7.80 & 232.93
% 225.13 + 7.80
\\
Hugging Face & 5027.95 & 0.20 & 4.46 & 166.67
% 162.21 + 4.46
\\
\bottomrule
\end{tabular}%
}
\label{tab:EQ1-Efficiency}
\end{table}


\paragraph{Result}
\cref{tab:EQ1-Efficiency} presents the efficiency measurement.
Overall, the quantized models demonstrate strong performance with minimal overhead. The \textit{HNSW} indexing adds negligible processing time, requiring less than 10 seconds additional overhead per table.


% Our results shows that quantized models substantially increase throughput while decreasing latency. Specifically, both \texttt{float16} and \texttt{int8} quantization methods enhance throughput by approximately $5\times$ and reduce latency by around $10\times$ compared to the \texttt{float32} baseline. However, the performance difference between \texttt{float16} and \texttt{int8} is minimal, likely due to the additional quantization and processing steps involved.

% A detailed results table is available in our artifact (\cref{sec:OpenScience}).

% \cref{tab:EQ1-Efficiency} presents the efficiency metrics for embedding creation across the evaluated models. The results highlight the effectiveness of the quantized versions in maintaining strong performance while optimizing resource usage. Based on this evaluation, we selected the \textit{int8} quantized model for the embedding database, as it offers superior efficiency in both creation and usage while achieving a relatively fast creation speed.

% \cref{tab:EQ1_embeddings} shows that fine-tuning on real package names yields a substantial boost in F1, presumably by capturing domain-specific subwords (\eg py, react, huggingface). Qualitatively, suspiciously similar names (\eg ``lod4sh'' vs. ``lodash'') cluster closer together in t-SNE plots, improving detection recall.






\subsection{EQ2: Neighbor Search Accuracy}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ2 Summary:}
Our embedding-based neighbor search algorithm, combined with the sorting algorithm, achieves accuracy comparable to prior work while capturing richer semantic information.
\end{tcolorbox}
\fi


\paragraph{Methods} We use a threshold of 0.93 obtained from EQ1. To evaluate neighbor search performance, we analyzed suspicious packages identified by \texttt{Typomind}.


\paragraph{Results} 
% Additionally, \cref{tab:similarity results} showcases the neighbors identified using results from \texttt{typomind}.

Our neighbor search algorithm accurately detected 99\% of the \NumTypomindData real-world package confusion attacks from \texttt{NeupaneDB}~\cite{neupane2023beyondTyposquatting}.
Moreover, our method effectively flagged impersonation squatting attacks targeting hierarchical package names, notably identifying reported cases on Hugging Face \cite{protectai_huggingface} and Golang \cite{socketMaliciousGoModule} that earlier methods overlooked. These findings confirm that our neighbor search algorithm achieves state-of-the-art performance.
% Our approach successfully captured all package confusions previously identified by SOTA methods. The neighbor search algorithm demonstrated the ability to detect 99\% of real typosquatting attacks flagged by prior research and the previously human-triaged production data from our partner.

% Production data revealed that the algorithm effectively identified advanced threats, including compound squatting, impersonation squatting, and command squatting, and extended the knowledge to platforms such as Maven, Golang, and Hugging Face.
% For example, our system detected a \textit{compound squatting} attack on \texttt{@typescripyt-eslint/eslint-plugin}, where the attacker used both a similar namespace and package identifier (\texttt{@typescript\_eslinter/eslint}) to mislead users.
% We found that prior tools were unable to identify this type of sophisticated attack.
% \JD{Next sentence: Is this becuase you tried them? If so, write ``We found that prior tools could not identify...''. If not, write ``Prior tools would be unable to...because they...''}

% Additionally, we successfully flagged impersonation squatting attacks targeting hierarchical package names. 
% % While no suspicious packages were identified in Golang, Maven, or Hugging Face due to the limited sample size in these registries, we are confident in the system’s capability to detect such threats. 
% Notably, it can flag the reported impersonation squatting attacks for Hugging Face from \cite{protectai_huggingface} and for Golang from \cite{socketMaliciousGoModule}, which prior work failed to identify.
% These findings highlight that our approach achieves SOTA performance in neighbor search accuracy.

\subsection{EQ3: Benignity Check Accuracy}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ3 Summary:}
Our metadata verification process significantly lowers the false-positive rate from 75.4\% to 5\%.
\end{tcolorbox}
\fi

% \WJ{I think this EQ becomes a system-level comparison now.}
\paragraph{Method}
Starting with the raw embedding output (\ie packages flagged for name-based suspicion), we apply the benignity check described in \cref{sec:SystemDesign-Step5}. 

% on the manually labeled data, \ie \SampledTypomindFPNum flagged packages from \cref{sec:ProblemState-CaseStudy2-FP}.
% Additionally, we also use the data from the \NumTypomindData true positive dataset to make sure \tool do not filter out true threats.
We first use cross-validation to learn the weights and measure false positive rates (FPR) using \texttt{NeupaneDB}.
We then computed SHAP values by learning the weighed sum through regression to quantify the importance of each metric defined in \cref{tab:typosquat_taxonomy}~\cite{lundberg2017unifiedSharpley}. The resulting SHAP plot provides clear insights into how these metrics contribute to our system's decision-making process.
Additionally, we use regression to learn the weights 

\begin{table*}[h]
\centering
\caption{
Accuracy metrics for \tool and \texttt{Typomind}, detailing true/false positives and negatives, as well as recall, precision, F1 score, and accuracy for active, stealthy, and benign threats.
Although Typomind flags all packages as suspicious, our benignity filter substantially reduces false positives in production. 
% Out of \SampledTypomindFPNum sampled package pairs from \cref{sec:ProblemState-CaseStudy2-FP}, only \EvaluatedFPPkgs were evaluated due to missing metadata in our commercial database. Similarly, from 
Due to availability of metadata, we evaluated \tool on 1,043 packages from \texttt{NeupaneDB} and 1,202 \texttt{ConfuDB}.
We note that \tool exhibits lower benign classification accuracy in \texttt{ConfuDB} because it flags more complex cases (\eg compound squatting) as potential threats, a nuance that \texttt{Typomind} does not capture.
}
% Out of \NumTypomindData real attacks, we evaluated only \EvaluatedTPPkgs, as most of these were removed by registries or contributors.
% \WJ{Need advice on how to improve this table}
% \WJ{Maybe include the TP, FP metrics separately in this table? How about a separate table/pie chart??}
% \WJ{
% Based on global definition:
% 356=306(True negative)+50(False negatives),
% 482=365(FP)+117(TP),
% 126=67(TP)+59(FP)
% }
% \WJ{Maybe I am dumb. I now feel like there is no need for separate definitions.}
% \WJ{TODO: Update the numbers based on recollected data.}
\label{tab:EQ5_Accuracy}
\resizebox{0.95\linewidth}{!}
{
\centering
\begin{tabular}{llll|llllllll|llllllll}
\toprule
Dataset   & Total & Threat Type & Sub-total & \multicolumn{8}{c|}{\tool} & \multicolumn{8}{c}{\texttt{Typomind}} \\
\cmidrule(lr){5-12} \cmidrule(lr){13-20}
          &       &             &           & TP  & FP  & TN  & FN & Recall & Prec. & F1   & Acc. & TP    & FP  & TN  & FN  & Recall & Prec. & F1   & Acc. \\
\midrule
 &  & Active      & 1,239     & 386 & 0   & 0   & 57 & 0.87   & 1.00  & 0.93 & 0.87 & 1000  & 0   & 0   & 239 & 0.80   & 1.00  & 0.89 & 0.81 \\
NeupaneDB & 1,840      & Stealthy    & 137       & 80  & 0   & 0   & 56 & 0.59   & 1.00  & 0.74 & 0.59 & 137   & 0   & 0   & 0   & 1.00   & 1.00  & 1.00 & 1.00 \\
 &       & Benign      & 464       & 0   & 72  & 391 & 0  & 0.00   & 0.00  & 0.00 & 0.84 & 0     & 464 & 0   & 0   & 0.00   & 0.00  & 0.00 & 0.00 \\
\midrule
   &  & Active      & 71        & 41  & 0   & 0   & 4  & 0.91   & 1.00  & 0.95 & 0.91 & 40    & 0   & 0   & 31  & 0.56   & 1.00  & 0.72 & 0.56 \\
ConfuDB  &  1,561     & Stealthy    & 230       & 110 & 0   & 0   & 24 & 0.82   & 1.00  & 0.90 & 0.82 & 111   & 0   & 0   & 113 & 0.50   & 1.00  & 0.66 & 0.50 \\
   &       & Benign      & 1,260     & 0   & 532 & 491 & 0  & 0.00   & 0.00  & 0.00 & 0.48 & 0     & 322 & 612 & 0   & 0.00   & 0.00  & 0.00 & 0.65 \\
% \midrule
% Total     & 3,401 &             & 3,401     & 609 & 595 & 816 & 137 & 0.82   & 0.51  & 0.62 & 0.66 & 1,288 & 786 & 612 & 383 & 0.77   & 0.62  & 0.69 & 0.62 \\
\bottomrule
\end{tabular}
}
\end{table*}

% \begin{itemize}[leftmargin=*, itemsep=0.2ex]
    % \item \textbf{Typomind true-positives}~\cite{neupane2023beyondTyposquatting}: Contains \TypomindTPNum flagged packages from NPM, PyPI, RubyGems.
    % \item \textbf{Human-Reviewed (Internal)}: \TODO{xx} flagged packages verified by security analysts, including .
    % \item \textbf{Manually labeled Typomind Outputs}: \SampledTypomindFPNum flagged packages verified in \cref{sec:ProblemState-CaseStudy2-FP}.
% \end{itemize}
%
\paragraph{Result}
% \cref{table:verifier_results} shows the results of our metadata verification:
% \TODO{Rewrite this paragraph}
% We evaluated \tool's metadata verification step on \EvaluatedFPPkgs confusing packages without malware injected. 
% All these packages were flagged as alarming by typomind due to confusing names.
% While our manual analysis shows that among these packages, 365 (76\%) are actually benign packages, and the rest 24\% are stealthy threats.
% our metadata verification step was able to correctly classify 425 as false-positive, \ie we reduced the false-positive rate from 75.4\% to 5\%.

\cref{fig:metadata-ablation} shows the SHAP value plot which indicates that our rules used in the benignity filter are useful. The plot highlights that distinct purpose (R2) and suspicious intent (R10) are the most influential features driving the model’s predictions, suggesting that packages showing suspicious or unusual behavior strongly push the model toward a malicious classification. Conversely, features such as known maintainers (R8) and fork packages (R3) generally pull the model’s output toward benign, indicating they lower the likelihood of a threat. Meanwhile, no clear description also shows a notable positive effect, aligning with the idea that missing documentation can be an indicator of malicious intent (R9). 


We use cross-validation to learn parameters and estimate the trade-off between reducing false positives and increasing false negatives. Our benignity filter correctly flagged 399 out of 464 (86\%) false positives in the human labeled set. In the real-world attack set, it failed to classify 53 true positives out of 443.\footnote{Only 443 out of 1,239 real attacks still have available metadata for our analysis.}
A detailed investigation revealed two main causes: (1) missing metadata and (2) attacks that had been removed, with the package now maintained by npm, which the filter considers a valid maintainer.


% \begin{enumerate}[leftmargin=*]
%   \item \textbf{Neupane \etal~\cite{neupane2023beyondTyposquatting} Dataset:}
%   % Contains 1,232 suspiciously named packages identified through string similarity. Our verifier classified 1,225 as genuine typosquats (true positives) and 7 as false positives (99\% accuracy).

%   \item \textbf{Socket Human-Reviewed Dataset:}
%   % A large set of 19,521 packages, each analyzed by Socket’s security analysts. Our tool achieved a 99.33\% True Negative Rate, incorrectly flagging only 0.67\% of legitimate packages.
% \end{enumerate}

% \begin{table}[h]
% \centering
% \small
% \caption{Performance of metadata verification on true-positive and false-positive data from \cite{neupane2023beyondTyposquatting}.}
% \label{table:verifier_results}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Dataset} & \textbf{Size} & \textbf{TP} & \textbf{FP} & \textbf{Metadata Unavailable}\\
% \midrule
% % True Positives Data & 1,239  & \TODO{1,225} & \TODO{7} \\
% False Positive Data & 665 & \TODO{41}   & \TODO{425}\\
% \bottomrule
% \end{tabular}
% \end{table}

These results confirm that supplementary heuristics beyond raw name similarity reduce false positives while retaining high recall for genuinely malicious confusions. 
% For instance, overlapping maintainer checks (\textbf{R6}) and explicit relocation detection (\textbf{R9}) proved especially effective in ecosystems like Maven, where hierarchical naming changes are common. Ultimately, 
% This benignity filter aligns our detection approach with the realities of open-source development and AI model publication, ensuring minimal ``false alarms'' (\textit{Req$_{3}$}) without missing critical threats.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/shap_summary_plot_v7.pdf}
    \caption{
    % \JD{Add more information to the caption to explain this plot. Then in the prose, explain the implication w.r.t. production data *if we were to use this plot* --- the estimated reduction in false positives and increase in false negatives.}
    SHAP value plot for metadata verification using learnable features (\ie excluding R13, R14) from \cref{tab:metadata_rules}. 
    The x-axis shows each feature's impact on model output (positive values are towards benginity). Feature values are color-coded: red indicates high values with a strong, direct impact on the model output.
    % \WJ{TODO: Talk about the relocated package only works for maven and golang packages and we add this metric based on experience.}
    % \WJ{Simply remove the relocated pkg from this plot? Otherwise we need to put the allowlist metric in.}
    % \WJ{Note: these are the metrics that are actually used to calculate the weighted score. For allowedlist and domain proxy, it will be a simple ``'pass'.}
    % \TODO{Recollecting data to fix the last four features (due to changed metric names).}
    % \JD{Refer to Table 3, say we are only showing the ones that are learnable (eg not allowlists).}
    % \WJ{TODO: Make sure y-axis matches the rules.}
    }
    \label{fig:metadata-ablation}
\end{figure}



\subsection{EQ4: Discovery of New Package Confusions}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ4 Summary:}
Our system effectively detected stealthy typosquatting attacks with malicious intent and identified two new attack types—\textit{impersonation squatting} and \textit{compound squatting}—in a production environment. Moreover, it uncovered these threats across three ecosystems that had not been explored in prior research.
\end{tcolorbox}
\fi

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/threat_type_distribution_filtered_log.pdf}
%     \caption{Production data from NPM showing removed packages flagged by \tool within the past three months.\TODO{Update this figure}}
%     \label{fig:product-removed-pkgs}
% \end{figure}



\paragraph{Method}
% We ran the typosquat detector on 5,000 packages on NPM, PyPI, Maven, and Hugging Face, then we use the attributes from FP verification step to classify each suspicious alert entry as malicious or benign.
% We also deployed our system in production for one month and report our statistics here.
To evaluate the effectiveness of our package confusion detection system, we deployed \tool in a production environment for three month. During this period, flagged packages were analyzed using a commercial malware scanner and reviewed by threat analysts for detailed insights.


\iffalse
\begin{table}[h]
\centering
\caption{Production threat counts triaged by human operators over a three-month period, including removals by threat type.}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Threat Type} & \textbf{Total Count} & \textbf{Removed}  \\
\toprule
Malware \JD{active?}      & 1667 & 1647 \\
Stealthy Confusions \JD{stealthy?}    & 633  & 220  \\
Vulnerability \JD{} & 340  & 235  \\
Other         & 10   & 1    \\
\bottomrule
\end{tabular}

\label{tab:threat_counts}
\end{table}
\fi


\paragraph{Result}
% \JD{Just refer to Table 6. Expand on the brief notes that are in Table 6.}
% \cref{tab:threat_counts} provides a detailed breakdown of typosquatting packages detected by \tool which were flagged by security analysts within three months.
% \WJ{TODO: Table 6 talks about ConfuDB, let's talk about the details here...}
% \cref{tab:threat_counts} 

% presents a detailed breakdown of the threat categories detected by \tool and subsequently triaged by security analysts over a three‐month period. 
\cref{tab:EQ5_Accuracy} presents the package confusion attack that \tool identified during three months deployment period (\ie \texttt{ConfuDB}). 
Within this database, 1,260 packages were labeled as false positives.\footnote{The high false positive rate in \texttt{ConfuDB} is primarily due to the difficulty of confirming stealthy confusion attacks, which were left unreviewed by analysts.}
Among the remaining 301 suspicious packages, 230 (76\%) exhibited stealthy confusion attack behavior, 63 (21\%) were identified as malware, 6 (2\%) were categorized as vulnerabilities, and 2 (0.7\%) fell into other anomaly categories.
% The vulnerability category includes packages with dangerous functionalities and improper handling of user input (\eg deleting files outside the \texttt{tmp} folder). 
% Additionally, the dataset revealed less common threats, such as spyware-only attacks.







Analysis of production data shows that \tool effectively detects advanced threats (\cref{sec:PracticeAnalysis}) -- including compound, impersonation, and command squatting -- across platforms such as Maven, Golang, and Hugging Face. 
For example, it identified an \textit{impersonation squatting} attack on Maven: the malicious package \texttt{\seqsplit{io.github.leetcrunch:scribejava-core}} mimicked the legitimate \texttt{com.github.scribejava}. Although the package content appears identical at first glance, it contains code that injects a network call to steal user credentials. Similarly, our system detected malware on a Golang package, where the username \texttt{boltdb-go} attempted to impersonate \texttt{boltdb}. 
\tool also flagged suspicious Hugging Face packages, such as \texttt{\seqsplit{TheBlock/Mistral-7B-Instruct-v0.2-GGUF}}, mimicking benign package, \texttt{\seqsplit{TheBloke/Mistral-7B-Instruct-v0.2-GGUF}}, which has 92K monthly downloads. 
We were also able to capture \textit{domain conufsions}, such ass Go packages from \texttt{github.phpd.cn} and \texttt{github.hscsec.cn} which look like proxies of official Golang packages but contain malware that either sets up a rogue MySQL server to steal sensitive files or intercepts and downgrades secure traffic to redirect data to attacker-controlled endpoints.
Additionally, we uncovered a compound squatting attack on \texttt{@typescript-eslint/eslint-plugin}, where an attacker used a similar namespace and package identifier (\texttt{@typescript\_eslinter/eslint}) to mislead users.
% —a sophisticated threat that previous tools failed to detect.











% In addition to these packages
% \TODO{Golang proxy/mirrors}

% \TODO{Maven relocations}


% These results highlight the diverse nature of typosquatting threats and the effectiveness of our system in identifying and categorizing them for further mitigation efforts.
% \TODO{69} were harmless forks/extensions, and \TODO{105} remained under investigation. Several AI-based impersonations on Hugging Face illustrate how attackers could exploit well-known model authors (\eg \texttt{meta-llama}).

\iffalse
\begin{table}[h]
\centering
\small
\caption{Package Confusions uncovered in randomly selected 10000 packages from NPM, PyPI, Rubygem, and all packages from Maven, Golang, and Hugging Face.\JD{Remove this for now. Appendix for arXiv. We cannot include it until the ConfuDB reflects all the true positives (etc.) from this table, otherwise it seems we have multiple datasets which is a problem.}
\WJ{Here we can talk about some details in Maven, Golang, and HF. If we have space... }
}
\label{tab:EQ4_novelty}
\begin{tabular}{lcc}
\toprule
\textbf{Registry} & \textbf{Suspicious}& \textbf{Total Scanned} \\
\midrule
NPM         & 229 & 10,000 \\
PyPI        & 301  & 10,000 \\
RubyGem       & 450 & 10,000 \\
\midrule
Maven       & 2,188  & 632,998 \\
Golang       & 2,047  & 1,910,648 \\
Hugging Face & 26  & 815,534 \\
\bottomrule
\end{tabular}
\end{table}
\fi


\subsection{EQ5: System Accuracy and Efficiency}

\ifNOBOXES
\begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
\textbf{EQ5 Summary:}
Our system exhibits slower processing speeds due to additional sorting and LLM verification steps but maintains acceptable efficiency and strong scalability for deployment on SPRs.
\end{tcolorbox}
\fi


\paragraph{Method}
% 12\JD{This EQ should be about accuracy and efficiency. Not efficiency and scalability. Use what are (currently) Table 6 and Table 9 (the two big double-column ones)}
We evaluated the end-to-end system accuracy and efficiency  (\ie latency and throughput) by running \tool and \texttt{Typomind} on  \texttt{NeupaneDB}
and \texttt{ConfuDB}.
% The latency metrics include similarity comparison, neighbor search, false-positive (FP) verification, and overall system latency. Throughput was calculated as the number of packages processed per second.

\paragraph{Result}

\cref{tab:EQ5_Accuracy} presents the accuracy measurements.
Across the datasets, \tool generally exhibits stronger performance in detecting active threats, achieving higher recall than \texttt{Typomind} while both maintain perfect precision. In the case of stealthy threats, the results are mixed: \texttt{Typomind} excels in the \texttt{NeupaneDB} dataset with a perfect recall, yet its performance declines in \texttt{ConfuDB} where \tool outperforms it. For benign cases, the accuracy differs notably, as \texttt{Typomind} misclassifies benign packages in \texttt{NeupaneDB}, whereas it performs better on \texttt{ConfuDB}, though still lower than \tool's performance in that category. Overall, \tool demonstrates strengths in detecting active and stealthy threats across datasets, reducing the FPR from 77\% (\cref{sec:ProblemState-CaseStudy2-FP}) to 13\% (72/543).
% while \texttt{Typomind}’s performance appears more variable and sensitive to the dataset characteristics.





\begin{table}[h]
\centering
\caption{System latency and throughput measurements. 
\tool has higher latency due to the additional benignity filter.
Our industry partner considers this performance overhead acceptable.}
\label{tab:eval-system-efficiency}
\resizebox{0.7\linewidth}{!}
{
\begin{tabular}{lll}
\toprule
           & \tool  & \texttt{Typomind} \\
\midrule
Latency (ms/pkg)    & 6816 & 120      \\
Throughput (\#pkgs/s) & 0.08  & 0.26    \\         \bottomrule            
\end{tabular}
}
\end{table}


\cref{tab:eval-system-efficiency} indicates that while \tool incurs significantly higher latency (6816 ms per package vs. 120 ms for Typomind) and lower throughput (0.08 vs. 0.26 packages/s), these trade-offs are acceptable in production because the additional latency—stemming from benignity filtering and reliance on LLM-based metadata verification—substantially enhances detection accuracy by reducing false positives. Despite slower processing due to dependencies on the OpenAI API and o3-mini, the system scales effectively across large registries, and future work may optimize inference times without compromising accuracy.
Notably, \tool significantly reduces the workload on analysts, resulting in notable cost savings.









\iffalse
\cref{tab:eval-system-efficiency} summarizes the performance metrics of our system across various registries, demonstrating acceptable latency with variations based on registry size and complexity. For example, NPM shows a latency of 14.17 seconds per package, while PyPI achieves a faster latency of 7.22 seconds. RubyGems, with its larger package set and more complex naming structures, has a higher latency of 46.94 seconds per package. Our system also effectively supports previously unaddressed registries like Maven, Golang, and Hugging Face, which exhibit latencies of 5.98, 5.04, and 16.05 seconds per package, respectively. Importantly, it achieves substantial speed improvements in the neighbor search step, outperforming prior work by 73\%--91\% across all registries.




Although our system prioritizes accuracy over latency, it maintains performance levels suitable for production deployment. The relatively low throughput is primarily due to reliance on the OpenAI API and o3-mini for metadata verification, which introduces significant inference times (\eg NPM and PyPI require 11.3 and 6.81 seconds per package, respectively, for false-positive verification). However, the inclusion of LLMs greatly enhances detection accuracy, making this trade-off worthwhile. The additional latency from sorting and LLM verification is critical to reducing false positives and ensuring high detection accuracy. Furthermore, the system scales effectively, efficiently managing large registries like NPM and PyPI. Future work could explore optimizing LLM inference or incorporating smaller, more efficient models to boost throughput without compromising accuracy.
\fi








\iffalse
\begin{table*}[ht]
    \centering
    \caption{System Latency and Throughput Metrics Across Registries: This comparison includes similarity latency, overall neighbor search latency, false-positive verification latency, and system latency. All metrics are evaluated using identical inputs, ensuring fair comparability across systems. Each latency value represents the time taken to compare a given unpopular package with all popular packages in the registry. Prior work does not support Maven, Golang, and Hugging Face because their names are too long to process~\cite{neupane2023beyondTyposquatting}.
    % The FP verification for Maven, Golang, and Hugging Face was manually triggered (no suspicious neighbors identified).
    \JD{Copy this table to the appendix for arXiv. For main body, present only system latency as seconds/pkg over the ecosystem, for each of typomind, confuguard, and ossgadget. Then simplify the caption (a lot).}
    }
    % For NPM, PyPI, and RubyGems, we use the same popularity threshold (15,000 weekly downloads) as \cite{neupane2023beyondTyposquatting} to be directly comparable to prior work.}
    % \footnotesize
    \label{tab:registry_performance}
    \resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{lc|cc|cc}
        \toprule
        \textbf{Registry} & \textbf{\# Pop. Pkgs.} & \textbf{\textit{Typomind} Neighbor Search Latency (s/pkg)} & \textbf{\tool Neighbor Search Latency (s/pkg)} & \textbf{FP Verification Latency (s/pkg)} & \textbf{System Latency (s/pkg)} \\
        \midrule
        NPM        & \NumAllPopNPMPkgs       & 10.83 & 2.87  & 11.3 & 14.17  \\
        PyPI       & \NumAllPopPyPIPkgs      & 4.45  & 0.41  & 6.67 & 8.94 \\
        RubyGems   & \NumAllPopRubyPkgs      & 34.62  & 1.37  & 15.73 & 23.98  \\
        \midrule
        Maven      & \NumAllPopMavenPkgs     & N/A & 5.98  & 8.08 & 14.06  \\
        Golang     & \NumAllPopGolangPkgs    & N/A & 5.04  & 12.04 & 17.08  \\
        Hugging Face         & \NumAllPopHFPkgs        & N/A & 1.23 & 14.82 & 16.05  \\

        \bottomrule
    \end{tabular}
    }
\end{table*}

\fi


% \subsection{EQ6: Coverage in Hierarchical Ecosystems}
% \WJ{Should we move this to EQ4 or simply cut this?}
% We compare single-embedding vs.\ multi-embedding approaches for Maven, Golang, and Hugging Face. Single-embedding treats package names as one string, while multi-embedding splits them into components (\eg \texttt{groupId:artifactId}, \texttt{author/model}).


% \subsection{EQ6: Comparison to SOTA Methods}
% \begin{tcolorbox} [width=\linewidth, colback=yellow!30!white, top=1pt, bottom=1pt, left=2pt, right=2pt]
% \textbf{EQ6 Summary:}
% Compared to SOTA methods, our system is good at capturing additional typosquatting categories, achieves a substantially lower false-positive rate, and maintains acceptable latency, making it well-suited for deployment on SPR backends while remaining effective for frontend on-demand requests.
% \end{tcolorbox}

\iffalse
\WJ{Below are the old evaluation results}
\subsection{Methodology}
\label{sec:Eval-Methodology}

\paragraph{EQ1: Embedding Model Effectiveness}


\paragraph{EQ2: Efficienct and Scalability}
We evaluate throughput (\texttt{packages/min}) and average query latency under two usage patterns:
\begin{itemize}[leftmargin=*]
    \item \textbf{Batch Scanning}: Processing a large snapshot (\TODO{N=500k} packages).
    \item \textbf{Real-Time Checks}: On-demand scanning for newly published packages in a mock CI pipeline.
\end{itemize}
We measure performance with/without embedding quantization, and with different indexing approaches (IVFFlat vs.\ brute force).



\subsection{Results and Analysis}
\label{sec:Eval-Results}

We now present empirical findings that address each research question.
\paragraph{EQ1: Embedding Model Effectiveness}



\paragraph{EQ2: Efficienct and Scalability}
Table~\ref{tab:EQ2_perf} shows placeholder results:




\paragraph{EQ3: }


\paragraph{EQ3: Metadata Verification Accuracy}
\cref{tab:EQ3_metadataFPR} details the reduction in FPR:

\begin{table}[h]
\centering
\footnotesize
\caption{Impact of metadata-based rules on false positives (EQ3).}
\label{tab:EQ3_metadataFPR}
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{FPR (Before)} & \textbf{FPR (After)} \\
\midrule
Typomind       & \TODO{12.1\%} & \TODO{3.4\%} \\
\tool & \TODO{7.6\%}  & \TODO{1.2\%} \\
\bottomrule
\end{tabular}
\end{table}

We observe a \TODO{80\%} decrease in false positives, especially from rules like \emph{fork identification} (Rule~3) and \emph{overlapping maintainers} (Rule~8), confirming that many suspiciously named packages are in fact benign packages.



\paragraph{EQ4: Discovery of New Typosquats}





\paragraph{EQ5: Coverage in Hierarchical Ecosystems}
Table~\ref{tab:EQ5_hierarchical} shows that hierarchical embeddings catch \TODO{x\%} more attacks involving \emph{impersionation squatting} or \emph{compound squatting}:

\begin{table}[h]
\centering
\footnotesize
\caption{Typosquats identified with single vs.\ hierarchical embeddings (EQ5).}
\label{tab:EQ5_hierarchical}
\begin{tabular}{lcc}
\toprule
\textbf{Ecosystem} & \textbf{Single-Embedding} & \textbf{Hierarchical Embedding}\\
\midrule
Maven         & \TODO{xx} & \TODO{xx} \\
Golang        & \TODO{xx} & \TODO{xx} \\
Hugging Face  & \TODO{xx} & \TODO{xx} \\
\bottomrule
\end{tabular}
\end{table}

Examples include \texttt{facebook-llama} vs.\ \texttt{meta-llama}. Single-level name checks fail to detect this subtle impersonation, but multi-level embeddings spot the near-duplicate \texttt{author} dimension.



\subsection{Summary of Findings and Relation to System Guarantees}

Our empirical results reinforce the efficacy of each design choice. Table~\ref{tab:requirements_fulfillment} summarizes how each of our production requirements (\S\ref{sec: ProblemState-Prod}) and security guarantees (\S\ref{sec: ProblemState-Security}) is met:

% \begin{table}[h]
% \centering
% \footnotesize
% \caption{How we fulfill production requirements and security guarantees. \TODO{Update this table with the latest requirement changes)}}
% \label{tab:requirements_fulfillment}
% \begin{tabular}{p{0.32\columnwidth} p{0.55\columnwidth}}
% \toprule
% \textbf{Req./Guarantee} & \textbf{Implementation/Outcome} \\
% \midrule
% \textbf{Req$_{1}$: Frequent Metadata} & Step~1 ensures daily ingestion; catch newly released packages \\
% \textbf{Req$_{2}$: Manageable FP}    & Step~5 reduces false positives to \TODO{<x\%} in multiple datasets \\
% \textbf{Req$_{3}$: Cost-Effectiveness} & ANN + quantization achieve \TODO{xk pkgs/min} and short CI-latency \\
% \midrule
% \textbf{G$_{1}$: High Recall} & Intent-based name checks detect potential future malware \\
% \textbf{G$_{2}$: Low FPR}     & Nine metadata rules filter forks/brand expansions \\
% \textbf{G$_{3}$: Timely Detection} & Parallel scanning finishes 1M pkgs in \TODO{x hours} \\
% \textbf{G$_{4}$: Ecosystem Compatibility} & Hierarchical approach for Maven/Hugging Face etc. \\
% \bottomrule
% \end{tabular}
% \end{table}

Overall, these experiments confirm that our approach:
\begin{itemize}[leftmargin=*]
    \item \textbf{Captures malicious intent} by embedding domain-specific patterns (\textbf{EQ1}).
    \item \textbf{Operates at scale} via quantization and ANN indexing (\textbf{EQ2}), scanning large registries cost-effectively (\textbf{Req$_{3}$}).
    \item \textbf{Minimizes false positives} (\textbf{EQ3}) by verifying suspicious names against real-world metadata heuristics—achieving an acceptably low alert volume for practitioners (\textbf{Req$_{2}$}).
    \item \textbf{Extends coverage} to hierarchical ecosystems (\textbf{EQ5}), catching author squatting that simpler methods miss.
\end{itemize}

Hence, we conclude that our end-to-end pipeline meets the demands of large-scale software supply chain security (\cref{sec: ProblemState-Prod}), bridging both academic detection criteria and real-world deployability.
\fi
\iffalse

\section{Evaluation}
We evaluate \tool design decisions and compare with existing work.
\cref{sec:Eval-exp-setup} describes our experimental setup, including the dataset preparation

\subsection{Experimental Setup}
\label{sec:Eval-exp-setup}
\subsection{Datasets}
\TODO{@Berk: Dataset structure: typosquat pkg, target pkg/legitimate pkg, registry, TP/FP/TN, typosquat category, source}
\WJ{Not sure if we need separate subsections}
%-------------------------------------------------------------------------------
\textbf{Datasets:}
\begin{itemize}
    \item The ground-truth dataset collected in typomind + latest blog posts including HF typosquats
    \item Ground-truth labeled in typomind new data
    \item Socket human reviewed typosquats
    \item Package name hallucination from LLM~\cite{spracklen2024LLMPackageHallucinations}
\end{itemize}

\subsection{State-Of-The-Art Baselines}

\textbf{Baselines:}
\TODO{@Berk: (1) Set up the environments for typomind and typobound (2) evaluation pipeline infrastructure
% (input: tool (--input=typomind) --registry=NPM --suspicious_pkg=loadsh)
(3) connect the pipeline to our typosquat system using REST API.}

\begin{itemize}
    \item Typomind
    \item Typobound
    \item Socket typosquat detection system v1
\end{itemize}

\subsection{Research Questions}
In our evaluation, we ask the following questions:

\TODO{@Berk: Go to check Synk, Reverselab, etc. and add the latest data to our dataset.}
\begin{itemize}
    \item \textbf{RQ1:} How does the embedding model perform? \WJ{TODO: Embedding similarity measure + embedding visualization}
    \item \textbf{RQ2:} What is the efficiency of embedding-based approach?
    \item \textbf{RQ3:} To what extent the metadata validation can help improve the accuracy? \WJ{We will need some ablation study on this}
    \item \textbf{RQ4:} Can the system find new typosquats? \WJ{Production data + full scanning (sampling)}

    all packages = popular pkgs (50K) + unpop pkgs
    500K -> sample 16K (less than 1day)
    \item \textbf{RQ5:} Can we detect typosquats on Maven, Golang?
    \item \textbf{RQ6:} Are there typosquats on Hugging Face?

\end{itemize}

\subsection{Results and Analysis}
%-------------------------------------------------------------------------------
\subsubsection{Empirical Analysis}
\label{sec:results-empiricalAnalysis}
\begin{itemize}
    \item \textbf{RQ1:} What are the malicious \textit{contents} in the packages?
    \item \textbf{RQ2:} What are the \textit{intents} in the suspicious packages?
    \item \textbf{RQ3:} xxxx
\end{itemize}

\paragraph{Golang}
We analyzed the distribution of namespace, author names, and pacakge names.
Init function during package installation.

\paragraph{Maven}
We analyzed the distribution of group\_id and package\_id.

For some Maven packages, the attacker can use relocated package to do typosquat attack. Example:
\texttt{io.github.spring-cattle:cattle-commons} -> \texttt{io.gitlab.spring-cattle:cattle-commons}


\subsection{RQ1: Embedding Model}
\TODO{Compare to Table 3 in typomind paper}
Table 3: Performance of detection rules

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/embedding_comparison_distances.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/embedding_comparison_scores.pdf}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}
\fi
\section{Measurement Study}
\fi

\section{Discussion}
% \WJ{@Jamie Please take a look at this section as well.}
% \subsection{Responsible Disclosure}
% We promptly notified the respective package ecosystems of all malicious packages identified in \cref{xx} that remained publicly accessible at the time of discovery. Specifically, we reported \TODO{xx} malicious packages to NPM, \TODO{yy} to PyPI, ..., and \TODO{zz} to Hugging Face. Each ecosystem has since taken disciplinary measures against these packages, and as a result, they have been removed from public access.

\subsection{Three Lessons Learned from Production}
% \TODO{Mik: yeah, I'm wondering about the ontology.  maybe we can add more cases or argue that the number of types could be smaller....}
% \WJ{@Mik, it would be great if you could provide any insights for the next two subsections.}

% \JD{\textbf{Lesson 1: TOOLNAME prevents real typosquats.}}
\paragraph{Lesson 1: \tool prevents real confusion attacks}
Over the past two months, we deployed the system in our industry partner's production environment, during which we identified and confirmed \NumProdReviewedTPTypos threats, with \NumProdUnreviewedTypos additional threats under review.
% \JD{The number here is 2000 but the abstract says 1200.}
%For the typosquatting threats we identified on NPM, PyPI, and Golang, we ran a commercial malware scanner on those packages. This results in the removal of \NumRemovedPkgs packages within one month.

\iffalse
Additionally, our system successfully identified 12 suspicious packages on Maven, 4 packages on Golang, and \TODO{xx} packages on Hugging Face during deployment. However, after closer analyzing these packages, we found that the Maven packages are all false-positives which has missing package relocation metadata. Within Maven Central, our system returns 9 alarms while after manual analysis, those are the packages with relocation information while the relocation information is not present in the metadata in our database.
\JD{Were they taken down...? Say more.}
\fi

% \JD{\textbf{Lesson 2: False Positives Harm Our Customers.}}
\paragraph{Lesson 2: False Positives Harm Our Customers.}
% \JD{Write this instead as ``table 3 represents the most recent...'', but I think you should tell more stories about false positive behaviors so the reader sees your system is based on experience}
% \JD{This doesn't include any actual story or lesson about customer impact. If we do not have those, we should cut it. But you told me a story about customer complaining about you flagging their whole thing as malicious, do you have other stories? Don't want to deanonymize but you can be vague.}
Table 3 represents the most recent metadata verification rules. In deployment, we found additional cases where the package has a very similar name and the README of their package was missing which made our system classify it as suspicious stealth attack.
That package was owned by one of our partner's customers and served as a ``transitive package''. %and therefore the package owner did not write a README.
The customer felt that being flagged harmed their reputation. %In such case, from a registry perspective, a false-positive will harm the reputation of the customer in the community if the package is flagged as suspicious.
We added an allow-list tied to our partner's customers. % avoid this case, we added an allowed list so that if the package is from our customers, then it should be considered as legitimate.
% For example, for R9, there are many relocated packages that exist both on Golang and Maven.
% Continuous enhancements to the system have been made through meticulous manual data analysis. For instance, we have incorporated additional rules to further minimize false positives, as detailed in \cref{tab:metadata_rules}.

% \JD{\textbf{Lesson 1: Ontology Matters.}}
\paragraph{Lesson 3: Ontology Matters.}
\tool is deployed as part of a security analysis pipeline, with human analysts to triage the results (\cref{sec:SystemDesign-Step6}).
Analysts need to be able to interpret the results of \tool.
%Despite these improvements, we have identified ontological limitations in our current approach to categorizing typosquatting threats.
The analysts feel that the package confusion categories outlined in \cref{tab:typosquat_taxonomy} do not provide sufficient explanatory power.
We propose to refine the confusion taxonomy to consider the malicious content and intent behind each package, including a risk-level classification.
%This would allow a more nuanced assessment of each threat.
%This enhanced taxonomy can be developed by leveraging the metrics discussed in \cref{sec:SystemDesign-Step5}, allowing for a more nuanced assessment of each threat's severity and nature.

\iffalse
\paragraph{Lesson 4: We need more sophisticated analysis to discern intent.}
\WJ{I believe somewhere we should talk about this finding "detection of typosquatting attacks is not a simple security problem about detecting typos and malware, but need more about intent hidden inside the package, combined with consideration of common engineering practices"}
To bolster a more straightforward blocking policy, there is still a need to integrate additional tools and algorithms. One such tool is a differential scanner, which compares packages to identify discrepancies that may indicate malicious alterations. Additionally, implementing a grey-list system will enable us to place suspicious packages with low-risk levels into a monitored category. These grey-listed packages will undergo continuous surveillance of their behavior and content, ensuring that any emerging threats are promptly detected and addressed. By adopting these strategies, we aim to enhance the robustness and responsiveness of our typosquatting defense mechanisms.
\fi





\subsection{Limitations and Security Analysis}
% \subsection{Security Analysis}
\label{sec:Discussion-LimitatinoAndSecurityAnalysis}

This section discusses our system limitations and how attackers might bypass \tool.
% \subsection{Limitations and Security Analysis}
\label{sec:SystemDesign-Limitation}

\paragraph{Gaming Metrics}
% \JD{I added this, seemed relevant?}
Our system relies on software metrics to gauge the likelihood that a package is a confusion attack.
These metrics might be gamed.
There has been little formal study of the feasibility of gaming these metrics, but recent work suggests both the possibility and some real-world examples~\cite{he2024FakeStars}.

% \WJ{Moved from \$5.7 here.}
% \JD{Distinguishing the primary limitation from your design and the secondary limitation from your implementation.}

% \TODO{Update examples here}
 % \JD{It would be helpful for analyzing the security properties you achieve. You would typically do that analysis as part of \$5 or \$6, with subsection titled  ``Security Analysis'' that gives a conceptual/theoretical discussion of how much of the attack surface your system is addressing and how easy it might be to bypass.}
\paragraph{Limitations in Neighbor Search}
\WJ{This phenomenon is known as the “curse of dimensionality” and it can affect the performance of vector semantic similarity search in certain scenarios}
One significant limitation of \tool is its inability to handle short names or acronyms.
\textit{FastText} struggles with short words (\eg \texttt{xml} vs. \texttt{yml} have a similarity score of 0.7).
The model’s reliance on character \textit{n}-grams often fails to capture subtle similarities effectively in such cases, providing an avenue for attackers to exploit short package names.
%Although FastText emphasizes subword representations to improve embeddings, this focus reduces its ability to account for visual ambiguity or phonetic similarity (\eg \texttt{google} and \texttt{g00gle} appear less similar in the embedding space).
To mitigate this, we implemented a list of potential substitutions to identify cases of visual or phonetic ambiguity, and we
% However, this approach introduces additional computational overhead, slowing down the system.
combine embedding similarity with Levenshtein distance.
This hybrid approach improves neighbor search for short names, but increases the computational cost does not fully resolve the concern.
%Recent research demonstrates the potential of AI tools to generate sound-based squatting attacks~\cite{valentim2024xsquatter, valentim2022soundsquatting}, further exacerbating threats to the software supply chain.


% Efficient and accurate false-positive verification is critical for maintaining system reliability. Currently, we rely on OpenAI's commercial LLM API, specifically \texttt{GPT-4o}. While effective, transitioning to a local LLM or a model fine-tuned specifically for package typosquatting could significantly reduce latency and enhance the efficiency of FP verification.

\paragraph{Bypassing Metadata Verification}
Using an LLM for benign filtering introduces correctness risks (hallucination, jailbreaking~\cite{yi2024jailbreak}).
Adversaries might exploit this by tailoring their package name or metadata to persuade the LLM that the package is trustworthy using techniques like prompt injection or model hijacking~\cite{liu2023promptInjectionAttack, zou2023universalandTransferableAdvAttackonLM}. % to manipulate the model's responses. For example, attackers might insert model metadata (\eg README) that makes an LLM consider a malicious package legitimate. %, such as misrepresenting a malicious username as belonging to a trusted maintainer or aligning fake functionality with that of a legitimate package. Since the LLM is integral to verifying maintainers and functionality, such attacks could compromise the system’s defenses and allow malicious packages to bypass detection.
% \Andreas{Can an attacker who is aware of the defenses provided by your system design a typosquatting package to circumvent them? If so, what would that look like? If not, why not?}


\subsection{Future Directions}
\label{sec:Discussion-FutureWork}
\WJ{TODO: Talk about the limitation on Hugging Face models and cite our naming paper. Then in the discussion, talk about the proposed approach.}



\paragraph{Enhancing Representations for Package Confusion Detection:}
\label{sec:Discussion-FutureWork-Representation}
Improving the representation of package names is crucial for more robust detection. While FastText captures semantic similarity through subword embeddings, it struggles with typographical variations, particularly for short words or acronyms. Fine-tuning FastText or training a more efficient model on domain-specific corpora including both correct and misspelled terms can address these limitations.
Augmenting training data with synthetic typos and incorporating typo normalization or correction techniques before embedding generation can significantly reduce errors. Advanced models, such as transformer-based architectures fine-tuned with contrastive learning on typo-specific datasets, present a promising alternative for enhancing detection accuracy and reliability.
This approach has proven effective in combating domain typosquatting, but no research has been conducted targeting package typosquatting~\cite{koide2023phishreplicant}. One challenge is the limited availability of data for verifying package squatting cases.
LLMs might help here~\cite{tozzi2024packageHallucinatinoThreat}.




\paragraph{Mitigating LLM Hallucination in Code Generation}
The increasing use of LLMs for code generation has introduced new challenges, as these models often hallucinate package names or generate commands for nonexistent or maliciously similar packages~\cite{spracklen2024LLMPackageHallucinations}. These hallucinations pose serious threats to the security of the software supply chain~\cite{vulcan2023CanyouTrustChatGPTPKGRecommendation, tozzi2024packageHallucinatinoThreat}. Addressing this issue requires implementing typo or hallucination correction mechanisms in LLM-based package recommenders. Verifying package legitimacy, detecting typos, and integrating contextual checks can prevent the propagation of incorrect package names, reducing the risks associated with hallucinations.

\paragraph{Meta-Learning for Malicious Package Detection}
Meta-learning approaches offer significant potential for improving malicious package detection. By leveraging anomaly detection techniques and metadata analysis~\cite{halder2024maliciousPackageDetectionUsingMetadataInfo}, systems can dynamically adapt to evolving attack strategies. Meta-learning frameworks could analyze patterns across registries and rapidly identify emerging threats, enhancing the scalability and robustness of detection systems. Integrating such frameworks will be key to staying ahead of increasingly sophisticated attackers.

\section{Conclusion}

We present \tool, an embedding-based package confusion detection system.
Based on real-world attack patterns, we refined the package confusion definition and developed a taxonomy based on engineering practices.
\tool is being used in production at our industry partner and contributed to \NumReviewedThreats confirmed package confusion threats in three months.
Compared to SOTA methods, our system is good at capturing additional confusion categories, achieves a substantially lower false-positive rate, and maintains acceptable latency, making it well-suited for deployment on SPR backends while remaining effective for frontend on-demand requests.
We shared our insights from production experience, customer feedback, the need for an improved ontology, and outlined future directions.

\section*{Data Availability and Research Ethics}
\label{sec:OpenScience}
Our artifact is available at: 
\url{https://github.com/confuguard/confuguard}.
%The artifacts associated with this project are:
%  the results from our study of prior confusion attacks (\cref{sec:EmpiricalAnalysis});
%  the implementation of \tool 
%   (\cref{sec:SystemDesign});
%  % (3) the model and embedding database we created (\cref{sec:SystemDesign-Step2});
%  % (4) the other prototypes of our system (\cref{sec:SystemDesign-Step3}--\cref{sec:SystemDesign-Step6});
%  and
%  our evaluation scripts and datasets (\cref{sec:Eval}).
  For replicability, we include all results and the code of the system, except the commercial metadata database (\cref{sec:SystemDesign-Step1-Metadata}). % created by our industry partner .
%  A comparable database can be made via registry APIs.

Our work poses limited ethical concerns.
%In our empirical study and system design,
We analyzed public packages 
and disclosed suspicious ones to our industry partner.
%, whose analysts shared concerns with registry operators.
%Like similar defenses, \tool protects engineers and end-users from bad actors. %the intent is to harm attackers.

\iffalse
%a favorable risk-benefit tradeoff
  
This section discusses the ethical implications of designing, implementing, and deploying a system to detect typosquatting attacks across multiple software registries.
We also outline the risks and benefits associated with sharing detailed detection methods and results.

We attest that:
\begin{itemize}[itemsep=0.1ex]
    \item Our data collection and handling was consistent with conference guidelines and policies.
    \item Our study uses only publicly available metadata in SPRs, eliminating privacy risks and not requiring IRB approval.
    %\item We will share our methods and, where possible, sanitized datasets under open licenses to enable replication while protecting user security.
    \item We promptly disclosed all confirmed malicious/suspicious packages to the registry maintainers.
    \item To the best of our ability, the research team has evaluated all ethical implications, ensuring that all activities are conducted responsibly and that future actions will continue to uphold these standards.
    \end{itemize}
\fi



% \JD{Please begin with an enumeration of all artifacts. ``The artifacts associated with this project are: X Y Z.'' (Then the bullet list should cover all the artifacts}

% \begin{itemize}[left=0.1cm]
% \item \textbf{Artifacts Shared.}
% We will publish our case study, system’s prototype implementation (excluding the commercial database) (\cref{sec:SystemDesign-Step2}, \cref{sec:SystemDesign-Step3}--\cref{sec:SystemDesign-Step6}), and an anonymized summary of evaluation results (\cref{sec:Eval}). This includes our case study scripts and data, as well as the typosquatting taxonomy (\cref{sec:ProblemState-CaseStudy1-TP}).
% \item \textbf{Artifacts Withheld.} To avoid enabling new attacks or disrupting ongoing investigations, we will not publicly list any active, unreleased, or unresolved suspicious packages. We likewise will not share raw metadata tied to live threats if doing so could aid adversaries or disclose unresolved vulnerabilities.
% As informed by the authors from \cite{spracklen2024LLMPackageHallucinations}, we will not share the raw dataset from their work but their data is available by request. We will also not share the commercial metadata database (\cref{sec:SystemDesign-Step1-Metadata}).
% \end{itemize}

%\noindent By diligently balancing transparency and caution, we aim to advance the scientific understanding of typosquatting while protecting the broader software community from emerging threats.

% An artifact will be made available following the USENIX process and once it completes the review process from our industry partner.
%Resources will be made available in a public repository at this anonymous GitHub repository: \url{https://github.com/xxx}\TODO{Update the URL}.

% \JD{Must finish the conclusion.}

% \JD{Review the references for nits. For example, the first few are missing URLs; [17] does not have a venue; it would be nice to write the acronyms of venues for the lazy reviewer (CCS, USENIX, ICSE, WWW, etc.)}

% \section*{Data availability}
% \label{sec: artifact}
%

\pagebreak
% \JD{Here is copy-paste from Kelechi, please revise}

\iffalse
\textit{The call for papers states that an extra page is allotted to discuss ethics considerations and compliance with the open science policy. This page contains that content.}
% \WJ{TODO: This needs to be shorten to one page.}
\section{Research Ethics} 
\label{sec:ethics}

This section discusses the ethical implications of designing, implementing, and deploying a system to detect typosquatting attacks across multiple software registries.
We also outline the risks and benefits associated with sharing detailed detection methods and results.

We attest that:
\begin{itemize}[itemsep=0.1ex]
    \item Our data collection and handling was consistent with conference guidelines and policies.
    \item Our study uses only publicly available metadata in SPRs, eliminating privacy risks and not requiring IRB approval.
    %\item We will share our methods and, where possible, sanitized datasets under open licenses to enable replication while protecting user security.
    \item We promptly disclosed all confirmed malicious/suspicious packages to the registry maintainers.
    \item To the best of our ability, the research team has evaluated all ethical implications, ensuring that all activities are conducted responsibly and that future actions will continue to uphold these standards.
    \end{itemize}

% Our typosquat detection system analyzes package repositories across six major open-source ecosystems: NPM, PyPI, RubyGems, Maven, Golang, and Hugging Face.

\subsection{Data Collection and Scope}
We obtain comprehensive package metadata --- including package names, versions, download statistics, maintainers, and other relevant attributes --- from a private database maintained by our industry partner, a software supply chain security company.
This method ensures that data collection is both systematic and compliant with legal and ethical standards. % as the database aggregates publicly available information without the need for additional scraping or direct interaction with repository maintainers.

These data sources do not include sensitive personal information (SPI) about package maintainers or users.
By relying solely on metadata necessary for detecting suspicious naming patterns and assessing package-level attributes, we minimize privacy concerns. % and focus exclusively on information that is already publicly accessible.

% \JD{Move this block to the opening remarks of \$9 because it's a summary of the whole section.}


\subsection{Responsible Disclosure}
Throughout our study, we promptly reported newly discovered malicious or suspicious packages to the respective registry maintainers.
We delayed publication of any technical indicators that could facilitate ongoing attacks until the registries took corrective actions (\eg removal of malicious packages).
This procedure protects users from live threats while still allowing the broader community to learn from our findings and detection strategies.

\subsection{Risks and Benefits}
% \WJ{Need improvement on this subsection or we can cut it.}
% \JD{LGTM actually, please keep it as-is}
In our judgment, the potential benefits of providing actionable insights and better protective measures against typosquatting outweigh the risks. %, especially when combined with careful disclosure practices.
%We discuss the risks and benefits of our decision in detail below.

We perceive two main risks:

\begin{itemize} \item \textbf{Highlighting Attack Vectors.} By publicizing the detection pipeline, we inform potential attackers about its limitations or thresholds, potentially encouraging them to find new ways to evade detection. \item \textbf{Unintentional Disclosure of Live Threats.} Publishing lists of suspected malicious packages could draw attention to attacks still under investigation, particularly if registry maintainers have not yet removed them. Sharing such data prematurely could increase user exposure to live threats. \end{itemize}

We perceive two main benefits:

\begin{itemize} \item \textbf{Improved Ecosystem Security.} Disclosing our detection approach and findings helps registries, maintainers, and security practitioners find malicious packages faster, reducing exposure to malware.
\item \textbf{Shared Knowledge for the Community.} By openly describing our methods, we enable other researchers and industry professionals to replicate and extend our techniques, contributing to broader, collaborative improvements in supply chain security. \end{itemize}

\section{Compliance with Open Science Policy}
\label{sec:OpenScience}

\WJ{@Mik, please double check the sharable artifact content below.}
We acknowledge the USENIX Security open science policy, which encourages authors to share data and artifacts.
The artifacts associated with this project are:
  (1) the case study we conducted on analyzing prior typosquatting data (\cref{sec:ProblemState-CaseStudy1-TP}, \cref{sec:ProblemState-CaseStudy2-FP});
  (2) the commercial metadata database (\cref{sec:SystemDesign-Step1-Metadata});
  (3) the embedding database we created (\cref{sec:SystemDesign-Step2});
  (4) the other prototypes of our system (\cref{sec:SystemDesign-Step3}--\cref{sec:SystemDesign-Step6});
  and
  (5) our evaluation scripts and datasets (\cref{sec:Eval}).

% \JD{Please begin with an enumeration of all artifacts. ``The artifacts associated with this project are: X Y Z.'' (Then the bullet list should cover all the artifacts}

\begin{itemize}[left=0.1cm]
\item \textbf{Artifacts Shared.}
We will publish our case study, system’s prototype implementation (excluding the commercial database) (\cref{sec:SystemDesign-Step2}, \cref{sec:SystemDesign-Step3}--\cref{sec:SystemDesign-Step6}), and an anonymized summary of evaluation results (\cref{sec:Eval}). This includes our case study scripts and data, as well as the typosquatting taxonomy (\cref{sec:ProblemState-CaseStudy1-TP}).
\item \textbf{Artifacts Withheld.} To avoid enabling new attacks or disrupting ongoing investigations, we will not publicly list any active, unreleased, or unresolved suspicious packages. We likewise will not share raw metadata tied to live threats if doing so could aid adversaries or disclose unresolved vulnerabilities.
As informed by the authors from \cite{spracklen2024LLMPackageHallucinations}, we will not share the raw dataset from their work but their data is available by request. We will also not share the commercial metadata database (\cref{sec:SystemDesign-Step1-Metadata}).
\end{itemize}

%\noindent By diligently balancing transparency and caution, we aim to advance the scientific understanding of typosquatting while protecting the broader software community from emerging threats.

An artifact will be made available following the USENIX process and once it completes the review process from our industry partner.
%Resources will be made available in a public repository at this anonymous GitHub repository: \url{https://github.com/xxx}\TODO{Update the URL}.
\fi

\iffalse
\section{Compliance with Open Science Policy} \label{sec:OpenScience}


We acknowledge that USENIX Security has an open science policy: that authors are expected to openly share their research artifacts by default.
The research artifacts associated with this study are:
\begin{itemize}
    \item Raw transcripts of interviews
    \item Anonymized transcripts of interviews
    \item Interview protocol
    \item Codebook
\end{itemize}

\myparagraph{Things we have shared}
The main contribution of this work is the system design and evaluation results.
We share the prototype of our system in the artifact, as well as the data-driven analysis which includes the typosquat taxonomy (\cref{sec: SystemDesign-Step5})
Our full typosquat taxonomy is included in~\cref{sec:appendix-FPTaxonomy}. We also include the disclosed \TODO{xx} suspicious packages from (\cref{sec:Eval-Results}).

\myparagraph{Things we cannot share}
For undisclosed suspicious packages, we were not able to share them. We are also not able to share more details about the company implementation. We are also informed not to share the dataset from \cite{spracklen2024LLMPackageHallucinations}.
\fi


{
\normalsize
% \begin{spacing}{0.5}
\bibliographystyle{plain}
\bibliography{bibliography/Reference, bibliography/WenxinZotero, bibliography/DualityLab}
% \end{spacing}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \WJ{Notes for appendices:
% These appendices may be included to assist reviewers who may have questions that fall outside the stated contribution of the paper on which your work is to be evaluated, or to provide details that would only be of interest to a small minority of readers. The paper should be self-contained without appendices.
% To accommodate additional material requested by reviewers, the revisions for papers that previously received an “Invite for Major Revision” or “Accept on Shepherd Approval” decision can use up to 14 typeset pages for the main body of the paper, excluding the one page for discussing ethics considerations and compliance with the open science policy, the bibliography, and well-marked appendices.
% Once accepted, the final version should be no longer than 20 pages, including the bibliography and any appendices.
% }

\clearpage

\iffalse
\appendix
% \section{LLM Filter for Data Analysis}

% \WJ{This section should include: (1)This filter was developed using \texttt{gpt-4o}, with implementation details provided in Appendix~\ref{Appendix-LLMFilter}. (2) The frequency of each category is summarized in Appendix~\ref{Appendix-LLMFilter}}.

% This appendix presents the designed of our LLM filter and data used in \cref{sec:ProblemState-CaseStudy2-FP}.


\section{Taxonomy of Engineering Practices}
\label{Appendix-taxonomy}

\cref{fig:FP-taxonomy} presents the taxonomy we created in \cref{sec:ProblemState-CaseStudy2-FP}.


\section{More System Implementation Details}

\subsection{Visualization of Embedding}
\label{Appendix-EmbeddingVis}
\cref{fig:embeddingSpace} shows a \texttt{UMAP} visualization of 100K NPM packages~\cite{mcinnes2018umap}.
% starting with the letter ``a''.
Clusters often form around minor spelling variations,
% (\eg \texttt{color-string} vs.\ \texttt{color-name}),
demonstrating how embeddings capture both lexical and semantic relationships. This grouping is central to detecting maliciously similar names that differ by a single character or switched letters.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/NPM_embeddings_visualization_100k.pdf}
    \caption{UMAP visualization of embedding space of 10K NPM packages.
    % \TODO{Update this figure.}\WJ{Likely we will cut this figure since it does not include much useful knowledge. We can put it in the appendix instead.}
    }
    \label{fig:embeddingSpace}
\end{figure}




\subsection{Detailed Embedding Efficiency Measurement}
\label{Appendix-Efficiency}
\cref{tab:EQ1-Efficiency}
highlights the efficiency of embedding models across different quantization formats (\texttt{float32}, \texttt{float16}, and \texttt{int8}), indexing overhead, and memory usage. While \texttt{float16} and \texttt{int8} offer comparable throughput and latency due to similar I/O and storage overheads, \texttt{int8} exhibits slightly higher latency in embedding creation due to additional quantization steps. Across ecosystems, \texttt{float16} and \texttt{int8} achieve significant improvements in processing speed and memory efficiency compared to \texttt{float32}, demonstrating their suitability for scalable embedding-based systems.








\subsection{Implementation of metadata verification rules}
\label{Appendix-MetadataVerification}
Algorithm~\ref{alg:FPRules} shows our implementation of the engineering practice categories. We apply both heuristic rules and LLM-based rules to measure the attributes from \cref{tab:typosquat_taxonomy} and use this algorithm to check if a suspicious pair should be classified as true-positive or false-positive.

\begin{algorithm}[H]
\caption{Heuristic Rules for False-positive Verification}
\begin{algorithmic}[1]

\STATE \textbf{Input:} $\text{typoDoc}, \text{legitDoc}, \text{registry}$
\STATE \textbf{Output:} $\text{Boolean}, \text{metrics}, \text{explanation}$

\STATE \textbf{Initialize:} Set all keys in $\text{metrics}$ to $\text{None}$

\STATE \textbf{for each} key in $\text{metrics}$:
\STATE \quad $\text{metrics[key]} \gets \text{None}$

\STATE Populate $\text{metrics}$ using helper checks:
\STATE \quad $\text{metrics} \gets \_\text{check\_package\_naming\_and\_purpose}(\text{typoDoc}, \text{legitDoc})$
\STATE \quad $\text{metrics["overlapped\_maintainers"]} \gets \_\text{has\_overlapped\_maintainers}(\text{typoDoc}, \text{legitDoc}, \text{registry})$
\STATE \quad $\text{metrics["comprehensive\_metadata"]} \gets \_\text{has\_comprehensive\_metadata}(\text{typoDoc})$
\STATE \quad $\text{metrics["active\_development"]} \gets \_\text{is\_actively\_developed}(\text{typoDoc})$

\STATE \textbf{If} $\neg \text{metrics["is\_adversarial\_name"]}$ \textbf{and} $\neg \text{metrics["is\_suspicious"]}$:
\STATE \quad \textbf{If} $\text{metrics["obvious\_not\_typosquat"]}$ \textbf{or} $\text{metrics["fork\_naming"]}$ \textbf{or} $\text{isTest}$ \textbf{or}
\STATE \quad \quad $\text{metrics["is\_known\_maintainer"]}$ \textbf{or} $\text{metrics["distinct\_purpose"]}$ \textbf{or} $\text{metrics["overlapped\_maintainers"]}$:
\STATE \quad \quad \textbf{return} $(\text{True}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\text{metrics["is\_adversarial\_name"]}$ \textbf{and} $\neg \text{metrics["distinct\_purpose"]}$:
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\neg \text{metrics["comprehensive\_metadata"]}$ \textbf{or} $\neg \text{metrics["distinct\_purpose"]}$ \textbf{and}
\STATE \quad $\neg \text{metrics["is\_adversarial\_name"]}$ \textbf{or} $\neg \text{metrics["active\_development"]}$:
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\text{metrics["is\_adversarial\_name"]}$ \textbf{and} ($\neg \text{metrics["distinct\_purpose"]}$ \textbf{or}
\STATE \quad $\neg \text{metrics["comprehensive\_metadata"]}$):
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE $\text{isDeprecated} \gets \_\text{check\_deprecated}(\text{typoDoc})$
\STATE \textbf{If} $\text{isDeprecated}$ \textbf{and} $\text{metrics["is\_adversarial\_name"]}$:
\STATE \quad \textbf{return} $(\text{True}, \text{metrics}, \text{explanation})$

\STATE \textbf{If} $\text{metrics["is\_suspicious"]}$:
\STATE \quad \textbf{return} $(\text{False}, \text{metrics}, \text{explanation})$

\STATE \textbf{Return:} $(\text{True}, \text{metrics}, \text{explanation})$

\end{algorithmic}
\label{alg:FPRules}
\end{algorithm}

\begin{table*}[h]
\centering
\caption{Evaluation of embedding model efficiency, \textit{HNSW} indexing overhead, and memory usage. The throughput and latency differences between \texttt{float16} and \texttt{int8} are minimal due to similar overall I/O and storage overheads, as well as the efficient handling of embeddings using \texttt{pgvector}. However, some increased latency in \texttt{int8} embedding creation is observed due to additional quantization and processing steps required for integer-based representations.}
% Updated \texttt{float16} and \texttt{int8} quantization formats.}
\resizebox{0.95\textwidth}{!}{%
\scriptsize
\begin{tabular}{l l r r r r r r r}
\toprule
\textbf{Quantization} & \textbf{Ecosystem} & \textbf{Throughput} & \textbf{Avg Batch} & \textbf{Avg Pkg} & \textbf{Total} & \textbf{PG Table} & \textbf{PG DB} & \textbf{Indexing} \\
& & \textbf{(batches/s)} & \textbf{Latency (s)} & \textbf{Latency (\textmu s)} & \textbf{Time (s)} & \textbf{Size (GB)} & \textbf{Size (GB)} & \textbf{Time (s)} \\
\midrule
\multirow{6}{*}{float32}
& Hugging Face & 4.20 & 1062.44 & 2380.68 & 1941.68 & 9.25 & 45.26 & 4.46 \\
& Maven & 4.59 & 697.16 & 2180.61 & 1380.76 & 6.57 & 46.97 & 4.91 \\
& Golang & 2.42 & 3778.33 & 4135.75 & 7905.49 & 21.36 & 54.81 & 7.80 \\
& NPM & 3.73 & 7463.84 & 2678.89 & 13269.07 & 21.04 & 60.64 & 7.42 \\
& PyPI & 9.57 & 312.03 & 1045.52 & 602.95 & 2.62 & 61.49 & 4.39 \\
& RubyGems & 9.54 & 116.86 & 1049.14 & 220.00 & 0.94 & 61.80 & 4.07 \\
\midrule
\multirow{6}{*}{float16}
& Hugging Face & 19.67 & 203.86 & 508.61 & 414.82 & 3.22 & 36.73 & 4.75 \\
& Maven & 20.44 & 156.54 & 489.42 & 309.90 & 2.51 & 34.74 & 4.77 \\
& Golang & 19.07 & 520.25 & 524.55 & 1002.68 & 7.56 & 29.70 & 4.79 \\
& NPM & 36.94 & 669.87 & 270.72 & 1340.91 & 6.69 & 22.20 & 5.97 \\
& PyPI & 38.33 & 76.03 & 261.04 & 150.54 & 0.77 & 21.38 & 4.23 \\
& RubyGems & 37.88 & 28.09 & 264.40 & 55.44 & 0.28 & 21.02 & 4.35 \\
\midrule
\multirow{6}{*}{int8}
& Hugging Face & 22.12 & 192.31 & 452.20 & 368.81 & 3.22 & 36.73 & 4.84 \\
& Maven & 18.32 & 161.71 & 546.01 & 345.73 & 2.51 & 34.72 & 4.81 \\
& Golang & 19.23 & 499.12 & 520.22 & 994.40 & 7.56 & 29.70 & 4.75 \\
& NPM & 35.28 & 719.06 & 283.46 & 1404.03 & 6.69 & 22.20 & 5.28 \\
& PyPI & 37.57 & 80.66 & 266.35 & 153.60 & 0.77 & 21.37 & 4.31 \\
& RubyGems & 39.71 & 26.70 & 252.35 & 52.92 & 0.28 & 21.00 & 4.23 \\
\bottomrule
\end{tabular}%
}
\label{tab:EQ1-Efficiency}
\end{table*}

\iffalse
\subsection{Pseudo-code for \texttt{TypoSim} Similarity Function}

The TypoSim similarity function (Algorithm~\ref{alg:typosim}) provides \tool with the capability to assess similarity between package names and detect potential typosquatting threats.
The algorithm integrates multiple dimensions of similarity, including Normalized Damerau-Levenshtein Distance, N-gram Similarity, Phonetic Similarity, Typosquat-Specific Similarity, and Substring Similarity, each designed to capture distinct patterns of name manipulation.
The function then computes a maximum similarity score across these metrics.
At the end, it returns both the maximum similarity value and the individual similarity scores for further analysis.

\iffalse
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small, columns=fullflexible]
class TypoSim:
    def __init__(self):
        self.max_sim = 0

    def __call__(self, name1, name2):
        similarities = {
            "normalize_damerau_levenshtein": self.calculate_normalized_damerau_levenshtein(name1, name2),
            "n_gram_similarity": self.calculate_n_gram_similarity(name1, name2),
            "phonetic_similarity": self.calculate_phonetic_similarity(name1, name2),
            "typosquat_similarity": self.calculate_typosquat_similarity(name1, name2),
            "substring_similarity": self.calculate_substring_similarity(name1, name2),
        }
        self.max_sim = max(similarities.values())
        return self.max_sim, similarities

    def calculate_normalized_damerau_levenshtein(self, name1, name2):
        if not name1 or not name2: return 0.0
        max_len = max(len(name1), len(name2))
        return 1 - (distance(name1, name2) / max_len)

    def calculate_n_gram_similarity(self, s1, s2, n=2):
        if len(s1) < n or len(s2) < n: return 0.0
        return intersection(s1_ngrams, s2_ngrams) / union(s1_ngrams, s2_ngrams)

    def calculate_phonetic_similarity(self, s1, s2):
        if not s1 or not s2: return 0.0
        return avg(soundex_similarity(s1, s2), metaphone_similarity(s1, s2))

    def calculate_typosquat_similarity(self, s1, s2):
        s1, s2 = normalize(s1, s2)
        apply_substitutions()
        return calculate_similarity()

    def calculate_substring_similarity(self, s1, s2):
        overlaps = find_overlaps(split(s1), split(s2))
        return compute_similarity(overlaps) + bonuses(overlaps)
\end{lstlisting}
\fi

\begin{algorithm}[h]
\caption{TypoSim Similarity Computation}
\label{alg:typosim}
\begin{algorithmic}[1]

\STATE \textbf{Initialize} max\_sim $\gets$ 0

\STATE \textbf{Function} TypoSim(name1, name2):
\STATE \quad similarities $\gets$ \{
\STATE \quad \quad \texttt{"normalize\_damerau\_levenshtein"} $\gets$ CalculateNormalizedDamerauLevenshtein(name1, name2),
\STATE \quad \quad \texttt{"n\_gram\_similarity"} $\gets$ CalculateNGramSimilarity(name1, name2),
\STATE \quad \quad \texttt{"phonetic\_similarity"} $\gets$ CalculatePhoneticSimilarity(name1, name2),
\STATE \quad \quad \texttt{"typosquat\_similarity"} $\gets$ CalculateTyposquatSimilarity(name1, name2),
\STATE \quad \quad \texttt{"substring\_similarity"} $\gets$ CalculateSubstringSimilarity(name1, name2)
\STATE \quad \}
\STATE \quad max\_sim $\gets$ \textbf{MAX}(similarities.values())
\STATE \quad \textbf{return} (max\_sim, similarities)

\STATE \textbf{Function} CalculateNormalizedDamerauLevenshtein(name1, name2):
\STATE \quad \textbf{if} name1 = \texttt{NULL} or name2 = \texttt{NULL}:
\STATE \quad \quad \textbf{return} 0.0
\STATE \quad max\_len $\gets$ \textbf{MAX}(\textbf{LENGTH}(name1), \textbf{LENGTH}(name2))
\STATE \quad \textbf{return} $1 - \frac{\text{distance}(name1, name2)}{\text{max\_len}}$

\STATE \textbf{Function} CalculateNGramSimilarity(s1, s2, n):
\STATE \quad \textbf{if} \textbf{LENGTH}(s1) $<$ n or \textbf{LENGTH}(s2) $<$ n:
\STATE \quad \quad \textbf{return} 0.0
\STATE \quad \textbf{return} $\frac{\text{INTERSECTION}(s1\_ngrams, s2\_ngrams)}{\text{UNION}(s1\_ngrams, s2\_ngrams)}$

\STATE \textbf{Function} CalculatePhoneticSimilarity(s1, s2):
\STATE \quad \textbf{if} s1 = \texttt{NULL} or s2 = \texttt{NULL}:
\STATE \quad \quad \textbf{return} 0.0
\STATE \quad \textbf{return} $\text{AVG}(\text{SoundexSimilarity}(s1, s2), \text{MetaphoneSimilarity}(s1, s2))$

\STATE \textbf{Function} CalculateTyposquatSimilarity(s1, s2):
\STATE \quad s1, s2 $\gets$ Normalize(s1, s2)
\STATE \quad ApplySubstitutions()
\STATE \quad \textbf{return} CalculateSimilarity()

\STATE \textbf{Function} CalculateSubstringSimilarity(s1, s2):
\STATE \quad overlaps $\gets$ FindOverlaps(Split(s1), Split(s2))
\STATE \quad \textbf{return} ComputeSimilarity(overlaps) + Bonuses(overlaps)

\end{algorithmic}
\end{algorithm}


\iffalse
\begin{algorithm}[h]
\small
% \caption{Heuristic Rules for False-positive Verification. \TODO{Replace with a figure.} \WJ{TODO: Move this to appendix}}
\caption{Heuristic Rules for False-positive Verification}
\label{algo:heuristic_false_positive}

\KwIn{\texttt{typoDoc}, \texttt{legitDoc}, \texttt{registry}}
\KwOut{(\texttt{Boolean}, \texttt{metrics}, \texttt{explanation})}

\BlankLine
\CommentSty{/* Initialize all metrics keys to None */}
\ForEach{key in \texttt{metrics}}{
    \texttt{metrics[key]} $\gets$ \texttt{None}\;
}

\BlankLine
\CommentSty{/* Populate metrics via helper checks */}
\texttt{metrics} $\gets$ \texttt{\_check\_package\_naming\_and\_purpose}(\texttt{typoDoc}, \texttt{legitDoc})\;
\texttt{metrics["overlapped\_maintainers"]} $\gets$ \texttt{\_has\_overlapped\_maintainers}(\texttt{typoDoc}, \texttt{legitDoc}, \texttt{registry})\;
\texttt{metrics["comprehensive\_metadata"]} $\gets$ \texttt{\_has\_comprehensive\_metadata}(\texttt{typoDoc})\;
\texttt{metrics["active\_development"]} $\gets$ \texttt{\_is\_actively\_developed}(\texttt{typoDoc})\;

\BlankLine
\CommentSty{/* Early non-typosquat checks */}
\If{( $\neg$\texttt{metrics["is\_adversarial\_name"]}) \textbf{and} ( $\neg$\texttt{metrics["is\_suspicious"]})}{
    \If{\texttt{metrics["obvious\_not\_typosquat"]} \textbf{or}
       \texttt{metrics["fork\_naming"]} \textbf{or}
       \texttt{isTest} \textbf{or}
       \texttt{metrics["is\_known\_maintainer"]} \textbf{or}
       \texttt{metrics["distinct\_purpose"]} \textbf{or}
       \texttt{metrics["overlapped\_maintainers"]}}{
        \Return (\texttt{True}, \texttt{metrics}, \texttt{explanation})\;
    }
}

\BlankLine
\CommentSty{/* Potential typosquat cases */}
\If{\texttt{metrics["is\_adversarial\_name"]} \textbf{and}
   ($\neg$\texttt{metrics["distinct\_purpose"]})}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\If{( $\neg$\texttt{metrics["comprehensive\_metadata"]} \textbf{or}
     $\neg$\texttt{metrics["distinct\_purpose"]})
    \textbf{and}
    ( $\neg$\texttt{metrics["is\_adversarial\_name"]} \textbf{or}
      $\neg$\texttt{metrics["active\_development"]})}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\If{\texttt{metrics["is\_adversarial\_name"]} \textbf{and}
   ( $\neg$\texttt{metrics["distinct\_purpose"]} \textbf{or}
     $\neg$\texttt{metrics["comprehensive\_metadata"]})}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\BlankLine
\CommentSty{/* Check for deprecation */}
\texttt{isDeprecated} $\gets$ \texttt{\_check\_deprecated}(\texttt{typoDoc})\;
\If{(\texttt{isDeprecated} \textbf{and} \texttt{metrics["is\_adversarial\_name"]})}{
    \Return (\texttt{True}, \texttt{metrics}, \texttt{explanation})\;
}

\BlankLine
\CommentSty{/* Final suspicious check */}
\If{\texttt{metrics["is\_suspicious"]}}{
    \Return (\texttt{False}, \texttt{metrics}, \texttt{explanation})\;
}

\BlankLine
\CommentSty{/* Otherwise, likely false positive */}
\Return (\texttt{True}, \texttt{metrics}, \texttt{explanation})\;

\end{algorithm}
\fi

\fi
\fi

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks