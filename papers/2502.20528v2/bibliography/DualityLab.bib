
@misc{beyer_us_2023,
	title = {U.{S}. {Federal} and {State} {Regulation} of {Internet} of {Things} ({IoT}) {Devices}},
	url = {https://jsis.washington.edu/news/u-s-federal-and-state-regulation-of-internet-of-things-iot-devices-2019-2022/},
	author = {Beyer, J and Jacob, S and Lii, E and Osburn, L and Pierson, S and Quirk, C and Su, D and Tanaka, C},
	month = mar,
	year = {2023},
}

@misc{jlbeyer_us_2023,
	title = {U.{S}. {Federal} and {State} {Regulation} of {Internet} of {Things} ({IoT}) {Devices}},
	url = {https://jsis.washington.edu/news/u-s-federal-and-state-regulation-of-internet-of-things-iot-devices-2019-2022/},
	abstract = {The attached dataset collects U.S. regulations at the federal and state levels regarding Internet of Things (IoT) devices. Our dataset covers all existing regulation to the year 2022. We do},
	language = {en},
	urldate = {2023-09-28},
	journal = {The Henry M. Jackson School of International Studies},
	author = {jlbeyer},
	month = mar,
	year = {2023},
}

@misc{noauthor_cyber_2022,
	title = {Cyber {Resilience} {Act} {\textbar} {Shaping} {Europe}’s digital future},
	url = {https://digital-strategy.ec.europa.eu/en/library/cyber-resilience-act},
	abstract = {The proposal for a regulation on cybersecurity requirements for products with digital elements, known as the Cyber Resilience Act, bolsters cybersecurity rules to ensure more secure hardware and software products.},
	language = {en},
	urldate = {2023-09-28},
	month = sep,
	year = {2022},
}

@misc{noauthor_cybersecurity_nodate,
	title = {Cybersecurity {Labelling} {Scheme} ({CLS})},
	url = {https://www.csa.gov.sg/our-programmes/certification-and-labelling-schemes/cybersecurity-labelling-scheme},
	abstract = {The Cyber Security Agency of Singapore (CSA) has launched the Cybersecurity Labelling Scheme (CLS) for consumer smart devices, as part of efforts to improve Internet of Things (IoT) security, raise overall cyber hygiene levels and better secure Singapore's cyberspace.},
	language = {en},
	urldate = {2023-09-28},
	journal = {Default},
}

@misc{noauthor_uk_2023,
	title = {The {UK} {Product} {Security} and {Telecommunications} {Infrastructure} ({Product} {Security}) regime},
	url = {https://www.gov.uk/government/publications/the-uk-product-security-and-telecommunications-infrastructure-product-security-regime},
	abstract = {The UK’s consumer connectable product security regime will come into effect on 29 April 2024. Businesses involved in the supply chains of these products will need to be compliant with this legislative framework from that date.},
	language = {en},
	urldate = {2023-09-28},
	journal = {GOV.UK},
	month = aug,
	year = {2023},
}

@misc{noauthor_hb_nodate,
	title = {{HB} 2307 {Consumer} {Data} {Protection} {Act}; establishes a framework for controlling and processing personal data.},
	url = {https://lis.virginia.gov/cgi-bin/legp604.exe?212+sum+HB2307},
	urldate = {2023-09-28},
}

@misc{noauthor_hb2395_nodate,
	title = {{HB2395} 2019 {Regular} {Session} - {Oregon} {Legislative} {Information} {System}},
	url = {https://olis.oregonlegislature.gov/liz/2019R1/Measures/Overview/HB2395},
	urldate = {2023-09-28},
}

@misc{noauthor_bill_nodate,
	title = {Bill {Text} - {SB}-327 {Information} privacy: connected devices.},
	url = {https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB327},
	urldate = {2023-09-28},
}

@article{konev_survey_2022,
	title = {A {Survey} on {Threat}-{Modeling} {Techniques}: {Protected} {Objects} and {Classification} of {Threats}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	shorttitle = {A {Survey} on {Threat}-{Modeling} {Techniques}},
	url = {https://www.mdpi.com/2073-8994/14/3/549},
	doi = {10.3390/sym14030549},
	abstract = {Information security is one of the most important attributes of distributed systems that often operate on unreliable networks. Enabling security features during the development of a distributed system requires the careful analysis of potential attacks or threats in different contexts, a process often referred to as «threat modeling». Information protection should be comprehensive, but it is also necessary to take into account the possibility of the emergence of threats specific to a certain information system. Many public and private organizations are still trying to implement system models and the threats directed at them on their own. The main reason for this is the lack of useful and high-quality methodologies that can help developers design system models. This review explores a variety of the literature on confidentiality- and integrity-aware system design methodologies, as well as threat classification methods, and identifies key issues that may be referenced by organizations to make design system processes easier. In particular, this article takes a look at the extent to which existing methodologies cover objects of protection and methods of classifying threats, as well as whether there are such models of systems in which the object itself and the threats directed at it are described. This includes whether the compiled models exhibit symmetry or asymmetry. This literature research shows that methodologies appear to be heterogeneous and versatile, since existing methodologies often only focus on one object of protection (a system). Based on the given analysis, it can be concluded that the existing methodologies only relate superficially to the description of system models and threats, and it is necessary to develop a more complete abstract model of the protected object and threats aimed at it in order to make this model suitable for any organization and protect it against most threats.},
	language = {en},
	number = {3},
	urldate = {2023-09-26},
	journal = {Symmetry},
	author = {Konev, Anton and Shelupanov, Alexander and Kataev, Mikhail and Ageeva, Valeriya and Nabieva, Alina},
	month = mar,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {asymmetry, attack, confidentiality, information security, integrity, model, object, symmetry, system, threat, threat classification methods, unauthorized},
	pages = {549},
}

@article{ling_systematic_2020,
	title = {A {Systematic} {Literature} {Review} of {Information} {Sources} for {Threat} {Modeling} in the {Power} {Systems} {Domain}},
	volume = {12332},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7948005/},
	doi = {10.1007/978-3-030-58295-1_4},
	abstract = {Power systems are one of the critical infrastructures that has seen an increase in cyber security threats due to digitalization. The digitalization also affects the size and complexity of the infrastructure and therefore makes it more difficult to gain an overview in order to secure the entire power system from attackers. One method of how to gain an overview of possible vulnerabilities and security threats is to use threat modeling. In threat modeling, information regarding the vulnerabilities and possible attacks of power systems is required to create an accurate and useful model. There are several different sources for this information. In this paper we conduct a systematic literature review to find which information sources that have been used in power system threat modeling research. Six different information sources were found: expert knowledge, logs \& alerts, previous research, system’s state, vulnerability scoring \& databases, and vulnerability scanners.},
	urldate = {2023-09-26},
	journal = {Critical Information Infrastructures Security},
	author = {Ling, Engla and Lagerström, Robert and Ekstedt, Mathias},
	month = aug,
	year = {2020},
	pmid = {null},
	pmcid = {PMC7948005},
	pages = {47--58},
}

@article{xiong_cyber_2022,
	title = {Cyber security threat modeling based on the {MITRE} {Enterprise} {ATT}\&{CK} {Matrix}},
	volume = {21},
	issn = {1619-1374},
	url = {https://doi.org/10.1007/s10270-021-00898-7},
	doi = {10.1007/s10270-021-00898-7},
	abstract = {Enterprise systems are growing in complexity, and the adoption of cloud and mobile services has greatly increased the attack surface. To proactively address these security issues in enterprise systems, this paper proposes a threat modeling language for enterprise security based on the MITRE Enterprise ATT\&CK Matrix. It is designed using the Meta Attack Language framework and focuses on describing system assets, attack steps, defenses, and asset associations. The attack steps in the language represent adversary techniques as listed and described by MITRE. This entity-relationship model describes enterprise IT systems as a whole; by using available tools, the proposed language enables attack simulations on its system model instances. These simulations can be used to investigate security settings and architectural changes that might be implemented to secure the system more effectively. Our proposed language is tested with a number of unit and integration tests. This is visualized in the paper with two real cyber attacks modeled and simulated.},
	language = {en},
	number = {1},
	urldate = {2023-09-26},
	journal = {Software and Systems Modeling},
	author = {Xiong, Wenjun and Legrand, Emeline and Åberg, Oscar and Lagerström, Robert},
	month = feb,
	year = {2022},
	keywords = {Attack simulations, Domain-specific language, Enterprise systems, Threat modeling},
	pages = {157--177},
}

@article{tatam_review_2021,
	title = {A review of threat modelling approaches for {APT}-style attacks},
	volume = {7},
	issn = {24058440},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405844021000748},
	doi = {10.1016/j.heliyon.2021.e05969},
	abstract = {Threats are potential events, intentional or not, that compromise the conﬁdentiality, integrity, and/or availability of information systems. Defending against threats and attacks requires actionable threat intelligence. Using this intelligence to minimise risk, requires a systematic methodology or framework that recognises every possible threat scenario. This can be done with Threat Modelling (TM), which assists with identifying, understanding and providing visibility of threats affecting an organisation. The focus of this study is to determine TM limitations, strengths, and any perceivable gaps. It has also focused on identifying any possible enhancements that may improve TM performance and efﬁciency when modelling sophisticated attacks such as Advanced Persistent Threats (APT).},
	language = {en},
	number = {1},
	urldate = {2023-09-26},
	journal = {Heliyon},
	author = {Tatam, Matt and Shanmugam, Bharanidharan and Azam, Sami and Kannoorpatti, Krishnan},
	month = jan,
	year = {2021},
	pages = {e05969},
}

@misc{rodsan_threat_nodate,
	title = {Threat {Modeling} {Security} {Fundamentals} - {Training}},
	url = {https://learn.microsoft.com/en-us/training/paths/tm-threat-modeling-fundamentals/},
	abstract = {This learning path takes you through the four main phases of threat modeling, explains the differences between each data-flow diagram element, walks you through the threat modeling framework, recommends different tools and gives you a step-by-step guide on creating proper data-flow diagrams.},
	language = {en-us},
	urldate = {2023-09-26},
	author = {rodsan},
}

@misc{noauthor_digital_nodate,
	title = {Digital {Trust} \& {Safety} {Partnership}},
	url = {https://dtspartnership.org/wp-content/uploads/2021/12/DTSP_Safe_Framework.pdf},
	month = dec,
}

@misc{noauthor_notitle_nodate,
}

@misc{tspa_content_nodate,
	title = {Content {Moderation} and {Operations}},
	url = {https://www.tspa.org/curriculum/ts-fundamentals/content-moderation-and-operations/what-is-content-moderation/},
	author = {TSPA},
}

@inproceedings{pang_binpacc_2006,
	address = {New York, NY, USA},
	series = {{IMC} '06},
	title = {binpacc},
	isbn = {978-1-59593-561-8},
	shorttitle = {binpac},
	url = {https://dl.acm.org/doi/10.1145/1177080.1177119},
	doi = {10.1145/1177080.1177119},
	abstract = {A key step in the semantic analysis of network traffic is to parse the traffic stream according to the high-level protocols it contains. This process transforms raw bytes into structured, typed, and semantically meaningful data fields that provide a high-level representation of the traffic. However, constructing protocol parsers by hand is a tedious and error-prone affair due to the complexity and sheer number of application protocols.This paper presents binpac, a declarative language and compiler designed to simplify the task of constructing robust and efficient semantic analyzers for complex network protocols. We discuss the design of the binpac language and a range of issues in generating efficient parsers from high-level specifications. We have used binpac to build several protocol parsers for the "Bro" network intrusion detection system, replacing some of its existing analyzers (handcrafted in C++), and supplementing its operation with analyzers for new protocols. We can then use Bro's powerful scripting language to express application-level analysis of network traffic in high-level terms that are both concise and expressive. binpac is now part of the open-source Bro distribution.},
	urldate = {2023-09-15},
	publisher = {Association for Computing Machinery},
	author = {Pang, Ruoming and Paxson, Vern and Sommer, Robin and Peterson, Larry},
	month = oct,
	year = {2006},
	keywords = {parser generator, protocol},
	pages = {289--300},
}

@misc{guo_empirical_2023,
	title = {An {Empirical} {Study} of {Malicious} {Code} {In} {PyPI} {Ecosystem}},
	url = {http://arxiv.org/abs/2309.11021},
	abstract = {PyPI provides a convenient and accessible package management platform to developers, enabling them to quickly implement specific functions and improve work efficiency. However, the rapid development of the PyPI ecosystem has led to a severe problem of malicious package propagation. Malicious developers disguise malicious packages as normal, posing a significant security risk to end-users. To this end, we conducted an empirical study to understand the characteristics and current state of the malicious code lifecycle in the PyPI ecosystem. We first built an automated data collection framework and collated a multi-source malicious code dataset containing 4,669 malicious package files. We preliminarily classified these malicious code into five categories based on malicious behaviour characteristics. Our research found that over 50\% of malicious code exhibits multiple malicious behaviours, with information stealing and command execution being particularly prevalent. In addition, we observed several novel attack vectors and anti-detection techniques. Our analysis revealed that 74.81\% of all malicious packages successfully entered end-user projects through source code installation, thereby increasing security risks. A real-world investigation showed that many reported malicious packages persist in PyPI mirror servers globally, with over 72\% remaining for an extended period after being discovered. Finally, we sketched a portrait of the malicious code lifecycle in the PyPI ecosystem, effectively reflecting the characteristics of malicious code at different stages. We also present some suggested mitigations to improve the security of the Python open-source ecosystem.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Guo, Wenbo and Xu, Zhengzi and Liu, Chengwei and Huang, Cheng and Fang, Yong and Liu, Yang},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11021 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{guo_empirical_2023-1,
	title = {An {Empirical} {Study} of {Malicious} {Code} {In} {PyPI} {Ecosystem}},
	url = {http://arxiv.org/abs/2309.11021},
	abstract = {PyPI provides a convenient and accessible package management platform to developers, enabling them to quickly implement specific functions and improve work efficiency. However, the rapid development of the PyPI ecosystem has led to a severe problem of malicious package propagation. Malicious developers disguise malicious packages as normal, posing a significant security risk to end-users. To this end, we conducted an empirical study to understand the characteristics and current state of the malicious code lifecycle in the PyPI ecosystem. We first built an automated data collection framework and collated a multi-source malicious code dataset containing 4,669 malicious package files. We preliminarily classified these malicious code into five categories based on malicious behaviour characteristics. Our research found that over 50\% of malicious code exhibits multiple malicious behaviours, with information stealing and command execution being particularly prevalent. In addition, we observed several novel attack vectors and anti-detection techniques. Our analysis revealed that 74.81\% of all malicious packages successfully entered end-user projects through source code installation, thereby increasing security risks. A real-world investigation showed that many reported malicious packages persist in PyPI mirror servers globally, with over 72\% remaining for an extended period after being discovered. Finally, we sketched a portrait of the malicious code lifecycle in the PyPI ecosystem, effectively reflecting the characteristics of malicious code at different stages. We also present some suggested mitigations to improve the security of the Python open-source ecosystem.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Guo, Wenbo and Xu, Zhengzi and Liu, Chengwei and Huang, Cheng and Fang, Yong and Liu, Yang},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11021 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{xia_universal_2023,
	title = {Universal {Fuzzing} via {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.04748},
	doi = {10.48550/arXiv.2308.04748},
	abstract = {Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 76 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 47 bugs already confirmed by developers as previously unknown.},
	urldate = {2023-09-20},
	author = {Xia, Chunqiu Steven and Paltenghi, Matteo and Tian, Jia Le and Pradel, Michael and Zhang, Lingming},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04748 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{shostack_elevation_nodate,
	title = {Elevation of {Privilege}: {Drawing} {Developers} into {Threat} {Modeling}},
	abstract = {This paper presents Elevation of Privilege, a game designed to draw people who are not security practitioners into the craft of threat modeling. The game uses a variety of techniques to do so in an enticing, supportive and non-threatening way. The subject of security tools for software engineering has not generally been studied carefully. This paper shares the objectives and design of the game, as well as tradeoffs made and lessons learned while building it. It concludes with discussion of other areas where games may help information security professionals reach important goals.},
	language = {en},
	author = {Shostack, Adam},
}

@misc{noauthor_fundamentals_nodate,
	title = {Fundamentals of {Threat} {Modeling} from {Security} {Innovation} {\textbar} {NICCS}},
	url = {https://niccs.cisa.gov/education-training/catalog/security-innovation/fundamentals-threat-modeling},
	abstract = {Threat modeling is a structured activity for identifying and evaluating application threats and vulnerabilities.},
	language = {en},
	urldate = {2023-09-20},
}

@article{steingartner_threat_2021,
	title = {Threat {Defense}: {Cyber} {Deception} {Approach} and {Education} for {Resilience} in {Hybrid} {Threats} {Model}},
	volume = {13},
	issn = {2073-8994},
	shorttitle = {Threat {Defense}},
	url = {https://www.mdpi.com/2073-8994/13/4/597},
	doi = {10.3390/sym13040597},
	abstract = {This paper aims to explore the cyber-deception-based approach and to design a novel conceptual model of hybrid threats that includes deception methods. Security programs primarily focus on prevention-based strategies aimed at stopping attackers from getting into the network. These programs attempt to use hardened perimeters and endpoint defenses by recognizing and blocking malicious activities to detect and stop attackers before they can get in. Most organizations implement such a strategy by fortifying their networks with defense-in-depth through layered prevention controls. Detection controls are usually placed to augment prevention at the perimeter, and not as consistently deployed for in-network threat detection. This architecture leaves detection gaps that are difﬁcult to ﬁll with existing security controls not speciﬁcally designed for that role. Rather than using prevention alone, a strategy that attackers have consistently succeeded against, defenders are adopting a more balanced strategy that includes detection and response. Most organizations deploy an intrusion detection system (IDS) or next-generation ﬁrewall that picks up known attacks or attempts to pattern match for identiﬁcation. Other detection tools use monitoring, trafﬁc, or behavioral analysis. These reactive defenses are designed to detect once they are attacked yet often fail. They also have some limitations because they are not designed to catch credential harvesting or attacks based on what appears as authorized access. They are also often seen as complex and prone to false positives, adding to analyst alert fatigue. The security industry has focused recent innovation on ﬁnding more accurate ways to recognize malicious activity with technologies such as user and entity behavioral analytics (UEBA), big data, artiﬁcial intelligence (AI), and deception.},
	language = {en},
	number = {4},
	urldate = {2023-09-20},
	journal = {Symmetry},
	author = {Steingartner, William and Galinec, Darko and Kozina, Andrija},
	month = apr,
	year = {2021},
	pages = {597},
}

@inproceedings{silva_machine_2018,
	title = {Machine learning in incident categorization automation},
	doi = {10.23919/CISTI.2018.8399244},
	abstract = {IT incident management process requires a correct categorization to attribute incident tickets to the right resolution group and obtain an operational system as quickly as possible, having the lowest possible impact on the business and costumers. In this work, we introduce a module to automatically categorize incident tickets, turning the responsible teams for incident management more productive. This module can be integrated as an extension into an incident ticket system (ITS), which contributes to reduce the time wasted on incident ticket route and reduce the amount of errors on incident categorization. To automate the classification, we use a support vector machine (SVM), obtaining an accuracy of 89\%, approximately, on a dataset of real-world incident tickets.},
	booktitle = {2018 13th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	author = {Silva, Sara and Pereira, Rubén and Ribeiro, Ricardo},
	month = jun,
	year = {2018},
	keywords = {Artificial neural networks, Automated incident categorization, Categorization, Classification algorithms, Companies, Feature extraction, Incident management, Incident management process, Machine learning, SVM, Support vector machines, Text categorization},
	pages = {1--6},
}

@inproceedings{jiang_empirical_2022,
	address = {Los Angeles CA USA},
	title = {An {Empirical} {Study} of {Artifacts} and {Security} {Risks} in the {Pre}-trained {Model} {Supply} {Chain}},
	isbn = {978-1-4503-9885-5},
	url = {https://dl.acm.org/doi/10.1145/3560835.3564547},
	doi = {10.1145/3560835.3564547},
	abstract = {Deep neural networks achieve state-of-the-art performance on many tasks, but require increasingly complex architectures and costly training procedures. Engineers can reduce costs by reusing a pre-trained model (PTM) and fine-tuning it for their own tasks. To facilitate software reuse, engineers collaborate around model hubs, collections of PTMs and datasets organized by problem domain. Although model hubs are now comparable in popularity and size to other software ecosystems, the associated PTM supply chain has not yet been examined from a software engineering perspective.},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {Proceedings of the 2022 {ACM} {Workshop} on {Software} {Supply} {Chain} {Offensive} {Research} and {Ecosystem} {Defenses}},
	publisher = {ACM},
	author = {Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan and Indarapu, Aryan and Hyatt, Matt and Schorlemmer, Taylor R. and Thiruvathukal, George K. and Davis, James C.},
	month = nov,
	year = {2022},
	pages = {105--114},
}

@inproceedings{singla2023empirical,
	title = {An empirical study on using large language models to analyze software supply chain security failures},
	booktitle = {Proceedings of the 2023 {ACM} {Workshop} on {Software} {Supply} {Chain} {Offensive} {Research} and {Ecosystem} {Defenses} ({SCORED} 2023)},
	author = {Singla, Tanmay and Anandayuvaraj, Dharun and Kalu, Kelechi G and Schorlemmer, Taylor R and Davis, James C},
	year = {2023},
}

@inproceedings{okafor_sok_2022,
	address = {Los Angeles CA USA},
	title = {{SoK}: {Analysis} of {Software} {Supply} {Chain} {Security} by {Establishing} {Secure} {Design} {Properties}},
	isbn = {978-1-4503-9885-5},
	shorttitle = {{SoK}},
	url = {https://dl.acm.org/doi/10.1145/3560835.3564556},
	doi = {10.1145/3560835.3564556},
	abstract = {This paper systematizes knowledge about secure software supply chain patterns. It identifies four stages of a software supply chain attack and proposes three security properties crucial for a secured supply chain: transparency, validity, and separation. The paper describes current security approaches and maps them to the proposed security properties, including research ideas and case studies of supply chains in practice. It discusses the strengths and weaknesses of current approaches relative to known attacks and details the various security frameworks put out to ensure the security of the software supply chain. Finally, the paper highlights potential gaps in actor and operation-centered supply chain security techniques.},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {Proceedings of the 2022 {ACM} {Workshop} on {Software} {Supply} {Chain} {Offensive} {Research} and {Ecosystem} {Defenses}},
	publisher = {ACM},
	author = {Okafor, Chinenye and Schorlemmer, Taylor R. and Torres-Arias, Santiago and Davis, James C.},
	month = nov,
	year = {2022},
	pages = {15--24},
}

@inproceedings{amusuo_systematically_2023,
	title = {Systematically {Detecting} {Packet} {Validation} {Vulnerabilities} in {Embedded} {Network} {Stacks}},
	url = {http://arxiv.org/abs/2308.10965},
	abstract = {Embedded Network Stacks (ENS) enable lowresource devices to communicate with the outside world, facilitating the development of the Internet of Things and CyberPhysical Systems. Some defects in ENS are thus high-severity cybersecurity vulnerabilities: they are remotely triggerable and can impact the physical world. While prior research has shed light on the characteristics of defects in many classes of software systems, no study has described the properties of ENS defects nor identified a systematic technique to expose them. The most common automated approach to detecting ENS defects is feedback-driven randomized dynamic analysis (“fuzzing”), a costly and unpredictable technique.},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2023)},
	author = {Amusuo, Paschal C. and Méndez, Ricardo Andrés Calvo and Xu, Zhongwei and Machiry, Aravind and Davis, James C.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10965 [cs]},
	keywords = {Computer Science - Software Engineering, D.2.5},
}

@inproceedings{Shen2021BackdoorPTMAttackcanTransfertoAll,
	title = {Backdoor {Pre}-trained {Models} {Can} {Transfer} to {All}},
	url = {http://arxiv.org/abs/2111.00197},
	doi = {10.1145/3460120.3485370},
	abstract = {Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Shen, Lujia and Ji, Shouling and Zhang, Xuhong and Li, Jinfeng and Chen, Jing and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Wang, Ting},
	month = nov,
	year = {2021},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {3141--3158},
}

@inproceedings{Juuti2019ProtectingAgastDNNModelStealingAttack,
	title = {{PRADA}: {Protecting} {Against} {DNN} {Model} {Stealing} {Attacks}},
	shorttitle = {{PRADA}},
	doi = {10.1109/EuroSP.2019.00044},
	abstract = {Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.},
	booktitle = {2019 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Juuti, Mika and Szyller, Sebastian and Marchal, Samuel and Asokan, N.},
	month = jun,
	year = {2019},
	keywords = {Adversarial machine learning, Business, Computational modeling, Data mining, Mathematical model, Neural networks, Predictive models, Training, deep neural network, model extraction, model stealing},
	pages = {512--527},
}

@inproceedings{yen_semi-automated_2021,
	address = {New York, NY, USA},
	series = {{SIGCOMM} '21},
	title = {Semi-automated protocol disambiguation and code generation},
	isbn = {9781450383837},
	url = {https://dl.acm.org/doi/10.1145/3452296.3472910},
	doi = {10.1145/3452296.3472910},
	abstract = {For decades, Internet protocols have been specified using natural language. Given the ambiguity inherent in such text, it is not surprising that protocol implementations have long exhibited bugs. In this paper, we apply natural language processing (NLP) to effect semi-automated generation of protocol implementations from specification text. Our system, Sage, can uncover ambiguous or under-specified sentences in specifications; once these are clarified by the author of the protocol specification, Sage can generate protocol code automatically. Using Sage, we discover 5 instances of ambiguity and 6 instances of under-specification in the ICMP RFC; after fixing these, Sage is able to automatically generate code that interoperates perfectly with Linux implementations. We show that Sage generalizes to sections of BFD, IGMP, and NTP and identify additional conceptual components that Sage needs to support to generalize to complete, complex protocols like BGP and TCP.},
	urldate = {2023-09-15},
	publisher = {Association for Computing Machinery},
	author = {Yen, Jane and Lévai, Tamás and Ye, Qinyuan and Ren, Xiang and Govindan, Ramesh and Raghavan, Barath},
	month = aug,
	year = {2021},
	keywords = {natural language, protocol specifications},
	pages = {272--286},
}

@inproceedings{bangert_nail_2014,
	title = {Nail: {A} {Practical} {Tool} for {Parsing} and {Generating} {Data} {Formats}},
	isbn = {9781931971164},
	shorttitle = {Nail},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/bangert},
	urldate = {2023-09-15},
	author = {Bangert, Julian and Zeldovich, Nickolai},
	year = {2014},
	pages = {615--628},
}

@article{narayan_survey_2015,
	title = {A {Survey} of {Automatic} {Protocol} {Reverse} {Engineering} {Tools}},
	volume = {48},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/2840724},
	doi = {10.1145/2840724},
	abstract = {Computer network protocols define the rules in which two entities communicate over a network of unique hosts. Many protocol specifications are unknown, unavailable, or minimally documented, which prevents thorough analysis of the protocol for security purposes. For example, modern botnets often use undocumented and unique application-layer communication protocols to maintain command and control over numerous distributed hosts. Inferring the specification of closed protocols has numerous advantages, such as intelligent deep packet inspection, enhanced intrusion detection system algorithms for communications, and integration with legacy software packages. The multitude of closed protocols coupled with existing time-intensive reverse engineering methodologies has spawned investigation into automated approaches for reverse engineering of closed protocols. This article summarizes and organizes previously presented automatic protocol reverse engineering tools by approach. Approaches that focus on reverse engineering the finite state machine of a target protocol are separated from those that focus on reverse engineering the protocol format.},
	number = {3},
	urldate = {2023-09-15},
	journal = {ACM Computing Surveys},
	author = {Narayan, John and Shukla, Sandeep K. and Clancy, T. Charles},
	month = dec,
	year = {2015},
	keywords = {Protocol reverse engineering, communication security},
	pages = {40:1--40:26},
}

@inproceedings{comparetti_prospex_2009,
	title = {Prospex: {Protocol} {Specification} {Extraction}},
	shorttitle = {Prospex},
	doi = {10.1109/SP.2009.14},
	abstract = {Protocol reverse engineering is the process of extracting application-level specifications for network protocols. Such specifications are very useful in a number of security-related contexts, for example, to perform deep packet inspection and black-box fuzzing, or to quickly understand custom botnet command and control (C\&C) channels. Since manual reverse engineering is a time-consuming and tedious process, a number of systems have been proposed that aim to automate this task. These systems either analyze network traffic directly or monitor the execution of the application that receives the protocol messages. While previous systems show that precise message formats can be extracted automatically, they do not provide a protocol specification.The reason is that they do not reverse engineer the protocol state machine. In this paper, we focus on closing this gap by presenting a system that is capable of automatically inferring state machines. This greatly enhances the results of automatic protocol reverse engineering, while further reducing the need for human interaction. We extend previous work that focuses on behavior-based message format extraction, and introduce techniques for identifying and clustering different types of messages not only based on their structure, but also according to the impact of each message on server behavior. Moreover, we present an algorithm for extracting the state machine. We have applied our techniques to a number of real-world protocols, including the command and control protocol used by a malicious bot. Our results demonstrate that we are able to extract format specifications for different types of messages and meaningful protocol state machines. We use these protocol specifications to automatically generate input for a stateful fuzzer, allowing us to discover security vulnerabilities in real-world applications.},
	booktitle = {2009 30th {IEEE} {Symposium} on {Security} and {Privacy}},
	author = {Comparetti, Paolo Milani and Wondracek, Gilbert and Kruegel, Christopher and Kirda, Engin},
	month = may,
	year = {2009},
	note = {ISSN: 2375-1207},
	keywords = {Command and control systems, Data mining, Humans, Inspection, Monitoring, Network servers, Protocols, Reverse engineering, Security, Telecommunication traffic},
	pages = {110--125},
}

@misc{hou_large_2023,
	title = {Large {Language} {Models} for {Software} {Engineering}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Large {Language} {Models} for {Software} {Engineering}},
	url = {http://arxiv.org/abs/2308.10620},
	doi = {10.48550/arXiv.2308.10620},
	abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We collect and analyze 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study.},
	urldate = {2023-09-14},
	author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
	month = sep,
	year = {2023},
	note = {arXiv:2308.10620 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{gurdur_broo_cyber-physical_2021,
	title = {Cyber-physical systems research and education in 2030: {Scenarios} and strategies},
	volume = {21},
	issn = {2452-414X},
	shorttitle = {Cyber-physical systems research and education in 2030},
	url = {https://www.sciencedirect.com/science/article/pii/S2452414X20300674},
	doi = {10.1016/j.jii.2020.100192},
	abstract = {Future cyber-physical systems (CPS), such as smart cities, collaborative robots, autonomous vehicles or intelligent transport systems, are expected to be highly intelligent, electrified, and connected. This study explores a focal question about how these new characteristics may affect the education and research related to CPS in 2030, the date identified by the United Nations to achieve the Agenda for Sustainable Development. To this end, first, we have conducted a trend spotting activity, seeking to identify possible influencing factors that may have a great impact on the future of CPS education and research. These factors were clustered in a total of 12 trends – four certainties; namely connectivity, electrification, data and automation – and eight uncertainties; namely intelligence, data ethics, labour market, lifelong learning, higher education, trust in technology, technological development speed, and sustainable development goals. After that, two of the eight uncertainties are identified and used to construct a scenario matrix, which includes four scenarios. These two uncertainties – the so-called strategic uncertainties – are: fulfilment of sustainable development goals and the nature of the technological development, respectively. These two important uncertainties are considered to build the scenarios due to their potential impact on the research and education of CPS. For instance, sustainable development goals are significant targets for many initiatives, organisations and countries. While 2030 is the deadline to achieve these goals, the relationship between the sustainable development goals related to CPS research and education is not studied well. Similarly, the speed of technological development is seen as a driving force behind future CPS. However, the effect of this speed to CPS research and education environment is not known. Different outcomes of the chosen two uncertainties are, then, combined with the remaining trends and uncertainties. Consequently, four scenarios are derived. The Terminator scenario illustrates a dystopian future where profit is the driving force behind technological progress and sustainable development goals are not accomplished. In contrast, The Iron Giant scenario represents the successful implementation of the sustainable development goals where technological development is the force behind the accomplishment of these goals. The scenario called Slow Progress represents a future where gradual technological improvements are present, but sustainability is still not seen as concerning the issue. The Humanist scenario illustrates a future where slow technological development is happening yet sustainable development goals are successfully implemented. Finally, the scenarios are used to initiate discussions by illustrating what the future of research and education could look like and a list of strategies for future CPS research and education environments is proposed. To this end, we invite educators, researchers, institutions and governments to develop the necessary strategies to enable data-orientated, continuous, interdisciplinary, collaborative, ethical, and sustainable research and education by improving digital fluency, advancing digital equality, contributing to new ways of teaching complex thinking, expanding access to learning platforms and preparing next generations to adapt for a rapidly changing future of work conditions.},
	urldate = {2023-09-13},
	journal = {Journal of Industrial Information Integration},
	author = {Gürdür Broo, Didem and Boman, Ulf and Törngren, Martin},
	month = mar,
	year = {2021},
	keywords = {2030, Cyber-physical systems, Future of engineering research, Future of research, Future studies, Research and education, Scenario planning TAIDA framework},
	pages = {100192},
}

@article{skorenkyy_use_2021,
	title = {Use of augmented reality-enabled prototyping of cyber-physical systems for improving cyber-security education},
	volume = {1840},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1840/1/012026},
	doi = {10.1088/1742-6596/1840/1/012026},
	abstract = {The use of augmented reality-enabled scenarios in cybersecurity teaching is proposed in the article to respond to new requirements for the rapid adoption of new technologies and profound knowledge of cybersecurity issues by professionals. Implementation of project-type activities based on real cybersecurity issues in application fields of cyber-physical systems is suggested to improve the competence forming. A use-case of agricultural cyber-physical system of systems is discussed as a viable example of augmented reality-enabled prototyping of cybersecurity risk-aware architecture. The necessary steps are analysis of general and businessspecific tasks on cybersecurity, creation of a list of competencies, formalized in educational standards and curricula, development of gaming scenarios for the formation of hard and soft skills, development of the scenario management system for AR interfaces. The system using AR tools can be easily adapted to different cybersecurity training activities. Industrial cyber-physical systems may be vulnerable due to insecure wireless connectivity, lack of encryption, inadequate access policy. The project-based learning complex is focused on the implementation of a data acquisition, storage and processing platform for new sensor networks and instruments. Representing all the diverse information on different layers will be greatly improved by use of the developed holographic projection AR tools.},
	language = {en},
	number = {1},
	urldate = {2023-09-13},
	journal = {Journal of Physics: Conference Series},
	author = {Skorenkyy, Yu and Kozak, R and Zagorodna, N and Kramar, O and Baran, I},
	month = mar,
	year = {2021},
	pages = {012026},
}

@article{noauthor_notitle_nodate-1,
}

@book{committee_on_21st_century_cyber-physical_systems_education_21st_2016,
	address = {Washington, D.C.},
	title = {A 21st {Century} {Cyber}-{Physical} {Systems} {Education}},
	isbn = {978-0-309-45163-5},
	url = {https://www.nap.edu/catalog/23686},
	urldate = {2023-09-13},
	publisher = {National Academies Press},
	collaborator = {{Committee on 21st Century Cyber-Physical Systems Education} and {Computer Science and Telecommunications Board} and {Division on Engineering and Physical Sciences} and {National Academies of Sciences, Engineering, and Medicine}},
	month = dec,
	year = {2016},
	doi = {10.17226/23686},
	keywords = {Computers and Information Technology--Internet and Networking, Computers and Information Technology--Policy, Reviews and Evaluations},
}

@inproceedings{rajamaki_industry-university_2018,
	title = {Industry-university collaboration on {IoT} cyber security education: {Academic} course: "{Resilience} of {Internet} of {Things} and cyber-physical systems"},
	shorttitle = {Industry-university collaboration on {IoT} cyber security education},
	doi = {10.1109/EDUCON.2018.8363477},
	abstract = {Internet of Things (IoT) and Cyber-Physical Systems (CPS) are an expansive heterogeneous field, where technologies are replaced faster than the engineering staff. As a consequence, traditions for education and graduation of engineers of IoT and CPS are not yet established. Cybersecurity is essential for new innovations, economic growth and public safety. On the other hand, Center for Cyber Safety and Education estimates that by 2022 there is a deficit of 350 000 cybersecurity specialists in the private sector in Europe. This paper has two contributions: First, it presents an example how an industry-university collaboration on IoT cyber security education could be organized. Then, the data be produced during the collaboration course is analyzed and compared to literature for answering to the main research question: How can future educational competences with regard to resilient cyber-physical systems be understood? Due to the multifaceted nature of the cybersecurity, previous development of degree programmes tend to synthesize either technical or societal subjects. Real multi-disciplinary synthesis has remained complicated. Our main conclusion is that future competences with regard to CPS are multidisciplinary including many industrial sectors and academic disciplines as well as multiple theories. On the other hand, traditional security thinking is not possible within new heavily interconnected systems of systems, and we should move towards resilience thinking.},
	booktitle = {2018 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	author = {Rajamäki, Jyri},
	month = apr,
	year = {2018},
	note = {ISSN: 2165-9567},
	keywords = {Computer security, Cyber-physical systems, Education, Internet of Things, Internet of things, Research and development, Resilience, cyber-physical systems, higher education, research and technological development, resilience},
	pages = {1969--1977},
}

@misc{seshia_explorations_nodate,
	title = {Explorations in {Cyber}-{Physical} {Systems} {Education}},
	url = {https://cacm.acm.org/magazines/2022/5/260355-explorations-in-cyber-physical-systems-education/fulltext?mobile=false},
	abstract = {Explorations in CPS education and related research projects over the past two decades unveil where CPS learning is headed.},
	language = {en},
	urldate = {2023-09-13},
	author = {Seshia, Sanjit A.},
}

@misc{noauthor_cpssec_nodate,
	title = {{CPSSEC} {\textbar} {Homeland} {Security}},
	url = {https://www.dhs.gov/science-and-technology/cpssec},
	urldate = {2023-09-13},
}

@inproceedings{salgado_cybersecurity_2021,
	address = {Florianópolis, Brazil},
	title = {Cybersecurity in {Aviation}: the {STPA}-{Sec} {Method} {Applied} to the {TCAS} {Security}},
	isbn = {978-1-66547-831-1},
	shorttitle = {Cybersecurity in {Aviation}},
	url = {https://ieeexplore.ieee.org/document/9672578/},
	doi = {10.1109/LADC53747.2021.9672578},
	urldate = {2023-09-13},
	booktitle = {2021 10th {Latin}-{American} {Symposium} on {Dependable} {Computing} ({LADC})},
	publisher = {IEEE},
	author = {Salgado, Mayara Lopes and De Sousa, Marcelo Santiago},
	month = nov,
	year = {2021},
	pages = {1--10},
}

@article{sahay_comparative_2022,
	title = {A {Comparative} {Risk} {Analysis} on {CyberShip} {System} with {STPA}-{Sec}, {STRIDE} and {CORAS}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2212.10830},
	doi = {10.48550/ARXIV.2212.10830},
	abstract = {The widespread use of software-intensive cyber systems in critical infrastructures such as ships (CyberShips) has brought huge benefits, yet it has also opened new avenues for cyber attacks to potentially disrupt operations. Cyber risk assessment plays a vital role in identifying cyber threats and vulnerabilities that can be exploited to compromise cyber systems. A number of methodologies have been proposed to carry out these analyses. This paper evaluates and compares the application of three risk assessment methodologies: system theoretic process analysis (STPA-Sec), STRIDE and CORAS for identifying threats and vulnerabilities in a CyberShip system. We specifically selected these three methodologies because they identify threats not only at the component level, but also threats or hazards caused due to the interaction between components, resulting in sets of threats identified with each methodology and relevant differences. Moreover, STPA-Sec which is a variant of the STPA is widely used for safety and security analysis of cyber physical systems (CPS); CORAS offers a framework to perform cyber risk assessment in a top-down approach that aligns with STPA-Sec; and STRIDE (Spoofing, Tampering, Repudiation, Information disclosure, Denial of Service, Elevation of Privilege) considers threat at the component level as well as during the interaction that is similar to STPA-Sec. As a result of this analysis, this paper highlights the pros and cons of these methodologies, illustrates areas of special applicability, and suggests that their complementary use as threats identified through STRIDE can be used as an input to CORAS and STPA-Sec to make these methods more structured.},
	urldate = {2023-09-13},
	author = {Sahay, Rishikesh and Estay, D. A. Sepulveda and Meng, Weizhi and Jensen, Christian D. and Barfod, Michael Bruhn},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Cryptography and Security (cs.CR), FOS: Computer and information sciences},
}

@techreport{national_institute_of_standards_and_technology_nist_2023,
	address = {Gaithersburg, MD},
	title = {The {NIST} {Cybersecurity} {Framework} 2.0},
	url = {https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.29.ipd.pdf},
	number = {NIST CSWP 29 ipd},
	urldate = {2023-09-10},
	institution = {National Institute of Standards and Technology},
	author = {National Institute of Standards {and} Technology},
	year = {2023},
	doi = {10.6028/NIST.CSWP.29.ipd},
	pages = {1--52},
}

@inproceedings{ladisa_sok_2023,
	title = {{SoK}: {Taxonomy} of {Attacks} on {Open}-{Source} {Software} {Supply} {Chains}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP46215.2023.10179304},
	abstract = {The widespread dependency on open-source software makes it a fruitful target for malicious actors, as demonstrated by recurring attacks. The complexity of today’s open-source supply chains results in a significant attack surface, giving attackers numerous opportunities to reach the goal of injecting malicious code into open-source artifacts that is then downloaded and executed by victims.This work proposes a general taxonomy for attacks on open-source supply chains, independent of specific programming languages or ecosystems, and covering all supply chain stages from code contributions to package distribution. Taking the form of an attack tree, it covers 107 unique vectors, linked to 94 real-world incidents, and mapped to 33 mitigating safeguards.User surveys conducted with 17 domain experts and 134 software developers positively validated the correctness, comprehensiveness and comprehensibility of the taxonomy, as well as its suitability for various use-cases. Survey participants also assessed the utility and costs of the identified safeguards, and whether they are used.},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Ladisa, Piergiorgio and Plate, Henrik and Martinez, Matias and Barais, Olivier},
	month = may,
	year = {2023},
	note = {ISSN: 2375-1207},
	keywords = {Attack, Costs, Malware, Open Source, Security, Software Supply Chain, Supply chains, Surveys, Taxonomy, Visualization},
	pages = {1509--1526},
}

@article{ibrahim_security_2018,
	title = {A security review of local government using {NIST} {CSF}: a case study},
	volume = {74},
	issn = {0920-8542, 1573-0484},
	shorttitle = {A security review of local government using {NIST} {CSF}},
	url = {http://link.springer.com/10.1007/s11227-018-2479-2},
	doi = {10.1007/s11227-018-2479-2},
	abstract = {Abstract
            Evaluating cyber security risk is a challenging task regardless of an organisation’s nature of business or size, however, an essential activity. This paper uses the National Institute of Standards and Technology (NIST) cyber security framework (CSF) to assess the cyber security posture of a local government organisation in Western Australia. Our approach enabled the quantification of risks for specific NIST CSF core functions and respective categories and allowed making recommendations to address the gaps discovered to attain the desired level of compliance. This has led the organisation to strategically target areas related to their people, processes, and technologies, thus mitigating current and future threats.},
	language = {en},
	number = {10},
	urldate = {2023-09-10},
	journal = {The Journal of Supercomputing},
	author = {Ibrahim, Ahmed and Valli, Craig and McAteer, Ian and Chaudhry, Junaid},
	month = oct,
	year = {2018},
	pages = {5171--5186},
}

@article{ibrahim_correction_2019,
	title = {Correction to: {A} security review of local government using {NIST} {CSF}: a case study},
	volume = {75},
	issn = {0920-8542, 1573-0484},
	shorttitle = {Correction to},
	url = {http://link.springer.com/10.1007/s11227-019-02972-w},
	doi = {10.1007/s11227-019-02972-w},
	language = {en},
	number = {9},
	urldate = {2023-09-10},
	journal = {The Journal of Supercomputing},
	author = {Ibrahim, Ahmed and Valli, Craig and McAteer, Ian and Chaudhry, Junaid},
	month = sep,
	year = {2019},
	pages = {6158--6158},
}

@incollection{nagar_case_2023,
	address = {Singapore},
	title = {A {Case} {Study} in the {Application} of {STPA}-sec and {CHASSIS} for {Socio}-{Technical} {Cyber} {Security} {Risk} {Management} in {Health} {Care} from {Developing} {Nations}},
	volume = {578},
	isbn = {978-981-19765-9-9 978-981-19766-0-5},
	url = {https://link.springer.com/10.1007/978-981-19-7660-5_33},
	language = {en},
	urldate = {2023-09-10},
	booktitle = {Intelligent {Sustainable} {Systems}},
	publisher = {Springer Nature Singapore},
	author = {Kaberuka, Joseph and Johnson, Christopher},
	editor = {Nagar, Atulya K. and Singh Jat, Dharm and Mishra, Durgesh Kumar and Joshi, Amit},
	year = {2023},
	doi = {10.1007/978-981-19-7660-5_33},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {383--390},
}

@techreport{wetzel_nice_2023,
	address = {Gaithersburg, MD},
	title = {{NICE} {Framework} {Competency} {Areas}:: {Preparing} a {Job}-{Ready} {Workforce}},
	shorttitle = {{NICE} {Framework} {Competency} {Areas}},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2023/NIST.IR.8355.pdf},
	number = {NIST IR 8355},
	urldate = {2023-09-10},
	institution = {National Institute of Standards and Technology},
	author = {Wetzel, Karen},
	year = {2023},
	doi = {10.6028/NIST.IR.8355},
	pages = {NIST IR 8355},
}

@techreport{petersen_workforce_2020,
	title = {Workforce {Framework} for {Cybersecurity} ({NICE} {Framework})},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-181r1.pdf},
	abstract = {This publication from the National Initiative for Cybersecurity Education (NICE)
            describes the Workforce Framework for Cybersecurity (NICE Framework), a fundamental
            reference for describing and sharing information about cybersecurity work. It expresses
            that work as Task statements and describes Knowledge and Skill statements that provide a
            foundation for learners including students, job seekers, and employees. The use of these
            statements helps students to develop skills, job seekers to demonstrate competencies,
            and employees to accomplish tasks. As a common, consistent lexicon that categorizes and
            describes cybersecurity work, the NICE Framework improves communication about how to
            identify, recruit, develop, and retain cybersecurity talent. The NICE Framework is a
            reference source from which organizations or sectors can develop additional publications
            or tools that meet their needs to define or provide guidance on different aspects of
            cybersecurity education, training, and workforce development.},
	urldate = {2023-09-10},
	institution = {National Institute of Standards and Technology},
	author = {Petersen, Rodney and Santos, Danielle and Smith, Matthew C. and Wetzel, Karen A. and Witte, Greg},
	month = nov,
	year = {2020},
	doi = {10.6028/NIST.SP.800-181r1},
}

@article{garbelini_greyhound_2022,
	title = {Greyhound: {Directed} {Greybox} {Wi}-{Fi} {Fuzzing}},
	volume = {19},
	issn = {1941-0018},
	shorttitle = {Greyhound},
	doi = {10.1109/TDSC.2020.3014624},
	abstract = {The recent rise in complex Wi-Fi vulnerabilities, such as KRACK and Dragonslayer, indicates the critical need for effective Wi-Fi protocol testing tools. In this article, we conceptualize, design and implement a directed fuzzing methodology named Greyhound that automatically tests the Wi-Fi client implementations against vulnerabilities such as crashes or non-compliant behaviors. Leveraging a holistic Wi-Fi protocol model, Greyhound directs the fuzzer in specific states of target Wi-Fi client. By exchanging mutated packets with a Wi-Fi client, Greyhound aims to induce the client to exhibit anomalous behaviors that badly deviate from Wi-Fi protocols. We have implemented Greyhound and evaluated it on a variety of real-world Wi-Fi clients, including smartphone, Raspberry Pi, IoT device microcontrollers and a medical device. Our evaluation indicates that Greyhound not only automatically discovers known vulnerabilities (including KRACK and Dragonslayer) that would require specialized verification otherwise, but, more importantly, it also has uncovered four new vulnerabilities in popular Wi-Fi client devices. All discovered vulnerabilities have been confirmed by manufacturers and they have been assigned three different common vulnerability exposure (CVE) IDs. We also win a bug bounty of 2,200 USD for discovering the security vulnerabilities. Furthermore, our evaluation with three existing Wi-Fi fuzz testing tools reveals that all such tools fail to discover any of the vulnerabilities (including crashes) uncovered by Greyhound. Last but not the least, we have deployed Greyhound to test the Wi-Fi client implementation on automotive head units. Greyhound automatically discovers KRACK, Dragonslayer and other anomalies in these Wi-Fi implementations. Such a real world try-out justifies the necessity and efficacy of Greyhound.},
	number = {2},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Garbelini, Matheus E. and Wang, Chundong and Chattopadhyay, Sudipta},
	month = mar,
	year = {2022},
	keywords = {Authentication, Communication system security, Fuzzing, Greybox fuzzing, Protocols, Tools, Wi-Fi client, Wireless communication, Wireless fidelity, software security},
	pages = {817--834},
}

@inproceedings{ren_z-fuzzer_2021,
	address = {New York, NY, USA},
	series = {{WiSec} '21},
	title = {Z-{Fuzzer}: device-agnostic fuzzing of {Zigbee} protocol implementation},
	isbn = {9781450383493},
	shorttitle = {Z-{Fuzzer}},
	url = {https://dl.acm.org/doi/10.1145/3448300.3468296},
	doi = {10.1145/3448300.3468296},
	abstract = {With the proliferation of the Internet of Things (IoT) devices, Zigbee is widely adopted as a resource-efficient wireless protocol. Recently, severe vulnerabilities in Zigbee protocol implementations have compromised IoT devices from different manufacturers. It becomes imperative to perform security testing on Zigbee protocol implementations. However, it is not a trivial task to apply the existing vulnerability detection techniques such as fuzzing to Zigbee protocol implementations. In particular, it remains a significant obstacle to deal with low-level hardware events. Many existing protocol fuzzing tools lack a proper execution environment for the Zigbee protocol, which communicates via a radio channel instead of the Internet. To bridge the above gap, we develop a device-agnostic fuzzing platform named Z-Fuzzer to detect security vulnerabilities in Zigbee protocol implementations. Z-Fuzzer provides a software simulation environment with pre-defined peripherals and hardware interrupts configurations to simulate Zigbee protocol execution on real IoT devices. We first extend the existing protocol fuzzing framework's capabilities with a proxy server to bridge communication with the Zigbee protocol execution. Second, we generate more high-quality test cases with code-coverage heuristics. We compare Z-Fuzzer with advanced protocol fuzzing tools, BooFuzz and Peach fuzzer, on top of Z-Fuzzer's simulation platform. Our results show that Z-Fuzzer can achieve higher code coverage in a mainstream Zigbee protocol implementation called Z-Stack. Z-Fuzzer has detected more vulnerabilities using fewer test cases than BooFuzz and Peach. Three of them have been assigned CVE IDs with high CVSS scores (7.5{\textasciitilde}8.2).},
	urldate = {2023-09-10},
	publisher = {Association for Computing Machinery},
	author = {Ren, Mengfei and Ren, Xiaolei and Feng, Huadong and Ming, Jiang and Lei, Yu},
	month = jun,
	year = {2021},
	keywords = {IoT network, Zigbee protocol, fuzzing},
	pages = {347--358},
}

@inproceedings{luo_bleem_2023,
	title = {Bleem: {Packet} {Sequence} {Oriented} {Fuzzing} for {Protocol} {Implementations}},
	isbn = {9781939133373},
	shorttitle = {Bleem},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/luo-zhengxiong},
	urldate = {2023-09-08},
	author = {Luo, Zhengxiong and Yu, Junze and Zuo, Feilong and Liu, Jianzhong and Jiang, Yu and Chen, Ting and Roychoudhury, Abhik and Sun, Jiaguang},
	year = {2023},
	pages = {4481--4498},
}

@inproceedings{zhao_seqfuzzer_2019,
	title = {{SeqFuzzer}: {An} {Industrial} {Protocol} {Fuzzing} {Framework} from a {Deep} {Learning} {Perspective}},
	shorttitle = {{SeqFuzzer}},
	doi = {10.1109/ICST.2019.00016},
	abstract = {Industrial networks are the cornerstone of modern industrial control systems. Performing security checks of industrial communication processes helps detect unknown risks and vulnerabilities. Fuzz testing is a widely used method for performing security checks that takes advantage of automation. However, there is a big challenge to carry out security checks on industrial network due to the increasing variety and complexity of industrial communication protocols. In this case, existing approaches usually take a long time to model the protocol for generating test cases, which is labor-intensive and time-consuming. This becomes even worse when the target protocol is stateful. To help in addressing this problem, we employed a deep learning model to learn the structures of protocol frames and deal with the temporal features of stateful protocols. We propose a fuzzing framework named SeqFuzzer which automatically learns the protocol frame structures from communication traffic and generates fake but plausible messages as test cases. For proving the usability of our approach, we applied SeqFuzzer to widely-used Ethernet for Control Automation Technology (EtherCAT) devices and successfully detected several security vulnerabilities.},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Zhao, Hui and Li, Zhihui and Wei, Hansheng and Shi, Jianqi and Huang, Yanhong},
	month = apr,
	year = {2019},
	note = {ISSN: 2159-4848},
	keywords = {Computer architecture, Data models, Decoding, Deep learning, Fuzzing, Protocols, Security, industrial safety, deep learning, vulnerability mining, self learning, fuzzing, EtherCAT},
	pages = {59--67},
}

@article{gilardi_chatgpt_2023,
	title = {{ChatGPT} {Outperforms} {Crowd}-{Workers} for {Text}-{Annotation} {Tasks}},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/2303.15056},
	doi = {10.1073/pnas.2305016120},
	abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
	number = {30},
	urldate = {2023-09-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	month = jul,
	year = {2023},
	note = {arXiv:2303.15056 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	pages = {e2305016120},
}

@article{bendler_competency_2023,
	title = {Competency {Models} for {Information} {Security} and {Cybersecurity} {Professionals}: {Analysis} of {Existing} {Work} and a {New} {Model}},
	volume = {23},
	issn = {1946-6226, 1946-6226},
	shorttitle = {Competency {Models} for {Information} {Security} and {Cybersecurity} {Professionals}},
	url = {https://dl.acm.org/doi/10.1145/3573205},
	doi = {10.1145/3573205},
	abstract = {Competency models are widely adopted frameworks that are used to improve human resource functions and education. However, the characteristics of competency models related to the information security and cybersecurity domains are not well understood. To bridge this gap, this study investigates the current state of competency models related to the security domain through qualitative content analysis. Additionally, based on the competency model analysis, an evidence-based competency model is proposed. Examining the content of 27 models, we found that the models can benefit target groups in many different ways, ranging from policymaking to performance management. Owing to their many uses, competency models can arguably help to narrow the skills gap from which the profession is suffering. Nonetheless, the models have their shortcomings. First, the models do not cover all of the topics specified by the Cybersecurity Body of Knowledge (i.e., no model is complete). Second, by omitting social, personal, and methodological competencies, many models reduce the competency profile of a security expert to professional competencies. Addressing the limitations of previous work, the proposed competency model provides a holistic view of the competencies required by security professionals for job achievement and can potentially benefit both the education system and the labor market. To conclude, the implications of the competency model analysis and use cases of the proposed model are discussed.},
	language = {en},
	number = {2},
	urldate = {2023-09-06},
	journal = {ACM Transactions on Computing Education},
	author = {Bendler, Daniel and Felderer, Michael},
	month = jun,
	year = {2023},
	pages = {1--33},
}

@article{noauthor_student_nodate,
	title = {Student {Misconceptions} about {Cybersecurity} {Concepts}: {Analysis} of {Think}-{Aloud} {Interviews}},
	issn = {24722707},
	abstract = {We conducted an observational study to document student misconceptions about cybersecurity using thematic analysis of 25 think-aloud interviews. By understanding patterns in student misconceptions, we provide a basis for developing rigorous evidence-based recommendations for improving teaching and assessment methods in cybersecurity and inform future research. This study is the first to explore student cognition and reasoning about cybersecurity. We interviewed students from three diverse institutions. During these interviews, students grappled with security scenarios designed to probe their understanding of cybersecurity, especially adversarial thinking. We analyzed student statements using a structured qualitative method, novice-led paired thematic analysis, to document patterns in student misconceptions and problematic reasoning that transcend institutions, scenarios, or demographics. Themes generated from this analysis describe a taxonomy of misconceptions but not their causes or remedies. Four themes emerged: overgeneralizations, conflated concepts, biases, and incorrect assumptions. Together, these themes reveal that students generally failed to grasp the complexity and subtlety of possible vulnerabilities, threats, risks, and mitigations, suggesting a need for instructional methods that engage students in reasoning about complex scenarios with an adversarial mindset. These findings can guide teachers’ attention during instruction and inform the development of cybersecurity assessment tools that enable cross-institutional assessments that measure the effectiveness of pedagogies.},
	language = {en},
	journal = {Journal of Cybersecurity Education Research and Practice},
}

@article{carreras_guzman_comparative_2021,
	title = {A {Comparative} {Study} of {STPA}-{Extension} and the {UFoI}-{E} {Method} for {Safety} and {Security} {Co}-analysis},
	volume = {211},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832021001745},
	doi = {10.1016/j.ress.2021.107633},
	language = {en},
	urldate = {2023-09-06},
	journal = {Reliability Engineering \& System Safety},
	author = {Carreras Guzman, Nelson H. and Zhang, Jin and Xie, Jing and Glomsrud, Jon Arne},
	month = jul,
	year = {2021},
	pages = {107633},
}

@techreport{pope_systemic_2021,
	title = {Systemic {Theoretic} {Process} {Analysis} ({STPA}) {Used} for {Cyber} {Security} and {Agile} {Software} {Development}},
	language = {English},
	urldate = {2023-09-06},
	institution = {Lawrence Livermore National Laboratory},
	author = {Pope, Gregory M.},
	month = mar,
	year = {2021},
	pages = {33},
}

@inproceedings{ricci_pestle_2021,
	address = {Vienna Austria},
	title = {{PESTLE} {Analysis} of {Cybersecurity} {Education}},
	isbn = {978-1-4503-9051-4},
	url = {https://dl.acm.org/doi/10.1145/3465481.3469184},
	doi = {10.1145/3465481.3469184},
	language = {en},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {ACM},
	author = {Ricci, Sara and Janout, Vladimir and Parker, Simon and Jerabek, Jan and Hajny, Jan and Chatzopoulou, Argyro and Badonnel, Remi},
	month = aug,
	year = {2021},
	pages = {1--8},
}

@inproceedings{weiss_reflective_2016,
	address = {Memphis Tennessee USA},
	title = {A {Reflective} {Approach} to {Assessing} {Student} {Performance} in {Cybersecurity} {Exercises}},
	isbn = {978-1-4503-3685-7},
	url = {https://dl.acm.org/doi/10.1145/2839509.2844646},
	doi = {10.1145/2839509.2844646},
	language = {en},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 47th {ACM} {Technical} {Symposium} on {Computing} {Science} {Education}},
	publisher = {ACM},
	author = {Weiss, Richard and Locasto, Michael E. and Mache, Jens},
	month = feb,
	year = {2016},
	pages = {597--602},
}

@inproceedings{svabensky_evaluating_2022,
	address = {Providence RI USA},
	title = {Evaluating {Two} {Approaches} to {Assessing} {Student} {Progress} in {Cybersecurity} {Exercises}},
	isbn = {978-1-4503-9070-5},
	url = {https://dl.acm.org/doi/10.1145/3478431.3499414},
	doi = {10.1145/3478431.3499414},
	language = {en},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 53rd {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Švábenský, Valdemar and Weiss, Richard and Cook, Jack and Vykopal, Jan and Čeleda, Pavel and Mache, Jens and Chudovský, Radoslav and Chattopadhyay, Ankur},
	month = feb,
	year = {2022},
	pages = {787--793},
}

@article{weiss_cybersecurity_2017,
	title = {Cybersecurity {Education} and {Assessment} in {EDURange}},
	volume = {15},
	issn = {1540-7993},
	url = {http://ieeexplore.ieee.org/document/7945208/},
	doi = {10.1109/MSP.2017.54},
	number = {3},
	urldate = {2023-09-06},
	journal = {IEEE Security \& Privacy},
	author = {Weiss, Richard and Turbak, Franklyn and Mache, Jens and Locasto, Michael E.},
	year = {2017},
	pages = {90--95},
}

@article{hajny_framework_2021,
	title = {Framework, {Tools} and {Good} {Practices} for {Cybersecurity} {Curricula}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9469727/},
	doi = {10.1109/ACCESS.2021.3093952},
	urldate = {2023-09-06},
	journal = {IEEE Access},
	author = {Hajny, Jan and Ricci, Sara and Piesarskas, Edmundas and Levillain, Olivier and Galletta, Letterio and De Nicola, Rocco},
	year = {2021},
	pages = {94723--94747},
}

@inproceedings{straub_assessment_2020,
	address = {Virtual On line},
	title = {Assessment of {Cybersecurity} {Competition} {Teams} as {Experiential} {Education} {Exercises}},
	url = {http://peer.asee.org/34187},
	doi = {10.18260/1-2--34187},
	urldate = {2023-09-06},
	booktitle = {2020 {ASEE} {Virtual} {Annual} {Conference} {Content} {Access} {Proceedings}},
	publisher = {ASEE Conferences},
	author = {Straub, Jeremy},
	month = jun,
	year = {2020},
	pages = {34187},
}

@inproceedings{bell_developing_2015,
	address = {Seattle, Washington},
	title = {Developing and {Piloting} a {Quantitative} {Assessment} {Tool} for {Cybersecurity} {Courses}},
	url = {http://peer.asee.org/23835},
	doi = {10.18260/p.23835},
	urldate = {2023-09-06},
	booktitle = {2015 {ASEE} {Annual} {Conference} and {Exposition} {Proceedings}},
	publisher = {ASEE Conferences},
	author = {Bell, Richard and Vasserman, Eugene and Sayre, Eleanor},
	month = jun,
	year = {2015},
	pages = {26.496.1--26.496.13},
}

@inproceedings{anjum_uncovering_2023,
	title = {Uncovering {Software} {Supply} {Chains} {Vulnerability}: {A} {Review} of {Attack} {Vectors}, {Stakeholders}, and {Regulatory} {Frameworks}},
	shorttitle = {Uncovering {Software} {Supply} {Chains} {Vulnerability}},
	doi = {10.1109/COMPSAC57700.2023.00281},
	abstract = {The proliferation of cyberattacks in the software supply chain (SSC) domain is a critical concern making them a formidable threat to software security and compromising its integrity and credibility which needs to be seriously acknowledged and investigated. This paper aims to conduct a comprehensive study of the various tactics and techniques employed by cybercriminals in this domain along with a focus on exploring the effect of software supply chain stakeholders’ traits, limitations, and actions on the probability of a successful attack. Furthermore, this research also identifies the regulatory tools and protocols administrating software supply chains that support decreasing an organization’s proneness to these challenges. Our study adopts a rigorous methodology to investigate the frequency of attacks, current defense techniques, and gaps combined with an overview highlighting where ransomware attacks occur amidst this discussion. The findings will provide valuable insights concerning the recent trends in disrupting the security and efficiency of the software supply chain and offer recommendations to researchers, organizations, and practitioners to remain cautious and proactive in their cybersecurity posture.},
	booktitle = {2023 {IEEE} 47th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Anjum, Nafisa and Sakib, Nazmus and Rodriguez-Cardenas, Juanjose and Brookins, Corey and Norouzinia, Ava and Shavers, Asia and Dominguez, Miranda and Nassif, Marie and Shahriar, Hossain},
	month = jun,
	year = {2023},
	note = {ISSN: 0730-3157},
	keywords = {Market research, Organizations, Protocols, Ransomware, Ransomware Attacks, Regulatory frameworks, Security, Software, Software Supply Chain Attacks, Stakeholders, Supply chains},
	pages = {1816--1821},
}

@misc{duan_towards_2020,
	title = {Towards {Measuring} {Supply} {Chain} {Attacks} on {Package} {Managers} for {Interpreted} {Languages}},
	url = {http://arxiv.org/abs/2002.01139},
	abstract = {Package managers have become a vital part of the modern software development process. They allow developers to reuse third-party code, share their own code, minimize their codebase, and simplify the build process. However, recent reports showed that package managers have been abused by attackers to distribute malware, posing significant security risks to developers and end-users. For example, eslint-scope, a package with millions of weekly downloads in Npm, was compromised to steal credentials from developers. To understand the security gaps and the misplaced trust that make recent supply chain attacks possible, we propose a comparative framework to qualitatively assess the functional and security features of package managers for interpreted languages. Based on qualitative assessment, we apply well-known program analysis techniques such as metadata, static, and dynamic analysis to study registry abuse. Our initial efforts found 339 new malicious packages that we reported to the registries for removal. The package manager maintainers confirmed 278 (82\%) from the 339 reported packages where three of them had more than 100,000 downloads. For these packages we were issued official CVE numbers to help expedite the removal of these packages from infected victims. We outline the challenges of tailoring program analysis tools to interpreted languages and release our pipeline as a reference point for the community to build on and help in securing the software supply chain.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Duan, Ruian and Alrawi, Omar and Kasturi, Ranjita Pai and Elder, Ryan and Saltaformaggio, Brendan and Lee, Wenke},
	month = dec,
	year = {2020},
	note = {arXiv:2002.01139 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{duan_towards_2020-1,
	title = {Towards {Measuring} {Supply} {Chain} {Attacks} on {Package} {Managers} for {Interpreted} {Languages}},
	url = {http://arxiv.org/abs/2002.01139},
	abstract = {Package managers have become a vital part of the modern software development process. They allow developers to reuse third-party code, share their own code, minimize their codebase, and simplify the build process. However, recent reports showed that package managers have been abused by attackers to distribute malware, posing significant security risks to developers and end-users. For example, eslint-scope, a package with millions of weekly downloads in Npm, was compromised to steal credentials from developers. To understand the security gaps and the misplaced trust that make recent supply chain attacks possible, we propose a comparative framework to qualitatively assess the functional and security features of package managers for interpreted languages. Based on qualitative assessment, we apply well-known program analysis techniques such as metadata, static, and dynamic analysis to study registry abuse. Our initial efforts found 339 new malicious packages that we reported to the registries for removal. The package manager maintainers confirmed 278 (82\%) from the 339 reported packages where three of them had more than 100,000 downloads. For these packages we were issued official CVE numbers to help expedite the removal of these packages from infected victims. We outline the challenges of tailoring program analysis tools to interpreted languages and release our pipeline as a reference point for the community to build on and help in securing the software supply chain.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Duan, Ruian and Alrawi, Omar and Kasturi, Ranjita Pai and Elder, Ryan and Saltaformaggio, Brendan and Lee, Wenke},
	month = dec,
	year = {2020},
	note = {arXiv:2002.01139 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{sherman_cats_2019,
	title = {The {CATS} {Hackathon}: {Creating} and {Refining} {Test} {Items} for {Cybersecurity} {Concept} {Inventories}},
	shorttitle = {The {CATS} {Hackathon}},
	url = {http://arxiv.org/abs/1901.09286},
	abstract = {For two days in February 2018, 17 cybersecurity educators and professionals from government and industry met in a "hackathon" to refine existing draft multiple-choice test items, and to create new ones, for a Cybersecurity Concept Inventory (CCI) and Cybersecurity Curriculum Assessment (CCA) being developed as part of the Cybersecurity Assessment Tools (CATS) Project. We report on the results of the CATS Hackathon, discussing the methods we used to develop test items, highlighting the evolution of a sample test item through this process, and offering suggestions to others who may wish to organize similar hackathons. Each test item embodies a scenario, question stem, and five answer choices. During the Hackathon, participants organized into teams to (1) Generate new scenarios and question stems, (2) Extend CCI items into CCA items, and generate new answer choices for new scenarios and stems, and (3) Review and refine draft CCA test items. The CATS Project provides rigorous evidence-based instruments for assessing and evaluating educational practices; these instruments can help identify pedagogies and content that are effective in teaching cybersecurity. The CCI measures how well students understand basic concepts in cybersecurity---especially adversarial thinking---after a first course in the field. The CCA measures how well students understand core concepts after completing a full cybersecurity curriculum.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Sherman, Alan T. and Oliva, Linda and Golaszewski, Enis and Phatak, Dhananjay and Scheponik, Travis and Herman, Geoffrey L. and Choi, Dong San and Offenberger, Spencer E. and Peterson, Peter and Dykstra, Josiah and Bard, Gregory V. and Chattopadhyay, Ankur and Sharevski, Filipo and Verma, Rakesh and Vrecenar, Ryan},
	month = jan,
	year = {2019},
	note = {arXiv:1901.09286 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{sherman_creating_2017,
	title = {Creating a {Cybersecurity} {Concept} {Inventory}: {A} {Status} {Report} on the {CATS} {Project}},
	shorttitle = {Creating a {Cybersecurity} {Concept} {Inventory}},
	url = {http://arxiv.org/abs/1706.05092},
	abstract = {We report on the status of our Cybersecurity Assessment Tools (CATS) project that is creating and validating a concept inventory for cybersecurity, which assesses the quality of instruction of any first course in cybersecurity. In fall 2014, we carried out a Delphi process that identified core concepts of cybersecurity. In spring 2016, we interviewed twenty-six students to uncover their understandings and misconceptions about these concepts. In fall 2016, we generated our first assessment tool--a draft Cybersecurity Concept Inventory (CCI), comprising approximately thirty multiple-choice questions. Each question targets a concept; incorrect answers are based on observed misconceptions from the interviews. This year we are validating the draft CCI using cognitive interviews, expert reviews, and psychometric testing. In this paper, we highlight our progress to date in developing the CCI. The CATS project provides infrastructure for a rigorous evidence-based improvement of cybersecurity education. The CCI permits comparisons of different instructional methods by assessing how well students learned the core concepts of the field (especially adversarial thinking), where instructional methods refer to how material is taught (e.g., lab-based, case-studies, collaborative, competitions, gaming). Specifically, the CCI is a tool that will enable researchers to scientifically quantify and measure the effect of their approaches to, and interventions in, cybersecurity education.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Sherman, Alan T. and Oliva, Linda and DeLatte, David and Golaszewski, Enis and Neary, Michael and Patsourakos, Konstantinos and Phatak, Dhananjay and Scheponik, Travis and Herman, Geoffrey L. and Thompson, Julia},
	month = jun,
	year = {2017},
	note = {arXiv:1706.05092 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{sherman_experiences_2020,
	title = {Experiences and {Lessons} {Learned} {Creating} and {Validating} {Concept} {Inventories} for {Cybersecurity}},
	url = {http://arxiv.org/abs/2004.05248},
	abstract = {We reflect on our ongoing journey in the educational Cybersecurity Assessment Tools (CATS) Project to create two concept inventories for cybersecurity. We identify key steps in this journey and important questions we faced. We explain the decisions we made and discuss the consequences of those decisions, highlighting what worked well and what might have gone better. The CATS Project is creating and validating two concept inventories---conceptual tests of understanding---that can be used to measure the effectiveness of various approaches to teaching and learning cybersecurity. The Cybersecurity Concept Inventory (CCI) is for students who have recently completed any first course in cybersecurity; the Cybersecurity Curriculum Assessment (CCA) is for students who have recently completed an undergraduate major or track in cybersecurity. Each assessment tool comprises 25 multiple-choice questions (MCQs) of various difficulties that target the same five core concepts, but the CCA assumes greater technical background. Key steps include defining project scope, identifying the core concepts, uncovering student misconceptions, creating scenarios, drafting question stems, developing distractor answer choices, generating educational materials, performing expert reviews, recruiting student subjects, organizing workshops, building community acceptance, forming a team and nurturing collaboration, adopting tools, and obtaining and using funding. Creating effective MCQs is difficult and time-consuming, and cybersecurity presents special challenges. Because cybersecurity issues are often subtle, where the adversarial model and details matter greatly, it is challenging to construct MCQs for which there is exactly one best but non-obvious answer. We hope that our experiences and lessons learned may help others create more effective concept inventories and assessments in STEM.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Sherman, Alan T. and Herman, Geoffrey L. and Oliva, Linda and Peterson, Peter A. H. and Golaszewski, Enis and Poulsen, Seth and Scheponik, Travis and Gorti, Akshita},
	month = apr,
	year = {2020},
	note = {arXiv:2004.05248 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{noauthor_cybersecurity_nodate-1,
	title = {Cybersecurity: {Exploring} core concepts through six scenarios},
	shorttitle = {Cybersecurity},
	url = {https://www.tandfonline.com/doi/epdf/10.1080/01611194.2017.1362063?needAccess=true&role=button},
	language = {en},
	urldate = {2023-09-05},
	note = {ISSN: 0161-1194},
}

@article{poulsen_psychometric_2022,
	title = {Psychometric {Evaluation} of the {Cybersecurity} {Concept} {Inventory}},
	volume = {22},
	issn = {1946-6226, 1946-6226},
	url = {https://dl.acm.org/doi/10.1145/3451346},
	doi = {10.1145/3451346},
	abstract = {We present a psychometric evaluation of a revised version of the
              Cybersecurity Concept Inventory (CCI)
              , completed by 354 students from 29 colleges and universities. The CCI is a conceptual test of understanding created to enable research on instruction quality in cybersecurity education. This work extends previous expert review and small-scale pilot testing of the CCI. Results show that the CCI aligns with a curriculum many instructors expect from an introductory cybersecurity course, and that it is a valid and reliable tool for assessing what conceptual cybersecurity knowledge students learned.},
	language = {en},
	number = {1},
	urldate = {2023-09-05},
	journal = {ACM Transactions on Computing Education},
	author = {Poulsen, Seth and Herman, Geoffrey L. and Peterson, Peter A. H. and Golaszewski, Enis and Gorti, Akshita and Oliva, Linda and Scheponik, Travis and Sherman, Alan T.},
	month = mar,
	year = {2022},
	pages = {1--18},
}

@inproceedings{herman_psychometric_2023,
	address = {Toronto ON Canada},
	title = {Psychometric {Evaluation} of the {Cybersecurity} {Curriculum} {Assessment}},
	isbn = {978-1-4503-9431-4},
	url = {https://dl.acm.org/doi/10.1145/3545945.3569762},
	doi = {10.1145/3545945.3569762},
	language = {en},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 54th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education} {V}. 1},
	publisher = {ACM},
	author = {Herman, Geoffrey L. and Huang, Shan and Peterson, Peter A. and Oliva, Linda and Golaszewski, Enis and Sherman, Alan T.},
	month = mar,
	year = {2023},
	pages = {228--234},
}

@article{malaivongs_cyber_2022,
	title = {Cyber {Trust} {Index}: {A} {Framework} for {Rating} and {Improving} {Cybersecurity} {Performance}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Cyber {Trust} {Index}},
	url = {https://www.mdpi.com/2076-3417/12/21/11174},
	doi = {10.3390/app122111174},
	abstract = {Cybersecurity risk is among the top risks that every organization must consider and manage, especially during this time wherein technology has become an integral part of our lives; however, there is no efficient and simplified measurement method that organizations or regulators could use, as frequently as they need, to evaluate and compare the outcome of cybersecurity efforts that have been put in place. Consequently, this has resulted in an absence of critical data for cybersecurity improvement. This research proposes a Cyber Trust Index (CTI), a novel and simplified framework for evaluating, benchmarking, and improving organizations’ cybersecurity performance. Methods: The researchers analyzed prominent scientific research papers and widely used security standards to develop baseline security controls that serve as a measurement foundation. Then, they identified Control Enablers and Capability Tiers that were used as base measures and measurement methods. The CTI framework was evaluated by experts and tested with 35 organizations from the critical information infrastructure (CII) sector, as well as other generic sectors, in Thailand to confirm its validity and reliability in real organization settings and identify the priorities and factors that can contribute to better cybersecurity performance. Results: The CTI has two key elements: the baseline controls and rating methods. The baseline controls comprise 12 dimensions, 25 clusters, and 70 controls. The rating methods utilize five control enablers and five capability tiers to compute scores. A binary questionnaire is used to capture data for the rating process. Based on a statistical analysis of CTI results from 35 pilot organizations, 28.57\% are in the beginner group with high-risk exposure, 31.43\% are in the leader group with low-risk exposure, and 40\% of organizations are in between (the intermediate and advanced groups). Two key factors distinguish between the beginner and leader groups: (1) an internal factor, which is the Control Enablers; and (2) an external factor, which is the influence of a cyber regulating body. Our study confirms that Control Enablers in higher Tiers will help organizations achieve better cybersecurity performance (R = 0.98021) and highlights the significance of cyber regulating bodies by showing a shear difference of 197.53\% in cyber performance between highly regulated and low-regulated industries. Conclusions: This research reveals key insights into the importance of Control Enablers, which are the internal factors that organizations must leverage to drive better cybersecurity performance, and the positive return on enforcement, which emphasizes the need for cyber regulating bodies. The CTI framework has proven to be valid and efficient for measuring cybersecurity performance. At the very least, a step-wise roadmap is provided for organizations and regulators to adopt and adapt the CTI framework for their cybersecurity measurement and improvement mission.},
	language = {en},
	number = {21},
	urldate = {2023-09-05},
	journal = {Applied Sciences},
	author = {Malaivongs, Sasawat and Kiattisin, Supaporn and Chatjuthamard, Pattanaporn},
	month = jan,
	year = {2022},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {control enabler, cyber resilience, cyber trust index, cybersecurity performance measurement, cybersecurity rating},
	pages = {11174},
}

@article{nilsen_developmental_2017,
	title = {A {Developmental} {Study} on {Assessing} the {Cybersecurity} {Competency} of {Organizational} {Information} {System} {Users}},
	url = {https://digitalcommons.kennesaw.edu/ccerp/2017/research/1},
	journal = {KSU Proceedings on Cybersecurity Education, Research and Practice},
	author = {Nilsen, Richard and Levy, Yair and Terrell, Steven and Beyer, Dawn},
	month = oct,
	year = {2017},
}

@article{goode_expert_2018,
	title = {Expert assessment of organizational cybersecurity programs and development of vignettes to measure cybersecurity countermeasures awareness},
	volume = {6},
	issn = {2325-4688},
	url = {http://www.iiakm.org/ojakm/articles/2018/OJAKM_Volume6_1pp67-80.php},
	doi = {10.36965/OJAKM.2018.6(1)67-80},
	abstract = {As organizational reliance on technology increases, cybersecurity attacks become more attractive to attackers and increasingly devastating to organizations. Due to lacking knowledge and skills, humans are often considered the most susceptible threat vector for cyber attacks. Previous studies in information systems (IS) literature have confirmed awareness techniques to be the first step in increasing employee cybersecurity-related knowledge, promoting security conscious decision-making, and the prevention of naive IS security behaviors. While training initiatives exist within many organizations, there appears to be a limited number of empirical research studies that focus on what security education, training, and awareness (SETA) programs should encompass. This includes topics to be covered, the most valuable method for delivery, and to what degree these factors play a part in the IS security practice of employees. The aim of this study was to use subject-matter experts (SMEs) to validate: 1) the key topics needed for two SETA program types (typical \& socio-technical), 2) the measurement criteria for employees’ cybersecurity countermeasures awareness (CCA), 3) weights for the three CCA categories (awareness of policy, SETA, \& monitoring) in the overall CCA measure, and 4) two SETA programs content with integrated vignette-based assessments for CCA. A Delphi methodology was utilized to gather feedback from 21 SMEs regarding cybersecurity topics for organizational SETA programs, validation of SETA training content, and to develop a vignette based measure of CCA. Results show that awareness of the organizational cybersecurity policy was the most important category for the overall CCA measure with 41\%, followed by awareness of SETA program content, with 34\%, while awareness of monitoring was 25\%. The paper concludes with discussions and future research agenda.},
	language = {en},
	number = {1},
	urldate = {2023-09-05},
	journal = {Online Journal of Applied Knowledge Management},
	author = {Goode, Jodi and Levy, Yair and Hovav, Anat and Smith, James},
	month = apr,
	year = {2018},
	pages = {67--80},
}

@article{workman_study_2022,
	title = {A {Study} of {Cybersecurity} {Education} {Using} a {Present}-{Test}-{Practice}-{Assess} {Model}},
	volume = {65},
	issn = {0018-9359, 1557-9638},
	url = {https://ieeexplore.ieee.org/document/9452797/},
	doi = {10.1109/TE.2021.3086025},
	number = {1},
	urldate = {2023-09-05},
	journal = {IEEE Transactions on Education},
	author = {Workman, Michael D. and Luevanos, J. Anthony and Mai, Bin},
	month = feb,
	year = {2022},
	pages = {40--45},
}

@inproceedings{kula_visualizing_2014,
	title = {Visualizing the {Evolution} of {Systems} and {Their} {Library} {Dependencies}},
	doi = {10.1109/VISSOFT.2014.29},
	abstract = {System maintainers face several challenges stemming from a system and its library dependencies evolving separately. Novice maintainers may lack the historical knowledge required to efficiently manage an inherited system. While some libraries are regularly updated, some systems keep a dependency on older versions. On the other hand, maintainers may be unaware that other systems have settled on a different version of a library. In this paper, we visualize how the dependency relation between a system and its dependencies evolves from two perspectives. Our system-centric dependency plots (SDP) visualize the successive library versions a system depends on over time. The radial layout and heat-map metaphor provide visual clues about the change in dependencies along the system's release history. From this perspective, maintainers can navigate to a library-centric dependants diffusion plot (LDP). The LDP is a time-series visualization that shows the diffusion of users across the different versions of a library. We demonstrate on real-world systems how maintainers can benefit from our visualizations through four case scenarios.},
	booktitle = {2014 {Second} {IEEE} {Working} {Conference} on {Software} {Visualization}},
	author = {Kula, Raula Gaikovina and De Roover, Coen and German, Daniel and Ishio, Takashi and Inoue, Katsuro},
	month = sep,
	year = {2014},
	keywords = {Color, Evolution (biology), Layout, Libraries, Shape, Software, Software Evolution, Software Maintenance, Software Reuse, Visualization},
	pages = {127--136},
}

@misc{decan_evolution_2018,
	title = {On the evolution of technical lag in the npm package dependency network},
	url = {http://arxiv.org/abs/1806.01545},
	abstract = {Software packages developed and distributed through package managers extensively depend on other packages. These dependencies are regularly updated, for example to add new features, resolve bugs or fix security issues. In order to take full advantage of the benefits of this type of reuse, developers should keep their dependencies up to date by relying on the latest releases. In practice, however, this is not always possible, and packages lag behind with respect to the latest version of their dependencies. This phenomenon is described as technical lag in the literature. In this paper, we perform an empirical study of technical lag in the npm dependency network by investigating its evolution for over 1.4M releases of 120K packages and 8M dependencies between these releases. We explore how technical lag increases over time, taking into account the release type and the use of package dependency constraints. We also discuss how technical lag can be reduced by relying on the semantic versioning policy.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
	month = jun,
	year = {2018},
	note = {arXiv:1806.01545 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{parekh_identifying_2018,
	title = {Identifying {Core} {Concepts} of {Cybersecurity}: {Results} of {Two} {Delphi} {Processes}},
	volume = {61},
	issn = {1557-9638},
	shorttitle = {Identifying {Core} {Concepts} of {Cybersecurity}},
	doi = {10.1109/TE.2017.2715174},
	abstract = {This paper presents and analyzes results of two Delphi processes that polled cybersecurity experts to rate cybersecurity topics based on importance, difficulty, and timelessness. These ratings can be used to identify core concepts–cross-cutting ideas that connect knowledge in the discipline. The first Delphi process identified core concepts that should be learned in any first course on cybersecurity. The second identified core concepts that any cybersecurity professional should know upon graduating from college. Despite the rapidly growing demand for cybersecurity professionals, it is not clear what defines foundational cybersecurity knowledge. Initial data from the Delphi processes lay a foundation for defining the core concepts of the field and, consequently, provide a common starting point to accelerate the development of rigorous cybersecurity education practices. These results provide a foundation for developing evidence-based educational cybersecurity assessment tools that will identify and measure effective methods for teaching cybersecurity. The Delphi results can also be used to inform the development of curricula, learning exercises, and other educational materials and policies.},
	number = {1},
	journal = {IEEE Transactions on Education},
	author = {Parekh, Geet and DeLatte, David and Herman, Geoffrey L. and Oliva, Linda and Phatak, Dhananjay and Scheponik, Travis and Sherman, Alan T.},
	month = feb,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Education},
	keywords = {Cats, Computer science, Computer security, Concept inventory, Delphi process, Education, Engineering profession, Tools, assessment tools, conceptual learning, cybersecurity, cybersecurity assessment tools (CATS), information assurance, student assessment},
	pages = {11--20},
}

@article{parekh_identifying_2018-1,
	title = {Identifying {Core} {Concepts} of {Cybersecurity}: {Results} of {Two} {Delphi} {Processes}},
	volume = {61},
	issn = {1557-9638},
	shorttitle = {Identifying {Core} {Concepts} of {Cybersecurity}},
	doi = {10.1109/TE.2017.2715174},
	abstract = {This paper presents and analyzes results of two Delphi processes that polled cybersecurity experts to rate cybersecurity topics based on importance, difficulty, and timelessness. These ratings can be used to identify core concepts–cross-cutting ideas that connect knowledge in the discipline. The first Delphi process identified core concepts that should be learned in any first course on cybersecurity. The second identified core concepts that any cybersecurity professional should know upon graduating from college. Despite the rapidly growing demand for cybersecurity professionals, it is not clear what defines foundational cybersecurity knowledge. Initial data from the Delphi processes lay a foundation for defining the core concepts of the field and, consequently, provide a common starting point to accelerate the development of rigorous cybersecurity education practices. These results provide a foundation for developing evidence-based educational cybersecurity assessment tools that will identify and measure effective methods for teaching cybersecurity. The Delphi results can also be used to inform the development of curricula, learning exercises, and other educational materials and policies.},
	number = {1},
	journal = {IEEE Transactions on Education},
	author = {Parekh, Geet and DeLatte, David and Herman, Geoffrey L. and Oliva, Linda and Phatak, Dhananjay and Scheponik, Travis and Sherman, Alan T.},
	month = feb,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Education},
	keywords = {Cats, Computer science, Computer security, Concept inventory, Delphi process, Education, Engineering profession, Tools, assessment tools, conceptual learning, cybersecurity, cybersecurity assessment tools (CATS), information assurance, student assessment},
	pages = {11--20},
}

@misc{noauthor_ffiec_nodate,
	title = {{FFIEC} {Cybersecurity} {Awareness}},
	url = {https://www.ffiec.gov/cyberassessmenttool.htm},
	urldate = {2023-09-05},
}

@misc{noauthor_review_nodate,
	title = {Review the {FFIEC} {Cybersecurity} {Assessment} {Tool} ({CAT}) {\textbar} {Mass}.gov},
	url = {https://www.mass.gov/info-details/review-the-ffiec-cybersecurity-assessment-tool-cat},
	abstract = {Identify your financial institution's risks and cybersecurity preparedness using the Federal Financial Institutions Examination Council (FFIEC) Cybersecurity Assessment Tool.},
	language = {en},
	urldate = {2023-09-05},
}

@inproceedings{herman_psychometric_2023-1,
	address = {Toronto ON Canada},
	title = {Psychometric {Evaluation} of the {Cybersecurity} {Curriculum} {Assessment}},
	isbn = {978-1-4503-9431-4},
	url = {https://dl.acm.org/doi/10.1145/3545945.3569762},
	doi = {10.1145/3545945.3569762},
	language = {en},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 54th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education} {V}. 1},
	publisher = {ACM},
	author = {Herman, Geoffrey L. and Huang, Shan and Peterson, Peter A. and Oliva, Linda and Golaszewski, Enis and Sherman, Alan T.},
	month = mar,
	year = {2023},
	pages = {228--234},
}

@inproceedings{swain_cybersecurity_2022,
	title = {Cybersecurity for {Everybody} - {A} {Multi}-{Tier} {Approach} to {Cyber} {Security} {Education}, {Training}, and {Awareness} in the {Undergraduate} {Curriculum}},
	url = {https://peer.asee.org/cybersecurity-for-everybody-a-multi-tier-approach-to-cyber-security-education-training-and-awareness-in-the-undergraduate-curriculum},
	urldate = {2023-09-05},
	author = {Swain, Nikunja},
	month = aug,
	year = {2022},
}

@inproceedings{svabensky_toolset_2021,
	title = {Toolset for {Collecting} {Shell} {Commands} and {Its} {Application} in {Hands}-on {Cybersecurity} {Training}},
	doi = {10.1109/FIE49875.2021.9637052},
	abstract = {This Full Paper in the Innovative Practice category presents and evaluates a technical innovation for hands-on classes. When learning cybersecurity, operating systems, or networking, students perform practical tasks using a broad range of command-line tools. Collecting and analyzing data about the command usage can reveal valuable insights into how students progress and where they make mistakes. However, few learning environments support recording and inspecting command-line inputs, and setting up an efficient infrastructure for this purpose is challenging. To aid engineering and computing educators, we share the design and implementation of an open-source toolset for logging commands that students execute on Linux machines. Compared to basic solutions, such as shell history files, the toolset's novelty and added value are threefold. First, its configuration is automated so that it can be easily used in classes on different topics. Second, it collects metadata about the command execution, such as a timestamp, hostname, and IP address. Third, all data are instantly forwarded to central storage in a unified, semi-structured format. This enables automated processing of the data, both in real-time and post hoc, to enhance the instructors' understanding of student actions. The toolset works independently of the teaching content, the training network's topology, or the number of students working in parallel. We demonstrated the toolset's value in two learning environments at four training sessions. Over two semesters, 50 students played educational cybersecurity games using a Linux command-line interface. Each training session lasted approximately two hours, during which we recorded 4439 shell commands. The semiautomated data analysis revealed different solution patterns, used tools, and misconceptions of students. Our insights from creating the toolset and applying it in teaching practice are relevant for instructors, researchers, and developers of learning environments. We provide the software and data resulting from this work so that others can use them in their hands-on classes.},
	booktitle = {2021 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Švábenský, Valdemar and Vykopal, Jan and Tovarňák, Daniel and Čeleda, Pavel},
	month = oct,
	year = {2021},
	note = {ISSN: 2377-634X},
	keywords = {Computer security, Linux, Operating systems, Pandemics, Real-time systems, Syslog, Technological innovation, Training, command-line history, cybersecurity education, educational data mining, host-based monitoring, learning analytics, learning technology, sandbox, virtual machines},
	pages = {1--9},
}

@article{decan_empirical_2019,
	title = {An empirical comparison of dependency network evolution in seven software packaging ecosystems},
	volume = {24},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-017-9589-y},
	doi = {10.1007/s10664-017-9589-y},
	abstract = {Nearly every popular programming language comes with one or more package managers. The software packages distributed by such package managers form large software ecosystems. These packaging ecosystems contain a large number of package releases that are updated regularly and that have many dependencies to other package releases. While packaging ecosystems are extremely useful for their respective communities of developers, they face challenges related to their scale, complexity, and rate of evolution. Typical problems are backward incompatible package updates, and the risk of (transitively) depending on packages that have become obsolete or inactive. This manuscript uses the libraries.io dataset to carry out a quantitative empirical analysis of the similarities and differences between the evolution of package dependency networks for seven packaging ecosystems of varying sizes and ages: Cargo for Rust, CPAN for Perl, CRAN for R, npm for JavaScript, NuGet for the .NET platform, Packagist for PHP, and RubyGems for Ruby. We propose novel metrics to capture the growth, changeability, reusability and fragility of these dependency networks, and use these metrics to analyze and compare their evolution. We observe that the dependency networks tend to grow over time, both in size and in number of package updates, while a minority of packages are responsible for most of the package updates. The majority of packages depend on other packages, but only a small proportion of packages accounts for most of the reverse dependencies. We observe a high proportion of “fragile” packages due to a high and increasing number of transitive dependencies. These findings are instrumental for assessing the quality of a package dependency network, and improving it through dependency management tools and imposed policies.},
	language = {en},
	number = {1},
	urldate = {2023-09-05},
	journal = {Empirical Software Engineering},
	author = {Decan, Alexandre and Mens, Tom and Grosjean, Philippe},
	month = feb,
	year = {2019},
	keywords = {Dependency network, Package manager, Software ecosystem, Software evolution, Software repository mining},
	pages = {381--416},
}

@misc{noauthor_life_nodate,
	title = {The {Life} {Cycles} of {Open} {Source} {Projects} - {Linux} {Foundation}},
	url = {https://lfx.linuxfoundation.org/blog/the-life-cycles-of-open-source-projects/},
	abstract = {This article helps you look at each of the project life cycle stages, determine where your project is, and, at a high-level, show how you can move your project successfully through each stage.},
	language = {en},
	urldate = {2023-09-05},
}

@article{zerouali_formal_2019,
	title = {A formal framework for measuring technical lag in component repositories — and its application to npm},
	volume = {31},
	copyright = {© 2019 John Wiley \& Sons, Ltd.},
	issn = {2047-7481},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2157},
	doi = {10.1002/smr.2157},
	abstract = {Reusable Open Source Software (OSS) components for major programming languages are available in package repositories. Developers rely on package management tools to automate deployments, specifying which package releases satisfy the needs of their applications. However, these specifications may lead to deploying package releases that are outdated, or otherwise undesirable, because they do not include bug fixes, security fixes, or new functionality. In contrast, automatically updating to a more recent release may introduce incompatibility issues. To capture this delicate balance, we formalise a generic model of technical lag, a concept that quantifies to which extent a deployed collection of components is outdated, with respect to the ideal deployment. We operationalise this model for the npm package manager. We empirically analyze the history of package update practices and technical lag for more than 500K packages with about 4M package releases over a seven-year period. We consider both development and runtime dependencies, and study both direct and transitive dependencies. We also analyze the technical lag of external GitHub applications depending on npm packages. We report our findings, suggesting the need for more awareness of, and integrated tool support for, controlling technical lag in software libraries.},
	language = {en},
	number = {8},
	urldate = {2023-09-05},
	journal = {Journal of Software: Evolution and Process},
	author = {Zerouali, Ahmed and Mens, Tom and Gonzalez-Barahona, Jesus and Decan, Alexandre and Constantinou, Eleni and Robles, Gregorio},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2157},
	keywords = {empirical analysis, semantic versioning, software repository mining, software reuse, technical lag},
	pages = {e2157},
}

@inproceedings{wan_are_2021,
	title = {Are {Machine} {Learning} {Cloud} {APIs} {Used} {Correctly}?},
	doi = {10.1109/ICSE43902.2021.00024},
	abstract = {Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efficiently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70\% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wan, Chengcheng and Liu, Shicheng and Hoffmann, Henry and Maire, Michael and Lu, Shan},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Internet, Machine learning, Manuals, Open source software, Semantics, Software engineering, Software systems, cloud API, machine learning, software engineering},
	pages = {125--137},
}

@inproceedings{eck_understanding_2019,
	title = {Understanding {Flaky} {Tests}: {The} {Developer}'s {Perspective}},
	shorttitle = {Understanding {Flaky} {Tests}},
	url = {http://arxiv.org/abs/1907.01466},
	doi = {10.1145/3338906.3338945},
	abstract = {Flaky tests are software tests that exhibit a seemingly random outcome (pass or fail) when run against the same, identical code. Previous work has examined fixes to flaky tests and has proposed automated solutions to locate as well as fix flaky tests--we complement it by examining the perceptions of software developers about the nature, relevance, and challenges of this phenomenon. We asked 21 professional developers to classify 200 flaky tests they previously fixed, in terms of the nature of the flakiness, the origin of the flakiness, and the fixing effort. We complement this analysis with information about the fixing strategy. Subsequently, we conducted an online survey with 121 developers with a median industrial programming experience of five years. Our research shows that: The flakiness is due to several different causes, four of which have never been reported before, despite being the most costly to fix; flakiness is perceived as significant by the vast majority of developers, regardless of their team's size and project's domain, and it can have effects on resource allocation, scheduling, and the perceived reliability of the test suite; and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness. Data and materials [https://doi.org/10.5281/zenodo.3265785].},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Eck, Moritz and Palomba, Fabio and Castelluccio, Marco and Bacchelli, Alberto},
	month = aug,
	year = {2019},
	note = {arXiv:1907.01466 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {830--840},
}

@article{hora_how_2018,
	title = {How do developers react to {API} evolution? {A} large-scale empirical study},
	volume = {26},
	issn = {1573-1367},
	shorttitle = {How do developers react to {API} evolution?},
	url = {https://doi.org/10.1007/s11219-016-9344-4},
	doi = {10.1007/s11219-016-9344-4},
	abstract = {Software engineering research now considers that no system is an island, but it is part of an ecosystem involving other systems, developers, and users. When a framework or a library evolves, its clients often must adapt. For example, client developers might need to adapt to functionalities, client systems might need to be adapted to a new API, and client users might need to adapt to a new user interface. The consequences of these changes are yet unclear: what proportion of the ecosystem might be expected to react, how long might it take for a change to diffuse in the ecosystem, do all clients react in the same way? This paper reports an exploratory study aimed at observing API evolution and its impact on a large software ecosystem, Pharo, which has about 3600 distinct systems, and 6 years of evolution. We analyze 118 API changes in the context of method replacement and suggestion, and answer research questions regarding the magnitude, duration, extension, and consistency of such changes in the ecosystem. The results of this study help to characterize the impact of API evolution in large software ecosystems and provide the basis to better understand how such impact can be alleviated.},
	language = {en},
	number = {1},
	urldate = {2023-09-05},
	journal = {Software Quality Journal},
	author = {Hora, André and Robbes, Romain and Valente, Marco Tulio and Anquetil, Nicolas and Etien, Anne and Ducasse, Stéphane},
	month = mar,
	year = {2018},
	keywords = {API deprecation, API evolution, Empirical study, Software ecosystem},
	pages = {161--191},
}

@misc{braiek_testing_2018,
	title = {On {Testing} {Machine} {Learning} {Programs}},
	url = {http://arxiv.org/abs/1812.02257},
	doi = {10.48550/arXiv.1812.02257},
	abstract = {Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Braiek, Houssem Ben and Khomh, Foutse},
	month = dec,
	year = {2018},
	note = {arXiv:1812.02257 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{noauthor_testing_nodate,
	title = {Testing {Machine} {Learning} {Systems} in {Industry}: {An} {Empirical} {Study} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/9793981},
	urldate = {2023-09-05},
}

@misc{nahar_dataset_2023,
	title = {A {Dataset} and {Analysis} of {Open}-{Source} {Machine} {Learning} {Products}},
	url = {http://arxiv.org/abs/2308.04328},
	doi = {10.48550/arXiv.2308.04328},
	abstract = {Machine learning (ML) components are increasingly incorporated into software products, yet developers face challenges in transitioning from ML prototypes to products. Academic researchers struggle to propose solutions to these challenges and evaluate interventions because they often do not have access to close-sourced ML products from industry. In this study, we define and identify open-source ML products, curating a dataset of 262 repositories from GitHub, to facilitate further research and education. As a start, we explore six broad research questions related to different development activities and report 21 findings from a sample of 30 ML products from the dataset. Our findings reveal a variety of development practices and architectural decisions surrounding different types and uses of ML models that offer ample opportunities for future research innovations. We also find very little evidence of industry best practices such as model testing and pipeline automation within the open-source ML products, which leaves room for further investigation to understand its potential impact on the development and eventual end-user experience for the products.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Nahar, Nadia and Zhang, Haoran and Lewis, Grace and Zhou, Shurui and Kästner, Christian},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04328 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{fan_what_2021,
	title = {What makes a popular academic {AI} repository?},
	volume = {26},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-020-09916-6},
	doi = {10.1007/s10664-020-09916-6},
	abstract = {Many AI researchers are publishing code, data and other resources that accompany their papers in GitHub repositories. In this paper, we refer to these repositories as academic AI repositories. Our preliminary study shows that highly cited papers are more likely to have popular academic AI repositories (and vice versa). Hence, in this study, we perform an empirical study on academic AI repositories to highlight good software engineering practices of popular academic AI repositories for AI researchers. We collect 1,149 academic AI repositories, in which we label the top 20\% repositories that have the most number of stars as popular, and we label the bottom 70\% repositories as unpopular. The remaining 10\% repositories are set as a gap between popular and unpopular academic AI repositories. We propose 21 features to characterize the software engineering practices of academic AI repositories. Our experimental results show that popular and unpopular academic AI repositories are statistically significantly different in 11 of the studied features—indicating that the two groups of repositories have significantly different software engineering practices. Furthermore, we find that the number of links to other GitHub repositories in the README file, the number of images in the README file and the inclusion of a license are the most important features for differentiating the two groups of academic AI repositories. Our dataset and code are made publicly available to share with the community.},
	language = {en},
	number = {1},
	urldate = {2023-09-05},
	journal = {Empirical Software Engineering},
	author = {Fan, Yuanrui and Xia, Xin and Lo, David and Hassan, Ahmed E. and Li, Shanping},
	month = jan,
	year = {2021},
	keywords = {Academic AI repository, Mining software repositories, Software popularity},
	pages = {2},
}

@misc{noauthor_msr_nodate,
	title = {{MSR} 2023 - {Technical} {Papers} - {MSR} 2023},
	url = {https://conf.researchr.org/track/msr-2023/msr-2023-technical-papers?#program},
	abstract = {Welcome to the website of the Mining Software Repositories 2023 conference! 
The Mining Software Repositories (MSR) conference is the premier conference for data science, machine learning, and artificial intelligence in software engineering. The goal of the conference is to improve software engineering practices by uncovering interesting and actionable information about software systems and projects using the vast amounts of software data such as source control systems, defect tracking systems, code review repositories, archived communications between project personnel, question-and-answer ...},
	urldate = {2023-09-05},
}

@misc{pinckney_large_2023,
	title = {A {Large} {Scale} {Analysis} of {Semantic} {Versioning} in {NPM}},
	url = {http://arxiv.org/abs/2304.00394},
	doi = {10.48550/arXiv.2304.00394},
	abstract = {The NPM package repository contains over two million packages and serves tens of billions of downloads per-week. Nearly every single JavaScript application uses the NPM package manager to install packages from the NPM repository. NPM relies on a "semantic versioning" ('semver') scheme to maintain a healthy ecosystem, where bug-fixes are reliably delivered to downstream packages as quickly as possible, while breaking changes require manual intervention by downstream package maintainers. In order to understand how developers use semver, we build a dataset containing every version of every package on NPM and analyze the flow of updates throughout the ecosystem. We build a time-travelling dependency resolver for NPM, which allows us to determine precisely which versions of each dependency would have been resolved at different times. We segment our analysis to allow for a direct analysis of security-relevant updates (those that introduce or patch vulnerabilities) in comparison to the rest of the ecosystem. We find that when developers use semver correctly, critical updates such as security patches can flow quite rapidly to downstream dependencies in the majority of cases (90.09\%), but this does not always occur, due to developers' imperfect use of both semver version constraints and semver version number increments. Our findings have implications for developers and researchers alike. We make our infrastructure and dataset publicly available under an open source license.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Pinckney, Donald and Cassano, Federico and Guha, Arjun and Bell, Jonathan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00394 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{noauthor_onedrive_nodate,
	title = {{OneDrive}},
	url = {https://onedrive.live.com/?authkey=%21AMpxMNHnOmKVwGs&id=E7ADA0AA11290381%211254&cid=E7ADA0AA11290381&parId=root&parQt=sharedby&o=OneUp},
	urldate = {2023-09-05},
}

@inproceedings{kudrjavets_are_2023,
	title = {Are {We} {Speeding} {Up} or {Slowing} {Down}? {On} {Temporal} {Aspects} of {Code} {Velocity}},
	shorttitle = {Are {We} {Speeding} {Up} or {Slowing} {Down}?},
	url = {http://arxiv.org/abs/2303.04293},
	doi = {10.1109/MSR59073.2023.00046},
	abstract = {This paper investigates how the duration of various code review periods changes over a projects' lifetime. We study four open-source software (OSS) projects: Blender, FreeBSD, LLVM, and Mozilla. We mine and analyze the characteristics of 283,235 code reviews that cover, on average, seven years' worth of development. Our main conclusion is that neither the passage of time or the project's size impact code velocity. We find that (a) the duration of various code review periods (time-to-first-response, time-to-accept, and time-to-merge) for FreeBSD, LLVM, and Mozilla either becomes shorter or stays the same; no directional trend is present for Blender, (b) an increase in the size of the code bases (annually 3-17\%) does not accompany a decrease in code velocity, and (c) for FreeBSD, LLVM, and Mozilla, the 30-day moving median stays in a fixed range for time-to-merge. These findings do not change with variabilities in code churn metrics, such as the number of commits or distinct authors of code changes.},
	urldate = {2023-09-05},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Kudrjavets, Gunnar and Nagappan, Nachiappan and Rastogi, Ayushi},
	month = may,
	year = {2023},
	note = {arXiv:2303.04293 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {267--271},
}

@inproceedings{oliveira_dont_2023,
	title = {Don't {Forget} the {Exception}! {Considering} {Robustness} {Changes} to {Identify} {Design} {Problems}},
	doi = {10.1109/MSR59073.2023.00064},
	abstract = {Modern programming languages, such as Java, use exception-handling mechanisms to guarantee the robustness of software systems. Although important, the quality of exception code is usually poor and neglected by developers. Indiscriminate robustness changes (e.g., the addition of empty catch blocks) can indicate design decisions that negatively impact the internal quality of software systems. As it is known in the literature, multiple occurrences of poor code structures, namely code smells, are strong indicators of design problems. Still, existing studies focus mainly on the correlation of maintainability smells with design problems. However, using only these smells may not be enough since developers need more context (e.g., system domain) to identify the problems in certain scenarios. Moreover, these studies do not explore how changes in the exceptional code of the methods combined with maintainability smells can give complementary evidence of design problems. By covering both regular and exception codes, the developer can have more context about the system and find complementary code smells that reinforce the presence of design problems. This work aims to leverage the identification of design problems by tracking poor robustness changes combined with maintainability smells. We investigated the correlation between robustness changes and maintainability smells on the commit history of more than 160k methods from different releases of 10 open-source software systems. We observed that maintainability smells can be worsened or even introduced when robustness changes are performed. This scenario mainly happened for the smells Feature Envy, Long Method, and Dispersed Coupling. We also analyzed the co-occurrence between robustness and maintainability smells. We identified that the empty catch block and catch throwable robustness smells were the ones that co-occurred the most with maintainability smells related to the Concern Overload and Misplaced Concern design problems. The contribution of our work is to reveal that poor exception code, usually neglected by developers, negatively impacts the quality of methods and classes, signaled by the maintainability smells. Therefore, existing code smell detecting tools can be enhanced to leverage robustness changes to identify design problems.},
	author = {Oliveira, Anderson and Correia, João and Sousa, Leonardo and Assunção, Wesley and Coutinho, Daniel and Garcia, Alessandro and Oizumi, Willian and Barbosa Vieira da Silva, Caio and Uchôa, Anderson and Alves Pereira, Juliana},
	month = may,
	year = {2023},
}

@misc{noauthor_msr23_preprintpdf_nodate,
	title = {{MSR23}\_Preprint.pdf},
	url = {https://www.dropbox.com/s/a4a3fzm36n65iax/MSR23_Preprint.pdf?dl=0},
	abstract = {Shared with Dropbox},
	language = {en},
	urldate = {2023-09-05},
	journal = {Dropbox},
}

@misc{van_dam_enriching_2023,
	title = {Enriching {Source} {Code} with {Contextual} {Data} for {Code} {Completion} {Models}: {An} {Empirical} {Study}},
	shorttitle = {Enriching {Source} {Code} with {Contextual} {Data} for {Code} {Completion} {Models}},
	url = {http://arxiv.org/abs/2304.12269},
	abstract = {Transformer-based pre-trained models have recently achieved great results in solving many software engineering tasks including automatic code completion which is a staple in a developer's toolkit. While many have striven to improve the code-understanding abilities of such models, the opposite -- making the code easier to understand -- has not been properly investigated. In this study, we aim to answer whether making code easier to understand through using contextual data improves the performance of pre-trained code language models for the task of code completion. We consider type annotations and comments as two common forms of additional contextual information that often help developers understand code better. For the experiments, we study code completion in two granularity levels; token and line completion and take three recent and large-scale language models for source code: UniXcoder, CodeGPT, and InCoder with five evaluation metrics. Finally, we perform the Wilcoxon Signed Rank test to gauge significance and measure the effect size. Contrary to our expectations, all models perform better if type annotations are removed (albeit the effect sizes are small). For comments, we find that the models perform better in the presence of multi-line comments (again with small effect sizes). Based on our observations, we recommend making proper design choices when training, fine-tuning, or simply selecting such models given the intended data and application. Better evaluations and multi-modal techniques can also be further investigated to improve the practicality and accuracy of auto-completions.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {van Dam, Tim and Izadi, Maliheh and van Deursen, Arie},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12269 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{wang_automl_2023,
	address = {Melbourne, Australia},
	title = {{AutoML} from {Software} {Engineering} {Perspective}: {Landscapes} and {Challenges}},
	isbn = {9798350311846},
	shorttitle = {{AutoML} from {Software} {Engineering} {Perspective}},
	url = {https://ieeexplore.ieee.org/document/10173951/},
	doi = {10.1109/MSR59073.2023.00019},
	abstract = {Machine learning (ML) has been widely adopted in modern software, but the manual configuration of ML (e.g., hyper-parameter configuration) poses a significant challenge to software developers. Therefore, automated ML (AutoML), which seeks the optimal configuration of ML automatically, has received increasing attention from the software engineering community. However, to date, there is no comprehensive understanding of how AutoML is used by developers and what challenges developers encounter in using AutoML for software development. To fill this knowledge gap, we conduct the first study on understanding the use and challenges of AutoML from software developers’ perspective. We collect and analyze 1,554 AutoML downstream repositories, 769 AutoML-related Stack Overflow questions, and 1,437 relevant GitHub issues. The results suggest the increasing popularity of AutoML in a wide range of topics, but also the lack of relevant expertise. We manually identify specific challenges faced by developers for AutoML-enabled software. Based on the results, we derive a series of implications for AutoML framework selection, framework development, and research.},
	language = {en},
	urldate = {2023-09-05},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wang, Chao and Chen, Zhenpeng and Zhou, Minghui},
	month = may,
	year = {2023},
	pages = {39--51},
}

@misc{wang_hodor_2023,
	title = {{HODOR}: {Shrinking} {Attack} {Surface} on {Node}.js via {System} {Call} {Limitation}},
	shorttitle = {{HODOR}},
	url = {http://arxiv.org/abs/2306.13984},
	doi = {10.48550/arXiv.2306.13984},
	abstract = {Node.js provides Node.js applications with system interaction capabilities using system calls. However, such convenience comes with a price, i.e., the attack surface of JavaScript arbitrary code execution (ACE) vulnerabilities is expanded to the system call level. There lies a noticeable gap between existing protection techniques in the JavaScript code level (either by code debloating or read-write-execute permission restriction) and a targeted defense for emerging critical system call level exploitation. To fill the gap, we design and implement HODOR, a lightweight runtime protection system based on enforcing precise system call restrictions when running a Node.js application. HODOR achieved this by addressing several nontrivialial technical challenges. First, HODOR requires to construct high-quality call graphs for both the Node.js application (in JavaScript) and its underlying Node.js framework (in JavaScript and C/C++). Specifically, HODOR incorporates several important optimizations in both the JavaScript and C/C++ level to improve the state-of-the-art tools for building more precise call graphs. Then, HODOR creates the main-thread whitelist and the thread-pool whitelist respectively containing the identified necessary system calls based on the call graphs mappings. Finally, with the whitelists, HODOR implements lightweight system call restriction using the Linux kernel feature Secure Computing Mode (seccomp) to shrink the attack surface. We utilize HODOR to protect 83 real-world Node.js applications compromised by arbitrary code/command execution attacks. HODOR could reduce the attack surface to 16.75\% on average with negligible runtime overhead (i.e., {\textless}3\%).},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Wang, Wenya and Lin, Xingwei and Wang, Jingyi and Gao, Wang and Gu, Dawu and Lv, Wei and Wang, Jiashui},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13984 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{abbadini_cage4deno_2023,
	address = {New York, NY, USA},
	series = {{ASIA} {CCS} '23},
	title = {{Cage4Deno}: {A} {Fine}-{Grained} {Sandbox} for {Deno} {Subprocesses}},
	isbn = {9798400700989},
	shorttitle = {{Cage4Deno}},
	url = {https://dl.acm.org/doi/10.1145/3579856.3595799},
	doi = {10.1145/3579856.3595799},
	abstract = {Deno is a runtime for JavaScript and TypeScript that is receiving great interest by developers, and is increasingly used for the construction of back-ends of web applications. A primary goal of Deno is to provide a secure and isolated environment for the execution of JavaScript programs. It also supports the execution of subprocesses, unfortunately without providing security guarantees. In this work we propose Cage4Deno, a set of modifications to Deno enabling the creation of fine-grained sandboxes for the execution of subprocesses. The design of Cage4Deno satisfies the compatibility, transparency, flexibility, usability, security, and performance needs of a modern sandbox. The realization of these requirements partially stems from the use of Landlock and eBPF, two robust and efficient security technologies. Significant attention has been paid to the design of a flexible and compact policy model consisting of RWX permissions, which can be automatically created, and deny rules to declare exceptions. The sandbox effectiveness is demonstrated by successfully blocking a number of exploits for recent CVEs, while runtime experiments prove its efficiency. The proposal is associated with an open-source implementation.},
	urldate = {2023-08-29},
	booktitle = {Proceedings of the 2023 {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Abbadini, Marco and Facchinetti, Dario and Oldani, Gianluca and Rossi, Matthew and Paraboschi, Stefano},
	month = jul,
	year = {2023},
	keywords = {Access control, Deno, JavaScript runtime, Sandboxing, Subprocess},
	pages = {149--162},
}

@inproceedings{park_l2fuzz_2022,
	title = {{L2Fuzz}: {Discovering} {Bluetooth} {L2CAP} {Vulnerabilities} {Using} {Stateful} {Fuzz} {Testing}},
	shorttitle = {{L2Fuzz}},
	doi = {10.1109/DSN53405.2022.00043},
	abstract = {Bluetooth Basic Rate/Enhanced Data Rate (BR/EDR) is a wireless technology used in billions of devices. Recently, several Bluetooth fuzzing studies have been conducted to detect vulnerabilities in Bluetooth devices, but they fall short of effectively generating malformed packets. In this paper, we propose L2FUZZ, a stateful fuzzer to detect vulnerabilities in Bluetooth BR/EDR Logical Link Control and Adaptation Protocol (L2CAP) layer. By selecting valid commands for each state and mutating only the core fields of packets, L2FUZZ can generate valid malformed packets that are less likely to be rejected by the target device. Our experimental results confirmed that: (1) L2FUZZ generates up to 46 times more malformed packets with a much less packet rejection ratio compared to the existing techniques, and (2) L2FUZZ detected five zero-day vulnerabilities from eight real-world Bluetooth devices.},
	booktitle = {2022 52nd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Park, Haram and Nkuba, Carlos Kayembe and Woo, Seunghoon and Lee, Heejo},
	month = jun,
	year = {2022},
	note = {ISSN: 2158-3927},
	keywords = {Bluetooth, Codes, Fuzz Testing, Fuzzing, Protocols, Reliability, Security, Wireless Security, Wireless communication},
	pages = {343--354},
}

@article{friedberg_stpa-safesec_2017,
	title = {{STPA}-{SafeSec}: {Safety} and security analysis for cyber-physical systems},
	volume = {34},
	issn = {22142126},
	shorttitle = {{STPA}-{SafeSec}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214212616300850},
	doi = {10.1016/j.jisa.2016.05.008},
	language = {en},
	urldate = {2023-08-29},
	journal = {Journal of Information Security and Applications},
	author = {Friedberg, Ivo and McLaughlin, Kieran and Smith, Paul and Laverty, David and Sezer, Sakir},
	month = jun,
	year = {2017},
	pages = {183--196},
}

@inproceedings{young_systems_2013,
	address = {New Orleans Louisiana USA},
	title = {Systems thinking for safety and security},
	isbn = {978-1-4503-2015-3},
	url = {https://dl.acm.org/doi/10.1145/2523649.2530277},
	doi = {10.1145/2523649.2530277},
	language = {en},
	urldate = {2023-08-29},
	booktitle = {Proceedings of the 29th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Young, William and Leveson, Nancy},
	month = dec,
	year = {2013},
	pages = {1--8},
}

@inproceedings{gurdur_visual_2018,
	title = {Visual analytics for cyber-physical systems development: {Blending} design thinking and systems thinking},
	author = {Gürdür, Didem and Törngren, Martin},
	year = {2018},
}

@article{broo_transdisciplinarity_2022,
	title = {Transdisciplinarity and three mindsets for sustainability in the age of cyber-physical systems},
	volume = {27},
	issn = {2452414X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2452414X2100087X},
	doi = {10.1016/j.jii.2021.100290},
	language = {en},
	urldate = {2023-08-29},
	journal = {Journal of Industrial Information Integration},
	author = {Broo, Didem Gürdür},
	month = may,
	year = {2022},
	pages = {100290},
}

@incollection{drevin_pilot_2018,
	address = {Cham},
	title = {A {Pilot} {Study} in {Cyber} {Security} {Education} {Using} {CyberAIMs}: {A} {Simulation}-{Based} {Experiment}},
	volume = {531},
	isbn = {978-3-319-99733-9 978-3-319-99734-6},
	shorttitle = {A {Pilot} {Study} in {Cyber} {Security} {Education} {Using} {CyberAIMs}},
	url = {https://link.springer.com/10.1007/978-3-319-99734-6_4},
	language = {en},
	urldate = {2023-08-29},
	booktitle = {Information {Security} {Education} – {Towards} a {Cybersecure} {Society}},
	publisher = {Springer International Publishing},
	author = {Zoto, Erjon and Kowalski, Stewart and Frantz, Christopher and Lopez-Rojas, Edgar and Katt, Basel},
	editor = {Drevin, Lynette and Theocharidou, Marianthi},
	year = {2018},
	doi = {10.1007/978-3-319-99734-6_4},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {40--54},
}

@article{zoto_socio-technical_2019,
	title = {A {Socio}-technical {Systems} {Approach} to {Design} and {Support} {Systems} {Thinking} in {Cybersecurity} and {Risk} {Management} {Education}},
	issn = {22559922},
	url = {https://csimq-journals.rtu.lv/article/view/csimq.2019-18.04},
	doi = {10.7250/csimq.2019-18.04},
	number = {18},
	urldate = {2023-08-29},
	journal = {Complex Systems Informatics and Modeling Quarterly},
	author = {Zoto, Erjon and Kianpour, Mazaher and {Department of Information Security and Communication Technology, Norwegian University of Science and Technology, Teknologivegen 22, 2815 Gjøvik, Norway} and Kowalski, Stewart James and {Department of Information Security and Communication Technology, Norwegian University of Science and Technology, Teknologivegen 22, 2815 Gjøvik, Norway} and Lopez-Rojas, Edgar Alonso and {Department of Information Security and Communication Technology, Norwegian University of Science and Technology, Teknologivegen 22, 2815 Gjøvik, Norway}},
	month = apr,
	year = {2019},
	pages = {65--75},
}

@inproceedings{enright_building_2020,
	title = {Building capacity for systems thinking in higher education cybersecurity programs},
	volume = {8},
	isbn = {2641-4554},
	author = {Enright, Esther and Justice, Connie and Loo, Sin Ming and Taylor, Eleanor and Sample, Char and Shelton, D Cragin},
	year = {2020},
	note = {Issue: 1},
	pages = {8--8},
}

@article{journal_of_online_trust_and_safety_second_2022,
	title = {The {Second} {Issue}},
	volume = {1},
	shorttitle = {Vol. 1 {No}. 2 (2022)},
	url = {https://tsjournal.org/index.php/jots/issue/view/2},
	language = {en-US},
	number = {2},
	urldate = {2022-06-07},
	journal = {Journal of Online Trust and Safety},
	author = {{Journal of Online Trust and Safety}},
	year = {2022},
	keywords = {open access, scholarly publishing, trust and safety},
}

@inproceedings{klein_sel4_2009,
	title = {{seL4}: {Formal} verification of an {OS} kernel},
	booktitle = {Proc. of the 22nd {ACM} {SOSP}},
	author = {Klein, Gerwin and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal and Norrish, Michael and {others}},
	year = {2009},
	pages = {207--220},
}

@book{eset_research_osxkeydnap_nodate,
	title = {{OSX}/{Keydnap} spreads via signed {Transmission} application},
	author = {{ESET Research}},
}

@misc{corbet_cracking_2011,
	title = {The cracking of kernel.org},
	url = {http://www.linuxfoundation.org/news-media/blogs/browse/2011/08/cracking-kernelorg},
	author = {Corbet, Jonathan},
	year = {2011},
}

@misc{noauthor_production-grade_nodate,
	title = {Production-{Grade} {Container} {Orchestration}},
}

@book{rsa_research_kingslayer-supply_nodate,
	title = {Kingslayer-{A} {Supply} {Chain} {Attack}},
	author = {{RSA Research}},
}

@misc{corbet_attempt_2003,
	title = {An attempt to backdoor the kernel},
	url = {http://lwn.net/Articles/57135/},
	author = {Corbet, Jonathan},
	year = {2003},
}

@misc{juniper_2015-12_nodate,
	title = {2015-12 {Out} of {Cycle} {Security} {Bulletin}: {ScreenOS}: {Multiple} {Security} issues with {ScreenOS} ({CVE}-2015-7755, {CVE}-2015-7756)},
	url = {https://kb.juniper.net/InfoCenter/index?page=content&id=JSA10713},
	author = {{Juniper}},
}

@misc{network_world_juniper_nodate,
	title = {Juniper firewalls compromised by bad code: {What} you need to know},
	author = {{Network World}},
}

@misc{checkoway_systematic_2016,
	title = {A {Systematic} {Analysis} of the {Juniper} {Dual} {EC} {Incident}},
	author = {Checkoway, Stephen and Cohney, Shaanan and Garman, Christina and Green, Matthew and Heninger, Nadia and Maskiewicz, Jacob and Rescorla, Eric and Shacham, Hovav and Weinmann, Ralf-Philipp},
	year = {2016},
	note = {Published: Cryptology ePrint Archive, Report 2016/376},
}

@misc{noauthor_-toto_nodate,
	title = {in-toto {Jenkins} plugin},
}

@misc{noauthor_jenkins_nodate,
	title = {Jenkins: {Build} great things at any scale},
}

@book{idrees_patel_janus_nodate,
	title = {Janus {Vulnerability}},
	author = {{Idrees Patel}},
}

@book{isoiec_jtc_1sc_27_it_security_techniques_isoiec_nodate,
	title = {{ISO}/{IEC} 27034:2011 {Information} technology – {Security} techniques – {Application} security},
	author = {{ISO/IEC JTC 1/SC 27 IT Security techniques}},
}

@misc{jeff_erickson_inside_nodate,
	title = {Inside {OilRig} – {Tracking} {Iran}'s {Busiest} {Hacker} {Crew} {On} {Its} {Global} {Rampage}},
	author = {{Jeff Erickson}},
}

@misc{apple_computers_ios_2016,
	title = {{iOS} {Security} {Guide}},
	author = {{Apple Computers}},
	year = {2016},
}

@misc{noauthor_-toto_nodate-1,
	title = {in-toto {Specifications}},
}

@misc{noauthor_-toto_nodate-2,
	title = {in-toto {Specification}: {Version} 0.9},
}

@misc{noauthor_-toto_nodate-3,
	title = {in-toto {Metadata} {Examples}},
}

@misc{noauthor_-toto-webhook_nodate,
	title = {in-toto-webhook},
}

@misc{noauthor_-toto_nodate-4,
	title = {in-toto {Java}},
}

@misc{noauthor_go_nodate,
	title = {Go in-toto verification},
}

@inproceedings{hawblitzel_ironclad_2014,
	title = {Ironclad apps: {End}-to-end security via automated full-system verification},
	booktitle = {Proc. of the 11th {USENIX} {OSDI}},
	author = {Hawblitzel, Chris and Howell, Jon and Lorch, Jacob R. and Narayan, Arjun and Parno, Bryan and Zhang, Danfeng and Zill, Brian},
	year = {2014},
	pages = {165--181},
}

@book{thomas_reed_handbrake_nodate,
	title = {{HandBrake} hacked to drop new variant of {Proton} malware},
	author = {{Thomas Reed}},
}

@inproceedings{hanawa_large-scale_2010,
	title = {Large-scale software testing environment using cloud computing technology for dependable parallel and distributed systems},
	booktitle = {Software {Testing}, {Verification}, and {Validation} {Workshops} ({ICSTW}), 2010 {Third} {International} {Conference} on},
	publisher = {IEEE},
	author = {Hanawa, Toshihiro and Banzai, Takayuki and Koizumi, Hitoshi and Kanbayashi, Ryo and Imada, Takayuki and Sato, Mitsuhisa},
	year = {2010},
	pages = {428--433},
}

@misc{felix_glaser_exploring_nodate,
	title = {Exploring container security: {Digging} into {Grafeas} container image metadata},
	author = {{Felix Glaser}},
}

@misc{noauthor_grafeas_nodate,
	title = {Grafeas + in-toto},
}

@inproceedings{gruhn_security_2013,
	title = {Security of public continuous integration services},
	booktitle = {Proc. of the 9th {ACM} {International} {Symposium} on {Open} {Collaboration}},
	author = {Gruhn, Volker and Hannebauer, Christoph and John, Christian},
	year = {2013},
	pages = {15},
}

@misc{noauthor_grafeas_2017,
	title = {Grafeas},
	url = {https://grafeas.io/},
	year = {2017},
}

@book{wired_google_nodate,
	title = {'{Google}' {Hackers} had ability to alter source code'},
	author = {{Wired}},
}

@misc{google_binary_nodate,
	title = {Binary {Authorization}},
	url = {https://cloud.google.com/binary-authorization/},
	author = {{Google}},
}

@misc{gnu_savannah_compromise2010_2010,
	title = {Compromise2010},
	url = {https://savannah.gnu.org/maintenance/Compromise2010/},
	author = {{GNU Savannah}},
	year = {2010},
}

@misc{kuhn_news_2003,
	title = {News: {IMPORTANT}: {Information} {Regarding} {Savannah} {Restoration} for {All} {Users}},
	url = {https://savannah.gnu.org/forum/forum.php?forum_id=2752},
	author = {Kuhn, Bradley M.},
	year = {2003},
}

@misc{gitolite_git_nodate,
	title = {git as a deployment tool},
	author = {{gitolite}},
}

@misc{gerwitz_git_nodate,
	title = {A {Git} {Horror} {Story}: {Repository} {Integrity} {With} {Signed} {Commits}},
	author = {Gerwitz, Mike},
}

@misc{github_inc_public_2012,
	title = {Public {Key} {Security} {Vulnerability} and {Mitigation}},
	author = {{GitHub, Inc.}},
	year = {2012},
}

@misc{noauthor_critical_nodate,
	title = {Critical {Git} flaw},
}

@misc{colin_walters_weaning_nodate,
	title = {weaning distributions off tarballs: extended verification of git tags},
	author = {{Colin Walters}},
}

@misc{git_scm_signing_nodate,
	title = {Signing {Your} {Work}},
	author = {{Git SCM}},
}

@misc{colin_walters_github_nodate,
	title = {Github: cgwalters/git-evtag},
	author = {{Colin Walters}},
}

@misc{torres-arias_omitting_2016,
	title = {On {Omitting} {Commits} and {Committing} {Omissions}: {Preventing} {Git} {Metadata} {Tampering} {That} ({Re})introduces {Software} {Vulnerabilities}},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/torres-arias},
	publisher = {USENIX Association},
	author = {Torres-Arias, Santiago and Ammula, Anil Kumar and Curtmola, Reza and Cappos, Justin},
	month = aug,
	year = {2016},
	note = {ISBN: 978-1-931971-32-4
Pages: 379–395
Place: Austin, TX
Publication Title: 25th USENIX Security Symposium (USENIX Security 16)},
}

@misc{noauthor_git_nodate,
	title = {Git},
}

@misc{the_freebsd_project_freebsdorg_2012,
	title = {{FreeBSD}.org intrusion announced {November} 17th 2012},
	url = {http://www.freebsd.org/news/2012-compromise.html},
	author = {{The FreeBSD Project}},
	year = {2012},
}

@book{gentoo_linux_incident_nodate,
	title = {Incident {Reports}/2018-06-28 {Github}},
	author = {{Gentoo Linux}},
}

@misc{patrick_gray_gentoo_2003,
	title = {Gentoo {Linux} server compromised},
	url = {https://www.zdnet.com/article/gentoo-linux-server-compromised/},
	author = {{Patrick Gray}},
	year = {2003},
}

@misc{symantec_w32duqu_nodate,
	title = {W32.{Duqu}: {The} precursor to the next {Stuxnet}},
	author = {{Symantec}},
}

@book{martin_brinkmann_attention_nodate,
	title = {Attention: {Some} {Fosshub} downloads compromised},
	author = {{Martin Brinkmann}},
}

@misc{naked_security_flame_nodate,
	title = {Flame malware used man-in-the-middle attack against {Windows} {Update}},
	author = {{Naked Security}},
}

@inproceedings{fitzgerald_continuous_2014,
	title = {Continuous software engineering and beyond: trends and challenges},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Rapid} {Continuous} {Software} {Engineering}},
	publisher = {ACM},
	author = {Fitzgerald, Brian and Stol, Klaas-Jan},
	year = {2014},
	pages = {1--9},
}

@misc{frields_infrastructure_2008,
	title = {Infrastructure report, 2008-08-22 {UTC} 1200},
	url = {https://www.redhat.com/archives/fedora-announce-list/2008-August/msg00012.html},
	author = {Frields, Paul W.},
	year = {2008},
}

@misc{noauthor_expensivewall_nodate,
	title = {{ExpensiveWall}: {A} {Dangerous} {Packed} {Malware} {On} {Google} {Play}},
}

@inproceedings{feng_tips_2014,
	title = {Tips: {Context}-aware implicit user identification using touch screen in uncontrolled environments},
	booktitle = {Proceedings of the 15th {Workshop} on {Mobile} {Computing} {Systems} and {Applications}},
	publisher = {ACM},
	author = {Feng, Tao and Yang, Jun and Yan, Zhixian and Tapia, Emmanuel Munguia and Shi, Weidong},
	year = {2014},
	pages = {9},
}

@misc{smith_security_2011,
	title = {Security incident on {Fedora} infrastructure on 23 {Jan} 2011},
	url = {https://lists.fedoraproject.org/pipermail/announce/2011-January/002911.html},
	author = {Smith, Jared K.},
	year = {2011},
}

@misc{noauthor_fedora_nodate,
	title = {Fedora servers breached after external compromise},
}

@book{jeremy_kirk_new_2010,
	title = {New malware overwrites software updaters},
	author = {{Jeremy Kirk}},
	year = {2010},
}

@incollection{begin_build_2006,
	title = {Build, configuration, integration and testing tools for large software projects: {Etics}},
	booktitle = {Rapid {Integration} of {Software} {Engineering} {Techniques}},
	publisher = {Springer},
	author = {Bégin, Marc-Elian and Sancho, Guillermo Diez-Andino and Meglio, Alberto Di and Ferro, Enrico and Ronchieri, Elisabetta and Selmi, Matteo and Żurek, Marian},
	year = {2006},
	pages = {81--97},
}

@article{ellison_evaluating_2010,
	title = {Evaluating and mitigating software supply chain security risks},
	author = {Ellison, Robert J. and Goodenough, John B. and Weinstock, Charles B. and Woody, Carol},
	year = {2010},
	note = {Publisher: CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
}

@misc{david_a_wheeler_software_nodate,
	title = {Software {Configuration} {Management} {Security}},
	author = {{David A. Wheeler}},
}

@book{duvall_continuous_2007,
	title = {Continuous integration: improving software quality and reducing risk},
	publisher = {Pearson Education},
	author = {Duvall, Paul M. and Matyas, Steve and Glover, Andrew},
	year = {2007},
}

@misc{jared_newman_gauss_nodate,
	title = {Gauss {Malware}: {What} {You} {Need} to {Know}},
	author = {{Jared Newman}},
}

@misc{noauthor_docker_nodate,
	title = {Docker {Swarm} overview},
}

@misc{django_how_nodate,
	title = {How {Is} {Django} {Formed}?},
	author = {{Django}},
}

@misc{django_meet_nodate,
	title = {Meet the team},
	author = {{Django}},
}

@misc{debian_debian_2003,
	title = {Debian {Investigation} {Report} after {Server} {Compromises}},
	url = {https://www.debian.org/News/2003/20031202},
	author = {{Debian}},
	year = {2003},
}

@misc{project_debian_nodate,
	title = {Debian {Upstream} {Guide}},
	author = {Project, The Debian},
}

@inproceedings{devanbu_techniques_1998,
	title = {Techniques for trusted software engineering},
	booktitle = {Proceedings of the 20th international conference on {Software} engineering},
	publisher = {IEEE Computer Society},
	author = {Devanbu, Premkumar T. and Fong, Philip WL and Stubblebine, Stuart G.},
	year = {1998},
	pages = {126--135},
}

@misc{debian_security_2012,
	title = {Security breach on the {Debian} wiki 2012-07-25},
	url = {https://wiki.debian.org/DebianWiki/SecurityIncident2012},
	author = {{Debian}},
	year = {2012},
}

@misc{madduck_thoughts_nodate,
	title = {Thoughts on a new upload process},
	author = {{Madduck}},
}

@misc{datadog_core_2019,
	title = {Core integrations of the {Datadog} {Agent}},
	url = {https://github.com/DataDog/integrations-core},
	author = {{Datadog}},
	year = {2019},
}

@misc{noauthor_forbes_2018,
	title = {Forbes {Cloud} 100: \#19 {Datadog}},
	year = {2018},
}

@misc{noauthor_ubuntu_nodate,
	title = {Ubuntu {Insights}: about canonical},
}

@misc{noauthor_debian_nodate,
	title = {Some {Debian} {Project} machines compromised},
}

@misc{datadog_yubikey_2019,
	title = {Yubikey at {Datadog}},
	url = {https://github.com/DataDog/yubikey},
	author = {{Datadog}},
	year = {2019},
}

@misc{datadog_datadog_2019,
	title = {Datadog agent integrations downloader},
	url = {https://github.com/DataDog/integrations-core/tree/master/integrations_core_downloader},
	author = {{Datadog}},
	year = {2019},
}

@misc{datadog_datadog_2019-1,
	title = {Datadog agent integrations developer tool integration with {\textbackslash}sysname},
	url = {https://github.com/DataDog/integrations-core/blob/master/datadog_checks_dev/datadog_checks/dev/tooling/signing.py},
	author = {{Datadog}},
	year = {2019},
}

@misc{datadog_datadog_2019-2,
	title = {Datadog agent integrations developer tool},
	url = {https://datadog-checks-base.readthedocs.io/en/latest/datadog_checks_dev.cli.html},
	author = {{Datadog}},
	year = {2019},
}

@misc{noauthor_customers_nodate,
	title = {Customers {\textbar} {Datadog}},
}

@misc{noauthor_datadog_nodate,
	title = {{DataDog} {Release} 6.6.0},
}

@inproceedings{cui_when_2013,
	title = {When {Firmware} {Modifications} {Attack}: {A} {Case} {Study} of {Embedded} {Exploitation}.},
	booktitle = {{NDSS}},
	author = {Cui, Ang and Costello, Michael and Stolfo, Salvatore J.},
	year = {2013},
}

@misc{noauthor_datadog_nodate-1,
	title = {Datadog: {Modern} monitoring \& analytics},
}

@book{crowdstrike_securing_nodate,
	title = {Securing the supply chain},
	author = {{CrowdStrike}},
}

@misc{microsoft_certificate_nodate,
	title = {Certificate revocation lists},
	author = {{Microsoft}},
}

@inproceedings{costin_large-scale_2014,
	address = {San Diego, CA},
	title = {A {Large}-{Scale} {Analysis} of the {Security} of {Embedded} {Firmwares}},
	isbn = {978-1-931971-15-7},
	url = {https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/costin},
	booktitle = {23rd {USENIX} {Security} {Symposium} ({USENIX} {Security} 14)},
	publisher = {USENIX Association},
	author = {Costin, Andrei and Zaddach, Jonas and Francillon, Aurélien and Balzarotti, Davide},
	month = aug,
	year = {2014},
	pages = {95--110},
}

@book{trend_micro_cyber_safety_solutions_team_new_2019,
	title = {New {Magecart} {Attack} {Delivered} {Through} {Compromised} {Advertising} {Supply} {Chain}},
	author = {{Trend Micro Cyber Safety Solutions Team}},
	year = {2019},
}

@book{trend_micro_cyber_safety_solutions_team_new_2019-1,
	title = {New {Magecart} {Attack} {Delivered} {Through} {Compromised} {Advertising} {Supply} {Chain}},
	author = {{Trend Micro Cyber Safety Solutions Team}},
	year = {2019},
}

@misc{noauthor_codesonar_nodate,
	title = {{CodeSonar}},
}

@misc{codeship_continuous_nodate,
	title = {Continuous {Delivery} with {Codeship}: {Fast}, {Secure}, and fully customizable},
	author = {{Codeship}},
}

@misc{noauthor_cloud_nodate,
	title = {Cloud {Native} {Computing} {Foundation}},
}

@book{noauthor_china_nodate,
	title = {China, {GitHub} and the man-in-the-middle},
}

@misc{swati_khandelwal_ccleaner_2018,
	title = {{CCleaner} {Attack} {Timeline}–{Here}'s {How} {Hackers} {Infected} 2.3 {Million} {PCs}},
	url = {https://thehackernews.com/2018/04/ccleaner-malware-attack.html},
	author = {{Swati Khandelwal}},
	year = {2018},
}

@book{christian_nutt_cloud_nodate,
	title = {Cloud source host {Code} {Spaces} hacked, developers lose code},
	author = {{Christian Nutt}},
}

@book{arstechnica_meet_nodate,
	title = {Meet “{Great} {Cannon}”, the man-in-the-middle weapon {China} used on {GitHub}},
	author = {{arsTechnica}},
}

@inproceedings{chen_auditable_2014,
	title = {Auditable {Version} {Control} {Systems}.},
	booktitle = {{NDSS}},
	author = {Chen, Bo and Curtmola, Reza},
	year = {2014},
}

@article{cappos_package_nodate,
	title = {Package management security},
	author = {Cappos, Justin and Samuel, Justin and Baker, Scott and Hartman, John H.},
}

@inproceedings{cappos_look_2008,
	title = {A look in the mirror: {Attacks} on package managers},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security},
	publisher = {ACM},
	author = {Cappos, Justin and Samuel, Justin and Baker, Scott and Hartman, John H.},
	year = {2008},
	pages = {565--574},
}

@book{dona_sarkar_note_nodate,
	title = {A note about the unintentional release of builds today},
	author = {{Dona Sarkar}},
}

@misc{joseph_cox_open_nodate,
	title = {Open {Database} {Exposes} {Millions} of {Job} {Seekers}` {Personal} {Information}},
	author = {{Joseph Cox}},
}

@book{chirgwin_microsoft_nodate,
	title = {Microsoft deletes deleterious file deletion bug from {Windows} 10},
	author = {Chirgwin, Richard},
}

@book{coppock_windows_nodate,
	title = {Windows {Update} not working after {October} 2018 patch? {Here}{\textbackslash}rqs how to fix it},
	author = {Coppock, Mark},
}

@misc{kelly_massive_nodate,
	title = {Massive {Windows} 10 {Update} {Starts} {Causing} {Problems}},
	author = {Kelly, Gordon},
}

@book{edward_iskra_vulnerable_2017,
	title = {Vulnerable {Wallets} and the {Suspicious} {File}},
	author = {{Edward Iskra}},
	year = {2017},
}

@book{dennis_fisher_researcher_nodate,
	title = {Researcher {Finds} {Tor} {Exit} {Node} {Adding} {Malware} to {Binaries}},
	author = {{Dennis Fisher}},
}

@book{andy_greenberg_macos_nodate,
	title = {{MacOS} {Update} {Accidentally} {Undoes} {Apple}'s “{Root}” {Bug} {Patch}},
	author = {{Andy Greenberg}},
}

@book{warren_major_nodate,
	title = {Major new {iOS} bug can crash {iPhones}},
	author = {Warren, Tom},
}

@inproceedings{banzai_d-cloud_2010,
	title = {D-cloud: {Design} of a software testing environment for reliable distributed systems using cloud computing technology},
	booktitle = {Proceedings of the 2010 10th {IEEE}/{ACM} {International} {Conference} on {Cluster}, {Cloud} and {Grid} {Computing}},
	publisher = {IEEE Computer Society},
	author = {Banzai, Takayuki and Koizumi, Hitoshi and Kanbayashi, Ryo and Imada, Takayuki and Hanawa, Toshihiro and Sato, Mitsuhisa},
	year = {2010},
	pages = {631--636},
}

@article{bachmann_developing_2014,
	title = {Developing secure software.},
	volume = {38},
	number = {4},
	journal = {Datenschutz und Datensicherheit},
	author = {Bachmann, Ruediger and Brucker, Achim D.},
	year = {2014},
	pages = {257--261},
}

@misc{canonical_bazaar_nodate,
	title = {Bazaar},
	author = {{Canonical}},
}

@misc{noauthor_operation_nodate,
	title = {Operation {Aurora}},
}

@misc{forums_numix_nodate,
	title = {{NUMIX} {GNOME} 3.20},
	author = {Forums, Archlinux},
}

@misc{arch_linux_official_nodate,
	title = {Official {Repositories} – {ArchWiki}},
	author = {{Arch Linux}},
}

@misc{archlinux_wiki__developers_howto_nodate,
	title = {{HOWTO} {Be} {A} {Packager}},
	author = {{Archlinux Wiki – Developers}},
}

@misc{arch_linux_ca-certificates_nodate,
	title = {ca-certificates 20160507-1},
	author = {{Arch Linux}},
}

@misc{noauthor_-toto_nodate,
	title = {in-toto transport for apt},
}

@misc{noauthor_apt_nodate,
	title = {Apt},
}

@misc{pauli_icloud_2015,
	title = {{iCloud} phishing attack hooks 39 {iOS} apps and {WeChat}},
	author = {Pauli, Darren},
	year = {2015},
	note = {Published: theregister},
}

@article{antal_information_2015,
	title = {Information revealed from scrolling interactions on mobile devices},
	volume = {56},
	journal = {Pattern Recognition Letters},
	author = {Antal, Margit and Bokor, Zsolt and Szabó, László Zsolt},
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {7--13},
}

@misc{apache_infrastructure_team_apacheorg_2010,
	title = {apache.org incident report for 04/09/2010},
	url = {https://blogs.apache.org/infra/entry/apache_org_04_09_2010},
	author = {{Apache Infrastructure Team}},
	year = {2010},
}

@misc{apache_infrastructure_team_apacheorg_2009,
	title = {apache.org incident report for 8/28/2009},
	url = {https://blogs.apache.org/infra/entry/apache_org_downtime_report},
	author = {{Apache Infrastructure Team}},
	year = {2009},
}

@misc{freeman_yet_2014,
	title = {Yet {Another} {Android} {Master} {Key} {Bug}},
	url = {http://www.saurik.com/id/19},
	author = {Freeman, Jay},
	year = {2014},
}

@misc{verduzu_xposed_2013,
	title = {Xposed {Patch} for {Master} {Key} and {Bug} 9695860 {Vulnerabilities}},
	url = {https://www.xda-developers.com/xposed-patch-for-master-key-and-bug-9695860-vulnerabilities/},
	author = {Verduzu, Will},
	year = {2013},
}

@misc{chitu_android_2013,
	title = {The {Android} {Bug} 8219321},
	url = {https://googlesystem.blogspot.com/2013/07/the-8219321-android-bug.html#gsc.tab=0},
	author = {Chitu, Alex},
	year = {2013},
}

@inproceedings{andreetto_glite_2008,
	title = {The {gLite} workload management system},
	volume = {119},
	booktitle = {J. of {Physics}: {Conf}. {Series}},
	publisher = {IOP Publishing},
	author = {Andreetto, Paolo and Andreozzi, Sergio and Avellino, Giuseppe and Beco, Stefano and Cavallini, Andrea and Cecchi, Marco and Ciaschini, Vincenzo and Dorise, Alvise and Giacomini, Francesco and Gianelle, Alessio and {others}},
	year = {2008},
	note = {Issue: 6},
	pages = {062007},
}

@book{barb_darrow_adobe_nodate,
	title = {Adobe source code breach; it's bad, real bad},
	author = {{Barb Darrow}},
}

@misc{arkin_adobe_2012,
	title = {Adobe to {Revoke} {Code} {Signing} {Certificate}},
	url = {https://blogs.adobe.com/conversations/2012/09/adobe-to-revoke-code-signing-certificate.html},
	author = {Arkin, Brad},
	year = {2012},
}

@misc{noauthor_dynamic_nodate,
	title = {Dynamic {Admission} {Control}},
}

@misc{consortium_for_information__software_quality_tool--tool_nodate,
	title = {Tool-to-{Tool} {Software} {Bill} of {Materials} {Exchange}},
	author = {{Consortium for Information \& Software Quality}},
}

@inproceedings{torres-arias_-toto_2019,
	address = {Santa Clara, CA},
	title = {in-toto: {Providing} farm-to-table guarantees for bits and bytes},
	isbn = {978-1-939133-06-9},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/torres-arias},
	booktitle = {28th {USENIX} {Security} {Symposium} ({USENIX} {Security} 19)},
	publisher = {USENIX Association},
	author = {Torres-Arias, Santiago and Afzali, Hammad and Kuppusamy, Trishank Karthik and Curtmola, Reza and Cappos, Justin},
	month = aug,
	year = {2019},
	pages = {1393--1410},
}

@misc{catalin_cimpanu_microsoft_nodate,
	title = {Microsoft, {FireEye} confirm {SolarWinds} supply chain attack},
	author = {{Catalin Cimpanu}},
}

@misc{national_telecommunications_and_information_administration_ntia_nodate,
	title = {{NTIA} {Software} {Component} {Transparency}},
	author = {{National Telecommunications and Information Administration}},
}

@misc{microsoft_security_response_center_customer_nodate,
	title = {Customer {Guidance} on {Recent} {Nation}-{State} {Cyber} {Attacks}},
	author = {{Microsoft Security Response Center}},
}

@misc{fireeye_highly_nodate,
	title = {Highly {Evasive} {Attacker} {Leverages} {SolarWinds} {Supply} {Chain} to {Compromise} {Multiple} {Global} {Victims} {With} {SUNBURST} {Backdoor}},
	author = {{FireEye}},
}

@misc{chris_williams_how_nodate,
	title = {How one developer just broke {Node}, {Babel} and thousands of projects in 11 lines of {JavaScript}},
	author = {{Chris Williams}},
}

@misc{thompson_reflections_nodate,
	title = {Reflections on {Trusting} {Trust}},
	author = {Thompson, Ken},
}

@misc{noauthor_software_nodate,
	title = {Software {Supply} {Chain} {Compromises}},
}

@misc{noauthor_recording_nodate,
	title = {Recording build information},
}

@misc{noauthor_tools-python_nodate,
	title = {tools-python},
}

@misc{noauthor_cyclonedx_nodate,
	title = {{CycloneDX} is {SBOM}: {Software} {Bill} of {Materials}},
}

@misc{torres-arias_celebrating_nodate,
	title = {Celebrating 1,000,000 entries in {Rekor}},
	author = {Torres-Arias, Santiago},
}

@misc{noauthor_isoiec_nodate,
	title = {{ISO}/{IEC} 5962:2021 ({SPDX} 2.2)},
}

@misc{noauthor_universal_nodate,
	title = {Universal “netmask” npm package, used by 270,000+ projects, vulnerable to octal input data: server-side request forgery, remote file inclusion, local file inclusion, and more ({CVE}-2021-28918)},
}

@misc{noauthor_npm_nodate,
	title = {{NPM} – {Ansi}-red},
}

@article{lindberg_coordinating_2016,
	title = {Coordinating {Interdependencies} in {Online} {Communities}: {A} {Study} of an {Open} {Source} {Software} {Project}},
	volume = {27},
	issn = {1047-7047},
	shorttitle = {Coordinating {Interdependencies} in {Online} {Communities}},
	url = {http://pubsonline.informs.org/doi/10.1287/isre.2016.0673},
	doi = {10.1287/isre.2016.0673},
	abstract = {To manage work interdependencies, online communities draw on a variety of arm’s length coordination mechanisms offered by information technology platforms and associated practices. However, “unresolved interdependencies” remain that cannot be addressed by such arm’s length mechanisms. These interdependencies reflect, for example, unidentified or emerging knowledge-based dependencies between the community members or unaccounted relationships between ongoing community tasks. At the same time, online communities cannot resort to hierarchical coordination mechanisms such as incentives or command structures to address such interdependencies. So, how do they manage such interdependencies? To address this question, we conduct an exploratory, theory-generating case study involving qualitative and computational analyses of development activities within an open source software community: Rubinius. We analyze the ongoing management of interdependencies within the community and find that unresolved interdependencies are associated with alternatively structured sequences of activities, which we define as routines. In particular, we observe that two distinct classes of interdependencies—development and developer interdependencies—are associated with alternative forms of routine variation. We identify two generalized routine components—direct implementation and knowledge integration, which address these two distinct classes of unresolved interdependencies. In particular, direct implementation deals with development interdependencies within the code that are not already coordinated through modular interfaces, while knowledge integration resolves unaccounted interdependencies between developers. We conclude with implications for research into organizing principles for online communities and note the significance of our findings for the study of coordination in organization studies in general.},
	number = {4},
	urldate = {2018-10-20},
	journal = {Information Systems Research},
	author = {Lindberg, Aron and Berente, Nicholas and Gaskin, James and Lyytinen, Kalle},
	month = dec,
	year = {2016},
	pages = {751--772},
}

@article{fang_understanding_2009,
	title = {Understanding {Sustained} {Participation} in {Open} {Source} {Software} {Projects}},
	volume = {25},
	issn = {0742-1222},
	url = {http://www.jstor.org/stable/40398952},
	abstract = {[Prior research into open source software (OSS) developer participation has emphasized individuals' motivations for joining these volunteer communities, but it has failed to explain why people stay or leave in the long run. Building upon Lave and Wenger's theory of legitimate peripheral participation (LPP), this paper offers a longitudinal investigation of one OSS community in which sustained participation is hypothesized to be associated with the coevolution of two major elements of LPP theory: "situated learning" (the process of acting knowledgeably and purposefully in the world) and "identity construction" (the process of being identified within the community). To test this hypothesis, data were collected from multiple sources, including online public project documents, electronic mail messages, tracker messages, and log files. Results from qualitative analyses revealed that initial conditions to participate did not effectively predict long-term participation, but that situated learning and identity construction behaviors were positively linked to sustained participation. Furthermore, this study reveals that sustained participants distinguished themselves by consistently engaging in situated learning that both made conceptual (advising others) and practical contributions (improving the code). Implications and future research are discussed.]},
	number = {4},
	urldate = {2018-10-31},
	journal = {Journal of Management Information Systems},
	author = {Fang, Yulin and Neufeld, Derrick},
	year = {2009},
	pages = {9--50},
}

@misc{noauthor_software_nodate-1,
	title = {The {Software} {Package} {Data} {Exchange}},
}

@article{mallapragada_user-generated_nodate,
	title = {User-{Generated} {Open} {Source} {Products}: {Founder}'s {Social} {Capital} and {Time} to {Product} {Release}},
	issn = {07322399, 1526548X},
	url = {http://www.jstor.org/stable/41488288},
	abstract = {Volunteer users employ collaborative Internet technologies to develop open source products, a form of usergenerated content, where time to product release is a crucial measure of project success. The open source community features two separate but related subcommunities: developer users who contribute time and effort to develop products and end users who act as collaborative testers and provide feedback. We develop hypotheses concerning how the location of the project's founders in the social network of developer users, the interplay of developer users and end users, and project and product characteristics affect time to product release. We use data on 817 development projects from SourceForge, a large open source community forum, to calibrate a split hazard model to test the hypotheses. That model supports the two-community conceptualization and most of the related hypotheses. The results have theoretical and managerial implications; for example, a pivotal position of founders in the developer user community can reduce time to product release by up to 31\%, and projects in which users are more engaged can experience an 11\% shorter time to product release compared with those projects in which they are not.},
	number = {3},
	journal = {Marketing Science},
	author = {Mallapragada, Girish and Grewal, Rajdeep and Lilien, Gary},
	note = {Publisher: INFORMS},
	pages = {474--492},
}

@article{di_tullio_governance_2013,
	title = {The {Governance} and {Control} of {Open} {Source} {Software} {Projects}},
	volume = {30},
	issn = {07421222},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=94649915&site=ehost-live},
	doi = {10.2753/MIS0742-1222300303},
	abstract = {A comprehensive set of governance mechanisms and dimensions were investigated to identify combinations of mechanisms that are effectively used together in on-going volunteer-based open source software (OSS) projects. Three configurations were identified: Defined Community, Open Community, and Authoritarian Community. Notably, Defined Community governance had the strongest coordination and project climate and had the most extensive use of outcome, behavior, and clan control mechanisms (controller driven). The controls in the Defined Community governance configuration appear to effectively enable open, coordinated contribution and participation from a wide variety of talented developers (one of the virtues of open source development) while managing the development process and outcomes. The results add to our theoretical understanding of control in different types of information systems projects, as the combination of control modes found in OSS projects is different from those found in previous research for internal or outsourced information systems development projects. This could be due to unique features of OSS projects, such as volunteer participation and the controller being part of the development team. The results provide guidance for practitioners about how to combine 19 identified governance mechanisms into effective project governance that stimulates productive participation.},
	number = {3},
	urldate = {2017-11-29},
	journal = {Journal of Management Information Systems},
	author = {Di Tullio, Dany and Staples, D. Sandy},
	year = {2013},
	keywords = {COMPUTER software development, CONTROL theory (Sociology), INFORMATION resources management, IS development, MANAGEMENT, MANAGEMENT of teams in the workplace, OSS, Open source software, configuration theory, control modes and mechanisms, control theory, coordination, development activity, governance, open source software projects, project climate},
	pages = {49--80},
}

@article{carcioppolo_human_2021,
	title = {Human versus chatbot: {Understanding} the role of emotion in health marketing communication for vaccines},
	issn = {0742-6046},
	shorttitle = {Human versus chatbot},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8441681/},
	doi = {10.1002/mar.21556},
	abstract = {Based on the theoretical framework of agency effect, this study examined the role of affect in influencing the effects of chatbot versus human brand representatives in the context of health marketing communication about HPV vaccines. We conducted a 2 (perceived agency: chatbot vs. human) × 3 (affect elicitation: embarrassment, anger, neutral) between‐subject lab experiment with 142 participants, who were randomly assigned to interact with either a perceived chatbot or a human representative. Key findings from self‐reported and behavioral data highlight the complexity of consumer–chatbot communication. Specifically, participants reported lower interaction satisfaction with the chatbot than with the human representative when anger was evoked. However, participants were more likely to disclose concerns of HPV risks and provide more elaborate answers to the perceived human representative when embarrassment was elicited. Overall, the chatbot performed comparably to the human representative in terms of perceived usefulness and influence over participants' compliance intention in all emotional contexts. The findings complement the Computers as Social Actors paradigm and offer strategic guidelines to capitalize on the relative advantages of chatbot versus human representatives.},
	urldate = {2022-01-12},
	journal = {Psychology \& Marketing},
	author = {Carcioppolo, Nicholas and Chuan, Ching‐Hua and Carcioppolo, Nicholas and Chuan, Ching‐Hua},
	month = jul,
	year = {2021},
	pmid = {34539051},
	pmcid = {PMC8441681},
	pages = {10.1002/mar.21556},
}

@misc{seals_executive_nodate,
	title = {Executive {Order} {Would} {Strengthen} {Cybersecurity} {Requirements} for {Federal} {Agencies}},
	author = {Seals, Tara},
}

@article{fitzgerald_transformation_2006,
	title = {The {Transformation} of {Open} {Source} {Software}},
	volume = {30},
	issn = {0276-7783},
	url = {http://www.jstor.org/stable/25148740},
	doi = {10.2307/25148740},
	abstract = {A frequent characterization of open source software is the somewhat outdated, mythical one of a collective of supremely talented software hackers freely volunteering their services to produce uniformly high-quality software. I contend that the open source software phenomenon has metamorphosed into a more mainstream and commercially viable form, which I label as OSS 2.0. I illustrate this transformation using a framework of process and product factors, and discuss how the bazaar metaphor, which up to now has been associated with the open source development process, has actually shifted to become a metaphor better suited to the OSS 2.0 product delivery and support process. Overall the OSS 2.0 phenomenon is significantly different from its free software antecedent. Its emergence accentuates the fundamental alteration of the basic ground rules in the software landscape, signifying the end of the proprietary-driven model that has prevailed for the past 20 years or so. Thus, a clear understanding of the characteristics of the emergent OSS 2.0 phenomenon is required to address key challenges for research and practice.},
	number = {3},
	urldate = {2018-10-16},
	journal = {MIS Quarterly},
	author = {Fitzgerald, Brian},
	year = {2006},
	pages = {587--598},
}

@article{kaynar_taxonomy_2016,
	title = {A taxonomy for attack graph generation and usage in network security},
	volume = {29},
	issn = {2214-2126},
	url = {https://www.sciencedirect.com/science/article/pii/S2214212616300011},
	doi = {https://doi.org/10.1016/j.jisa.2016.02.001},
	journal = {Journal of Information Security and Applications},
	author = {Kaynar, Kerem},
	year = {2016},
	keywords = {Exploit, Full attack graph, Reachability analysis, Vulnerability, Weakness},
	pages = {27--56},
}

@misc{joseph_menn_exclusive_nodate,
	title = {Exclusive: {Software} vendors would have to disclose breaches to {U}.{S}. government users under new order: draft},
	author = {Joseph Menn, Christopher Bing, Nandita Bose},
}

@misc{united_states_white_house_briefing_room_executive_nodate,
	title = {Executive {Order} on {Improving} the {Nation}'s {Cybersecurity}},
	url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/},
	author = {{United States White House Briefing Room}},
}

@misc{noauthor_press_nodate,
	title = {Press {Briefing} by {Press} {Secretary} {Jen} {Psaki} and {Deputy} {National} {Security} {Advisor} for {Cyber} and {Emerging} {Technology} {Anne} {Neuberger}, {February} 17, 2021},
}

@misc{synopsys_2020_nodate,
	title = {2020 {Open} {Source} {Security} and {Risk} {Analysis} ({OSSRA}) {Report}},
	author = {{Synopsys}},
}

@misc{elkind_us_nodate,
	title = {The {U}.{S}. {Spent} \$2.2 {Million} on a {Cybersecurity} {System} {That} {Wasn}’t {Implemented} — and {Might} {Have} {Stopped} a {Major} {Hack}},
	author = {Elkind, Peter and Gillum, Jack},
}

@article{acquisti_nudges_2017,
	title = {Nudges for privacy and security: {Understanding} and assisting users’ choices online},
	volume = {50},
	number = {3},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Acquisti, Alessandro and Adjerid, Idris and Balebako, Rebecca and Brandimarte, Laura and Cranor, Lorrie Faith and Komanduri, Saranga and Leon, Pedro Giovanni and Sadeh, Norman and Schaub, Florian and Sleeper, Manya and {others}},
	year = {2017},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--41},
}

@misc{journal_docker_nodate,
	title = {Docker {Hub} {Breach}, {What} to {Do} {Now}?},
	author = {Journal, Container},
}

@misc{blog_scaling_nodate,
	title = {Scaling {Docker}'s business to serve millions more developers: {Storage}},
	author = {Blog, Jean-Laurent de Morlhon – Docker},
}

@misc{brooks_protecting_nodate,
	title = {Protecting the {Electric} {Grid} with {DBOM}; {A} {Proof} of {Concept} {Demonstration}},
	author = {Brooks, Richard},
}

@misc{noauthor_go_nodate,
	title = {Go {Module} {Mirror}, {Index}, and {Checksum} {Database}},
}

@misc{noauthor_sigstore_nodate,
	title = {sigstore: {A} non-profit, public good software signing \& transparency service},
}

@misc{noauthor_codenotary_nodate,
	title = {codenotary: {End}-to-end tampering/manipulation protection for your software development cycle},
}

@inproceedings{newman_sigstore_2021,
	title = {Sigstore: software signing for everybody},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Newman, Zachary and Meyers, John Speed and Torres-Arias, Santiago},
	year = {2021},
}

@misc{noauthor_debian_nodate,
	title = {Debian {Bookworm} {Release} {Information}},
}

@misc{noauthor_red_nodate,
	title = {Red {Hat} {Enterprise} {Linux}},
}

@misc{noauthor_build_nodate,
	title = {Build {Any} {Application} {Anywhere}},
}

@misc{dinformation_objet_nodate,
	title = {Objet: {Sandworm} intrusion set campaign targeting {Centreon} systems},
	author = {d'information, Premier Ministre / Secrétariat Général de la Défense et de la Sécurité Nationale / Agence nationale de la sécurité des systèmes},
}

@article{maccormack_exploring_2012,
	title = {Exploring the duality between product and organizational architectures: {A} test of the “mirroring” hypothesis},
	volume = {41},
	issn = {0048-7333},
	shorttitle = {Exploring the duality between product and organizational architectures},
	url = {http://www.sciencedirect.com/science/article/pii/S0048733312001205},
	doi = {10.1016/j.respol.2012.04.011},
	abstract = {A variety of academic studies argue that a relationship exists between the structure of an organization and the design of the products that this organization produces. Specifically, products tend to “mirror” the architectures of the organizations in which they are developed. This dynamic occurs because the organization's governance structures, problem solving routines and communication patterns constrain the space in which it searches for new solutions. Such a relationship is important, given that product architecture has been shown to be an important predictor of product performance, product variety, process flexibility and even the path of industry evolution. We explore this relationship in the software industry. Our research takes advantage of a natural experiment, in that we observe products that fulfill the same function being developed by very different organizational forms. At one extreme are commercial software firms, in which the organizational participants are tightly-coupled, with respect to their goals, structure and behavior. At the other, are open source software communities, in which the participants are much more loosely-coupled by comparison. The mirroring hypothesis predicts that these different organizational forms will produce products with distinctly different architectures. Specifically, loosely-coupled organizations will develop more modular designs than tightly-coupled organizations. We test this hypothesis, using a sample of matched-pair products. We find strong evidence to support the mirroring hypothesis. In all of the pairs we examine, the product developed by the loosely-coupled organization is significantly more modular than the product from the tightly-coupled organization. We measure modularity by capturing the level of coupling between a product's components. The magnitude of the differences is substantial—up to a factor of six, in terms of the potential for a design change in one component to propagate to others. Our results have significant managerial implications, in highlighting the impact of organizational design decisions on the technical structure of the artifacts that these organizations subsequently develop.},
	number = {8},
	urldate = {2017-09-06},
	journal = {Research Policy},
	author = {MacCormack, Alan and Baldwin, Carliss and Rusnak, John},
	month = oct,
	year = {2012},
	keywords = {Architecture, Modularity, Open-source software, Organizational design, Product design},
	pages = {1309--1324},
}

@article{singh_small-world_2010,
	title = {The {Small}-world {Effect}: {The} {Influence} of {Macro}-level {Properties} of {Developer} {Collaboration} {Networks} on {Open}-source {Project} {Success}},
	volume = {20},
	issn = {1049-331X},
	shorttitle = {The {Small}-world {Effect}},
	url = {http://doi.acm.org/10.1145/1824760.1824763},
	doi = {10.1145/1824760.1824763},
	abstract = {In this study we investigate the impact of community-level networks—relationships that exist among developers in an OSS community—on the productivity of member developers. Specifically, we argue that OSS community networks characterized by small-world properties would positively influence the productivity of the member developers by providing them with speedy and reliable access to more quantity and variety of information and knowledge resources. Specific hypotheses are developed and tested using longitudinal data on a large panel of 4,279 projects from 15 different OSS communities hosted at Sourceforge. Our results suggest that significant variation exists in small-world properties of OSS communities at Sourceforge. After accounting for project, foundry, and time-specific observed and unobserved effects, we found a statistically significant relationship between small-world properties of a community and the technical and commercial success of the software produced by its members. In contrast to the findings of prior research, we also found the lack of a significant relationship between closeness and betweenness centralities of the project teams and their success. These results were robust to a number of controls and model specifications.},
	number = {2},
	urldate = {2018-01-16},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Singh, Param Vir},
	month = sep,
	year = {2010},
	keywords = {Collaborative Software Development, online community, open source software development, productivity, small world networks, social networks, team formation},
	pages = {6:1--6:27},
}

@misc{noauthor_debian_nodate-1,
	title = {Debian {Buster} {Release} {Information}},
}

@article{maccormack_exploring_2006,
	title = {Exploring the {Structure} of {Complex} {Software} {Designs}: {An} {Empirical} {Study} of {Open} {Source} and {Proprietary} {Code}},
	volume = {52},
	issn = {0025-1909},
	shorttitle = {Exploring the {Structure} of {Complex} {Software} {Designs}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1060.0552},
	doi = {10.1287/mnsc.1060.0552},
	abstract = {This paper reports data from a study that seeks to characterize the differences in design structure between complex software products. We use design structure matrices (DSMs) to map dependencies between the elements of a design and define metrics that allow us to compare the structures of different designs. We use these metrics to compare the architectures of two software products—the Linux operating system and the Mozilla Web browser—that were developed via contrasting modes of organization: specifically, open source versus proprietary development. We then track the evolution of Mozilla, paying attention to a purposeful “redesign” effort undertaken with the intention of making the product more “modular.” We find significant differences in structure between Linux and the first version of Mozilla, suggesting that Linux had a more modular architecture. Yet we also find that the redesign of Mozilla resulted in an architecture that was significantly more modular than that of its predecessor and, indeed, than that of Linux. Our results, while exploratory, are consistent with a view that different modes of organization are associated with designs that possess different structures. However, they also suggest that purposeful managerial actions can have a significant impact in adapting a design’s structure. This latter result is important given recent moves to release proprietary software into the public domain. These moves are likely to fail unless the product possesses an “architecture for participation.”},
	number = {7},
	urldate = {2017-09-06},
	journal = {Management Science},
	author = {MacCormack, Alan and Rusnak, John and Baldwin, Carliss Y.},
	month = jul,
	year = {2006},
	pages = {1015--1030},
}

@inproceedings{blincoe_all_2013,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2013},
	title = {Do {All} {Task} {Dependencies} {Require} {Coordination}? {The} {Role} of {Task} {Properties} in {Identifying} {Critical} {Coordination} {Needs} in {Software} {Projects}},
	isbn = {978-1-4503-2237-9},
	shorttitle = {Do {All} {Task} {Dependencies} {Require} {Coordination}?},
	url = {http://doi.acm.org/10.1145/2491411.2491440},
	doi = {10.1145/2491411.2491440},
	abstract = {Several methods exist to detect the coordination needs within software teams. Evidence exists that developers’ awareness about coordination needs improves work performance. Distinguishing with certainty between critical and trivial coordination needs and identifying and prioritizing which specific tasks a pair of developers should coordinate about remains an open problem. We investigate what work dependencies should be considered when establishing coordination needs within a development team. We use our conceptualization of work dependencies named Proximity and leverage machine learning techniques to analyze what additional task properties are indicative of coordination needs. In a case study of the Mylyn project, we were able to identify from all potential coordination requirements a subset of 17\% that are most critical. We define critical coordination requirements as those that can cause the most disruption to task duration when left unmanaged. These results imply that coordination awareness tools could be enhanced to make developers aware of only the coordination needs that can bring about the highest performance benefit.},
	urldate = {2017-11-04},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Blincoe, Kelly and Valetto, Giuseppe and Damian, Daniela},
	year = {2013},
	keywords = {Awareness, Collaborative Software Development, Coordination Requirements, Machine Learning, Task Dependencies, proximity},
	pages = {213--223},
}

@misc{noauthor_open_nodate,
	title = {Open {Source} {Software} {Supply} {Chain} {Security}},
	url = {https://www.linuxfoundation.org/en/resources/publications/open-source-software-supply-chain-security/},
	language = {en-US},
	urldate = {2021-04-10},
	note = {Publication Title: Linux Foundation},
}

@article{kovalenko_does_2018,
	title = {Does reviewer recommendation help developers?},
	issn = {0098-5589},
	doi = {10.1109/TSE.2018.2868367},
	abstract = {Selecting reviewers for code changes is a critical step for an efficient code review process. Recent studies propose automated reviewer recommendation algorithms to support developers in this task. However, the evaluation of recommendation algorithms, when done apart from their target systems and users (i.e. code review tools and change authors), leaves out important aspects: perception of recommendations, influence of recommendations on human choices, and their effect on user experience. This study is the first to evaluate a reviewer recommender in vivo. We compare historical reviewers and recommendations for over 21,000 code reviews performed with a deployed recommender in a company environment and set to measure the influence of recommendations on users' choices, along with other performance metrics. Having found no evidence of influence, we turn to the users of the recommender. Through interviews and a survey we find that, though perceived as relevant, reviewer recommendations rarely provide additional value for the respondents. We confirm this finding with a larger study at another company. The confirmation of this finding brings up a case for more user-centric approaches to designing and evaluating the recommenders. Finally, we investigate information needs of developers during reviewer selection and discuss promising directions for the next generation of reviewer recommendation tools.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kovalenko, V. and Tintarev, N. and Pasynkov, E. and Bird, C. and Bacchelli, A.},
	year = {2018},
	keywords = {Code Review, Companies, Empirical Software Engineering, In vivo, Measurement, Recommender systems, Reviewer Recommendation, Software, Software engineering, Tools},
	pages = {1--1},
}

@inproceedings{joblin_developer_2015,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '15},
	title = {From {Developer} {Networks} to {Verified} {Communities}: {A} {Fine}-grained {Approach}},
	isbn = {978-1-4799-1934-5},
	shorttitle = {From {Developer} {Networks} to {Verified} {Communities}},
	url = {http://dl.acm.org/citation.cfm?id=2818754.2818824},
	abstract = {Effective software engineering demands a coordinated effort. Unfortunately, a comprehensive view on developer coordination is rarely available to support software-engineering decisions, despite the significant implications on software quality, software architecture, and developer productivity. We present a fine-grained, verifiable, and fully automated approach to capture a view on developer coordination, based on commit information and source-code structure, mined from version-control systems. We apply methodology from network analysis and machine learning to identify developer communities automatically. Compared to previous work, our approach is fine-grained, and identifies statistically significant communities using order-statistics and a community-verification technique based on graph conductance. To demonstrate the scalability and generality of our approach, we analyze ten open-source projects with complex and active histories, written in various programming languages. By surveying 53 open-source developers from the ten projects, we validate the authenticity of inferred community structure with respect to reality. Our results indicate that developers of open-source projects form statistically significant community structures and this particular view on collaboration largely coincides with developers' perceptions of real-world collaboration.},
	urldate = {2019-04-21},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Software} {Engineering} - {Volume} 1},
	publisher = {IEEE Press},
	author = {Joblin, Mitchell and Mauerer, Wolfgang and Apel, Sven and Siegmund, Janet and Riehle, Dirk},
	year = {2015},
	pages = {563--573},
}

@inproceedings{adams_coordination_2009,
	title = {Coordination and productivity issues in free software: {The} role of brooks' law},
	shorttitle = {Coordination and productivity issues in free software},
	doi = {10.1109/ICSM.2009.5306308},
	abstract = {Proponents of the free software paradigm have argued that some of the most established software engineering principles do not fully apply when considered in an open, distributed approach found in free software development. The objective of this research is to empirically examine the Brooks' law in a free software context. The principle is separated out into its two primary premises: the first is based on a developer's ability to become productive when joining a new team; the second premise relates to the quality of coordination as the team grows. Three large projects are studied for this purpose: KDE, Plone and Evince. Based on empirical evidence, the paper provides two main contributions: based on the first premise of Brooks' law, it claims that coordination costs increase only in a very specific phase for free software projects. After that, these costs become quasi-constant. Secondly, it shows that a ramp up period exists in free software projects, and this period marks the divide between projects that are successful at engaging new contributors from others that only benefit from occasional new contributors.},
	booktitle = {2009 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	author = {Adams, P. J. and Capiluppi, A. and Boldyreff, C.},
	month = sep,
	year = {2009},
	keywords = {Brooks' law, Collaborative software, Computer bugs, Costs, Evince, KDE, Plone, Productivity, Programming profession, Software engineering, Software maintenance, Software quality, Testing, coordination issue, free software development, productivity issue, public domain software, software engineering, software engineering principles},
	pages = {319--328},
}

@inproceedings{wen_software_2007,
	title = {Software {Systems} as {Complex} {Networks}},
	isbn = {978-1-4244-1327-0 978-1-4244-1328-7},
	url = {http://ieeexplore.ieee.org/document/4341879/},
	doi = {10.1109/COGINF.2007.4341879},
	urldate = {2019-04-20},
	booktitle = {6th {IEEE} {International} {Conference} on {Cognitive} {Informatics}},
	publisher = {IEEE},
	author = {Wen, Lian and Kirk, Diana and Dromey, R. G.},
	month = aug,
	year = {2007},
	keywords = {Application software, Complex networks, Control systems, Hardware, Java, Java libraries, Kirk field collapse effect, Neurons, Software libraries, Software maintenance, Software systems, complex networks, component dependency network, large-scale systems, scale-free network model, software libraries, software maintenance, software reengineering, software systems, systems re-engineering},
	pages = {106--115},
}

@article{mangalaraj_distributed_2014,
	title = {Distributed {Cognition} in {Software} {Design}: {An} {Experimental} {Investigation} of the {Role} of {Design} {Patterns} and {Collaboration}},
	volume = {38},
	issn = {02767783, 21629730},
	shorttitle = {Distributed {Cognition} in {Software} {Design}},
	url = {https://misq.org/distributed-cognition-in-software-design-an-experimental-investigation-of-the-role-of-design-patterns-and-collaboration.html},
	doi = {10.25300/MISQ/2014/38.1.12},
	abstract = {Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.},
	language = {en},
	number = {1},
	urldate = {2018-11-23},
	journal = {MIS Quarterly},
	author = {Mangalaraj, George and Nerur, Sridhar and Mahapatra, RadhaKanta and Price, Kenneth H.},
	month = jan,
	year = {2014},
	pages = {249--274},
}

@article{bolici_stigmergic_2016,
	series = {Special {Issue} of {Cognitive} {Systems} {Research} – {Human}-{Human} {Stigmergy}},
	title = {Stigmergic coordination in {FLOSS} development teams: {Integrating} explicit and implicit mechanisms},
	volume = {38},
	issn = {1389-0417},
	shorttitle = {Stigmergic coordination in {FLOSS} development teams},
	url = {http://www.sciencedirect.com/science/article/pii/S1389041715000339},
	doi = {10.1016/j.cogsys.2015.12.003},
	abstract = {The vast majority of literature on coordination in team-based projects has drawn on a conceptual separation between explicit (e.g. plans, feedback) and implicit coordination mechanisms (e.g. mental maps, shared knowledge). This analytic distinction presents some limitations in explaining how coordination is reached in organizations characterized by distributed teams, scarce face to face meetings and fuzzy and changing lines of authority, as in free/libre open source software (FLOSS) development. Analyzing empirical illustrations from two FLOSS projects, we highlight the existence of a peculiar model, stigmergic coordination, which includes aspects of both implicit and explicit mechanisms. The work product itself (implicit) and the characteristics under which it is shared (explicit) play an under-appreciated role in helping software developers manage dependencies as they arise. We develop this argument beyond the existing literature by working with an existing coordination framework, considering the role that the codebase itself might play at each step. We also discuss the features and the practices to support stigmergic coordination in distributed teams, as well as recommendations for future research.},
	urldate = {2019-01-09},
	journal = {Cognitive Systems Research},
	author = {Bolici, Francesco and Howison, James and Crowston, Kevin},
	month = jun,
	year = {2016},
	keywords = {Coordination mechanisms, Distributed teams, FLOSS teams, Stigmergic coordination},
	pages = {14--22},
}

@article{von_krogh_community_2003,
	series = {Open {Source} {Software} {Development}},
	title = {Community, joining, and specialization in open source software innovation: a case study},
	volume = {32},
	issn = {0048-7333},
	shorttitle = {Community, joining, and specialization in open source software innovation},
	url = {http://www.sciencedirect.com/science/article/pii/S0048733303000507},
	doi = {10.1016/S0048-7333(03)00050-7},
	abstract = {This paper develops an inductive theory of the open source software (OSS) innovation process by focussing on the creation of Freenet, a project aimed at developing a decentralized and anonymous peer-to-peer electronic file sharing network. We are particularly interested in the strategies and processes by which new people join the existing community of software developers, and how they initially contribute code. Analyzing data from multiple sources on the Freenet software development process, we generate the constructs of “joining script”, “specialization”, “contribution barriers”, and “feature gifts”, and propose relationships among these. Implications for theory and research are discussed.},
	number = {7},
	urldate = {2018-11-05},
	journal = {Research Policy},
	author = {von Krogh, Georg and Spaeth, Sebastian and Lakhani, Karim R},
	month = jul,
	year = {2003},
	keywords = {Collective action, Community, Innovation, Open source software, Virtual teams},
	pages = {1217--1241},
}

@article{lindberg_coordinating_2016,
	title = {Coordinating {Interdependencies} in {Online} {Communities}: {A} {Study} of an {Open} {Source} {Software} {Project}},
	volume = {27},
	issn = {1047-7047},
	shorttitle = {Coordinating {Interdependencies} in {Online} {Communities}},
	url = {http://pubsonline.informs.org/doi/10.1287/isre.2016.0673},
	doi = {10.1287/isre.2016.0673},
	abstract = {To manage work interdependencies, online communities draw on a variety of arm’s length coordination mechanisms offered by information technology platforms and associated practices. However, “unresolved interdependencies” remain that cannot be addressed by such arm’s length mechanisms. These interdependencies reflect, for example, unidentified or emerging knowledge-based dependencies between the community members or unaccounted relationships between ongoing community tasks. At the same time, online communities cannot resort to hierarchical coordination mechanisms such as incentives or command structures to address such interdependencies. So, how do they manage such interdependencies? To address this question, we conduct an exploratory, theory-generating case study involving qualitative and computational analyses of development activities within an open source software community: Rubinius. We analyze the ongoing management of interdependencies within the community and find that unresolved interdependencies are associated with alternatively structured sequences of activities, which we define as routines. In particular, we observe that two distinct classes of interdependencies—development and developer interdependencies—are associated with alternative forms of routine variation. We identify two generalized routine components—direct implementation and knowledge integration, which address these two distinct classes of unresolved interdependencies. In particular, direct implementation deals with development interdependencies within the code that are not already coordinated through modular interfaces, while knowledge integration resolves unaccounted interdependencies between developers. We conclude with implications for research into organizing principles for online communities and note the significance of our findings for the study of coordination in organization studies in general.},
	number = {4},
	urldate = {2018-10-20},
	journal = {Information Systems Research},
	author = {Lindberg, Aron and Berente, Nicholas and Gaskin, James and Lyytinen, Kalle},
	month = dec,
	year = {2016},
	pages = {751--772},
}

@article{spinellis_how_2004,
	title = {How is open source affecting software development?},
	volume = {21},
	issn = {0740-7459},
	doi = {10.1109/MS.2004.1259204},
	number = {1},
	journal = {IEEE Software},
	author = {Spinellis, D. and Szyperski, C.},
	month = jan,
	year = {2004},
	keywords = {Automation, Business communication, Law, Licenses, Open source software, Operating systems, Packaging, Programming, Software libraries, Software packages},
	pages = {28--33},
}

@article{shaikh_folding_2016,
	title = {Folding and {Unfolding}: {Balancing} {Openness} and {Transparency} in {Open} {Source} {Communities}},
	volume = {27},
	issn = {1047-7047},
	shorttitle = {Folding and {Unfolding}},
	url = {https://pubsonline-informs-org.ezproxy.lib.purdue.edu/doi/abs/10.1287/isre.2016.0646},
	doi = {10.1287/isre.2016.0646},
	abstract = {Open source communities rely on the espoused premise of complete openness and transparency of source code and development process. Yet, openness and transparency at times need to be balanced out with moments of less open and transparent work. Through our detailed study of Linux Kernel development, we build a theory that explains that transparency and openness are nuanced and changing qualities that certain developers manage as they use multiple digital technologies and create, in moments of needs, more opaque and closed digital spaces of work. We refer to these spaces as digital folds. Our paper contributes to the extant literature by providing a process theory of how transparency and openness are balanced with opacity and closure in open source communities according to the needs of the development work; by conceptualizing the nature of digital folds and their role in providing quiet spaces of work; and, by articulating how the process of digital folding and unfolding is made far more possible by select elite actors’ navigating the line between the pragmatics of coding and the accepted ideology of openness and transparency.},
	number = {4},
	urldate = {2017-12-17},
	journal = {Information Systems Research},
	author = {Shaikh, Maha and Vaast, Emmanuelle},
	month = sep,
	year = {2016},
	pages = {813--833},
}

@article{mallapragada_user-generated_2012,
	title = {User-{Generated} {Open} {Source} {Products}: {Founder}'s {Social} {Capital} and {Time} to {Product} {Release}},
	volume = {31},
	issn = {0732-2399},
	shorttitle = {User-{Generated} {Open} {Source} {Products}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mksc.1110.0690},
	doi = {10.1287/mksc.1110.0690},
	abstract = {Volunteer users employ collaborative Internet technologies to develop open source products, a form of user-generated content, where time to product release is a crucial measure of project success. The open source community features two separate but related subcommunities: developer users who contribute time and effort to develop products and end users who act as collaborative testers and provide feedback. We develop hypotheses concerning how the location of the project's founders in the social network of developer users, the interplay of developer users and end users, and project and product characteristics affect time to product release. We use data on 817 development projects from SourceForge, a large open source community forum, to calibrate a split hazard model to test the hypotheses. That model supports the two-community conceptualization and most of the related hypotheses. The results have theoretical and managerial implications; for example, a pivotal position of founders in the developer user community can reduce time to product release by up to 31 and projects in which users are more engaged can experience an 11 time to product release compared with those projects in which they are not.},
	number = {3},
	urldate = {2016-01-05},
	journal = {Marketing Science},
	author = {Mallapragada, Girish and Grewal, Rajdeep and Lilien, Gary},
	month = feb,
	year = {2012},
	pages = {474--492},
}

@article{germonprez_theory_2016,
	title = {A {Theory} of {Responsive} {Design}: {A} {Field} {Study} of {Corporate} {Engagement} with {Open} {Source} {Communities}},
	issn = {1047-7047},
	shorttitle = {A {Theory} of {Responsive} {Design}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/isre.2016.0662},
	doi = {10.1287/isre.2016.0662},
	abstract = {Although our general knowledge about open source communities is extensive, we are only beginning to understand the increasingly common practices by which corporations design software through engagement with these communities. In response, we combine design theorizing with field-study research (1) to analyze rich qualitative data from over 40 corporations participating in the Linux open source community and (2) to synthesize the observed corporate-open source community engagements into a new type of information systems design theory that we call responsive design. Empirically, we document how corporate participants in these contexts respond to market decisions, interdependent ideologies, and distributed relationships by continuously establishing and maintaining connections with community members; connections that stem from the social and material rules inherent in the open source community. Based on these observations, we create the theory of responsive design as a particular form of corporate software design which, beyond the inclusion of external participants, distinguishes itself from traditional monocentric design in which one corporation controls a dedicated team of software designers focused on solving an isolated and singular organizational problem. Guided by the principles of interconnection, opportunism, and domestication, we define responsive design as the kind of design approach that enables corporate participants to create and maintain productive design practices in response to the complex and dynamic landscapes of activities that are the foundation of corporate-communal engagements. We conclude with a discussion of the theoretical and practical implications of this new form of corporate software design.},
	urldate = {2016-11-28},
	journal = {Information Systems Research},
	author = {Germonprez, Matt and Kendall, Julie E. and Kendall, Kenneth E. and Mathiassen, Lars and Young, Brett and Warner, Brian},
	month = nov,
	year = {2016},
}

@inproceedings{joblin_classifying_2017,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '17},
	title = {Classifying {Developers} into {Core} and {Peripheral}: {An} {Empirical} {Study} on {Count} and {Network} {Metrics}},
	isbn = {978-1-5386-3868-2},
	shorttitle = {Classifying {Developers} into {Core} and {Peripheral}},
	url = {https://doi.org/10.1109/ICSE.2017.23},
	doi = {10.1109/ICSE.2017.23},
	abstract = {Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work.},
	urldate = {2018-10-28},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Joblin, Mitchell and Apel, Sven and Hunsen, Claus and Mauerer, Wolfgang},
	year = {2017},
	pages = {164--174},
}

@article{fitzgerald_transformation_2006,
	title = {The {Transformation} of {Open} {Source} {Software}},
	volume = {30},
	issn = {0276-7783},
	url = {http://www.jstor.org/stable/25148740},
	doi = {10.2307/25148740},
	abstract = {A frequent characterization of open source software is the somewhat outdated, mythical one of a collective of supremely talented software hackers freely volunteering their services to produce uniformly high-quality software. I contend that the open source software phenomenon has metamorphosed into a more mainstream and commercially viable form, which I label as OSS 2.0. I illustrate this transformation using a framework of process and product factors, and discuss how the bazaar metaphor, which up to now has been associated with the open source development process, has actually shifted to become a metaphor better suited to the OSS 2.0 product delivery and support process. Overall the OSS 2.0 phenomenon is significantly different from its free software antecedent. Its emergence accentuates the fundamental alteration of the basic ground rules in the software landscape, signifying the end of the proprietary-driven model that has prevailed for the past 20 years or so. Thus, a clear understanding of the characteristics of the emergent OSS 2.0 phenomenon is required to address key challenges for research and practice.},
	number = {3},
	urldate = {2018-10-16},
	journal = {MIS Quarterly},
	author = {Fitzgerald, Brian},
	year = {2006},
	pages = {587--598},
}

@article{fang_understanding_2009,
	title = {Understanding {Sustained} {Participation} in {Open} {Source} {Software} {Projects}},
	volume = {25},
	issn = {0742-1222},
	url = {http://www.jstor.org/stable/40398952},
	abstract = {[Prior research into open source software (OSS) developer participation has emphasized individuals' motivations for joining these volunteer communities, but it has failed to explain why people stay or leave in the long run. Building upon Lave and Wenger's theory of legitimate peripheral participation (LPP), this paper offers a longitudinal investigation of one OSS community in which sustained participation is hypothesized to be associated with the coevolution of two major elements of LPP theory: "situated learning" (the process of acting knowledgeably and purposefully in the world) and "identity construction" (the process of being identified within the community). To test this hypothesis, data were collected from multiple sources, including online public project documents, electronic mail messages, tracker messages, and log files. Results from qualitative analyses revealed that initial conditions to participate did not effectively predict long-term participation, but that situated learning and identity construction behaviors were positively linked to sustained participation. Furthermore, this study reveals that sustained participants distinguished themselves by consistently engaging in situated learning that both made conceptual (advising others) and practical contributions (improving the code). Implications and future research are discussed.]},
	number = {4},
	urldate = {2018-10-31},
	journal = {Journal of Management Information Systems},
	author = {Fang, Yulin and Neufeld, Derrick},
	year = {2009},
	pages = {9--50},
}

@article{ducheneaut_socialization_2005,
	title = {Socialization in an {Open} {Source} {Software} {Community}: {A} {Socio}-{Technical} {Analysis}},
	volume = {14},
	issn = {1573-7551},
	shorttitle = {Socialization in an {Open} {Source} {Software} {Community}},
	url = {https://doi.org/10.1007/s10606-005-9000-1},
	doi = {10.1007/s10606-005-9000-1},
	abstract = {Open Source Software (OSS) development is often characterized as a fundamentally new way to develop software. Past analyses and discussions, however, have treated OSS projects and their organization mostly as a static phenomenon. Consequently, we do not know how these communities of software developers are sustained and reproduced over time through the progressive integration of new members. To shed light on this issue I report on my analyses of socialization in a particular OSS community. In particular, I document the relationships OSS newcomers develop over time with both the social and material aspects of a project. To do so, I combine two mutually informing activities: ethnography and the use of software specially designed to visualize and explore the interacting networks of human and material resources incorporated in the email and code databases of OSS. Socialization in this community is analyzed from two perspectives: as an individual learning process and as a political process. From these analyses it appears that successful participants progressively construct identities as software craftsmen, and that this process is punctuated by specific rites of passage. Successful participants also understand the political nature of software development and progressively enroll a network of human and material allies to support their efforts. I conclude by discussing how these results could inform the design of software to support socialization in OSS projects, as well as practical implications for the future of these projects.},
	language = {en},
	number = {4},
	urldate = {2018-10-29},
	journal = {Computer Supported Cooperative Work (CSCW)},
	author = {DUCHENEAUT, NICOLAS},
	month = aug,
	year = {2005},
	keywords = {Open Source, actor-network, learning, socialization, software development},
	pages = {323--368},
}

@article{di_tullio_governance_2013,
	title = {The {Governance} and {Control} of {Open} {Source} {Software} {Projects}},
	volume = {30},
	issn = {07421222},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=94649915&site=ehost-live},
	doi = {10.2753/MIS0742-1222300303},
	abstract = {A comprehensive set of governance mechanisms and dimensions were investigated to identify combinations of mechanisms that are effectively used together in on-going volunteer-based open source software (OSS) projects. Three configurations were identified: Defined Community, Open Community, and Authoritarian Community. Notably, Defined Community governance had the strongest coordination and project climate and had the most extensive use of outcome, behavior, and clan control mechanisms (controller driven). The controls in the Defined Community governance configuration appear to effectively enable open, coordinated contribution and participation from a wide variety of talented developers (one of the virtues of open source development) while managing the development process and outcomes. The results add to our theoretical understanding of control in different types of information systems projects, as the combination of control modes found in OSS projects is different from those found in previous research for internal or outsourced information systems development projects. This could be due to unique features of OSS projects, such as volunteer participation and the controller being part of the development team. The results provide guidance for practitioners about how to combine 19 identified governance mechanisms into effective project governance that stimulates productive participation.},
	number = {3},
	urldate = {2017-11-29},
	journal = {Journal of Management Information Systems},
	author = {Di Tullio, Dany and Staples, D. Sandy},
	year = {2013},
	keywords = {COMPUTER software development, CONTROL theory (Sociology), INFORMATION resources management, IS development, MANAGEMENT, MANAGEMENT of teams in the workplace, OSS, Open source software, configuration theory, control modes and mechanisms, control theory, coordination, development activity, governance, open source software projects, project climate},
	pages = {49--80},
}

@inproceedings{de_souza_seeking_2005,
	address = {Sanibel Island, Florida, USA},
	title = {Seeking the source: software source code as a social and technical artifact},
	isbn = {978-1-59593-223-5},
	shorttitle = {Seeking the source},
	url = {http://portal.acm.org/citation.cfm?doid=1099203.1099239},
	doi = {10.1145/1099203.1099239},
	language = {en},
	urldate = {2018-11-06},
	booktitle = {Proceedings of the 2005 international {ACM} {SIGGROUP} conference on {Supporting} group work - {GROUP} '05},
	publisher = {ACM Press},
	author = {de Souza, Cleidson and Froehlich, Jon and Dourish, Paul},
	year = {2005},
	pages = {197},
}

@article{dahlander_progressing_2010,
	title = {Progressing to the {Center}: {Coordinating} {Project} {Work}},
	volume = {22},
	issn = {1047-7039},
	shorttitle = {Progressing to the {Center}},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/orsc.1100.0571},
	doi = {10.1287/orsc.1100.0571},
	abstract = {Project forms of organizing are theorized to rely upon horizontal as opposed to vertical lines of authority, but few have examined how this shift affects progression—how people advance in an organization. We argue that progression without hierarchy unfolds when people assume lateral authority over project tasks without managing people. With a longitudinal study of a mature, collectively managed open source software project, we predict the individual behaviors that enable progression to lateral authority roles at two different stages. Although technical contributions are initially important, coordination work is more critical at a subsequent stage. We then explore how lateral authority roles affect subsequent behavior—after gaining authority, individuals spend significantly more time coordinating project work. Our research shows how people progress to the center as opposed to up a hierarchy, and how progression differs by stage and specifies the theoretical relationship between lateral authority roles and the coordination of project work.},
	number = {4},
	journal = {Organization Science},
	author = {Dahlander, Linus and O'Mahony, Siobhan},
	month = sep,
	year = {2010},
	pages = {961--979},
}

@article{cataldo_coordination_2013,
	title = {Coordination {Breakdowns} and {Their} {Impact} on {Development} {Productivity} and {Software} {Failures}},
	volume = {39},
	issn = {0098-5589},
	doi = {10.1109/TSE.2012.32},
	abstract = {The success of software development projects depends on carefully coordinating the effort of many individuals across the multiple stages of the development process. In software engineering, modularization is the traditional technique intended to reduce the interdependencies among modules that constitute a system. Reducing technical dependencies, the theory argues, results in a reduction of work dependencies between teams developing interdependent modules. Although that research stream has been quite influential, it considers a static view of the problem of coordination in engineering activities. Building on a dynamic view of coordination, we studied the relationship between socio-technical congruence and software quality and development productivity. In order to investigate the generality of our findings, our analyses were performed on two large-scale projects from two companies with distinct characteristics in terms of product and process maturity. Our results revealed that the gaps between coordination requirements and the actual coordination activities carried out by the developers significantly increased software failures. Our analyses also showed that higher levels of congruence are associated with improved development productivity. Finally, our results showed the congruence between dependencies and coordinative actions is critical both in mature development settings as well as in novel and dynamic development contexts.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Cataldo, M. and Herbsleb, J. D.},
	month = mar,
	year = {2013},
	keywords = {Complexity theory, Context, Coordination Requirements, Metrics/measurement, Organizations, Programming, coordination activity, coordination breakdowns, development process, development productivity, dynamic development contexts, engineering activity, interdependent modules, large-scale projects, modularization, organizational management and coordination, process maturity, product maturity, productivity, quality analysis and evaluation, socio-technical congruence, software development projects, software engineering, software failures, software metrics, software quality, software reliability, technical dependency, work dependency},
	pages = {343--360},
}

@article{david_dynamics_2008,
	title = {Dynamics of innovation in an “open source” collaboration environment: lurking, laboring, and launching {FLOSS} projects on {SourceForge}},
	volume = {17},
	shorttitle = {Dynamics of innovation in an “open source” collaboration environment},
	url = {https://econpapers.repec.org/article/oupindcch/v_3a17_3ay_3a2008_3ai_3a4_3ap_3a647-710.htm},
	abstract = {A systems analysis perspective is adopted to examine the critical properties of the Free/Libre/Open Source Software (FLOSS) mode of innovation, as reflected on the SourceForge platform ( SF.net ). This approach re-scales March's ( 1991 ) framework and applies it to characterize the “innovation system” of a “distributed organization” of interacting agents in a virtual collaboration environment, rather than to innovation within a firm. March ( 1991 ) views the process of innovation at the organizational level as the coupling of sub-processes of exploration and exploitation. Correspondingly, the innovation system of the virtual collaboration environment represented by SF.net is an emergent property of two “coupled” processes: one involves the interactions among agents searching the locale for information and knowledge resources to use in designing novel software products (i.e., exploration ), and the other involves the mobilization of individuals’ capabilities for application in the software development projects that become established on the platform (i.e., exploitation ). The micro-dynamics of this system are studied empirically by constructing transition probability matrices representing the movements of 222,835 SF.net users among seven different activity states, which range from “lurking” (not contributing or contributing to projects without becoming a member) to “laboring” (joining one or more projects as members), and to “launching” (founding one or more projects) within each successive 6-month interval. The estimated probabilities are found to form first-order Markov chains describing ergodic processes. This makes it possible the computation of the equilibrium distribution of agents among the states, thereby suppressing transient effects and revealing persisting patterns of project joining and project launching. The latter show the FLOSS innovation process on SF.net to be highly dissipative: a very large proportion of the registered “developers” fail to become even minimally active on the platform. There is nevertheless an active core of mobile project joiners, and a (still smaller) core of project founders who persist in creating new projects. The structure of these groups’ interactions (as displayed within the 3-year period examined) is investigated in detail, and it is shown that it would be sufficient to sustain both the exploration and exploitation phases of the platform's global dynamics. Copyright 2008 , Oxford University Press.},
	number = {4},
	urldate = {2017-11-05},
	journal = {Industrial and Corporate Change},
	author = {David, Paul and Rullani, Francesco},
	year = {2008},
	pages = {647--710},
}

@article{daniel_implications_2017,
	title = {Implications of {Alter} {Project} {Resources} and {Participant} {Roles} for {Open} {Source} {Software} {Project} {Commercial} {Success}},
	url = {https://aisel.aisnet.org/icis2017/SocialMedia/Presentations/2},
	journal = {ICIS 2017 Proceedings},
	author = {Daniel, Sherae and Stewart, Katherine},
	month = dec,
	year = {2017},
}

@article{crowston_core-periphery_2017,
	title = {Core-periphery communication and the success of free/libre open source software projects},
	volume = {8},
	issn = {1869-0238},
	url = {https://doi.org/10.1186/s13174-017-0061-4},
	doi = {10.1186/s13174-017-0061-4},
	abstract = {We examine the relationship between communications by core and peripheral members and Free/Libre Open Source Software project success. The study uses data from 74 projects in the Apache Software Foundation Incubator. We conceptualize project success in terms of success building a community, as assessed by graduation from the Incubator. We compare successful and unsuccessful projects on volume of communication and on use of inclusive pronouns as an indication of efforts to create intimacy among team members. An innovation of the paper is that use of inclusive pronouns is measured using natural language processing techniques. We also compare the volume and content of communication produced by core (committer) and peripheral members and by those peripheral members who are later elected to be core members. We find that volume of communication is related to project success but use of inclusive pronouns does not distinguish successful projects. Core members exhibit more contribution and use of inclusive pronouns than peripheral members.},
	language = {en},
	number = {1},
	urldate = {2018-11-08},
	journal = {Journal of Internet Services and Applications},
	author = {Crowston, Kevin and Shamshurin, Ivan},
	month = jul,
	year = {2017},
	keywords = {Apache software foundation, Communication, Core and periphery, Free/libre open source software (FLOSS), Inclusive pronouns, Natural language processing, Project success},
	pages = {10},
}

@article{bonaccorsi_why_2003,
	title = {Why {Open} {Source} software can succeed},
	volume = {32},
	issn = {0048-7333},
	url = {http://www.sciencedirect.com/science/article/pii/S0048733303000519},
	doi = {10.1016/S0048-7333(03)00051-9},
	abstract = {The paper discusses three key economic problems raised by the emergence of Open Source: motivation, co-ordination, and diffusion. First, the movement took off through the activity of a community that did not follow profit motivations. Second, a hierarchical co-ordination emerged without proprietary rights. Third, Open Source systems diffused in environments dominated by proprietary standards. The paper shows that recent developments in the theory of diffusion of technologies with network externality may help to explain these phenomena. A simulation model based on heterogeneous agents is developed in order to identify the relevant factors in the diffusion of the technology.},
	number = {7},
	urldate = {2012-03-05},
	journal = {Research Policy},
	author = {Bonaccorsi, Andrea and Rossi, Cristina},
	month = jul,
	year = {2003},
	keywords = {Diffusion, Network externality, open source},
	pages = {1243--1258},
}

@article{aksulu_comprehensive_2010,
	title = {A {Comprehensive} {Review} and {Synthesis} of {Open} {Source} {Research}},
	volume = {11},
	copyright = {Copyright Association for Information Systems Nov 2010},
	issn = {15369323},
	url = {http://search.proquest.com/docview/846782866/abstract/DC752158F34E46A2PQ/1},
	abstract = {The open source movement has grown steadily and matured in recent years, and this growth has been mirrored by a rise in open source related research. The objective of this paper is to pause and reflect on the state of the field. We start by conducting a comprehensive literature review of open source research, and organize the resulting 618 peer-reviewed articles into a taxonomy. Elements of this taxonomy are defined and described. We then draw on a number of existing categorization schemes to develop a framework to situate open source research within a wider nomological network. Building on concepts from systems theory, we propose a holistic framework of open source research. This framework incorporates current research, as represented by the taxonomy, identifies gaps and areas of overlap, and charts a path for future work. [PUBLICATION ABSTRACT]},
	language = {English},
	number = {11},
	urldate = {2018-10-16},
	journal = {Journal of the Association for Information Systems; Atlanta},
	author = {Aksulu, Altay and Wade, Michael},
	month = nov,
	year = {2010},
	keywords = {Computers–Information Science And Information Theory, Open source software, Studies, Vocabularies \& taxonomies},
	pages = {576--656},
}

@article{baldwin_architecture_2006,
	title = {The {Architecture} of {Participation}: {Does} {Code} {Architecture} {Mitigate} {Free} {Riding} in the {Open} {Source} {Development} {Model}?},
	volume = {52},
	issn = {0025-1909},
	shorttitle = {The {Architecture} of {Participation}},
	url = {http://www.jstor.org/stable/20110584},
	abstract = {[This paper argues that the architecture of a codebase is a critical factor that lies at the heart of the open source development process. We define two observable properties of an architecture: (1) modularity and (2) option value. Developers can often make informed judgments about modularity and option value from early, partially implemented code releases. We show that codebases that are more modular or have more option value (1) increase developers' incentives to join and remain involved in an open source development effort and (2) decrease the amount of free riding in equilibrium. These effects occur because modularity and option value create opportunities for the exchange of valuable work among developers, opportunities that do not exist in codebases that are not modular or have no option value.]},
	number = {7},
	urldate = {2018-12-10},
	journal = {Management Science},
	author = {Baldwin, Carliss Y. and Clark, Kim B.},
	year = {2006},
	pages = {1116--1127},
}

@article{baldwin_hidden_2014,
	title = {Hidden structure: {Using} network methods to map system architecture},
	volume = {43},
	issn = {0048-7333},
	shorttitle = {Hidden structure},
	url = {http://www.sciencedirect.com/science/article/pii/S0048733314001012},
	doi = {10.1016/j.respol.2014.05.004},
	abstract = {In this paper, we describe an operational methodology for characterizing the architecture of complex technical systems and demonstrate its application to a large sample of software releases. Our methodology is based upon directed network graphs, which allows us to identify all of the direct and indirect linkages between the components in a system. We use this approach to define three fundamental architectural patterns, which we label core–periphery, multi-core, and hierarchical. Applying our methodology to a sample of 1286 software releases from 17 applications, we find that the majority of releases possess a “core–periphery” structure. This architecture is characterized by a single dominant cyclic group of components (the “Core”) that is large relative to the system as a whole as well as to other cyclic groups in the system. We show that the size of the Core varies widely, even for systems that perform the same function. These differences appear to be associated with different models of development – open, distributed organizations develop systems with smaller Cores, while closed, co-located organizations develop systems with larger Cores. Our findings establish some “stylized facts” about the fine-grained structure of large, real-world technical systems, serving as a point of departure for future empirical work.},
	number = {8},
	journal = {Research Policy},
	author = {Baldwin, Carliss and MacCormack, Alan and Rusnak, John},
	month = oct,
	year = {2014},
	keywords = {Architecture, Dominant designs, Modularity, Product design, Software},
	pages = {1381--1397},
}

@article{gamba_valuing_2009,
	title = {Valuing {Modularity} as a {Real} {Option}},
	volume = {55},
	issn = {00251909, 15265501},
	abstract = {We provide a general valuation approach for capital budgeting decisions involving the modularization in the design of a system. Within the framework developed by Baldwin and Clark (Baldwin, C. Y., K. B. Clark. 2000. Design Rules: The Power of Modularity. MIT Press, Cambridge, MA), we implement a valuation approach using a numerical procedure based on the least-squares Monte Carlo method proposed by Longstaff and Schwartz (Longstaff, F A., E. S. Schwartz. 2001. Valuing American options by simulation: A simple leastsquares approach. Rev. Financial Stud. 14(1) 113-147). The approach is accurate, general, and flexible.},
	journal = {Management Science},
	author = {Gamba, Andrea and Fusari, Nicola},
	year = {2009},
	pages = {1877--1896},
}

@article{sinkovits_fast_2016,
	title = {Fast determination of structurally cohesive subgroups in large networks},
	volume = {17},
	issn = {1877-7503},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5423759/},
	doi = {10.1016/j.jocs.2016.10.005},
	abstract = {Structurally cohesive subgroups are a powerful and mathematically rigorous way to characterize network robustness. Their strength lies in the ability to detect strong connections among vertices that not only have no neighbors in common, but that may be distantly separated in the graph. Unfortunately, identifying cohesive subgroups is a computationally intensive problem, which has limited empirical assessments of cohesion to relatively small graphs of at most a few thousand vertices. We describe here an approach that exploits the properties of cliques, k-cores and vertex separators to iteratively reduce the complexity of the graph to the point where standard algorithms can be used to complete the analysis. As a proof of principle, we apply our method to the cohesion analysis of a 29,462-vertex biconnected component extracted from a 128,151-vertex co-authorship data set.},
	number = {Pt 1},
	urldate = {2019-10-17},
	journal = {Journal of computational science},
	author = {Sinkovits, Robert S. and Moody, James and Oztan, B. Tolga and White, Douglas R.},
	month = nov,
	year = {2016},
	pmid = {28503215},
	pmcid = {PMC5423759},
	pages = {62--72},
}

@article{borgatti_graph-theoretic_2006,
	title = {A {Graph}-theoretic perspective on centrality},
	volume = {28},
	issn = {0378-8733},
	url = {http://www.sciencedirect.com/science/article/pii/S0378873305000833},
	doi = {10.1016/j.socnet.2005.11.005},
	abstract = {The concept of centrality is often invoked in social network analysis, and diverse indices have been proposed to measure it. This paper develops a unified framework for the measurement of centrality. All measures of centrality assess a node's involvement in the walk structure of a network. Measures vary along four key dimensions: type of nodal involvement assessed, type of walk considered, property of walk assessed, and choice of summary measure. If we cross-classify measures by type of nodal involvement (radial versus medial) and property of walk assessed (volume versus length), we obtain a four-fold polychotomization with one cell empty which mirrors Freeman's 1979 categorization. At a more substantive level, measures of centrality summarize a node's involvement in or contribution to the cohesiveness of the network. Radial measures in particular are reductions of pair-wise proximities/cohesion to attributes of nodes or actors. The usefulness and interpretability of radial measures depend on the fit of the cohesion matrix to the one-dimensional model. In network terms, a network that is fit by a one-dimensional model has a core-periphery structure in which all nodes revolve more or less closely around a single core. This in turn implies that the network does not contain distinct cohesive subgroups. Thus, centrality is shown to be intimately connected with the cohesive subgroup structure of a network.},
	number = {4},
	journal = {Social Networks},
	author = {Borgatti, Stephen P. and Everett, Martin G.},
	month = oct,
	year = {2006},
	pages = {466--484},
}

@article{everett_unpacking_2020,
	title = {Unpacking {Burt}’s constraint measure},
	volume = {62},
	journal = {Social Networks},
	author = {Everett, Martin G and Borgatti, Stephen P},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {50--57},
}

@article{shaikh_folding_2016-1,
	title = {Folding and {Unfolding}: {Balancing} {Openness} and {Transparency} in {Open} {Source} {Communities}},
	volume = {27},
	issn = {1047-7047},
	shorttitle = {Folding and {Unfolding}},
	url = {https://pubsonline-informs-org.ezproxy.lib.purdue.edu/doi/abs/10.1287/isre.2016.0646},
	doi = {10.1287/isre.2016.0646},
	abstract = {Open source communities rely on the espoused premise of complete openness and transparency of source code and development process. Yet, openness and transparency at times need to be balanced out with moments of less open and transparent work. Through our detailed study of Linux Kernel development, we build a theory that explains that transparency and openness are nuanced and changing qualities that certain developers manage as they use multiple digital technologies and create, in moments of needs, more opaque and closed digital spaces of work. We refer to these spaces as digital folds. Our paper contributes to the extant literature by providing a process theory of how transparency and openness are balanced with opacity and closure in open source communities according to the needs of the development work; by conceptualizing the nature of digital folds and their role in providing quiet spaces of work; and, by articulating how the process of digital folding and unfolding is made far more possible by select elite actors’ navigating the line between the pragmatics of coding and the accepted ideology of openness and transparency.},
	number = {4},
	urldate = {2017-12-17},
	journal = {Information Systems Research},
	author = {Shaikh, Maha and Vaast, Emmanuelle},
	month = sep,
	year = {2016},
	pages = {813--833},
}

@article{granados_transparency_2013,
	title = {Transparency {Strategy}: {Competing} with {Information} in a {Digital} {World}},
	volume = {37},
	issn = {0276-7783},
	shorttitle = {Transparency {Strategy}},
	url = {http://www.jstor.org/stable/43825928},
	abstract = {We contend that in order to compete effectively in a digital business environment, firms should develop a transparency strategy by selectively disclosing information outside the boundaries of the firm. We make the case for transparency strategy by showing why it is relevant in the digital business world, and the consequences of not having such a strategy. We then provide some foundations to develop the strategy and make a call for research.},
	number = {2},
	urldate = {2021-06-03},
	journal = {MIS Quarterly},
	author = {Granados, Nelson and Gupta, Alok},
	year = {2013},
	pages = {637--641},
}

@misc{noauthor_full_nodate,
	title = {Full article: {Accountability} and {Transparency}: {Siamese} {Twins}, {Matching} {Parts}, {Awkward} {Couple}?},
	url = {https://www.tandfonline.com/doi/full/10.1080/01402382.2010.486122?casa_token=ad2J0YpQZ-QAAAAA%3AVshS-1-FH42-YVz7xqKYKiHExF8QZYDpvcNUDmZT7dQO2w6cou2ogCFj_U7R30ZaeX5dIPoY8Q8m},
	urldate = {2022-01-22},
}

@misc{atamel_accountability_2014,
	title = {Accountability and {Sense} of {Ownership} in {Software} {Development}},
	url = {https://meteatamel.wordpress.com/2014/11/19/accountability-and-sense-of-ownership-in-software-development/},
	abstract = {One of the most overlooked concepts in software development is accountability. Accountability means a team is totally responsible for successful implementation and execution of a piece of software …},
	language = {en},
	urldate = {2022-01-21},
	author = {Atamel, Mete},
	month = nov,
	year = {2014},
	note = {Publication Title: Mete Atamel},
}

@article{hood_accountability_2010,
	title = {Accountability and {Transparency}: {Siamese} {Twins}, {Matching} {Parts}, {Awkward} {Couple}?},
	volume = {33},
	url = {https://doi.org/10.1080/01402382.2010.486122},
	doi = {10.1080/01402382.2010.486122},
	abstract = {This paper contrasts three possible ways of thinking about the relationship between accountability and transparency as principles of governance: as ‘Siamese twins’, not really distinguishable; as ‘matching parts’ that are separable but nevertheless complement one another smoothly to produce good governance; and as ‘awkward couple’, involving elements that are potentially or actually in tension with one another. It then identifies three possible ways in which we could establish the accuracy or plausibility of each of those three characterisations. One is a search for true essences that would separate true from spurious meanings. A second is to dimensionalise each of the concepts and explore the matrix of connectivity between the dimensions. A third is to explore the multiple frames for thinking about accountability and transparency that come from the neo-Durkheimian analysis of the limited number of elementary ways of life developed by the late Mary Douglas and her followers. The analytic utility of this approach is that it allows us to identify four different ways of thinking about transparency and accountability and the links between them.},
	number = {5},
	journal = {West European Politics},
	author = {Hood, Christopher},
	year = {2010},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01402382.2010.486122},
	pages = {989--1009},
}

@article{brunswicker_transparency_2018,
	title = {Transparency as design choice of open data contests},
	volume = {69},
	issn = {23301635},
	url = {http://doi.wiley.com/10.1002/asi.24033},
	doi = {10.1002/asi.24033},
	language = {en},
	number = {10},
	urldate = {2019-08-25},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Brunswicker*, Sabine and Jensen, Bjørn and Song, Zhounan and Majchrzak, Ann},
	month = oct,
	year = {2018},
	keywords = {innovation},
	pages = {1205--1222},
}

@article{shay_opening_2004,
	title = {Opening the {Sources} of {Accountability}},
	volume = {9},
	url = {https://papers.ssrn.com/abstract=858084},
	abstract = {This paper scrutinizes the concept of accountability in light of free and open source software. On the view that increasing accountability grants value to society by motivating those most likely and able to prevent risk and harm to do so, I argue that developing software collaboratively, licensing it openly, and distributing its source code freely are promising first steps in the long journey to rehabilitate accountability in our highly computerized society, and that at the same time these practices change our very understanding of what accountability is. The paper analyzes the concept of accountability in an open environment and explores the implications in two mission-critical application fields in which software plays a significant role: electronic voting, and electronic medical records. It further considers the potential remedies to accountability's erosion that free and open source software offer, and the ways in which accountability can be generalized to collective action if we understand it less as punishability and more as a culture that encourages the prevention of risk and harm. With such reconceptualized accountability in mind, I find that code visibility, a self-imposed standard of care and sensible licensing arrangements are a potent, practical, and effective alternative to the strict liability standards offered as a solution to the accountability problem by earlier scholars.},
	language = {en},
	number = {11},
	urldate = {2022-01-22},
	journal = {First Monday},
	author = {Shay, David},
	year = {2004},
	keywords = {accountability, free and open source software, liability, software, software legal aspects, software norms},
}

@article{armisen_formative_2016,
	title = {Formative and {Summative} {Feedback} in {Solution} {Generation}: {The} {Role} of {Community} and {Decision} {Support} {System} in {Open} {Source} {Software}},
	shorttitle = {Formative and {Summative} {Feedback} in {Solution} {Generation}},
	url = {https://aisel.aisnet.org/icis2016/SocialMedia/Presentations/20},
	journal = {ICIS 2016 Proceedings},
	author = {Armisen, Albert and Majchrzak, Ann and Brunswicker*, Sabine},
	month = dec,
	year = {2016},
}

@article{brunswicker_coherence_2019,
	series = {The {Digital} {Transformation} of {Innovation} and {Entrepreneurship}},
	title = {Coherence or flexibility? {The} paradox of change for developers’ digital innovation trajectory on open platforms},
	volume = {48},
	issn = {0048-7333},
	shorttitle = {Coherence or flexibility?},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733319300794},
	doi = {10.1016/j.respol.2019.03.016},
	abstract = {Innovation is a cumulative process in which past knowledge created by others can be both a source for predictable outcomes and also a barrier to significant change. The recent literature on digital innovation suggests that open platforms, which encourage their developers to build upon each other's knowledge when innovating their add-on apps in the periphery, face a related paradox. Developers face the tension of either being coherent with the past, or flexible to adjust to the future. In this paper, we examine how the trade-off between coherent and flexible search mechanisms affects the individual developer's choice of innovating a certain app as well as his or her cumulative impact, i.e., the degree of modifications to the app. We study an open platform in the multi-disciplinary field of nanotechnology, in which 480 developers peen innovating their add-on apps in the periphery, face a related paradox. Developers face the tension of either being coherent with the past, or flexible to adjust to the future. In this paper, we examine how the trade-off between coherent and flexible search mechanisms affects the individual developer's choice of innovating a certain app as well as his or her cumulative impact, i.e., the degree of modifications to the app. We study an open platform in the multi-disciplinary field of nanotechnology, in which 480 developers perform more than 30,000 problem-solving actions over a period of 10 years. We use relational event modeling to differentially assess the effect of the coherent and flexible search strategies. We find that developers are significantly more likely to choose a certain app that is consistent with both a coherent and flexible strategy. However, a coherent strategy leads to greater cumulative impact on an app compared to a strategy of being mutually coherent and flexible. Thus, our findings indicate both a complementary and a contradictory logic in how the tension between coherence and flexibility unfolds. We make contributions to the recent literature on digital innovation as well as the innovation literature more broadly. Further, our results inform innovation policy and platform design.},
	language = {en},
	number = {8},
	urldate = {2021-03-02},
	journal = {Research Policy},
	author = {Brunswicker*, Sabine and Schecter, Aaron},
	month = oct,
	year = {2019},
	keywords = {Cumulative innovation, Digital innovation, Digital platform, Dynamics, Individual innovation trajectory, Knowledge translation, Paradox of change, Platform architecture, Relational events},
	pages = {103771},
}

@article{brunswicker_creating_2016,
	title = {Creating impact in the digital space: digital practice dependency in communities of digital scientific innovations},
	volume = {Online (09 September 2016)},
	issn = {0138-9130, 1588-2861},
	shorttitle = {Creating impact in the digital space},
	url = {http://link.springer.com/article/10.1007/s11192-016-2106-z},
	doi = {10.1007/s11192-016-2106-z},
	abstract = {Modern science has become collaborative and digital. The Internet has supported the emergence of scientific digital platforms that globally connect programmers and users of novel digital scientific products such as scientific interactive software tools. These digital scientific innovations complement traditional text-based products like journal publications. This article is focused on the scientific impact of a platform’s programming community that produces these digital scientific innovations. The article’s main theoretical argument is that beyond an individual’s contribution efforts to these innovations, a new social structure affects his scientific recognition through citations of his tools in text-based publications. Taking a practice theory lens, we introduce the concept of a digital practice structure that emerges from the digital innovation work practice, performed by programmers who jointly work on a tool. This digital practice creates dependence forces among the community members in an analogy to Newton’s gravity concept. Our model represents such dependencies in a spatial autocorrelative model. We empirically estimate this model using data of the programming community of nanoHUB in which 477 nanotechnology tool programmers have contributed more than 715 million lines of code. Our results show that a programmer’s contributions to digital innovations may have positive effects, while the digital practice structure creates negative dependency effects. Colloquially speaking, being surrounded by star performers can be harmful. Our findings suggest that modeling scientific impact needs to account for a scientist’s contribution to programming communities that produce digital scientific innovations and the digital work structures in which these contributions are embedded.},
	language = {en},
	urldate = {2016-09-14},
	journal = {Scientometrics},
	author = {Brunswicker*, Sabine and Matei, Sorin Adam and Zentner, Michael and Zentner, Lynn and Klimeck, Gerhard},
	month = sep,
	year = {2016},
	pages = {1--26},
}

@article{brunswicker_digital_2019,
	title = {Digital {Folding} in {OSS} {Co}-development: {The} {Effect} of {Depth} of {Cohesive} {Involvement} and {Openness} to {Knowledge} {Diversity} on {Sustained} {Development} {Performance}},
	journal = {Working Paper, RCODI, Purdue University.},
	author = {Brunswicker*, Sabine and Mukherjee, Sabine},
	year = {2019},
}

@article{brunswicker_microstructure_2020,
	title = {The {Microstructure} of {Complex} {Design} {Architectures}: {A} {Theory} of {Design} {Network} {Motifs}},
	volume = {2020},
	issn = {0065-0668},
	shorttitle = {The {Microstructure} of {Complex} {Design} {Architectures}},
	url = {https://journals.aom.org/doi/abs/10.5465/AMBPP.2020.21853abstract},
	doi = {10.5465/AMBPP.2020.21853abstract},
	abstract = {The established stream of literature on design architectures argues that designers should aim for modular architecture in order increase the system’s technical performance by lowering the propagation costs of the system’s design: A system with a small stable “core”, a cycle of coupled parts of the system, and a large variable periphery reduce the risk of technical feature and increase opportunity for innovation. However, such a core-periphery view ignores the micro-level dependency structures that emerge in open collaboration when a large number of developers produce a complex technical system at distance, virtually, and outside of formal employment relationships. In this paper, we develop a theory of design network motifs that accounts for the smallest design dependencies in the system that emerge in situ. Informed by network theory, we introduce the concept of a design network motif to describe distinct patterns of design interdependencies within the smallest substructure of a system architecture. We develop a theory of design network motifs describe a system’s microstructure and explain how certain motif patterns affect the performance of a system as a whole. We empirically examine our design network motif theory using architectural data of the Open Stack repository NOVA, consisting of 2359 files and 1545532 lines of code. We extract all potential design network motifs created by 872 developers for 5 years. Our results show that design network motifs offer a new way to explain hidden dependencies in complex system architectures. Surprisingly, it is not just cycling that significantly impact a system’s performance but instead, a “feed-forward loop” motif. Our results contribute to the literature on design architectures and open collaboration more broadly."},
	number = {1},
	urldate = {2021-02-16},
	journal = {Academy of Management Proceedings},
	author = {Brunswicker, Sabine and Mukherjee, Satyam and Brunswicker*, Sabine and Mukherjee, Satyam},
	month = jul,
	year = {2020},
	pages = {21853},
}

@article{brunswicker_evolution_2021,
	title = {Evolution of {Coordination} {Structures} in {OSS} {Development}: {An} {Exponential} {Random} {Graph} {Model}},
	abstract = {In complex Open Source Software (OSS) development projects, coordination structures emerge from dyadic code interactions of dispersed developers. Prior research uses methods of network science to understand the evolution of OSS developer coordination. In this paper, we examine how endogenous network properties predict the formation of OSS coordination structures. We specify an Exponential Random Graph Model (p*) to study the significance of two relational mechanisms: Preferential attachment and knowledge similarity. We empirically estimate our model using granular development data of 619 developers in Nova, one of the oldest projects of OpenStack, working on 2597 Python files between 2012 and 2016. We find statistical evidence of positive non-linear (rather than linear) preferential attachment which cause skewed distributions distinct from a power-law. Further, we show that knowledge similarity has a significantly positive effect. Non-linear preferential attachment explains the formation of developer relationships across long evolutionary periods but knowledge similarity impacts only early stages. Our findings contribute to literature on software evolution and management},
	journal = {Academy of Management Best Paper Proceedings},
	author = {Brunswicker*, Sabine and Mukherjee, Satyam},
	year = {2021},
}

@book{watson_structured_1996,
	title = {Structured testing: {A} testing methodology using the cyclomatic complexity metric},
	volume = {500},
	number = {235},
	publisher = {US Department of Commerce, Technology Administration, National Institute of …},
	author = {Watson, Arthur Henry and Wallace, Dolores R and McCabe, Thomas J},
	year = {1996},
}

@misc{noauthor_national_nodate,
	title = {National {Vulnerability} {Database} ({NVD})},
}

@article{lamb_reproducible_2021,
	title = {Reproducible {Builds}: {Increasing} the {Integrity} of {Software} {Supply} {Chains}},
	doi = {10.1109/MS.2021.3073045},
	journal = {IEEE Software},
	author = {Lamb, C. and Zacchiroli, S.},
	year = {2021},
	pages = {0--0},
}

@inproceedings{navarro_leija_reproducible_2020,
	title = {Reproducible {Containers}},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	author = {Navarro Leija, Omar S and Shiptoski, Kelly and Scott, Ryan G and Wang, Baojun and Renner, Nicholas and Newton, Ryan R and Devietti, Joseph},
	year = {2020},
	pages = {167--182},
}

@article{ruohonen_case_2018,
	title = {A case study on software vulnerability coordination},
	volume = {103},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584917305116},
	doi = {10.1016/j.infsof.2018.06.005},
	abstract = {Context: Coordination is a fundamental tenet of software engineering. Coordination is required also for identifying discovered and disclosed software vulnerabilities with Common Vulnerabilities and Exposures (CVEs). Motivated by recent practical challenges, this paper examines the coordination of CVEs for open source projects through a public mailing list. Objective: The paper observes the historical time delays between the assignment of CVEs on a mailing list and the later appearance of these in the National Vulnerability Database (NVD). Drawing from research on software engineering coordination, software vulnerabilities, and bug tracking, the delays are modeled through three dimensions: social networks and communication practices, tracking infrastructures, and the technical characteristics of the CVEs coordinated. Method: Given a period between 2008 and 2016, a sample of over five thousand CVEs is used to model the delays with nearly fifty explanatory metrics. Regression analysis is used for the modeling. Results: The results show that the CVE coordination delays are affected by different abstractions for noise and prerequisite constraints. These abstractions convey effects from the social network and infrastructure dimensions. Particularly strong effect sizes are observed for annual and monthly control metrics, a control metric for weekends, the degrees of the nodes in the CVE coordination networks, and the number of references given in NVD for the CVEs archived. Smaller but visible effects are present for metrics measuring the entropy of the emails exchanged, traces to bug tracking systems, and other related aspects. The empirical signals are weaker for the technical characteristics. Conclusion: Software vulnerability and CVE coordination exhibit all typical traits of software engineering coordination in general. The coordination perspective elaborated and the case studied open new avenues for further empirical inquiries as well as practical improvements for the contemporary CVE coordination.},
	journal = {Information and Software Technology},
	author = {Ruohonen, Jukka and Rauti, Sampsa and Hyrynsalmi, Sami and Leppänen, Ville},
	year = {2018},
	keywords = {CVE, CVSS, CWE, Coordination, MITRE, NIST, NVD, Open source, Social network, Vulnerability},
	pages = {239--257},
}

@article{martin_docker_2018,
	title = {Docker ecosystem–vulnerability analysis},
	volume = {122},
	journal = {Computer Communications},
	author = {Martin, Antony and Raponi, Simone and Combe, Théo and Di Pietro, Roberto},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {30--43},
}

@article{gordon_integrating_2020,
	title = {Integrating cost–benefit analysis into the {NIST} {Cybersecurity} {Framework} via the {Gordon}–{Loeb} {Model}},
	volume = {6},
	issn = {2057-2085},
	url = {https://doi.org/10.1093/cybsec/tyaa005},
	doi = {10.1093/cybsec/tyaa005},
	abstract = {The National Institute for Standards and Technology (NIST) Cybersecurity Framework has rapidly become a widely accepted approach to facilitating cybersecurity risk management within organizations. An insightful aspect of the NIST Cybersecurity Framework is its explicit recognition that the activities associated with managing cybersecurity risk are organization specific. The NIST Framework also recognizes that organizations should evaluate their cybersecurity risk management on a cost–benefit basis. The NIST Framework, however, does not provide guidance on how to carry out such a cost–benefit analysis. This article provides an approach for integrating cost–benefit analysis into the NIST Cybersecurity Framework. The Gordon–Loeb (GL) Model for cybersecurity investments is proposed as a basis for deriving a cost-effective level of spending on cybersecurity activities and for selecting the appropriate NIST Implementation Tier level. The analysis shows that the GL Model provides a logical approach to use when considering the cost–benefit aspects of cybersecurity investments during an organization’s process of selecting the most appropriate NIST Implementation Tier level. In addition, the cost–benefit approach provided in this article helps to identify conditions under which there is an incentive to move to a higher NIST Implementation Tier.},
	number = {1},
	journal = {Journal of Cybersecurity},
	author = {Gordon, Lawrence A and Loeb, Martin P and Zhou, Lei},
	month = mar,
	year = {2020},
	note = {\_eprint: https://academic.oup.com/cybersecurity/article-pdf/6/1/tyaa005/32979473/tyaa005.pdf},
}

@article{tartler_revealing_2012,
	title = {Revealing and repairing configuration inconsistencies in large-scale system software},
	volume = {14},
	issn = {1433-2787},
	doi = {10.1007/s10009-012-0225-2},
	abstract = {System software typically offers a large amount of compile-time options and variability. A good example is the Linux kernel, which provides more than 10,000 configurable features, growing rapidly. This allows users to tailor it with respect to a broad range of supported hardware architectures and application domains. From the maintenance point of view, compile-time configurability poses big challenges. The configuration model (the selectable features and their constraints as presented to the user) and the configurability that is actually implemented in the code have to be kept in sync, which, if performed manually, is a tedious and error-prone task. In the case of Linux, this has led to numerous defects in the source code, many of which are actual bugs. In order to ensure consistency between the variability expressed in the code and the configuration models, we propose an approach that extracts variability from both into propositional logic. This reveals inconsistencies between variability as expressed by the C Preprocessor (CPP) and an explicit variability model, which manifest themselves in seemingly conditional code that is in fact unconditional. We evaluate our approach with the Linux, for which our tool detects 1,766 configurability defects, which turned out as dead/superfluous source code and bugs. Our findings have led to numerous source-code improvements and bug fixes in Linux: 123 patches (49 merged) fix 364 defects, 147 of which have been confirmed by the corresponding Linux developers and 20 as fixing a previously unknown bug.},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Tartler, Reinhard and Sincero, Julio and Dietrich, Christian and Schröder-Preikschat, Wolfgang and Lohmann, Daniel},
	year = {2012},
	pages = {531--551},
}

@inproceedings{machiry_spider_2020,
	title = {{SPIDER}: {Enabling} {Fast} {Patch} {Propagation} {In} {Related} {Software} {Repositories}},
	doi = {10.1109/SP40000.2020.00038},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Machiry, A. and Redini, N. and Camellini, E. and Kruegel, C. and Vigna, G.},
	year = {2020},
	pages = {1562--1579},
}

@inproceedings{rosenblum_extracting_2010,
	title = {Extracting compiler provenance from program binaries},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN}-{SIGSOFT} workshop on {Program} analysis for software tools and engineering},
	author = {Rosenblum, Nathan E and Miller, Barton P and Zhu, Xiaojin},
	year = {2010},
	pages = {21--28},
}

@inproceedings{rosenblum_recovering_2011,
	title = {Recovering the toolchain provenance of binary code},
	booktitle = {Proceedings of the 2011 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	author = {Rosenblum, Nathan and Miller, Barton P and Zhu, Xiaojin},
	year = {2011},
	pages = {100--110},
}

@misc{jones_identifying_2013,
	title = {Identifying a software developer based on debugging information},
	publisher = {Google Patents},
	author = {Jones, Angela Richards and Patel, Kaushik},
	month = nov,
	year = {2013},
}

@inproceedings{hemel_finding_2011,
	title = {Finding software license violations through binary code clone detection},
	booktitle = {Proceedings of the 8th {Working} {Conference} on {Mining} {Software} {Repositories}},
	author = {Hemel, Armijn and Kalleberg, Karl Trygve and Vermaas, Rob and Dolstra, Eelco},
	year = {2011},
	pages = {63--72},
}

@article{montandon_mining_2021,
	title = {Mining the {Technical} {Roles} of {GitHub} {Users}},
	volume = {131},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584920302275},
	doi = {10.1016/j.infsof.2020.106485},
	abstract = {Context:Modern software development demands high levels of technical specialization. These conditions make IT companies focus on creating cross-functional teams, such as frontend, backend, and mobile developers. In this context, the success of software projects is highly influenced by the expertise of these teams in each field. Objective:In this paper, we investigate machine-learning based approaches to automatically identify the technical roles of open source developers. Method:For this, we first build a ground truth with 2284 developers labeled in six different roles: backend, frontend, full-stack, mobile, devops, and data science. Then, we build three different machine-learning models used to identify these roles. Results:These models presented competitive results for precision (0.88) and AUC (0.89) when identifying all six roles. Moreover, our results show that programming-languages are the most relevant features to predict the investigated roles. Conclusion:The approach proposed in this paper can assist companies during their hiring process, such as by recommending developers with the expertise required by job positions.},
	journal = {Information and Software Technology},
	author = {Montandon, João Eduardo and Valente, Marco Tulio and Silva, Luciana L.},
	year = {2021},
	keywords = {Developers profiles, GitHub, Machine learning, Technical expertise, Technical roles},
	pages = {106485},
}

@inproceedings{yu_software_2006,
	title = {Software and {Biological} {Evolvability}: {A} {Comparison} {Using} {Key} {Properties}},
	doi = {10.1109/SOFTWARE-EVOLVABILITY.2006.11},
	booktitle = {2006 {Second} {International} {IEEE} {Workshop} on {Software} {Evolvability} ({SE}'06)},
	author = {Yu, L. and Ramaswamy, S.},
	year = {2006},
	pages = {82--88},
}

@article{carillo_what_2017,
	title = {What makes a good contributor? {Understanding} contributor behavior within large {Free}/{Open} {Source} {Software} projects – {A} socialization perspective},
	volume = {26},
	issn = {0963-8687},
	url = {https://www.sciencedirect.com/science/article/pii/S0963868716301196},
	doi = {10.1016/j.jsis.2017.03.001},
	abstract = {Attracting new contributors is a necessary but not a sufficient condition, to ensure the survival and long-term success of Free/Open Source Software (FOSS) projects. The well-being of a FOSS project depends on the turning of project newcomers into {\textbackslash}lqgood contributors{\textbackslash}rq that is to say into individuals that substantially contribute to the project - but also that perform citizenship behaviors that protect and nurture its community. This study is a mixed-methods investigation of the socialization factors that influence contributor performance in large FOSS projects. A qualitative research component resulted into the development of a FOSS socialization framework as well as into the identification of key FOSS project citizenship behaviors. A conceptual model was then developed and empirically examined with 367 contributors from 12 large FOSS projects. The model hypothesizes the mediating effect of two proximal socialization variables, social identification and social integration, between FOSS newcomer socialization factors and contributor performance (conceptualized as task performance and community citizenship behaviors). The results demonstrate the influence of social identification and social integration in predicting contributor performance, as well as the importance of key socialization factors that are: task segregation, task purposefulness, interaction intensity, and supportiveness. Theoretical and practical implications are discussed.},
	number = {4},
	journal = {The Journal of Strategic Information Systems},
	author = {Carillo, Kevin and Huff, Sid and Chawner, Brenda},
	year = {2017},
	keywords = {Citizenship behaviors, Free/Open Source Software community, Free/Open Source Software project, Mixed-methods, Socialization},
	pages = {322--359},
}

@inproceedings{nehaniv_what_2006,
	title = {What {Software} {Evolution} and {Biological} {Evolution} {Don}'t {Have} in {Common}},
	doi = {10.1109/SOFTWARE-EVOLVABILITY.2006.18},
	booktitle = {2006 {Second} {International} {IEEE} {Workshop} on {Software} {Evolvability} ({SE}'06)},
	author = {Nehaniv, C. L. and Hewitt, J. and Christianson, B. and Wernick, P.},
	year = {2006},
	pages = {58--65},
}

@misc{codecovio_bash_nodate,
	title = {Bash {Uploader} {Security} {Update}},
	urldate = {2021-04-15},
	author = {{codecov.io}},
}

@inproceedings{kikas_structure_2017,
	title = {Structure and {Evolution} of {Package} {Dependency} {Networks}},
	doi = {10.1109/MSR.2017.55},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Kikas, R. and Gousios, G. and Dumas, M. and Pfahl, D.},
	year = {2017},
	pages = {102--112},
}

@inproceedings{xiaofan_chen_improving_2011,
	title = {Improving automated documentation to code traceability by combining retrieval techniques},
	doi = {10.1109/ASE.2011.6100057},
	booktitle = {2011 26th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2011)},
	author = {{Xiaofan Chen} and Grundy, J.},
	year = {2011},
	pages = {223--232},
}

@inproceedings{zerouali_empirical_2018,
	title = {An empirical analysis of technical lag in npm package dependencies},
	booktitle = {International {Conference} on {Software} {Reuse}},
	publisher = {Springer},
	author = {Zerouali, Ahmed and Constantinou, Eleni and Mens, Tom and Robles, Gregorio and González-Barahona, Jesús},
	year = {2018},
	pages = {95--110},
}

@misc{kuppusamy_pep_2014,
	title = {{PEP} 480 – {Surviving} a {Compromise} of {PyPI}},
	url = {http://legacy.python.org/dev/peps/pep-0480/},
	author = {Kuppusamy, Trishank and Diaz, Vladimir and Stufft, Donald and Cappos, Justin},
	year = {2014},
}

@misc{barry_warsaw_pep_2000,
	title = {{PEP} 1 – {PEP} {Purpose} and {Guidelines}},
	url = {https://peps.python.org/pep-0001/},
	author = {Barry Warsaw, Jeremy Hylton, David Goodger, Nick Coghlan},
	year = {2000},
}

@inproceedings{gamblin_spack_2015,
	title = {The {Spack} package manager: bringing order to {HPC} software chaos},
	booktitle = {{SC}'15: {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R and Lee, Gregory L and Moody, Adam and de Supinski, Bronis R and Futral, Scott},
	year = {2015},
	pages = {1--12},
}

@misc{kuppusamy_pep_2013,
	title = {{PEP} 458 – {Securing} the {Link} from {PyPI} to the {End} {User}},
	url = {http://legacy.python.org/dev/peps/pep-0458/},
	author = {Kuppusamy, Trishank and Diaz, Vladimir and Stufft, Donald and Cappos, Justin},
	year = {2013},
}

@misc{noauthor_google_nodate,
	title = {Google {Infrastructure} {Security} {Design} {Overview}},
}

@misc{lorenc_cosign_nodate,
	title = {Cosign {Image} {Signatures}},
	author = {Lorenc, Dan},
}

@misc{noauthor_policy-based_nodate,
	title = {Policy-based control for cloud native environments},
}

@misc{noauthor_google_nodate-1,
	title = {Google {Container} {Registry}},
}

@misc{noauthor_cloud_nodate,
	title = {Cloud {Native} {CI}/{CD}},
}

@misc{noauthor_worlds_nodate,
	title = {the world's enterprise open source leader},
}

@misc{noauthor_universal_nodate,
	title = {The {Universal} {Operating} {System}},
}

@misc{noauthor_lightweight_nodate,
	title = {A lightweight {Linux} {Distribution}},
}

@misc{noauthor_arch_nodate,
	title = {Arch {Linux} {Security} {Incident} {Response} 2021-0001},
}

@misc{noauthor_debian_nodate,
	title = {Debian {Contributors} ftp.debian.org package maintainers data source},
}

@misc{noauthor_cve-2019-0757_nodate,
	title = {{CVE}-2019-0757: {NuGet} {Package} {Manager} {Tampering} {Vulnerability}},
}

@misc{center_software_nodate,
	title = {Software {Identification} ({SWID}) {Tagging} {SWID}},
	author = {Center, NIST: Cyber Security Resource},
}

@misc{noauthor_always-graph_nodate,
	title = {Always-{On} {Graph} {Database} {Platform}},
}

@misc{noauthor_purl_nodate,
	title = {{pURL}: a {Package} “mostly universal” {URL}.},
}

@inproceedings{francalanci_empirical_2008,
	address = {Boston, MA},
	title = {Empirical {Analysis} of the {Bug} {Fixing} {Process} in {Open} {Source} {Projects}},
	isbn = {978-0-387-09684-1},
	abstract = {Monitoring the performance of processes is often considered critical in classic engineering fields. However, in the area of software engineering (and especially in the Open Source context) it seems that the literature has not yet taken into consideration the problem of identifying the process characteristics and performance of debugging. The aim of this paper is the identification of the performance characteristics of the bug fixing process of Open Source applications, focusing on continuity and efficiency indicators. The importance of such indicators is even more relevant today, since Open Source software is now adopted also in many business contexts. We have analyzed the debugging process of 9 active and popular Open Source projects, collecting a dataset comprising more than 65,000 closed bugs. Results have highlighted four types of bug fixing processes that can be distinguished by considering temporal continuity and efficiency dimensions.},
	booktitle = {Open {Source} {Development}, {Communities} and {Quality}},
	publisher = {Springer US},
	author = {Francalanci, Chiara and Merlo, Francesco},
	editor = {Russo, Barbara and Damiani, Ernesto and Hissam, Scott and Lundell, Björn and Succi, Giancarlo},
	year = {2008},
	pages = {187--196},
}

@misc{noauthor_find_nodate,
	title = {Find, install and publish {Python} packages with the {Python} {Package} {Index}},
}

@misc{noauthor_debian_nodate-1,
	title = {Debian {Package} {Tracking} {System}},
}

@inproceedings{anandayuvaraj_incorporating_2023,
	title = {Incorporating {Failure} {Knowledge} into {Design} {Decisions} for {IoT} {Systems}: {A} {Controlled} {Experiment} on {Novices}},
	booktitle = {5th {International} {Workshop} on {Software} {Engineering} {Research} \& {Practices} for the {Internet} of {Things} ({SERP4IoT} 2023)},
	author = {Anandayuvaraj, Dharun and Thulluri, Pujita and Figueroa, Justin and Shandilya, Harshit and Davis, James C},
	year = {2023},
}

@inproceedings{davis_reusing_2023,
	title = {Reusing {Deep} {Learning} {Models}: {Challenges} and {Directions} in {Software} {Engineering}},
	booktitle = {Proceedings of the {IEEE} {John} {Vincent} {Atanasoff} {Symposium} on {Modern} {Computing} ({JVA}'23)},
	author = {Davis, James C and Jajal, Purvish and Jiang, Wenxin and Schorlemmer, Taylor R and Synovic, Nicholas and Thiruvathukal, George K},
	year = {2023},
}

@inproceedings{checkoway_systematic_2016,
	address = {New York, NY, USA},
	series = {{CCS} '16},
	title = {A {Systematic} {Analysis} of the {Juniper} {Dual} {EC} {Incident}},
	isbn = {978-1-4503-4139-4},
	url = {https://doi.org/10.1145/2976749.2978395},
	doi = {10.1145/2976749.2978395},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Checkoway, Stephen and Maskiewicz, Jacob and Garman, Christina and Fried, Joshua and Cohney, Shaanan and Green, Matthew and Heninger, Nadia and Weinmann, Ralf-Philipp and Rescorla, Eric and Shacham, Hovav},
	year = {2016},
	note = {event-place: Vienna, Austria},
	keywords = {VPN, dual EC DRBG, juniper, pseudorandom number generator},
	pages = {468--479},
}

@misc{corbet_kernelorg_2011,
	title = {kernel.org status: hints on how to check your machine for intrusion},
	url = {https://lwn.net/Articles/461237/},
	author = {{Corbet}},
	year = {2011},
}

@misc{graham_context_2014,
	title = {Context {Threat} {Intelligence} — {The} {Monju} {Incident}},
	url = {https://www.contextis.com/en/blog/context-threat-intelligence-the-monju-incident},
	author = {Graham, Mark},
	year = {2014},
}

@misc{erciccione_warning_2019,
	title = {Warning: {The} binaries of the {CLI} wallet were compromised for a short time},
	url = {https://web.getmonero.org/2019/11/19/warning-compromised-binaries.html},
	author = {{ErCiccione}},
	year = {2019},
}

@misc{zetter_google_2010,
	title = {'{Google}' {Hackers} {Had} {Ability} to {Alter} {Source} {Code}},
	url = {https://www.wired.com/2010/03/source-code-hacks/},
	author = {Zetter, Kim},
	year = {2010},
}

@misc{leyden_apple_2015,
	title = {Apple cleans up {iOS} {App} {Store} after first big malware attack},
	url = {https://www.theregister.com/2015/09/21/xcodeghost_apple_ios_store_malware_zapped/},
	author = {Leyden, John},
	year = {2015},
}

@misc{nichols_classic_2016,
	title = {Classic {Shell}, {Audacity} downloads infected with retro {MBR} nuke nasty},
	url = {https://www.theregister.co.uk/2016/08/04/classicshell_audicity_infection/},
	author = {Nichols, Shaun},
	year = {2016},
}

@misc{schwartz_aur-general_2018,
	title = {[aur-general] acroread package compromised},
	url = {https://lists.archlinux.org/pipermail/aur-general/2018-July/034152.html},
	author = {Schwartz, Eli},
	year = {2018},
}

@misc{database_malicious_2020,
	title = {Malicious {Package} in load-from-cwd-or-npm},
	url = {https://github.com/advisories/GHSA-jxf5-7x3j-8j9m},
	author = {Database, GitHub Advisory},
	year = {2020},
}

@misc{eilertsen_backdoor_2022,
	title = {Backdoor {Found} in {Themes} and {Plugins} from {AccessPress} {Themes}},
	url = {https://jetpack.com/blog/backdoor-found-in-themes-and-plugins-from-accesspress-themes/},
	author = {Eilertsen, Harald},
	year = {2022},
}

@misc{popov_changes_2021,
	title = {Changes to {Git} commit workflow},
	url = {https://news-web.php.net/php.internals/113838},
	author = {Popov, Nikita},
	year = {2021},
}

@misc{robbins_gentoo_2003,
	title = {{GENTOO} {LINUX} {SECURITY} {ANNOUNCEMENT} 200312-01},
	url = {https://archives.gentoo.org/gentoo-announce/message/7b0581416ddd91522c14513cb789f17a},
	author = {Robbins, Daniel},
	year = {2003},
}

@misc{corbet_backdooring_2007,
	title = {The backdooring of {WordPress}},
	url = {https://lwn.net/Articles/224997/},
	author = {{Corbet}},
	year = {2007},
}

@misc{debian_debian_2003,
	title = {Debian {Investigation} {Report} after {Server} {Compromises}},
	url = {https://www.debian.org/News/2003/20031202},
	author = {{Debian}},
	year = {2003},
}

@misc{sharma_heres_2021,
	title = {Here's how a researcher broke into {Microsoft} {VS} {Code}'s {GitHub}},
	url = {https://www.bleepingcomputer.com/news/security/heres-how-a-researcher-broke-into-microsoft-vs-codes-github/},
	author = {Sharma, Ax},
	year = {2021},
}

@misc{hunter_ii_compromised_2018,
	title = {Compromised npm {Package}: event-stream},
	url = {https://medium.com/intrinsic/compromised-npm-package-event-stream-d47d08605502},
	author = {Hunter II, Thomas},
	year = {2018},
}

@misc{chauchefoin_php_2022,
	title = {{PHP} {Supply} {Chain} {Attack} on {PEAR}},
	url = {https://blog.sonarsource.com/php-supply-chain-attack-on-pear/},
	author = {Chauchefoin, Thomas},
	year = {2022},
}

@article{avizienis_basic_2004,
	title = {Basic concepts and taxonomy of dependable and secure computing},
	volume = {1},
	number = {1},
	journal = {IEEE transactions on dependable and secure computing},
	author = {Avizienis, Algirdas and Laprie, J-C and Randell, Brian and Landwehr, Carl},
	year = {2004},
	note = {Publisher: IEEE},
	pages = {11--33},
}

@misc{pichai_important_2023,
	title = {An important next step on our {AI} journey},
	url = {https://blog.google/technology/ai/bard-google-ai-search-updates/},
	author = {Pichai, Sundar},
	year = {2023},
}

@misc{schulman_introducing_2023,
	title = {Introducing {ChatGPT}},
	url = {https://openai.com/blog/chatgpt},
	author = {Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob and Menick, Jacob and Weng, Jiayi and Uribe, Juan Felipe Ceron and Fedus, Liam},
	year = {2023},
}

@techreport{manyika_overview_2023,
	title = {An overview of {Bard}: an early experiment with generative {AI}},
	url = {https://ai.google/static/documents/google-about-bard.pdf},
	institution = {Google AI},
	author = {Manyika, James},
	year = {2023},
}

@misc{ghahramani_lamda_2023,
	title = {{LaMDA}: our breakthrough conversation technology},
	url = {https://blog.google/technology/ai/lamda/},
	author = {Ghahramani, Zoubin},
	year = {2023},
}

@inproceedings{melo_pathology_2021,
	title = {The {Pathology} of {Failures} in {IoT} {Systems}},
	booktitle = {Computational {Science} and {Its} {Applications}–{ICCSA} 2021: 21st {International} {Conference}, {Cagliari}, {Italy}, {September} 13–16, 2021, {Proceedings}, {Part} {IX} 21},
	publisher = {Springer},
	author = {Melo, Mário and Aquino, Gibeon},
	year = {2021},
	pages = {437--452},
}

@article{yao_top_2023,
	title = {Top 6 {NLP} {Language} {Models} {Transforming} {AI} {In} 2023},
	url = {https://www.topbots.com/top-6-nlp-language-models-transforming-ai-in-2023/},
	journal = {TOPBOTS},
	author = {Yao, Mariya},
	year = {2023},
}

@article{gavara_claude_2023,
	title = {Claude vs {ChatGPT}},
	url = {https://medium.com/@exceed73/claude-vs-chatgpt-a2b87f9e089b},
	journal = {Medium},
	author = {Gavara, José Ignacio},
	year = {2023},
}

@techreport{sonatype_state_2022,
	title = {State of the {Software} {Supply} {Chain}},
	url = {https://www.sonatype.com/state-of-the-software-supply-chain/open-source-supply-demand-security},
	number = {8th Annual},
	institution = {Sonatype},
	author = {{Sonatype}},
	year = {2022},
}

@misc{synopsys_2023_nodate,
	title = {2023 {OSSRA} {Report}},
	url = {https://www.synopsys.com/software-integrity/engage/ossra/rep-ossra-2023-pdf},
	abstract = {The report offers recommendations for security, legal, risk, and development teams to better understand the security and risk landscape accompanying open source development and use.},
	language = {en},
	urldate = {2023-06-22},
	author = {{Synopsys}},
	note = {Publication Title: Synopsys},
}

@book{jones_economics_2011,
	title = {The economics of software quality},
	publisher = {Addison-Wesley Professional},
	author = {Jones, Capers and Bonsignour, Olivier},
	year = {2011},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	year = {2021},
	note = {\_eprint: 2107.13586},
}

@misc{openai_gpt_2023,
	title = {{GPT} {Best} {Practices}},
	url = {https://platform.openai.com/docs/guides/gpt-best-practices},
	author = {{OpenAI}},
	year = {2023},
}

@misc{mandour_gpt-35_2023,
	title = {{GPT}-3.5 model architecture},
	url = {https://iq.opengenus.org/gpt-3-5-model/},
	author = {Mandour, Ahmed},
	year = {2023},
}

@misc{openai_openai_2023,
	title = {{OpenAI} {Platform} - {GPT}-3.5 {Models}},
	url = {https://platform.openai.com/docs/models/gpt-3-5},
	author = {{OpenAI}},
	year = {2023},
}

@misc{openai_gpt_2023-1,
	title = {{GPT} {Best} {Practices}},
	url = {https://platform.openai.com/docs/guides/gpt-best-practices},
	author = {{OpenAI}},
	year = {2023},
}

@misc{buruk_academic_2023,
	title = {Academic {Writing} with {GPT}-3.5: {Reflections} on {Practices}, {Efficacy} and {Transparency}},
	author = {Buruk, Oğuz 'Oz'},
	year = {2023},
	note = {\_eprint: 2304.11079},
}

@misc{noauthor_openai_2023,
	title = {{OpenAI} {Platform}},
	url = {https://platform.openai.com/docs/model-index-for-researchers},
	year = {2023},
}

@article{white_chatgpt_2023,
	title = {Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design},
	journal = {arXiv preprint arXiv:2303.07839},
	author = {White, Jules and Hays, Sam and Fu, Quchen and Spencer-Smith, Jesse and Schmidt, Douglas C},
	year = {2023},
}

@article{white_prompt_2023,
	title = {A prompt pattern catalog to enhance prompt engineering with chatgpt},
	journal = {arXiv preprint arXiv:2302.11382},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
	year = {2023},
}

@techreport{enduring_security_framework_securing_2022,
	title = {Securing the {Software} {Supply} {Chain}: {Recommended} {Practices} {Guide} for {Developers}},
	language = {en},
	institution = {Cybersecurity and Infrastructure Security Agency},
	author = {{Enduring Security Framework}},
	month = aug,
	year = {2022},
}

@techreport{european_union_agency_for_cybersecurity_enisa_2022,
	address = {LU},
	title = {{ENISA} threat landscape 2022},
	url = {https://data.europa.eu/doi/10.2824/764318},
	language = {en},
	urldate = {2023-06-19},
	institution = {Publications Office},
	author = {{European Union Agency for Cybersecurity}},
	year = {2022},
}

@techreport{european_union_agency_for_cybersecurity_good_2023,
	title = {Good {Practices} for {Supply} {Chain} {Cybersecurity}},
	language = {en},
	institution = {European Union Agency for Cybersecurity.},
	author = {{European Union Agency for Cybersecurity}},
	year = {2023},
}

@techreport{european_union_agency_for_cybersecurity_enisa_2021,
	type = {Report/{Study}},
	title = {{ENISA} {Threat} {Landscape} 2021},
	url = {https://www.enisa.europa.eu/publications/enisa-threat-landscape-2021},
	abstract = {This is the ninth edition of the ENISA Threat Landscape (ETL) report, an annual report on the status of the cybersecurity threat landscape that identifies prime threats, major trends observed with respect to threats, threat actors and attack techniques, and also describes relevant mitigation measures. In the process of constantly improving our methodology for the development of threat landscapes, this year’s work has been supported by a newly formatted ENISA ad hoc Working Group on Cybersecurity Threat Landscapes (CTL). In this report we discuss the first 8 cybersecurity threat categories. Supply chain threats, the 9th category, were analysed in detail, in a dedicated ENISA report.},
	language = {en},
	urldate = {2022-06-21},
	institution = {European Union Agency for Cybersecurity},
	author = {{European Union Agency for Cybersecurity}},
	month = oct,
	year = {2021},
	keywords = {Government},
}

@techreport{european_union_agency_for_cybersecurity_enisa_2021-1,
	title = {{ENISA} threat landscape for supply chain attacks.},
	url = {https://data.europa.eu/doi/10.2824/168593},
	abstract = {This report aims at mapping and studying the supply chain attacks that were discovered from January 2020 to early July 2021. Based on the trends and patterns observed, supply chain attacks increased in number and sophistication in the year 2020 and this trend is continuing in 2021, posing an increasing risk for organizations. It is estimated that there will be four times more supply chain attacks in 2021 than in 2020. With half of the attacks being attributed to Advanced Persistence Threat (APT) actors, their complexity and resources greatly exceed the more common non-targeted attacks, and, therefore, there is an increasing need for new protective methods that incorporate suppliers in order to guarantee that organizations remain secure.},
	language = {en},
	urldate = {2022-06-21},
	institution = {European Union Agency for Cybersecurity},
	author = {{European Union Agency for Cybersecurity}},
	month = jul,
	year = {2021},
	keywords = {Government},
}

@mastersthesis{maxam_threat_2023,
	title = {Threat {Hunting} in {Cybersecurity}: {A} {Comprehensive} {Study}},
	school = {University of Unknown},
	author = {Maxam, A.},
	month = jun,
	year = {2023},
	note = {Published: = https://davisjam.github.io//files/publications/Maxam-ThreatHunt-2023-MScThesis.pdf},
}

@misc{gill_what_2023,
	title = {What is {Open}-{Source} {Intelligence}?},
	publisher = {SANS Institute},
	author = {Gill, Ritu},
	year = {2023},
	note = {Published: = https://www.sans.org/blog/what-is-open-source-intelligence/},
}

@misc{the_recorded_future_team_what_2022,
	title = {What {Is} {Open} {Source} {Intelligence} and {How} {Is} it {Used}?},
	publisher = {Recorded Future},
	author = {{The Recorded Future Team}},
	year = {2022},
	note = {Published: = https://www.recordedfuture.com/open-source-intelligence-definition},
}

@misc{google_cloud_software_2023,
	title = {Software supply chain security {\textbar} {Google} {Cloud}},
	author = {{Google Cloud}},
	year = {2023},
	note = {Published: = https://cloud.google.com/software-supply-chain-security/docs/overview},
}

@inproceedings{alberts_systemic_2011,
	title = {A {Systemic} {Approach} for {Assessing} {Software} {Supply}-{Chain} {Risk}},
	doi = {10.1109/HICSS.2011.36},
	abstract = {In today's business environment, multiple organizations must routinely work together in software supply chains when acquiring, developing, operating, and maintaining software products. The programmatic and product complexity inherent in software supply chains increases the risk that defects, vulnerabilities, and malicious code will be inserted into a delivered software product. As a result, effective risk management is essential for establishing and maintaining software supply-chain assurance over time. The Software Engineering Institute (SEI) is developing a systemic approach for assessing and managing software supply-chain risks. This paper highlights the basic approach being implemented by SEI researchers and provides a summary of the status of this work.},
	booktitle = {2011 44th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Alberts, Christopher J. and Dorofee, Audrey J. and Creel, Rita and Ellison, Robert J. and Woody, Carol},
	month = jan,
	year = {2011},
	note = {ISSN: 1530-1605},
	pages = {1--8},
}

@article{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	number = {8},
	journal = {OpenAI blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and {others}},
	year = {2019},
	pages = {9},
}

@book{national_research_council_software_2007,
	title = {Software for dependable systems: {Sufficient} evidence?},
	publisher = {National Academies Press},
	author = {{National Research Council} and {others}},
	year = {2007},
}

@misc{cncf_security_technical_advisory_group_catalog_2023,
	title = {Catalog of {Supply} {Chain} {Compromises}},
	url = {https://github.com/cncf/tag-security/tree/main/supply-chain-security/compromises},
	author = {{CNCF Security Technical Advisory Group}},
	year = {2023},
}

@misc{cncf_entry_2023,
	title = {Entry 2 of the catalog},
	url = {https://github.com/cncf/tag-security/blob/main/supply-chain-security/compromises/2022/fantasy.md},
	author = {{CNCF}},
	year = {2023},
}

@misc{openai_openai_2023-1,
	title = {{OpenAI} {Platform}},
	url = {https://platform.openai.com/docs/api-reference/chat},
	author = {{OpenAI}},
	year = {2023},
}

@misc{kasi_post_nodate,
	title = {The post mortem paradox: a {Delphi} study of {IT} specialist perceptions},
	shorttitle = {The post mortem paradox},
	url = {https://www.tandfonline.com/doi/epdf/10.1057/palgrave.ejis.3000727?needAccess=true&role=button},
	abstract = {While post mortem evaluation (PME) has long been advocated as a means of improving development practices by learning from IT project failures, few organizations conduct PMEs. The purpose of the study is to explain this discrepancy between theory and practice. This paper integrates findings from a Delphi study of what experienced practitioners perceive as the most important barriers to conducting PMEs with insights from organizational learning theory. The results suggest that there are critical tensions between development practices and learning contexts in many organizations, and adopting PMEs in these cases is likely to reinforce organizational learning dysfunctions rather than improve current development practices. Based on these findings, we argue that the PME literature has underestimated the limits to learning in most IT organizations and we propose to explore paradoxical thinking to help researchers frame continued inquiry into PME and to help managers overcome learning dysfunctions as they push for more widespread use of PMEs.},
	urldate = {2023-06-16},
	author = {Kasi, Vijay and Keil, Mark and Mathiassen, Lars and Pedersen, Keld},
	doi = {10.1057/palgrave.ejis.3000727},
}

@article{rahman_identification_2009,
	title = {Identification of sources of failures and their propagation in critical infrastructures from 12 years of public failure reports},
	volume = {5},
	issn = {1475-3219},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJCIS.2009.024872},
	doi = {10.1504/IJCIS.2009.024872},
	abstract = {Understanding the origin of infrastructure failures and their propagation patterns in critical infrastructures can provide important information for secure and reliable infrastructure design. Among the critical infrastructures, the Communication and Information Technology Infrastructure (CITI) is crucial, as it provides the basic mechanism for sharing information among all infrastructures. Failures in CITI can disrupt the effective functionality of the other critical infrastructures. Conversely, failures in the other infrastructures can also propagate to CITI, and hence disrupt the operation of all systems. In this study, we used public domain failure reports to identify the origin of these failures and their propagation patterns. We analysed 347 infrastructure failure cases reported from 1994 to 2005 in the Association for Computing Machinery's (ACM) RISKS forum. We studied these reports to determine the causes of infrastructure failures and their impact on CITI and other critical infrastructures in a number of dimensions, such as the origin of failures, impacts of failures in spatial and temporal dimensions, their effect on public safety and how failures propagate from one infrastructure to another. The results obtained from the analysis of these real-life failure cases, which occurred over a considerable timespan, should be useful to researchers and practitioners. This paper also discusses the difficulties and limitations of using public domain data in academic research.},
	number = {3},
	urldate = {2023-06-16},
	journal = {International Journal of Critical Infrastructures},
	author = {Rahman, Hafiz Abdur and Beznosov, Konstantin and Marti, Jose R.},
	month = jan,
	year = {2009},
	keywords = {CITI, ICT infrastructure, communications, critical infrastructures, cyber interdependency, failure identification, failure propagation, information sharing, information technology, infrastructure failure, infrastructure interdependencies, telecommunications},
	pages = {220--244},
}

@techreport{cybersecurity_and_infrastructure_security_agency_defending_2021,
	title = {Defending {Against} {Software} {Supply} {Chain} {Attacks}},
	institution = {Cybersecurity and Infrastructure Security Agency},
	author = {{Cybersecurity and Infrastructure Security Agency}},
	month = apr,
	year = {2021},
}

@misc{kaspersky_shadowhammer_2019,
	title = {{ShadowHammer}: {Malicious} updates for {ASUS} laptops},
	url = {https://www.kaspersky.com/blog/shadow-hammer-teaser/26149/},
	author = {{Kaspersky}},
	year = {2019},
}

@article{geer_for_2020,
	title = {For good measure: {Counting} broken links: {A} quant’s view of software supply chain security},
	volume = {45},
	number = {4},
	journal = {USENIX; Login},
	author = {Geer, Dan and Tozer, Bentz and Meyers, John Speed},
	year = {2020},
}

@article{ma_api_2019,
	title = {An {API} semantics-aware malware detection method based on deep learning},
	volume = {2019},
	journal = {Security and Communication Networks},
	author = {Ma, Xin and Guo, Shize and Bai, Wei and Chen, Jun and Xia, Shiming and Pan, Zhisong},
	year = {2019},
	note = {Publisher: Hindawi Limited},
	pages = {1--9},
}

@inproceedings{huddleston_how_2021,
	title = {How {VMware} {Exploits} {Contributed} to {SolarWinds} {Supply}-chain {Attack}},
	booktitle = {2021 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	publisher = {IEEE},
	author = {Huddleston, Josh and Ji, Pu and Bhunia, Suman and Cogan, Joel},
	year = {2021},
	pages = {760--765},
}

@inproceedings{pedersen_barriers_2004,
	title = {Barriers for {Post} mortem evaluations in {Systems} {Development}},
	language = {English},
	booktitle = {{UKAIS} {Conference}, {Glasgow}, {UK}. ; {Conference} date: 19-05-2010},
	author = {Pedersen, Keld},
	year = {2004},
}

@misc{international_organization_for_standardization_iso_isoiec_2001,
	title = {{ISO}/{IEC} 9126: {Information} technology – {Software} product evaluation – {Quality} characteristics and guidelines for their use},
	author = {{International Organization for Standardization (ISO)}},
	year = {2001},
	note = {Published: International Organization for Standardization},
}

@misc{gokkaya_software_2023,
	title = {Software supply chain: review of attacks, risk assessment strategies and security controls},
	shorttitle = {Software supply chain},
	url = {http://arxiv.org/abs/2305.14157},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Gokkaya, Betul and Aniello, Leonardo and Halak, Basel},
	month = may,
	year = {2023},
	note = {Issue: arXiv:2305.14157
arXiv: 2305.14157 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{goel_low-power_2021,
	title = {Low-{Power} {Multi}-{Camera} {Object} {Re}-{Identification} using {Hierarchical} {Neural} {Networks}},
	volume = {2021},
	booktitle = {{ACM}/{IEEE} {International} {Symposium} on {Low} {Power} {Electronics} and {Design} ({ISLPED})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Wang, Haobo and Davis, James C and Thiruvathukal, George K and Lu, Yung-Hsiang},
	year = {2021},
}

@inproceedings{herbsleb_global_2007,
	title = {Global software engineering: {The} future of socio-technical coordination},
	booktitle = {Future of {Software} {Engineering} ({FOSE}'07)},
	publisher = {IEEE},
	author = {Herbsleb, James D},
	year = {2007},
	pages = {188--198},
}

@inproceedings{veselsky_establishing_2022,
	title = {Establishing trust in vehicle-to-vehicle coordination: a sensor fusion approach},
	booktitle = {Proceedings of the 23rd {Annual} {International} {Workshop} on {Mobile} {Computing} {Systems} and {Applications}},
	author = {Veselsky, Jakob and West, Jack and Ahlgren, Isaac and Thiruvathukal, George K and Klingensmith, Neil and Goel, Abhinav and Jiang, Wenxin and Davis, James C and Lee, Kyuin and Kim, Younghyun},
	year = {2022},
	pages = {128--128},
}

@inproceedings{goel_efficient_2022,
	title = {Efficient {Computer} {Vision} on {Edge} {Devices} with {Pipeline}-{Parallel} {Hierarchical} {Neural} {Networks}},
	booktitle = {Asia and {South} {Pacific} {Design} {Automation} {Conference} ({ASP}-{DAC})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Thiruvathukal, George and Davis, James C and Lu, Yung-Hsiang},
	year = {2022},
}

@article{smite_empirical_2010,
	title = {Empirical evidence in global software engineering: a systematic review},
	volume = {15},
	number = {1},
	journal = {Empirical software engineering},
	author = {Šmite, Darja and Wohlin, Claes and Gorschek, Tony and Feldt, Robert},
	year = {2010},
	note = {Publisher: Springer},
	pages = {91--118},
}

@inproceedings{xu_empirical_2022,
	title = {An {Empirical} {Study} on the {Impact} of {Parameters} on {Mobile} {App} {Energy} {Usage}},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	year = {2022},
}

@inproceedings{barlas_exploiting_2022,
	title = {Exploiting {Input} {Sanitization} for {Regex} {Denial} of {Service}},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Barlas, Efe and Du, Xin and Davis, James C.},
	year = {2022},
}

@inproceedings{gopalakrishna_if_2022,
	title = {“{If} security is required”: {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	booktitle = {4th {International} {Workshop} on {Software} {Engineering} {Research} \& {Practices} for the {Internet} of {Things} ({SERP4IoT} 2022)},
	author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C},
	year = {2022},
}

@misc{banna_experience_2021,
	title = {An {Experience} {Report} on {Machine} {Learning} {Reproducibility}: {Guidance} for {Practitioners} and {TensorFlow} {Model} {Garden} {Contributors}},
	author = {Banna, Vishnu and Chinnakotla, Akhil and Yan, Zhengxin and Vegesana, Ani and Vivek, Naveen and Krishnappa, Kruthi and Jiang, Wenxin and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
	year = {2021},
	note = {Publication Title: https://arxiv.org/abs/2107.00821},
}

@inproceedings{wittern_empirical_2019,
	title = {An empirical study of {GraphQL} schemas},
	booktitle = {International {Conference} on {Service}-{Oriented} {Computing} ({ICSOC})},
	publisher = {Springer, Cham},
	author = {Wittern, Erik and Cha, Alan and Davis, James C and Baudart, Guillaume and Mandel, Louis},
	year = {2019},
	pages = {3--19},
}

@inproceedings{davis_testing_2019,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	year = {2019},
	pages = {427--439},
}

@inproceedings{davis_why_2019,
	title = {Why aren’t regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	author = {Davis, James C and Michael IV, Louis G and Coghlan, Christy A and Servant, Francisco and Lee, Dongyoon},
	year = {2019},
	pages = {443--454},
}

@inproceedings{davis_nodefz_2017,
	title = {Node.fz: {Fuzzing} the server-side event-driven architecture},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Computer} {Systems} ({EuroSys})},
	author = {Davis, James and Thekumparampil, Arun and Lee, Dongyoon},
	year = {2017},
	pages = {145--160},
}

@inproceedings{cha_principled_2020,
	title = {A principled approach to {GraphQL} query cost analysis},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C and Mandel, Louis and Laredo, Jim A},
	year = {2020},
	pages = {257--268},
}

@inproceedings{michael_regexes_2019,
	title = {Regexes are hard: {Decision}-making, difficulties, and risks in programming regular expressions},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Michael, Louis G and Donohue, James and Davis, James C and Lee, Dongyoon and Servant, Francisco},
	year = {2019},
	pages = {415--426},
}

@inproceedings{davis_impact_2018,
	title = {The impact of regular expression denial of service ({ReDoS}) in practice: an empirical study at the ecosystem scale},
	booktitle = {Proceedings of the 2018 26th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE})},
	author = {Davis, James C and Coghlan, Christy A and Servant, Francisco and Lee, Dongyoon},
	year = {2018},
	pages = {246--256},
}

@article{herbold_large-scale_2020,
	title = {Large-{Scale} {Manual} {Validation} of {Bug} {Fixing} {Commits}: {A} {Fine}-grained {Analysis} of {Tangling}},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Herbold, Steffen and Trautsch, Alexander and Ledel, Benjamin and Aghamohammadi, Alireza and Ghaleb, Taher Ahmed and Chahal, Kuljit Kaur and Bossenmaier, Tim and Nagaria, Bhaveet and Makedonski, Philip and Ahmadabadi, Matin Nili and {others}},
	year = {2020},
}

@inproceedings{davis_using_2021,
	title = {Using selective memoization to defeat regular expression denial of service ({ReDoS})},
	booktitle = {2021 {IEEE} symposium on security and privacy ({SP})},
	publisher = {IEEE},
	author = {Davis, James C and Servant, Francisco and Lee, Dongyoon},
	year = {2021},
	pages = {1--17},
}

@inproceedings{davis_sense_2018,
	title = {A sense of time for javascript and {Node}.js: {First}-class timeouts as a cure for event handler poisoning},
	booktitle = {27th {USENIX} {Security} {Symposium} ({USENIX} {Security})},
	author = {Davis, James C and Williamson, Eric R and Lee, Dongyoon},
	year = {2018},
	pages = {343--359},
}

@inproceedings{sejfia_practical_2022,
	title = {Practical {Automated} {Detection} of {Malicious} npm {Packages}},
	booktitle = {2022 {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Sejfia, Adriana and Schäfer, Max},
	year = {2022},
}

@article{prana_out_2021,
	title = {Out of sight, out of mind? {How} vulnerable dependencies affect open-source projects},
	volume = {26},
	number = {4},
	journal = {Empirical Software Engineering},
	author = {Prana, Gede Artha Azriadi and Sharma, Abhishek and Shar, Lwin Khin and Foo, Darius and Santosa, Andrew E and Sharma, Asankhaya and Lo, David},
	year = {2021},
	note = {Publisher: Springer},
	pages = {1--34},
}

@misc{dellavecchia_how_2022,
	title = {How a {Rogue} {Developer} {Ruined} {Millions} of {Software} ({Happened} {This} {Weekend})},
	abstract = {TLDR: A software developer who made some highly used open-source software, decided to go rogue and inject a bug into his software, making{\textbackslash}ldots},
	author = {Dellavecchia, Anthony},
	month = jan,
	year = {2022},
	note = {Publication Title: Medium},
}

@article{emami_naeini_influence_2018,
	title = {The influence of friends and experts on privacy decision making in {IoT} scenarios},
	volume = {2},
	number = {CSCW},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Emami Naeini, Pardis and Degeling, Martin and Bauer, Lujo and Chow, Richard and Cranor, Lorrie Faith and Haghighat, Mohammad Reza and Patterson, Heather},
	year = {2018},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--26},
}

@inproceedings{emami-naeini_exploring_2019,
	title = {Exploring how privacy and security factor into {IoT} device purchase behavior},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Emami-Naeini, Pardis and Dixon, Henry and Agarwal, Yuvraj and Cranor, Lorrie Faith},
	year = {2019},
	pages = {1--12},
}

@inproceedings{emami-naeini_ask_2020,
	title = {Ask the experts: {What} should be on an {IoT} privacy and security label?},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Emami-Naeini, Pardis and Agarwal, Yuvraj and Cranor, Lorrie Faith and Hibshi, Hanan},
	year = {2020},
	pages = {447--464},
}

@article{bowen_naturalistic_2008,
	title = {Naturalistic inquiry and the saturation concept: a research note},
	volume = {8},
	number = {1},
	journal = {Qualitative research},
	author = {Bowen, Glenn A},
	year = {2008},
	note = {Publisher: Sage Publications Sage UK: London, England},
	pages = {137--152},
}

@misc{noauthor_software_2021,
	title = {Software {Bill} of {Materials} {Elements} and {Considerations}},
	url = {https://www.federalregister.gov/documents/ 2021/06/02/2021-11592/software-bill-of-materials-elements-and-considerations},
	abstract = {The Executive Order on Improving the Nation's Cybersecurity directs the Department of Commerce, in coordination with the National Telecommunications and Information Administration (NTIA), to publish the minimum elements for a Software Bill of Materials (SBOM). Through this Notice, following from...},
	month = jun,
	year = {2021},
	note = {Publication Title: Federal Register},
}

@book{hennink_qualitative_2020,
	title = {Qualitative research methods},
	publisher = {Sage},
	author = {Hennink, Monique and Hutter, Inge and Bailey, Ajay},
	year = {2020},
}

@article{chen_towards_2004,
	title = {Towards a theory of supply chain management: the constructs and measurements},
	volume = {22},
	number = {2},
	journal = {Journal of operations management},
	author = {Chen, Injazz J and Paulraj, Antony},
	year = {2004},
	note = {Publisher: Elsevier},
	pages = {119--150},
}

@article{bauer_comparing_2016,
	title = {Comparing reuse practices in two large software-producing companies},
	volume = {117},
	journal = {Journal of Systems and Software},
	author = {Bauer, Veronika and Vetro, Antonio},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {545--582},
}

@article{petersen_choosing_2017,
	title = {Choosing component origins for software intensive systems: {In}-house, cots, oss or outsourcing?—a case survey},
	volume = {44},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Petersen, Kai and Badampudi, Deepika and Shah, Syed Muhammad Ali and Wnuk, Krzysztof and Gorschek, Tony and Papatheocharous, Efi and Axelsson, Jakob and Sentilles, Severine and Crnkovic, Ivica and Cicchetti, Antonio},
	year = {2017},
	note = {Publisher: IEEE},
	pages = {237--261},
}

@article{ayala_selection_2011,
	title = {Selection of third party software in {Off}-{The}-{Shelf}-based software development—{An} interview study with industrial practitioners},
	volume = {84},
	number = {4},
	journal = {Journal of Systems and Software},
	author = {Ayala, Claudia and Hauge, Øyvind and Conradi, Reidar and Franch, Xavier and Li, Jingyue},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {620--637},
}

@article{jadhav_evaluating_2009,
	title = {Evaluating and selecting software packages: {A} review},
	volume = {51},
	number = {3},
	journal = {Information and software technology},
	author = {Jadhav, Anil S and Sonar, Rajendra M},
	year = {2009},
	note = {Publisher: Elsevier},
	pages = {555--563},
}

@misc{kasi_post_nodate-1,
	title = {The post mortem paradox: a {Delphi} study of {IT} specialist perceptions},
	shorttitle = {The post mortem paradox},
	url = {https://www.tandfonline.com/doi/epdf/10.1057/palgrave.ejis.3000727?needAccess=true&role=button},
	abstract = {While post mortem evaluation (PME) has long been advocated as a means of improving development practices by learning from IT project failures, few organizations conduct PMEs. The purpose of the study is to explain this discrepancy between theory and practice. This paper integrates findings from a Delphi study of what experienced practitioners perceive as the most important barriers to conducting PMEs with insights from organizational learning theory. The results suggest that there are critical tensions between development practices and learning contexts in many organizations, and adopting PMEs in these cases is likely to reinforce organizational learning dysfunctions rather than improve current development practices. Based on these findings, we argue that the PME literature has underestimated the limits to learning in most IT organizations and we propose to explore paradoxical thinking to help researchers frame continued inquiry into PME and to help managers overcome learning dysfunctions as they push for more widespread use of PMEs.},
	urldate = {2023-06-16},
	author = {Kasi, Vijay and Keil, Mark and Mathiassen, Lars and Pedersen, Keld},
	doi = {10.1057/palgrave.ejis.3000727},
}

@misc{npm_scope_nodate,
	title = {Scope},
	author = {{NPM}},
}

@misc{npm_proxy_nodate,
	title = {Proxy},
	author = {{NPM}},
}

@misc{github_best_nodate,
	title = {Best practices for securing your build system},
	author = {{GitHub}},
}

@misc{noauthor_-toto_nodate,
	title = {In-toto ongoing integrations},
	urldate = {2022-08-04},
}

@misc{noauthor_git_nodate,
	title = {Git - {Signing} {Your} {Work}},
	urldate = {2022-08-04},
}

@article{he_automating_2022,
	title = {Automating {Dependency} {Updates} in {Practice}: {An} {Exploratory} {Study} on {GitHub} {Dependabot}},
	shorttitle = {Automating {Dependency} {Updates} in {Practice}},
	doi = {10.48550/ARXIV.2206.07230},
	urldate = {2022-07-31},
	author = {He, Runzhi and He, Hao and Zhang, Yuxia and Zhou, Minghui},
	year = {2022},
	keywords = {FOS: Computer and information sciences, Human-Computer Interaction (cs.HC), Software Engineering (cs.SE)},
}

@misc{noauthor_enable_nodate,
	title = {Enable {Dependabot} by milgradesec · {Pull} {Request} \#4317 · caddyserver/caddy},
	abstract = {This PR enables github dependabot for go.mod dependencies and github actions in workflows. Scans every day and If it finds outdated deps will create PRs.},
	urldate = {2022-07-31},
}

@misc{github_secure_nodate,
	title = {Secure your supply chain},
	author = {{GitHub}},
}

@misc{noauthor_secure_2020,
	title = {Secure at every step: {What} is software supply chain security and why does it matter?},
	shorttitle = {Secure at every step},
	urldate = {2022-07-31},
	month = sep,
	year = {2020},
}

@misc{noauthor_about_nodate,
	title = {About code scanning},
	urldate = {2022-07-31},
}

@inproceedings{wyss_what_2022,
	title = {What the fork?: finding hidden code clones in npm},
	isbn = {978-1-4503-9221-1},
	shorttitle = {What the fork?},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510168},
	doi = {10.1145/3510003.3510168},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Wyss, Elizabeth and De Carli, Lorenzo and Davidson, Drew},
	year = {2022},
	note = {Place: Pittsburgh Pennsylvania},
	pages = {2415--2426},
}

@inproceedings{liu_demystifying_2022,
	title = {Demystifying the vulnerability propagation and its evolution via dependency trees in the {NPM} ecosystem},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510142},
	doi = {10.1145/3510003.3510142},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Chengwei and Chen, Sen and Fan, Lingling and Chen, Bihuan and Liu, Yang and Peng, Xin},
	month = may,
	year = {2022},
	note = {Place: Pittsburgh Pennsylvania},
	pages = {672--684},
}

@inproceedings{zhou_automated_2017,
	title = {Automated identification of security issues from commit messages and bug reports},
	isbn = {978-1-4503-5105-8},
	url = {https://dl.acm.org/doi/10.1145/3106237.3117771},
	doi = {10.1145/3106237.3117771},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhou, Yaqin and Sharma, Asankhaya},
	month = aug,
	year = {2017},
	note = {Place: Paderborn Germany},
	pages = {914--919},
}

@inproceedings{leite_uedashboard_2015,
	title = {{UEDashboard}: awareness of unusual events in commit histories},
	isbn = {978-1-4503-3675-8},
	shorttitle = {{UEDashboard}},
	url = {https://dl.acm.org/doi/10.1145/2786805.2803184},
	doi = {10.1145/2786805.2803184},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Leite, Larissa and Treude, Christoph and Figueira Filho, Fernando},
	month = aug,
	year = {2015},
	note = {Place: Bergamo Italy},
	pages = {978--981},
}

@inproceedings{goswami_investigating_2020,
	title = {Investigating {The} {Reproducibility} of {NPM} {Packages}},
	isbn = {978-1-72815-619-4},
	url = {https://ieeexplore.ieee.org/document/9240695/},
	doi = {10.1109/ICSME46990.2020.00071},
	urldate = {2022-07-19},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Goswami, Pronnoy and Gupta, Saksham and Li, Zhiyuan and Meng, Na and Yao, Daphne},
	month = sep,
	year = {2020},
	note = {Place: Adelaide, Australia},
	pages = {677--681},
}

@article{goyal_identifying_2018,
	title = {Identifying unusual commits on {GitHub}},
	volume = {30},
	issn = {20477473},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/smr.1893},
	doi = {10.1002/smr.1893},
	number = {1},
	urldate = {2022-07-21},
	journal = {Journal of Software: Evolution and Process},
	author = {Goyal, Raman and Ferreira, Gabriel and Kästner, Christian and Herbsleb, James},
	month = jan,
	year = {2018},
	pages = {e1893},
}

@inproceedings{sejfia_practical_2022,
	title = {Practical automated detection of malicious npm packages},
	isbn = {978-1-4503-9221-1},
	doi = {10.1145/3510003.3510104},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Sejfia, Adriana and Schäfer, Max},
	month = may,
	year = {2022},
	note = {Place: Pittsburgh Pennsylvania},
	pages = {1681--1692},
}

@misc{microsoft_oss_2020,
	title = {{OSS} {Detect} {Backdoor}},
	abstract = {Collection of tools for analyzing open source packages. - OSS Detect Backdoor · microsoft/OSSGadget Wiki},
	urldate = {2022-07-18},
	author = {{Microsoft}},
	year = {2020},
}

@article{duan_towards_2020,
	title = {Towards {Measuring} {Supply} {Chain} {Attacks} on {Package} {Managers} for {Interpreted} {Languages}},
	doi = {10.48550/ARXIV.2002.01139},
	urldate = {2022-07-18},
	author = {Duan, Ruian and Alrawi, Omar and Kasturi, Ranjita Pai and Elder, Ryan and Saltaformaggio, Brendan and Lee, Wenke},
	year = {2020},
	keywords = {Cryptography and Security (cs.CR), FOS: Computer and information sciences},
}

@misc{constantin_npm_2018,
	title = {Npm {Attackers} {Sneak} a {Backdoor} into {Node}.js {Deployments} through {Dependencies}},
	urldate = {2022-07-18},
	author = {Constantin, Lucian},
	month = may,
	year = {2018},
}

@inproceedings{bird_does_2009,
	title = {Does distributed development affect software quality? {An} empirical case study of {Windows} {Vista}},
	doi = {10.1109/ICSE.2009.5070550},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	author = {Bird, Christian and Nagappan, Nachiappan and Devanbu, Premkumar and Gall, Harald and Murphy, Brendan},
	month = may,
	year = {2009},
	keywords = {Availability, Costs, Cultural differences, Geography, Global communication, Government, Programming, Software engineering, Software quality, Testing},
	pages = {518--528},
}

@misc{foy_hijacking_2021,
	title = {The {Hijacking} of {Perl}.com},
	abstract = {Handling the response to our missing domain},
	urldate = {2022-07-18},
	author = {foy, brian d},
	month = feb,
	year = {2021},
}

@inproceedings{bird_dont_2011,
	title = {Don't touch my code!: examining the effects of ownership on software quality},
	isbn = {978-1-4503-0443-6},
	shorttitle = {Don't touch my code!},
	doi = {10.1145/2025113.2025119},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th {European} conference on {Foundations} of software engineering - {SIGSOFT}/{FSE} '11},
	publisher = {ACM Press},
	author = {Bird, Christian and Nagappan, Nachiappan and Murphy, Brendan and Gall, Harald and Devanbu, Premkumar},
	year = {2011},
	note = {Place: Szeged, Hungary},
	pages = {4},
}

@misc{rubin_embedded_2021,
	title = {Embedded {Malware} in {NPM}: {Coa}, {Rc}, {Ua}-parser - {FOSSA}},
	shorttitle = {Embedded {Malware} in {NPM}},
	url = {https://fossa.com/blog/embedded-malware-npm-coa-rc-ua-parser/},
	urldate = {2022-07-18},
	author = {Rubin, G. Polasani \{and\} S.},
	month = nov,
	year = {2021},
}

@inproceedings{nagappan_influence_2008,
	title = {The influence of organizational structure on software quality: an empirical case study},
	isbn = {978-1-60558-079-1},
	shorttitle = {The influence of organizational structure on software quality},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368160},
	doi = {10.1145/1368088.1368160},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering - {ICSE} '08},
	publisher = {ACM Press},
	author = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor},
	year = {2008},
	note = {Place: Leipzig, Germany},
	pages = {521},
}

@inproceedings{ohm_backstabbers_2020,
	address = {Cham},
	title = {Backstabber's {Knife} {Collection}: {A} {Review} of {Open} {Source} {Software} {Supply} {Chain} {Attacks}},
	isbn = {978-3-030-52683-2},
	abstract = {A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems further down the chain. Recent years saw a number of supply chain attacks that leverage the increasing use of open source during software development, which is facilitated by dependency managers that automatically resolve, download and install hundreds of open source packages throughout the software life cycle. Even though many approaches for detection and discovery of vulnerable packages exist, no prior work has focused on malicious packages. This paper presents a dataset as well as analysis of 174 malicious software packages that were used in real-world attacks on open source software supply chains and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed. This work is meant to facilitate the future development of preventive and detective safeguards by open source and research communities.},
	booktitle = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	publisher = {Springer International Publishing},
	author = {Ohm, Marc and Plate, Henrik and Sykosch, Arnold and Meier, Michael},
	editor = {Maurice, Clémentine and Bilge, Leyla and Stringhini, Gianluca and Neves, Nuno},
	year = {2020},
	pages = {23--43},
}

@book{almubayed_practical_2019,
	title = {Practical approach to automate the discovery and eradication of opensource software vulnerabilities at scale},
	publisher = {Blackhat USA},
	author = {Almubayed, A.},
	year = {2019},
}

@inproceedings{ferreira_containing_2021,
	title = {Containing {Malicious} {Package} {Updates} in npm with a {Lightweight} {Permission} {System}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9402108/},
	doi = {10.1109/ICSE43902.2021.00121},
	urldate = {2022-07-18},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Ferreira, Gabriel and Jia, Limin and Sunshine, Joshua and Kastner, Christian},
	month = may,
	year = {2021},
	note = {Place: Madrid, ES},
	pages = {1334--1346},
}

@inproceedings{ladisa_sok_2023,
	address = {Los Alamitos, CA, USA},
	title = {{SoK}: {Taxonomy} of {Attacks} on {Open}-{Source} {Software} {Supply} {Chains}},
	url = {https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179304},
	doi = {10.1109/SP46215.2023.10179304},
	abstract = {The widespread dependency on open-source software makes it a fruitful target for malicious actors, as demonstrated by recurring attacks. The complexity of today’s open-source supply chains results in a significant attack surface, giving attackers numerous opportunities to reach the goal of injecting malicious code into open-source artifacts that is then downloaded and executed by victims.This work proposes a general taxonomy for attacks on open-source supply chains, independent of specific programming languages or ecosystems, and covering all supply chain stages from code contributions to package distribution. Taking the form of an attack tree, it covers 107 unique vectors, linked to 94 real-world incidents, and mapped to 33 mitigating safeguards.User surveys conducted with 17 domain experts and 134 software developers positively validated the correctness, comprehensiveness and comprehensibility of the taxonomy, as well as its suitability for various use-cases. Survey participants also assessed the utility and costs of the identified safeguards, and whether they are used.},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE Computer Society},
	author = {Ladisa, P. and Plate, H. and Martinez, M. and Barais, O.},
	month = may,
	year = {2023},
	keywords = {costs, malware, security, supply chains, surveys, taxonomy, visualization},
	pages = {1509--1526},
}

@misc{docs_npm-audit_nodate,
	title = {npm-audit},
	abstract = {Run a security audit},
	urldate = {2022-07-18},
	author = {Docs, npm},
}

@misc{cisa_malware_nodate,
	title = {Malware {Discovered} in {Popular} {NPM} {Package}, ua-parser-js},
	urldate = {2022-07-18},
	author = {{CISA}},
}

@misc{noauthor_scim_2022,
	title = {{SCIM}: {Supply} {Chain} {Integrity} {Model}.},
	url = {https://github.com/microsoft/scim},
	year = {2022},
}

@inproceedings{zahan_what_2022,
	title = {What are {Weak} {Links} in the npm {Supply} {Chain}?},
	doi = {10.1109/ICSE-SEIP55303.2022.9794068},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Zahan, Nusrat and Zimmermann, Thomas and Godefroid, Patrice and Murphy, Brendan and Maddila, Chandra and Williams, Laurie},
	month = may,
	year = {2022},
	keywords = {Metadata, Open source software, Security, Software Ecosystem, Software engineering, Software measurement, Supply Chain Security, Supply chains, Weak link Signal, npm},
	pages = {331--340},
}

@misc{noauthor_security_2021,
	title = {Security issue: compromised npm packages of ua-parser-js (0.7.29, 0.8.0, 1.0.0) - {Questions} about deprecated npm package ua-parser-js · {Issue} \#536 · faisalman/ua-parser-js},
	shorttitle = {Security issue},
	urldate = {2022-07-18},
	year = {2021},
}

@techreport{joint_task_force_transformation_initiative_security_2013,
	title = {Security and {Privacy} {Controls} for {Federal} {Information} {Systems} and {Organizations}},
	number = {NIST SP 800-53r4},
	urldate = {2022-06-30},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Transformation Initiative}},
	month = apr,
	year = {2013},
	doi = {10.6028/NIST.SP.800-53r4},
	pages = {NIST SP 800--53r4},
}

@techreport{ross_systems_2018,
	address = {Gaithersburg, MD},
	title = {Systems security engineering: considerations for a multidisciplinary approach in the engineering of trustworthy secure systems, volume 1},
	shorttitle = {Systems security engineering},
	url = {http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v1.pdf},
	number = {NIST SP 800-160v1},
	urldate = {2022-06-30},
	institution = {National Institute of Standards and Technology},
	author = {Ross, Ron and McEvilley, Michael and Oren, Janet Carrier},
	month = mar,
	year = {2018},
	doi = {10.6028/NIST.SP.800-160v1},
	pages = {NIST SP 800--160v1},
}

@misc{international_organization_for_standardization_and_the_international_electrotechnical_commission_isoiec_nodate,
	title = {{ISO}/{IEC} 27002:2013- {Code} of practice for information security controls},
	shorttitle = {{ISO}/{IEC} 27002},
	abstract = {Information technology — Security techniques — Code of practice for information security controls},
	urldate = {2022-06-30},
	author = {{International Organization for Standardization and the International Electrotechnical Commission}},
}

@misc{international_organization_for_standardization_and_the_international_electrotechnical_commission_isoiec_nodate-1,
	title = {{ISO}/{IEC} 27036-4:2016 - {Information} security for supplier relationships},
	shorttitle = {{ISO}/{IEC} 27036-4},
	abstract = {Information technology — Security techniques — Information security for supplier relationships — Part 4: Guidelines for security of cloud services},
	urldate = {2022-06-30},
	author = {{International Organization for Standardization and the International Electrotechnical Commission}},
}

@misc{international_organization_for_standardization_and_the_international_electrotechnical_commission_isoiec_nodate-2,
	title = {{ISO}/{IEC} 20243-1:2018 - {Mitigating} maliciously tainted and counterfeit products},
	shorttitle = {{ISO}/{IEC} 20243-1},
	abstract = {Information technology — Open Trusted Technology ProviderTM Standard (O-TTPS) — Mitigating maliciously tainted and counterfeit products — Part 1: Requirements and recommendations},
	urldate = {2022-06-30},
	author = {{International Organization for Standardization and the International Electrotechnical Commission}},
}

@techreport{boyens_supply_2015,
	title = {Supply {Chain} {Risk} {Management} {Practices} for {Federal} {Information} {Systems} and {Organizations}},
	number = {NIST SP 800-161},
	urldate = {2022-06-30},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon M. and Paulsen, Celia and Moorthy, Rama and Bartol, Nadya},
	month = apr,
	year = {2015},
	doi = {10.6028/NIST.SP.800-161},
	pages = {NIST SP 800--161},
}

@misc{international_organization_for_standardization_and_the_international_electrotechnical_commission_iso_nodate,
	title = {{ISO} - {ISO}/{IEC} 27001 — {Information} security management},
	abstract = {Providing security for any kind of digital information, the ISO/IEC 27000 family of standards is designed for any size of organization.},
	urldate = {2022-06-30},
	author = {{International Organization for Standardization and the International Electrotechnical Commission}},
}

@misc{datadog_secure_2019,
	title = {Secure {Publication} of {Datadog} {Agent} {Integrations} with {TUF} and in toto},
	url = {https://www.datadoghq.com/blog/engineering/secure-publication-of-datadog-agent-integrations-with-tuf-and-in-toto/},
	abstract = {How to guarantee end-to-end security when using automation to package and publish Datadog Agent integrations},
	language = {en},
	author = {{Datadog}},
	month = jun,
	year = {2019},
	note = {Publication Title: Secure Publication of Datadog Agent Integrations with TUF and in toto},
}

@misc{telecommunications_minimum_2021,
	title = {The {Minimum} {Elements} {For} a {Software} {Bill} of {Materials} ({SBOM})},
	abstract = {The Executive Order (14028) on Improving the Nation’s Cybersecurity directs the Department of Commerce, in coordination with the National Telecommunications and Information Administration (NTIA), to publish the “minimum elements” for a Software Bill of Materials (SBOM). This report builds on the work of NTIA’s SBOM multistakeholder process, as well as the responses to a},
	author = {Telecommunications, National and Administration, Information},
	month = jul,
	year = {2021},
}

@techreport{standards_minimum_2006,
	title = {Minimum {Security} {Requirements} for {Federal} {Information} and {Information} {Systems}},
	url = {https://csrc.nist.gov/publications/detail/fips/200/final},
	language = {en},
	number = {Federal Information Processing Standard (FIPS) 200},
	institution = {U.S. Department of Commerce},
	author = {Standards, National Institute of and {Technology}},
	month = mar,
	year = {2006},
	doi = {10.6028/NIST.FIPS.200},
}

@inproceedings{vu_lastpymile_2021,
	address = {Athens Greece},
	title = {{LastPyMile}: identifying the discrepancy between sources and packages},
	isbn = {978-1-4503-8562-6},
	shorttitle = {{LastPyMile}},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468592},
	doi = {10.1145/3468264.3468592},
	language = {en},
	urldate = {2022-06-28},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Vu, Duc-Ly and Massacci, Fabio and Pashchenko, Ivan and Plate, Henrik and Sabetta, Antonino},
	month = aug,
	year = {2021},
	pages = {780--792},
}

@article{lamb_reproducible_2022,
	title = {Reproducible {Builds}: {Increasing} the {Integrity} of {Software} {Supply} {Chains}},
	volume = {39},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Reproducible {Builds}},
	url = {https://ieeexplore.ieee.org/document/9403390/},
	doi = {10.1109/MS.2021.3073045},
	number = {2},
	journal = {IEEE Software},
	author = {Lamb, Chris and Zacchiroli, Stefano},
	month = mar,
	year = {2022},
	pages = {62--70},
}

@misc{noauthor_software_nodate,
	title = {Software {Bill} of {Materials}},
	abstract = {A “software bill of materials” (SBOM) has emerged as a key building block in software security and software supply chain risk management. A SBOM is a nested inventory, a list of ingredients that make up software components.},
}

@article{torres-arias_-toto_2019,
	title = {in-toto: {Providing} farm-to-table guarantees for bits and bytes},
	shorttitle = {in-toto},
	url = {https://par.nsf.gov/biblio/10159903-toto-providing-farm-table-guarantees-bits-bytes},
	doi = {10.5555/3361338.3361435},
	abstract = {The software development process is quite complex and involves a number of independent actors. Developers check source code into a version control system, the code is compiled into software at a build farm, and CI/CD systems run multiple tests to ensure the software’s quality among a myriad of other operations. Finally, the software is packaged for distribution into a delivered product, to be consumed by end users. An attacker that is able to compromise any single step in the process can maliciously modify the software and harm any of the software’s users. To address these issues, we designed in-toto, a framework that cryptographically ensures the integrity of the software supply chain. in-toto grants the end user the ability to verify the software’s supply chain from the project’s inception to its deployment. We demonstrate in-toto’s effectiveness on 30 software supply chain compromises that affected hundreds of million of users and showcase in-toto’s usage over cloud-native, hybrid-cloud and cloud-agnostic applications. in-toto is integrated into products and open source projects that are used by millions of people daily.},
	language = {en},
	urldate = {2022-06-18},
	journal = {Proc. of the 28th USENIX Security Symposium},
	author = {Torres-Arias, Santiago and Afzali, Hammad and Kuppusamy, Trishank Karthik and Curtmola, Reza and Cappos, Justin},
	month = aug,
	year = {2019},
}

@techreport{paulsen_criticality_2018,
	title = {Criticality {Analysis} {Process} {Model}: {Prioritizing} {Systems} and {Components}},
	shorttitle = {Criticality {Analysis} {Process} {Model}},
	url = {https://csrc.nist.gov/publications/detail/nistir/8179/final},
	abstract = {In the modern world, where complex systems and systems-of-systems are integral to the functioning of society and businesses, it is increasingly important to be able to understand and manage risks that these systems and components may present to the missions that they support. However, in the world of finite resources, it is not possible to apply equal protection to all assets. This publication describes a comprehensive Criticality Analysis Process Model – a structured method of prioritizing programs, systems, and components based on their importance to the goals of an organization and the impact that their inadequate operation or loss may present to those goals. A criticality analysis can help organizations identify and better understand the systems, subsystems, components, and subcomponents that are most essential to their operations and the environment in which they operate. That understanding facilitates better decision making related to the management of an organization’s...},
	language = {en},
	number = {NIST Internal or Interagency Report (NISTIR) 8179},
	institution = {National Institute of Standards and Technology},
	author = {Paulsen, Celia and Boyens, Jon and Bartol, Nadya and Winkler, Kris},
	month = apr,
	year = {2018},
	doi = {10.6028/NIST.IR.8179},
}

@techreport{boyens_cybersecurity_2022,
	title = {Cybersecurity {Supply} {Chain} {Risk} {Management} {Practices} for {Systems} and {Organizations}},
	url = {https://csrc.nist.gov/publications/detail/sp/800-161/rev-1/final},
	abstract = {Organizations are concerned about the risks associated with products and services that may potentially contain malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the supply chain. These risks are associated with an enterprise’s decreased visibility into and understanding of how the technology they acquire is developed, integrated, and deployed or the processes, procedures, standards, and practices used to ensure the security, resilience, reliability, safety, integrity, and quality of the products and services. This publication provides guidance to organizations on identifying, assessing, and mitigating cybersecurity risks throughout the supply chain at all levels of their organizations. The publication integrates cybersecurity supply chain risk management (C-SCRM) into risk management activities by applying a multilevel, C-SCRM-specific approach, including guidance on the development of C-SCRM strategy implementation...},
	language = {en},
	number = {NIST Special Publication (SP) 800-161 Rev. 1},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon and Smith, Angela and Bartol, Nadya and Winkler, Kris and Holbrook, Alex and Fallon, Matthew},
	month = may,
	year = {2022},
	doi = {10.6028/NIST.SP.800-161r1},
}

@article{ameri_cybert_2021,
	title = {{CyBERT}: {Cybersecurity} {Claim} {Classification} by {Fine}-{Tuning} the {BERT} {Language} {Model}},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2624-800X},
	shorttitle = {{CyBERT}},
	url = {https://www.mdpi.com/2624-800X/1/4/31},
	doi = {10.3390/jcp1040031},
	abstract = {We introduce CyBERT, a cybersecurity feature claims classifier based on bidirectional encoder representations from transformers and a key component in our semi-automated cybersecurity vetting for industrial control systems (ICS). To train CyBERT, we created a corpus of labeled sequences from ICS device documentation collected across a wide range of vendors and devices. This corpus provides the foundation for fine-tuning BERT’s language model, including a prediction-guided relabeling process. We propose an approach to obtain optimal hyperparameters, including the learning rate, the number of dense layers, and their configuration, to increase the accuracy of our classifier. Fine-tuning all hyperparameters of the resulting model led to an increase in classification accuracy from 76\% obtained with BertForSequenceClassification’s original architecture to 94.4\% obtained with CyBERT. Furthermore, we evaluated CyBERT for the impact of randomness in the initialization, training, and data-sampling phases. CyBERT demonstrated a standard deviation of ±0.6\% during validation across 100 random seed values. Finally, we also compared the performance of CyBERT to other well-established language models including GPT2, ULMFiT, and ELMo, as well as neural network models such as CNN, LSTM, and BiLSTM. The results showed that CyBERT outperforms these models on the validation accuracy and the F1 score, validating CyBERT’s robustness and accuracy as a cybersecurity feature claims classifier.},
	language = {en},
	number = {4},
	urldate = {2023-08-25},
	journal = {Journal of Cybersecurity and Privacy},
	author = {Ameri, Kimia and Hempel, Michael and Sharif, Hamid and Lopez Jr., Juan and Perumalla, Kalyan},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {BERT, CYVET, classification, cybersecurity, natural language processing, transfer learning},
	pages = {615--637},
}

@inproceedings{vasilakis_preventing_2021,
	address = {New York, NY, USA},
	series = {{CCS} '21},
	title = {Preventing {Dynamic} {Library} {Compromise} on {Node}.js via {RWX}-{Based} {Privilege} {Reduction}},
	isbn = {978-1-4503-8454-4},
	url = {https://dl.acm.org/doi/10.1145/3460120.3484535},
	doi = {10.1145/3460120.3484535},
	abstract = {Third-party libraries ease the development of large-scale software systems. However, libraries often execute with significantly more privilege than needed to complete their task. Such additional privilege is sometimes exploited at runtime via inputs passed to a library, even when the library itself is not actively malicious. We present Mir, a system addressing dynamic compromise by introducing a fine-grained read-write-execute (RWX) permission model at the boundaries of libraries: every field of every free variable name in the context of an imported library is governed by a permission set. To help specify the permissions given to existing code, Mir's automated inference generates default permissions by analyzing how libraries are used by their clients. Applied to over 1,000 JavaScript libraries for Node.js, Mir shows practical security (61/63 attacks mitigated), performance (2.1s for static analysis and +1.93\% for dynamic enforcement), and compatibility (99.09\%) characteristics---and enables a novel quantification of privilege reduction.},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Vasilakis, Nikos and Staicu, Cristian-Alexandru and Ntousakis, Grigoris and Kallas, Konstantinos and Karel, Ben and DeHon, André and Pradel, Michael},
	month = nov,
	year = {2021},
	keywords = {program analysis, supply-chain attacks, third-party libraries},
	pages = {1821--1838},
}

@inproceedings{bishop1999vulnerabilities,
	title = {Vulnerabilities analysis},
	booktitle = {Proceedings of the recent advances in intrusion detection},
	publisher = {Citeseer},
	author = {Bishop, Matt},
	year = {1999},
	pages = {125--136},
}

@misc{stufft_pgp_2018,
	title = {{PGP} signatures are not displayed {\textbar} {Issue} \#3356},
	url = {https://github.com/pypi/warehouse/issues/3356},
	abstract = {This is the same issue as \#703. Uploaded PGP signatures are not visible in Warehouse. See e.g. https://pypi.python.org/pypi/cryptography vs https://pypi.org/project/cryptography/ .},
	language = {en},
	urldate = {2023-08-21},
	journal = {GitHub},
	author = {Stufft, Donald},
	month = mar,
	year = {2018},
}

@inproceedings{tang_empirical_2021,
	title = {An {Empirical} {Study} of {Refactorings} and {Technical} {Debt} in {Machine} {Learning} {Systems}},
	doi = {10.1109/ICSE43902.2021.00033},
	abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major cross-cutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Best practices, Complex systems, Data mining, Deep learning, Open source software, Software engineering, Tools, empirical studies, machine learning systems, refactoring, software evolution, software repository mining, technical debt},
	pages = {238--250},
}

@misc{noauthor_defeating_nodate,
	title = {Defeating {Solar} {Designer}'s {Non}-executable {Stack} {Patch}},
	url = {https://insecure.org/sploits/non-executable.stack.problems.html},
	urldate = {2023-08-19},
}

@inproceedings{mcbride_security_2018,
	address = {USA},
	series = {{EWSN} \&\#x2019;18},
	title = {Security {Analysis} of {Contiki} {IoT} {Operating} {System}},
	isbn = {978-0-9949886-2-1},
	abstract = {The Internet of Things (IoT) has introduced a myriad of ways in which devices can interact with each other. The IoT concept provides opportunities for novel and useful applications but at the same time, concerns have been raised over potential security issues caused by buggy IoT software. It is therefore imperative to detect and fix these bugs in order to minimise the risk of IoT devices becoming the target or source of attacks. In this paper, we focus our investigation on the underlying IoT operating system (OS), which is critical for the overall security of IoT devices. We picked Contiki as our case study since it is a very popular IoT OS and we have access to part of the development team, allowing us to discuss potential vulnerabilities with them so that fixes can be implemented quickly. Using static program analysis tools and techniques, we are able to scan the source code of the Contiki OS systematically in order to identify, analyse and patch vulnerabilities. Our main contribution is a holistic and systematic analysis of Contiki, starting with an exploration of its metrics, fundamental architecture, and finally some of its vulnerabilities. Our analysis produced relevant data on the number of unsafe functions in use, as well as the bug density; both of which provide an indication of the overall security of the inspected system. Our effort led to the finding of two major issues, described in two Common Vulnerabilities and Exposures (CVE) reports.},
	urldate = {2023-08-18},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Embedded} {Wireless} {Systems} and {Networks}},
	publisher = {Junction Publishing},
	author = {McBride, Jack and Arief, Budi and Hernandez-Castro, Julio},
	month = feb,
	year = {2018},
	keywords = {Contiki, Internet of Things, Security analysis},
	pages = {278--283},
}

@article{al-boghdady_presence_2021,
	title = {The {Presence}, {Trends}, and {Causes} of {Security} {Vulnerabilities} in {Operating} {Systems} of {IoT}’s {Low}-{End} {Devices}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/7/2329},
	doi = {10.3390/s21072329},
	abstract = {Internet of Things Operating Systems (IoT OSs) run, manage and control IoT devices. Therefore, it is important to secure the source code for IoT OSs, especially if they are deployed on devices used for human care and safety. In this paper, we report the results of our investigations of the security status and the presence of security vulnerabilities in the source code of the most popular open source IoT OSs. Through this research, three Static Analysis Tools (Cppcheck, Flawfinder and RATS) were used to examine the code of sixteen different releases of four different C/C++ IoT OSs, with 48 examinations, regarding the presence of vulnerabilities from the Common Weakness Enumeration (CWE). The examination reveals that IoT OS code still suffers from errors that lead to security vulnerabilities and increase the opportunity of security breaches. The total number of errors in IoT OSs is increasing from version to the next, while error density, i.e., errors per 1K of physical Source Lines of Code (SLOC) is decreasing chronologically for all IoT Oss, with few exceptions. The most prevalent vulnerabilities in IoT OS source code were CWE-561, CWE-398 and CWE-563 according to Cppcheck, (CWE-119!/CWE-120), CWE-120 and CWE-126 according to Flawfinder, and CWE-119, CWE-120 and CWE-134 according to RATS. Additionally, the CodeScene tool was used to investigate the development of the evolutionary properties of IoT OSs and the relationship between them and the presence of IoT OS vulnerabilities. CodeScene reveals strong positive correlation between the total number of security errors within IoT OSs and SLOC, as well as strong negative correlation between the total number of security errors and Code Health. CodeScene also indicates strong positive correlation between security error density (errors per 1K SLOC) and the presence of hotspots (frequency of code changes and code complexity), as well as strong negative correlation between security error density and the Qualitative Team Experience, which is a measure of the experience of the IoT OS developers.},
	language = {en},
	number = {7},
	urldate = {2023-08-18},
	journal = {Sensors},
	author = {Al-Boghdady, Abdullah and Wassif, Khaled and El-Ramly, Mohammad},
	month = jan,
	year = {2021},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {C/C++ static analysis, common weakness enumeration, internet of things operating systems, internet of things security, security vulnerability},
	pages = {2329},
}

@inproceedings{hellman_characterizing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Characterizing user behaviors in open-source software user forums: an empirical study},
	isbn = {978-1-4503-9342-3},
	shorttitle = {Characterizing user behaviors in open-source software user forums},
	url = {https://dl.acm.org/doi/10.1145/3528579.3529178},
	doi = {10.1145/3528579.3529178},
	language = {en},
	urldate = {2023-08-18},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Hellman, Jazlyn and Chen, Jiahao and Uddin, Md. Sami and Cheng, Jinghui and Guo, Jin L. C.},
	month = may,
	year = {2022},
	pages = {46--55},
}

@article{amusuo_systematically_nodate,
	title = {Systematically {Detecting} {Packet} {Validation} {Vulnerabilities} in {Embedded} {Network} {Stacks}},
	abstract = {Embedded Network Stacks (ENS) enable communication with cyber-physical systems. Many defects in ENS are high-severity cybersecurity vulnerabilities: they are remotely triggerable and can impact the physical world. The most common automated approach to detecting ENS defects is feedback-driven random dynamic analysis (“fuzzing”), a costly and unpredictable technique. While prior research has shed light on the characteristics of defects in many classes of software systems, no study has described the properties of known ENS defects nor identified a systematic technique for exposing them.},
	language = {en},
	author = {Amusuo, Paschal and Mendez, Ricardo Andres Calvo and Xu, Zhongwei and Machiry, Aravind and Davis, James C},
}

@article{peters2023sustainability,
	title = {Sustainability in computing education: {A} systematic literature review},
	journal = {arXiv preprint arXiv:2305.10369},
	author = {Peters, A-K and Capilla, R and Coroamă, VC and Heldal, R and Lago, P and Leifler, O and Moreira, A and Fernandes, JP and Penzenstadler, B and Porras, J and {others}},
	year = {2023},
}

@article{sarkar2022like,
	title = {What is it like to program with artificial intelligence?},
	journal = {arXiv preprint arXiv:2208.06213},
	author = {Sarkar, Advait and Gordon, Andrew D and Negreanu, Carina and Poelitz, Christian and Ragavan, Sruti Srinivasa and Zorn, Ben},
	year = {2022},
}

@techreport{braden_requirements_1989,
	type = {Request for {Comments}},
	title = {Requirements for {Internet} {Hosts} - {Communication} {Layers}},
	url = {https://datatracker.ietf.org/doc/rfc1122},
	abstract = {This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. [STANDARDS-TRACK]},
	number = {RFC 1122},
	urldate = {2023-08-15},
	institution = {Internet Engineering Task Force},
	author = {Braden, Robert T.},
	month = oct,
	year = {1989},
	doi = {10.17487/RFC1122},
	note = {Num Pages: 116},
}

@misc{latacora_pgp_2019,
	title = {The {PGP} {Problem}},
	url = {https://latacora.micro.blog/2019/07/16/the-pgp-problem.html},
	urldate = {2023-08-15},
	journal = {Latacora},
	author = {Latacora},
	month = jul,
	year = {2019},
}

@misc{noauthor_latacora_nodate,
	title = {Latacora - {The} {PGP} {Problem}},
	url = {https://latacora.micro.blog/2019/07/16/the-pgp-problem.html},
	urldate = {2023-08-15},
}

@misc{green_whats_2014,
	title = {What’s the matter with {PGP}?},
	url = {https://blog.cryptographyengineering.com/2014/08/13/whats-matter-with-pgp/},
	abstract = {Last Thursday, Yahoo announced their plans to support end-to-end encryption using a fork of Google’s end-to-end email extension. This is a Big Deal. With providers like Google and Yahoo onboa…},
	language = {en},
	urldate = {2023-08-15},
	journal = {A Few Thoughts on Cryptographic Engineering},
	author = {Green, Matthew},
	month = aug,
	year = {2014},
}

@techreport{robert_t_braden_requirements_1989,
	type = {Request for {Comments}},
	title = {Requirements for {Internet} {Hosts} - {Application} and {Support}},
	url = {https://datatracker.ietf.org/doc/rfc1123},
	abstract = {This RFC is an official specification for the Internet community. It incorporates by reference, amends, corrects, and supplements the primary protocol standards documents relating to hosts. [STANDARDS-TRACK]},
	number = {RFC 1123},
	urldate = {2023-02-26},
	institution = {Internet Engineering Task Force},
	author = {{Robert T. Braden}},
	month = oct,
	year = {1989},
	doi = {10.17487/RFC1123},
	note = {Num Pages: 98},
}

@misc{noauthor_codeql_nodate,
	title = {{CodeQL}},
	url = {https://codeql.github.com/},
	urldate = {2023-08-15},
}

@misc{ami_false_2023,
	title = {"{False} negative -- that one is going to kill you": {Understanding} {Industry} {Perspectives} of {Static} {Analysis} based {Security} {Testing}},
	shorttitle = {"{False} negative -- that one is going to kill you"},
	url = {http://arxiv.org/abs/2307.16325},
	doi = {10.48550/arXiv.2307.16325},
	abstract = {The demand for automated security analysis techniques, such as static analysis based security testing (SAST) tools continues to increase. To develop SASTs that are effectively leveraged by developers for finding vulnerabilities, researchers and tool designers must understand how developers perceive, select, and use SASTs, what they expect from the tools, whether they know of the limitations of the tools, and how they address those limitations. This paper describes a qualitative study that explores the assumptions, expectations, beliefs, and challenges experienced by developers who use SASTs. We perform in-depth, semi-structured interviews with 20 practitioners who possess a diverse range of software development expertise, as well as a variety of unique security, product, and organizational backgrounds. We identify \$17\$ key findings that shed light on developer perceptions and desires related to SASTs, and also expose gaps in the status quo - challenging long-held beliefs in SAST design priorities. Finally, we provide concrete future directions for researchers and practitioners rooted in an analysis of our findings.},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Ami, Amit Seal and Moran, Kevin and Poshyvanyk, Denys and Nadkarni, Adwait},
	month = aug,
	year = {2023},
	note = {arXiv:2307.16325 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{zahan_openssf_2023,
	title = {{OpenSSF} {Scorecard}: {On} the {Path} {Toward} {Ecosystem}-{Wide} {Automated} {Security} {Metrics}},
	issn = {1540-7993},
	shorttitle = {{OpenSSF} {Scorecard}},
	url = {https://www.computer.org/csdl/magazine/sp/5555/01/10163720/1Oiui5hEZ5C},
	doi = {10.1109/MSEC.2023.3279773},
	abstract = {The OpenSSF Scorecard project is an automated tool to monitor the security health of open source software. This study evaluates the applicability of the Scorecard tool and compares the security practices and gaps in the npm and PyPI ecosystems.},
	language = {English},
	number = {01},
	urldate = {2023-08-15},
	journal = {IEEE Security \& Privacy},
	author = {Zahan, Nusrat and Kanakiya, Parth and Hambleton, Brian and Shohan, Shohanuzzaman and Williams, Laurie},
	month = jun,
	year = {2023},
	note = {Publisher: IEEE Computer Society},
	pages = {2--14},
}

@article{brown2020language,
	title = {Language models are few-shot learners},
	volume = {33},
	journal = {Advances in neural information processing systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and {others}},
	year = {2020},
	pages = {1877--1901},
}

@article{kojima2022large,
	title = {Large language models are zero-shot reasoners},
	volume = {35},
	journal = {Advances in neural information processing systems},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	year = {2022},
	pages = {22199--22213},
}

@article{wing_computational_2008,
	title = {Computational thinking and thinking about computing},
	volume = {366},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2008.0118},
	doi = {10.1098/rsta.2008.0118},
	abstract = {Computational thinking will influence everyone in every field of endeavour. This vision poses a new educational challenge for our society, especially for our children. In thinking about computing, we need to be attuned to the three drivers of our field: science, technology and society. Accelerating technological advances and monumental societal demands force us to revisit the most basic scientific questions of computing.},
	language = {en},
	number = {1881},
	urldate = {2023-08-15},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Wing, Jeannette M},
	month = oct,
	year = {2008},
	pages = {3717--3725},
}

@book{kim1999introduction,
	title = {Introduction to systems thinking},
	volume = {16},
	publisher = {Pegasus Communications Waltham, MA},
	author = {Kim, Daniel H},
	year = {1999},
}

@inproceedings{beyer_advances_2022,
	address = {Berlin, Heidelberg},
	title = {Advances in {Automatic} {Software} {Testing}: {Test}-{Comp} 2022},
	isbn = {978-3-030-99428-0},
	shorttitle = {Advances in {Automatic} {Software} {Testing}},
	url = {https://doi.org/10.1007/978-3-030-99429-7_18},
	doi = {10.1007/978-3-030-99429-7_18},
	abstract = {Test-Comp 2022 is the 4th edition of the Competition on Software Testing. Research competitions are a means to provide annual comparative evaluations. Test-Comp focusses on fully automatic software test generators for C programs. The results of the competition shall be reproducible and provide an overview of the current state of the art in the area of automatic test-generation. The competition was based on 4 236 test-generation tasks for C programs. Each test-generation task consisted of a program and a test specification (error coverage, branch coverage). Test-Comp 2022 had 12 participating test generators from 5 countries.},
	urldate = {2023-08-14},
	booktitle = {International {Conference} on {Fundamental} {Approaches} to {Software} {Engineering}},
	publisher = {Springer-Verlag},
	author = {Beyer, Dirk},
	month = apr,
	year = {2022},
	keywords = {BenchExec, Benchmarking, Bug Finding, CoVeriTeam, Competition, Program Analysis, SV-Benchmarks, Software Bugs, Software Testing, Software Validation, Test Coverage, Test Validation, Test-Case Generation, Test-Comp, Test-Suites, TestCov},
	pages = {321--335},
}

@inproceedings{cardwell_packetdrill_2013,
	title = {packetdrill: {Scriptable} {Network} {Stack} {Testing}, from {Sockets} to {Packets}},
	isbn = {978-1-931971-01-0},
	shorttitle = {packetdrill},
	url = {https://www.usenix.org/conference/atc13/technical-sessions/presentation/cardwell},
	language = {en},
	urldate = {2023-08-15},
	author = {Cardwell, Neal and Cheng, Yuchung and Brakmo, Lawrence and Mathis, Matt and Raghavan, Barath and Dukkipati, Nandita and Chu, Hsiao-keng Jerry and Terzis, Andreas and Herbert, Tom},
	year = {2013},
	pages = {213--218},
}

@inproceedings{pedrosa_analyzing_2015,
	address = {USA},
	series = {{NSDI}'15},
	title = {Analyzing protocol implementations for interoperability},
	isbn = {978-1-931971-21-8},
	abstract = {We propose PIC, a tool that helps developers search for non-interoperabilities in protocol implementations. We formulate this problem using intersection of the sets of messages that one protocol participant can send but another will reject as non-compliant. PIC leverages symbolic execution to characterize these sets and uses two novel techniques to scale to real-world implementations. First, it uses joint symbolic execution, in which receiver-side program analysis is constrained based on sender-side constraints, dramatically reducing the number of execution paths to consider. Second, it incorporates a search strategy that steers symbolic execution toward likely non-interoperabilities. We show that PIC is able to find multiple previously unknown noninteroperabilities in large and mature implementations of the SIP and SPDY (v2 through v3.1) protocols, some of which have since been fixed by the respective developers.},
	urldate = {2023-08-14},
	booktitle = {Proceedings of the 12th {USENIX} {Conference} on {Networked} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Pedrosa, Luis and Fogel, Ari and Kothari, Nupur and Govindan, Ramesh and Mahajan, Ratul and Millstein, Todd},
	month = may,
	year = {2015},
	pages = {485--498},
}

@article{natella_stateafl_2022,
	title = {{StateAFL}: {Greybox} fuzzing for stateful network servers},
	volume = {27},
	issn = {1382-3256},
	shorttitle = {{StateAFL}},
	url = {https://doi.org/10.1007/s10664-022-10233-3},
	doi = {10.1007/s10664-022-10233-3},
	abstract = {Fuzzing network servers is a technical challenge, since the behavior of the target server depends on its state over a sequence of multiple messages. Existing solutions are costly and difficult to use, as they rely on manually-customized artifacts such as protocol models, protocol parsers, and learning frameworks. The aim of this work is to develop a greybox fuzzer (StateAFL) for network servers that only relies on lightweight analysis of the target program, with no manual customization, in a similar way to what the AFL fuzzer achieved for stateless programs. The proposed fuzzer instruments the target server at compile-time, to insert probes on memory allocations and network I/O operations. At run-time, it infers the current protocol state of the target server by taking snapshots of long-lived memory areas, and by applying a fuzzy hashing algorithm (Locality-Sensitive Hashing) to map memory contents to a unique state identifier. The fuzzer incrementally builds a protocol state machine for guiding fuzzing. We implemented and released StateAFL as open-source software. As a basis for reproducible experimentation, we integrated StateAFL with a large set of network servers for popular protocols, with no manual customization to accomodate for the protocol. The experimental results show that the fuzzer can be applied with no manual customization on a large set of network servers for popular protocols, and that it can achieve comparable, or even better code coverage and bug detection than customized fuzzing. Moreover, our qualitative analysis shows that states inferred from memory better reflect the server behavior than only using response codes from messages.},
	number = {7},
	urldate = {2023-08-14},
	journal = {Empirical Software Engineering},
	author = {Natella, Roberto},
	month = dec,
	year = {2022},
	keywords = {Fuzzing, Network servers, Security},
}

@article{barroso_case_2007,
	title = {The {Case} for {Energy}-{Proportional} {Computing}},
	volume = {40},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/4404806/},
	doi = {10.1109/MC.2007.443},
	language = {en},
	number = {12},
	urldate = {2023-08-14},
	journal = {Computer},
	author = {Barroso, Luiz André and Hölzle, Urs},
	month = dec,
	year = {2007},
	pages = {33--37},
}

@incollection{abramson_performance_2019,
	address = {Cham},
	title = {Performance {Evaluation} and {Analysis} of {Linear} {Algebra} {Kernels} in the {Prototype} {Tianhe}-3 {Cluster}},
	volume = {11416},
	isbn = {978-3-030-18644-9 978-3-030-18645-6},
	url = {http://link.springer.com/10.1007/978-3-030-18645-6_6},
	abstract = {As the supercomputing system entering the exascale era, power consumption becomes a major concern in the system design. Among all the novel techniques for reducing power consumption, ARM architecture is gaining popularity in the HPC community due to its low power footprint and high energy eﬃciency. As one of the initiatives for addressing the exascale challenges in China, Tianhe-3 supercomputer has adopted the technology roadmap of using the many-core ARM architecture with home-built phytium-2000+ and matrix-2000+ processors. In this paper, we evaluate several linear algebra kernels such as matrixmatrix multiplication, matrix-vector multiplication and triangular solver with both sparse and dense datasets. These linear algebra kernels are good performance indicators of the prototype Tianhe-3 cluster. Comprehensive analysis is performed using rooﬂine model to identify the directions for performance optimization from both hardware and software perspectives. In addition, we compare the performance of phytium2000+ and matrix-2000+ with widely used KNL processor. We believe this paper provides valuable experiences and insights as work-in-progress towards exascale for the HPC community.},
	language = {en},
	urldate = {2023-08-12},
	booktitle = {Supercomputing {Frontiers}},
	publisher = {Springer International Publishing},
	author = {You, Xin and Yang, Hailong and Luan, Zhongzhi and Liu, Yi and Qian, Depei},
	editor = {Abramson, David and De Supinski, Bronis R.},
	year = {2019},
	doi = {10.1007/978-3-030-18645-6_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {86--105},
}

@incollection{goos_specomp_2001,
	address = {Berlin, Heidelberg},
	title = {{SPEComp}: {A} {New} {Benchmark} {Suite} for {Measuring} {Parallel} {Computer} {Performance}},
	volume = {2104},
	isbn = {978-3-540-42346-1 978-3-540-44587-6},
	shorttitle = {{SPEComp}},
	url = {http://link.springer.com/10.1007/3-540-44587-0_1},
	abstract = {We present a new benchmark suite for parallel computers. SPEComp targets mid-size parallel servers. It includes a number of science/engineering and data processing applications. Parallelism is expressed in the OpenMP API. The suite includes two data sets, Medium and Large, of approximately 1.6 and 4 GB in size. Our overview also describes the organization developing SPEComp, issues in creating OpenMP parallel benchmarks, the benchmarking methodology underlying SPEComp, and basic performance characteristics.},
	language = {en},
	urldate = {2023-08-12},
	booktitle = {{OpenMP} {Shared} {Memory} {Parallel} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Aslot, Vishal and Domeika, Max and Eigenmann, Rudolf and Gaertner, Greg and Jones, Wesley B. and Parady, Bodo},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Eigenmann, Rudolf and Voss, Michael J.},
	year = {2001},
	doi = {10.1007/3-540-44587-0_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--10},
}

@misc{singla_empirical_2023,
	title = {An {Empirical} {Study} on {Using} {Large} {Language} {Models} to {Analyze} {Software} {Supply} {Chain} {Security} {Failures}},
	url = {http://arxiv.org/abs/2308.04898},
	abstract = {As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68\% and Bard had an accuracy of 58\% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.},
	language = {en},
	urldate = {2023-08-11},
	publisher = {arXiv},
	author = {Singla, Tanmay and Anandayuvaraj, Dharun and Kalu, Kelechi G. and Schorlemmer, Taylor R. and Davis, James C.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04898 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{felder2012engineering,
	title = {Engineering education: {A} tale of two paradigms},
	journal = {Shaking the foundations of Geo-Engineering education},
	author = {Felder, Richard M},
	year = {2012},
	pages = {9--14},
}

@article{mccabe_shaking_nodate,
	title = {Shaking the {Foundations} of {Geo}-engineering {Education}},
	abstract = {Engineering education is in a turbulent period. Chronic industry complaints about skill deficiencies in engineering graduates, high attrition rates of engineering students with good academic performance records, the worldwide adoption of outcomes-based engineering program accreditation, and findings from both cognitive science and thousands of educational research studies showing serious deficiencies in traditional teaching methods have all provoked calls for changes in how engineering curricula are structured, delivered, and assessed. As might be expected, many academic staff members and administrators are less than enthusiastic about the proposed changes, arguing that the traditional system functions well and needs no radical revision. The ongoing debate involves four focal issues: how engineering curricula should be structured, how engineering courses should be taught and assessed, who should teach, and how the teachers should be prepared. This paper outlines two conflicting educational paradigms and the position on each of these four issues that each one reflects—the traditional paradigm, which has dominated engineering education since its inception, and the emerging alternative—and offers predictions about the eventual resolution.},
	language = {en},
	author = {McCabe, Bryan},
}

@article{prince_inductive_2006,
	title = {Inductive {Teaching} and {Learning} {Methods}: {Definitions}, {Comparisons}, and {Research} {Bases}},
	volume = {95},
	issn = {10694730},
	shorttitle = {Inductive {Teaching} and {Learning} {Methods}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2006.tb00884.x},
	doi = {10.1002/j.2168-9830.2006.tb00884.x},
	abstract = {Traditional engineering instruction is deductive, beginning with theories and progressing to the applications of those theories. Alternative teaching approaches are more inductive. Topics are introduced by presenting specific observations, case studies or problems, and theories are taught or the students are helped to discover them only after the need to know them has been established. This study reviews several of the most commonly used inductive teaching methods, including inquiry learning, problembased learning, project-based learning, case-based teaching, discovery learning, and just-in-time teaching. The paper defines each method, highlights commonalities and specific differences, and reviews research on the effectiveness of the methods. While the strength of the evidence varies from one method to another, inductive methods are consistently found to be at least equal to, and in general more effective than, traditional deductive methods for achieving a broad range of learning outcomes.},
	language = {en},
	number = {2},
	urldate = {2023-08-11},
	journal = {Journal of Engineering Education},
	author = {Prince, Michael J. and Felder, Richard M.},
	month = apr,
	year = {2006},
	pages = {123--138},
}

@book{felder2016teaching,
	title = {Teaching and learning {STEM}: {A} practical guide},
	publisher = {John Wiley \& Sons},
	author = {Felder, Richard M and Brent, Rebecca},
	year = {2016},
}

@misc{ami_false_2023-1,
	title = {"{False} negative -- that one is going to kill you": {Understanding} {Industry} {Perspectives} of {Static} {Analysis} based {Security} {Testing}},
	shorttitle = {"{False} negative -- that one is going to kill you"},
	url = {http://arxiv.org/abs/2307.16325},
	abstract = {The demand for automated security analysis techniques, such as static analysis based security testing (SAST) tools continues to increase. To develop SASTs that are effectively leveraged by developers for finding vulnerabilities, researchers and tool designers must understand how developers perceive, select, and use SASTs, what they expect from the tools, whether they know of the limitations of the tools, and how they address those limitations. This paper describes a qualitative study that explores the assumptions, expectations, beliefs, and challenges experienced by developers who use SASTs. We perform in-depth, semi-structured interviews with 20 practitioners who possess a diverse range of software development expertise, as well as a variety of unique security, product, and organizational backgrounds. We identify 17 key findings that shed light on developer perceptions and desires related to SASTs, and also expose gaps in the status quo –challenging long-held beliefs in SAST design priorities. Finally, we provide concrete future directions for researchers and practitioners rooted in an analysis of our findings.},
	language = {en},
	urldate = {2023-08-11},
	publisher = {arXiv},
	author = {Ami, Amit Seal and Moran, Kevin and Poshyvanyk, Denys and Nadkarni, Adwait},
	month = aug,
	year = {2023},
	note = {arXiv:2307.16325 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{vo_integrating_2006,
	title = {Integrating systems thinking into {IS} education},
	volume = {23},
	issn = {10927026},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sres.720},
	doi = {10.1002/sres.720},
	language = {en},
	number = {1},
	urldate = {2023-08-11},
	journal = {Systems Research and Behavioral Science},
	author = {Vo, Huy V. and Chae, Bongsug and Olson, David L.},
	month = jan,
	year = {2006},
	pages = {107--121},
}

@article{shin_framework_2022,
	title = {A framework for supporting systems thinking and computational thinking through constructing models},
	volume = {50},
	issn = {0020-4277, 1573-1952},
	url = {https://link.springer.com/10.1007/s11251-022-09590-9},
	doi = {10.1007/s11251-022-09590-9},
	abstract = {We face complex global issues such as climate change that challenge our ability as humans to manage them. Models have been used as a pivotal science and engineering tool to investigate, represent, explain, and predict phenomena or solve problems that involve multi-faceted systems across many fields. To fully explain complex phenomena or solve problems using models requires both systems thinking (ST) and computational thinking (CT). This study proposes a theoretical framework that uses modeling as a way to integrate ST and CT. We developed a framework to guide the complex process of developing curriculum, learning tools, support strategies, and assessments for engaging learners in ST and CT in the context of modeling. The framework includes essential aspects of ST and CT based on selected literature, and illustrates how each modeling practice draws upon aspects of both ST and CT to support explaining phenomena and solving problems. We use computational models to show how these ST and CT aspects are manifested in modeling.},
	language = {en},
	number = {6},
	urldate = {2023-08-11},
	journal = {Instructional Science},
	author = {Shin, Namsoo and Bowers, Jonathan and Roderick, Steve and McIntyre, Cynthia and Stephens, A. Lynn and Eidin, Emil and Krajcik, Joseph and Damelin, Daniel},
	month = dec,
	year = {2022},
	pages = {933--960},
}

@article{hamidi_complementary_2023,
	title = {A {Complementary} {View} to {Computational} {Thinking} and {Its} {Interplay} with {Systems} {Thinking}},
	volume = {13},
	issn = {2227-7102},
	url = {https://www.mdpi.com/2227-7102/13/2/201},
	doi = {10.3390/educsci13020201},
	abstract = {Computational Thinking (CT) pervasively shares its methods, practices, and dispositions across other disciplines as a new way of thinking about problem-solving. Few studies have been carried out studying CT from an Information Systems (IS) perspective. This study elaborates on how systems thinking (ST), an acknowledged theory in the IS ﬁeld, bonds to CT to address some wellknown common issues related to CT such as reductionism and dogmatism, and to supplement the computing nature of CT with behavioral and societal facets involved in its implications. We studied how ST is applied to CT research in the literature. To do so, two primary approaches have been identiﬁed that link ST and CT. First, ST is embedded in CT practices meaning that ST is considered as a component of CT. Second, ST and CT are parallelly studied, and ST is considered as a supplementary concept to CT. Correspondingly, we propose a complementary approach that looks at CT from the ST lenses to provide a clearer picture of CT in an educational context. Moreover, we expect this new perspective can help to broaden the development of educational CT concepts and scenarios by including new notions such as framework, interpretation, norms, paradigm, and context.},
	language = {en},
	number = {2},
	urldate = {2023-08-11},
	journal = {Education Sciences},
	author = {Hamidi, Ali and Mirijamdotter, Anita and Milrad, Marcelo},
	month = feb,
	year = {2023},
	pages = {201},
}

@inproceedings{easterbrook_computational_2014,
	address = {Stockholm, Sweden},
	title = {From {Computational} {Thinking} to {Systems} {Thinking}: {A} conceptual toolkit for sustainability computing:},
	shorttitle = {From {Computational} {Thinking} to {Systems} {Thinking}},
	url = {https://www.atlantis-press.com/article/13446},
	doi = {10.2991/ict4s-14.2014.28},
	language = {en},
	urldate = {2023-08-11},
	author = {Easterbrook, Steve},
	year = {2014},
}

@article{mullen_learning_2017,
	title = {Learning by doing, {High} {Performance} {Computing} education in the {MOOC} era},
	volume = {105},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731517300217},
	doi = {10.1016/j.jpdc.2017.01.015},
	abstract = {The High Performance Computing (HPC) community has spent decades developing tools that teach practitioners to harness the power of parallel and distributed computing. To create scalable and flexible educational experiences for practitioners in all phases of a career, we turn to Massively Open Online Courses (MOOCs). We detail the design of a unique self-paced online course that incorporates a focus on parallel solutions, personalization, and hands-on practice to familiarize student–users with their target system. Course material is presented through the lens of common HPC use cases and the strategies for parallelizing them. Using personalized paths, we teach researchers how to recognize the alignment between scientific applications and traditional HPC use cases, so they can focus on learning the parallelization strategies key to their workplace success. At the conclusion of their learning path, students should be capable of achieving performance gains on their HPC system.},
	language = {en},
	urldate = {2023-08-11},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Mullen, Julia and Byun, Chansup and Gadepally, Vijay and Samsi, Siddharth and Reuther, Albert and Kepner, Jeremy},
	month = jul,
	year = {2017},
	pages = {105--115},
}

@article{asaduzzaman_think--parallel_nodate,
	title = {Think-in-{Parallel} for {Developing} {Energy}-{Efficient} {Computer} {Applications}},
	abstract = {Academic research and engineering challenge both require high performance computing (HPC), which can be achieved through parallel programming. The existing curricula of most universities do not properly address the major transition from single-core to multicore systems and sequential to parallel programming. They focus on applying application program interface (API) libraries and open multiprocessing (OpenMP), message passing interface (MPI), and compute unified device architecture (CUDA)/GPU techniques. This approach misses the goal of developing students’ long-term ability to solve real-life problems by ‘thinking in parallel’. In this article, we propose a novel approach to teach parallel computing that will prepare science, technology, engineering, and mathematics (STEM) students for present and future computation challenges. Using multicore/manycore architecture and popular challenging problems from areas like computer science, proposed approach teaches how to analyze and develop efficient solutions for the problems. As a preliminary attempt, we introduce multithreaded parallel programming to our science and engineering students. Based on the feedbacks from information technology (IT) professionals and Student Outcomes Assessment Reports, proposed approach has potential to provide adequate knowledge so that students can fulfill the growing industry demands for HPC. Based on our Steady State Heat Equation experiment, CUDA/GPU parallel programming may achieve up to 241x speedup while simulating heat transfer on a 5000x5000 thin surface.},
	language = {en},
	journal = {International Journal of Computer Applications},
	author = {Asaduzzaman, A and Asmatulu, R and Rahman, M},
}

@inproceedings{qasem_gentle_2019,
	address = {Denver, CO, USA},
	title = {A {Gentle} {Introduction} to {Heterogeneous} {Computing} for {CS1} {Students}},
	isbn = {978-1-72815-975-1},
	url = {https://ieeexplore.ieee.org/document/8943087/},
	doi = {10.1109/EduHPC49559.2019.00007},
	abstract = {Heterogeneous architectures have emerged as a dominant platform, not only in high-performance computing but also in mobile processing, cloud computing and the Internet of Things (IoTs). Because the undergraduate computer science curriculum is over-crowded in its current state, it is difﬁcult to include a new course as a required part of the curriculum without increasing the number of hours to graduation. Integration of heterogeneous computing content requires a module-based approach, such as those undertaken for introducing parallel and distributed computing.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {2019 {IEEE}/{ACM} {Workshop} on {Education} for {High}-{Performance} {Computing} ({EduHPC})},
	publisher = {IEEE},
	author = {Qasem, Apan},
	month = nov,
	year = {2019},
	pages = {10--16},
}

@inproceedings{raj_high_2020,
	address = {Trondheim Norway},
	title = {High {Performance} {Computing} {Education}: {Current} {Challenges} and {Future} {Directions}},
	isbn = {978-1-4503-8293-9},
	shorttitle = {High {Performance} {Computing} {Education}},
	url = {https://dl.acm.org/doi/10.1145/3437800.3439203},
	doi = {10.1145/3437800.3439203},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the {Working} {Group} {Reports} on {Innovation} and {Technology} in {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena},
	month = jun,
	year = {2020},
	pages = {51--74},
}

@inproceedings{whalen_renewable_2023,
	title = {Renewable {Energy} {Projects} {Enhance} {Pedagogy} in {Foundational} {ECE} {Course}},
	language = {en},
	booktitle = {American {Society} of {Engineering} {Education} ({ASEE})},
	author = {Whalen, Devin Connor},
	year = {2023},
}

@article{asaduzzaman_think--parallel_2013,
	title = {Think-in-{Parallel} for {Developing} {Energy}-{Efficient} {Computer} {Applications}},
	abstract = {Academic research and engineering challenge both require high performance computing (HPC), which can be achieved through parallel programming. The existing curricula of most universities do not properly address the major transition from single-core to multicore systems and sequential to parallel programming. They focus on applying application program interface (API) libraries and open multiprocessing (OpenMP), message passing interface (MPI), and compute unified device architecture (CUDA)/GPU techniques. This approach misses the goal of developing students’ long-term ability to solve real-life problems by ‘thinking in parallel’. In this article, we propose a novel approach to teach parallel computing that will prepare science, technology, engineering, and mathematics (STEM) students for present and future computation challenges. Using multicore/manycore architecture and popular challenging problems from areas like computer science, proposed approach teaches how to analyze and develop efficient solutions for the problems. As a preliminary attempt, we introduce multithreaded parallel programming to our science and engineering students. Based on the feedbacks from information technology (IT) professionals and Student Outcomes Assessment Reports, proposed approach has potential to provide adequate knowledge so that students can fulfill the growing industry demands for HPC. Based on our Steady State Heat Equation experiment, CUDA/GPU parallel programming may achieve up to 241x speedup while simulating heat transfer on a 5000x5000 thin surface.},
	language = {en},
	journal = {International Journal of Computer Applications},
	author = {Asaduzzaman, A and Asmatulu, R and Rahman, M},
	year = {2013},
}

@article{mullen_learning_2017-1,
	title = {Learning by doing, {High} {Performance} {Computing} education in the {MOOC} era},
	volume = {105},
	issn = {07437315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731517300217},
	doi = {10.1016/j.jpdc.2017.01.015},
	abstract = {The High Performance Computing (HPC) community has spent decades developing tools that teach practitioners to harness the power of parallel and distributed computing. To create scalable and flexible educational experiences for practitioners in all phases of a career, we turn to Massively Open Online Courses (MOOCs). We detail the design of a unique self-paced online course that incorporates a focus on parallel solutions, personalization, and hands-on practice to familiarize student–users with their target system. Course material is presented through the lens of common HPC use cases and the strategies for parallelizing them. Using personalized paths, we teach researchers how to recognize the alignment between scientific applications and traditional HPC use cases, so they can focus on learning the parallelization strategies key to their workplace success. At the conclusion of their learning path, students should be capable of achieving performance gains on their HPC system.},
	language = {en},
	urldate = {2023-08-11},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Mullen, Julia and Byun, Chansup and Gadepally, Vijay and Samsi, Siddharth and Reuther, Albert and Kepner, Jeremy},
	month = jul,
	year = {2017},
	pages = {105--115},
}

@inproceedings{qasem_gentle_2019-1,
	address = {Denver, CO, USA},
	title = {A {Gentle} {Introduction} to {Heterogeneous} {Computing} for {CS1} {Students}},
	isbn = {978-1-72815-975-1},
	url = {https://ieeexplore.ieee.org/document/8943087/},
	doi = {10.1109/EduHPC49559.2019.00007},
	abstract = {Heterogeneous architectures have emerged as a dominant platform, not only in high-performance computing but also in mobile processing, cloud computing and the Internet of Things (IoTs). Because the undergraduate computer science curriculum is over-crowded in its current state, it is difﬁcult to include a new course as a required part of the curriculum without increasing the number of hours to graduation. Integration of heterogeneous computing content requires a module-based approach, such as those undertaken for introducing parallel and distributed computing.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {2019 {IEEE}/{ACM} {Workshop} on {Education} for {High}-{Performance} {Computing} ({EduHPC})},
	publisher = {IEEE},
	author = {Qasem, Apan},
	month = nov,
	year = {2019},
	pages = {10--16},
}

@inproceedings{raj_high_2020-1,
	address = {Trondheim Norway},
	title = {High {Performance} {Computing} {Education}: {Current} {Challenges} and {Future} {Directions}},
	isbn = {978-1-4503-8293-9},
	shorttitle = {High {Performance} {Computing} {Education}},
	url = {https://dl.acm.org/doi/10.1145/3437800.3439203},
	doi = {10.1145/3437800.3439203},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the {Working} {Group} {Reports} on {Innovation} and {Technology} in {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena},
	month = jun,
	year = {2020},
	pages = {51--74},
}

@article{anistyasari_exploring_2018,
	title = {Exploring {Computational} {Thinking} to {Improve} {Energy}-{Efficient} {Programming} {Skills}},
	volume = {197},
	issn = {2261-236X},
	url = {https://www.matec-conferences.org/10.1051/matecconf/201819715011},
	doi = {10.1051/matecconf/201819715011},
	abstract = {The increasing of ICT utilization brought the increasing of energy consumption which causes global emission. In fact, ICT utilization produces two percent of global emission of CO2. Most studies focus on reducing global emission of CO2 through energy-efficient software and hardware utilizations. However, how to improve energy-efficient programming skills for students has not been investigated well. To address this issue, this work proposes exploring computational thinking which is a teaching learning model adopted computer works. Computational thinking is applied in Fundamental programming subject for eight meetings. Energy-efficient programming skill of students is evaluated before and after the implementation of computational thinking. In addition, the instruments to evaluate energy-efficient programming skill is adapted from previous related work. Finally, statistical analysis reveals that computational thinking improves student’s skill in energy-efficient programming.},
	language = {en},
	urldate = {2023-08-11},
	journal = {MATEC Web of Conferences},
	author = {Anistyasari, Yeni and {Ekohariadi} and Kurniawan, Ari},
	editor = {Abdullah, Ade Gafar and Nandiyanto, Asep Bayu Dani},
	year = {2018},
	pages = {15011},
}

@inproceedings{schwandt_design_2015,
	address = {Tallinn, Estonia},
	title = {Design of lab exercises for teaching energy-efficient digital design},
	isbn = {978-1-4799-1908-6},
	url = {http://ieeexplore.ieee.org/document/7095959/},
	doi = {10.1109/EDUCON.2015.7095959},
	abstract = {To meet the demands for education, engineering curricula intend go beyond teaching technical knowledge. In addition to classic modules, our department considers sustainability subjects such as energy awareness. This paper describes the implementation of a teaching plan for energy-efficient design in hands-on labs and visualizes the possibilities to bring the methods of reducing power consumption closer to students and observe the size of the effects.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {2015 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	publisher = {IEEE},
	author = {Schwandt, Andrea and Winzker, Marco and Abu Shanab, Shatha},
	month = mar,
	year = {2015},
	pages = {112--117},
}

@inproceedings{penzenstadler_teach_2011,
	address = {Honolulu, HI, USA},
	title = {Teach sustainability in software engineering?},
	isbn = {978-1-4577-0349-2},
	url = {http://ieeexplore.ieee.org/document/5876124/},
	doi = {10.1109/CSEET.2011.5876124},
	abstract = {Sustainability is becoming an important topic in IT—as contribution of IT to safeguard our future, and as evolving market segment. IT’s high productivity in combination with short life cycles and, on the other hand, growing resource problems of our planet, lead to a necessity that software engineers take their share of responsibility for sustainability. Therefore, we need to include the concept of sustainability into the university curriculum of computer science.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {2011 24th {IEEE}-{CS} {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	publisher = {IEEE},
	author = {Penzenstadler, Birgit and Fleischmann, Andreas},
	month = may,
	year = {2011},
	pages = {454--458},
}

@inproceedings{torre_presence_2017,
	address = {Buenos Aires, Argentina},
	title = {On the {Presence} of {Green} and {Sustainable} {Software} {Engineering} in {Higher} {Education} {Curricula}},
	isbn = {978-1-5386-2795-2},
	url = {http://ieeexplore.ieee.org/document/7964623/},
	doi = {10.1109/SECM.2017.4},
	abstract = {Nowadays, software is pervasive in our everyday lives. Its sustainability and environmental impact have become major factors to be considered in the development of software systems. Millennials–the newer generation of university students–are particularly keen to learn about and contribute to a more sustainable and green society. The need for training on green and sustainable topics in software engineering has been reﬂected in a number of recent studies. The goal of this paper is to get a ﬁrst understanding of what is the current state of teaching sustainability in the software engineering community, what are the motivations behind the current state of teaching, and what can be done to improve it. To this end, we report the ﬁndings from a targeted survey of 33 academics on the presence of green and sustainable software engineering in higher education. The major ﬁndings from the collected data suggest that sustainability is under-represented in the curricula, while the current focus of teaching is on energy efﬁciency delivered through a fact-based approach. The reasons vary from lack of awareness, teaching material and suitable technologies, to the high effort required to teach sustainability. Finally, we provide recommendations for educators willing to teach sustainability in software engineering that can help to suit millennial students needs.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {2017 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} {Curricula} for {Millennials} ({SECM})},
	publisher = {IEEE},
	author = {Torre, Damiano and Procaccianti, Giuseppe and Fucci, Davide and Lutovac, Sonja and Scanniello, Giuseppe},
	month = may,
	year = {2017},
	pages = {54--60},
}

@inproceedings{maleki_understanding_2017,
	address = {Orlando, FL},
	title = {Understanding the impact of object oriented programming and design patterns on energy efficiency},
	isbn = {978-1-5386-3470-7},
	url = {http://ieeexplore.ieee.org/document/8323605/},
	doi = {10.1109/IGCC.2017.8323605},
	abstract = {With billions of lines of code being deployed and running on cloud servers, PCs, as well as battery-driven mobile phones, embedded systems, and IoT devices, software energy efficiency will play an increasingly important role in green IT. In the past decades, object oriented programming (OOP) has become the de facto standard for commercial software design. However, very little is known about the impact of OOP and design patterns on software energy efficiency. In this paper, we conduct a comprehensive analysis on a series of OOP features and design patterns and find out that their influences on energy efficiency vary greatly. Some of them have negligible impact while others could significantly degrade performance and increase energy consumption. If used appropriately, design patterns can also help improve performance and reduce energy consumption.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {2017 {Eighth} {International} {Green} and {Sustainable} {Computing} {Conference} ({IGSC})},
	publisher = {IEEE},
	author = {Maleki, Sepideh and Fu, Cuijiao and Banotra, Arun and Zong, Ziliang},
	month = oct,
	year = {2017},
	pages = {1--6},
}

@inproceedings{pinto_comprehensive_2016,
	address = {Raleigh, NC, USA},
	title = {A {Comprehensive} {Study} on the {Energy} {Efficiency} of {Java}’s {Thread}-{Safe} {Collections}},
	isbn = {978-1-5090-3806-0},
	url = {http://ieeexplore.ieee.org/document/7816451/},
	doi = {10.1109/ICSME.2016.34},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Pinto, Gustavo and Liu, Kenan and Castor, Fernando and Liu, Yu David},
	month = oct,
	year = {2016},
	pages = {20--31},
}

@article{procaccianti_empirical_2016,
	title = {Empirical evaluation of two best practices for energy-efficient software development},
	volume = {117},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121216000777},
	doi = {10.1016/j.jss.2016.02.035},
	abstract = {Background. Energy eﬃciency is an increasingly important property of software. A large number of empirical studies have been conducted on the topic. However, current state-of-the-Art does not provide empirically-validated guidelines for developing energy-eﬃcient software. Aim. This study aims at assessing the impact, in terms of energy savings, of best practices for achieving software energy eﬃciency, elicited from previous work. By doing so, it identiﬁes which resources are affected by the practices and the possible trade-offs with energy consumption.
Method. We performed an empirical experiment in a controlled environment, where we applied two different Green Software practices to two software applications, namely query optimization in MySQL Server and usage of “sleep” instruction in the Apache web server. We then performed a comparison of the energy consumption at system-level and at resource-level, before and after applying the practice.
Results. Our results show that both practices are effective in improving software energy eﬃciency, reducing consumption up to 25\%. We observe that after applying the practices, resource usage is more energy-proportional i.e., increasing CPU usage increases energy consumption in an almost linear way. We also provide our reﬂections on empirical experimentation in software energy eﬃciency.
Conclusions. Our contribution shows that signiﬁcant improvements in software energy eﬃciency can be gained by applying best practices during design and development. Future work will be devoted to further validate best practices, and to improve their reusability.},
	language = {en},
	urldate = {2023-08-10},
	journal = {Journal of Systems and Software},
	author = {Procaccianti, Giuseppe and Fernández, Héctor and Lago, Patricia},
	month = jul,
	year = {2016},
	pages = {185--198},
}

@misc{kalliamvakou_eirini_research_2022,
	title = {Research: quantifying {GitHub} {Copilot}’s impact on developer productivity and happiness},
	url = {https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/},
	author = {{Kalliamvakou, Eirini}},
	year = {2022},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure speciﬁc qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM.},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@inproceedings{karita_software_2019,
	address = {Salvador Brazil},
	title = {Software industry awareness on green and sustainable software engineering: a state-of-the-practice survey},
	isbn = {978-1-4503-7651-8},
	shorttitle = {Software industry awareness on green and sustainable software engineering},
	url = {https://dl.acm.org/doi/10.1145/3350768.3350770},
	doi = {10.1145/3350768.3350770},
	abstract = {Sustainable computing is a rapidly growing research area spanning several areas of computer science. In the software engineering ﬁeld, the topic has received increasing attention in recent years, with several studies addressing a range of concerns. However, few studies have demonstrated the awareness of software practitioners about the underlying concepts of sustainability in the software development practice. In this eﬀect, in this study, we aim to provide some evidence about the practitioners’ perception about the adoption of sustainability in software development, under four main perspectives: economic, social, environmental and technical. To accomplish such a goal, we carried out a survey study with twentyﬁve software engineers involved in projects in diﬀerent domains. The yielded results indicate an overall lack of knowledge about the topic, in particular regarding the concepts about sustainable software, although it is a common understanding that sustainability should be treated as a quality attribute and should support the interaction between sustainability and the software development life cycle phases. Among the observed perspectives, the respondents indicate that the technical dimension is the most relevant and explored so far. This study contributes to the ﬁeld with initial evidence and can be seen as a ﬁrst step towards establishing a common understanding about how the software industry is receptive to the use of sustainability concepts in software development practices.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the {XXXIII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Karita, Leila and Mourão, Brunna C. and Machado, Ivan},
	month = sep,
	year = {2019},
	pages = {501--510},
}

@inproceedings{manotas_empirical_2016,
	address = {Austin Texas},
	title = {An empirical study of practitioners' perspectives on green software engineering},
	isbn = {978-1-4503-3900-1},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884810},
	doi = {10.1145/2884781.2884810},
	abstract = {The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the ﬁeld. This paper describes the ﬁrst empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report ﬁndings from a quantitative, targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major ﬁndings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Manotas, Irene and Bird, Christian and Zhang, Rui and Shepherd, David and Jaspan, Ciera and Sadowski, Caitlin and Pollock, Lori and Clause, James},
	month = may,
	year = {2016},
	pages = {237--248},
}

@inproceedings{manotas_seeds_2014,
	address = {Hyderabad India},
	title = {{SEEDS}: a software engineer's energy-optimization decision support framework},
	isbn = {978-1-4503-2756-5},
	shorttitle = {{SEEDS}},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568297},
	doi = {10.1145/2568225.2568297},
	abstract = {Reducing the energy usage of software is becoming more important in many environments, in particular, batterypowered mobile devices, embedded systems and data centers. Recent empirical studies indicate that software engineers can support the goal of reducing energy usage by making design and implementation decisions in ways that take into consideration how such decisions impact the energy usage of an application. However, the large number of possible choices and the lack of feedback and information available to software engineers necessitates some form of automated decision-making support.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Manotas, Irene and Pollock, Lori and Clause, James},
	month = may,
	year = {2014},
	pages = {503--514},
}

@inproceedings{te_brinke_design_2013,
	address = {Coimbra Portugal},
	title = {A design method for modular energy-aware software},
	isbn = {978-1-4503-1656-9},
	url = {https://dl.acm.org/doi/10.1145/2480362.2480584},
	doi = {10.1145/2480362.2480584},
	abstract = {Nowadays reducing the overall energy consumption of software is important. A well-known solution is extending the functionality of software with energy optimizers, which monitor the energy consumption of software and adapt it accordingly. To make such extensions manageable and to cope with the complexity of the software, modular design of energyaware software is necessary. Therefore, this paper proposes a dedicated design method for energy-aware software.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 28th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Te Brinke, Steven and Malakuti, Somayeh and Bockisch, Christoph and Bergmans, Lodewijk and Akşit, Mehmet},
	month = mar,
	year = {2013},
	pages = {1180--1182},
}

@inproceedings{te_brinke_tool-supported_2014,
	address = {Gyeongju Republic of Korea},
	title = {A tool-supported approach for modular design of energy-aware software},
	isbn = {978-1-4503-2469-4},
	url = {https://dl.acm.org/doi/10.1145/2554850.2554964},
	doi = {10.1145/2554850.2554964},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 29th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Te Brinke, Steven and Malakuti, Somayeh and Bockisch, Christoph and Bergmans, Lodewijk and Akşit, Mehmet and Katz, Shmuel},
	month = mar,
	year = {2014},
	pages = {1206--1212},
}

@inproceedings{vasic_making_2009,
	address = {Barcelona Spain},
	title = {Making cluster applications energy-aware},
	isbn = {978-1-60558-585-7},
	url = {https://dl.acm.org/doi/10.1145/1555271.1555281},
	doi = {10.1145/1555271.1555281},
	abstract = {Power consumption has become a critical issue in large scale clusters. Existing solutions for addressing the servers’ energy consumption suggest “shrinking” the set of active machines, at least until the more power-proportional hardware devices become available. This paper demonstrates that leveraging the sleeping state, however, may lead to unacceptably poor performance and low data availability if the distributed services are not aware of the power management’s actions. Therefore, we present an architecture for cluster services in which the deployed services overcome this problem by actively participating in any action taken by the power management. We propose, implement, and evaluate modiﬁcations for the Hadoop Distributed File System and the MapReduce clone that make them capable of operating efﬁciently under limited power budgets.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 1st workshop on {Automated} control for datacenters and clouds},
	publisher = {ACM},
	author = {Vasić, Nedeljko and Barisits, Martin and Salzgeber, Vincent and Kostic, Dejan},
	month = jun,
	year = {2009},
	pages = {37--42},
}

@article{milano_large_2023,
	title = {Large language models challenge the future of higher education},
	volume = {5},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00644-2},
	doi = {10.1038/s42256-023-00644-2},
	language = {en},
	number = {4},
	urldate = {2023-08-10},
	journal = {Nature Machine Intelligence},
	author = {Milano, Silvia and McGrane, Joshua A. and Leonelli, Sabina},
	month = mar,
	year = {2023},
	pages = {333--334},
}

@inproceedings{sehgal2010evaluating,
	title = {Evaluating performance and energy in file system server workloads.},
	volume = {10},
	booktitle = {{FAST}},
	author = {Sehgal, Priya and Tarasov, Vasily and Zadok, Erez},
	year = {2010},
	pages = {253--266},
}

@article{dayarathna_data_2016,
	title = {Data {Center} {Energy} {Consumption} {Modeling}: {A} {Survey}},
	volume = {18},
	issn = {1553-877X},
	shorttitle = {Data {Center} {Energy} {Consumption} {Modeling}},
	url = {http://ieeexplore.ieee.org/document/7279063/},
	doi = {10.1109/COMST.2015.2481183},
	abstract = {Data centers are critical, energy-hungry infrastructures that run large-scale Internet-based services. Energy consumption models are pivotal in designing and optimizing energy-efﬁcient operations to curb excessive energy consumption in data centers. In this paper, we survey the state-of-the-art techniques used for energy consumption modeling and prediction for data centers and their components. We conduct an in-depth study of the existing literature on data center power modeling, covering more than 200 models. We organize these models in a hierarchical structure with two main branches focusing on hardware-centric and software-centric power models. Under hardware-centric approaches we start from the digital circuit level and move on to describe higher-level energy consumption models at the hardware component level, server level, data center level, and ﬁnally systems of systems level. Under the software-centric approaches we investigate power models developed for operating systems, virtual machines and software applications. This systematic approach allows us to identify multiple issues prevalent in power modeling of different levels of data center systems, including: i) few modeling efforts targeted at power consumption of the entire data center ii) many state-of-the-art power models are based on a few CPU or server metrics, and iii) the effectiveness and accuracy of these power models remain open questions. Based on these observations, we conclude the survey by describing key challenges for future research on constructing effective and accurate data center power models.},
	language = {en},
	number = {1},
	urldate = {2023-08-10},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Dayarathna, Miyuru and Wen, Yonggang and Fan, Rui},
	year = {2016},
	pages = {732--794},
}

@article{davis2023conversations,
	title = {Conversations with {ChatGPT} about {C} programming: {An} ongoing study},
	journal = {Figshare},
	author = {Davis, James C and Lu, Yung-Hsiang and Thiruvathukal, George K},
	year = {2023},
}

@inproceedings{pearce_examining_2023,
	address = {San Francisco, CA, USA},
	title = {Examining {Zero}-{Shot} {Vulnerability} {Repair} with {Large} {Language} {Models}},
	isbn = {978-1-66549-336-9},
	url = {https://ieeexplore.ieee.org/document/10179324/},
	doi = {10.1109/SP46215.2023.10179324},
	abstract = {Human developers can produce code with cybersecurity bugs. Can emerging ‘smart’ code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI’s Codex and AI21’s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information—both semantically and syntactically—with natural languages. We perform a large scale study of five commercially available, blackbox, “off-the-shelf” LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, handcrafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100\% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model’s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
	month = may,
	year = {2023},
	pages = {2339--2356},
}

@techreport{shehabi_united_2016,
	title = {United {States} {Data} {Center} {Energy} {Usage} {Report}},
	url = {http://www.osti.gov/servlets/purl/1372902/},
	language = {en},
	number = {LBNL--1005775, 1372902},
	urldate = {2023-08-10},
	author = {Shehabi, Arman and Smith, Sarah and Sartor, Dale and Brown, Richard and Herrlin, Magnus and Koomey, Jonathan and Masanet, Eric and Horner, Nathaniel and Azevedo, Inês and Lintner, William},
	month = jun,
	year = {2016},
	doi = {10.2172/1372902},
	pages = {LBNL--1005775, 1372902},
}

@misc{bayer_cysecbert_2022,
	title = {{CySecBERT}: {A} {Domain}-{Adapted} {Language} {Model} for the {Cybersecurity} {Domain}},
	shorttitle = {{CySecBERT}},
	url = {http://arxiv.org/abs/2212.02974},
	abstract = {The field of cybersecurity is evolving fast. Experts need to be informed about past, current and - in the best case - upcoming threats, because attacks are becoming more advanced, targets bigger and systems more complex. As this cannot be addressed manually, cybersecurity experts need to rely on machine learning techniques. In the texutual domain, pre-trained language models like BERT have shown to be helpful, by providing a good baseline for further fine-tuning. However, due to the domain-knowledge and many technical terms in cybersecurity general language models might miss the gist of textual information, hence doing more harm than good. For this reason, we create a high-quality dataset and present a language model specifically tailored to the cybersecurity domain, which can serve as a basic building block for cybersecurity systems that deal with natural language. The model is compared with other models based on 15 different domain-dependent extrinsic and intrinsic tasks as well as general tasks from the SuperGLUE benchmark. On the one hand, the results of the intrinsic tasks show that our model improves the internal representation space of words compared to the other models. On the other hand, the extrinsic, domain-dependent tasks, consisting of sequence tagging and classification, show that the model is best in specific application scenarios, in contrast to the others. Furthermore, we show that our approach against catastrophic forgetting works, as the model is able to retrieve the previously trained domain-independent knowledge. The used dataset and trained model are made publicly available},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Bayer, Markus and Kuehn, Philipp and Shanehsaz, Ramin and Reuter, Christian},
	month = dec,
	year = {2022},
	note = {arXiv:2212.02974 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{ferrag_revolutionizing_2023,
	title = {Revolutionizing {Cyber} {Threat} {Detection} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2306.14263},
	abstract = {Natural Language Processing (NLP) domain is experiencing a revolution due to the capabilities of Pre-trained Large Language Models ( LLMs), fueled by ground-breaking Transformers architecture, resulting into unprecedented advancements. Their exceptional aptitude for assessing probability distributions of text sequences is the primary catalyst for outstanding improvement of both the precision and efficiency of NLP models. This paper introduces for the first time SecurityLLM, a pre-trained language model designed for cybersecurity threats detection. The SecurityLLM model is articulated around two key generative elements: SecurityBERT and FalconLLM. SecurityBERT operates as a cyber threat detection mechanism, while FalconLLM is an incident response and recovery system. To the best of our knowledge, SecurityBERT represents the inaugural application of BERT in cyber threat detection. Despite the unique nature of the input data and features, such as the reduced significance of syntactic structures in content classification, the suitability of BERT for this duty demonstrates unexpected potential, thanks to our pioneering study. We reveal that a simple classification model, created from scratch, and consolidated with LLMs, exceeds the performance of established traditional Machine Learning (ML) and Deep Learning (DL) methods in cyber threat detection, like Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN). The experimental analysis, conducted using a collected cybersecurity dataset, proves that our SecurityLLM model can identify fourteen (14) different types of attacks with an overall accuracy of 98\% Index Terms—Security, Attacks Detection, Generative AI, FalconLLM, BERT, Large Language Models.},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Ferrag, Mohamed Amine and Ndhlovu, Mthandazo and Tihanyi, Norbert and Cordeiro, Lucas C. and Debbah, Merouane and Lestable, Thierry},
	month = jun,
	year = {2023},
	note = {arXiv:2306.14263 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{patterson_carbon_2021,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models—T5, Meena, GShard, Switch Transformer, and GPT-3—and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): ● Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. ● Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. ● Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	language = {en},
	journal = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	year = {2021},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{mcdonald_great_2022,
	title = {Great {Power}, {Great} {Responsibility}: {Recommendations} for {Reducing} {Energy} for {Training} {Language} {Models}},
	shorttitle = {Great {Power}, {Great} {Responsibility}},
	url = {http://arxiv.org/abs/2205.09646},
	doi = {10.18653/v1/2022.findings-naacl.151},
	abstract = {The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. We characterize the impact of these settings on metrics such as computational performance and energy consumption through experiments conducted on a high performance computing system as well as popular cloud computing platforms. These techniques can lead to significant reduction in energy consumption when training language models or their use for inference. For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15{\textbackslash}\% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	author = {McDonald, Joseph and Li, Baolin and Frey, Nathan and Tiwari, Devesh and Gadepally, Vijay and Samsi, Siddharth},
	year = {2022},
	note = {arXiv:2205.09646 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Performance},
	pages = {1962--1970},
}

@article{anagnostopoulou_barely_2012,
	title = {Barely alive memory servers: {Keeping} data active in a low-power state},
	volume = {8},
	issn = {1550-4832, 1550-4840},
	shorttitle = {Barely alive memory servers},
	url = {https://dl.acm.org/doi/10.1145/2367736.2367742},
	doi = {10.1145/2367736.2367742},
	abstract = {Current resource provisioning schemes in Internet services leave servers less than 50\% utilized almost all the time. At this level of utilization, the servers' energy efficiency is substantially lower than at peak utilization. A solution to this problem could be dynamically consolidating workloads into fewer servers and turning others off. However, services typically resist doing so, because of high response times during reactivation in handling traffic spikes. Moreover, services often want the memory and/or storage of all servers to be readily available at all times.
            
              In this article, we propose a family of
              barely alive
              active low-power server states that facilitates both fast reactivation and access to memory while in a low-power state. We compare these states to previously proposed active and idle states. In particular, we investigate the impact of load bursts in each energy-saving scheme. We also evaluate the additional benefits of memory access under low-power states with a study of a search service using a cooperative main-memory cache. Finally, we propose a system that combines a barely-alive state with the off state. We find that the barely alive states can reduce service energy consumption by up to 38\%, compared to an energy-oblivious system. We also find that these energy savings are consistent across a large parameter space.},
	language = {en},
	number = {4},
	urldate = {2023-08-10},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	author = {Anagnostopoulou, Vlasia and Biswas, Susmit and Saadeldeen, Heba and Savage, Alan and Bianchini, Ricardo and Yang, Tao and Franklin, Diana and Chong, Frederic T.},
	month = oct,
	year = {2012},
	pages = {1--20},
}

@article{heath_load_2001,
	title = {Load balancing and unbalancing for power and performance in cluster-based systems},
	copyright = {Open},
	url = {https://scholarship.libraries.rutgers.edu/esploro/outputs/technicalDocumentation/991031550246604646},
	doi = {10.7282/T3-AGFW-YT73},
	abstract = {In this paper we address power conservation for clusters of workstations or PCs, such as those that support a large number of research/teaching organizations and most Internet companies. Our approach is to develop systems that dynamically turn cluster nodes on –to be able to handle the load imposed on the system efﬁciently – and off – to save power under lighter load. The key componentof our systems is an algorithm that makes load balancing and unbalancing decisions by considering both the total load imposed on the cluster and the power and performance implications of turning nodes off. The algorithm is implemented in three different ways: (1) at the application level for a cluster-based, locality-conscious network server; (2) at the operating system level for an operating system for clustered cycle servers; and (3) by application/operating system negotiation, again using the cluster-oriented operating system and a negotiation API. Our experimental results are very favorable, showing that our systems conserve both power and energy in comparison to traditional systems, which do not adapt their conﬁgurations.},
	language = {en},
	urldate = {2023-08-10},
	author = {Heath, Taliver and Carrera, Enrique V. and Pinheiro, Eduardo and Bianchini, Ricardo},
	year = {2001},
	note = {Medium: application/pdf
Publisher: Rutgers University},
}

@incollection{imielinski_scheduling_1996,
	address = {Boston, MA},
	title = {Scheduling for {Reduced} {CPU} {Energy}},
	volume = {353},
	isbn = {978-0-7923-9697-0},
	url = {http://link.springer.com/10.1007/978-0-585-29603-6_17},
	abstract = {The energy usage of computer systems is becoming more important, especially for battery operated systems. Displays, disks, and cpus, in that order, use the most energy. Reducing the energy used by displays and disks has been studied elsewhere; this paper considers a new method for reducing the energy used by the cpu. We introduce a new metric for cpu energy performance, millions-of-instructions-per-joule (MIPJ). We examine a class of methods to reduce MIPJ that are characterized by dynamic control of system clock speed by the operating system scheduler. Reducing clock speed alone does not reduce MIPJ, since to do the same work the system must run longer. However, a number of methods are available for reducing energy with reduced clock-speed, such as reducing the voltage [Chandrakasan et al 1992][Horowitz 1993] or using reversible [Younis and Knight 1993] or adiabatic logic [Athas et al 1994]. What are the right scheduling algorithms for taking advantage of reduced clock-speed, especially in the presence of applications demanding ever more instructions-per-second? We consider several methods for varying the clock speed dynamically under control of the operating system, and examine the performance of these methods against workstation traces. The primary result is that by adjusting the clock speed at a ﬁne grain, substantial CPU energy can be saved with a limited impact on performance.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Mobile {Computing}},
	publisher = {Springer US},
	author = {Weiser, Mark and Welch, Brent and Demers, Alan and Shenker, Scott},
	editor = {Imielinski, Tomasz and Korth, Henry F.},
	year = {1996},
	doi = {10.1007/978-0-585-29603-6_17},
	note = {Series Title: The Kluwer International Series in Engineering and Computer Science},
	pages = {449--471},
}

@article{sehgal_optimizing_2010,
	title = {Optimizing energy and performance for server-class file system workloads},
	volume = {6},
	issn = {1553-3077, 1553-3093},
	url = {https://dl.acm.org/doi/10.1145/1837915.1837918},
	doi = {10.1145/1837915.1837918},
	abstract = {Recently, power has emerged as a critical factor in designing components of storage systems, especially for power-hungry data centers. While there is some research into power-aware storage stack components, there are no systematic studies evaluating each component's impact separately. Various factors like workloads, hardware configurations, and software configurations impact the performance and energy efficiency of the system. This article evaluates the file system's impact on energy consumption and performance. We studied several popular Linux file systems, with various mount and format options, using the FileBench workload generator to emulate four server workloads: Web, database, mail, and fileserver, on two different hardware configurations. The file system design, implementation, and available features have a significant effect on CPU/disk utilization, and hence on performance and power. We discovered that default file system options are often suboptimal, and even poor. In this article we show that a careful matching of expected workloads and hardware configuration to a single software configuration—the file system—can improve power-performance efficiency by a factor ranging from 1.05 to 9.4 times.},
	language = {en},
	number = {3},
	urldate = {2023-08-10},
	journal = {ACM Transactions on Storage},
	author = {Sehgal, Priya and Tarasov, Vasily and Zadok, Erez},
	month = sep,
	year = {2010},
	pages = {1--31},
}

@article{daradkeh_cloud_2022,
	title = {Cloud {Workload} and {Data} {Center} {Analytical} {Modeling} and {Optimization} {Using} {Deep} {Machine} {Learning}},
	volume = {2},
	issn = {2673-8732},
	url = {https://www.mdpi.com/2673-8732/2/4/37},
	doi = {10.3390/network2040037},
	abstract = {Predicting workload demands can help to achieve elastic scaling by optimizing data center conﬁguration, such that increasing/decreasing data center resources provides an accurate and efﬁcient conﬁguration. Predicting workload and optimizing data center resource conﬁguration are two challenging tasks. In this work, we investigate workload and data center modeling to help in predicting workload and data center operation that is used as an experimental environment to evaluate optimized elastic scaling for real data center traces. Three methods of machine learning are used and compared with an analytical approach to model the workload and data center actions. Our approach is to use an analytical model as a predictor to evaluate and test the optimization solution set and ﬁnd the best conﬁguration and scaling actions before applying it to the real data center. The results show that machine learning with an analytical approach can help to ﬁnd the best prediction values of workload demands and evaluate the scaling and resource capacity required to be provisioned. Machine learning is used to ﬁnd the optimal conﬁguration and to solve the elasticity scaling boundary values. Machine learning helps in optimization by reducing elastic scaling violation and conﬁguration time and by categorizing resource conﬁguration with respect to scaling capacity values. The results show that the conﬁguration cost and time are minimized by the best provisioning actions.},
	language = {en},
	number = {4},
	urldate = {2023-08-10},
	journal = {Network},
	author = {Daradkeh, Tariq and Agarwal, Anjali},
	month = nov,
	year = {2022},
	pages = {643--669},
}

@article{gao2014machine,
	title = {Machine learning applications for data center optimization},
	author = {Gao, Jim},
	year = {2014},
}

@article{wu_improving_2015,
	title = {Improving {Data} {Center} {Energy} {Efficiency} {Using} a {Cyber}-physical {Systems} {Approach}: {Integration} of {Building} {Information} {Modeling} and {Wireless} {Sensor} {Networks}},
	volume = {118},
	issn = {18777058},
	shorttitle = {Improving {Data} {Center} {Energy} {Efficiency} {Using} a {Cyber}-physical {Systems} {Approach}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877705815021360},
	doi = {10.1016/j.proeng.2015.08.481},
	abstract = {The increase in data center operating costs is driving innovation to improve their energy efficiency . Previous research has investigated computational and physical control intervention strategies to alleviate the competition between energy consumption and thermal performance in data center operation. This study contributes to the body of knowledge by proposing a cyber-physical systems (CPS) approach to innovatively integrate building information modeling (BIM ) and wireless sensor networks (WSN). In the proposed framework, wireless sensors are deployed strategically to monitor thermal performance parameters in response to runtime server load distribution. Sensor data are collected and contextualized in reference to the building information model that captures the geometric and functional characteristics of the data center, which will be used as inputs of continuous simulations aiming to predict real-time thermal performance of server working environment. Comparing the simulation results against historical performance data via machine learning and data mining, facility managers can quickly pinpoint thermal hot zones and actuate intervention procedures to improve energy efficiency. This BIM -WSN integration also facilitates smarter power management by capping runtime power demand within peak power capacity of data centers and alerting power outage emergencies. This paper lays out the BIM-WSN integration framework, explains the working mechanism, and discusses the feasibility of implementation in future work.},
	language = {en},
	urldate = {2023-08-10},
	journal = {Procedia Engineering},
	author = {Wu, Wei and Li, Wenjia and Law, Deify and Na, Woonki},
	year = {2015},
	pages = {1266--1273},
}

@article{chi_cooperatively_2021,
	title = {Cooperatively {Improving} {Data} {Center} {Energy} {Efficiency} {Based} on {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	volume = {14},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/14/8/2071},
	doi = {10.3390/en14082071},
	abstract = {The problem of high power consumption in data centers is becoming more and more prominent. In order to improve the energy efﬁciency of data centers, cooperatively optimizing the energy of IT systems and cooling systems has become an effective way. In this paper, a model-free deep reinforcement learning (DRL)-based joint optimization method MAD3C is developed to overcome the high-dimensional state and action space problems of the data center energy optimization. A hybrid AC-DDPG cooperative multi-agent framework is devised for the improvement of the cooperation between the IT and cooling systems for further energy efﬁciency improvement. In the framework, a scheduling baseline comparison method is presented to enhance the stability of the framework. Meanwhile, an adaptive score is designed for the architecture in consideration of multi-dimensional resources and resource utilization improvement. Experiments show that our proposed approach can effectively reduce energy for data centers through the cooperative optimization while guaranteeing training stability and improving resource utilization.},
	language = {en},
	number = {8},
	urldate = {2023-08-10},
	journal = {Energies},
	author = {Chi, Ce and Ji, Kaixuan and Song, Penglei and Marahatta, Avinab and Zhang, Shikui and Zhang, Fa and Qiu, Dehui and Liu, Zhiyong},
	month = apr,
	year = {2021},
	pages = {2071},
}

@inproceedings{townend_invited_2019,
	address = {San Francisco East Bay, CA, USA},
	title = {Invited {Paper}: {Improving} {Data} {Center} {Efficiency} {Through} {Holistic} {Scheduling} {In} {Kubernetes}},
	isbn = {978-1-72811-442-2},
	shorttitle = {Invited {Paper}},
	url = {https://ieeexplore.ieee.org/document/8705815/},
	doi = {10.1109/SOSE.2019.00030},
	abstract = {Data centers are the infrastructure that underpins modern distributed service-oriented systems. They are complex systems-of-systems, with many interacting elements, that consume vast amounts of power. Demand for such facilities is growing rapidly, leading to significant global environmental impact. The data center industry has conducted much research into efficiency improvements, but this has mostly been at the physical infrastructure level. Research into software-based solutions for improving efficiency is greatly needed. However, most current research does not take a holistic view of the data center that considers virtual and physical infrastructures as well as business process. This is crucial if a solution is to be applied in a realistic setting. This paper describes the complex, system-of-systems nature of data centers, and discusses the service models used in the industry. We describe a holistic scheduling system that replaces the default scheduler in the Kubernetes container system, taking into account both software and hardware models. We discuss the initial results of deploying this scheme in a real data center, where power consumption reductions of 10-20\% were observed. We show that by introducing hardware modelling into a software-based solution, an intelligent scheduler can make significant improvements in data center efficiency. We conclude by looking at some of the future work that needs to be performed in this area.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {2019 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	publisher = {IEEE},
	author = {Townend, Paul and Clement, Stephen and Burdett, Dan and Yang, Renyu and Shaw, Joe and Slater, Brad and Xu, Jie},
	month = apr,
	year = {2019},
	pages = {156--15610},
}

@inproceedings{leinonen_using_2023,
	address = {Toronto ON Canada},
	title = {Using {Large} {Language} {Models} to {Enhance} {Programming} {Error} {Messages}},
	isbn = {978-1-4503-9431-4},
	url = {https://dl.acm.org/doi/10.1145/3545945.3569770},
	doi = {10.1145/3545945.3569770},
	abstract = {A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix them. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings of the 54th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education} {V}. 1},
	publisher = {ACM},
	author = {Leinonen, Juho and Hellas, Arto and Sarsa, Sami and Reeves, Brent and Denny, Paul and Prather, James and Becker, Brett A.},
	month = mar,
	year = {2023},
	pages = {563--569},
}

@inproceedings{ahmed_recommending_2023,
	address = {Melbourne, Australia},
	title = {Recommending {Root}-{Cause} and {Mitigation} {Steps} for {Cloud} {Incidents} using {Large} {Language} {Models}},
	isbn = {978-1-66545-701-9},
	url = {https://ieeexplore.ieee.org/document/10172904/},
	doi = {10.1109/ICSE48619.2023.00149},
	abstract = {Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require signiﬁcant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artiﬁcial intelligence has resulted in state-ofthe-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the ﬁrst large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, ﬁne-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efﬁcacy and future potential of using artiﬁcial intelligence for resolving cloud incidents.},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Ahmed, Toufique and Ghosh, Supriyo and Bansal, Chetan and Zimmermann, Thomas and Zhang, Xuchao and Rajmohan, Saravan},
	month = may,
	year = {2023},
	pages = {1737--1749},
}

@article{ozkaya_application_2023,
	title = {Application of {Large} {Language} {Models} to {Software} {Engineering} {Tasks}: {Opportunities}, {Risks}, and {Implications}},
	volume = {40},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Application of {Large} {Language} {Models} to {Software} {Engineering} {Tasks}},
	url = {https://ieeexplore.ieee.org/document/10109345/},
	doi = {10.1109/MS.2023.3248401},
	language = {en},
	number = {3},
	urldate = {2023-08-10},
	journal = {IEEE Software},
	author = {Ozkaya, Ipek},
	month = may,
	year = {2023},
	pages = {4--8},
}

@misc{white_chatgpt_2023,
	title = {{ChatGPT} {Prompt} {Patterns} for {Improving} {Code} {Quality}, {Refactoring}, {Requirements} {Elicitation}, and {Software} {Design}},
	url = {http://arxiv.org/abs/2303.07839},
	abstract = {This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating an API speciﬁcation from a requirements list. This paper provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classiﬁes patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {White, Jules and Hays, Sam and Fu, Quchen and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.07839 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{white_chatgpt_2023-1,
	title = {{ChatGPT} {Prompt} {Patterns} for {Improving} {Code} {Quality}, {Refactoring}, {Requirements} {Elicitation}, and {Software} {Design}},
	url = {http://arxiv.org/abs/2303.07839},
	abstract = {This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating an API speciﬁcation from a requirements list. This paper provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classiﬁes patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {White, Jules and Hays, Sam and Fu, Quchen and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.07839 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{white_chatgpt_2023-2,
	title = {{ChatGPT} {Prompt} {Patterns} for {Improving} {Code} {Quality}, {Refactoring}, {Requirements} {Elicitation}, and {Software} {Design}},
	url = {http://arxiv.org/abs/2303.07839},
	abstract = {This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating an API speciﬁcation from a requirements list. This paper provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classiﬁes patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.},
	language = {en},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {White, Jules and Hays, Sam and Fu, Quchen and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.07839 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@incollection{chen_understanding_2020,
	address = {Cham},
	title = {Understanding the {Security} {Risks} of {Docker} {Hub}},
	volume = {12308},
	isbn = {9783030589509 9783030589516},
	url = {http://link.springer.com/10.1007/978-3-030-58951-6_13},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Computer {Security} – {ESORICS} 2020},
	publisher = {Springer International Publishing},
	author = {Liu, Peiyu and Ji, Shouling and Fu, Lirong and Lu, Kangjie and Zhang, Xuhong and Lee, Wei-Han and Lu, Tao and Chen, Wenzhi and Beyah, Raheem},
	editor = {Chen, Liqun and Li, Ninghui and Liang, Kaitai and Schneider, Steve},
	year = {2020},
	doi = {10.1007/978-3-030-58951-6_13},
	pages = {257--276},
}

@inproceedings{thomas_investigating_2016,
	title = {Investigating {Commercial} \{{Pay}-{Per}-{Install}\} and the {Distribution} of {Unwanted} {Software}},
	isbn = {978-1-931971-32-4},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/thomas},
	language = {en},
	urldate = {2023-08-10},
	author = {Thomas, Kurt and Crespo, Juan A. Elices and Rasti, Ryan and Picod, Jean-Michel and Phillips, Cait and Decoste, Marc-André and Sharp, Chris and Tirelo, Fabio and Tofigh, Ali and Courteau, Marc-Antoine and Ballard, Lucas and Shield, Robert and Jagpal, Nav and Rajab, Moheeb Abu and Mavrommatis, Panayiotis and Provos, Niels and Bursztein, Elie and McCoy, Damon},
	year = {2016},
	pages = {721--739},
}

@inproceedings{kwon_catching_2017,
	address = {San Diego, CA},
	title = {Catching {Worms}, {Trojan} {Horses} and {PUPs}: {Unsupervised} {Detection} of {Silent} {Delivery} {Campaigns}},
	isbn = {978-1-891562-46-4},
	shorttitle = {Catching {Worms}, {Trojan} {Horses} and {PUPs}},
	url = {https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/catching-worms-trojan-horses-and-pups-unsupervised-detection-silent-delivery-campaigns/},
	doi = {10.14722/ndss.2017.23220},
	language = {en},
	urldate = {2023-08-10},
	booktitle = {Proceedings 2017 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Kwon, Bum Jun and Srinivas, Virinchi and Deshpande, Amol and Dumitras, Tudor},
	year = {2017},
}

@inproceedings{kwon_catching_2017-1,
	title = {Catching {Worms}, {Trojan} {Horses} and {PUPs}: {Unsupervised} {Detection} of {Silent} {Delivery} {Campaigns}},
	shorttitle = {Catching {Worms}, {Trojan} {Horses} and {PUPs}},
	url = {http://arxiv.org/abs/1611.02787},
	doi = {10.14722/ndss.2017.23220},
	abstract = {The growing commoditization of the underground economy has given rise to malware delivery networks, which charge fees for quickly delivering malware or unwanted software to a large number of hosts. To provide this service, a key method is the orchestration of silent delivery campaigns, which involve a group of downloaders that receive remote commands and that deliver their payloads without any user interaction. These campaigns have not been characterized systematically, unlike other aspects of malware delivery networks. Moreover, silent delivery campaigns can evade detection by relying on inconspicuous downloaders on the client side and on disposable domain names on the server side. We describe Beewolf, a system for detecting silent delivery campaigns from Internet-wide records of download events. The key observation behind our system is that the downloaders involved in these campaigns frequently retrieve payloads in lockstep. Beewolf identifies such locksteps in an unsupervised and deterministic manner. By exploiting novel techniques and empirical observations, Beewolf can operate on streaming data. We utilize Beewolf to study silent delivery campaigns at scale, on a data set of 33.3 million download events. This investigation yields novel findings, e.g. malware distributed through compromised software update channels, a substantial overlap between the delivery ecosystems for malware and unwanted software, and several types of business relationships within these ecosystems. Beewolf achieves over 92\% true positives and fewer than 5\% false positives. Moreover, Beewolf can detect suspicious downloaders a median of 165 days ahead of existing anti-virus products and payload-hosting domains a median of 196 days ahead of existing blacklists.},
	urldate = {2023-08-10},
	booktitle = {Proceedings 2017 {Network} and {Distributed} {System} {Security} {Symposium}},
	author = {Kwon, Bum Jun and Srinivas, Virinchi and Deshpande, Amol and Dumitraş, Tudor},
	year = {2017},
	note = {arXiv:1611.02787 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{kotzias_certified_2015,
	address = {New York, NY, USA},
	series = {{CCS} '15},
	title = {Certified {PUP}: {Abuse} in {Authenticode} {Code} {Signing}},
	isbn = {978-1-4503-3832-5},
	shorttitle = {Certified {PUP}},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813665},
	doi = {10.1145/2810103.2813665},
	abstract = {Code signing is a solution to verify the integrity of software and its publisher's identity, but it can be abused by malware and potentially unwanted programs (PUP) to look benign. This work performs a systematic analysis of Windows Authenticode code signing abuse, evaluating the effectiveness of existing defenses by certification authorities. We identify a problematic scenario in Authenticode where timestamped signed malware successfully validates even after the revocation of their code signing certificate. We propose hard revocations as a solution. We build an infrastructure that automatically analyzes potentially malicious executables, selects those signed, clusters them into operations, determines if they are PUP or malware, and produces a certificate blacklist. We use our infrastructure to evaluate 356 K samples from 2006-2015. Our analysis shows that most signed samples are PUP (88\%-95\%) and that malware is not commonly signed (5\%-12\%). We observe PUP rapidly increasing over time in our corpus. We measure the effectiveness of CA defenses such as identity checks and revocation, finding that 99.8\% of signed PUP and 37\% of signed malware use CA-issued certificates and only 17\% of malware certificates and 15\% of PUP certificates have been revoked. We observe most revocations lack an accurate revocation reason. We analyze the code signing infrastructure of the 10 largest PUP operations exposing that they heavily use file and certificate polymorphism and that 7 of them have multiple certificates revoked. Our infrastructure also generates a certificate blacklist 9x larger than current ones.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Kotzias, Platon and Matic, Srdjan and Rivera, Richard and Caballero, Juan},
	month = oct,
	year = {2015},
	keywords = {PUP, code signing, malware, windows authenticode},
	pages = {465--478},
}

@inproceedings{kotzias_measuring_2016,
	title = {Measuring \{{PUP}\} {Prevalence} and \{{PUP}\} {Distribution} through \{{Pay}-{Per}-{Install}\} {Services}},
	isbn = {978-1-931971-32-4},
	url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/kotzias},
	language = {en},
	urldate = {2023-08-09},
	author = {Kotzias, Platon and Bilge, Leyla and Caballero, Juan},
	year = {2016},
	pages = {739--756},
}

@inproceedings{kim_certified_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {Certified {Malware}: {Measuring} {Breaches} of {Trust} in the {Windows} {Code}-{Signing} {PKI}},
	isbn = {978-1-4503-4946-8},
	shorttitle = {Certified {Malware}},
	url = {https://dl.acm.org/doi/10.1145/3133956.3133958},
	doi = {10.1145/3133956.3133958},
	abstract = {Digitally signed malware can bypass system protection mechanisms that install or launch only programs with valid signatures. It can also evade anti-virus programs, which often forego scanning signed binaries. Known from advanced threats such as Stuxnet and Flame, this type of abuse has not been measured systematically in the broader malware landscape. In particular, the methods, effectiveness window, and security implications of code-signing PKI abuse are not well understood. We propose a threat model that highlights three types of weaknesses in the code-signing PKI. We overcome challenges specific to code-signing measurements by introducing techniques for prioritizing the collection of code signing certificates that are likely abusive. We also introduce an algorithm for distinguishing among different types of threats. These techniques allow us to study threats that breach the trust encoded in the Windows code signing PKI. The threats include stealing the private keys associated with benign certificates and using them to sign malware or by impersonating legitimate companies that do not develop software and, hence, do not own code-signing certificates. Finally, we discuss the actionable implications of our findings and propose concrete steps for improving the security of the code-signing ecosystem.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Doowon and Kwon, Bum Jun and Dumitraş, Tudor},
	month = oct,
	year = {2017},
	keywords = {code signing, compromised certificates, malware, pki, windows authenticode},
	pages = {1435--1448},
}

@inproceedings{alrawi_chains_2016,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '16 {Companion}},
	title = {Chains of {Distrust}: {Towards} {Understanding} {Certificates} {Used} for {Signing} {Malicious} {Applications}},
	isbn = {978-1-4503-4144-8},
	shorttitle = {Chains of {Distrust}},
	url = {https://dl.acm.org/doi/10.1145/2872518.2888610},
	doi = {10.1145/2872518.2888610},
	abstract = {Digital certificates are key component of trust used by many operating systems. Modern operating systems implement a form of digital signature verification for various applications, including kernel driver installation, software execution, etc. Digital signatures rely on digital certificates that authenticate the signature, which then verify the validity of a given signature for a signed binary. Malware attempts to subvert the chain of trust through several techniques to achieve execution, evasion, and persistence. In this paper, we examine a large corpus of malware (\$3.3\$ million samples) to extract digital signatures and their corresponding certificates. We examine several characteristics of the digital certificates to study features in the process of malware authorship that will potentially be used for characterizing and classifying malware. We look at many features including the certificate's chain length, the issue and expiration year, the validity duration of a certificate, the issuing country, validity, top issuing certificate authorities (CAs), and others, highlighting potentially discriminatory features.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 25th {International} {Conference} {Companion} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Alrawi, Omar and Mohaisen, Aziz},
	month = apr,
	year = {2016},
	keywords = {authenticode, certificates, malware},
	pages = {451--456},
}

@inproceedings{yoshida_understanding_2018,
	title = {Understanding the {Origins} of {Weak} {Cryptographic} {Algorithms} {Used} for {Signing} {Android} {Apps}},
	volume = {02},
	doi = {10.1109/COMPSAC.2018.10324},
	abstract = {Android applications are digitally signed using developers' signing keys. As each key is associated with a developer, it can be used to establish trust between applications published by the author (that is, apps signed with the same key are allowed to update themselves if package names are identical, or access each other's resources). However, if a digital signature is generated using a weak algorithm such as MD5, then apps signed with the corresponding key are exposed to several risks (such as hijacking apps with fake updates or granting permissions to a malicious app). In this work, we analyze several Android apps to identify the threats caused using weak algorithms. Our study uncovered the following findings: Of the more than one million apps collected from Google Play, 223 and 52,866 were digitally signed using the weak algorithms of 512-bit RSA key and MD5, respectively. We identified the causal mechanisms of generating certificates that employ weak algorithms, and that they can be attributed to app-building frameworks and online app-building services. Based on these findings, we provide guidelines for stakeholders of the Android app distribution ecosystem.},
	booktitle = {2018 {IEEE} 42nd {Annual} {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Yoshida, Kanae and Imai, Hironori and Serizawa, Nana and Mori, Tatsuya and Kanaoka, Akira},
	month = jul,
	year = {2018},
	note = {ISSN: 0730-3157},
	keywords = {Android, Androids, Code Signing, Cryptographic Algorithms, Digital Signature, Digital signatures, Google, Humanoid robots, Metadata, Public key cryptography},
	pages = {713--718},
}

@inproceedings{liu_end--end_2015,
	address = {New York, NY, USA},
	series = {{IMC} '15},
	title = {An {End}-to-{End} {Measurement} of {Certificate} {Revocation} in the {Web}'s {PKI}},
	isbn = {978-1-4503-3848-6},
	url = {https://dl.acm.org/doi/10.1145/2815675.2815685},
	doi = {10.1145/2815675.2815685},
	abstract = {Critical to the security of any public key infrastructure (PKI) is the ability to revoke previously issued certificates. While the overall SSL ecosystem is well-studied, the frequency with which certificates are revoked and the circumstances under which clients (e.g., browsers) check whether certificates are revoked are still not well-understood. In this paper, we take a close look at certificate revocations in the Web's PKI. Using 74 full IPv4 HTTPS scans, we find that a surprisingly large fraction (8\%) of the certificates served have been revoked, and that obtaining certificate revocation information can often be expensive in terms of latency and bandwidth for clients. We then study the revocation checking behavior of 30 different combinations of web browsers and operating systems; we find that browsers often do not bother to check whether certificates are revoked (including mobile browsers, which uniformly never check). We also examine the CRLSet infrastructure built into Google Chrome for disseminating revocations; we find that CRLSet only covers 0.35\% of all revocations. Overall, our results paint a bleak picture of the ability to effectively revoke certificates today.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 2015 {Internet} {Measurement} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yabing and Tome, Will and Zhang, Liang and Choffnes, David and Levin, Dave and Maggs, Bruce and Mislove, Alan and Schulman, Aaron and Wilson, Christo},
	month = oct,
	year = {2015},
	keywords = {certificates, crlset, extended validation, https, pki, revocation, ssl, tls, web browsers, x.509},
	pages = {183--196},
}

@inproceedings{huang_analyzing_2014,
	title = {Analyzing {Forged} {SSL} {Certificates} in the {Wild}},
	doi = {10.1109/SP.2014.13},
	abstract = {The SSL man-in-the-middle attack uses forged SSL certificates to intercept encrypted connections between clients and servers. However, due to a lack of reliable indicators, it is still unclear how commonplace these attacks occur in the wild. In this work, we have designed and implemented a method to detect the occurrence of SSL man-in-the-middle attack on a top global website, Facebook. Over 3 million real-world SSL connections to this website were analyzed. Our results indicate that 0.2\% of the SSL connections analyzed were tampered with forged SSL certificates, most of them related to antivirus software and corporate-scale content filters. We have also identified some SSL connections intercepted by malware. Limitations of the method and possible defenses to such attacks are also discussed.},
	booktitle = {2014 {IEEE} {Symposium} on {Security} and {Privacy}},
	author = {Huang, Lin Shung and Rice, Alex and Ellingsen, Erling and Jackson, Collin},
	month = may,
	year = {2014},
	note = {ISSN: 2375-1207},
	keywords = {Browsers, Cryptography, Java, Protocols, SSL, Servers, Sockets, certificates, man-in-the-middle attack},
	pages = {83--97},
}

@inproceedings{holz_ssl_2011,
	address = {New York, NY, USA},
	series = {{IMC} '11},
	title = {The {SSL} landscape: a thorough analysis of the x.509 {PKI} using active and passive measurements},
	isbn = {978-1-4503-1013-0},
	shorttitle = {The {SSL} landscape},
	url = {https://dl.acm.org/doi/10.1145/2068816.2068856},
	doi = {10.1145/2068816.2068856},
	abstract = {The SSL and TLS infrastructure used in important protocols like HTTPs and IMAPs is built on an X.509 public-key infrastructure (PKI). X.509 certificates are thus used to authenticate services like online banking, shopping, e-mail, etc. However, it always has been felt that the certification processes of this PKI may lack in stringency, resulting in a deployment where many certificates do not meet the requirements of a secure PKI. This paper presents a comprehensive analysis of X.509 certificates in the wild. To shed more light on the state of the deployed and actually used X.509 PKI, we obtained and evaluated data from many different sources. We conducted HTTPs scans of a large number of popular HTTPs servers over a 1.5-year time span, including scans from nine locations distributed over the globe. To compare certification properties of highly ranked hosts with the global picture, we included a third-party scan of the entire IPv4 space in our analyses. Furthermore, we monitored live SSL/TLS traffic on a 10Gbps uplink of a large research network. This allows us to compare the properties of the deployed PKI with the part of the PKI that is being actively accessed by users. Our analysis reveals that the quality of certification lacks in stringency, due to a number of reasons among which incorrect certification chains or invalid certificate subjects give the most cause for concern. Similar concerns can be raised for the properties of certification chains and many self-signed certificates used in the deployed X.509 PKI. Our findings confirm what has long been believed -- namely that the X.509 PKI we often use in our everyday's lives is in a sorry state.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 2011 {ACM} {SIGCOMM} conference on {Internet} measurement conference},
	publisher = {Association for Computing Machinery},
	author = {Holz, Ralph and Braun, Lothar and Kammenhuber, Nils and Carle, Georg},
	month = nov,
	year = {2011},
	keywords = {HTTPS, SSL, TLS, X.509, certificates, public key infrastructure},
	pages = {427--444},
}

@inproceedings{cangialosi_measurement_2016,
	address = {New York, NY, USA},
	series = {{CCS} '16},
	title = {Measurement and {Analysis} of {Private} {Key} {Sharing} in the {HTTPS} {Ecosystem}},
	isbn = {978-1-4503-4139-4},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978301},
	doi = {10.1145/2976749.2978301},
	abstract = {The semantics of online authentication in the web are rather straightforward: if Alice has a certificate binding Bob's name to a public key, and if a remote entity can prove knowledge of Bob's private key, then (barring key compromise) that remote entity must be Bob. However, in reality, many websites' and the majority of the most popular ones-are hosted at least in part by third parties such as Content Delivery Networks (CDNs) or web hosting providers. Put simply: administrators of websites who deal with (extremely) sensitive user data are giving their private keys to third parties. Importantly, this sharing of keys is undetectable by most users, and widely unknown even among researchers. In this paper, we perform a large-scale measurement study of key sharing in today's web. We analyze the prevalence with which websites trust third-party hosting providers with their secret keys, as well as the impact that this trust has on responsible key management practices, such as revocation. Our results reveal that key sharing is extremely common, with a small handful of hosting providers having keys from the majority of the most popular websites. We also find that hosting providers often manage their customers' keys, and that they tend to react more slowly yet more thoroughly to compromised or potentially compromised keys.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Cangialosi, Frank and Chung, Taejoong and Choffnes, David and Levin, Dave and Maggs, Bruce M. and Mislove, Alan and Wilson, Christo},
	month = oct,
	year = {2016},
	keywords = {CDN, HTTPs, PKI, SSL, TLS, certificates, content delivery network, key management, key sharing, public key infrastructure},
	pages = {628--640},
}

@inproceedings{garfinkel_johnny_2005,
	address = {New York, NY, USA},
	series = {{SOUPS} '05},
	title = {Johnny 2: a user test of key continuity management with {S}/{MIME} and {Outlook} {Express}},
	isbn = {978-1-59593-178-8},
	shorttitle = {Johnny 2},
	url = {https://dl.acm.org/doi/10.1145/1073001.1073003},
	doi = {10.1145/1073001.1073003},
	abstract = {Secure email has struggled with signifcant obstacles to adoption, among them the low usability of encryption software and the cost and overhead of obtaining public key certificates. Key continuity management (KCM) has been proposed as a way to lower these barriers to adoption, by making key generation, key management, and message signing essentially automatic. We present the first user study of KCM-secured email, conducted on naïve users who had no previous experience with secure email. Our secure email prototype, CoPilot, color-codes messages depending on whether they were signed and whether the signer was previously known or unknown. This interface makes users signicantly less susceptible to social engineering attacks overall, but new-identity attacks (from email addresses never seen before) are still effective. Also, naïve users do use the Sign and Encrypt button on the Outlook Express toolbar when the situation seems to warrant it, even without explicit instruction, although some falsely hoped that Encrypt would protect a secret message even when sent directly to an attacker. We conclude that KCM is a workable model for improving email security today, but work is needed to alert users to "phishing" attacks.},
	urldate = {2023-08-09},
	booktitle = {Proceedings of the 2005 symposium on {Usable} privacy and security},
	publisher = {Association for Computing Machinery},
	author = {Garfinkel, Simson L. and Miller, Robert C.},
	month = jul,
	year = {2005},
	keywords = {Usability, e-commerce, user interaction design, user studies},
	pages = {13--24},
}

@misc{green_whats_2014,
	title = {What’s the matter with {PGP}?},
	url = {https://blog.cryptographyengineering.com/2014/08/13/whats-matter-with-pgp/},
	abstract = {Last Thursday, Yahoo announced their plans to support end-to-end encryption using a fork of Google’s end-to-end email extension. This is a Big Deal. With providers like Google and Yahoo onboa…},
	language = {en},
	urldate = {2023-08-09},
	journal = {A Few Thoughts on Cryptographic Engineering},
	author = {Green, Matthew},
	month = aug,
	year = {2014},
}

@misc{latacora_pgp_2019,
	title = {The {PGP} {Problem}},
	url = {https://latacora.micro.blog/2019/07/16/the-pgp-problem.html},
	urldate = {2023-08-09},
	journal = {Latacora},
	author = {Latacora},
	month = jul,
	year = {2019},
}

@techreport{cooper_security_2018,
	title = {Security {Considerations} for {Code} {Signing}},
	url = {https://csrc.nist.gov/Pubs/cswp/5/security-considerations-for-code-signing/Final},
	abstract = {A wide range of software products (also known as code)—including firmware, operating systems, mobile applications, and application container images—must be distributed and updated in a secure and automatic way to prevent forgery and tampering. Digitally signing code provides both data integrity to prove that the code was not modified, and source authentication to identify who signed the code. This paper describes features and architectural relationships of typical code signing solutions that are widely deployed today. It defines code signing use cases and identifies some security problems that can arise when applying code signing solutions to those use cases. Finally, it provides recommendations for avoiding those problems and resources for more information.},
	language = {en},
	number = {NIST CSWP 5},
	urldate = {2023-08-09},
	institution = {U.S. Department of Commerce},
	author = {Cooper, David and Regenscheid, Andrew and Souppaya, Murugiah and Bean, Christopher and Boyle, Michael and Cooley, Dorothy and Jenkins, Michael},
	month = jan,
	year = {2018},
	doi = {10.6028/NIST.CSWP.5},
}

@inproceedings{almakhdhub_murai_2020,
	address = {San Diego, CA},
	title = {\${\textbackslash}mu\${RAI}: {Securing} {Embedded} {Systems} with {Return} {Address} {Integrity}},
	isbn = {978-1-891562-61-7},
	shorttitle = {\${\textbackslash}mu\${RAI}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2020/02/24016.pdf},
	doi = {10.14722/ndss.2020.24016},
	abstract = {Embedded systems are deployed in security critical environments and have become a prominent target for remote attacks. Microcontroller-based systems (MCUS) are particularly vulnerable due to a combination of limited resources and low level programming which leads to bugs. Since MCUS are often a part of larger systems, vulnerabilities may jeopardize not just the security of the device itself but that of other systems as well. For example, exploiting a WiFi System on Chip (SoC) allows an attacker to hijack the smart phone’s application processor.},
	language = {en},
	urldate = {2023-08-08},
	booktitle = {Proceedings 2020 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Almakhdhub, Naif Saleh and Clements, Abraham A. and Bagchi, Saurabh and Payer, Mathias},
	year = {2020},
}

@inproceedings{clements_aces_2018,
	title = {\{{ACES}\}: {Automatic} {Compartments} for {Embedded} {Systems}},
	isbn = {978-1-939133-04-5},
	shorttitle = {\{{ACES}\}},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/clements},
	language = {en},
	urldate = {2023-08-08},
	author = {Clements, Abraham A. and Almakhdhub, Naif Saleh and Bagchi, Saurabh and Payer, Mathias},
	year = {2018},
	pages = {65--82},
}

@inproceedings{clements_protecting_2017,
	title = {Protecting {Bare}-{Metal} {Embedded} {Systems} with {Privilege} {Overlays}},
	doi = {10.1109/SP.2017.37},
	abstract = {Embedded systems are ubiquitous in every aspect of modern life. As the Internet of Thing expands, our dependence on these systems increases. Many of these interconnected systems are and will be low cost bare-metal systems, executing without an operating system. Bare-metal systems rarely employ any security protection mechanisms and their development assumptions (unrestricted access to all memory and instructions), and constraints(runtime, energy, and memory) makes applying protections challenging. To address these challenges we present EPOXY, an LLVM-based embedded compiler. We apply a novel technique, called privilege overlaying, wherein operations requiring privileged execution are identified and only these operations execute in privileged mode. This provides the foundation on which code-integrity, adapted control-flow hijacking defenses, and protections for sensitive IO are applied. We also design fine-grained randomization schemes, that work within the constraints of bare-metal systems to provide further protection against control-flow and data corruption attacks. These defenses prevent code injection attacks and ROP attacks from scaling across large sets of devices. We evaluate the performance of our combined defense mechanisms for a suite of 75 benchmarks and 3 real-world IoT applications. Our results for the application case studies show that EPOXY has, on average, a 1.8\% increase in execution time and a 0.5\% increase in energy usage.},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Clements, Abraham A. and Almakhdhub, Naif Saleh and Saab, Khaled S. and Srivastava, Prashast and Koo, Jinkyu and Bagchi, Saurabh and Payer, Mathias},
	month = may,
	year = {2017},
	note = {ISSN: 2375-1207},
	keywords = {Bare-metal, Cyber-Security, Diversity, Embedded Systems, Embedded systems, Hardware, Memory management, Privilege Overlay, Random access memory, Registers, Runtime, Security},
	pages = {289--303},
}

@article{noauthor_blockchain-based_nodate,
	title = {Blockchain-{Based} {Platform} to {Fight} {Disinformation} {Using} {Crowd} {Wisdom} and {Artificial} {Intelligence}},
}

@misc{sourceforge_compare_nodate,
	title = {Compare, {Download} \& {Develop} {Open} {Source} \& {Business} {Software} - {SourceForge}},
	url = {https://sourceforge.net/},
	urldate = {2023-07-19},
	author = {SourceForge},
}

@misc{gitlb_devsecops_nodate,
	title = {The {DevSecOps} {Platform}},
	url = {https://about.gitlab.com/},
	abstract = {From planning to production, bring teams together in one application. Ship secure code more efficiently to deliver value faster.},
	language = {en-us},
	urldate = {2023-07-19},
	author = {GitLb},
}

@misc{noauthor_github_nodate,
	title = {{GitHub}: {Let}’s build from here},
	shorttitle = {{GitHub}},
	url = {https://github.com/},
	abstract = {GitHub is where over 100 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and fea...},
	language = {en},
	urldate = {2023-07-19},
	journal = {GitHub},
}

@book{european_commission_directorate_general_for_communications_networks_content_and_technology_impact_2021,
	address = {LU},
	title = {The impact of open source software and hardware on technological independence, competitiveness and innovation in the {EU} economy: final study report.},
	shorttitle = {The impact of open source software and hardware on technological independence, competitiveness and innovation in the {EU} economy},
	url = {https://data.europa.eu/doi/10.2759/430161},
	language = {en},
	urldate = {2023-07-19},
	publisher = {Publications Office},
	author = {{European Commission. Directorate General for Communications Networks, Content and Technology.}},
	year = {2021},
}

@book{european_commission_directorate_general_for_communications_networks_content_and_technology_impact_2021-1,
	address = {LU},
	title = {The impact of open source software and hardware on technological independence, competitiveness and innovation in the {EU} economy: final study report.},
	shorttitle = {The impact of open source software and hardware on technological independence, competitiveness and innovation in the {EU} economy},
	url = {https://data.europa.eu/doi/10.2759/430161},
	language = {en},
	urldate = {2023-07-19},
	publisher = {Publications Office},
	author = {{European Commission. Directorate General for Communications Networks, Content and Technology.}},
	year = {2021},
}

@misc{linux_kernel_organization_linux_nodate,
	title = {The {Linux} {Kernel} {Archives} - {About}},
	url = {https://www.kernel.org/category/about.html},
	urldate = {2023-07-19},
	author = {Linux Kernel Organization},
}

@book{raymond_cathedral_1999,
	address = {Beijing ; Cambridge, Mass},
	edition = {1st ed},
	title = {The cathedral \& the bazaar: musings on {Linux} and open source by an accidental revolutionary},
	isbn = {978-1-56592-724-7},
	shorttitle = {The cathedral \& the bazaar},
	publisher = {O'Reilly},
	author = {Raymond, Eric S.},
	year = {1999},
	keywords = {Computer software, Development, Hackers, Linux, Operating systems (Computers)},
}

@misc{noauthor_gnu_nodate,
	title = {The {GNU} {Operating} {System} and the {Free} {Software} {Movement}},
	url = {https://www.gnu.org/home.en.html},
	urldate = {2023-07-19},
}

@misc{opensource_what_nodate,
	title = {What is open source?},
	shorttitle = {What is open source?},
	url = {https://opensource.com/resources/what-open-source},
	abstract = {What is open source?},
	language = {en},
	urldate = {2023-07-19},
	author = {opensource},
}

@misc{sullivan_ecdsa_2014,
	title = {{ECDSA}: {The} digital signature algorithm of a better internet},
	shorttitle = {{ECDSA}},
	url = {http://blog.cloudflare.com/ecdsa-the-digital-signature-algorithm-of-a-better-internet/},
	abstract = {This blog post is dedicated to the memory of Dr. Scott Vanstone, popularizer of elliptic curve cryptography and inventor of the ECDSA algorithm. He passed away on March 2, 2014.},
	language = {en},
	urldate = {2023-07-19},
	journal = {The Cloudflare Blog},
	author = {Sullivan, Nick},
	month = mar,
	year = {2014},
}

@misc{sigstore_sigstore_nodate,
	title = {sigstore},
	url = {https://www.sigstore.dev/undefined/},
	abstract = {sign. verify. protect. Make sure your software is what it claims to be.},
	language = {en},
	urldate = {2023-07-19},
	journal = {sigstore},
	author = {sigstore},
}

@misc{stufft_removing_2023,
	title = {Removing {PGP} from {PyPI} - {The} {Python} {Package} {Index}},
	url = {https://blog.pypi.org/posts/2023-05-23-removing-pgp/},
	abstract = {PyPI has removed support for uploading PGP signatures with new releases.},
	language = {en},
	urldate = {2023-05-24},
	author = {Stufft, Donald},
	month = may,
	year = {2023},
}

@misc{npm_about_nodate,
	title = {About {PGP} registry signatures (deprecated) {\textbar} npm {Docs}},
	url = {https://docs.npmjs.com/about-pgp-signatures-for-packages-in-the-public-registry},
	abstract = {Documentation for the npm registry, website, and command-line interface},
	language = {en},
	urldate = {2023-07-19},
	journal = {npm Docs},
	author = {npm},
}

@book{winters_software_2020,
	address = {Beijing [China]},
	edition = {First edition},
	title = {Software engineering at {Google}: lessons learned from programming over time},
	isbn = {978-1-4920-8279-8},
	shorttitle = {Software engineering at {Google}},
	publisher = {O'Reilly Media},
	author = {Winters, Titus and Manshreck, Tom and Wright, Hyrum},
	year = {2020},
	keywords = {Google (Firm), Software engineering},
}

@misc{stealthpath_solarwinds_nodate,
	title = {The {SolarWinds} {Silver} {Lining}: {Motivating} the {Move} to {Zero} {Trust}},
	url = {https://www.cisa.gov/sites/default/files/ICSJWG-Archive/QNL_MAR_21/SolarWinds_Response_White_Paper_Finala_S508C.pdf},
	publisher = {Cybsersecurity and Infrastructure Security Agency},
	author = {StealthPath},
}

@techreport{synopsys_2023_2023,
	title = {2023 {OSSRA} {Report}},
	url = {https://www.synopsys.com/software-integrity/engage/ossra/rep-ossra-2023-pdf},
	abstract = {The report offers recommendations for security, legal, risk, and development teams to better understand the security and risk landscape accompanying open source development and use.},
	language = {en},
	urldate = {2023-07-17},
	author = {synopsys},
	year = {2023},
}

@misc{department_of_defense_open_2003,
	title = {Open {Source} {Software} ({OSS}) in the {Department} of {Defense} ({DoD})},
	publisher = {Department of Defense},
	author = {Department of Defense},
	month = may,
	year = {2003},
}

@misc{gsa_18f_18f_nodate,
	title = {{18F}: {Digital} service delivery {\textbar} {Open} source policy},
	shorttitle = {{18F}},
	url = {https://18f.gsa.gov/open-source-policy/},
	abstract = {18F builds effective, user-centric digital services focused on the interaction between government and the people and businesses it serves.},
	language = {en-US},
	urldate = {2023-07-17},
	journal = {Open source policy},
	author = {GSA: 18F},
}

@misc{wales_letter_2021,
	title = {Letter from {CISA} to {Senator} {Wyden}},
	url = {https://www.documentcloud.org/documents/20969575-wyden_response_signed},
	urldate = {2023-07-17},
	author = {Wales, Brandon},
	month = jun,
	year = {2021},
}

@misc{engle_three_2021,
	title = {Three {Vulnerabilities} {Exposed} {During} {SolarWinds} {Attack} \& {How} {It} {Could} {Have} {Been} {Prevented}},
	url = {https://www.cpomagazine.com/cyber-security/three-vulnerabilities-exposed-during-solarwinds-attack-how-it-could-have-been-prevented/},
	abstract = {Cybersecurity professionals were left in the dark as the SolarWinds attack unfolded. Looking at how this could have been prevented, three distinct vulnerabilities stand out.},
	language = {en-US},
	urldate = {2023-07-17},
	journal = {CPO Magazine},
	author = {Engle, Mike},
	month = mar,
	year = {2021},
}

@inproceedings{wang_mttm_2023,
	title = {{MTTM}: {Metamorphic} {Testing} for {Textual} {Content} {Moderation} {Software}},
	shorttitle = {{MTTM}},
	doi = {10.1109/ICSE48619.2023.00200},
	abstract = {The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9\%, 51\%, and 82.5\% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2\% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness 0\% 5.9\% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wang, Wenxuan and Huang, Jen-tse and Wu, Weibin and Zhang, Jianping and Huang, Yizhan and Li, Shuqing and He, Pinjia and Lyu, Michael R.},
	month = may,
	year = {2023},
	note = {ISSN: 1558-1225},
	keywords = {NLP software, Perturbation methods, Social networking (online), Software, Software algorithms, Software performance, Software testing, Systematics, Web and internet services, metamorphic relations, textual content moderation},
	pages = {2387--2399},
}

@article{costello1984software,
	title = {Software engineering under deadline pressure},
	volume = {9},
	number = {5},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Costello, Scott H},
	year = {1984},
	note = {Publisher: ACM New York, NY, USA},
	pages = {15--19},
}

@article{kuutila_time_2020,
	title = {Time {Pressure} in {Software} {Engineering}: {A} {Systematic} {Review}},
	volume = {121},
	issn = {09505849},
	shorttitle = {Time {Pressure} in {Software} {Engineering}},
	url = {http://arxiv.org/abs/1901.05771},
	doi = {10.1016/j.infsof.2020.106257},
	abstract = {Large project overruns and overtime work have been reported in the software industry, resulting in additional expense for companies and personal issues for developers. The present work aims to provide an overview of studies related to time pressure in software engineering; specifically, existing definitions, possible causes, and metrics relevant to time pressure were collected, and a mapping of the studies to software processes and approaches was performed. Moreover, we synthesize results of existing quantitative studies on the effects of time pressure on software development, and offer practical takeaways for practitioners and researchers, based on empirical evidence. Our search strategy examined 5,414 sources, found through repository searches and snowballing. Applying inclusion and exclusion criteria resulted in the selection of 102 papers, which made relevant contributions related to time pressure in software engineering. The majority of high quality studies report increased productivity and decreased quality under time pressure. Frequent categories of studies focus on quality assurance, cost estimation, and process simulation. It appears that time pressure is usually caused by errors in cost estimation. The effect of time pressure is most often identified during software quality assurance. The majority of empirical studies report increased productivity under time pressure, while the most cost estimation and process simulation models assume that compressing the schedule increases the total needed hours. We also find evidence of the mediating effect of knowledge on the effects of time pressure, and that tight deadlines impact tasks with an algorithmic nature more severely. Future research should better contextualize quantitative studies to account for the existing conflicting results and to provide an understanding of situations when time pressure is either beneficial or harmful.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Information and Software Technology},
	author = {Kuutila, Miikka and Mäntylä, Mika and Farooq, Umar and Claes, Maëlick},
	month = may,
	year = {2020},
	note = {arXiv:1901.05771 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {106257},
}

@phdthesis{lehtinen2014development,
	type = {{PhD} {Thesis}},
	title = {Development and evaluation of a lightweight root cause analysis method in software project retrospectives},
	school = {Aalto University},
	author = {Lehtinen, Timo OA and {others}},
	year = {2014},
	note = {Publisher: Aalto University},
}

@inproceedings{matthies_additional_2019,
	address = {Montreal, QC, Canada},
	title = {An {Additional} {Set} of ({Automated}) {Eyes}: {Chatbots} for {Agile} {Retrospectives}},
	isbn = {978-1-72812-262-5},
	shorttitle = {An {Additional} {Set} of ({Automated}) {Eyes}},
	url = {https://ieeexplore.ieee.org/document/8823641/},
	doi = {10.1109/BotSE.2019.00017},
	abstract = {Recent advances in natural-language processing and data analysis allow software bots to become virtual team members, providing an additional set of automated eyes and additional perspectives for informing and supporting teamwork. In this paper, we propose employing chatbots in the domain of software development with a focus on supporting analyses and measurements of teams’ project data. The software project artifacts produced by agile teams during regular development activities, e.g. commits in a version control system, represent detailed information on how a team works and collaborates. Analyses of this data are especially relevant for agile retrospective meetings, where adaptations and improvements to the executed development process are discussed. Development teams can use these measurements to track the progress of identiﬁed improvement actions over development iterations. Chatbots provide a convenient user interface for interacting with the outcomes of retrospectives and the associated measurements in a chat-based channel that is already being employed by team members.},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Bots} in {Software} {Engineering} ({BotSE})},
	publisher = {IEEE},
	author = {Matthies, Christoph and Dobrigkeit, Franziska and Hesse, Guenter},
	month = may,
	year = {2019},
	pages = {34--37},
}

@inproceedings{przybylek_making_2017,
	title = {Making agile retrospectives more awesome},
	url = {https://fedcsis.org/proceedings/2017/drp/423.html},
	doi = {10.15439/2017F423},
	abstract = {According to the textbook [23], Scrum exists only in its entirety, where every component is essential to Scrum’s success. However, in many organizational environments some of the components are omitted or modified in a way that is not aligned with the Scrum guidelines. Usually, such deviations result in missing the full benefits of Scrum [24]. Thereby, a Scrum process should be frequently inspected and any deviations should be corrected [23]. In this paper, we report on an Action Research project conducted in Intel Technology Poland to revise the work practices related to the Retrospective. During the focus group discussion in the company, retrospectives were generally judged ineffective because “the same things are discussed over and over”. To cope with this challenge, we revitalized retrospectives by adopting collaborative games. The feedback received from three Scrum teams indicates that our approach improved participants’ creativity, involvement, and communication, and produced better results than the standard retrospective.},
	language = {en},
	urldate = {2023-07-05},
	author = {Przybyłek, Adam and Kotecka, Dagmara},
	month = sep,
	year = {2017},
	pages = {1211--1216},
}

@article{bain2018iot,
	title = {Bain predicts the {IoT} market will more than double by 2021},
	url = {https://www.bain.com/about/media-center/press-releases/2018/bain-predicts-the-iot-market-will-more-than-double-by-2021/},
	urldate = {2023-07-05},
	author = {{Bain \& Company}},
	year = {2018},
}

@incollection{li_understanding_2017,
	address = {Cham},
	title = {Understanding {Knowledge} {Management} in {Agile} {Software} {Development} {Practice}},
	volume = {10412},
	isbn = {978-3-319-63557-6 978-3-319-63558-3},
	url = {http://link.springer.com/10.1007/978-3-319-63558-3_17},
	abstract = {Knowledge management in agile software development has typically been treated as a broad topic resulting in major classifications of its schools and concepts. What inherent knowledge is involved in everyday agile practice and how agile teams manage it is not well understood. To address these questions, we performed a Systematic Literature Review of 48 relevant empirical studies selected from reputed databases. Using a thematic analysis approach to the synthesis, we discovered that (a) agile teams use three knowledge management strategies: discussions, artifacts and visualisations to manage knowledge (b) there are three types of software engineering knowledge: team progress as project knowledge; requirements as product knowledge; and coding techniques as process knowledge. (c) this knowledge is presented in several everyday agile practices. A theoretical model describing how knowledge management strategies and knowledge types are related to agile practices is also presented. These results will help agile practitioners become aware of the specific knowledge types and knowledge management strategies and enable them to better manage them in everyday agile practices. Researchers can further investigate and build upon these findings through empirical studies.},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {Knowledge {Science}, {Engineering} and {Management}},
	publisher = {Springer International Publishing},
	author = {Andriyani, Yanti and Hoda, Rashina and Amor, Robert},
	editor = {Li, Gang and Ge, Yong and Zhang, Zili and Jin, Zhi and Blumenstein, Michael},
	year = {2017},
	doi = {10.1007/978-3-319-63558-3_17},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {195--207},
}

@book{baumeister_agile_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}: 18th {International} {Conference}, {XP} 2017, {Cologne}, {Germany}, {May} 22-26, 2017, {Proceedings}},
	volume = {283},
	isbn = {978-3-319-57632-9 978-3-319-57633-6},
	shorttitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	url = {http://link.springer.com/10.1007/978-3-319-57633-6},
	language = {en},
	urldate = {2023-07-05},
	publisher = {Springer International Publishing},
	editor = {Baumeister, Hubert and Lichter, Horst and Riebisch, Matthias},
	year = {2017},
	doi = {10.1007/978-3-319-57633-6},
}

@book{gonccalves2013getting,
	title = {Getting value out of {Agile} retrospectives},
	publisher = {Leanpub},
	author = {Gonçalves, Luis and Linders, Ben},
	year = {2013},
}

@book{derby2006agile,
	title = {Agile retrospectives: {Making} good teams great},
	publisher = {Pragmatic Bookshelf},
	author = {Derby, Esther and Larsen, Diana and Schwaber, Ken},
	year = {2006},
}

@book{rubin2012essential,
	title = {Essential {Scrum}: {A} practical guide to the most popular {Agile} process},
	publisher = {Addison-Wesley},
	author = {Rubin, Kenneth S},
	year = {2012},
}

@incollection{walker_counteracting_2019,
	address = {Cham},
	title = {Counteracting {Agile} {Retrospective} {Problems} with {Retrospective} {Activities}},
	volume = {1060},
	isbn = {978-3-030-28004-8 978-3-030-28005-5},
	url = {http://link.springer.com/10.1007/978-3-030-28005-5_41},
	abstract = {Retrospective meetings are a fundamental part of Agile software development methods. Eﬀective retrospectives can positively impact teamwork, productivity, and work satisfaction. In this paper, we focus on problems that commonly occur during these meetings. To address them, we suggest retrospective activities that are already used in practice. These activities provide structure and guide the team through the meeting. We created a mapping between a selection of these activities and the problems they attempt to solve. We evaluated the created mapping through multiple case studies with software development teams in educational and professional contexts. Our results verify the existence of a relationship between speciﬁc retrospective activities and connected retrospective problems. Furthermore, using observational studies we could verify parts of the created problem-activity mapping.},
	language = {en},
	urldate = {2023-07-05},
	booktitle = {Systems, {Software} and {Services} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Matthies, Christoph and Dobrigkeit, Franziska and Ernst, Alexander},
	editor = {Walker, Alastair and O'Connor, Rory V. and Messnarz, Richard},
	year = {2019},
	doi = {10.1007/978-3-030-28005-5_41},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {532--545},
}

@book{NAP11923,
	address = {Washington, DC},
	title = {Software for dependable systems: {Sufficient} evidence?},
	isbn = {978-0-309-10394-7},
	url = {https://www.nap.edu/catalog/11923/software-for-dependable-systems-sufficient-evidence},
	abstract = {The focus of Software for Dependable Systems is a set of fundamental principles that underlie software system dependability and that suggest a different approach to the development and assessment of dependable software., it is difficult to assess the dependability of software. The field of software engineering suffers from a pervasive lack of evidence about the incidence and severity of software failures; about the dependability of existing software systems; about the efficacy of existing and proposed development methods; about the benefits of certification schemes; and so on. There are many anecdotal reports, which2̆014although often useful for indicating areas of concern or highlighting promising avenues of research2̆014do little to establish a sound and complete basis for making policy decisions regarding dependability. The committee regards claims of extraordinary dependability that are sometimes made on this basis for the most critical of systems as unsubstantiated, and perhaps irresponsible. This difficulty regarding the lack of evidence for system dependability leads to two conclusions: (1) that better evidence is needed, so that approaches aimed at improving the dependability of software can be objectively assessed, and (2) that, for now, the pursuit of dependability in software systems should focus on the construction and evaluation of evidence.committee also recognized the importance of adopting the practices that are already known and used by the best developers; this report gives a sample of such practices. Some of these (such as systematic configuration management and automated regression testing) are relatively easy to adopt; others (such as constructing hazard analyses and threat models, exploiting formal notations when appropriate, and applying static analysis to code) will require new training for many developers. However valuable, though, these practices are in themselves no silver bullet, and new techniques and methods will be required in order to build future software systems to the level of dependability that will be required.},
	publisher = {The National Academies Press},
	author = {{National Research Council}},
	editor = {Jackson, Daniel and Thomas, Martyn and Millett, Lynette I.},
	year = {2007},
	doi = {10.17226/11923},
	keywords = {ASE23-DA, IoTFailurePaper},
}

@misc{hhs_department_surgeon_2023,
	type = {Text},
	title = {Surgeon {General} {Issues} {New} {Advisory} {About} {Effects} {Social} {Media} {Use} {Has} on {Youth} {Mental} {Health}},
	url = {https://www.hhs.gov/about/news/2023/05/23/surgeon-general-issues-new-advisory-about-effects-social-media-use-has-youth-mental-health.html},
	abstract = {Surgeon General Urges Action to Ensure Social Media Environments are Healthy and Safe, as Previously-Advised National Youth Mental Health Crisis Continues},
	language = {en},
	urldate = {2023-07-03},
	journal = {HHS.gov},
	author = {{HHS Department}},
	month = may,
	year = {2023},
	note = {Last Modified: 2023-05-23T11:39:35-0400},
}

@book{wooldridge_introductory_2013,
	address = {Mason, OH},
	edition = {5th ed},
	title = {Introductory econometrics: a modern approach},
	isbn = {978-1-111-53104-1},
	shorttitle = {Introductory econometrics},
	publisher = {South-Western Cengage Learning},
	author = {Wooldridge, Jeffrey M.},
	year = {2013},
	note = {OCLC: ocn827938223},
	keywords = {Econometrics},
}

@book{felderer_contemporary_2020,
	address = {Cham},
	title = {Contemporary empirical methods in software engineering},
	isbn = {978-3-030-32488-9},
	language = {eng},
	publisher = {Springer},
	editor = {Felderer, Michael and Travassos, Guilherme H.},
	year = {2020},
}

@book{wohlin_experimentation_2012,
	address = {New York},
	title = {Experimentation in software engineering},
	isbn = {978-3-642-29043-5},
	publisher = {Springer},
	author = {Wohlin, Claes},
	year = {2012},
}

@book{remler_research_2015,
	address = {Los Angeles},
	edition = {Second edition},
	title = {Research methods in practice: strategies for description and causation},
	isbn = {978-1-4522-7640-3},
	shorttitle = {Research methods in practice},
	publisher = {SAGE},
	author = {Remler, Dahlia K. and Van Ryzin, Gregg G.},
	year = {2015},
	keywords = {Methodology Study and teaching (Graduate), Research},
}

@article{costello_casino_nodate,
	title = {A {Casino} {Benefits} the {Mental} {Health} of {Cherokee} {Children}},
	language = {en},
	author = {Costello, Jane},
}

@book{noauthor_notitle_nodate,
}

@misc{noauthor_research_2023,
	title = {Research {Methods} in {Practice}},
	url = {https://us.sagepub.com/en-us/nam/research-methods-in-practice/book255056},
	abstract = {Strategies for Description and Causation},
	language = {en},
	urldate = {2023-06-29},
	journal = {SAGE Publications Inc},
	month = jun,
	year = {2023},
}

@article{meyer_natural_1995,
	title = {Natural and {Quasi}-{Experiments} in {Economics}},
	volume = {13},
	issn = {0735-0015},
	url = {https://www.jstor.org/stable/1392369},
	doi = {10.2307/1392369},
	abstract = {Using research designs patterned after randomized experiments, many recent economic studies examine outcome measures for treatment groups and comparison groups that are not randomly assigned. By using variation in explanatory variables generated by changes in state laws, government draft mechanisms, or other means, these studies obtain variation that is readily examined and is plausibly exogenous. This article describes the advantages of these studies and suggests how they can be improved. It also provides aids in judging the validity of inferences that they draw. Design complications such as multiple treatment and comparison groups and multiple preintervention or postintervention observations are advocated.},
	number = {2},
	urldate = {2023-06-29},
	journal = {Journal of Business \& Economic Statistics},
	author = {Meyer, Bruce D.},
	year = {1995},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {151--161},
}

@techreport{sonatype_state_2022,
	title = {State of the {Software} {Supply} {Chain}},
	url = {https://www.sonatype.com/state-of-the-software-supply-chain/open-source-supply-demand-security},
	number = {8th Annual},
	institution = {Sonatype},
	author = {Sonatype},
	year = {2022},
}

@misc{noauthor_open_nodate,
	title = {Open {Source} {Dependency} {Management}: {Trends} and {Recommendations}},
	shorttitle = {Open {Source} {Dependency} {Management}},
	url = {https://www.sonatype.com/state-of-the-software-supply-chain/open-source-supply-demand-security},
	abstract = {There has been an astonishing 742\% average annual increase in Software Supply Chain attacks over the past three years according to @Sonatype's recent report.},
	language = {en-us},
	urldate = {2023-06-28},
}

@misc{Yuan2023BackdoorAtackstoPTFoundationalModels,
	title = {Backdoor {Attacks} to {Pre}-trained {Unified} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2302.09360},
	abstract = {The rise of pre-trained unified foundation models breaks down the barriers between different modalities and tasks, providing comprehensive support to users with unified architectures. However, the backdoor attack on pre-trained models poses a serious threat to their security. Previous research on backdoor attacks has been limited to uni-modal tasks or single tasks across modalities, making it inapplicable to unified foundation models. In this paper, we make proof-of-concept level research on the backdoor attack for pre-trained unified foundation models. Through preliminary experiments on NLP and CV classification tasks, we reveal the vulnerability of these models and suggest future research directions for enhancing the attack approach.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Yuan, Zenghui and Liu, Yixin and Zhang, Kai and Zhou, Pan and Sun, Lichao},
	month = feb,
	year = {2023},
	keywords = {Computer Science - Cryptography and Security},
}

@article{Wang2022Backdoor4TLwithPTM,
	title = {Backdoor {Attacks} {Against} {Transfer} {Learning} {With} {Pre}-{Trained} {Deep} {Learning} {Models}},
	volume = {15},
	issn = {1939-1374},
	doi = {10.1109/TSC.2020.3000900},
	abstract = {Transfer learning provides an effective solution for feasibly and fast customize accurate Student models, by transferring the learned knowledge of pre-trained Teacher models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this article, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly adopted defenses: pruning-based, retraining-based and input pre-processing-based defenses. Specifically, ({\textbackslash}mathcal AA) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating pruning-based and/or retraining-based defenses. ({\textbackslash}mathcal BB) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the input pre-processing-based defense, while guaranteeing that selected neuron(s) can be significantly activated. ({\textbackslash}mathcal CC) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the 98.4 and 97.2 percent classification accuracy as the genuine model on clean image and time series inputs while improving 27.9\%-100\%27.9\%-100\% and 27.1\%-56.1\%27.1\%-56.1\% attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.},
	number = {3},
	journal = {IEEE Transactions on Services Computing},
	author = {Wang, Shuo and Nepal, Surya and Rudolph, Carsten and Grobler, Marthie and Chen, Shangyu and Chen, Tianle},
	month = may,
	year = {2022},
	keywords = {Biological system modeling, Data models, Electrocardiography, Learning systems, Task analysis, Training, Web service, backdoor attack, deep neural network, pre-trained model, transfer learning},
	pages = {1526--1539},
}

@inproceedings{Cappos2008AttacksonPackageManagers,
	address = {New York, NY, USA},
	series = {{CCS} '08},
	title = {A look in the mirror: attacks on package managers},
	isbn = {978-1-59593-810-7},
	shorttitle = {A look in the mirror},
	url = {https://doi.org/10.1145/1455770.1455841},
	doi = {10.1145/1455770.1455841},
	abstract = {This work studies the security of ten popular package managers. These package managers use different security mechanisms that provide varying levels of usability and resilience to attack. We find that, despite their existing security mechanisms, all of these package managers have vulnerabilities that can be exploited by a man-in-the-middle or a malicious mirror. While all current package managers suffer from vulnerabilities, their security is also positively or negatively impacted by the distribution's security practices. Weaknesses in package managers are more easily exploited when distributions use third-party mirrors as official mirrors. We were successful in using false credentials to obtain an official mirror on all five of the distributions we attempted. We also found that some security mechanisms that control where a client obtains metadata and packages from may actually decrease security. We analyze current package managers to show that by exploiting vulnerabilities, an attacker with a mirror can compromise or crash hundreds to thousands of clients weekly. The problems we disclose are now being corrected by many different package manager maintainers.},
	urldate = {2023-01-20},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security},
	publisher = {Association for Computing Machinery},
	author = {Cappos, Justin and Samuel, Justin and Baker, Scott and Hartman, John H.},
	month = oct,
	year = {2008},
	keywords = {mirrors, package management, replay attack},
	pages = {565--574},
}

@inproceedings{davis_reusing_2023,
	title = {Reusing {Deep} {Learning} {Models}: {Challenges} and {Directions} in {Software} {Engineering}},
	abstract = {Deep neural networks (DNNs) achieve state-of-theart performance in many areas, including computer vision, system configuration, and question-answering. However, DNNs are expensive to develop, both in intellectual effort (e.g., devising new architectures) and computational costs (e.g., training). Reusing DNNs is a promising direction to amortize costs within a company and across the computing industry. As with any new technology, however, there are many challenges in re-using DNNs. These challenges include both missing technical capabilities and missing engineering practices.},
	language = {en},
	booktitle = {Proceedings of the {IEEE} {John} {Vincent} {Atanasoff} {Symposium} on {Modern} {Computing} ({JVA}’23)},
	author = {Davis, James C and Jajal, Purvish and Jiang, Wenxin and Schorlemmer, Taylor R and Synovic, Nicholas and Thiruvathukal, George K},
	year = {2023},
}

@article{menolli_organizational_2013,
	title = {{ORGANIZATIONAL} {LEARNING} {APPLIED} {TO} {SOFTWARE} {ENGINEERING}: {A} {SYSTEMATIC} {REVIEW}},
	volume = {23},
	issn = {0218-1940, 1793-6403},
	shorttitle = {{ORGANIZATIONAL} {LEARNING} {APPLIED} {TO} {SOFTWARE} {ENGINEERING}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218194013500356},
	doi = {10.1142/S0218194013500356},
	abstract = {Organizational learning assists the companies to improve significantly their processes by means of experiences reuse, making knowledge accessible to the whole organization. In software engineering it is important that the acquired knowledge is stored and systematically reused. This paper aims to present a systematic review, by identifying in which software engineering areas are the organizational learning studies concentrated, and how the organizational learning concepts are being applied in software engineering. This systematic review identified 2496 papers. After eliminating the duplicate titles and those not related to the review, 1184 papers remained. Applying the exclusion criteria, the number of papers was reduced to 68. These papers were analyzed and classified according to the software engineering areas defined in the SWEBOK, and the main organizational learning theories and techniques. It was observed that many software engineering researches apply organizational learning concepts without being aware of it.},
	language = {en},
	number = {08},
	urldate = {2023-06-21},
	journal = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Menolli, Andre and Reinehr, Sheila and Malucelli, Andreia},
	month = oct,
	year = {2013},
	pages = {1153--1175},
}

@article{mehta_maximizing_2022,
	title = {Maximizing integrative learning in software development teams: {A} systematic review of key drivers and future research agenda},
	volume = {190},
	issn = {01641212},
	shorttitle = {Maximizing integrative learning in software development teams},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121222000838},
	doi = {10.1016/j.jss.2022.111345},
	abstract = {Software development is a complex phenomenon that requires software teams to integrate diverse knowledge and expertise to provide innovative solutions. A critical success factor for software teams is their integrative learning capability which is driven by multiple factors. We systematically review and synthesize extant literature to identify critical antecedents of integrative learning in software teams and uncover knowledge gaps to inform future research. Searching multiple databases, 447 papers were identified, of which 32 were selected for the final analysis after a rigorous data extraction process. We performed open and axial coding and affinity diagramming to thematically analyze each study for antecedents associated with integrative learning. We also triangulated our results by content analyzing the studies using a software-based text analysis tool. The findings indicate five crucial drivers of integrative learning in software teams: social harmony, process agility, team design, technological maturity, and project environment. The study also highlights the value of recognizing the inter-team and intra-team contexts in team learning. Based on the study findings, we propose a model of integrative learning in software teams, discuss future research agendas, and offer practical insights and recommendations to software professionals.},
	language = {en},
	urldate = {2023-06-21},
	journal = {Journal of Systems and Software},
	author = {Mehta, Anju and Mehta, Nikhil and Bindal, Ishaan},
	month = aug,
	year = {2022},
	pages = {111345},
}

@inproceedings{saha2022mining,
	title = {Mining root cause knowledge from cloud service incident investigations for {AIOps}},
	booktitle = {Proceedings of the 44th international conference on software engineering: {Software} engineering in practice},
	author = {Saha, Amrita and Hoi, Steven CH},
	year = {2022},
	pages = {197--206},
}

@inproceedings{li_going_2022,
	address = {Charlotte, NC, USA},
	title = {Going through the {Life} {Cycle} of {Faults} in {Clouds}: {Guidelines} on {Fault} {Handling}},
	isbn = {978-1-66545-132-1},
	shorttitle = {Going through the {Life} {Cycle} of {Faults} in {Clouds}},
	url = {https://ieeexplore.ieee.org/document/9978764/},
	doi = {10.1109/ISSRE55969.2022.00022},
	abstract = {Faults are the primary culprits of breaking the high availability of cloud systems, even leading to costly outages. As the scale and complexity of clouds increase, it becomes extraordinarily difﬁcult to understand, detect and diagnose faults. During outages, engineers record the detailed information of the whole life cycle of faults (i.e., fault occurrence, fault detection, fault identiﬁcation, and fault mitigation) in the form of postmortems. In this paper, we conduct a quantitative and qualitative study on 354 public post-mortems collected in three popular large-scale clouds, 97.7\% of which spans from 2015 to 2021. By reviewing and analyzing post-mortems, we go through the life cycle of faults in clouds and obtain 10 major ﬁndings. Based on these ﬁndings, we further reach a series of actionable guidelines for better fault handling.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2022 {IEEE} 33rd {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Li, Xiaoyun and Yu, Guangba and Chen, Pengfei and Chen, Hongyang and Chen, Zhekang},
	month = oct,
	year = {2022},
	pages = {121--132},
}

@misc{ahmed_recommending_2023,
	title = {Recommending {Root}-{Cause} and {Mitigation} {Steps} for {Cloud} {Incidents} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2301.03797},
	abstract = {Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require signiﬁcant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artiﬁcial intelligence has resulted in state-ofthe-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the ﬁrst large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, ﬁne-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efﬁcacy and future potential of using artiﬁcial intelligence for resolving cloud incidents.},
	language = {en},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Ahmed, Toufique and Ghosh, Supriyo and Bansal, Chetan and Zimmermann, Thomas and Zhang, Xuchao and Rajmohan, Saravan},
	month = feb,
	year = {2023},
	note = {arXiv:2301.03797 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{yang_characterizing_2022,
	address = {Baltimore, MD, USA},
	title = {Characterizing and {Mitigating} {Anti}-patterns of {Alerts} in {Industrial} {Cloud} {Systems}},
	isbn = {978-1-66541-693-1},
	url = {https://ieeexplore.ieee.org/document/9833624/},
	doi = {10.1109/DSN53405.2022.00047},
	abstract = {Alerts are crucial for requesting prompt human intervention upon cloud anomalies. The quality of alerts signiﬁcantly affects the cloud reliability and the cloud provider’s business revenue. In practice, we observe on-call engineers being hindered from quickly locating and ﬁxing faulty cloud services because of the vast existence of misleading, non-informative, non-actionable alerts. We call the ineffectiveness of alerts “antipatterns of alerts”. To better understand the anti-patterns of alerts and provide actionable measures to mitigate anti-patterns, in this paper, we conduct the ﬁrst empirical study on the practices of mitigating anti-patterns of alerts in an industrial cloud system. We study the alert strategies and the alert processing procedure at Huawei Cloud, a leading cloud provider. Our study combines the quantitative analysis of millions of alerts in two years and a survey with eighteen experienced engineers. As a result, we summarized four individual anti-patterns and two collective antipatterns of alerts. We also summarize four current reactions to mitigate the anti-patterns of alerts, and the general preventative guidelines for the conﬁguration of alert strategy. Lastly, we propose to explore the automatic evaluation of the Quality of Alerts (QoA), including the indicativeness, precision, and handleability of alerts, as a future research direction that assists in the automatic detection of alerts’ anti-patterns. The ﬁndings of our study are valuable for optimizing cloud monitoring systems and improving the reliability of cloud services.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2022 52nd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	publisher = {IEEE},
	author = {Yang, Tianyi and Shen, Jiacheng and Su, Yuxin and Ren, Xiaoxue and Yang, Yongqiang and Lyu, Michael R.},
	month = jun,
	year = {2022},
	pages = {393--401},
}

@inproceedings{wang_how_2021,
	address = {Wuhan, China},
	title = {How {Long} {Will} it {Take} to {Mitigate} this {Incident} for {Online} {Service} {Systems}?},
	isbn = {978-1-66542-587-2},
	url = {https://ieeexplore.ieee.org/document/9700215/},
	doi = {10.1109/ISSRE52982.2021.00017},
	abstract = {Online service systems may encounter a large number of incidents, which should be mitigated as soon as possible to minimize the service disruption time and ensure high service availability. The ability to predict TTM (Time To Mitigation) of incidents can help service teams better organize the maintenance eﬀorts. Although there are many traditional bug-ﬁxing time prediction methods, we ﬁnd that there are not readily available for incident-TTM prediction due to the characteristics of incidents. To better understand how incidents are mitigated, we conduct the ﬁrst empirical study of incident TTM on 20 large-scale online service systems in Microsoft. We investigate the time distribution in the main stages of the incident life cycle and explore factors aﬀecting TTM. Based on our empirical ﬁndings, we propose TTMPred, a deep-learning-based approach for incident-TTM prediction in a continuous triage scenario. Our model designs a two-level attention-based bidirectional GRU model to capture both the semantic information in text data and the temporal information in incremental discussions. And based on a novel continuous loss function, it builds a regression model to achieve accurate TTM prediction as much as possible at each time point of prediction. Our experiments on four largescale online service systems in Microsoft show that TTMPred is eﬀective and signiﬁcantly outperforms the compared approaches. For example, TTMPred improves the state-of-the-art regressionbased approach by 25.66\% on average in terms of MAE (Mean Absolute Error).},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2021 {IEEE} 32nd {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Wang, Weijing and Chen, Junjie and Yang, Lin and Zhang, Hongyu and Zhao, Pu and Qiao, Bo and Kang, Yu and Lin, Qingwei and Rajmohan, Saravanakumar and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei},
	month = oct,
	year = {2021},
	pages = {36--46},
}

@inproceedings{li_actionable_2022,
	address = {Singapore Singapore},
	title = {Actionable and interpretable fault localization for recurring failures in online service systems},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549092},
	doi = {10.1145/3540250.3549092},
	abstract = {Fault localization is challenging in an online service system due to its monitoring data’s large volume and variety and complex dependencies across/within its components (e.g., services or databases). Furthermore, engineers require fault localization solutions to be actionable and interpretable, which existing research approaches cannot satisfy. Therefore, the common industry practice is that, for a specific online service system, its experienced engineers focus on localization for recurring failures based on the knowledge accumulated about the system and historical failures. More specifically, 1) they can identify the underlying root causes and take mitigation actions when pinpointing a group of indicative metrics on the faulty component; 2) their diagnosis knowledge is roughly based on how one failure might affect the components in the whole system.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Li, Zeyan and Zhao, Nengwen and Li, Mingjie and Lu, Xianglin and Wang, Lixin and Chang, Dongdong and Nie, Xiaohui and Cao, Li and Zhang, Wenchi and Sui, Kaixin and Wang, Yanhua and Du, Xu and Duan, Guoqiang and Pei, Dan},
	month = nov,
	year = {2022},
	pages = {996--1008},
}

@inproceedings{wu_identifying_2021,
	address = {Wuhan, China},
	title = {Identifying {Root}-{Cause} {Metrics} for {Incident} {Diagnosis} in {Online} {Service} {Systems}},
	isbn = {978-1-66542-587-2},
	url = {https://ieeexplore.ieee.org/document/9700253/},
	doi = {10.1109/ISSRE52982.2021.00022},
	abstract = {Incidents in online service systems could incur poor user experience and tremendous economic loss. To reduce the inﬂuence of incidents and guarantee service reliability, it is critical to identify root-cause metrics for engineers with clues to assist incident diagnosis. However, it is a challenging task due to the complicated dependencies and huge volume of various metrics in large-scale systems. Existing approaches are based on either anomaly detection or correlation analysis, performing not well in terms of accuracy or efﬁciency. To better understand the problem of root-cause metric identiﬁcation, we conduct a preliminary study based on real-world data analysis and interactions with engineers. The key observation is that root-cause metrics should satisfy two requirements. One is that the metric is expected to behave abnormally during the incident; the other is that the anomaly pattern should meet physical meaning and engineers’ demand. Motivated by the ﬁndings obtained from the study, we propose an effective approach named PatternMatcher to identifying root-cause metrics accurately. Speciﬁcally, PatternMatcher contains three steps, where coarse-grained anomaly detection aiming to ﬁlter out normal metrics, anomaly pattern classiﬁcation aiming to ﬁlter out unimportant anomaly patterns, and rootcause metric ranking. An extensive study on four real-world datasets including 113 incident cases from a large commercial bank demonstrates that PatternMatcher outperforms all baseline approaches, achieving top-3 average accuracy of 0.91. Moreover, we have deployed PatternMatcher in practice and shared some successful cases from real deployment.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2021 {IEEE} 32nd {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Wu, Canhua and Zhao, Nengwen and Wang, Lixin and Yang, Xiaoqin and Li, Shining and Zhang, Ming and Jin, Xing and Wen, Xidao and Nie, Xiaohui and Zhang, Wenchi and Sui, Kaixin and Pei, Dan},
	month = oct,
	year = {2021},
	pages = {91--102},
}

@article{wedel_aiops_2021,
	title = {{AIOps} – {A} {Systematic} {Literature} {Review}},
	abstract = {It is becoming increasingly complicated for IT operations to manage the vast amounts of data generated by modern IT systems. The use of Artificial Intelligence for IT operations (AIOps) counteracts this problem. This study aims to determine the state of the art of literature on AIOps. Therefore, I systematically evaluate the research findings in a multi-stage process. The results of 15 selected papers are compiled, analyzed, and thematically structured. The state of the studies is discussed with respect to the functions, implementations, tools, benefits, challenges, and trends of AIOps. The paper highlights anomalies and gaps in current literature. The results of this paper provide a synthesized overview for further research.},
	language = {en},
	journal = {Management in the Digital Age},
	author = {Wedel, FH},
	year = {2021},
}

@inproceedings{chen_towards_2020,
	address = {Virtual Event USA},
	title = {Towards intelligent incident management: why we need it and how we make it},
	isbn = {978-1-4503-7043-1},
	shorttitle = {Towards intelligent incident management},
	url = {https://dl.acm.org/doi/10.1145/3368089.3417055},
	doi = {10.1145/3368089.3417055},
	abstract = {The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Chen, Zhuangbin and Kang, Yu and Li, Liqun and Zhang, Xu and Zhang, Hongyu and Xu, Hui and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Dang, Yingnong and Gao, Feng and Zhao, Pu and Qiao, Bo and Lin, Qingwei and Zhang, Dongmei and Lyu, Michael R.},
	month = nov,
	year = {2020},
	pages = {1487--1497},
}

@article{tondel_information_2014,
	title = {Information security incident management: {Current} practice as reported in the literature},
	volume = {45},
	issn = {01674048},
	shorttitle = {Information security incident management},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404814000819},
	doi = {10.1016/j.cose.2014.05.003},
	abstract = {This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identiﬁed practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identiﬁed research needs within information security incident management.},
	language = {en},
	urldate = {2023-06-20},
	journal = {Computers \& Security},
	author = {Tøndel, Inger Anne and Line, Maria B. and Jaatun, Martin Gilje},
	month = sep,
	year = {2014},
	pages = {42--57},
}

@article{lehtinen_development_2011,
	title = {Development and evaluation of a lightweight root cause analysis method ({ARCA} method) – {Field} studies at four software companies},
	volume = {53},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584911001212},
	doi = {10.1016/j.infsof.2011.05.005},
	abstract = {Objective: This paper presents a lightweight RCA method, named the ARCA method, and its empirical evaluation. In the ARCA method, the target problem detection is based on a focus group meeting. This is in contrast to prior RCA methods, where the target problem detection is based on problem sampling, requiring heavy startup investments.
Method: The ARCA method was created with the framework of design science. We evaluated it through ﬁeld studies at four medium-sized software companies using interviews and query forms to collect feedback from the case attendees. A total of ﬁve key representatives of the companies were interviewed, and 30 case participants answered the query forms. The output of the ARCA method was also evaluated by the case attendees, i.e., a total 757 target problem causes and 124 related corrective actions.
Results: The case attendees considered the ARCA method useful and easy to use, which indicates that it is beneﬁcial for process improvement and problem prevention. In each case, 24–77 target problem root causes were processed and 13–40 corrective actions were developed. The effort of applying the method was 89 man-hours, on average.
Conclusion: The ARCA method required an acceptable level of effort and resulted in numerous high-quality corrective actions. In contrast to the current company practices, the method is an efﬁcient method to detect new process improvement opportunities and develop new process improvement ideas. Additionally, it is easy to use.},
	language = {en},
	number = {10},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Lehtinen, Timo O.A. and Mäntylä, Mika V. and Vanhanen, Jari},
	month = oct,
	year = {2011},
	pages = {1045--1061},
}

@inproceedings{krogstie2009using,
	title = {Using project wiki history to reflect on the project process},
	booktitle = {2009 42nd hawaii international conference on system sciences},
	publisher = {IEEE},
	author = {Krogstie, Birgit R},
	year = {2009},
	pages = {1--10},
}

@article{dingsoyr_what_2009,
	title = {What {Do} {We} {Know} about {Knowledge} {Management}? {Practical} {Implications} for {Software} {Engineering}},
	volume = {26},
	issn = {0740-7459, 1937-4194},
	shorttitle = {What {Do} {We} {Know} about {Knowledge} {Management}?},
	url = {https://ieeexplore.ieee.org/document/4814968/},
	doi = {10.1109/MS.2009.82},
	language = {en},
	number = {3},
	urldate = {2023-06-20},
	journal = {IEEE Software},
	author = {Dingsøyr, Torgeir and Bjørnson, Finn Olav and Shull, Forrest},
	month = may,
	year = {2009},
	pages = {100--103},
}

@article{amrit_human_2014,
	title = {Human factors in software development: {On} its underlying theories and the value of learning from related disciplines. {A} guest editorial introduction to the special issue},
	volume = {56},
	issn = {09505849},
	shorttitle = {Human factors in software development},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584914001608},
	doi = {10.1016/j.infsof.2014.07.006},
	language = {en},
	number = {12},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Amrit, Chintan and Daneva, Maya and Damian, Daniela},
	month = dec,
	year = {2014},
	pages = {1537--1542},
}

@incollection{abrahamsson_organizational_2007,
	address = {Berlin, Heidelberg},
	title = {Organizational {Learning} {Through} {Project} {Postmortem} {Reviews} – {An} {Explorative} {Case} {Study}},
	volume = {4764},
	isbn = {978-3-540-74765-9 978-3-540-75381-0},
	url = {http://link.springer.com/10.1007/978-3-540-75381-0_13},
	abstract = {A central issue in knowledge management and software process improvement is to learn from experience. In software engineering, most experience is gathered in projects, which makes project experience a prime source for learning. Many companies conduct postmortem reviews, but we have found few companies that analyze the outcome of several reviews to facilitate learning on an organizational level. This paper reports an explorative study of what we can learn from analyzing postmortem review reports of twelve projects in a medium-size software company.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Software {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dingsøyr, Torgeir and Moe, Nils Brede and Schalken, Joost and Stålhane, Tor},
	editor = {Abrahamsson, Pekka and Baddoo, Nathan and Margaria, Tiziana and Messnarz, Richard},
	year = {2007},
	doi = {10.1007/978-3-540-75381-0_13},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {136--147},
}

@article{dingsoyr_postmortem_2005,
	title = {Postmortem reviews: purpose and approaches in software engineering},
	volume = {47},
	issn = {09505849},
	shorttitle = {Postmortem reviews},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584904001296},
	doi = {10.1016/j.infsof.2004.08.008},
	abstract = {Conducting postmortems is a simple and practical method for organizational learning. Yet, not many companies have implemented such practices, and in a survey, few expressed satisfaction with how postmortems were conducted. In this article, we discuss the importance of postmortem reviews as a method for knowledge sharing in software projects, and give an overview of known such processes in the field of software engineering. In particular, we present three lightweight methods for conducting postmortems found in the literature, and discuss what criteria companies should use in defining their way of conducting postmortems.},
	language = {en},
	number = {5},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Dingsøyr, Torgeir},
	month = mar,
	year = {2005},
	pages = {293--303},
}

@article{savaryleblanc_software_2023,
	title = {Software assistants in software engineering: {A} systematic mapping study},
	volume = {53},
	issn = {0038-0644, 1097-024X},
	shorttitle = {Software assistants in software engineering},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spe.3170},
	doi = {10.1002/spe.3170},
	abstract = {The increasing essential complexity of software systems makes current software engineering methods and practices fall short in many occasions. Software assistants have the ability to help humans achieve a variety of tasks, including the development of software. Such assistants, which show human-like competences such as autonomy and intelligence, help software engineers do their job by empowering them with new knowledge. This article investigates the research efforts that have been conducted on the creation of assistants for software design, construction and maintenance paying special attention to the user-assistant interactions. To this end, we followed the standard systematic mapping study method to identify and classify relevant works in the state of the art. Out of the 7580 articles resulting from the automatic search, we identified 112 primary studies that present works which qualify as software assistants. We provide all the resources needed to reproduce our study. We report on the trends and goals of the assistants, the tasks they perform, how they interact with users, the technologies and mechanisms they exploit to embed intelligence and provide knowledge, and their level of automation. We propose a classification of software assistants based on interactions and present an analysis of the different automation patterns. As outcomes of our study, we provide a classification of software assistants dealing with the design, construction and maintenance phases of software development, we discuss the results, identify open lines of work and challenges and call for new innovative and rigorous research efforts in this field.},
	language = {en},
	number = {3},
	urldate = {2023-06-20},
	journal = {Software: Practice and Experience},
	author = {Savary‐Leblanc, Maxime and Burgueño, Lola and Cabot, Jordi and Le Pallec, Xavier and Gérard, Sébastien},
	month = mar,
	year = {2023},
	pages = {856--892},
}

@article{anquetil_software_2007,
	title = {Software maintenance seen as a knowledge management issue},
	volume = {49},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584906001029},
	doi = {10.1016/j.infsof.2006.07.007},
	language = {en},
	number = {5},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Anquetil, Nicolas and De Oliveira, Káthia M. and De Sousa, Kleiber D. and Batista Dias, Márcio G.},
	month = may,
	year = {2007},
	pages = {515--529},
}

@article{haamann_systematic_nodate,
	title = {{SYSTEMATIC} {APPROACHES} {FOR} {ORGANISATIONAL} {LEARNING} - {A} {LITERATURE} {REVIEW}},
	abstract = {The activity of developing high-quality information systems (IS) is a highly volatile and knowledgeintensive process. Nonetheless, only few IS developing companies seem to be advanced in evaluating and processing their knowledge. Despite a variety of existing approaches, there is no systematic overview of these and how far they support organisational learning. We conduct a systematic literature review of highly ranked journals and relevant textbooks to provide such an overview. We provide a list of eight systematic learning approaches and analyse how they contribute to the activities of knowledge creation, retention and transfer. Thereby, we aim to improve the current situation of organisational learning in IS developing companies. Whereas organisations need to become more open to systematic organisational learning approaches, research is in need to evaluate existing approaches and develop holistic strategies for building learning organisations.},
	language = {en},
	author = {Haamann, Thilo and Basten, Dirk},
}

@incollection{misra_technical_2019,
	address = {Cham},
	title = {Technical and {Managerial} {Difficulties} in {Postmortem} {Analysis} in {Software} {Projects}},
	volume = {11623},
	isbn = {978-3-030-24307-4 978-3-030-24308-1},
	url = {http://link.springer.com/10.1007/978-3-030-24308-1_5},
	abstract = {Software is successfully applied in a wide variety of areas. However, software projects have suﬀered from poor reputation by repeatedly bursting deadlines, costs or failing to fully meet user requirements. Postmortem Analysis is an activity to analyze what happened in projects in search of understanding the failures occurred and the achieved successes. Despite bringing interesting data for improving future projects, Postmortem Analysis is often neglected in organizations. This article seeks to identify and analyze the technical and managerial diﬃculties that exist in its accomplishment through bibliographical research. As a result, it is possible to conclude that the main diﬃculties for realizing postmortem activities are the shortage of time, lack of management support, conﬂicts between stakeholders, diﬃculty in extracting and collecting data, lack of agreement regarding evaluation criteria, lack of standards for achievement, and lack of useful or eﬃcient historical data.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2019},
	publisher = {Springer International Publishing},
	author = {Vieira, Felipe J. R. and Oliveira, Manoela R. and Do Nascimento, Rogério P. C. and Soares, Michel S.},
	editor = {Misra, Sanjay and Gervasi, Osvaldo and Murgante, Beniamino and Stankova, Elena and Korkhov, Vladimir and Torre, Carmelo and Rocha, Ana Maria A.C. and Taniar, David and Apduhan, Bernady O. and Tarantino, Eufemia},
	year = {2019},
	doi = {10.1007/978-3-030-24308-1_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {59--69},
}

@article{dyba_reflective_2014,
	title = {The {Reflective} {Software} {Engineer}: {Reflective} {Practice}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	shorttitle = {The {Reflective} {Software} {Engineer}},
	url = {https://ieeexplore.ieee.org/document/6834681/},
	doi = {10.1109/MS.2014.97},
	language = {en},
	number = {4},
	urldate = {2023-06-20},
	journal = {IEEE Software},
	author = {Dybå, Tore and Maiden, Neil and Glass, Robert},
	month = jul,
	year = {2014},
	pages = {32--36},
}

@incollection{hutchison_trends_2005,
	address = {Berlin, Heidelberg},
	title = {Trends in {Learning} {Software} {Organizations}: {Current} {Needs} and {Future} {Solutions}},
	volume = {3782},
	isbn = {978-3-540-30465-4 978-3-540-31620-6},
	shorttitle = {Trends in {Learning} {Software} {Organizations}},
	url = {http://link.springer.com/10.1007/11590019_7},
	abstract = {The 7th learning software organizations workshop focused on interdisciplinary research on several aspects of learning: From personal competence development to cultural and technological frameworks for organization-wide knowledge-sharing (“knowledge management in software engineering”). We put special emphasis on experience reports and empirical work.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Professional {Knowledge} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Birk, Andreas and Dingsøyr, Torgeir},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Althoff, Klaus-Dieter and Dengel, Andreas and Bergmann, Ralph and Nick, Markus and Roth-Berghofer, Thomas},
	year = {2005},
	doi = {10.1007/11590019_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {70--75},
}

@article{jorgensen_impact_2009,
	title = {The {Impact} of {Lessons}-{Learned} {Sessions} on {Effort} {Estimation} and {Uncertainty} {Assessments}},
	volume = {35},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/4752843/},
	doi = {10.1109/TSE.2009.2},
	abstract = {Inaccurate estimates of software development effort is a frequently reported cause of IT-project failures. We report results from a study that investigated the effect of introducing lessons-learned sessions on estimation accuracy and the assessment of uncertainty. Twenty software professionals were randomly allocated to a Learning group or a Control group and instructed to estimate and complete the same five development tasks. Those in the Learning group but not those in the Control group were instructed to spend at least 30 minutes on identifying, analyzing, and summarizing their effort estimation and uncertainty assessment experience after completing each task. We found that the estimation accuracy and the realism of the uncertainty assessment were not better in the Learning group than in the Control group. A follow-up study with 83 software professionals was completed to better understand this lack of improvement from lessons-learned sessions. The follow-up study found that receiving feedback about other software professionals’ estimation performance led to more realistic uncertainty assessments than receiving the same feedback of one’s own estimates. Lessons-learned sessions, not only in estimation contexts, have to be carefully designed to avoid wasting resources on learning processes that stimulate rather than reduce learning biases.},
	language = {en},
	number = {3},
	urldate = {2023-06-20},
	journal = {IEEE Transactions on Software Engineering},
	author = {Jorgensen, M. and Gruschke, T.M.},
	month = may,
	year = {2009},
	pages = {368--383},
}

@article{tsai_programmer_2010,
	title = {Programmer perceptions of knowledge-sharing behavior under social cognitive theory},
	volume = {37},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417410004409},
	doi = {10.1016/j.eswa.2010.05.029},
	abstract = {Despite the importance of the software industry, little research using social cognitive perspective has focused on the software industry. This study thus examines key factors, including self-efﬁcacy, expectancy theory and organizational climate, on the software workers to intent to share knowledge, using a social cognitive framework. Programmers and software workers in Taiwan were surveyed to test the proposed research model. Conﬁrmatory factor analysis (CFA) and structural equation modeling (SEM) technique were used to analyze the data and evaluate the research model. Results showed that that the research model ﬁt the data well and the main determinants of knowledge-sharing behavior were the encouraging intentions of knowledge intensive workers. We conﬁrm our hypothesis that knowledge sharing self-efﬁcacy and outcome expectancy, as well as organizational climate, will affect individual intentions to share knowledge. Additionally, organizational climate and perceived managerial incentive were found to positively encourage knowledge-sharing behavior. Research and practical implications are described.},
	language = {en},
	number = {12},
	urldate = {2023-06-20},
	journal = {Expert Systems with Applications},
	author = {Tsai, Ming-Ten and Cheng, Nai-Chang},
	month = dec,
	year = {2010},
	pages = {8479--8485},
}

@article{bjornson_improving_2009,
	title = {Improving the effectiveness of root cause analysis in post mortem analysis: {A} controlled experiment},
	volume = {51},
	issn = {09505849},
	shorttitle = {Improving the effectiveness of root cause analysis in post mortem analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095058490800030X},
	doi = {10.1016/j.infsof.2008.02.003},
	abstract = {Retrospective analysis is a way to share knowledge following the completion of a project or major milestone. However, in the busy workday of a software project, there is rarely time for such reviews and there is a need for eﬀective methods that will yield good results quickly without the need for external consultants or experts. Building on an existing method for retrospective analysis and theories of group involvement, we propose improvements to the root cause analysis phase of a lightweight retrospective analysis method known as post mortem analysis (PMA). In particular, to facilitate brainstorming during the root cause analysis phase of the PMA, we propose certain processual changes to facilitate more active individual participation and the use of less rigidly structured diagrams. We conducted a controlled experiment to compare this new variation of the method with the existing one, and conclude that in our setting of small software teams with no access to an experienced facilitator, the new variation is more eﬀective when it comes to identifying possible root causes of problems and successes. The modiﬁed method also produced more speciﬁc starting points for improving the software development process.},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Bjørnson, Finn Olav and Wang, Alf Inge and Arisholm, Erik},
	month = jan,
	year = {2009},
	pages = {150--161},
}

@article{aurum_investigating_2008,
	title = {Investigating {Knowledge} {Management} practices in software development organisations – {An} {Australian} experience},
	volume = {50},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584907000602},
	doi = {10.1016/j.infsof.2007.05.005},
	abstract = {This study, using both quantitative and qualitative methods, investigates current practice of Knowledge Management (KM) in Software Engineering (SE) processes in two Australian companies on the basis that they both claimed to apply KM practices in their software development work. It also describes the KM activities and KM process used in SE practice, and examines the enablers of KM process for SE in terms of leadership, technology, culture, process and measurement.},
	language = {en},
	number = {6},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Aurum, Aybüke and Daneshgar, Farhad and Ward, James},
	month = may,
	year = {2008},
	pages = {511--533},
}

@inproceedings{chau_knowledge_2003,
	address = {Linz, Austria},
	title = {Knowledge sharing: agile methods vs. {Tayloristic} methods},
	isbn = {978-0-7695-1963-0},
	shorttitle = {Knowledge sharing},
	url = {http://ieeexplore.ieee.org/document/1231427/},
	doi = {10.1109/ENABL.2003.1231427},
	abstract = {This paper presents a comparative analysis of knowledge sharing approaches of agile and Tayloristic (traditional) software development teams. Issues of knowledge creation, knowledge conversion and transfer, continuous learning, competence management and team composition are discussed. Experience repositories and other tools for knowledge dissemination are examined.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {{WET} {ICE} 2003. {Proceedings}. {Twelfth} {IEEE} {International} {Workshops} on {Enabling} {Technologies}: {Infrastructure} for {Collaborative} {Enterprises}, 2003.},
	publisher = {IEEE Comput. Soc},
	author = {Chau, T. and Maurer, F. and Melnik, G.},
	year = {2003},
	pages = {302--307},
}

@article{chua_knowledge_2008,
	title = {Knowledge transfer and organizational learning in {IS} offshore sourcing},
	volume = {36},
	issn = {03050483},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305048306001009},
	doi = {10.1016/j.omega.2006.06.008},
	abstract = {Offshore sourcing is the trend where companies look for cheaper offshore resource options to reduce their baseline costs. This involves the retrenchment of more expensive onshore resources to be replaced with cheaper offshore resources. A key activity is the transfer of knowledge from the onshore resources to the offshore resources. This paper is written from an organizational learning perspective, looking at how a global IS department in a multinational bank went about transferring its business application support and development experiences to another insourced location. Speciﬁcally, we examine how knowledge is transferred for the ﬁve IS body of knowledge (BOK) areas, namely, technology, application domain, IS application, organizational and IS development process knowledge. We ﬁnd that whilst some areas of the IS BOK are easily grafted, some require intense vicarious and experiential learning using rich media, whilst others are more difﬁcult to transfer. The ﬁndings extend the literature on knowledge transfer and organizational learning in the context of the IS BOK.},
	language = {en},
	number = {2},
	urldate = {2023-06-20},
	journal = {Omega},
	author = {Chua, A and Pan, S},
	month = apr,
	year = {2008},
	pages = {267--281},
}

@article{reed_maps_nodate,
	title = {Maps, {Context}, and {Tribal} {Knowledge}: {On} the {Structure} and {Use} of {Post}-{Incident} {Analysis} {Artifacts} in {Software} {Development} and {Operations}},
	language = {en},
	author = {Reed, J Paul},
}

@article{lehtinen_recurring_2017,
	title = {Recurring opinions or productive improvements—what agile teams actually discuss in retrospectives},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-016-9464-2},
	doi = {10.1007/s10664-016-9464-2},
	abstract = {Team-level retrospectives are widely used in agile and lean software development, yet little is known about what is actually discussed during retrospectives or their outcomes. In this paper, we synthesise the outcomes of sprint retrospectives in a large, distributed, agile software development organisation. This longitudinal case study analyses data from 37 teamlevel retrospectives for almost 3 years. We report the outcomes of the retrospectives, their perceived importance for process improvement and relatVed action proposals. Most discussions were related to topics close to and controllable by the team. However, the discussions might suffer from participant bias, and in cases where they are not supported by hard evidence, they might not reflect reality, but rather the sometimes strong opinions of the participants. Some discussions were related to topics that could not be resolved at the team level due to their complexity. Certain topics recurred over a long period of time, either reflecting issues that can and have been solved previously, but that recur naturally as development proceeds, or reflecting waste since they cannot be resolved or improved on by the team due to a lack of controllability or their complexity. For example, the discussion on estimation accuracy did not reflect the true situation and improving the estimates was complicated. On the other hand, discussions on the high number of known bugs recurred despite effective improvements as development proceeded.},
	language = {en},
	number = {5},
	urldate = {2023-06-20},
	journal = {Empirical Software Engineering},
	author = {Lehtinen, Timo O. A. and Itkonen, Juha and Lassenius, Casper},
	month = oct,
	year = {2017},
	pages = {2409--2452},
}

@article{dingsoyr_managing_2014,
	title = {Managing {Knowledge} in {Global} {Software} {Development} {Projects}},
	volume = {16},
	issn = {1520-9202},
	url = {http://ieeexplore.ieee.org/document/6471710/},
	doi = {10.1109/MITP.2013.19},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {IT Professional},
	author = {Dingsoyr, Torgeir and Smite, Darja},
	month = jan,
	year = {2014},
	pages = {22--29},
}

@inproceedings{dorairaj_knowledge_2012,
	address = {Dallas, TX, USA},
	title = {Knowledge {Management} in {Distributed} {Agile} {Software} {Development}},
	isbn = {978-1-4673-2622-3 978-0-7695-4804-3},
	url = {http://ieeexplore.ieee.org/document/6298093/},
	doi = {10.1109/Agile.2012.17},
	abstract = {Software development teams need highly valuable knowledge to carry out knowledge-intensive development activities. Agile teams are cross-functional teams that promote sharing of project-speciﬁc knowledge through frequent faceto-face interaction, effective communication and customer collaboration. Knowledge sharing is difﬁcult for distributed Agile teams due to spatial, temporal, and cultural barriers, which negatively affect face-to-face interaction, communication and collaboration. There seems to be very few studies that focus on knowledge management in distributed Agile teams. Through a Grounded Theory study that involved 45 participants from 28 different software companies in the USA, India and Australia, we investigate distributed software development from the speciﬁc perspective of Agile teams. In this paper, we describe how Agile teams gather, store, share and use knowledge in distributed software development.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2012 {Agile} {Conference}},
	publisher = {IEEE},
	author = {Dorairaj, Siva and Noble, James and Malik, Petra},
	month = aug,
	year = {2012},
	pages = {64--73},
}

@article{dingsoyr_key_2019,
	title = {Key {Lessons} {From} {Tailoring} {Agile} {Methods} for {Large}-{Scale} {Software} {Development}},
	volume = {21},
	issn = {1520-9202, 1941-045X},
	url = {https://ieeexplore.ieee.org/document/8657809/},
	doi = {10.1109/MITP.2018.2876984},
	abstract = {The authors provide advice from one of the largest development programs in Norway, where 12 scrum teams combined agile practices with traditional project management. The Perform program delivered 12 releases over a four-year period, ﬁnishing on budget and on time. The authors summarize 10 key lessons on ﬁve crucial topics that are relevant to other large development projects seeking to combine scrum with traditional project management.},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {IT Professional},
	author = {Dingsoyr, Torgeir and Dyba, Tore and Gjertsen, Mette and Jacobsen, Anette Odgaard and Mathisen, Tor-Erik and Nordfjord, Jan Ole and Roe, Kjetil and Strand, Kjetil},
	month = jan,
	year = {2019},
	pages = {34--41},
}

@article{rus_knowledge_2002,
	title = {Knowledge management in software engineering},
	volume = {19},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/1003450/},
	doi = {10.1109/MS.2002.1003450},
	language = {en},
	number = {3},
	urldate = {2023-06-20},
	journal = {IEEE Software},
	author = {Rus, I. and Lindvall, M.},
	month = may,
	year = {2002},
	pages = {26--38},
}

@inproceedings{ward_knowledge_2004,
	address = {Melbourne, Vic., Australia},
	title = {Knowledge management in software engineering - describing the process},
	isbn = {978-0-7695-2089-6},
	url = {http://ieeexplore.ieee.org/document/1290466/},
	doi = {10.1109/ASWEC.2004.1290466},
	abstract = {The management of knowledge and experience are key means by which systematic software development and process improvement occur. Within the domain of Software Engineering (SE), quality continues to remain an issue of concern. Although remedies such as fourth generation programming languages, structured techniques and object-oriented technology have been promoted, a “silver bullet” has yet to be found. Knowledge Management (KM) gives organisations the opportunity to appreciate the challenges and complexities inherent in software development. This paper reports on two case studies that investigate KM in SE at two IT organisations. Structured interviews were conducted, with the assistance of a qualitative questionnaire. The results were used to describe current practices for KM in SE, to investigate the nature of KM activities in these organisations, and to explain the impact of leadership, technology, culture and measurement as enablers of the KM process for SE.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2004 {Australian} {Software} {Engineering} {Conference}. {Proceedings}.},
	publisher = {IEEE},
	author = {Ward, J. and Aurum, A.},
	year = {2004},
	pages = {137--146},
}

@article{ouriques_knowledge_2019,
	title = {Knowledge {Management} {Strategies} and {Processes} in {Agile} {Software} {Development}: {A} {Systematic} {Literature} {Review}},
	volume = {29},
	issn = {0218-1940, 1793-6403},
	shorttitle = {Knowledge {Management} {Strategies} and {Processes} in {Agile} {Software} {Development}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218194019500153},
	doi = {10.1142/S0218194019500153},
	abstract = {Knowledge-intensive companies that adopt Agile Software Development (ASD) rely on efficient implementation of Knowledge Management (KM) strategies to promote different Knowledge Processes (KPs) to gain competitive advantage. This study aims to explore how companies that adopt ASD implement KM strategies utilizing practices that promote the KPs in the different organizational layers. Through a systematic literature review, we analyzed 32 primary studies, selected by automated search and snowballing in the extant literature. To analyze the data, we applied narrative synthesis. Most of the identified KM practices implement personalization strategies (81\%), supported by codification (19\%). Our review shows that the primary studies do not report KM practices in the strategic layer and two of them in the product portfolio layer; on the other hand, in the project layer, the studies report 33 practices that implement personalization strategy, and seven practices that implement codification. KM strategies in ASD promote mainly the knowledge transfer process with practices that stimulates social interaction to share tacit knowledge in the project layer. As a result of using informal communication, a significant amount of knowledge can be lost or not properly transferred to other individuals and, instead of propagating the knowledge, it remains inside a few individuals’ minds.},
	language = {en},
	number = {03},
	urldate = {2023-06-20},
	journal = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Ouriques, Raquel Andrade Barros and Wnuk, Krzysztof and Gorschek, Tony and Svensson, Richard Berntsson},
	month = mar,
	year = {2019},
	pages = {345--380},
}

@book{schwartz_encyclopedia_2006,
	address = {Hershey, PA},
	title = {Encyclopedia of knowledge management},
	isbn = {978-1-59140-573-3 978-1-59140-574-0},
	abstract = {"This encyclopedia is a research reference work documenting the past, present, and possible future directions of knowledge management"--Provided by publisher},
	language = {en},
	publisher = {Idea Group Reference},
	editor = {Schwartz, David G.},
	year = {2006},
	note = {OCLC: ocm60393529},
	keywords = {Information networks, Information resources management, Knowledge management, Management, Organizational learning},
}

@article{lehtinen_tool_2014,
	title = {A tool supporting root cause analysis for synchronous retrospectives in distributed software teams},
	volume = {56},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584914000159},
	doi = {10.1016/j.infsof.2014.01.004},
	abstract = {Objective: This paper presents a real-time cloud-based software tool (ARCA-tool) we developed to support RCA in distributed teams and its initial empirical evaluation. The feasibility of using RCA with distributed teams is also evaluated.
Method: We compared our tool with 35 existing RCA software tools. We conducted field studies of four distributed agile software teams at two international software product companies. The teams conducted RCA collaboratively in synchronous retrospective meetings by using the tool we developed. We collected the data using observations, interviews and questionnaires.
Results: Comparison revealed that none of the existing 35 tools matched all the features of our ARCA-tool. The team members found ARCA-tool to be an essential part of their distributed retrospectives. They considered the software as efficient and very easy to learn and use. Additionally, the team members perceived RCA to be a vital part of the retrospectives. In contrast to the prior retrospective practices of the teams, the introduced RCA method was evaluated as efficient and easy to use.
Conclusion: RCA is a useful practice in synchronous distributed retrospectives. However, it requires software tool support for enabling real-time view and co-creation of a cause-effect diagram. ARCA-tool supports synchronous RCA, and includes support for logging problems and causes, problem prioritization, cause-effect diagramming, and logging of process improvement proposals. It enables conducting RCA in distributed retrospectives.},
	language = {en},
	number = {4},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Lehtinen, Timo O.A. and Virtanen, Risto and Viljanen, Juha O. and Mäntylä, Mika V. and Lassenius, Casper},
	month = apr,
	year = {2014},
	pages = {408--437},
}

@article{wohlin_general_2015,
	title = {A general theory of software engineering: {Balancing} human, social and organizational capitals},
	volume = {109},
	issn = {01641212},
	shorttitle = {A general theory of software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121215001740},
	doi = {10.1016/j.jss.2015.08.009},
	abstract = {There exists no generally accepted theory in software engineering, and at the same time a scientiﬁc discipline needs theories. Some laws, hypotheses and conjectures exist, but yet no generally accepted theory. Several researchers and initiatives emphasize the need for theory in the discipline. The objective of this paper is to formulate a theory of software engineering. The theory is generated from empirical observations of industry practice, including several case studies and many years of experience in working closely between academia and industry. The theory captures the balancing of three different intellectual capitals: human, social and organizational capitals, respectively. The theory is formulated using a method for building theories in software engineering. It results in a theory where the relationships between the three different intellectual capitals are explored and explained. The theory is illustrated based on an industrial case study, where it is shown how decisions made in industry practice are explainable with the formulated theory, and the consequences of the decisions are made explicit. Based on the positive results, it is concluded that the theory may have a good explanatory power, although more evaluations are needed.},
	language = {en},
	urldate = {2023-06-20},
	journal = {Journal of Systems and Software},
	author = {Wohlin, Claes and Šmite, Darja and Moe, Nils Brede},
	month = nov,
	year = {2015},
	pages = {229--242},
}

@article{narayanan_matter_2009,
	title = {A {Matter} of {Balance}: {Specialization}, {Task} {Variety}, and {Individual} {Learning} in a {Software} {Maintenance} {Environment}},
	volume = {55},
	issn = {0025-1909, 1526-5501},
	shorttitle = {A {Matter} of {Balance}},
	url = {https://pubsonline.informs.org/doi/10.1287/mnsc.1090.1057},
	doi = {10.1287/mnsc.1090.1057},
	abstract = {Specialization at work has been recognized as a key driver of learning and productivity since the days of Adam Smith. More recently, researchers have noted that exposure to task variety can enhance learning. We examine how exposure to specialization and variety jointly drive employee productivity in a real-life setting. We analyze a data set covering 88 individuals who worked on 5,711 maintenance tasks in an offshore software support services operation. We find that, as expected, specialization enhances productivity. However, exposure to variety has a nonlinear influence on productivity; i.e., “too much variety” can impede learning. We also find that achieving a proper balance between specialization and exposure to a variety leads to the highest productivity. We capture this balance using an adaptation of the Herfindahl-Hirschman Index from the economics literature. In addition, we examine how the productivity of individuals in a workgroup is affected by member entry and exit, with the latter specified in terms of the degree of specialized experience and the degree of variety experience lost from the workgroup when a member exits. Our analysis reveals that the degree of variety experience lost has a greater impact on productivity than the degree of specialized experience that is lost.},
	language = {en},
	number = {11},
	urldate = {2023-06-20},
	journal = {Management Science},
	author = {Narayanan, Sriram and Balasubramanian, Sridhar and Swaminathan, Jayashankar M.},
	month = nov,
	year = {2009},
	pages = {1861--1876},
}

@inproceedings{vasanthapriyan_survey_2015,
	address = {Vancouver, BC, Canada},
	title = {A {Survey} on {Knowledge} {Management} in {Software} {Engineering}},
	isbn = {978-1-4673-9598-4},
	url = {http://ieeexplore.ieee.org/document/7322153/},
	doi = {10.1109/QRS-C.2015.48},
	abstract = {Software development is a knowledge intensive and collaborative activity. The success of the project totally depends on knowledge and experience of the developers. Increasing knowledge creation and sharing among software engineers are uphill tasks in software development environments. The field of knowledge management has emerged into this field to improve the productivity of the software by effective and efficient knowledge creation, sharing and transferring. In other words, knowledge management for software engineering aims at facilitating knowledge flow and utilization across every phases of a software engineering process. Therefore, adaptation of various knowledge management practices by software engineering organizations is essential. This survey identified the knowledge management involvement in software engineering in different perspectives in the recent literature and guide future research in this area.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} - {Companion}},
	publisher = {IEEE},
	author = {Vasanthapriyan, Shanmuganathan and Tian, Jing and Xiang, Jianwen},
	month = aug,
	year = {2015},
	pages = {237--244},
}

@article{schalken_method_2006,
	title = {A method to draw lessons from project postmortem databases},
	volume = {11},
	issn = {1077-4866, 1099-1670},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spip.251},
	doi = {10.1002/spip.251},
	language = {en},
	number = {1},
	urldate = {2023-06-20},
	journal = {Software Process: Improvement and Practice},
	author = {Schalken, Joost and Brinkkemper, Sjaak and Van Vliet, Hans},
	month = jan,
	year = {2006},
	pages = {35--46},
}

@article{rashid_systematic_2019,
	title = {A systematic examination of knowledge loss in open source software projects},
	volume = {46},
	issn = {02684012},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0268401217310095},
	doi = {10.1016/j.ijinfomgt.2018.11.015},
	abstract = {Objective: The objective of this work is to deeply and systematically investigate the phenomenon of knowledge loss due to contributor turnover in OSS projects as presented in the state-of-the-art literature and to synthesise the information presented on the topic. Furthermore, based on the learning arising from our investigation it is our intention to identify mechanisms to reduce the overall effects of knowledge loss in OSS projects.
Methodology: We use the snowballing methodology to identify the relevant literature on knowledge loss due to contributor turnover in OSS projects. This robust methodology for a literature review includes research question, search strategy, inclusion, exclusion, quality criteria, and data synthesis. The search strategy, and inclusion, exclusions and quality criteria are applied as a part of snowballing procedure. Snowballing is considered an efficient and reliable way to conduct a systematic literature review, providing a robust alternative to mechanically searching individual databases for given topics.
Result: Knowledge sharing in OSS projects is abundant but there is no evidence of a formal strategy or practice to manage knowledge. Due to the dynamic and diverse nature of OSS projects, knowledge management is considered a challenging task and there is a need for a proactive mechanism to share knowledge in the OSS community for knowledge to be reused in the future by the OSS project contributors. From the collection of papers found using snowballing, we consolidated various themes on knowledge loss due to contributor turnover in OSS projects and identified 11 impacts due to knowledge loss in OSS projects, and 10 mitigations to manage with knowledge loss in OSS projects.
Conclusion: In this paper, we propose future research directions to investigate integration of proactive knowledge retention practices with the existing OSS practices to reduce the current knowledge loss problem. We suggest that there is insufficient attention paid to KM in general in OSS, in particular there would appear to an absence of proactive measures to reduce the potential impact of knowledge loss. We also propose the need for a KM evaluation metric in OSS projects, similar to the ones that evaluate health of online communities, which should help to inform potential consumers of the OSS of the KM status on a project, something that is not existent today.},
	language = {en},
	urldate = {2023-06-20},
	journal = {International Journal of Information Management},
	author = {Rashid, Mehvish and Clarke, Paul M. and O’Connor, Rory V.},
	month = jun,
	year = {2019},
	pages = {104--123},
}

@article{tiwana_empirical_2004,
	title = {An empirical study of the effect of knowledge integration on software development performance},
	volume = {46},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584904000618},
	doi = {10.1016/j.infsof.2004.03.006},
	abstract = {Although the role of integrating application domain knowledge with technical knowledge is implicitly recognized in software engineering practice, no large scale study has attempted to validate this empirically in a ﬁeld setting. In this paper, a large-scale empirical study of 232 software development projects in 232 software development organizations shows that higher integration of business application domain knowledge with technical knowledge during the software development process increases software development effectiveness, reduces defect density throughout the development trajectory, lowers warranty defects, and increases software development efﬁciency. The ﬁndings highlight the inﬂuence of knowledge integration on various dimensions of software development performance.},
	language = {en},
	number = {13},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Tiwana, Amrit},
	month = oct,
	year = {2004},
	pages = {899--906},
}

@article{alahyari_exploratory_2019,
	title = {An exploratory study of waste in software development organizations using agile or lean approaches: {A} multiple case study at 14 organizations},
	volume = {105},
	issn = {09505849},
	shorttitle = {An exploratory study of waste in software development organizations using agile or lean approaches},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095058491830171X},
	doi = {10.1016/j.infsof.2018.08.006},
	abstract = {Objective: This paper explores the concept of waste in agile/lean software development organizations and how it is deﬁned, used, prioritized, reduced, or eliminated in practice
Method: The data were collected using semi-structured open-interviews. 23 practitioners from 14 embedded software development organizations were interviewed representing two core roles in each organization.
Results: Various wastes, categorized in 10 diﬀerent categories, were identiﬁed by the respondents. From the mentioned wastes, not all were necessarily waste per se but could be symptoms caused by wastes. From the seven wastes of lean, Task-switching was ranked as the most important, and Extra-features, as the least important wastes according to the respondents’ opinion. However, most companies do not have their own or use an established deﬁnition of waste, more importantly, very few actively identify or try to eliminate waste in their organizations beyond local initiatives on project level.
Conclusion: In order to identify, recognize and eliminate waste, a common understanding, and a joint and holistic view of the concept is needed. It is also important to optimize the whole organization and the whole product, as waste on one level can be important on another, thus sub-optimization should be avoided. Furthermore, to achieve a sustainable and eﬀective waste handling, both the short-term and the long-term perspectives need to be considered.},
	language = {en},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Alahyari, Hiva and Gorschek, Tony and Berntsson Svensson, Richard},
	month = jan,
	year = {2019},
	pages = {78--94},
}

@article{li_application_2013,
	title = {Application of knowledge-based approaches in software architecture: {A} systematic mapping study},
	volume = {55},
	issn = {09505849},
	shorttitle = {Application of knowledge-based approaches in software architecture},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584912002315},
	doi = {10.1016/j.infsof.2012.11.005},
	abstract = {Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classiﬁcation and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions.
Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011.
Results: Fifty-ﬁve studies were selected and classiﬁed according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation.
Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which ‘‘Embedded software’’ has received the most attention.},
	language = {en},
	number = {5},
	urldate = {2023-06-20},
	journal = {Information and Software Technology},
	author = {Li, Zengyang and Liang, Peng and Avgeriou, Paris},
	month = may,
	year = {2013},
	pages = {777--794},
}

@inproceedings{souza_da_silva_filho_applying_2015,
	address = {Barcelona, Spain},
	title = {Applying {Knowledge} {Codification} in a {Post}-mortem {Process} - {A} {Practical} {Experience}:},
	isbn = {978-989-758-096-3 978-989-758-097-0 978-989-758-098-7},
	shorttitle = {Applying {Knowledge} {Codification} in a {Post}-mortem {Process} - {A} {Practical} {Experience}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005376301530165},
	doi = {10.5220/0005376301530165},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Enterprise} {Information} {Systems}},
	publisher = {SCITEPRESS - Science and and Technology Publications},
	author = {Souza Da Silva Filho, Erivan and Viana, Davi and Conte, Tayana},
	year = {2015},
	pages = {153--165},
}

@article{basten_approaches_2018,
	title = {Approaches for {Organizational} {Learning}: {A} {Literature} {Review}},
	volume = {8},
	issn = {2158-2440, 2158-2440},
	shorttitle = {Approaches for {Organizational} {Learning}},
	url = {http://journals.sagepub.com/doi/10.1177/2158244018794224},
	doi = {10.1177/2158244018794224},
	abstract = {Organizational learning (OL) enables organizations to transform individual knowledge into organizational knowledge. Organizations struggle to implement practical approaches due to the lack of concrete prescriptions. We performed a literature review to identify OL approaches and linked these approaches to OL theories. We synthesized 18 OL approaches across three domains: people (seven approaches), processes (nine), and technologies (two). Furthermore, we suggest two directions for future research: referring to the evaluation of our results and addressing the contingencies of OL effectiveness. Our mapping guides organizations in the design of learning processes to improve long-term performance. Although relying on a single approach is unlikely to comprehensively enable OL, our mapping facilitates the combination of several approaches aligned with organizational culture and processes.},
	language = {en},
	number = {3},
	urldate = {2023-06-20},
	journal = {SAGE Open},
	author = {Basten, Dirk and Haamann, Thilo},
	month = apr,
	year = {2018},
	pages = {215824401879422},
}

@article{desouza_experiences_2005,
	title = {Experiences with conducting project postmortems: reports versus stories},
	volume = {10},
	issn = {1077-4866, 1099-1670},
	shorttitle = {Experiences with conducting project postmortems},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spip.224},
	doi = {10.1002/spip.224},
	language = {en},
	number = {2},
	urldate = {2023-06-20},
	journal = {Software Process: Improvement and Practice},
	author = {Desouza, Kevin C. and Dingsøyr, Torgeir and Awazu, Yukika},
	month = apr,
	year = {2005},
	pages = {203--215},
}

@inproceedings{desouza_experiences_2005-1,
	address = {Big Island, HI, USA},
	title = {Experiences with {Conducting} {Project} {Postmortems}: {Reports} vs. {Stories} and {Practitioner} {Perspective}},
	isbn = {978-0-7695-2268-5},
	shorttitle = {Experiences with {Conducting} {Project} {Postmortems}},
	url = {http://ieeexplore.ieee.org/document/1385717/},
	doi = {10.1109/HICSS.2005.256},
	abstract = {The most popular unit of work in organizations is a project. Managing knowledge in and about projects is salient for successful project management. Explicit knowledge is easier to manage than tacit knowledge as it is an outcome of work. Tacit knowledge is abstract and is managed in a cursory mode in projects. In this paper, we will discuss how postmortems can be used to capture tacit experiences in projects. Conducting a postmortem, either after a milestone or at the end of a project, is salient in order to gauge what has been learnt, what were the main issues faced, and what can be used to improve the processes of work in the future. The conducting of postmortems aids in articulation of tacit experiences into explicit forms, this enables for experiences to be better reused in the future. Re-using of postmortem findings depends heavily on the nature of the postmortem outcome. We will compare two kinds of postmortem outcomes –traditional reports and stories. Management must choose the right kind of postmortem report to calibrate depending on the project and learning outcomes. We also highlight lessons learnt from conducting postmortem reviews in several software organizations.},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Proceedings of the 38th {Annual} {Hawaii} {International} {Conference} on {System} {Sciences}},
	publisher = {IEEE},
	author = {Desouza, K.C. and Dingsoyr, T. and Awazu, Y.},
	year = {2005},
	pages = {233c--233c},
}

@book{garbajosa_agile_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}: 19th {International} {Conference}, {XP} 2018, {Porto}, {Portugal}, {May} 21–25, 2018, {Proceedings}},
	volume = {314},
	isbn = {978-3-319-91601-9 978-3-319-91602-6},
	shorttitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	url = {http://link.springer.com/10.1007/978-3-319-91602-6},
	language = {en},
	urldate = {2023-06-20},
	publisher = {Springer International Publishing},
	editor = {Garbajosa, Juan and Wang, Xiaofeng and Aguiar, Ademar},
	year = {2018},
	doi = {10.1007/978-3-319-91602-6},
}

@book{henninger_advances_2003,
	address = {Berlin : New York},
	series = {Lecture notes in computer science},
	title = {Advances in learning software organizations: 4th {International} {Workshop}, {LSO} 2002, {Chicago}, {IL}, {USA}, {August} 6, 2002: revised papers},
	isbn = {978-3-540-20591-3},
	shorttitle = {Advances in learning software organizations},
	language = {en},
	number = {2640},
	publisher = {Springer},
	editor = {Henninger, Scott and Maurer, F.},
	year = {2003},
	note = {Meeting Name: LSO 2002},
	keywords = {Congresses, Expert systems (Computer science), Software engineering},
}

@techreport{noauthor_securing_nodate,
	title = {Securing the {Software} {Supply} {Chain}: {Recommended} {Practices} {Guide} for {Developers}},
	language = {en},
}

@techreport{european_union_agency_for_cybersecurity_good_2023,
	title = {Good {Practices} for {Supply} {Chain} {Cybersecurity}},
	language = {en},
	institution = {European Union Agency for Cybersecurity.},
	author = {European Union Agency for Cybersecurity},
	year = {2023},
}

@techreport{european_union_agency_for_cybersecurity_enisa_2022,
	address = {LU},
	title = {{ENISA} threat landscape 2022},
	shorttitle = {{ENISA} threat landscape 2022},
	url = {https://data.europa.eu/doi/10.2824/764318},
	language = {en},
	urldate = {2023-06-19},
	institution = {Publications Office},
	author = {{European Union Agency for Cybersecurity.}},
	year = {2022},
}

@misc{noauthor_good_nodate,
	type = {Report/{Study}},
	title = {Good {Practices} for {Supply} {Chain} {Cybersecurity}},
	url = {https://www.enisa.europa.eu/publications/good-practices-for-supply-chain-cybersecurity},
	abstract = {The report provides an overview of the current supply chain cybersecurity practices followed by essential and important entities in the EU, based on the results of a 2022 ENISA study which focused on investments of cybersecurity budgets among organisations in the EU.},
	language = {en},
	urldate = {2023-06-19},
	journal = {ENISA},
}

@article{geer_counting_2020,
	title = {Counting {Broken} {Links}: {A} {Quant}’s {View} of {Software} {Supply} {Chain} {Security}},
	volume = {45},
	language = {en},
	number = {4},
	author = {Geer, Dan and Tozer, Bentz and Meyers, John Speed},
	year = {2020},
}

@misc{kasi_post_nodate,
	title = {The post mortem paradox: a {Delphi} study of {IT} specialist perceptions},
	shorttitle = {The post mortem paradox},
	url = {https://www.tandfonline.com/doi/epdf/10.1057/palgrave.ejis.3000727?needAccess=true&role=button},
	abstract = {While post mortem evaluation (PME) has long been advocated as a means of improving development practices by learning from IT project failures, few organizations conduct PMEs. The purpose of the study is to explain this discrepancy between theory and practice. This paper integrates findings from a Delphi study of what experienced practitioners perceive as the most important barriers to conducting PMEs with insights from organizational learning theory. The results suggest that there are critical tensions between development practices and learning contexts in many organizations, and adopting PMEs in these cases is likely to reinforce organizational learning dysfunctions rather than improve current development practices. Based on these findings, we argue that the PME literature has underestimated the limits to learning in most IT organizations and we propose to explore paradoxical thinking to help researchers frame continued inquiry into PME and to help managers overcome learning dysfunctions as they push for more widespread use of PMEs.},
	language = {en},
	urldate = {2023-06-16},
	author = {Kasi, Vijay and Keil, Mark and Mathiassen, Lars and Pedersen, Keld},
	doi = {10.1057/palgrave.ejis.3000727},
}

@article{rahman_identification_2009,
	title = {Identification of sources of failures and their propagation in critical infrastructures from 12 years of public failure reports},
	volume = {5},
	issn = {1475-3219},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJCIS.2009.024872},
	doi = {10.1504/IJCIS.2009.024872},
	abstract = {Understanding the origin of infrastructure failures and their propagation patterns in critical infrastructures can provide important information for secure and reliable infrastructure design. Among the critical infrastructures, the Communication and Information Technology Infrastructure (CITI) is crucial, as it provides the basic mechanism for sharing information among all infrastructures. Failures in CITI can disrupt the effective functionality of the other critical infrastructures. Conversely, failures in the other infrastructures can also propagate to CITI, and hence disrupt the operation of all systems. In this study, we used public domain failure reports to identify the origin of these failures and their propagation patterns. We analysed 347 infrastructure failure cases reported from 1994 to 2005 in the Association for Computing Machinery's (ACM) RISKS forum. We studied these reports to determine the causes of infrastructure failures and their impact on CITI and other critical infrastructures in a number of dimensions, such as the origin of failures, impacts of failures in spatial and temporal dimensions, their effect on public safety and how failures propagate from one infrastructure to another. The results obtained from the analysis of these real-life failure cases, which occurred over a considerable timespan, should be useful to researchers and practitioners. This paper also discusses the difficulties and limitations of using public domain data in academic research.},
	number = {3},
	urldate = {2023-06-16},
	journal = {International Journal of Critical Infrastructures},
	author = {Rahman, Hafiz Abdur and Beznosov, Konstantin and Marti, Jose R.},
	month = jan,
	year = {2009},
	note = {Publisher: Inderscience Publishers},
	keywords = {CITI, ICT infrastructure, communications, critical infrastructures, cyber interdependency, failure identification, failure propagation, information sharing, information technology, infrastructure failure, infrastructure interdependencies, telecommunications},
	pages = {220--244},
}

@article{lundell_practitioner_2011,
	title = {Practitioner perceptions of {Open} {Source} software in the embedded systems area},
	volume = {84},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121211000719},
	doi = {10.1016/j.jss.2011.03.020},
	language = {en},
	number = {9},
	urldate = {2023-06-16},
	journal = {Journal of Systems and Software},
	author = {Lundell, Björn and Lings, Brian and Syberfeldt, Anna},
	month = sep,
	year = {2011},
	pages = {1540--1549},
}

@article{lundell_open_2010,
	title = {Open source in {Swedish} companies: where are we?: {Open} source in {Swedish} companies},
	volume = {20},
	issn = {13501917},
	shorttitle = {Open source in {Swedish} companies},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1365-2575.2010.00348.x},
	doi = {10.1111/j.1365-2575.2010.00348.x},
	abstract = {Open Source (OS) is a phenomenon of increasing signiﬁcance for organizations, offering the prospect of effective alternative business solutions and new business opportunities. A number of surveys have been conducted in various countries with the purpose of understanding the state of practice with respect to OS in companies. In this paper we report on a study of the perceptions of OS and the uptake of OS products and development models in Swedish companies. The study used purposeful sampling of companies that have an expressed interest in OS, and the survey was conducted using a set of pre-prepared questions. Its goal was to investigate the extent to which OS has inﬂuenced business thinking, as seen from the standpoint of stakeholders. We found that uptake is much higher than reported in earlier studies, but as with previous studies, activity is still concentrated in small and medium-sized enterprises (SMEs). There is increased evidence of interest beyond the simple use of OS components at the infrastructure level. Further, a signiﬁcant proportion of the companies studied are supporting the OS community as well as beneﬁting from it. Support includes participation in existing projects and the release of new software under OS licenses.},
	language = {en},
	number = {6},
	urldate = {2023-06-16},
	journal = {Information Systems Journal},
	author = {Lundell, Björn and Lings, Brian and Lindqvist, Edvin},
	month = nov,
	year = {2010},
	pages = {519--535},
}

@article{noauthor_defending_nodate,
	title = {Defending {Against} {Software} {Supply} {Chain} {Attacks}},
	language = {en},
}

@inproceedings{pedersen_barriers_2004,
	title = {Barriers for {Post} mortem evaluations in {Systems} {Development}: {UKAIS} {Conference}, {Glasgow}, {UK}.},
	shorttitle = {Barriers for {Post} mortem evaluations in {Systems} {Development}},
	author = {Pedersen, Keld},
	year = {2004},
}

@article{miller_p-element_1992,
	title = {P-element homologous sequences are tandemly repeated in the genome of {Drosophila} guanche},
	volume = {89},
	issn = {0027-8424},
	doi = {10.1073/pnas.89.9.4018},
	abstract = {In Drosophila guanche, P-homologous sequences were found to be located in a tandem repetitive array (copy number: 20-50) at a single genomic site. The cytological position on the polytene chromosomes was determined by in situ hybridization (chromosome O: 85C). Sequencing of one complete repeat unit (3.25 kilobases) revealed high sequence similarity between the central coding region comprising exons 0 to 2 and the corresponding section of the Drosophila melanogaster P element. The rest of the sequence has diverged considerably. Exon 3 has no coding function and the inverted repeats have disappeared. The P homologues of D. guanche apparently have lost their mobility but have retained the coding capacity for a protein similar to the 66-kDa P-element repressor of D. melanogaster. Divergence between different repeat units indicates early amplification of the sequence at this particular genomic site. The presence of a common P-element site at 85C in Drosophila subobscura, Drosophila madeirensis, and D. guanche suggests that clustering of the sequence at this location took place before the phylogenetic radiation of the three species.},
	language = {eng},
	number = {9},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Miller, W. J. and Hagemann, S. and Reiter, E. and Pinsker, W.},
	month = may,
	year = {1992},
	pmid = {1315047},
	pmcid = {PMC525623},
	keywords = {Amino Acid Sequence, Animals, Base Sequence, Biological Evolution, Blotting, Southern, Cloning, Molecular, DNA Transposable Elements, Drosophila, Molecular Sequence Data, Repetitive Sequences, Nucleic Acid, Repressor Proteins, Sequence Alignment},
	pages = {4018--4022},
}

@article{noauthor_notitle_nodate,
}

@techreport{noauthor_notitle_nodate-1,
}

@misc{gokkaya_software_2023,
	title = {Software supply chain: review of attacks, risk assessment strategies and security controls},
	shorttitle = {Software supply chain},
	url = {http://arxiv.org/abs/2305.14157},
	abstract = {The software product is a source of cyber-attacks that target organizations by using their software supply chain as a distribution vector. As the reliance of software projects on open-source or proprietary modules is increasing drastically, SSC is becoming more and more critical and, therefore, has attracted the interest of cyber attackers. While existing studies primarily focus on software supply chain attacks' prevention and detection methods, there is a need for a broad overview of attacks and comprehensive risk assessment for software supply chain security. This study conducts a systematic literature review to fill this gap. We analyze the most common software supply chain attacks by providing the latest trend of analyzed attacks, and we identify the security risks for open-source and third-party software supply chains. Furthermore, this study introduces unique security controls to mitigate analyzed cyber-attacks and risks by linking them with real-life security incidence and attacks.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Gokkaya, Betul and Aniello, Leonardo and Halak, Basel},
	month = may,
	year = {2023},
	note = {arXiv:2305.14157 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{ellison_evaluating_2010,
	title = {Evaluating and {Mitigating} {Software} {Supply} {Chain} {Security} {Risks}},
	copyright = {In Copyright},
	url = {https://kilthub.cmu.edu/articles/Evaluating_and_Mitigating_Software_Supply_Chain_Security_Risks/6573497/1},
	doi = {10.1184/R1/6573497.V1},
	abstract = {The Department of Defense (DoD) is concerned that security vulnerabilities could be inserted into software that has been developed outside of the DoD's supervision or control. This report presents an initial analysis of how to evaluate and mitigate the risk that such unauthorized insertions have been made. The analysis is structured in terms of actions that should be taken in each phase of the DoD acquisition life cycle.},
	urldate = {2023-06-16},
	author = {Ellison, Robert J. and Goodenough, John and Weinstock, Charles B. and Woody, Carol C.},
	year = {2010},
	note = {Artwork Size: 1158599 Bytes
Publisher: Carnegie Mellon University},
	keywords = {80309 Software Engineering, FOS: Computer and information sciences},
	pages = {1158599 Bytes},
}

@misc{nunes_evaluating_2023,
	title = {Evaluating {GPT}-3.5 and {GPT}-4 {Models} on {Brazilian} {University} {Admission} {Exams}},
	url = {http://arxiv.org/abs/2303.17003},
	abstract = {The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M{\textbackslash}'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87\%, largely surpassing GPT-3.5 by 11 points. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Nunes, Desnes and Primi, Ricardo and Pires, Ramon and Lotufo, Roberto and Nogueira, Rodrigo},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17003 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{papernot_limitations_2016,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	doi = {10.1109/EuroSP.2016.36},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	booktitle = {2016 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Biological neural networks, Distortion, Force, Machine learning, Neurons, Training},
	pages = {372--387},
}

@article{geer_extracting_2023,
	title = {Extracting {Unlearned} {Lessons} from {Past} {Poor} {Choices} lest {They} be {Learned} the {Hard} {Way} in the {Future}},
	language = {en},
	author = {Geer, Dan},
	year = {2023},
}

@inproceedings{menezes2018diversity,
	title = {Diversity in software engineering},
	booktitle = {Proceedings of the 11th {International} workshop on cooperative and human aspects of software engineering},
	author = {Menezes, Álvaro and Prikladnicki, Rafael},
	year = {2018},
	pages = {45--48},
}

@misc{ohm_you_2023,
	title = {You {Can} {Run} {But} {You} {Can}'t {Hide}: {Runtime} {Protection} {Against} {Malicious} {Package} {Updates} {For} {Node}.js},
	shorttitle = {You {Can} {Run} {But} {You} {Can}'t {Hide}},
	url = {http://arxiv.org/abs/2305.19760},
	doi = {10.48550/arXiv.2305.19760},
	abstract = {Maliciously prepared software packages are an extensively leveraged weapon for software supply chain attacks. The detection of malicious packages is undoubtedly of high priority and many academic and commercial approaches have been developed. In the inevitable case of an attack, one needs resilience against malicious code. To this end, we present a runtime protection for Node.js that automatically limits a package's capabilities to an established minimum. The detection of required capabilities as well as their enforcement at runtime has been implemented and evaluated against known malicious attacks. Our approach was able to prevent 9/10 historic attacks with a median install-time overhead of less than 0.6 seconds and a median runtime overhead of less than 0.2 seconds.},
	urldate = {2023-06-01},
	author = {Ohm, Marc and Pohl, Timo and Boes, Felix},
	month = may,
	year = {2023},
	note = {arXiv:2305.19760 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{wang_effective_nodate,
	title = {Effective {ReDoS} {Detection} by {Principled} {Vulnerability} {Modeling} and {Exploit} {Generation}},
	language = {en},
	author = {Wang, Xinyi},
}

@inproceedings{minelli_i_2015,
	address = {Florence, Italy},
	title = {I {Know} {What} {You} {Did} {Last} {Summer} - {An} {Investigation} of {How} {Developers} {Spend} {Their} {Time}},
	isbn = {978-1-4673-8159-8},
	url = {http://ieeexplore.ieee.org/document/7181430/},
	doi = {10.1109/ICPC.2015.12},
	abstract = {Developing software is a complex mental activity, requiring extensive technical knowledge and abstraction capabilities. The tangible part of development is the use of tools to read, inspect, edit, and manipulate source code, usually through an IDE (integrated development environment). Common claims about software development include that program comprehension takes up half of the time of a developer, or that certain UI (user interface) paradigms of IDEs offer insufﬁcient support to developers. Such claims are often based on anecdotal evidence, throwing up the question of whether they can be corroborated on more solid grounds.},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {2015 {IEEE} 23rd {International} {Conference} on {Program} {Comprehension}},
	publisher = {IEEE},
	author = {Minelli, Roberto and Mocci, Andrea and Lanza, Michele},
	month = may,
	year = {2015},
	pages = {25--35},
}

@techreport{snyk_snyk_2022,
	title = {Snyk {Top} 10: {Open} {Source} {Vulnerabilities} in 2022},
	url = {https://resources.snyk.io/c/snyk-top-10-vulns},
	urldate = {2023-05-29},
	author = {{SNYK}},
	year = {2022},
}

@article{tsvetanov_effect_2021,
	title = {The effect of the {Colonial} {Pipeline} shutdown on gasoline prices},
	volume = {209},
	issn = {01651765},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165176521003992},
	doi = {10.1016/j.econlet.2021.110122},
	abstract = {On May 7, 2021, the Colonial Pipeline system was shut down for 6 days in response to a cyberattack. Using daily regular gasoline price data at the city level and employing a difference-in-differences approach to address potential demand-side confounding factors, we find that the shutdown led to a 4 cents-per-gallon increase in average gasoline prices in affected areas, with the estimated impact varying across locations based on their access to alternative means of fuel supply. Although the overall effect was initially slow, it persisted even after the reopening of the pipeline.},
	language = {en},
	urldate = {2023-05-29},
	journal = {Economics Letters},
	author = {Tsvetanov, Tsvetan and Slaria, Srishti},
	month = dec,
	year = {2021},
	pages = {110122},
}

@techreport{krasner_cost_2022,
	title = {The {Cost} of {Poor} {Software} {Quality} in {US}: {A} 2022 {Report}},
	url = {https://www.synopsys.com/content/dam/synopsys/sig-assets/reports/cpsq-report-nov-22-1.pdf},
	urldate = {2023-05-29},
	institution = {Consortium for Information \& Software Quality (CISQ)},
	author = {Krasner, Herb},
	year = {2022},
}

@inproceedings{vu_towards_2020,
	address = {New York, NY, USA},
	series = {{CCS} '20},
	title = {Towards {Using} {Source} {Code} {Repositories} to {Identify} {Software} {Supply} {Chain} {Attacks}},
	isbn = {9781450370899},
	url = {https://dl.acm.org/doi/10.1145/3372297.3420015},
	doi = {10.1145/3372297.3420015},
	abstract = {Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100,000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (e.g., those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI) suggests that the technique is suitable for lightweight analysis of real-world packages.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Vu, Duc Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
	month = nov,
	year = {2020},
	keywords = {code injection, lightweight analysis, package repositories, software supply chain attacks, source code repositories},
	pages = {2093--2095},
}

@inproceedings{ladisa_towards_2022,
	address = {New York, NY, USA},
	series = {{SCORED}'22},
	title = {Towards the {Detection} of {Malicious} {Java} {Packages}},
	isbn = {9781450398855},
	url = {https://dl.acm.org/doi/10.1145/3560835.3564548},
	doi = {10.1145/3560835.3564548},
	abstract = {Open-source software supply chain attacks aim at infecting downstream users by poisoning open-source packages. The common way of consuming such artifacts is through package repositories and the development of vetting strategies to detect such attacks is ongoing research. Despite its popularity, the Java ecosystem is the less explored one in the context of supply chain attacks. In this paper, we present simple-yet-effective indicators of malicious behavior that can be observed statically through the analysis of Java bytecode. Then we evaluate how such indicators and their combinations perform when detecting malicious code injections. We do so by injecting three malicious payloads taken from real-world examples into the Top-10 most popular Java libraries from libraries.io. We found that the analysis of strings in the constant pool and of sensitive APIs in the bytecode instructions aid in the task of detecting malicious Java packages by significantly reducing the information, thus, making also manual triage possible.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 2022 {ACM} {Workshop} on {Software} {Supply} {Chain} {Offensive} {Research} and {Ecosystem} {Defenses}},
	publisher = {Association for Computing Machinery},
	author = {Ladisa, Piergiorgio and Plate, Henrik and Martinez, Matias and Barais, Olivier and Ponta, Serena Elisa},
	month = nov,
	year = {2022},
	keywords = {malware detection, open-source security, supply chain attacks},
	pages = {63--72},
}

@inproceedings{scalco_feasibility_2022,
	address = {New York, NY, USA},
	series = {{ARES} '22},
	title = {On the feasibility of detecting injections in malicious npm packages},
	isbn = {9781450396707},
	url = {https://dl.acm.org/doi/10.1145/3538969.3543815},
	doi = {10.1145/3538969.3543815},
	abstract = {Open-source packages typically have their source code available on a source code repository (e.g., on GitHub), but developers prefer to use pre-built artifacts directly from the package repositories (such as npm for JavaScript). Between the source code and the distributed artifacts, there could be differences that pose security risks (e.g., attackers deploy malicious code during package installation) in the software supply chain. Existing package scanners focus on the entire artifact of a package to detect this kind of attacks. These procedures are not only time consuming, but also generate high irrelevant alerts (FPs). An approach called LastPyMile by Vu et al. (ESEC/FSE’21) has been shown to be effective in detecting discrepancies and reducing false alerts in vetting Python packages on PyPI by focusing only on the differences between the source and the package. In this work, we propose to port that approach to scan JavaScript packages in the npm ecosystem. We presented a preliminary evaluation of our implementation on a set of real malicious npm packages and the top popular packages. The results show that while being 20.7x faster than git-log approach, our approach managed to reduce the percentage of false alerts produced by package scanner by 69\%.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Scalco, Simone and Paramitha, Ranindya and Vu, Duc-Ly and Massacci, Fabio},
	month = aug,
	year = {2022},
	keywords = {JavaScript, Open source software, npm, software supply chain},
	pages = {1--8},
}

@inproceedings{liang_malicious_2021,
	title = {Malicious {Packages} {Lurking} in {User}-{Friendly} {Python} {Package} {Index}},
	doi = {10.1109/TrustCom53373.2021.00091},
	abstract = {Python has gradually become one of the most important programming languages through artificial intelligence's development. PIP, a package management tool for Python, offers one-click installation, allowing developers to utilize other people's code to speed up development. However, any registered member can easily upload packages to the repository that stores third-party packages. This functionality is used by attackers to poison the package index, i.e., to publish enormous malicious pip packages for installing backdoor, gathering information, etc. To know the situation of third-party packages in the Python community ecosystem, we establish the criteria for judging packages' suspicious or malicious behavior by analyzing the code logic in disclosed malicious packages. With the gained findings, we propose and implement Pip Poisoning Detector (PPD), an approach based on anomaly detection. PPD evaluated 228,723 packages, and after human inspection, we found 63 malicious and 238 suspicious ones among the output 5,699 results. The experimental results prove that our approach is effective and can significantly reduce review workload by 97.51\%.},
	booktitle = {2021 {IEEE} 20th {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Liang, Genpei and Zhou, Xiangyu and Wang, Qingyu and Du, Yutong and Huang, Cheng},
	month = oct,
	year = {2021},
	note = {ISSN: 2324-9013},
	keywords = {Codes, Ecosystems, Feature extraction, Indexes, Manuals, Privacy, Toxicology, anomaly detection, malicious Python, pip poisoning},
	pages = {606--613},
}

@misc{boucher_trojan_2023,
	title = {Trojan {Source}: {Invisible} {Vulnerabilities}},
	shorttitle = {Trojan {Source}},
	url = {http://arxiv.org/abs/2111.00169},
	doi = {10.48550/arXiv.2111.00169},
	abstract = {We present a new type of attack in which source code is maliciously encoded so that it appears different to a compiler and to the human eye. This attack exploits subtleties in text-encoding standards such as Unicode to produce source code whose tokens are logically encoded in a different order from the one in which they are displayed, leading to vulnerabilities that cannot be perceived directly by human code reviewers. 'Trojan Source' attacks, as we call them, pose an immediate threat both to first-party software and of supply-chain compromise across the industry. We present working examples of Trojan Source attacks in C, C++, C\#, JavaScript, Java, Rust, Go, Python, SQL, Bash, Assembly, and Solidity. We propose definitive compiler-level defenses, and describe other mitigating controls that can be deployed in editors, repositories, and build pipelines while compilers are upgraded to block this attack. We document an industry-wide coordinated disclosure for these vulnerabilities; as they affect most compilers, editors, and repositories, the exercise teaches how different firms, open-source communities, and other stakeholders respond to vulnerability disclosure.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Boucher, Nicholas and Anderson, Ross},
	month = mar,
	year = {2023},
	note = {arXiv:2111.00169 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages},
}

@inproceedings{sejfia_practical_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Practical automated detection of malicious npm packages},
	isbn = {9781450392211},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510104},
	doi = {10.1145/3510003.3510104},
	abstract = {The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Sejfia, Adriana and Schäfer, Max},
	month = jul,
	year = {2022},
	keywords = {malware detection, supply chain security},
	pages = {1681--1692},
}

@inproceedings{sejfia_practical_2022-1,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Practical automated detection of malicious npm packages},
	isbn = {9781450392211},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510104},
	doi = {10.1145/3510003.3510104},
	abstract = {The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Sejfia, Adriana and Schäfer, Max},
	month = jul,
	year = {2022},
	keywords = {malware detection, supply chain security},
	pages = {1681--1692},
}

@inproceedings{pfretzschner_identification_2017,
	address = {New York, NY, USA},
	series = {{ARES} '17},
	title = {Identification of {Dependency}-based {Attacks} on {Node}.js},
	isbn = {9781450352574},
	url = {https://dl.acm.org/doi/10.1145/3098954.3120928},
	doi = {10.1145/3098954.3120928},
	abstract = {Node.js executes server-side JavaScript-based code. By design Node.js and JavaScript support global variables, monkey-patching, and shared cache of loaded modules. This paper discusses four attacks that exploit these weaknesses, which are: leakage of global variables, manipulation of global variables, manipulation of local variables, and manipulation of the dependency tree. In addition, it describes the static code analysis that we implemented for T.J. Watson Libraries for Analysis (WALA) to detect the identified attacks and the evaluation of the analysis. The analysis is integrated into OpenWhisk, an open source serverless cloud platform.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Pfretzschner, Brian and ben Othmane, Lotfi},
	month = aug,
	year = {2017},
	keywords = {Cloud computing, Dependency-based attack, Node.js, Software security},
	pages = {1--6},
}

@inproceedings{lamowski_sandcrust_2017,
	address = {New York, NY, USA},
	series = {{PLOS}'17},
	title = {Sandcrust: {Automatic} {Sandboxing} of {Unsafe} {Components} in {Rust}},
	isbn = {9781450351539},
	shorttitle = {Sandcrust},
	url = {https://dl.acm.org/doi/10.1145/3144555.3144562},
	doi = {10.1145/3144555.3144562},
	abstract = {System-level development has been dominated by traditional programming languages such as C and C++ for decades. These languages are inherently unsafe regarding memory management. Even experienced developers make mistakes that open up security holes or compromise the safety properties of software. The Rust programming language is targeted at the systems domain and aims to eliminate memory-related programming errors by enforcing a strict memory model at the language and compiler level. Unfortunately, these compile-time guarantees no longer hold when a Rust program is linked against a library written in unsafe C, which is commonly required for functionality where an implementation in Rust is not yet available. In this paper, we present Sandcrust, an easy-to-use sand-boxing solution for isolating code and data of a C library in a separate process. This isolation protects the Rust-based main program from any memory corruption caused by bugs in the unsafe library, which would otherwise invalidate the memory safety guarantees of Rust. Sandcrust is based on the Rust macro system and requires no modification to the compiler or runtime, but only straightforward annotation of functions that call the library's API.},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 9th {Workshop} on {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lamowski, Benjamin and Weinhold, Carsten and Lackorzynski, Adam and Härtig, Hermann},
	month = oct,
	year = {2017},
	pages = {51--57},
}

@inproceedings{barlas_exploiting_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Exploiting input sanitization for regex denial of service},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510047},
	doi = {10.1145/3510003.3510047},
	abstract = {Web services use server-side input sanitization to guard against harmful input. Some web services publish their sanitization logic to make their client interface more usable, e.g., allowing clients to debug invalid requests locally. However, this usability practice poses a security risk. Specifically, services may share the regexes they use to sanitize input strings --- and regex-based denial of service (ReDoS) is an emerging threat. Although prominent service outages caused by ReDoS have spurred interest in this topic, we know little about the degree to which live web services are vulnerable to ReDoS. In this paper, we conduct the first black-box study measuring the extent of ReDoS vulnerabilities in live web services. We apply the Consistent Sanitization Assumption: that client-side sanitization logic, including regexes, is consistent with the sanitization logic on the server-side. We identify a service's regex-based input sanitization in its HTML forms or its API, find vulnerable regexes among these regexes, craft ReDoS probes, and pinpoint vulnerabilities. We analyzed the HTML forms of 1,000 services and the APIs of 475 services. Of these, 355 services publish regexes; 17 services publish unsafe regexes; and 6 services are vulnerable to ReDoS through their APIs (6 domains; 15 subdomains). Both Microsoft and Amazon Web Services patched their web services as a result of our disclosure. Since these vulnerabilities were from API specifications, not HTML forms, we proposed a ReDoS defense for a popular API validation library, and our patch has been merged. To summarize: in client-visible sanitization logic, some web services advertise ReDoS vulnerabilities in plain sight. Our results motivate short-term patches and long-term fundamental solutions. "Make measurable what cannot be measured." -Galileo Galilei},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Barlas, Efe and Du, Xin and Davis, James C.},
	month = jul,
	year = {2022},
	keywords = {ReDoS, algorithmic complexity attacks, denial of service, empirical software engineering, regular expressions, web security},
	pages = {883--895},
}

@misc{banna2021experience,
	title = {An experience report on machine learning reproducibility: {Guidance} for practitioners and {TensorFlow} model garden contributors},
	url = {https://arxiv.org/abs/2107.00821},
	author = {Banna, Vishnu and Chinnakotla, Akhil and Yan, Zhengxin and Vegesana, Ani and Vivek, Naveen and Krishnappa, Kruthi and Jiang, Wenxin and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
	year = {2021},
}

@inproceedings{xu_empirical_2022,
	title = {An {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Xu, Qiang and Davis, James C and Hu, Y Charlie and Jindal, Abhilash},
	year = {2022},
	pages = {12},
}

@inproceedings{shen_towards_2023,
	title = {Towards {Automated} {Identification} of {Layering} {Violations} in {Embedded} {Applications} ({WIP})},
	abstract = {For portability, embedded systems software follows a layered design to reduce dependence on particular hardware behavior. We consider the problem of identifying layering violations: instances where the embedded application accesses non-adjacent layers. This paper presents our preliminary work to detect a class of layering violations called Non Conventional MMIO Accesses (NCMAs). We find them by searching for direct Memory Mapped Input Output (MMIO) accesses made outside of the Hardware Abstraction Layer (HAL). For evaluation, we curated a list of 988 applications spanning 5 Real Time Operating Systems (RTOSes) – the first large dataset of compilable embedded applications. Our system identified 380 NCMAs. We reported these issues to the corresponding developers and found interesting reasons for committing layering violations. We have open-sourced our tool and the collected dataset to foster future research.},
	language = {en},
	booktitle = {{ACM} {SIGPLAN}/{SIGBED} {International} {Conference} on {Languages}, {Compilers}, and {Tools} for {Embedded} {Systems} ({LCTES})},
	author = {Shen, Mingjie and Davis, James C and Machiry, Aravind},
	year = {2023},
}

@inproceedings{hassan_improving_2023,
	title = {Improving {Developers}' {Understanding} of {Regex} {Denial} of {Service} {Tools} through {Anti}-{Patterns} and {Fix} {Strategies}},
	abstract = {Regular expressions are used for diverse purposes, including input validation and ﬁrewalls. Unfortunately, they can also lead to a security vulnerability called ReDoS (Regular Expression Denial of Service), caused by a super-linear worstcase execution time during regex matching. Due to the severity and prevalence of ReDoS, past work proposed automatic tools to detect and ﬁx regexes. Although these tools were evaluated in automatic experiments, their usability has not yet been studied; usability has not been a focus of prior work. Our insight is that the usability of existing tools to detect and ﬁx regexes will improve if we complement them with anti-patterns and ﬁx strategies of vulnerable regexes.},
	language = {en},
	booktitle = {{IEEE} {Security} \& {Privacy}},
	author = {Hassan, Sk Adnan and Aamir, Zainab and Lee, Dongyoon and Davis, James C and Servant, Francisco},
	year = {2023},
}

@inproceedings{gopalakrishna_if_2022,
	title = {"{If} security is required": {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	language = {en},
	booktitle = {4th {International} {Workshop} on {Software} {Engineering} {Research} \& {Practices} for the {Internet} of {Things} ({SERP4IoT})},
	author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
	year = {2022},
	pages = {8},
}

@inproceedings{srinivasan_towards_2023,
	title = {Towards {Rehosting} {Embedded} {Applications} as {Linux} {Applications}},
	abstract = {Dynamic analysis of embedded firmware is a necessary capability for many security tasks, e.g., vulnerability detection. Rehosting is a technique that enables dynamic analysis by facilitating the execution of firmware in a host environment decoupled from the actual hardware. Current rehosting techniques focus on high-fidelity execution of the entire firmware. Consequently, these techniques try to execute firmware in an emulated environment, with precise models of hardware (i.e., peripheral) interactions. However, these techniques are hard to scale and have various drawbacks.},
	language = {en},
	author = {Srinivasan, Jayashree and Tanksalkar, Sai Ritvik and Amusuo, Paschal C and Davis, James C and Machiry, Aravind},
	year = {2023},
}

@article{davis_exploring_2022,
	title = {Exploring the {Process} and {Outcomes} of {Leading} a {Study} {Abroad} {Program} {Using} {Real}-{Time} {Perspectives}},
	volume = {34},
	issn = {2380-8144, 1085-4568},
	url = {http://frontiersjournal.org/index.php/Frontiers/article/view/654},
	doi = {10.36366/frontiers.v34i2.654},
	abstract = {A study abroad program can be a transformative experience for students, but these programs rely on the support of program leaders who play a crucial mentoring role. Program leaders can also learn from their experiences abroad, but their experiences are less studied than those of students. To better prepare future program leaders, this paper describes the experiences of program leaders during a two-week study abroad program. We introduce a novel data collection approach, autonomous focus groups, to capture real-time perspectives. Through the lens of Kolb’s Experiential Learning Theory, we characterize the process of leading a study abroad program and the conceptualizations that program leaders form about students during their time abroad. Our findings provide a more nuanced view of the day-to-day experience of leading a study abroad program than previous studies, which can inform the preparation and training provided to program leaders.},
	language = {en},
	number = {2},
	urldate = {2023-05-29},
	journal = {Frontiers: The Interdisciplinary Journal of Study Abroad},
	author = {Davis, Kirsten and Deters, Jessica and Ozkan, Desen and Davis, James and Murzi, Homero},
	month = aug,
	year = {2022},
	pages = {351--381},
}

@misc{yuan_backdoor_2023,
	title = {Backdoor {Attacks} to {Pre}-trained {Unified} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2302.09360},
	abstract = {The rise of pre-trained unified foundation models breaks down the barriers between different modalities and tasks, providing comprehensive support to users with unified architectures. However, the backdoor attack on pre-trained models poses a serious threat to their security. Previous research on backdoor attacks has been limited to uni-modal tasks or single tasks across modalities, making it inapplicable to unified foundation models. In this paper, we make proof-of-concept level research on the backdoor attack for pre-trained unified foundation models. Through preliminary experiments on NLP and CV classification tasks, we reveal the vulnerability of these models and suggest future research directions for enhancing the attack approach.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Yuan, Zenghui and Liu, Yixin and Zhang, Kai and Zhou, Pan and Sun, Lichao},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09360 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{jiang2023ptmtorrentGlobus,
	title = {{PTMTorrent}: {Globus} share},
	url = {https://app.globus.org/file-manager?origin_id=55e17a6e-9d8f-11ed-a2a2-8383522b48d9},
	abstract = {Due to the cost of developing and training deep learning models from scratch, machine learning engineers have begun to reuse pre-trained models (PTMs) and ﬁne-tune them for downstream tasks. PTM registries known as “model hubs” support engineers in distributing and reusing deep learning models. PTM packages include pre-trained weights, documentation, model architectures, datasets, and metadata. Mining the information in PTM packages will enable the discovery of engineering phenomena and tools to support software engineers. However, accessing this information is difﬁcult — there are many PTM registries, and both the registries and the individual packages may have rate limiting for accessing the data.},
	language = {en},
	urldate = {2023-05-26},
	journal = {Proceedings of the 20th Annual Conference on Mining Software Repositories — Data and Tool Showcase Track (MSR-Data’23)},
	author = {Jiang, Wenxin and Synovic, Nicholas and Jajal, Purvish and Schorlemmer, Taylor R. and Tewari, Arav and Pareek, Bhavesh and Thiruvathukal, George K. and Davis, James C.},
	month = mar,
	year = {2023},
	keywords = {Computer Science - Software Engineering},
}

@article{goel2022tree,
	title = {Tree-based unidirectional neural networks for low-power computer vision},
	journal = {IEEE Design \& Test},
	author = {Goel, Abhinav and Tung, Caleb and Eliopoulos, Nick and Wang, Amy and Davis, James C and Thiruvathukal, George K and Lu, Yung-Hsiang},
	year = {2022},
	note = {Publisher: IEEE},
}

@inproceedings{jiang_empirical_2023,
	title = {An {Empirical} {Study} of {Pre}-{Trained} {Model} {Reuse} in the {Hugging} {Face} {Deep} {Learning} {Model} {Registry}},
	abstract = {Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as stateof-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.},
	language = {en},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Jiang, Wenxin and Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R and Sethi, Rohan and Lu, Yung-Hsiang and Thiruvathukal, George K and Davis, James C},
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{montes_discrepancies_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Discrepancies among pre-trained deep neural networks: a new threat to model zoo reliability},
	isbn = {978-1-4503-9413-0},
	shorttitle = {Discrepancies among pre-trained deep neural networks},
	url = {https://dl.acm.org/doi/10.1145/3540250.3560881},
	doi = {10.1145/3540250.3560881},
	abstract = {Training deep neural networks (DNNs) takes significant time and resources. A practice for expedited deployment is to use pre-trained deep neural networks (PTNNs), often from model zoos--collections of PTNNs; yet, the reliability of model zoos remains unexamined. In the absence of an industry standard for the implementation and performance of PTNNs, engineers cannot confidently incorporate them into production systems. As a first step, discovering potential discrepancies between PTNNs across model zoos would reveal a threat to model zoo reliability. Prior works indicated existing variances in deep learning systems in terms of accuracy. However, broader measures of reliability for PTNNs from model zoos are unexplored. This work measures notable discrepancies between accuracy, latency, and architecture of 36 PTNNs across four model zoos. Among the top 10 discrepancies, we find differences of 1.23\%-2.62\% in accuracy and 9\%-131\% in latency. We also find mismatches in architecture for well-known DNN architectures (e.g., ResNet and AlexNet). Our findings call for future works on empirical validation, automated tools for measurement, and best practices for implementation.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Montes, Diego and Peerapatanapokin, Pongpatapee and Schultz, Jeff and Guo, Chengjun and Jiang, Wenxin and Davis, James C.},
	month = nov,
	year = {2022},
	keywords = {Empirical software engineering, Model zoos, Neural networks, Software reuse},
	pages = {1605--1609},
}

@article{hu_evolution_2023,
	title = {Evolution of {Winning} {Solutions} in the 2021 {Low}-{Power} {Computer} {Vision} {Challenge}},
	abstract = {Mobile and embedded devices are becoming ubiquitous. Applications such as rescue with autonomous robots and event analysis on traffic cameras rely on devices with limited power supply and computational sources. Thus, the demand for efficient computer vision algorithms increases. Since 2015, we have organized the IEEE Low-Power Computer Vision Challenge to advance the state of the art in low-power computer vision. We describe the competition organizing details including the challenge design, the reference solution, the dataset, the referee system, and the evolution of the solutions from two winning teams. We examine the winning teams’ development patterns and design decisions, focusing on their techniques to balance power consumption and accuracy. We conclude that a successful competition needs a well-designed reference solution and automated referee system, and a solution with modularized components is more likely to win. We hope this paper provides guidelines for future organizers and contestants of computer vision competitions.},
	language = {en},
	journal = {IEEE Intelligent Systems},
	author = {Hu, Xiao and Jiao, Ziteng and Kocher, Ayden and Wu, Zhenyu and Liu, Junjie and Davis, James C and Lu, Yung-Hsiang},
	year = {2023},
}

@inproceedings{jiang_ptmtorrent_2023,
	title = {{PTMTorrent}: {A} {Dataset} for {Mining} {Open}-source {Pre}-trained {Model} {Packages}},
	shorttitle = {{PTMTorrent}},
	url = {http://arxiv.org/abs/2303.08934},
	abstract = {Due to the cost of developing and training deep learning models from scratch, machine learning engineers have begun to reuse pre-trained models (PTMs) and ﬁne-tune them for downstream tasks. PTM registries known as “model hubs” support engineers in distributing and reusing deep learning models. PTM packages include pre-trained weights, documentation, model architectures, datasets, and metadata. Mining the information in PTM packages will enable the discovery of engineering phenomena and tools to support software engineers. However, accessing this information is difﬁcult — there are many PTM registries, and both the registries and the individual packages may have rate limiting for accessing the data.},
	language = {en},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the 20th {Annual} {Conference} on {Mining} {Software} {Repositories} — {Data} and {Tool} {Showcase} {Track} ({MSR}-{Data}’23)},
	author = {Jiang, Wenxin and Synovic, Nicholas and Jajal, Purvish and Schorlemmer, Taylor R. and Tewari, Arav and Pareek, Bhavesh and Thiruvathukal, George K. and Davis, James C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08934 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{veselsky2022v2vtrustHotMobile,
	address = {New York, NY, USA},
	series = {{HotMobile} '22},
	title = {Poster: {Establishing} trust in vehicle-to-vehicle coordination: {A} sensor fusion approach},
	isbn = {978-1-4503-9218-1},
	url = {https://doi.org/10.1145/3508396.3517075},
	doi = {10.1145/3508396.3517075},
	abstract = {As we add more autonomous and semi-autonomous vehicles (AVs) to our roads, their effects on passenger and pedestrian safety are becoming more important. Despite extensive testing before deployment, AV systems are not perfect at identifying hazards in the roadway. Although a particular AV’s sensors and software may not be 100\% accurate at identifying hazards, there is an untapped pool of information held by other AVs in the vicinity that could be used to quickly and accurately identify roadway hazards before they present a safety threat.},
	booktitle = {Proceedings of the 23rd annual international workshop on mobile computing systems and applications ({HotMobile})},
	publisher = {Association for Computing Machinery},
	author = {Veselsky, Jakob and West, Jack and Ahlgren, Isaac and Thiruvathukal, George K. and Klingensmith, Neil and Goel, Abhinav and Jiang, Wenxin and Davis, James C. and Lee, Kyuin and Kim, Younghyun},
	year = {2022},
	pages = {128},
}

@inproceedings{goel2021low,
	title = {Low-power multi-camera object re-identification using hierarchical neural networks},
	volume = {2021},
	booktitle = {{ACM}/{IEEE} {International} {Symposium} on {Low} {Power} {Electronics} and {Design} ({ISLPED})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Wang, Haobo and Davis, James C and Thiruvathukal, George K and Lu, Yung-Hsiang},
	year = {2021},
}

@misc{woodruff_pgp_2023,
	title = {{PGP} signatures on {PyPI}: worse than useless},
	shorttitle = {{PGP} signatures on {PyPI}},
	url = {https://blog.yossarian.net/2023/05/21/PGP-signatures-on-PyPI-worse-than-useless},
	language = {en},
	urldate = {2023-05-24},
	author = {Woodruff, William},
	month = may,
	year = {2023},
}

@misc{noauthor_pypi_nodate,
	title = {{PyPI} · {The} {Python} {Package} {Index}},
	url = {https://pypi.org/},
	abstract = {The Python Package Index (PyPI) is a repository of software for the Python programming language.},
	language = {en},
	urldate = {2023-05-24},
	journal = {PyPI},
}

@misc{kula_impact_2017,
	title = {On the {Impact} of {Micro}-{Packages}: {An} {Empirical} {Study} of the npm {JavaScript} {Ecosystem}},
	shorttitle = {On the {Impact} of {Micro}-{Packages}},
	url = {http://arxiv.org/abs/1709.04638},
	abstract = {The rise of user-contributed Open Source Software (OSS) ecosystems demonstrate their prevalence in the software engineering discipline. Libraries work together by depending on each other across the ecosystem. From these ecosystems emerges a minimized library called a micro-package. Micro- packages become problematic when breaks in a critical ecosystem dependency ripples its effects to unsuspecting users. In this paper, we investigate the impact of micro-packages in the npm JavaScript ecosystem. Specifically, we conducted an empirical in- vestigation with 169,964 JavaScript npm packages to understand (i) the widespread phenomena of micro-packages, (ii) the size dependencies inherited by a micro-package and (iii) the developer usage cost (ie., fetch, install, load times) of using a micro-package. Results of the study find that micro-packages form a significant portion of the npm ecosystem. Apart from the ease of readability and comprehension, we show that some micro-packages have long dependency chains and incur just as much usage costs as other npm packages. We envision that this work motivates the need for developers to be aware of how sensitive their third-party dependencies are to critical changes in the software ecosystem.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Kula, Raula Gaikovina and Ouni, Ali and German, Daniel M. and Inoue, Katsuro},
	month = sep,
	year = {2017},
	note = {arXiv:1709.04638 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{liu_demystifying_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Demystifying the vulnerability propagation and its evolution via dependency trees in the {NPM} ecosystem},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510142},
	doi = {10.1145/3510003.3510142},
	abstract = {Third-party libraries with rich functionalities facilitate the fast development of JavaScript software, leading to the explosive growth of the NPM ecosystem. However, it also brings new security threats that vulnerabilities could be introduced through dependencies from third-party libraries. In particular, the threats could be excessively amplified by transitive dependencies. Existing research only considers direct dependencies or reasoning transitive dependencies based on reachability analysis, which neglects the NPM-specific dependency resolution rules as adapted during real installation, resulting in wrongly resolved dependencies. Consequently, further fine-grained analysis, such as precise vulnerability propagation and their evolution over time in dependencies, cannot be carried out precisely at a large scale, as well as deriving ecosystem-wide solutions for vulnerabilities in dependencies. To fill this gap, we propose a knowledge graph-based dependency resolution, which resolves the inner dependency relations of dependencies as trees (i.e., dependency trees), and investigates the security threats from vulnerabilities in dependency trees at a large scale. Specifically, we first construct a complete dependency-vulnerability knowledge graph (DVGraph) that captures the whole NPM ecosystem (over 10 million library versions and 60 million well-resolved dependency relations). Based on it, we propose a novel algorithm (DTResolver) to statically and precisely resolve dependency trees, as well as transitive vulnerability propagation paths, for each package by taking the official dependency resolution rules into account. Based on that, we carry out an ecosystem-wide empirical study on vulnerability propagation and its evolution in dependency trees. Our study unveils lots of useful findings, and we further discuss the lessons learned and solutions for different stakeholders to mitigate the vulnerability impact in NPM based on our findings. For example, we implement a dependency tree based vulnerability remediation method (DTReme) for NPM packages, and receive much better performance than the official tool (npm audit fix).},
	urldate = {2023-05-23},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Chengwei and Chen, Sen and Fan, Lingling and Chen, Bihuan and Liu, Yang and Peng, Xin},
	month = jul,
	year = {2022},
	pages = {672--684},
}

@inproceedings{trockman_adding_2018,
	address = {New York, NY, USA},
	series = {{ICSE} '18},
	title = {Adding sparkle to social coding: an empirical study of repository badges in the npm ecosystem},
	isbn = {978-1-4503-5638-1},
	shorttitle = {Adding sparkle to social coding},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180209},
	doi = {10.1145/3180155.3180209},
	abstract = {In fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist.},
	urldate = {2023-05-23},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Trockman, Asher and Zhou, Shurui and Kästner, Christian and Vasilescu, Bogdan},
	month = may,
	year = {2018},
	pages = {511--522},
}

@article{zimmermann_small_nodate,
	title = {Small {World} with {High} {Risks}: {A} {Study} of {Security} {Threats} in the npm {Ecosystem}},
	abstract = {The popularity of JavaScript has lead to a large ecosystem of third-party packages available via the npm software package registry. The open nature of npm has boosted its growth, providing over 800,000 free and reusable software packages. Unfortunately, this open nature also causes security risks, as evidenced by recent incidents of single packages that broke or attacked software running on millions of computers. This paper studies security risks for users of npm by systematically analyzing dependencies between packages, the maintainers responsible for these packages, and publicly reported security issues. Studying the potential for running vulnerable or malicious code due to third-party dependencies, we ﬁnd that individual packages could impact large parts of the entire ecosystem. Moreover, a very small number of maintainer accounts could be used to inject malicious code into the majority of all packages, a problem that has been increasing over time. Studying the potential for accidentally using vulnerable code, we ﬁnd that lack of maintenance causes many packages to depend on vulnerable code, even years after a vulnerability has become public. Our results provide evidence that npm suffers from single points of failure and that unmaintained packages threaten large code bases. We discuss several mitigation techniques, such as trusted maintainers and total ﬁrst-party security, and analyze their potential effectiveness.},
	language = {en},
	author = {Zimmermann, Markus and Staicu, Cristian-Alexandru and Pradel, Michael},
}

@article{fountaLargeScaleCrowdsourcing,
	title = {Large {Scale} {Crowdsourcing} and {Characterization} of {Twitter} {Abusive} {Behavior}},
	abstract = {In recent years online social networks have suffered an increase in sexism, racism, and other types of aggressive and cyberbullying behavior, often manifesting itself through offensive, abusive, or hateful language. Past scientiﬁc work focused on studying these forms of abusive activity in popular online social networks, such as Facebook and Twitter. Building on such work, we present an eight month study of the various forms of abusive behavior on Twitter, in a holistic fashion. Departing from past work, we examine a wide variety of labeling schemes, which cover different forms of abusive behavior. We propose an incremental and iterative methodology that leverages the power of crowdsourcing to annotate a large collection of tweets with a set of abuse-related labels. By applying our methodology and performing statistical analysis for label merging or elimination, we identify a reduced but robust set of labels to characterize abuse-related tweets. Finally, we offer a characterization of our annotated dataset of 80 thousand tweets, which we make publicly available for further scientiﬁc exploration.},
	language = {en},
	author = {Founta, Antigoni-Maria and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas},
	year = {2018},
	pages = {10},
}

@misc{noauthor_docker_nodate,
	title = {Docker {Hub} {Container} {Image} {Library} {\textbar} {App} {Containerization}},
	url = {https://hub.docker.com/},
	urldate = {2023-05-18},
}

@inproceedings{ferreira_containing_2021,
	title = {Containing {Malicious} {Package} {Updates} in npm with a {Lightweight} {Permission} {System}},
	doi = {10.1109/ICSE43902.2021.00121},
	abstract = {The large amount of third-party packages available in fast-moving software ecosystems, such as Node.js/npm, enables attackers to compromise applications by pushing malicious updates to their package dependencies. Studying the npm repository, we observed that many packages in the npm repository that are used in Node.js applications perform only simple computations and do not need access to filesystem or network APIs. This offers the opportunity to enforce least-privilege design per package, protecting applications and package dependencies from malicious updates. We propose a lightweight permission system that protects Node.js applications by enforcing package permissions at runtime. We discuss the design space of solutions and show that our system makes a large number of packages much harder to be exploited, almost for free.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ferreira, Gabriel and Jia, Limin and Sunshine, Joshua and Kästner, Christian},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Ecosystems, Runtime, Security, Software, design trade-offs, malicious package updates, package management, permission system, sandboxing, security, supplychain security},
	pages = {1334--1346},
}

@misc{zahan_openssf_2023,
	title = {{OpenSSF} {Scorecard}: {On} the {Path} {Toward} {Ecosystem}-wide {Automated} {Security} {Metrics}},
	shorttitle = {{OpenSSF} {Scorecard}},
	url = {http://arxiv.org/abs/2208.03412},
	abstract = {The OpenSSF Scorecard project is an automated tool to monitor the security health of open-source software. This study evaluates the applicability of the Scorecard tool and compares the security practices and gaps in the npm and PyPI ecosystems.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Zahan, Nusrat and Kanakiya, Parth and Hambleton, Brian and Shohan, Shohanuzzaman and Williams, Laurie},
	month = jan,
	year = {2023},
	note = {arXiv:2208.03412 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{noauthor_why_nodate,
	title = {Why {Package} {Signing} is not the {Holy} {Grail} · caremad},
	url = {https://caremad.io/posts/2013/07/packaging-signing-not-holy-grail/},
	urldate = {2023-05-12},
}

@article{jhaver_evaluating_2021,
	title = {Evaluating the {Effectiveness} of {Deplatforming} as a {Moderation} {Strategy} on {Twitter}},
	volume = {5},
	url = {https://dl.acm.org/doi/10.1145/3479525},
	doi = {10.1145/3479525},
	abstract = {Deplatforming refers to the permanent ban of controversial public figures with large followings on social media sites. In recent years, platforms like Facebook, Twitter and YouTube have deplatformed many influencers to curb the spread of offensive speech. We present a case study of three high-profile influencers who were deplatformed on Twitter---Alex Jones, Milo Yiannopoulos, and Owen Benjamin. Working with over 49M tweets, we found that deplatforming significantly reduced the number of conversations about all three individuals on Twitter. Further, analyzing the Twitter-wide activity of these influencers' supporters, we show that the overall activity and toxicity levels of supporters declined after deplatforming. We contribute a methodological framework to systematically examine the effectiveness of moderation interventions and discuss broader implications of using deplatforming as a moderation strategy.},
	number = {CSCW2},
	urldate = {2023-05-12},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Jhaver, Shagun and Boylston, Christian and Yang, Diyi and Bruckman, Amy},
	month = oct,
	year = {2021},
	keywords = {content moderation, freedom of speech, platform governance},
	pages = {381:1--381:30},
}

@article{jhaver_online_2018,
	title = {Online {Harassment} and {Content} {Moderation}: {The} {Case} of {Blocklists}},
	volume = {25},
	issn = {1073-0516},
	shorttitle = {Online {Harassment} and {Content} {Moderation}},
	url = {https://dl.acm.org/doi/10.1145/3185593},
	doi = {10.1145/3185593},
	abstract = {Online harassment is a complex and growing problem. On Twitter, one mechanism people use to avoid harassment is the blocklist, a list of accounts that are preemptively blocked from interacting with a subscriber. In this article, we present a rich description of Twitter blocklists – why they are needed, how they work, and their strengths and weaknesses in practice. Next, we use blocklists to interrogate online harassment – the forms it takes, as well as tactics used by harassers. Specifically, we interviewed both people who use blocklists to protect themselves, and people who are blocked by blocklists. We find that users are not adequately protected from harassment, and at the same time, many people feel that they are blocked unnecessarily and unfairly. Moreover, we find that not all users agree on what constitutes harassment. Based on our findings, we propose design interventions for social network sites with the aim of protecting people from harassment, while preserving freedom of speech.},
	number = {2},
	urldate = {2023-05-12},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Jhaver, Shagun and Ghoshal, Sucheta and Bruckman, Amy and Gilbert, Eric},
	month = mar,
	year = {2018},
	keywords = {Online harassment, blocking mechanisms, blocklists, gamergate, moderation},
	pages = {12:1--12:33},
}

@inproceedings{wei_theres_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {“{There}’s so much responsibility on users right now:” {Expert} {Advice} for {Staying} {Safer} {From} {Hate} and {Harassment}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {“{There}’s so much responsibility on users right now},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581229},
	doi = {10.1145/3544548.3581229},
	abstract = {Online hate and harassment poses a threat to the digital safety of people globally. In light of this risk, there is a need to equip as many people as possible with advice to stay safer online. We interviewed 24 experts to understand what threats and advice internet users should prioritize to prevent or mitigate harm. As part of this, we asked experts to evaluate 45 pieces of existing hate-and-harassment-specific digital-safety advice to understand why they felt advice was viable or not. We find that experts frequently had competing perspectives for which threats and advice they would prioritize. We synthesize sources of disagreement, while also highlighting the primary threats and advice where experts concurred. Our results inform immediate efforts to protect users from online hate and harassment, as well as more expansive socio-technical efforts to establish enduring safety.},
	urldate = {2023-05-12},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Miranda and Consolvo, Sunny and Kelley, Patrick Gage and Kohno, Tadayoshi and Roesner, Franziska and Thomas, Kurt},
	month = apr,
	year = {2023},
	keywords = {Security and privacy, advice, harassment, hate},
	pages = {1--17},
}

@inproceedings{mahar_squadbox_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Squadbox: {A} {Tool} to {Combat} {Email} {Harassment} {Using} {Friendsourced} {Moderation}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Squadbox},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174160},
	doi = {10.1145/3173574.3174160},
	abstract = {Communication platforms have struggled to provide effective tools for people facing harassment online. We conducted interviews with 18 recipients of online harassment to understand their strategies for coping, finding that they often resorted to asking friends for help. Inspired by these findings, we explore the feasibility of friendsourced moderation as a technique for combating online harassment. We present Squadbox, a tool to help recipients of email harassment coordinate a "squad" of friend moderators to shield and support them during attacks. Friend moderators intercept email from strangers and can reject, organize, and redirect emails, as well as collaborate on filters. Squadbox is designed to let its users implement highly customized workflows, as we found in interviews that harassment and preferences for mitigating it vary widely. We evaluated Squadbox on five pairs of friends in a field study, finding that participants could comfortably navigate around privacy and personalization concerns.},
	urldate = {2023-05-12},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mahar, Kaitlin and Zhang, Amy X. and Karger, David},
	month = apr,
	year = {2018},
	keywords = {crowdsourcing, email, friendsourcing, moderation, online harassment, private messages, social media},
	pages = {1--13},
}

@inproceedings{whitten_why_1999,
	address = {USA},
	series = {{SSYM}'99},
	title = {Why {Johnny} can't encrypt: a usability evaluation of {PGP} 5.0},
	shorttitle = {Why {Johnny} can't encrypt},
	abstract = {User errors cause or contribute to most computer security failures, yet user interfaces for security still tend to be clumsy, confusing, or near-nonexistent. Is this simply due to a failure to apply standard user interface design techniques to security? We argue that, on the contrary, effective security requires a different usability standard, and that it will not be achieved through the user interface design techniques appropriate to other types of consumer software. To test this hypothesis, we performed a case study of a security program which does have a good user interface by general standards: PGP 5.0. Our case study used a cognitive walkthrough analysis together with a laboratory user test to evaluate whether PGP 5.0 can be successfully used by cryptography novices to achieve effective electronic mail security. The analysis found a number of user interface design flaws that may contribute to security failures, and the user test demonstrated that when our test participants were given 90 minutes in which to sign and encrypt a message using PGP 5.0, the majority of them were unable to do so successfully. We conclude that PGP 5.0 is not usable enough to provide effective security for most computer users, despite its attractive graphical user interface, supporting our hypothesis that user interface design for effective security remains an open problem. We close with a brief description of our continuing work on the development and application of user interface design principles and techniques for security.},
	urldate = {2023-05-11},
	booktitle = {Proceedings of the 8th conference on {USENIX} {Security} {Symposium} - {Volume} 8},
	publisher = {USENIX Association},
	author = {Whitten, Alma and Tygar, J. D.},
	month = aug,
	year = {1999},
	pages = {14},
}

@misc{noauthor_security_nodate,
	title = {Security},
	url = {https://huggingface.co/docs/hub/security},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-05-09},
}

@misc{noauthor_hugging_2023,
	title = {Hugging {Face} – {The} {AI} community building the future.},
	url = {https://huggingface.co/},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-05-09},
	month = may,
	year = {2023},
}

@misc{relations_pgp_nodate,
	title = {{PGP} vs. sigstore: {A} {Recap} of the {Match} at {Maven} {Central}},
	shorttitle = {{PGP} vs. sigstore},
	url = {https://blog.sonatype.com/pgp-vs.-sigstore-a-recap-of-the-match-at-maven-central},
	abstract = {We put code-signing tools PGP and sigstore in a head-to-head match with Maven Central users to find a winner. The results may surprise you.},
	language = {en-us},
	urldate = {2023-05-09},
	author = {Relations, Sonatype Developer},
}

@misc{noauthor_maven_nodate,
	title = {Maven – {Introduction}},
	url = {https://maven.apache.org/what-is-maven.html},
	urldate = {2023-05-09},
}

@misc{noauthor_maven_nodate-1,
	title = {Maven – {Maven} {Central} {Repository}},
	url = {https://maven.apache.org/repository/index.html},
	urldate = {2023-05-09},
}

@misc{noauthor_openpgp_nodate,
	title = {{OpenPGP}},
	url = {https://www.openpgp.org/},
	abstract = {Email encryption. For all operating systems. Standing the test of time.},
	language = {en},
	urldate = {2023-05-09},
	journal = {OpenPGP},
}

@misc{project_gnu_2023,
	title = {The {GNU} {Privacy} {Guard}},
	copyright = {https://gnupg.org/copying.html},
	url = {https://gnupg.org/},
	abstract = {GnuPG is a free implementation of OpenPGP},
	language = {en},
	urldate = {2023-05-09},
	author = {Project, The People of the GnuPG},
	month = apr,
	year = {2023},
	note = {Publisher: The GnuPG Project},
}

@misc{whittacker_hacker_2016,
	title = {Hacker explains how he put "backdoor" in hundreds of {Linux} {Mint} downloads},
	url = {https://www.zdnet.com/article/hacker-hundreds-were-tricked-into-installing-linux-mint-backdoor/},
	abstract = {The hacker said their prime motivation for the backdoor was to build a botnet.},
	language = {en},
	urldate = {2023-05-09},
	journal = {ZDNET},
	author = {Whittacker, Zack},
	month = feb,
	year = {2016},
}

@misc{chowdhury_better_2022,
	title = {Better {Call} {Saltzer} {\textbackslash}\& {Schroeder}: {A} {Retrospective} {Security} {Analysis} of {SolarWinds} {\textbackslash}\& {Log4j}},
	shorttitle = {Better {Call} {Saltzer} {\textbackslash}\& {Schroeder}},
	url = {http://arxiv.org/abs/2211.02341},
	doi = {10.48550/arXiv.2211.02341},
	abstract = {Saltzer {\textbackslash}\& Schroeder's principles aim to bring security to the design of computer systems. We investigate SolarWinds Orion update and Log4j to unpack the intersections where observance of these principles could have mitigated the embedded vulnerabilities. The common principles that were not observed include {\textbackslash}emph\{fail safe defaults\}, {\textbackslash}emph\{economy of mechanism\}, {\textbackslash}emph\{complete mediation\} and {\textbackslash}emph\{least privilege\}. Then we explore the literature on secure software development interventions for developers to identify usable analysis tools and frameworks that can contribute towards improved observance of these principles. We focus on a system wide view of access of codes, checking access paths and aiding application developers with safe libraries along with an appropriate security task list for functionalities.},
	urldate = {2023-05-09},
	publisher = {arXiv},
	author = {Chowdhury, Partha Das and Tahaei, Mohammad and Rashid, Awais},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02341 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{poeplau_symbolic_2020,
	address = {USA},
	series = {{SEC}'20},
	title = {Symbolic execution with {SYMCC}: don't interpret, compile!},
	isbn = {978-1-939133-17-5},
	shorttitle = {Symbolic execution with {SYMCC}},
	abstract = {A major impediment to practical symbolic execution is speed, especially when compared to near-native speed solutions like fuzz testing. We propose a compilation-based approach to symbolic execution that performs better than state-of-the-art implementations by orders of magnitude. We present SYMCC, an LLVM-based C and C++ compiler that builds concolic execution right into the binary. It can be used by software developers as a drop-in replacement for clang and clang++, and we show how to add support for other languages with little effort. In comparison with KLEE, SYMCC is faster by up to three orders of magnitude and an average factor of 12. It also outperforms QSYM, a system that recently showed great performance improvements over other implementations, by up to two orders of magnitude and an average factor of 10. Using it on real-world software, we found that our approach consistently achieves higher coverage, and we discovered two vulnerabilities in the heavily tested OpenJPEG project, which have been confirmed by the project maintainers and assigned CVE identifiers.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 29th {USENIX} {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Poeplau, Sebastian and Francillon, Aurélien},
	month = aug,
	year = {2020},
	pages = {181--198},
}

@inproceedings{poeplau_symbolic_2020-1,
	address = {USA},
	series = {{SEC}'20},
	title = {Symbolic execution with {SYMCC}: don't interpret, compile!},
	isbn = {978-1-939133-17-5},
	shorttitle = {Symbolic execution with {SYMCC}},
	abstract = {A major impediment to practical symbolic execution is speed, especially when compared to near-native speed solutions like fuzz testing. We propose a compilation-based approach to symbolic execution that performs better than state-of-the-art implementations by orders of magnitude. We present SYMCC, an LLVM-based C and C++ compiler that builds concolic execution right into the binary. It can be used by software developers as a drop-in replacement for clang and clang++, and we show how to add support for other languages with little effort. In comparison with KLEE, SYMCC is faster by up to three orders of magnitude and an average factor of 12. It also outperforms QSYM, a system that recently showed great performance improvements over other implementations, by up to two orders of magnitude and an average factor of 10. Using it on real-world software, we found that our approach consistently achieves higher coverage, and we discovered two vulnerabilities in the heavily tested OpenJPEG project, which have been confirmed by the project maintainers and assigned CVE identifiers.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 29th {USENIX} {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Poeplau, Sebastian and Francillon, Aurélien},
	month = aug,
	year = {2020},
	pages = {181--198},
}

@inproceedings{lyu_mopt_2019,
	address = {USA},
	series = {{SEC}'19},
	title = {{MOPT}: optimized mutation scheduling for fuzzers},
	isbn = {978-1-939133-06-9},
	shorttitle = {{MOPT}},
	abstract = {Mutation-based fuzzing is one of the most popular vulnerability discovery solutions. Its performance of generating interesting test cases highly depends on the mutation scheduling strategies. However, existing fuzzers usually follow a specific distribution to select mutation operators, which is inefficient in finding vulnerabilities on general programs. Thus, in this paper, we present a novel mutation scheduling scheme MOPT, which enables mutation-based fuzzers to discover vulnerabilities more efficiently. MOPT utilizes a customized Particle Swarm Optimization (PSO) algorithm to find the optimal selection probability distribution of operators with respect to fuzzing effectiveness, and provides a pacemaker fuzzing mode to accelerate the convergence speed of PSO. We applied MOPT to the state of the-art fuzzers AFL, AFLFast and VUzzer, and implemented MOPT-AFL, -AFLFast and -VUzzer respectively, and then evaluated them on 13 real world open-source programs. The results showed that, MOPT-AFL could find 170\% more security vulnerabilities and 350\% more crashes than AFL. MOPT-AFLFast and MOPT-VUzzer also outperform their counterparts. Furthermore, the extensive evaluation also showed that MOPT provides a good rationality, compatibility and steadiness, while introducing negligible costs.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 28th {USENIX} {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Lyu, Chenyang and Ji, Shouling and Zhang, Chao and Li, Yuwei and Lee, Wei-Han and Song, Yu and Beyah, Raheem},
	month = aug,
	year = {2019},
	pages = {1949--1966},
}

@inproceedings{cho_intriguer_2019,
	address = {New York, NY, USA},
	series = {{CCS} '19},
	title = {Intriguer: {Field}-{Level} {Constraint} {Solving} for {Hybrid} {Fuzzing}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {Intriguer},
	url = {https://dl.acm.org/doi/10.1145/3319535.3354249},
	doi = {10.1145/3319535.3354249},
	abstract = {Hybrid fuzzing, which combines fuzzing and concolic execution, is promising in light of the recent performance improvements in concolic engines. We have observed that there is room for further improvement: symbolic emulation is still slow, unnecessary constraints dominate solving time, resources are overly allocated, and hard-to-trigger bugs are missed. To address these problems, we present a new hybrid fuzzer named Intriguer. The key idea of Intriguer is field-level constraint solving, which optimizes symbolic execution with field-level knowledge. Intriguer performs instruction-level taint analysis and records execution traces without data transfer instructions like mov. Intriguer then reduces the execution traces for tainted instructions that accessed a wide range of input bytes, and infers input fields to build field transition trees. With these optimizations, Intriguer can efficiently perform symbolic emulation for more relevant instructions and invoke a solver for complicated constraints only. Our evaluation results indicate that Intriguer outperforms the state-of-the-art fuzzers: Intriguer found all the bugs in the LAVA-M(5h) benchmark dataset for ground truth performance, and also discovered 43 new security bugs in seven real-world programs. We reported the bugs and received 23 new CVEs.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Cho, Mingi and Kim, Seoyoung and Kwon, Taekyoung},
	month = nov,
	year = {2019},
	keywords = {constraint solving, fuzzing, hybrid fuzzing},
	pages = {515--530},
}

@misc{noauthor_google_nodate,
	title = {Google {Scholar}},
	url = {https://scholar.google.com/},
	urldate = {2023-05-06},
}

@misc{noauthor_criu_nodate,
	title = {{CRIU}},
	url = {https://criu.org/Main_Page},
	urldate = {2023-05-06},
}

@misc{noauthor_more_nodate,
	title = {More about {AFL} — {AFL} 2.53b documentation},
	url = {https://afl-1.readthedocs.io/en/latest/about_afl.html},
	urldate = {2023-05-06},
}

@misc{noauthor_universal_nodate,
	title = {Universal {TUN}/{TAP} device driver — {The} {Linux} {Kernel} documentation},
	url = {https://docs.kernel.org/networking/tuntap.html},
	urldate = {2023-05-06},
}

@article{landis_measurement_1977,
	title = {The {Measurement} of {Observer} {Agreement} for {Categorical} {Data}},
	volume = {33},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2529310},
	doi = {10.2307/2529310},
	abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
	number = {1},
	urldate = {2023-05-06},
	journal = {Biometrics},
	author = {Landis, J. Richard and Koch, Gary G.},
	year = {1977},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {159--174},
}

@inproceedings{poncelet_so_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {So {Many} {Fuzzers}, {So} {Little} {Time}✱: {Experience} from {Evaluating} {Fuzzers} on the {Contiki}-{NG} {Network} ({Hay}){Stack}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {So {Many} {Fuzzers}, {So} {Little} {Time}✱},
	url = {https://dl.acm.org/doi/10.1145/3551349.3556946},
	doi = {10.1145/3551349.3556946},
	abstract = {Fuzz testing (“fuzzing”) is a widely-used and effective dynamic technique to discover crashes and security vulnerabilities in software, supported by numerous tools, which keep improving in terms of their detection capabilities and speed of execution. In this paper, we report our findings from using state-of-the-art mutation-based and hybrid fuzzers (AFL, Angora, Honggfuzz, Intriguer, MOpt-AFL, QSym, and SymCC) on a non-trivial code base, that of Contiki-NG, to expose and fix serious vulnerabilities in various layers of its network stack, during a period of more than three years. As a by-product, we provide a Git-based platform which allowed us to create and apply a new, quite challenging, open-source bug suite for evaluating fuzzers on real-world software vulnerabilities. Using this bug suite, we present an impartial and extensive evaluation of the effectiveness of these fuzzers, and measure the impact that sanitizers have on it. Finally, we offer our experiences and opinions on how fuzzing tools should be used and evaluated in the future.},
	urldate = {2023-05-05},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Poncelet, Clement and Sagonas, Konstantinos and Tsiftes, Nicolas},
	month = jan,
	year = {2023},
	keywords = {Contiki-NG, IoT, Software security, coverage-guided fuzzing, fuzz testing, hybrid fuzzing, security testing},
	pages = {1--12},
}

@book{alur_principles_2015,
	title = {Principles of {Cyber}-{Physical} {Systems}},
	isbn = {978-0-262-02911-7},
	abstract = {A foundational text that offers a rigorous introduction to the principles of design, specification, modeling, and analysis of cyber-physical systems.A cyber-physical system consists of a collection of computing devices communicating with one another and interacting with the physical world via sensors and actuators in a feedback loop. Increasingly, such systems are everywhere, from smart buildings to medical devices to automobiles. This textbook offers a rigorous and comprehensive introduction to the principles of design, specification, modeling, and analysis of cyber-physical systems. The book draws on a diverse set of subdisciplines, including model-based design, concurrency theory, distributed algorithms, formal methods of specification and verification, control theory, real-time systems, and hybrid systems, explaining the core ideas from each that are relevant to system design and analysis.The book explains how formal models provide mathematical abstractions to manage the complexity of a system design. It covers both synchronous and asynchronous models for concurrent computation, continuous-time models for dynamical systems, and hybrid systems for integrating discrete and continuous evolution. The role of correctness requirements in the design of reliable systems is illustrated with a range of specification formalisms and the associated techniques for formal verification. The topics include safety and liveness requirements, temporal logic, model checking, deductive verification, stability analysis of linear systems, and real-time scheduling algorithms. Principles of modeling, specification, and analysis are illustrated by constructing solutions to representative design problems from distributed algorithms, network protocols, control design, and robotics.This book provides the rapidly expanding field of cyber-physical systems with a long-needed foundational text by an established authority. It is suitable for classroom use or as a reference for professionals.},
	language = {en},
	publisher = {MIT Press},
	author = {Alur, Rajeev},
	month = apr,
	year = {2015},
	note = {Google-Books-ID: E7QICAAAQBAJ},
	keywords = {Computers / Computer Engineering, Technology \& Engineering / Electronics / General},
}

@inproceedings{liu_characterizing_2022,
	address = {Melbourne, Australia},
	series = {{ASE} '21},
	title = {Characterizing transaction-reverting statements in ethereum smart contracts},
	isbn = {978-1-66540-337-5},
	url = {https://dl.acm.org/doi/10.1109/ASE51524.2021.9678597},
	doi = {10.1109/ASE51524.2021.9678597},
	abstract = {Smart contracts are programs stored on blockchains to execute transactions. When input constraints or security properties are violated at runtime, the transaction being executed by a smart contract needs to be reverted to avoid undesirable consequences. On Ethereum, the most popular blockchain that supports smart contracts, developers can choose among three transaction-reverting statements (i.e., require, if...revert, and if...throw) to handle anomalous transactions. While these transaction-reverting statements are vital for preventing smart contracts from exhibiting abnormal behaviors or suffering malicious attacks, there is limited understanding of how they are used in practice. In this work, we perform the first empirical study to characterize transaction-reverting statements in Ethereum smart contracts. We measured the prevalence of these statements in 3,866 verified smart contracts from popular dapps and built a taxonomy of their purposes via manually analyzing 557 transaction-reverting statements. We also compared template contracts and their corresponding custom contracts to understand how developers customize the use of transaction-reverting statements. Finally, we analyzed the security impact of transaction-reverting statements by removing them from smart contracts and comparing the mutated contracts against the original ones. Our study led to important findings. For example, we found that transaction-reverting statements are commonly used to perform seven types of authority verifications or validity checks, and missing such statements may compromise the security of smart contracts. We also found that current smart contract security analyzers cannot effectively handle transaction-reverting statements when detecting security vulnerabilities. Our findings can shed light on further research in the broad area of smart contract quality assurance and provide practical guidance to smart contract developers on the appropriate use of transaction-reverting statements.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Liu, Lu and Wei, Lili and Zhang, Wuqi and Wen, Ming and Liu, Yepang and Cheung, Shing-Chi},
	month = jun,
	year = {2022},
	keywords = {empirical study, ethereum, security vulnerability, smart contract, transaction-reverting statement},
	pages = {630--641},
}

@inproceedings{richter_are_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Are {Neural} {Bug} {Detectors} {Comparable} to {Software} {Developers} on {Variable} {Misuse} {Bugs}?},
	isbn = {978-1-4503-9475-8},
	url = {https://dl.acm.org/doi/10.1145/3551349.3561156},
	doi = {10.1145/3551349.3561156},
	abstract = {Debugging, that is, identifying and fixing bugs in software, is a central part of software development. Developers are therefore often confronted with the task of deciding whether a given code snippet contains a bug, and if yes, where. Recently, data-driven methods have been employed to learn this task of bug detection, resulting (amongst others) in so called neural bug detectors. Neural bug detectors are trained on millions of buggy and correct code snippets. Given the “neural learning” procedure, it seems likely that neural bug detectors – on the specific task of finding bugs – have a performance similar to human software developers. For this work, we set out to substantiate or refute such a hypothesis. We report on the results of an empirical study with over 100 software developers, targeting the comparison of humans and neural bug detectors. As detection task, we chose a specific form of bugs (variable misuse bugs) for which neural bug detectors have recently made significant progress. Our study shows that despite the fact that neural bug detectors see millions of such misuse bugs during training, software developers – when conducting bug detection as a majority decision – are slightly better than neural bug detectors on this class of bugs. Altogether, we find a large overlap in the performance, both for classifying code as buggy and for localizing the buggy line in the code. In comparison to developers, one of the two evaluated neural bug detectors, however, raises a higher number of false alarms in our study.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Richter, Cedric and Haltermann, Jan and Jakobs, Marie-Christine and Pauck, Felix and Schott, Stefan and Wehrheim, Heike},
	month = jan,
	year = {2023},
	keywords = {Bug detection, empirical study., variable misuse bugs},
	pages = {1--12},
}

@inproceedings{wang_characterizing_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Characterizing and detecting bugs in {WeChat} mini-programs},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510114},
	doi = {10.1145/3510003.3510114},
	abstract = {Built on the WeChat social platform, WeChat Mini-Programs are widely used by more than 400 million users every day. Consequently, the reliability of Mini-Programs is particularly crucial. However, WeChat Mini-Programs suffer from various bugs related to execution environment, lifecycle management, asynchronous mechanism, etc. These bugs have seriously affected users' experience and caused serious impacts. In this paper, we conduct the first empirical study on 83 WeChat Mini-Program bugs, and perform an in-depth analysis of their root causes, impacts and fixes. From this study, we obtain many interesting findings that can open up new research directions for combating WeChat Mini-Program bugs. Based on the bug patterns found in our study, we further develop WeDetector to detect WeChat Mini-Program bugs. Our evaluation on 25 real-world Mini-Programs has found 11 previously unknown bugs, and 7 of them have been confirmed by developers.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Tao and Xu, Qingxin and Chang, Xiaoning and Dou, Wensheng and Zhu, Jiaxin and Xie, Jinhui and Deng, Yuetang and Yang, Jianbo and Yang, Jiaheng and Wei, Jun and Huang, Tao},
	month = jul,
	year = {2022},
	keywords = {WeChat mini-programs, bug detection, empirical study},
	pages = {363--375},
}

@inproceedings{tufano_using_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Using pre-trained models to boost code review automation},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510621},
	doi = {10.1145/3510003.3510621},
	abstract = {Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tufano, Rosalia and Masiero, Simone and Mastropaolo, Antonio and Pascarella, Luca and Poshyvanyk, Denys and Bavota, Gabriele},
	month = jul,
	year = {2022},
	keywords = {code review, empirical study, machine learning on code},
	pages = {2291--2302},
}

@inproceedings{florez_retrieving_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Retrieving data constraint implementations using fine-grained code patterns},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510167},
	doi = {10.1145/3510003.3510167},
	abstract = {Business rules are an important part of the requirements of software systems that are meant to support an organization. These rules describe the operations, definitions, and constraints that apply to the organization. Within the software system, business rules are often translated into constraints on the values that are required or allowed for data, called data constraints. Business rules are subject to frequent changes, which in turn require changes to the corresponding data constraints in the software. The ability to efficiently and precisely identify where data constraints are implemented in the source code is essential for performing such necessary changes. In this paper, we introduce Lasso, the first technique that automatically retrieves the method and line of code where a given data constraint is enforced. Lasso is based on traceability link recovery approaches and leverages results from recent research that identified line-of-code level implementation patterns for data constraints. We implement three versions of Lasso that can retrieve data constraint implementations when they are implemented with any one of 13 frequently occurring patterns. We evaluate the three versions on a set of 299 data constraints from 15 real-world Java systems, and find that they improve method-level link recovery by 30\%, 70\%, and 163\%, in terms of true positives within the first 10 results, compared to their text-retrieval-based baseline. More importantly, the Lasso variants correctly identify the line of code implementing the constraint inside the methods for 68\% of the 299 constraints.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Florez, Juan Manuel and Perry, Jonathan and Wei, Shiyi and Marcus, Andrian},
	month = jul,
	year = {2022},
	keywords = {business rule, code pattern, data constraint, empirical study, fine-grained traceability, traceability link recovery},
	pages = {1893--1905},
}

@inproceedings{tuna_bug_2022,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIP} '22},
	title = {Bug tracking process smells in practice},
	isbn = {978-1-4503-9226-6},
	url = {https://dl.acm.org/doi/10.1145/3510457.3513080},
	doi = {10.1145/3510457.3513080},
	abstract = {Software teams use bug tracking (BT) tools to report and manage bugs. Each record in a bug tracking system (BTS) is a reporting entity consisting of several information fields. The contents of the reports are similar across different tracking tools, though not the same. The variation in the workflow between teams prevents defining an ideal process of running BTS. Nevertheless, there are best practices reported both in white and gray literature. Developer teams may not adopt the best practices in their BT process. This study investigates the non-compliance of developers with best practices, so-called smells, in the BT process. We mine bug reports of four projects in the BTS of JetBrains, a software company, to observe the prevalence of BT smells in an industrial setting. Also, we survey developers to see (1) if they recognize the smells, (2) their perception of the severity of the smells, and (3) the potential benefits of a BT process smell detection tool. We found that (1) smells occur, and their detection requires a solid understanding of the BT practices of the projects, (2) smell severity perception varies across smell types, and (3) developers considered that a smell detection tool would be useful for six out of the 12 smell categories.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {Association for Computing Machinery},
	author = {Tuna, Erdem and Kovalenko, Vladimir and Tüzün, Eray},
	month = oct,
	year = {2022},
	keywords = {bug tracking smells, bug tracking system, developer perception, empirical study, process smell},
	pages = {77--86},
}

@inproceedings{shi_evaluation_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {On the evaluation of neural code summarization},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510060},
	doi = {10.1145/3510003.3510060},
	abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Besides, we discover and resolve an important and previously unknown bug in BLEU calculation in a commonly-used software package. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code preprocessing choices can have a large (from -18\% to +25\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
	month = jul,
	year = {2022},
	keywords = {code summarization, deep learning, empirical study, evaluation},
	pages = {1597--1608},
}

@inproceedings{li_nufix_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Nufix: escape from {NuGet} dependency maze},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Nufix},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510118},
	doi = {10.1145/3510003.3510118},
	abstract = {Developers usually suffer from {\textless}u{\textgreater}d{\textless}/u{\textgreater}ependency {\textless}u{\textgreater}m{\textless}/u{\textgreater}aze (DM) issues, i.e., package dependency constraints are violated when a project's platform or dependencies are changed. This problem is especially serious in .NET ecosystem due to its fragmented platforms (e.g., .NET Framework, .NET Core, and .NET Standard). Fixing DM issues is challenging due to the complexity of dependency constraints: multiple DM issues often occur in one project; solving one DM issue usually causes another DM issue cropping up; the exponential search space of possible dependency combinations is also a barrier. In this paper, we aim to help .NET developers tackle the DM issues. First, we empirically studied a set of real DM issues, learning their common fixing strategies and developers' preferences in adopting these strategies. Based on these findings, we propose NuFix, an automated technique to repair DM issues. NuFix formulates the repair task as a binary integer linear optimization problem to effectively derive an optimal fix in line with the learnt developers' preferences. The experiment results and expert validation show that NuFix can generate high-quality fixes for all the DM issues with 262 popular .NET projects. Encouragingly, 20 projects (including affected projects such as Dropbox) have approved and merged our generated fixes, and shown great interests in our technique.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Li, Zhenming and Wang, Ying and Lin, Zeqi and Cheung, Shing-Chi and Lou, Jian-Guang},
	month = jul,
	year = {2022},
	keywords = {.NET, NuGet, dependencies, empirical study},
	pages = {1545--1557},
}

@inproceedings{zhu_learning_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Learning and programming challenges of rust: a mixed-methods study},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Learning and programming challenges of rust},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510164},
	doi = {10.1145/3510003.3510164},
	abstract = {Rust is a young systems programming language designed to provide both the safety guarantees of high-level languages and the execution performance of low-level languages. To achieve this design goal, Rust provides a suite of safety rules and checks against those rules at the compile time to eliminate many memory-safety and thread-safety issues. Due to its safety and performance, Rust's popularity has increased significantly in recent years, and it has already been adopted to build many safety-critical software systems. It is critical to understand the learning and programming challenges imposed by Rust's safety rules. For this purpose, we first conducted an empirical study through close, manual inspection of 100 Rust-related Stack Overflow questions. We sought to understand (1) what safety rules are challenging to learn and program with, (2) under which contexts a safety rule becomes more difficult to apply, and (3) whether the Rust compiler is sufficiently helpful in debugging safety-rule violations. We then performed an online survey with 101 Rust programmers to validate the findings of the empirical study. We invited participants to evaluate program variants that differ from each other, either in terms of violated safety rules or the code constructs involved in the violation, and compared the participants' performance on the variants. Our mixed-methods investigation revealed a range of consistent findings that can benefit Rust learners, practitioners, and language designers.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Shuofei and Zhang, Ziyi and Qin, Boqin and Xiong, Aiping and Song, Linhai},
	month = jul,
	year = {2022},
	keywords = {empirical study, online survey, programming challenges, rust},
	pages = {1269--1281},
}

@inproceedings{nema_analyzing_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Analyzing user perspectives on mobile app privacy at scale},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510079},
	doi = {10.1145/3510003.3510079},
	abstract = {In this paper we present a methodology to analyze users' concerns and perspectives about privacy at scale. We leverage NLP techniques to process millions of mobile app reviews and extract privacy concerns. Our methodology is composed of a binary classifier that distinguishes between privacy and non-privacy related reviews. We use clustering to gather reviews that discuss similar privacy concerns, and employ summarization metrics to extract representative reviews to summarize each cluster. We apply our methods on 287M reviews for about 2M apps across the 29 categories in Google Play to identify top privacy pain points in mobile apps. We identified approximately 440K privacy related reviews. We find that privacy related reviews occur in all 29 categories, with some issues arising across numerous app categories and other issues only surfacing in a small set of app categories. We show empirical evidence that confirms dominant privacy themes - concerns about apps requesting unnecessary permissions, collection of personal information, frustration with privacy controls, tracking and the selling of personal data. As far as we know, this is the first large scale analysis to confirm these findings based on hundreds of thousands of user inputs. We also observe some unexpected findings such as users warning each other not to install an app due to privacy issues, users uninstalling apps due to privacy reasons, as well as positive reviews that reward developers for privacy friendly apps. Finally we discuss the implications of our method and findings for developers and app stores.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nema, Preksha and Anthonysamy, Pauline and Taft, Nina and Peddinti, Sai Teja},
	month = jul,
	year = {2022},
	keywords = {empirical, mobile apps, nlp, privacy},
	pages = {112--124},
}

@inproceedings{wang_no_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence},
	isbn = {978-1-4503-9413-0},
	shorttitle = {No more fine-tuning?},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549113},
	doi = {10.1145/3540250.3549113},
	abstract = {Pre-trained models have been shown effective in many code intelligence tasks. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream data, while in practice, the scenarios with scarce data are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this paper, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with three code intelligence tasks including defect prediction, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all three tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26\% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chaozheng and Yang, Yuanhang and Gao, Cuiyun and Peng, Yun and Zhang, Hongyu and Lyu, Michael R.},
	month = nov,
	year = {2022},
	keywords = {code intelligence, empirical study, prompt tuning},
	pages = {382--394},
}

@inproceedings{wang_exploratory_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	month = aug,
	year = {2021},
	keywords = {Unmanned aerial vehicles, empirical study, software bugs},
	pages = {20--31},
}

@inproceedings{kim_empirical_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An empirical study of deep transfer learning-based program repair for {Kotlin} projects},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3558967},
	doi = {10.1145/3540250.3558967},
	abstract = {Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning. This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307\%. Furthermore, we confirmed that the performance was improved by 532\% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696\%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Misoo and Kim, Youngkyoung and Jeong, Hohyeon and Heo, Jinseok and Kim, Sungoh and Chung, Hyunhee and Lee, Eunseok},
	month = nov,
	year = {2022},
	keywords = {Deep learning-based program repair, Empirical study, Industrial Kotlin project, SonarQube defects, Transfer learning},
	pages = {1441--1452},
}

@inproceedings{maddila_nalanda_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Nalanda: a socio-technical graph platform for building software analytics tools at enterprise scale},
	isbn = {978-1-4503-9413-0},
	shorttitle = {Nalanda},
	url = {https://dl.acm.org/doi/10.1145/3540250.3558949},
	doi = {10.1145/3540250.3558949},
	abstract = {Software development is information-dense knowledge work that requires collaboration with other developers and awareness of artifacts such as work items, pull requests, and file changes. With the speed of development increasing, information overload and information discovery are challenges for people developing and maintaining these systems. Finding information about similar code changes and experts is difficult for software engineers, especially when they work in large software systems or have just recently joined a project. In this paper, we build a large scale data platform named Nalanda platform to address the challenges of information overload and discovery. Nalanda contains two subsystems: (1) a large scale socio-technical graph system, named Nalanda graph system, and (2) a large scale index system, named Nalanda index system that aims at satisfying the information needs of software developers. To show the versatility of the Nalanda platform, we built two applications: (1) a software analytics application with a news feed named MyNalanda that has Daily Active Users (DAU) of 290 and Monthly Active Users (MAU) of 590, and (2) a recommendation system for related work items and pull requests that accomplished similar tasks (artifact recommendation) and a recommendation system for subject matter experts (expert recommendation), augmented by the Nalanda socio-technical graph. Initial studies of the two applications found that developers and engineering managers are favorable toward continued use of the news feed application for information discovery. The studies also found that developers agreed that a system like Nalanda artifact and expert recommendation application could reduce the time spent and the number of places needed to visit to find information.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Maddila, Chandra and Shanbhogue, Suhas and Agrawal, Apoorva and Zimmermann, Thomas and Bansal, Chetan and Forsgren, Nicole and Agrawal, Divyanshu and Herzig, Kim and van Deursen, Arie},
	month = nov,
	year = {2022},
	keywords = {Collaborative software development, Empirical study, Recommender Systems for Software Engineering, Socio-Technical Graphs},
	pages = {1246--1256},
}

@inproceedings{liang_understanding_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Understanding skills for {OSS} communities on {GitHub}},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549082},
	doi = {10.1145/3540250.3549082},
	abstract = {The development of open source software (OSS) is a broad field which requires diverse skill sets. For example, maintainers help lead the project and promote its longevity, technical writers assist with documentation, bug reporters identify defects in software, and developers program the software. However, it is unknown which skills are used in OSS development as well as OSS contributors' general attitudes towards skills in OSS. In this paper, we address this gap by administering a survey to a diverse set of 455 OSS contributors. Guided by these responses as well as prior literature on software development expertise and social factors of OSS, we develop a model of skills in OSS that considers the many contexts OSS contributors work in. This model has 45 skills in the following 9 categories: technical skills, working styles, problem solving, contribution types, project-specific skills, interpersonal skills, external relations, management, and characteristics. Through a mix of qualitative and quantitative analyses, we find that OSS contributors are actively motivated to improve skills and perceive many benefits in sharing their skills with others. We then use this analysis to derive a set of design implications and best practices for those who incorporate skills into OSS tools and platforms, such as GitHub.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Liang, Jenny T. and Zimmermann, Thomas and Ford, Denae},
	month = nov,
	year = {2022},
	keywords = {empirical study, open source software, skills, survey},
	pages = {170--182},
}

@inproceedings{wei_api_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {{API} recommendation for machine learning libraries: how far are we?},
	isbn = {978-1-4503-9413-0},
	shorttitle = {{API} recommendation for machine learning libraries},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549124},
	doi = {10.1145/3540250.3549124},
	abstract = {Application Programming Interfaces (APIs) are designed to help developers build software more effectively. Recommending the right APIs for specific tasks is gaining increasing attention among researchers and developers. However, most of the existing approaches are mainly evaluated for general programming tasks using statically typed programming languages such as Java. Little is known about their practical effectiveness and usefulness for machine learning (ML) programming tasks with dynamically typed programming languages such as Python, whose paradigms are fundamentally different from general programming tasks. This is of great value considering the increasing popularity of ML and the large number of new questions appearing on question answering websites. In this work, we set out to investigate the effectiveness of existing API recommendation approaches for Python-based ML programming tasks from Stack Overflow (SO). Specifically, we conducted an empirical study of six widely-used Python-based ML libraries using two state-of-the-art API recommendation approaches, i.e., BIKER and DeepAPI. We found that the existing approaches perform poorly for two main reasons: (1) Python-based ML tasks often require significant long API sequences; and (2) there are common API usage patterns in Python-based ML programming tasks that existing approaches cannot handle. Inspired by our findings, we proposed a simple but effective frequent itemset mining-based approach, i.e., FIMAX, to boost API recommendation approaches, i.e., enhance existing API recommendation approaches for Python-based ML programming tasks by leveraging the common API usage information from SO questions. Our evaluation shows that FIMAX improves existing state-of-the-art API recommendation approaches by up to 54.3\% and 57.4\% in MRR and MAP, respectively. Our user study with 14 developers further demonstrates the practicality of FIMAX for API recommendation.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Moshi and Huang, Yuchao and Wang, Junjie and Shin, Jiho and Harzevili, Nima Shiri and Wang, Song},
	month = nov,
	year = {2022},
	keywords = {API recommendation, Python-based machine learning library, empirical software engineering},
	pages = {370--381},
}

@inproceedings{shi_are_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Are we building on the rock? on the importance of data preprocessing for code summarization},
	isbn = {978-1-4503-9413-0},
	shorttitle = {Are we building on the rock?},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549145},
	doi = {10.1145/3540250.3549145},
	abstract = {Code summarization, the task of generating useful comments given the code, has long been of interest. Most of the existing code summarization models are trained and validated on widely-used code comment benchmark datasets. However, little is known about the quality of the benchmark datasets built from real-world projects. Are the benchmark datasets as good as expected? To bridge the gap, we conduct a systematic research to assess and improve the quality of four benchmark datasets widely used for code summarization tasks. First, we propose an automated code-comment cleaning tool that can accurately detect noisy data caused by inappropriate data preprocessing operations from existing benchmark datasets. Then, we apply the tool to further assess the data quality of the four benchmark datasets, based on the detected noises. Finally, we conduct comparative experiments to investigate the impact of noisy data on the performance of code summarization models. The results show that these data preprocessing noises widely exist in all four benchmark datasets, and removing these noisy data leads to a significant improvement on the performance of code summarization. We believe that the findings and insights will enable a better understanding of data quality in code summarization tasks, and pave the way for relevant research and practice.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Lin and Mu, Fangwen and Chen, Xiao and Wang, Song and Wang, Junjie and Yang, Ye and Li, Ge and Xia, Xin and Wang, Qing},
	month = nov,
	year = {2022},
	keywords = {Code Summarization, Data Quality, Empirical Study},
	pages = {107--119},
}

@inproceedings{shi_large-scale_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Large-scale analysis of non-termination bugs in real-world {OSS} projects},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549129},
	doi = {10.1145/3540250.3549129},
	abstract = {Termination is a crucial program property. Non-termination bugs can be subtle to detect and may remain hidden for long before they take effect. Many real-world programs still suffer from vast consequences (e.g., no response) caused by non-termination bugs. As a classic problem, termination proving has been studied for many years. Many termination checking tools and techniques have been developed and demonstrated effectiveness on existing well-established benchmarks. However, the capability of these tools in finding practical non-termination bugs has yet to be tested on real-world projects. To fill in this gap, in this paper, we conducted the first large-scale empirical study of non-termination bugs in real-world OSS projects. Specifically, we first devoted substantial manual efforts in collecting and analyzing 445 non-termination bugs from 3,142 GitHub commits and provided a systematic classifi-cation of the bugs based on their root causes. We constructed a new benchmark set characterizing the real-world bugs with simplified programs, including a non-termination dataset with 56 real and reproducible non-termination bugs and a termination dataset with 58 fixed programs. With the constructed benchmark, we evaluated five state-of-the-art termination analysis tools. The results show that the capabilities of the tested tools to make correct verdicts have obviously dropped compared with the existing benchmarks. Meanwhile, we identified the challenges and limitations that these tools face by analyzing the root causes of their unhandled bugs. Fi-nally, we summarized the challenges and future research directions for detecting non-termination bugs in real-world projects.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Xiuhan and Xie, Xiaofei and Li, Yi and Zhang, Yao and Chen, Sen and Li, Xiaohong},
	month = nov,
	year = {2022},
	keywords = {Benchmarking, Empirical Study, Non-termination Bug},
	pages = {256--268},
}

@inproceedings{shen_comprehensive_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {A comprehensive study of deep learning compiler bugs},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468591},
	doi = {10.1145/3468264.3468591},
	abstract = {There are increasing uses of deep learning (DL) compilers to generate optimized code, boosting the runtime performance of DL models on specific hardware. Like their traditional counterparts, DL compilers can generate incorrect code, resulting in unexpected model behaviors that may cause catastrophic consequences in mission-critical systems. On the other hand, the DL models processed by DL compilers differ fundamentally from imperative programs in that the program logic in DL models is implicit. As such, various characteristics of the bugs arising from traditional compilers need to be revisited in the context of DL compilers. In this paper, we present the first systematic study of DL compiler bugs by analyzing 603 bugs arising in three popular DL compilers (i.e., TVM from Apache, Glow from Facebook, and nGraph from Intel). We analyzed these bugs according to their root causes, symptoms, and the stages where they occur during compilation. We obtain 12 findings, and provide a series of valuable guidelines for future work on DL compiler bug detection and debugging. For example, a large portion (nearly 20\%) of DL compiler bugs are related to types, especially tensor types. The analysis of these bugs helps design new mutation operators (e.g., adding type cast for a tensor to promote implicit type conversion in subsequent tensor computations) to facilitate type-related bug detection. Further, we developed TVMfuzz as a proof-of-concept application of our findings to test the TVM DL compiler. It generates new tests based on TVM's original test suite. They expose 8 TVM bugs that are missed by the original test suite. The result demonstrates the usefulness of our findings.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shen, Qingchao and Ma, Haoyang and Chen, Junjie and Tian, Yongqiang and Cheung, Shing-Chi and Chen, Xiang},
	month = aug,
	year = {2021},
	keywords = {Compiler Testing, Deep Learning, Deep Learning Compiler Bug, Empirical Study},
	pages = {968--980},
}

@inproceedings{tao_demystifying_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {Demystifying “bad” error messages in data science libraries},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468560},
	doi = {10.1145/3468264.3468560},
	abstract = {Error messages are critical starting points for debugging. Unfortunately, they seem to be notoriously cryptic, confusing, and uninformative. Yet, it still remains a mystery why error messages receive such bad reputations, especially given that they are merely very short pieces of natural language text. In this paper, we empirically demystify the causes and fixes of "bad" error messages, by qualitatively studying 201 Stack Overflow threads and 335 GitHub issues. We specifically focus on error messages encountered in data science development, which is an increasingly important but not well studied domain. We found that the causes of "bad" error messages are far more complicated than poor phrasing or flawed articulation of error message content. Many error messages are inherently and inevitably misleading or uninformative, since libraries do not know user intentions and cannot "see" external errors. Fixes to error-message-related issues mostly involve source code changes, while exclusive message content updates only take up a small portion. In addition, whether an error message is informative or helpful is not always clear-cut; even error messages that clearly pinpoint faults and resolutions can still cause confusion for certain users. These findings thus call for a more in-depth investigation on how error messages should be evaluated and improved in the future.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tao, Yida and Chen, Zhihui and Liu, Yepang and Xuan, Jifeng and Xu, Zhiwu and Qin, Shengchao},
	month = aug,
	year = {2021},
	keywords = {Error message, data science, debugging aid, empirical study},
	pages = {818--829},
}

@inproceedings{shi_first_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {A first look at developers’ live chat on {Gitter}},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468562},
	doi = {10.1145/3468264.3468562},
	abstract = {Modern communication platforms such as Gitter and Slack play an increasingly critical role in supporting software teamwork, especially in open source development.Conversations on such platforms often contain intensive, valuable information that may be used for better understanding OSS developer communication and collaboration. However, little work has been done in this regard. To bridge the gap, this paper reports a first comprehensive empirical study on developers' live chat, investigating when they interact, what community structures look like, which topics are discussed, and how they interact. We manually analyze 749 dialogs in the first phase, followed by an automated analysis of over 173K dialogs in the second phase. We find that developers tend to converse more often on weekdays, especially on Wednesdays and Thursdays (UTC), that there are three common community structures observed, that developers tend to discuss topics such as API usages and errors, and that six dialog interaction patterns are identified in the live chat communities. Based on the findings, we provide recommendations for individual developers and OSS communities, highlight desired features for platform vendors, and shed light on future research directions. We believe that the findings and insights will enable a better understanding of developers' live chat, pave the way for other researchers, as well as a better utilization and mining of knowledge embedded in the massive chat history.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shi, Lin and Chen, Xiao and Yang, Ye and Jiang, Hanzhi and Jiang, Ziyou and Niu, Nan and Wang, Qing},
	month = aug,
	year = {2021},
	keywords = {Empirical Study, Live chat, Open source, Team communication},
	pages = {391--403},
}

@article{mckenna_jisc_2004,
	title = {{JISC} (draft) {Software} {Quality} {Assurance} {Policy}},
	language = {en},
	author = {McKenna, Richard and Committee, Joint Information Systems},
	year = {2004},
	keywords = {⛔ No DOI found},
}

@inproceedings{anandayuvaraj_reflecting_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Reflecting on {Recurring} {Failures} in {IoT} {Development}},
	isbn = {978-1-4503-9475-8},
	url = {https://dl.acm.org/doi/10.1145/3551349.3559545},
	doi = {10.1145/3551349.3559545},
	abstract = {As IoT systems are given more responsibility and autonomy, they offer greater benefits, but also carry greater risks. We believe this trend invigorates an old challenge of software engineering: how to develop high-risk software-intensive systems safely and securely under market pressures? As a first step, we conducted a systematic analysis of recent IoT failures to identify engineering challenges. We collected and analyzed 22 news reports and studied the sources, impacts, and repair strategies of failures in IoT systems. We observed failure trends both within and across application domains. We also observed that failure themes have persisted over time. To alleviate these trends, we outline a research agenda toward a Failure-Aware Software Development Life Cycle for IoT development. We propose an encyclopedia of failures and an empirical basis for system postmortems, complemented by appropriate automated tools.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Anandayuvaraj, Dharun and Davis, James C.},
	month = jan,
	year = {2023},
	keywords = {Cyber-Physical Systems, Embedded Systems, Failure Analysis, Internet of Things, Safety-Critical Software, Software Engineering},
	pages = {1--5},
}

@inproceedings{amusuo_reflections_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Reflections on software failure analysis},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3560879},
	doi = {10.1145/3540250.3560879},
	abstract = {Failure studies are important in revealing the root causes, behaviors, and life cycle of defects in software systems. These studies either focus on understanding the characteristics of defects in specific classes of systems, or the characteristics of a specific type of defect in the systems it manifests in. Failure studies have influenced various software engineering research directions, especially in the area of software evolution, defect detection, and program repair. In this paper, we reflect on the conduct of failure studies in software engineering. We reviewed a sample of 52 failure study papers. We identified several recurring problems in these studies, some of which hinder the ability of software engineering community to trust or replicate the results. Based on our findings, we suggest future research directions, including identifying and analyzing failure causal chains, standardizing the conduct of failure studies, and tool support for faster defect analysis.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Amusuo, Paschal C. and Sharma, Aishwarya and Rao, Siddharth R. and Vincent, Abbey and Davis, James C.},
	month = nov,
	year = {2022},
	keywords = {Failure analysis, empirical software engineering, software defects},
	pages = {1615--1620},
}

@inproceedings{wang_pynose_2022,
	address = {Melbourne, Australia},
	series = {{ASE} '21},
	title = {{PyNose}: a test smell detector for python},
	isbn = {978-1-66540-337-5},
	shorttitle = {{PyNose}},
	url = {https://dl.acm.org/doi/10.1109/ASE51524.2021.9678615},
	doi = {10.1109/ASE51524.2021.9678615},
	abstract = {Similarly to production code, code smells also occur in test code, where they are called test smells. Test smells have a detrimental effect not only on test code but also on the production code that is being tested. To date, the majority of the research on test smells has been focusing on programming languages such as Java and Scala. However, there are no available automated tools to support the identification of test smells for Python, despite its rapid growth in popularity in recent years. In this paper, we strive to extend the research to Python, build a tool for detecting test smells in this language, and conduct an empirical analysis of test smells in Python projects. We started by gathering a list of test smells from existing research and selecting test smells that can be considered language-agnostic or have similar functionality in Python's standard Unittest framework. In total, we identified 17 diverse test smells. Additionally, we searched for Python-specific test smells by mining frequent code change patterns that can be considered as either fixing or introducing test smells. Based on these changes, we proposed our own test smell called Suboptimal Assert. To detect all these test smells, we developed a tool called PyNose in the form of a plugin to PyCharm, a popular Python IDE. Finally, we conducted a large-scale empirical investigation aimed at analyzing the prevalence of test smells in Python code. Our results show that 98\% of the projects and 84\% of the test suites in the studied dataset contain at least one test smell. Our proposed Suboptimal Assert smell was detected in as much as 70.6\% of the projects, making it a valuable addition to the list.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Wang, Tongjie and Golubev, Yaroslav and Smirnov, Oleg and Li, Jiawei and Bryksin, Timofey and Ahmed, Iftekhar},
	month = jun,
	year = {2022},
	keywords = {code change patterns, code smells, empirical studies, mining software repositories, python, test smells},
	pages = {593--605},
}

@inproceedings{zheng_why_2022,
	address = {Melbourne, Australia},
	series = {{ASE} '21},
	title = {Why do developers remove lambda expressions in {Java}?},
	isbn = {978-1-66540-337-5},
	url = {https://dl.acm.org/doi/10.1109/ASE51524.2021.9678600},
	doi = {10.1109/ASE51524.2021.9678600},
	abstract = {Java 8 has introduced lambda expressions, a core feature of functional programming. Since its introduction, there is an increasing trend of lambda adoptions in Java projects. Developers often adopt lambda expressions to simplify code, avoid code duplication or simulate other functional features. However, we observe that lambda expressions can also incur different types of side effects (i.e., performance issues and memory leakages) or even severe bugs, and developers also frequently remove lambda expressions in their implementations. Consequently, the advantages of utilizing lambda expressions can be significantly compromised by the collateral side effects. In this study, we present the first large-scale, quantitative and qualitative empirical study to characterize and understand inappropriate usages of lambda expressions. Particularly, we summarized seven main reasons for the removal of lambdas as well as seven common migration patterns. For instance, we observe that lambdas using customized functional interfaces are more likely to be removed by developers. Moreover, from a complementary perspective, we performed a user study over 30 developers to seek the underlying reasons why they remove lambda expressions in practice. Finally, based on our empirical results, we made suggestions on scenarios to avoid lambda usages for Java developers and also pointed out future directions for researchers.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Zheng, Mingwei and Yang, Jun and Wen, Ming and Zhu, Hengcheng and Liu, Yepang and Jin, Hai},
	month = jun,
	year = {2022},
	keywords = {empirical study, lambda expression},
	pages = {67--78},
}

@inproceedings{coblenz_garbage_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Garbage collection makes rust easier to use: a randomized controlled trial of the bronze garbage collector},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Garbage collection makes rust easier to use},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510107},
	doi = {10.1145/3510003.3510107},
	abstract = {Rust is a general-purpose programming language that is both type-and memory-safe. Rust does not use a garbage collector, but rather achieves these properties through a sophisticated, but complex, type system. Doing so makes Rust very efficient, but makes Rust relatively hard to learn and use. We designed Bronze, an optional, library-based garbage collector for Rust. To see whether Bronze could make Rust more usable, we conducted a randomized controlled trial with volunteers from a 633--person class, collecting data from 428 students in total. We found that for a task that required managing complex aliasing, Bronze users were more likely to complete the task in the time available, and those who did so required only about a third as much time (4 hours vs. 12 hours). We found no significant difference in total time, even though Bronze users re-did the task without Bronze afterward. Surveys indicated that ownership, borrowing, and lifetimes were primary causes of the challenges that users faced when using Rust.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Coblenz, Michael and Mazurek, Michelle L. and Hicks, Michael},
	month = jul,
	year = {2022},
	keywords = {empirical study of programming languages, garbage collection, programming education, rust, usability of programming languages},
	pages = {1021--1032},
}

@inproceedings{lambiase_good_2022,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIS} '22},
	title = {Good fences make good neighbours? on the impact of cultural and geographical dispersion on community smells},
	isbn = {978-1-4503-9227-3},
	shorttitle = {Good fences make good neighbours?},
	url = {https://dl.acm.org/doi/10.1145/3510458.3513015},
	doi = {10.1145/3510458.3513015},
	abstract = {Software development is de facto a social activity that often involves people from all places to join forces globally. In such common instances, project managers must face social challenges, e.g., personality conflicts and language barriers, which often amount literally to "culture shock". In this paper, we seek to analyze and illustrate how cultural and geographical dispersion---that is, how much a community is diverse in terms of its members' cultural attitudes and geographical collocation---influence the emergence of collaboration and communication problems in open-source communities, a.k.a. community smells, the socio-technical precursors of unforeseen, often nasty organizational conditions amounting collectively to the phenomenon called social debt. We perform an extensive empirical study on cultural characteristics of GitHub developers, and build a regression model relating the two types of dispersion---cultural and geographical---with the emergence of four types of community smells, i.e., Organizational Silo, Lone Wolf, Radio Silence, and Black Cloud. Results indicate that cultural and geographical factors influence collaboration and communication within open-source communities, to an extent which incites---or even more interestingly mitigates, in some cases---community smells, e.g., Lone Wolf, in development teams. Managers can use these findings to address their own organizational structure and tentatively diagnose any nasty phenomena related to the conditions under study. To what extent does the global and multi-cultural nature of software engineering influence software processes welfare? More specifically, does an increase in "globalization" of software activities negatively or positively influence known nasty effects common in the process of software construction? Rotating around these questions, this research finds that there is in fact evidence of the aforementioned influence but it does not provide for positive effects only. Specifically, a decrease of globalization does not necessarily bode positively on conditions such as lone developers working in an individualistic fashion---a phenomenon known as "lone wolf" effect---and other nasty organizational phenomena potentially slowing down or halting software construction and maintenance activities.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 2022 {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society}},
	publisher = {Association for Computing Machinery},
	author = {Lambiase, Stefano and Catolino, Gemma and Tamburri, Damian A. and Serebrenik, Alexander and Palomba, Fabio and Ferrucci, Filomena},
	month = oct,
	year = {2022},
	keywords = {community smells, cultural dispersion, empirical studies, global software engineering, software organizational structures},
	pages = {67--78},
}

@inproceedings{hu_practitioners_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Practitioners' expectations on automated code comment generation},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510152},
	doi = {10.1145/3510003.3510152},
	abstract = {Good comments are invaluable assets to software projects, as they help developers understand and maintain projects. However, due to some poor commenting practices, comments are often missing or inconsistent with the source code. Software engineering practitioners often spend a significant amount of time and effort reading and understanding programs without or with poor comments. To counter this, researchers have proposed various techniques to automatically generate code comments in recent years, which can not only save developers time writing comments but also help them better understand existing software projects. However, it is unclear whether these techniques can alleviate comment issues and whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by interviewing and surveying practitioners about their expectations of research in code comment generation. We then compared what practitioners need and the current state-of-the-art research by performing a literature review of papers on code comment generation techniques published in the premier publication venues from 2010 to 2020. From this comparison, we highlighted the directions where researchers need to put effort to develop comment generation techniques that matter to practitioners.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Xing and Xia, Xin and Lo, David and Wan, Zhiyuan and Chen, Qiuyuan and Zimmermann, Thomas},
	month = jul,
	year = {2022},
	keywords = {code comment generation, empirical study, practitioners' expectations},
	pages = {1693--1705},
}

@inproceedings{mastropaolo_using_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Using deep learning to generate complete log statements},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3511561},
	doi = {10.1145/3510003.3511561},
	abstract = {Logging is a practice widely adopted in several phases of the software lifecycle. For example, during software development log statements allow engineers to verify and debug the system by exposing fine-grained information of the running software. While the benefits of logging are undisputed, taking proper decisions about where to inject log statements, what information to log, and at which log level (e.g., error, warning) is crucial for the logging effectiveness. In this paper, we present LANCE (Log stAtemeNt reCommEnder), the first approach supporting developers in all these decisions. LANCE features a Text-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456 Java methods. LANCE takes as input a Java method and injects in it a full log statement, including a human-comprehensible logging message and properly choosing the needed log level and the statement location. Our results show that LANCE is able to (i) properly identify the location in the code where to inject the statement in 65.9\% of Java methods requiring it; (ii) selecting the proper log level in 66.2\% of cases; and (iii) generate a completely correct log statement including a meaningful logging message in 15.2\% of cases.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Mastropaolo, Antonio and Pascarella, Luca and Bavota, Gabriele},
	month = jul,
	year = {2022},
	keywords = {empirical study, logging, machine learning on code},
	pages = {2279--2290},
}

@inproceedings{ciancarini_issues_2022,
	address = {New York, NY, USA},
	series = {{ICSE}-{SEIP} '22},
	title = {Issues in the adoption of the scaled agile framework},
	isbn = {978-1-4503-9226-6},
	url = {https://dl.acm.org/doi/10.1145/3510457.3513028},
	doi = {10.1145/3510457.3513028},
	abstract = {Agile methods were originally introduced for small sized, co-located teams. Their successful products immediately brought up the issue of adapting the methods also for large and distributed organizations engaged in projects to build major, complex products. Currently the most popular multi-teams agile method is the Scaled Agile Framework (SAFe) which, however, is subject to criticism: it appears to be quite demanding and expensive in terms of human resource and project management practices. Moreover, SAFe allegedly goes against some of the principles of agility. This research attempts to gather a deeper understanding of the matter first reviewing and analysing the studies published on this topic via a multivocal literature review and then with an extended empirical investigation on the matters that appear most controversial via the direct analysis of the work of 25 respondents from 17 different companies located in eight countries. Thus, the originality of this research is in the systemic assessment of the "level of fexibil-ity" of SAFe, highlighting the challenges of adopting this framework as it relates to decision making, structure, and the technical and managerial competencies of the company. The results show that SAFe can be an effective and adequate approach if the company is ready to invest a significant effort and resources into it both in the form of providing time for SAFe to be properly absorbed and specific training for individuals.},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {Association for Computing Machinery},
	author = {Ciancarini, Paolo and Kruglov, Artem and Pedrycz, Witod and Salikhov, Dilshat and Succi, Giancarlo},
	month = oct,
	year = {2022},
	keywords = {distributed agile, empirical software engineering, surveys},
	pages = {175--184},
}

@inproceedings{he_empirical_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An empirical study of log analysis at {Microsoft}},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3558963},
	doi = {10.1145/3540250.3558963},
	abstract = {Logs are crucial to the management and maintenance of software systems. In recent years, log analysis research has achieved notable progress on various topics such as log parsing and log-based anomaly detection. However, the real voices from front-line practitioners are seldom heard. For example, what are the pain points of log analysis in practice? In this work, we conduct a comprehensive survey study on log analysis at Microsoft. We collected feedback from 105 employees through a questionnaire of 13 questions and individual interviews with 12 employees. We summarize the format, scenario, method, tool, and pain points of log analysis. Additionally, by comparing the industrial practices with academic research, we discuss the gaps between academia and industry, and future opportunities on log analysis with four inspiring findings. Particularly, we observe a huge gap exists between log anomaly detection research and failure alerting practices regarding the goal, technique, efficiency, etc. Moreover, data-driven log parsing, which has been widely studied in recent research, can be alternatively achieved by simply logging template IDs during software development. We hope this paper could uncover the real needs of industrial practitioners and the unnoticed yet significant gap between industry and academia, and inspire interesting future directions that converge efforts from both sides.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {He, Shilin and Zhang, Xu and He, Pinjia and Xu, Yong and Li, Liqun and Kang, Yu and Ma, Minghua and Wei, Yining and Dang, Yingnong and Rajmohan, Saravanakumar and Lin, Qingwei},
	month = nov,
	year = {2022},
	keywords = {Empirical Study, Log Analysis, Software Reliability},
	pages = {1465--1476},
}

@inproceedings{lou_testing_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Testing of autonomous driving systems: where are we and where should we go?},
	isbn = {978-1-4503-9413-0},
	shorttitle = {Testing of autonomous driving systems},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549111},
	doi = {10.1145/3540250.3549111},
	abstract = {Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners’ needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Lou, Guannan and Deng, Yao and Zheng, Xi and Zhang, Mengshi and Zhang, Tianyi},
	month = nov,
	year = {2022},
	keywords = {Autonomous Driving, Empirical Study, Software Testing},
	pages = {31--43},
}

@inproceedings{dyer_exploratory_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An exploratory study on the predominant programming paradigms in {Python} code},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549158},
	doi = {10.1145/3540250.3549158},
	abstract = {Python is a multi-paradigm programming language that fully supports object-oriented (OO) programming. The language allows writing code in a non-procedural imperative manner, using procedures, using classes, or in a functional style. To date, no one has studied what paradigm(s), if any, are predominant in Python code and projects. In this work, we first define a technique to classify Python files into predominant paradigm(s). We then automate our approach and evaluate it against human judgements, showing over 80\% agreement. We then analyze over 100k open-source Python projects, automatically classifying each source file and investigating the paradigm distributions. The results indicate Python developers tend to heavily favor OO features. We also observed a positive correlation between OO and procedural paradigms and the size of the project. And despite few files or projects being predominantly functional, we still found many functional feature uses.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Dyer, Robert and Chauhan, Jigyasa},
	month = nov,
	year = {2022},
	keywords = {Python, empirical study, programming paradigms},
	pages = {684--695},
}

@inproceedings{he_large-scale_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {A large-scale empirical study on {Java} library migrations: prevalence, trends, and rationales},
	isbn = {978-1-4503-8562-6},
	shorttitle = {A large-scale empirical study on {Java} library migrations},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468571},
	doi = {10.1145/3468264.3468571},
	abstract = {With the rise of open-source software and package hosting platforms, reusing 3rd-party libraries has become a common practice. Due to various failures during software evolution, a project may remove a used library and replace it with another library, which we call library migration. Despite substantial research on dependency management, the understanding of how and why library migrations occur is still lacking. Achieving this understanding may help practitioners optimize their library selection criteria, develop automated approaches to monitor dependencies, and provide migration suggestions for their libraries or software projects. In this paper, through a fine-grained commit-level analysis of 19,652 Java GitHub projects, we extract the largest migration dataset to-date (1,194 migration rules, 3,163 migration commits). We show that 8,065 (41.04\%) projects having at least one library removal, 1,564 (7.96\%, lower-bound) to 5,004 (25.46\%, upper-bound) projects have at least one migration, and a median project with migrations has 2 to 4 migrations in total. We discover that library migrations are dominated by several domains (logging, JSON, testing and web service) presenting a long tail distribution. Also, migrations are highly unidirectional in that libraries are either mostly abandoned or mostly chosen in our project corpus. A thematic analysis on related commit messages, issues, and pull requests identifies 14 frequently mentioned migration reasons (e.g., lack of maintenance, usability, integration, etc), 7 of which are not discussed in previous work. Our findings can be operationalized into actionable insights for package hosting platforms, project maintainers, and library developers. We provide a replication package at {\textless}a{\textgreater}https://doi.org/10.5281/zenodo.4816752{\textless}/a{\textgreater}.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {He, Hao and He, Runzhi and Gu, Haiqiao and Zhou, Minghui},
	month = aug,
	year = {2021},
	keywords = {empirical software engineering, evolution and maintenance, library migration, mining software repositories},
	pages = {478--490},
}

@inproceedings{wen_empirical_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, and cloud providers can leverage these findings and implications to better understand and further explore the serverless computing paradigm.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	keywords = {Application Development, Empirical Study, Serverless Computing, Stack Overflow},
	pages = {416--428},
}

@inproceedings{anandayuvaraj_reflecting_2022,
	title = {Reflecting on {Recurring} {Failures} in {IoT} {Development}},
	abstract = {As IoT systems are given more responsibility and autonomy, they offer greater benefits, but also carry greater risks. We believe this trend invigorates an old challenge of software engineering: how to develop high-risk software-intensive systems safely and securely under market pressures? As a first step, we conducted a systematic analysis of recent IoT failures to identify engineering challenges. We collected and analyzed 22 news reports and studied the sources, impacts, and repair strategies of failures in IoT systems. We observed failure trends both within and across application domains. We also observed that failure themes have persisted over time. To alleviate these trends, we outline a research agenda toward a Failure-Aware Software Development Life Cycle for IoT development. We propose an encyclopedia of failures and an empirical basis for system postmortems, complemented by appropriate automated tools.},
	booktitle = {Automated {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results} ({ASE}-{NIER})},
	author = {Anandayuvaraj, Dharun and Davis, James C},
	year = {2022},
}

@inproceedings{anandayuvaraj_incorporating_2023,
	title = {Incorporating {Failure} {Knowledge} into {Design} {Decisions} for {IoT} {Systems}: {A} {Controlled} {Experiment} on {Novices}},
	abstract = {Internet of Things (IoT) systems allow software to directly interact with the physical world. Recent IoT failures can be attributed to recurring software design ﬂaws, suggesting IoT software engineers may not be learning from past failures. We examine the use of failure stories to improve IoT system designs. We conducted an experiment to evaluate the inﬂuence of failure-related learning treatments on design decisions. Our experiment used a between-subjects comparison of novices (computer engineering students) completing a design questionnaire. There were three treatments: a control group (N=7); a group considering a set of design guidelines (N=8); and a group considering failure stories (proposed treatment, N=6). We measured their design decisions and their design rationales. All subjects made comparable decisions. Their rationales varied by treatment: subjects treated with guidelines and failure stories made greater use of criticality as a rationale, while subjects exposed to failure stories more frequently used safety as a rationale. Building on these ﬁndings, we suggest several research directions toward a failure-aware IoT engineering process.},
	booktitle = {Software {Engineering} {Research} \& {Practices} for the {Internet} of {Things} ({SERP4IoT})},
	author = {Anandayuvaraj, Dharun and Thulluri, Pujita and Figueroa, Justin and Shandilya, Harshit and Davis, James C},
	year = {2023},
}

@misc{tom-m_math_2021,
	title = {The {Math} {Behind} {Proven} in {Use} - {EngineerZone} {Spotlight} - {EZ} {Blogs} - {EngineerZone}},
	url = {https://ez.analog.com/ez-blogs/b/engineerzone-spotlight/posts/the-math-behind-proven-in-use},
	abstract = {Analog Devices' blog written by employees that highlights innovations, thought leaders and industry trends.},
	language = {en},
	urldate = {2023-04-29},
	author = {{Tom-M}},
	month = may,
	year = {2021},
}

@misc{international_electrotechnical_commission_iec_nodate,
	title = {{IEC} 61508-1:2010 {\textbar} {IEC} {Webstore} {\textbar} functional safety, smart city},
	url = {https://webstore.iec.ch/publication/5515},
	urldate = {2023-04-29},
	author = {{International Electrotechnical Commission}},
}

@misc{noauthor_tcp_nodate,
	title = {{TCP} {RFC} 793},
	url = {https://www.ietf.org/rfc/rfc793.txt},
	urldate = {2023-04-28},
}

@misc{noauthor_nvd_nodate,
	title = {{NVD} - {Home}},
	url = {https://nvd.nist.gov/},
	urldate = {2023-04-20},
}

@inproceedings{gopalakrishna_if_2022-1,
	title = {“{If} security is required”: {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	shorttitle = {“{If} security is required”},
	doi = {10.1145/3528227.3528565},
	abstract = {The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices. This research seeks to characterize engineering processes and security practices for ML-enabled IoT systems through the lens of the engineering lifecycle. We collected data from practitioners through a survey (N=25) and interviews (N=4). We found that security processes and engineering methods vary by company. Respondents emphasized the engineering cost of security analysis and threat modeling, and trade-offs with business needs. Engineers reduce their security investment if it is not an explicit requirement. The threats of IP theft and reverse engineering were a consistent concern among practitioners when deploying ML for IoT devices. Based on our findings, we recommend further research into understanding engineering cost, compliance, and security trade-offs.},
	booktitle = {2022 {IEEE}/{ACM} 4th {International} {Workshop} on {Software} {Engineering} {Research} and {Practices} for the {IoT} ({SERP4IoT})},
	author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
	month = may,
	year = {2022},
	keywords = {Computer security, Costs, Cyber-Physical Systems, Embedded Systems, Internet of Things, Interviews, Machine Learning, Machine learning, Prototypes, Reverse engineering, Security and Privacy, Software Engineering},
	pages = {1--8},
}

@article{jin_understanding_2012,
	title = {Understanding and detecting real-world performance bugs},
	volume = {47},
	issn = {0362-1340},
	url = {https://dl.acm.org/doi/10.1145/2345156.2254075},
	doi = {10.1145/2345156.2254075},
	abstract = {Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs. This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs. Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.},
	number = {6},
	urldate = {2023-04-19},
	journal = {ACM SIGPLAN Notices},
	author = {Jin, Guoliang and Song, Linhai and Shi, Xiaoming and Scherpelz, Joel and Lu, Shan},
	month = jun,
	year = {2012},
	keywords = {characteristics study, performance bugs, rule-based bug detection},
	pages = {77--88},
}

@article{fawzi_robustness_2017,
	title = {The {Robustness} of {Deep} {Networks}: {A} {Geometrical} {Perspective}},
	volume = {34},
	issn = {1558-0792},
	shorttitle = {The {Robustness} of {Deep} {Networks}},
	doi = {10.1109/MSP.2017.2740965},
	abstract = {Deep neural networks have recently shown impressive classification performance on a diverse set of visual tasks. When deployed in real-world (noise-prone) environments, it is equally important that these classifiers satisfy robustness guarantees: small perturbations applied to the samples should not yield significant loss to the performance of the predictor. The goal of this article is to discuss the robustness of deep networks to a diverse set of perturbations that may affect the samples in practice, including adversarial perturbations, random noise, and geometric transformations. This article further discusses the recent works that build on the robustness analysis to provide geometric insights on the classifier's decision surface, which help in developing a better understanding of deep networks. Finally, we present recent solutions that attempt to increase the robustness of deep networks. We hope this review article will contribute to shedding light on the open research challenges in the robustness of deep networks and stir interest in the analysis of their fundamental properties.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Classification, Machine learning, Neural networks, Robustness, Visualization},
	pages = {50--62},
}

@inproceedings{chen_copy_2022,
	title = {Copy, {Right}? {A} {Testing} {Framework} for {Copyright} {Protection} of {Deep} {Learning} {Models}},
	shorttitle = {Copy, {Right}?},
	doi = {10.1109/SP46214.2022.9833747},
	abstract = {Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, Jialuo and Wang, Jingyi and Peng, Tinglan and Sun, Youcheng and Cheng, Peng and Ji, Shouling and Ma, Xingjun and Li, Bo and Song, Dawn},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Adaptation models, Biological system modeling, Computational modeling, Deep learning, Measurement, Training, Watermarking},
	pages = {824--841},
}

@inproceedings{koc_satune_2022,
	address = {Melbourne, Australia},
	series = {{ASE} '21},
	title = {{SATune}: a study-driven auto-tuning approach for configurable software verification tools},
	isbn = {978-1-66540-337-5},
	shorttitle = {{SATune}},
	url = {https://dl.acm.org/doi/10.1109/ASE51524.2021.9678761},
	doi = {10.1109/ASE51524.2021.9678761},
	abstract = {Many program verification tools can be customized via run-time configuration options that trade off performance, precision, and soundness. However, in practice, users often run tools under their default configurations, because understanding these tradeoffs requires significant expertise. In this paper, we ask how well a single, default configuration can work in general, and we propose SATUNE, a novel tool for automatically configuring program verification tools for given target programs. To answer our question, we gathered a dataset that runs four well-known program verification tools against a range of C and Java benchmarks, with results labeled as correct, incorrect, or inconclusive (e.g., timeout). Examining the dataset, we find there is generally no one-size-fits-all best configuration. Moreover, a statistical analysis shows that many individual configuration options do not have simple tradeoffs: they can be better or worse depending on the program. Motivated by these results, we developed SATUNE, which constructs configurations using a meta-heuristic search. The search is guided by a surrogate fitness function trained on our dataset. We compare the performance of SATUNE to three baselines: a single configuration with the most correct results in our dataset; the most precise configuration followed by the most correct configuration (if needed); and the most precise configuration followed by random search (also if needed). We find that SATUNE outperforms these approaches by completing more correct tasks with high precision. In summary, our work shows that good configurations for verification tools are not simple to find, and SATUNE takes an important step towards automating the process of finding them.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Koc, Ugur and Mordahl, Austin and Wei, Shiyi and Foster, Jeffrey S. and Porter, Adam A.},
	month = jun,
	year = {2022},
	keywords = {empirical software engineering, software analysis, testing, validation},
	pages = {330--342},
}

@misc{information_commissioners_office_ico_2023,
	title = {{ICO} fines {TikTok} £12.7 million for misusing children’s data},
	url = {https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2023/04/ico-fines-tiktok-127-million-for-misusing-children-s-data/},
	abstract = {The Information Commissioner’s Office (ICO) has issued a £12,700,000 fine to TikTok Information Technologies UK Limited and TikTok Inc (TikTok) for a number of breaches of data protection law, including failing to use children’s personal data lawfully.},
	language = {en},
	urldate = {2023-04-16},
	author = {{Information Commissioner’s Office}},
	month = apr,
	year = {2023},
	note = {Publisher: ICO},
}

@inproceedings{chang_inputs_2009,
	title = {Inputs of {Coma}: {Static} {Detection} of {Denial}-of-{Service} {Vulnerabilities}},
	shorttitle = {Inputs of {Coma}},
	doi = {10.1109/CSF.2009.13},
	abstract = {As networked systems grow in complexity, they are increasingly vulnerable to denial-of-service (DoS) attacks involving resource exhaustion. A single malicious "input of coma" can trigger high-complexity behavior such as deep recursion in a carelessly implemented server, exhausting CPU time or stack space and making the server unavailable to legitimate clients. These DoS attacks exploit the semantics of the target application, are rarely associated with network traffic anomalies, and are thus extremely difficult to detect using conventional methods.We present SAFER, a static analysis tool for identifying potential DoS vulnerabilities and the root causes of resource-exhaustion attacks before the software is deployed. Our tool combines taint analysis with control dependency analysis to detect high-complexity control structures whose execution can be triggered by untrusted network inputs.When evaluated on real-world networked applications, SAFER discovered previously unknown DoS vulnerabilities in the Expat XML parser and the SQLite library, as well as a new attack on a previously patched version of the wu-ftpd server. This demonstrates the importance of understanding and repairing the root causes of DoS vulnerabilities rather than simply blocking known malicious inputs.},
	booktitle = {2009 22nd {IEEE} {Computer} {Security} {Foundations} {Symposium}},
	author = {Chang, Richard and Jiang, Guofei and Ivancic, Franjo and Sankaranarayanan, Sriram and Shmatikov, Vitaly},
	month = jul,
	year = {2009},
	note = {ISSN: 2377-5459},
	keywords = {Application software, Buffer overflow, Computer bugs, Computer crashes, Computer crime, Computer security, Denial-of-service, National electric code, Network servers, Safety, XML, network security, static analysis},
	pages = {186--199},
}

@inproceedings{kothari_finding_2011,
	address = {New York, NY, USA},
	series = {{SIGCOMM} '11},
	title = {Finding protocol manipulation attacks},
	isbn = {978-1-4503-0797-0},
	url = {https://dl.acm.org/doi/10.1145/2018436.2018440},
	doi = {10.1145/2018436.2018440},
	abstract = {We develop a method to help discover manipulation attacks in protocol implementations. In these attacks, adversaries induce honest nodes to exhibit undesirable behaviors by misrepresenting their intent or network conditions. Our method is based on a novel combination of static analysis with symbolic execution and dynamic analysis with concrete execution. The former finds code paths that are likely vulnerable, and the latter emulates adversarial actions that lead to effective attacks. Our method is precise (i.e., no false positives) and we show that it scales to complex protocol implementations. We apply it to four diverse protocols, including TCP, the 802.11 MAC, ECN, and SCTP, and show that it is able to find all manipulation attacks that have been previously reported for these protocols. We also find a previously unreported attack for SCTP. This attack is a variant of a TCP attack but must be mounted differently in SCTP because of subtle semantic differences between the two protocols.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} 2011 conference},
	publisher = {Association for Computing Machinery},
	author = {Kothari, Nupur and Mahajan, Ratul and Millstein, Todd and Govindan, Ramesh and Musuvathi, Madanlal},
	month = aug,
	year = {2011},
	keywords = {manipulation attacks, symbolic execution},
	pages = {26--37},
}

@inproceedings{zhang_statically_2021,
	address = {New York, NY, USA},
	series = {{CCS} '21},
	title = {Statically {Discovering} {High}-{Order} {Taint} {Style} {Vulnerabilities} in {OS} {Kernels}},
	isbn = {978-1-4503-8454-4},
	url = {https://dl.acm.org/doi/10.1145/3460120.3484798},
	doi = {10.1145/3460120.3484798},
	abstract = {Static analysis is known to yield numerous false alarms when used in bug finding, especially for complex vulnerabilities in large code bases like the Linux kernel. One important class of such complex vulnerabilities is what we call "high-order taint style vulnerability", where the taint flow from the user input to the vulnerable site crosses the boundary of a single entry function invocation (i.e., syscall). Due to the large scope and high precision requirement, few have attempted to solve the problem. In this paper, we present SUTURE, a highly precise and scalable static analysis tool capable of discovering high-order vulnerabilities in OS kernels. SUTURE employs a novel summary-based high-order taint flow construction approach to efficiently enumerate the cross-entry taint flows, while incorporating multiple innovative enhancements on analysis precision that are unseen in existing tools, resulting in a highly precise inter-procedural flow-, context-, field-, index-, and opportunistically path-sensitive static taint analysis. We apply SUTURE to discover high-order taint vulnerabilities in multiple Android kernels from mainstream vendors (e.g., Google, Samsung, Huawei), the results show that SUTURE can both confirm known high-order vulnerabilities and uncover new ones. So far, SUTURE generates 79 true positive warning groups, of which 19 have been confirmed by the vendors, including a high severity vulnerability rated by Google. SUTURE also achieves a reasonable false positive rate (51.23\%) perceived by users of our tool.},
	urldate = {2023-04-16},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Hang and Chen, Weiteng and Hao, Yu and Li, Guoren and Zhai, Yizhuo and Zou, Xiaochen and Qian, Zhiyun},
	month = nov,
	year = {2021},
	keywords = {OS kernels, static program analysis, vulnerability discovery},
	pages = {811--824},
}

@misc{dowd_this_2021,
	title = {This {Dangerous} {TikTok} {Challenge} {Just} {Killed} a 12-{Year}-{Old}},
	url = {https://web.archive.org/web/20230124210038/https://www.vice.com/en/article/pkbxm9/tiktok-blackout-challenge-kill-children},
	abstract = {It's already claimed the lives of at least three kids in the U.S. this year alone.},
	language = {en},
	urldate = {2023-04-14},
	journal = {Vice},
	author = {Dowd, Trone},
	month = jul,
	year = {2021},
	keywords = {Blackout Challenge, Death, Police, Social Media},
}

@misc{newton_child_2021,
	title = {The child safety problem on platforms is worse than we knew},
	url = {https://web.archive.org/web/20230330144813/https://www.theverge.com/2021/5/12/22432863/child-safety-platforms-thorn-report-snap-facebook-youtube-tiktok},
	abstract = {Minors aren’t supposed to use Snap, Facebook, and YouTube — but they are, and experience abuse, harassment, and sexual solicitation},
	language = {en-US},
	urldate = {2023-04-14},
	journal = {The Verge},
	author = {Newton, Casey},
	month = may,
	year = {2021},
}

@misc{tiktok_guardians_2021,
	title = {Guardian's {Guide}},
	url = {https://web.archive.org/web/20230413213943/https://www.tiktok.com/safety/en/guardians-guide/},
	abstract = {Resources for parents, guardians, and caregivers to help you understand TikTok's tools and controls we’ve built into the product to keep our community safe.},
	language = {en},
	urldate = {2023-04-14},
	journal = {TikTok},
	author = {{TikTok}},
	month = mar,
	year = {2021},
}

@misc{keenan_new_2023,
	title = {New features for teens and families on {TikTok}},
	url = {https://web.archive.org/web/20230412012154/https://newsroom.tiktok.com/en-us/new-features-for-teens-and-families-on-tiktok-us},
	abstract = {Today we're announcing new features for teens, families, and our broader community. We believe digital experiences should bring joy and play a positi},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {Keenan, Cormac},
	month = mar,
	year = {2023},
}

@misc{evans_new_2021,
	title = {New {Family} {Pairing} resources offer digital safety advice from teens},
	url = {https://web.archive.org/web/20230311000442/https://newsroom.tiktok.com/en-us/new-family-pairing-resources-offer-digital-safety-advice-from-teens},
	abstract = {By Alexandra Evans, Head of Child Safety Public Policy in Europe At TikTok, we're committed to working in partnership with parents and guardians as they support their teen's digital journey. We recogn},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {Evans, Alexandra},
	month = sep,
	year = {2021},
}

@article{smith_formal_nodate,
	title = {Formal {Verification} of {TCP} and {T}/{TCP}},
	abstract = {In this thesis we present a formal abstract specification for TCP/IP transport level protocols and formally verify that TCP satisfies this specification. We first verify a formal model of TCP where we assume it has unbounded counters. With bounded counters, TCP requires several timing mechanisms to function correctly. We also model TCP with these timing mechanisms and verify that it also satisfies our specification. We also present a formal description of an experimental protocol called T/TCP which is designed to provide the same service as TCP, but with optimizations to make it efficient for transactions. Even with unbounded counters this protocol does not provide the same service as TCP as it may deliver the same message twice. Even though the service provide by T/TCP is not exactly the same as TCP, its behavior may be acceptable for some applications. Therefore, we define a weaker specification that captures this behavior of T/TCP while maintaining the other correctness properties of our initial specification. We then verify that T/TCP satisfies this weaker specification.},
	language = {en},
	author = {Smith, Mark Anthony Shawn},
	keywords = {⛔ No DOI found},
}

@article{zhang_automated_nodate,
	title = {Automated {Veriﬁcation} of {Customizable} {Middlebox} {Properties} with {Gravel}},
	abstract = {Building a formally-veriﬁed software middlebox is attractive for network reliability. In this paper, we explore the feasibility of verifying “almost unmodiﬁed” software middleboxes. Our key observation is that software middleboxes are already designed and implemented in a modular way (e.g., Click). Further, to achieve high performance, the number of operations each element or module performs is ﬁnite and small. These two characteristics place them within reach of automated veriﬁcation through symbolic execution.},
	language = {en},
	author = {Zhang, Kaiyuan and Zhuo, Danyang and Akella, Aditya and Wang, Arvind Krishnamurthy Xi},
	keywords = {⛔ No DOI found},
}

@misc{tiktok_tiktok_2018,
	title = {{TikTok} {News} and {Top} {Stories} {\textbar} {TikTok} {Newsroom}},
	url = {https://web.archive.org/web/20230414084706/https://newsroom.tiktok.com/en-us/},
	urldate = {2023-04-14},
	author = {{TikTok}},
	month = aug,
	year = {2018},
}

@misc{kelly_tiktok_2022,
	title = {{TikTok} may push potentially harmful content to teens within minutes, study finds {\textbar} {CNN} {Business}},
	url = {https://web.archive.org/web/20230411112722/https://www.cnn.com/2022/12/15/tech/tiktok-teens-study-trnd/index.html},
	abstract = {TikTok may surface potentially harmful content related to suicide and eating disorders to teenagers within minutes of them creating an account, a new study suggests, likely adding to growing scrutiny of the app's impact on its youngest users.},
	language = {en},
	urldate = {2023-04-14},
	journal = {CNN},
	author = {Kelly, Samantha Murphy},
	month = dec,
	year = {2022},
}

@misc{zhong_third_2020,
	title = {A {Third} of {TikTok}’s {U}.{S}. {Users} {May} {Be} 14 or {Under}, {Raising} {Safety} {Questions}},
	url = {https://web.archive.org/web/20230330091416/https://www.nytimes.com/2020/08/14/technology/tiktok-underage-users-ftc.html},
	abstract = {Three current and former employees expressed concerns about the Chinese-owned app’s safeguards for preteen children.},
	language = {en-US},
	urldate = {2023-04-14},
	journal = {The New York Times},
	author = {Zhong, Raymond and Frenkel, Sheera},
	month = aug,
	year = {2020},
	keywords = {Children and Childhood, Children's Online Privacy Protection Act, Computer Security, Federal Trade Commission, Microsoft Corp, Musical.ly Inc, Privacy, Social Media, TikTok (ByteDance), Video Recordings, Downloads and Streaming},
}

@misc{evans_furthering_2021,
	title = {Furthering our safety and privacy commitments for teens on {TikTok}},
	url = {https://web.archive.org/web/20221018153406/https://newsroom.tiktok.com/en-au/furthering-our-safety-and-privacy-commitments-for-teens-tiktok},
	abstract = {By Alexandra Evans, Head of Child Safety Public Policy and Aruna Sharma, Global Head of Privacy  People from all walks of life come to TikTok to be entertained, spark joy in their everyday, and share},
	language = {en-au},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {Evans, Alexandra},
	month = aug,
	year = {2021},
}

@misc{tiktok_tiktok_2019,
	title = {{TikTok} for {Younger} {Users}},
	url = {https://web.archive.org/web/20230322110612/https://newsroom.tiktok.com/en-us/tiktok-for-younger-users},
	abstract = {We want our users to have their best experience online, which means being able to create and have fun while feeling safe and comfortable. This post is part of our Community Well-Being series that aims},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {{TikTok}},
	month = dec,
	year = {2019},
}

@misc{tiktok_tiktoks_2019,
	title = {{TikTok}'s {Top} 10 {Tips} for {Parents}},
	url = {https://web.archive.org/web/20221006001304/https://newsroom.tiktok.com/en-us/tiktoks-top-10-tips-for-parents},
	abstract = {TikTok is a short form video platform that offers in-app editing, effects, and sounds to help users develop imaginative memes and creative content. We know it's important that parents can feel comfort},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {{TikTok}},
	month = oct,
	year = {2019},
}

@misc{noauthor_nvd_nodate,
	title = {{NVD} - {Home}},
	url = {https://nvd.nist.gov/},
	urldate = {2023-04-14},
}

@misc{elizabeth_our_2021,
	title = {Our work to design an age-appropriate experience on {TikTok}},
	url = {https://web.archive.org/web/20230130014809/https://newsroom.tiktok.com/en-us/our-work-to-design-an-age-appropriate-experience-on-tiktok/},
	abstract = {Every day, creators of all backgrounds come to TikTok to connect with a global community and bring joy to hundreds of millions of people aro},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {Elizabeth, Tracy},
	month = may,
	year = {2021},
}

@misc{collins_tiktok_2020,
	title = {{TikTok} introduces {Family} {Pairing}},
	url = {https://web.archive.org/web/20230412022455/https://newsroom.tiktok.com/en-us/tiktok-introduces-family-pairing},
	abstract = {More than ever, families are turning to internet platforms like TikTok to stay entertained, informed, and connected. That was, of course, happeni},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {Collins, Jeff},
	month = apr,
	year = {2020},
}

@misc{elizabeth_our_2021-1,
	title = {Our work to keep {TikTok} a place for people 13 and over},
	url = {https://web.archive.org/web/20230315224514/https://newsroom.tiktok.com/en-eu/our-work-to-keep-tiktok-a-place-for-people-13-and-over-eu},
	abstract = {Every day, creators of all backgrounds come to TikTok to connect with a global community and bring joy to hundreds of millions of people aro},
	language = {en-eu},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	author = {Elizabeth, Tracy},
	month = may,
	year = {2021},
}

@misc{noauthor_facebook_nodate,
	title = {The {Facebook} {Papers}},
	url = {https://facebookpapers.com/},
	language = {en-US},
	urldate = {2023-04-14},
	journal = {The Facebook Papers},
}

@misc{noauthor_update_2019,
	title = {An update on our platform {API} for researchers},
	url = {https://newsroom.tiktok.com/en-us/an-update-on-our-platform-api-for-researchers},
	abstract = {Earlier this year we announced the development of research APIs as part of our commitment to bring transparency to how our platform operates. We support independent research and have spent the last fe},
	language = {en-us},
	urldate = {2023-04-14},
	journal = {Newsroom {\textbar} TikTok},
	month = aug,
	year = {2019},
}

@misc{a_b_c_news_young_2019,
	title = {Young kids could be seeing mature content on {TikTok}. {Here}’s how to keep them safe},
	url = {https://web.archive.org/web/20220720081827/https://abcnews.go.com/GMA/Living/young-kids-mature-content-tiktok-heres-safe/story?id=66366182},
	abstract = {TikTok, popular with teens, is attracting a younger audience, who could be exposed to mature discussions, violence and profanity. How to help keep kids safe.},
	language = {en},
	urldate = {2023-04-14},
	journal = {ABC News},
	author = {{A. B. C. News}},
	month = oct,
	year = {2019},
}

@inproceedings{romano_empirical_2021,
	title = {An {Empirical} {Study} of {Bugs} in {WebAssembly} {Compilers}},
	doi = {10.1109/ASE51524.2021.9678776},
	abstract = {WebAssembly is the newest programming language for the Web. It defines a portable bytecode format for use as a compilation target for programs developed in high-level languages such as C, C++, and Rust. As a result, WebAssembly binaries are generally created by WebAssembly compilers rather than being written manually. To port native code to the Web, WebAssembly compilers need to address the differences between the source and target languages and dissimilarities in their execution environments. A deep understanding of the bugs in WebAssembly compilers can help compiler developers determine where to focus development and testing efforts. In this paper, we conduct two empirical studies to understand the characteristics of the bugs found in WebAssembly compilers. First, we perform a qualitative analysis of bugs in Emscripten, the most widely-used WebAssembly compiler. We investigate 146 bug reports in Emscripten related to the unique challenges WebAssembly compilers encounter compared with traditional compilers. Second, we provide a quantitative analysis of 1,054 bugs in three open-source WebAssembly compilers, AssemblyScript, Emscripten, and Rustc/Wasm-Bindgen. We analyze these bugs along three dimensions: lifecycle, impact, and sizes of bug-inducing inputs and bug fixes. These studies deepen our understanding of WebAssembly compiler bugs. We hope that the findings of our study will shed light on opportunities to design practical tools for testing and debugging WebAssembly compilers.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Romano, Alan and Liu, Xinyue and Kwon, Yonghwi and Wang, Weihang},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {C++ languages, Codes, Computer bugs, Debugging, High level languages, Ports (computers), Statistical analysis, WebAssembly, compiler, qualitative study, quantitative study},
	pages = {42--54},
}

@inproceedings{li_empirical_2021,
	title = {An {Empirical} {Study} on {Obsolete} {Issue} {Reports}},
	doi = {10.1109/ASE51524.2021.9678543},
	abstract = {Issue reports record valuable maintenance details. Developers write issue numbers into code comprehension and researchers mine knowledge from issue reports to assist various programming tasks. Although issue reports are useful, some of them can be obsolete, in that their corresponding commits are overwritten or rolled back, with the evolution of software. The obsolete issue reports can invalidate their references and descriptions, and can have far-reaching impacts on the approaches built on them. To explore their impacts, we conduct the first empirical study to analyze obsolete issue reports.To measure how an issue report becomes obsolete, we define an obsolete ratio of an issue report as its deleted lines over all its modified lines. To support our analysis, we build a tool, ICLINKER, that builds the links between an issue report and its commits, and calculates the obsolete ratio for each issue report. In our study, we analyze 70,180 commits and 46,257 issue reports that are collected from 5 Apache projects. We explore two research questions, which concern the distributions of obsolete issue reports and the obsolete references in code comments. Our findings to these research questions enrich the knowledge on obsolete issue reports, and some are even counterintuitive. For example, we find that obsolete issue reports are mixed with other issue reports. Even when recent issue reports are obsolete, some old issue reports keep up-to-date.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Li, Zexuan and Zhong, Hao},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Codes, Maintenance engineering, Programming, Software, Software engineering, Task analysis},
	pages = {1317--1321},
}

@inproceedings{ksontini_refactorings_2021,
	title = {Refactorings and {Technical} {Debt} in {Docker} {Projects}: {An} {Empirical} {Study}},
	shorttitle = {Refactorings and {Technical} {Debt} in {Docker} {Projects}},
	doi = {10.1109/ASE51524.2021.9678585},
	abstract = {Software containers, such as Docker, are recently considered as the mainstream technology of providing reusable software artifacts. Developers can easily build and deploy their applications based on the large number of reusable Docker images that are publicly available. Thus, a current popular trend in industry is to move towards the containerization of their applications. However, container-based projects compromise different components including the Docker and Docker-compose files, and several other dependencies to the source code combining different containers and facilitating the interactions with them. Similar to any other complex systems, container-based projects are prone to various quality and technical debt issues related to different artifacts: Docker and Docker-compose files, and regular source code ones. Unfortunately, there is a gap of knowledge in how container-based projects actually evolve and are maintained.In this paper, we address the above gap by studying refactorings, i.e., structural changes while preserving the behavior, applied in open-source Docker projects, and the technical debt issues they alleviate. We analyzed 68 projects, consisting of 19,5 MLOC, along with 193 manually examined commits. The results indicate that developers refactor these Docker projects for a variety of reasons that are specific to the configuration, combination and execution of containers, leading to several new technical debt categories and refactoring types compared to existing refactoring domains. For instance, refactorings for reducing the image size of Dockerfiles, improving the extensibility of Docker-compose files, and regular source code refactorings are mainly associated with the evolution of Docker and Docker-compose files. We also introduced 24 new Docker-specific refactorings and technical debt categories, respectively, and defined different best practices. The implications of this study will assist practitioners, tool builders, and educators in improving the quality of Docker projects.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Ksontini, Emna and Kessentini, Marouane and Ferreira, Thiago do N. and Hassan, Foyzul},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Codes, Complex systems, Containers, Docker, Industries, Market research, Open source software, Taxonomy, containers, maintenance, refactoring, technical debt},
	pages = {781--791},
}

@inproceedings{hu_towards_2021,
	title = {Towards {Exploring} the {Limitations} of {Active} {Learning}: {An} {Empirical} {Study}},
	shorttitle = {Towards {Exploring} the {Limitations} of {Active} {Learning}},
	doi = {10.1109/ASE51524.2021.9678672},
	abstract = {Deep neural networks (DNNs) are increasingly deployed as integral parts of software systems. However, due to the complex interconnections among hidden layers and massive hyperparameters, DNNs must be trained using a large number of labeled inputs, which calls for extensive human effort for collecting and labeling data. Spontaneously, to alleviate this growing demand, multiple state-of-the-art studies have developed different metrics to select a small yet informative dataset for the model training. These research works have demonstrated that DNN models can achieve competitive performance using a carefully selected small set of data. However, the literature lacks proper investigation of the limitations of data selection metrics, which is crucial to apply them in practice. In this paper, we fill this gap and conduct an extensive empirical study to explore the limits of data selection metrics. Our study involves 15 data selection metrics evaluated over 5 datasets (2 image classification tasks and 3 text classification tasks), 10 DNN architectures, and 20 labeling budgets (ratio of training data being labeled). Our findings reveal that, while data selection metrics are usually effective in producing accurate models, they may induce a loss of model robustness (against adversarial examples) and resilience to compression. Overall, we demonstrate the existence of a trade-off between labeling effort and different model qualities. This paves the way for future research in devising data selection metrics considering multiple quality criteria.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Hu, Qiang and Guo, Yuejun and Cordy, Maxime and Xie, Xiaofei and Ma, Wei and Papadakis, Mike and Traon, Yves Le},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	keywords = {Data models, Deep learning, Image coding, Measurement, Robustness, Text categorization, Training data, active learning, data selection, deep learning, empirical study},
	pages = {917--929},
}

@incollection{zhong_standup4npr_2023,
	address = {New York, NY, USA},
	title = {{StandUp4NPR}: {Standardizing} {SetUp} for {Empirically} {Comparing} {Neural} {Program} {Repair} {Systems}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {{StandUp4NPR}},
	url = {https://doi.org/10.1145/3551349.3556943},
	abstract = {Recently, the emerging trend in automatic program repair is to apply deep neural networks to generate fixed code from buggy ones, called NPR (Neural Program Repair). However, the existing NPR systems are trained and evaluated under very different settings (e.g., different training data, inconsistent evaluation data, wide-ranged candidate numbers), which makes it hard to draw fair-enough conclusions when comparing them. Motivated by this, we first build a standard benchmark dataset and an extensive framework tool to mitigate threats for the comparison. The dataset consists of a training set, a validation set and an evaluation set with 144,641, 13,739 and 13,706 bug-fix pairs of Java respectively. The tool supports selecting specific training, validation, and evaluation datasets and automatically conducting the pipeline of training and evaluating NPR models, as well as easily integrating new NPR models by implementing well-defined interfaces. Then, based on the benchmark and tool, we conduct a comprehensive empirical comparison of six SOTA NPR systems w.r.t the repairability, inclination and generalizability. The experimental results reveal deeper characteristics of compared NPR systems and subvert some existing comparative conclusions, which further verify the necessity of unifying the experimental setups in exploring the progresses of NPR systems. Meanwhile, we reveal some common features of NPR systems (e.g., they are good at dealing with code-delete bugs). Finally, we identify some promising research directions derived from our findings.},
	number = {97},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhong, Wenkang and Ge, Hongliang and Ai, Hongfei and Li, Chuanyi and Liu, Kui and Ge, Jidong and Luo, Bin},
	month = jan,
	year = {2023},
	keywords = {dataset, empirical study, neural program repair},
	pages = {1--13},
}

@incollection{yin_empirical_2023,
	address = {New York, NY, USA},
	title = {Empirical {Study} of {System} {Resources} {Abused} by {IoT} {Attackers}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3556901},
	abstract = {IoT devices have been under frequent attacks in recent years, causing severe impacts. Previous research has shown the evolution and features of some specific IoT malware families or stages of IoT attacks through offline sample analysis. However, we still lack a systematic observation of various system resources abused by active attackers and the malicious intentions behind these behaviors. This makes it difficult to design appropriate protection strategies to defend against existing attacks and possible future variants. In this paper, we fill this gap by analyzing 117,862 valid attack sessions captured by our dedicated high-interaction IoT honeypot, HoneyAsclepius, and further discover the intentions in our designed workflow. HoneyAsclepius enables high capture capability as well as continuous behavior monitoring during active attack sessions in real-time. Through a large-scale deployment, we collected 11,301,239 malicious behaviors originating from 50,594 different attackers. Based on this information, we further separate the behaviors in different attack sessions targeting distinct categories of system resources, estimate the temporal relations and summarize their malicious intentions behind. Inspired by such investigations, we present several key insights about abusive behaviors of the file, network, process, and special capability resources, and further propose practical defense strategies to better protect IoT devices.},
	number = {39},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yin, Zijing and Xu, Yiwen and Zhou, Chijin and Jiang, Yu},
	month = jan,
	year = {2023},
	keywords = {Behavior Intention, IoT Attack, System Resource Abuse},
	pages = {1--13},
}

@incollection{wu_understanding_2023,
	address = {New York, NY, USA},
	title = {Understanding and {Predicting} {Docker} {Build} {Duration}: {An} {Empirical} {Study} of {Containerized} {Workflow} of {OSS} {Projects}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {Understanding and {Predicting} {Docker} {Build} {Duration}},
	url = {https://doi.org/10.1145/3551349.3556940},
	abstract = {Docker building is a critical component of containerized workflow, which automates the process by which sources are packaged and transformed into container images. If not run properly, Docker builds can bring long durations (i.e., slow builds), which increases the cost in human and computing resources, and thus inevitably affect the software development. However, the current status and remedy for the duration cost in Docker builds remain unclear and need an in-depth study. To fill this gap, this paper provides the first empirical investigation on 171,439 Docker builds from 5,833 open source software (OSS) projects. Starting with an exploratory study, the Docker build durations can be characterized in real-world projects, and the developers’ perceptions of slow builds are obtained via a comprehensive survey. Driven by the results of our exploratory study, we propose a prediction modeling of Docker build duration, leveraging 27 handcrafted features from build-related context and configuration and 8 regression algorithms for the prediction task. Our results demonstrate that Random Forest model provides the superior performance with a Spearman’s correlation of 0.781, outperforming the baseline random model by 82.9\% in RMSE, 90.6\% in MAE, and 94.4\% in MAPE, respectively. The implications of this study will facilitate research and assist practitioners in improving the Docker build process.},
	number = {111},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Yiwen and Zhang, Yang and Xu, Kele and Wang, Tao and Wang, Huaimin},
	month = jan,
	year = {2023},
	keywords = {Build duration, Containerization, Docker, Duration prediction},
	pages = {1--13},
}

@incollection{wang_empirical_2023,
	address = {New York, NY, USA},
	title = {An {Empirical} {Study} on {Numerical} {Bugs} in {Deep} {Learning} {Programs}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3559561},
	abstract = {The task of a deep learning (DL) program is to train a model with high precision and apply it to different scenarios. A DL program often involves massive numerical calculations. Therefore, the robustness and stability of the numerical calculations are dominant in the quality of DL programs. Indeed, numerical bugs are common in DL programs, producing NaN (Not-a-Number) and INF (Infinite). A numerical bug may render the DL models inaccurate, causing the DL applications unusable. In this work, we conduct the first empirical study on numerical bugs in DL programs by analyzing the programs implemented on the top of two popular DL libraries (i.e., TensorFlow and PyTorch). Specifically, We collect a dataset of 400 numerical bugs in DL programs. Then, we classify these numerical bugs into nine categories based on their root causes and summarize two findings. Finally, we provide the implications of our study on detecting numerical bugs in DL programs.},
	number = {173},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Gan and Wang, Zan and Chen, Junjie and Chen, Xiang and Yan, Ming},
	month = jan,
	year = {2023},
	keywords = {Deep Learning, Empirical Study, Numerical Bug},
	pages = {1--5},
}

@incollection{latendresse_not_2023,
	address = {New York, NY, USA},
	title = {Not {All} {Dependencies} are {Equal}: {An} {Empirical} {Study} on {Production} {Dependencies} in {NPM}},
	isbn = {978-1-4503-9475-8},
	shorttitle = {Not {All} {Dependencies} are {Equal}},
	url = {https://doi.org/10.1145/3551349.3556896},
	abstract = {Modern software systems are often built by leveraging code written by others in the form of libraries and packages to accelerate their development. While there are many benefits to using third-party packages, software projects often become dependent on a large number of software packages. Consequently, developers are faced with the difficult challenge of maintaining their project dependencies by keeping them up-to-date and free of security vulnerabilities. However, how often are project dependencies used in production where they could pose a threat to their project’s security? We conduct an empirical study on 100 JavaScript projects using the Node Package Manager (npm) to quantify how often project dependencies are released to production and analyze their characteristics and their impact on security. Our results indicate that less than 1\% of the installed dependencies are released to production. Our analysis reveals that the functionality of a package is not enough to determine if it will be released to production or not. In fact, 59\% of the installed dependencies configured as runtime dependencies are not used in production, and 28.2\% of the dependencies configured as development dependencies are used in production, debunking two common assumptions of dependency management. Findings also indicate that most security alerts target dependencies not used in production, making them highly unlikely to be a risk for the security of the software. Our study unveils a more complex side of dependency management: not all dependencies are equal. Dependencies used in production are more sensitive to security exposure and should be prioritized. However, current tools lack the appropriate support in identifying production dependencies.},
	number = {73},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Latendresse, Jasmine and Mujahid, Suhaib and Costa, Diego Elias and Shihab, Emad},
	month = jan,
	year = {2023},
	keywords = {dependencies, npm, security, third-party packages},
	pages = {1--12},
}

@incollection{dissanayake_empirical_2023,
	address = {New York, NY, USA},
	title = {An {Empirical} {Study} of {Automation} in {Software} {Security} {Patch} {Management}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3556969},
	abstract = {Several studies have shown that automated support for different activities of the security patch management process has great potential for reducing delays in installing security patches. However, it is also important to understand how automation is used in practice, its limitations in meeting real-world needs and what practitioners really need, an area that has not been empirically investigated in the existing software engineering literature. This paper reports an empirical study aimed at investigating different aspects of automation for security patch management using semi-structured interviews with 17 practitioners from three different organisations in the healthcare domain. The findings are focused on the role of automation in security patch management for providing insights into the as-is state of automation in practice, the limitations of current automation, how automation support can be enhanced to effectively meet practitioners’ needs, and the role of the human in an automated process. Based on the findings, we have derived a set of recommendations for directing future efforts aimed at developing automated support for security patch management.},
	number = {7},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Dissanayake, Nesara and Jayatilaka, Asangi and Zahedi, Mansooreh and Babar, Muhammad Ali},
	month = jan,
	year = {2023},
	keywords = {patch management, security updates, vulnerability management},
	pages = {1--13},
}

@inproceedings{zhao_empirical_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An empirical investigation of practical log anomaly detection for online service systems},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473933},
	doi = {10.1145/3468264.3473933},
	abstract = {Log data is an essential and valuable resource of online service systems, which records detailed information of system running status and user behavior. Log anomaly detection is vital for service reliability engineering, which has been extensively studied. However, we find that existing approaches suffer from several limitations when deploying them into practice, including 1) inability to deal with various logs and complex log abnormal patterns; 2) poor interpretability; 3) lack of domain knowledge. To help understand these practical challenges and investigate the practical performance of existing work quantitatively, we conduct the first empirical study and an experimental study based on large-scale real-world data. We find that logs with rich information indeed exhibit diverse abnormal patterns (e.g., keywords, template count, template sequence, variable value, and variable distribution). However, existing approaches fail to tackle such complex abnormal patterns, producing unsatisfactory performance. Motivated by obtained findings, we propose a generic log anomaly detection system named LogAD based on ensemble learning, which integrates multiple anomaly detection approaches and domain knowledge, so as to handle complex situations in practice. About the effectiveness of LogAD, the average F1-score achieves 0.83, outperforming all baselines. Besides, we also share some success cases and lessons learned during our study. To our best knowledge, we are the first to investigate practical log anomaly detection in the real world deeply. Our work is helpful for practitioners and researchers to apply log anomaly detection to practice to enhance service reliability.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Nengwen and Wang, Honglin and Li, Zeyan and Peng, Xiao and Wang, Gang and Pan, Zhu and Wu, Yong and Feng, Zhen and Wen, Xidao and Zhang, Wenchi and Sui, Kaixin and Pei, Dan},
	month = aug,
	year = {2021},
	keywords = {Log Anomaly Detection, Online Service Systems, Practical Challenges},
	pages = {1404--1415},
}

@inproceedings{ye_empirical_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An empirical study of {GUI} widget detection for industrial mobile games},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473935},
	doi = {10.1145/3468264.3473935},
	abstract = {With the widespread adoption of smartphones in our daily life, mobile games experienced increasing demand over the past years. Meanwhile, the quality of mobile games has been continuously drawing more and more attention, which can greatly affect the player experience. For better quality assurance, general-purpose testing has been extensively studied for mobile apps. However, due to the unique characteristic of mobile games, existing mobile testing techniques may not be directly suitable and applicable. To better understand the challenges in mobile game testing, in this paper, we first initiate an early step to conduct an empirical study towards understanding the challenges and pain points of mobile game testing process at our industrial partner NetEase Games. Specifically, we first conduct a survey from the mobile test development team at NetEase Games via both scrum interviews and questionnaires. We found that accurate and effective GUI widget detection for mobile games could be the pillar to boost the automation of mobile game testing and other downstream analysis tasks in practice. We then continue to perform comparative studies to investigate the effectiveness of state-of-the-art general-purpose mobile app GUI widget detection methods in the context of mobile games. To this end, we also develop a technique to automatically collect GUI widgets region information of industrial mobile games, which is equipped with a heuristic-based data cleaning method for quality refinement of the labeling results. Our evaluation shows that: (1) Existing GUI widget detection methods for general-purpose mobile apps cannot perform well on industrial mobile games. (2) Mobile game exhibits obvious difference from other general-purpose mobile apps in the perspective GUI widgets. Our further in-depth analysis reveals high diversity and density characteristics of mobile game GUI widgets could be the major reasons that post the challenges for existing methods, which calls for new research methods and better industry practices. To enable further research along this line, we construct the very first GUI widget detection benchmark, specially designed for mobile games, incorporating both our collected dataset and the state-of-the-art widget detection methods for mobile apps, which could also be the basis for further study of many downstream quality assurance tasks (e.g., testing and analysis) for mobile games.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ye, Jiaming and Chen, Ke and Xie, Xiaofei and Ma, Lei and Huang, Ruochen and Chen, Yingfeng and Xue, Yinxing and Zhao, Jianjun},
	month = aug,
	year = {2021},
	keywords = {Deep Learning, GUI Detection, Game Testing},
	pages = {1427--1437},
}

@inproceedings{wen_empirical_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, and cloud providers can leverage these findings and implications to better understand and further explore the serverless computing paradigm.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	keywords = {Application Development, Empirical Study, Serverless Computing, Stack Overflow},
	pages = {416--428},
}

@inproceedings{todericiu_students_2021,
	address = {New York, NY, USA},
	series = {{EASEAI} 2021},
	title = {Students perception on the impact of their involvement in the learning process: an empirical study},
	isbn = {978-1-4503-8624-1},
	shorttitle = {Students perception on the impact of their involvement in the learning process},
	url = {https://dl.acm.org/doi/10.1145/3472673.3473964},
	doi = {10.1145/3472673.3473964},
	abstract = {Nowadays, when the changes that appear in programming paradigms and in software process development methodologies are extremely frequent, teaching a Software Engineering related course has become a demanding task. To all these are added changes caused by the dynamics of the society and the traits of the current learners and how they learn. To cope with the challenges mentioned above, the paper proposes a complementary method for Project Based Learning in teaching two Software Engineering related courses, at undergraduate and master level at Babeş-Bolyai University. Its contribution is twofold: firstly, it frames a new pedagogical approach based on “Students Generating Questions” as a learning strategy, defined in a collaborative way. The approach is supported by an e-learning platform designed as smart learning environment. Secondly, it investigates through a quantitative and qualitative analysis, the students perceptions, their feedback and learning experiences on the use of applying this learning method. The results of the survey indicate that the proposed learning method helped students to better regulate their learning and to achieve their goals. It also revealed some advantages reported by the students such as reduction of test anxiety, productive collaborative learning and the creation of a question bank which represents a consistent and comprehensive material for training during the semester and for their exam preparation.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Education} through {Advanced} {Software} {Engineering} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Todericiu, Ioana and Serban, Camelia and Vescan, Andreea},
	month = aug,
	year = {2021},
	keywords = {collaborative learning, elaboration, feedback, retrieval practice, students involvement},
	pages = {39--46},
}

@inproceedings{he_large-scale_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {A large-scale empirical study on {Java} library migrations: prevalence, trends, and rationales},
	isbn = {978-1-4503-8562-6},
	shorttitle = {A large-scale empirical study on {Java} library migrations},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468571},
	doi = {10.1145/3468264.3468571},
	abstract = {With the rise of open-source software and package hosting platforms, reusing 3rd-party libraries has become a common practice. Due to various failures during software evolution, a project may remove a used library and replace it with another library, which we call library migration. Despite substantial research on dependency management, the understanding of how and why library migrations occur is still lacking. Achieving this understanding may help practitioners optimize their library selection criteria, develop automated approaches to monitor dependencies, and provide migration suggestions for their libraries or software projects. In this paper, through a fine-grained commit-level analysis of 19,652 Java GitHub projects, we extract the largest migration dataset to-date (1,194 migration rules, 3,163 migration commits). We show that 8,065 (41.04\%) projects having at least one library removal, 1,564 (7.96\%, lower-bound) to 5,004 (25.46\%, upper-bound) projects have at least one migration, and a median project with migrations has 2 to 4 migrations in total. We discover that library migrations are dominated by several domains (logging, JSON, testing and web service) presenting a long tail distribution. Also, migrations are highly unidirectional in that libraries are either mostly abandoned or mostly chosen in our project corpus. A thematic analysis on related commit messages, issues, and pull requests identifies 14 frequently mentioned migration reasons (e.g., lack of maintenance, usability, integration, etc), 7 of which are not discussed in previous work. Our findings can be operationalized into actionable insights for package hosting platforms, project maintainers, and library developers. We provide a replication package at {\textless}a{\textgreater}https://doi.org/10.5281/zenodo.4816752{\textless}/a{\textgreater}.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {He, Hao and He, Runzhi and Gu, Haiqiao and Zhou, Minghui},
	month = aug,
	year = {2021},
	keywords = {empirical software engineering, evolution and maintenance, library migration, mining software repositories},
	pages = {478--490},
}

@inproceedings{chirkova_empirical_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {Empirical study of transformers for source code},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468611},
	doi = {10.1145/3468264.3468611},
	abstract = {Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chirkova, Nadezhda and Troshin, Sergey},
	month = aug,
	year = {2021},
	keywords = {code completion, function naming, neural networks, transformer, variable misuse detection},
	pages = {703--715},
}

@inproceedings{yi_empirical_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An empirical study of blockchain system vulnerabilities: modules, types, and patterns},
	isbn = {978-1-4503-9413-0},
	shorttitle = {An empirical study of blockchain system vulnerabilities},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549105},
	doi = {10.1145/3540250.3549105},
	abstract = {Blockchain, as a distributed ledger technology, becomes increasingly popular, especially for enabling valuable cryptocurrencies and smart contracts. However, the blockchain software systems inevitably have many bugs. Although bugs in smart contracts have been extensively investigated, security bugs of the underlying blockchain systems are much less explored. In this paper, we conduct an empirical study on blockchain’s system vulnerabilities from four representative blockchains, Bitcoin, Ethereum, Monero, and Stellar. Specifically, we first design a systematic filtering process to effectively identify 1,037 vulnerabilities and their 2,317 patches from 34,245 issues/PRs (pull requests) and 85,164 commits on GitHub. We thus build the first blockchain vulnerability dataset, which is available at https://github.com/VPRLab/BlkVulnDataset. We then perform unique analyses of this dataset at three levels, including (i) file-level vulnerable module categorization by identifying and correlating module paths across projects, (ii) text-level vulnerability type clustering by natural language processing and similarity-based sentence clustering, and (iii) code-level vulnerability pattern analysis by generating and clustering code change signatures that capture both syntactic and semantic information of patch code fragments. Our analyses reveal three key findings: (i) some blockchain modules are more susceptible than the others; notably, each of the modules related to consensus, wallet, and networking has over 200 issues; (ii) about 70\% of blockchain vulnerabilities are of traditional types, but we also identify four new types specific to blockchains; and (iii) we obtain 21 blockchain-specific vulnerability patterns that capture unique blockchain attributes and statuses, and demonstrate that they can be used to detect similar vulnerabilities in other popular blockchains, such as Dogecoin, Bitcoin SV, and Zcash.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yi, Xiao and Wu, Daoyuan and Jiang, Lingxiao and Fang, Yuzhou and Zhang, Kehuan and Zhang, Wei},
	month = nov,
	year = {2022},
	keywords = {Blockchain Security, Data Mining, System Vulnerability},
	pages = {709--721},
}

@inproceedings{obrien_23_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {23 shades of self-admitted technical debt: an empirical study on machine learning software},
	isbn = {978-1-4503-9413-0},
	shorttitle = {23 shades of self-admitted technical debt},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549088},
	doi = {10.1145/3540250.3549088},
	abstract = {In software development, the term “technical debt” (TD) is used to characterize short-term solutions and workarounds implemented in source code which may incur a long-term cost. Technical debt has a variety of forms and can thus affect multiple qualities of software including but not limited to its legibility, performance, and structure. In this paper, we have conducted a comprehensive study on the technical debts in machine learning (ML) based software. TD can appear differently in ML software by infecting the data that ML models are trained on, thus affecting the functional behavior of ML systems. The growing inclusion of ML components in modern software systems have introduced a new set of TDs. Does ML software have similar TDs to traditional software? If not, what are the new types of ML specific TDs? Which ML pipeline stages do these debts appear? Do these debts differ in ML tools and applications and when they get removed? Currently, we do not know the state of the ML TDs in the wild. To address these questions, we mined 68,820 self-admitted technical debts (SATD) from all the revisions of a curated dataset consisting of 2,641 popular ML repositories from GitHub, along with their introduction and removal. By applying an open-coding scheme and following upon prior works, we provide a comprehensive taxonomy of ML SATDs. Our study analyzes ML SATD type organizations, their frequencies within stages of ML software, the differences between ML SATDs in applications and tools, and quantifies the removal of ML SATDs. The findings discovered suggest implications for ML developers and researchers to create maintainable ML systems.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {OBrien, David and Biswas, Sumon and Imtiaz, Sayem and Abdalkareem, Rabe and Shihab, Emad and Rajan, Hridesh},
	month = nov,
	year = {2022},
	keywords = {data science, machine learning, open-source, technical debt},
	pages = {734--746},
}

@inproceedings{nong_generating_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Generating realistic vulnerabilities via neural code editing: an empirical study},
	isbn = {978-1-4503-9413-0},
	shorttitle = {Generating realistic vulnerabilities via neural code editing},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549128},
	doi = {10.1145/3540250.3549128},
	abstract = {The availability of large-scale, realistic vulnerability datasets is essential both for benchmarking existing techniques and for developing effective new data-driven approaches for software security. Yet such datasets are critically lacking. A promising solution is to generate such datasets by injecting vulnerabilities into real-world programs, which are richly available. Thus, in this paper, we explore the feasibility of vulnerability injection through neural code editing. With a synthetic dataset and a real-world one, we investigate the potential and gaps of three state-of-the-art neural code editors for vulnerability injection. We find that the studied editors have critical limitations on the real-world dataset, where the best accuracy is only 10.03\%, versus 79.40\% on the synthetic dataset. While the graph-based editors are more effective (successfully injecting vulnerabilities in up to 34.93\% of real-world testing samples) than the sequence-based one (0 success), they still suffer from complex code structures and fall short for long edits due to their insufficient designs of the preprocessing and deep learning (DL) models. We reveal the promise of neural code editing for generating realistic vulnerable samples, as they help boost the effectiveness of DL-based vulnerability detectors by up to 49.51\% in terms of F1 score. We also provide insights into the gaps in current editors (e.g., they are good at deleting but not at replacing code) and actionable suggestions for addressing them (e.g., designing effective editing primitives).},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nong, Yu and Ou, Yuzhe and Pradel, Michael and Chen, Feng and Cai, Haipeng},
	month = nov,
	year = {2022},
	keywords = {benchmarking, data augmentation, data generation, datasets, deep learning, software vulnerability, vulnerability detection},
	pages = {1097--1109},
}

@inproceedings{ma_empirical_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An empirical investigation of missing data handling in cloud node failure prediction},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3558946},
	doi = {10.1145/3540250.3558946},
	abstract = {Cloud computing systems have become increasingly popular in recent years. A typical cloud system utilizes millions of computing nodes as the basic infrastructure. Node failure has been identified as one of the most prevalent causes of cloud system downtime. To improve the reliability of cloud systems, many previous studies collected monitoring metrics from nodes and built models to predict node failures before the failures happen. However, based on our experience with large-scale real-world cloud systems in Microsoft, we find that the task of predicting node failure is severely hampered by missing data. There is a large amount of missing data, and the online latest data utilized for prediction is even worse. As a result, the real-time performance of the node prediction model is limited. In this paper, we first characterize the missing data problem for node failure prediction. Then, we evaluate several existing data interpolation approaches, and find that node dimension interpolation approaches outperform time dimension ones and deep learning based interpolation is the best for early prediction. Our findings can help academics and engineers address the missing data problem in cloud node failure prediction and other data-driven software engineering scenarios.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Minghua and Liu, Yudong and Tong, Yuang and Li, Haozhe and Zhao, Pu and Xu, Yong and Zhang, Hongyu and He, Shilin and Wang, Lu and Dang, Yingnong and Rajmohan, Saravanakumar and Lin, Qingwei},
	month = nov,
	year = {2022},
	keywords = {Cloud systems, Missing data, Node failure prediction},
	pages = {1453--1464},
}

@inproceedings{kim_empirical_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An empirical study of deep transfer learning-based program repair for {Kotlin} projects},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3558967},
	doi = {10.1145/3540250.3558967},
	abstract = {Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning. This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307\%. Furthermore, we confirmed that the performance was improved by 532\% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696\%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Misoo and Kim, Youngkyoung and Jeong, Hohyeon and Heo, Jinseok and Kim, Sungoh and Chung, Hyunhee and Lee, Eunseok},
	month = nov,
	year = {2022},
	keywords = {Deep learning-based program repair, Empirical study, Industrial Kotlin project, SonarQube defects, Transfer learning},
	pages = {1441--1452},
}

@inproceedings{he_empirical_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {An empirical study of log analysis at {Microsoft}},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3558963},
	doi = {10.1145/3540250.3558963},
	abstract = {Logs are crucial to the management and maintenance of software systems. In recent years, log analysis research has achieved notable progress on various topics such as log parsing and log-based anomaly detection. However, the real voices from front-line practitioners are seldom heard. For example, what are the pain points of log analysis in practice? In this work, we conduct a comprehensive survey study on log analysis at Microsoft. We collected feedback from 105 employees through a questionnaire of 13 questions and individual interviews with 12 employees. We summarize the format, scenario, method, tool, and pain points of log analysis. Additionally, by comparing the industrial practices with academic research, we discuss the gaps between academia and industry, and future opportunities on log analysis with four inspiring findings. Particularly, we observe a huge gap exists between log anomaly detection research and failure alerting practices regarding the goal, technique, efficiency, etc. Moreover, data-driven log parsing, which has been widely studied in recent research, can be alternatively achieved by simply logging template IDs during software development. We hope this paper could uncover the real needs of industrial practitioners and the unnoticed yet significant gap between industry and academia, and inspire interesting future directions that converge efforts from both sides.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {He, Shilin and Zhang, Xu and He, Pinjia and Xu, Yong and Li, Liqun and Kang, Yu and Ma, Minghua and Wei, Yining and Dang, Yingnong and Rajmohan, Saravanakumar and Lin, Qingwei},
	month = nov,
	year = {2022},
	keywords = {Empirical Study, Log Analysis, Software Reliability},
	pages = {1465--1476},
}

@inproceedings{di_grazia_evolution_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {The evolution of type annotations in python: an empirical study},
	isbn = {978-1-4503-9413-0},
	shorttitle = {The evolution of type annotations in python},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549114},
	doi = {10.1145/3540250.3549114},
	abstract = {Type annotations and gradual type checkers attempt to reveal errors and facilitate maintenance in dynamically typed programming languages. Despite the availability of these features and tools, it is currently unclear how quickly developers are adopting them, what strategies they follow when doing so, and whether adding type annotations reveals more type errors. This paper presents the first large-scale empirical study of the evolution of type annotations and type errors in Python. The study is based on an analysis of 1,414,936 type annotation changes, which we extract from 1,123,393 commits among 9,655 projects. Our results show that (i) type annotations are getting more popular, and once added, often remain unchanged in the projects for a long time, (ii) projects follow three evolution patterns for type annotation usage -- regular annotation, type sprints, and occasional uses -- and that the used pattern correlates with the number of contributors, (iii) more type annotations help find more type errors (0.704 correlation), but nevertheless, many commits (78.3\%) are committed despite having such errors. Our findings show that better developer training and automated techniques for adding type annotations are needed, as most code still remains unannotated, and they call for a better integration of gradual type checking into the development process.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Di Grazia, Luca and Pradel, Michael},
	month = nov,
	year = {2022},
	keywords = {Python, Type annotations, empirical study, type errors},
	pages = {209--220},
}

@inproceedings{fritzsch_resume-driven_2021,
	title = {Résumé-{Driven} {Development}: {A} {Definition} and {Empirical} {Characterization}},
	shorttitle = {Résumé-{Driven} {Development}},
	doi = {10.1109/ICSE-SEIS52602.2021.00011},
	abstract = {Technologies play an important role in the hiring process for software professionals. Within this process, several studies revealed misconceptions and bad practices which lead to suboptimal recruitment experiences. In the same context, grey literature anecdotally coined the term Résumé-Driven Development (RDD), a phenomenon describing the overemphasis of trending technologies in both job offerings and resumes as an interaction between employers and applicants. While RDD has been sporadically mentioned in books and online discussions, there are so far no scientific studies on the topic, despite its potential negative consequences. We therefore empirically investigated this phenomenon by surveying 591 software professionals in both hiring (130) and technical (558) roles and identified RDD facets in substantial parts of our sample: 60\% of our hiring professionals agreed that trends influence their job offerings, while 82\% of our software professionals believed that using trending technologies in their daily work makes them more attractive for prospective employers. Grounded in the survey results, we conceptualize a theory to frame and explain Résumé-Driven Development. Finally, we discuss influencing factors and consequences and propose a definition of the term. Our contribution provides a foundation for future research and raises awareness for a potentially systemic trend that may broadly affect the software industry.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society} ({ICSE}-{SEIS})},
	author = {Fritzsch, Jonas and Wyrich, Marvin and Bogner, Justus and Wagner, Stefan},
	month = may,
	year = {2021},
	keywords = {Interviews, Market research, Recruitment, Resumes, Software, Software engineering, Software quality, career development, hiring, software development, survey, technology, theory},
	pages = {19--28},
}

@inproceedings{chen_empirical_2021,
	title = {An {Empirical} {Study} on {Deployment} {Faults} of {Deep} {Learning} {Based} {Mobile} {Applications}},
	doi = {10.1109/ICSE43902.2021.00068},
	abstract = {Deep learning (DL) is moving its step into a growing number of mobile software applications. These software applications, named as DL based mobile applications (abbreviated as mobile DL apps) integrate DL models trained using large-scale data with DL programs. A DL program encodes the structure of a desirable DL model and the process by which the model is trained using training data. Due to the increasing dependency of current mobile apps on DL, software engineering (SE) for mobile DL apps has become important. However, existing efforts in SE research community mainly focus on the development of DL models and extensively analyze faults in DL programs. In contrast, faults related to the deployment of DL models on mobile devices (named as deployment faults of mobile DL apps) have not been well studied. Since mobile DL apps have been used by billions of end users daily for various purposes including for safety-critical scenarios, characterizing their deployment faults is of enormous importance. To fill in the knowledge gap, this paper presents the first comprehensive study to date on the deployment faults of mobile DL apps. We identify 304 real deployment faults from Stack Overflow and GitHub, two commonly used data sources for studying software faults. Based on the identified faults, we construct a fine-granularity taxonomy consisting of 23 categories regarding to fault symptoms and distill common fix strategies for different fault symptoms. Furthermore, we suggest actionable implications and research avenues that can potentially facilitate the deployment of DL models on mobile devices.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Chen, Zhenpeng and Yao, Huihan and Lou, Yiling and Cao, Yanbin and Liu, Yuanqiang and Wang, Haoyu and Liu, Xuanzhe},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Data models, Mobile applications, Mobile handsets, Software, Software development management, Software engineering, Taxonomy, deep learning, mobile applications, deployment faults},
	pages = {674--685},
}

@inproceedings{romano_empirical_2021-1,
	title = {An {Empirical} {Analysis} of {UI}-{Based} {Flaky} {Tests}},
	doi = {10.1109/ICSE43902.2021.00141},
	abstract = {Flaky tests have gained attention from the research community in recent years and with good reason. These tests lead to wasted time and resources, and they reduce the reliability of the test suites and build systems they affect. However, most of the existing work on flaky tests focus exclusively on traditional unit tests. This work ignores UI tests that have larger input spaces and more diverse running conditions than traditional unit tests. In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involving rerunning test suites multiple times. In this paper, we perform a study on flaky UI tests. We analyze 235 flaky UI test samples found in 62 projects from both web and Android environments. We identify the common underlying root causes of flakiness in the UI tests, the strategies used to manifest the flaky behavior, and the fixing strategies used to remedy flaky UI tests. The findings made in this work can provide a foundation for the development of detection and prevention techniques for flakiness arising in UI tests.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Romano, Alan and Song, Zihe and Grandhi, Sampath and Yang, Wei and Wang, Weihang},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Android, Reliability, Software development management, Software engineering, flaky UI test, flaky test, web},
	pages = {1585--1597},
}

@inproceedings{han_empirical_2021,
	title = {An {Empirical} {Study} of the {Landscape} of {Open} {Source} {Projects} in {Baidu}, {Alibaba}, and {Tencent}},
	doi = {10.1109/ICSE-SEIP52600.2021.00039},
	abstract = {Open source software has drawn more and more attention from researchers, developers and companies nowadays. Meanwhile, many Chinese technology companies are embracing open source and choosing to open source their projects. Nevertheless, most previous studies are concentrated on international companies such as Microsoft or Google, while the practical values of open source projects of Chinese technology companies remain unclear. To address this issue, we conduct a mixed-method study to investigate the landscape of projects open sourced by three large Chinese technology companies, namely Baidu, Alibaba, and Tencent (BAT). We study the categories and characteristics of open source projects, the developer's perceptions towards open sourcing effort for these companies, and the internationalization effort of their open source projects. We collected 1,000 open source projects that were open sourced by BAT in GitHub and performed an online survey that received 101 responses from developers of these projects. Some key findings include: (1) BAT prefer to open source frontend development projects, (2) 88\% of the respondents are positive towards open sourcing software projects in their respective companies, (3) 64\% of the respondents reveal that the most common motivations for BAT to open source their projects are the desire to gain fame, expand their influence and gain recruitment advantage, (4) respondents believe that the most common internationalization effort is "providing an English version of readme files", (5) projects with more internationalization effort (i.e., include an English readme file) are more popular. Our findings provide directions for software engineering researchers and provide practical suggestions to software developers and Chinese technology companies.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Han, Junxiao and Deng, Shuiguang and Lo, David and Zhi, Chen and Yin, Jianwei and Xia, Xin},
	month = may,
	year = {2021},
	keywords = {Companies, Internet, Open source software, Recruitment, Software development management, Software engineering, Web and internet services},
	pages = {298--307},
}

@inproceedings{sun_empirical_2021,
	title = {An {Empirical} {Assessment} of {Global} {COVID}-19 {Contact} {Tracing} {Applications}},
	doi = {10.1109/ICSE43902.2021.00101},
	abstract = {The rapid spread of COVID-19 has made manual contact tracing difficult. Thus, various public health authorities have experimented with automatic contact tracing using mobile applications (or "apps"). These apps, however, have raised security and privacy concerns. In this paper, we propose an automated security and privacy assessment tool - COVIDGUARDIAN - which combines identification and analysis of Personal Identification Information (PII), static program analysis and data flow analysis, to determine security and privacy weaknesses. Furthermore, in light of our findings, we undertake a user study to investigate concerns regarding contact tracing apps. We hope that COVIDGUARDIAN, and the issues raised through responsible disclosure to vendors, can contribute to the safe deployment of mobile contact tracing. As part of this, we offer concrete guidelines, and highlight gaps between user requirements and app performance.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Sun, Ruoxi and Wang, Wei and Xue, Minhui and Tyson, Gareth and Camtepe, Seyit and Ranasinghe, Damith C.},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {COVID-19, Data privacy, Manuals, Mobile application, Mobile applications, Privacy, Software security privacy, Telecommunication traffic, Tools, contact tracing},
	pages = {1085--1097},
}

@inproceedings{pan_can_2021,
	title = {Can {Program} {Synthesis} be {Used} to {Learn} {Merge} {Conflict} {Resolutions}? {An} {Empirical} {Analysis}},
	shorttitle = {Can {Program} {Synthesis} be {Used} to {Learn} {Merge} {Conflict} {Resolutions}?},
	doi = {10.1109/ICSE43902.2021.00077},
	abstract = {Forking structure is widespread in the open-source repositories and that causes a significant number of merge conflicts. In this paper, we study the problem of textual merge conflicts from the perspective of Microsoft Edge, a large, highly collaborative fork off the main Chromium branch with significant merge conflicts. Broadly, this study is divided into two sections. First, we empirically evaluate textual merge conflicts in Microsoft Edge and classify them based on the type of files, location of conflicts in a file, and the size of conflicts. We found that 28\% of the merge conflicts are 1-2 line changes, and many resolutions have frequent patterns. Second, driven by these findings, we explore Program Synthesis (for the first time) to learn patterns and resolve structural merge conflicts. We propose a novel domain-specific language (DSL) that captures many of the repetitive merge conflict resolution patterns and learn resolution strategies as programs in this DSL from example resolutions. We found that the learned strategies can resolve 11.4\% of the conflicts ( 41\% of 1-2 line changes) that arise in the C++ files with 93.2\% accuracy.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Pan, Rangeet and Le, Vu and Nagappan, Nachiappan and Gulwani, Sumit and Lahiri, Shuvendu and Kaufman, Mike},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {C++ languages, Chromium, Collaboration, DSL, Domain specific languages, Open source software, Software engineering, automated fixing, merge conflict, program synthesis},
	pages = {785--796},
}

@inproceedings{foundjem_onboarding_2021,
	title = {Onboarding vs. {Diversity}, {Productivity} and {Quality} — {Empirical} {Study} of the {OpenStack} {Ecosystem}},
	doi = {10.1109/ICSE43902.2021.00097},
	abstract = {Despite the growing success of open-source software ecosystems (SECOs), their sustainability depends on the recruitment and involvement of ever-larger contributors. As such, onboarding, i.e., the socio-technical adaptation of new contributors to a SECO, forms a significant aspect of a SECO's growth that requires substantial resources. Unfortunately, despite theoretical models and initial user studies to examine the potential benefits of onboarding, little is known about the process of SECO onboarding, nor about the socio-technical benefits and drawbacks of contributors' onboarding experience in a SECO. To address these, we first carry out an observational study of 72 new contributors during an OpenStack onboarding event to provide a catalog of teaching content, teaching strategies, onboarding challenges, and expected benefits. Next, we empirically validate the extent to which diversity, productivity, and quality benefits are achieved by mining code changes, reviews, and contributors' issues with(out) OpenStack onboarding experience. Among other findings, our study shows a significant correlation with increasing gender diversity (65\% for both females and non-binary contributors) and patch acceptance rates (13.5\%). Onboarding also has a significant negative correlation with the time until a contributor's first commit and bug-proneness of contributions.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Foundjem, Armstrong and Eghan, Ellis and Adams, Bram},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Correlation, Ecosystems, Education, Gender issues, Onboarding, Mentoring, Collaboration, contributors, knowledge transfer, Software ecosystems, Open source, Productivity, Software engineering, Sustainable development},
	pages = {1033--1045},
}

@inproceedings{almanee_too_2021,
	title = {Too {Quiet} in the {Library}: {An} {Empirical} {Study} of {Security} {Updates} in {Android} {Apps}' {Native} {Code}},
	shorttitle = {Too {Quiet} in the {Library}},
	doi = {10.1109/ICSE43902.2021.00122},
	abstract = {Android apps include third-party native libraries to increase performance and to reuse functionality. Native code is directly executed from apps through the Java Native Interface or the Android Native Development Kit. Android developers add precompiled native libraries to their projects, enabling their use. Unfortunately, developers often struggle or simply neglect to update these libraries in a timely manner. This results in the continuous use of outdated native libraries with unpatched security vulnerabilities years after patches became available. To further understand such phenomena, we study the security updates in native libraries in the most popular 200 free apps on Google Play from Sept. 2013 to May 2020. A core difficulty we face in this study is the identification of libraries and their versions. Developers often rename or modify libraries, making their identification challenging. We create an approach called LibRARIAN (LibRAry veRsion IdentificAtioN) that accurately identifies native libraries and their versions as found in Android apps based on our novel similarity metric bin2sim. LibRARIAN leverages different features extracted from libraries based on their metadata and identifying strings in read-only sections. We discovered 53/200 popular apps (26.5\%) with vulnerable versions with known CVEs between Sept. 2013 and May 2020, with 14 of those apps remaining vulnerable. We find that app developers took, on average, 528.71±40.20 days to apply security patches, while library developers release a security patch after 54.59 ± 8.12 days-a 10 times slower rate of update.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Almanee, Sumaya and Ünal, Arda and Payer, Mathias and Garcia, Joshua},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Apps and App Store Analysis, Empirical Software Engineering, Evolution and maintenance, Feature extraction, Internet, Libraries, Measurement, Mining Software Repositories, Mobile applications, Security, Software Security, Software engineering, Task analysis},
	pages = {1347--1359},
}

@inproceedings{trinkenreich_empirical_2022,
	title = {An {Empirical} {Investigation} on the {Challenges} {Faced} by {Women} in the {Software} {Industry}: {A} {Case} {Study}},
	shorttitle = {An {Empirical} {Investigation} on the {Challenges} {Faced} by {Women} in the {Software} {Industry}},
	doi = {10.1145/3510458.3513018},
	abstract = {Context: Addressing women's under-representation in the soft-ware industry, a widely recognized concern, requires attracting as well as retaining more women. Hearing from women practitioners, particularly those positioned in multi-cultural settings, about their challenges and and adopting their lived experienced solutions can support the design of programs to resolve the under-representation issue. Goal: We investigated the challenges women face in global software development teams, particularly what motivates women to leave their company; how those challenges might break down according to demographics; and strategies to mitigate the identified challenges. Method: To achieve this goal, we conducted an ex-ploratory case study in Ericsson, a global technology company. We surveyed 94 women and employed mixed-methods to analyze the data. Results: Our findings reveal that women face socio-cultural challenges, including work-life balance issues, benevolent and hos-tile sexism, lack of recognition and peer parity, impostor syndrome, glass ceiling bias effects, the prove-it-again phenomenon, and the maternal wall. The participants of our research provided different suggestions to address/mitigate the reported challenges, including sabbatical policies, flexibility of location and time, parenthood support, soft skills training for managers, equality of payment and opportunities between genders, mentoring and role models to sup-port career growth, directives to hire more women, inclusive groups and events, women's empowerment, and recognition for women's success. The framework of challenges and suggestions can inspire further initiatives both in academia and industry to onboard and retain women. Women represent less than 24\% of employees in software development industry and experience various types of prejudice and bias. Even in companies that care about Diversity \& Inclusion, “untying the mooring ropes” of socio-cultural problems is hard. Hearing from women, especially those working in a multi-cultural organization, about their challenges and adopting their suggestions can be vital to design programs and resolve the under-representation issue. In this work we work closely with a large software development or-ganization which invests and believes in diversity and inclusion. We listened to women and the challenges they face in global soft-ware development teams of this company and what these women suggest reduce the problems and increase retention. Our research showed that women face work-life balance issues and encounter invisible barriers that prevent them from rising to top positions. They also suffer micro-aggression and sexism, need to show com-petence constantly, be supervised in essential tasks, and receive less work after becoming mothers. Moreover, women miss having more female colleagues, lack self-confidence and recognition. The women from the company suggested sabbatical policies, the flexibil-ity of location and time, parenthood support, soft skills training for managers, equality of opportunities, role models to support career growth, directives to hire more women, support groups, and more interaction between women, inclusive groups and events, women's empowerment by publishing their success stories in media and recognizing their achievements. Our results had been shared with the company Human Resources department and management and they considered the diagnosis helpful and will work on actions to mitigate the challenges that women still perceive.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society} ({ICSE}-{SEIS})},
	author = {Trinkenreich, Bianca and Britto, Ricardo and Gerosa, Marco A. and Steinmacher, Igor},
	month = may,
	year = {2022},
	keywords = {Auditory system, Companies, Engineering profession, Face recognition, Industries, Software, Training, diversity, gender, inclusion, software engineering, women},
	pages = {24--35},
}

@inproceedings{luukkainen_aspa_2022,
	title = {{ASPA}: {A} {Static} {Analyser} to {Support} {Learning} and {Continuous} {Feedback} on {Programming} {Courses}. {An} {Empirical} {Validation}},
	shorttitle = {{ASPA}},
	doi = {10.1145/3510456.3514149},
	abstract = {For decades there have been arguments how to teach programming in the basic courses. Supportive intervention methods to improve students’ learning and methods to improve assessment process have been widely studied. There are various successful methods to each topic separately, but only a few of them fit for both. In this work, we aimed at validating ASPA a static analyser tool that supports learning and continuous feedback on programming courses. For this purpose, we designed and conduct an empirical study among 236 students enrolled in the basic programming course, that voluntary adopted the tools during the project development activities. We first profiled the students, then, evaluated the attitude toward using ASPA, the perceived ease of use, and the perceived usefulness. Results showed that ASPA is a good helper for the entire course and especially the student’s programming assignments, and it also helps to improve the students’ grades.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} {Education} and {Training} ({ICSE}-{SEET})},
	author = {Luukkainen, Roope and Kasurinen, Jussi and Nikula, Uolevi and Lenarduzzi, Valentina},
	month = may,
	year = {2022},
	keywords = {CS1, Empirical Software Engineering, Planning, Programming Education, Programming profession, Software, Software Education, Software engineering, Static Analysis Tools, Static analysis, Training},
	pages = {29--39},
}

@inproceedings{zhou_cross-company_2022,
	title = {A {Cross}-{Company} {Ethnographic} {Study} on {Software} {Teams} for {DevOps} and {Microservices}: {Organization}, {Benefits}, and {Issues}},
	shorttitle = {A {Cross}-{Company} {Ethnographic} {Study} on {Software} {Teams} for {DevOps} and {Microservices}},
	doi = {10.1145/3510457.3513054},
	abstract = {Context: DevOps and microservices are acknowledged to be important new paradigms to tackle contemporary software demands and provide capabilities for rapid and reliable software development. Industrial reports show that they are quickly adopted together in massive software companies. However, because of the technical and organizational requirements, many difficulties against efficient implementation of the both emerge in real software teams. Objectives: This study aims to discover the organization, benefits and issues of software teams using DevOps \& microservices from an immersive perspective. Method: An ethnographic study was carried out in three companies with different business, size, products, customers, and degree of globalization. All the three companies claimed their adoption of DevOps and microservices. Seven months (cumulative) of participant observations and nine interviews with practitioners were conducted to collect the data of software teams related to DevOps and microservices. A cross-company empirical investigation using grounded theory was done by analyzing the archive data. Results: The virtual software teams were organized for adopting DevOps and microservices under the stubborn organizational structure. The adoption of DevOps and microservices brings benefits to rapid delivery, ability improvements and burden reduction, whilst the high cost and lack of practical guidance were emerged. Two major issues of adopting DevOps and microservices in software teams (i.e. fragmentary DevOps and abuse of microservices) were found common in the companies. Moreover, our observations and interviews reflect that in software teams, the relationship between DevOps and microservices is not significant, which differs from the relationship described in the previous studies. Four lessons for practitioners and four implications for researchers were discussed based on our findings. Conclusion: Our findings contribute to the understanding of the organization, benefits and issues of adopting DevOps and microservices from an immersive perspective of software teams.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Zhou, Xin and Huang, Huang and Zhang, He and Huang, Xin and Shao, Dong and Zhong, Chenxin},
	month = may,
	year = {2022},
	keywords = {Companies, Costs, DevOps, Microservice architectures, Pipelines, Software, Switches, Teamwork, ethnographic study, interview, microservices, participant observation},
	pages = {1--10},
}

@inproceedings{li_testing_2022,
	title = {Testing {Machine} {Learning} {Systems} in {Industry}: {An} {Empirical} {Study}},
	shorttitle = {Testing {Machine} {Learning} {Systems} in {Industry}},
	doi = {10.1145/3510457.3513036},
	abstract = {Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitioners’ experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Li†, Shuyue and Guo†, Jiaqi and Lou, Jian-Guang and Fan, Ming and Liu‡, Ting and Zhang, Dongmei},
	month = may,
	year = {2022},
	keywords = {Analytical models, Data collection, Industries, Machine learning, Software systems, Software testing, Technological innovation, machine learning, software testing, survey},
	pages = {263--272},
}

@inproceedings{zhu_empirical_2022,
	title = {An {Empirical} {Study} on {Quality} {Issues} of {eBay}'s {Big} {Data} {SQL} {Analytics} {Platform}},
	doi = {10.1145/3510457.3513034},
	abstract = {Big data SQL analytics platform has evolved as the key infrastructure for business data analysis. Compared with traditional costly commercial RDBMS, scalable solutions with open-source projects, such as SQL-on-Hadoop, are more popular and attractive to enter-prises. In eBay, we build Carmel, a company-wide interactive SQL analytics platform based on Apache Spark. Carmel has been serving thousands of customers from hundreds of teams globally for more than 3 years. Meanwhile, despite the popularity of open-source based big data SQL analytics platforms, few empirical studies on service quality issues (e.g., job failure) were carried out for them. However, a deep understanding of service quality issues and taking right mitigation are significant to the ease of manual maintenance efforts. To fill this gap, we conduct a comprehensive empirical study on 1,884 real-word service quality issues from Carmel. We summa-rize the common symptoms and identify the root causes with typical cases. Stakeholders including system developers, researchers, and platform maintainers can benefit from our findings and implications. Furthermore, we also present lessons learned from critical cases in our daily practice, as well as insights to motivate automatic tool support and future research directions.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Zhu, Feng and Xu, Lijie and Ma, Gang and Ji, Shuping and Wang, Jie and Wang, Gang and Zhang, Hongyi and Wan, Kun and Wang, Mingming and Zhang, Xingchao and Wang, Yuming and Li, Jingpin},
	month = may,
	year = {2022},
	keywords = {Big Data, Empirical Study, Fault diagnosis, Maintenance engineering, Manuals, Open Source, Pain, Reliability engineering, SQL on Hadoop, Software reliability},
	pages = {33--42},
}

@inproceedings{de_stefano_empirical_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {An empirical study on the current adoption of quantum programming},
	isbn = {978-1-4503-9223-5},
	url = {https://dl.acm.org/doi/10.1145/3510454.3522679},
	doi = {10.1145/3510454.3522679},
	abstract = {Quantum computing is no longer just a scientific curiosity; it is rapidly evolving into a commercially viable technology that has the potential to surpass the limitations of classical computation. As a result of this transition, a new discipline known as quantum software engineering has emerged, which is needed to describe unique methodologies for developing large-scale quantum applications. In the pursue of building this new body of knowledge, we undertake a mining study to elicit the purposes quantum programming is being used for, and steer further research.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {De Stefano, Manuel},
	month = oct,
	year = {2022},
	pages = {310--312},
}

@inproceedings{de_stefano_empirical_2022-1,
	address = {Pittsburgh Pennsylvania},
	title = {An empirical study on the current adoption of quantum programming},
	isbn = {978-1-4503-9223-5},
	url = {https://dl.acm.org/doi/10.1145/3510454.3522679},
	doi = {10.1145/3510454.3522679},
	language = {en},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {ACM},
	author = {De Stefano, Manuel},
	month = may,
	year = {2022},
	pages = {310--312},
}

@inproceedings{yin_empirical_2022,
	title = {An {Empirical} {Study} on {Implicit} {Constraints} in {Smart} {Contract} {Static} {Analysis}},
	doi = {10.1145/3510457.3513076},
	abstract = {Smart contracts are usually financial-related, which makes them attractive attack targets. Many static analysis tools have been developed to facilitate the contract audit process, but not all of them take account of two special features of smart contracts: (1) The external variables, like time, are constrained by real-world factors; (2) The internal variables persist between executions. Since these features import implicit constraints into contracts, they significantly affect the performance of static tools, such as causing errors in reachability analysis and resulting in false positives. In this paper, we conduct a systematic study on implicit constraints from three aspects. First, we summarize the implicit constraints in smart contracts. Second, we evaluate the impact of such constraints on the state-of-the-art static tools. Third, we propose a lightweight but effective mitigation method named ConSym to deal with such constraints and integrate it into OSIRIS. The evaluation result shows that ConSym can filter out 96\% of false positives and reduce false negatives by two-thirds.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Yin, Tingting and Zhang, Chao and Ni, Yuandong and Wu, Yixiong and Wong, Taiyu and Luo, Xiapu and Li, Zheming and Guo, Yu},
	month = may,
	year = {2022},
	keywords = {Code audit, Implicit constraints, Reachability analysis, Smart contract, Smart contracts, Software engineering, Static analysis, Systematics, Time factors},
	pages = {31--32},
}

@inproceedings{lu_software_2022,
	title = {Software engineering for {Responsible} {AI}: {An} empirical study and operationalised patterns},
	shorttitle = {Software engineering for {Responsible} {AI}},
	doi = {10.1145/3510457.3513063},
	abstract = {AI ethics principles and guidelines are typically high-level and do not provide concrete guidance on how to develop responsible AI systems. To address this shortcoming, we perform an empirical study involving interviews with 21 scientists and engineers to understand the practitioners' views on AI ethics principles and their implementation. Our major findings are: (1) the current practice is often a done-once-and-forget type of ethical risk assessment at a particular development step, which is not sufficient for highly uncertain and continual learning AI systems; (2) ethical requirements are either omitted or mostly stated as high-level objectives, and not specified explicitly in verifiable way as system outputs or outcomes; (3) although ethical requirements have the characteristics of cross-cutting quality and non-functional requirements amenable to architecture and design analysis, system-level architecture and design are under-explored; (4) there is a strong desire for continuously monitoring and validating AI systems post deployment for ethical requirements but current operation practices provide limited guidance. To address these findings, we suggest a preliminary list of patterns to provide operationalised guidance for developing responsible AI systems.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Douglas, David and Sanderson, Conrad},
	month = may,
	year = {2022},
	keywords = {AI, Artificial intelligence, Computer architecture, DevOps, Ethics, Interviews, Monitoring, Risk management, Software engineering, artificial intelligence, ethics, machine learning, responsible AI, software architecture, software engineering},
	pages = {241--242},
}

@inproceedings{hort_privileged_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Privileged and unprivileged groups: an empirical study on the impact of the age attribute on fairness},
	isbn = {978-1-4503-9292-1},
	shorttitle = {Privileged and unprivileged groups},
	url = {https://dl.acm.org/doi/10.1145/3524491.3527308},
	doi = {10.1145/3524491.3527308},
	language = {en},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Equitable} {Data} and {Technology}},
	publisher = {ACM},
	author = {Hort, Max and Sarro, Federica},
	month = may,
	year = {2022},
	pages = {17--24},
}

@inproceedings{imai_is_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Is {GitHub} copilot a substitute for human pair-programming?: an empirical study},
	isbn = {978-1-4503-9223-5},
	shorttitle = {Is {GitHub} copilot a substitute for human pair-programming?},
	url = {https://dl.acm.org/doi/10.1145/3510454.3522684},
	doi = {10.1145/3510454.3522684},
	language = {en},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {ACM},
	author = {Imai, Saki},
	month = may,
	year = {2022},
	pages = {319--321},
}

@inproceedings{fraser_evosuite_2011,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} '11},
	title = {{EvoSuite}: automatic test suite generation for object-oriented software},
	isbn = {978-1-4503-0443-6},
	shorttitle = {{EvoSuite}},
	url = {https://dl.acm.org/doi/10.1145/2025113.2025179},
	doi = {10.1145/2025113.2025179},
	abstract = {To find defects in software, one needs test cases that execute the software systematically, and oracles that assess the correctness of the observed behavior when running these test cases. This paper presents EvoSuite, a tool that automatically generates test cases with assertions for classes written in Java code. To achieve this, EvoSuite applies a novel hybrid approach that generates and optimizes whole test suites towards satisfying a coverage criterion. For the produced test suites, EvoSuite suggests possible oracles by adding small and effective sets of assertions that concisely summarize the current behavior; these assertions allow the developer to detect deviations from expected behavior, and to capture the current behavior in order to protect against future defects breaking this behavior.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th {European} conference on {Foundations} of software engineering},
	publisher = {Association for Computing Machinery},
	author = {Fraser, Gordon and Arcuri, Andrea},
	month = sep,
	year = {2011},
	keywords = {assertion generation, search based soft- ware testing, test case generation},
	pages = {416--419},
}

@inproceedings{pacheco_feedback-directed_2007,
	title = {Feedback-{Directed} {Random} {Test} {Generation}},
	doi = {10.1109/ICSE.2007.37},
	abstract = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
	booktitle = {29th {International} {Conference} on {Software} {Engineering} ({ICSE}'07)},
	author = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ernst, Michael D. and Ball, Thomas},
	month = may,
	year = {2007},
	note = {ISSN: 1558-1225},
	keywords = {Contracts, Error correction codes, Feedback, Filters, Law, Legal factors, Object oriented modeling, Open source software, Software testing, System testing},
	pages = {75--84},
}

@inproceedings{serra_effectiveness_2019,
	title = {On the {Effectiveness} of {Manual} and {Automatic} {Unit} {Test} {Generation}: {Ten} {Years} {Later}},
	shorttitle = {On the {Effectiveness} of {Manual} and {Automatic} {Unit} {Test} {Generation}},
	doi = {10.1109/MSR.2019.00028},
	abstract = {Good unit tests play a paramount role when it comes to foster and evaluate software quality. However, writing effective tests is an extremely costly and time consuming practice. To reduce such a burden for developers, researchers devised ingenious techniques to automatically generate test suite for existing code bases. Nevertheless, how automatically generated test cases fare against manually written ones is an open research question. In 2008, Bacchelli et.al. conducted an initial case study comparing automatic and manually generated test suites. Since in the last ten years we have witnessed a huge amount of work on novel approaches and tools for automatic test generation, in this paper we revise their study using current tools as well as complementing their research method by evaluating these tools' ability in finding regressions. Preprint [https://doi.org/10.5281/zenodo.2595232], dataset [https://doi.org/10.6084/m9.figshare.7628642].},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Serra, Domenico and Grano, Giovanni and Palomba, Fabio and Ferrucci, Filomena and Gall, Harald C. and Bacchelli, Alberto},
	month = may,
	year = {2019},
	note = {ISSN: 2574-3864},
	keywords = {Automatic Test Case Generation, Empirical Studies, Fault detection, Manuals, Production, Software, Software Testing, Test pattern generators, Tools},
	pages = {121--125},
}

@inproceedings{fonseca_empirical_2017,
	address = {New York, NY, USA},
	series = {{EuroSys} '17},
	title = {An {Empirical} {Study} on the {Correctness} of {Formally} {Verified} {Distributed} {Systems}},
	isbn = {978-1-4503-4938-3},
	url = {https://dl.acm.org/doi/10.1145/3064176.3064183},
	doi = {10.1145/3064176.3064183},
	abstract = {Recent advances in formal verification techniques enabled the implementation of distributed systems with machine-checked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation of the results and verification practices. This paper thoroughly analyzes three state-of-the-art, formally verified implementations of distributed systems: Iron-Fleet, Verdi, and Chapar. Through code review and testing, we found a total of 16 bugs, many of which produce serious consequences, including crashing servers, returning incorrect results to clients, and invalidating verification guarantees. These bugs were caused by violations of a wide-range of assumptions on which the verified components relied. Our results revealed that these assumptions referred to a small fraction of the trusted computing base, mostly at the interface of verified and unverified components. Based on our observations, we have built a testing toolkit called PK, which focuses on testing these parts and is able to automate the detection of 13 (out of 16) bugs.},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Computer} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fonseca, Pedro and Zhang, Kaiyuan and Wang, Xi and Krishnamurthy, Arvind},
	month = apr,
	year = {2017},
	pages = {328--343},
}

@article{ieee_ieee_2001,
	title = {{IEEE} {Standard} for {IEEE} {Information} {Technology} - {Portable} {Operating} {System} {Interface} ({POSIX}({TM}))},
	doi = {10.1109/IEEESTD.2001.93364},
	abstract = {This standard defines a standard operating system interface and environment, including a command interpreter (or "shell"), and common utility programs to support applications portability at the source code level. It is the single common revision to IEEE Std 1003.1-1996, IEEE Std 1003.2-1992, and the Base Specifications of The Open Group Single UNIX(TM)† Specification, Version 2. This standard is intended to be used by both applications developers and system implementors and comprises four major components (each in an associated volume)},
	journal = {IEEE Std 1003.1-2001 (Revision of IEEE Std 1003.1-1996 and IEEE Std 1003.2-1992)},
	author = {{IEEE}},
	month = dec,
	year = {2001},
	note = {Conference Name: IEEE Std 1003.1-2001 (Revision of IEEE Std 1003.1-1996 and IEEE Std 1003.2-1992)},
	keywords = {CPU, FIFO, application program interface (API), argument, asynchronous, basic regular expression (BRE), batch job, batch system, built-in utility, byte, child, command language interpreter, extended regular expression (ERE), file access control mechanism},
	pages = {1--3678},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Standards} {Association}},
	url = {https://standards.ieee.org},
	language = {en},
	urldate = {2023-04-12},
	journal = {IEEE Standards Association},
}

@misc{albertoni_reproducibility_2023,
	title = {Reproducibility of {Machine} {Learning}: {Terminology}, {Recommendations} and {Open} {Issues}},
	shorttitle = {Reproducibility of {Machine} {Learning}},
	url = {http://arxiv.org/abs/2302.12691},
	doi = {10.48550/arXiv.2302.12691},
	abstract = {Reproducibility is one of the core dimensions that concur to deliver Trustworthy Artificial Intelligence. Broadly speaking, reproducibility can be defined as the possibility to reproduce the same or a similar experiment or method, thereby obtaining the same or similar results as the original scientists. It is an essential ingredient of the scientific method and crucial for gaining trust in relevant claims. A reproducibility crisis has been recently acknowledged by scientists and this seems to affect even more Artificial Intelligence and Machine Learning, due to the complexity of the models at the core of their recent successes. Notwithstanding the recent debate on Artificial Intelligence reproducibility, its practical implementation is still insufficient, also because many technical issues are overlooked. In this survey, we critically review the current literature on the topic and highlight the open issues. Our contribution is three-fold. We propose a concise terminological review of the terms coming into play. We collect and systematize existing recommendations for achieving reproducibility, putting forth the means to comply with them. We identify key elements often overlooked in modern Machine Learning and provide novel recommendations for them. We further specialize these for two critical application domains, namely the biomedical and physical artificial intelligence fields.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Albertoni, Riccardo and Colantonio, Sara and Skrzypczyński, Piotr and Stefanowski, Jerzy},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12691 [cs]},
	keywords = {68T99, A.1, Computer Science - Artificial Intelligence, I.2},
}

@article{hartley_dtoolai_2020,
	title = {{dtoolAI}: {Reproducibility} for {Deep} {Learning}},
	volume = {1},
	issn = {2666-3899},
	shorttitle = {{dtoolAI}},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389920300933},
	doi = {10.1016/j.patter.2020.100073},
	abstract = {Deep learning, a set of approaches using artificial neural networks, has generated rapid recent advancements in machine learning. Deep learning does, however, have the potential to reduce the reproducibility of scientific results. Model outputs are critically dependent on the data and processing approach used to initially generate the model, but this provenance information is usually lost during model training. To avoid a future reproducibility crisis, we need to improve our deep-learning model management. The FAIR principles for data stewardship and software/workflow implementation give excellent high-level guidance on ensuring effective reuse of data and software. We suggest some specific guidelines for the generation and use of deep-learning models in science and explain how these relate to the FAIR principles. We then present dtoolAI, a Python package that we have developed to implement these guidelines. The package implements automatic capture of provenance information during model training and simplifies model distribution.},
	language = {en},
	number = {5},
	urldate = {2023-04-12},
	journal = {Patterns},
	author = {Hartley, Matthew and Olsson, Tjelvar S. G.},
	month = aug,
	year = {2020},
	keywords = {AI, FAIR data, artificial intelligence, data, data management, deep learning, machine learning, provenance, reproducibility},
	pages = {100073},
}

@article{norman_commentary_nodate,
	title = {{COMMENTARY}: {HUMAN} {ERROR} {AND} {THE} {DESIGN} {OF} {COMPUTER} {SYSTEMS}},
	journal = {Communications of the ACM},
	author = {Norman, Donald},
}

@inproceedings{johnson_software_2000,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Software {Support} for {Incident} {Reporting} {Systems} in {Safety}-{Critical} {Applications}},
	isbn = {978-3-540-40891-8},
	doi = {10.1007/3-540-40891-6_9},
	abstract = {Incident reporting systems are playing an increasingly important role in the development and maintenance of safety-critical applications. The perceived success of the FAA’s Aviation Safety Reporting System (ASRS) and the FDA’s MedWatch has led to the establishment of similar national and international schemes. These enable individuals and groups to report their safety concerns in a confidential or anonymous manner. Unfortunately, many of these systems are becoming victims of their own success. The ASRS and MedWatch have both now received over 500,000 submissions. In the past, these systems have relied upon conventional database technology to support the indexing and retrieval of individual reports. However, there are several reasons why this technology is inadequate for many large-scale reporting schemes. In particular, the problems of query formation often result in poor precision and recall. This, in turn, has profound implications for safety-critical applications. Users may fail to identify similar incidents within national or international collections. This paper, therefore, shows how several alternative software architectures support incident report systems in safety-critical applications.},
	language = {en},
	booktitle = {Computer {Safety}, {Reliability} and {Security}},
	publisher = {Springer},
	author = {Johnson, Chris},
	editor = {Koornneef, Floor and van der Meulen, Meine},
	year = {2000},
	keywords = {Incident Reporting, Query Formation, Relational Database, Software Failure, Software Support},
	pages = {96--106},
}

@inproceedings{zhang_statically_2021-1,
	address = {New York, NY, USA},
	series = {{CCS} '21},
	title = {Statically {Discovering} {High}-{Order} {Taint} {Style} {Vulnerabilities} in {OS} {Kernels}},
	isbn = {978-1-4503-8454-4},
	url = {https://dl.acm.org/doi/10.1145/3460120.3484798},
	doi = {10.1145/3460120.3484798},
	abstract = {Static analysis is known to yield numerous false alarms when used in bug finding, especially for complex vulnerabilities in large code bases like the Linux kernel. One important class of such complex vulnerabilities is what we call "high-order taint style vulnerability", where the taint flow from the user input to the vulnerable site crosses the boundary of a single entry function invocation (i.e., syscall). Due to the large scope and high precision requirement, few have attempted to solve the problem. In this paper, we present SUTURE, a highly precise and scalable static analysis tool capable of discovering high-order vulnerabilities in OS kernels. SUTURE employs a novel summary-based high-order taint flow construction approach to efficiently enumerate the cross-entry taint flows, while incorporating multiple innovative enhancements on analysis precision that are unseen in existing tools, resulting in a highly precise inter-procedural flow-, context-, field-, index-, and opportunistically path-sensitive static taint analysis. We apply SUTURE to discover high-order taint vulnerabilities in multiple Android kernels from mainstream vendors (e.g., Google, Samsung, Huawei), the results show that SUTURE can both confirm known high-order vulnerabilities and uncover new ones. So far, SUTURE generates 79 true positive warning groups, of which 19 have been confirmed by the vendors, including a high severity vulnerability rated by Google. SUTURE also achieves a reasonable false positive rate (51.23\%) perceived by users of our tool.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Hang and Chen, Weiteng and Hao, Yu and Li, Guoren and Zhai, Yizhuo and Zou, Xiaochen and Qian, Zhiyun},
	month = nov,
	year = {2021},
	keywords = {OS kernels, static program analysis, vulnerability discovery},
	pages = {811--824},
}

@inproceedings{yan_machine-learning-guided_2017,
	address = {New York, NY, USA},
	series = {{ACSAC} '17},
	title = {Machine-{Learning}-{Guided} {Typestate} {Analysis} for {Static} {Use}-{After}-{Free} {Detection}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134620},
	doi = {10.1145/3134600.3134620},
	abstract = {Typestate analysis relies on pointer analysis for detecting temporal memory safety errors, such as use-after-free (UAF). For large programs, scalable pointer analysis is usually imprecise in analyzing their hard "corner cases", such as infeasible paths, recursion cycles, loops, arrays, and linked lists. Due to a sound over-approximation of the points-to information, a large number of spurious aliases will be reported conservatively, causing the corresponding typestate analysis to report a large number of false alarms. Thus, the usefulness of typestate analysis for heap-intensive clients, like UAF detection, becomes rather limited, in practice. We introduce Tac, a static UAF detector that bridges the gap between typestate and pointer analyses by machine learning. Tac learns the correlations between program features and UAF-related aliases by using a Support Vector Machine (SVM) and applies this knowledge to further disambiguate the UAF-related aliases reported imprecisely by the pointer analysis so that only the ones validated by its SVM classifier are further investigated by the typestate analysis. Despite its unsoundness, Tac represents a practical typestate analysis approach for UAF detection. We have implemented Tac in LLVM-3.8.0 and evaluated it using a set of eight open-source C/C++ programs. The results show that Tac is effective (in terms of finding 5 known CVE vulnerabilities, 1 known bug, and 8 new bugs with a low false alarm rate) and scalable (in terms of analyzing a large codebase with 2,098 KLOC in just over 4 hours).},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Hua and Sui, Yulei and Chen, Shiping and Xue, Jingling},
	month = dec,
	year = {2017},
	keywords = {machine learning, static analysis, use-after-free, vulnerability detection},
	pages = {42--54},
}

@inproceedings{chen_static_2015,
	address = {New York, NY, USA},
	series = {{CCS} '15},
	title = {Static {Detection} of {Packet} {Injection} {Vulnerabilities}: {A} {Case} for {Identifying} {Attacker}-controlled {Implicit} {Information} {Leaks}},
	isbn = {978-1-4503-3832-5},
	shorttitle = {Static {Detection} of {Packet} {Injection} {Vulnerabilities}},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813643},
	doi = {10.1145/2810103.2813643},
	abstract = {Off-path packet injection attacks are still serious threats to the Internet and network security. In recent years, a number of studies have discovered new variations of packet injection attacks, targeting critical protocols such as TCP. We argue that such recurring problems need a systematic solution. In this paper, we design and implement PacketGuardian, a precise static taint analysis tool that comprehensively checks the packet handling logic of various network protocol implementations. The analysis operates in two steps. First, it identifies the critical paths and constraints that lead to accepting an incoming packet. If paths with weak constraints exist, a vulnerability may be revealed immediately. Otherwise, based on "secret" protocol states in the constraints, a subsequent analysis is performed to check whether such states can be leaked to an attacker. In the second step, observing that all previously reported leaks are through implicit flows, our tool supports implicit flow tainting, which is a commonly excluded feature due to high volumes of false alarms caused by it. To address this challenge, we propose the concept of attacker-controlled implicit information leaks, and prioritize our tool to detect them, which effectively reduces false alarms without compromising tool effectiveness. We use PacketGuardian on 6 popular protocol implementations of TCP, SCTP, DCCP, and RTP, and uncover new vulnerabilities in Linux kernel TCP as well as 2 out of 3 RTP implementations. We validate these vulnerabilities and confirm that they are indeed highly exploitable.},
	urldate = {2023-04-09},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Qi Alfred and Qian, Zhiyun and Jia, Yunhan Jack and Shao, Yuru and Mao, Zhuoqing Morley},
	month = oct,
	year = {2015},
	keywords = {implicit information leakage, network protocol security, side channel detection, static analysis},
	pages = {388--400},
}

@inproceedings{chen_static_2015-1,
	address = {New York, NY, USA},
	series = {{CCS} '15},
	title = {Static {Detection} of {Packet} {Injection} {Vulnerabilities}: {A} {Case} for {Identifying} {Attacker}-controlled {Implicit} {Information} {Leaks}},
	isbn = {978-1-4503-3832-5},
	shorttitle = {Static {Detection} of {Packet} {Injection} {Vulnerabilities}},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813643},
	doi = {10.1145/2810103.2813643},
	abstract = {Off-path packet injection attacks are still serious threats to the Internet and network security. In recent years, a number of studies have discovered new variations of packet injection attacks, targeting critical protocols such as TCP. We argue that such recurring problems need a systematic solution. In this paper, we design and implement PacketGuardian, a precise static taint analysis tool that comprehensively checks the packet handling logic of various network protocol implementations. The analysis operates in two steps. First, it identifies the critical paths and constraints that lead to accepting an incoming packet. If paths with weak constraints exist, a vulnerability may be revealed immediately. Otherwise, based on "secret" protocol states in the constraints, a subsequent analysis is performed to check whether such states can be leaked to an attacker. In the second step, observing that all previously reported leaks are through implicit flows, our tool supports implicit flow tainting, which is a commonly excluded feature due to high volumes of false alarms caused by it. To address this challenge, we propose the concept of attacker-controlled implicit information leaks, and prioritize our tool to detect them, which effectively reduces false alarms without compromising tool effectiveness. We use PacketGuardian on 6 popular protocol implementations of TCP, SCTP, DCCP, and RTP, and uncover new vulnerabilities in Linux kernel TCP as well as 2 out of 3 RTP implementations. We validate these vulnerabilities and confirm that they are indeed highly exploitable.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Qi Alfred and Qian, Zhiyun and Jia, Yunhan Jack and Shao, Yuru and Mao, Zhuoqing Morley},
	month = oct,
	year = {2015},
	keywords = {implicit information leakage, network protocol security, side channel detection, static analysis},
	pages = {388--400},
}

@inproceedings{cao_principled_2019,
	address = {New York, NY, USA},
	series = {{CCS} '19},
	title = {Principled {Unearthing} of {TCP} {Side} {Channel} {Vulnerabilities}},
	isbn = {978-1-4503-6747-9},
	url = {https://dl.acm.org/doi/10.1145/3319535.3354250},
	doi = {10.1145/3319535.3354250},
	abstract = {Recent work has showcased the presence of subtle TCP side channels in modern operating systems, that can be exploited by off-path adversaries to launch pernicious attacks such as hijacking a connection. Unfortunately, most work to date is on the manual discovery of such side-channels, and patching them subsequently. In this work we ask "Can we develop a principled approach that can lead to the automated discovery of such hard-to-find TCP side-channels?" We identify that the crux of why such side-channels exist is the violation of the non-interference property between simultaneous TCP connections i.e., there exist cases wherein a change in state of one connection implicitly leaks some information to a different connection (controlled possibly by an attacker). To find such non-interference property violations, we argue that model-checking is a natural fit. However, because of limitations with regards to its scalability, there exist many challenges in using model checking. Specifically, these challenges relate to (a) making the TCP code base self-contained and amenable to model checking and (b) limiting the search space of model checking and yet achieving reasonable levels of code coverage. We develop a tool that we call SCENT (for Side Channel Excavation Tool) that addresses these challenges in a mostly automated way. At the heart of SCENT is an automated downscaling component that transforms the TCP code base in a consistent way to achieve both a reduction in the state space complexity encountered by the model checker and the number and types of inputs needed for verification. Our extensive evaluations show that SCENT leads to the discovery of 12 new side channel vulnerabilities in the Linux and FreeBSD kernels. In particular, a real world validation with one class of vulnerabilities shows that an off-path attacker is able to infer whether two arbitrary hosts are communicating with each other, within slightly more than 1 minute, on average.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Yue and Wang, Zhongjie and Qian, Zhiyun and Song, Chengyu and Krishnamurthy, Srikanth V. and Yu, Paul},
	month = nov,
	year = {2019},
	keywords = {model-checking, side-channels, tcp},
	pages = {211--224},
}

@misc{noauthor_contiki_nodate,
	title = {Contiki {Vulnerabilities}},
	url = {https://docs.google.com/document/u/1/d/1ZwUKaOm7aCb-0wduFmXeq31-m0eKVcW_D__T7jID9HU/edit?usp=embed_facebook},
	language = {en},
	urldate = {2023-04-10},
	journal = {Google Docs},
}

@article{musuvathi_model_nodate,
	title = {Model {Checking} {Large} {Network} {Protocol} {Implementations}},
	abstract = {Network protocols must work. The eﬀects of protocol speciﬁcation or implementation errors range from reduced performance, to security breaches, to bringing down entire networks. However, network protocols are diﬃcult to test due to the exponential size of the state space they deﬁne. Ideally, a protocol implementation must be validated against all possible events (packet arrivals, packet losses, timeouts, etc.) in all possible protocol states. Conventional means of testing can explore only a minute fraction of these possible combinations.},
	language = {en},
	author = {Musuvathi, Madanlal and Engler, Dawson R},
}

@misc{boofuzz_boofuzz_nodate,
	title = {boofuzz: {Network} {Protocol} {Fuzzing} for {Humans} — boofuzz 0.4.1 documentation},
	url = {https://boofuzz.readthedocs.io/en/stable/},
	urldate = {2023-04-10},
	author = {{BooFuzz}},
}

@article{song_spfuzz_2019,
	title = {{SPFuzz}: {A} {Hierarchical} {Scheduling} {Framework} for {Stateful} {Network} {Protocol} {Fuzzing}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{SPFuzz}},
	doi = {10.1109/ACCESS.2019.2895025},
	abstract = {In recent years, the fuzzing technology is widely used to detect the software vulnerabilities owing to the coverage improvement in the target program and the easiness of use. However, it is less efficient to fuzz the stateful protocols due to the difficulties like maintaining states and dependencies of messages. To address these challenges, we present SPFuzz, a framework for building flexible, coverage-guided stateful protocol fuzzing. We define a language in SPFuzz to describe the protocol specifications, protocol states transitions and dependencies for generating valuable test cases, maintaining correct messages in session states and handling protocol dependencies by updating message data in time. The SPFuzz adopts a three-level mutation strategy, namely head, content, and sequence mutation strategy to drive the fuzzing process to cover more paths, in conjunction with the method to randomly assign weights to messages and strategies. We use the following metrics to evaluate the performance of SPFuzz and other frameworks upon three protocol implementations, i.e., Proftpd, Oftpd, and OpenSSL, which are three-granularity coverages specifically function, basic block, and edge. In experiments, the SPFuzz framework outperforms the existing stateful protocol fuzzing tool Boofuzz by an average of 69.12\% in three granularities coverage tests. This demonstrates that the SPFuzz has the ability to explore more and deeper paths of the target program. We further triggered CVE-2015-0291 in OpenSSL 1.0.2 with the SPFuzz, which proves the validity and utility of our framework.},
	journal = {IEEE Access},
	author = {Song, Congxi and Yu, Bo and Zhou, Xu and Yang, Qiang},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {AFL, Engines, Fuzzing, Protocols, Schedules, Security, Servers, Tools, coverage, fuzzing, network protocol, software security},
	pages = {18490--18499},
}

@misc{noauthor_sgpfuzzer_nodate,
	title = {{SGPFuzzer}: {A} {State}-{Driven} {Smart} {Graybox} {Protocol} {Fuzzer} for {Network} {Protocol} {Implementations} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/9200640},
	urldate = {2023-04-10},
}

@article{noauthor_notitle_nodate,
}

@misc{noauthor_nsdi_nodate,
	title = {{NSDI} '04 {Technical} {Paper}},
	url = {https://www.usenix.org/legacy/event/nsdi04/tech/full_papers/musuvathi/musuvathi_html/},
	urldate = {2023-04-10},
}

@article{lockefeer_formal_2016,
	series = {Formal {Methods} for {Industrial} {Critical} {Systems} ({FMICS}’2014)},
	title = {Formal specification and verification of {TCP} extended with the {Window} {Scale} {Option}},
	volume = {118},
	issn = {0167-6423},
	url = {https://www.sciencedirect.com/science/article/pii/S0167642315001835},
	doi = {10.1016/j.scico.2015.08.005},
	abstract = {We formally verify that TCP satisfies its requirements when extended with the Window Scale Option. With the aid of our μCRL specification and the LTSmin toolset, we verify that our specification of unidirectional TCP Data Transfer extended with the Window Scale Option does not deadlock, and that its external behaviour is branching bisimilar to a FIFO queue for a significantly large instance. Separately, we verify that a connection may only be closed if both entities accept the CLOSE call from the Application Layer. Finally, we recommend a rewording of the specification regarding how a zero window is probed, ensuring deadlocks do not arise as a result of misinterpretation.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Science of Computer Programming},
	author = {Lockefeer, Lars and Williams, David M. and Fokkink, Wan},
	month = mar,
	year = {2016},
	keywords = {CRL, Process algebra, Sliding window protocol, Transmission control protocol, Window scale option},
	pages = {3--23},
}

@misc{zimperium_labs_freertos_nodate,
	title = {{FreeRTOS} {TCP}/{IP} {Stack} {Vulnerabilities} - {The} {Details}},
	url = {https://www.zimperium.com/blog/freertos-tcpip-stack-vulnerabilities-details/},
	abstract = {Researcher: Ori Karliner (@oriHCX) Following our blog from last month, this blog will cover the technical details of our findings. If you suspect that any},
	language = {en-US},
	urldate = {2023-04-09},
	journal = {Zimperium},
	author = {{Zimperium Labs}},
}

@misc{microsoft_project_nodate,
	title = {Project {Everest}},
	url = {https://www.microsoft.com/en-us/research/project/project-everest-verified-secure-implementations-https-ecosystem/},
	abstract = {This project proposes to deﬁnitively solve the problem of a brittle HTTPS ecosystem by constructing a more secure, high performance, standards-compliant, veriﬁed implementation of the full HTTPS ecosystem. Unlike other veriﬁed software projects, our expedition aims to deploy Everest within existing software as a drop-in replacement in mainstream web browsers, servers, and other popular tools.},
	language = {en-US},
	urldate = {2023-04-07},
	journal = {Microsoft Research},
	author = {{Microsoft}},
}

@inproceedings{pirelli_automated_2022,
	title = {Automated {Verification} of {Network} {Function} {Binaries}},
	isbn = {978-1-939133-27-4},
	url = {https://www.usenix.org/conference/nsdi22/presentation/pirelli},
	language = {en},
	urldate = {2023-04-07},
	author = {Pirelli, Solal and Valentukonytė, Akvilė and Argyraki, Katerina and Candea, George},
	year = {2022},
	pages = {585--600},
}

@inproceedings{hawblitzel_ironfleet_2015,
	address = {New York, NY, USA},
	series = {{SOSP} '15},
	title = {{IronFleet}: proving practical distributed systems correct},
	isbn = {978-1-4503-3834-9},
	shorttitle = {{IronFleet}},
	url = {https://dl.acm.org/doi/10.1145/2815400.2815428},
	doi = {10.1145/2815400.2815428},
	abstract = {Distributed systems are notorious for harboring subtle bugs. Verification can, in principle, eliminate these bugs a priori, but verification has historically been difficult to apply at full-program scale, much less distributed-system scale. We describe a methodology for building practical and provably correct distributed systems based on a unique blend of TLA-style state-machine refinement and Hoare-logic verification. We demonstrate the methodology on a complex implementation of a Paxos-based replicated state machine library and a lease-based sharded key-value store. We prove that each obeys a concise safety specification, as well as desirable liveness requirements. Each implementation achieves performance competitive with a reference system. With our methodology and lessons learned, we aim to raise the standard for distributed systems from "tested" to "correct."},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the 25th {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {Association for Computing Machinery},
	author = {Hawblitzel, Chris and Howell, Jon and Kapritsos, Manos and Lorch, Jacob R. and Parno, Bryan and Roberts, Michael L. and Setty, Srinath and Zill, Brian},
	month = oct,
	year = {2015},
	pages = {1--17},
}

@inproceedings{zaostrovnykh_verifying_2019,
	address = {New York, NY, USA},
	series = {{SOSP} '19},
	title = {Verifying software network functions with no verification expertise},
	isbn = {978-1-4503-6873-5},
	url = {https://dl.acm.org/doi/10.1145/3341301.3359647},
	doi = {10.1145/3341301.3359647},
	abstract = {We present the design and implementation of Vigor, a software stack and toolchain for building and running software network middleboxes that are guaranteed to be correct, while preserving competitive performance and developer productivity. Developers write the core of the middlebox---the network function (NF)---in C, on top of a standard packet-processing framework, putting persistent state in data structures from Vigor's library; the Vigor toolchain then automatically verifies that the resulting software stack correctly implements a specification, which is written in Python. Vigor has three key features: network function developers need no verification expertise, and the verification process does not require their assistance (push-button verification); the entire software stack is verified, down to the hardware (full-stack verification); and verification can be done in a pay-as-you-go manner, i.e., instead of investing upfront a lot of time in writing and verifying a complete specification, one can specify one-off properties in a few lines of Python and verify them without concern for the rest. We developed five representative NFs---a NAT, a Maglev load balancer, a MAC-learning bridge, a firewall, and a traffic policer---and verified with Vigor that they satisfy standards-derived specifications, are memory-safe, and do not crash or hang. We show that they provide competitive performance. The Vigor framework is available at http://vigor.epfl.ch.},
	urldate = {2023-04-07},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {Association for Computing Machinery},
	author = {Zaostrovnykh, Arseniy and Pirelli, Solal and Iyer, Rishabh and Rizzo, Matteo and Pedrosa, Luis and Argyraki, Katerina and Candea, George},
	month = oct,
	year = {2019},
	pages = {275--290},
}

@misc{noauthor_iwl_nodate,
	title = {{IWL} {Network} {Emulators} and {Protocol} {Testers}},
	url = {https://www.iwl.com},
	abstract = {IWL creates both network emulators – to mimic the types of conditions an app or device would encounter in cloud, Internet, mobile networks, and protocol testers – to test apps and devices for conformance/robustness to industry standards.},
	language = {en-US},
	urldate = {2023-04-07},
	journal = {IWL},
}

@book{desikan_software_2006,
	title = {Software {Testing}: {Principles} and {Practice}},
	isbn = {978-81-7758-121-8},
	shorttitle = {Software {Testing}},
	abstract = {"Software Testing: Principles and Practices is a comprehensive treatise on software testing. It provides a pragmatic view of testing, addressing emerging areas like extreme testing and ad hoc testing"--Resource description page.},
	language = {en},
	publisher = {Pearson Education India},
	author = {Desikan, Srinivasan and Ramesh, Gopalaswamy},
	year = {2006},
	note = {Google-Books-ID: Yt2yRW6du9wC},
}

@misc{opengroup_ieee_nodate,
	title = {{IEEE} {Std} 1003.1-2017 ({Revision} of {IEEE} {Std} 1003.1-2008)},
	url = {https://pubs.opengroup.org/onlinepubs/9699919799/functions/contents.html},
	urldate = {2023-04-07},
	journal = {IEEE Std 1003.1-2017 (Revision of IEEE Std 1003.1-2008)},
	author = {{Opengroup}},
}

@misc{forescout_project_nodate,
	title = {Project {Memoria}},
	url = {https://www.forescout.com/research-labs/project-memoria/},
	abstract = {Project Memoria Securing TCP/IP Stacks Vedere Labs launched its Project Memoria initiative in 2020 with the mission of providing the cybersecurity community with the most extensive study to date of TCP/IP stacks security. Under Project Memoria, Forescout researchers collaborate with industry peers, universities and research institutes to analyze common mistakes associated with vulnerabilities in TCP/IP […]},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Forescout},
	author = {{Forescout}},
}

@misc{picotcp_picotcp_nodate,
	title = {picotcp},
	url = {http://picotcp.altran.be/},
	abstract = {PicoTCP is a free TCP/IP stack implementation},
	language = {en-US},
	urldate = {2023-04-07},
	journal = {picotcp},
	author = {{PicoTCP}},
}

@misc{lwip_lwip_nodate,
	title = {{lwIP}: {Overview}},
	url = {https://www.nongnu.org/lwip/2_1_x/index.html},
	urldate = {2023-04-07},
	author = {{lwIP}},
}

@misc{freertos_freertos_nodate,
	title = {{FreeRTOS} - {Market} leading {RTOS} ({Real} {Time} {Operating} {System}) for embedded systems with {Internet} of {Things} extensions},
	url = {https://www.freertos.org/index.html},
	abstract = {Market-leading MIT licensed open source real-time operating system (RTOS) for microcontrollers and microprocessors. Includes IoT and general purpose libraries.},
	language = {en-US},
	urldate = {2023-04-07},
	journal = {FreeRTOS},
	author = {{FreeRTOS}},
}

@misc{stanoev_contiki-ng_nodate,
	title = {Contiki-{NG}: {The} {OS} for {Next} {Generation} {IoT} {Devices}},
	url = {https://github.com/contiki-ng/contiki-ng/wiki/Home},
	abstract = {Contiki-NG: The OS for Next Generation IoT Devices - contiki-ng/contiki-ng},
	language = {en},
	urldate = {2023-04-07},
	journal = {GitHub},
	author = {Stanoev, Alex},
}

@misc{noauthor_openai_2023,
	title = {{OpenAI} {Cookbook}},
	copyright = {MIT},
	url = {https://github.com/openai/openai-cookbook/blob/6df6ceff470eeba26a56de131254e775292eac22/techniques_to_improve_reliability.md},
	abstract = {Examples and guides for using the OpenAI API},
	urldate = {2023-04-06},
	publisher = {OpenAI},
	month = apr,
	year = {2023},
	note = {original-date: 2022-03-11T02:08:53Z},
}

@misc{noauthor_ece_nodate,
	title = {{ECE} 595-{SE} (25 unread) {\textbar} {Piazza} {QA}},
	url = {https://piazza.com/class/ky206e6fsrq10c},
	urldate = {2023-04-03},
}

@inproceedings{panichella_how_2015,
	address = {Bremen, Germany},
	title = {How can i improve my app? {Classifying} user reviews for software maintenance and evolution},
	isbn = {978-1-4673-7532-0},
	shorttitle = {How can i improve my app?},
	url = {http://ieeexplore.ieee.org/document/7332474/},
	doi = {10.1109/ICSM.2015.7332474},
	abstract = {App Stores, such as Google Play or the Apple Store, allow users to provide feedback on apps by posting review comments and giving star ratings. These platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps. Previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks. However, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task. In this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) Natural Language Processing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify app reviews into the proposed categories. We show that the combined use of these techniques allows to achieve better results (a precision of 75\% and a recall of 74\%) than results obtained using each technique individually (precision of 70\% and a recall of 67\%).},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Panichella, Sebastiano and Di Sorbo, Andrea and Guzman, Emitza and Visaggio, Corrado A. and Canfora, Gerardo and Gall, Harald C.},
	month = sep,
	year = {2015},
	pages = {281--290},
}

@article{vijayakumar_automated_2017,
	title = {Automated risk identification using {NLP} in cloud based development environments},
	issn = {1868-5145},
	url = {https://doi.org/10.1007/s12652-017-0503-7},
	doi = {10.1007/s12652-017-0503-7},
	abstract = {In typical software development practice, the risk assessment is not being done in an integrated manner along with the SDLC life cycle. Mostly, risk assessment is a reactive process carried out either during the deployment process or during the evaluation of software for business. Humans are involved in the risk assessment process which is time consuming, error prone and expensive. The risks identified is also not immediately reflected upon the various people in the software development value chain. This causes the churning rate to find or alleviate risks in the future. In typical SDLC, risks may be developed while coding and it may be evident and would takes different shape as different version of the software gets updated over a period of time. Security aspects of the software solution depends on the code which needs to be consistently tracked on a continuous basis to monitor changes and its related risks, without which vulnerabilities and weakness identification will be reactive. It is always essential to identify risks based on the experience from others that is where the use of risk assessment frameworks would be handy along with vulnerability and weakness database such as common weakness enumeration, common vulnerability enumeration and Exploit DB. In this paper, NLP is implemented using deep learning techniques. This paper addresses the need for automated risk assessments with the help of NLP to auto identify the risks on the analysis of weakness and vulnerabilities.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Vijayakumar, K. and Arun, C.},
	month = may,
	year = {2017},
	keywords = {Deep learning, NLP, Risk assessment, Risk identification, Vulnerabilities, Weakness},
}

@article{garousi_nlp-assisted_2020,
	title = {{NLP}-assisted software testing: {A} systematic mapping of the literature},
	volume = {126},
	issn = {09505849},
	shorttitle = {{NLP}-assisted software testing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584920300744},
	doi = {10.1016/j.infsof.2020.106321},
	abstract = {Objective: Our objective is to summarize the state-of-the-art in NLP-assisted software testing which could benefit practitioners to potentially utilize those NLP-based techniques. Moreover, this can benefit researchers in providing an overview of the research landscape.
Method: To address the above need, we conducted a survey in the form of a systematic literature mapping (classification). After compiling an initial pool of 95 papers, we conducted a systematic voting, and our final pool included 67 technical papers.
Results: This review paper provides an overview of the contribution types presented in the papers, types of NLP approaches used to assist software testing, types of required input requirements, and a review of tool support in this area. Some key results we have detected are: (1) only four of the 38 tools (11\%) presented in the papers are available for download; (2) a larger ratio of the papers (30 of 67) provided a shallow exposure to the NLP aspects (almost no details).
Conclusion: This paper would benefit both practitioners and researchers by serving as an “index” to the body of knowledge in this area. The results could help practitioners utilizing the existing NLP-based techniques; this in turn reduces the cost of test-case design and decreases the amount of human resources spent on test activities. After sharing this review with some of our industrial collaborators, initial insights show that this review can indeed be useful and beneficial to practitioners.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Information and Software Technology},
	author = {Garousi, Vahid and Bauer, Sara and Felderer, Michael},
	month = oct,
	year = {2020},
	pages = {106321},
}

@inproceedings{ernst_natural_2017,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Natural {Language} is a {Programming} {Language}: {Applying} {Natural} {Language} {Processing} to {Software} {Development}},
	volume = {71},
	isbn = {978-3-95977-032-3},
	shorttitle = {Natural {Language} is a {Programming} {Language}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2017/7135},
	doi = {10.4230/LIPIcs.SNAPL.2017.4},
	urldate = {2023-03-21},
	booktitle = {2nd {Summit} on {Advances} in {Programming} {Languages} ({SNAPL} 2017)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Ernst, Michael D.},
	editor = {Lerner, Benjamin S. and Bodík, Rastislav and Krishnamurthi, Shriram},
	year = {2017},
	note = {ISSN: 1868-8969},
	keywords = {natural language processing, program analysis, software development},
	pages = {4:1--4:14},
}

@article{bajwa_object_nodate,
	title = {Object {Oriented} {Software} {Modeling} {Using} {NLP} {Based} {Knowledge} {Extraction}},
	abstract = {This paper presents a natural language processing based automated system for NL text to OO modeling the user requirements and generating code in multi-languages. A new rule-based model is presented for analyzing the natural languages (NL) and extracting the relative and required information from the given software requirement notes by the user. User writes the requirements in simple English in a few paragraphs and the designed system incorporates NLP methods to analyze the given script. First the NL text is semantically analyzed to extract classes, objects and their respective, attributes, methods and associations. Then UML diagrams are generated on the bases of previously extracted information. The designed system also provides with the respective code automatically of the already generated diagrams. The designed system provides a quick and reliable way to generate UML diagrams to save the time and budget of both the user and system analyst.},
	language = {en},
	author = {Bajwa, Imran Sarwar and Samad, Ali and Mumtaz, Shahzad},
}

@article{sloman_policy_1994,
	title = {Policy driven management for distributed systems},
	volume = {2},
	issn = {1064-7570, 1573-7705},
	url = {http://link.springer.com/10.1007/BF02283186},
	doi = {10.1007/BF02283186},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {Journal of Network and Systems Management},
	author = {Sloman, Morris},
	month = dec,
	year = {1994},
	pages = {333--360},
}

@article{zhao_natural_2022,
	title = {Natural {Language} {Processing} for {Requirements} {Engineering}: {A} {Systematic} {Mapping} {Study}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Natural {Language} {Processing} for {Requirements} {Engineering}},
	url = {https://dl.acm.org/doi/10.1145/3444689},
	doi = {10.1145/3444689},
	abstract = {Natural Language Processing for Requirements Engineering (NLP4RE) is an area of research and development that seeks to apply natural language processing (NLP) techniques, tools, and resources to the requirements engineering (RE) process, to support human analysts to carry out various linguistic analysis tasks on textual requirements documents, such as detecting language issues, identifying key domain concepts, and establishing requirements traceability links. This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field. Following the guidance of systematic review, the mapping study is directed by five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of tool development, and the usage of NLP technologies. Our main results are as follows: (i) we identify a total of 404 primary studies relevant to NLP4RE, which were published over the past 36 years and from 170 different venues; (ii) most of these studies (67.08\%) are solution proposals, assessed by a laboratory experiment or an example application, while only a small percentage (7\%) are assessed in industrial settings; (iii) a large proportion of the studies (42.70\%) focus on the requirements analysis phase, with quality defect detection as their central task and requirements specification as their commonly processed document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP tools) are extracted from these studies, but only 17 of them (13.08\%) are available for download; (v) 231 different NLP technologies are also identified, comprising 140 NLP techniques, 66 NLP tools, and 25 NLP resources, but most of them—particularly those novel NLP techniques and specialized tools—are used infrequently; by contrast, commonly used NLP technologies are traditional analysis techniques (e.g., POS tagging and tokenization), general-purpose tools (e.g., Stanford CoreNLP and GATE) and generic language lexicons (WordNet and British National Corpus). The mapping study not only provides a collection of the literature in NLP4RE but also, more importantly, establishes a structure to frame the existing literature through categorization, synthesis and conceptualization of the main theoretical concepts and relationships that encompass both RE and NLP aspects. Our work thus produces a
              conceptual framework
              of NLP4RE. The framework is used to identify research gaps and directions, highlight technology transfer needs, and encourage more synergies between the RE community, the NLP one, and the software and systems practitioners. Our results can be used as a starting point to frame future studies according to a well-defined terminology and can be expanded as new technologies and novel solutions emerge.},
	language = {en},
	number = {3},
	urldate = {2023-03-21},
	journal = {ACM Computing Surveys},
	author = {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca, Erol-Valeriu and Batista-Navarro, Riza T.},
	month = apr,
	year = {2022},
	pages = {1--41},
}

@book{danezis_privacy_2014,
	title = {Privacy and {Data} {Protection} by {Design} - from policy to engineering},
	url = {http://arxiv.org/abs/1501.03726},
	abstract = {Privacy and data protection constitute core values of individuals and of democratic societies. There have been decades of debate on how those values -and legal obligations- can be embedded into systems, preferably from the very beginning of the design process. One important element in this endeavour are technical mechanisms, known as privacy-enhancing technologies (PETs). Their effectiveness has been demonstrated by researchers and in pilot implementations. However, apart from a few exceptions, e.g., encryption became widely used, PETs have not become a standard and widely used component in system design. Furthermore, for unfolding their full benefit for privacy and data protection, PETs need to be rooted in a data governance strategy to be applied in practice. This report contributes to bridging the gap between the legal framework and the available technological implementation measures by providing an inventory of existing approaches, privacy design strategies, and technical building blocks of various degrees of maturity from research and development. Starting from the privacy principles of the legislation, important elements are presented as a first step towards a design process for privacy-friendly systems and services. The report sketches a method to map legal obligations to design strategies, which allow the system designer to select appropriate techniques for implementing the identified privacy requirements. Furthermore, the report reflects limitations of the approach. It concludes with recommendations on how to overcome and mitigate these limits.},
	urldate = {2023-03-20},
	author = {Danezis, George and Domingo-Ferrer, Josep and Hansen, Marit and Hoepman, Jaap-Henk and Metayer, Daniel Le and Tirtea, Rodica and Schiffner, Stefan},
	year = {2014},
	doi = {10.2824/38623},
	note = {arXiv:1501.03726 [cs]},
	keywords = {94A60, Computer Science - Cryptography and Security, D.4.6, H.2.0, K.4.1},
}

@article{camara_information_2007,
	title = {Information policies and open source software in developing countries},
	volume = {58},
	issn = {1532-2890},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20444},
	doi = {10.1002/asi.20444},
	abstract = {Many authors propose that open source software (OSS) is a good strategy to bring information and communication technologies to developing countries. Nevertheless, the use of OSS needs to be more than just adopting Linux as the standard for operating systems. Adoption of OSS is not only a choice of software, but also a means of acquiring knowledge. Developing countries have to use OSS as a way to gain knowledge about the technology itself and as a way of creating technology products that fit their specific needs. In this article, the authors introduce a model of OSS based on its essential characteristics to understand how developing countries may use OSS to achieve their development goals. The authors argue that there are two defining properties of any open source software. The first property is the potential for shared conceptualization and the second is the potential for modularity. By assessing how each OSS project satisfies these two conditions, a taxonomy is built for open source projects. This taxonomy will help the development of more sensible policies to promote the use of open source in developing countries.},
	language = {en},
	number = {1},
	urldate = {2023-03-20},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Câmara, Gilberto and Fonseca, Frederico},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20444},
	pages = {121--132},
}

@techreport{paulk_capability_1993,
	address = {Fort Belvoir, VA},
	title = {Capability {Maturity} {Model} for {Software}, {Version} 1.1:},
	shorttitle = {Capability {Maturity} {Model} for {Software}, {Version} 1.1},
	url = {http://www.dtic.mil/docs/citations/ADA263403},
	language = {en},
	urldate = {2023-03-16},
	institution = {Defense Technical Information Center},
	author = {Paulk, Mark C. and Curtis, Bill and Chrissis, Mary B. and Weber, Charles V.},
	month = feb,
	year = {1993},
	doi = {10.21236/ADA263403},
}

@article{alberts_acquisition_nodate,
	title = {Acquisition {Security} {Framework} ({ASF}): {An} {Acquisition} and {Supplier} {Perspective} on {Managing} {Software}-{Intensive} {Systems}’ {Cybersecurity} {Risk}},
	abstract = {Supply chain cyber risks stem from many organizational dependencies, including processing, transmitting, and storing data; information technology; and communications technology. These risks are broad, significant, and growing as outsourcing options expand. Important mission capabilities can be undermined by an adversary’s cyber attack on the organization’s contracted third parties, even when the organization does not explicitly contract for technology. Virtually all products and services an organization acquires are supported by or integrate with information technology that includes third-party components/services. Practices critical to monitoring and managing these risks can be scattered across the organization, resulting in inconsistencies, gaps, and slow response to disruptions. The Acquisition Security Framework (ASF) contains leading practices that support programs acquiring/building a secure, resilient software-reliant system to manage these risks. It defines the organizational roles that must effectively collaborate to engineer systematic resilience processes to avoid gaps and inconsistencies. It also establishes how an organization should ensure it has effective supply chain risk management that supports its mission and objectives. The ASF contains proven and effective goals and leading practices, and it is consistent with supply chain risk management guidelines from the International Organization for Standardization (ISO), National Institute of Standards and Technology (NIST), and Department of Homeland Security (DHS).},
	language = {en},
	author = {Alberts, Christopher and Bandor, Michael and Wallen, Charles M and Woody, Carol},
}

@article{wallen_addressing_2022,
	title = {Addressing {Supply} {Chain} {Risk} and {Resilience} for {Software} {Reliant} {Systems}},
	language = {en},
	author = {Wallen, Charles M},
	year = {2022},
}

@incollection{sethi_using_1995,
	address = {Boston, MA},
	title = {Using a {Classification} of {Management} {Policies} for {Policy} {Specification} and {Policy} {Transformation}},
	isbn = {978-1-4757-5517-6 978-0-387-34890-2},
	url = {http://link.springer.com/10.1007/978-0-387-34890-2_4},
	abstract = {Policies are derived from management goals and deﬁne the desired behavior of distributed heterogeneous systems, applications, and networks. To apply and deal with this idea, a number of concepts have been deﬁned. Numerous policy deﬁnitions, policy hierarchies and policy models have evolved which are all very different, as they were developed from diverse points of view and without a common policy classiﬁcation.},
	language = {en},
	urldate = {2023-03-14},
	booktitle = {Integrated {Network} {Management} {IV}},
	publisher = {Springer US},
	author = {Wies, René},
	editor = {Sethi, Adarshpal S. and Raynaud, Yves and Faure-Vincent, Fabienne},
	year = {1995},
	doi = {10.1007/978-0-387-34890-2_4},
	pages = {44--56},
}

@article{park_energy_2009,
	title = {Energy consumption reduction technology in manufacturing — {A} selective review of policies, standards, and research},
	volume = {10},
	issn = {1229-8557, 2005-4602},
	url = {http://link.springer.com/10.1007/s12541-009-0107-z},
	doi = {10.1007/s12541-009-0107-z},
	language = {en},
	number = {5},
	urldate = {2023-03-13},
	journal = {International Journal of Precision Engineering and Manufacturing},
	author = {Park, Cheol-Woo and Kwon, Kye-Si and Kim, Wook-Bae and Min, Byung-Kwon and Park, Sung-Jun and Sung, In-Ha and Yoon, Young Sik and Lee, Kyung-Soo and Lee, Jong-Hang and Seok, Jongwon},
	month = dec,
	year = {2009},
	pages = {151--173},
}

@article{noauthor_objects_nodate,
	title = {Objects {First} with {Java} {A} {Practical} {Introduction} {Using} {BlueJ} 6th {Edition} {Barnes} {Solutions} {Manual}},
	language = {en},
}

@article{olaya_engineering_2017,
	title = {An engineering perspective for policy design: self-organizing crime as an evolutionary social system},
	volume = {20},
	issn = {1084-4791, 1936-4830},
	shorttitle = {An engineering perspective for policy design},
	url = {http://link.springer.com/10.1007/s12117-016-9282-3},
	doi = {10.1007/s12117-016-9282-3},
	language = {en},
	number = {1-2},
	urldate = {2023-03-13},
	journal = {Trends in Organized Crime},
	author = {Olaya, Camilo and Guzmán, Laura and Gomez-Quintero, Juliana},
	month = jun,
	year = {2017},
	pages = {55--84},
}

@article{busetti_causality_2023,
	title = {Causality is good for practice: policy design and reverse engineering},
	issn = {0032-2687, 1573-0891},
	shorttitle = {Causality is good for practice},
	url = {https://link.springer.com/10.1007/s11077-023-09493-7},
	doi = {10.1007/s11077-023-09493-7},
	abstract = {Relevance to practice is an open issue for scholars in public policy and public administration. One major problem is the need to produce knowledge that can guide practitioners designing and implementing public interventions in specific contexts. This article claims that investigating the causal mechanisms of policy programs—i.e., modeling why and how they produce outcomes—can contribute to such knowledge. In this regard, mechanisms offer essential information to guide practitioners when replicating, adjusting, and designing interventions. Unfortunately, not all models of mechanisms can inform practice. The article proposes a strategy for design research and practice inspired by reverse engineering: selecting successful programs, causal modeling, assessing the target context, and designing. Scholars should model mechanisms by identifying the program and non-program elements that contribute to the outcome of interest and abstracting their causal powers. Practitioners can use these models, diagnose their target context, and adjust designs to deal with context-specific problems. The proposed research agenda may enhance orientation to practice and offer a middle ground between the search for abstract, general relationships, and single-case analyses.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Policy Sciences},
	author = {Busetti, Simone},
	month = feb,
	year = {2023},
}

@inproceedings{anthonysamy_privacy_2017,
	title = {Privacy {Requirements}: {Present} \& {Future}},
	shorttitle = {Privacy {Requirements}},
	doi = {10.1109/ICSE-SEIS.2017.3},
	abstract = {Software systems are increasingly open, handle large amounts of personal or other sensitive data and are intricately linked with the daily lives of individuals and communities. This poses a range of privacy requirements. Such privacy requirements are typically treated as instances of requirements pertaining to compliance, traceability, access control, verification or usability. Though important, such approaches assume that the scope for the privacy requirements can be established a priori and that such scope does not vary drastically once the system is deployed. User data and information, however, exists in an open, hyper-connected and potentially "unbounded" environment. Furthermore, "privacy requirements - present"and "privacy requirements - future" may differ significantly as the privacy implications are often emergent a posteriori. Effective treatment of privacy requirements, therefore, requires techniques and approaches that fit with the inherent openness and fluidity of the environment through which user data and information flows. This paper surveys state of the art and presents some potential directions in the way privacy requirements should be treated. We reflect on the limitations of existing approaches with regards to unbounded privacy requirements and highlight a set of key challenges for requirements engineering research with regards to managing privacy in such unbounded settings.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Society} {Track} ({ICSE}-{SEIS})},
	author = {Anthonysamy, Pauline and Rashid, Awais and Chitchyan, Ruzanna},
	month = may,
	year = {2017},
	keywords = {Data privacy, Law, Monitoring, Privacy, Software, Unified modeling language},
	pages = {13--22},
}

@article{banares-alcantara_perspectives_2010,
	title = {Perspectives on the potential roles of engineers in the formulation, implementation and enforcement of policies},
	volume = {34},
	issn = {00981354},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135409002592},
	doi = {10.1016/j.compchemeng.2009.10.010},
	abstract = {This article discusses and aims to motivate a potential future role of engineers as developers of methods and tools to support policy development, thus extending the reach of the engineering profession further into a normative role. Policies and policy development are ﬁrst analysed from the systems theory viewpoint and the ideas behind some proof-of-concept software systems supporting policy development are then introduced. Three speciﬁc stages from the policy cycle are addressed in particular: policy formulation, implementation and enforcement. The discussion of these issues is done in the context of the similarities and differences between two strands of human endeavour: process design and policy development.},
	language = {en},
	number = {3},
	urldate = {2023-03-13},
	journal = {Computers \& Chemical Engineering},
	author = {Bañares-Alcántara, René},
	month = mar,
	year = {2010},
	pages = {267--276},
}

@inproceedings{kaushalya_overview_2018,
	title = {An {Overview} of {Social} {Engineering} in the {Context} of {Information} {Security}},
	doi = {10.1109/ICETAS.2018.8629126},
	abstract = {Social engineering in the context of information security is the exploitation of human psychology to gain access into secure data. Human emotion can act as both a strength and a weakness. When it comes to the world booming with technology, human emotions which are completely unrelated to the matter is made to relate through social engineering. Social engineering employs `traps' to pry on human emotion and its vulnerability, taking advantage of the flaws of human psychology. Information security breaches utilising social engineering techniques are vast, so that social engineering in this context is a topic which could not be neglected. This research paper presents an overview of social engineering attacks and suggested defence mechanisms. An introduction to social engineering attacks are given, with context to the current trends and related vulnerabilities. Main reasons for the spread of social engineering attacks in the current context are also presented. Attack frameworks are presented and defence approaches are proposed at the end.},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Engineering} {Technologies} and {Applied} {Sciences} ({ICETAS})},
	author = {Kaushalya, S.A.D.T.P. and Randeniya, R. M. R. S. B. and Liyanage, A. D. S.},
	month = nov,
	year = {2018},
	keywords = {Attack Framework Model, Conferences, Defence Approaches, Information security, Organizations, Password, Personnel, Psychology, Social Engineering},
	pages = {1--6},
}

@inproceedings{otto_addressing_2007,
	title = {Addressing {Legal} {Requirements} in {Requirements} {Engineering}},
	doi = {10.1109/RE.2007.65},
	abstract = {Legal texts, such as regulations and legislation, are playing an increasingly important role in requirements engineering and system development. Monitoring systems for requirements and policy compliance has been recognized in the requirements engineering community as a key area for research. Similarly, regulatory compliance is critical in systems that are governed by regulations and law, especially given that non-compliance can result in both financial and criminal penalties. Working with legal texts can be very challenging, however, because they contain numerous ambiguities, cross-references, domain-specific definitions, and acronyms, and are frequently amended via new regulations and case law. Requirements engineers and compliance auditors must be able to identify relevant regulations, extract requirements and other key concepts, and monitor compliance throughout the software lifecycle. This paper surveys research efforts over the past 50 years in handling legal texts for systems development. These efforts include the use of symbolic logic, logic programming, first-order temporal logic, deontic logic, defeasible logic, goal modeling, and semi-structured representations. This survey can aid requirements engineers and auditors to better specify, monitor, and test software systems for compliance.},
	booktitle = {15th {IEEE} {International} {Requirements} {Engineering} {Conference} ({RE} 2007)},
	author = {Otto, Paul N. and Anton, Annie I.},
	month = oct,
	year = {2007},
	note = {ISSN: 2332-6441},
	keywords = {Computer science, Government, Information security, Law, Legal factors, Legislation, Logic programming, Monitoring, Software systems, Systems engineering and theory},
	pages = {5--14},
}

@inproceedings{notario_pripare_2015,
	title = {{PRIPARE}: {Integrating} {Privacy} {Best} {Practices} into a {Privacy} {Engineering} {Methodology}},
	shorttitle = {{PRIPARE}},
	doi = {10.1109/SPW.2015.22},
	abstract = {Data protection authorities worldwide have agreed on the value of considering privacy-by-design principles when developing privacy-friendly systems and software. However, on the technical plane, a profusion of privacy-oriented guidelines and approaches coexists, which provides partial solutions to the overall problem and aids engineers during different stages of the system development lifecycle. As a result, engineers find difficult to understand what they should do to make their systems abide by privacy by design, thus hindering the adoption of privacy engineering practices. This paper reviews existing best practices in the analysis and design stages of the system development lifecycle, introduces a systematic methodology for privacy engineering that merges and integrates them, leveraging their best features whilst addressing their weak points, and describes its alignment with current standardization efforts.},
	booktitle = {2015 {IEEE} {Security} and {Privacy} {Workshops}},
	author = {Notario, Nicolas and Crespo, Alberto and Martin, Yod-Samuel and Del Alamo, Jose M. and Metayer, Daniel Le and Antignac, Thibaud and Kung, Antonio and Kroener, Inga and Wright, David},
	month = may,
	year = {2015},
	keywords = {Computer architecture, Data privacy, Guidelines, Law, Methodology, Privacy, Privacy Engineering, Privacy Impact Assessment, Privacy by Design, Requirements Operationalization, Risk management, System Development Lifecycle, Systematics},
	pages = {151--158},
}

@inproceedings{alharthi_social_2021,
	title = {Social {Engineering} {Infosec} {Policies} ({SE}-{IPS})},
	url = {https://aircconline.com/csit/papers/vol11/csit110104.pdf},
	doi = {10.5121/csit.2021.110104},
	abstract = {The sudden increase in employees working primarily or even exclusively at home has generated unique societal and economic circumstances which makes the protection of information assets a major problem for organizations. The application of security policies is essential for mitigating the risk of social engineering attacks. However, incorporating and enforcing successful security policies in an organization is not a straightforward task. To that end, this paper develops a model of Social Engineering InfoSec Policies (SE-IPs) and investigates the incorporation of those SE-IPs in organizations. This paper proposes a customizable model of SE-IPs that can be adopted by a wide variety of organizations. The authors designed and distributed a survey to measure the incorporation level of formal SE-IPs in organizations. After collecting and analyzing the data which included over fifteen hundred responses, the authors found that on average, organizations incorporated just over fifty percent of the identified formal Social Engineering InfoSec Policies.},
	language = {en},
	urldate = {2023-03-13},
	booktitle = {Computer {Science} \& {Information} {Technology} ({CS} \& {IT})},
	publisher = {AIRCC Publishing Corporation},
	author = {Alharthi, Dalal and Regan, Amelia},
	month = jan,
	year = {2021},
	pages = {57--74},
}

@article{mcgeoch_policy_2020,
	title = {A policy analysis of the {Australian} water industry : relevance to physical asset management and organisational performance},
	copyright = {Open, © M McGeoch 2020},
	shorttitle = {A policy analysis of the {Australian} water industry},
	url = {https://researchportal.scu.edu.au/esploro/outputs/doctoral/991012893900402368},
	doi = {10.25918/THESIS.79},
	abstract = {This thesis examines the effectiveness of policy development within the Australian Water Industry and the relationship between the development of public and organisational policy in relation to asset management. This thesis analyses the synergy between public policy and organisational polices in relation to the management of public water assets. Effective policy planning is increasingly being recognised as a vital part of a public organisation’s responsibility to mitigate threats and maximise life-cycle income and cost-effectiveness of their assets. An effective water policy provides a blueprint from which a more effective management of water assets in Australia is possible.},
	language = {en},
	urldate = {2023-03-13},
	author = {McGeoch, Mary},
	collaborator = {Brunetto, Yvonne and Farr-Wharton, Rodney and Brown, Kerry A},
	year = {2020},
	note = {Medium: application/pdf
Publisher: Southern Cross University},
	keywords = {Analysis, Asset, Australia, Organisational, Performance, Policy, Water},
}

@inproceedings{mazo_framework_2016,
	title = {Framework for {Engineering} {Complex} {Security} {Requirements} {Patterns}},
	doi = {10.1109/ICITCS.2016.7740336},
	abstract = {Security management and business assets protection have been a paramount concern for many years. Due to the flood of arising innovative technologies such as cloud computing or big data, security approaches have constantly evolved toward more sophisticate solutions, aiming to tackle always more complex security issues. Nowadays, integrated frameworks are necessary to manage this complexity. Pattern-based approaches for reusing security solutions have proven its usefulness, but mostly in the frame of simple security matters. Acknowledging this, the scientific community has recently considered how these patterns could also be used to address the complexity caused by the association of multiple security criteria. Approaches based on the combination of "simple" security patterns have emerged and have resulted in the elaboration of methods for designing systems of security patterns and systems managing these collections of patterns. Nonetheless, in that domain, we have observed that researches are mostly focused on the definition of security solutions and do not address the complexity of the security requirements yet. In this paper we present a proposal for addressing this issue by means of a framework for engineering reusable security patterns for complex systems called COPERATE (COmPlex sEcurity Requirements pAtTErns). To show the feasibility of our approach, this framework is used for defining a complex security requirement and its corresponding pattern for an excerpt of a case taken from the cloud-computing domain.},
	booktitle = {2016 6th {International} {Conference} on {IT} {Convergence} and {Security} ({ICITCS})},
	author = {Mazo, Raul and Feltus, Christophe},
	month = sep,
	year = {2016},
	keywords = {Authentication, Cloud computing, Context, Encryption, Information systems, Requirements engineering},
	pages = {1--5},
}

@inproceedings{feltus_building_2009,
	title = {Building a {Responsibility} {Model} {Including} {Accountability}, {Capability} and {Commitment}},
	doi = {10.1109/ARES.2009.45},
	abstract = {This paper aims at building a responsibility model based on the concepts of accountability, capability and commitment. The model's objectives are firstly to help organizations for verifying the organizational structure and detecting policy problems and inconsistency. Secondly, the paper brings up a conceptual framework to support organization for defining their corporate, security and access control policies. Our work provides a preliminary review of the researches performed in that field and proposes, based on the analyses, an UML responsibility model and a definition of all its concepts. Thereafter, to propose a formal representation of the model, we have selected the suitable language and logic system. The analyze highlights that an important variable is whether the responsibility is perceived at a user or at a company level.},
	booktitle = {2009 {International} {Conference} on {Availability}, {Reliability} and {Security}},
	author = {Feltus, Christophe and Petit, Michaël},
	month = mar,
	year = {2009},
	keywords = {Access control, Availability, Buildings, Business, Computer science, Computer security, Logic, Performance analysis, Testing, Unified modeling language},
	pages = {412--419},
}

@inproceedings{feltus_preliminary_2008,
	title = {Preliminary {Literature} {Review} of {Policy} {Engineering} {Methods}; {Toward} {Responsibility} {Concept}},
	doi = {10.1109/ICTTA.2008.4529912},
	abstract = {This paper introduces a preliminary review of the research currently performed in the field of Policy. This review aims to understand the approaches covered by main research streams in that area and to highlight the advantages of the essential and most renowned solutions. The review of the literature quickly provides a plethora of publications that presents innovative proposals on the matter of policy conceptual model, engineering methods, elicitation languages, as well as cases studies. It also brings out that the papers most often refer rather evasively to the organizational model layers when aligning and positioning their theory with organizational concepts. Consequently, it sounds useful to orient and improve our own developments in the purpose of ameliorate that issue. Based on that overview's results, we are able to orient our researches more deeply by proposing an innovative approach that focuses in one hand on a policy model designed to take into account the responsibility of stakeholders and in the other hand on policy engineering method that takes care of business process while at the same time using requirement engineering principles. Responsibility is a notion that remains rarely addressed and that however embodies important and well-know concepts like accountability, capability and commitment. Moreover, responsibility constitutes a fundamental notion of management theory and is consequently identified as a meaningful bridge toward organizational artifacts. Exploiting process to define policy seems likewise to offer new research opportunities since process organizations become a more widely spread structured approach.},
	booktitle = {2008 3rd {International} {Conference} on {Information} and {Communication} {Technologies}: {From} {Theory} to {Applications}},
	author = {Feltus, Christophe},
	month = apr,
	year = {2008},
	keywords = {Access control, Acoustical engineering, Bridges, Business requirement, Corporate Governance, Design engineering, Ethics, IT Governance, Performance analysis, Permission, Policy concept, Proposals, Responsibility, Right management, Security, Technological innovation},
	pages = {1--6},
}

@article{allen_testing_2000,
	title = {Testing the persuasiveness of evidence: {Combining} narrative and statistical forms},
	volume = {17},
	issn = {0882-4096},
	shorttitle = {Testing the persuasiveness of evidence},
	url = {https://doi.org/10.1080/08824090009388781},
	doi = {10.1080/08824090009388781},
	abstract = {This study provides an experimental test for the conclusions of the Allen and Preiss (1997) meta‐analysis that statistical evidence is more persuasive than narrative evidence. This investigation extends that finding to consider the case where a message combines statistical and narrative evidence to determine if a combination of evidence is more effective than a single form of support. This investigation using 15 messages and 1,270 participants finds that a message combining narrative and statistical evidence is more persuasive than a message using either narrative or statistical evidence alone.},
	number = {4},
	urldate = {2023-03-15},
	journal = {Communication Research Reports},
	author = {Allen, Mike and Bruflat, Rebecca and Fucilla, Renée and Kramer, Michael and McKellips, Steve and Ryan, Daniel J. and Spiegelhoff, Marieke},
	month = sep,
	year = {2000},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/08824090009388781},
	pages = {331--336},
}

@misc{noauthor_small_nodate,
	title = {Small {World} with {High} {Risks}},
}

@book{hsu_random_2016,
	address = {Washington, DC, US},
	series = {Methodological issues and strategies in clinical research, 4th ed},
	title = {Random sampling, randomization, and equivalence of contrasted groups in psychotherapy outcome research},
	isbn = {978-1-4338-2091-5 978-1-4338-2140-0 978-1-4338-2092-2},
	abstract = {This reprinted article originally appeared in Journal of Consulting and Clinical Psychology, 1989 (Feb), Vol 57(1), 131-137. (The following abstract of the original article appeared in record 1989-26784-001.) Random sampling and random assignment (randomization) are some of the most popular methods of equating contrasted groups on pre-existing nuisance variables. However, the small samples typically used in psychotherapy outcome studies raise some questions about the extent to which these methods eliminate the pretreatment nonequivalence of groups in this area of research. This article identifies conditions under which equivalence is likely (and unlikely) to be attained with simple random sampling and randomization in psychotherapy efficacy studies of the kind examined in recent meta-analyses. Some consequences of nonequivalence are viewed as manifestations of Simpson's paradox. Misinterpretations of estimates of the relative efficacy of treatments are expected in view of belief in the law of small numbers. The minimum sample sizes needed to protect against nonequivalence are compared with those needed to satisfy several other criteria. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	publisher = {American Psychological Association},
	author = {Hsu, Louis M.},
	year = {2016},
	doi = {10.1037/14805-010},
	note = {Pages: 152},
	keywords = {Experimental Methods, Experimentation, Psychotherapeutic Outcomes, Psychotherapy, Random Sampling},
}

@misc{noauthor_iot_nodate,
	title = {{IoT} connected devices worldwide 2019-2030},
	url = {https://www.statista.com/statistics/1183457/iot-connected-devices-worldwide/},
	abstract = {The number of Internet of Things (IoT) devices worldwide is forecast to almost triple from 9.7 billion in 2020 to more than 29 billion IoT devices in 2030.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Statista},
}

@book{noauthor_designing_2015,
	title = {Designing for the {Internet} of {Things}},
	isbn = {978-1-4919-2521-8},
	publisher = {O'Reilly Media, Inc.},
	month = feb,
	year = {2015},
	keywords = {DA-SERP4IoT23},
}

@article{oriwoh_guidelines_2013,
	series = {The 4th {International} {Conference} on {Emerging} {Ubiquitous} {Systems} and {Pervasive} {Networks} ({EUSPN}-2013) and the 3rd {International} {Conference} on {Current} and {Future} {Trends} of {Information} and {Communication} {Technologies} in {Healthcare} ({ICTH})},
	title = {Guidelines for {Internet} of {Things} {Deployment} {Approaches} – {The} {Thing} {Commandments}},
	volume = {21},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050913008119},
	doi = {10.1016/j.procs.2013.09.018},
	abstract = {The Internet of Things (IoT) describes the interconnection of objects (or Things) for various purposes including identification, communication, sensing, and data collection. “Things” in this context range from traditional computing devices like Personal Computers (PC) to general household objects embedded with capabilities for sensing and/or communication through the use of technologies such as Radio Frequency Identification (RFID). This conceptual paper, from a philosophical viewpoint, introduces an initial set of guiding principles - also referred to in the paper as commandments - that can be applied by all the stakeholders involved in the IoT during its introduction, deployment and thereafter.},
	language = {en},
	urldate = {2023-03-09},
	journal = {Procedia Computer Science},
	author = {Oriwoh, Edewede and Sant, Paul and Epiphaniou, Gregory},
	month = jan,
	year = {2013},
	keywords = {Commandments, DA-SERP4IoT23, Internet of Things, Principles, Security, Users},
	pages = {122--131},
}

@article{AMZN-20XX,
	title = {The ultimate {IoT} security best practices guide},
	url = {https://pages.awscloud.com/rs/112-TZM-766/images/IoT_Security_Best_Practices_Guide_design_v3.1.pdf},
	author = {{Amazon}},
	keywords = {DA-SERP4IoT23},
}

@article{ENISA-2017,
	title = {Baseline security recommendations for {IoT} in the context of critical information infrastructures},
	url = {https://www.enisa.europa.eu/publications/baseline-security-recommendations-for-iot/@@download/fullReport},
	author = {{European Union Agency for Network and Information Security}},
	year = {2017},
	keywords = {DA-SERP4IoT23},
}

@misc{anandayuvaraj2022FailureAwareSDLC,
	title = {Towards a failure-aware {SDLC} for internet of things},
	url = {https://arxiv.org/abs/2206.13562},
	author = {Anandayuvaraj, Dharun and Thulluri, Pujita and Figueroa, Jutin and Shandilya, Harshit and Davis, James C},
	year = {2022},
}

@article{huang_what_2022,
	chapter = {Technology},
	title = {What {Is} {Mastodon} and {Why} {Are} {People} {Leaving} {Twitter} for {It}?},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2022/11/07/technology/mastodon-twitter-elon-musk.html},
	abstract = {Since Elon Musk took ownership of Twitter, some of its users have migrated to Mastodon, an alternative social platform.},
	language = {en-US},
	urldate = {2023-03-10},
	journal = {The New York Times},
	author = {Huang, Kalley},
	month = nov,
	year = {2022},
	keywords = {Mobile Applications, Musk, Elon, Open-Source Software, Rumors and Misinformation, Social Media, Twitter},
}

@incollection{mullaney2021your,
	title = {Your {Computer} {Is} on {Fire}},
	booktitle = {Your {Computer} {Is} on {Fire}},
	publisher = {MIT Press},
	author = {Mullaney, Thomas S and Peters, Benjamin and Hicks, Mar and Philip, Kavita},
	year = {2021},
	pages = {20--21},
}

@article{borges_whats_2018,
	title = {What’s in a {GitHub} {Star}? {Understanding} {Repository} {Starring} {Practices} in a {Social} {Coding} {Platform}},
	volume = {146},
	issn = {0164-1212},
	shorttitle = {What’s in a {GitHub} {Star}?},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218301961},
	doi = {10.1016/j.jss.2018.09.016},
	abstract = {Besides a git-based version control system, GitHub integrates several social coding features. Particularly, GitHub users can star a repository, presumably to manifest interest or satisfaction with an open source project. However, the real and practical meaning of starring a project was never the subject of an in-depth and well-founded empirical investigation. Therefore, we provide in this paper a throughout study on the meaning, characteristics, and dynamic growth of GitHub stars. First, by surveying 791 developers, we report that three out of four developers consider the number of stars before using or contributing to a GitHub project. Then, we report a quantitative analysis on the characteristics of the top-5,000 most starred GitHub repositories. We propose four patterns to describe stars growth, which are derived after clustering the time series representing the number of stars of the studied repositories; we also reveal the perception of 115 developers about these growth patterns. To conclude, we provide a list of recommendations to open source project managers (e.g., on the importance of social media promotion) and to GitHub users and Software Engineering researchers (e.g., on the risks faced when selecting projects by GitHub stars).},
	language = {en},
	urldate = {2023-03-09},
	journal = {Journal of Systems and Software},
	author = {Borges, Hudson and Tulio Valente, Marco},
	month = dec,
	year = {2018},
	keywords = {GitHub stars, Social coding, Software popularity},
	pages = {112--129},
}

@article{mosier_application_1986,
	title = {Application of guidelines for designing user interface software},
	volume = {5},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/01449298608914497},
	doi = {10.1080/01449298608914497},
	abstract = {A survey was conducted of people who had received a report on guidelines for designing user interface software. Analysis of questionnaire responses indicates that respondents considered guidelines useful, that they have used guidelines in various stages of design, and that they plan to use guidelines again. However, respondents also reported significant problems in the practical application of guidelines. Respondents had difficulty locating relevant guidelines within the report, choosing which guidelines would actually be used, establishing priorities among the selected guidelines, and translating generally worded guidelines into specific design rules.},
	number = {1},
	urldate = {2023-03-09},
	journal = {Behaviour \& Information Technology},
	author = {MOSIER, JANE N. and SMITH, SIDNEY L.},
	month = jan,
	year = {1986},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01449298608914497},
	pages = {39--46},
}

@article{philip_software_1998,
	title = {Software design guidelines for event-driven programming},
	volume = {41},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121297100097},
	doi = {10.1016/S0164-1212(97)10009-7},
	abstract = {This paper deals with software design principles and guidelines to improve the reusability and maintainability of event-driven (E-D) programs. The paper examines how well the principles of structured software design from the procedural programming field can be applied to the event-driven environment. Taking into account the unique characteristics of event-driven programming (E-DP), additional guidelines that are specific to this field are proposed. The guidelines presented here deal with modularizing event procedures, graphical representation of E-D programs, sharing data between event/general procedures, using user-defined objects, and developing cohesive procedures and forms.},
	language = {en},
	number = {2},
	urldate = {2023-03-09},
	journal = {Journal of Systems and Software},
	author = {Philip, George C},
	month = may,
	year = {1998},
	keywords = {Event-driven programming, Software design, Software engineering, Software maintenance},
	pages = {79--91},
}

@inproceedings{beautement_compliance_2008,
	address = {Lake Tahoe California USA},
	title = {The compliance budget: managing security behaviour in organisations},
	isbn = {978-1-60558-341-9},
	shorttitle = {The compliance budget},
	url = {https://dl.acm.org/doi/10.1145/1595676.1595684},
	doi = {10.1145/1595676.1595684},
	abstract = {A significant number of security breaches result from employees’ failure to comply with security policies. Many organizations have tried to change or influence security behaviour, but found it a major challenge. Drawing on previous research on usable security and economics of security, we propose a new approach to managing employee security behaviour. We conducted interviews with 17 employees from two major commercial organizations, asking why they do or don’t comply with security policies. Our results show that key factors in the compliance decision are the actual and anticipated cost and benefits of compliance to the individual employee, and perceived cost and benefits to the organization. We present a new paradigm – the Compliance Budget - as a means of understanding how individuals perceive the costs and benefits of compliance with organisational security goals, and identify a range of approaches that security managers can use to influence employee’s perceptions (which, in turn, influence security behaviour). The Compliance Budget should be understood and managed in the same way as any financial budget, as compliance directly affects, and can place a cap on, effectiveness of organisational security measures.},
	language = {en},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 2008 {New} {Security} {Paradigms} {Workshop}},
	publisher = {ACM},
	author = {Beautement, Adam and Sasse, M. Angela and Wonham, Mike},
	month = sep,
	year = {2008},
	pages = {47--58},
}

@inproceedings{amusuo_reflections_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Reflections on software failure analysis},
	isbn = {978-1-4503-9413-0},
	url = {https://doi.org/10.1145/3540250.3560879},
	doi = {10.1145/3540250.3560879},
	abstract = {Failure studies are important in revealing the root causes, behaviors, and life cycle of defects in software systems. These studies either focus on understanding the characteristics of defects in specific classes of systems, or the characteristics of a specific type of defect in the systems it manifests in. Failure studies have influenced various software engineering research directions, especially in the area of software evolution, defect detection, and program repair. In this paper, we reflect on the conduct of failure studies in software engineering. We reviewed a sample of 52 failure study papers. We identified several recurring problems in these studies, some of which hinder the ability of software engineering community to trust or replicate the results. Based on our findings, we suggest future research directions, including identifying and analyzing failure causal chains, standardizing the conduct of failure studies, and tool support for faster defect analysis.},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Amusuo, Paschal C. and Sharma, Aishwarya and Rao, Siddharth R. and Vincent, Abbey and Davis, James C.},
	month = nov,
	year = {2022},
	keywords = {Failure analysis, empirical software engineering, software defects},
	pages = {1615--1620},
}

@misc{noauthor_address_nodate,
	title = {address prof davis feedback v2 on {Social} {Media} {Platforms} {\textbar} {Trello}},
	url = {https://trello.com/c/v1omw5RU/131-address-prof-davis-feedback-v2},
	urldate = {2023-03-09},
}

@article{valett_summary_1989,
	title = {A summary of software measurement experiences in the software engineering laboratory},
	volume = {9},
	number = {2},
	journal = {Journal of Systems and Software},
	author = {Valett, Jon D and McGarry, Frank E},
	year = {1989},
	note = {Citation Keys: Valett1989SWMeasurementExperiencesinSELab},
	pages = {137--148},
}

@misc{twitter_safety_nodate,
	title = {Safety and security},
	url = {https://web.archive.org/web/20220308081252/https://help.twitter.com/en/safety-and-security},
	language = {en},
	urldate = {2023-03-08},
	author = {{Twitter}},
}

@misc{snapchat_snapchat_nodate,
	title = {Snapchat {Transparency} {Report} {\textbar} {Snapchat} {Transparency}},
	url = {https://web.archive.org/web/20220308081253/https://values.snap.com/privacy/transparency},
	abstract = {Twice a year, we publish transparency reports to provide insight and visibility into the nature and volume of content and accounts reported to us.},
	language = {en},
	urldate = {2023-03-08},
	author = {{Snapchat}},
}

@misc{meta_transparency_nodate,
	title = {Transparency reports {\textbar} {Transparency} {Center}},
	url = {https://web.archive.org/web/20220308081251/https://transparency.fb.com/data/},
	abstract = {Facebook publishes regular reports into how we enforce our policies.},
	language = {en},
	urldate = {2023-03-08},
	author = {{Meta}},
}

@misc{meta_staying_nodate,
	title = {Staying {Safe}},
	url = {https://web.archive.org/web/20220308081255/https://www.facebook.com/help/592679377575472/?helpref=uf_share},
	urldate = {2023-03-08},
	author = {{Meta}},
}

@misc{esafety_commissioner_safety_nodate,
	title = {Safety by {Design}},
	url = {https://web.archive.org/web/20220308081249/https://www.esafety.gov.au/industry/safety-by-design},
	abstract = {Safety by Design is about making sure that the foundations and scaffolds are in place to build safe and positive online environments.},
	language = {en-AU},
	urldate = {2023-03-08},
	journal = {eSafety Commissioner},
	author = {{eSafety Commissioner}},
}

@misc{discord_transparency_nodate,
	title = {Transparency {Reports}},
	url = {https://web.archive.org/web/20220308081258/https://discord.com/tags/transparency-reports},
	urldate = {2023-03-08},
	author = {{Discord}},
}

@misc{tiktok_safety_nodate,
	title = {Safety {Center}},
	url = {https://www.tiktok.com/safety/en/},
	abstract = {We're committed to making TikTok a safe place for creativity and expression. TikTok's Safety Center offers valuable tools and resources to encourage a positive and safe environment for our community.},
	language = {en},
	urldate = {2023-03-08},
	journal = {TikTok},
	author = {{TikTok}},
}

@misc{noauthor_principles_nodate,
	title = {Principles and background},
	url = {https://www.esafety.gov.au/industry/safety-by-design/principles-and-background},
	abstract = {The Safety by Design principles provide industry with realistic, actionable and achievable measures to better protect and safeguard users from online harms.},
	language = {en-AU},
	urldate = {2023-03-08},
	journal = {eSafety Commissioner},
}

@article{kostovaPrivacyEngineeringMeets2020,
	title = {Privacy {Engineering} {Meets} {Software} {Engineering}. {On} the {Challenges} of {Engineering} {Privacy} {By} {Design}},
	url = {http://arxiv.org/abs/2007.08613},
	abstract = {Current day software development relies heavily on the use of service architectures and on agile iterative development methods to design, implement, and deploy systems. These practices result in systems made up of multiple services that introduce new data ﬂows and evolving designs that escape the control of a single designer. Academic privacy engineering literature typically abstracts away such conditions of software production in order to achieve generalizable results. Yet, through a systematic study of the literature, we show that proposed solutions inevitably make assumptions about software architectures, development methods and scope of designer control that are misaligned with current practices. These misalignments are likely to pose an obstacle to operationalizing privacy engineering solutions in the wild. Speciﬁcally, we identify important limitations in the approaches that researchers take to design and evaluate privacy enhancing technologies which ripple to proposals for privacy engineering methodologies. Based on our analysis, we delineate research and actions needed to re-align research with practice, changes that serve a precondition for the operationalization of academic privacy results in common software engineering practices.},
	language = {en},
	urldate = {2021-03-24},
	journal = {arXiv:2007.08613 [cs]},
	author = {Kostova, Blagovesta and Gürses, Seda and Troncoso, Carmela},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08613},
	keywords = {Computer Science - Software Engineering},
}

@article{myers_combating_2023,
	chapter = {Technology},
	title = {Combating {Disinformation} {Wanes} at {Social} {Media} {Giants}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/02/14/technology/disinformation-moderation-social-media.html},
	abstract = {As the companies have shed jobs recently, many teams assigned to combat false and misleading information have taken a hit.},
	language = {en-US},
	urldate = {2023-02-21},
	journal = {The New York Times},
	author = {Myers, Steven Lee and Grant, Nico},
	month = feb,
	year = {2023},
	keywords = {Alphabet Inc, Anti-Semitism, Censorship, Clegg, Nick, Computers and the Internet, Conspiracy Theories, Coronavirus (2019-nCoV), Facebook Inc, Google Inc, Hate Speech, Instagram Inc, Layoffs and Job Reductions, Meta Platforms Inc, Musk, Elon, Rumors and Misinformation, Social Media, Twitter, YouTube.com},
}

@book{salvendy_handbook_2012,
	address = {Hoboken, NJ},
	edition = {4},
	title = {Handbook of human factors and ergonomics},
	isbn = {978-0-470-52838-9},
	language = {eng},
	publisher = {Wiley},
	editor = {Salvendy, Gavriel},
	year = {2012},
}

@misc{hancock_journal_2022,
	title = {Journal of {Online} {Trust} and {Safety}},
	url = {https://tsjournal.org/index.php/jots},
	language = {en-US},
	urldate = {2022-06-07},
	author = {Hancock, Jeff},
	year = {2022},
	keywords = {open access, scholarly publishing, trust and safety},
}

@article{ralph2020empirical,
	title = {Empirical standards for software engineering research},
	journal = {arXiv preprint arXiv:2010.03525},
	author = {Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bianculli, Domenico and Diaz, Jessica and Dittrich, Yvonne and Ernst, Neil and Felderer, Michael and Feldt, Robert and Filieri, Antonio and {others}},
	year = {2020},
}

@misc{ralph_empirical_nodate,
	title = {Empirical {Standards}},
	url = {https://acmsigsoft.github.io/EmpiricalStandards/docs/},
	abstract = {Empirical standards for conducting and evaluating research in software engineering},
	language = {en},
	urldate = {2023-01-12},
	journal = {ACM SIGSOFT},
	author = {Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bermbach, David and others},
}

@article{hasib_threats_2009,
	title = {Threats of online social networks},
	volume = {9},
	abstract = {In the recent years, we have witnessed a dramatic rise in popularity of online social networking services, with several Social Network Sites (SNSs) such as Myspace, Facebook, Blogger, You Tube, Yahoo! Groups etc are now among the most visited websites globally. However, since such forums are relatively easy to access and the users are often not aware of the size and the nature of the audience accessing their profiles, they often reveal more information which is not appropriate to a public forum. As a result, such commercial and social site may often generate a number of privacy and security related threats for the members. This paper highlights the commercial and social benefits of safe and well-informed use of SNSs and emphasizes the most important threats to users of SNSs as well as illustrates the fundamental factors behind these threats. It also presents policy and technical recommendations to improve privacy and security without compromising the benefits of information sharing through SNSs. Key words:},
	number = {11},
	journal = {International Journal of Computer Science and Network Security},
	author = {Hasib, Abdullah Al},
	year = {2009},
	pages = {288--293},
}

@article{arumugam_detection_2023,
	title = {Detection and {Verification} of {Cloned} {Profiles} in {Online} {Social} {Networks} {Using} {MapReduce} {Based} {Clustering} and {Classification}},
	volume = {11},
	copyright = {Copyright (c) 2023},
	issn = {2147-6799},
	url = {https://www.ijisae.org/index.php/IJISAE/article/view/2459},
	abstract = {The use of online social networks has become an unavoidable part of today's humans’ life. The augmented usage of these online social networks has also increased the misusage through spreading spam messages, false reviews and fake news. Fake accounts are one such method in which attackers clone the profiles of innocent victims to harm them and damage their reputations. In some situations, fake accounts are also created to steal money from other users. In recent days, the detection of such cloned or fake profiles has perceived a wide range of attention from researchers. However, the existing methods lack accuracy and precision in detecting cloned profiles. In this paper, an effort has been made to identify the cloned profiles using clustering and classification process. The method collects the fake and possible cloned profiles and computes the additional relationship attributes for performing cloned profile detection. The parallel k-means clustering is carried out to group the suspicious profiles that are similar to the real ones. Then the parallel SVM classification is applied to the cluster in which the real profile is grouped. Finally, the classification results are verified using the attribute and network similarity measure. The various stages of the proposed model are implemented using the MapReduce framework which is more suitable for big data. The experimental analysis and results indicate that the proposed model has better performance than other competitors with an accuracy and precision of 98.19\% and 98.96\% for the MIB twitter dataset and 98.90\% and 99.17\% for the synthetic dataset created for the study.},
	language = {en},
	number = {1},
	urldate = {2023-01-25},
	journal = {International Journal of Intelligent Systems and Applications in Engineering},
	author = {Arumugam, Saravanan and Venugopal, Vineetha},
	month = jan,
	year = {2023},
	note = {Number: 1},
	keywords = {mapreduce},
	pages = {195--207},
}

@misc{ebay_about_1999,
	title = {About {eBay}: {Press} {Releases}},
	shorttitle = {Press {Releases}},
	url = {https://web.archive.org/web/20000815064513/http://pages.ebay.com/community/aboutebay/releases/9901.html},
	urldate = {2023-03-06},
	journal = {eBay},
	author = {{eBay}},
	month = jan,
	year = {1999},
}

@article{pinkney_putting_2002,
	title = {Putting {Blame} {Where} {Blame} {Is} {Due}: {Software} {Manufacturer} and {Customer} {Liability} for {Security}-{Related} {Software} {Failure}},
	volume = {13},
	shorttitle = {Putting {Blame} {Where} {Blame} {Is} {Due}},
	url = {https://heinonline.org/HOL/P?h=hein.journals/albnyst13&i=51},
	language = {eng},
	number = {1},
	urldate = {2023-03-03},
	journal = {Albany Law Journal of Science \& Technology},
	author = {Pinkney, Kevin R.},
	year = {2002},
	pages = {43--82},
}

@article{lin_survey_2022,
	title = {A survey of transformers},
	volume = {3},
	issn = {2666-6510},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651022000146},
	doi = {10.1016/j.aiopen.2022.10.001},
	abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
	language = {en},
	urldate = {2023-03-03},
	journal = {AI Open},
	author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
	month = jan,
	year = {2022},
	keywords = {Deep learning, Pre-trained models, Self-attention, Transformer},
	pages = {111--132},
}

@article{brants_large_nodate,
	title = {Large {Language} {Models} in {Machine} {Translation}},
	abstract = {This paper reports on the beneﬁts of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.},
	language = {en},
	author = {Brants, Thorsten and Popat, Ashok C and Xu, Peng and Och, Franz J and Dean, Jeffrey},
}

@article{kowsari_text_2019,
	title = {Text {Classification} {Algorithms}: {A} {Survey}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Text {Classification} {Algorithms}},
	url = {https://www.mdpi.com/2078-2489/10/4/150},
	doi = {10.3390/info10040150},
	abstract = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
	language = {en},
	number = {4},
	urldate = {2023-03-03},
	journal = {Information},
	author = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
	month = apr,
	year = {2019},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {document classification, text analysis, text categorization, text classification, text mining, text representation},
	pages = {150},
}

@article{nuzzo_text_2010,
	title = {Text {Mining} approaches for {Automated} {Literature} {Knowledge} {Extraction} and {Representation}},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-60750-588-4-954},
	doi = {10.3233/978-1-60750-588-4-954},
	urldate = {2023-03-02},
	journal = {MEDINFO 2010},
	author = {Nuzzo, Angelo and Mulas, Francesca and Gabetta, Matteo and Arbustini, Eloisa and Zupan, Blaž and Larizza, Cristiana and Bellazzi, Riccardo},
	year = {2010},
	note = {Publisher: IOS Press},
	pages = {954--958},
}

@inproceedings{wallace_lessons_1999,
	title = {Lessons from 342 medical device failures},
	doi = {10.1109/HASE.1999.809487},
	abstract = {Most complex systems today contain software, and systems failures activated by software faults can provide lessons for software development practices and software quality assurance. This paper presents an analysis of software-related failures of medical devices that caused no death or injury but led to recalls by the manufacturers. The analysis categorizes the failures by their symptoms and faults, and discusses methods of preventing and detecting faults in each category. The nature of the faults provides lessons about the value of generally accepted quality practices for prevention and detection methods applied prior to system release. It also provides some insight into the need for formal requirements specification and for improved testing of complex hardware-software systems.},
	booktitle = {Proceedings 4th {IEEE} {International} {Symposium} on {High}-{Assurance} {Systems} {Engineering}},
	author = {Wallace, D.R. and Kuhn, D.R.},
	month = nov,
	year = {1999},
	keywords = {Animals, Computer aided manufacturing, Costs, Design engineering, Diseases, Fault detection, Information technology, Medical diagnostic imaging, Medical treatment, Space technology},
	pages = {123--131},
}

@article{zollers_no_2005,
	title = {No {More} {Soft} {Landings} for {Software}: {Liability} for {Defects} in an {Industry} {That} {Has} {Come} of {Age}},
	volume = {21},
	copyright = {Copyright Santa Clara University May 2005},
	issn = {08823383},
	shorttitle = {No {More} {Soft} {Landings} for {Software}},
	url = {https://www.proquest.com/docview/218726761/abstract/15A8A7F1AE9A4FF8PQ/1},
	abstract = {The power of software can be seen everywhere: It flies airplanes, monitors medical patients and nuclear power plants, and even helps us drive our cars. Indeed, software is no longer confined to the domain of business systems that control inventory, issue payroll checks, and keep track of accounts receivable and payable. It extends beyond the desktop computer with its word processing and data management capabilities and now routinely interfaces with human beings in their daily lives and in unseen ways. The consequence, however, is that some software can cause physical injury if it is defective. While many early commentators have speculated about the liability regime when such a condition occurs, it is now time to take stock of how the law is developing and should develop when software foreseeably causes physical injury. This article will first examine the characteristics of software and its evolutionary creep into our lives in Part II. In Parts III and IV, we will review the literature about software litigation and the eras through which it has progressed. Part V reviews the origins of strict product liability and the policies underlying it to determine whether software, which has hitherto enjoyed immunity from strict liability, fits into the strict liability context. Lastly, in Part VI we argue for the adoption of a strict liability regime for software failure that produces physical injury and offer supporting arguments for why such a move is both necessary and sensible.},
	language = {English},
	number = {4},
	urldate = {2023-03-02},
	journal = {Santa Clara Computer and High - Technology Law Journal},
	author = {Zollers, Frances E. and McMullin, Andrew and Hurd, Sandra N. and Shears, Peter},
	month = may,
	year = {2005},
	note = {Num Pages: 38
Place: Santa Clara, United States
Publisher: Santa Clara University},
	keywords = {Computers, Failure, Fault tolerance, Law, Law--Computer Applications, Liability, Litigation, Microprocessors, Nuclear power plants, Semantics, Software, Software industry, Studies, Technology: Comprehensive Works, Transistors},
	pages = {745--782},
}

@article{wong_role_2009,
	title = {The {Role} of {Software} in {Recent} {Catastrophic} {Accidents}},
	language = {en},
	author = {Wong, W Eric and Debroy, Vidroha and Restrepo, Andrew},
	year = {2009},
}

@phdthesis{howard_analysis_1997,
	address = {United States -- Pennsylvania},
	type = {Ph.{D}.},
	title = {An analysis of security incidents on the {Internet} 1989-1995},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/304361939/abstract/63A4197A2B54F60PQ/1},
	abstract = {This research analyzed trends in Internet security through an investigation of 4,299 security-related incidents on the Internet reported to the CERT\${\textbackslash}sp{\textbackslash}circler\$ Coordination Center (CERT\${\textbackslash}sp{\textbackslash}circler\$/CC) from 1989 to 1995. Prior to this research, our knowledge of security problems on the Internet was limited and primarily anecdotal. This information could not be effectively used to determine what government policies and programs should be, or to determine the effectiveness of current policies and programs. This research accomplished the following: (1) development of a taxonomy for the classification of Internet attacks and incidents, (2) organization, classification, and analysis of incident records available at the CERT\${\textbackslash}sp{\textbackslash}circler\$/CC, and (3) development of recommendations to improve Internet security, and to gather and distribute information about Internet security.
With the exception of denial-of-service attacks, security incidents were generally found to be decreasing relative to the size of the Internet. The probability of any severe incident not being reported to the CERT\${\textbackslash}sp{\textbackslash}circler\$/CC was estimated to be between 0\% and 4\%. The probability that an incident would be reported if it was above average in terms of duration and number of sites, was around 1 out of 2.6. Estimates based on this research indicated that a typical Internet domain was involved in no more than around one incident per year, and a typical Internet host in around one incident every 45 years.
The taxonomy of computer and network attacks developed for this research was used to present a summary of the relative frequency of various methods of operation and corrective actions. This was followed by an analysis of three subgroups: (1) a case study of one site that reported all incidents, (2) 22 incidents that were identified, by various measures as being the most severe in the records, and (3) denial-of-service incidents. Data from all incidents and these three subgroups were used to estimate the total Internet incident activity during the period of the research. This was followed by a critical evaluation of the utility of the taxonomy developed for this research. The analysis concludes with recommendations for Internet users, Internet suppliers, response teams, and the U.S. government.},
	language = {English},
	urldate = {2023-03-02},
	school = {Carnegie Mellon University},
	author = {Howard, John Douglas},
	year = {1997},
	note = {ISBN: 9780591522594
Publication Title: ProQuest Dissertations and Theses},
	keywords = {Case studies, Classification, Computer crime, Computer science, Computer security, Copyright, Data encryption, Denial of service attacks, Disclosure, Estimates, False alarms, Information science, Information systems, Information warfare, Internet, Political science, Public policy, Software, Studies, Taxonomy, Trends, World Wide Web},
}

@article{rahman_identification_2009,
	title = {Identification of sources of failures and their propagation in critical infrastructures from 12 years of public failure reports},
	volume = {5},
	issn = {1475-3219},
	doi = {10.1504/IJCIS.2009.024872},
	abstract = {Understanding the origin of infrastructure failures and their propagation patterns in critical infrastructures can provide important information for secure and reliable infrastructure design. Among the critical infrastructures, the Communication and Information Technology Infrastructure (CITI) is crucial, as it provides the basic mechanism for sharing information among all infrastructures. Failures in CITI can disrupt the effective functionality of the other critical infrastructures. Conversely, failures in the other infrastructures can also propagate to CITI, and hence disrupt the operation of all systems. In this study, we used public domain failure reports to identify the origin of these failures and their propagation patterns. We analysed 347 infrastructure failure cases reported from 1994 to 2005 in the Association for Computing Machinery's (ACM) RISKS forum. We studied these reports to determine the causes of infrastructure failures and their impact on CITI and other critical infrastructures in a number of dimensions, such as the origin of failures, impacts of failures in spatial and temporal dimensions, their effect on public safety and how failures propagate from one infrastructure to another. The results obtained from the analysis of these real-life failure cases, which occurred over a considerable timespan, should be useful to researchers and practitioners. This paper also discusses the difficulties and limitations of using public domain data in academic research.},
	language = {eng},
	number = {3},
	journal = {International journal of critical infrastructures},
	author = {Rahman, Hafiz Abdur and Beznosov, Konstantin and Marti, Jose R.},
	year = {2009},
	note = {Place: Amsterdam
Publisher: Inderscience Publishers},
	keywords = {Applied sciences, Computer science; control theory; systems, Computer systems and distributed systems. User interface, ENVIRONMENTAL JOURNALS, Exact sciences and technology, Forecasts and trends, Information systems. Data bases, Information technology, Infrastructures, Memory organisation. Data processing, Public software, RISK, SAFETY AND EMERGENCY JOURNALS, Risk, Reliability and Safety, Security and Emergency Management, Software},
	pages = {220--244},
}

@misc{noauthor_investigations_nodate,
	title = {Investigations {\textbar} {CSB}},
	url = {https://www.csb.gov/investigations/},
	urldate = {2023-03-02},
}

@misc{noauthor_rail_2017,
	title = {Rail {Accident} {Investigation} {Branch} reports},
	url = {https://www.gov.uk/raib-reports},
	abstract = {Find reports of RAIB investigations into rail accidents and incidents},
	language = {en},
	urldate = {2023-03-02},
	journal = {GOV.UK},
	month = oct,
	year = {2017},
}

@misc{noauthor_maude_nodate,
	title = {{MAUDE} - {Manufacturer} and {User} {Facility} {Device} {Experience}},
	url = {https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfMAUDE/Search.cfm},
	urldate = {2023-03-02},
}

@misc{noauthor_asrs_nodate,
	title = {{ASRS} - {Aviation} {Safety} {Reporting} {System}},
	url = {https://asrs.arc.nasa.gov/},
	urldate = {2023-03-02},
}

@misc{commissioner_medwatch_2022,
	title = {{MedWatch}: {The} {FDA} {Safety} {Information} and {Adverse} {Event} {Reporting} {Program}},
	shorttitle = {{MedWatch}},
	url = {https://www.fda.gov/safety/medwatch-fda-safety-information-and-adverse-event-reporting-program},
	abstract = {Reporting on prescription/OTC medicines, non-vaccine biologicals, medical devices, special nutritional products, cosmetics and non-prescription human drug.},
	language = {en},
	urldate = {2023-03-02},
	journal = {FDA},
	author = {Commissioner, Office of the},
	month = sep,
	year = {2022},
	note = {Publisher: FDA},
}

@incollection{eloff_software_2018,
	address = {Cham},
	title = {Software {Failures}: {An} {Overview}},
	isbn = {978-3-319-61334-5},
	shorttitle = {Software {Failures}},
	url = {https://doi.org/10.1007/978-3-319-61334-5_2},
	abstract = {Since major software failures often result in disasters ranging from financial loss to loss of lives, preventing their recurrence is absolutely necessary. A post-mortem investigation is required to identify their root cause and implement appropriate countermeasures. Current approaches to software failure investigations are limited and often result in returning the software system back to normal execution as quickly as possible. In the process of doing so, the software system is left vulnerable to a reoccurrence of the same type of software failures. This chapter defines the concept of a software failure and then reviews the problems of major software failures. The aim is to determine how to improve the accuracy of their root-cause analysis in order to prevent the reoccurrence of major accidents. A review of recent cases of major software failures from different industries, such as the medical domain, is given to demonstrate the reality and seriousness of software failures. These software failures are then analysed so as to identify limitations and establish requirements for improvement of the software investigation process. These requirements form the basis for the design of a near-miss management system (NMS).},
	language = {en},
	urldate = {2023-03-02},
	booktitle = {Software {Failure} {Investigation}: {A} {Near}-{Miss} {Analysis} {Approach}},
	publisher = {Springer International Publishing},
	author = {Eloff, Jan and Bella, Madeleine Bihina},
	editor = {Eloff, Jan and Bihina Bella, Madeleine},
	year = {2018},
	doi = {10.1007/978-3-319-61334-5_2},
	keywords = {ASE23-DA, Requirements for software failure investigation, Root-cause analysis, Software failure investigations, Software failures},
	pages = {7--24},
}

@misc{datareportal_global_2023,
	title = {Global {Social} {Media} {Statistics}},
	url = {https://datareportal.com/social-media-users},
	abstract = {All the latest stats for social media use around the world, including active user numbers for Facebook, Instagram, WeChat, TikTok, WhatsApp, and many more.},
	language = {en-GB},
	urldate = {2023-03-02},
	author = {{DataReportal}},
	month = jan,
	year = {2023},
}

@inproceedings{johnson_software_2000,
	address = {Berlin, Heidelberg},
	series = {{SAFECOMP} '00},
	title = {Software {Support} for {Incident} {Reporting} {Systems} in {Safety}-{Critical} {Applications}},
	isbn = {978-3-540-41186-4},
	abstract = {Incident reporting systems are playing an increasingly important role in the development and maintenance of safety-critical applications. The perceived success of the FAA's Aviation Safety Reporting System (ASRS) and the FDA's MedWatch has led to the establishment of similar national and international schemes. These enable individuals and groups to report their safety concerns in a confidential or anonymous manner. Unfortunately, many of these systems are becoming victims of their own success. The ASRS and MedWatch have both now received over 500,000 submissions. In the past, these systems have relied upon conventional database technology to support the indexing and retrieval of individual reports. However, there are several reasons why this technology is inadequate for many large-scale reporting schemes. In particular, the problems of query formation often result in poor precision and recall. This, in turn, has profound implications for safety-critical applications. Users may fail to identify similar incidents within national or international collections. This paper, therefore, shows how several alternative software architectures support incident report systems in safety-critical applications.},
	urldate = {2023-03-01},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Computer} {Safety}, {Reliability} and {Security}},
	publisher = {Springer-Verlag},
	author = {Johnson, Chris},
	month = oct,
	year = {2000},
	keywords = {ASE23-DA},
	pages = {96--106},
}

@article{perrow2011software,
	title = {Software failures, security, and cyberattacks},
	volume = {20},
	number = {3},
	journal = {TATuP-Zeitschrift für Technikfolgenabschätzung in Theorie und Praxis},
	author = {Perrow, Charles},
	year = {2011},
	keywords = {ASE23-DA},
	pages = {41--46},
}

@inproceedings{zahan_what_2022,
	title = {What are {Weak} {Links} in the npm {Supply} {Chain}?},
	doi = {10.1145/3510457.3513044},
	abstract = {Modern software development frequently uses third-party packages, raising the concern of supply chain security attacks. Many attackers target popular package managers, like npm, and their users with supply chain attacks. In 2021 there was a 650\% year-on-year growth in security attacks by exploiting Open Source Software's supply chain. Proactive approaches are needed to predict package vulnerability to high-risk supply chain attacks. The goal of this work is to help software developers and security specialists in measuring npm supply chain weak link signals to prevent future supply chain attacks by empirically studying npm package metadata. In this paper, we analyzed the metadata of 1.63 million JavaScript npm packages. We propose six signals of security weaknesses in a software supply chain, such as the presence of install scripts, maintainer accounts associated with an expired email domain, and inactive packages with inactive maintainers. One of our case studies identified 11 malicious packages from the install scripts signal. We also found 2,818 maintainer email addresses associated with expired domains, allowing an attacker to hijack 8,494 packages by taking over the npm accounts. We obtained feedback on our weak link signals through a survey responded to by 470 npm package developers. The majority of the developers supported three out of our six proposed weak link signals. The developers also indicated that they would want to be notified about weak links signals before using third-party packages. Additionally, we discussed eight new signals suggested by package developers.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Zahan, Nusrat and Zimmermann, Thomas and Godefroid, Patrice and Murphy, Brendan and Maddila, Chandra and Williams, Laurie},
	month = may,
	year = {2022},
	keywords = {Metadata, Open source software, Security, Software Ecosystem, Software engineering, Software measurement, Supply Chain Security, Supply chains, Weak link Signal, npm},
	pages = {331--340},
}

@misc{noauthor_microsoftscim_2023,
	title = {microsoft/scim},
	url = {https://github.com/microsoft/scim},
	abstract = {Supply Chain Integrity Model},
	urldate = {2023-02-23},
	publisher = {Microsoft},
	month = feb,
	year = {2023},
	note = {original-date: 2021-05-26T22:43:15Z},
}

@misc{noauthor_how_2022,
	title = {How {Cybersecurity} {Policy} {Has} {Changed} {Since} the {SolarWinds} {Attack}},
	url = {https://securityintelligence.com/articles/how-cybersecurity-policy-changed-since-solarwinds-attacks/},
	abstract = {The U.S. government and software industry have taken some major actions in the last few years. What did they learn from the recent cyberattacks?},
	language = {en-US},
	urldate = {2023-02-22},
	journal = {Security Intelligence},
	month = aug,
	year = {2022},
	note = {Section: Government},
}

@misc{chowdhury_better_2022,
	title = {Better {Call} {Saltzer} {\textbackslash}\& {Schroeder}: {A} {Retrospective} {Security} {Analysis} of {SolarWinds} {\textbackslash}\& {Log4j}},
	shorttitle = {Better {Call} {Saltzer} {\textbackslash}\& {Schroeder}},
	url = {http://arxiv.org/abs/2211.02341},
	abstract = {Saltzer {\textbackslash}\& Schroeder's principles aim to bring security to the design of computer systems. We investigate SolarWinds Orion update and Log4j to unpack the intersections where observance of these principles could have mitigated the embedded vulnerabilities. The common principles that were not observed include {\textbackslash}emph\{fail safe defaults\}, {\textbackslash}emph\{economy of mechanism\}, {\textbackslash}emph\{complete mediation\} and {\textbackslash}emph\{least privilege\}. Then we explore the literature on secure software development interventions for developers to identify usable analysis tools and frameworks that can contribute towards improved observance of these principles. We focus on a system wide view of access of codes, checking access paths and aiding application developers with safe libraries along with an appropriate security task list for functionalities.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Chowdhury, Partha Das and Tahaei, Mohammad and Rashid, Awais},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02341 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{bush_zero_2023,
	title = {From {Zero} to 100},
	volume = {66},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3573127},
	doi = {10.1145/3573127},
	abstract = {Demystifying zero trust and its implications on enterprise people, process, and technology.},
	language = {en},
	number = {2},
	urldate = {2023-02-20},
	journal = {Communications of the ACM},
	author = {Bush, Matthew and Mashatan, Atefeh},
	month = feb,
	year = {2023},
	pages = {48--55},
}

@article{enck_top_2022,
	title = {Top {Five} {Challenges} in {Software} {Supply} {Chain} {Security}: {Observations} {From} 30 {Industry} and {Government} {Organizations}},
	volume = {20},
	issn = {1558-4046},
	shorttitle = {Top {Five} {Challenges} in {Software} {Supply} {Chain} {Security}},
	doi = {10.1109/MSEC.2022.3142338},
	abstract = {Software is complex, not only due to the code within a given project, but also due to the vast ecosystem of dependencies and transitive dependencies upon which each project relies. Recent years have observed a sharp uptick of attacks on the software supply chain spurring invigorated interest by industry and government alike. We held three summits with a diverse set of organizations and report on the top five challenges in software supply chain security.},
	number = {2},
	journal = {IEEE Security \& Privacy},
	author = {Enck, William and Williams, Laurie},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Security \& Privacy},
	keywords = {Codes, Ecosystems, Government, Industries, Privacy, Software development management, Supply chains},
	pages = {96--100},
}

@article{enck_top_2022-1,
	title = {Top {Five} {Challenges} in {Software} {Supply} {Chain} {Security}: {Observations} {From} 30 {Industry} and {Government} {Organizations}},
	volume = {20},
	issn = {1558-4046},
	shorttitle = {Top {Five} {Challenges} in {Software} {Supply} {Chain} {Security}},
	doi = {10.1109/MSEC.2022.3142338},
	abstract = {Software is complex, not only due to the code within a given project, but also due to the vast ecosystem of dependencies and transitive dependencies upon which each project relies. Recent years have observed a sharp uptick of attacks on the software supply chain spurring invigorated interest by industry and government alike. We held three summits with a diverse set of organizations and report on the top five challenges in software supply chain security.},
	number = {2},
	journal = {IEEE Security \& Privacy},
	author = {Enck, William and Williams, Laurie},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Security \& Privacy},
	keywords = {Codes, Ecosystems, Government, Industries, Privacy, Software development management, Supply chains},
	pages = {96--100},
}

@misc{ohm_supporting_2021,
	title = {Supporting the {Detection} of {Software} {Supply} {Chain} {Attacks} through {Unsupervised} {Signature} {Generation}},
	url = {http://arxiv.org/abs/2011.02235},
	doi = {10.48550/arXiv.2011.02235},
	abstract = {Trojanized software packages used in software supply chain attacks constitute an emerging threat. Unfortunately, there is still a lack of scalable approaches that allow automated and timely detection of malicious software packages and thus most detections are based on manual labor and expertise. However, it has been observed that most attack campaigns comprise multiple packages that share the same or similar malicious code. We leverage that fact to automatically reproduce manually identified clusters of known malicious packages that have been used in real world attacks, thus, reducing the need for expert knowledge and manual inspection. Our approach, AST Clustering using MCL to mimic Expertise (ACME), yields promising results with a \$F\_\{1\}\$ score of 0.99. Signatures are automatically generated based on characteristic code fragments from clusters and are subsequently used to scan the whole npm registry for unreported malicious packages. We are able to identify and report six malicious packages that have been removed from npm consequentially. Therefore, our approach can support analysts by reducing manual labor and hence may be employed to timely detect possible software supply chain attacks.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Ohm, Marc and Kempf, Lukas and Boes, Felix and Meier, Michael},
	month = mar,
	year = {2021},
	note = {arXiv:2011.02235 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@article{levy_poisoning_2003,
	title = {Poisoning the software supply chain},
	volume = {1},
	issn = {1558-4046},
	doi = {10.1109/MSECP.2003.1203227},
	abstract = {To the indiscriminate and opportunistic attacker, breaking into a software package's development and distribution site and waiting until unsuspecting users install it is more efficient than locating and hacking into users' systems individually. Starting in 2002 and continuing in to 2003, we've seen new emphasis on this type of attack. All the recent activity has showcased the trend that attacks against open-source software distribution sites are increasing. The author looks at how softwares distribution-both open source and proprietary-can invite attacks.},
	number = {3},
	journal = {IEEE Security \& Privacy},
	author = {Levy, E.},
	month = may,
	year = {2003},
	note = {Conference Name: IEEE Security \& Privacy},
	keywords = {Computer security, Cryptography, Horses, Open source software, Packaging, Privacy, Programming, Software packages, Software tools, Supply chains},
	pages = {70--73},
}

@misc{duan_towards_2020,
	title = {Towards {Measuring} {Supply} {Chain} {Attacks} on {Package} {Managers} for {Interpreted} {Languages}},
	url = {http://arxiv.org/abs/2002.01139},
	doi = {10.48550/arXiv.2002.01139},
	abstract = {Package managers have become a vital part of the modern software development process. They allow developers to reuse third-party code, share their own code, minimize their codebase, and simplify the build process. However, recent reports showed that package managers have been abused by attackers to distribute malware, posing significant security risks to developers and end-users. For example, eslint-scope, a package with millions of weekly downloads in Npm, was compromised to steal credentials from developers. To understand the security gaps and the misplaced trust that make recent supply chain attacks possible, we propose a comparative framework to qualitatively assess the functional and security features of package managers for interpreted languages. Based on qualitative assessment, we apply well-known program analysis techniques such as metadata, static, and dynamic analysis to study registry abuse. Our initial efforts found 339 new malicious packages that we reported to the registries for removal. The package manager maintainers confirmed 278 (82\%) from the 339 reported packages where three of them had more than 100,000 downloads. For these packages we were issued official CVE numbers to help expedite the removal of these packages from infected victims. We outline the challenges of tailoring program analysis tools to interpreted languages and release our pipeline as a reference point for the community to build on and help in securing the software supply chain.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {Duan, Ruian and Alrawi, Omar and Kasturi, Ranjita Pai and Elder, Ryan and Saltaformaggio, Brendan and Lee, Wenke},
	month = dec,
	year = {2020},
	note = {arXiv:2002.01139 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{vu_towards_2020,
	address = {Virtual Event USA},
	title = {Towards {Using} {Source} {Code} {Repositories} to {Identify} {Software} {Supply} {Chain} {Attacks}},
	isbn = {978-1-4503-7089-9},
	url = {https://dl.acm.org/doi/10.1145/3372297.3420015},
	doi = {10.1145/3372297.3420015},
	abstract = {Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100 000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (e.g., those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI) suggests that the technique is suitable for lightweight analysis of real-world packages.},
	language = {en},
	urldate = {2023-02-15},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Vu, Duc Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
	month = oct,
	year = {2020},
	pages = {2093--2095},
}

@inproceedings{chou_-depth_2006,
	title = {An {In}-depth {Study} of the {Software} {Supply} {Chain}},
	doi = {10.1109/INDIN.2006.275656},
	abstract = {We examine a software supply chain based on a company case study. We discuss the nature of the software industry and study the characteristics of a typical software product and how it differs from other products that were traditionally focused on in supply chain management. We also discuss why certain well-known concepts in supply chain management are not applicable in a software supply chain.},
	booktitle = {2006 4th {IEEE} {International} {Conference} on {Industrial} {Informatics}},
	author = {Chou, Mabel C. and Ruchika, A.},
	month = aug,
	year = {2006},
	note = {ISSN: 2378-363X},
	keywords = {Books, Companies, Computer industry, Demand forecasting, Logistics, Manufacturing industries, Production, Raw materials, Service supply chain, Software focused products, Software focused supply chain, Supply chain management, Supply chains},
	pages = {753--758},
}

@inproceedings{ohm_towards_2020,
	address = {New York, NY, USA},
	series = {{ARES} '20},
	title = {Towards detection of software supply chain attacks by forensic artifacts},
	isbn = {978-1-4503-8833-7},
	url = {https://doi.org/10.1145/3407023.3409183},
	doi = {10.1145/3407023.3409183},
	abstract = {Third-party dependencies may introduce security risks to the software supply chain and hence yield harm to their dependent software. There are many known cases of malicious open source packages posing risks to developers and end users. However, while efforts are made to detect vulnerable open source packages, malicious packages are not yet considered explicitly. In order to tackle this problem we perform an exploratory case study on previously occurred attacks on the software supply chain with respect to observable artifacts created. Based on gained insights, we propose Buildwatch, a framework for dynamic analysis of software and its third-party dependencies. We noticed that malicious packages introduce a significant amount of new artifacts during installation when compared to benign versions of the same package. The paper presents a first analysis of observable artifacts of malicious packages as well as a possible mitigation strategy that might lead to more insight in long term.},
	urldate = {2023-02-14},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ohm, Marc and Sykosch, Arnold and Meier, Michael},
	month = aug,
	year = {2020},
	keywords = {DevSecOps, application security, malware, supply chain attack},
	pages = {1--6},
}

@article{harutyunyan_managing_2020,
	title = {Managing {Your} {Open} {Source} {Supply} {Chain}-{Why} and {How}?},
	volume = {53},
	issn = {1558-0814},
	doi = {10.1109/MC.2020.2983530},
	abstract = {Reports on the European Commission report on the use of free/libre and open source software (FLOSS). According this study, FLOSS saves the European economy roughly €114 billion per year directly and up to €399 billion per year overall.1 FLOSS components are an essential part of software infrastructure ranging from operating systems and web servers to media players and much more. Companies use such components to address their product requirements for nondifferentiating functionalities while focusing their internal development efforts on core differentiating features. By extension, software supply chains consist of multiple supplier tiers that all feed into each other and thus accumulate open source software that eventually gets into companies that sell complex products, such as original equipment manufacturers (OEMs).},
	number = {6},
	journal = {Computer},
	author = {Harutyunyan, Nikolay},
	month = jun,
	year = {2020},
	note = {Conference Name: Computer},
	keywords = {Management, Open source software, Supply chains},
	pages = {77--81},
}

@article{wang_feasibility_2021,
	title = {On the {Feasibility} of {Detecting} {Software} {Supply} {Chain} {Attacks}},
	abstract = {The Supply chain attack is the stealthy and sophisticated cyberattack that aims to compromise a target by exploiting weaknesses and vulnerabilities in its supply chain. Recent supply chain attacks (e.g., SolarWinds attack) have compromised some of the most secured IT infrastructures of government agencies and enterprises. The European Union Agency for Cybersecurity, ENISA, has predicted that there will be 3 times more supply chain attacks in 2021 than in 2020.},
	language = {en},
	author = {Wang, Xinyuan},
	year = {2021},
}

@article{benedetti_automatic_2022,
	title = {Automatic {Security} {Assessment} of {GitHub} {Actions} {Workflows}},
	abstract = {The demand for quick and reliable DevOps operations pushed distributors of repository platforms to implement workflows. Workflows allow automating code management operations directly on the repository hosting the software. However, this feature also introduces security issues that directly affect the repository, its content, and all the software supply chains in which the hosted code is involved in. Hence, an attack exploiting vulnerable workflows can affect disruptively large software ecosystems. To empirically assess the importance of this problem, in this paper, we focus on the de-facto main distributor (i.e., GitHub). We developed a security assessment methodology for GitHub Actions workflows, which are widely adopted in software supply chains. We implemented the methodology in a tool (GHAST) and applied it on 50 open-source projects. The experimental results are worrisome as they allowed identifying a total of 24,905 security issues (all reported to the corresponding stakeholders), thereby indicating that the problem is open and demands further research and investigation.},
	language = {en},
	journal = {Los Angeles},
	author = {Benedetti, Giacomo},
	year = {2022},
}

@article{hejderup_use_nodate,
	title = {On the {Use} of {Tests} for {Software} {Supply} {Chain} {Threats}},
	abstract = {Development teams are increasingly investing in automating the updating of third-party libraries to limit the patch time of zero-day exploits such as the Equifax breach. GitHub bots such as Dependabot and Renovate build such functionality by leveraging existing test infrastructure in repositories to test and evaluate new library updates. However, two recent studies suggest that test suites in projects lack effectiveness and coverage to reliably find regressions in third-party libraries. Adequate test coverage and effectiveness are critical in discovering new vulnerabilities and weaknesses from third-party libraries. The recent Log4Shell incident exemplifies this, as projects will likely not have adequate tests for logging libraries. This position paper discusses the weaknesses and challenges of current testing practices and techniques from a supply chain security perspective. We highlight two key challenges that researchers and practitioners need to address: (1) the lack of resources and best practices for testing the uses of third-party libraries and (2) enhancing the reliability of automating library updates.},
	language = {en},
	author = {Hejderup, Joseph},
}

@article{vu_poster_2020,
	title = {Poster: {Towards} {Using} {Source} {Code} {Repositories} to {Identify} {Software} {Supply} {Chain} {Attacks}},
	abstract = {Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100 000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (e.g., those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI) suggests that the technique is suitable for lightweight analysis of real-world packages.},
	language = {en},
	author = {Vu, Duc-Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
	year = {2020},
}

@inproceedings{ohm_towards_2020-1,
	address = {Virtual Event Ireland},
	title = {Towards detection of software supply chain attacks by forensic artifacts},
	isbn = {978-1-4503-8833-7},
	url = {https://dl.acm.org/doi/10.1145/3407023.3409183},
	doi = {10.1145/3407023.3409183},
	abstract = {Third-party dependencies may introduce security risks to the software supply chain and hence yield harm to their dependent software. There are many known cases of malicious open source packages posing risks to developers and end users. However, while efforts are made to detect vulnerable open source packages, malicious packages are not yet considered explicitly. In order to tackle this problem we perform an exploratory case study on previously occurred attacks on the software supply chain with respect to observable artifacts created. Based on gained insights, we propose Buildwatch, a framework for dynamic analysis of software and its third-party dependencies. We noticed that malicious packages introduce a significant amount of new artifacts during installation when compared to benign versions of the same package. The paper presents a first analysis of observable artifacts of malicious packages as well as a possible mitigation strategy that might lead to more insight in long term.},
	language = {en},
	urldate = {2022-12-19},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {ACM},
	author = {Ohm, Marc and Sykosch, Arnold and Meier, Michael},
	month = aug,
	year = {2020},
	pages = {1--6},
}

@inproceedings{hejderup_use_2022,
	address = {Los Angeles CA USA},
	title = {On the {Use} of {Tests} for {Software} {Supply} {Chain} {Threats}},
	isbn = {978-1-4503-9885-5},
	url = {https://dl.acm.org/doi/10.1145/3560835.3564557},
	doi = {10.1145/3560835.3564557},
	abstract = {Development teams are increasingly investing in automating the updating of third-party libraries to limit the patch time of zero-day exploits such as the Equifax breach. GitHub bots such as Dependabot and Renovate build such functionality by leveraging existing test infrastructure in repositories to test and evaluate new library updates. However, two recent studies suggest that test suites in projects lack effectiveness and coverage to reliably find regressions in third-party libraries. Adequate test coverage and effectiveness are critical in discovering new vulnerabilities and weaknesses from third-party libraries. The recent Log4Shell incident exemplifies this, as projects will likely not have adequate tests for logging libraries. This position paper discusses the weaknesses and challenges of current testing practices and techniques from a supply chain security perspective. We highlight two key challenges that researchers and practitioners need to address: (1) the lack of resources and best practices for testing the uses of third-party libraries and (2) enhancing the reliability of automating library updates.},
	language = {en},
	urldate = {2022-12-19},
	booktitle = {Proceedings of the 2022 {ACM} {Workshop} on {Software} {Supply} {Chain} {Offensive} {Research} and {Ecosystem} {Defenses}},
	publisher = {ACM},
	author = {Hejderup, Joseph},
	month = nov,
	year = {2022},
	pages = {47--49},
}

@inproceedings{vu_towards_2020-1,
	address = {Virtual Event USA},
	title = {Towards {Using} {Source} {Code} {Repositories} to {Identify} {Software} {Supply} {Chain} {Attacks}},
	isbn = {978-1-4503-7089-9},
	url = {https://dl.acm.org/doi/10.1145/3372297.3420015},
	doi = {10.1145/3372297.3420015},
	abstract = {Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100 000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (e.g., those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI) suggests that the technique is suitable for lightweight analysis of real-world packages.},
	language = {en},
	urldate = {2022-12-19},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Vu, Duc Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
	month = oct,
	year = {2020},
	pages = {2093--2095},
}

@inproceedings{naldurg_modeling_2003,
	address = {Fairfax VA},
	title = {Modeling insecurity: policy engineering for survivability},
	isbn = {978-1-58113-784-2},
	shorttitle = {Modeling insecurity},
	url = {https://dl.acm.org/doi/10.1145/1036921.1036931},
	doi = {10.1145/1036921.1036931},
	abstract = {We present an access-control policy specification and verification process that is well-suited to model survivability of information resources under threat of compromise. Our process differs from the traditional policy engineeringmethodology in many ways. First, we contend that traditional safetyproperty modeling cannot provide any guarantees when the policy enforcement mechanisms axe compromised. Therefore, we extend traditional access control specifications by modeling insecure states and transitions explicitly, to describe possible system behavior after compromise.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 2003 {ACM} workshop on {Survivable} and self-regenerative systems: in association with 10th {ACM} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Naldurg, Prasad and Campbell, Roy H.},
	month = oct,
	year = {2003},
	pages = {91--98},
}

@inproceedings{celik_iotguard_2019,
	address = {San Diego, CA},
	title = {{IoTGuard}: {Dynamic} {Enforcement} of {Security} and {Safety} {Policy} in {Commodity} {IoT}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{IoTGuard}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_07A-1_Celik_paper.pdf},
	doi = {10.14722/ndss.2019.23326},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has changed the way we live, play, and work. To date, the traditional approach to securing IoT has treated devices individually. However, in practice, it has been recently shown that the interactions among devices are often the real cause of safety and security violations. In this paper, we present IOTGUARD, a dynamic, policy-based enforcement system for IoT, which protects users from unsafe and insecure device states by monitoring the behavior of IoT and triggeraction platform apps. IOTGUARD operates in three phases: (a) implementation of a code instrumentor that adds extra logic to an app’s source code to collect app’s information at runtime, (b) storing the apps’ information in a dynamic model that represents the runtime execution behavior of apps, and (c) identifying IoT safety and security policies, and enforcing relevant policies on the dynamic model of individual apps or sets of interacting apps. We demonstrate IOTGUARD on 20 ﬂawed apps and ﬁnd that IOTGUARD correctly enforces 12 of the 12 policy violations. In addition, we evaluate IOTGUARD on 35 SmartThings IoT and 30 IFTTT trigger-action platform market apps executed in a simulated smart home. IOTGUARD enforces 11 unique policies and blocks 16 states in six (17.1\%) SmartThings and ﬁve (16.6\%) IFTTT apps. IOTGUARD imposes only 17.3\% runtime overhead on an app and 19.8\% for ﬁve interacting apps. Through this effort, we introduce a rigorously grounded system for enforcing correct operation of IoT devices through systematically identiﬁed IoT policies, demonstrating the effectiveness and value of monitoring IoT apps with tools such as IOTGUARD.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Celik, Z. Berkay and Tan, Gang and McDaniel, Patrick},
	year = {2019},
}

@inproceedings{ding_iotsafe_2021,
	address = {Virtual},
	title = {{IoTSafe}: {Enforcing} {Safety} and {Security} {Policy} with {Real} {IoT} {Physical} {Interaction} {Discovery}},
	isbn = {978-1-891562-66-2},
	shorttitle = {{IoTSafe}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5A-2_24368_paper.pdf},
	doi = {10.14722/ndss.2021.24368},
	abstract = {The Internet of Things (IoT) platforms bring signiﬁcant convenience for increased home automation. Especially, these platforms provide many new features for managing multiple IoT devices to control their physical surroundings. However, these features also bring new safety and security challenges. For example, an attacker can manipulate IoT devices to launch attacks through unexpected physical interactions. Unfortunately, very few existing research investigates the physical interactions among IoT devices and their impacts on IoT safety and security. In this paper, we propose a novel dynamic safety and security policy enforcement system called IOTSAFE, which can capture and manage real physical interactions considering contextual features on smart home platforms. To identify real physical interactions of IoT devices, we present a runtime physical interaction discovery approach, which employs both static analysis and dynamic testing techniques to identify runtime physical interactions among IoT devices. In addition, IOTSAFE constructs physical models for temporal physical interactions, which can predict incoming risky situations and block unsafe device states accordingly. We implement a prototype of IOTSAFE on the SmartThings platform. Our extensive evaluations demonstrate that IOTSAFE effectively identiﬁes 39 real physical interactions among 130 potential interactions in our experimental environment. IOTSAFE also successfully predicts risky situations related to temporal physical interactions with nearly 96\% accuracy and prevents highly risky conditions.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Ding, Wenbo and Hu, Hongxin and Cheng, Long},
	year = {2021},
}

@article{lee_policy-enforced_nodate,
	title = {Policy-enforced linking of untrusted components},
	language = {en},
	author = {Lee, Eunyoung and Appel, Andrew W},
}

@article{wu_adversarial_nodate,
	title = {Adversarial {Policy} {Training} against {Deep} {Reinforcement} {Learning}},
	abstract = {Reinforcement learning is a set of goal-oriented learning algorithms, through which an agent could learn to behave in an environment, by performing certain actions and observing the reward which it gets from those actions. Integrated with deep neural networks, it becomes deep reinforcement learning, a new paradigm of learning methods. Recently, deep reinforcement learning demonstrates great potential in many applications such as playing video games, mastering GO competition, and even performing autonomous pilot. However, coming together with these great successes is adversarial attacks, in which an adversary could force a well-trained agent to behave abnormally by tampering the input to the agent’s policy network or training an adversarial agent to exploit the weakness of the victim.},
	language = {en},
	author = {Wu, Xian and Guo, Wenbo and Wei, Hua and Xing, Xinyu},
}

@inproceedings{ferraiuolo_policy_2022,
	address = {Los Angeles CA USA},
	title = {Policy {Transparency}: {Authorization} {Logic} {Meets} {General} {Transparency} to {Prove} {Software} {Supply} {Chain} {Integrity}},
	isbn = {978-1-4503-9885-5},
	shorttitle = {Policy {Transparency}},
	url = {https://dl.acm.org/doi/10.1145/3560835.3564549},
	doi = {10.1145/3560835.3564549},
	abstract = {Building reliable software is challenging because today’s software supply chains are built and secured from tools and individuals from a broad range of organizations with complex trust relationships. In this setting, tracking the origin of each piece of software and understanding the security and privacy implications of using it is essential. In this work we aim to secure software supply chains by using verifiable policies in which the origin of information and the trust assumptions are first-order concerns and abusive evidence is discoverable. To do so, we propose Policy Transparency, a new paradigm in which policies are based on authorization logic and all claims issued in this policy language are made transparent by inclusion in a transparency log. Achieving this goal in a real-world setting is non-trivial and to do so we propose a novel software architecture called PolyLog. We find that this combination of authorization logic and transparency logs is mutually beneficial – transparency logs allow authorization logic claims to be widely available aiding in discovery of abuse, and making claims interpretable with policies allows misbehavior captured in the transparency logs to be handled proactively.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 2022 {ACM} {Workshop} on {Software} {Supply} {Chain} {Offensive} {Research} and {Ecosystem} {Defenses}},
	publisher = {ACM},
	author = {Ferraiuolo, Andrew and Behjati, Razieh and Santoro, Tiziano and Laurie, Ben},
	month = nov,
	year = {2022},
	pages = {3--13},
}

@article{mehta_qapla_nodate,
	title = {Qapla: {Policy} compliance for database-backed systems},
	abstract = {Many database-backed systems store conﬁdential data that is accessed on behalf of users with different privileges. Policies governing access are often ﬁne-grained, being speciﬁc to users, time, accessed columns and rows, values in the database (e.g., user roles), and operators used in queries (e.g., aggregators, group by, and join). Today, applications are often relied upon to issue policy compliant queries or ﬁlter the results of non-compliant queries, which is vulnerable to application errors. Qapla provides an alternate approach to policy enforcement that neither depends on application correctness, nor on specialized database support. In Qapla, policies are speciﬁc to rows and columns and may additionally refer to the querier’s identity and time, are speciﬁed in SQL, and stored in the database itself. We prototype Qapla in a database adapter, and evaluate it by enforcing applicable policies in the HotCRP conference management system and a system for managing academic job applications.},
	language = {en},
	author = {Mehta, Aastha and Elnikety, Eslam and Harvey, Katura and Garg, Deepak and Druschel, Peter},
}

@inproceedings{gamble_security_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Security policy foundations in context {UNITY}},
	isbn = {978-1-4503-0581-5},
	url = {https://dl.acm.org/doi/10.1145/1988630.1988633},
	doi = {10.1145/1988630.1988633},
	abstract = {Security certification includes assessing an information system to verify its compliance with diverse, pre-selected security controls. The goal of certification is to identify where controls are implemented correctly and where they are violated, creating potential vulnerability risks. Certification complexity is magnified in software composed of systems of systems where there are limited formal methodologies to express management policies, given a set of security control properties, and verify them against the interaction of the participating components and their individual security policy implementations. In this paper, we extend Context UNITY, a formal, distributed, and context aware coordination language to support policy controls. The new language features enforce security controls and provide a means to declare policy specifics in a manner similar to declaring variable types. We use these features in a specification to show how verifying system compliance with selected security controls, such as those found in the NIST SP800-53 document, can be accomplished.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Software} {Engineering} for {Secure} {Systems}},
	publisher = {ACM},
	author = {Gamble, M. Todd and Gamble, Rose F. and Hale, Matthew L.},
	month = may,
	year = {2011},
	pages = {8--14},
}

@article{li_automatic_nodate,
	title = {Automatic {Policy} {Generation} for {Inter}-{Service} {Access} {Control} of {Microservices}},
	abstract = {Cloud applications today are often composed of many microservices. To prevent a microservice from being abused by other (compromised) microservices, inter-service access control is applied. However, the complexity of ﬁne-grained access control policies, along with the large-scale and dynamic nature of microservices, makes the current manual conﬁgurationbased access control unsuitable. This paper presents AUTOARMOR, the ﬁrst attempt to automate inter-service access control policy generation for microservices, with two fundamental techniques: (1) a static analysis-based request extraction mechanism that automatically obtains the invocation logic among microservices, and (2) a graph-based policy management mechanism that generates corresponding access control policies with on-demand policy update. Our evaluation on popular microservice applications shows that AUTOARMOR is able to generate ﬁne-grained inter-service access control policies and update them timely based on changes in the application, with only a minor runtime overhead. By seamlessly integrating with the lifecycle of microservices, it does not require any changes to existing code and infrastructures.},
	language = {en},
	author = {Li, Xing and Chen, Yan and Lin, Zhiqiang and Wang, Xiao and Chen, Jim Hao},
}

@article{gautam_improving_nodate,
	title = {Improving {Password} {Generation} {Through} the {Design} of a {Password} {Composition} {Policy} {Description} {Language}},
	abstract = {Password managers help users more effectively manage their passwords, yet the adoption of password generation is minimal. One explanation for this problem is that websites’ password composition policies (PCPs) can reject generated passwords, creating a usability impediment. To address this issue, we design a PCP language that websites use to describe their PCP and that managers use to generate compliant passwords. We develop this language using an iterative process involving an extensive collection of PCPs scraped from the Web. We provide libraries for adopting our PCP language into websites and password managers and build proof-of-concept prototypes to verify the real-world feasibility of our PCP language. Using a 25-person user study, we demonstrate that our language and libraries are easy to pick up and correctly use for novice developers. Finally, we replicate and extend past research evaluating Web PCPs, showing that half of PCPs fail to require passwords that resist ofﬂine attacks when considering that users prefer certain character classes when selecting their passwords.},
	language = {en},
	author = {Gautam, Anuj and Lalani, Shan and Ruoti, Scott},
}

@inproceedings{ledru_validation_2015,
	title = {Validation of a {Security} {Policy} by the {Test} of {Its} {Formal} {B} {Specification} – {A} {Case} {Study}},
	doi = {10.1109/FormaliSE.2015.9},
	abstract = {This paper discusses the use of test and animation techniques to validate an access control policy, expressed in Secure UML. It reports on a case study of secure medical information system, where the Secure UML model expresses both functional and security models of the information system. It is translated into a specification in the B language. The case study takes advantage of animation tools associated to the B language to validate the functional model, to perform systematic test of permission rules and to investigate potential insiders attacks.},
	booktitle = {2015 {IEEE}/{ACM} 3rd {FME} {Workshop} on {Formal} {Methods} in {Software} {Engineering}},
	author = {Ledru, Yves and Idani, Akram and Richier, Jean-Luc},
	month = may,
	year = {2015},
	keywords = {Access control, Animation, B method, Computational modeling, Formal specification, Medical diagnostic imaging, Medical services, Test, Unified modeling language, Validation},
	pages = {6--12},
}

@inproceedings{kafali_how_2017,
	title = {How {Good} {Is} a {Security} {Policy} against {Real} {Breaches}? {A} {HIPAA} {Case} {Study}},
	shorttitle = {How {Good} {Is} a {Security} {Policy} against {Real} {Breaches}?},
	doi = {10.1109/ICSE.2017.55},
	abstract = {Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for what. A breach corresponds to a norm violation. We develop a semantic similarity metric for pairwise comparison between the norm that represents a policy clause and the norm that has been violated by a reported breach. We use the US Health Insurance Portability and Accountability Act (HIPAA) as a case study. Our investigation of a subset of the breaches reported by the US Department of Health and Human Services (HHS) reveals the gaps between HIPAA and reported breaches, leading to a coverage of 65\%. Additionally, our classification of the 1,577 HHS breaches shows that 44\% of the breaches are accidental misuses and 56\% are malicious misuses. We find that HIPAA's gaps regarding accidental misuses are significantly larger than its gaps regarding malicious misuses.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Kafali, Özgür and Jones, Jasmine and Petruso, Megan and Williams, Laurie and Singh, Munindar P.},
	month = may,
	year = {2017},
	note = {ISSN: 1558-1225},
	keywords = {Cognition, Measurement, Medical services, Ontologies, Security, Security and privacy breaches, Semantics, Taxonomy, breach ontology, semantic similarity, social norms},
	pages = {530--540},
}

@inproceedings{meng_secure_2018,
	address = {Gothenburg Sweden},
	title = {Secure coding practices in {Java}: challenges and vulnerabilities},
	isbn = {978-1-4503-5638-1},
	shorttitle = {Secure coding practices in {Java}},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180201},
	doi = {10.1145/3180155.3180201},
	abstract = {The Java platform and its third-party libraries provide useful features to facilitate secure coding. However, misusing them can cost developers time and effort, as well as introduce security vulnerabilities in software. We conducted an empirical study on StackOverflow posts, aiming to understand developers’ concerns on Java secure coding, their programming obstacles, and insecure coding practices. We observed a wide adoption of the authentication and authorization features provided by Spring Security—a third-party framework designed to secure enterprise applications. We found that programming challenges are usually related to APIs or libraries, including the complicated cross-language data handling of cryptography APIs, and the complex Java-based or XML-based approaches to configure Spring Security. In addition, we reported multiple security vulnerabilities in the suggested code of accepted answers on the StackOverflow forum. The vulnerabilities included disabling the default protection against Cross-Site Request Forgery (CSRF) attacks, breaking SSL/TLS security through bypassing certificate validation, and using insecure cryptographic hash functions. Our findings reveal the insufficiency of secure coding assistance and documentation, as well as the huge gap between security theory and coding practices.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Meng, Na and Nagy, Stefan and Yao, Danfeng (Daphne) and Zhuang, Wenjie and Argoty, Gustavo Arango},
	month = may,
	year = {2018},
	pages = {372--383},
}

@inproceedings{aghajani_software_2019,
	title = {Software {Documentation} {Issues} {Unveiled}},
	doi = {10.1109/ICSE.2019.00122},
	abstract = {(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Aghajani, Emad and Nagy, Csaba and Vega-Márquez, Olga Lucero and Linares-Vásquez, Mario and Moreno, Laura and Bavota, Gabriele and Lanza, Michele},
	month = may,
	year = {2019},
	note = {ISSN: 1558-1225},
	keywords = {Documentation, Documentation, Empirical Study, Information services, Interviews, Maintenance engineering, Software, Taxonomy, Tools},
	pages = {1199--1210},
}

@inproceedings{azhakesan_sharing_2020,
	address = {Seoul South Korea},
	title = {Sharing at scale: an open-source-software-based license compliance ecosystem},
	isbn = {978-1-4503-7123-0},
	shorttitle = {Sharing at scale},
	url = {https://dl.acm.org/doi/10.1145/3377813.3381351},
	doi = {10.1145/3377813.3381351},
	abstract = {The amount of open-source-software (OSS) used in the global software engineering community is already enormous and still growing. This includes both the products we develop and the development tools we use to create them. It is meanwhile rare to find examples of products that do not contain open source components. Although, using open source components in products does have many advantages, it is very important that one also manages the use of the open source components in a licensecompliant way.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Azhakesan, Aran and Paulisch, Frances},
	month = jun,
	year = {2020},
	pages = {130--131},
}

@inproceedings{garcia_bluejay_2021,
	title = {Bluejay: {A} {Cross}-{Tooling} {Audit} {Framework} {For} {Agile} {Software} {Teams}},
	shorttitle = {Bluejay},
	doi = {10.1109/ICSE-SEET52601.2021.00038},
	abstract = {Agile software teams are expected to follow a number of specific Team Practices (TPs) during each iteration, such as estimating the effort ("points") required to complete user stories and coordinating the management of the codebase with the delivery of features. For software engineering instructors trying to teach such TPs to student teams, manually auditing teams if teams are following the TPs and improving over time is tedious, time-consuming and error-prone. It is even more difficult when those TPs involve two or more tools. For example, starting work on a feature in a project-management tool such as Pivotal Tracker should usually be followed relatively quickly by the creation of a feature branch on GitHub. Merging a feature branch on GitHub should usually be followed relatively quickly by deploying the new feature to a staging server for customer feedback. Few systems are designed specifically to audit such TPs, and existing ones, as far as we know, are limited to a single specific tool. We present Bluejay, an open-source extensible platform that uses the APIs of multiple tools to collect raw data, synthesize it into TP measurements, and present dashboards to audit the TPs. A key insight in Bluejay's design is that TPs can be expressed in terminology similar to that used for modeling and auditing Service Level Agreement (SLA) compliance. Bluejay therefore builds on mature tools used in that ecosystem and adapts them for describing, auditing, and reporting on TPs. Bluejay currently consumes data from five different widely-used development tools, and can be customized by connecting it to any service with a REST API. Video showcase available at governify.io/showcase/bluejay},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} {Education} and {Training} ({ICSE}-{SEET})},
	author = {García, César and Guerrero, Alejandro and Zeitsoff, Joshua and Korlakunta, Srujay and Fernandez, Pablo and Fox, Armando and Ruiz-Cortés, Antonio},
	month = may,
	year = {2021},
	keywords = {Servers, Service level agreements, Software development management, Software engineering, Terminology, Tools, Training, team practice, agile, team dashboard, team practice agreement},
	pages = {283--288},
}

@inproceedings{ju_case_2021,
	title = {A {Case} {Study} of {Onboarding} in {Software} {Teams}: {Tasks} and {Strategies}},
	shorttitle = {A {Case} {Study} of {Onboarding} in {Software} {Teams}},
	doi = {10.1109/ICSE43902.2021.00063},
	abstract = {Developers frequently move into new teams or environments across software companies. Their onboarding experience is correlated with productivity, job satisfaction, and other short-term and long-term outcomes. The majority of the onboarding process comprises engineering tasks such as fixing bugs or implementing small features. Nevertheless, we do not have a systematic view of how tasks influence onboarding. In this paper, we present a case study of Microsoft, where we interviewed 32 developers moving into a new team and 15 engineering managers onboarding a new developer into their team - to understand and characterize developers' onboarding experience and expectations in relation to the tasks performed by them while onboarding. We present how tasks interact with new developers through three representative themes: learning, confidence building, and socialization. We also discuss three onboarding strategies as inferred from the interviews that managers commonly use unknowingly, and discuss their pros and cons and offer situational recommendations. Furthermore, we triangulate our interview findings with a developer survey (N = 189) and a manager survey (N = 37) and find that survey results suggest that our findings are representative and our recommendations are actionable. Practitioners could use our findings to improve their onboarding processes, while researchers could find new research directions from this study to advance the understanding of developer onboarding. Our research instruments and anonymous data are available at https://zenodo.org/record/4455937\#.YCOQCs 0lFd.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ju, An and Sajnani, Hitesh and Kelly, Scot and Herzig, Kim},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Instruments, Interviews, Productivity, Software, Software engineering, Systematics, Task analysis, confidence, learning, onboarding, socialization, software development teams, teams},
	pages = {613--623},
}

@inproceedings{noauthor_no_2022,
	address = {Pittsburgh, PA, USA},
	title = {[{No} title found]},
	isbn = {978-1-66549-598-1},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	publisher = {IEEE},
	year = {2022},
}

@article{lupu_conflicts_1999,
	title = {Conflicts in policy-based distributed systems management},
	volume = {25},
	issn = {1939-3520},
	doi = {10.1109/32.824414},
	abstract = {Modern distributed systems contain a large number of objects and must be capable of evolving, without shutting down the complete system, to cater for changing requirements. There is a need for distributed, automated management agents whose behavior also has to dynamically change to reflect the evolution of the system being managed. Policies are a means of specifying and influencing management behavior within a distributed system, without coding the behavior into the manager agents. Our approach is aimed at specifying implementable policies, although policies may be initially specified at the organizational level and then refined to implementable actions. We are concerned with two types of policies. Authorization policies specify what activities a manager is permitted or forbidden to do to a set of target objects and are similar to security access-control policies. Obligation policies specify what activities a manager must or must not do to a set of target objects and essentially define the duties of a manager. Conflicts can arise in the set of policies. Conflicts may also arise during the refinement process between the high level goals and the implementable policies. The system may have to cater for conflicts such as exceptions to normal authorization policies. The paper reviews policy conflicts, focusing on the problems of conflict detection and resolution. We discuss the various precedence relationships that can be established between policies in order to allow inconsistent policies to coexist within the system and present a conflict analysis tool which forms part of a role based management framework. Software development and medical environments are used as example scenarios.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Lupu, E.C. and Sloman, M.},
	month = nov,
	year = {1999},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Authorization, Computer Society, File systems, Humans, Security},
	pages = {852--869},
}

@article{cohen_four_2021,
	title = {The {Four} {Pillars} of {Research} {Software} {Engineering}},
	volume = {38},
	issn = {1937-4194},
	doi = {10.1109/MS.2020.2973362},
	abstract = {We present four elements we believe are key to providing a comprehensive and sustainable support for research software engineering: software development, community, training, and policy. We also show how the wider developer community can learn from, and engage with, these activities.},
	number = {1},
	journal = {IEEE Software},
	author = {Cohen, Jeremy and Katz, Daniel S. and Barker, Michelle and Chue Hong, Neil and Haines, Robert and Jay, Caroline},
	month = jan,
	year = {2021},
	note = {Conference Name: IEEE Software},
	keywords = {Engineering profession, Software engineering, Software reliability, Sustainable development, Training},
	pages = {97--105},
}

@inproceedings{noauthor_no_2022-1,
	address = {Pittsburgh, PA, USA},
	title = {[{No} title found]},
	isbn = {978-1-66549-598-1},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	publisher = {IEEE},
	year = {2022},
}

@inproceedings{noauthor_no_2022-2,
	address = {Pittsburgh, PA, USA},
	title = {[{No} title found]},
	isbn = {978-1-66549-598-1},
	abstract = {Software developers are inundated with responsibility to incorporate privacy artifacts into software design from the onset in line with best practices. However, little is understood about the struggles developers face implementing privacy into software design. This PhD will undertake: (1) a Systematic Literature Review (SLR) to understand developers interpretation or lack thereof of privacy regulations while incorporating privacy into software systems; (2) two task-based studies to analyze software developers’ privacy compliance to ascertain whether or not they are able to comply with regulatory standards in implementing privacy into software design; (3) analyze mental models adopted by developers when trying to ameliorate their struggles, and (4) then design and evaluate a framework that helps developers make informed privacy decisions.},
	language = {en},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	publisher = {IEEE},
	year = {2022},
}

@misc{wrensch_difference_2020,
	title = {The difference between a policy, procedure, standard and guideline},
	url = {https://www.michalsons.com/blog/the-difference-between-a-policy-procedure-standard-and-a-guideline/42265},
	abstract = {Michalsons - We believe that you clearly need to illustrate the differences between a policy, a procedure, a standard and a guideline.},
	language = {en-GB},
	urldate = {2023-02-27},
	journal = {Michalsons},
	author = {Wrensch, Jesse-Lee},
	month = mar,
	year = {2020},
}

@inproceedings{shokri_membership_2017,
	title = {Membership {Inference} {Attacks} {Against} {Machine} {Learning} {Models}},
	doi = {10.1109/SP.2017.41},
	abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	month = may,
	year = {2017},
	note = {ISSN: 2375-1207},
	keywords = {Data models, Google, Predictive models, Privacy, Sociology, Statistics, Training},
	pages = {3--18},
}

@techreport{padlipsky_perspective_1982,
	type = {Request for {Comments}},
	title = {Perspective on the {ARPANET} reference model},
	url = {https://datatracker.ietf.org/doc/rfc871},
	abstract = {This RFC is primarily intended as a perspective on the ARM and points out some of the differences between the ARM and the ISORM which were expressed by members in NWG general meetings, NWG protocol design committee meetings, the ARPA Internet Working Group, and private conversations over the intervening years. Originally published as M82-47 by the MITRE Corporation, Bedford, Massachusetts.},
	number = {RFC 871},
	urldate = {2023-02-26},
	institution = {Internet Engineering Task Force},
	author = {Padlipsky, M},
	month = sep,
	year = {1982},
	doi = {10.17487/RFC0871},
	note = {Num Pages: 29},
}

@inproceedings{chen_towards_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {Towards training reproducible deep learning models},
	isbn = {978-1-4503-9221-1},
	url = {https://doi.org/10.1145/3510003.3510163},
	doi = {10.1145/3510003.3510163},
	abstract = {Reproducibility is an increasing concern in Artificial Intelligence (AI), particularly in the area of Deep Learning (DL). Being able to reproduce DL models is crucial for AI-based systems, as it is closely tied to various tasks like training, testing, debugging, and auditing. However, DL models are challenging to be reproduced due to issues like randomness in the software (e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There are various practices to mitigate some of the aforementioned issues. However, many of them are either too intrusive or can only work for a specific usage context. In this paper, we propose a systematic approach to training reproducible DL models. Our approach includes three main parts: (1) a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains, (2) a unified framework which leverages a record-and-replay technique to mitigate software-related randomness and a profile-and-patch technique to control hardware-related non-determinism, and (3) a reproducibility guideline which explains the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Case study results show our approach can successfully reproduce six open source and one commercial DL models.},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Boyuan and Wen, Mingzhi and Shi, Yong and Lin, Dayi and Rajbahadur, Gopi Krishnan and Jiang, Zhen Ming (Jack)},
	month = jul,
	year = {2022},
	keywords = {artificial intelligence, deep learning, reproducibility, software engineering},
	pages = {2202--2214},
}

@misc{noauthor_application_nodate,
	title = {Application {Domains}},
	url = {https://www.incose.org/incose-member-resources/working-groups/Application},
	abstract = {Working Groups, Application Domains},
	urldate = {2023-02-24},
	journal = {INCOSE},
	keywords = {ASE23-DA},
}

@misc{noauthor_chatgpt_nodate,
	title = {{ChatGPT}},
	url = {https://chat.openai.com},
	abstract = {A conversational AI system that listens, learns, and challenges},
	urldate = {2023-02-24},
	keywords = {ASE23-DA},
}

@misc{noauthor_sentence-transformersall-minilm-l6-v2_2022,
	title = {sentence-transformers/all-{MiniLM}-{L6}-v2 · {Hugging} {Face}},
	url = {https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-02-24},
	month = nov,
	year = {2022},
	keywords = {ASE23-DA},
}

@article{hgomaa_survey_2013,
	title = {A {Survey} of {Text} {Similarity} {Approaches}},
	volume = {68},
	issn = {09758887},
	url = {http://research.ijcaonline.org/volume68/number13/pxc3887118.pdf},
	doi = {10.5120/11638-7118},
	abstract = {Measuring the similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and text summarization. This survey discusses the existing works on text similarity through partitioning them into three approaches; String-based, Corpus-based and Knowledgebased similarities. Furthermore, samples of combination between these similarities are presented.},
	language = {en},
	number = {13},
	urldate = {2023-02-24},
	journal = {International Journal of Computer Applications},
	author = {H.Gomaa, Wael and A. Fahmy, Aly},
	month = apr,
	year = {2013},
	keywords = {ASE23-DA},
	pages = {13--18},
}

@misc{noauthor_facebookbart-large-mnli_2023,
	title = {facebook/bart-large-mnli · {Hugging} {Face}},
	url = {https://huggingface.co/facebook/bart-large-mnli},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-02-23},
	month = jan,
	year = {2023},
	keywords = {ASE23-DA},
}

@misc{noauthor_bert-base-uncased_2022,
	title = {bert-base-uncased · {Hugging} {Face}},
	url = {https://huggingface.co/bert-base-uncased},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-02-23},
	month = nov,
	year = {2022},
}

@article{noauthor_testing_2008,
	title = {Testing experience te ; the magazine for professional testers},
	issn = {1866-5705},
	url = {http://www.bibliothek.uni-regensburg.de/ezeit/?2497714},
	language = {English},
	urldate = {2023-02-23},
	year = {2008},
	note = {Publisher: Berlin Díaz \& Hilterscheid Unternehmensberatung 2008-},
	keywords = {ASE23-DA},
}

@article{avizienis_basic_2004,
	title = {Basic concepts and taxonomy of dependable and secure computing},
	volume = {1},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2004.2},
	abstract = {This paper gives the main definitions relating to dependability, a generic concept including a special case of such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
	number = {1},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Avizienis, A. and Laprie, J.-C. and Randell, B. and Landwehr, C.},
	month = jan,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {ASE23-DA, Availability, Books, Communication system security, Fault tolerance, Index Terms- Dependability, Maintenance, Safety, Standardization, Taxonomy, Uncertainty, attacks, errors, failures, fault forecasting., fault removal, fault tolerance, faults, security, trust, vulnerabilities},
	pages = {11--33},
}

@article{hiltunen_supporting_1999,
	title = {Supporting customized failure models for distributed software},
	volume = {6},
	issn = {0967-1846, 1361-6390},
	url = {https://iopscience.iop.org/article/10.1088/0967-1846/6/3/302},
	doi = {10.1088/0967-1846/6/3/302},
	abstract = {The cost of employing software fault tolerance techniques in distributed systems is strongly related to the type of failures to be tolerated. For example, in terms of the amount of redundancy required and execution time, tolerating a processor crash is much cheaper than tolerating arbitrary (or Byzantine) failures. This paper describes an approach to constructing conﬁgurable services for distributed systems that allows easy customization of the type of failures to tolerate. Using this approach, it is possible to conﬁgure custom services across a spectrum of possibilities, from a very efﬁcient but unreliable server group that does not tolerate any failures, to a less efﬁcient but reliable group that tolerates crash, omission, timing, or arbitrary failures. The approach is based on building conﬁgurable services as collections of software modules called micro-protocols. Each micro-protocol implements a different semantic property or property variant, and interacts with other micro-protocols using an event-driven model provided by a runtime system. In addition to facilitating the choice of failure model, the approach allows service properties such as message ordering and delivery atomicity to be customized for each application.},
	language = {en},
	number = {3},
	urldate = {2023-02-23},
	journal = {Distributed Systems Engineering},
	author = {Hiltunen, Matti A and Immanuel, Vijaykumar and Schlichting, Richard D},
	month = sep,
	year = {1999},
	keywords = {ASE23-DA},
	pages = {103--111},
}

@article{wong_be_2017,
	title = {Be more familiar with our enemies and pave the way forward: {A} review of the roles bugs played in software failures},
	volume = {133},
	issn = {0164-1212},
	shorttitle = {Be more familiar with our enemies and pave the way forward},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121217301334},
	doi = {10.1016/j.jss.2017.06.069},
	abstract = {There has been an increasing frequency of failures due to defective software that cost millions of dollars. Recent high profile incidents have drawn increased attention to the risks of failed software systems to the public. Yet aside from the Therac-25 case, very few incidents of software failure causing humans harm have been proven and widely reported. With increased government oversight and the expanded use of social networking for real time reporting of problems, we are only beginning to understand the potential for major injury or death related to software failures. However, debugging defective software can be costly and time consuming. Moreover, undetected bugs could induce great harm to the public when software systems are applied in safety-critical areas, such as consumer products, public infrastructure, transportation systems, etc. Therefore, it is vital that we remove these bugs as early as possible. To gain more understanding of the nature of these bugs, we review the reported software failures that have impacted the health, safety, and welfare of the public. A focus on lessons learned and implications for future software systems is also provided which acts as guidelines for engineers to improve the quality of their products and avoid similar failures from happening.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Journal of Systems and Software},
	author = {Wong, W. Eric and Li, Xuelin and Laplante, Philip A.},
	month = nov,
	year = {2017},
	keywords = {ASE23-DA, Accidents, Bugged software systems, Lessons learned, Mishaps, Software failures},
	pages = {68--94},
}

@inproceedings{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	doi = {10.1109/ICSME46990.2020.00027},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	keywords = {ASE23-DA, Conferences, Interviews, Monitoring, Production, Software maintenance, Software systems, empirical studies, incident response, software failures, software monitoring},
	pages = {185--195},
}

@article{neumann_risks_nodate,
	title = {The {RISKS} {Digest}},
	url = {http://catless.ncl.ac.uk/Risks/},
	abstract = {The web page of the RISKS digest moderated by Peter G. Neumman of SRI},
	language = {en},
	urldate = {2022-10-26},
	journal = {The RISKS Digest},
	author = {Neumann, Peter G.},
	keywords = {ASE23-DA},
}

@article{wang_demystifying_2022,
	title = {Demystifying {Regular} {Expression} {Bugs}: {A} comprehensive study on regular expression bug causes, fixes, and testing},
	volume = {27},
	abstract = {Regular expressions cause string-related bugs and open security vulnerabilities for DOS attacks. However, beyond ReDoS (Regular expression Denial of Service), little is known about the extent to which regular expression issues affect software development and how these issues are addressed in practice. We conduct an empirical study of 356 merged regex-related pull request bugs from Apache, Mozilla, Facebook, and Google GitHub repositories. We identify and classify the nature of the regular expression problems, the fixes, and the related changes in the test code.

The most important findings in this paper are as follows: 1) incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3\%). The remaining root causes are incorrect API usage (9.3\%) and other code issues that require regular expression changes in the fix (29.5\%), 2) fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests, 3) most (51\%) of the regex-related pull requests do not contain test code changes. Certain regex bug types (e.g., compile error, performance issues, regex representation) are less likely to include test code changes than others, and 4) the dominant type of test code changes in regex-related pull requests is test case addition (75\%). The results of this study contribute to a broader understanding of the practical problems faced by developers when using, fixing, and testing regular expressions.},
	number = {1},
	journal = {Springer US},
	author = {Wang, Peipei and Brown, Chris and Jennings, Jamie A. and Stolee, Kathryn T.},
	year = {2022},
	pages = {1--35},
}

@inproceedings{wang_exploring_2019,
	address = {Hangzhou, China},
	title = {Exploring {Regular} {Expression} {Evolution}},
	isbn = {978-1-72810-592-5},
	url = {https://doi.org/10.1109/SANER.2019.8667972},
	doi = {10.1109/SANER.2019.8667972},
	abstract = {Although there are tools to help developers understand the matching behaviors between a regular expression and a string, regular-expression related faults are still common. Learning developers' behavior through the change history of regular expressions can identify common edit patterns, which can inform the creation of mutation and repair operators to assist with testing and fixing regular expressions. In this work, we explore how regular expressions evolve over time, focusing on the characteristics of regular expression edits, the syntactic and semantic difference of the edits, and the feature changes of edits. Our exploration uses two datasets. First, we look at GitHub projects that have a regular expression in their current version and look back through the commit logs to collect the regular expressions' edit history. Second, we collect regular expressions composed by study participants during problem-solving tasks. Our results show that 1) 95\% of the regular expressions from GitHub are not edited, 2) most edited regular expressions have a syntactic distance of 4-6 characters from their predecessors, 3) over 50\% of the edits in GitHub tend to expand the scope of regular expression, and 4) the number of features used indicates the regular expression language usage increases over time. This work has implications for supporting regular expression repair and mutation to ensure test suite quality.},
	booktitle = {2019 {IEEE} 26th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	publisher = {IEEE},
	author = {Wang, Peipei and Bai, Gina R. and Stolee, Kathryn T.},
	month = feb,
	year = {2019},
	pages = {502--513},
}

@inproceedings{wang_how_2018,
	address = {Lake Buena Vista FL USA},
	title = {How {Well} {Are} {Regular} {Expressions} {Tested} in the {Wild}?},
	url = {https://doi.org/10.1145/3236024.3236072},
	doi = {10.1145/3236024.3236072},
	abstract = {Developers report testing their regular expressions less than the rest of their code. In this work, we explore how thoroughly tested regular expressions are by examining open source projects.

Using standard metrics of coverage, such as line and branch coverage, gives an incomplete picture of the test coverage of regular expressions. We adopt graph-based coverage metrics for the DFA representation of regular expressions, providing fine-grained test coverage metrics. Using over 15,000 tested regular expressions in 1,225 Java projects on GitHub,we measure node, edge, and edge-pair coverage. Our results show that only 17\% of the regular expressions in the repositories are tested at all. For those that are tested, the median number of test inputs is two. For nearly 42\% of the tested regular expressions, only one test input is used. Average node and edge coverage levels on the DFAs for tested regular expressions are 59\% and 29\%, respectively. Due to the lack of testing of regular expressions, we explore whether a string generation tool for regular expressions, Rex, achieves high coverage levels. With some exceptions, we found that tools such as Rex can be used to write test inputs with similar coverage to the developer tests.},
	booktitle = {2018 26th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE} '18)},
	publisher = {ACM},
	author = {Wang, Peipei and Stolee, Kathryn T.},
	month = nov,
	year = {2018},
	pages = {668--678},
}

@misc{perez_discovering_2022,
	title = {Discovering {Language} {Model} {Behaviors} with {Model}-{Written} {Evaluations}},
	url = {http://arxiv.org/abs/2212.09251},
	doi = {10.48550/arXiv.2212.09251},
	abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{kasi_post_2008,
	title = {The post mortem paradox: a {Delphi} study of {IT} specialist perceptions},
	volume = {17},
	issn = {0960-085X, 1476-9344},
	url = {https://www.tandfonline.com/doi/full/10.1057/palgrave.ejis.3000727},
	doi = {10.1057/palgrave.ejis.3000727},
	abstract = {While post mortem evaluation (PME) has long been advocated as a means of improving development practices by learning from IT project failures, few organizations conduct PMEs. The purpose of the study is to explain this discrepancy between theory and practice. This paper integrates findings from a Delphi study of what experienced practitioners perceive as the most important barriers to conducting PMEs with insights from organizational learning theory. The results suggest that there are critical tensions between development practices and learning contexts in many organizations, and adopting PMEs in these cases is likely to reinforce organizational learning dysfunctions rather than improve current development practices. Based on these findings, we argue that the PME literature has underestimated the limits to learning in most IT organizations and we propose to explore paradoxical thinking to help researchers frame continued inquiry into PME and to help managers overcome learning dysfunctions as they push for more widespread use of PMEs.},
	language = {en},
	number = {1},
	urldate = {2022-02-14},
	journal = {European Journal of Information Systems},
	author = {Kasi, Vijay and Keil, Mark and Mathiassen, Lars and Pedersen, Keld},
	month = feb,
	year = {2008},
	keywords = {ASE23-DA, DA\_NSFGRFP},
	pages = {62--78},
}

@article{ratasich_roadmap_2019,
	title = {A {Roadmap} {Toward} the {Resilient} {Internet} of {Things} for {Cyber}-{Physical} {Systems}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2891969},
	abstract = {The Internet of Things (IoT) is a ubiquitous system connecting many different devices - the things - which can be accessed from the distance. The cyber-physical systems (CPSs) monitor and control the things from the distance. As a result, the concepts of dependability and security get deeply intertwined. The increasing level of dynamicity, heterogeneity, and complexity adds to the system's vulnerability, and challenges its ability to react to faults. This paper summarizes the state of the art of existing work on anomaly detection, fault-tolerance, and self-healing, and adds a number of other methods applicable to achieve resilience in an IoT. We particularly focus on non-intrusive methods ensuring data integrity in the network. Furthermore, this paper presents the main challenges in building a resilient IoT for the CPS, which is crucial in the era of smart CPS with enhanced connectivity (an excellent example of such a system is connected autonomous vehicles). It further summarizes our solutions, work-in-progress and future work to this topic to enable “Trustworthy IoT for CPS”. Finally, this framework is illustrated on a selected use case: a smart sensor infrastructure in the transport domain.},
	journal = {IEEE Access},
	author = {Ratasich, Denise and Khalid, Faiq and Geissler, Florian and Grosu, Radu and Shafique, Muhammad and Bartocci, Ezio},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {ASE23-DA, Anomaly detection, Cyber-physical systems, Internet of Things, Internet of Things (IoT), Monitoring, Resilience, Robustness, Safety, Security, cyber-physical systems (CPS), long-term dependability and security, monitoring, resilience, self-adaptation, self-healing},
	pages = {13260--13283},
}

@misc{obrien_musks_2022,
	title = {Musk's {Twitter} disbands its {Trust} and {Safety} advisory group},
	url = {https://apnews.com/article/elon-musk-twitter-inc-technology-business-a9b795e8050de12319b82b5dd7118cd7},
	abstract = {Elon Musk's Twitter has dissolved its Trust and Safety Council, the advisory group of around 100 independent civil, human rights and other organizations that the company formed in 2016 to address hate speech, child exploitation, suicide, self-harm and other problems on the platform.},
	language = {en},
	urldate = {2023-02-21},
	journal = {AP NEWS},
	author = {O'Brien, Matt and Ortutay, Barbara},
	month = dec,
	year = {2022},
	note = {Section: Elon Musk},
}

@inproceedings{thomas_sok_2021,
	title = {{SoK}: {Hate}, {Harassment}, and the {Changing} {Landscape} of {Online} {Abuse}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP40001.2021.00028},
	abstract = {We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so.},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Thomas, Kurt and Akhawe, Devdatta and Bailey, Michael and Boneh, Dan and Bursztein, Elie and Consolvo, Sunny and Dell, Nicola and Durumeric, Zakir and Kelley, Patrick Gage and Kumar, Deepak and McCoy, Damon and Meiklejohn, Sarah and Ristenpart, Thomas and Stringhini, Gianluca},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Cognition, Computer security, Distance measurement, Privacy, Social networking (online), Taxonomy, at-risk, emerging-threats, harassment, hate},
	pages = {247--267},
}

@article{gillespie_content_2020,
	title = {Content moderation, {AI}, and the question of scale},
	volume = {7},
	issn = {2053-9517},
	url = {https://doi.org/10.1177/2053951720943234},
	doi = {10.1177/2053951720943234},
	abstract = {AI seems like the perfect response to the growing challenges of content moderation on social media platforms: the immense scale of the data, the relentlessness of the violations, and the need for human judgments without wanting humans to have to make them. The push toward automated content moderation is often justified as a necessary response to the scale: the enormity of social media platforms like Facebook and YouTube stands as the reason why AI approaches are desirable, even inevitable. But even if we could effectively automate content moderation, it is not clear that we should.},
	number = {2},
	urldate = {2023-02-15},
	journal = {Big Data \& Society},
	author = {Gillespie, Tarleton},
	month = jul,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	pages = {2053951720943234},
}

@article{zhang_fuzzing_2023,
	title = {Fuzzing {Configurations} of {Program} {Options}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3580597},
	doi = {10.1145/3580597},
	abstract = {While many real-world programs are shipped with configurations to enable/disable functionalities, fuzzers have mostly been applied to test single configurations of these programs. In this work, we first conduct an empirical study to understand how program configurations affect fuzzing performance. We find that limiting a campaign to a single configuration can result in failing to cover a significant amount of code. We also observe that different program configurations contribute differing amounts of code coverage, challenging the idea that each one can be efficiently fuzzed individually. Motivated by these two observations we propose ConfigFuzz, which can fuzz configurations along with normal inputs. ConfigFuzz transforms the target program to encode its program options within part of the fuzzable input, so existing fuzzers’ mutation operators can be reused to fuzz program configurations. We instantiate ConfigFuzz on 6 configurable, common fuzzing targets, and integrate their executions in FuzzBench. In our evaluation, ConfigFuzz outperforms two baseline fuzzers in four targets, while the results are mixed in the other targets due to program size and configuration space. We also analyze the options fuzzed by ConfigFuzz and how they affect the performance.},
	urldate = {2023-02-14},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Zhang, Zenong and Klees, George and Wang, Eric and Hicks, Michael and Wei, Shiyi},
	month = feb,
	year = {2023},
	note = {Just Accepted},
}

@article{zhang_fuzzing_2023-1,
	title = {Fuzzing {Configurations} of {Program} {Options}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3580597},
	doi = {10.1145/3580597},
	abstract = {While many real-world programs are shipped with configurations to enable/disable functionalities, fuzzers have mostly been applied to test single configurations of these programs. In this work, we first conduct an empirical study to understand how program configurations affect fuzzing performance. We find that limiting a campaign to a single configuration can result in failing to cover a significant amount of code. We also observe that different program configurations contribute differing amounts of code coverage, challenging the idea that each one can be efficiently fuzzed individually. Motivated by these two observations we propose ConfigFuzz, which can fuzz configurations along with normal inputs. ConfigFuzz transforms the target program to encode its program options within part of the fuzzable input, so existing fuzzers’ mutation operators can be reused to fuzz program configurations. We instantiate ConfigFuzz on 6 configurable, common fuzzing targets, and integrate their executions in FuzzBench. In our evaluation, ConfigFuzz outperforms two baseline fuzzers in four targets, while the results are mixed in the other targets due to program size and configuration space. We also analyze the options fuzzed by ConfigFuzz and how they affect the performance.},
	urldate = {2023-02-14},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Zhang, Zenong and Klees, George and Wang, Eric and Hicks, Michael and Wei, Shiyi},
	month = feb,
	year = {2023},
	note = {Just Accepted},
}

@inproceedings{torres-arias_-toto_2019,
	title = {in-toto: {Providing} farm-to-table guarantees for bits and bytes},
	isbn = {978-1-939133-06-9},
	shorttitle = {in-toto},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/torres-arias},
	language = {en},
	urldate = {2022-07-01},
	author = {Torres-Arias, Santiago and Afzali, Hammad and Kuppusamy, Trishank Karthik and Curtmola, Reza and Cappos, Justin},
	year = {2019},
	pages = {1393--1410},
}

@article{marashdih_enhanced_2023,
	title = {An {Enhanced} {Static} {Taint} {Analysis} {Approach} to {Detect} {Input} {Validation} {Vulnerability}},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157823000095},
	doi = {10.1016/j.jksuci.2023.01.009},
	abstract = {The detection of feasible paths helps to minimize the false positive rate. However, the previous works did not consider the feasibility of the program paths during the analysis detection of input validation vulnerabilities, which led to false positive results. They also needed to validate the usage of the proper sanitization functions for each context of the user input. Therefore, we proposed an enhanced static taint analysis approach to analyse the source code and track the tainted inputs in the program. It started by examining the source code to find the feasibility of each path in the program. The tainted variables were tracked through the analysis until the sink statement, which executes the tainted variables. An algorithm was built to enhance the static analyzer to handle the variables handling functions in PHP. The proposed approach was evaluated with SARD datasets and large-scale PHP programs. The results indicated that the precision in detecting XSS vulnerability was approximately 44\% better than WAP and 26\% better than RIPS, and its precision in detecting SQL injection was about 10\% better than WAP and 19\% better than RIPS. Furthermore, the proposed approach outperformed previous symbolic execution studies regarding the number of detected vulnerabilities.},
	language = {en},
	urldate = {2023-02-12},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Marashdih, Abdalla Wasef and Zaaba, Zarul Fitri and Suwais, Khaled},
	month = jan,
	year = {2023},
	keywords = {Input validation vulnerability, SQLi, Static analysis, Taint analysis, XSS},
}

@inproceedings{koishybayev_characterizing_2022,
	title = {Characterizing the {Security} of {Github} \{{CI}\} {Workflows}},
	isbn = {9781939133311},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/koishybayev},
	urldate = {2023-02-10},
	author = {Koishybayev, Igibek and Nahapetyan, Aleksandr and Zachariah, Raima and Muralee, Siddharth and Reaves, Bradley and Kapravelos, Alexandros and Machiry, Aravind},
	year = {2022},
	pages = {2747--2763},
}

@article{cohen1960coefficient,
	title = {A coefficient of agreement for nominal scales},
	volume = {20},
	number = {1},
	journal = {Educational and psychological measurement},
	author = {Cohen, Jacob},
	year = {1960},
	pages = {37--46},
}

@article{magnusson_simics_2002,
	title = {Simics: {A} full system simulation platform},
	volume = {35},
	issn = {1558-0814},
	shorttitle = {Simics},
	doi = {10.1109/2.982916},
	abstract = {Full system simulation seeks to strike a balance between accuracy and performance. Many of its possibilities have been obvious to practitioners in both academia and industry for quite some time, perhaps decades, but Simics supports more of these possibilities within a single framework than other tools do. Simics is a platform for full system simulation that can run actual firmware and completely unmodified kernel and driver code. It is sufficiently abstract to achieve tolerable performance levels, and it provides both functional accuracy for running commercial workloads and sufficient timing accuracy to interface to detailed hardware models. Simics can also run a heterogeneous network of systems from different vendors within the same framework. Exceptionally fast, Simics can easily add new components and leverage older ones within a practical abstraction level. It offers a platform with a rich API and a powerful scripting environment for use in a broad range of applications.},
	number = {2},
	journal = {Computer},
	author = {Magnusson, P.S. and Christensson, M. and Eskilson, J. and Forsgren, D. and Hallberg, G. and Hogberg, J. and Larsson, F. and Moestedt, A. and Werner, B.},
	month = feb,
	year = {2002},
	note = {Conference Name: Computer},
	keywords = {Application software, Automatic control, Automation, Computational modeling, Computer simulation, Context modeling, Costs, Hardware, Shape, Timing},
	pages = {50--58},
}

@inproceedings{clements_halucinator_2020,
	address = {USA},
	series = {{SEC}'20},
	title = {{HALucinator}: firmware re-hosting through abstraction layer emulation},
	isbn = {978-1-939133-17-5},
	shorttitle = {{HALucinator}},
	abstract = {Given the increasing ubiquity of online embedded devices, analyzing their firmware is important to security, privacy, and safety. The tight coupling between hardware and firmware and the diversity found in embedded systems makes it hard to perform dynamic analysis on firmware. However, firmware developers regularly develop code using abstractions, such as Hardware Abstraction Layers (HALs), to simplify their job. We leverage such abstractions as the basis for the re-hosting and analysis of firmware. By providing high-level replacements for HAL functions (a process termed High-Level Emulation - HLE), we decouple the hardware from the firmware. This approach works by first locating the library functions in a firmware sample, through binary analysis, and then providing generic implementations of these functions in a full-system emulator. We present these ideas in a prototype system, HALucinator, able to re-host firmware, and allow the virtual device to be used normally. First, we introduce extensions to existing library matching techniques that are needed to identify library functions in binary firmware, to reduce collisions, and for inferring additional function names. Next, we demonstrate the re-hosting process, through the use of simplified handlers and peripheral models, which make the process fast, flexible, and portable between firmware samples and chip vendors. Finally, we demonstrate the practicality of HLE for security analysis, by supplementing HALucinator with the American Fuzzy Lop fuzzer, to locate multiple previously-unknown vulnerabilities in firmware middleware libraries.},
	urldate = {2023-02-07},
	booktitle = {Proceedings of the 29th {USENIX} {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Clements, Abraham A. and Gustafson, Eric and Scharnowski, Tobias and Grosen, Paul and Fritz, David and Kruegel, Christopher and Vigna, Giovanni and Bagchi, Saurabh and Payer, Mathias},
	month = aug,
	year = {2020},
	pages = {1201--1218},
}

@article{gustafson_toward_nodate,
	title = {Toward the {Analysis} of {Embedded} {Firmware} through {Automated} {Re}-hosting},
	abstract = {The recent paradigm shift introduced by the Internet of Things (IoT) has brought embedded systems into focus as a target for both security analysts and malicious adversaries. Typiﬁed by their lack of standardized hardware, diverse software, and opaque functionality, IoT devices present unique challenges to security analysts due to the tight coupling between their ﬁrmware and the hardware for which it was designed. In order to take advantage of modern program analysis techniques, such as fuzzing or symbolic execution, with any kind of scale or depth, analysts must have the ability to execute ﬁrmware code in emulated (or virtualized) environments. However, these emulation environments are rarely available and are cumbersome to create through manual reverse engineering, greatly limiting the analysis of binary ﬁrmware. In this work, we explore the problem of ﬁrmware re-hosting, the process by which ﬁrmware is migrated from its original hardware environment into a virtualized one. We show that an approach capable of creating virtual, interactive environments in an automated manner is a necessity to enable ﬁrmware analysis at scale. We present the ﬁrst proof-of-concept system aiming to achieve this goal, called PRETENDER, which uses observations of the interactions between the original hardware and the ﬁrmware to automatically create models of peripherals, and allows for the execution of the ﬁrmware in a fully-emulated environment. Unlike previous approaches, these models are interactive, stateful, and transferable, meaning they are designed to allow the program to receive and process new input, a requirement of many analyses. We demonstrate our approach on multiple hardware platforms and ﬁrmware samples, and show that the models are ﬂexible enough to allow for virtualized code execution, the exploration of new code paths, and the identiﬁcation of security vulnerabilities.},
	language = {en},
	author = {Gustafson, Eric and Muench, Marius and Spensky, Chad and Redini, Nilo and Machiry, Aravind and Fratantonio, Yanick and Francillon, Aurélien and Balzarotti, Davide and Choe, Yung Ryn and Kruegel, Christopher and Vigna, Giovanni},
}

@article{feng_p2im_nodate,
	title = {{P2IM}: {Scalable} and {Hardware}-independent {Firmware} {Testing} via {Automatic} {Peripheral} {Interface} {Modeling}},
	abstract = {Dynamic testing or fuzzing of embedded ﬁrmware is severely limited by hardware-dependence and poor scalability, partly contributing to the widespread vulnerable IoT devices. We propose a software framework that continuously executes a given ﬁrmware binary while channeling inputs from an off-the-shelf fuzzer, enabling hardware-independent and scalable ﬁrmware testing. Our framework, using a novel technique called P2IM, abstracts diverse peripherals and handles ﬁrmware I/O on the ﬂy based on automatically generated models. P2IM is oblivious to peripheral designs and generic to ﬁrmware implementations, and therefore, applicable to a wide range of embedded devices. We evaluated our framework using 70 sample ﬁrmware and 10 ﬁrmware from real devices, including a drone, a robot, and a PLC. It successfully executed 79\% of the sample ﬁrmware without any manual assistance. We also performed a limited fuzzing test on the real ﬁrmware, which unveiled 7 unique unknown bugs.},
	language = {en},
	author = {Feng, Bo and Mera, Alejandro and Lu, Long},
}

@article{corteggiani_inception_nodate,
	title = {Inception: {System}-{Wide} {Security} {Testing} of {Real}-{World} {Embedded} {Systems} {Software}},
	language = {en},
	author = {Corteggiani, Nassim and Camurati, Giovanni and Francillon, Aurelien},
}

@misc{noauthor_klee_nodate,
	title = {{KLEE}},
	url = {http://klee.github.io/},
	urldate = {2023-02-07},
}

@misc{noauthor_qemu_nodate,
	title = {{QEMU}},
	url = {https://www.qemu.org/},
	urldate = {2023-02-07},
}

@inproceedings{scharnowski_fuzzware_2022,
	title = {Fuzzware: {Using} {Precise} \{{MMIO}\} {Modeling} for {Effective} {Firmware} {Fuzzing}},
	isbn = {978-1-939133-31-1},
	shorttitle = {Fuzzware},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/scharnowski},
	language = {en},
	urldate = {2023-02-07},
	author = {Scharnowski, Tobias and Bars, Nils and Schloegel, Moritz and Gustafson, Eric and Muench, Marius and Vigna, Giovanni and Kruegel, Christopher and Holz, Thorsten and Abbasi, Ali},
	year = {2022},
	pages = {1239--1256},
}

@article{Saltzer1975ProtectionInformationinCS,
	title = {The protection of information in computer systems},
	volume = {63},
	issn = {1558-2256},
	doi = {10.1109/PROC.1975.9939},
	abstract = {This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures-whether hardware or software-that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysts of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading.},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Saltzer, J.H. and Schroeder, M.D.},
	month = sep,
	year = {1975},
	keywords = {Access control, Authorization, Computer architecture, Data security, Information security, Modems, Permission, Protection, Terminology},
	pages = {1278--1308},
}

@inproceedings{zhang_debloating_2022,
	title = {Debloating {Address} {Sanitizer}},
	isbn = {978-1-939133-31-1},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/zhang-yuchen},
	language = {en},
	urldate = {2023-02-06},
	author = {Zhang, Yuchen and Pang, Chengbin and Portokalidis, Georgios and Triandopoulos, Nikos and Xu, Jun},
	year = {2022},
	pages = {4345--4363},
}

@inproceedings{bulekov_morphuzz_2022,
	title = {Morphuzz: {Bending} ({Input}) {Space} to {Fuzz} {Virtual} {Devices}},
	isbn = {978-1-939133-31-1},
	shorttitle = {Morphuzz},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/bulekov},
	language = {en},
	urldate = {2023-02-04},
	author = {Bulekov, Alexander and Das, Bandan and Hajnoczi, Stefan and Egele, Manuel},
	year = {2022},
	pages = {1221--1238},
}

@misc{zhuang_comprehensive_2020,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1911.02685},
	doi = {10.48550/arXiv.1911.02685},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{Zimmermann2019SecurityThreatsinNPMEcosystem,
	title = {Small {World} with {High} {Risks}: {A} {Study} of {Security} {Threats} in the npm {Ecosystem}},
	doi = {10.5555/3361338.3361407},
	abstract = {The popularity of JavaScript has lead to a large ecosystem of third-party packages available via the npm software package registry. The open nature of npm has boosted its growth, providing over 800,000 free and reusable software packages. Unfortunately, this open nature also causes security risks, as evidenced by recent incidents of single packages that broke or attacked software running on millions of computers. This paper studies security risks for users of npm by systematically analyzing dependencies between packages, the maintainers responsible for these packages, and publicly reported security issues. Studying the potential for running vulnerable or malicious code due to third-party dependencies, we ﬁnd that individual packages could impact large parts of the entire ecosystem. Moreover, a very small number of maintainer accounts could be used to inject malicious code into the majority of all packages, a problem that has been increasing over time. Studying the potential for accidentally using vulnerable code, we ﬁnd that lack of maintenance causes many packages to depend on vulnerable code, even years after a vulnerability has become public. Our results provide evidence that npm suffers from single points of failure and that unmaintained packages threaten large code bases. We discuss several mitigation techniques, such as trusted maintainers and total ﬁrst-party security, and analyze their potential effectiveness.},
	booktitle = {{USENIX} {Security} {Symposium}},
	author = {Zimmermann, Markus and Staicu, Cristian-Alexandru and Pradel, Michael},
	year = {2019},
}

@inproceedings{Zahan2022WeakLinksinNPMSupplyChain,
	title = {What are {Weak} {Links} in the npm {Supply} {Chain}?},
	url = {https://www.microsoft.com/en-us/research/publication/what-are-weak-links-in-the-npm-supply-chain/},
	abstract = {Modern software development frequently uses third-party packages, raising the concern of supply chain security attacks. Many attackers target popular package managers, like npm, and their users with supply chain attacks. In 2021 there was a 650\% year-on-year growth in security attacks by exploiting Open Source Software's supply chain. Proactive approaches are needed to predict package vulnerability to high-risk supply chain attacks. The goal of this work is to help software developers and security specialists in measuring npm supply chain weak link signals to prevent future supply chain attacks by empirically studying npm package metadata. In this paper, we analyzed the metadata of 1.63 million JavaScript npm packages. We propose six signals of security weaknesses in a software supply chain, such as the presence of install scripts, maintainer accounts associated with an expired email domain, and inactive packages with inactive maintainers. One of our case studies identified 11 malicious packages from the install scripts signal. We also found 2,818 maintainer email addresses associated with expired domains, allowing an attacker to hijack 8,494 packages by taking over the npm accounts. We obtained feedback on our weak link signals through a survey responded to by 470 npm package developers. The majority of the developers supported three out of our six proposed weak link signals. The developers also indicated that they would want to be notified about weak links signals before using third-party packages. Additionally, we discussed eight new signals suggested by package developers.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Zahan, Nusrat and Zimmermann, Tom and Godefroid, Patrice and Murphy, Brendan and Maddila, Chandra and Williams, Laurie},
	month = may,
	year = {2022},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{newman_sigstore_2022,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {Sigstore: {Software} {Signing} for {Everybody}},
	isbn = {978-1-4503-9450-5},
	shorttitle = {Sigstore},
	url = {https://doi.org/10.1145/3548606.3560596},
	doi = {10.1145/3548606.3560596},
	abstract = {Software supply chain compromises are on the rise. From the effects of XCodeGhost to SolarWinds, hackers have identified that targeting weak points in the supply chain allows them to compromise high-value targets such as U.S. government agencies and corporate targets such as Google and Microsoft. Software signing, a promising mitigation for many of these attacks, has seen limited adoption in open-source and enterprise ecosystems. In this paper, we propose Sigstore, a system to provide widespread software signing capabilities. To do so, we designed the system to provide baseline artifact signing capabilities that minimize the adoption barrier for developers. To this end, Sigstore leverages three distinct mechanisms: First, it uses a protocol similar to ACME to authenticate developers through OIDC, tying signatures to existing and widely-used identities. Second, it enables developers to use ephemeral keys to sign their artifacts, reducing the inconvenience and risk of key management. Finally, Sigstore enables user authentication by means of artifact and identity logs, bringing transparency to software signatures. Sigstore is quickly becoming a critical piece of Internet infrastructure with more than 2.2M signatures over critical software such as Kubernetes and Distroless.},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Newman, Zachary and Meyers, John Speed and Torres-Arias, Santiago},
	month = nov,
	year = {2022},
	keywords = {code signing, distributed systems, security, software transparency},
	pages = {2353--2367},
}

@inproceedings{newman_sigstore_2022-1,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {Sigstore: {Software} {Signing} for {Everybody}},
	isbn = {978-1-4503-9450-5},
	shorttitle = {Sigstore},
	url = {https://doi.org/10.1145/3548606.3560596},
	doi = {10.1145/3548606.3560596},
	abstract = {Software supply chain compromises are on the rise. From the effects of XCodeGhost to SolarWinds, hackers have identified that targeting weak points in the supply chain allows them to compromise high-value targets such as U.S. government agencies and corporate targets such as Google and Microsoft. Software signing, a promising mitigation for many of these attacks, has seen limited adoption in open-source and enterprise ecosystems. In this paper, we propose Sigstore, a system to provide widespread software signing capabilities. To do so, we designed the system to provide baseline artifact signing capabilities that minimize the adoption barrier for developers. To this end, Sigstore leverages three distinct mechanisms: First, it uses a protocol similar to ACME to authenticate developers through OIDC, tying signatures to existing and widely-used identities. Second, it enables developers to use ephemeral keys to sign their artifacts, reducing the inconvenience and risk of key management. Finally, Sigstore enables user authentication by means of artifact and identity logs, bringing transparency to software signatures. Sigstore is quickly becoming a critical piece of Internet infrastructure with more than 2.2M signatures over critical software such as Kubernetes and Distroless.},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Newman, Zachary and Meyers, John Speed and Torres-Arias, Santiago},
	month = nov,
	year = {2022},
	keywords = {code signing, distributed systems, security, software transparency},
	pages = {2353--2367},
}

@inproceedings{cramer_ts_nodate,
	title = {T\&{S} {Engineering} in {OSS} {SMPs}},
	url = {https://www.overleaf.com/project/6320bd0e3583907364821961},
	language = {en},
	urldate = {2023-02-03},
	author = {Cramer, Geoffrey and Maxam, William P. and Li, Qianqian and Davis, James C.},
}

@misc{noauthor_project_nodate,
	title = {Project risk management using multiple criteria decision-making technique and decision tree analysis: a case study of {Indian} oil refinery},
	shorttitle = {Project risk management using multiple criteria decision-making technique and decision tree analysis},
	url = {https://www.tandfonline.com/doi/epdf/10.1080/09537287.2011.586379?needAccess=true&role=button},
	language = {en},
	urldate = {2023-02-03},
	note = {ISSN: 0953-7287},
}

@misc{trust_and_safety_professional_association_policy_nodate,
	title = {Policy {Development}},
	url = {https://www.tspa.org/curriculum/ts-fundamentals/policy/policy-development/},
	abstract = {What Is Policy? Policy Policy is the set of rules and principles that a platform uses to govern the conduct of its users. It often, but not always, exists in two forms: an external document providing an overview of the company’s expectations of user behavior and an internal document detailing exactly how and when to},
	language = {en-US},
	urldate = {2023-02-02},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
}

@article{confessore_cambridge_2018,
	chapter = {U.S.},
	title = {Cambridge {Analytica} and {Facebook}: {The} {Scandal} and the {Fallout} {So} {Far}},
	issn = {0362-4331},
	shorttitle = {Cambridge {Analytica} and {Facebook}},
	url = {https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html},
	abstract = {Revelations that digital consultants to the Trump campaign misused the data of millions of Facebook users set off a furor on both sides of the Atlantic. This is how The Times covered it.},
	language = {en-US},
	urldate = {2023-02-02},
	journal = {The New York Times},
	author = {Confessore, Nicholas},
	month = apr,
	year = {2018},
	keywords = {Cambridge Analytica, Data-Mining and Database Marketing, Facebook Inc, Great Britain Withdrawal from EU (Brexit), News and News Media, Presidential Election of 2016, Rumors and Misinformation},
}

@misc{trust_and_safety_professional_association_what_nodate,
	title = {What {We} {Do}},
	url = {https://www.tspa.org/what-we-do/},
	abstract = {The Trust \& Safety Professional Association (TSPA) is a 501(c)(6) non-partisan membership association that supports the global community of professionals who develop and enforce principles, policies, and practices that define acceptable behavior and content online and/or facilitated by digital technologies. TSPA works to create and foster a global community of trust and safety professionals, collaborating},
	language = {en-US},
	urldate = {2023-02-02},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
}

@article{bastien_usability_2010,
	title = {Usability testing: a review of some methodological and technical aspects of the method},
	volume = {79},
	issn = {13865056},
	shorttitle = {Usability testing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1386505608002098},
	doi = {10.1016/j.ijmedinf.2008.12.004},
	abstract = {The aim of this paper is to review some work conducted in the ﬁeld of user testing that aims at specifying or clarifying the test procedures and at deﬁning and developing tools to help conduct user tests. The topics that have been selected were considered relevant for evaluating applications in the ﬁeld of medical and health care informatics. These topics are: the number of participants that should take part in a user test, the test procedure, remote usability evaluation, usability testing tools, and evaluating mobile applications.},
	language = {en},
	number = {4},
	urldate = {2023-02-02},
	journal = {International Journal of Medical Informatics},
	author = {Bastien, J.M. Christian},
	month = apr,
	year = {2010},
	pages = {e18--e23},
}

@article{kuppusamy_diplomat_nodate,
	title = {Diplomat: {Using} {Delegations} to {Protect} {Community} {Repositories}},
	abstract = {Community repositories, such as Docker Hub, PyPI, and RubyGems, are bustling marketplaces that distribute software. Even though these repositories use common software signing techniques (e.g., GPG and TLS), attackers can still publish malicious packages after a server compromise. This is mainly because a community repository must have immediate access to signing keys in order to certify the large number of new projects that are registered each day.},
	language = {en},
	author = {Kuppusamy, Trishank Karthik and Torres-Arias, Santiago and Diaz, Vladimir and Cappos, Justin},
}

@misc{noauthor_diplomat_nodate,
	title = {Diplomat: {Using} {Delegations} to {Protect} {Community} {Repositories} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/kuppusamy},
	urldate = {2023-02-02},
}

@article{lamb_reproducible_2022,
	title = {Reproducible {Builds}: {Increasing} the {Integrity} of {Software} {Supply} {Chains}},
	volume = {39},
	issn = {1937-4194},
	shorttitle = {Reproducible {Builds}},
	doi = {10.1109/MS.2021.3073045},
	abstract = {Although it is possible to increase confidence in free and open source software by reviewing its source code, trusting code is not the same as trusting its executable counterparts. This article examines reproducible builds, an approach that can determine whether generated binaries correspond to the original source code.},
	number = {2},
	journal = {IEEE Software},
	author = {Lamb, Chris and Zacchiroli, Stefano},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Software},
	keywords = {Buildings, Linux, Metadata, Reproducibility of results, Software, Supply chains, Tools},
	pages = {62--70},
}

@inproceedings{ernawati_it_2012,
	address = {Bandung},
	title = {{IT} risk management framework based on {ISO} 31000:2009},
	isbn = {978-1-4673-2375-8 978-1-4673-2376-5},
	shorttitle = {{IT} risk management framework based on {ISO} 31000},
	url = {https://ieeexplore.ieee.org/document/6339352/},
	doi = {10.1109/ICSEngT.2012.6339352},
	abstract = {Utilization of Information Technology (IT) in an enterprise, in addition to achieve benefit from the implementation of IT should come along with the risks (Information Technology Risk) that may affect the achievement of corporate goals. IT risk management will always involving the company's overall risk management for IT risk will impact enterprise itself, thus a framework is required as a tool to integrate the IT risks with ERM.},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {2012 {International} {Conference} on {System} {Engineering} and {Technology} ({ICSET})},
	publisher = {IEEE},
	author = {Ernawati, Tati and {Suhardi} and Nugroho, Doddi R.},
	month = sep,
	year = {2012},
	pages = {1--8},
}

@article{sepp_neves_towards_2015,
	title = {Towards a common oil spill risk assessment framework – {Adapting} {ISO} 31000 and addressing uncertainties},
	volume = {159},
	issn = {03014797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0301479715300487},
	doi = {10.1016/j.jenvman.2015.04.044},
	abstract = {Oil spills are a transnational problem, and establishing a common standard methodology for Oil Spill Risk Assessments (OSRAs) is thus paramount in order to protect marine environments and coastal communities. In this study we ﬁrstly identiﬁed the strengths and weaknesses of the OSRAs carried out in various parts of the globe. We then searched for a generic and recognized standard, i.e. ISO 31000, in order to design a method to perform OSRAs in a scientiﬁc and standard way. The new framework was tested for the Lebanon oil spill that occurred in 2006 employing ensemble oil spill modeling to quantify the risks and uncertainties due to unknown spill characteristics. The application of the framework generated valuable visual instruments for the transparent communication of the risks, replacing the use of risk tolerance levels, and thus highlighting the priority areas to protect in case of an oil spill.},
	language = {en},
	urldate = {2023-02-02},
	journal = {Journal of Environmental Management},
	author = {Sepp Neves, Antonio Augusto and Pinardi, Nadia and Martins, Flavio and Janeiro, Joao and Samaras, Achilleas and Zodiatis, George and De Dominicis, Michela},
	month = aug,
	year = {2015},
	pages = {158--168},
}

@inproceedings{rabbi_review_2008,
	address = {Phuket, Thailand},
	title = {A {Review} of {Software} {Risk} {Management} for {Selection} of {Best} {Tools} and {Techniques}},
	isbn = {978-0-7695-3263-9},
	url = {http://ieeexplore.ieee.org/document/4617465/},
	doi = {10.1109/SNPD.2008.127},
	abstract = {The aim of this research paper is to study risk management system and to find some tools and techniques recommended by different journals and articles. We have gone through different approaches in context of risk management. We have taken risk management paradigm introduced by Software Engineering Institute as our standard to analyze different techniques and tools. Different features have been mined out from those models and trying to show shortcoming as well as asset qualities of those. Our approach is to find out best or suitable tools for software risk management in software development industries.},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {2008 {Ninth} {ACIS} {International} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking}, and {Parallel}/{Distributed} {Computing}},
	publisher = {IEEE},
	author = {Rabbi, Md. Forhad and Mannan, Khan Olid Bin},
	year = {2008},
	pages = {773--778},
}

@article{rahimi_hybrid_2018,
	title = {Hybrid {Approach} to {Construction} {Project} {Risk} {Management} with {Simultaneous} {FMEA}/{ISO} 31000/{Evolutionary} {Algorithms}: {Empirical} {Optimization} {Study}},
	volume = {144},
	issn = {0733-9364, 1943-7862},
	shorttitle = {Hybrid {Approach} to {Construction} {Project} {Risk} {Management} with {Simultaneous} {FMEA}/{ISO} 31000/{Evolutionary} {Algorithms}},
	url = {https://ascelibrary.org/doi/10.1061/%28ASCE%29CO.1943-7862.0001486},
	doi = {10.1061/(ASCE)CO.1943-7862.0001486},
	abstract = {Uncertainty and risks have been the inherent characteristics of large-scale projects. Although practitioners have applied different project risk management standards, numerous uncertainties, and risks in large-scale construction projects have led to significant failures in fulfilling a project’s goals. Therefore, in this study, a hybrid approach based on failure mode effects analysis (FMEA)/ISO 31000 has been proposed to identify, evaluate, and control the problem effectively. This hybrid approach is not a very accurate approach in providing an appropriate risk response; hence, a mixed-integer programming (MIP) model has been proposed to select the optimized risk response strategies for the project. In the present study, a model based on synergies among project risk responses was developed that is capable of considering the various criteria in the objective function and optimizing them based on the defined projects. Risk response selection for a large-scale project is a complex problem. Because of the nondeterministic polynomial time (NP)-hardness of the presented model, two metaheuristic algorithms, namely, the self-adaptive imperialist competitive algorithm and invasive weed optimization, were developed to solve the proposed MIP model. A large-scale high-rise residential building was evaluated as a case study to investigate the model proposed in this study empirically. DOI: 10.1061/(ASCE)CO.1943-7862.0001486. © 2018 American Society of Civil Engineers.},
	language = {en},
	number = {6},
	urldate = {2023-02-02},
	journal = {Journal of Construction Engineering and Management},
	author = {Rahimi, Yaser and Tavakkoli-Moghaddam, Reza and Iranmanesh, Seyed Hossein and Vaez-Alaei, Maliheh},
	month = jun,
	year = {2018},
	pages = {04018043},
}

@inproceedings{samuel_survivable_2010,
	address = {New York, NY, USA},
	series = {{CCS} '10},
	title = {Survivable key compromise in software update systems},
	isbn = {978-1-4503-0245-6},
	url = {https://doi.org/10.1145/1866307.1866315},
	doi = {10.1145/1866307.1866315},
	abstract = {Today's software update systems have little or no defense against key compromise. As a result, key compromises have put millions of software update clients at risk. Here we identify three classes of information whose authenticity and integrity are critical for secure software updates. Analyzing existing software update systems with our framework, we find their ability to communicate this information securely in the event of a key compromise to be weak or nonexistent. We also find that the security problems in current software update systems are compounded by inadequate trust revocation mechanisms. We identify core security principles that allow software update systems to survive key compromise. Using these ideas, we design and implement TUF, a software update framework that increases resilience to key compromise.},
	urldate = {2023-02-02},
	booktitle = {Proceedings of the 17th {ACM} conference on {Computer} and communications security},
	publisher = {Association for Computing Machinery},
	author = {Samuel, Justin and Mathewson, Nick and Cappos, Justin and Dingledine, Roger},
	month = oct,
	year = {2010},
	keywords = {authentication, delegation, key compromise, key management, revocation, software updates, threshold signatures},
	pages = {61--72},
}

@article{casola_novel_2020,
	title = {A novel {Security}-by-{Design} methodology: {Modeling} and assessing security by {SLAs} with a quantitative approach},
	volume = {163},
	issn = {01641212},
	shorttitle = {A novel {Security}-by-{Design} methodology},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121220300042},
	doi = {10.1016/j.jss.2020.110537},
	language = {en},
	urldate = {2023-02-02},
	journal = {Journal of Systems and Software},
	author = {Casola, Valentina and De Benedictis, Alessandra and Rak, Massimiliano and Villano, Umberto},
	month = may,
	year = {2020},
	pages = {110537},
}

@book{cavoukian_privacy_2013,
	title = {Privacy and security by design: {An} enterprise architecture approach},
	publisher = {Information and Privacy Commissioner of Ontario, Canada},
	author = {Cavoukian, Ann and Dixon, Mark},
	year = {2013},
}

@article{ni_formal_2016,
	title = {A formal model and risk assessment method for security-critical real-time embedded systems},
	volume = {58},
	issn = {01674048},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404816000079},
	doi = {10.1016/j.cose.2016.01.005},
	abstract = {Risk assessment at the early stage of software development can effectively reduce potential security ﬂaws in the software, thus reduce the cost of testing and maintenance. However, there are very few standardized risk assessment methods toward the design models of security-critical RTESs (real-time embedded systems). This paper deﬁnes a formal model called OMR (Object–Message–Role) using Z notation for the security-critical RTESs. Comparing with the existing models for RTESs, OMR is able to specify both the functional and security aspects of the system as an integrated model, which directly provides the input for risk assessment. A risk assessment method RAMES (risk assessment method for embedded systems) based on OMR is then proposed. RAMES is complianced with the risk management process standardized by ISO 31000. To perform the risk analysis in RAMES, an algorithm RAOMR is designed based on the analysis of the message ﬂows and security constraints in OMR. The illustration of a case study shows that RAMES is able to evaluate the risk level of the system model, and locate the high-risky objects and messages.},
	language = {en},
	urldate = {2023-02-01},
	journal = {Computers \& Security},
	author = {Ni, Siru and Zhuang, Yi and Gu, Jingjing and Huo, Ying},
	month = may,
	year = {2016},
	pages = {199--215},
}

@inproceedings{seering_designing_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Designing {User} {Interface} {Elements} to {Improve} the {Quality} and {Civility} of {Discourse} in {Online} {Commenting} {Behaviors}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300836},
	doi = {10.1145/3290605.3300836},
	abstract = {Ensuring high-quality, civil social interactions remains a vexing challenge in many online spaces. In the present work, we introduce a novel approach to address this problem: using psychologically "embedded'' CAPTCHAs containing stimuli intended to prime positive emotions and mindsets. An exploratory randomized experiment (N = 454 Mechanical Turk workers) tested the impact of eight new CAPTCHA designs implemented on a simulated, politically charged comment thread. Results revealed that the two interventions that were the most successful at activating positive affect also significantly increased the positivity of tone and analytical complexity of argumentation in participants' responses. A focused follow-up experiment (N = 120 Mechanical Turk workers) revealed that exposure to CAPTCHAs featuring image sets previously validated to evoke low-arousal positive emotions significantly increased the positivity of sentiment and the levels of complexity and social connectedness in participants' posts. We offer several explanations for these results and discuss the practical and ethical implications of designing interfaces to influence discourse in online forums.},
	urldate = {2023-02-01},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Seering, Joseph and Fang, Tianmi and Damasco, Luca and Chen, Mianhong 'Cherie' and Sun, Likang and Kaufman, Geoff},
	month = may,
	year = {2019},
	keywords = {captchas, commenting, online communities, persuasive design, user interfaces},
	pages = {1--14},
}

@misc{Gu2019BadNets,
	title = {{BadNets}: {Identifying} {Vulnerabilities} in the {Machine} {Learning} {Model} {Supply} {Chain}},
	shorttitle = {{BadNets}},
	url = {http://arxiv.org/abs/1708.06733},
	doi = {10.48550/arXiv.1708.06733},
	abstract = {Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a {\textbackslash}emph\{BadNet\}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of \{25\}{\textbackslash}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.},
	publisher = {arXiv},
	author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{Liang2021PruningandQuantization,
	title = {Pruning and quantization for deep neural network acceleration: {A} survey},
	volume = {461},
	issn = {0925-2312},
	shorttitle = {Pruning and quantization for deep neural network acceleration},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221010894},
	doi = {10.1016/j.neucom.2021.07.045},
	abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.},
	language = {en},
	urldate = {2022-12-09},
	journal = {Neurocomputing},
	author = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
	month = oct,
	year = {2021},
	keywords = {Convolutional neural network, Low-bit mathematics, Neural network acceleration, Neural network pruning, Neural network quantization},
	pages = {370--403},
}

@misc{trust_and_safety_professional_association_new_2020,
	title = {New organizations dedicated to online trust and safety},
	url = {https://www.tspa.org/2020/06/17/new-organizations-dedicated-to-online-trust-and-safety/},
	abstract = {We want to take a moment to acknowledge the fraught times we are living in right now. Some people are awakening to the brutal reality of systemic racism and police violence that others have long experienced. People are on the streets, protesting and demanding social justice. All of this is happening at the same time that people around the world are struggling with and dying from COVID-19, with many more facing extreme economic or mental health hardships.},
	language = {en-US},
	urldate = {2023-01-31},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
	month = jun,
	year = {2020},
}

@misc{kostova_privacy_2020,
	title = {Privacy {Engineering} {Meets} {Software} {Engineering}. {On} the {Challenges} of {Engineering} {Privacy} {ByDesign}},
	url = {http://arxiv.org/abs/2007.08613},
	abstract = {Current day software development relies heavily on the use of service architectures and on agile iterative development methods to design, implement, and deploy systems. These practices result in systems made up of multiple services that introduce new data flows and evolving designs that escape the control of a single designer. Academic privacy engineering literature typically abstracts away such conditions of software production in order to achieve generalizable results. Yet, through a systematic study of the literature, we show that proposed solutions inevitably make assumptions about software architectures, development methods and scope of designer control that are misaligned with current practices. These misalignments are likely to pose an obstacle to operationalizing privacy engineering solutions in the wild. Specifically, we identify important limitations in the approaches that researchers take to design and evaluate privacy enhancing technologies which ripple to proposals for privacy engineering methodologies. Based on our analysis, we delineate research and actions needed to re-align research with practice, changes that serve a precondition for the operationalization of academic privacy results in common software engineering practices.},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Kostova, Blagovesta and Gürses, Seda and Troncoso, Carmela},
	month = jul,
	year = {2020},
	note = {arXiv:2007.08613 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{foukia_pisces_2016,
	title = {{PISCES}: {A} framework for privacy by design in {IoT}},
	shorttitle = {{PISCES}},
	doi = {10.1109/PST.2016.7907022},
	abstract = {We present PISCES (Privacy Incorporated and SeCurity Enhanced Systems) framework, which aims at establishing foundations for implementing Privacy and Security by Design (PSD) in the scope of the Internet of Things (IoT). PISCES operates a strict separation between data provider and data controller, where providers are managers of their data privacy, and controllers are accountable for the privacy and protection of the data provided. This role separation is ensured by the Controller Smart Data System (CSDS) of the Smart Data System (SDS), that handles data along with its privacy settings (metadata), defined by the user, offering the possibility of private data management for IoT. The SDS also balances user privacy against the need to access information in case of law-enforcement organization activities (e.g., police investigations in fight against crime). This is made possible thanks to the building of a Privacy Validation Chain (PVC) allowing the data owner and/or any intermediary (data controllers, data processors) to know easily by whom, and to which purpose, the data is used, thus asserting that the user rights are respected or not. Finally, PISCES is thought for Internet users and service providers to get a reasonable bargain when monetizing user data; it makes necessary to define fair and mutually acceptable conditions for using the services and the data. These conditions can give incentives for the user to allow more access to his data and for the service provider to allow free usage to some services.},
	booktitle = {2016 14th {Annual} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Foukia, Noria and Billard, David and Solana, Eduardo},
	month = dec,
	year = {2016},
	keywords = {Data protection, Electronic mail, Europe, Internet, Minimization, Privacy},
	pages = {706--713},
}

@article{semantha_systematic_2020,
	title = {A {Systematic} {Literature} {Review} on {Privacy} by {Design} in the {Healthcare} {Sector}},
	volume = {9},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/9/3/452},
	doi = {10.3390/electronics9030452},
	abstract = {In this digital age, we are observing an exponential proliferation of sophisticated hardwareand software-based solutions that are able to interact with the users at almost every sensitive aspect of our lives, collecting and analysing a range of data about us. These data, or the derived information out of it, are often too personal to fall into unwanted hands, and thus users are almost always wary of the privacy of such private data that are being continuously collected through these digital mediums. To further complicate the issue, the infringement cases of such databanks are on a sharp rise. Several frameworks have been devised in various parts of the globe to safeguard the issue of data privacy; in parallel, constant research is also being conducted on closing the loopholes within these frameworks. This study aimed to analyse the contemporary privacy by design frameworks to identify the key limitations. Seven contemporary privacy by design frameworks were examined in-depth in this research that was based on a systematic literature review. The result, targeted at the healthcare sector, is expected to produce a high degree of fortiﬁcation against data breaches in the personal information domain.},
	language = {en},
	number = {3},
	urldate = {2023-01-31},
	journal = {Electronics},
	author = {Semantha, Farida Habib and Azam, Sami and Yeo, Kheng Cher and Shanmugam, Bharanidharan},
	month = mar,
	year = {2020},
	pages = {452},
}

@article{rubinstein_privacy_2012,
	title = {Privacy by {Design}: {A} {Counterfactual} {Analysis} of {Google} and {Facebook} {Privacy} {Incidents}},
	issn = {1556-5068},
	shorttitle = {Privacy by {Design}},
	url = {http://www.ssrn.com/abstract=2128146},
	doi = {10.2139/ssrn.2128146},
	abstract = {Regulators here and abroad have embraced “privacy by design” as a critical element of their ongoing revision of current privacy laws. The underlying idea is to “build in” privacy—in the form of Fair Information Practices or (“FIPs”)—when creating software products and services. But FIPs are not self-executing. Rather, privacy by design requires the translation of FIPs into engineering and usability principles and practices. The best way to ensure that software includes the broad goals of privacy as described in the FIPs and any related corporate privacy guidelines is by including them in the definition of software “requirements.” And a main component of making a specification or requirement for software design is to make it concrete, specific, and preferably associated with a metric. Equally important is developing software interfaces and other visual elements that are focused around end-user goals, needs, wants, and constraints.},
	language = {en},
	urldate = {2023-01-31},
	journal = {SSRN Electronic Journal},
	author = {Rubinstein, Ira and Good, Nathan},
	year = {2012},
}

@inproceedings{wong_bringing_2019,
	address = {Glasgow Scotland Uk},
	title = {Bringing {Design} to the {Privacy} {Table}: {Broadening} “{Design}” in “{Privacy} by {Design}” {Through} the {Lens} of {HCI}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Bringing {Design} to the {Privacy} {Table}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300492},
	doi = {10.1145/3290605.3300492},
	abstract = {In calls for privacy by design (PBD), regulators and privacy scholars have investigated the richness of the concept of "privacy." In contrast, "design" in HCI is comprised of rich and complex concepts and practices, but has received much less attention in the PBD context. Conducting a literature review of HCI publications discussing privacy and design, this paper articulates a set of dimensions along which design relates to privacy, including: the purpose of design, which actors do design work in these settings, and the envisioned beneficiaries of design work. We suggest new roles for HCI and design in PBD research and practice: utilizing valuesand critically-oriented design approaches to foreground social values and help define privacy problem spaces. We argue such approaches, in addition to current "design to solve privacy problems" efforts, are essential to the full realization of PBD, while noting the politics involved when choosing design to address privacy.},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wong, Richmond Y. and Mulligan, Deirdre K.},
	month = may,
	year = {2019},
	pages = {1--17},
}

@article{fitera_analysis_nodate,
	title = {Analysis of {DTLS} {Implementations} {Using} {Protocol} {State} {Fuzzing}},
	abstract = {Recent years have witnessed an increasing number of protocols relying on UDP. Compared to TCP, UDP offers performance advantages such as simplicity and lower latency. This has motivated its adoption in Voice over IP, tunneling technologies, IoT, and novel Web protocols. To protect sensitive data exchange in these scenarios, the DTLS protocol has been developed as a cryptographic variation of TLS. DTLS’s main challenge is to support the stateless and unreliable transport of UDP. This has forced protocol designers to make choices that affect the complexity of DTLS, and to incorporate features that need not be addressed in the numerous TLS analyses.},
	language = {en},
	author = {Fitera, Paul and Jonsson, Bengt and Sagonas, Konstantinos and Merget, Robert and Somorovsky, Juraj and de Ruiter, Joeri},
}

@misc{diaspora_diasporadiaspora_nodate,
	title = {diaspora/diaspora},
	url = {https://github.com/diaspora/diaspora},
	abstract = {A privacy-aware, distributed, open source social network.},
	urldate = {2022-03-26},
	publisher = {diaspora},
	author = {{diaspora}},
	note = {original-date: 2010-09-15T05:20:04Z},
	keywords = {decentralized, distributed, federated, hacktoberfest, rails, ruby, social-network},
}

@misc{mastodon_mastodonmastodon_nodate,
	title = {mastodon/mastodon},
	copyright = {AGPL-3.0},
	url = {https://github.com/mastodon/mastodon},
	abstract = {Your self-hosted, globally interconnected microblogging community},
	urldate = {2022-03-26},
	publisher = {Mastodon},
	author = {{mastodon}},
	note = {original-date: 2016-02-22T15:01:25Z},
	keywords = {activity-stream, activitypub, docker, mastodon, microblog, social-network, webfinger},
}

@misc{esafety_safety_nodate,
	title = {Safety by {Design}},
	url = {https://www.esafety.gov.au/industry/safety-by-design},
	abstract = {Safety by Design is about making sure that the foundations and scaffolds are in place to build safe and positive online environments.},
	language = {en-AU},
	urldate = {2023-01-25},
	journal = {eSafety},
	author = {{eSafety}},
}

@inproceedings{alrawi_sok_2019,
	title = {{SoK}: {Security} {Evaluation} of {Home}-{Based} {IoT} {Deployments}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP.2019.00013},
	abstract = {Home-based IoT devices have a bleak reputation regarding their security practices. On the surface, the insecurities of IoT devices seem to be caused by integration problems that may be addressed by simple measures, but this work finds that to be a naive assumption. The truth is, IoT deployments, at their core, utilize traditional compute systems, such as embedded, mobile, and network. These components have many unexplored challenges such as the effect of over-privileged mobile applications on embedded devices. Our work proposes a methodology that researchers and practitioners could employ to analyze security properties for home-based IoT devices. We systematize the literature for home-based IoT using this methodology in order to understand attack techniques, mitigations, and stakeholders. Further, we evaluate 45 devices to augment the systematized literature in order to identify neglected research areas. To make this analysis transparent and easier to adapt by the community, we provide a public portal to share our evaluation data and invite the community to contribute their independent findings.},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Alrawi, Omar and Lever, Chaz and Antonakakis, Manos and Monrose, Fabian},
	month = may,
	year = {2019},
	note = {ISSN: 2375-1207},
	keywords = {Amazon-Echo, Apple-TV, Authentication, Belkin, Cloud-Endpoint, Cortana, Encryption, FireTV, Google-Home, Home-Security-System, Home-based, HomePod, Internet of Things, Internet-of-Things, IoT-Security, Mobile applications, Mobile-Application, Nest-Cam, Network, Password, Phillips-Hue, Privacy, Roku, Roomba, Security, Siri, Smart-Devices, Smart-Lights, Smart-TV, Stakeholders, TP-Link},
	pages = {1362--1380},
}

@article{avizienis_basic_2004,
	title = {Basic concepts and taxonomy of dependable and secure computing},
	volume = {1},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2004.2},
	abstract = {This paper gives the main definitions relating to dependability, a generic concept including a special case of such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
	number = {1},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Avizienis, A. and Laprie, J.-C. and Randell, B. and Landwehr, C.},
	month = jan,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Availability, Books, Communication system security, Fault tolerance, Index Terms- Dependability, Maintenance, Safety, Standardization, Taxonomy, Uncertainty, attacks, errors, failures, fault forecasting., fault removal, fault tolerance, faults, security, trust, vulnerabilities},
	pages = {11--33},
}

@article{ehecatl_morales-trujillo_systematic_2019,
	title = {A {Systematic} {Mapping} {Study} on {Privacy} by {Design} in {Software} {Engineering}},
	copyright = {info:eu-repo/semantics/openAccess},
	issn = {0717-5000},
	url = {https://ruidera.uclm.es/xmlui/handle/10578/22819},
	doi = {10.19153/cleiej.22.1.4},
	abstract = {Protecting personal data in current software systems is a complex issue that requires legal regulations and constraints to manage personal data as well as a methodological support to develop software systems that would safeguard data privacy of their respective users. Privacy by Design (PbD) approach has been proposed to address this issue and has been applied to systems development in a variety of application domains. The aim of this work is to determine the presence of PbD and its extent in software development efforts. A systematic mapping study was conducted in order to identify relevant literature that collects PbD principles and goals in software development as well as methods and/or practices that support privacy aware software development. 53 selected papers address PbD mostly from a theoretical perspective with proposals validation based primarily on experiences or examples. The findings suggest that there is a need to develop privacy-aware methods to be integrated at all stages of software development life cycle and validate them in industrial settings.},
	language = {en},
	urldate = {2023-01-25},
	author = {Ehécatl Morales-Trujillo, Miguel and García-Mireles, Gabriel Alberto and Matla-Cruz, Erick Orlando and Piattini, Mario},
	month = apr,
	year = {2019},
	note = {Accepted: 2019-12-11T11:10:47Z
Publisher: Clei Electronic Journal},
}

@misc{perrino_using_2022,
	title = {Using ‘safety by design’ to address online harms},
	url = {https://www.brookings.edu/techstream/using-safety-by-design-to-address-online-harms/},
	abstract = {Policymakers are increasingly embracing “safety by design” in regulatory and legislative proposals.},
	language = {en-US},
	urldate = {2023-01-25},
	journal = {Brookings},
	author = {Perrino, John},
	month = jul,
	year = {2022},
}

@article{viera_understanding_2005,
	title = {Understanding interobserver agreement: the kappa statistic},
	volume = {37},
	issn = {0742-3225},
	shorttitle = {Understanding interobserver agreement},
	abstract = {Items such as physical exam findings, radiographic interpretations, or other diagnostic tests often rely on some degree of subjective interpretation by observers. Studies that measure the agreement between two or more observers should include a statistic that takes into account the fact that observers will sometimes agree or disagree simply by chance. The kappa statistic (or kappa coefficient) is the most commonly used statistic for this purpose. A kappa of 1 indicates perfect agreement, whereas a kappa of 0 indicates agreement equivalent to chance. A limitation of kappa is that it is affected by the prevalence of the finding under observation. Methods to overcome this limitation have been described.},
	language = {eng},
	number = {5},
	journal = {Family Medicine},
	author = {Viera, Anthony J. and Garrett, Joanne M.},
	month = may,
	year = {2005},
	pmid = {15883903},
	keywords = {Family Practice, Health Services Research, Models, Statistical, Observer Variation, United States},
	pages = {360--363},
}

@misc{galantino_trust_2019,
	title = {Trust \& {Safety} {Engineering} @ {GitHub}},
	url = {https://www.youtube.com/watch?v=UC3Y9rx1jFQ&t=190},
	abstract = {GitHub is the \#1 open source platform in the world, with over 30 million users working in over 100 million repositories. How do we protect our users from harassment while encouraging happy, healthy communities at such a large scale? In this session, I'll introduce the concept of Trust \& Safety work in online platforms, talk a bit about different models used to tackle this problem, and then walk through some engineering challenges and trade-offs faced at GitHub. 

Learning objective: User safety and privacy, just like security, needs to be built into the platform from the ground up. It is the job of every engineer writing user-facing code to understand and use these best practices.

Speaker:

Lexi Galantino
Community \& Safety Engineer, GitHub},
	urldate = {2022-06-11},
	author = {Galantino, Lexi},
	month = may,
	year = {2019},
}

@misc{noauthor_use_nodate,
	title = {The {Use} of {Likely} {Invariants} as {Feedback} for {Fuzzers} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
	urldate = {2023-01-19},
}

@inproceedings{wen_memlock_2020,
	address = {New York, NY, USA},
	series = {{ICSE} '20},
	title = {{MemLock}: memory usage guided fuzzing},
	isbn = {978-1-4503-7121-6},
	shorttitle = {{MemLock}},
	url = {https://doi.org/10.1145/3377811.3380396},
	doi = {10.1145/3377811.3380396},
	abstract = {Uncontrolled memory consumption is a kind of critical software security weaknesses. It can also become a security-critical vulnerability when attackers can take control of the input to consume a large amount of memory and launch a Denial-of-Service attack. However, detecting such vulnerability is challenging, as the state-of-the-art fuzzing techniques focus on the code coverage but not memory consumption. To this end, we propose a memory usage guided fuzzing technique, named MemLock, to generate the excessive memory consumption inputs and trigger uncontrolled memory consumption bugs. The fuzzing process is guided with memory consumption information so that our approach is general and does not require any domain knowledge. We perform a thorough evaluation for MemLock on 14 widely-used real-world programs. Our experiment results show that MemLock substantially outperforms the state-of-the-art fuzzing techniques, including AFL, AFLfast, PerfFuzz, FairFuzz, Angora and QSYM, in discovering memory consumption bugs. During the experiments, we discovered many previously unknown memory consumption bugs and received 15 new CVEs.},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Cheng and Wang, Haijun and Li, Yuekang and Qin, Shengchao and Liu, Yang and Xu, Zhiwu and Chen, Hongxu and Xie, Xiaofei and Pu, Geguang and Liu, Ting},
	month = oct,
	year = {2020},
	keywords = {fuzz testing, memory consumption, software vulnerability},
	pages = {765--777},
}

@inproceedings{cramer_ts_nodate,
	title = {T\&{S} {Engineering} in {OSS} {SMPs}},
	url = {https://www.overleaf.com/project/6320bd0e3583907364821961},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2023-01-18},
	author = {Cramer, Geoffrey},
}

@misc{chaffey_global_2022,
	title = {Global social media statistics research summary 2022},
	url = {https://www.smartinsights.com/social-media-marketing/social-media-strategy/new-global-social-media-research/},
	abstract = {Global social media research summary June 2022: The most recent and relevant statistics to help inform your social media marketing strategy.},
	language = {en-US},
	urldate = {2023-01-16},
	journal = {Smart Insights},
	author = {Chaffey, Dave},
	month = aug,
	year = {2022},
}

@techreport{auxier_social_2021,
	title = {Social {Media} {Use} in 2021},
	url = {https://pewresearch-org-preprod.go-vip.co/internet/2021/04/07/social-media-use-in-2021/},
	abstract = {A majority of Americans say they use YouTube and Facebook, while use of Instagram, Snapchat and TikTok is especially common among adults under 30.},
	language = {en-US},
	urldate = {2023-01-16},
	institution = {Pew Research Center},
	author = {Auxier, Brook and Anderson, Monica},
	month = apr,
	year = {2021},
}

@techreport{mcclain_more_2022,
	title = {More so than adults, {U}.{S}. teens value people feeling safe online over being able to speak freely},
	url = {https://pewresearch-org-preprod.go-vip.co/fact-tank/2022/08/30/more-so-than-adults-u-s-teens-value-people-feeling-safe-online-over-being-able-to-speak-freely/},
	abstract = {A majority of teens say a welcoming, safe online environment is more important than people being able to speak their minds freely online.},
	language = {en-US},
	urldate = {2023-01-16},
	institution = {Pew Research Center},
	author = {Mcclain, Colleen},
	month = aug,
	year = {2022},
}

@techreport{vogels_online_2021,
	title = {Online harassment occurs most often on social media, but strikes in other places, too},
	url = {https://www.pewresearch.org/fact-tank/2021/02/16/online-harassment-occurs-most-often-on-social-media-but-strikes-in-other-places-too/},
	abstract = {Three-quarters of U.S. adults who have recently faced some kind of online harassment say it happened on social media.},
	language = {en-US},
	urldate = {2022-11-02},
	institution = {Pew Research Center},
	author = {Vogels, Emily a},
	month = feb,
	year = {2021},
}

@techreport{vogels_state_2021,
	title = {The {State} of {Online} {Harassment}},
	url = {https://www.pewresearch.org/internet/2021/01/13/the-state-of-online-harassment/},
	abstract = {Roughly four-in-ten Americans have experienced online harassment, with half of this group citing politics as the reason they think they were targeted. Growing shares face more severe online abuse such as sexual harassment or stalking},
	language = {en-US},
	urldate = {2022-11-02},
	institution = {Pew Research Center},
	author = {Vogels, Emily A.},
	month = jan,
	year = {2021},
}

@article{silva_operating_2019,
	title = {Operating {Systems} for {Internet} of {Things} {Low}-{End} {Devices}: {Analysis} and {Benchmarking}},
	volume = {6},
	issn = {2327-4662},
	shorttitle = {Operating {Systems} for {Internet} of {Things} {Low}-{End} {Devices}},
	doi = {10.1109/JIOT.2019.2939008},
	abstract = {In the era of the Internet of Things (IoT), billions of wirelessly connected embedded devices rapidly became part of our daily lives. As a key tool for each Internet-enabled object, embedded operating systems (OSes) provide a set of services and abstractions which eases the development and speedups the deployment of IoT solutions at scale. This article starts by discussing the requirements of an IoT-enabled OS, taking into consideration the major concerns when developing solutions at the network edge, followed by a deep comparative analysis and benchmarking on Contiki-NG, RIOT, and Zephyr. Such OSes were considered as the best representative of their class considering the main key-points that best define an OS for resource-constrained IoT devices: low-power consumption, real-time capabilities, security awareness, interoperability, and connectivity. While evaluating each OS under different network conditions, the gathered results revealed distinct behaviors for each OS feature, mainly due to differences in kernel and network stack implementations.},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Silva, Miguel and Cerdeira, David and Pinto, Sandro and Gomes, Tiago},
	month = dec,
	year = {2019},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Benchmarking, Computer architecture, Hardware, Internet of Things, Internet of Things (IoT), Kernel, Real-time systems, Security, embedded systems, low-end devices, operating systems (OSes)},
	pages = {10375--10383},
}

@misc{mastodon_more_2017,
	title = {More community management needed · {Issue} \#811},
	url = {https://github.com/mastodon/mastodon/issues/811},
	abstract = {With the surge in new users, I think there ought to be more people reviewing reported toots. I\&\#39;ve seen quite a few trolls on the timeline! So who is doing moderation/community management now? c...},
	language = {en},
	urldate = {2023-01-13},
	journal = {GitHub},
	author = {{mastodon}},
	month = apr,
	year = {2017},
}

@misc{noauthor_nyx_nodate,
	title = {Nyx: {Greybox} {Hypervisor} {Fuzzing} using {Fast} {Snapshots} and {Affine} {Types} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/schumilo},
	urldate = {2023-01-13},
}

@misc{noauthor_kafl_nodate,
	title = {{kAFL}: {Hardware}-{Assisted} {Feedback} {Fuzzing} for {OS} {Kernels} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/schumilo},
	urldate = {2023-01-13},
}

@inproceedings{carabas_fuzzing_2017,
	title = {Fuzzing the {Linux} kernel},
	doi = {10.1109/SAI.2017.8252193},
	abstract = {The development of Linux is one of the most prominent examples of free and open-source software collaboration. The kernel is used in many projects including Android which has over 82\% in market share worldwide. Therefore, Linux's kernel security has a lot of impact on IT\&C industry.},
	booktitle = {2017 {Computing} {Conference}},
	author = {Carabas, Costin and Carabas, Mihai},
	month = jul,
	year = {2017},
	keywords = {Computer bugs, Kernel, Linux, Security, Tools, fuzzing, kernel, security},
	pages = {839--843},
}

@misc{noauthor_kafl_nodate-1,
	title = {{kAFL}: {Hardware}-{Assisted} {Feedback} {Fuzzing} for {OS} {Kernels} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/schumilo},
	urldate = {2023-01-13},
}

@inproceedings{robe_pair_2022,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2022},
	title = {Pair programming conversations with agents vs. developers: challenges and opportunities for {SE} community},
	isbn = {978-1-4503-9413-0},
	shorttitle = {Pair programming conversations with agents vs. developers},
	url = {https://doi.org/10.1145/3540250.3549127},
	doi = {10.1145/3540250.3549127},
	abstract = {Recent research has shown feasibility of an interactive pair-programming conversational agent, but implementing such an agent poses three challenges: a lack of benchmark datasets, absence of software engineering specific labels, and the need to understand developer conversations. To address these challenges, we conducted a Wizard of Oz study with 14 participants pair programming with a simulated agent and collected 4,443 developer-agent utterances. Based on this dataset, we created 26 software engineering labels using an open coding process to develop a hierarchical classification scheme. To understand labeled developer-agent conversations, we compared the accuracy of three state-of-the-art transformer-based language models, BERT, GPT-2, and XLNet, which performed interchangeably. In order to begin creating a developer-agent dataset, researchers and practitioners need to conduct resource intensive Wizard of Oz studies. Presently, there exists vast amounts of developer-developer conversations on video hosting websites. To investigate the feasibility of using developer-developer conversations, we labeled a publicly available developer-developer dataset (3,436 utterances) with our hierarchical classification scheme and found that a BERT model trained on developer-developer data performed {\textasciitilde}10\% worse than the BERT trained on developer-agent data, but when using transfer-learning, accuracy improved. Finally, our qualitative analysis revealed that developer-developer conversations are more implicit, neutral, and opinionated than developer-agent conversations. Our results have implications for software engineering researchers and practitioners developing conversational agents.},
	urldate = {2023-01-12},
	booktitle = {Proceedings of the 30th {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Robe, Peter and Kuttal, Sandeep K. and AuBuchon, Jake and Hart, Jacob},
	month = nov,
	year = {2022},
	keywords = {Classification, Conversational agents, Datasets, Labels, Language models, Pair programming conversations, Pair programming questions},
	pages = {319--331},
}

@inproceedings{ebert_communicative_2018,
	title = {Communicative {Intention} in {Code} {Review} {Questions}},
	doi = {10.1109/ICSME.2018.00061},
	abstract = {During code review, developers request clarifications, suggest improvements, or ask for explanations about the rationale behind the implementation choices. We envision the emergence of tools to support developers during code review based on the automatic analysis of the argumentation structure and communicative intentions conveyed by developers' comments. As a preliminary step towards this goal, we conducted an exploratory case study by manually classifying 499 questions extracted from 399 Android code reviews to understand the real communicative intentions they convey. We observed that the majority of questions actually serve information seeking goals. Still, they represent less than half of the annotated sample, with other questions being used to serve a wider variety of developers' communication goals, including suggestions, request for action, and criticism. Based on our findings we formulate hypotheses on communicative intentions in code reviews that should be confirmed or rejected by follow-up studies.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Ebert, Felipe and Castor, Fernando and Novielli, Nicole and Serebrenik, Alexander},
	month = sep,
	year = {2018},
	note = {ISSN: 2576-3148},
	keywords = {Data mining, Guidelines, Labeling, Manuals, Message systems, Software, Tools, questions, communicative intention, code reviews, exploratory case study, Android},
	pages = {519--523},
}

@article{mchugh_interrater_2012,
	title = {Interrater reliability: the kappa statistic},
	volume = {22},
	issn = {1330-0962},
	shorttitle = {Interrater reliability},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/},
	abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen’s kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from −1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen’s suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
	number = {3},
	urldate = {2023-01-10},
	journal = {Biochemia Medica},
	author = {McHugh, Mary L.},
	month = oct,
	year = {2012},
	pmid = {23092060},
	pmcid = {PMC3900052},
	pages = {276--282},
}

@misc{trust_and_safety_professional_association_tspa_nodate,
	title = {{TSPA} {Job} {Board}},
	url = {http://web.archive.org/web/20221219211619/https://www.tspa.org/explore/job-board/},
	abstract = {Planning your next career move and seeking new opportunities in the field of trust and safety? Check out the job postings below and click on any link to apply. Have a job to list? Submit a job by filling out this form. T\&S employees at our corporate supporter companies receive free membership to TSPA. These},
	language = {en-US},
	urldate = {2022-11-04},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
}

@misc{stanford_internet_observatory_trust_nodate,
	title = {Trust and {Safety} {Project}},
	url = {http://web.archive.org/web/20230105001534/https://cyber.fsi.stanford.edu/io/content/sio-trust-and-safety-project},
	language = {en},
	urldate = {2022-06-07},
	author = {{Stanford Internet Observatory}},
}

@misc{the_federation_federation_nodate,
	title = {The {Federation} - a statistics hub},
	url = {https://the-federation.info/},
	abstract = {Node list and statistics for The Federation and Fediverse},
	language = {en},
	urldate = {2022-10-26},
	author = {{The Federation}},
}

@misc{trust_and_safety_foundation_project_case_nodate,
	title = {Case {Studies}},
	url = {http://web.archive.org/web/20230101182825/https://trustandsafetyfoundation.org/case-studies/},
	abstract = {Featured Case Studies All Case Studies},
	language = {en-US},
	urldate = {2022-06-07},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
}

@misc{diaspora_foundation_diaspora_nodate,
	title = {diaspora* {Discourse}},
	url = {https://discourse.diasporafoundation.org/},
	abstract = {Project Discussions and Support},
	language = {en},
	urldate = {2022-04-29},
	journal = {diaspora* Discourse},
	author = {{Diaspora Foundation}},
}

@article{arangoHateSpeechDetection,
	title = {Hate {Speech} {Detection} is {Not} as {Easy} as {You} {May} {Think}: {A} {Closer} {Look} at {Model} {Validation}},
	abstract = {Hate speech is an important problem that is seriously affecting the dynamics and usefulness of online social communities. Large scale social platforms are currently investing important resources into automatically detecting and classifying hateful content, without much success. On the other hand, the results reported by state-of-the-art systems indicate that supervised approaches achieve almost perfect performance but only within specific datasets. In this work, we analyze this apparent contradiction between existing literature and actual applications. We study closely the experimental methodology used in prior work and their generalizability to other datasets. Our findings evidence methodological issues, as well as an important dataset bias. As a consequence, performance claims of the current state-of-the-art have become significantly overestimated. The problems that we have found are mostly related to data overfitting and sampling issues. We discuss the implications for current research and re-conduct experiments to give a more accurate picture of the current state-of-the art methods.},
	language = {en},
	author = {Arango, Aymé and Pérez, Jorge and Poblete, Barbara},
	pages = {9},
}

@misc{trust_and_safety_professional_association_senior_2022,
	title = {Senior {Security} {Engineer}, {Trust} \& {Safety}},
	url = {https://web.archive.org/web/20220808140939/https://www.tspa.org/job/senior-security-engineer-trust-safety/},
	abstract = {This content was reproduced from the employer’s website on July 3, 2022. Please visit their website below for the most up-to-date information about this position. The GitLab DevOps platform empowers 100,000+ organizations to deliver software faster and more efficiently. We are one of the world’s largest all-remote companies with 1,600+ team members and values that guide a culture where people embrace},
	language = {en-US},
	urldate = {2022-11-04},
	journal = {Trust \& Safety Professional Association},
	author = {{Trust and Safety Professional Association}},
	month = jul,
	year = {2022},
}

@misc{samuelson_how_2022,
	title = {How {Pinterest} built its {Trust} \& {Safety} team},
	url = {https://web.archive.org/web/20220912101106/https://medium.com/pinterest-engineering/how-pinterest-built-its-trust-safety-team-8d6c026dd4b9},
	language = {en},
	urldate = {2022-09-21},
	journal = {How Pinterest built its Trust \& Safety team},
	author = {Samuelson, Maisy},
	month = apr,
	year = {2022},
}

@misc{wikimedia_foundation_trust_2022,
	title = {Trust and {Safety}},
	url = {https://web.archive.org/web/20221017105630/http://meta.wikimedia.org/wiki/Trust_and_Safety},
	language = {en},
	urldate = {2022-02-25},
	author = {{Wikimedia Foundation}},
	month = feb,
	year = {2022},
}

@misc{modi_how_2022,
	title = {How our content abuse defense systems work to keep members safe},
	url = {https://web.archive.org/web/20221126095132/https://engineering.linkedin.com/blog/2022/how-our-content-abuse-defense-systems-work-to-keep-members-safe},
	abstract = {To create a safe and trusted experience for our members, our Trust \& Safety (TnS) team strives to keep content that violates our Professional Community Policies off of LinkedIn. In this blog post, we’ll provide insight into how we try to ensure conversations remain respectful and professional on our platform.},
	language = {en},
	urldate = {2022-09-21},
	journal = {LinkedIn Engineering},
	author = {Modi, Sanket},
	month = jan,
	year = {2022},
}

@misc{spectrum_labs_why_2022,
	title = {Why is {Trust} and {Safety} {Important}},
	url = {https://web.archive.org/web/20230108180811/https://www.spectrumlabsai.com/trust-and-safety},
	abstract = {Discover what Trust and Safety is, the metrics for tracking performance, and it can impact customer acquisition, engagement, and retention.},
	language = {en},
	urldate = {2022-02-25},
	author = {{Spectrum Labs}},
	year = {2022},
}

@misc{trust_and_safety_foundation_project_takedowns_2021,
	title = {Takedowns on {YouTube} skyrocket during pandemic as {AI} replaces human moderators (2020)},
	url = {https://web.archive.org/web/20220812124646/https://trustandsafetyfoundation.org/blog/takedowns-on-youtube-skyrocket-during-pandemic-as-ai-replaces-human-moderators-2020/},
	abstract = {Summary: YouTube has always faced an uphill battle when it comes to content moderation. A decade ago, the job was impossible to handle with only human moderators. According to stats […]},
	language = {en-US},
	urldate = {2022-11-09},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = dec,
	year = {2021},
}

@misc{cloudflare_trust_2021,
	title = {Trust \& {Safety} {Engineering} {Team}},
	url = {https://web.archive.org/web/20220128033140/https://www.builtinaustin.com/job/engineer/software-engineer-trust-safety-engineering-team/76783},
	abstract = {Cloudflare is hiring for a Software Engineer - Trust \& Safety Engineering Team in Austin. Find more details about the job and how to apply at Built In Austin.},
	language = {en},
	urldate = {2022-06-11},
	journal = {Built In Austin},
	author = {{Cloudflare}},
	month = dec,
	year = {2021},
}

@misc{trust_and_safety_foundation_project_twitters_2021,
	title = {Twitter's self-deleting tweets feature creates new moderation problems (2020)},
	url = {http://web.archive.org/web/20220707045306/https://trustandsafetyfoundation.org/blog/twitters-self-deleting-tweets-feature-creates-new-moderation-problems-2020/},
	language = {en-US},
	urldate = {2022-11-07},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = sep,
	year = {2021},
}

@misc{xie_building_2021,
	title = {Building a {Label}-{Based} {Enforcement} {Pipeline} for {Trust} \& {Safety}},
	url = {http://web.archive.org/web/20221221111741/https://medium.com/pinterest-engineering/building-a-label-based-enforcement-pipeline-for-trust-safety-4b05a409cb5d},
	abstract = {Sharon Xie{\textbar} Software Engineer, Trust \& Safety},
	language = {en},
	urldate = {2022-09-15},
	journal = {Pinterest Engineering Blog},
	author = {Xie, Sharon},
	month = may,
	year = {2021},
}

@misc{trust_and_safety_foundation_project_facebook_2021,
	title = {Facebook ad policies make it difficult for women’s health startups to advertise (2018)},
	url = {http://web.archive.org/web/20220707005633/https://trustandsafetyfoundation.org/blog/facebook-ad-policies-make-it-difficult-for-womens-health-startups-to-advertise-2018/},
	abstract = {Summary: Going back many years, there have been accusations that Facebook has a double standard when it comes to men’s and women’s health products in advertising. In 2018, a VentureBeat investigation highlighted […]},
	language = {en-US},
	urldate = {2022-11-09},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = may,
	year = {2021},
}

@misc{trust_and_safety_foundation_project_youtubes_2021,
	title = {{YouTube}'s new policy on {Nazi} content results in removal of historical and education videos (2019)},
	url = {http://web.archive.org/web/20230101182807/https://trustandsafetyfoundation.org/blog/youtubes-new-policy-on-nazi-content-results-in-removal-of-historical-and-education-videos-2019/},
	language = {en-US},
	urldate = {2022-11-07},
	journal = {Trust and Safety Foundation Project},
	author = {{Trust and Safety Foundation Project}},
	month = mar,
	year = {2021},
}

@misc{khan_what_2020,
	title = {What is {Trust} and {Safety}?},
	url = {http://web.archive.org/web/20220707103612/https://blogs.gartner.com/akif-khan/what-is-trust-and-safety/},
	abstract = {Trust – noun.  firm belief in the reliability, truth, or ability of someone or something Safety – noun. the condition of being protected from or unlikely to cause danger, risk, or injury Together, these two words – trust and safety – describe the change in attitude and mindset that we have been seeing over the […]},
	language = {en},
	urldate = {2022-02-25},
	journal = {Akif Khan},
	author = {Khan, Akif},
	month = aug,
	year = {2020},
}

@misc{feerst_your_2019,
	title = {Your {Speech}, {Their} {Rules}: {Meet} the {People} {Who} {Guard} the {Internet}},
	shorttitle = {Your {Speech}, {Their} {Rules}},
	url = {http://web.archive.org/web/20221217212157/https://onezero.medium.com/your-speech-their-rules-meet-the-people-who-guard-the-internet-ab58fe6b9231},
	abstract = {Tech platform trust and safety employees are charged with policing the impossible. They open up to Medium’s head of trust and safety.},
	language = {en},
	urldate = {2022-06-07},
	journal = {OneZero},
	author = {Feerst, Alex},
	month = mar,
	year = {2019},
}

@misc{jeong_internet_2018,
	title = {The {Internet} of {Garbage} by {Sarah} {Jeong}},
	url = {http://web.archive.org/web/20221221094434/https://www.theverge.com/2018/8/28/17777330/internet-of-garbage-book-sarah-jeong-online-harassment},
	abstract = {"The internet is, and always has been, mostly garbage," argues Sarah Jeong in her book about the intractable problem of online harassment.},
	language = {en},
	urldate = {2022-07-18},
	journal = {The Verge},
	author = {Jeong, Sarah},
	month = aug,
	year = {2018},
}

@misc{lawson_mastodon_2018,
	title = {Mastodon and the challenges of abuse in a federated system},
	url = {http://web.archive.org/web/20230107102416/https://nolanlawson.com/2018/08/31/mastodon-and-the-challenges-of-abuse-in-a-federated-system/},
	abstract = {This post will probably only make sense to those deeply involved in Mastodon and the fediverse. So if that’s not your thing, or you’re not interested in issues of social media and safet…},
	language = {en},
	urldate = {2022-07-18},
	journal = {Read the Tea Leaves},
	author = {Lawson, Nolan},
	year = {2018},
}

@misc{hunt_trust_2017,
	title = {Trust and safety 101},
	url = {http://web.archive.org/web/20220819234233/https://www.csoonline.com/article/3206127/trust-and-safety-101.html},
	abstract = {Creating a trust and safety team, even if it consists of a small group of part-time employees, can pay dividends in brand equity and user trust.},
	language = {en},
	urldate = {2022-02-25},
	journal = {CSO Online},
	author = {Hunt, Pete},
	month = jul,
	year = {2017},
}

@misc{leong_consensual_2017,
	title = {Consensual {Software}: {How} to {Prioritize} {User} {Safety}},
	shorttitle = {Consensual {Software}},
	url = {http://web.archive.org/web/20230101040655/https://www.infoq.com/articles/consensual-software/},
	abstract = {This article covers how consensual software will help address online harassment and abuse vectors before they get out of hand. It also covers some features the GitHub Community \& Safety team has built and how we review features from other teams.},
	language = {en},
	urldate = {2022-06-03},
	journal = {InfoQ},
	author = {Leong, Danielle},
	month = may,
	year = {2017},
}

@misc{shi_what_2016,
	title = {What the {Heck} is {Trust} and {Safety}?},
	url = {http://web.archive.org/web/20221203164121/https://www.linkedin.com/pulse/what-heck-trust-safety-kenny-shi},
	abstract = {Here are some of the companies that have “Trust and Safety” initiatives: eBay, Airbnb, Twitter, Upwork, TaskRabbit, etc. What is Trust and Safety? How does it compare and contrast with fraud? When I google the word fraud, here is what it says: “wrongful or criminal deception intended to result in fi},
	language = {en},
	urldate = {2022-02-25},
	author = {Shi, Kenny},
	year = {2016},
}

@misc{gitlab_trust_nodate,
	title = {Trust \& {Safety} {Team}},
	url = {http://web.archive.org/web/20220530025357/https://about.gitlab.com/handbook/engineering/security/security-operations/trustandsafety/},
	abstract = {GitLab.com Trust \& Safety Team Overview},
	language = {en},
	urldate = {2022-06-07},
	journal = {GitLab},
	author = {{GitLab}},
}

@misc{leong_adding_2017,
	title = {Adding {Community} \& {Safety} checks to new features},
	url = {http://web.archive.org/web/20221209134254/https://github.blog/2017-01-31-community-and-safety-feature-reviews/},
	abstract = {With the continuous shipping nature at GitHub, it’s easy for the most well-intentioned feature to accidentally become the vector of abuse and harassment. The Community \& Safety engineering team focuses on building community management tools and maintaining user safety, but we also review new features our colleagues have written to ensure there are no accidental […]},
	language = {en-US},
	urldate = {2022-06-11},
	journal = {The GitHub Blog},
	author = {Leong, Danielle},
	month = jan,
	year = {2017},
}

@misc{aufranc_cnxsoft_ti_2012,
	title = {{TI} {Releases} {TI}-{RTOS}, a {Free} {Real} {Time} {Operating} {System} for {MCUs} - {CNX} {Software}},
	url = {https://www.cnx-software.com/2012/12/07/ti-releases-ti-rtos-a-free-real-time-operating-system-for-mcus/},
	abstract = {Texas Instruments announced TI-RTOS, a complete real-time operating system based on a preemptive multithreading kernel for its MCU platforms. TI-RTOSs},
	language = {en-US},
	urldate = {2023-01-09},
	journal = {CNX Software - Embedded Systems News},
	author = {Aufranc (CNXSoft), Jean-Luc},
	month = dec,
	year = {2012},
}

@inproceedings{poncelet_so_2022,
	title = {So {Many} {Fuzzers}, {So} {Little} {Time} : {Experience} from {Evaluating} {Fuzzers} on the {Contiki}-{NG} {Network} ({Hay}){Stack}},
	shorttitle = {So {Many} {Fuzzers}, {So} {Little} {Time}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:ri:diva-61138},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2023-01-06},
	author = {Poncelet, Clément and Sagonas, Konstantinos and Tsiftes, Nicolas},
	year = {2022},
}

@misc{noauthor_afl_nodate,
	title = {{AFL}++ : {Combining} {Incremental} {Steps} of {Fuzzing} {Research} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/woot20/presentation/fioraldi},
	urldate = {2023-01-06},
}

@inproceedings{andronidis_snapfuzz_2022,
	title = {{SnapFuzz}: {An} {Efficient} {Fuzzing} {Framework} for {Network} {Applications}},
	shorttitle = {{SnapFuzz}},
	url = {http://arxiv.org/abs/2201.04048},
	doi = {10.1145/3533767.3534376},
	abstract = {In recent years, fuzz testing has benefited from increased computational power and important algorithmic advances, leading to systems that have discovered many critical bugs and vulnerabilities in production software. Despite these successes, not all applications can be fuzzed efficiently. In particular, stateful applications such as network protocol implementations are constrained by their low fuzzing throughput and the need to develop fuzzing harnesses that reset their state and isolate their side effects. In this paper, we present SnapFuzz, a novel fuzzing framework for network applications. SnapFuzz offers a robust architecture that transforms slow asynchronous network communication into fast synchronous communication, snapshots the target at the latest point at which it is safe to do so, speeds up all file operations by redirecting them to a custom in-memory filesystem, and removes the need for many fragile modifications, such as configuring time delays or writing clean-up scripts, together with several other improvements. Using SnapFuzz, we fuzzed five popular networking applications: LightFTP, TinyDTLS, Dnsmasq, LIVE555 and Dcmqrscp. We report impressive performance speedups of 62.8x, 41.2x, 30.6x, 24.6x, and 8.4x, respectively, with significantly simpler fuzzing harnesses in all cases. Through its performance advantage, SnapFuzz has also found 12 extra crashes compared to AFLNet in these applications.},
	urldate = {2023-01-06},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	author = {Andronidis, Anastasios and Cadar, Cristian},
	month = jul,
	year = {2022},
	note = {arXiv:2201.04048 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	pages = {340--351},
}

@misc{noauthor_tcp-fuzz_nodate,
	title = {{TCP}-{Fuzz}: {Detecting} {Memory} and {Semantic} {Bugs} in {TCP} {Stacks} with {Fuzzing} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/atc21/presentation/zou},
	urldate = {2023-01-05},
}

@article{newman_operating_nodate,
	title = {An {Operating} {System} {Bug} {Exposes} 200 {Million} {Critical} {Devices}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/vxworks-vulnerabilities-urgent11/},
	abstract = {VxWorks is designed as a secure, "real-time" operating system for continuously functioning devices, like medical equipment, elevator controllers, or satellite modems.},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {critical infrastructure, iot, vulnerabilities},
}

@article{greenberg_hackers_nodate,
	title = {Hackers {Remotely} {Kill} a {Jeep} on the {Highway}—{With} {Me} in {It}},
	issn = {1059-1028},
	url = {https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/},
	abstract = {I was driving 70 mph on the edge of downtown St. Louis when the exploit began to take hold.},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Wired},
	author = {Greenberg, Andy},
	note = {Section: tags},
	keywords = {cars, chrysler, cybersecurity, defcon, safety, self-driving cars, threat level, vulnerabilities},
}

@misc{noauthor_amnesia33_nodate,
	title = {{AMNESIA}:33},
	shorttitle = {{AMNESIA}},
	url = {https://www.forescout.com/research-labs/amnesia33/},
	abstract = {AMNESIA:33 AMNESIA:33 Vedere Labs discovered 33 vulnerabilities impacting millions of IoT, OT and IT devices that present an immediate risk for organizations worldwide. Read Report 4 Critical Vulnerabilities 150+ Vendors Affected 1M+ IoT, OT \& IT Devices The Global Impact of AMNESIA:33 AMNESIA:33 is a set of 33 vulnerabilities that impact four open source TCP/IP […]},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {Forescout},
}

@misc{adams_iot_2021,
	title = {{IoT} device attacks double in the first half of 2021, and remote work may shoulder some of the blame},
	url = {https://www.techrepublic.com/article/iot-device-attacks-double-in-the-first-half-of-2021-and-remote-work-may-shoulder-some-of-the-blame/},
	abstract = {The smart home could be ripe for IoT device attacks as cybercriminals rake in record ransomware payments. Remote work may be responsible for the increase in attacks, Kaspersky says.},
	language = {en-US},
	urldate = {2023-01-04},
	journal = {TechRepublic},
	author = {Adams, R. Dallon},
	month = sep,
	year = {2021},
}

@article{hahm_operating_2016,
	title = {Operating {Systems} for {Low}-{End} {Devices} in the {Internet} of {Things}: {A} {Survey}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Operating {Systems} for {Low}-{End} {Devices} in the {Internet} of {Things}},
	doi = {10.1109/JIOT.2015.2505901},
	abstract = {The Internet of Things (IoT) is projected to soon interconnect tens of billions of new devices, in large part also connected to the Internet. IoT devices include both high-end devices which can use traditional go-to operating systems (OSs) such as Linux, and low-end devices which cannot, due to stringent resource constraints, e.g., very limited memory, computational power, and power supply. However, large-scale IoT software development, deployment, and maintenance requires an appropriate OS to build upon. In this paper, we thus analyze in detail the specific requirements that an OS should satisfy to run on low-end IoT devices, and we survey applicable OSs, focusing on candidates that could become an equivalent of Linux for such devices, i.e., a one-size-fits-most, open source OS for low-end IoT devices.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Hahm, Oliver and Baccelli, Emmanuel and Petersen, Hauke and Tsiftes, Nicolas},
	month = oct,
	year = {2016},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Embedded software, Hardware, Internet of Things (IoT), Internet of things, Operating systems, Random access memory, Standards, low-power electronics, operating system (OS)},
	pages = {720--734},
}

@misc{social_web_working_group_activitypub_2018,
	title = {{ActivityPub}},
	url = {https://w3c.github.io/activitypub/#security-considerations},
	urldate = {2023-01-02},
	author = {{Social Web Working Group}},
	month = jan,
	year = {2018},
}

@inproceedings{Wittern2016JSPackageEcosystem,
	address = {Austin Texas},
	title = {A look at the dynamics of the {JavaScript} package ecosystem},
	isbn = {978-1-4503-4186-8},
	url = {https://dl.acm.org/doi/10.1145/2901739.2901743},
	doi = {10.1145/2901739.2901743},
	abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node.js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem’s growth and activity, into conﬂicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
	month = may,
	year = {2016},
	pages = {351--361},
}

@article{aslan_comprehensive_2020,
	title = {A {Comprehensive} {Review} on {Malware} {Detection} {Approaches}},
	doi = {10.1109/ACCESS.2019.2963724},
	abstract = {According to the recent studies, malicious software (malware) is increasing at an alarming rate, and some malware can hide in the system by using different obfuscation techniques. In order to protect computer systems and the Internet from the malware, the malware needs to be detected before it affects a large number of systems. Recently, there have been made several studies on malware detection approaches. However, the detection of malware still remains problematic. Signature-based and heuristic-based detection approaches are fast and efficient to detect known malware, but especially signature-based detection approach has failed to detect unknown malware. On the other hand, behavior-based, model checking-based, and cloud-based approaches perform well for unknown and complicated malware; and deep learning-based, mobile devices-based, and IoT-based approaches also emerge to detect some portion of known and unknown malware. However, no approach can detect all malware in the wild. This shows that to build an effective method to detect malware is a very challenging task, and there is a huge gap for new studies and methods. This paper presents a detailed review on malware detection approaches and recent detection methods which use these approaches. Paper goal is to help researchers to have a general idea of the malware detection approaches, pros and cons of each detection approach, and methods that are used in these approaches.},
	journal = {IEEE Access},
	author = {Aslan, Omer and Samet, Refik},
	year = {2020},
	keywords = {Computer viruses, Cyber security, Encryption, Feature extraction, Internet, malware classification, malware detection approaches, malware features},
}

@article{oikonomou_contiki-ng_2022,
	title = {The {Contiki}-{NG} open source operating system for next generation {IoT} devices},
	volume = {18},
	issn = {2352-7110},
	url = {https://www.softxjournal.com/article/S2352-7110(22)00062-0/fulltext},
	doi = {10.1016/j.softx.2022.101089},
	language = {English},
	urldate = {2022-12-26},
	journal = {SoftwareX},
	author = {Oikonomou, George and Duquennoy, Simon and Elsts, Atis and Eriksson, Joakim and Tanaka, Yasuyuki and Tsiftes, Nicolas},
	month = jun,
	year = {2022},
	note = {Publisher: Elsevier},
	keywords = {Contiki-NG, Internet of Things, Resource-Constrained Devices},
}

@misc{dabic_sampling_2021,
	title = {Sampling {Projects} in {GitHub} for {MSR} {Studies}},
	url = {http://arxiv.org/abs/2103.04682},
	abstract = {Almost every Mining Software Repositories (MSR) study requires, as first step, the selection of the subject software repositories. These repositories are usually collected from hosting services like GitHub using specific selection criteria dictated by the study goal. For example, a study related to licensing might be interested in selecting projects explicitly declaring a license. Once the selection criteria have been defined, utilities such as the GitHub APIs can be used to "query" the hosting service. However, researchers have to deal with usage limitations imposed by these APIs and a lack of required information. For example, the GitHub search APIs allow 30 requests per minute and, when searching repositories, only provide limited information (e.g., the number of commits in a repository is not included). To support researchers in sampling projects from GitHub, we present GHS (GitHub Search), a dataset containing 25 characteristics (e.g., number of commits, license, etc.) of 735,669 repositories written in 10 programming languages. The set of characteristics has been derived by looking for frequently used project selection criteria in MSR studies and the dataset is continuously updated to (i) always provide fresh data about the existing projects, and (ii) increase the number of indexed projects. The GHS dataset can be queried through a web application we built that allows to set many combinations of selection criteria needed for a study and download the information of matching repositories: https://seart-ghs.si.usi.ch.},
	language = {en},
	urldate = {2022-12-23},
	publisher = {arXiv},
	author = {Dabic, Ozren and Aghajani, Emad and Bavota, Gabriele},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04682 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bui_vul4j_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{Vul4J}: a dataset of reproducible {Java} vulnerabilities geared towards the study of program repair techniques},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{Vul4J}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528482},
	doi = {10.1145/3524842.3528482},
	abstract = {In this work we present Vul4J, a Java vulnerability dataset where each vulnerability is associated to a patch and, most importantly, to a Proof of Vulnerability (PoV) test case. We analyzed 1803 fix commits from 912 real-world vulnerabilities in the Project KB knowledge base to extract the reproducible vulnerabilities, i.e., vulnerabilities that can be triggered by one or more PoV test cases. To this aim, we ran the test suite of the application in both, the vulnerable and secure versions, to identify the corresponding PoVs. Furthermore, if no PoV test case was spotted, then we wrote it ourselves. As a result, Vul4J includes 79 reproducible vulnerabilities from 51 open-source projects, spanning 25 different Common Weakness Enumeration (CWE) types. To the extent of our knowledge, this is the first dataset of its kind created for Java. Particularly, it targets the study of Automated Program Repair (APR) tools, where PoVs are often necessary in order to identify plausible patches. We made our dataset and related tools publically available on GitHub.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Bui, Quang-Cuong and Scandariato, Riccardo and Ferreyra, Nicolás E. Díaz},
	month = may,
	year = {2022},
	pages = {464--468},
}

@inproceedings{buchkova_dasea_2022,
	address = {Pittsburgh Pennsylvania},
	title = {{DaSEA}: a dataset for software ecosystem analysis},
	isbn = {978-1-4503-9303-4},
	shorttitle = {{DaSEA}},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528004},
	doi = {10.1145/3524842.3528004},
	abstract = {Software package managers facilitate reuse and rapid construction of software systems. Since evermore software is distributed via package managers, researchers and practitioners require explicit data of software dependency networks that are opaquely formed by dependency relations between software packages. To reason about increasingly complex software products and ecosystems, researchers and practitioners rely either on publicly available datasets like the seemingly unattended libraries.io [14] or they mine problem-specific data from software ecosystems repeatedly and non-transparently. Therefore, we present the DaSEA dataset, which contains metadata of software packages, their versions, and dependencies from multiple ecosystems (currently six programming languages and five operating system package managers). Alongside the dataset, we provide an extensible open-source tool under the same name that is used to create updated versions of the DaSEA dataset allowing studies of evolution of software ecosystems.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Buchkova, Petya and Hinnerskov, Joakim Hey and Olsen, Kasper and Pfeiffer, Rolf-Helge},
	month = may,
	year = {2022},
	pages = {388--392},
}

@inproceedings{zacchiroli_large-scale_2022,
	address = {Pittsburgh Pennsylvania},
	title = {A large-scale dataset of (open source) license text variants},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3528491},
	doi = {10.1145/3524842.3528491},
	abstract = {We introduce a large-scale dataset of the complete texts of free/open source software (FOSS) license variants. To assemble it we have collected from the Software Heritage archive—the largest publicly available archive of FOSS source code with accompanying development history—all versions of files whose names are commonly used to convey licensing terms to software users and developers. The dataset consists of 6.5 million unique license files that can be used to conduct empirical studies on open source licensing, training of automated license classifiers, natural language processing (NLP) analyses of legal texts, as well as historical and phylogenetic studies on FOSS licensing.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Zacchiroli, Stefano},
	month = may,
	year = {2022},
	pages = {757--761},
}

@misc{grotov_large-scale_2022,
	title = {A {Large}-{Scale} {Comparison} of {Python} {Code} in {Jupyter} {Notebooks} and {Scripts}},
	url = {http://arxiv.org/abs/2203.16718},
	abstract = {In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. In this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.},
	language = {en},
	urldate = {2022-12-23},
	publisher = {arXiv},
	author = {Grotov, Konstantin and Titov, Sergey and Sotnikov, Vladimir and Golubev, Yaroslav and Bryksin, Timofey},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16718 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{hartel_operationalizing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Operationalizing threats to {MSR} studies by simulation-based testing},
	isbn = {978-1-4503-9303-4},
	url = {https://dl.acm.org/doi/10.1145/3524842.3527960},
	doi = {10.1145/3524842.3527960},
	abstract = {Quantitative studies on the border between Mining Software Repository (MSR) and Empirical Software Engineering (ESE) apply data analysis methods, like regression modeling, statistic tests or correlation analysis, to commits or pulls to better understand the software development process. Such studies assure the validity of the reported results by following a sound methodology. However, with increasing complexity, parts of the methodology can still go wrong. This may result in MSR/ESE studies with undetected threats to validity. In this paper, we propose to systematically protect against threats by operationalizing their treatment using simulations. A simulation substitutes observed and unobserved data, related to an MSR/ESE scenario, with synthetic data, carefully defined according to plausible assumptions on the scenario. Within a simulation, unobserved data becomes transparent, which is the key difference to a real study, necessary to detect threats to an analysis methodology. Running an analysis methodology on synthetic data may detect basic technical bugs and misinterpretations, but it also improves the trust in the methodology. The contribution of a simulation is to operationalize testing the impact of important assumptions. Assumptions still need to be rated for plausibility. We evaluate simulation-based testing by operationalizing undetected threats in the context of four published MSR/ESE studies. We recommend that future research uses such more systematic treatment of threats, as a contribution against the reproducibility crisis.},
	language = {en},
	urldate = {2022-12-23},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Härtel, Johannes and Lämmel, Ralf},
	month = may,
	year = {2022},
	pages = {86--97},
}

@article{obie_violation_nodate,
	title = {On the {Violation} of {Honesty} in {Mobile} {Apps}: {Automated} {Detection} and {Categories}},
	abstract = {Human values such as integrity, privacy, curiosity, security, and honesty are guiding principles for what people consider important in life. Such human values may be violated by mobile software applications (apps), and the negative effects of such human value violations can be seen in various ways in society. In this work, we focus on the human value of honesty. We present a model to support the automatic identification of violations of the value of honesty from app reviews from an end-user perspective. Beyond the automatic detection of honesty violations by apps, we also aim to better understand different categories of honesty violations expressed by users in their app reviews. The result of our manual analysis of our honesty violations dataset shows that honesty violations can be characterised into ten categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. Based on these results, we argue for a conscious effort in developing more honest software artefacts including mobile apps, and the promotion of honesty as a key value in software development practices. Furthermore, we discuss the role of app distribution platforms as enforcers of ethical systems supporting human values, and highlight some proposed next steps for human values in software engineering (SE) research.},
	language = {en},
	author = {Obie, Humphrey O and Ilekura, Idowu and Du, Hung},
}

@inproceedings{thomas_sok_2021,
	title = {{SoK}: {Hate}, {Harassment}, and the {Changing} {Landscape} of {Online} {Abuse}},
	shorttitle = {{SoK}},
	url = {https://ieeexplore.ieee.org/document/9519435},
	doi = {10.1109/SP40001.2021.00028},
	abstract = {We argue that existing security, privacy, and antiabuse protections fail to address the growing threat of online hate and harassment. In order for our community to understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy draws on over 150 interdisciplinary research papers that cover disparate threats ranging from intimate partner violence to coordinated mobs. In the process, we identify seven classes of attacks—such as toxic content and surveillance—that each stem from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey that hate and harassment is a pervasive, growing experience for online users, particularly for at-risk communities like young adults and people who identify as LGBTQ+. Responding to each class of hate and harassment requires a unique strategy and we highlight five such potential research directions that ultimately empower individuals, communities, and platforms to do so.},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Thomas, Kurt and Akhawe, Devdatta and Bailey, Michael and Boneh, Dan and Bursztein, Elie and Consolvo, Sunny and Dell, Nicola and Durumeric, Zakir and Kelley, Patrick Gage and Kumar, Deepak and McCoy, Damon and Meiklejohn, Sarah and Ristenpart, Thomas and Stringhini, Gianluca},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Cognition, Computer security, Distance measurement, Privacy, Social networking (online), Taxonomy, at-risk, emerging-threats, harassment, hate},
	pages = {247--267},
}

@misc{tunstall_announcing_nodate,
	title = {Announcing {Evaluation} on the {Hub}},
	url = {https://huggingface.co/blog/eval-on-the-hub},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-12-19},
	author = {Tunstall, Lewis and Thakur, Abhishek and Thrush, Tristan and Luccioni, Sasha and Werra, Leandro von and Rajani, Nazneen and Piktus, Ola and Sanseviero, Omar and Kiela, Douwe},
}

@misc{koscik_identifying_2018,
	title = {Identifying {Abuse} {Vectors}},
	url = {https://web.archive.org/web/20220818200307/https://spinecone.gitbooks.io/identifying-abuse-vectors/content/},
	urldate = {2022-06-02},
	author = {Koscik, Terian},
	year = {2018},
}

@misc{noauthor_snpsfuzzer_nodate,
	title = {{SNPSFuzzer}: {A} {Fast} {Greybox} {Fuzzer} for {Stateful} {Network} {Protocols} using {Snapshots}},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{strubell_energy_2020,
	title = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},
	doi = {10.1609/aaai.v34i09.7123},
	abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
	language = {en},
	number = {09},
	urldate = {2022-12-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = apr,
	year = {2020},
	note = {Number: 09},
	pages = {13693--13696},
}

@article{thompson_facebook_2022,
	chapter = {Technology},
	title = {Facebook {Failed} to {Stop} {Ads} {Threatening} {Election} {Workers}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2022/12/01/technology/facebook-ads-threats.html},
	abstract = {The ads, submitted by researchers, were rejected by YouTube and TikTok.},
	language = {en-US},
	urldate = {2022-12-05},
	journal = {The New York Times},
	author = {Thompson, Stuart A.},
	month = dec,
	year = {2022},
	keywords = {Computers and the Internet, Facebook Inc, Global Witness, Meta Platforms Inc, Midterm Elections (2022), New York University, Online Advertising, Right-Wing Extremism and Alt-Right, Rumors and Misinformation, Social Media, Threats and Threatening Messages, TikTok (ByteDance), YouTube.com},
}

@article{thompson_reflections_1984,
	title = {Reflections on trusting trust},
	volume = {27},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/358198.358210},
	doi = {10.1145/358198.358210},
	abstract = {To what extent should one trust a statement that a program is free of Trojan horses? Perhaps it is more important to trust the people who wrote the software.},
	language = {en},
	number = {8},
	urldate = {2022-12-05},
	journal = {Communications of the ACM},
	author = {Thompson, Ken},
	month = aug,
	year = {1984},
	pages = {761--763},
}

@misc{team_product_2022,
	title = {Product {Surveys}: {The} {Do}'s and {Don}'ts},
	shorttitle = {Product {Surveys}},
	url = {https://trustmary.com/surveys/product-surveys-dos-and-donts/},
	abstract = {Being able to create product surveys that are on point is not a walk in the park. This article will help you tackle the pain points.},
	language = {en-US},
	urldate = {2022-11-06},
	journal = {Trustmary},
	author = {team, Trustmary},
	month = feb,
	year = {2022},
}

@article{noauthor_surviving_nodate,
	title = {Surviving {Software} {Dependencies}},
	language = {en},
	pages = {24},
}

@article{potvin_why_2016,
	title = {Why {Google} stores billions of lines of code in a single repository},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2854146},
	doi = {10.1145/2854146},
	abstract = {Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world.},
	language = {en},
	number = {7},
	urldate = {2022-12-05},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	month = jun,
	year = {2016},
	pages = {78--87},
}

@article{pike_interpreting_2005,
	title = {Interpreting the {Data}: {Parallel} {Analysis} with {Sawzall}},
	volume = {13},
	issn = {1058-9244, 1875-919X},
	shorttitle = {Interpreting the {Data}},
	url = {http://www.hindawi.com/journals/sp/2005/962135/abs/},
	doi = {10.1155/2005/962135},
	abstract = {Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design – including the separation into two phases, the form of the programming language, and the properties of the aggregators – exploits the parallelism inherent in having data and computation distributed across many machines.},
	language = {en},
	number = {4},
	urldate = {2022-12-05},
	journal = {Scientific Programming},
	author = {Pike, Rob and Dorward, Sean and Griesemer, Robert and Quinlan, Sean},
	year = {2005},
	pages = {277--298},
}

@article{winters_software_nodate,
	title = {Software {Engineering} at {Google}},
	language = {en},
	author = {Winters, Titus and Manshreck, Tom and Wright, Hyrum},
	pages = {602},
}

@book{noauthor_notitle_nodate,
}

@book{schorlemmer_sok_2022,
	title = {Sok: {Analysis} of {Software} {Supply} {Chain} {Security} by {Establishing} {Secure} {Design} {Properties}},
	volume = {1},
	isbn = {978-1-4503-9885-5},
	publisher = {Association for Computing Machinery},
	author = {Schorlemmer, Taylor R and Davis, James C},
	year = {2022},
	doi = {10.1145/3560835.3564556},
	note = {Publication Title: Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses (SCORED '22), November 11, 2022, Los Angeles, CA, USA
Issue: 1},
	keywords = {Software Supply Chain Attacks, Security Properties},
}

@book{gopalakrishna__nodate,
	title = {“ {If} security is required ”: {Engineering} and {Security} {Practices} for {Machine} {Learning}-based {IoT} {Devices}},
	volume = {1},
	isbn = {978-1-4503-9332-4},
	publisher = {Association for Computing Machinery},
	author = {Gopalakrishna, Nikhil Krishna and Bland, Forrest Lee and Davis, James C},
	doi = {10.1145/3528227.3528565},
	note = {Publication Title: 4th International Workshop on Software Engineering Research and Practice for the IoT (SERP4IoT'22), May 19, 2022, Pittsburgh, PA, USA
Issue: 1},
	keywords = {Internet of Things, Machine Learning, Security and, cyber-, internet of things, machine learning, security and privacy},
}

@book{newman_sigstore_nodate,
	title = {Sigstore : {Software} {Signing} for {Everybody}},
	volume = {1},
	isbn = {978-1-4503-9450-5},
	publisher = {Association for Computing Machinery},
	author = {Newman, Zachary and Meyers, John Speed},
	doi = {10.1145/3548606.3560596},
	note = {Publication Title: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (CCS '22), November 7â•ﬁ11, 2022, Los Angeles, CA, USA
Issue: 1},
	keywords = {2022, acm reference format, and santiago torres-arias, code signing, distributed systems, john speed meyers, security, software transparency, zachary newman},
}

@book{synovic_empirical_nodate,
	title = {An {Empirical} {Study} of {Artifacts} and {Security} {Risks} in the {Pre}-trained {Model} {Supply} {Chain}},
	volume = {1},
	isbn = {978-1-4503-9885-5},
	publisher = {Association for Computing Machinery},
	author = {Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R and Thiruvathukal, George K and Davis, James C},
	doi = {10.1145/3560835.3564547},
	note = {Publication Title: Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses (SCORED '22), November 11, 2022, Los Angeles, CA, USA
Issue: 1},
	keywords = {Deep neural networks, Empirical software engineering, Machine learning, Model hubs, Software reuse, Software supply chain, acm reference format, deep neural networks, empirical software engineering, machine learning, model hubs, software reuse, software supply chain},
}

@article{whitten_why_2005,
	title = {Why {Johnny} {Can} ’ t {Encrypt}},
	volume = {1999},
	abstract = {U ER ERRORS CAUSE OR CONTRIBUTE TO MOST COMPUTER SECURITY FAILURES, yet user interfaces for security still tend to be clumsy, confusing, or near nonexistent. Is this simply because of a failure to apply standard user interface design techniques to security? We argue that, on the contrary, effective security requires a different usability standard, and that it will not be achieved through the user interface design techniques appropriate to other types of consumer software.1 To test this hypothesis, we performed a case study of a security program that does have a good user interface by general standards: PGP 5.0. Our case study used a cognitive walkthrough analysis together with a laboratory user test to evaluate whether PGP 5.0 can be used successfully by cryptography novices to achieve effective electronic mail security. The analysis found a number of user interface design flaws that may contribute to security failures, and the user test demonstrated that when our test participants were given 90 minutes in which to sign and encrypt a message using PGP 5.0, the majority of them were unable to do so successfully. We conclude that PGP 5.0 is not usable enough to provide effective security for most computer users, despite its attractive graphical user interface, supporting our hypothesis that user interface design for effective security remains an open problem. We close with a brief description of our continuing work on the development and application of user interface design principles and techniques for security.},
	number = {October},
	journal = {Security},
	author = {Whitten, a and Tygar, Jd},
	year = {2005},
	note = {ISBN: 05960082799780596008277},
	keywords = {definition, deriving usability standard for pgp, why johnny can't encrypt},
	pages = {679--702},
}

@misc{noauthor_notitle_nodate-1,
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{lella_enisa_2021,
	title = {{ENISA} threat landscape for supply chain attacks.},
	url = {https://data.europa.eu/doi/10.2824/168593},
	abstract = {This report aims at mapping and studying the supply chain attacks that were discovered from January 2020 to early July 2021. Based on the trends and patterns observed, supply chain attacks increased in number and sophistication in the year 2020 and this trend is continuing in 2021, posing an increasing risk for organizations. It is estimated that there will be four times more supply chain attacks in 2021 than in 2020. With half of the attacks being attributed to Advanced Persistence Threat (APT) actors, their complexity and resources greatly exceed the more common non-targeted attacks, and, therefore, there is an increasing need for new protective methods that incorporate suppliers in order to guarantee that organizations remain secure.},
	language = {en},
	urldate = {2022-06-21},
	institution = {European Union Agency for Cybersecurity},
	editor = {Lella, Ifigeneia and Theocharidou, Marianthi and Tsekmezoglou, Eleni and Malatras, Apostolos},
	month = jul,
	year = {2021},
	keywords = {Government},
}

@techreport{enduring_security_framework_securing_2022,
	title = {Securing the {Software} {Supply} {Chain}: {Recommended} {Practices} {Guide} for {Customers}},
	url = {https://media.defense.gov/2022/Nov/17/2003116445/-1/-1/0/ESF_SECURING_THE_SOFTWARE_SUPPLY_CHAIN_CUSTOMER.PDF},
	institution = {Cybersecurity and Infrastructure Security Agency},
	author = {Enduring Security Framework},
	month = oct,
	year = {2022},
}

@misc{noauthor_supply-chain_nodate,
	title = {Supply-chain {Levels} for {Software} {Artifacts}},
	url = {http://slsa.dev/},
	abstract = {Security framework to ensure software supply chain integrity},
	language = {en},
	urldate = {2022-11-29},
	journal = {SLSA},
}

@misc{white_house_executive_2021,
	title = {Executive {Order} on {Improving} the {Nation}'s {Cybersecurity}},
	url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/},
	abstract = {By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:Section 1.},
	language = {en-US},
	urldate = {2022-11-29},
	journal = {The White House},
	author = {White House},
	month = may,
	year = {2021},
}

@misc{cimpanu_microsoft_2020,
	title = {Microsoft, {FireEye} confirm {SolarWinds} supply chain attack},
	url = {https://www.zdnet.com/article/microsoft-fireeye-confirm-solarwinds-supply-chain-attack/},
	abstract = {Known victims so far include the US Treasury, the US NTIA, and FireEye itself.},
	language = {en},
	urldate = {2022-11-29},
	journal = {ZDNET},
	author = {Cimpanu, Catalin},
	month = dec,
	year = {2020},
}

@misc{microsoft_security_response_center_customer_2020,
	title = {Customer {Guidance} on {Recent} {Nation}-{State} {Cyber} {Attacks} – {Microsoft} {Security} {Response} {Center}},
	url = {https://msrc-blog.microsoft.com/2020/12/13/customer-guidance-on-recent-nation-state-cyber-attacks/},
	language = {en-US},
	urldate = {2022-11-29},
	author = {{Microsoft Security Response Center}},
	month = dec,
	year = {2020},
}

@techreport{nissen_deliver_2018,
	title = {Deliver {Uncompromised}: {A} {Strategy} for {Supply} {Chain} {Security} and {Resilience}},
	url = {https://www.mitre.org/sites/default/files/2021-11/prs-18-2417-deliver-uncompromised-MITRE-study-26AUG2019.pdf},
	language = {en},
	institution = {The MITRE Center for Technology \& National Security},
	author = {Nissen, Christopher and Gronager, John and Metzger, Robert and Rishikof, Harvey},
	month = aug,
	year = {2018},
	pages = {56},
}

@misc{noauthor_deliver_2018,
	title = {Deliver {Uncompromised}: {A} {Strategy} for {Supply} {Chain} {Security} and {Resilience} in {Response} to the {Changing} {Character} of {War}},
	shorttitle = {Deliver {Uncompromised}},
	url = {https://www.mitre.org/news-insights/publication/deliver-uncompromised-strategy-supply-chain-security-and-resilience},
	abstract = {The nature of warfare is changing, bringing new threats to the defense supply chain that must be addressed. This report examines options that span legislation and regulation, policy and adminis¬tration, acquisition and oversight, programs and technology.},
	language = {en},
	urldate = {2022-11-29},
	journal = {MITRE},
	month = aug,
	year = {2018},
}

@article{kim_promoting_2022,
	title = {Promoting {Online} {Civility} {Through} {Platform} {Architecture}},
	volume = {1},
	copyright = {Copyright (c) 2022 Journal of Online Trust and Safety},
	issn = {2770-3142},
	url = {https://tsjournal.org/index.php/jots/article/view/54},
	doi = {10.54501/jots.v1i4.54},
	abstract = {This study tests whether the architecture of a social media platform can encourage conversations among users to be more civil. It was conducted in collaboration with Nextdoor, a networking platform for neighbors within a defined geographic area. The study involved: (1) prompting users to move popular posts from the neighborhood-wide feed to new groups dedicated to the topic and (2) an experiment that randomized the announcement of community guidelines to members who join those newly formed groups. We examined the impact of each intervention on the level of civility, moral values reflected in user comments, and user’s submitted reports of inappropriate content. In a large quantitative analysis of comments posted to Nextdoor, the results indicate that platform architecture can shape the civility of conversations. Comments within groups were more civil and less frequently reported to Nextdoor moderators than the comments on the neighborhood-wide posts. In addition, comments in groups where new members were shown guidelines were less likely to be reported to moderators and were expressed in a more morally virtuous tone than comments in groups where new members were not presented with guidelines. This research demonstrates the importance of considering the design, structure, and affordance of the online environment when online platforms seek to promote civility and other pro-social behaviors.},
	language = {en},
	number = {4},
	urldate = {2022-11-22},
	journal = {Journal of Online Trust and Safety},
	author = {Kim, Jisu and McDonald, Curtis and Meosky, Paul and Katsaros, Matthew and Tyler, Tom},
	month = sep,
	year = {2022},
	note = {Number: 4},
	keywords = {Social media},
}

@incollection{taibi_resem_2022,
	address = {Cham},
	title = {Resem: {Searching} {Regular} {Expression} {Patterns} with {Semantics} and {Input}/{Output} {Examples}},
	volume = {13709},
	isbn = {978-3-031-21387-8 978-3-031-21388-5},
	shorttitle = {Resem},
	url = {https://link.springer.com/10.1007/978-3-031-21388-5_35},
	abstract = {Regular expression is widely known as a powerful and generalpurpose text processing tool for programming. Though the regular expression is highly versatile, there are various diﬃculties in using them. One promising approach to reduce the burden of the pattern composition is reuse by referring to past usages. Still, several source code-specialized search engines have been proposed, they are not suitable for the scenario of reusing regular expression patterns. The purpose of this study is the eﬃcient reuse of regular expression patterns. To achieve the purpose, we propose a usage retrieval system Resem specialized in regular expression patterns. Resem adopts two key features: search by semantics and collecting input/output examples. Resem will smoothly connect what to do to how to do in the implementation process of string manipulation.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Takeshige, Hiroki and Matsumoto, Shinsuke and Kusumoto, Shinji},
	editor = {Taibi, Davide and Kuhrmann, Marco and Mikkonen, Tommi and Klünder, Jil and Abrahamsson, Pekka},
	year = {2022},
	doi = {10.1007/978-3-031-21388-5_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {511--517},
}

@inproceedings{padhye_jqf_2019,
	address = {New York, NY, USA},
	series = {{ISSTA} 2019},
	title = {{JQF}: coverage-guided property-based testing in {Java}},
	isbn = {9781450362245},
	shorttitle = {{JQF}},
	url = {https://doi.org/10.1145/3293882.3339002},
	doi = {10.1145/3293882.3339002},
	abstract = {We present JQF, a platform for performing coverage-guided fuzz testing in Java. JQF is designed both for practitioners, who wish to find bugs in Java programs, as well as for researchers, who wish to implement new fuzzing algorithms. Practitioners write QuickCheck-style test methods that take inputs as formal parameters. JQF instruments the test program's bytecode and continuously executes tests using inputs that are generated in a coverage-guided fuzzing loop. JQF's input-generation mechanism is extensible. Researchers can implement custom fuzzing algorithms by extending JQF's Guidance interface. A Guidance instance responds to code coverage events generated during the execution of a test case, such as function calls and conditional jumps, and provides the next input. We describe several guidances that currently ship with JQF, such as: semantic fuzzing with Zest, binary fuzzing with AFL, and complexity fuzzing with PerfFuzz. JQF is a mature tool that is open-source and publicly available. At the time of writing, JQF has been successful in discovering 42 previously unknown bugs in widely used open-source software such as OpenJDK, Apache Commons, and the Google Closure Compiler.},
	urldate = {2022-11-21},
	publisher = {Association for Computing Machinery},
	author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik},
	month = jul,
	year = {2019},
	keywords = {Coverage-guided fuzzing, QuickCheck, property-based testing},
	pages = {398--401},
}

@misc{noauthor_kahns_2016,
	title = {Kahn's algorithm for {Topological} {Sorting}},
	url = {https://www.geeksforgeeks.org/topological-sorting-indegree-based-solution/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-us},
	urldate = {2022-11-21},
	journal = {GeeksforGeeks},
	month = apr,
	year = {2016},
	note = {Section: Graph},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-11-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@article{kotti_impact_2022,
	title = {Impact of {Software} {Engineering} {Research} in {Practice}: {A} {Patent} and {Author} {Survey} {Analysis}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {Impact of {Software} {Engineering} {Research} in {Practice}},
	url = {http://arxiv.org/abs/2204.03366},
	doi = {10.1109/TSE.2022.3208210},
	abstract = {Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4,354 SE patents citing 1,690 SE papers published in four leading SE venues between 1975-2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26\% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.},
	urldate = {2022-11-18},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kotti, Zoe and Gousios, Georgios and Spinellis, Diomidis},
	year = {2022},
	note = {arXiv:2204.03366 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {1--19},
}

@inproceedings{schumilo_nyx-net_2022,
	address = {New York, NY, USA},
	series = {{EuroSys} '22},
	title = {Nyx-net: network fuzzing with incremental snapshots},
	isbn = {9781450391627},
	shorttitle = {Nyx-net},
	url = {https://doi.org/10.1145/3492321.3519591},
	doi = {10.1145/3492321.3519591},
	abstract = {Coverage-guided fuzz testing ("fuzzing") has become mainstream and we have observed lots of progress in this research area recently. However, it is still challenging to efficiently test network services with existing coverage-guided fuzzing methods. In this paper, we introduce the design and implementation of Nyx-Net, a novel snapshot-based fuzzing approach that can successfully fuzz a wide range of targets spanning servers, clients, games, and even Firefox's Inter-Process Communication (IPC) interface. Compared to state-of-the-art methods, Nyx-Net improves test throughput by up to 300x and coverage found by up to 70\%. Additionally, Nyx-Net is able to find crashes in two of ProFuzzBench's targets that no other fuzzer found previously. When using Nyx-Net to play the game Super Mario, Nyx-Net shows speedups of 10--30x compared to existing work. Moreover, Nyx-Net is able to find previously unknown bugs in servers such as Lighttpd, clients such as MySQL client, and even Firefox's IPC mechanism---demonstrating the strength and versatility of the proposed approach. Lastly, our prototype implementation was awarded a \$20.000 bug bounty for enabling fuzzing on previously unfuzzable code in Firefox and solving a long-standing problem at Mozilla.},
	urldate = {2022-11-17},
	publisher = {Association for Computing Machinery},
	author = {Schumilo, Sergej and Aschermann, Cornelius and Jemmett, Andrea and Abbasi, Ali and Holz, Thorsten},
	month = mar,
	year = {2022},
	keywords = {fuzzing, software security, testing},
	pages = {166--180},
}

@inproceedings{chen_sfuzz_2022,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {{SFuzz}: {Slice}-based {Fuzzing} for {Real}-{Time} {Operating} {Systems}},
	isbn = {9781450394505},
	shorttitle = {{SFuzz}},
	url = {https://doi.org/10.1145/3548606.3559367},
	doi = {10.1145/3548606.3559367},
	abstract = {Real-Time Operating System (RTOS) has become the main category of embedded systems. It is widely used to support tasks requiring real-time response such as printers and switches. The security of RTOS has been long overlooked as it was running in special environments isolated from attackers. However, with the rapid development of IoT devices, tremendous RTOS devices are connected to the public network. Due to the lack of security mechanisms, these devices are extremely vulnerable to a wide spectrum of attacks. Even worse, the monolithic design of RTOS combines various tasks and services into a single binary, which hinders the current program testing and analysis techniques working on RTOS. In this paper, we propose SFuzz, a novel slice-based fuzzer, to detect security vulnerabilities in RTOS. Our insight is that RTOS usually divides a complicated binary into many separated but single-minded tasks. Each task accomplishes a particular event in a deterministic way and its control flow is usually straightforward and independent. Therefore, we identify such code from the monolithic RTOS binary and synthesize a slice for effective testing. Specifically, SFuzz first identifies functions that handle user input, constructs call graphs that start from callers of these functions, and leverages forward slicing to build the execution tree based on the call graphs and pruning the paths independent of external inputs. Then, it detects and handles roadblocks within the coarse-grain scope that hinder effective fuzzing, such as instructions unrelated to the user input. And then, it conducts coverage-guided fuzzing on these code snippets. Finally, SFuzz leverages forward and backward slicing to track and verify each path constraint and determine whether a bug discovered in the fuzzer is a real vulnerability. SFuzz successfully discovered 77 zero-day bugs on 35 RTOS samples, and 67 of them have been assigned CVE or CNVD IDs. Our empirical evaluation shows that SFuzz outperforms the state-of-the-art tools (e.g., UnicornAFL) on testing RTOS.},
	urldate = {2022-11-17},
	publisher = {Association for Computing Machinery},
	author = {Chen, Libo and Cai, Quanpu and Ma, Zhenbang and Wang, Yanhao and Hu, Hong and Shen, Minghang and Liu, Yue and Guo, Shanqing and Duan, Haixin and Jiang, Kaida and Xue, Zhi},
	month = nov,
	year = {2022},
	keywords = {concolic execution, rtos, slice-based fuzzing, taint analysis},
	pages = {485--498},
}

@inproceedings{cappos_look_2008,
	address = {New York, NY, USA},
	series = {{CCS} '08},
	title = {A look in the mirror: attacks on package managers},
	isbn = {978-1-59593-810-7},
	shorttitle = {A look in the mirror},
	url = {https://doi.org/10.1145/1455770.1455841},
	doi = {10.1145/1455770.1455841},
	abstract = {This work studies the security of ten popular package managers. These package managers use different security mechanisms that provide varying levels of usability and resilience to attack. We find that, despite their existing security mechanisms, all of these package managers have vulnerabilities that can be exploited by a man-in-the-middle or a malicious mirror. While all current package managers suffer from vulnerabilities, their security is also positively or negatively impacted by the distribution's security practices. Weaknesses in package managers are more easily exploited when distributions use third-party mirrors as official mirrors. We were successful in using false credentials to obtain an official mirror on all five of the distributions we attempted. We also found that some security mechanisms that control where a client obtains metadata and packages from may actually decrease security. We analyze current package managers to show that by exploiting vulnerabilities, an attacker with a mirror can compromise or crash hundreds to thousands of clients weekly. The problems we disclose are now being corrected by many different package manager maintainers.},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security},
	publisher = {Association for Computing Machinery},
	author = {Cappos, Justin and Samuel, Justin and Baker, Scott and Hartman, John H.},
	month = oct,
	year = {2008},
	keywords = {mirrors, package management, replay attack},
	pages = {565--574},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-11-15},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{newman_sigstore_2022,
	address = {New York, NY, USA},
	series = {{CCS} '22},
	title = {Sigstore: {Software} {Signing} for {Everybody}},
	isbn = {978-1-4503-9450-5},
	shorttitle = {Sigstore},
	url = {https://doi.org/10.1145/3548606.3560596},
	doi = {10.1145/3548606.3560596},
	abstract = {Software supply chain compromises are on the rise. From the effects of XCodeGhost to SolarWinds, hackers have identified that targeting weak points in the supply chain allows them to compromise high-value targets such as U.S. government agencies and corporate targets such as Google and Microsoft. Software signing, a promising mitigation for many of these attacks, has seen limited adoption in open-source and enterprise ecosystems. In this paper, we propose Sigstore, a system to provide widespread software signing capabilities. To do so, we designed the system to provide baseline artifact signing capabilities that minimize the adoption barrier for developers. To this end, Sigstore leverages three distinct mechanisms: First, it uses a protocol similar to ACME to authenticate developers through OIDC, tying signatures to existing and widely-used identities. Second, it enables developers to use ephemeral keys to sign their artifacts, reducing the inconvenience and risk of key management. Finally, Sigstore enables user authentication by means of artifact and identity logs, bringing transparency to software signatures. Sigstore is quickly becoming a critical piece of Internet infrastructure with more than 2.2M signatures over critical software such as Kubernetes and Distroless.},
	urldate = {2022-11-14},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Newman, Zachary and Meyers, John Speed and Torres-Arias, Santiago},
	month = nov,
	year = {2022},
	keywords = {code signing, distributed systems, security, software transparency},
	pages = {2353--2367},
}

@inproceedings{kariyappa_maze_2021,
	address = {Nashville, TN, USA},
	title = {{MAZE}: {Data}-{Free} {Model} {Stealing} {Attack} {Using} {Zeroth}-{Order} {Gradient} {Estimation}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{MAZE}},
	url = {https://ieeexplore.ieee.org/document/9577631/},
	doi = {10.1109/CVPR46437.2021.01360},
	abstract = {High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE – a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.},
	language = {en},
	urldate = {2022-11-08},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kariyappa, Sanjay and Prakash, Atul and Qureshi, Moinuddin K},
	month = jun,
	year = {2021},
	pages = {13809--13818},
}

@misc{li_data_2022,
	title = {Data {Stealing} {Attack} on {Medical} {Images}: {Is} it {Safe} to {Export} {Networks} from {Data} {Lakes}?},
	shorttitle = {Data {Stealing} {Attack} on {Medical} {Images}},
	url = {http://arxiv.org/abs/2206.03391},
	abstract = {In privacy-preserving machine learning, it is common that the owner of the learned model does not have any physical access to the data. Instead, only a secured remote access to a data lake is granted to the model owner without any ability to retrieve data from the data lake. Yet, the model owner may want to export the trained model periodically from the remote repository and a question arises whether this may cause is a risk of data leakage. In this paper, we introduce the concept of data stealing attack during the export of neural networks. It consists in hiding some information in the exported network that allows the reconstruction outside the data lake of images initially stored in that data lake. More precisely, we show that it is possible to train a network that can perform lossy image compression and at the same time solve some utility tasks such as image segmentation. The attack then proceeds by exporting the compression decoder network together with some image codes that leads to the image reconstruction outside the data lake. We explore the feasibility of such attacks on databases of CT and MR images, showing that it is possible to obtain perceptually meaningful reconstructions of the target dataset, and that the stolen dataset can be used in turns to solve a broad range of tasks. Comprehensive experiments and analyses show that data stealing attacks should be considered as a threat for sensitive imaging data sources.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Li, Huiyu and Ayache, Nicholas and Delingette, Hervé},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03391 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{orekondy_prediction_2020,
	title = {Prediction {Poisoning}: {Towards} {Defenses} {Against} {DNN} {Model} {Stealing} {Attacks}},
	shorttitle = {Prediction {Poisoning}},
	url = {http://arxiv.org/abs/1906.10908},
	abstract = {High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85\${\textbackslash}times\$ with minimal impact on the utility for benign users.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Orekondy, Tribhuvanesh and Schiele, Bernt and Fritz, Mario},
	month = mar,
	year = {2020},
	note = {arXiv:1906.10908 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{laorden_threat_2010,
	address = {Berlin, Heidelberg},
	series = {Advances in {Intelligent} and {Soft} {Computing}},
	title = {A {Threat} {Model} {Approach} to {Threats} and {Vulnerabilities} in {On}-line {Social} {Networks}},
	abstract = {On-line Social Networks (OSN) have become one of the most used Internet services. However, as happens with every new technology, they are prone to several security issues. Despite privacy concerns begin to emerge, there are still other dangerous vulnerabilities that affect security and threaten organisations and users assets. In this paper, we present the first Threat Modelling approach in Online Social Networks that intends to identify the threats and vulnerabilities that can be exploited. Next, we define what we call the Circle of Risk (CoR), a graphical definition of every security aspect involved in the threat modelling.},
	language = {en},
	booktitle = {Computational {Intelligence} in {Security} for {Information} {Systems} 2010},
	publisher = {Springer},
	author = {Laorden, Carlos and Sanz, Borja and Alvarez, Gonzalo and Bringas, Pablo G.},
	editor = {Herrero, Álvaro and Corchado, Emilio and Redondo, Carlos and Alonso, Ángel},
	year = {2010},
	keywords = {On-line Social Networks, privacy, threat modelling, web security},
}

@misc{wikipedia_fediverse_2022,
	title = {Fediverse},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Fediverse&oldid=1117783694},
	abstract = {The fediverse (a portmanteau of "federation" and "universe") is an ensemble of federated (i.e. interconnected) servers that are used for web publishing (i.e. social networking, microblogging, blogging, or websites) and file hosting, but which, while independently hosted, can communicate with each other. 
On different servers (instances), users can create so-called identities. These identities are able to communicate over the boundaries of the instances because the software running on the servers supports one or more communication protocols that follow an open standard. As an identity on the fediverse, users are able to post text and other media, or to follow posts by other identities. In some cases, users can show or share data (video, audio, text, and other files) publicly or to a selected group of identities and allow other identities to edit other users' data (such as a calendar or an address book).},
	language = {en},
	urldate = {2022-10-26},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1117783694},
}

@misc{wikipedia_activity_2022,
	title = {Activity stream},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Activity_stream&oldid=1109477763},
	abstract = {An activity stream is a list of recent activities performed by an individual, typically on a single website. For example, Facebook's News Feed is an activity stream. Since the introduction of the News Feed on September 6, 2006, other major websites have introduced similar implementations for their own users. Since the proliferation of activity streams on websites, there have been calls to standardize the format so that websites could interact with a stream provided by another website. The Activity Streams project, for example, is an effort to develop an activity stream protocol to syndicate activities across social web applications. Several major websites with activity stream implementations have already opened up their activity streams to developers to use, including Facebook and MySpace.Though activity stream arises from social networking, nowadays it has become an essential part of business software. Enterprise social software is used in different types of companies to organize their internal communication and acts as an addition to traditional corporate intranet. Collaboration software like Jive Software, Yammer, and Chatter offer activity stream as a separate product. At the same time other software providers such as tibbr, Central Desktop, and Wrike offer activity stream as an integrated part of their collaboration software solution.Activity streams come in two different variations:

Generic feeds: all users see the same content in the activity stream.
Personalised feeds: each user gets bespoke items as well as custom ranking of each element in the feed.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1109477763},
}

@misc{wikipedia_comparison_2022,
	title = {Comparison of microblogging and similar services},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Comparison_of_microblogging_and_similar_services&oldid=1085241013},
	abstract = {The tables below compare general and technical information for some notable active microblogging services, and also social network services that have status updates.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = apr,
	year = {2022},
	note = {Page Version ID: 1085241013},
}

@article{marwick_far-right_2022,
	title = {Far-{Right} {Online} {Radicalization}: {A} {Review} of the {Literature}},
	shorttitle = {Far-{Right} {Online} {Radicalization}},
	url = {https://citap.pubpub.org/pub/jq7l6jny/release/1},
	doi = {10.21428/bfcb0bff.e9492a11},
	abstract = {This literature review examines cross-disciplinary work on radicalization to situate, historicize, frame, and better understand the present concerns around online radicalization and far-right extremist and fringe movements. We find that research on radicalization is inextricably linked to the post-9/11 context in which it emerged, and as a result is overly focused on studying the other. Applying this research to the spread of far-right ideas online does not account for the ways in which the far-right’s endorsement of white supremacy and racism holds historical, normative precedent in the United States. Further, radicalization research is rife with uncertainties, ranging from definitional ambiguity to an inability to identify any simplistic, causal models capable of fully explaining the conditions under which radicalization occurs. Instead, there are multiple possible pathways to radicalization, and while the internet does not cause individuals to adopt far-right extremist or fringe beliefs, some technological affordances may aid adoption of these beliefs through gradual processes of socialization. We conclude that the term “radicalization” does not serve as a useful analytical frame for studying the spread of far-right and fringe ideas online. Instead, potential analytical frameworks better suited to studying these phenomena include theories prominent in the study of online communities, conversion, mainstreaming, and sociotechnical theories of media effects.},
	language = {en},
	urldate = {2022-11-04},
	journal = {The Bulletin of Technology \& Public Life},
	author = {Marwick, Alice and Clancy, Benjamin and Furl, Katherine},
	month = may,
	year = {2022},
}

@misc{masnick_hey_2022,
	title = {Hey {Elon}: {Let} {Me} {Help} {You} {Speed} {Run} {The} {Content} {Moderation} {Learning} {Curve}},
	url = {https://www.techdirt.com/2022/11/02/hey-elon-let-me-help-you-speed-run-the-content-moderation-learning-curve/},
	abstract = {It’s kind of a rite of passage for any new social media network. They show up, insist that they’re the “platform for free speech” without quite understanding what that actua…},
	language = {en-US},
	urldate = {2022-11-04},
	journal = {Techdirt},
	author = {Masnick, Mike},
	month = nov,
	year = {2022},
}

@misc{singhalSoKContentModeration2022,
	title = {{SoK}: {Content} {Moderation} in {Social} {Media}, from {Guidelines} to {Enforcement}, and {Research} to {Practice}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2206.14855},
	doi = {10.48550/arXiv.2206.14855},
	abstract = {To counter online abuse and misinformation, social media platforms have been establishing content moderation guidelines and employing various moderation policies. The goal of this paper is to study these community guidelines and moderation practices, as well as the relevant research publications to identify the research gaps, differences in moderation techniques, and challenges that should be tackled by the social media platforms and the research community at large. In this regard, we study and analyze in the US jurisdiction the fourteen most popular social media content moderation guidelines and practices, and consolidate them. We then introduce three taxonomies drawn from this analysis as well as covering over one hundred interdisciplinary research papers about moderation strategies. We identified the differences between the content moderation employed in mainstream social media platforms compared to fringe platforms. We also highlight the implications of Section 230, the need for transparency and opacity in content moderation, why platforms should shift from a one-size-fits-all model to a more inclusive model, and lastly, we highlight why there is a need for a collaborative human-AI system.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Singhal, Mohit and Ling, Chen and Paudel, Pujan and Thota, Poojitha and Kumarswamy, Nihal and Stringhini, Gianluca and Nilizadeh, Shirin},
	month = oct,
	year = {2022},
	note = {arXiv:2206.14855 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@article{memon_role_2018,
	title = {The role of online social networking on deliberate self-harm and suicidality in adolescents: {A} systematized review of literature},
	volume = {60},
	issn = {0019-5545},
	shorttitle = {The role of online social networking on deliberate self-harm and suicidality in adolescents},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6278213/},
	doi = {10.4103/psychiatry.IndianJPsychiatry_414_17},
	abstract = {Social media use by minors has significantly increased and has been linked to depression and suicidality. Simultaneously, age-adjusted suicide rates have steadily increased over the past decade in the United States with suicide being the second most common cause of death in youth. Hence, the increase in suicide rate parallels the simultaneous increase in social media use. In addition, the rate of nonsuicidal self-injury ranges between 14\% and 21\% among young people. Evidence suggests that self-harming youth is more active on online social networks than youth who do not engage in self-harm behavior. The role of online social networking on deliberates self-harm and suicidality in adolescents with a focus on negative influence was assessed by conducting a systematized literature review. A literature search on “PubMed” and “Ovid Medline” using a combination of MeSH terms yielded nine articles for data extraction satisfying predefined inclusion/exclusion criteria. It was found that social networking websites are utilized by suicidal and self-harming youth as a medium to communicate with and to seek social support from other users. Online social networking also leads to increased exposure to and engagement in self-harm behavior due to users receiving negative messages promoting self-harm, emulating self-injurious behavior of others, and adopting self-harm practices from shared videos. Greater time spent on social networking websites led to higher psychological distress, an unmet need for mental health support, poor self-rated mental health, and increased suicidal ideation. In conclusion, greater time spent on online social networking promotes self-harm behavior and suicidal ideation in vulnerable adolescents.},
	number = {4},
	urldate = {2022-11-02},
	journal = {Indian Journal of Psychiatry},
	author = {Memon, Aksha M. and Sharma, Shiva G. and Mohite, Satyajit S. and Jain, Shailesh},
	year = {2018},
	pmid = {30581202},
	pmcid = {PMC6278213},
	pages = {384--392},
}

@misc{international_standards_organization_iso_2018,
	title = {{ISO} 31000:2018(en), {Risk} management — {Guidelines}},
	url = {https://www.iso.org/obp/ui/#iso:std:iso:31000},
	urldate = {2022-07-07},
	author = {{International Standards Organization}},
	year = {2018},
}

@inproceedings{xiao_integrated_2003,
	title = {Integrated {TCP}/{IP} protocol software testing for vulnerability detection},
	doi = {10.1109/ICCNMC.2003.1243061},
	abstract = {Many security holes stem from the defects in network protocol implementations. This paper presents an industry best practice of integrated TCP/IP network protocol testing that targets software robustness vulnerabilities. The deployed test system consists of a versatile test engine, a protocol data unit generator and a few auxiliary tools. The specially designed kernel test engine supporting IP/TCP/UDP as carrier protocols drives predefined fault-injected PDU (protocol data unit) to the network unit under test. Its novel callback mechanism and virtual network device connection capability cost-effectively enhance user controlled testing intelligence for verifying protocols with complicated state transitions. The PDU generator aims to provide a systematic solution for rapid test case creation, which is based on new strengthened BNF (Backus-Naur form) language for protocol specification mutation and fault injection. Established on this system, we propose an integrated industry test environment for network protocol code assessment. Initial experiments and case studies with multicast protocols unveiled several robustness violations, which have significant security impacts.},
	booktitle = {2003 {International} {Conference} on {Computer} {Networks} and {Mobile} {Computing}, 2003. {ICCNMC} 2003.},
	author = {Xiao, Shu and Deng, Lijun and Li, Sheng and Wang, Xiangrong},
	month = oct,
	year = {2003},
	keywords = {Best practices, Computer industry, Data security, Engines, IP networks, Protocols, Robustness, Software testing, System testing, TCPIP},
	pages = {311--319},
}

@inproceedings{corteggiani_inception_2018,
	title = {Inception: \{{System}-{Wide}\} {Security} {Testing} of \{{Real}-{World}\} {Embedded} {Systems} {Software}},
	isbn = {9781939133045},
	shorttitle = {Inception},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/corteggiani},
	urldate = {2022-10-27},
	author = {Corteggiani, Nassim and Camurati, Giovanni and Francillon, Aurélien},
	year = {2018},
	pages = {309--326},
}

@inproceedings{gros_refuzz_2022,
	address = {Nagasaki Japan},
	title = {{ReFuzz} - {Structure} {Aware} {Fuzzing} of the {Resilient} {File} {System} ({ReFS})},
	isbn = {978-1-4503-9140-5},
	url = {https://dl.acm.org/doi/10.1145/3488932.3523260},
	doi = {10.1145/3488932.3523260},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {Proceedings of the 2022 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Groß, Tobias and Schleier, Tobias and Müller, Tilo},
	month = may,
	year = {2022},
	pages = {589--601},
}

@inproceedings{aschermann_redqueen_2019,
	address = {San Diego, CA},
	title = {{REDQUEEN}: {Fuzzing} with {Input}-to-{State} {Correspondence}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{REDQUEEN}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_04A-2_Aschermann_paper.pdf},
	doi = {10.14722/ndss.2019.23371},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Aschermann, Cornelius and Schumilo, Sergej and Blazytko, Tim and Gawlik, Robert and Holz, Thorsten},
	year = {2019},
}

@inproceedings{ma_you_2021,
	address = {Chicago IL USA},
	title = {“{You} have said too much”: {Java}-like verbosity anti-patterns in {Python} codebases},
	isbn = {978-1-4503-9089-7},
	shorttitle = {“{You} have said too much”},
	url = {https://dl.acm.org/doi/10.1145/3484272.3484960},
	doi = {10.1145/3484272.3484960},
	abstract = {As a popular language for teaching introductory programming, Java can profoundly influence beginner programmers with its coding style and idioms. Despite its many advantages, the paradigmatic coding style in Java is often described as verbose. As a result, when writing code in more concise languages, such programmers tend to emulate the familiar Java coding idioms, thus neglecting to take advantage of the more succinct counterparts in those languages. As a result of such verbosity, not only the overall code quality suffers, but the verbose non-idiomatic patterns also render code hard to understand and maintain. In this paper, we study the incidences of Java-like verbosity as they occur in Python codebases. We present a collection of Java-Like Verbosity Anti-patterns and our pilot study of their presence in representative open-source Python codebases. We discuss our findings as a call for action to computing educators, particularly those who work with introductory students. We need novel pedagogical interventions that encourage budding programmers to write concise idiomatic code in any language.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 2021 {ACM} {SIGPLAN} {International} {Symposium} on {SPLASH}-{E}},
	publisher = {ACM},
	author = {Ma, Yuzhi and Tilevich, Eli},
	month = oct,
	year = {2021},
	pages = {13--18},
}

@article{Jiang2022PTMSupplyChain,
	title = {An {Empirical} {Study} of {Artifacts} and {Security} {Risks} in the {Pre}-trained {Model} {Supply} {Chain}},
	abstract = {Deep neural networks achieve state-of-the-art performance on many tasks, but require increasingly complex architectures and costly training procedures. Engineers can reduce costs by reusing a pre-trained model (PTM) and fine-tuning it for their own tasks. To facilitate software reuse, engineers collaborate around model hubs, collections of PTMs and datasets organized by problem domain. Although model hubs are now comparable in popularity and size to other software ecosystems, the associated PTM supply chain has not yet been examined from a software engineering perspective.},
	language = {en},
	journal = {Los Angeles},
	author = {Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan},
	year = {2022},
	pages = {10},
}

@misc{noauthor_biggest_nodate,
	title = {Biggest social media platforms 2022},
	url = {https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/},
	abstract = {Facebook, YouTube, and WhatsApp are the most popular social networks worldwide, each with at least two billion active users.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Statista},
}

@inproceedings{li_testing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Testing machine learning systems in industry: an empirical study},
	isbn = {978-1-4503-9226-6},
	shorttitle = {Testing machine learning systems in industry},
	url = {https://dl.acm.org/doi/10.1145/3510457.3513036},
	doi = {10.1145/3510457.3513036},
	language = {en},
	urldate = {2022-10-24},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Li, Shuyue and Guo, Jiaqi and Lou, Jian-Guang and Fan, Ming and Liu, Ting and Zhang, Dongmei},
	month = may,
	year = {2022},
	pages = {263--272},
}

@inproceedings{wang_profactory_2022,
	title = {\{{ProFactory}\}: {Improving} \{{IoT}\} {Security} via {Formalized} {Protocol} {Customization}},
	isbn = {9781939133311},
	shorttitle = {\{{ProFactory}\}},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/wang-fei},
	urldate = {2022-10-21},
	author = {Wang, Fei and Wu, Jianliang and Nan, Yuhong and Aafer, Yousra and Zhang, Xiangyu and Xu, Dongyan and Payer, Mathias},
	year = {2022},
	pages = {3879--3896},
}

@article{noauthor_extraction_nodate,
	title = {Extraction and {Management} of {Rationale}},
	language = {en},
	pages = {3},
}

@article{li_unifuzz_nodate,
	title = {{UNIFUZZ}: {A} {Holistic} and {Pragmatic} {Metrics}-{Driven} {Platform} for {Evaluating} {Fuzzers}},
	abstract = {A ﬂurry of fuzzing tools (fuzzers) have been proposed in the literature, aiming at detecting software vulnerabilities effectively and efﬁciently. To date, it is however still challenging to compare fuzzers due to the inconsistency of the benchmarks, performance metrics, and/or environments for evaluation, which buries the useful insights and thus impedes the discovery of promising fuzzing primitives. In this paper, we design and develop UNIFUZZ, an open-source and metrics-driven platform for assessing fuzzers in a comprehensive and quantitative manner. Speciﬁcally, UNIFUZZ to date has incorporated 35 usable fuzzers, a benchmark of 20 real-world programs, and six categories of performance metrics. We ﬁrst systematically study the usability of existing fuzzers, ﬁnd and ﬁx a number of ﬂaws, and integrate them into UNIFUZZ. Based on the study, we propose a collection of pragmatic performance metrics to evaluate fuzzers from six complementary perspectives. Using UNIFUZZ, we conduct in-depth evaluations of several prominent fuzzers including AFL [1], AFLFast [2], Angora [3], Honggfuzz [4], MOPT [5], QSYM [6], T-Fuzz [7] and VUzzer64 [8]. We ﬁnd that none of them outperforms the others across all the target programs, and that using a single metric to assess the performance of a fuzzer may lead to unilateral conclusions, which demonstrates the signiﬁcance of comprehensive metrics. Moreover, we identify and investigate previously overlooked factors that may signiﬁcantly affect a fuzzer’s performance, including instrumentation methods and crash analysis tools. Our empirical results show that they are critical to the evaluation of a fuzzer. We hope that our ﬁndings can shed light on reliable fuzzing evaluation, so that we can discover promising fuzzing primitives to effectively facilitate fuzzer designs in the future.},
	language = {en},
	author = {Li, Yuwei and Ji, Shouling and Chen, Yuan and Liang, Sizhuang and Lee, Wei-Han and Chen, Yueyao and Lyu, Chenyang and Wu, Chunming and Beyah, Raheem and Cheng, Peng and Lu, Kangjie and Wang, Ting},
	pages = {18},
}

@misc{zhou_unifuzz_2020,
	title = {{UniFuzz}: {Optimizing} {Distributed} {Fuzzing} via {Dynamic} {Centralized} {Task} {Scheduling}},
	shorttitle = {{UniFuzz}},
	url = {http://arxiv.org/abs/2009.06124},
	doi = {10.48550/arXiv.2009.06124},
	abstract = {Fuzzing is one of the most efficient technology for vulnerability detection. Since the fuzzing process is computing-intensive and the performance improved by algorithm optimization is limited, recent research seeks to improve fuzzing performance by utilizing parallel computing. However, parallel fuzzing has to overcome challenges such as task conflicts, scalability in a distributed environment, synchronization overhead, and workload imbalance. In this paper, we design and implement UniFuzz, a distributed fuzzing optimization based on a dynamic centralized task scheduling. UniFuzz evaluates and distributes seeds in a centralized manner to avoid task conflicts. It uses a "request-response" scheme to dynamically distribute fuzzing tasks, which avoids workload imbalance. Besides, UniFuzz can adaptively switch the role of computing cores between evaluating, and fuzzing, which avoids the potential bottleneck of seed evaluation. To improve synchronization efficiency, UniFuzz shares different fuzzing information in a different way according to their characteristics, and the average overhead of synchronization is only about 0.4{\textbackslash}\%. We evaluated UniFuzz with real-world programs, and the results show that UniFuzz outperforms state-of-the-art tools, such as AFL, PAFL and EnFuzz. Most importantly, the experiment reveals a counter-intuitive result that parallel fuzzing can achieve a super-linear acceleration to the single-core fuzzing. We made a detailed explanation and proved it with additional experiments. UniFuzz also discovered 16 real-world vulnerabilities.},
	urldate = {2022-10-20},
	author = {Zhou, Xu and Wang, Pengfei and Liu, Chenyifan and Yue, Tai and Liu, Yingying and Song, Congxi and Lu, Kai and Yin, Qidi},
	month = sep,
	year = {2020},
	note = {arXiv:2009.06124 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{noauthor_notitle_nodate-2,
}

@inproceedings{bohme_coverage-based_2016,
	address = {New York, NY, USA},
	series = {{CCS} '16},
	title = {Coverage-based {Greybox} {Fuzzing} as {Markov} {Chain}},
	isbn = {9781450341394},
	url = {https://doi.org/10.1145/2976749.2978428},
	doi = {10.1145/2976749.2978428},
	abstract = {Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few "high-frequency" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented the exponential schedule by extending AFL. In 24 hours, AFLFAST exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFAST produces at least an order of magnitude more unique crashes than AFL.},
	urldate = {2022-10-20},
	publisher = {Association for Computing Machinery},
	author = {Böhme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
	month = oct,
	year = {2016},
	keywords = {foundations, fuzzing, software security, testing efficiency, vulnerability detection},
	pages = {1032--1043},
}

@inproceedings{davis_impact_2018,
	address = {Lake Buena Vista FL USA},
	title = {The impact of regular expression denial of service ({ReDoS}) in practice: an empirical study at the ecosystem scale},
	isbn = {978-1-4503-5573-5},
	shorttitle = {The impact of regular expression denial of service ({ReDoS}) in practice},
	url = {https://dl.acm.org/doi/10.1145/3236024.3236027},
	doi = {10.1145/3236024.3236027},
	abstract = {Regular expressions (regexes) are a popular and powerful means of automatically manipulating text. Regexes are also an understudied denial of service vector (ReDoS). If a regex has super-linear worst-case complexity, an attacker may be able to trigger this complexity, exhausting the victim’s CPU resources and causing denial of service. Existing research has shown how to detect these superlinear regexes, and practitioners have identified super-linear regex anti-pattern heuristics that may lead to such complexity. In this paper, we empirically study three major aspects of ReDoS that have hitherto been unexplored: the incidence of super-linear regexes in practice, how they can be prevented, and how they can be repaired. In the ecosystems of two of the most popular programming languages Ð JavaScript and Python ś we detected thousands of super-linear regexes affecting over 10,000 modules across diverse application domains. We also found that the conventional wisdom for super-linear regex anti-patterns has few false negatives but many false positives; these anti-patterns appear to be necessary, but not sufficient, signals of super-linear behavior. Finally, we found that when faced with a super-linear regex, developers favor revising it over truncating input or developing a custom parser, regardless of whether they had been shown examples of all three fix strategies. These findings motivate further research into ReDoS, since many modules are vulnerable to it and existing mechanisms to avoid it are insufficient. We believe that ReDoS vulnerabilities are a larger threat in practice than might have been guessed.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Davis, James C. and Coghlan, Christy A. and Servant, Francisco and Lee, Dongyoon},
	month = oct,
	year = {2018},
	pages = {246--256},
}

@inproceedings{zou_tcp-fuzz_2021,
	title = {\{{TCP}-{Fuzz}\}: {Detecting} {Memory} and {Semantic} {Bugs} in \{{TCP}\} {Stacks} with {Fuzzing}},
	isbn = {978-1-939133-23-6},
	shorttitle = {\{{TCP}-{Fuzz}\}},
	url = {https://www.usenix.org/conference/atc21/presentation/zou},
	language = {en},
	urldate = {2022-10-19},
	author = {Zou, Yong-Hao and Bai, Jia-Ju and Zhou, Jielong and Tan, Jianfeng and Qin, Chenggang and Hu, Shi-Min},
	year = {2021},
	pages = {489--502},
}

@inproceedings{abdallah_tasharok_2022,
	title = {{TASHAROK}: {Using} {Mechanism} {Design} for {Enhancing} {Security} {Resource} {Allocation} in {Interdependent} {Systems}},
	shorttitle = {{TASHAROK}},
	doi = {10.1109/SP46214.2022.9833591},
	abstract = {We consider interdependent systems managed by multiple defenders that are under the threat of stepping-stone attacks. We model such systems via game-theoretic models and incorporate the effect of behavioral probability weighting that is used to model biases in human decision-making, as descended from the field of behavioral economics. We then incorporate into our framework called TASHAROK, two types of tax-based mechanisms for such interdependent security games where the central regulator incentivizes defenders to invest well in securing their assets so as to achieve the socially optimal outcome. We first show that due to the nature of our interdependent security game, no reliable tax-based mechanism can incentivize the socially optimal investment profile while maintaining a weakly balanced budget. We then show the effect of behavioral probability weighting bias on the amount of taxes paid by defenders, and prove that higher biases make defenders pay more taxes under the two mechanisms. We then explore voluntary participation in tax-based mechanisms. To evaluate our mechanisms, we use four representative real-world interdependent systems where we compare the game-theoretic optimal investments to the socially optimal investments under the two mechanisms. We show that the mechanisms yield higher decrease in the social cost for behavioral decision-makers compared to rational decision-makers.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Abdallah, Mustafa and Woods, Daniel and Naghizadeh, Parinaz and Khalil, Issa and Cason, Timothy and Sundaram, Shreyas and Bagchi, Saurabh},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Attack graphs., Behavioral decision-making, Behavioral sciences, Costs, Decision making, Finance, Games, Interdependent systems, Mechanism design, Regulators, Security, Security games},
	pages = {249--266},
}

@inproceedings{addison_controlling_2002,
	address = {ZAF},
	series = {{SAICSIT} '02},
	title = {Controlling software project risks: an empirical study of methods used by experienced project managers},
	isbn = {978-1-58113-596-1},
	shorttitle = {Controlling software project risks},
	abstract = {The failure rate of software projects has been proven to be very high, and the incidence of failure is becoming worse as more companies venture into software development. Risk management is a collection of methods aimed at minimising or reducing the effects of project failure. This research report has focused on experienced project manager's perceptions of software project risks and controls. It reports on the more significant risks and controls that are utilised to reduce the occurrence of the risk factors, or minimise the impact of various risks. Risk factors involved in software projects along with controls to mitigate these factors were identified in the literature. These were then used in an empirical study to determine their importance and frequency of occurrence. The effectiveness of various controls to reduce the occurrence of risk factors was also identified and discussed. Experienced project managers were found to use certain controls more than inexperienced project managers, particularly 'assign responsibilities to team members' and 'stabilise requirements and specifications'.},
	urldate = {2022-06-17},
	booktitle = {Proceedings of the 2002 annual research conference of the {South} {African} institute of computer scientists and information technologists on {Enablement} through technology},
	publisher = {South African Institute for Computer Scientists and Information Technologists},
	author = {Addison, Tom and Vallabh, Seema},
	month = sep,
	year = {2002},
	keywords = {control methods, management, risks},
	pages = {128--140},
}

@inproceedings{cao_what_2022,
	title = {What the {Fork}? {Finding} and {Analyzing} {Malware} in {GitHub} {Forks}},
	shorttitle = {What the {Fork}?},
	url = {https://www.ndss-symposium.org/ndss-paper/auto-draft-275/},
	language = {en-US},
	urldate = {2022-10-17},
	author = {Cao, Alan and Dolan-Gavitt, Brendan},
	month = apr,
	year = {2022},
}

@inproceedings{trabelsi_abusing_2013,
	title = {Abusing social networks with abuse reports: {A} coalition attack for social networks},
	shorttitle = {Abusing social networks with abuse reports},
	abstract = {In Social Network websites, the users can report the bad behaviors of other users. In order to do so, they can create a kind of escalation ticket called abuse report in which they detail the infraction made by the “bad” user and help the website moderator to decide on a penalty. Today Social Networks count billions of users, the handling of the abuse reports is no more executed manually by moderators; they currently rely on some algorithms that automatically block the “bad” users until a moderator takes care of the case. In this paper we purport to demonstrate how such algorithms are maliciously used by attackers to illegally block innocent victims. We also propose to automate such an attack to demonstrate the big damage that can be caused in current social network websites. We also took the case study of Facebook as proof of concept.},
	booktitle = {2013 {International} {Conference} on {Security} and {Cryptography} ({SECRYPT})},
	author = {Trabelsi, Slim and Bouafif, Hana},
	month = jul,
	year = {2013},
	keywords = {Abuse Report, Attack, Browsers, Coalition, Computer crime, DoS, Facebook, Organizations, Servers, Social Networks, Tin},
	pages = {1--6},
}

@article{schelter_automatically_nodate,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	language = {en},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	pages = {8},
}

@article{yang_complex_2022,
	title = {Complex {Python} {Features} in the {Wild}},
	abstract = {While Python is increasingly popular, program analysis tooling for Python is lagging. This is due, in part, to complex features of the Python language—features with difficult to understand and model semantics. Besides the “usual suspects”, reflection and dynamic execution, complex Python features include context managers, decorators, and generators, among others. This paper explores how often and in what ways developers use certain complex features. We analyze over 3 million Python files mined from GitHub to address three research questions: (i) How often do developers use certain complex Python features? (ii) In what ways do developers use these features? (iii) Does use of complex features increase or decrease over time? Our findings show that usage of dynamic features that pose a threat to static analysis is infrequent. On the other hand, usage of context managers and decorators is surprisingly widespread. Our actionable result is a list of Python features that any “minimal syntax” ought to handle in order to capture developers’ use of the Python language. We hope that understanding the usage of Python features will help tool-builders improve Python tools, which can in turn lead to more correct, secure, and performant Python code.},
	language = {en},
	author = {Yang, Yi and Milanova, Ana and Hirzel, Martin},
	year = {2022},
	pages = {12},
}

@misc{baudart_mining_2020,
	title = {Mining {Documentation} to {Extract} {Hyperparameter} {Schemas}},
	url = {http://arxiv.org/abs/2006.16984},
	abstract = {AI automation tools need machine-readable hyperparameter schemas to define their search spaces. At the same time, AI libraries often come with good human-readable documentation. While such documentation contains most of the necessary information, it is unfortunately not ready to consume by tools. This paper describes how to automatically mine Python docstrings in AI libraries to extract JSON Schemas for their hyperparameters. We evaluate our approach on 119 transformers and estimators from three different libraries and find that it is effective at extracting machine-readable schemas. Our vision is to reduce the burden to manually create and maintain such schemas for AI automation tools and broaden the reach of automation to larger libraries and richer schemas.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Baudart, Guillaume and Kirchner, Peter D. and Hirzel, Martin and Kate, Kiran},
	month = jul,
	year = {2020},
	note = {arXiv:2006.16984 [cs, stat]},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{shi_evaluation_2022,
	title = {On the {Evaluation} of {Neural} {Code} {Summarization}},
	url = {http://arxiv.org/abs/2107.07112},
	doi = {10.1145/3510003.3510060},
	abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18{\textbackslash}\% to +25{\textbackslash}\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
	month = may,
	year = {2022},
	note = {arXiv:2107.07112 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {1597--1608},
}

@inproceedings{shi_evaluation_2022-1,
	title = {On the {Evaluation} of {Neural} {Code} {Summarization}},
	url = {http://arxiv.org/abs/2107.07112},
	doi = {10.1145/3510003.3510060},
	abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18{\textbackslash}\% to +25{\textbackslash}\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
	month = may,
	year = {2022},
	note = {arXiv:2107.07112 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	pages = {1597--1608},
}

@inproceedings{baudart_pipeline_2021,
	title = {Pipeline {Combinators} for {Gradual} {AutoML}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/a3b36cb25e2e0b93b5f334ffb4e4064e-Abstract.html},
	abstract = {Automated machine learning (AutoML) can make data scientists more productive.  But if machine learning is totally automated, that leaves no room for data scientists to apply their intuition.  Hence, data scientists often prefer not total but gradual automation, where they control certain choices and AutoML explores the rest.  Unfortunately, gradual AutoML is cumbersome with state-of-the-art tools, requiring large non-compositional code changes.  More concise compositional code can be achieved with combinators, a powerful concept from functional programming.  This paper introduces a small set of orthogonal combinators for composing machine-learning operators into pipelines.  It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers.  On that foundation, this paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study.},
	urldate = {2022-10-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Baudart, Guillaume and Hirzel, Martin and Kate, Kiran and Ram, Parikshit and Shinnar, Avi and Tsay, Jason},
	year = {2021},
	pages = {19705--19718},
}

@misc{zhu_automatic_2019,
	title = {Automatic {Code} {Summarization}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Automatic {Code} {Summarization}},
	url = {http://arxiv.org/abs/1909.04352},
	abstract = {Background: During software maintenance and development, the comprehension of program code is key to success. High-quality comments can help us better understand programs, but they're often missing or outmoded in today's programs. Automatic code summarization is proposed to solve these problems. During the last decade, huge progress has been made in this field, but there is a lack of an up-to-date survey. Aims: We studied publications concerning code summarization in the field of program comprehension to investigate state-of-the-art approaches. By reading and analyzing relevant articles, we aim at obtaining a comprehensive understanding of the current status of automatic code summarization. Method: In this paper, we performed a systematic literature review over the automatic source code summarization field. Furthermore, we synthesized the obtained data and investigated different approaches. Results: We successfully collected and analyzed 41 selected studies from the different research communities. We exhaustively investigated and described the data extraction techniques, description generation methods, evaluation methods and relevant artifacts of those works. Conclusions: Our systematic review provides an overview of the state of the art, and we also discuss further research directions. By fully elaborating current approaches in the field, our work sheds light on future research directions of program comprehension and comment generation.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Zhu, Yuxiang and Pan, Minxue},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04352 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{zhu_automatic_2019-1,
	title = {Automatic {Code} {Summarization}: {A} {Systematic} {Literature} {Review}},
	shorttitle = {Automatic {Code} {Summarization}},
	url = {http://arxiv.org/abs/1909.04352},
	abstract = {Background: During software maintenance and development, the comprehension of program code is key to success. High-quality comments can help us better understand programs, but they're often missing or outmoded in today's programs. Automatic code summarization is proposed to solve these problems. During the last decade, huge progress has been made in this field, but there is a lack of an up-to-date survey. Aims: We studied publications concerning code summarization in the field of program comprehension to investigate state-of-the-art approaches. By reading and analyzing relevant articles, we aim at obtaining a comprehensive understanding of the current status of automatic code summarization. Method: In this paper, we performed a systematic literature review over the automatic source code summarization field. Furthermore, we synthesized the obtained data and investigated different approaches. Results: We successfully collected and analyzed 41 selected studies from the different research communities. We exhaustively investigated and described the data extraction techniques, description generation methods, evaluation methods and relevant artifacts of those works. Conclusions: Our systematic review provides an overview of the state of the art, and we also discuss further research directions. By fully elaborating current approaches in the field, our work sheds light on future research directions of program comprehension and comment generation.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Zhu, Yuxiang and Pan, Minxue},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04352 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{rak-amnouykit_raise_2022,
	address = {Virtual South Korea},
	title = {The raise of machine learning hyperparameter constraints in {Python} code},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534400},
	doi = {10.1145/3533767.3534400},
	abstract = {Machine-learning operators often have correctness constraints that cut across multiple hyperparameters and/or data. Violating these constraints causes the operator to raise runtime exceptions, but those are usually documented only informally or not at all. This paper presents the first interprocedural weakest-precondition analysis for Python to extract hyperparameter constraints. The analysis is mostly static, but to make it tractable for typical Python idioms in machine-learning libraries, it selectively switches to the concrete domain for some cases. This paper demonstrates the analysis by extracting hyperparameter constraints for 181 operators from a total of 8 ML libraries, where it achieved high precision and recall and found real bugs. Our technique advances static analysis for Python and is a step towards safer and more robust machine learning.},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Rak-amnouykit, Ingkarat and Milanova, Ana and Baudart, Guillaume and Hirzel, Martin and Dolby, Julian},
	month = jul,
	year = {2022},
	pages = {580--592},
}

@inproceedings{rak-amnouykit_extracting_2021,
	address = {Virtual, United States},
	title = {Extracting {Hyperparameter} {Constraints} from {Code}},
	url = {https://hal.archives-ouvertes.fr/hal-03401683},
	abstract = {Machine-learning operators often have correctness constraints that cut across multiple hyperparameters and/or data. Violating these constraints causes runtime exceptions, but they are usually documented only informally or not at all. This paper presents a weakest precondition analysis for Python code. We demonstrate our analysis by extracting hyperparameter constraints for 45 sklearn operators. Our analysis is a step towards safer and more robust machine learning.},
	urldate = {2022-10-13},
	booktitle = {{ICLR} {Workshop} on {Security} and {Safety} in {Machine} {Learning} {Systems}},
	author = {Rak-Amnouykit, Ingkarat and Milanova, Ana and Baudart, Guillaume and Hirzel, Martin and Dolby, Julian},
	month = may,
	year = {2021},
}

@inproceedings{niesler_hera_2021,
	address = {Virtual},
	title = {{HERA}: {Hotpatching} of {Embedded} {Real}-time {Applications}},
	isbn = {978-1-891562-66-2},
	shorttitle = {{HERA}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6B-2_24159_paper.pdf},
	doi = {10.14722/ndss.2021.24159},
	language = {en},
	urldate = {2022-10-13},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Niesler, Christian and Surminski, Sebastian and Davi, Lucas},
	year = {2021},
}

@article{noauthor_so_nodate,
	title = {So {Many} {Fuzzers}, {So} {Little} {Time}},
	language = {en},
	pages = {11},
}

@article{jero_automated_nodate,
	title = {Automated {Attack} {Discovery} in {TCP} {Congestion} {Control} {Using} a {Model}-guided {Approach}},
	abstract = {One of the most important goals of TCP is to ensure fairness and prevent congestion collapse by implementing congestion control. Various attacks against TCP congestion control have been reported over the years, most of which have been discovered through manual analysis. In this paper, we propose an automated method that combines the generality of implementation-agnostic fuzzing with the precision of runtime analysis to ﬁnd attacks against implementations of TCP congestion control. It uses a model-guided approach to generate abstract attack strategies, by leveraging a state machine model of TCP congestion control to ﬁnd vulnerable state machine paths that an attacker could exploit to increase or decrease the throughput of a connection to his advantage. These abstract strategies are then mapped to concrete attack strategies, which consist of sequences of actions such as injection or modiﬁcation of acknowledgements and a logical time for injection. We design and implement a virtualized platform, TCPWN, that consists of a a proxy-based attack injector and a TCP congestion control state tracker that uses only network trafﬁc to create and inject these concrete attack strategies. We evaluated 5 TCP implementations from 4 Linux distributions and Windows 8.1. Overall, we found 11 classes of attacks, of which 8 are new.},
	language = {en},
	author = {Jero, Samuel and Hoque, Endadul and Choffnes, David and Mislove, Alan and Nita-Rotaru, Cristina},
	pages = {15},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/},
	urldate = {2022-10-07},
}

@inproceedings{corteggiani_inception_2018-1,
	title = {Inception: \{{System}-{Wide}\} {Security} {Testing} of \{{Real}-{World}\} {Embedded} {Systems} {Software}},
	isbn = {978-1-939133-04-5},
	shorttitle = {Inception},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/corteggiani},
	language = {en},
	urldate = {2022-10-07},
	author = {Corteggiani, Nassim and Camurati, Giovanni and Francillon, Aurélien},
	year = {2018},
	pages = {309--326},
}

@misc{noauthor_firehydrant_nodate,
	title = {{FireHydrant}: {Incident} management for every developer},
	url = {https://firehydrant.com/},
	abstract = {FireHydrant helps teams more easily maintain service catalogs, respond to incidents, communicate through status pages, and learn with retrospectives.},
	language = {en},
	urldate = {2022-10-06},
	journal = {Incident management for every developer},
}

@inproceedings{michael_regexes_2019,
	title = {Regexes are {Hard}: {Decision}-{Making}, {Difficulties}, and {Risks} in {Programming} {Regular} {Expressions}},
	shorttitle = {Regexes are {Hard}},
	doi = {10.1109/ASE.2019.00047},
	abstract = {Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Michael, Louis G. and Donohue, James and Davis, James C. and Lee, Dongyoon and Servant, Francisco},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Decision making, Face, Interviews, Programming profession, Software, Tools, regular expressions, developer process, qualitative research},
	pages = {415--426},
}

@inproceedings{kapur_towards_2019,
	address = {Montreal, QC, Canada},
	title = {Towards a {Knowledge} {Warehouse} and {Expert} {System} for the {Automation} of {SDLC} {Tasks}},
	isbn = {978-1-72813-393-5},
	url = {https://ieeexplore.ieee.org/document/8812829/},
	doi = {10.1109/ICSSP.2019.00011},
	abstract = {Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Software} and {System} {Processes} ({ICSSP})},
	publisher = {IEEE},
	author = {Kapur, Ritu and Sodhi, Balwinder},
	month = may,
	year = {2019},
	pages = {5--8},
}

@article{boenisch_systematic_2021,
	title = {A {Systematic} {Review} on {Model} {Watermarking} for {Neural} {Networks}},
	volume = {4},
	issn = {2624-909X},
	url = {http://arxiv.org/abs/2009.12153},
	doi = {10.3389/fdata.2021.729663},
	abstract = {Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. This work presents a taxonomy identifying and analyzing different classes of watermarking schemes for ML models. It introduces a unified threat model to allow structured reasoning on and comparison of the effectiveness of watermarking methods in different scenarios. Furthermore, it systematizes desired security requirements and attacks against ML model watermarking. Based on that framework, representative literature from the field is surveyed to illustrate the taxonomy. Finally, shortcomings and general limitations of existing approaches are discussed, and an outlook on future research directions is given.},
	urldate = {2022-09-27},
	journal = {Frontiers in Big Data},
	author = {Boenisch, Franziska},
	month = nov,
	year = {2021},
	note = {arXiv:2009.12153 [cs]},
	keywords = {A.1, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Multimedia, I.2},
	pages = {729663},
}

@article{sawant_learning-based_nodate,
	title = {Learning-based {Identiﬁcation} of {Coding} {Best} {Practices} from {Software} {Documentation}},
	abstract = {Automatic identiﬁcation of coding best practices can scale the development of code and application analyzers. We present Doc2BP, a deep learning tool to identify coding best practices in software documentation. Natural language descriptions are mapped to an informative embedding space, optimized under the dual objectives of binary and few shot classiﬁcation. The binary objective powers general classiﬁcation into known best practice categories using a deep learning classiﬁer. The few shot objective facilitates example-based classiﬁcation into novel categories by matching embeddings with user-provided examples at run-time, without having to retrain the underlying model. We analyze the effects of manually and synthetically labeled examples, context, and cross-domain information.},
	language = {en},
	author = {Sawant, Neela and Sengamedu, Srinivasan H},
	pages = {10},
}

@misc{dellavecchia_how_2022,
	title = {How a {Rogue} {Developer} {Ruined} {Millions} of {Software} (happened this weekend)},
	url = {https://medium.com/@anthonyjdella/how-a-rogue-developer-ruined-millions-of-software-happened-this-weekend-8602af1f8e07},
	abstract = {TLDR: A software developer who made some highly used open-source software, decided to go rogue and inject a bug into his software, making…},
	language = {en},
	urldate = {2022-05-08},
	journal = {Medium},
	author = {Dellavecchia, Anthony},
	month = jan,
	year = {2022},
}

@article{wang_backdoor_2022,
	title = {Backdoor {Attacks} {Against} {Transfer} {Learning} {With} {Pre}-{Trained} {Deep} {Learning} {Models}},
	volume = {15},
	issn = {1939-1374},
	doi = {10.1109/TSC.2020.3000900},
	abstract = {Transfer learning provides an effective solution for feasibly and fast customize accurate Student models, by transferring the learned knowledge of pre-trained Teacher models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this article, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly adopted defenses: pruning-based, retraining-based and input pre-processing-based defenses. Specifically, (\${\textbackslash}mathcal A\$A) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating pruning-based and/or retraining-based defenses. (\${\textbackslash}mathcal B\$B) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the input pre-processing-based defense, while guaranteeing that selected neuron(s) can be significantly activated. (\${\textbackslash}mathcal C\$C) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the 98.4 and 97.2 percent classification accuracy as the genuine model on clean image and time series inputs while improving \$27.9\%-100\%\$27.9\%-100\% and \$27.1\%-56.1\%\$27.1\%-56.1\% attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.},
	number = {3},
	journal = {IEEE Transactions on Services Computing},
	author = {Wang, Shuo and Nepal, Surya and Rudolph, Carsten and Grobler, Marthie and Chen, Shangyu and Chen, Tianle},
	year = {2022},
	keywords = {Biological system modeling, Data models, Electrocardiography, Learning systems, Task analysis, Training, Web service, backdoor attack, deep neural network, pre-trained model, transfer learning},
	pages = {1526--1539},
}

@inproceedings{Liu2022LoneNeuron,
	address = {Los Angeles},
	title = {{LoneNeuron}: a {Highly}-{Effective} {Feature}-{Domain} {Neural} {Trojan} {Using} {Invisible} and {Polymorphic} {Watermarks}},
	abstract = {The wide adoption of deep neural networks (DNNs) in real-world applications raises increasing security concerns. Neural Trojans embedded in pre-trained neural networks are a harmful attack against the DNN model supply chain. They generate false outputs when certain stealthy triggers appear in the inputs. While data-poisoning attacks have been well studied in the literature, code-poisoning and model-poisoning backdoors only start to attract attention until recently. We present a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks. With high attack specificity, LoneNeuron achieves a 100\% attack success rate, while not affecting the main task performance. With LoneNeuron’s unique watermark polymorphism property, the same feature-domain trigger is resolved to multiple watermarks in the pixel domain, which further improves watermark randomness, stealthiness, and resistance against Trojan detection. Extensive experiments show that LoneNeuron could escape stateof-the-art Trojan detectors. LoneNeuron is also the first effective backdoor attack against vision transformers (ViTs).},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Liu, Zeyan and Li, Fengjun and Li, Zhu and Luo, Bo},
	year = {2022},
}

@inproceedings{Decan2018SecurityVulnerabilitiesinNPMDependencyNetwork,
	title = {On the impact of security vulnerabilities in the npm package dependency network},
	url = {https://dl.acm.org/doi/10.1145/3196398.3196401},
	doi = {10.1145/3196398.3196401},
	abstract = {Security vulnerabilities are among the most pressing problems in open source software package libraries. It may take a long time to discover and fix vulnerabilities in packages. In addition, vulnerabilities may propagate to dependent packages, making them vulnerable too. This paper presents an empirical study of nearly 400 security reports over a 6-year period in the npm dependency network containing over 610k JavaScript packages. Taking into account the severity of vulnerabilities, we analyse how and when these vulnerabilities are discovered and fixed, and to which extent they affect other packages in the packaging ecosystem in presence of dependency constraints. We report our findings and provide guidelines for package maintainers and tool developers to improve the process of dealing with security issues.},
	booktitle = {International {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Decan, Alexandre and Mens, Tom and Constantinou, Eleni},
	year = {2018},
	pages = {181--191},
}

@misc{Chakraborty2018AdversarialAtatcksanddefences,
	title = {Adversarial {Attacks} and {Defences}: {A} {Survey}},
	url = {https://arxiv.org/abs/1810.00069},
	abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
	publisher = {arXiv},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_multi-modal_2020,
	address = {London UK},
	title = {Multi-modal synthesis of regular expressions},
	isbn = {978-1-4503-7613-6},
	url = {https://dl.acm.org/doi/10.1145/3385412.3385988},
	doi = {10.1145/3385412.3385988},
	abstract = {In this paper, we propose a multi-modal synthesis technique for automatically constructing regular expressions (regexes) from a combination of examples and natural language. Using multiple modalities is useful in this context because natural language alone is often highly ambiguous, whereas examples in isolation are often not sufficient for conveying user intent. Our proposed technique first parses the English description into a so-called hierarchical sketch that guides our programming-by-example (PBE) engine. Since the hierarchical sketch captures crucial hints, the PBE engine can leverage this information to both prioritize the search as well as make useful deductions for pruning the search space.},
	language = {en},
	urldate = {2022-09-21},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Chen, Qiaochu and Wang, Xinyu and Ye, Xi and Durrett, Greg and Dillig, Isil},
	month = jun,
	year = {2020},
	pages = {487--502},
}

@inproceedings{Huang2011AdversarialML,
	title = {Adversarial {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/2046684.2046692},
	doi = {10.1145/2046684.2046692},
	abstract = {In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging ﬁeld of study: adversarial machine learning—the study of eﬀective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-speciﬁc factors that limit an adversary’s capabilities; introduce two models for modeling an adversary’s capabilities; explore the limits of an adversary’s knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.},
	booktitle = {{ACM} workshop on {Security} and {Artificial} {Intelligence}},
	publisher = {IEEE},
	author = {Huang, Ling and Joseph, Anthony D and Nelson, Blaine and Rubinstein, Benjamin I P and Tygar, J D},
	year = {2011},
	pages = {43--58},
}

@article{han_pre-trained_2021,
	title = {Pre-trained models: {Past}, present and future},
	abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	year = {2021},
}

@inproceedings{Sanyal2022ModelStealing,
	title = {Towards {Data}-{Free} {Model} {Stealing} in a {Hard} {Label} {Setting}},
	abstract = {Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework1 that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim’s gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-ofthe-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Sanyal, Sunandini and Addepalli, Sravanti and Babu, R Venkatesh},
	year = {2022},
	pages = {15284--15293},
}

@article{Saha2020BackdoorAttacks,
	title = {Hidden {Trigger} {Backdoor} {Attacks}},
	volume = {34},
	doi = {10.1609/aaai.v34i07.6871},
	abstract = {With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a speciﬁc small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classiﬁcation settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
	year = {2020},
	pages = {11957--11965},
}

@inproceedings{Ohm2020ReviewofOpenSourceSSCAttacks,
	title = {Backstabber’s {Knife} {Collection}: {A} {Review} of {Open} {Source} {Software} {Supply} {Chain} {Attacks}},
	abstract = {A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems further down the chain. Recent years saw a number of supply chain attacks that leverage the increasing use of open source during software development, which is facilitated by dependency managers that automatically resolve, download and install hundreds of open source packages throughout the software life cycle. Even though many approaches for detection and discovery of vulnerable packages exist, no prior work has focused on malicious packages. This paper presents a dataset as well as analysis of 174 malicious software packages that were used in real-world attacks on open source software supply chains and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed. This work is meant to facilitate the future development of preventive and detective safeguards by open source and research communities.},
	booktitle = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	publisher = {Springer},
	author = {Ohm, Marc and Plate, Henrik and Sykosch, Arnold and Meier, Michael},
	editor = {Maurice, Clémentine and Bilge, Leyla and Stringhini, Gianluca and Neves, Nuno},
	year = {2020},
	keywords = {Application security, Malware, Software supply chain, Taxonomy, attacks},
	pages = {23--43},
}

@inproceedings{Du2013SSCRiskManagement,
	title = {Towards {An} {Analysis} of {Software} {Supply} {Chain} {Risk} {Management}},
	volume = {1},
	abstract = {Nowadays, software supply chain participants have become international distributors, which make software supply chain more and more complex. This complexity makes manager understand, acquire, monitor and manage software supply chain products and processes more difficult than ever, and then relevant security problems happen, such as software with security holes. But most of security problems are different from other supply chains. Therefore, based on system’s perspective and current several analysis methods of software supply chain, the paper analyzes and summarizes software supply chain risks, software supply chain risk management methods, and puts forward some basic risk management practices to protect software supply chain’s security. Finally, the paper discusses the future research direction to software supply chain.},
	booktitle = {Proceedings of the {World} {Congress} on {Engineering} and {Computer} {Science}},
	author = {Du, Shixian and Lu, Tianbo and Zhao, Lingling and Xu, Bing and Guo, Xiaobo and Yang, Hongyu},
	year = {2013},
}

@article{Akhtar2018AdversarialAttacksonDLinCV,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	volume = {6},
	abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = {2018},
	keywords = {Computational modeling, Computer vision, Deep learning, Machine learning, Neural networks, Perturbation methods, Predictive models, Task analysis, adversarial learning, adversarial perturbation, black-box attack, perturbation detection, white-box attack},
	pages = {14410--14430},
}

@inproceedings{xu_fuzzing_2019,
	title = {Fuzzing {File} {Systems} via {Two}-{Dimensional} {Input} {Space} {Exploration}},
	doi = {10.1109/SP.2019.00035},
	abstract = {File systems, a basic building block of an OS, are too big and too complex to be bug free. Nevertheless, file systems rely on regular stress-testing tools and formal checkers to find bugs, which are limited due to the ever-increasing complexity of both file systems and OSes. Thus, fuzzing, proven to be an effective and a practical approach, becomes a preferable choice, as it does not need much knowledge about a target. However, three main challenges exist in fuzzing file systems: mutating a large image blob that degrades overall performance, generating image-dependent file operations, and reproducing found bugs, which is difficult for existing OS fuzzers. Hence, we present JANUS, the first feedback-driven fuzzer that explores the two-dimensional input space of a file system, i.e., mutating metadata on a large image, while emitting image-directed file operations. In addition, JANUS relies on a library OS rather than on traditional VMs for fuzzing, which enables JANUS to load a fresh copy of the OS, thereby leading to better reproducibility of bugs. We evaluate JANUS on eight file systems and found 90 bugs in the upstream Linux kernel, 62 of which have been acknowledged. Forty-three bugs have been fixed with 32 CVEs assigned. In addition, JANUS achieves higher code coverage on all the file systems after fuzzing 12 hours, when compared with the state-of-the-art fuzzer Syzkaller for fuzzing file systems. JANUS visits 4.19x and 2.01x more code paths in Btrfs and ext4, respectively. Moreover, JANUS is able to reproduce 88-100\% of the crashes, while Syzkaller fails on all of them.},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Xu, Wen and Moon, Hyungon and Kashyap, Sanidhya and Tseng, Po-Ning and Kim, Taesoo},
	month = may,
	year = {2019},
	note = {ISSN: 2375-1207},
	keywords = {Computer bugs, File systems, Filesystem, Fuzzing, Kernel, Libraries, Linux, Metadata},
	pages = {818--834},
}

@article{humphrey_characterizing_1988,
	title = {Characterizing the software process: a maturity framework},
	volume = {5},
	issn = {1937-4194},
	shorttitle = {Characterizing the software process},
	doi = {10.1109/52.2014},
	abstract = {A description is given of a software-process maturity framework that has been developed to provide the US Department of Defense with a means to characterize the capabilities of software-development organizations. This software-development process-maturity model reasonably represents the actual ways in which software-development organizations improve. It provides a framework for assessing these organizations and identifying the priority areas for immediate improvement. It also helps identify those places where advanced technology can be most valuable in improving the software-development process. The framework can be used by any software organization to assess its own capabilities and identify the most important areas for improvement.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Software},
	author = {Humphrey, W.S.},
	month = mar,
	year = {1988},
	note = {Conference Name: IEEE Software},
	keywords = {Computer industry, Costs, Industrial control, Job shop scheduling, Metals industry, Process control, Software engineering, Software maintenance, Software measurement, Software quality},
	pages = {73--79},
}

@inproceedings{mayer_model_2009,
	title = {A model to assess the maturity level of the {Risk} {Management} process in information security},
	doi = {10.1109/INMW.2009.5195935},
	abstract = {The Risk Management (RM) process comprises coordinated activities aimed at guiding and controlling an organization as far as risks are concerned. These activities encompass the definition of the context of analysis, assessment, treatment, acceptance, as well as the communication and the monitoring of information security risks. Organizations should implement RM in a consistent, systematic manner in order to achieve compliance with current laws, standards and regulations, and also meet mandatory requirements for the certification of an Information Security Management System. However, in the context of information security, no reference was found in literature for a model to assess the maturity level of an RM process. In order to overcome this problem, this study describes the structure of a model for the assessment of the maturity level of the RM process in the realm of Information Security. The designed model basically consists of a set of best practices, totally aligned with standard ISO/IEC 27005 and comprised of: (1) three stages; (2) five maturity levels; (3) forty-three control objectives; (4) one control map; (5) one assessment instrument relative to the maturity level of the activities of the RM process; (6) an accountability matrix relative to each activity of the process and also a (7) risk scorecard.},
	booktitle = {2009 {IFIP}/{IEEE} {International} {Symposium} on {Integrated} {Network} {Management}-{Workshops}},
	author = {Mayer, Janice and Lemes Fagundes, Leonardo},
	month = jun,
	year = {2009},
	keywords = {Communication system control, Context, IEC standards, ISO standards, Information Security, Information analysis, Information security, Maturity Model, Monitoring, Risk Management, Risk analysis, Risk management, Standards organizations},
	pages = {61--70},
}

@article{Wang2022EvilModel2,
	title = {{EvilModel} 2.0: {Bringing} {Neural} {Network} {Models} into {Malware} {Attacks}},
	doi = {10.1016/j.cose.2022.102807},
	journal = {Computers \& Security},
	author = {Wang, Zhi and Liu, Chaoge and Cui, Xiang and Yin, Jie and Wang, Xutong},
	year = {2022},
}

@inproceedings{chapman_exploring_2016,
	address = {Saarbrücken Germany},
	title = {Exploring regular expression usage and context in {Python}},
	isbn = {978-1-4503-4390-9},
	url = {https://dl.acm.org/doi/10.1145/2931037.2931073},
	doi = {10.1145/2931037.2931073},
	abstract = {Due to the popularity and pervasive use of regular expressions, researchers have created tools to support their creation, validation, and use. However, little is known about the context in which regular expressions are used, the features that are most common, and how behaviorally similar regular expressions are to one another.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 25th {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Chapman, Carl and Stolee, Kathryn T.},
	month = jul,
	year = {2016},
	pages = {282--293},
}

@techreport{chaganti_stegomalware_2021,
	type = {preprint},
	title = {Stegomalware: {A} {Systematic} {Survey} of {Malware} {Hiding} and {Detection} in {Images}, {Machine} {Learning} {Models} and {Research} {Challenges}},
	shorttitle = {Stegomalware},
	url = {https://www.techrxiv.org/articles/preprint/Stegomalware_A_Systematic_Survey_of_Malware_Hiding_and_Detection_in_Images_Machine_Learning_Models_and_Research_Challenges/16755457/1},
	abstract = {Malware distribution to the victim network is commonly performed through ﬁle attachments in phishing email or downloading illegitimate ﬁles from the internet, when the victim interacts with the source of infection. To detect and prevent the malware distribution in the victim machine, the existing end device security applications may leverage sophisticated techniques such as signature-based or anomaly-based, machine learning techniques. The well-known ﬁle formats Portable Executable (PE) for Windows and Executable and Linkable Format (ELF) for Linux based operating system are used for malware analysis and the malware detection capabilities of these ﬁles has been well advanced for real time detection. But the malware payload hiding in multimedia like cover images using steganography detection has been a challenge for enterprises, as these are rarely seen and usually act as a stager in sophisticated attacks. In this article, to our knowledge, we are the ﬁrst to try to address the knowledge gap between the current progress in image steganography and steganalysis academic research focusing on data hiding and the review of the stegomalware (malware payload hiding in images) targeting enterprises with cyberattacks current status. We present the stegomalware history, generation tools, ﬁle format speciﬁcation description. Based on our ﬁndings, we perform the detail review of the image steganography techniques including the recent Generative Adversarial Networks (GAN) based models and the image steganalysis methods including the Deep Learning (DL) models for hiding data detection. Additionally, the stegomalware detection framework for enterprise is proposed for anomaly based stegomalware detection emphasizing the architecture details for different network environments. Finally, the research opportunities and challenges in stegomalware generation and detection are presented based on our ﬁndings.},
	language = {en},
	urldate = {2022-09-14},
	author = {chaganti, Raj and R, vinayakumar and Alazab, Mamoun and Pham, Tuan},
	month = oct,
	year = {2021},
	doi = {10.36227/techrxiv.16755457.v1},
}

@misc{demirkiran_ensemble_2022,
	title = {An {Ensemble} of {Pre}-trained {Transformer} {Models} {For} {Imbalanced} {Multiclass} {Malware} {Classification}},
	url = {http://arxiv.org/abs/2112.13236},
	abstract = {Classification of malware families is crucial for a comprehensive understanding of how they can infect devices, computers, or systems. Thus, malware identification enables security researchers and incident responders to take precautions against malware and accelerate mitigation. API call sequences made by malware are widely utilized features by machine and deep learning models for malware classification as these sequences represent the behavior of malware. However, traditional machine and deep learning models remain incapable of capturing sequence relationships between API calls. On the other hand, the transformer-based models process sequences as a whole and learn relationships between API calls due to multi-head attention mechanisms and positional embeddings. Our experiments demonstrate that the transformer model with one transformer block layer surpassed the widely used base architecture, LSTM. Moreover, BERT or CANINE, pre-trained transformer models, outperformed in classifying highly imbalanced malware families according to evaluation metrics, F1-score, and AUC score. Furthermore, the proposed bagging-based random transformer forest (RTF), an ensemble of BERT or CANINE, has reached the state-of-the-art evaluation scores on three out of four datasets, particularly state-of-the-art F1-score of 0.6149 on one of the commonly used benchmark dataset.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Demirkıran, Ferhat and Çayır, Aykut and Ünal, Uğur and Dağ, Hasan},
	month = jun,
	year = {2022},
	note = {arXiv:2112.13236 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noppel_backdooring_2022,
	title = {Backdooring {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2204.09498},
	abstract = {Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate blinding attacks that can fully disguise an ongoing attack against the machine learning model. Similar to neural backdoors, we modify the model's prediction upon trigger presence but simultaneously also fool the provided explanation. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of such attacks for different explanation types in the image domain, before we resume to conduct a red-herring attack against malware classification.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Noppel, Maximilian and Peter, Lukas and Wressnegger, Christian},
	month = apr,
	year = {2022},
	note = {arXiv:2204.09498 [cs]},
	keywords = {68T99, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{huang_backdoor_2022,
	title = {Backdoor {Defense} via {Decoupling} the {Training} {Process}},
	url = {http://arxiv.org/abs/2202.03423},
	abstract = {Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via {\textbackslash}emph\{self-supervised learning\} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a {\textbackslash}emph\{semi-supervised fine-tuning\} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at {\textbackslash}url\{https://github.com/SCLBD/DBD\}.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Huang, Kunzhe and Li, Yiming and Wu, Baoyuan and Qin, Zhan and Ren, Kui},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03423 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{guo_aeva_2022,
	title = {{AEVA}: {Black}-box {Backdoor} {Detection} {Using} {Adversarial} {Extreme} {Value} {Analysis}},
	shorttitle = {{AEVA}},
	url = {http://arxiv.org/abs/2110.14880},
	abstract = {Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor is often embedded in the target DNNs through injecting a backdoor trigger into training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Existing backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device deployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution; a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) to detect backdoors in black-box neural networks. AEVA is based on an extreme value analysis of the adversarial map, computed from the monte-carlo gradient estimation. Evidenced by extensive experiments across multiple popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Guo, Junfeng and Li, Ang and Liu, Cong},
	month = feb,
	year = {2022},
	note = {arXiv:2110.14880 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{do_towards_2022,
	title = {Towards {Effective} and {Robust} {Neural} {Trojan} {Defenses} via {Input} {Filtering}},
	url = {http://arxiv.org/abs/2202.12154},
	abstract = {Trojan attacks on deep neural networks are both dangerous and surreptitious. Over the past few years, Trojan attacks have advanced from using only a single input-agnostic trigger and targeting only one class to using multiple, input-specific triggers and targeting multiple classes. However, Trojan defenses have not caught up with this development. Most defense methods still make inadequate assumptions about Trojan triggers and target classes, thus, can be easily circumvented by modern Trojan attacks. To deal with this problem, we propose two novel "filtering" defenses called Variational Input Filtering (VIF) and Adversarial Input Filtering (AIF) which leverage lossy data compression and adversarial learning respectively to effectively purify potential Trojan triggers in the input at run time without making assumptions about the number of triggers/target classes or the input dependence property of triggers. In addition, we introduce a new defense mechanism called "Filtering-then-Contrasting" (FtC) which helps avoid the drop in classification accuracy on clean data caused by "filtering", and combine it with VIF/AIF to derive new defenses of this kind. Extensive experimental results and ablation studies show that our proposed defenses significantly outperform well-known baseline defenses in mitigating five advanced Trojan attacks including two recent state-of-the-art while being quite robust to small amounts of training data and large-norm triggers.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Do, Kien and Harikumar, Haripriya and Le, Hung and Nguyen, Dung and Tran, Truyen and Rana, Santu and Nguyen, Dang and Susilo, Willy and Venkatesh, Svetha},
	month = jul,
	year = {2022},
	note = {arXiv:2202.12154 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{noppel_backdooring_2022-1,
	title = {Backdooring {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2204.09498},
	abstract = {Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate blinding attacks that can fully disguise an ongoing attack against the machine learning model. Similar to neural backdoors, we modify the model's prediction upon trigger presence but simultaneously also fool the provided explanation. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of such attacks for different explanation types in the image domain, before we resume to conduct a red-herring attack against malware classification.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Noppel, Maximilian and Peter, Lukas and Wressnegger, Christian},
	month = apr,
	year = {2022},
	note = {arXiv:2204.09498 [cs]},
	keywords = {68T99, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{li_backdoor_2022,
	title = {Backdoor {Learning}: {A} {Survey}},
	issn = {2162-2388},
	shorttitle = {Backdoor {Learning}},
	doi = {10.1109/TNNLS.2022.3182979},
	abstract = {Backdoor attack intends to embed hidden backdoors into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, there is still no comprehensive and timely review of it. In this article, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields (i.e., adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at https://github.com/THUYimingLi/backdoor-learning-resources.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Li, Yiming and Jiang, Yong and Li, Zhifeng and Xia, Shu-Tao},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {AI security, Deep learning, Predictive models, Schedules, Security, Task analysis, Taxonomy, Training, backdoor attack, backdoor defense, backdoor learning, deep learning},
	pages = {1--18},
}

@inproceedings{dong_black-box_2021,
	address = {Montreal, QC, Canada},
	title = {Black-box {Detection} of {Backdoor} {Attacks} with {Limited} {Information} and {Data}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710786/},
	doi = {10.1109/ICCV48922.2021.01617},
	abstract = {Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor detection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a blackbox backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor detection, we also propose a simple strategy for reliable predictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Dong, Yinpeng and Yang, Xiao and Deng, Zhijie and Pang, Tianyu and Xiao, Zihao and Su, Hang and Zhu, Jun},
	month = oct,
	year = {2021},
	pages = {16462--16471},
}

@article{zhang_department_nodate,
	title = {Department of {Computer} {Science} {Cornell} {University} {Ithaca}, {NY} 14853},
	abstract = {Backdoors are often installed by attackers who have compromised a system to ease their subsequent return to the system. We consider the problem of identifying a large class of backdoors, namely those providing interactive access on non-standard ports, by passively monitoring a site’s Internet access link. We develop a general algorithm for detecting interactive trafﬁc based on packet size and timing characteristics, and a set of protocol-speciﬁc algorithms that look for signatures distinctive to particular protocols. We evaluate the algorithms on large Internet access traces and ﬁnd that they perform quite well. In addition, some of the algorithms are amenable to preﬁltering using a stateless packet ﬁlter, which yields a major performance increase at little or no loss of accuracy. However, the success of the algorithms is tempered by the discovery that large sites have many users who routinely access what are in fact benign backdoors, such as servers running on nonstandard ports not to hide, but for mundane administrative reasons. Hence, backdoor detection also requires a signiﬁcant policy component for separating allowable backdoor access from surreptitious access.},
	language = {en},
	author = {Zhang, Yin and Paxson, Vern},
	pages = {15},
}

@misc{goldwasser_planting_2022,
	title = {Planting {Undetectable} {Backdoors} in {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2204.06974},
	abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06974 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{goldwasser_planting_2022-1,
	title = {Planting {Undetectable} {Backdoors} in {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2204.06974},
	abstract = {Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate "backdoor key", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is "clean" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an "adversarially robust" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Goldwasser, Shafi and Kim, Michael P. and Vaikuntanathan, Vinod and Zamir, Or},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06974 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{shen_backdoor_2021,
	title = {Backdoor {Pre}-trained {Models} {Can} {Transfer} to {All}},
	url = {http://arxiv.org/abs/2111.00197},
	doi = {10.1145/3460120.3485370},
	abstract = {Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Shen, Lujia and Ji, Shouling and Zhang, Xuhong and Li, Jinfeng and Chen, Jing and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Wang, Ting},
	month = nov,
	year = {2021},
	note = {arXiv:2111.00197 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {3141--3158},
}

@inproceedings{chapman_exploring_2017,
	title = {Exploring regular expression comprehension},
	doi = {10.1109/ASE.2017.8115653},
	abstract = {The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluate the understandability of various regex language features. We further analyze regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [{\textbackslash}d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics.},
	booktitle = {2017 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chapman, Carl and Wang, Peipei and Stolee, Kathryn T.},
	month = oct,
	year = {2017},
	keywords = {Automata, Concrete, Measurement, Pattern matching, Regular expression comprehension, Standards, Syntactics, Tools, equivalence class, regex representations},
	pages = {405--416},
}

@article{thomas_protecting_nodate,
	title = {Protecting accounts from credential stufﬁng with password breach alerting},
	abstract = {Protecting accounts from credential stufﬁng attacks remains burdensome due to an asymmetry of knowledge: attackers have wide-scale access to billions of stolen usernames and passwords, while users and identity providers remain in the dark as to which accounts require remediation. In this paper, we propose a privacy-preserving protocol whereby a client can query a centralized breach repository to determine whether a speciﬁc username and password combination is publicly exposed, but without revealing the information queried. Here, a client can be an end user, a password manager, or an identity provider. To demonstrate the feasibility of our protocol, we implement a cloud service that mediates access to over 4 billion credentials found in breaches and a Chrome extension serving as an initial client. Based on anonymous telemetry from nearly 670,000 users and 21 million logins, we ﬁnd that 1.5\% of logins on the web involve breached credentials. By alerting users to this breach status, 26\% of our warnings result in users migrating to a new password, at least as strong as the original. Our study illustrates how secure, democratized access to password breach alerting can help mitigate one dimension of account hijacking.},
	language = {en},
	author = {Thomas, Kurt and Pullman, Jennifer and Yeo, Kevin and Raghunathan, Ananth and Kelley, Patrick Gage and Invernizzi, Luca and Benko, Borbala and Pietraszek, Tadek and Patel, Sarvar and Boneh, Dan and Bursztein, Elie},
	pages = {18},
}

@techreport{joint_task_force_interagency_working_group_security_2020,
	title = {Security and {Privacy} {Controls} for {Information} {Systems} and {Organizations}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf},
	abstract = {This publication provides a catalog of security and privacy controls for information systems and organizations to protect organizational operations and assets, individuals, other organizations, and the Nation from a diverse set of threats and risks, including hostile attacks, human errors, natural disasters, structural failures, foreign intelligence entities, and privacy risks. The controls are flexible and customizable and implemented as part of an organization-wide process to manage risk. The controls address diverse requirements derived from mission and business needs, laws, executive orders, directives, regulations, policies, standards, and guidelines. Finally, the consolidated control catalog addresses security and privacy from a functionality perspective (i.e., the strength of functions and mechanisms provided by the controls) and from an assurance perspective (i.e., the measure of confidence in the security or privacy capability provided by the controls). Addressing functionality and assurance helps to ensure that information technology products and the systems that rely on those products are sufficiently trustworthy.},
	language = {en},
	urldate = {2022-09-10},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Interagency Working Group}},
	month = sep,
	year = {2020},
	doi = {10.6028/NIST.SP.800-53r5},
	note = {Edition: Revision 5},
}

@techreport{joint_task_force_interagency_working_group_security_2020-1,
	title = {Security and {Privacy} {Controls} for {Information} {Systems} and {Organizations}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf},
	abstract = {This publication provides a catalog of security and privacy controls for information systems and organizations to protect organizational operations and assets, individuals, other organizations, and the Nation from a diverse set of threats and risks, including hostile attacks, human errors, natural disasters, structural failures, foreign intelligence entities, and privacy risks. The controls are flexible and customizable and implemented as part of an organization-wide process to manage risk. The controls address diverse requirements derived from mission and business needs, laws, executive orders, directives, regulations, policies, standards, and guidelines. Finally, the consolidated control catalog addresses security and privacy from a functionality perspective (i.e., the strength of functions and mechanisms provided by the controls) and from an assurance perspective (i.e., the measure of confidence in the security or privacy capability provided by the controls). Addressing functionality and assurance helps to ensure that information technology products and the systems that rely on those products are sufficiently trustworthy.},
	language = {en},
	urldate = {2022-09-10},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Interagency Working Group}},
	month = sep,
	year = {2020},
	doi = {10.6028/NIST.SP.800-53r5},
	note = {Edition: Revision 5},
}

@techreport{Greer2019CPSandIoT,
	address = {Gaithersburg, MD},
	title = {Cyber-physical systems and internet of things},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1900-202.pdf},
	language = {en},
	number = {NIST SP 1900-202},
	urldate = {2021-10-14},
	institution = {National Institute of Standards and Technology},
	author = {Greer, Christopher and Burns, Martin and Wollman, David and Griffor, Edward},
	year = {2019},
	doi = {10.6028/NIST.SP.1900-202},
}

@article{Seymour2022Blackhat,
	title = {Weaponizing data science for social engineering: {Automated} {E2E} spear phishing on {Twitter}},
	abstract = {Introduction and Abstract 1
Background 2 Machine Learning: An Offensive Approach 3 High­level Description of Tool 3 Target Discovery 4 Automated Spear Phishing 5
Conclusion},
	journal = {Black Hat USA},
	author = {Seymour, John and Tully, Philip},
	year = {2016},
}

@article{Goldblum2022DatasetSecurity,
	title = {Dataset {Security} for {Machine} {Learning}: {Data} {Poisoning}, {Backdoor} {Attacks}, and {Defenses}},
	abstract = {As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Madry, Aleksander and Li, Bo and Goldstein, Tom},
	year = {2022},
	keywords = {Backdoor Attacks, Data Poisoning, Data models, Dataset Security, Security, Servers, Toxicology, Training, Training data, Unsolicited e-mail},
}

@inproceedings{bognar_mind_2022,
	title = {Mind the {Gap}: {Studying} the {Insecurity} of {Provably} {Secure} {Embedded} {Trusted} {Execution} {Architectures}},
	shorttitle = {Mind the {Gap}},
	doi = {10.1109/SP46214.2022.9833735},
	abstract = {The security claims of a system can be supported or refuted by different kinds of evidence. On the one hand, attack research uses empirical, experimental, inductive methods to refute security claims. If motivated and competent attackers do not succeed in breaking a specific security property, this provides some support (but no definite proof) that the system is secure.On the other hand, formal methods use mathematical, deductive methods that can prove the security of a model of the system. The process of constructing a proof can uncover vulnerabilities that can then be fixed. The use of formal methods can be very powerful and is attractive because it seems to provide irrefutable evidence of security. However, that evidence applies only to the mathematical model, not to any actual system, and, hence, it is important to understand the gap between the model and the real-world system.In this paper, we present a case study that examines this gap for two embedded security architectures that use formal methods to prove their security properties. Despite strong formal evidence for security, we discover numerous attacks against the implementations, all of which falsify proven security properties. These attacks range from exploiting simple programming errors to a novel DMA-based side-channel attack. The simple attacks demonstrate that the construction of systems and proofs is error-prone, while some of the more sophisticated attacks serve as examples to show that formal methods alone can never guarantee the security of a real-world system.From our case study, we also distill actionable guidelines on how to provide stronger evidence for the security of a system.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Bognar, Marton and Van Bulck, Jo and Piessens, Frank},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	keywords = {Codes, Cognition, Mathematical models, Privacy, Programming, Security, Side-channel attacks},
	pages = {1638--1655},
}

@inproceedings{shi_benchmarking_2016,
	title = {Benchmarking {State}-of-the-{Art} {Deep} {Learning} {Software} {Tools}},
	doi = {10.1109/CCBD.2016.029},
	abstract = {Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
	booktitle = {2016 7th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} ({CCBD})},
	author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
	month = nov,
	year = {2016},
	keywords = {Benchmark testing, Convolutional Neural Networks, Deep Learning, Feed-forward Neural Networks, GPU, Graphics processing units, Instruction sets, Machine learning, Neural networks, Recurrent Neural Networks, Tools, Training},
	pages = {99--104},
}

@inproceedings{chang_ercbench_2010,
	title = {{ERCBench}: {An} {Open}-{Source} {Benchmark} {Suite} for {Embedded} and {Reconfigurable} {Computing}},
	shorttitle = {{ERCBench}},
	doi = {10.1109/FPL.2010.85},
	abstract = {Researchers in embedded and reconfigurable computing are often hindered by a lack of suitable benchmarks with which to accurately evaluate their work. Without a suitable benchmark suite, researchers use either outdated, unrealistic benchmarks or spend valuable time creating their own. In this paper, we present ERCBench - a freely-available, open-source benchmark suite geared towards embedded and reconfigurable computing research. ERCBench benchmarks represent a variety of application areas, including multimedia processing, wireless communications, and cryptography. They consist of synthesizable Verilog models for hardware accelerators and hybrid hardware/software applications that combine software-based control flow with hardware-based computation tasks.},
	booktitle = {2010 {International} {Conference} on {Field} {Programmable} {Logic} and {Applications}},
	author = {Chang, Daniel W. and Jenkins, Christipher D. and Garcia, Philip C. and Gilani, Syed Z. and Aguilera, Paula and Nagarajan, Aishwarya and Anderson, Michael J. and Kenny, Matthew A. and Bauer, Sean M. and Schulte, Michael J. and Compton, Katherine},
	month = aug,
	year = {2010},
	note = {ISSN: 1946-1488},
	keywords = {Benchmark testing, Decoding, Encryption, Field programmable gate arrays, Hardware, Software, Wireless communication, benchmarks, embedded computing, open-source, reconfigurable computing},
	pages = {408--413},
}

@inproceedings{helmuth_psb2_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {{PSB2}: the second program synthesis benchmark suite},
	isbn = {978-1-4503-8350-9},
	shorttitle = {{PSB2}},
	url = {http://doi.org/10.1145/3449639.3459285},
	doi = {10.1145/3449639.3459285},
	abstract = {For the past six years, researchers in genetic programming and other program synthesis disciplines have used the General Program Synthesis Benchmark Suite to benchmark many aspects of automatic program synthesis systems. These problems have been used to make notable progress toward the goal of general program synthesis: automatically creating the types of software that human programmers code. Many of the systems that have attempted the problems in the original benchmark suite have used it to demonstrate performance improvements granted through new techniques. Over time, the suite has gradually become outdated, hindering the accurate measurement of further improvements. The field needs a new set of more difficult benchmark problems to move beyond what was previously possible. In this paper, we describe the 25 new general program synthesis benchmark problems that make up PSB2, a new benchmark suite. These problems are curated from a variety of sources, including programming katas and college courses. We selected these problems to be more difficult than those in the original suite, and give results using PushGP showing this increase in difficulty. These new problems give plenty of room for improvement, pointing the way for the next six or more years of general program synthesis research.},
	urldate = {2022-09-05},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Helmuth, Thomas and Kelly, Peter},
	month = jun,
	year = {2021},
	keywords = {automatic program synthesis, benchmarking, genetic programming},
	pages = {785--794},
}

@inproceedings{tempero_qualitas_2010,
	title = {The {Qualitas} {Corpus}: {A} {Curated} {Collection} of {Java} {Code} for {Empirical} {Studies}},
	shorttitle = {The {Qualitas} {Corpus}},
	doi = {10.1109/APSEC.2010.46},
	abstract = {In order to increase our ability to use measurement to support software development practise we need to do more analysis of code. However, empirical studies of code are expensive and their results are difficult to compare. We describe the Qualitas Corpus, a large curated collection of open source Java systems. The corpus reduces the cost of performing large empirical studies of code and supports comparison of measurements of the same artifacts. We discuss its design, organisation, and issues associated with its development.},
	booktitle = {2010 {Asia} {Pacific} {Software} {Engineering} {Conference}},
	author = {Tempero, Ewan and Anslow, Craig and Dietrich, Jens and Han, Ted and Li, Jing and Lumpe, Markus and Melton, Hayden and Noble, James},
	month = nov,
	year = {2010},
	note = {ISSN: 1530-1362},
	keywords = {Benchmark testing, Empirical studies, Java, Libraries, Pragmatics, Software, Software engineering, curated code corpus, experimental infrastructure},
	pages = {336--345},
}

@inproceedings{Amershi2019SE4ML,
	title = {Software {Engineering} for {Machine} {Learning}: {A} {Case} {Study}},
	doi = {10.1109/ICSE-SEIP.2019.00042},
	abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workﬂow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workﬂow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identiﬁed three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difﬁcult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difﬁcult to handle as distinct modules than traditional software components — models may be “entangled” in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
	year = {2019},
}

@inproceedings{Idowu2021AssetManagementinML,
	title = {Asset {Management} in {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
	booktitle = {International {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	year = {2021},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{liu2018trojaning,
	title = {Trojaning {Attack} on {Neural} {Networks}},
	abstract = {With the fast spread of machine learning techniques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neuron networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driving). We first inverse the neuron network to generate a general trojan trigger, and then retrain the model with external datasets to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that are used to train the model. In practice, the datasets are usually not shared due to privacy or copyright concerns. We use five different applications to demonstrate the power of our attack, and perform a deep analysis on the possible factors that affect the attack. The results show that our attack is highly effective and efficient. The trojaned behaviors can be successfully triggered (with nearly 100\% possibility) without affecting its test accuracy for normal input data. Also, it only takes a small amount of time to attack a complex neuron network model. In the end, we also discuss possible defense against such attacks.},
	booktitle = {Network and {Distributed} {Systems} {Security} ({NDSS}) {Symposium}},
	author = {Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
	year = {2018},
}

@article{zerouali2022impact,
	title = {On the impact of security vulnerabilities in the npm and {RubyGems} dependency networks},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Zerouali, Ahmed and Mens, Tom and Decan, Alexandre and De Roover, Coen},
	year = {2022},
}

@article{reason_contribution_1990,
	title = {The {Contribution} of {Latent} {Human} {Failures} to the {Breakdown} of {Complex} {Systems}},
	volume = {327},
	issn = {0080-4622},
	url = {http://www.jstor.org/stable/55319},
	abstract = {Several recent accidents in complex high-risk technologies had their primary origins in a variety of delayed-action human failures committed long before an emergency state could be recognized. These disasters were due to the adverse conjunction of a large number of causal factors, each one necessary but singly insufficient to achieve the catastrophic outcome. Although the errors and violations of those at the immediate human-system interface often feature large in the post-accident investigations, it is evident that these `front-line' operators are rarely the principal instigators of system breakdown. Their part is often to provide just those local triggering conditions necessary to manifest systemic weaknesses created by fallible decisions made earlier in the organizational and managerial spheres. The challenge facing the human reliability community is to find ways of identifying and neutralizing these latent failures before they combine with local triggering events to breach the system's defences. New methods of risk assessment and risk management are needed if we are to achieve any significant improvements in the safety of complex, well-defended, socio-technical systems. This paper distinguishes between active and latent human failures and proposes a general framework for understanding the dynamics of accident causation. It also suggests ways in which current methods of protection may be enhanced, and concludes by discussing the unusual structural features of `high-reliability' organizations.},
	number = {1241},
	urldate = {2022-08-25},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Reason, J.},
	year = {1990},
	note = {Publisher: The Royal Society},
	pages = {475--484},
}

@article{reason_contribution_1990-1,
	title = {The {Contribution} of {Latent} {Human} {Failures} to the {Breakdown} of {Complex} {Systems}},
	volume = {327},
	issn = {0080-4622},
	url = {http://www.jstor.org/stable/55319},
	abstract = {Several recent accidents in complex high-risk technologies had their primary origins in a variety of delayed-action human failures committed long before an emergency state could be recognized. These disasters were due to the adverse conjunction of a large number of causal factors, each one necessary but singly insufficient to achieve the catastrophic outcome. Although the errors and violations of those at the immediate human-system interface often feature large in the post-accident investigations, it is evident that these `front-line' operators are rarely the principal instigators of system breakdown. Their part is often to provide just those local triggering conditions necessary to manifest systemic weaknesses created by fallible decisions made earlier in the organizational and managerial spheres. The challenge facing the human reliability community is to find ways of identifying and neutralizing these latent failures before they combine with local triggering events to breach the system's defences. New methods of risk assessment and risk management are needed if we are to achieve any significant improvements in the safety of complex, well-defended, socio-technical systems. This paper distinguishes between active and latent human failures and proposes a general framework for understanding the dynamics of accident causation. It also suggests ways in which current methods of protection may be enhanced, and concludes by discussing the unusual structural features of `high-reliability' organizations.},
	number = {1241},
	urldate = {2022-08-25},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Reason, J.},
	year = {1990},
	note = {Publisher: The Royal Society},
	pages = {475--484},
}

@inproceedings{himeno_discussion_2021,
	address = {Held Online},
	title = {Discussion {Structure} {Prediction} {Based} on a {Two}-step {Method}},
	url = {https://aclanthology.org/2021.ranlp-1.61},
	abstract = {Conversations are often held in laboratories and companies. A summary is vital to grasp the content of a discussion for people who did not attend the discussion. If the summary is illustrated as an argument structure, it is helpful to grasp the discussion's essentials immediately. Our purpose in this paper is to predict a link structure between nodes that consist of utterances in a conversation: classification of each node pair into “linked” or “not-linked.” One approach to predict the structure is to utilize machine learning models. However, the result tends to over-generate links of nodes. To solve this problem, we introduce a two-step method to the structure prediction task. We utilize a machine learning-based approach as the first step: a link prediction task. Then, we apply a score-based approach as the second step: a link selection task. Our two-step methods dramatically improved the accuracy as compared with one-step methods based on SVM and BERT.},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Himeno, Takumi and Shimada, Kazutaka},
	month = sep,
	year = {2021},
	pages = {538--546},
}

@article{idowu_asset_2022,
	title = {Asset {Management} in {Machine} {Learning}: {State}-of-research and {State}-of-practice},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Asset {Management} in {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3543847},
	doi = {10.1145/3543847},
	abstract = {Machine learning components are essential for today’s software systems, causing a need to adapt traditional software engineering practices when developing machine-learning-based systems. This need is pronounced due to many development-related challenges of machine learning components such as asset, experiment, and dependency management. Recently, many asset management tools addressing these challenges have become available. It is essential to understand the support such tools offer to facilitate research and practice on building new management tools with native supports for machine learning and software engineering assets.
            This article positions machine learning asset management as a discipline that provides improved methods and tools for performing operations on machine learning assets. We present a feature-based survey of 18 state-of-practice and 12 state-of-research tools supporting machine-learning asset management. We overview their features for managing the types of assets used in machine learning experiments. Most state-of-research tools focus on tracking, exploring, and retrieving assets to address development concerns such as reproducibility, while the state-of-practice tools also offer collaboration and workflow-execution-related operations. In addition, assets are primarily tracked intrusively from the source code through APIs and managed via web dashboards or command-line interfaces. We identify asynchronous collaboration and asset reusability as directions for new tools and techniques.},
	language = {en},
	urldate = {2022-08-08},
	journal = {ACM Computing Surveys},
	author = {Idowu, Samuel and Strüber, Daniel and Berger, Thorsten},
	month = jun,
	year = {2022},
	pages = {3543847},
}

@techreport{security_technical_advisory_group_software_2021,
	title = {Software {Supply} {Chain} {Best} {Practices}},
	url = {https://project.linuxfoundation.org/hubfs/CNCF_SSCP_v1.pdf},
	language = {en},
	urldate = {2022-07-07},
	institution = {Cloud Native Computing Foundation},
	author = {Security Technical Advisory Group},
	month = may,
	year = {2021},
	keywords = {Define, Industry},
}

@techreport{security_technical_advisory_group_secure_2022,
	title = {The {Secure} {Software} {Factory}: {A} reference architecture to securing the software supply chain},
	url = {https://github.com/cncf/tag-security/blob/main/supply-chain-security/secure-software-factory/Secure_Software_Factory_Whitepaper.pdf},
	language = {en},
	institution = {Cloud Native Computing Foundation},
	author = {Security Technical Advisory Group},
	month = jun,
	year = {2022},
}

@inproceedings{vasilakis_breakapp_2018,
	address = {San Diego, CA},
	title = {{BreakApp}: {Automated}, {Flexible} {Application} {Compartmentalization}},
	isbn = {978-1-891562-49-5},
	shorttitle = {{BreakApp}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_08-3_Vasilakis_paper.pdf},
	doi = {10.14722/ndss.2018.23131},
	abstract = {Developers of large-scale software systems may use third-party modules to reduce costs and accelerate release cycles, at some risk to safety and security. BREAKAPP exploits module boundaries to automate compartmentalization of systems and enforce security policies, enhancing reliability and security. BREAKAPP transparently spawns modules in protected compartments while preserving their original behavior. Optional high-level policies decouple security assumptions made during development from requirements imposed for module composition and use. These policies allow ﬁne-tuning trade-offs such as security and performance based on changing threat models or load patterns. Evaluation of BREAKAPP with a prototype implementation for JavaScript demonstrates feasibility by enabling simpliﬁed security hardening of existing systems with low performance overhead.},
	language = {en},
	urldate = {2022-08-06},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Vasilakis, Nikos and Karel, Ben and Roessler, Nick and Dautenhahn, Nathan and DeHon, Andre and Smith, Jonathan M.},
	year = {2018},
}

@techreport{solarwinds_setting_2021,
	title = {Setting the {New} {Standard} in {Secure} {Software} {Development} {The} {SolarWinds} {Next}-{Generation} {Build} {System}},
	url = {https://www.solarwinds.com/resources/whitepaper/setting-the-new-standard-in-secure-software-development-the-solarwinds-next-generation-build-system/delivery},
	language = {en},
	urldate = {2022-06-21},
	institution = {solarwinds},
	author = {Solarwinds},
	month = dec,
	year = {2021},
	keywords = {Industry, White Paper},
}

@misc{lockheed_martin_cyber_2022,
	title = {Cyber {Kill} {Chain}®},
	url = {https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html},
	abstract = {Developed by Lockheed Martin, the Cyber Kill Chain® framework is part of the Intelligence Driven Defense® model for identification and prevention of cyber intrusions activity. The model identifies what the adversaries must complete in order to achieve their objective.},
	language = {en},
	urldate = {2022-08-06},
	journal = {Lockheed Martin},
	author = {Lockheed Martin},
	month = jun,
	year = {2022},
}

@misc{Ladisa2022TaxonomyofAttackonOSSSupplyChains,
	title = {Taxonomy of {Attacks} on {Open}-{Source} {Software} {Supply} {Chains}},
	url = {http://arxiv.org/abs/2204.04008},
	abstract = {The widespread dependency on open-source software makes it a fruitful target for malicious actors, as demonstrated by recurring attacks. The complexity of today’s opensource supply chains results in a signiﬁcant attack surface, giving attackers numerous opportunities to reach the goal of injecting malicious code into open-source artifacts that is then downloaded and executed by victims.},
	publisher = {arXiv},
	author = {Ladisa, Piergiorgio and Plate, Henrik and Martinez, Matias and Barais, Olivier},
	year = {2022},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering, Taxonomy, attacks},
}

@misc{noauthor_empirical_nodate,
	title = {An {Empirical} {Study} of {Trust} \& {Safety} {Risk} {Management} in {Social} {Media} {Platforms}},
	url = {https://www.overleaf.com/project/6286b30078a93521c96ac2e1},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-08-02},
}

@inproceedings{Xu2017SecureSupplyChainManagementSystemBasedonPublicLedger,
	title = {{CoC}: {Secure} {Supply} {Chain} {Management} {System} {Based} on {Public} {Ledger}},
	abstract = {Modern supply chain is a complex system and plays an important role for different sectors under the globalization economic integration background. Supply chain management system is proposed to handle the increasing complexity and improve the efficiency of flows of goods. It is also useful to prevent potential frauds and guarantee trade compliance. Currently, most companies maintain their own IT system for supply chain management. However, this approach has some limitations that prevent one to get most of the supply chain information. Using emerging decentralized ledger technology to build supply chain management system is a promising direction. However, decentralized ledger usually suffers from low performance and lack of capability to protect information stored on the ledger. To overcome these challenges, we propose CoC, a novel supply chain management system based on hybrid decentralized ledger. We develop an efficient block construction method with the model and security mechanism to prevent unauthorized access to data stored on the ledger.},
	booktitle = {International {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Xu, Lei and Chen, Lin and Gao, Zhimin and Lu, Yang and Shi, Weidong},
	year = {2017},
	keywords = {Companies, Customer services, Security, Supply chain management, Supply chains, Transportation},
}

@inproceedings{xu_coc_2017,
	title = {{CoC}: {Secure} {Supply} {Chain} {Management} {System} {Based} on {Public} {Ledger}},
	shorttitle = {{CoC}},
	doi = {10.1109/ICCCN.2017.8038514},
	abstract = {Modern supply chain is a complex system and plays an important role for different sectors under the globalization economic integration background. Supply chain management system is proposed to handle the increasing complexity and improve the efficiency of flows of goods. It is also useful to prevent potential frauds and guarantee trade compliance. Currently, most companies maintain their own IT system for supply chain management. However, this approach has some limitations that prevent one to get most of the supply chain information. Using emerging decentralized ledger technology to build supply chain management system is a promising direction. However, decentralized ledger usually suffers from low performance and lack of capability to protect information stored on the ledger. To overcome these challenges, we propose CoC, a novel supply chain management system based on hybrid decentralized ledger. We develop an efficient block construction method with the model and security mechanism to prevent unauthorized access to data stored on the ledger.},
	booktitle = {2017 26th {International} {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Xu, Lei and Chen, Lin and Gao, Zhimin and Lu, Yang and Shi, Weidong},
	month = jul,
	year = {2017},
	keywords = {Companies, Customer services, Security, Supply chain management, Supply chains, Transportation},
	pages = {1--6},
}

@misc{dirk_argument_2005,
	title = {Argument {Diagramming} of {Meeting} {Conversations}},
	url = {https://ris.utwente.nl/ws/files/246973183/Rienks2005argument.pdf},
	abstract = {this paper we introduce the Twente Argument Schema, a diagramming model, developed in order to structure textual units by providing an annotation enabling people as well as automatic systems to find answers to questions related to the decision making process. To obtain these structures we consider (remote-control) design meetings used in the EU project AMI    (Op den Akker et al. [2005]) from which the transcripts are known. As design can be considered an `ill-structured' or `wicked' problem, the approach in a collaborative problem solving process one encounters in these kinds of meetings is generally through a lot of argumentative discourse [Buckingham Shum,  2003]. We have tried to identify the various functions of the argumentative aspects of the di\#erent contributions made by the participants and have defined labels to relate these contributions to each other. The resulting structure provides extra insight into which issues were debated and which statements were put forward. The schema contains labels for transcript fragments as well as labels for relations between these fragments. The resulting structure captures the discussions and can be aligned with models structuring arguments developed by argumentation theorists (c.f. Toulmin [1958])},
	author = {Dirk, Rutger Rienks},
	year = {2005},
}

@misc{tao_why_2019,
	title = {Why {We} {Can} {Not} {Resist} {Social} {Media}?},
	shorttitle = {Why {We} {Can} {Not} {Resist} {Social} {Media}?},
	url = {https://yujietao.me/blog/social-media.html},
	abstract = {While there has been many efforts trying to convince people not using social media, I, on the hand, wish to explore why people can't resist it and what's the human needs behind it. Our tendency to expression, interaction and self-presentation drive us to use social media.},
	language = {en},
	urldate = {2022-04-22},
	author = {Tao, Yujie},
	year = {2019},
}

@inproceedings{Tan2022DLSupplyChain,
	address = {Pittsburgh Pennsylvania},
	title = {An exploratory study of deep learning supply chain},
	abstract = {Deep learning becomes the driving force behind many contemporary technologies and has been successfully applied in many fields. Through software dependencies, a multi-layer supply chain (SC) with a deep learning framework as the core and substantial downstream projects as the periphery has gradually formed and is constantly developing. However, basic knowledge about the structure and characteristics of the SC is lacking, which hinders effective support for its sustainable development. Previous studies on software SC usually focus on the packages in different registries without paying attention to the SCs derived from a single project. We present an empirical study on two deep learning SCs: TensorFlow and PyTorch SCs. By constructing and analyzing their SCs, we aim to understand their structure, application domains, and evolutionary factors. We find that both SCs exhibit a short and sparse hierarchy structure. Overall, the relative growth of new projects increases month by month. Projects have a tendency to attract downstream projects shortly after the release of their packages, later the growth becomes faster and tends to stabilize. We propose three criteria to identify vulnerabilities and identify 51 types of packages and 26 types of projects involved in the two SCs. A comparison reveals their similarities and differences, e.g., TensorFlow SC provides a wealth of packages in experiment result analysis, while PyTorch SC contains more specific framework packages. By fitting the GAM model, we find that the number of dependent packages is significantly negatively associated with the number of downstream projects, but the relationship with the number of authors is nonlinear. Our findings ∗Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tan, Xin and Gao, Kai and Zhou, Minghui and Zhang, Li},
	year = {2022},
}

@book{toulmin_uses_2003,
	title = {The {Uses} of {Argument}},
	isbn = {978-0-521-53483-3},
	abstract = {Traditionally, logic has been claimed to be 'the science of rational argument', but the relevance to our everyday disputes of the formal logician's results has remained unclear. The abstract character of traditional logic cuts the subject off from practical considerations; Mr Toulmin enquires why this is so, and shows how an alternative conception can be of more general value. Starting from an examination of the actual procedures in different fields of argument - the practice, as opposed to the theory, of logic - he discloses a richer variety than is allowed for by any available system. He argues that jurisprudence rather than mathematics should be the logician's model in analysing rational procedures, and that logic should be a comparative and not a purely formal study. These suggestions lead to conclusions which many will consider controversial; though they will also be widely recognized as interesting and illuminating. This book extends into general philosophy lines of enquiry already sketched by Mr Toulmin in his earlier books on ethics and the philosophy of science. The ordinary reader will find in it the same clarity and intelligibility; and the professional philosopher will acknowledge the same power to break new ground (and circumvent old difficulties) by posing fresh and stimulating questions.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Toulmin, Stephen E.},
	month = jul,
	year = {2003},
	note = {Google-Books-ID: 8UYgegaB1S0C},
	keywords = {Philosophy / General, Philosophy / Logic, Psychology / Cognitive Psychology \& Cognition},
}

@article{haley_security_2008,
	title = {Security {Requirements} {Engineering}: {A} {Framework} for {Representation} and {Analysis}},
	volume = {34},
	issn = {1939-3520},
	shorttitle = {Security {Requirements} {Engineering}},
	doi = {10.1109/TSE.2007.70754},
	abstract = {This paper presents a framework for security requirements elicitation and analysis. The framework is based on constructing a context for the system, representing security requirements as constraints, and developing satisfaction arguments for the security requirements. The system context is described using a problem-oriented notation, then is validated against the security requirements through construction of a satisfaction argument. The satisfaction argument consists of two parts: a formal argument that the system can meet its security requirements and a structured informal argument supporting the assumptions expressed in the formal argument. The construction of the satisfaction argument may fail, revealing either that the security requirement cannot be satisfied in the context or that the context does not contain sufficient information to develop the argument. In this case, designers and architects are asked to provide additional design information to resolve the problems. We evaluate the framework by applying it to a security requirements analysis within an air traffic control technology evaluation project.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Haley, Charles and Laney, Robin and Moffett, Jonathan and Nuseibeh, Bashar},
	month = jan,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Air traffic control, Application software, Computer Society, Computer security, Credit cards, Data security, Information security, Internet, Requirements/Specifications, Security, Software engineering, Software/Software Engineering, Statistics},
	pages = {133--153},
}

@article{kelly_systematic_2004,
	title = {A {Systematic} {Approach} to {Safety} {Case} {Management}},
	volume = {113},
	issn = {0096-736X},
	url = {https://www.jstor.org/stable/44699541},
	abstract = {In Europe, over recent years, there has been a marked shift in the regulatory approach to ensuring system safety. Whereas compliance with prescriptive safety codes and standards was previously the norm, the responsibility has now shifted back onto the developers and operators to construct and present well reasoned arguments that their systems achieve acceptable levels of safety. These arguments (together with supporting evidence) are typically referred to as a "safety case". This paper describes the role and purpose of a safety case (as defined by current safety and regulatory standards). Safety arguments within safety cases are often poorly communicated. This paper presents a technique called GSN (Goal Structuring Notation) that is increasingly being used in safety-critical industries to improve the structure, rigor, and clarity of safety arguments. Based upon the GSN approach, the paper also describes how an evolutionary and systematic approach to safety case construction, in step with system development, can be facilitated.},
	urldate = {2022-07-27},
	journal = {SAE Transactions},
	author = {Kelly, Tim},
	year = {2004},
	note = {Publisher: SAE International},
	pages = {257--266},
}

@inproceedings{franqueira_risk_2011,
	title = {Risk and argument: {A} risk-based argumentation method for practical security},
	shorttitle = {Risk and argument},
	doi = {10.1109/RE.2011.6051659},
	abstract = {When showing that a software system meets certain security requirements, it is often necessary to work with formal and informal descriptions of the system behavior, vulnerabilities, and threats from potential attackers. In earlier work, Haley et al. [1] showed structured argumentation could deal with such mixed descriptions. However, incomplete and uncertain information, and limited resources force practitioners to settle for good-enough security. To deal with these conditions of practice, we extend the method of Haley et al. with risk assessment. The proposed method, RISA (RIsk assessment in Security Argumentation), uses public catalogs of security expertise to support the risk assessment, and to guide the security argumentation in identifying rebuttals and mitigations for security requirements satisfaction. We illustrate RISA with a realistic example of PIN Entry Device.},
	booktitle = {2011 {IEEE} 19th {International} {Requirements} {Engineering} {Conference}},
	author = {Franqueira, Virginia N. L. and Tun, Thein Than and Yu, Yijun and Wieringa, Roel and Nuseibeh, Bashar},
	month = aug,
	year = {2011},
	note = {ISSN: 2332-6441},
	keywords = {Argumentation, Catalogs, Common Attack Pattern Enumeration and Classification (CAPEC), Common Weakness Enumeration (CWE), Context, Cryptography, Risk Assessment, Risk management, Security Requirements, Software, Systematics},
	pages = {239--248},
}

@article{krigsman_annual_2009,
	title = {Annual cost of {IT} failure: \$6.2 trillion},
	url = {https://www.zdnet.com/article/annual-cost-of-it-failure-6-2-trillion/},
	journal = {ZDNet},
	author = {Krigsman, Michael},
	year = {2009},
}

@article{kelly_systematic_2004-1,
	title = {A {Systematic} {Approach} to {Safety} {Case} {Management}},
	volume = {113},
	issn = {0096-736X},
	url = {https://www.jstor.org/stable/44699541},
	abstract = {In Europe, over recent years, there has been a marked shift in the regulatory approach to ensuring system safety. Whereas compliance with prescriptive safety codes and standards was previously the norm, the responsibility has now shifted back onto the developers and operators to construct and present well reasoned arguments that their systems achieve acceptable levels of safety. These arguments (together with supporting evidence) are typically referred to as a "safety case". This paper describes the role and purpose of a safety case (as defined by current safety and regulatory standards). Safety arguments within safety cases are often poorly communicated. This paper presents a technique called GSN (Goal Structuring Notation) that is increasingly being used in safety-critical industries to improve the structure, rigor, and clarity of safety arguments. Based upon the GSN approach, the paper also describes how an evolutionary and systematic approach to safety case construction, in step with system development, can be facilitated.},
	urldate = {2022-07-27},
	journal = {SAE Transactions},
	author = {Kelly, Tim},
	year = {2004},
	note = {Publisher: SAE International},
	pages = {257--266},
}

@book{jurafsky2000speech,
	title = {Speech \& language processing},
	publisher = {Pearson Education India},
	author = {Jurafsky, Dan},
	year = {2000},
}

@misc{noauthor_csrb_nodate,
	title = {{CSRB} {\textbar} {CISA}},
	url = {https://www.cisa.gov/cyber-safety-review-board},
	urldate = {2022-07-27},
}

@article{carroll_making_2015,
	title = {Making {Sense} of {Ambiguity} through {Dialogue} and {Collaborative} {Action}},
	volume = {23},
	issn = {09660879},
	shorttitle = {Making {Sense} of {Ambiguity} through {Dialogue} and {Collaborative} {Action}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1468-5973.12075},
	doi = {10.1111/1468-5973.12075},
	language = {en},
	number = {2},
	urldate = {2022-05-23},
	journal = {Journal of Contingencies and Crisis Management},
	author = {Carroll, John S.},
	month = jun,
	year = {2015},
	pages = {59--65},
}

@misc{noauthor_azure_nodate,
	title = {Azure status history {\textbar} {Microsoft} {Azure}},
	url = {https://status.azure.com/en-us/status/history/},
	urldate = {2022-07-27},
}

@article{booch1996unified,
	title = {The unified modeling language},
	volume = {14},
	number = {13},
	journal = {Unix Review},
	author = {Booch, Grady and Jacobson, Ivar and Rumbaugh, James and {others}},
	year = {1996},
	pages = {5},
}

@misc{noauthor_google_nodate,
	title = {Google {Cloud} {Service} {Health}},
	url = {https://status.cloud.google.com/summary},
	urldate = {2022-07-27},
}

@inproceedings{amusuo2022SoftwareFailureAnalysis,
	title = {Reflections on software failure analysis},
	booktitle = {{ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} — {Ideas}, {Visions}, and {Reflections} track ({ESEC}/{FSE}-{IVR})},
	author = {Amusuo, Paschal and Sharma, Aishwarya and Rao, Siddharth R and Vincent, Abbey and Davis, James C},
	year = {2022},
}

@article{birks2008memoing,
	title = {Memoing in qualitative research: {Probing} data and processes},
	volume = {13},
	number = {1},
	journal = {Journal of research in nursing},
	author = {Birks, Melanie and Chapman, Ysanne and Francis, Karen},
	year = {2008},
	note = {Publisher: Sage Publications Sage UK: London, England},
	pages = {68--75},
}

@article{holton2007coding,
	title = {The coding process and its challenges},
	volume = {3},
	journal = {The Sage handbook of grounded theory},
	author = {Holton, Judith A},
	year = {2007},
	pages = {265--289},
}

@article{landis1977application,
	title = {An application of hierarchical kappa-type statistics in the assessment of majority agreement among multiple observers},
	journal = {Biometrics. Journal of the International Biometric Society},
	author = {Landis, J Richard and Koch, Gary G},
	year = {1977},
	note = {Publisher: JSTOR},
	pages = {363--374},
}

@article{jaradat2015complex,
	title = {Complex system governance requires systems thinking-how to find systems thinkers},
	volume = {6},
	number = {1-2},
	journal = {International Journal of System of Systems Engineering},
	author = {Jaradat, Raed M},
	year = {2015},
	note = {Publisher: Inderscience Publishers},
	pages = {53--70},
}

@article{davis2020lake,
	title = {The {Lake} {Urmia} vignette: a tool to assess understanding of complexity in socio-environmental systems},
	volume = {36},
	number = {2},
	journal = {System Dynamics Review},
	author = {Davis, Kirsten and Ghaffarzadegan, Navid and Grohs, Jacob and Grote, Dustin and Hosseinichimeh, Niyousha and Knight, David and Mahmoudi, Hesam and Triantis, Konstantinos},
	year = {2020},
	note = {Publisher: Wiley Online Library},
	pages = {191--222},
}

@book{burns_autonomy_2018,
	edition = {1},
	title = {Autonomy: {The} {Quest} to {Build} the {Driverless} {Car}―{And} {How} {It} {Will} {Reshape} {Our} {World}},
	isbn = {978-0-06-266112-8},
	language = {English},
	publisher = {Ecco},
	author = {Burns, Lawrence and Shulgan, Christopher},
	year = {2018},
}

@article{christensen_risk_2003,
	title = {Risk terminology—a platform for common understanding and better communication},
	volume = {103},
	issn = {0304-3894},
	url = {https://www.sciencedirect.com/science/article/pii/S0304389403000396},
	doi = {10.1016/S0304-3894(03)00039-6},
	abstract = {The sciences analyzing and describing risks are relatively new and developing, and the associated terminologies are developing as well. This has led to ambiguity in the use of terms, both between different risk sciences and between the different parties involved in risk debates. Only recently, major vocabularies have been compiled by authoritative agencies. Some of these vocabularies are examined and explained based on a division into fundamental and action oriented risk terms. Fundamental terms are associated with description and characterization of the chemical, biological and physical processes leading from risk source(s) to possible consequences/effects. The approach to these terms is based on a cause–effect skeleton. The action oriented terms cover administrative, scientific, sociological, etc. processes associated with the work of identifying, characterizing, regulating and communicating risks in the society, and their internal connection and iterative character have been illustrated. Focus is laid on engineering and toxicological risks, but to some extent, the thoughts presented may be extrapolated to other areas. Differences in applied terminology probably cannot be eliminated, but they can be identified and clarified for better understanding. With the present paper, the authors hope to contribute to reducing the probability of derailing risk discussions from the risk issue itself.},
	language = {en},
	number = {3},
	urldate = {2022-07-25},
	journal = {Journal of Hazardous Materials},
	author = {Christensen, Frans Møller and Andersen, Ole and Duijm, Nijs Jan and Harremoës, Poul},
	month = oct,
	year = {2003},
	keywords = {Cause–effect, Risk analysis, Risk assessment, Risk management, Risk terminology},
	pages = {181--203},
}

@inproceedings{basili_lessons_2002,
	address = {Orlando, FL, USA},
	title = {Lessons learned from 25 years of process improvement: the rise and fall of the {NASA} software engineering laboratory},
	isbn = {978-1-58113-472-8},
	shorttitle = {Lessons learned from 25 years of process improvement},
	url = {https://ieeexplore.ieee.org/document/1007957},
	doi = {10.1109/ICSE.2002.1007957},
	abstract = {For 25 years the NASA/GSFC Software Engineering Laboratory (SEL) has been a major resource in software process improvement activities. But due to a changing climate at NASA, agency reorganization, and budget cuts, the SEL has lost much of its impact. In this paper we describe the history of the SEL and give some lessons learned on what we did right, what we did wrong, and what others can learn from our experiences. We briefly describe the research that was conducted by the SEL, describe how we evolved our understanding of software process improvement, and provide a set of lessons learned and hypotheses that should enable future groups to learn from and improve on our quarter century of experiences.},
	language = {en},
	urldate = {2022-07-25},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Software} {Engineering}. {ICSE} 2002},
	publisher = {ACM},
	author = {Basili, V.R. and McGarry, F.E. and Pajerski, R. and Zelkowitz, M.V.},
	year = {2002},
	pages = {69--79},
}

@article{money_understanding_2021,
	title = {Understanding {Failed} {Software} {Projects} through {Forensic} {Analysis}},
	issn = {0887-4417, 2380-2057},
	url = {https://www.tandfonline.com/doi/full/10.1080/08874417.2021.1950076},
	doi = {10.1080/08874417.2021.1950076},
	abstract = {Software project failure has cost trillions of dollars over the past fifty years. The many reasons for failure are widely discussed in the project management and software development literatures. This paper assesses methods for software project forensic analysis to develop principles and practices that would provide a comprehensive and more complete understanding of the why, when, where, how, and what causes software project failure. This paper discusses the approaches followed to determine the causes of failures, examines the issues and challenges of assessing failures, and proposes a structured approach (forensic analysis) for determining the causes of software project failures.},
	language = {en},
	urldate = {2022-07-24},
	journal = {Journal of Computer Information Systems},
	author = {Money, William H. and Kaisler, Stephen H. and Cohen, Stephen J.},
	month = jul,
	year = {2021},
	pages = {1--14},
}

@article{chillarege1992orthogonal,
	title = {Orthogonal defect classification-a concept for in-process measurements},
	volume = {18},
	number = {11},
	journal = {IEEE Transactions on software Engineering},
	author = {Chillarege, Ram and Bhandari, Inderpal S and Chaar, Jarir K and Halliday, Michael J and Moebus, Diane S and Ray, Bonnie K and Wong, Man-Yuen},
	year = {1992},
	pages = {943--956},
}

@inproceedings{seaman2008defect,
	title = {Defect categorization: making use of a decade of widely varying historical data},
	booktitle = {Proceedings of the {Second} {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	author = {Seaman, Carolyn B and Shull, Forrest and Regardie, Myrna and Elbert, Denis and Feldmann, Raimund L and Guo, Yuepu and Godfrey, Sally},
	year = {2008},
	pages = {149--157},
}

@phdthesis{stringfellow2010accident,
	title = {Accident analysis and hazard analysis for human and organizational factors},
	school = {Massachusetts Institute of Technology},
	author = {Stringfellow, Margaret Virgina},
	year = {2010},
}

@article{nelson2008stamp,
	title = {A {STAMP} analysis of the {LEX} {COMAIR} 5191 accident},
	journal = {Master's thesis, Lund},
	author = {Nelson, Paul S},
	year = {2008},
	note = {Publisher: Citeseer},
}

@article{arnold2009qualitative,
	title = {A qualitative comparative analysis of {SOAM} and {STAMP} in {ATM} occurrence investigation},
	journal = {Master's Thesis, Lund University, Lund, Sweden},
	author = {Arnold, Richard},
	year = {2009},
}

@book{ohno1988toyota,
	title = {Toyota production system: beyond large-scale production},
	publisher = {Productivity press},
	author = {Ohno, Taiichi and Bodek, Norman},
	year = {1988},
}

@techreport{ellison_evaluating_2010,
	title = {Evaluating and {Mitigating} {Software} {Supply} {Chain} {Security} {Risks}},
	url = {https://apps.dtic.mil/sti/citations/ADA522538},
	abstract = {The Department of Defense DoD is concerned that security vulnerabilities could be inserted into software that has been developed outside of the DoDs supervision or control. This report presents an initial analysis of how to evaluate and mitigate the risk that such unauthorized insertions have been made. The analysis is structured in terms of actions that should be taken in each phase of the DoD acquisition life cycle},
	language = {en},
	urldate = {2022-06-21},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
	author = {Ellison, Robert J. and Goodenough, John B. and Weinstock, Charles B. and Woody, Carol},
	month = may,
	year = {2010},
	note = {Section: Technical Reports},
	keywords = {Government},
}

@techreport{boyens_cybersecurity_2022,
	title = {Cybersecurity {Supply} {Chain} {Risk} {Management} {Practices} for {Systems} and {Organizations}},
	url = {https://csrc.nist.gov/publications/detail/sp/800-161/rev-1/final},
	abstract = {Organizations are concerned about the risks associated with products and services that may potentially contain malicious functionality, are counterfeit, or are vulnerable due to poor manufacturing and development practices within the supply chain. These risks are associated with an enterprise’s decreased visibility into and understanding of how the technology they acquire is developed, integrated, and deployed or the processes, procedures, standards, and practices used to ensure the security, resilience, reliability, safety, integrity, and quality of the products and services.  This publication provides guidance to organizations on identifying, assessing, and mitigating cybersecurity risks throughout the supply chain at all levels of their organizations. The publication integrates cybersecurity supply chain risk management (C-SCRM) into risk management activities by applying a multilevel, C-SCRM-specific approach, including guidance on the development of C-SCRM strategy implementation...},
	language = {en},
	number = {NIST Special Publication (SP) 800-161 Rev. 1},
	urldate = {2022-06-16},
	institution = {National Institute of Standards and Technology},
	author = {Boyens, Jon and Smith, Angela and Bartol, Nadya and Winkler, Kris and Holbrook, Alex and Fallon, Matthew},
	month = may,
	year = {2022},
	doi = {10.6028/NIST.SP.800-161r1},
	keywords = {Government},
}

@incollection{Leveson2009SWSystemSafety,
	title = {Software {System} {Safety}},
	booktitle = {Safety {Design} for {Space} {Systems}},
	publisher = {Elsevier},
	author = {Leveson, Nancy G. and Weiss, Kathryn Anne},
	year = {2009},
}

@misc{Hata2022SWSupplyChainMapHowReuseNetworksExpand,
	title = {Software {Supply} {Chain} {Map}: {How} {Reuse} {Networks} {Expand}},
	url = {http://arxiv.org/abs/2204.06531},
	abstract = {Clone-and-own is a typical code reuse approach because of its simplicity and efficiency. Cloned software components are maintained independently by a new owner. These clone-and-own operations can be occurred sequentially, that is, cloned components can be cloned again and owned by other new owners on the supply chain. In general, code reuse is not documented well, consequently, appropriate changes like security patches cannot be propagated to descendant software projects. However, the OpenChain Project defined identifying and tracking source code reuses as responsibilities of FLOSS software staffs. Hence supporting source code reuse awareness is in a real need. This paper studies software reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses of source code can be identified by file-level clone set detection. Since change histories are associated with files, we can determine origins and destinations in reusing across multiple software by considering times. By building software supply chain maps, we find that clone-and-own is prevalent in FLOSS development, and set of files are reused widely and repeatedly. These observations open up future challenges of maintaining and tracking global software genealogies.},
	publisher = {arXiv},
	author = {Hata, Hideaki and Ishio, Takashi},
	year = {2022},
	keywords = {Computer Science - Software Engineering},
}

@misc{hata_software_2022,
	title = {Software {Supply} {Chain} {Map}: {How} {Reuse} {Networks} {Expand}},
	shorttitle = {Software {Supply} {Chain} {Map}},
	url = {http://arxiv.org/abs/2204.06531},
	abstract = {Clone-and-own is a typical code reuse approach because of its simplicity and efficiency. Cloned software components are maintained independently by a new owner. These clone-and-own operations can be occurred sequentially, that is, cloned components can be cloned again and owned by other new owners on the supply chain. In general, code reuse is not documented well, consequently, appropriate changes like security patches cannot be propagated to descendant software projects. However, the OpenChain Project defined identifying and tracking source code reuses as responsibilities of FLOSS software staffs. Hence supporting source code reuse awareness is in a real need. This paper studies software reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses of source code can be identified by file-level clone set detection. Since change histories are associated with files, we can determine origins and destinations in reusing across multiple software by considering times. By building software supply chain maps, we find that clone-and-own is prevalent in FLOSS development, and set of files are reused widely and repeatedly. These observations open up future challenges of maintaining and tracking global software genealogies.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Hata, Hideaki and Ishio, Takashi},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06531 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{aldiabat_data_2018,
	title = {Data {Saturation}: {The} {Mysterious} {Step} {In} {Grounded} {Theory} {Method}},
	issn = {2160-3715, 1052-0147},
	shorttitle = {Data {Saturation}},
	url = {https://nsuworks.nova.edu/tqr/vol23/iss1/18/},
	doi = {10.46743/2160-3715/2018.2994},
	abstract = {The aim of this paper is to provide a discussion that is broad in both depth and breadth, about the concept of data saturation in Grounded Theory. It is expected that this knowledge will provide a helpful resource for (a) the novice researcher using a Grounded Theory approach, or for (b) graduate students currently enrolled in a qualitative research course, and for (c) instructors who teach or supervise qualitative research projects. The following topics are discussed in this paper: (1) definition of data saturation in Grounded Theory (GT); (2) factors pertaining to data saturation; (3) factors that hinder data saturation; (4) the relationship between theoretical sampling and data saturation; (5) the relationship between constant comparative and data saturation; and (6) illustrative examples of strategies used during data collection to maximize the components of rigor that Yonge and Stewin (1988) described as Credibility, Transferability or Fittingness, Dependability or Auditability, and Confirmability.},
	language = {en},
	urldate = {2022-07-21},
	journal = {The Qualitative Report},
	author = {Aldiabat, Khaldoun and Le Navenec, Carole-Lynne},
	month = jan,
	year = {2018},
}

@misc{allspaw_blameless_2012,
	title = {Blameless {PostMortems} and a {Just} {Culture}},
	url = {https://www.etsy.com/codeascraft/blameless-postmortems},
	urldate = {2022-07-20},
	author = {Allspaw, John},
	year = {2012},
}

@article{kuhn2010practical,
	title = {Practical combinatorial testing},
	volume = {800},
	number = {142},
	journal = {NIST special Publication},
	author = {Kuhn, D Richard and Kacker, Raghu N and Lei, Yu and {others}},
	year = {2010},
	pages = {142},
}

@book{rosenthal2020chaos,
	title = {Chaos engineering: system resiliency in practice},
	publisher = {O'Reilly Media},
	author = {Rosenthal, Casey and Jones, Nora},
	year = {2020},
}

@article{riesener_derivation_2020,
	title = {Derivation of description features for engineering change request by aid of latent dirichlet allocation},
	volume = {1},
	issn = {2633-7762},
	url = {https://www.cambridge.org/core/journals/proceedings-of-the-design-society-design-conference/article/derivation-of-description-features-for-engineering-change-request-by-aid-of-latent-dirichlet-allocation/810E17C0CF8762F28069F43AD7692E50},
	doi = {10.1017/dsd.2020.98},
	abstract = {Complex products and shorter development cycles lead to an increasing number of engineering changes. In order to be able to process these changes more effectively and efficiently, this paper develops a description model as a first step towards a data driven approach of processing engineering change requests. The description model is systematically derived from literature using text mining and natural language processing techniques. An example of the application is given by an automated classification based on similarity calculations between new and historic engineering change requests.},
	language = {en},
	urldate = {2022-07-20},
	journal = {Proceedings of the Design Society: DESIGN Conference},
	author = {Riesener, M. and Dölle, C. and Mendl-Heinisch, M. and Schuh, G. and Keuper, A.},
	month = may,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	keywords = {automated classification, data mining, engineering change, latent dirichlet allocation, product development},
	pages = {697--706},
}

@misc{Aryal2022SurveyonAdvAttacks4MalwareAnalysis,
	title = {A {Survey} on {Adversarial} {Attacks} for {Malware} {Analysis}},
	url = {http://arxiv.org/abs/2111.08223},
	abstract = {Machine learning has witnessed tremendous growth in its adoption and advancement in the last decade. The evolution of machine learning from traditional algorithms to modern deep learning architectures has shaped the way today's technology functions. Its unprecedented ability to discover knowledge/patterns from unstructured data and automate the decision-making process led to its application in wide domains. High flying machine learning arena has been recently pegged back by the introduction of adversarial attacks. Adversaries are able to modify data, maximizing the classification error of the models. The discovery of blind spots in machine learning models has been exploited by adversarial attackers by generating subtle intentional perturbations in test samples. Increasing dependency on data has paved the blueprint for ever-high incentives to camouflage machine learning models. To cope with probable catastrophic consequences in the future, continuous research is required to find vulnerabilities in form of adversarial and design remedies in systems. This survey aims at providing the encyclopedic introduction to adversarial attacks that are carried out against malware detection systems. The paper will introduce various machine learning techniques used to generate adversarial and explain the structure of target files. The survey will also model the threat posed by the adversary and followed by brief descriptions of widely accepted adversarial algorithms. Work will provide a taxonomy of adversarial evasion attacks on the basis of attack domain and adversarial generation techniques. Adversarial evasion attacks carried out against malware detectors will be discussed briefly under each taxonomical headings and compared with concomitant researches. Analyzing the current research challenges in an adversarial generation, the survey will conclude by pinpointing the open future research directions.},
	publisher = {arXiv},
	author = {Aryal, Kshitiz and Gupta, Maanak and Abdelsalam, Mahmoud},
	year = {2022},
	keywords = {Computer Science - Cryptography and Security},
}

@article{hosny_modelhubai_nodate,
	title = {{ModelHub}.{AI}: {Dissemination} {Platform} for {Deep} {Learning} {Models}},
	abstract = {Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.},
	language = {en},
	author = {Hosny, Ahmed and Schwier, Michael and Berger, Christoph and Örnek, Evin P and Turan, Mehmet and Tran, Phi V and Isensee, Fabian and Maier-Hein, Klaus H and McKinley, Richard and Lu, Michael T and Hoffmann, Udo and Menze, Bjoern and Bakas, Spyridon and Fedorov, Andriy and Aerts, Hugo JWL},
	pages = {12},
}

@techreport{graham-cumming_details_2019,
	title = {Details of the {Cloudflare} outage on {July} 2, 2019},
	url = {https://web.archive.org/web/20190712160002/https://blog.cloudflare.com/ details-of-the-cloudflare-outage-on-july-2-2019/},
	author = {Graham-Cumming, John},
	year = {2019},
}

@article{kelly_dexcom_2019,
	title = {Dexcom {CEO} on remote monitoring outage: ‘{No} excuses. {We} can do better’},
	url = {https://www.medtechdive.com/news/dexcom-ceo-on-remote-monitoring-outage-no-excuses-we-can-do-better/569122/},
	journal = {MedTechDive},
	author = {Kelly, Susan},
	year = {2019},
}

@misc{dexcom_dexcom_2019,
	title = {Dexcom {Status} {Updates}},
	author = {Dexcom},
	year = {2019},
}

@techreport{nehman_cloudflares_2018,
	title = {Cloudflare’s plan to protect the whole internet comes into focus},
	url = {https://www.wired.com/story/cloudflare-spectrum-iot-protection/},
	author = {Nehman, Lily},
	year = {2018},
}

@article{abdul2019comprehensive,
	title = {A comprehensive study of security and privacy guidelines, threats, and countermeasures: {An} {IoT} perspective},
	volume = {8},
	number = {2},
	journal = {Journal of Sensor and Actuator Networks},
	author = {Abdul-Ghani, Hezam Akram and Konstantas, Dimitri},
	year = {2019},
	note = {Publisher: MDPI},
	pages = {22},
}

@article{ayewah_using_2008,
	title = {Using {Static} {Analysis} to {Find} {Bugs}},
	volume = {25},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/4602670/},
	doi = {10.1109/MS.2008.130},
	language = {en},
	number = {5},
	urldate = {2022-07-19},
	journal = {IEEE Software},
	author = {Ayewah, Nathaniel and Pugh, William and Hovemeyer, David and Morgenthaler, J. David and Penix, John},
	month = sep,
	year = {2008},
	pages = {22--29},
}

@inproceedings{johnson_why_2013,
	address = {San Francisco, CA, USA},
	title = {Why don't software developers use static analysis tools to find bugs?},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606613/},
	doi = {10.1109/ICSE.2013.6606613},
	abstract = {Using static analysis tools for automating code inspections can be beneﬁcial for software engineers. Such tools can make ﬁnding bugs, or software defects, faster and cheaper than manual inspections. Despite the beneﬁts of using static analysis tools to ﬁnd bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneﬁcial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers ﬁx defects.},
	language = {en},
	urldate = {2022-07-19},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
	month = may,
	year = {2013},
	pages = {672--681},
}

@article{cryst_introducing_2021,
	title = {Introducing the {Journal} of {Online} {Trust} and {Safety}},
	volume = {1},
	copyright = {Copyright (c) 2021},
	issn = {2770-3142},
	url = {https://tsjournal.org/index.php/jots/article/view/8},
	abstract = {Introducing the Journal of Online Trust and Safety},
	language = {en},
	number = {1},
	urldate = {2022-07-18},
	journal = {Journal of Online Trust and Safety},
	author = {Cryst, Elena and Grossman, Shelby and Hancock, Jeff and Stamos, Alex and Thiel, David},
	month = oct,
	year = {2021},
	note = {Number: 1},
	keywords = {open access, scholarly publishing, trust and safety},
}

@article{schon1968reflective,
	title = {The reflective practitioner},
	journal = {New York},
	author = {Schön, Donald A},
	year = {1968},
}

@article{ma_business_2007,
	title = {The business model of "{Software}-as-a-{Service}"},
	abstract = {In the recent years, the emergence of the Softwareas-a-Service (SaaS) business model has attracted great attentions from both researchers and practitioners. Under the SaaS business model, vendors deliver on-demand information processing services to user firms, and thus offering computing utility rather than the standalone software itself. The SaaS has become an attractive alternative to the traditional software delivery model, which typically requires users to purchase, install, and maintain software systems by themselves. In this work, we propose an analytical model to study the competition between the SaaS and the traditional COTS (Commercial off-the-shelf) solution for software applications. The competitive model considers heterogeneous users who differ in terms of their transaction volume, while the SaaS and COTS vendors differ in terms of their pricing structure, setup cost, and system customization. We conclude that when commercial software becomes more open, modulated, and standardized, the SaaS business model will take a significant market share. In the extreme case, it may dominate the whole software industry and drives the traditional software out of the market. We also show that it is never optimal for the SaaS vendors to exert their full lock-in power through harsh software contracts. Under certain conditions, we suggest SaaS vendors to offer their existing users an easy exit option rather than to establish switching barriers to lock them in.},
	language = {en},
	journal = {IEEE international conference on services computing},
	author = {Ma, Dan},
	year = {2007},
	pages = {9},
}

@book{brooks1995mythical,
	title = {The mythical man-month: essays on software engineering},
	publisher = {Pearson Education},
	author = {Brooks Jr, Frederick P},
	year = {1995},
}

@inproceedings{menzies_automated_2008,
	address = {Beijing, China},
	title = {Automated severity assessment of software defect reports},
	isbn = {978-1-4244-2613-3},
	url = {http://ieeexplore.ieee.org/document/4658083/},
	doi = {10.1109/ICSM.2008.4658083},
	abstract = {In mission critical systems, such as those developed by NASA, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue.},
	language = {en},
	urldate = {2022-07-18},
	booktitle = {2008 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	publisher = {IEEE},
	author = {Menzies, Tim and Marcus, Andrian},
	month = sep,
	year = {2008},
	pages = {346--355},
}

@inproceedings{eckhardt_are_2016,
	address = {Austin Texas},
	title = {Are "non-functional" requirements really non-functional?: an investigation of non-functional requirements in practice},
	isbn = {978-1-4503-3900-1},
	shorttitle = {Are "non-functional" requirements really non-functional?},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884788},
	doi = {10.1145/2884781.2884788},
	abstract = {Non-functional requirements (NFRs) are commonly distinguished from functional requirements by diﬀerentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also inﬂuences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions. As a result, they remain diﬃcult to analyze and test. Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classiﬁcation of 530 NFRs extracted from 11 industrial requirements speciﬁcations and analyze to which extent these NFRs describe system behavior. Our results suggest that most “non-functional” requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.},
	language = {en},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Eckhardt, Jonas and Vogelsang, Andreas and Fernández, Daniel Méndez},
	month = may,
	year = {2016},
	pages = {832--842},
}

@inproceedings{crosby2003denial,
	title = {Denial of service via algorithmic complexity attacks},
	booktitle = {12th {USENIX} security symposium ({USENIX} security 03)},
	author = {Crosby, Scott A and Wallach, Dan S},
	year = {2003},
}

@misc{rungeler_integration_2015,
	title = {Integration of the {Packetdrill} {Testing} {Tool} in {INET}},
	url = {http://arxiv.org/abs/1509.03127},
	abstract = {Google released in 2013 a script-based tool called packetdrill, which allows to test transport protocols like UDP and TCP on Linux and BSD-based operating systems. The scripts deﬁning a test-case allow to inject packets to the implementation under test, perform operations at the API controlling the transport protocol and verify the sending of packets, all at speciﬁed times. This paper describes a port of packetdrill to the INET framework for the OMNeT++ simulation environment providing a simple and powerful method of testing the transport protocols implemented in INET.},
	language = {en},
	urldate = {2022-07-17},
	publisher = {arXiv},
	author = {Rüngeler, Irene and Tüxen, Michael},
	month = sep,
	year = {2015},
	note = {Number: arXiv:1509.03127
arXiv:1509.03127 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Performance},
}

@inproceedings{goel_directed_2022,
	title = {Directed {Acyclic} {Graph}-based {Neural} {Networks} for {Tunable} {Low}-{Power} {Computer} {Vision}},
	abstract = {Processing visual data on mobile devices has many applications, e.g., emergency response and tracking. State-of-the-art computer vision techniques rely on large Deep Neural Networks (DNNs) that are usually too power-hungry to be deployed on resource-constrained edge devices. Many techniques improve DNN efficiency of DNNs by compromising accuracy. However, the accuracy and efficiency of these techniques cannot be adapted for diverse edge applications with different hardware constraints and accuracy requirements. This paper demonstrates that a recent, efficient tree-based DNN architecture, called the hierarchical DNN, can be converted into a Directed Acyclic Graph-based (DAG) architecture to provide tunable accuracy-efficiency tradeoff options. We propose a systematic method that identifies the connections that must be added to convert the tree to a DAG to improve accuracy. We conduct experiments on popular edge devices and show that increasing the connectivity of the DAG improves the accuracy to within 1\% of the existing high accuracy techniques. Our approach requires 93\% less memory, 43\% less energy, and 49\% fewer operations than the high accuracy techniques, thus providing more accuracy-efficiency configurations.},
	language = {en},
	booktitle = {{ACM}/{IEEE} international symposium on low power electronics and design ({ISLPED})},
	author = {Goel, Abhinav and Tung, Caleb and Eliopoulos, Nick and Hu, Xiao and Thiruvathukal, George K and Davis, James C and Lu, Yung-Hsiang},
	year = {2022},
	pages = {6},
}

@inproceedings{synovic_snapshot_2022,
	title = {Snapshot {Metrics} {Are} {Not} {Enough}: {Analyzing} {Software} {Repositories} with {Longitudinal} {Metrics}},
	abstract = {Software metrics capture information about software development processes and products. These metrics support decision-making, e.g., in team management or dependency selection. However, existing metrics tools measure only a snapshot of a software project. Little attention has been given to enabling engineers to reason about metric trends over time—longitudinal metrics that give insight about process, not just product. In this work, we present PRIME (PRocess MEtrics), a tool for computing and visualizing process metrics. The currently-supported metrics include productivity, issue density, issue spoilage, and bus factor. We illustrate the value of longitudinal data and conclude with a research agenda. The tool’s demo video can be watched at https://youtu.be/YigEHy3 JCo. The source code can be found at github.com/SoftwareSystemsLaboratory/prime.},
	language = {en},
	booktitle = {Automated {Software} {Engineering} - {Tool} {Demonstrations} ({ASE}-{Tools})},
	author = {Synovic, Nicholas and Hyatt, Matt and Sethi, Rohan and Thota, Sohini and Miller, Allan J and Jiang, Wenxin and Amobi, Emmanuel S and Pinderski, Austin and Laufer, Konstantin and Hayward, Nicholas J and Klingensmith, Neil and Davis, James C and Thiruvathukal, George K},
	year = {2022},
	pages = {4},
}

@article{runeson_guidelines_2009,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	volume = {14},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-008-9102-8},
	doi = {10.1007/s10664-008-9102-8},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	language = {en},
	number = {2},
	urldate = {2022-07-13},
	journal = {Empirical Software Engineering},
	author = {Runeson, Per and Höst, Martin},
	month = apr,
	year = {2009},
	pages = {131--164},
}

@misc{paul_graham_paulg_hypothesis_2017,
	type = {Tweet},
	title = {A hypothesis about how you might discover the next {Twitter}. https://t.co/{ral5ns6S7p}},
	url = {https://mobile.twitter.com/paulg/status/898919987657805825},
	language = {en},
	urldate = {2022-07-13},
	journal = {Twitter},
	author = {{Paul Graham [@paulg]}},
	month = aug,
	year = {2017},
}

@inproceedings{anvik_are_2020,
	address = {New York, NY, USA},
	title = {Are {Automatic} {Bug} {Report} {Summarizers} {Missing} the {Target}?},
	isbn = {978-1-4503-7963-2},
	url = {https://doi.org/10.1145/3387940.3392236},
	abstract = {Bug reports can be lengthy due to long descriptions and long conversation threads. Automatic summarization of the text in a bug report can reduce the time spent by software project members on understanding the content of a bug report. Quality of the bug report summaries have been historically evaluated using human-created gold-standard summaries. However, we believe this is not a good practice for two reasons. First, we observed high disagreement levels in the annotated summaries and the number of annotators to create gold-standard summaries was lower than the established value for stable annotation. We believe that creating a fixed summary length of 25\% of the word count of the corresponding bug report is not suitable for every time when a person refers to a bug report. Therefore, we propose an automatic sentence annotation method and an interface to customize the presented summary.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {Association for Computing Machinery},
	author = {Anvik, John and Galappaththi, Akalanka},
	month = jun,
	year = {2020},
	keywords = {bug report summarization, gold-standard summary, human annotation, software bug reports},
	pages = {149--152},
}

@article{nadia_engineering_2006,
	title = {Engineering change request management in a new product development process},
	volume = {9},
	issn = {1460-1060},
	url = {https://doi.org/10.1108/14601060610639999},
	doi = {10.1108/14601060610639999},
	abstract = {Purpose – The objective of this research was to compare the behavior of two methods of managing an engineering change request (ECR) process, namely, perform changes as they occur or in a batch. Design/methodology/approach – This comparison was accomplished by creating a computer model of a new product development (NPD) process and simulating ECR management. The model connects process design and process characteristics (teamwork, parallel activities) to process outcomes (development time, effort). The first method executes the ECR promptly and the rework is done as soon as the ECR is initiated. In the second method, ECRs are batched; in other words, a number of them are accumulated, and processing of the ECRs takes place when a batch of a certain size has accumulated. Thus, the change requests are grouped into a batch, and then, the section(s) of the process to effect the change(s) is (are) reworked. Findings – Batching ECRs was found to be superior to doing them one at a time. Research limitations/implications – Future work should focus on refining the computer model and differentiating ECRs by assigning priorities to incoming ECRs. Practical implications – For product development managers, processing ECRs in batches is preferable than attending to them on an individual basis. Nevertheless, in some situations ECRs require immediate attention. A mechanism will always be needed to deal with situations directly. Also, in terms of batching, ECRs could be processed in groups on a periodic basis. Periodically performing ECRs due to new design versions or prototypes in a timely manner is a good compromise between a random batch mode and doing them individually. Originality/value – The paper shows that batch processing is superior to executing ECRs promptly as they are received. This result has been shown through the use of a computer model of NPD. To the authors' knowledge, no other studies have used computer modeling to study this problem.},
	number = {1},
	urldate = {2022-07-12},
	journal = {European Journal of Innovation Management},
	author = {Nadia, Bhuiyan and Gregory, Gatard and Vince, Thomson},
	month = jan,
	year = {2006},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Change management, Product development},
	pages = {5--19},
}

@inproceedings{da_cunha_decision-making_2016,
	title = {Decision-{Making} in {Software} {Project} {Management}: {A} {Qualitative} {Case} {Study} of a {Private} {Organization}},
	shorttitle = {Decision-{Making} in {Software} {Project} {Management}},
	doi = {10.1145/2897586.2897598},
	abstract = {Context: In software project management, the decision-making process is a complex set of tasks largely based on specific knowledge and individual cultural background, as well as human relations. The factors that affect the decisions of the software project managers (SPMs) and their potential consequences require attention because project delays and failures are usually related to a series of poor decisions. Objective: To understand how SPMs make decisions based on how they interpret their experiences in the workplace, and also to identify antecedents and consequences of those decisions in order to increase the effectiveness of project management. Method: Semi-structured interviews were carried out with SPMs within a Brazilian large private organization. The data was analyzed using techniques from grounded theory approach. Results: We found that decision-making in software project management is based on knowledge sharing in which the SPM acts as a facilitator before making decisions. This phenomenon is influenced by individual factors, such as experience, communication, negotiation, self-control and systemic view of the project and by contextual factors such as the autonomy of the SPM and team members' technical competence. Also, these factors are mediated by cognitive biases. Conclusions: Due to the uncertainty and dynamism inherent in software projects, the SPMs focus on making, monitoring and adjusting decisions in an argument-driven way.},
	booktitle = {2016 {IEEE}/{ACM} {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	author = {da Cunha, José Adson O.G. and da Silva, Fabio Q. B. and de Moura, Hermano P. and Vasconcellos, Francisco J.S.},
	month = may,
	year = {2016},
	keywords = {Biological system modeling, Context, Decision making, Decision-Making, Grounded Theory, Interviews, Organizations, Project management, Software, Software Project Management},
	pages = {26--32},
}

@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {http://arxiv.org/abs/2006.05525},
	doi = {10.1007/s11263-021-01453-z},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation eﬀectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are brieﬂy reviewed and comments on future research are discussed and forwarded.},
	language = {en},
	number = {6},
	urldate = {2022-07-11},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
	month = jun,
	year = {2021},
	note = {arXiv:2006.05525 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1789--1819},
}

@article{microsoft_microsoft_2021,
	title = {Microsoft {Digital} {Defense} {Report} {OCTOBER} 2021},
	url = {https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RWMFIi},
	language = {en},
	author = {Microsoft},
	year = {2021},
	keywords = {Industry},
	pages = {134},
}

@techreport{center_for_security_and_emerging_technology_poison_2021,
	title = {Poison in the {Well}: {Securing} the {Shared} {Resources} of {Machine} {Learning}},
	shorttitle = {Poison in the {Well}},
	url = {https://cset.georgetown.edu/publication/poison-in-the-well/},
	abstract = {Modern machine learning often relies on open-source datasets, pretrained models, and machine learning libraries from across the internet, but are those resources safe to use? Previously successful digital supply chain attacks against cyber infrastructure suggest the answer may be no. This report introduces policymakers to these emerging threats and provides recommendations for how to secure the machine learning supply chain.},
	urldate = {2022-07-08},
	institution = {Center for Security and Emerging Technology},
	author = {{Center for Security and Emerging Technology} and Lohn, Andrew},
	month = jun,
	year = {2021},
	doi = {10.51593/2020CA013},
}

@techreport{technology_minimum_2006,
	title = {Minimum {Security} {Requirements} for {Federal} {Information} and {Information} {Systems}},
	url = {https://csrc.nist.gov/publications/detail/fips/200/final},
	abstract = {FIPS 200 is the second standard that was specified by the Information Technology Management Reform Act of 1996 (FISMA). It is an integral part of the risk management framework that the National Institute of Standards and Technology (NIST) has developed to assist federal agencies in providing levels of information security based on levels of risk. FIPS 200 specifies minimum security requirements for federal information and information systems and a risk-based process for selecting the security controls necessary to satisfy the minimum requirements.},
	language = {en},
	number = {Federal Information Processing Standard (FIPS) 200},
	urldate = {2022-06-23},
	institution = {U.S. Department of Commerce},
	author = {Technology, National Institute of Standards and},
	month = mar,
	year = {2006},
	doi = {10.6028/NIST.FIPS.200},
	keywords = {Government},
}

@techreport{microsoft_3_2021,
	title = {3 ways to mitigate risk when using private package feeds},
	url = {https://azure.microsoft.com/mediahandler/files/resourcefiles/3-ways-to-mitigate-risk-using-private-package-feeds/3%20Ways%20to%20Mitigate%20Risk%20When%20Using%20Private%20Package%20Feeds%20-%20v1.0.pdf},
	abstract = {Software today has become an assembly of components from a wide range of sources. Many organizations use public package feeds to take advantage of the open ecosystems they offer. Projects that consume packages from multiple public and private feeds may be exposed to supply chain vulnerabilities. 

This white paper discusses configurations that can introduce risk in your software supply chain, and how to mitigate these risks.},
	language = {en},
	urldate = {2022-06-21},
	institution = {Microsoft},
	author = {Microsoft},
	month = mar,
	year = {2021},
	keywords = {Industry, White Paper},
	pages = {15},
}

@misc{noauthor_introduction_2022,
	title = {Introduction},
	url = {http://slsa.dev/spec/v0.1/index},
	abstract = {Security framework to ensure software supply chain integrity},
	language = {en},
	urldate = {2022-07-07},
	journal = {SLSA},
	month = jul,
	year = {2022},
}

@techreport{cybersecurity_enisa_2021,
	type = {Report/{Study}},
	title = {{ENISA} {Threat} {Landscape} 2021},
	url = {https://www.enisa.europa.eu/publications/enisa-threat-landscape-2021},
	abstract = {This is the ninth edition of the ENISA Threat Landscape (ETL) report, an annual report on the status of the cybersecurity threat landscape that identifies prime threats, major trends observed with respect to threats, threat actors and attack techniques, and also describes relevant mitigation measures. In the process of constantly improving our methodology for the development of threat landscapes, this year’s work has been supported by a newly formatted ENISA ad hoc Working Group on Cybersecurity Threat Landscapes (CTL). In this report we discuss the first 8 cybersecurity threat categories. Supply chain threats, the 9th category, were analysed in detail, in a dedicated ENISA report.},
	language = {en},
	urldate = {2022-06-21},
	institution = {ENISA},
	author = {Cybersecurity., European Union Agency for},
	editor = {Lella, Ifigeneia and Tsekmezoglou, Eleni and Malatras, Apostolos},
	collaborator = {Ardagna, Claudio and Corbiaux, Stephen and Sfakianakis, Andreas and Douligeris, Christos},
	month = oct,
	year = {2021},
	keywords = {Government},
}

@misc{khandelwal_ccleaner_2018,
	title = {{CCleaner} {Attack} {Timeline}—{Here}'s {How} {Hackers} {Infected} 2.3 {Million} {PCs}},
	url = {https://thehackernews.com/2018/04/ccleaner-malware-attack.html},
	abstract = {Here's a brief timeline of CCleaner supply chain malware attack, explaining how and when hackers infected millions of computers.},
	language = {en},
	urldate = {2022-06-24},
	journal = {The Hacker News},
	author = {Khandelwal, Swati},
	month = apr,
	year = {2018},
	note = {Section: Article},
	keywords = {attacks},
}

@misc{ii_compromised_2018,
	title = {Compromised npm {Package}: event-stream},
	shorttitle = {Compromised npm {Package}},
	url = {https://medium.com/intrinsic-blog/compromised-npm-package-event-stream-d47d08605502},
	abstract = {Ownership of a popular npm package, event-stream, was transferred by the original author to a malicious user.},
	language = {en},
	urldate = {2022-06-24},
	journal = {intrinsic},
	author = {II, Thomas Hunter},
	month = nov,
	year = {2018},
	keywords = {attacks},
}

@misc{microsoft_defender_security_research_team_new_2017,
	title = {New ransomware, old techniques: {Petya} adds worm capabilities},
	shorttitle = {New ransomware, old techniques},
	url = {https://www.microsoft.com/security/blog/2017/06/27/new-ransomware-old-techniques-petya-adds-worm-capabilities/},
	abstract = {On June 27, 2017 reports of a ransomware infection began spreading across Europe. We saw the first infections in Ukraine, where more than 12,500 machines encountered the threat. We then observed infections in another 64 countries, including Belgium, Brazil, Germany, Russia, and the United States. The trend towards increasingly sophisticated malware behavior, highlighted by the…},
	language = {en-US},
	urldate = {2022-06-24},
	journal = {Microsoft Security Blog},
	author = {Microsoft Defender Security Research Team},
	month = jun,
	year = {2017},
	keywords = {attacks},
}

@techreport{ensor_shifting_2021,
	title = {Shifting left on security - {Securing} software supply chains},
	url = {https://cloud.google.com/solutions/shifting-left-on-security},
	language = {en},
	institution = {Google Cloud},
	author = {Ensor, Mike and Stevens, Drew},
	month = feb,
	year = {2021},
	keywords = {Industry, White Paper},
	pages = {36},
}

@misc{risktec_risk-based_nodate,
	title = {Risk-based decision making},
	url = {https://risktec.tuv.com/risktec-knowledge-bank/quantitative-probabilistic-risk-assessment/risk-based-decision-making/},
	abstract = {Risk-based decision making evolves around assessing the need to make choices that are often limited by external reasons for the 'best' result},
	language = {en-GB},
	urldate = {2022-07-07},
	author = {{Risktec}},
}

@incollection{lu_risk_2012,
	address = {Berlin, Heidelberg},
	series = {Intelligent {Systems} {Reference} {Library}},
	title = {Risk {Management} in {Decision} {Making}},
	isbn = {978-3-642-25755-1},
	url = {https://doi.org/10.1007/978-3-642-25755-1_1},
	abstract = {Organizational decision making often occurs in the face of uncertainty about whether a decision maker’s choices will lead to benefit or disaster. Risk is the potential that a decision will lead to a loss or an undesirable outcome. In fact, almost any human decision carries some risk, but some decisions are much more risky than others. Risk and decision making are two inter-related factors in organizational management, and they are both related to various uncertainties.},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Handbook on {Decision} {Making}: {Vol} 2: {Risk} {Management} in {Decision} {Making}},
	publisher = {Springer},
	author = {Lu, Jie and Jain, Lakhmi C. and Zhang, Guangquan},
	editor = {Lu, Jie and Jain, Lakhmi C. and Zhang, Guangquan},
	year = {2012},
	doi = {10.1007/978-3-642-25755-1_1},
	keywords = {Grassland Fire, Group Decision Support System, Risk Management, Risk Management Process, Supply Chain},
	pages = {3--7},
}

@article{defranco_content_2017,
	title = {A content analysis process for qualitative software engineering research},
	volume = {13},
	issn = {1614-5054},
	url = {https://doi.org/10.1007/s11334-017-0287-0},
	doi = {10.1007/s11334-017-0287-0},
	abstract = {A review of the qualitative research methods discussed in papers that study software engineering teams showed most of those papers did not follow a systematic process during the qualitative analysis. This finding is concerning as this deficiency in research analysis procedure may reduce the validity and/or completeness of the qualitative results. Such a lack of rigor may be a result of qualitative research not being as firmly established in software engineering as quantitative research methodologies. In engineering research, quantitative methods are typically more prevalent and qualitative analysis is part of a mixed-method analysis process. However, when researching teams, where human activity is abundant, qualitative analysis may need to take precedence. In this paper, we focus on the qualitative analysis method called content analysis with the goal of presenting a rigorous process for content analysis in the context of software engineering. We then present and demonstrate the use of that content analysis process for software engineering researchers using two examples. An analysis of 215 articles that were a result of a mapping study on software engineering team research is presented. Those papers were analyzed to determine which utilized a qualitative data analysis method in their research in addition to the rigor and type of qualitative analysis performed. We ultimately included 23 papers in this study. We present a mapping study and a content analysis process that include a straightforward way to select, code, and present data in both inductive and deductive studies. We demonstrated the process using the keywords from the papers included in this study as well as on a second data set that utilized responses from structured interview transcripts from practicing software engineers. The first dataset also resulted in a taxonomy to categorize software engineering team research. We presented and demonstrated a content analysis process in terms of software engineering in order to improve future qualitative software engineering research that would benefit from systematic content analysis.},
	language = {en},
	number = {2},
	urldate = {2022-07-07},
	journal = {Innovations in Systems and Software Engineering},
	author = {DeFranco, Joanna F. and Laplante, Phillip A.},
	month = sep,
	year = {2017},
	keywords = {Content analysis, Mapping study, Qualitative research, Software engineering teams},
	pages = {129--141},
}

@incollection{borissova_overview_2021,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {An {Overview} of {Multi}-criteria {Decision} {Making} {Models} and {Software} {Systems}},
	isbn = {978-3-030-72284-5},
	url = {https://doi.org/10.1007/978-3-030-72284-5_15},
	abstract = {The article describes an overview of proposed multi-criteria decision making models and their applications. Several research directions are discussed including multi-criteria models with a priori and exact articulation of DM preferences; models with fuzzy preferences; models with interactive participation of DM; models based on bi-level optimization, group decision making models and software systems for decision making. This overview concerns the latest achievements in multi-criteria decision making of the scientists from Bulgarian Academy of Sciences.},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Research in {Computer} {Science} in the {Bulgarian} {Academy} of {Sciences}},
	publisher = {Springer International Publishing},
	author = {Borissova, Daniela},
	editor = {Atanassov, Krassimir T.},
	year = {2021},
	doi = {10.1007/978-3-030-72284-5_15},
	keywords = {Bi-level optimization, Decision making, Group decision making, Multi-criteria methods, Software systems},
	pages = {305--323},
}

@article{jhaver_online_2018,
	title = {Online {Harassment} and {Content} {Moderation}: {The} {Case} of {Blocklists}},
	volume = {25},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Online {Harassment} and {Content} {Moderation}},
	url = {https://dl.acm.org/doi/10.1145/3185593},
	doi = {10.1145/3185593},
	abstract = {Online harassment is a complex and growing problem. On Twitter, one mechanism people use to avoid harassment is the
              blocklist
              , a list of accounts that are preemptively blocked from interacting with a subscriber. In this article, we present a rich description of Twitter blocklists – why they are needed, how they work, and their strengths and weaknesses in practice. Next, we use blocklists to interrogate online harassment – the forms it takes, as well as tactics used by harassers. Specifically, we interviewed both people who use blocklists to protect themselves, and people who are blocked by blocklists. We find that users are not adequately protected from harassment, and at the same time, many people feel that they are blocked unnecessarily and unfairly. Moreover, we find that not all users agree on what constitutes harassment. Based on our findings, we propose design interventions for social network sites with the aim of protecting people from harassment, while preserving freedom of speech.},
	language = {en},
	number = {2},
	urldate = {2022-07-07},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Jhaver, Shagun and Ghoshal, Sucheta and Bruckman, Amy and Gilbert, Eric},
	month = apr,
	year = {2018},
	pages = {1--33},
}

@inproceedings{alfantoukh_multi-stakeholder_2018,
	title = {Multi-{Stakeholder} {Consensus} {Decision}-{Making} {Framework} {Based} on {Trust}: {A} {Generic} {Framework}},
	shorttitle = {Multi-{Stakeholder} {Consensus} {Decision}-{Making} {Framework} {Based} on {Trust}},
	doi = {10.1109/CIC.2018.00012},
	abstract = {The decision-making process is one we encounter in every aspect of our lives. Decision-making becomes more challenging when dealing with multi-stakeholder decisions due to the existence of conflicts among them and the diversity in their expertise. As a result, the influence among them, which is represented by trust, is considered to be an important criterion when one is making a final decision. Such trust is a result of the interactions among those stakeholders. Rating is one of the methods that stakeholders use in their interactions to express their opinions of one another. It requires a decision that is agreed upon by everyone; of course, it might take several rounds to reach a final consensus decision. In this research study, we built a consensus decision-making framework based on trust. The trust framework has been proposed previously and is based on measurement theory. Then, we developed software to simulate the decision-making scenarios to study the rating convergences in these decision-making rounds and to investigate their convergences with and without trust. This simulator was designed to emulate humans' behaviors from a social science perspective. Our result showed that measurement theory-based trust is useful in the consensus-creating process, as it decreases the number of necessary rounds and even creates a consensus when an extreme conflict in preferences exists.},
	booktitle = {2018 {IEEE} 4th {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Alfantoukh, Lina and Ruan, Yefeng and Durresi, Arjan},
	month = oct,
	year = {2018},
	keywords = {Big Data, Collaboration, Computational modeling, Decision making, Decision-Making, Trust, Multistakeholder, Collaboration, big data, History, Mathematical model, Stakeholders},
	pages = {472--479},
}

@article{li_risk_2012,
	title = {Risk {Decision} {Making} {Based} on {Decision}-theoretic {Rough} {Set}: {A} {Three}-way {View} {Decision} {Model}},
	copyright = {Copyright Taylor and Francis Group, LLC},
	shorttitle = {Risk {Decision} {Making} {Based} on {Decision}-theoretic {Rough} {Set}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/18756891.2011.9727759},
	abstract = {Rough set theory has witnessed great success in data mining and knowledge discovery, which provides a good support for decision making on a certain data. However, a practical decision problem alway...},
	language = {en},
	urldate = {2022-07-07},
	journal = {International Journal of Computational Intelligence Systems},
	author = {Li, Huaxiong and Zhou, Xianzhong},
	month = mar,
	year = {2012},
	note = {Publisher: Taylor \& Francis Group},
}

@phdthesis{alfantoukh_multi-stakeholder_2019,
	type = {Thesis},
	title = {Multi-{Stakeholder} {Consensus} {Decision}-{Making} {Framework} {Based} on {Trust} and {Risk}},
	url = {https://scholarworks.iupui.edu/handle/1805/18885},
	abstract = {This thesis combines human and machine intelligence for consensus decision-making, and it contains four interrelated research areas. Before presenting the four research areas, this thesis presents a literature review on decision-making using two criteria: trust and risk. The analysis involves studying the individual and the multi-stakeholder decision-making. Also, it explores the relationship between trust and risk to provide insight on how to apply them when making any decision. This thesis presents a grouping procedure of the existing trust-based multi-stakeholder decision-making schemes by considering the group decision-making process and models. In the first research area, this thesis presents the foundation of building multi-stakeholder consensus decision-making (MSCDM). This thesis describes trust-based multi-stakeholder decision-making for water allocation to help the participants select a solution that comes from the best model. Several criteria are involved when deciding on a solution such as trust, damage, and benefit. This thesis considers Jain's fairness index as an indicator of reaching balance or equality for the stakeholder's needs. The preferred scenario is when having a high trust, low damages and high benefits. The worst scenario involves having low trust, high damage, and low benefit. The model is dynamic by adapting to the changes over time. The decision to select is the solution that is fair for almost everyone. In the second research area, this thesis presents a MSCDM, which is a generic framework that coordinates the decision-making rounds among stakeholders based on their influence toward each other, as represented by the trust relationship among them. This thesis describes the MSCDM framework that helps to find a decision the stakeholders can agree upon. Reaching a consensus decision might require several rounds where stakeholders negotiate by rating each other. This thesis presents the results of implementing MSCDM and evaluates the effect of trust on the consensus achievement and the reduction in the number of rounds needed to reach the final decision. This thesis presents Rating Convergence in the implemented MSCDM framework, and such convergence is a result of changes in the stakeholders' rating behavior in each round. This thesis evaluates the effect of trust on the rating changes by measuring the distance of the choices made by the stakeholders. Trust is useful in decreasing the distances. In the third research area, this thesis presents Rating Convergence in the implemented MSCDM framework, and such convergence is a result of changes in stakeholders' rating behavior in each round. This thesis evaluates the effect of trust on the rating changes by measuring the perturbation in the rating matrix. Trust is useful in increasing the rating matrix perturbation. Such perturbation helps to decrease the number of rounds. Therefore, trust helps to increase the speed of agreeing upon the same decision through the influence. In the fourth research area, this thesis presents Rating Aggregation operators in the implemented MSCDM framework. This thesis addresses the need for aggregating the stakeholders' ratings while they negotiate on the round of decisions to compute the consensus achievement. This thesis presents four aggregation operators: weighted sum (WS), weighted product (WP), weighted product similarity measure (WPSM), and weighted exponent similarity measure (WESM). This thesis studies the performance of those aggregation operators in terms of consensus achievement and the number of rounds needed. The consensus threshold controls the performance of these operators. The contribution of this thesis lays the foundation for developing a framework for MSCDM that facilitates reaching the consensus decision by accounting for the stakeholders' influences toward one another. Trust represents the influence.},
	language = {en},
	urldate = {2022-07-07},
	author = {Alfantoukh, Lina Abdulaziz},
	month = may,
	year = {2019},
	doi = {10.7912/C2/2360},
	note = {Accepted: 2019-04-18T14:20:27Z},
}

@article{aven_decision_2007,
	title = {A decision framework for risk management, with application to the offshore oil and gas industry},
	volume = {92},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832006000093},
	doi = {10.1016/j.ress.2005.12.009},
	abstract = {In this paper we present and discuss a decision framework for risk management. The framework comprises the basic elements: problem definition (challenges, goals and alternatives), stakeholders, concerns that affect the consequence analyses and the value judgments related to these consequences and analyses (frame conditions and constraints), identification of which consequence analyses to execute and the execution of these, managerial review and judgement, and the decision. The framework has novel aspects on the way of classifying the decision situations and characterising risks. The classification is based on the two dimensions, expected consequences, and uncertainties. Our starting point is the offshore oil and gas industry, but our framework and discussion is to a large extent general and could also be applied in other areas. An example is outlined to illustrate the use of the framework.},
	language = {en},
	number = {4},
	urldate = {2022-07-07},
	journal = {Reliability Engineering \& System Safety},
	author = {Aven, T. and Vinnem, J. E. and Wiencke, H. S.},
	month = apr,
	year = {2007},
	keywords = {ALARP, Decision framework, Risk and uncertainty},
	pages = {433--448},
}

@inproceedings{liu_stegonet_2020,
	address = {Austin USA},
	title = {{StegoNet}: {Turn} {Deep} {Neural} {Network} into a {Stegomalware}},
	isbn = {978-1-4503-8858-0},
	shorttitle = {{StegoNet}},
	url = {https://dl.acm.org/doi/10.1145/3427228.3427268},
	doi = {10.1145/3427228.3427268},
	abstract = {Deep Neural Networks (DNNs) are now presenting human-level performance on many real-world applications, and DNN-based intelligent services are becoming more and more popular across all aspects of our lives. Unfortunately, the ever-increasing DNN service implies a dangerous feature which has not yet been well studied–allowing the marriage of existing malware and DNN model for any pre-defined malicious purpose. In this paper, we comprehensively investigate how to turn DNN into a new breed evasive self-contained stegomalware, namely StegoNet, using model parameter as a novel payload injection channel, with no service quality degradation (i.e. accuracy) and the triggering event connected to the physical world by specified DNN inputs. A series of payload injection techniques which take advantage of a variety of unique neural network natures like complex structure, high error resilience capability and huge parameter size, are developed for both uncompressed models (with model redundancy) and deeply compressed models tailored for resource-limited devices (no model redundancy), including LSB substitution, resilience training, value mapping, and sign-mapping. We also proposed a set of triggering techniques like logits trigger, rank trigger and fine-tuned rank trigger to trigger StegoNet by specific physical events under realistic environment variations. We implement the StegoNet prototype on Nvidia Jetson TX2 testbed. Extensive experimental results and discussions on the evasiveness, integrity of proposed payload injection techniques, and the reliability and sensitivity of the triggering techniques, well demonstrate the feasibility and practicality of StegoNet.},
	language = {en},
	urldate = {2022-07-07},
	booktitle = {Annual {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Liu, Tao and Liu, Zihao and Liu, Qi and Wen, Wujie and Xu, Wenyao and Li, Ming},
	month = dec,
	year = {2020},
	pages = {928--938},
}

@inproceedings{Wang2021EvilModel,
	title = {{EvilModel}: {Hiding} {Malware} {Inside} of {Neural} {Network} {Models}},
	abstract = {Delivering malware covertly and evasively is critical to advanced malware campaigns. In this paper, we present a new method to covertly and evasively deliver malware through a neural network model. Neural network models are poorly explainable and have a good generalization ability. By embedding malware in neurons, the malware can be delivered covertly, with minor or no impact on the performance of neural network. Meanwhile, because the structure of the neural network model remains unchanged, it can pass the security scan of anti-virus engines. Experiments show that 36.9MB of malware can be embedded in a 178MB-AlexNet model within 1\% accuracy loss, and no suspicion is raised by anti-virus engines in VirusTotal, which verifies the feasibility of this method. With the widespread application of artificial intelligence, utilizing neural networks for attacks becomes a forwarding trend. We hope this work can provide a reference scenario for the defense on neural network-assisted attacks.},
	booktitle = {{IEEE} {Symposium} on {Computers} and {Communications} ({ISCC})},
	author = {Wang, Zhi and Liu, Chaoge and Cui, Xiang},
	year = {2021},
	keywords = {Artificial Intelligence, Artificial intelligence, Computational modeling, Computer security, Computer viruses, Malware, Market research, Neural Networks, Neurons, Steganography},
}

@article{goldhaber_exploring_2020,
	title = {Exploring the {Impact} of {Student} {Teaching} {Apprenticeships} on {Student} {Achievement} and {Mentor} {Teachers}},
	volume = {13},
	issn = {1934-5747},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7252591/},
	doi = {10.1080/19345747.2019.1698087},
	abstract = {We exploit within-teacher variation in the years that math and reading teachers in grades 4–8 host an apprentice (“student teacher”) in Washington State to estimate the causal effect of these apprenticeships on student achievement, both during the apprenticeship and afterwards. While the average causal effect of hosting a student teacher on student performance in the year of the apprenticeship is indistinguishable from zero in both math and reading, hosting a student teacher is found to have modest positive impacts on student math and reading achievement in a teacher’s classroom in following years. These findings suggest that schools and districts can participate in the student teaching process without fear of short-term decreases in student test scores while potentially gaining modest long-term test score increases.},
	number = {2},
	urldate = {2022-07-05},
	journal = {Journal of Research on Educational Effectiveness},
	author = {Goldhaber, Dan and Krieg, John M. and Theobald, Roddy},
	month = feb,
	year = {2020},
	pmid = {32537041},
	pmcid = {PMC7252591},
	pages = {213--234},
}

@article{quew-jones_enhancing_2022,
	title = {Enhancing apprenticeships within the {Higher} {Education} curriculum – an {Action} {Learning} and {Action} {Research} study},
	issn = {1476-7333, 1476-7341},
	url = {https://www.tandfonline.com/doi/full/10.1080/14767333.2022.2056135},
	doi = {10.1080/14767333.2022.2056135},
	abstract = {This Action Learning (AL)/Action Research study (AS) explores the practice of Action Learning (AL) to further higher education (H.E.) apprenticeships by collaboration between University Provider (UP) and employer. AL members aim to address complexity, bridging the gap between management education delivered by a work-based learning (WBL) apprenticeship course and translating into the apprentices’ workplace. Set members followed a systematic cycle of planning, action, observing and reﬂecting. This demonstrates how AL, as a methodology, supports apprenticeship ambassadors (who lead apprenticeships in their organisations) and UPs to solve complex problems through inquiry and critical reﬂection to enhance the apprenticeship curriculum. The principal ﬁndings from AL to cultivate stronger collaboration were clarity of WBL, value proposition and ownership expectation; support of translation of theory into practice; empowering the apprenticeship mindset and professional identity; and senior management buy-in.},
	language = {en},
	urldate = {2022-07-05},
	journal = {Action Learning: Research and Practice},
	author = {Quew-Jones, Rebecca},
	month = mar,
	year = {2022},
	pages = {1--19},
}

@misc{merten_identification_2017,
	type = {Dissertation},
	title = {Identification of {Software} {Features} in {Issue} {Tracking} {System} {Data}},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {https://archiv.ub.uni-heidelberg.de/volltextserver/22655/},
	abstract = {The knowledge of Software Features (SFs) is vital for software developers and requirements specialists during all software engineering phases: to understand and derive software requirements, to plan and prioritize implementation tasks, to update documentation, or to test whether the final product correctly implements the requested SF. In most software projects, SFs are managed in conjunction with other information such as bug reports, programming tasks, or refactoring tasks with the aid of Issue Tracking Systems (ITSs). Hence ITSs contains a variety of information that is only partly related to SFs.
In practice, however, the usage of ITSs to store SFs comes with two major problems: (1) ITSs are neither designed nor used as documentation systems. Therefore, the data inside an ITS is often uncategorized and SF descriptions are concealed in rather lengthy. (2) Although an SF is often requested in a single sentence, related information can be scattered among many issues. E.g. implementation tasks related to an SF are often reported in additional issues. Hence, the detection of SFs in ITSs is complicated: a manual search for the SFs implies reading, understanding and exploiting the Natural Language (NL) in many issues in detail. This is cumbersome and labor intensive, especially if related information is spread over more than one issue.
This thesis investigates whether SF detection can be supported automatically. First the problem is analyzed: (i) An empirical study shows that requests for important SFs reside in ITSs, making ITSs a good tar- get for SF detection. (ii) A second study identifies characteristics of the information and related NL in issues. These characteristics repre- sent opportunities as well as challenges for the automatic detection of SFs.
Based on these problem studies, the Issue Tracking Software Feature Detection Method (ITSoFD), is proposed. The method has two main components and includes an approach to preprocess issues. Both components address one of the problems associated with storing SFs in ITSs. ITSoFD is validated in three solution studies: (I) An empirical study researches how NL that describes SFs can be detected with techniques from Natural Language Processing (NLP) and Machine Learning. Issues are parsed and different characteristics of the issue and its NL are extracted. These characteristics are used to clas- sify the issue’s content and identify SF description candidates, thereby approaching problem (1). (II) An empirical study researches how issues that carry information potentially related to an SF can be detected with techniques from NLP and Information Retrieval. Characteristics of the issue’s NL are utilized to create a traceability network
 vii
of related issues, thereby approaching problem (2). (III) An empirical study researches how NL data in issues can be preprocessed using heuristics and hierarchical clustering. Code, stack traces, and other technical information is separated from NL. Heuristics are used to identify candidates for technical information and clustering improves the heuristic’s results. The technique can be applied to support components, I. and II.},
	language = {eng},
	urldate = {2022-07-06},
	author = {Merten, Thorsten},
	year = {2017},
	doi = {10.11588/heidok.00022655},
}

@article{kutomi_supporting_nodate,
	title = {Supporting {Support} {Engineers}},
	abstract = {The steady and uninterrupted availability of systems is essential for the mission of many companies and other organizations. This responsibility relies mostly upon support engineers, who are responsible to respond to incidents. Incident response is a unique type of task in software engineering, given it carries distinguishing characteristics like risks, pressure, incomplete information and urgency. Despite the importance of this task for many organizations, little can be found in the literature about the incident response task and model. To ﬁll the gap, we created a theoretical foundation to foster research on incident response. We conducted an interview study, asking 12 support engineers about their experiences dealing with outages, service degradation, and other incidents that demanded an urgent response. We used our 22 collected cases to identify important concepts of incidents and their dimensions, and created an ontology of incidents and a model of the incident response. To validate the usefulness of our results, we analyzed our incidents based on our ontology and model, providing some insights related to detection of incidents, investigation and the hand over process. We also provide analytical insights related to the prevention of resource limitation incidents. Finally, we validate the usefulness of our research by proposing an improvement on monitoring tools used by support engineers.},
	language = {en},
	author = {Kutomi, Esdras},
	pages = {69},
}

@inproceedings{ko_design_2011,
	address = {New York, NY, USA},
	series = {{iConference} '11},
	title = {Design, discussion, and dissent in open bug reports},
	isbn = {978-1-4503-0121-3},
	url = {https://doi.org/10.1145/1940761.1940776},
	doi = {10.1145/1940761.1940776},
	abstract = {While studies have considered computer-mediated decision-making in several domains, few have considered the unique challenges posed in software design. To address this gap, a qualitative study of 100 contentious open source bug reports was performed. The results suggest that the immeasurability of many software qualities and conflicts between achieving original design intent and serving changing user needs led to a high reliance on anecdote, speculation, and generalization. The visual presentation of threaded discussions aggravated these problems making it difficult to view design proposals and comparative critiques. The results raise several new questions about the interaction between authority and evidence in online design discussions.},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 2011 {iConference}},
	publisher = {Association for Computing Machinery},
	author = {Ko, Amy J. and Chilana, Parmit K.},
	month = feb,
	year = {2011},
	keywords = {Bugzilla, change requests, design rationale, open source},
	pages = {106--113},
}

@misc{sillito_failures_2020,
	title = {Failures and {Fixes}: {A} {Study} of {Software} {System} {Incident} {Response}},
	shorttitle = {Failures and {Fixes}},
	url = {http://arxiv.org/abs/2008.11192},
	abstract = {This paper presents the results of a research study related to software system failures, with the goal of understanding how we might better evolve, maintain and support software systems in production. We have qualitatively analyzed thirty incidents: fifteen collected through in depth interviews with engineers, and fifteen sampled from publicly published incident reports (generally produced as part of postmortem reviews). Our analysis focused on understanding and categorizing how failures occurred, and how they were detected, investigated and mitigated. We also captured analytic insights related to the current state of the practice and associated challenges in the form of 11 key observations. For example, we observed that failures can cascade through a system leading to major outages; and that often engineers do not understand the scaling limits of systems they are supporting until those limits are exceeded. We argue that the challenges we have identified can lead to improvements to how systems are engineered and supported.},
	urldate = {2022-07-05},
	publisher = {arXiv},
	author = {Sillito, Jonathan and Kutomi, Esdras},
	month = aug,
	year = {2020},
	note = {Number: arXiv:2008.11192
arXiv:2008.11192 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{nahar_collaboration_2022,
	title = {Collaboration {Challenges} in {Building} {ML}-{Enabled} {Systems}: {Communication}, {Documentation}, {Engineering}, and {Process}},
	abstract = {The introduction of machine learning (ML) components in software projects has created the need for software engineers to collaborate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through interviews with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common collaboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process, and collect recommendations to address these challenges.},
	language = {en},
	author = {Nahar, Nadia and Zhou, Shurui and Lewis, Grace and Kästner, Christian},
	year = {2022},
	pages = {13},
}

@article{ghorbani_interpretation_2019,
	title = {Interpretation of {Neural} {Networks} {Is} {Fragile}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4252},
	doi = {10.1609/aaai.v33i01.33013681},
	abstract = {In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientiﬁc robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. inﬂuence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.},
	language = {en},
	urldate = {2022-07-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	month = jul,
	year = {2019},
	pages = {3681--3688},
}

@inproceedings{torres-arias_-toto_2019,
	title = {in-toto: {Providing} farm-to-table guarantees for bits and bytes},
	isbn = {978-1-939133-06-9},
	shorttitle = {in-toto},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/torres-arias},
	language = {en},
	urldate = {2022-07-01},
	author = {Torres-Arias, Santiago and Afzali, Hammad and Kuppusamy, Trishank Karthik and Curtmola, Reza and Cappos, Justin},
	year = {2019},
	pages = {1393--1410},
}

@book{reinders_data_2021,
	address = {Berkeley, CA},
	title = {Data {Parallel} {C}++: {Mastering} {DPC}++ for {Programming} of {Heterogeneous} {Systems} using {C}++ and {SYCL}},
	isbn = {978-1-4842-5573-5 978-1-4842-5574-2},
	shorttitle = {Data {Parallel} {C}++},
	url = {http://link.springer.com/10.1007/978-1-4842-5574-2},
	language = {en},
	urldate = {2022-06-29},
	publisher = {Apress},
	author = {Reinders, James and Ashbaugh, Ben and Brodman, James and Kinsner, Michael and Pennycook, John and Tian, Xinmin},
	year = {2021},
	doi = {10.1007/978-1-4842-5574-2},
}

@inproceedings{chida2022lookaheads,
	title = {On lookaheads in regular expressions with backreferences},
	booktitle = {7th international conference on formal structures for computation and deduction},
	author = {Chida, Nariyoshi and Terauchi, Tachio},
	year = {2022},
}

@techreport{noauthor_dod_2015,
	title = {{DoD} {Cyber} {Strategy}},
	year = {2015},
}

@inproceedings{votipka_hacked_2021,
	address = {San Francisco, CA, USA},
	title = {{HackEd}: {A} {Pedagogical} {Analysis} of {Online} {Vulnerability} {Discovery} {Exercises}},
	isbn = {978-1-72818-934-5},
	shorttitle = {{HackEd}},
	url = {https://ieeexplore.ieee.org/document/9519464/},
	doi = {10.1109/SP40001.2021.00092},
	abstract = {Hacking exercises are a common tool for security education, but there is limited investigation of how they teach security concepts and whether they follow pedagogical best practices. This paper enumerates the pedagogical practices of 31 popular online hacking exercises. Speciﬁcally, we derive a set of pedagogical dimensions from the general learning sciences and educational literature, tailored to hacking exercises, and review whether and how each exercise implements each pedagogical dimension. In addition, we interview the organizers of 15 exercises to understand challenges and tradeoffs that may occur when choosing whether and how to implement each dimension.},
	language = {en},
	urldate = {2022-06-27},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Votipka, Daniel and Zhang, Eric and Mazurek, Michelle L.},
	month = may,
	year = {2021},
	pages = {1268--1285},
}

@misc{noauthor_5_2012,
	title = {5 {Reasons} {Software} {Projects} {Fail}: {The} {Role} of {Requirements}},
	shorttitle = {5 {Reasons} {Software} {Projects} {Fail}},
	url = {https://argondigital.com/blog/product-management/5-reasons-software-projects-fail-hint-its-often-due-to-incomplete-incorrect-requirements/},
	abstract = {Business analysts, in requirements gathering and accurate, complete requirements definition, can significantly impact whether software projects succeed or fail.},
	language = {en-US},
	urldate = {2022-02-04},
	journal = {ArgonDigital {\textbar} Making Technology a Strategic Advantage},
	month = mar,
	year = {2012},
}

@inproceedings{vetterl_counting_2019,
	title = {Counting {Outdated} {Honeypots}: {Legal} and {Useful}},
	shorttitle = {Counting {Outdated} {Honeypots}},
	doi = {10.1109/SPW.2019.00049},
	abstract = {Honeypots are intended to be covert and so little is known about how many are deployed or who is using them. We used protocol deviations at the SSH transport layer to fingerprint Kippo and Cowrie, the two most popular medium interaction SSH honeypots. Several Internet-wide scans over a one year period revealed the presence of thousands of these honeypots. Sending specific commands revealed their patch status and showed that many systems were not up to date: a quarter or more were not fully updated and by the time of our last scan 20\% of honeypots were still running Kippo, which had last been updated several years earlier. However, our paper reporting these results was rejected from a major conference on the basis that our interactions with the honeypots were illegal and hence the research was unethical. We later published a much redacted account of our research which described the fingerprinting but omitted the results we had gained from the issuing of commands to check the patch status. In the present work we provide the missing results, but start with an extended ethical justification for our research and a detailed legal analysis to show why we did not infringe cybersecurity laws.},
	booktitle = {2019 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Vetterl, Alexander and Clayton, Richard and Walden, Ian},
	month = may,
	year = {2019},
	keywords = {Authorization, Computer security, Internet, Law, Protocols, ethical issues, ethics, honeypots, intrusion detection, measurement, unauthorised access},
	pages = {224--229},
}

@article{symonds_innovating_2017,
	title = {Innovating the {Prioritization} of {Cyber} {Defense}},
	volume = {16},
	issn = {1445-3312},
	url = {http://www.jstor.org/stable/26502753},
	abstract = {The U.S. Department of Defense (DoD) faces a monumental undertaking in protecting the infrastructure that underpins the entirety of its operations: It must identify and prioritize key terrain to dynamically defend. This paper will examine the criteria to identify critical information systems and infrastructure, will review the process to identify key terrain in cyberspace, and will offer a recommendation on how to more effectively prioritize network defender operations using data analytics.},
	number = {2},
	urldate = {2022-02-19},
	journal = {Journal of Information Warfare},
	author = {Symonds, R},
	year = {2017},
	note = {Publisher: Peregrine Technical Solutions},
	pages = {12--18},
}

@inproceedings{sobb_assessment_2020,
	title = {Assessment of {Cyber} {Security} {Implications} of {New} {Technology} {Integrations} into {Military} {Supply} {Chains}},
	doi = {10.1109/SPW50608.2020.00038},
	abstract = {Military supply chains play a critical role in the acquisition and movement of goods for defence purposes. The disruption of these supply chain processes can have potentially devastating affects to the operational capability of military forces. The introduction and integration of new technologies into defence supply chains can serve to increase their effectiveness. However, the benefits posed by these technologies may be outweighed by significant consequences to the cyber security of the entire defence supply chain. Supply chains are complex Systems of Systems, and the introduction of an insecure technology into such a complex ecosystem may induce cascading system-wide failure, and have catastrophic consequences to military mission assurance. Subsequently, there is a need for an evaluative process to determine the extent to which a new technology will affect the cyber security of military supply chains. This work proposes a new model, the Military Supply Chain Cyber Implications Model (M-SCCIM), that serves to aid military decision makers in understanding the potential cyber security impact of introducing new technologies to supply chains. M-SCCIM is a multiphase model that enables understanding of cyber security and supply chain implications through the lenses of theoretical examinations, pilot applications and system wide implementations.},
	booktitle = {2020 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Sobb, Theresa May and Turnbull, Benjamin},
	month = may,
	year = {2020},
	keywords = {Computer crime, Lenses, Power system faults, Power system protection, Privacy, Security, Supply chains, Systems-of-systems, cyber, cyber-security.framework, military, supply chain},
	pages = {128--135},
}

@inproceedings{spooner_navigating_2018,
	address = {San Francisco, CA},
	title = {Navigating the {Insider} {Threat} {Tool} {Landscape}: {Low} {Cost} {Technical} {Solutions} to {Jump} {Start} an {Insider} {Threat} {Program}},
	isbn = {978-1-5386-8276-0},
	shorttitle = {Navigating the {Insider} {Threat} {Tool} {Landscape}},
	url = {https://ieeexplore.ieee.org/document/8424656/},
	doi = {10.1109/SPW.2018.00040},
	language = {en},
	urldate = {2022-06-27},
	booktitle = {2018 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	publisher = {IEEE},
	author = {Spooner, Derrick and Silowash, George and Costa, Daniel and Albrethsen, Michael},
	month = may,
	year = {2018},
	pages = {247--257},
}

@inproceedings{sion_automated_2021,
	address = {Atlanta, GA, USA},
	title = {Automated {Threat} {Analysis} and {Management} in a {Continuous} {Integration} {Pipeline}},
	isbn = {978-1-66543-170-5},
	url = {https://ieeexplore.ieee.org/document/9652652/},
	doi = {10.1109/SecDev51306.2021.00021},
	abstract = {Security and privacy threat modeling is commonly applied to systematically identify and address design-level security and privacy concerns in the early stages of architecture and design. Identifying and resolving these threats should remain a continuous concern during the development lifecycle. Especially with contemporary agile development practices, a single-shot upfront analysis becomes quickly outdated. Despite it being explicitly recommended by experts, existing threat modeling approaches focus largely on early development phases and provide limited support during later implementation phases.},
	language = {en},
	urldate = {2022-06-27},
	booktitle = {2021 {IEEE} {Secure} {Development} {Conference} ({SecDev})},
	publisher = {IEEE},
	author = {Sion, Laurens and Van Landuyt, Dimitri and Yskout, Koen and Verreydt, Stef and Joosen, Wouter},
	month = oct,
	year = {2021},
	pages = {30--37},
}

@article{frik_privacy_nodate,
	title = {Privacy and {Security} {Threat} {Models} and {Mitigation} {Strategies} of {Older} {Adults}},
	abstract = {Older adults (65+) are becoming primary users of emerging smart systems, especially in health care. However, these technologies are often not designed for older users and can pose serious privacy and security concerns due to their novelty, complexity, and propensity to collect and communicate vast amounts of sensitive information. Efforts to address such concerns must build on an in-depth understanding of older adults’ perceptions and preferences about data privacy and security for these technologies, and accounting for variance in physical and cognitive abilities. In semi-structured interviews with 46 older adults, we identiﬁed a range of complex privacy and security attitudes and needs speciﬁc to this population, along with common threat models, misconceptions, and mitigation strategies. Our work adds depth to current models of how older adults’ limited technical knowledge, experience, and age-related declines in ability amplify vulnerability to certain risks; we found that health, living situation, and ﬁnances play a notable role as well. We also found that older adults often experience usability issues or technical uncertainties in mitigating those risks—and that managing privacy and security concerns frequently consists of limiting or avoiding technology use. We recommend educational approaches and usable technical protections that build on seniors’ preferences.},
	language = {en},
	author = {Frik, Alisa and Nurgalieva, Leysan and Bernd, Julia and Lee, Joyce S and Schaub, Florian and Egelman, Serge},
	pages = {21},
}

@article{fulton_effect_nodate,
	title = {The {Effect} of {Entertainment} {Media} on {Mental} {Models} of {Computer} {Security}},
	abstract = {When people inevitably need to make decisions about their computer-security posture, they rely on their mental models of threats and potential targets. Research has demonstrated that these mental models, which are often incomplete or incorrect, are informed in part by ﬁctional portrayals in television and ﬁlm. Inspired by prior research in public health demonstrating that efforts to ensure accuracy in the portrayal of medical situations has had an overall positive effect on public medical knowledge, we explore the relationship between computer security and ﬁctional television and ﬁlm. We report on a semistructured interview study (n=19) investigating what users have learned about computer security from mass media and how they evaluate what is and is not realistic within ﬁctional portrayals. In addition to conﬁrming prior ﬁndings that television and ﬁlm shape users’ mental models of security, we identify speciﬁc misconceptions that appear to align directly with common ﬁctional tropes. We identify speciﬁc proxies that people use to evaluate realism and examine how they inﬂuence these misconceptions. We conclude with recommendations for security researchers as well as creators of ﬁctional media when considering how to improve people’s understanding of computer-security concepts and behaviors.},
	language = {en},
	author = {Fulton, Kelsey R and Gelles, Rebecca and McKay, Alexandra and Roberts, Richard and Abdi, Yasmin and Mazurek, Michelle L},
	pages = {19},
}

@article{filar_ask_nodate,
	title = {Ask {Me} {Anything}: {A} {Conversational} {Interface} to {Augment} {Information} {Security} {Workers}},
	abstract = {Security products often create more problems than they solve, drowning users in alerts without providing the context required to remediate threats. This challenge is compounded by a lack of experienced personnel and security tools with complex interfaces. These interfaces require users to become domain experts or rely on repetitive, time consuming tasks to turn this data deluge into actionable intelligence. In this paper we present Artemis, a conversational interface to endpoint detection and response (EDR) event data. Artemis leverages dialog to drive the automation of complex tasks and reduce the need to learn a structured query language. Designed to empower inexperienced and junior security workers to better understand their security environment, Artemis provides an intuitive platform to ask questions of alert data as users are guided through triage and hunt workﬂows. In this paper, we will discuss our user-centric design methodology, feedback from user interviews, and the design requirements generated upon completion of our study. We will also present core functionality, ﬁndings from scenario-based testing, and future research for the Artemis platform.},
	language = {en},
	author = {Filar, Bobby and Seymour, Richard J and Park, Matthew},
	pages = {8},
}

@article{busse_replication_nodate,
	title = {Replication: {No} {One} {Can} {Hack} {My} {Mind} {Revisiting} a {Study} on {Expert} and {Non}-{Expert} {Security} {Practices} and {Advice}},
	abstract = {A 2015 study by Iulia Ion, Rob Reeder, and Sunny Consolvo examined the self-reported security behavior of security experts and non-experts. They also analyzed what kind of security advice experts gave to non-experts and how realistic and effective they think typical advice is.},
	language = {en},
	author = {Busse, Karoline and Schäfer, Julia and Smith, Matthew},
	pages = {21},
}

@misc{anton_devil_2019,
	title = {Devil in the {Detail}: {Attack} {Scenarios} in {Industrial} {Applications}},
	shorttitle = {Devil in the {Detail}},
	url = {http://arxiv.org/abs/1905.10292},
	doi = {10.48550/arXiv.1905.10292},
	abstract = {In the past years, industrial networks have become increasingly interconnected and opened to private or public networks. This leads to an increase in efficiency and manageability, but also increases the attack surface. Industrial networks often consist of legacy systems that have not been designed with security in mind. In the last decade, an increase in attacks on cyber-physical systems was observed, with drastic consequences on the physical work. In this work, attack vectors on industrial networks are categorised. A real-world process is simulated, attacks are then introduced. Finally, two machine learning-based methods for time series anomaly detection are employed to detect the attacks. Matrix Profiles are employed more successfully than a predictor Long Short-Term Memory network, a class of neural networks.},
	urldate = {2022-06-27},
	publisher = {arXiv},
	author = {Anton, Simon D. Duque and Hafner, Alexander and Schotten, Hans Dieter},
	month = may,
	year = {2019},
	note = {Number: arXiv:1905.10292
arXiv:1905.10292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{alqhatani_there_nodate,
	title = {“{There} is nothing that {I} need to keep secret”: {Sharing} {Practices} and {Concerns} of {Wearable} {Fitness} {Data}},
	abstract = {There has been increasing use of commercial wearable devices for tracking fitness-related activities in the past few years. These devices sense and collect a variety of personal health and fitness data, which can be shared by users with different audiences. Yet, little is known about users’ practices for sharing information collected by these devices, and the concerns they have when disclosing this information across a variety of platforms. In this study, we conducted 30 semi-structured interviews with wearable fitness device users to understand their sharing intentions and practices, and to examine what they do to manage their privacy. We describe a set of common goals for sharing health and fitness information, which then influence users’ choices of the recipients and the specific practices they employ to share that information. Our findings indicate that participants were primarily concerned about acceptable norms and selfpresentation rather than the sensitivity of the information. Our results provide a set of common goals and practices which can inspire new applications and help improve existing platforms for sharing sensed fitness information.},
	language = {en},
	author = {Alqhatani, Abdulmajeed and Lipford, Heather Richter},
	pages = {15},
}

@inproceedings{everson_network_2020,
	title = {Network {Attack} {Surface} {Simplification} for {Red} and {Blue} {Teams}},
	doi = {10.1109/SecDev45635.2020.00027},
	abstract = {Network port scans are a key first step to developing a true understanding of a network-facing attack surface. However in large-scale networks, the data resulting from such scans can be too numerous for Red Teams to process for manual and semiautomatic testing. Indiscriminate port scans can also compromise a Red Team seeking to quickly gain a foothold on a network. A large attack surface can even complicate Blue Team activities like threat hunting. In this paper we provide a cluster analysis methodology designed to group similar hosts to reduce security team workload and Red Team observability. We also measure the Internet-facing network attack surface of 13 organizations by clustering their hosts based on similarity. Through a case study we demonstrate how the output of our clustering technique provides new insight to both Red and Blue Teams, allowing them to quickly identify potential high-interest points on the attack surface.},
	booktitle = {2020 {IEEE} {Secure} {Development} ({SecDev})},
	author = {Everson, Douglas and Cheng, Long},
	month = sep,
	year = {2020},
	keywords = {Cluster Analysis, Clustering algorithms, Complexity theory, Network Attack Surface, Organizations, Shape, Software algorithms, Surface treatment, Tools},
	pages = {74--80},
}

@inproceedings{acosta_cybersecurity_2020,
	title = {Cybersecurity {Deception} {Experimentation} {System}},
	doi = {10.1109/SecDev45635.2020.00022},
	abstract = {Cybersecurity deception research has had many successes in recent years. While early cyber deception systems (e.g., honeypots) were largely static, recent approaches leverage computational game theory and machine learning techniques to allow for dynamic deception strategies that can potentially observe, react, and adapt to an adversary in both the short and long term. However, applying these theoretical models and algorithms in real-world settings poses additional considerations that are not always apparent in theory. Currently, testbeds and experimentation platforms for dynamic deception are lacking, limiting the ability of researchers and analysts to test and evaluate these approaches using realistic scenarios and data. Honeypots are a technology where these dynamic deception methods can have a great impact on effectiveness. The basic technology to mimic nodes and network services has been used for several decades and is effective against less experienced adversaries, but is less useful against sophisticated intruders. Using adaptation, behavior-based model development and reasoning and other artificial intelligence techniques have the potential to make honeypots much more effective against experienced adversaries by making them less predictable and more targeted. Before these novel technologies can be used in the real world, it is critical that they are tested and validated on realistic systems and in realistic settings. We present the Cybersecurity Deception Experimentation System (CDES) that extends the Common Open Research Emulator (CORE) to provide a platform that is capable of evaluating dynamic deception algorithms. We also provide three case studies that demonstrate how CDES can be used to practically implement dynamic honeypots in increasingly complex scenarios and discuss some nuances of each implementation.},
	booktitle = {2020 {IEEE} {Secure} {Development} ({SecDev})},
	author = {Acosta, Jaime C. and Basak, Anjon and Kiekintveld, Christopher and Leslie, Nandi and Kamhoua, Charles},
	month = sep,
	year = {2020},
	keywords = {Computer security, Cyber Deception, Cybersecurity, Emulation, Graphical user interfaces, Monitoring, Network Security, Sockets, Software, Switches, Testbed, Tools},
	pages = {34--40},
}

@article{obada-obieh_challenges_nodate,
	title = {Challenges and {Threats} of {Mass} {Telecommuting}: {A} {Qualitative} {Study} of {Workers}},
	abstract = {This paper reports the security and privacy challenges and threats that people experience while working from home. We conducted semi-structured interviews with 24 participants working from home in the three weeks preceding the study. We asked questions related to participants’ challenges with telecommuting. Our results suggest that participants experienced challenges, threats, and potential outcomes of threats associated with the technological, human, organizational, and environmental dimensions. We also discovered two threat models: one in which the employer’s asset is at stake and another in which the employee’s privacy is compromised. We believe these insights can lead to better support for employees and possibly reduce cyber-attacks associated with telecommuting during the pandemic and beyond.},
	language = {en},
	author = {Obada-Obieh, Borke and Huang, Yue and Beznosov, Konstantin},
	pages = {21},
}

@article{brink_security_nodate,
	title = {{SECURITY} {AWARENESS} {TRAINING}: {SMALL} {INVESTMENT}, {LARGE} {REDUCTION} {IN} {RISK}},
	language = {en},
	author = {Brink, Derek E},
	pages = {14},
}

@techreport{cichonski_computer_2012,
	title = {Computer {Security} {Incident} {Handling} {Guide} : {Recommendations} of the {National} {Institute} of {Standards} and {Technology}},
	shorttitle = {Computer {Security} {Incident} {Handling} {Guide}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf},
	abstract = {Computer security incident response has become an important component of information technology (IT) programs. Because performing incident response effectively is a complex undertaking, establishing a successful incident response capability requires substantial planning and resources. This publication assists organizations in establishing computer security incident response capabilities and handling incidents efficiently and effectively. This publication provides guidelines for incident handling, particularly for analyzing incident-related data and determining the appropriate response to each incident. The guidelines can be followed independently of particular hardware platforms, operating systems, protocols, or applications.},
	language = {en},
	number = {NIST SP 800-61r2},
	urldate = {2022-04-12},
	institution = {National Institute of Standards and Technology},
	author = {Cichonski, Paul and Millar, Tom and Grance, Tim and Scarfone, Karen},
	month = aug,
	year = {2012},
	doi = {10.6028/NIST.SP.800-61r2},
	pages = {NIST SP 800--61r2},
}

@article{pennington_getting_nodate,
	title = {Getting {Started} with {ATT}\&{CK}},
	language = {en},
	author = {Pennington, Adam},
	pages = {45},
}

@article{noauthor_business_2015,
	title = {Business {Blackout}: {The} insurance implications of a cyber attack on the {US} power grid},
	language = {en},
	year = {2015},
	pages = {68},
}

@techreport{noauthor_ibm_nodate,
	title = {{IBM} {Cyber} {Report2022}},
}

@article{noauthor_cost_nodate,
	title = {Cost of a {Data} {Breach} {Report} 2020},
	language = {en},
	pages = {82},
}

@book{stadtler_supply_2008,
	address = {Berlin, Heidelberg},
	edition = {4th},
	title = {Supply {Chain} {Management} and {Advanced} {Planning}},
	isbn = {978-3-540-74511-2},
	url = {http://link.springer.com/10.1007/978-3-540-74512-9},
	abstract = {What is the essence of Supply Chain Management (SCM)? How does it relate to Advanced Planning? In which sense are the underlying planning concepts “advanced”? What are the origins of SCM? These as well as related questions will be answered in this chapter.},
	language = {en},
	urldate = {2022-06-24},
	publisher = {Springer Berlin Heidelberg},
	editor = {Stadtler, Hartmut and Kilger, Christoph},
	year = {2008},
	doi = {10.1007/978-3-540-74512-9},
}

@article{lamb_reproducible_2022,
	title = {Reproducible {Builds}: {Increasing} the {Integrity} of {Software} {Supply} {Chains}},
	volume = {39},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Reproducible {Builds}},
	url = {https://ieeexplore.ieee.org/document/9403390/},
	doi = {10.1109/MS.2021.3073045},
	number = {2},
	urldate = {2022-06-22},
	journal = {IEEE Software},
	author = {Lamb, Chris and Zacchiroli, Stefano},
	month = mar,
	year = {2022},
	pages = {62--70},
}

@techreport{paulsen_criticality_2018,
	title = {Criticality {Analysis} {Process} {Model}: {Prioritizing} {Systems} and {Components}},
	shorttitle = {Criticality {Analysis} {Process} {Model}},
	url = {https://csrc.nist.gov/publications/detail/nistir/8179/final},
	abstract = {In the modern world, where complex systems and systems-of-systems are integral to the functioning of society and businesses, it is increasingly important to be able to understand and manage risks that these systems and components may present to the missions that they support. However, in the world of finite resources, it is not possible to apply equal protection to all assets. This publication describes a comprehensive Criticality Analysis Process Model – a structured method of prioritizing programs, systems, and components based on their importance to the goals of an organization and the impact that their inadequate operation or loss may present to those goals. A criticality analysis can help organizations identify and better understand the systems, subsystems, components, and subcomponents that are most essential to their operations and the environment in which they operate. That understanding facilitates better decision making related to the management of an organization’s...},
	language = {en},
	number = {NIST Internal or Interagency Report (NISTIR) 8179},
	urldate = {2022-06-17},
	institution = {National Institute of Standards and Technology},
	author = {Paulsen, Celia and Boyens, Jon and Bartol, Nadya and Winkler, Kris},
	month = apr,
	year = {2018},
	doi = {10.6028/NIST.IR.8179},
}

@misc{kaczorowski_secure_2020,
	title = {Secure at every step: {What} is software supply chain security and why does it matter?},
	shorttitle = {Secure at every step},
	url = {https://github.blog/2020-09-02-secure-your-software-supply-chain-and-protect-against-supply-chain-threats-github-blog/},
	abstract = {The most important way to protect supply chain threats - scan code for security vulnerabilities, learn how to find vulnerabilities in code, and quickly patch them with dynamic code analysis tools.},
	language = {en-US},
	urldate = {2022-06-21},
	journal = {The GitHub Blog},
	author = {Kaczorowski, Maya},
	month = sep,
	year = {2020},
}

@misc{noauthor_3_nodate,
	title = {3 {Ways} to {Mitigate} {Risk} {When} {Using} {Private} {Package} {Feeds}},
	url = {https://azure.microsoft.com/en-us/resources/3-ways-to-mitigate-risk-using-private-package-feeds/},
	abstract = {Software today has become an assembly of components from a wide range of sources. Many organizations use public package feeds to take advantage of the open ecosystems they offer. Projects that consume packages from multiple public and private feeds may be exposed to supply chain vulnerabilitie...},
	language = {en},
	urldate = {2022-06-21},
}

@techreport{noauthor_shifting_nodate,
	title = {Shifting left on security: {Securing} software supply chains {\textbar} {Solutions}},
	shorttitle = {Shifting left on security},
	url = {https://cloud.google.com/solutions/shifting-left-on-security},
	language = {en},
	urldate = {2022-06-21},
}

@inproceedings{pletea_security_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {Security and emotion: sentiment analysis of security discussions on {GitHub}},
	isbn = {978-1-4503-2863-0},
	shorttitle = {Security and emotion},
	url = {https://doi.org/10.1145/2597073.2597117},
	doi = {10.1145/2597073.2597117},
	abstract = {Application security is becoming increasingly prevalent during software and especially web application development. Consequently, countermeasures are continuously being discussed and built into applications, with the goal of reducing the risk that unauthorized code will be able to access, steal, modify, or delete sensitive data. In this paper we gauged the presence and atmosphere surrounding security-related discussions on GitHub, as mined from discussions around commits and pull requests. First, we found that security related discussions account for approximately 10\% of all discussions on GitHub. Second, we found that more negative emotions are expressed in security-related discussions than in other discussions. These findings confirm the importance of properly training developers to address security concerns in their applications as well as the need to test applications thoroughly for security vulnerabilities in order to reduce frustration and improve overall project atmosphere.},
	urldate = {2022-06-18},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Pletea, Daniel and Vasilescu, Bogdan and Serebrenik, Alexander},
	month = may,
	year = {2014},
	keywords = {GitHub, Security, mining challenge, sentiment analysis},
	pages = {348--351},
}

@inproceedings{dors_reflective_2020,
	title = {Reflective {Practice} in {Software} {Development} {Studios}: {Findings} from an {Ethnographic} {Study}},
	shorttitle = {Reflective {Practice} in {Software} {Development} {Studios}},
	doi = {10.1109/CSEET49119.2020.9206217},
	abstract = {Over the last two decades, software educators have adopted new approaches, techniques, and tools for practical learning. Previous research has found that studio-based learning, which has been in use by some design and architecture courses, is suitable for learning the practical aspects of software engineering. The studies available recognize the presence of reflective practice in software development studios; however, they do not show empirical evidence of its contribution to learning. The goal of this study is to understand the use of reflective practice in software development studios and its contributions to the practical learning of software engineering. The qualitative data were collected using an ethnographic method through participant observation and from written students' self-reflections, which were analyzed using Cycle Coding Method supported by Atlas.ti Qualitative Data Analysis tool. The findings suggest that reflective practice promotes the emergence of new ideas while contributing to the development of skills that are valuable to software engineering professional practice. This research presents a qualitative look at how these are developed in the context of a particular software development studio.},
	booktitle = {2020 {IEEE} 32nd {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	author = {Dors, Tania Mara and Van Amstel, Frederick M. C. and Binder, Fabio and Reinehr, Sheila and Malucelli, Andreia},
	month = nov,
	year = {2020},
	note = {ISSN: 2377-570X},
	keywords = {Computer Science Practical Learning, Computer architecture, Education, Reflection, Reflective Practice, Software Engineering Education, Software Engineering Practical Skills Development, Software Practice, Software design, Software engineering, Studio-based Learning, Tools},
	pages = {1--10},
}

@article{yan_learning_2020,
	title = {Learning {From} {Design} {Failure}, {Collaboratively}},
	abstract = {Effective design requires iterative cycles of learning from failure, where design teams evaluate their designs to identify problems and make iterative improvement. However, existing learning from failure models do not always take collaborative process, a key aspect of design, into account. The ultimate goal is to design tools to better support young design teams’ collaborative learning from design failure process in an after-school design club with students in grades 4-6. In this article, we examine how teams use discourse to learn from design failure collaboratively by analyzing video data and group artifacts. Our findings suggest that the specific communication process can support or hinder the learning from failure in design contexts and that there’s a need to support team’s regulation process so they can benefit from their design failure.},
	language = {en},
	journal = {ICLS},
	author = {Yan, Shulong and Borge, Marcela},
	year = {2020},
	pages = {8},
}

@inproceedings{rashidi_you_2018,
	title = {"{You} don't want to be the next meme": {College} {Students}' {Workarounds} to {Manage} {Privacy} in the {Era} of {Pervasive} {Photography}},
	isbn = {978-1-939133-10-6},
	shorttitle = {"{You} don't want to be the next meme"},
	url = {https://www.usenix.org/conference/soups2018/presentation/rashidi},
	language = {en},
	urldate = {2022-06-17},
	author = {Rashidi, Yasmeen and Ahmed, Tousif and Patel, Felicia and Fath, Emily and Kapadia, Apu and Nippert-Eng, Christena and Su, Norman Makoto},
	year = {2018},
	pages = {143--157},
}

@inproceedings{pham_aflnet_2020,
	title = {{AFLNET}: {A} {Greybox} {Fuzzer} for {Network} {Protocols}},
	shorttitle = {{AFLNET}},
	doi = {10.1109/ICST46399.2020.00062},
	abstract = {Server fuzzing is difficult. Unlike simple command-line tools, servers feature a massive state space that can be traversed effectively only with well-defined sequences of input messages. Valid sequences are specified in a protocol. In this paper, we present AFLNET, the first greybox fuzzer for protocol implementations. Unlike existing protocol fuzzers, AFLNET takes a mutational approach and uses state-feedback to guide the fuzzing process. AFLNET is seeded with a corpus of recorded message exchanges between the server and an actual client. No protocol specification or message grammars are required. AFLNET acts as a client and replays variations of the original sequence of messages sent to the server and retains those variations that were effective at increasing the coverage of the code or state space. To identify the server states that are exercised by a message sequence, AFLNET uses the server's response codes. From this feedback, AFLNET identifies progressive regions in the state space, and systematically steers towards such regions. The case studies with AFLNET on two popular protocol implementations demonstrate a substantial performance boost over the state-of the-art. AFLNET discovered two new CVEs which are classified as critical (CVSS score CRITICAL 9.8).},
	booktitle = {2020 {IEEE} 13th {International} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Pham, Van-Thuan and Böhme, Marcel and Roychoudhury, Abhik},
	month = oct,
	year = {2020},
	note = {ISSN: 2159-4848},
	keywords = {Computer bugs, Data models, Fuzzing, Protocols, Security, Servers, Tools},
	pages = {460--465},
}

@misc{congdon1999techniques,
	title = {Techniques and recommendations for implementing valuable postmortems in software development projects},
	publisher = {May},
	author = {Congdon, Gloria C},
	year = {1999},
}

@article{norman1990commentary,
	title = {Commentary: {Human} error and the design of computer systems},
	volume = {33},
	number = {1},
	journal = {Communications of the ACM},
	author = {Norman, Donald A},
	year = {1990},
	note = {Publisher: Association for Computing Machinery, Inc.},
	pages = {4--7},
}

@article{tchankova_risk_2002,
	title = {Risk identification – basic stage in risk management},
	volume = {13},
	issn = {0956-6163},
	url = {https://doi.org/10.1108/09566160210431088},
	doi = {10.1108/09566160210431088},
	abstract = {In this paper risk identification is investigated as a basic stage in risk management. The risk identification phase as the first stage in the risk management process is presented and its leading role for effective risk management is proved. The basic terms that are necessary for building of the frame approach for risk identification are defined: sources of risk‐hazard, factor‐peril‐resources exposed to risk. A classification of risk sources – physical, social, political, operational, economic, legal and cognitive environment – is proposed. It allows covering all types of risk facing the organisation. A grouping of the resources exposed to risk such as physical, human, and financial resources is introduced. It is based on a practical consideration of the risk situations in the organisations.},
	number = {3},
	urldate = {2022-06-16},
	journal = {Environmental Management and Health},
	author = {Tchankova, Lubka},
	month = jan,
	year = {2002},
	note = {Publisher: MCB UP Ltd},
	keywords = {Risk, Risk management},
	pages = {290--297},
}

@inproceedings{gunawi2014bugs,
	title = {What bugs live in the cloud? a study of 3000+ issues in cloud systems},
	booktitle = {Proceedings of the {ACM} symposium on cloud computing ({SOCC})},
	author = {Gunawi, Haryadi S and Hao, Mingzhe and Leesatapornwongsa, Tanakorn and Patana-anake, Tiratat and Do, Thanh and Adityatama, Jeffry and Eliazar, Kurnia J and Laksono, Agung and Lukman, Jeffrey F and Martin, Vincentius and {others}},
	year = {2014},
	pages = {1--14},
}

@article{mazuera2019android,
	title = {The {Android} {OS} stack and its vulnerabilities: an empirical study},
	volume = {24},
	number = {4},
	journal = {Empirical Software Engineering},
	author = {Mazuera-Rozo, Alejandro and Bautista-Mora, Jairo and Linares-Vásquez, Mario and Rueda, Sandra and Bavota, Gabriele},
	year = {2019},
	note = {Publisher: Springer},
	pages = {2056--2101},
}

@inproceedings{li2021understanding,
	title = {Understanding and detecting performance bugs in markdown compilers},
	booktitle = {2021 36th {IEEE}/{ACM} international conference on automated software engineering ({ASE})},
	author = {Li, Penghui and Liu, Yinxi and Meng, Wei},
	year = {2021},
	note = {tex.organization: IEEE},
	pages = {892--904},
}

@inproceedings{chen2019understanding,
	title = {Understanding exception-related bugs in large-scale cloud systems},
	booktitle = {2019 34th {IEEE}/{ACM} international conference on automated software engineering ({ASE})},
	author = {Chen, Haicheng and Dou, Wensheng and Jiang, Yanyan and Qin, Feng},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {339--351},
}

@inproceedings{wang_argulens_2020,
	address = {New York, NY, USA},
	title = {{ArguLens}: {Anatomy} of {Community} {Opinions} {On} {Usability} {Issues} {Using} {Argumentation} {Models}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {{ArguLens}},
	url = {https://doi.org/10.1145/3313831.3376218},
	abstract = {In open-source software (OSS), the design of usability is often influenced by the discussions among community members on platforms such as issue tracking systems (ITSs). However, digesting the rich information embedded in issue discussions can be a major challenge due to the vast number and diversity of the comments. We propose and evaluate ArguLens, a conceptual framework and automated technique leveraging an argumentation model to support effective understanding and consolidation of community opinions in ITSs. Through content analysis, we anatomized highly discussed usability issues from a large, active OSS project, into their argumentation components and standpoints. We then experimented with supervised machine learning techniques for automated argument extraction. Finally, through a study with experienced ITS users, we show that the information provided by ArguLens supported the digestion of usability-related opinions and facilitated the review of lengthy issues. ArguLens provides the direction of designing valuable tools for high-level reasoning and effective discussion about usability.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wenting and Arya, Deeksha and Novielli, Nicole and Cheng, Jinghui and Guo, Jin L.C.},
	month = apr,
	year = {2020},
	keywords = {argumentation analysis, issue discussion analysis, online communities, open source software, usability},
	pages = {1--14},
}

@misc{noauthor_managed_nodate,
	title = {Managed {Detection} and {Response} ({MDR}) {Services} {\textbar} {Mandiant}},
	url = {https://www.mandiant.com/advantage/managed-defense},
	abstract = {Mandiant's Managed Detection and Response (MDR) service defends your business across endpoint, network, cloud, email, and operational technology.},
	language = {en},
	urldate = {2022-06-14},
}

@article{infocyte_wwwinfocytecom_nodate,
	title = {www.infocyte.com @{InfocyteInc}},
	language = {en},
	author = {Infocyte, Reserved and Hunt, Infocyte},
	pages = {2},
}

@misc{noauthor_falcon_2022,
	title = {Falcon {Overwatch}: {Managed} \& {Proactive} {Threat} {Hunting} {\textbar} {CrowdStrike}},
	shorttitle = {Falcon {Overwatch}},
	url = {https://www.crowdstrike.com/endpoint-security-products/falcon-overwatch-threat-hunting/},
	abstract = {Falcon OverWatch is a managed hunting service to help prioritize \& respond to threats created by security professionals. View the benefits of OverWatch here!},
	language = {en},
	urldate = {2022-02-19},
	journal = {crowdstrike.com},
	month = feb,
	year = {2022},
}

@misc{noauthor_detection_2022,
	title = {Detection and {Prevention} {\textbar} {CISA}},
	url = {https://www.cisa.gov/detection-and-prevention},
	urldate = {2022-06-08},
	month = jun,
	year = {2022},
}

@phdthesis{celikAUTOMATEDIOTSECURITY2019,
	title = {Automated {IoT} {Security} and {Privacy} {ANalysis}},
	language = {en},
	school = {The Pennsylvania State University},
	author = {Celik, Zeynel Berkay},
	year = {2019},
}

@misc{noauthor_all_2022,
	title = {All {Cyber} {Mission} {Force} {Teams} {Achieve} {Initial} {Operating} {Capability}},
	url = {https://www.defense.gov/News/News-Stories/Article/Article/984663/all-cyber-mission-force-teams-achieve-initial-operating-capability/},
	abstract = {All 133 of U.S. Cyber Command’s Cyber Mission Force teams have reached a threshold level of initial operating capacity and can execute their fundamental mission, Cybercom officials announced.},
	language = {en-US},
	urldate = {2022-06-08},
	journal = {U.S. Department of Defense},
	month = jun,
	year = {2022},
}

@misc{mcclanahan_169th_2019,
	title = {169th {Cyber} {Protection} {Team} – {Highly} {Capable}, {Always} {Ready}},
	url = {https://news.maryland.gov/ng/2019/11/06/169th-cyber-protection-team-highly-capable-always-ready/},
	language = {en-US},
	urldate = {2022-02-19},
	journal = {Maryland National Guard News},
	author = {McClanahan, Sarah M.},
	month = nov,
	year = {2019},
}

@misc{noauthor_threat_nodate,
	title = {Threat {Hunting}},
	url = {https://www.boozallen.com/expertise/cybersecurity/threat-hunting.html},
	abstract = {Our cyber threat hunting services can find advanced persistent threats to reduce operational, financial risk.},
	language = {en},
	urldate = {2022-02-19},
}

@article{trent_modelling_2019,
	title = {Modelling the {Cognitive} {Work} of {Cyber} {Protection} {Teams}},
	volume = {4},
	issn = {2474-2120},
	url = {http://www.jstor.org/stable/26623071},
	abstract = {Cyber Protection Teams (CPTs) defend our Nation’s critical military networks. While Cyber Security Service Providers are responsible for the continuous monitoring and vulnerability patching of networks, CPTs perform threat-oriented missions to defeat adversaries within and through cyberspace. The research we report here provides a descriptive workflow of cyber defense in CPTs as well as a prescriptive work model that all CPTs should be capable of executing. This paper describes how these models were developed and used to assess technologies and performance of CPTs. Such models offer a variety of benefits to practitioner and research communities, particularly when the domain of practice is closed to most researchers. This project demonstrates the need for continual curation of CPT work models as well as the need for models of work for the other types of cyber teams (i.e. Mission and Support) in the Cyber Mission Force.},
	number = {1},
	urldate = {2022-02-19},
	journal = {The Cyber Defense Review},
	author = {Trent, Stoney and Hoffman, Robert R. and Merritt, David and Smith, Sarah},
	year = {2019},
	note = {Publisher: Army Cyber Institute},
	pages = {125--136},
}

@misc{noauthor_ctir_nodate,
	title = {{CTIR} - {Threat} {Hunting} {\textbar}{\textbar} {Cisco} {Talos} {Intelligence} {Group} - {Comprehensive} {Threat} {Intelligence}},
	url = {https://talosintelligence.com/incident_response/hunting},
	urldate = {2022-06-14},
}

@misc{noauthor_executive_2021,
	title = {Executive {Order} on {Improving} the {Nation}'s {Cybersecurity}},
	url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/},
	abstract = {By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as follows:Section 1.},
	language = {en-US},
	urldate = {2022-06-14},
	journal = {The White House},
	month = may,
	year = {2021},
}

@incollection{lewkowicz_supporting_2010,
	address = {London},
	title = {Supporting {Reflection} in {Software} {Development} with {Everyday} {Working} {Tools}},
	isbn = {978-1-84996-210-0 978-1-84996-211-7},
	url = {http://link.springer.com/10.1007/978-1-84996-211-7_9},
	abstract = {Through their day-to-day usage collaboration tools collect data on the work process. These data can be used to aid participants’ retrospective reflection on the process. The paper shows how this can be done in software development project work. Through a case study we demonstrate how retrospective reflection was conducted by use of an industry approach to project retrospectives combined with the examination of historical data in Trac, an issue tracker. The data helped the team reconstruct the project trajectory by aiding the recall of significant events, leading to a shift in the team’s perspective on the project. The success of the toolaided retrospective reflection is attributed to its organization as well as the type of historical data examined through the tool and the tool features for navigating the data. These insights can be used to help project teams determine the potential of their tools to aid retrospective reflection.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of {COOP} 2010},
	publisher = {Springer London},
	author = {Krogstie, Birgit and Divitini, Monica},
	editor = {Lewkowicz, Myriam and Hassanaly, Parina and Wulf, Volker and Rohde, Markus},
	year = {2010},
	doi = {10.1007/978-1-84996-211-7_9},
	pages = {141--162},
}

@book{beyer2016site,
	title = {Site reliability engineering: {How} {Google} runs production systems},
	publisher = {" O'Reilly Media, Inc."},
	author = {Beyer, Betsy and Jones, Chris and Petoff, Jennifer and Murphy, Niall Richard},
	year = {2016},
}

@techreport{noauthor_isoiecieee_2017,
	title = {{ISO}/{IEC}/{IEEE} {International} {Standard} - {Systems} and software engineering -- {Software} life cycle processes},
	url = {https://ieeexplore.ieee.org/document/8100771/},
	language = {en},
	number = {12207:2017(E)},
	urldate = {2022-01-19},
	institution = {IEEE},
	year = {2017},
	doi = {10.1109/IEEESTD.2017.8100771},
	note = {ISBN: 9781504442534},
}

@techreport{ISO90003,
	address = {Geneva, CH},
	type = {Standard},
	title = {{ISO}/{IEC}/{IEEE} 90003:2018 {Software} engineering — {Guidelines} for the application of {ISO} 9001:2015 to computer software},
	institution = {International Organization for Standardization},
	year = {2018},
	note = {Volume: 2018
tex.key: ISO 90003:2018(E)},
}

@techreport{ISO9001,
	address = {Geneva, CH},
	type = {Standard},
	title = {{ISO} 9001: {Quality} management systems-requirements},
	institution = {International Organization for Standardization},
	year = {2015},
	note = {Volume: 2015
tex.key: ISO 9001:2015(E)},
}

@techreport{noauthor_ieee_2014,
	title = {{IEEE} {Standard} for {Software} {Quality} {Assurance} {Processes}},
	abstract = {Abstract: Requirements for initiating, planning, controlling, and executing the Software Quality Assurance processes of a software development or maintenance project are established in this standard. This standard is harmonized with the software life cycle process of ISO/IEC/IEEE 12207:2008 and the information content requirements of ISO/IEC/IEEE 15289:2011.},
	language = {en},
	year = {2014},
	pages = {138},
}

@book{humphrey1995discipline,
	title = {A discipline for software engineering},
	publisher = {Pearson Education India},
	author = {Humphrey, Watts S},
	year = {1995},
}

@article{lyytinen_learning_1999,
	title = {Learning failure in information systems development},
	volume = {9},
	issn = {1350-1917, 1365-2575},
	url = {http://doi.wiley.com/10.1046/j.1365-2575.1999.00051.x},
	doi = {10.1046/j.1365-2575.1999.00051.x},
	abstract = {Information systems development is a high-risk undertaking, and failures remain common despite advances in development tools and technologies. In this paper, we argue that one reason for this is the collapse of organizational intelligence required to deal with the complexities of systems development. Organizations fail to learn from their experience in systems development because of limits of organizational intelligence, disincentives for learning, organizational designs and educational barriers. Not only have many organizations failed to learn, but they have also learned to fail. Over time they accept and expect poor performance while creating organizational myths that perpetuate short-term optimization. This paper illustrates learning failure in systems development and recommends tactics for overcoming it.},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {Information Systems Journal},
	author = {Lyytinen, Kalle and Robey, Daniel},
	month = apr,
	year = {1999},
	pages = {85--101},
}

@inproceedings{buttler_rethinking_2012,
	address = {Graz, Austria},
	title = {Rethinking lessons learned capturing: using storytelling, root cause analysis, and collaboration engineering to capture lessons learned about project management},
	isbn = {978-1-4503-1242-4},
	shorttitle = {Rethinking lessons learned capturing},
	url = {http://dl.acm.org/citation.cfm?doid=2362456.2362460},
	doi = {10.1145/2362456.2362460},
	abstract = {Lessons learned are one way to retain experience and knowledge in project-based organizations, helping them to prevent reinventing the wheel or repeating past mistakes. However, there are several challenges that make capturing lessons learned a challenging endeavor. These include capturing knowledge about project management, allowing learning from mistakes, and handling the group processes within the project team. We introduce a novel approach combining elements from storytelling, root cause analysis, and collaboration engineering to address these challenges, and report on ﬁrst experiences utilizing this approach in a project in the oil and gas industry.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Knowledge} {Management} and {Knowledge} {Technologies} - i-{KNOW} '12},
	publisher = {ACM Press},
	author = {Buttler, Tanja and Lukosch, Stephan},
	year = {2012},
	pages = {1},
}

@article{baaz_appreciating_2010,
	title = {Appreciating {Lessons} {Learned}},
	volume = {27},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/5339121/},
	doi = {10.1109/MS.2009.198},
	language = {en},
	number = {4},
	urldate = {2022-06-14},
	journal = {IEEE Software},
	author = {Baaz, Anders and Holmberg, Lena and Nilsson, Agneta and Olsson, Helena Holmström and Sandberg, Anna Börjesson},
	month = jul,
	year = {2010},
	pages = {72--79},
}

@phdthesis{krogstie2010work,
	title = {The work-reflection-learning cycle in software engineering student projects: {Use} of collaboration tools},
	school = {Citeseer},
	author = {Krogstie, Birgit Rognebakke},
	year = {2010},
}

@misc{noauthor_california_2018,
	title = {California {Consumer} {Privacy} {Act} ({CCPA})},
	url = {https://oag.ca.gov/privacy/ccpa},
	abstract = {The California Consumer Privacy Act of 2018 (CCPA) gives consumers more control over the personal information that businesses collect about them and the CCPA regulations provide guidance on how to implement the law.},
	language = {en},
	urldate = {2022-06-13},
	journal = {State of California - Department of Justice - Office of the Attorney General},
	month = oct,
	year = {2018},
}

@inproceedings{tondel_using_2020,
	address = {Bari Italy},
	title = {Using {Situational} and {Narrative} {Analysis} for {Investigating} the {Messiness} of {Software} {Security}},
	isbn = {978-1-4503-7580-1},
	url = {https://dl.acm.org/doi/10.1145/3382494.3422162},
	doi = {10.1145/3382494.3422162},
	abstract = {Background: Software engineering work and its context often has characteristics of what in social science is termed ‘messy’; it has ephemeral and irregular qualities. This puts high demands on researchers doing inquiry and analysis. Aims: This paper aims to show what a combination of situational analysis (SA) and narrative analysis (NA) can bring to qualitative software engineering research, and in particular for situations characterised by mess. Method: SA and NA were applied to a case study on software security. Results: We found that these analysis methods helped us gain new insights and understandings and a broader perspective of the situation we are studying. Additionally, the methods helped collaboration in the analysis. Conclusion: We recommend applying and studying these and similar combinations of analysis approaches further.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 14th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {ACM},
	author = {Tøndel, Inger Anne and Cruzes, Daniela Soares and Jaatun, Martin Gilje},
	month = oct,
	year = {2020},
	pages = {1--6},
}

@inproceedings{noauthor_grounded_2015,
	address = {Florence, Italy},
	title = {Grounded {Theory} in {Software} {Engineering} {Research}: {A} {Critical} {Review} and {Guidelines}},
	isbn = {978-1-4799-1934-5},
	abstract = {Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.},
	language = {en},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	year = {2015},
}

@article{corbin1990grounded,
	title = {Grounded theory research: {Procedures}, canons, and evaluative criteria},
	volume = {13},
	number = {1},
	journal = {Qualitative sociology},
	author = {Corbin, Juliet M and Strauss, Anselm},
	year = {1990},
	note = {Number: 1
Publisher: Springer},
	pages = {3--21},
}

@misc{orchilles_cyber_2022,
	title = {Cyber {Kill} {Chain}, {MITRE} {ATT}\&{CK}, and {Purple} {Team} {\textbar} {SANS} {Institute}},
	url = {https://www.sans.org/blog/cyber-kill-chain-mitre-attack-purple-team/},
	urldate = {2022-06-08},
	author = {Orchilles, Jorge},
	month = mar,
	year = {2022},
}

@misc{cycraft_technologycorp_cycraft_2022,
	title = {{CyCraft} {Classroom}: {MITRE} {ATT}\&{CK} vs. {Cyber} {Kill} {Chain} vs. {Diamond} {Model}},
	shorttitle = {{CyCraft} {Classroom}},
	url = {https://medium.com/cycraft/cycraft-classroom-mitre-att-ck-vs-cyber-kill-chain-vs-diamond-model-1cc8fa49a20f},
	abstract = {In cybersecurity, there have been several approaches used to track and analyze the various characteristics of cyber intrusions by advanced…},
	language = {en},
	urldate = {2022-06-08},
	journal = {CyCraft},
	author = {{CyCraft TechnologyCorp}},
	month = feb,
	year = {2022},
}

@misc{noauthor_cyber_2022,
	title = {Cyber {Threat} {Hunting} {Solutions} {\textbar} {IBM}},
	url = {https://www.ibm.com/qradar/threat-hunting},
	abstract = {Incorporate IBM Security cyber threat hunting solutions into your security strategy to counter and mitigate threats more quickly.},
	language = {en-us},
	urldate = {2022-06-02},
	month = jun,
	year = {2022},
}

@misc{noauthor_threat_2022,
	title = {Threat {Hunting} {Tools}},
	url = {https://www.cyrebro.io/threat-hunting/},
	abstract = {Deal with Potential Challenges Via CYREBRO's Threat Hunting Tools to detect, block and protect from various cyber threats.},
	language = {en-US},
	urldate = {2022-06-02},
	journal = {CYREBRO},
	month = jun,
	year = {2022},
}

@article{curtis_software_1986,
	title = {Software psychology: {The} need for an interdisciplinary program},
	volume = {74},
	issn = {0018-9219},
	shorttitle = {Software psychology},
	url = {http://ieeexplore.ieee.org/document/1457864/},
	doi = {10.1109/PROC.1986.13596},
	language = {en},
	number = {8},
	urldate = {2022-05-06},
	journal = {Proceedings of the IEEE},
	author = {Curtis, B. and Soloway, E.M. and Brooks, R.E. and Black, J.B. and Ehrlich, K. and Ramsey, H.R.},
	year = {1986},
	note = {Number: 8},
	pages = {1092--1106},
}

@article{inayat_systematic_2015,
	title = {A systematic literature review on agile requirements engineering practices and challenges},
	volume = {51},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S074756321400569X},
	doi = {10.1016/j.chb.2014.10.046},
	abstract = {Unlike traditional software development methods, agile methods are marked by extensive collaboration, i.e. face-to-face communication. Although claimed to be beneﬁcial, the software development community as a whole is still unfamiliar with the role of the requirements engineering practices in agile methods. The term ‘‘agile requirements engineering’’ is used to deﬁne the ‘‘agile way’’ of planning, executing and reasoning about requirements engineering activities. Moreover, not much is known about the challenges posed by collaboration-oriented agile way of dealing with requirements engineering activities. Our goal is to map the evidence available about requirements engineering practices adopted and challenges faced by agile teams in order to understand how traditional requirements engineering issues are resolved using agile requirements engineering. We conducted a systematic review of literature published between 2002 and June 2013 and identiﬁed 21 papers, that discuss agile requirements engineering. We formulated and applied speciﬁc inclusion and exclusion criteria in two distinct rounds to determine the most relevant studies for our research goal. The review identiﬁed 17 practices of agile requirements engineering, ﬁve challenges traceable to traditional requirements engineering that were overcome by agile requirements engineering, and eight challenges posed by the practice of agile requirements engineering. However, our ﬁndings suggest that agile requirements engineering as a research context needs additional attention and more empirical results are required to better understand the impact of agile requirements engineering practices e.g. dealing with non-functional requirements and self-organising teams.},
	language = {en},
	urldate = {2022-05-06},
	journal = {Computers in Human Behavior},
	author = {Inayat, Irum and Salim, Siti Salwah and Marczak, Sabrina and Daneva, Maya and Shamshirband, Shahaboddin},
	month = oct,
	year = {2015},
	pages = {915--929},
}

@article{ambreen_empirical_2018,
	title = {Empirical research in requirements engineering: trends and opportunities},
	volume = {23},
	issn = {0947-3602, 1432-010X},
	shorttitle = {Empirical research in requirements engineering},
	url = {http://link.springer.com/10.1007/s00766-016-0258-2},
	doi = {10.1007/s00766-016-0258-2},
	language = {en},
	number = {1},
	urldate = {2022-05-06},
	journal = {Requirements Engineering},
	author = {Ambreen, Talat and Ikram, Naveed and Usman, Muhammad and Niazi, Mahmood},
	month = mar,
	year = {2018},
	note = {Number: 1},
	pages = {63--95},
}

@article{dermeval_applications_2016,
	title = {Applications of ontologies in requirements engineering: a systematic review of the literature},
	volume = {21},
	issn = {0947-3602, 1432-010X},
	shorttitle = {Applications of ontologies in requirements engineering},
	url = {http://link.springer.com/10.1007/s00766-015-0222-6},
	doi = {10.1007/s00766-015-0222-6},
	language = {en},
	number = {4},
	urldate = {2022-05-06},
	journal = {Requirements Engineering},
	author = {Dermeval, Diego and Vilela, Jéssyka and Bittencourt, Ig Ibert and Castro, Jaelson and Isotani, Seiji and Brito, Patrick and Silva, Alan},
	month = nov,
	year = {2016},
	note = {Number: 4},
	pages = {405--437},
}

@article{murphy_primacy_2006,
	title = {Primacy and {Recency} {Effects} on {Clicking} {Behavior}},
	volume = {11},
	issn = {1083-6101, 1083-6101},
	url = {https://academic.oup.com/jcmc/article/11/2/522-535/4617731},
	doi = {10.1111/j.1083-6101.2006.00025.x},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Journal of Computer-Mediated Communication},
	author = {Murphy, Jamie and Hofacker, Charles and Mizerski, Richard},
	month = jan,
	year = {2006},
	note = {Number: 2},
	pages = {522--535},
}

@article{bjork_recency-sensitive_1974,
	title = {Recency-sensitive retrieval processes in long-term free recall},
	volume = {6},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0010028574900097},
	doi = {10.1016/0010-0285(74)90009-7},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Cognitive Psychology},
	author = {Bjork, Robert A. and Whitten, William B.},
	month = apr,
	year = {1974},
	note = {Number: 2},
	pages = {173--189},
}

@article{murdock_serial_1962,
	title = {The serial position effect of free recall.},
	volume = {64},
	issn = {0022-1015},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0045106},
	doi = {10.1037/h0045106},
	language = {en},
	number = {5},
	urldate = {2022-04-30},
	journal = {Journal of Experimental Psychology},
	author = {Murdock, Bennet B.},
	month = nov,
	year = {1962},
	note = {Number: 5},
	pages = {482--488},
}

@article{bjork_recency-sensitive_1974-1,
	title = {Recency-sensitive retrieval processes in long-term free recall},
	volume = {6},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0010028574900097},
	doi = {10.1016/0010-0285(74)90009-7},
	language = {en},
	number = {2},
	urldate = {2022-04-30},
	journal = {Cognitive Psychology},
	author = {Bjork, Robert A. and Whitten, William B.},
	month = apr,
	year = {1974},
	note = {Number: 2},
	pages = {173--189},
}

@misc{noauthor_cyber_2022-1,
	title = {Cyber {Security} {Analyst} {Demographics} and {Statistics} [2022]: {Number} {Of} {Cyber} {Security} {Analysts} {In} {The} {US}},
	url = {https://www.zippia.com/cyber-security-analyst-jobs/demographics/},
	urldate = {2022-04-12},
	month = apr,
	year = {2022},
}

@techreport{cichonski_computer_2012,
	title = {Computer {Security} {Incident} {Handling} {Guide} : {Recommendations} of the {National} {Institute} of {Standards} and {Technology}},
	shorttitle = {Computer {Security} {Incident} {Handling} {Guide}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf},
	abstract = {Computer security incident response has become an important component of information technology (IT) programs. Because performing incident response effectively is a complex undertaking, establishing a successful incident response capability requires substantial planning and resources. This publication assists organizations in establishing computer security incident response capabilities and handling incidents efficiently and effectively. This publication provides guidelines for incident handling, particularly for analyzing incident-related data and determining the appropriate response to each incident. The guidelines can be followed independently of particular hardware platforms, operating systems, protocols, or applications.},
	language = {en},
	number = {NIST SP 800-61r2},
	urldate = {2022-04-12},
	institution = {National Institute of Standards and Technology},
	author = {Cichonski, Paul and Millar, Tom and Grance, Tim and Scarfone, Karen},
	month = aug,
	year = {2012},
	doi = {10.6028/NIST.SP.800-61r2},
	note = {Issue: NIST SP 800-61r2},
	pages = {NIST SP 800--61r2},
}

@article{pennington_getting_nodate,
	title = {Getting {Started} with {ATT}\&{CK}},
	language = {en},
	author = {Pennington, Adam},
	pages = {45},
}

@article{noauthor_business_2015,
	title = {Business {Blackout}: {The} insurance implications of a cyber attack on the {US} power grid},
	language = {en},
	year = {2015},
	pages = {68},
}

@book{noauthor_software_2014,
	title = {Software {Engineering} {Body} of {Knowledge}},
	isbn = {0-7695-5166-1},
	publisher = {IEEE},
	year = {2014},
}

@misc{noauthor_requirements_2022,
	type = {Journal},
	title = {Requirements {Engineering}},
	url = {https://www.springer.com/journal/766},
	abstract = {The journal provides a focus for the dissemination of new results about the elicitation, representation and validation of requirements of software intensive ...},
	language = {en},
	urldate = {2022-03-27},
	journal = {Springer},
	month = mar,
	year = {2022},
}

@techreport{noauthor_strategies_2011,
	address = {1788 Wilmington Pike Glen Mills, PA 19342 USA Phone: +1-484-450-0100},
	title = {Strategies for {Project} {Recovery}},
	url = {www.pmsolutions.com},
	urldate = {2022-03-26},
	institution = {PM Solutions},
	year = {2011},
}

@article{brink_security_nodate,
	title = {{SECURITY} {AWARENESS} {TRAINING}: {SMALL} {INVESTMENT}, {LARGE} {REDUCTION} {IN} {RISK}},
	language = {en},
	author = {Brink, Derek E},
	pages = {14},
}

@article{brink_quantifying_nodate,
	title = {Quantifying the {Value} of {Time} in {Cyber}-{Threat} {Detection} and {Response}},
	language = {en},
	author = {Brink, Derek E},
	pages = {9},
}

@phdthesis{emami2020informing,
	title = {Informing privacy and security decision making in an {IoT} world},
	school = {Carnegie Mellon University},
	author = {Emami-Naeini, Pardis},
	year = {2020},
}

@article{brink_quantifying_nodate-1,
	title = {Quantifying the {Value} of {Time} in {Cyber}-{Threat} {Detection} and {Response}},
	language = {en},
	author = {Brink, Derek E},
	pages = {9},
}

@article{dreyfus_socrates_nodate,
	title = {From {Socrates} to {Expert} {Systems}},
	language = {en},
	author = {Dreyfus, Hubert L and Dreyfus, Stuart E},
	pages = {17},
}

@article{nosco_industrial_2020,
	title = {The {Industrial} {Age} of {Hacking}},
	abstract = {There is a cognitive bias in the hacker community to select a piece of software and invest signiﬁcant human resources into ﬁnding bugs in that software without any prior indication of success. We label this strategy depth-ﬁrst search and propose an alternative: breadth-ﬁrst search. In breadthﬁrst search, humans perform minimal work to enable automated analysis on a range of targets before committing additional time and effort to research any particular one. We present a repeatable human study that leverages teams of varying skill while using automation to the greatest extent possible. Our goal is a process that is effective at ﬁnding bugs; has a clear plan for the growth, coaching, and efﬁcient use of team members; and supports measurable, incremental progress. We derive an assembly-line process that improves on what was once intricate, manual work. Our work provides evidence that the breadth-ﬁrst approach increases the effectiveness of teams.},
	language = {en},
	author = {Nosco, Tim and Ziegler, Jared and Clark, Zechariah},
	month = aug,
	year = {2020},
	pages = {19},
}

@article{trent_modelling_2019-1,
	title = {Modelling the {Cognitive} {Work} of {Cyber} {Protection} {Teams}},
	volume = {4},
	issn = {2474-2120},
	url = {http://www.jstor.org/stable/26623071},
	abstract = {Cyber Protection Teams (CPTs) defend our Nation’s critical military networks. While Cyber Security Service Providers are responsible for the continuous monitoring and vulnerability patching of networks, CPTs perform threat-oriented missions to defeat adversaries within and through cyberspace. The research we report here provides a descriptive workflow of cyber defense in CPTs as well as a prescriptive work model that all CPTs should be capable of executing. This paper describes how these models were developed and used to assess technologies and performance of CPTs. Such models offer a variety of benefits to practitioner and research communities, particularly when the domain of practice is closed to most researchers. This project demonstrates the need for continual curation of CPT work models as well as the need for models of work for the other types of cyber teams (i.e. Mission and Support) in the Cyber Mission Force.},
	number = {1},
	urldate = {2022-02-19},
	journal = {The Cyber Defense Review},
	author = {Trent, Stoney and Hoffman, Robert R. and Merritt, David and Smith, Sarah},
	year = {2019},
	note = {Number: 1
Publisher: Army Cyber Institute},
	pages = {125--136},
}

@article{symonds_innovating_2017,
	title = {Innovating the {Prioritization} of {Cyber} {Defense}},
	volume = {16},
	issn = {1445-3312},
	url = {http://www.jstor.org/stable/26502753},
	abstract = {The U.S. Department of Defense (DoD) faces a monumental undertaking in protecting the infrastructure that underpins the entirety of its operations: It must identify and prioritize key terrain to dynamically defend. This paper will examine the criteria to identify critical information systems and infrastructure, will review the process to identify key terrain in cyberspace, and will offer a recommendation on how to more effectively prioritize network defender operations using data analytics.},
	number = {2},
	urldate = {2022-02-19},
	journal = {Journal of Information Warfare},
	author = {Symonds, R},
	year = {2017},
	note = {Number: 2
Publisher: Peregrine Technical Solutions},
	pages = {12--18},
}

@misc{noauthor_threat_2022-1,
	title = {Threat {Hunting}},
	url = {https://www.boozallen.com/expertise/cybersecurity/threat-hunting.html},
	abstract = {Our cyber threat hunting services can find advanced persistent threats to reduce operational, financial risk.},
	language = {en},
	urldate = {2022-02-19},
	month = feb,
	year = {2022},
}

@article{noauthor_cost_nodate,
	title = {Cost of a {Data} {Breach} {Report} 2020},
	language = {en},
	pages = {82},
}

@techreport{noauthor_2019_2020,
	title = {2019 {TOP} {THREAT} {DETECTION} {TRENDS} {SURVEY}},
	urldate = {2022-02-18},
	institution = {Attivo Networks},
	year = {2020},
}

@misc{milea_hypothesis_2017,
	title = {Hypothesis in {Threat} {Hunting}},
	url = {https://medium.com/@demetriom/hypothesis-in-threat-hunting-4bea5446e34c},
	abstract = {Today’s threat landscape requires organizations to operate more proactively to keep up with advanced and persistent threats. There is no…},
	language = {en},
	urldate = {2022-02-18},
	journal = {Medium},
	author = {Milea, Demetrio},
	month = jul,
	year = {2017},
}

@phdthesis{bynum_cyber_2019,
	title = {{CYBER} {THREAT} {HUNTING}},
	school = {Utica College},
	author = {Bynum, Jon R},
	month = aug,
	year = {2019},
}

@article{gallagher_new_nodate,
	title = {New {Me}: {Understanding} {Expert} and {Non}-{Expert} {Perceptions} and {Usage} of the {Tor} {Anonymity} {Network}},
	abstract = {Proper use of an anonymity system requires adequate understanding of how it functions. Yet, there is surprisingly little research that looks into user understanding and usage of anonymity software. Improper use stemming from a lack of suﬃcient knowledge of the system has the potential to lead to deanonymization, which may hold severe personal consequences for the user. We report on the understanding and the use of the Tor anonymity system. Via semistructured interviews with 17 individuals (6 experts and 11 non-experts) we found that experts and non-experts view, understand, and use Tor in notably diﬀerent ways. Moreover, both groups exhibit behavior as well as gaps in understanding that could potentially compromise anonymity. Based on these ﬁndings, we provide several suggestions for improving the user experience of Tor to facilitate better user understanding of its operation, threat model, and limitations.},
	language = {en},
	author = {Gallagher, Kevin and Patil, Sameer and Memon, Nasir},
	pages = {15},
}

@inproceedings{mohanani_requirements_2014,
	address = {Hyderabad India},
	title = {Requirements fixation},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568235},
	doi = {10.1145/2568225.2568235},
	abstract = {There is a broad consensus that understanding system desiderata (requirements) and design creativity are both important for software engineering success. However, little research has addressed the relationship between design creativity and the way requirements are framed or presented. This paper therefore aims to investigate the possibility that the way desiderata are framed or presented can affect design creativity. Forty two participants took part in a randomized control trial where one group received desiderata framed as “requirements” while the other received desiderata framed as “ideas”. Participants produced design concepts which were judged for originality. Participants who received requirements framing produced significantly less original designs than participants who received ideas framing (MannWhitney U=116.5, p=0.004). We conclude that framing desiderata as “requirements” may cause requirements fixation where designers’ preoccupation with satisfying explicit requirements inhibits their creativity.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Mohanani, Rahul and Ralph, Paul and Shreeve, Ben},
	month = may,
	year = {2014},
	pages = {895--906},
}

@article{long_scalable_nodate,
	title = {Scalable {Methods} for {Conducting} {Cyber} {Threat} {Hunt} {Operations}},
	author = {Long, Michael C},
}

@article{lee_who_2016,
	title = {The {Who}, {What}, {Where}, {When}, {Why} and {How} of {Effective} {Threat} {Hunting}},
	language = {en},
	author = {Lee, Written Robert M and Lee, Rob},
	month = feb,
	year = {2016},
	pages = {14},
}

@article{lee_generating_nodate,
	title = {Generating {Hypotheses} for {Successful} {Threat} {Hunting}},
	language = {en},
	author = {Lee, Robert M and Bianco, David},
	pages = {12},
}

@misc{bianco_simple_nodate,
	type = {Blog},
	title = {A {Simple} {Hunting} {Maturity} {Model}},
	url = {http://detect-respond.blogspot.com/2015/10/a-simple-hunting-maturity-model.html},
	abstract = {UPDATE 2015-10-15:  I received some questions about the roles of automation in HMM0 and HMM4, which I addressed in a new section.  Also, I n...},
	language = {en},
	urldate = {2022-02-15},
	journal = {Enterprise Detection and Response},
	author = {Bianco, David},
	month = oct,
}

@article{ko_practical_2015,
	title = {A practical guide to controlled experiments of software engineering tools with human participants},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-013-9279-3},
	doi = {10.1007/s10664-013-9279-3},
	abstract = {Empirical studies, often in the form of controlled experiments, have been widely adopted in software engineering research as a way to evaluate the merits of new software engineering tools. However, controlled experiments involving human participants actually using new tools are still rare, and when they are conducted, some have serious validity concerns. Recent research has also shown that many software engineering researchers view this form of tool evaluation as too risky and too difficult to conduct, as they might ultimately lead to inconclusive or negative results. In this paper, we aim both to help researchers minimize the risks of this form of tool evaluation, and to increase their quality, by offering practical methodological guidance on designing and running controlled experiments with developers. Our guidance fills gaps in the empirical literature by explaining, from a practical perspective, options in the recruitment and selection of human participants, informed consent, experimental procedures, demographic measurements, group assignment, training, the selecting and design of tasks, the measurement of common outcome variables such as success and time on task, and study debriefing. Throughout, we situate this guidance in the results of a new systematic review of the tool evaluations that were published in over 1,700 software engineering papers published from 2001 to 2011.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Empirical Software Engineering},
	author = {Ko, Andrew J. and LaToza, Thomas D. and Burnett, Margaret M.},
	month = feb,
	year = {2015},
	note = {Number: 1},
	pages = {110--141},
}

@article{lee_sans_2018,
	title = {{SANS} 2018 {Threat} {Hunting} {Survey} {Results}},
	language = {en},
	author = {Lee, Robert M and Lee, Rob T},
	year = {2018},
	pages = {20},
}

@article{lee_hunter_2017,
	title = {The {Hunter} {Strikes} {Back}: {The} {SANS} 2017 {Threat} {Hunting} {Survey}},
	language = {en},
	author = {Lee, Written Rob and Lee, Robert M},
	year = {2017},
	pages = {26},
}

@article{morse_analytic_2015,
	title = {Analytic {Strategies} and {Sample} {Size}},
	volume = {25},
	issn = {1049-7323},
	url = {https://doi.org/10.1177/1049732315602867},
	doi = {10.1177/1049732315602867},
	number = {10},
	urldate = {2022-02-09},
	journal = {Qualitative Health Research},
	author = {Morse, Janice M.},
	month = oct,
	year = {2015},
	note = {Number: 10
Publisher: SAGE Publications Inc},
	pages = {1317--1318},
}

@article{guest_how_2006,
	title = {How {Many} {Interviews} {Are} {Enough}?: {An} {Experiment} with {Data} {Saturation} and {Variability}},
	volume = {18},
	issn = {1525-822X, 1552-3969},
	shorttitle = {How {Many} {Interviews} {Are} {Enough}?},
	url = {http://journals.sagepub.com/doi/10.1177/1525822X05279903},
	doi = {10.1177/1525822X05279903},
	abstract = {Guidelines for determining nonprobabilistic sample sizes are virtually nonexistent. Purposive samples are the most commonly used form of nonprobabilistic sampling, and their size typically relies on the concept of “saturation,” or the point at which no new information or themes are observed in the data. Although the idea of saturation is helpful at the conceptual level, it provides little practical guidance for estimating sample sizes, prior to data collection, necessary for conducting quality research. Using data from a study involving sixty in-depth interviews with women in two West African countries, the authors systematically document the degree of data saturation and variability over the course of thematic analysis. They operationalize saturation and make evidence-based recommendations regarding nonprobabilistic sample sizes for interviews. Based on the data set, they found that saturation occurred within the first twelve interviews, although basic elements for metathemes were present as early as six interviews. Variability within the data followed similar patterns.},
	language = {en},
	number = {1},
	urldate = {2022-02-09},
	journal = {Field Methods},
	author = {Guest, Greg and Bunce, Arwen and Johnson, Laura},
	month = feb,
	year = {2006},
	note = {Number: 1},
	pages = {59--82},
}

@article{morse_determining_2000,
	title = {Determining {Sample} {Size}},
	volume = {10},
	issn = {1049-7323},
	url = {https://doi.org/10.1177/104973200129118183},
	doi = {10.1177/104973200129118183},
	number = {1},
	urldate = {2022-02-09},
	journal = {Qualitative Health Research},
	author = {Morse, Janice M.},
	month = jan,
	year = {2000},
	note = {Number: 1
Publisher: SAGE Publications Inc},
	pages = {3--5},
}

@incollection{anderson_systems_nodate,
	address = {Cambridge, Massachusetts},
	title = {Systems thinking basics},
	booktitle = {Systems {Thinking} {Basics}},
	publisher = {Pegasus Communications, Inc.},
	author = {Anderson, Virginia and Johnson, Lauren},
}

@article{aronson_overview_nodate,
	title = {Overview of {Systems} {Thinking}},
	language = {en},
	author = {Aronson, Daniel},
	pages = {3},
}

@article{kim_introduction_nodate,
	title = {Introduction to {Systems} {Thinking}},
	language = {en},
	author = {Kim, Daniel H},
	pages = {21},
}

@article{arnold_definition_2015,
	title = {A {Definition} of {Systems} {Thinking}: {A} {Systems} {Approach}},
	volume = {44},
	issn = {18770509},
	shorttitle = {A {Definition} of {Systems} {Thinking}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915002860},
	doi = {10.1016/j.procs.2015.03.050},
	abstract = {This paper proposes a definition of systems thinking for use in a wide variety of disciplines, with particular emphasis on the development and assessment of systems thinking educational efforts. The definition was derived from a review of the systems thinking literature combined with the application of systems thinking to itself. Many different definitions of systems thinking can be found throughout the systems community, but key components of a singular definition can be distilled from the literature. This researcher considered these components both individually and holistically, then proposed a new definition of systems thinking that integrates these components as a system. The definition was tested for fidelity against a System Test and against three widely accepted system archetypes. Systems thinking is widely believed to be critical in handling the complexity facing the world in the coming decades; however, it still resides in the educational margins. In order for this important skill to receive mainstream educational attention, a complete definition is required. Such a definition has not yet been established. This research is an attempt to rectify this deficiency by providing such a definition.},
	language = {en},
	urldate = {2022-02-05},
	journal = {Procedia Computer Science},
	author = {Arnold, Ross D. and Wade, Jon P.},
	year = {2015},
	pages = {669--678},
}

@misc{noauthor_empirical_2022,
	title = {Empirical {Standards}},
	copyright = {CC0-1.0},
	url = {https://github.com/acmsigsoft/EmpiricalStandards},
	abstract = {Tools and standards for conducting and evaluating research in software engineering},
	urldate = {2022-02-05},
	publisher = {ACM Special Interest Group on Software Engineering},
	month = feb,
	year = {2022},
	note = {original-date: 2020-09-16T15:54:52Z},
	keywords = {empirical-standards, research, sigsoft, softwareengineering, standards},
}

@inproceedings{veras_errors_2010,
	address = {San Jose, CA, USA},
	title = {Errors on {Space} {Software} {Requirements}: {A} {Field} {Study} and {Application} {Scenarios}},
	isbn = {978-1-4244-9056-1},
	shorttitle = {Errors on {Space} {Software} {Requirements}},
	url = {http://ieeexplore.ieee.org/document/5635117/},
	doi = {10.1109/ISSRE.2010.30},
	abstract = {This paper presents a field study on real errors found in space software requirements documents. The goal is to understand and characterize the most frequent types of requirement problems in this critical application domain. To classify the software requirement errors analyzed we initially used a well-known existing taxonomy that was later extended in order to allow a more thorough analysis. The results of the study show a high rate of requirement errors (9.5 errors per each 100 requirements), which is surprising if we consider that the focus of the work is critical embedded software. Besides the characterization of the most frequent types of errors, the paper also proposes a set of operators that define how to inject realistic errors in requirement documents. This may be used in several scenarios, including: evaluating and training reviewers, estimating the number of requirement errors in real specifications, defining checklists for quick requirement verification, and defining benchmarks for requirements specifications.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {2010 {IEEE} 21st {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE},
	author = {Veras, Paulo C. and Villani, Emilia and Ambrosio, Ana Maria and Silva, Nuno and Vieira, Marco and Madeira, Henrique},
	month = nov,
	year = {2010},
	pages = {61--70},
}

@article{darlington_current_2002,
	title = {Current research in the engineering design requirement},
	volume = {216},
	issn = {0954-4054, 2041-2975},
	url = {http://journals.sagepub.com/doi/10.1243/0954405021520049},
	doi = {10.1243/0954405021520049},
	abstract = {The design requirement is a description of the desired solution to a problem. In engineering design, as in all other, a clear expression of a well-formulated design goal is vital for successful and e cient completion of the design task. The nature of the design requirement and the processes by which it is achieved have been the subject of a wide variety of research. The purpose of the paper is twofold. Firstly, it sets out to collate and discuss representative research in this area in order to give an overview of the current scope of the work. Secondly, it seeks to draw a comparison with the task of developing the design requirement for software and information systems and to initiate a discussion that considers to what extent the substantial body of research in software requirements engineering might help to give an understanding of the design requirement for the engineering design domain. A tentative characterization of the diVerences between the tasks in the two domains is presented, and representative papers from requirements engineering are used to suggest areas of overlap as a starting point for further investigation.},
	language = {en},
	number = {3},
	urldate = {2022-02-04},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
	author = {Darlington, M J and Culley, S J},
	month = mar,
	year = {2002},
	note = {Number: 3},
	pages = {375--388},
}

@incollection{easterbrook_selecting_2008,
	address = {London},
	title = {Selecting {Empirical} {Methods} for {Software} {Engineering} {Research}},
	isbn = {978-1-84800-043-8 978-1-84800-044-5},
	url = {http://link.springer.com/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Guide to {Advanced} {Empirical} {Software} {Engineering}},
	publisher = {Springer London},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5_11},
	pages = {285--311},
}

@incollection{ralph_comparing_2010,
	address = {Berlin, Heidelberg},
	title = {Comparing {Two} {Software} {Design} {Process} {Theories}},
	volume = {6105},
	isbn = {978-3-642-13334-3 978-3-642-13335-0},
	url = {http://link.springer.com/10.1007/978-3-642-13335-0_10},
	abstract = {This paper explores an ongoing conflict concerning the nature of software design. This conflict manifests itself as antagonism between managers and developers, debates about agile vs. plan-driven methodologies and aspiring developers’ dissatisfaction with their courses. One side views design as a plandriven information processing task involving rational decision-making (the Reason-Centric Perspective), while the other views design as an improvised, creative task involving naturalized decision-making (Action-Centric Perspective). Each perspective includes an epistemology, theory of human action and a software design process theory (an explanation of how software is created in practice). This paper reports the results of an exploratory questionnaire study that comparatively and empirically evaluated the two process theories. Results clearly favor the Action-Centric process theory: the Sensemaking-CoevolutionImplementation Framework.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Global {Perspectives} on {Design} {Science} {Research}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ralph, Paul},
	editor = {Winter, Robert and Zhao, J. Leon and Aier, Stephan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2010},
	doi = {10.1007/978-3-642-13335-0_10},
	note = {Series Editors: \_:n2166
Series Title: Lecture Notes in Computer Science},
	pages = {139--153},
}

@inproceedings{lutz_analyzing_1992,
	address = {San Diego, CA, USA},
	title = {Analyzing software requirements errors in safety-critical, embedded systems},
	isbn = {978-0-8186-3120-7},
	url = {http://ieeexplore.ieee.org/document/324825/},
	doi = {10.1109/ISRE.1993.324825},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {[1993] {Proceedings} of the {IEEE} {International} {Symposium} on {Requirements} {Engineering}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Lutz, R.R.},
	year = {1992},
	pages = {126--133},
}

@article{iqbal_requirements_2020,
	title = {Requirements engineering issues causing software development outsourcing failure},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0229785},
	doi = {10.1371/journal.pone.0229785},
	abstract = {Software development outsourcing is becoming more and more famous because of the advantages like cost abatement, process enhancement, and coping with the scarcity of needed resources. Studies confirm that unfortunately a large proportion of the software development outsourcing projects fails to realize anticipated benefits. Investigations into the failures of such projects divulge that in several cases software development outsourcing projects are failed because of the issues that are associated with requirements engineering process. The objective of this study is the identification and the ranking of the commonly occurring issues of the requirements engineering process in the case of software development outsourcing. For this purpose, contemporary literature has been assessed rigorously, issues faced by practitioners have been identified and three questionnaire surveys have been organized by involving experienced software development outsourcing practitioners. The Delphi technique, cut-off value method and 50\% rule have also been employed. The study explores 150 issues (129 issues from literature and 21 from industry) of requirements engineering process for software development outsourcing, groups the 150 issues into 7 identified categories and then extricates 43 customarily or commonly arising issues from the 150 issues. Founded on ‘frequency of occurrence’ the 43 customarily arising issues have been ranked with respect to respective categories (category-wise ranking) and with respect to all the categories (overall ranking). Categories of the customarily arising issues have also been ranked. The issues’ identification and ranking contribute to design proactive software project management plan for dealing with software development outsourcing failures and attaining conjectured benefits of the software development outsourcing.},
	language = {en},
	number = {4},
	urldate = {2022-02-04},
	journal = {PLOS ONE},
	author = {Iqbal, Javed and Ahmad, Rodina B. and Khan, Muzafar and {Fazal-e-Amin} and Alyahya, Sultan and Nizam Nasir, Mohd Hairul and Akhunzada, Adnan and Shoaib, Muhammad},
	editor = {Xin, Baogui},
	month = apr,
	year = {2020},
	note = {Number: 4},
	pages = {e0229785},
}

@inproceedings{sudin_role_2010,
	title = {The {Role} of a {Specification} in the {Design} {Process}: {A} {Case} {Study}},
	language = {en},
	author = {Sudin, M N and Ahmed-Kristensen, S and Andreasen, M M},
	month = may,
	year = {2010},
	pages = {10},
}

@article{kiritani_success_2015,
	title = {The {Success} or {Failure} of the {Requirements} {Definition} and {Study} of the {Causation} of the {Quantity} of {Trust} {Existence} {Between} {Stakeholders}},
	volume = {64},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050915026113},
	doi = {10.1016/j.procs.2015.08.476},
	abstract = {Requirements definition that is an important work process for a project and may determine the success or failure of system development project tends to draw an ambiguous conclusion, which will lead directly to the failure of such a system construction. To optimize the requirements definition process we present a model that the trust management process is integrated into the requirements definition process, as measures to minimize the gap between requirements caused due to a lack or discrepancy in communication that is produced easily in requirements definition and to use a negotiation method for solving problems in the following processes provided by the gap between requirements including tacit ones. We discuss a matter that building trust between the stakeholders in the requirements definition process is effective to optimize the requirements definition, which has been produced by the special characteristics of Japanese firms in the information system development, and we also describe the necessity and effectiveness of information system in Japan.},
	language = {en},
	urldate = {2022-02-04},
	journal = {Procedia Computer Science},
	author = {Kiritani, Keisuke and Ohashi, Masakazu},
	year = {2015},
	pages = {153--160},
}

@misc{noauthor_5_2012,
	title = {5 {Reasons} {Software} {Projects} {Fail}: {The} {Role} of {Requirements}},
	shorttitle = {5 {Reasons} {Software} {Projects} {Fail}},
	url = {https://argondigital.com/blog/product-management/5-reasons-software-projects-fail-hint-its-often-due-to-incomplete-incorrect-requirements/},
	abstract = {Business analysts, in requirements gathering and accurate, complete requirements definition, can significantly impact whether software projects succeed or fail.},
	language = {en-US},
	urldate = {2022-02-04},
	journal = {ArgonDigital {\textbar} Making Technology a Strategic Advantage},
	month = mar,
	year = {2012},
}

@article{ralph_sensemaking-coevolution-implementation_2015,
	title = {The {Sensemaking}-{Coevolution}-{Implementation} {Theory} of software design},
	volume = {101},
	issn = {01676423},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642314005395},
	doi = {10.1016/j.scico.2014.11.007},
	language = {en},
	urldate = {2022-02-04},
	journal = {Science of Computer Programming},
	author = {Ralph, Paul},
	month = apr,
	year = {2015},
	pages = {21--41},
}

@article{vessey_expertise_1985,
	title = {Expertise in debugging computer programs: {A} process analysis},
	volume = {23},
	issn = {00207373},
	shorttitle = {Expertise in debugging computer programs},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020737385800547},
	doi = {10.1016/S0020-7373(85)80054-7},
	language = {en},
	number = {5},
	urldate = {2022-01-29},
	journal = {International Journal of Man-Machine Studies},
	author = {Vessey, Iris},
	month = nov,
	year = {1985},
	note = {Number: 5},
	pages = {459--494},
}

@article{spohrer_novice_1986,
	title = {Novice mistakes: are the folk wisdoms correct?},
	volume = {29},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Novice mistakes},
	url = {https://dl.acm.org/doi/10.1145/6138.6145},
	doi = {10.1145/6138.6145},
	abstract = {An evaluation of two folk wisdoms serves to elucidate the underlying or "deep-structure" reasons for novice errors.},
	language = {en},
	number = {7},
	urldate = {2022-01-29},
	journal = {Communications of the ACM},
	author = {Spohrer, James C. and Soloway, Elliot},
	month = jul,
	year = {1986},
	note = {Number: 7},
	pages = {624--632},
}

@article{litzinger_engineering_2011,
	title = {Engineering {Education} and the {Development} of {Expertise}},
	volume = {100},
	issn = {10694730},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2011.tb00006.x},
	doi = {10.1002/j.2168-9830.2011.tb00006.x},
	abstract = {BACKGROUND Although engineering education has evolved in ways that improve the readiness of graduates to meet the challenges of the twenty-first century, national and international organizations continue to call for change. Future changes in engineering education should be guided by research on expertise and the learning processes that support its development.
PURPOSE The goals of this paper are: to relate key findings from studies of the development of expertise to engineering education, to summarize instructional practices that are consistent with these findings, to provide examples of learning experiences that are consistent with these instructional practices, and finally, to identify challenges to implementing such learning experiences in engineering programs. SCOPE/METHOD The research synthesized for this article includes that on the development of expertise, students’ approaches to learning, students’ responses to instructional practices, and the role of motivation in learning. In addition, literature on the dominant teaching and learning practices in engineering education is used to frame some of the challenges to implementing alternative approaches to learning.
CONCLUSION Current understanding of expertise, and the learning processes that develop it, indicates that engineering education should encompass a set of learning experiences that allow students to construct deep conceptual knowledge, to develop the ability to apply key technical and professional skills fluently, and to engage in a number of authentic engineering projects. Engineering curricula and teaching methods are often not well aligned with these goals. Curriculum-level instructional design processes should be used to design and implement changes that will improve alignment.},
	language = {en},
	number = {1},
	urldate = {2022-01-29},
	journal = {Journal of Engineering Education},
	author = {Litzinger, Thomas and Lattuca, Lisa R. and Hadgraft, Roger and Newstetter, Wendy},
	month = jan,
	year = {2011},
	note = {Number: 1},
	pages = {123--150},
}

@book{noauthor_how_2000,
	address = {Washington, D.C.},
	title = {How {People} {Learn}: {Brain}, {Mind}, {Experience}, and {School}: {Expanded} {Edition}},
	isbn = {978-0-309-07036-2},
	shorttitle = {How {People} {Learn}},
	url = {http://www.nap.edu/catalog/9853},
	language = {en},
	urldate = {2022-01-29},
	publisher = {National Academies Press},
	month = aug,
	year = {2000},
	doi = {10.17226/9853},
	note = {Pages: 9853},
}

@book{michelfelder_philosophy_2013,
	address = {Dordrecht},
	series = {Philosophy of {Engineering} and {Technology}},
	title = {Philosophy and {Engineering}: {Reflections} on {Practice}, {Principles} and {Process}},
	volume = {15},
	isbn = {978-94-007-7761-3 978-94-007-7762-0},
	shorttitle = {Philosophy and {Engineering}},
	url = {http://link.springer.com/10.1007/978-94-007-7762-0},
	language = {en},
	urldate = {2022-01-25},
	publisher = {Springer Netherlands},
	editor = {Michelfelder, Diane P and McCarthy, Natasha and Goldberg, David E.},
	year = {2013},
	doi = {10.1007/978-94-007-7762-0},
}

@incollection{ralph_proposal_2009,
	address = {Berlin, Heidelberg},
	title = {A {Proposal} for a {Formal} {Definition} of the {Design} {Concept}},
	volume = {14},
	isbn = {978-3-540-92965-9 978-3-540-92966-6},
	url = {http://link.springer.com/10.1007/978-3-540-92966-6_6},
	abstract = {A clear and unambiguous definition of the design concept would be useful for developing a cumulative tradition for research on design. In this article we suggest a formal definition for the concept design and propose a conceptual model linking concepts related to design projects. The definition of design incorporates seven elements: agent, object, environment, goals, primitives, requirements and constraints. The design project conceptual model is based on the view that projects are temporal trajectories of work systems that include human agents who work to design systems for stakeholders, and use resources and tools to accomplish this task. We demonstrate how these two suggestions can be useful by showing that 1) the definition of design can be used to classify design knowledge and 2) the conceptual model can be used to classify design approaches.},
	language = {en},
	urldate = {2022-01-24},
	booktitle = {Design {Requirements} {Engineering}: {A} {Ten}-{Year} {Perspective}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ralph, Paul and Wand, Yair},
	editor = {Lyytinen, Kalle and Loucopoulos, Pericles and Mylopoulos, John and Robinson, Bill and van der Aalst, Will and Mylopoulos, John and Sadeh, Norman M. and Shaw, Michael J. and Szyperski, Clemens},
	year = {2009},
	doi = {10.1007/978-3-540-92966-6_6},
	note = {Series Editors: \_:n2089
Series Title: Lecture Notes in Business Information Processing},
	pages = {103--136},
}

@article{milajerdi_poirot_2019,
	title = {{POIROT}: {Aligning} {Attack} {Behavior} with {Kernel} {Audit} {Records} for {Cyber} {Threat} {Hunting}},
	shorttitle = {{POIROT}},
	url = {http://arxiv.org/abs/1910.00056},
	doi = {10.1145/3319535.3363217},
	abstract = {Cyber threat intelligence (CTI) is being used to search for indicators of attacks that might have compromised an enterprise network for a long time without being discovered. To have a more effective analysis, CTI open standards have incorporated descriptive relationships showing how the indicators or observables are related to each other. However, these relationships are either completely overlooked in information gathering or not used for threat hunting. In this paper, we propose a system, called POIROT, which uses these correlations to uncover the steps of a successful attack campaign. We use kernel audits as a reliable source that covers all causal relations and information flows among system entities and model threat hunting as an inexact graph pattern matching problem. Our technical approach is based on a novel similarity metric which assesses an alignment between a query graph constructed out of CTI correlations and a provenance graph constructed out of kernel audit log records. We evaluate POIROT on publicly released real-world incident reports as well as reports of an adversarial engagement designed by DARPA, including ten distinct attack campaigns against different OS platforms such as Linux, FreeBSD, and Windows. Our evaluation results show that POIROT is capable of searching inside graphs containing millions of nodes and pinpoint the attacks in a few minutes, and the results serve to illustrate that CTI correlations could be used as robust and reliable artifacts for threat hunting.},
	urldate = {2022-01-23},
	journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Milajerdi, Sadegh M. and Eshete, Birhanu and Gjomemo, Rigel and Venkatakrishnan, V. N.},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.00056},
	keywords = {Computer Science - Cryptography and Security},
	pages = {1795--1812},
}

@article{rodeghero_please_2021,
	title = {Please {Turn} {Your} {Cameras} {On}: {Remote} {Onboarding} of {Software} {Developers} during a {Pandemic}},
	shorttitle = {Please {Turn} {Your} {Cameras} {On}},
	url = {http://arxiv.org/abs/2011.08130},
	abstract = {The COVID-19 pandemic has impacted the way that software development teams onboard new hires. Previously, most software developers worked in physical offices and new hires onboarded to their teams in the physical office, following a standard onboarding process. However, when companies transitioned employees to work from home due to the pandemic, there was little to no time to develop new onboarding procedures. In this paper, we present a survey of 267 new hires at Microsoft that onboarded to software development teams during the pandemic. We explored their remote onboarding process, including the challenges that the new hires encountered and their social connectedness with their teams. We found that most developers onboarded remotely and never had an opportunity to meet their teammates in person. This leads to one of the biggest challenges faced by these new hires, building a strong social connection with their team. We use these results to provide recommendations for onboarding remote hires.},
	urldate = {2022-01-20},
	journal = {arXiv:2011.08130 [cs]},
	author = {Rodeghero, Paige and Zimmermann, Thomas and Houck, Brian and Ford, Denae},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.08130},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
}

@inproceedings{ju_case_2021,
	title = {A {Case} {Study} of {Onboarding} in {Software} {Teams}: {Tasks} and {Strategies}},
	shorttitle = {A {Case} {Study} of {Onboarding} in {Software} {Teams}},
	doi = {10.1109/ICSE43902.2021.00063},
	abstract = {Developers frequently move into new teams or environments across software companies. Their onboarding experience is correlated with productivity, job satisfaction, and other short-term and long-term outcomes. The majority of the onboarding process comprises engineering tasks such as fixing bugs or implementing small features. Nevertheless, we do not have a systematic view of how tasks influence onboarding. In this paper, we present a case study of Microsoft, where we interviewed 32 developers moving into a new team and 15 engineering managers onboarding a new developer into their team - to understand and characterize developers' onboarding experience and expectations in relation to the tasks performed by them while onboarding. We present how tasks interact with new developers through three representative themes: learning, confidence building, and socialization. We also discuss three onboarding strategies as inferred from the interviews that managers commonly use unknowingly, and discuss their pros and cons and offer situational recommendations. Furthermore, we triangulate our interview findings with a developer survey (N = 189) and a manager survey (N = 37) and find that survey results suggest that our findings are representative and our recommendations are actionable. Practitioners could use our findings to improve their onboarding processes, while researchers could find new research directions from this study to advance the understanding of developer onboarding. Our research instruments and anonymous data are available at https://zenodo.org/record/4455937\#.YCOQCs 0lFd.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ju, An and Sajnani, Hitesh and Kelly, Scot and Herzig, Kim},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Instruments, Interviews, Productivity, Software, Software engineering, Systematics, Task analysis, confidence, learning, onboarding, socialization, software development teams, teams},
	pages = {613--623},
}

@article{baltes_towards_2018,
	title = {Towards a {Theory} of {Software} {Development} {Expertise}},
	url = {http://arxiv.org/abs/1807.06087},
	doi = {10.1145/3236024.3236061},
	abstract = {Software development includes diverse tasks such as implementing new features, analyzing requirements, and fixing bugs. Being an expert in those tasks requires a certain set of skills, knowledge, and experience. Several studies investigated individual aspects of software development expertise, but what is missing is a comprehensive theory. We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance. Our theory currently focuses on programming, but already provides valuable insights for researchers, developers, and employers. The theory describes important properties of software development expertise and which factors foster or hinder its formation, including how developers' performance may decline over time. Moreover, our quantitative results show that developers' expertise self-assessments are context-dependent and that experience is not necessarily related to expertise.},
	urldate = {2022-01-20},
	journal = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Baltes, Sebastian and Diehl, Stephan},
	month = oct,
	year = {2018},
	note = {arXiv: 1807.06087},
	keywords = {Computer Science - Software Engineering},
	pages = {187--200},
}

@article{lin_attack_nodate,
	title = {Attack {Tactic} {Labeling} for {Cyber} {Threat} {Hunting}},
	abstract = {Recently, the cyber attack has become more complex and targeted, making traditional security defense mechanisms based on the “Indicator of Compromise” ineffective. Furthermore, fail to consider attack kill chain may lead to a high false-positive rate for attack detection. To trace hackers’ behaviors and footprints, it is crucial to provide additional information such as attack tactics, techniques, and procedures in detecting attacks.},
	language = {en},
	author = {Lin, Sheng-Xiang and Li, Zong-Jyun and Chen, Tzu-Yang and Wu, Dong-Jie},
	pages = {7},
}

@inproceedings{sree_artificial_2021,
	title = {Artificial {Intelligence} {Based} {Predictive} {Threat} {Hunting} {In} {The} {Field} of {Cyber} {Security}},
	doi = {10.1109/GCAT52182.2021.9587507},
	abstract = {Artificial intelligence (AI) is a broad field of computer science that focuses on designing smart machines capable of performing tasks typically requiring human intelligence. Despite the fact that security solutions are growing progressively modern and stable, cyberattacks are still evolving and are at their extreme. The main reason is that conventional methods of malware detection fail. Cyber attackers are actively developing new ways to prevent defence programmes from infecting malware networks and servers. Most anti-malware and antivirus applications currently use signature-based detection to identify attacks, which is unsuccessful in detecting new threats. This is where Artificial Intelligence is most handy. The standardised models for threatened hunting and performance quantification from the start of hazard hunting to the end still allow methodological rigour and completeness to be studied remain undefined. The organised practise of hazard hunts seeks to disclose the presence of TTP in the field of detection that has not already been detected. In this study, a realistic and comprehensive model is outlined to detect attackers in six stages: aim, scale, equipment, planning, execution and input. This study describes Threat Hunting in an ecosystem as the constructive, analyst-driven scanning mechanism for attackers TTP. The model has been checked for real-world data sets using a variety of threats. The effectiveness and practicality of this research have been shown with and without a blueprint through danger hunts. In addition, the article presents an analysis of the concept of threat hunting based on data from Ukrainian electricity grid attacks in an online environment to highlight the effects of this model on threat hunting in a simulated environment. The findings of this analysis include an effective and repetitive way to search for and quantify honesty, coverage and rigour.},
	booktitle = {2021 2nd {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Sree, Vaddi Sowmya and Koganti, Chaitna Sri and Kalyana, Srinivas K and Anudeep, P.},
	month = oct,
	year = {2021},
	keywords = {Analytical models, Biological system modeling, Data models, Ecosystems, Hazards, Joint Targetting Cycle, Systematics, Threa tHunting Model, Threathunting, Tools},
	pages = {1--6},
}

@inproceedings{horta_neto_cyber_2020,
	title = {Cyber {Threat} {Hunting} {Through} {Automated} {Hypothesis} and {Multi}-{Criteria} {Decision} {Making}},
	doi = {10.1109/BigData50022.2020.9378213},
	abstract = {There are sophisticated cyber attacks that pose a high risk to institutions, especially when they are carefully planned and victims are unable to identify them. This is a preliminary result of executing the high-level cyber threat hunting through automated hypothesis-making and multi-criteria decision making using the binary attack-chaining tables identified in the networks. Firstly, the concepts required for threats modeling and the process of knowledge discovery in databases focused on high-level threat hunting were introduced. After, the knowledge discovered was used in an experiment that applied and evaluated the effectiveness of machine learning and decision-making algorithms in the method proposed to prioritize hypotheses in the screening phase. In addition, an automated hypothesis-making method to be used in production environments was also proposed. Finally, the results achieved in the experiment demonstrated that high-level threat hunting is a viable and more efficient alternative compared to manual process.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Horta Neto, Antonio José and Fernandes Pereira dos Santos, Anderson},
	month = dec,
	year = {2020},
	keywords = {Big Data, Decision making, Knowledge discovery, Machine learning, Machine learning algorithms, Manuals, Production, decision-making, knowledge discovery in databases, machine learning, threat hunting, threat intelligence},
	pages = {1823--1830},
}

@inproceedings{miazi_design_2017,
	title = {The {Design} of {Cyber} {Threat} {Hunting} {Games}: {A} {Case} {Study}},
	shorttitle = {The {Design} of {Cyber} {Threat} {Hunting} {Games}},
	doi = {10.1109/ICCCN.2017.8038527},
	abstract = {Cyber Threat Hunting is an emerging cyber security activity. Recent studies show that, although similar actions like threat hunting are being actively practiced in some organization, security administrator and policy makers are far from being satisfied with their effectiveness. Most security professionals lack expertise in data analytics while most people with data analytics skills lack security knowledge. To understand the necessity of threat hunting education at university level, we organized a \textit{Threat Hunting Competition} on campus with generated logs. In this paper, we identify skills needed for cyber threat hunting, describe the data generation process as well as the usage of logs to teach threat hunting at universities.},
	booktitle = {2017 26th {International} {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Miazi, Md Nazmus Sakib and Pritom, Mir Mehedi A. and Shehab, Mohamed and Chu, Bill and Wei, Jinpeng},
	month = jul,
	year = {2017},
	keywords = {Companies, Computer security, Data analysis, Forensics, Games},
	pages = {1--6},
}

@article{karuna_automating_2021,
	title = {Automating {Cyber} {Threat} {Hunting} {Using} {NLP}, {Automated} {Query} {Generation}, and {Genetic} {Perturbation}},
	url = {http://arxiv.org/abs/2104.11576},
	abstract = {Scaling the cyber hunt problem poses several key technical challenges. Detecting and characterizing cyber threats at scale in large enterprise networks is hard because of the vast quantity and complexity of the data that must be analyzed as adversaries deploy varied and evolving tactics to accomplish their goals. There is a great need to automate all aspects, and, indeed, the workflow of cyber hunting. AI offers many ways to support this. We have developed the WILEE system that automates cyber threat hunting by translating high-level threat descriptions into many possible concrete implementations. Both the (high-level) abstract and (low-level) concrete implementations are represented using a custom domain specific language (DSL). WILEE uses the implementations along with other logic, also written in the DSL, to automatically generate queries to confirm (or refute) any hypotheses tied to the potential adversarial workflows represented at various layers of abstraction.},
	urldate = {2022-01-19},
	journal = {arXiv:2104.11576 [cs]},
	author = {Karuna, Prakruthi and Hemberg, Erik and O'Reilly, Una-May and Rutar, Nick},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11576},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@inproceedings{agarwal_cyber_2021,
	title = {Cyber {Security} {Model} for {Threat} {Hunting}},
	doi = {10.1109/ICRITO51393.2021.9596199},
	abstract = {Data privacy and encryption will still be top security priorities. Threat controls are countermeasures or safeguards used to reduce the chances that a threat will exploit a vulnerability as there is also a lack of understanding and a systematic model on which to base threat hunting operations and quantifying their effectiveness from the start of a threat hunt engagement to the end, as well as analytic rigour and completeness analysis. Threat hunting is a systematic method that aims to discover the location of attacker tactics, techniques, and procedures (TTP) in an area that has not yet been detected by current detection technologies. Using six stages: purpose, scope, equip, plan review, execute, and feedback, this research outlines a survey on this research.},
	booktitle = {2021 9th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	author = {Agarwal, Anchit and Walia, Himdweep and Gupta, Himanshu},
	month = sep,
	year = {2021},
	keywords = {Analytical models, Computer crime, Cyber Security, Data Encryption, Data privacy, Encryption, Market research, Reliability, Systematics, TTP-Based Hunting, Threat Hunting},
	pages = {1--8},
}

@inproceedings{gao_system_2021,
	title = {A {System} for {Efficiently} {Hunting} for {Cyber} {Threats} in {Computer} {Systems} {Using} {Threat} {Intelligence}},
	doi = {10.1109/ICDE51399.2021.00309},
	abstract = {Log-based cyber threat hunting has emerged as an important solution to counter sophisticated cyber attacks. However, existing approaches require non-trivial efforts of manual query construction and have overlooked the rich external knowledge about threat behaviors provided by open-source Cyber Threat Intelligence (OSCTI). To bridge the gap, we build ThreatRaptor, a system that facilitates cyber threat hunting in computer systems using OSCTI. Built upon mature system auditing frameworks, ThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP pipeline that extracts structured threat behaviors from unstructured OSCTI text, (2) a concise and expressive domain-specific query language, TBQL, to hunt for malicious system activities, (3) a query synthesis mechanism that automatically synthesizes a TBQL query from the extracted threat behaviors, and (4) an efficient query execution engine to search the big system audit logging data.},
	booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Gao, Peng and Shao, Fei and Liu, Xiaoyuan and Xiao, Xusheng and Liu, Haoyuan and Qin, Zheng and Xu, Fengyuan and Mittal, Prateek and Kulkarni, Sanjeev R. and Song, Dawn},
	month = apr,
	year = {2021},
	note = {ISSN: 2375-026X},
	keywords = {Conferences, Data engineering, Data mining, Database languages, Manuals, Open source software, Pipelines, n/a},
	pages = {2705--2708},
}

@book{garcia-alfaro_security_2021,
	address = {Cham},
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {Security and {Privacy} in {Communication} {Networks}: 17th {EAI} {International} {Conference}, {SecureComm} 2021, {Virtual} {Event}, {September} 6–9, 2021, {Proceedings}, {Part} {I}},
	volume = {398},
	isbn = {978-3-030-90018-2 978-3-030-90019-9},
	shorttitle = {Security and {Privacy} in {Communication} {Networks}},
	url = {https://link.springer.com/10.1007/978-3-030-90019-9},
	language = {en},
	urldate = {2022-01-19},
	publisher = {Springer International Publishing},
	editor = {Garcia-Alfaro, Joaquin and Li, Shujun and Poovendran, Radha and Debar, Hervé and Yung, Moti},
	year = {2021},
	doi = {10.1007/978-3-030-90019-9},
}

@inproceedings{gao_enabling_2021,
	title = {Enabling {Efficient} {Cyber} {Threat} {Hunting} {With} {Cyber} {Threat} {Intelligence}},
	doi = {10.1109/ICDE51399.2021.00024},
	abstract = {Log-based cyber threat hunting has emerged as an important solution to counter sophisticated attacks. However, existing approaches require non-trivial efforts of manual query construction and have overlooked the rich external threat knowledge provided by open-source Cyber Threat Intelligence (OSCTI). To bridge the gap, we propose ThreatRaptor, a system that facilitates threat hunting in computer systems using OSCTI. Built upon system auditing frameworks, ThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP pipeline that extracts structured threat behaviors from unstructured OSCTI text, (2) a concise and expressive domain-specific query language, TBQL, to hunt for malicious system activities, (3) a query synthesis mechanism that automatically synthesizes a TBQL query for hunting, and (4) an efficient query execution engine to search the big audit logging data. Evaluations on a broad set of attack cases demonstrate the accuracy and efficiency of ThreatRaptor in practical threat hunting.},
	booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Gao, Peng and Shao, Fei and Liu, Xiaoyuan and Xiao, Xusheng and Qin, Zheng and Xu, Fengyuan and Mittal, Prateek and Kulkarni, Sanjeev R. and Song, Dawn},
	month = apr,
	year = {2021},
	note = {ISSN: 2375-026X},
	keywords = {Conferences, Data engineering, Data mining, Database languages, Manuals, Open source software, Pipelines, n/a},
	pages = {193--204},
}

@article{ajmal_last_2021,
	title = {Last {Line} of {Defense}: {Reliability} {Through} {Inducing} {Cyber} {Threat} {Hunting} {With} {Deception} in {SCADA} {Networks}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Last {Line} of {Defense}},
	doi = {10.1109/ACCESS.2021.3111420},
	abstract = {There exists a gap between existing security mechanisms and their ability to detect advancing threats. Antivirus and EDR (End Point Detection and Response) aim to detect and prevent threats; such security mechanisms are reactive. This approach did not prove to be effective in protecting against stealthy attacks. SCADA (Supervisory Control and Data Acquisition) security is crucial for any country. However, SCADA is always an easy target for adversaries due to a lack of security for heterogeneous devices. An attack on SCADA is mainly considered a national-level threat. Recent research on SCADA security has not considered “unknown threats,” which has left a gap in security. The proactive approach, such as threat hunting, is the need of the hour. In this research, we investigated that threat hunting in conjunction with cyber deception and kill chain has countervailing effects on detecting SCADA threats and mitigating them. We have used the concept of “decoy farm” in the SCADA network, where all attacks are engaged. Moreover, we present a novel threat detection and prevention approach for SCADA, focusing on unknown threats. To test the effectiveness of approach, we emulated several SCADA, Linux and Windows based attacks on a simulated SCADA network. We have concluded that our approach detects and prevents the attacker before using the current reactive approach and security mechanism for SCADA with enhanced protection for heterogeneous devices. The results and experiments show that the proposed threat hunting approach has significantly improved the threat detection ability.},
	journal = {IEEE Access},
	author = {Ajmal, Abdul Basit and Alam, Masoom and Khaliq, Awais Abdul and Khan, Shawal and Qadir, Zakria and Mahmud, M. A. Parvez},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Industrial Internet of Things (IIoT), Licenses, Open source software, Process control, Protocols, SCADA systems, Security, Threat hunting, Tools, cyber deception, decoys, honeypots, indicators of compromise (IOC), supervisory control and data acquisition (SCADA)},
	pages = {126789--126800},
}

@article{johnson_long-term_2017,
	title = {Long-term follow-up of psilocybin-facilitated smoking cessation},
	volume = {43},
	issn = {0095-2990, 1097-9891},
	url = {https://www.tandfonline.com/doi/full/10.3109/00952990.2016.1170135},
	doi = {10.3109/00952990.2016.1170135},
	abstract = {Background—A recent open-label pilot study (N=15) found that two to three moderate to high doses (20 and 30 mg/70 kg) of the serotonin 2A receptor agonist psilocybin, in combination with cognitive behavioral therapy (CBT) for smoking cessation, resulted in substantially higher 6month smoking abstinence rates than are typically observed with other medications or CBT alone.
Objectives—To assess long-term effects of a psilocybin-facilitated smoking cessation program at ≥12 months after psilocybin administration.
Methods—The present report describes biologically verified smoking abstinence outcomes of the previous pilot study at ≥12 months, and related data on subjective effects of psilocybin.
Results—All 15 participants completed a 12-month follow-up, and 12 (80\%) returned for a longterm (≥16 months) follow-up, with a mean interval of 30 months (range = 16 – 57 months) between target-quit date (i.e., first psilocybin session) and long-term follow-up. At 12-month follow-up, 10 participants (67\%) were confirmed as smoking abstinent. At long-term follow-up, nine participants (60\%) were confirmed as smoking abstinent. At 12-month follow-up 13 participants (86.7\%) rated their psilocybin experiences among the 5 most personally meaningful and spiritually significant experiences of their lives.
Conclusion—These results suggest that in the context of a structured treatment program, psilocybin holds considerable promise in promoting long-term smoking abstinence. The present study adds to recent and historical evidence suggesting high success rates when using classic psychedelics in the treatment of addiction. Further research investigating psilocybin-facilitated treatment of substance use disorders is warranted.},
	language = {en},
	number = {1},
	urldate = {2021-12-12},
	journal = {The American Journal of Drug and Alcohol Abuse},
	author = {Johnson, Matthew W. and Garcia-Romeu, Albert and Griffiths, Roland R.},
	month = jan,
	year = {2017},
	note = {Number: 1},
	pages = {55--60},
}

@article{roberson_color_nodate,
	title = {Color categories: {Confirmation} of the relativity hypothesis.},
	abstract = {The question of whether language affects our categorization of perceptual continua is of particular interest for the domain of color where constraints on categorization have been proposed both within the visual system and in the visual environment. Recent research (Roberson et al., 2000; Roberson et al., in press) found substantial evidence of cognitive color differences between different language communities, but concerns remained as to how representative might be a tiny, extremely remote community. The present study replicates and extends previous findings using additional paradigms among a larger community in a different visual environment. Adult semi-nomadic tribesmen in Southern Africa carried out similarity judgments, short-term memory and long-term learning tasks. They showed different cognitive organization of color to both English and another language with the five color terms. Moreover, Categorical Perception effects were found to differ even between languages with broadly similar color categories. The results provide further evidence of the tight relationship between language and cognition.},
	language = {en},
	author = {Roberson, Debi and Davidoff, Jules},
	pages = {70},
}

@article{van_dongen_cumulative_2003,
	title = {The {Cumulative} {Cost} of {Additional} {Wakefulness}: {Dose}-{Response} {Effects} on {Neurobehavioral} {Functions} and {Sleep} {Physiology} {From} {Chronic} {Sleep} {Restriction} and {Total} {Sleep} {Deprivation}},
	volume = {26},
	issn = {1550-9109, 0161-8105},
	shorttitle = {The {Cumulative} {Cost} of {Additional} {Wakefulness}},
	url = {https://academic.oup.com/sleep/article-lookup/doi/10.1093/sleep/26.2.117},
	doi = {10.1093/sleep/26.2.117},
	abstract = {Objectives: To inform the debate over whether human sleep can be chronically reduced without consequences, we conducted a doseresponse chronic sleep restriction experiment in which waking neurobehavioral and sleep physiological functions were monitored and compared to those for total sleep deprivation. Design: The chronic sleep restriction experiment involved randomization to one of three sleep doses (4 h, 6 h, or 8 h time in bed per night), which were maintained for 14 consecutive days. The total sleep deprivation experiment involved 3 nights without sleep (0 h time in bed). Each study also involved 3 baseline (pre-deprivation) days and 3 recovery days. Setting: Both experiments were conducted under standardized laboratory conditions with continuous behavioral, physiological and medical monitoring. Participants: A total of n = 48 healthy adults (ages 21–38) participated in the experiments. Interventions: Nocturnal sleep periods were restricted to 8 h, 6 h or 4 h per day for 14 days, or to 0 h for 3 days. All other sleep was prohibited. Results: Chronic restriction of sleep periods to 4 h or 6 h per night over 14 consecutive days resulted in significant cumulative, dose-dependent deficits in cognitive performance on all tasks. Subjective sleepiness ratings showed an acute response to sleep restriction but only small further increases on subsequent days, and did not significantly differentiate the 6 h and 4 h conditions. Polysomnographic variables and δ power in the nonREM sleep EEG—a putative marker of sleep homeostasis—displayed an acute response to sleep restriction with negligible further changes across the 14 restricted nights. Comparison of chronic sleep restriction to total sleep deprivation showed that the latter resulted in disproportionately large waking neurobehavioral and sleep δ power responses relative to how much sleep was lost. A statistical model revealed that, regardless of the mode of sleep deprivation, lapses in behavioral alertness were nearlinearly related to the cumulative duration of wakefulness in excess of 15.84 h (s.e. 0.73 h). Conclusions: Since chronic restriction of sleep to 6 h or less per night produced cognitive performance deficits equivalent to up to 2 nights of total sleep deprivation, it appears that even relatively moderate sleep restriction can seriously impair waking neurobehavioral functions in healthy adults. Sleepiness ratings suggest that subjects were largely unaware of these increasing cognitive deficits, which may explain why the impact of chronic sleep restriction on waking cognitive functions is often assumed to be benign. Physiological sleep responses to chronic restriction did not mirror waking neurobehavioral responses, but cumulative wakefulness in excess of a 15.84 h predicted performance lapses across all four experimental conditions. This suggests that sleep debt is perhaps best understood as resulting in additional wakefulness that has a neurobiological “cost” which accumulates over time.},
	language = {en},
	number = {2},
	urldate = {2021-12-12},
	journal = {Sleep},
	author = {Van Dongen, Hans P.A. and Maislin, Greg and Mullington, Janet M. and Dinges, David F.},
	month = mar,
	year = {2003},
	note = {Number: 2},
	pages = {117--126},
}

@article{georgiadou_assessing_2021,
	title = {Assessing {MITRE} {ATT}\&{CK} {Risk} {Using} a {Cyber}-{Security} {Culture} {Framework}},
	volume = {21},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/9/3267},
	doi = {10.3390/s21093267},
	abstract = {The MITRE ATT\&CK (Adversarial Tactics, Techniques, and Common Knowledge) Framework provides a rich and actionable repository of adversarial tactics, techniques, and procedures. Its innovative approach has been broadly welcomed by both vendors and enterprise customers in the industry. Its usage extends from adversary emulation, red teaming, behavioral analytics development to a defensive gap and SOC (Security Operations Center) maturity assessment. While extensive research has been done on analyzing speciﬁc attacks or speciﬁc organizational culture and human behavior factors leading to such attacks, a holistic view on the association of both is currently missing. In this paper, we present our research results on associating a comprehensive set of organizational and individual culture factors (as described on our developed cyber-security culture framework) with security vulnerabilities mapped to speciﬁc adversary behavior and patterns utilizing the MITRE ATT\&CK framework. Thus, exploiting MITRE ATT\&CK’s possibilities towards a scientiﬁc direction that has not yet been explored: security assessment and defensive design, a step prior to its current application domain. The suggested cyber-security culture framework was originally designed to aim at critical infrastructures and, more speciﬁcally, the energy sector. Organizations of these domains exhibit a co-existence and strong interaction of the IT (Information Technology) and OT (Operational Technology) networks. As a result, we emphasize our scientiﬁc effort on the hybrid MITRE ATT\&CK for Enterprise and ICS (Industrial Control Systems) model as a broader and more holistic approach. The results of our research can be utilized in an extensive set of applications, including the efﬁcient organization of security procedures as well as enhancing security readiness evaluation results by providing more insights into imminent threats and security risks.},
	language = {en},
	number = {9},
	urldate = {2021-12-10},
	journal = {Sensors},
	author = {Georgiadou, Anna and Mouzakitis, Spiros and Askounis, Dimitris},
	month = may,
	year = {2021},
	note = {Number: 9},
	pages = {3267},
}

@article{araujo_evidential_2021,
	title = {Evidential {Cyber} {Threat} {Hunting}},
	url = {http://arxiv.org/abs/2104.10319},
	abstract = {A formal cyber reasoning framework for automating the threat hunting process is described. The new cyber reasoning methodology introduces an operational semantics that operates over three subspaces -- knowledge, hypothesis, and action -- to enable human-machine co-creation of threat hypotheses and protective recommendations. An implementation of this framework shows that the approach is practical and can be used to generalize evidence-based multi-criteria threat investigations.},
	urldate = {2021-11-14},
	journal = {arXiv:2104.10319 [cs]},
	author = {Araujo, Frederico and Kirat, Dhilung and Shu, Xiaokui and Taylor, Teryl and Jang, Jiyong},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.10319},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{ebrahimi_national_nodate,
	title = {National {Guard} {Cyber} {Protection} {Teams} as a {Response} to {Cybersecurity} {Threats}},
	language = {en},
	author = {Ebrahimi, Alex and Leithner, Anika and Lowham, Elizabeth A and Tiscareño, Samantha and Battle, Martin and Elmjouie, Maria and Vieira, Madalyn and Watkins, Jake},
	pages = {70},
}

@inproceedings{wafula_carve_2019,
	title = {{CARVE}: {A} {Scientific} {Method}-{Based} {Threat} {Hunting} {Hypothesis} {Development} {Model}},
	shorttitle = {{CARVE}},
	doi = {10.1109/EIT.2019.8833792},
	abstract = {A threat hunting exercise is a hypothesis driven exploratory and explanatory research process, the exercise is inherently scientific in nature and lends itself to the application of the scientific method of hypothesis development. The exercise commences with exploratory steps in the threat hypothesis phase to develop a logical argument asserting an existential threat, then follows with explanatory steps in the threat hunt phase to validate the argument. To deem a threat credible, that is, valid and relevant, a threat hunting hypothesis must establish a correlational and causal relationship between the asserted threat and a targeted asset, the hypothesis must adhere to the constructs of the scientific method for the exercise to be defined and measured objectively, and yield valuable and repeatable outcomes. Lack of adherence to the scientific method increases the frequency of invalid and/or irrelevant propositions in threat hypotheses, which diminishes Return on Investment (ROI) in cybersecurity defensive efforts due to wasted cycles of threat hunting exercises. This paper proposes a scientific method-based model, Collect Analyze Relate Validate Establish (CARVE), which can be used to develop valid and relevant threat hunting hypotheses in the context of a given organization's information system and environment. The CARVE model is defined by the following five steps: Collect, Analyze, Relate, Validate, and Establish. The effectiveness of the model is demonstrated using a case study based on the technical alert United States Computer Emergency Readiness Team (US CERT) TA17-293A.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Electro} {Information} {Technology} ({EIT})},
	author = {Wafula, Kevin and Wang, Yong},
	month = may,
	year = {2019},
	note = {ISSN: 2154-0373},
	keywords = {Analytical models, CARVE Model, Computer security, Correlation, Credible Threat, Information systems, Intelligent sensors, Organizations, Threat, Threat Hunting Hypothesis},
	pages = {1--6},
}

@inproceedings{rasheed_threat_2017,
	title = {Threat {Hunting} {Using} {GRR} {Rapid} {Response}},
	doi = {10.1109/ICTCS.2017.22},
	abstract = {Cybercrimes have evolved, and their tactics and techniques are increasingly changing with an alerting pace. This calls for a change in the mindset used to implement security measures, by adopting the approach of continuously and constantly looking for attacks that pass through the deployed security solutions. This approach of searching through the networks for any evidence on threat activity, rather waiting for a breach notification is referred to as cyber threat hunting. This paper discusses the deployment of threat hunting process using GRR Rapid Response. Two experiments were conducted, in which, both remote code execution, client side exploits are tested, and successful exploitation was used to configure a backdoor to the victim's system to achieve persistence. The experiments show that threat hunting can be achieved by the study of the monitored system's normal patterns of behavior, which will help identify the indications and thresholds that can be used in threat hunting.},
	booktitle = {2017 {International} {Conference} on {New} {Trends} in {Computing} {Sciences} ({ICTCS})},
	author = {Rasheed, Hussein and Hadi, Ali and Khader, Mariam},
	month = oct,
	year = {2017},
	keywords = {Computer crime, Firewalls (computing), GRR Rapid Response, IoC, Monitoring, Organizations, Response, Servers, Threat Hunting, Threat Intelligence, Tools},
	pages = {155--160},
}

@phdthesis{alharbi_security_2021,
	address = {United States -- California},
	type = {Ph.{D}.},
	title = {A {Security} {Operation} {Center} {Maturity} {Model} ({SOC}-{MM}) in the {Context} of {Newly} {Emerging} {Cyber} {Threats}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2447834861/abstract/49F3D76000004605PQ/1},
	abstract = {Cyberattacks and threats from hardware and software system components are rapidly emerging as one of the biggest challenges that today’s businesses, government agencies, and individuals face. As a result, organizations are continuously trying to adapt and update their information assets.
One of the most popular preventive methods against these malicious attacks is establishing of a security operations center (SOC). SOCs are responsible for protecting mission-critical information, as well as detecting and responding to attacks; they are also tasked with planning and creating contingency procedures to meet not only known challenges but also emerging and as yet undiscovered ones. However, organizations are faced with a real challenge: The absence of a holistic framework and guidance on how to establish an SOC, which makes setting up an SOC a complex task. Each organization is trying to reinvent the wheel, which has led to a diversity of implementation forms. Consequently, without a consistent method or process to follow, there is no way to ensure that the essential attributes for establishing an SOC are met. There are also no clear mechanisms for determining the maturity level and capabilities of these SOCs.
To address these gaps, this research attempted to: formulate an up-to-date definition of SOC and identify SOC’s essential attributes, develop an SOC maturity model (SOC-MM) based upon the context of newly emerging cyber threats, design an SOC-MM tool that can identify current level of maturity in organization and provides recommendations on how to reach the next maturity level, and finally implement and evaluate the SOC-MM and SOC-MM tool through a case study approach to prove its effectiveness for organizations.
Primarily, a design since research approach was adopted, which is a proactive research methodology. This approach is appropriate for research areas that are not clearly defined, as in the case of SOCs. Following the DSR process, the research steps for this study were as follow: the problem was defined through literature review, the research objectives were then listed, then interview and survey questions were created based on information gathered from the literature review. Interviews and surveys were then conducted with SOC experts and CSOs to formulate SOC's definition, the attributes, and the SOC-MM. From the designed SOC-MM, the tool was created, then was sent to an organization to be evaluated along with the SOC-MM.
There is a lot of possible beneficiary of the proposed model and tool, such as in organization as a self-assessment tool, where they can use the tool to measure their level of maturity and work on the recommendations provided for them to enhance their SOC. Another method of implication is in consultancy companies as a part of their services that they provide for their customers; the model proposed provides new insights that can help determine maturity through different landscapes than these models. Furthermore, it can be used in the organization's audit department, where they can use the model as a part of the performance evaluation and provide the SOC with the proposed recommendation to ensure continued improvement of the SOC.
Keywords: Cyberattacks, security operations center (SOC), maturity model (MM), newly emerging cyber threats, security operations center maturity model (SOC-MM).},
	language = {English},
	urldate = {2021-11-08},
	school = {The Claremont Graduate University},
	author = {Alharbi, Norah},
	month = nov,
	year = {2021},
	note = {ISBN: 9798672151229},
	keywords = {Cyberattacks, Cybersecurity, Maturity model, Security operations center, Security operations center maturity model},
}

@article{chamkar_human_2021,
	title = {{THE} {HUMAN} {FACTOR} {CAPABILITIES} {IN} {SECURITY} {OPERATION} {CENTER} ({SOC})},
	issn = {0736-6981, 1936-1009},
	url = {https://www.tandfonline.com/doi/full/10.1080/07366981.2021.1977026},
	doi = {10.1080/07366981.2021.1977026},
	abstract = {The human factor is considered the weakest link in cybersecurity and inside the Security Operation Centers (SOC) and it represents the most important component at the same time. Human factor capabilities and challenges attracted the attention of researchers to address how these challenges can be reduced or mitigated. However, these research papers do not consider the complexity, unpredictability, interdependent and evolving nature of the SOC systems. This study aims to explore the human capabilities and weaknesses inside the Security Operation Centre. To this end, we employed survey bases questionaries alongside the daily observation of SOC analysts and interviews with SOC experts. Forty SOC analysts and five experts conducted the survey. The finding of this study will help SOC managers and SOC designers better understand the challenges faced by the SOC analysts and take into account the interdependent and evolving nature of the Security Operation Centers.},
	language = {en},
	urldate = {2021-11-08},
	journal = {EDPACS},
	author = {Chamkar, Samir Achraf and Maleh, Yassine and Gherabi, Noreddine},
	month = oct,
	year = {2021},
	pages = {1--14},
}

@inproceedings{majid_success_2019,
	address = {Bandung, Indonesia},
	title = {Success {Factors} for {Cyber} {Security} {Operation} {Center} ({SOC}) {Establishment}},
	isbn = {978-1-63190-198-0},
	url = {http://eudl.eu/doi/10.4108/eai.18-7-2019.2287841},
	doi = {10.4108/eai.18-7-2019.2287841},
	abstract = {The boundless in the digital world is one of the terms used to describe the present state where everything depends mostly on the use of technology. The increased dependency on these technology services has indirectly increased the risk of threats and cyber-attacks. One of the popular solutions to defend against these threats is by implementing the Cyber Security Operation Center (SOC) to monitor, track and handle the cyber incidents. However, there are a number of factors that affect the success of the SOC. Therefore, this paper aims to highlight the importance of the human, process and technology factors towards the establishment of SOC. A comparison of the previous establishment of SOC from the literature is made. The inputs from the literature come from the journal, proceeding, report starting from the year 2011 until 2018. From the result of the comparison, it presents the requirement of human, process, and technology to make sure the SOC work efficiently to defend against the cyber-attack.},
	language = {en},
	urldate = {2021-11-08},
	booktitle = {Proceedings of the {Proceedings} of the 1st {International} {Conference} on {Informatics}, {Engineering}, {Science} and {Technology}, {INCITEST} 2019, 18 {July} 2019, {Bandung}, {Indonesia}},
	publisher = {EAI},
	author = {Majid, M. and Ariffi, K},
	year = {2019},
}

@article{cothier_timeliness_1986,
	title = {Timeliness and {Measures} of {Effectiveness} in {Command} and {Control}},
	volume = {16},
	issn = {0018-9472},
	url = {http://ieeexplore.ieee.org/document/4309003/},
	doi = {10.1109/TSMC.1986.4309003},
	abstract = {A methodology for assessing the effectiveness of command, control, and communications systems is extended to include timeliness. The assessment is based on comparing the properties of the system to the mission requirements when both are expressed as loci in a commensurate space of measures of performance. The methodology for evaluating measures of effectiveness is illustrated through application to an idealized fire support system.},
	language = {en},
	number = {6},
	urldate = {2021-11-05},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Cothier, Philippe H. and Levis, Alexander H.},
	year = {1986},
	note = {Number: 6},
	pages = {844--853},
}

@article{handley_incorporating_nodate,
	title = {Incorporating {Heterogeneity} in {Command} {Center} {Interactions}},
	abstract = {One of the many complexities of multinational coalition operations stems from differences in culture, military procedures, and command and control processes between the cooperating command centers. These differences can effect the interactions between decision makers of different command centers and can affect the outcome of the coalition operation. A model can be used to study the effect on coalition performance due to interactions between heterogeneous command centers. A coalition model, composed of individual models of the five-stage interacting decision maker model, which has been modified to include subjective parameters, was used in a virtual experiment. The subjective parameters included in the decision maker model can be any attribute that characterizes the heterogeneity of the decision makers. In this case, the parameters of power distance and uncertainty avoidance were used, two of Hofstede’s [1991] cultural dimensions. Differences in these values across the cooperating command centers can cause communication and coordination difficulties. The accuracy and timeliness of the coalition’s response was used to evaluate its performance as a function of heterogeneity. Including the presence of heterogeneity in the coalition model, through the use of subjective parameters, is the first step in formalizing the process for developing adaptive coalition architectures.},
	language = {en},
	author = {Handley, Holly A H and Levis, Alexander H},
	pages = {15},
}

@article{gonda_creating_2013,
	title = {Creating {A} {Collaborative} {Virtual} {Command} {Center} {Among} {Four} {Separate} {Organizations} {In} {The} {United} {States} {Army}: {An} {Exploratory} {Case} {Study}},
	abstract = {While individual leadership skills are an important factor in transforming organizations, leading through a common mission and shared purpose of collaborative and efficient interaction requires leaders to participate in different ways. The purpose of this study was to determine how to create a collaborative and efficient virtual Command Center within the new leadership structure of a Life Cycle Management Command within the United States Army. This case study employed a variety of organization development tools to bring together leadership, change agents, and customers to create a new vision and strategic plan of how to operate effectively. Leadership used the results of this study to implement the structural changes and new processes required for a successful transformation to a virtual business.},
	language = {en},
	author = {Gonda, Teresa and Kohnke, Anne},
	year = {2013},
	pages = {20},
}

@article{tosh_elements_2020,
	title = {Elements of an {Effective} {Incident} {Command} {Center}},
	volume = {95},
	issn = {00256196},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0025619620306480},
	doi = {10.1016/j.mayocp.2020.06.026},
	language = {en},
	number = {9},
	urldate = {2021-11-04},
	journal = {Mayo Clinic Proceedings},
	author = {Tosh, Pritish K. and Bucks, Colin M. and O’Horo, John C. and DeMartino, Erin S. and Johnson, Jay M. and Callies, Byron I.},
	month = sep,
	year = {2020},
	note = {Number: 9},
	pages = {S3--S7},
}

@inproceedings{sundaramurthy_tale_2014,
	address = {Scottsdale, Arizona, USA},
	title = {A {Tale} of {Three} {Security} {Operation} {Centers}},
	isbn = {978-1-4503-3152-4},
	url = {http://dl.acm.org/citation.cfm?doid=2663887.2663904},
	doi = {10.1145/2663887.2663904},
	abstract = {Security researchers have been trying to understand functioning of a security operation center (SOC) and how security analysts perform their job. This eﬀort is motivated by the fact that security monitoring and analysis is not just a technical problem. Researchers must take into consideration the human and organizational factors for their research ideas to succeed. Much work towards this direction has been through interviews of security analysts in SOCs. Interviews, however useful, will not be always possible as analysts work in a high-stress and time constrained environment. Thus the understanding of operational challenges through interviews is quite shallow. There is also an issue of trust that limits the amount of information an analyst shares with an interviewing researcher. In our work, we take an anthropological approach to address this problem. Students with Computer Science background get trained in anthropological methods by an anthropologist and are embedded as security analysts in operation centers. Embedded students perform the same job as an analyst and see the operational world from the view point of an analyst. Through reﬂection on the observations made by the students we gain a holistic perspective of the challenges in operation centers. In this paper we report preliminary results on the ongoing ﬁeldwork at two corporate and a University SOC.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 2014 {ACM} {Workshop} on {Security} {Information} {Workers} - {SIW} '14},
	publisher = {ACM Press},
	author = {Sundaramurthy, Sathya Chandran and Case, Jacob and Truong, Tony and Zomlot, Loai and Hoffmann, Marcel},
	year = {2014},
	pages = {43--50},
}

@inproceedings{jacobs_classification_2013,
	address = {Johannesburg, South Africa},
	title = {Classification of {Security} {Operation} {Centers}},
	isbn = {978-1-4799-0808-0},
	url = {http://ieeexplore.ieee.org/document/6641054/},
	doi = {10.1109/ISSA.2013.6641054},
	abstract = {Security Operation Centers (SOCs) are a necessary service for organisations that want to address compliance and threat management. While there are frameworks in existence that addresses the technology aspects of these services, a holistic framework addressing processes, staffing and technology currently do not exist. Additionally, it would be useful for organizations and constituents considering building, buying or selling these services to measure the effectiveness and maturity of the provided services. In this paper, we propose a classification and rating scheme for SOC services, evaluating both the capabilities and the maturity of the services offered.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {2013 {Information} {Security} for {South} {Africa}},
	publisher = {IEEE},
	author = {Jacobs, Pierre and Arnab, Alapan and Irwin, Barry},
	month = aug,
	year = {2013},
	pages = {1--7},
}

@article{cho_capturing_2020,
	title = {Capturing {Tacit} {Knowledge} in {Security} {Operation} {Centers}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9007685/},
	doi = {10.1109/ACCESS.2020.2976076},
	abstract = {The use of tacit knowledge has previously been shown to help expedite problem-solving procedures in the setting of medical emergency responses, as individuals can use past experiences in present and future challenges. However, there is a lack of understanding in its application in IT and socio-technical management. This paper examines the thought processes observed in Security Operational Centre (SOC) analysts facing threat events to lay the groundwork for tacit knowledge management in SOCs. Based on Sternberg’s ﬁeldwork in tacit knowledge, we conducted semi-structured interviews with ten analysts to explore the key artefacts and individual traits that aid their approach to communication, and to examine the thought processes under hypothetical incident handling scenarios. The results highlight a unanimous pursuit of Root Cause Analysis (RCA) upon the outbreak of an incident and stages of decision-making when escalating to third party support providers. Using Business Process Modelling and Notation (BPMN), we show the procedural elements of tacit knowledge from several scenarios. The results also suggest that simulation environments and physical proximity with analysts and vendors can facilitate the transfer of tacit knowledge more effectively in SOCs.},
	language = {en},
	urldate = {2021-11-04},
	journal = {IEEE Access},
	author = {Cho, Selina Y. and Happa, Jassim and Creese, Sadie},
	year = {2020},
	pages = {42021--42041},
}

@article{noauthor_toward_2022,
	title = {Toward the {Cyber}-{Physical} {Frontier}: {Reflections} and {Directions}},
	language = {en},
	year = {2022},
	pages = {5},
}

@article{goldberg_alternative_nodate,
	title = {An {Alternative} "{Description} of {Personality}": {The} {Big}-{Five} {Factor} {Structure}},
	language = {en},
	author = {Goldberg, Lewis R},
	pages = {14},
}

@article{maznevski_cultural_2002,
	title = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}: {The} {Cultural} {Orientations} {Framework}},
	volume = {2},
	issn = {1470-5958, 1741-2838},
	shorttitle = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}},
	url = {http://journals.sagepub.com/doi/10.1177/147059580223001},
	doi = {10.1177/147059580223001},
	abstract = {This article describes a theoretically-grounded framework of cultural dimensions conceptualized and operationalized at the individual level of analysis, based on the work of anthropologists Kluckhohn and Strodtbeck. We present empirical data gathered from five countries – Canada, Mexico, the Netherlands, Taiwan, and the United States – to assess the validity of the framework. We then use the results to explore how the cultural orientations framework can add insight and new perspectives to critical questions in cross cultural management research.},
	language = {en},
	number = {3},
	urldate = {2021-10-14},
	journal = {International Journal of Cross Cultural Management},
	author = {Maznevski, Martha L. and Gomez, Carolina B. and DiStefano, Joseph J. and Noorderhaven, Niels G. and Wu, Pei-Chuan},
	month = dec,
	year = {2002},
	note = {Number: 3},
	pages = {275--295},
}

@article{kostova_interplay_nodate,
	title = {On the {Interplay} between {Requirements}, {Engineering}, and {Artiﬁcial} {Intelligence}},
	abstract = {With this paper, we present our reﬂections on the issues that are faced by the Requirements Engineering academic discipline and practice. The current reality of the Artiﬁcial Intelligence and Machine Learning hype penetrating from research into all industry sectors and all phases of system design and development is a transformative shift that inﬂuences the way Requirements Engineering is conducted and the nature of the systems that are engineered. We identify two sides of this transformation with regards to the Requirements Engineering discipline: (1) Artiﬁcial Intelligence tools are used more and more during the Requirements Engineering process, (2) the Requirements Engineering process for systems that include Artiﬁcial Intelligence is diﬀerent. By identifying and framing these changes, we pose questions about what it means to engineer requirements. Our analysis asks more questions than it answers. We hope to engage the Requirements Engineering academic community in a larger conversation about the role of Requirements Engineering in the changing world and about a possible new vision of engineering becoming secondary to requirements in Requirements Engineering.},
	language = {en},
	author = {Kostova, Blagovesta and Gurses, Seda and Delft, TU and Leuven, KU and Wegmann, Alain},
	pages = {5},
}

@inproceedings{gladisch_experience_2019,
	title = {Experience {Paper}: {Search}-{Based} {Testing} in {Automated} {Driving} {Control} {Applications}},
	shorttitle = {Experience {Paper}},
	doi = {10.1109/ASE.2019.00013},
	abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Measurement, Monitoring, Optimization, Software, Test pattern generators, Tools, automated driving, automated test generation, experience paper, search-based testing},
	pages = {26--37},
}

@inproceedings{wang_exploratory_2021,
	address = {Athens Greece},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	month = aug,
	year = {2021},
	pages = {20--31},
}

@inproceedings{hutchinson_model-driven_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Model-driven engineering practices in industry},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985882},
	doi = {10.1145/1985793.1985882},
	abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
	language = {en},
	urldate = {2021-09-14},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
	month = may,
	year = {2011},
	pages = {633--642},
}

@article{bei_cyber_2011,
	title = {Cyber defense competition: a tale of two teams},
	abstract = {Collegiate Cyber Defense Competitions have recently grown in popularity as a means of providing real-world experiences to students learning computer security at the college level. Preparation and training for these competitions focuses students on essential skills needed to defend networks against real threats and better prepares them for the problems and conditions they may encounter outside the protection of university run labs. This paper highlights the benefits of Cyber Defense Competitions and documents the experiences of two teams that trained and competed in the Northwest regional cyber defense competition. Both teams benefited from participating in the competition with students expressing positive learning experiences. Recommendations for other schools that may be interested in competing or setting up in-house cyber defense exercises will be presented.},
	language = {en},
	author = {Bei, Yan and Kesterson, Robert and Gwinnup, Kyle and Taylor, Carol},
	year = {2011},
	pages = {7},
}

@article{carlin_developing_2010,
	title = {Developing the {Cyber} {Defenders} of {Tomorrow} {With} {Regional} {Collegiate} {Cyber} {Defense} {Competitions} ({CCDC})},
	abstract = {With the projected higher demand for Network Systems Analysts and increasing computer crime, network security specialists are an organization’s ﬁrst line of defense. The principle function of this paper is to provide the evolution of Collegiate Cyber Defense Competitions (CCDC), event planning required, soliciting sponsors, recruiting personnel for the operations, red, white and blue teams. Information on one school’s preparation will be provided with a review of what could have been improved to prepare their team for the competition.},
	language = {en},
	author = {Carlin, Anna and Manson, Daniel P and Zhu, Jake},
	year = {2010},
	pages = {10},
}

@article{werlinger_integrated_2009,
	title = {An integrated view of human, organizational, and technological challenges of {IT} security management},
	volume = {17},
	issn = {0968-5227},
	url = {https://www.emerald.com/insight/content/doi/10.1108/09685220910944722/full/html},
	doi = {10.1108/09685220910944722},
	abstract = {Purpose – The purpose of this study is to determine the main challenges that IT security practitioners face in their organizations, including the interplay among human, organizational, and technological factors.},
	language = {en},
	number = {1},
	urldate = {2021-09-02},
	journal = {Information Management \& Computer Security},
	author = {Werlinger, Rodrigo and Hawkey, Kirstie and Beznosov, Konstantin},
	editor = {Furnell, Steven M.},
	month = mar,
	year = {2009},
	note = {Number: 1},
	pages = {4--19},
}

@article{werlinger_preparation_2010,
	title = {Preparation, detection, and analysis: the diagnostic work of {IT} security incident response},
	volume = {18},
	issn = {0968-5227},
	shorttitle = {Preparation, detection, and analysis},
	url = {https://www.emerald.com/insight/content/doi/10.1108/09685221011035241/full/html},
	doi = {10.1108/09685221011035241},
	abstract = {Purpose – The purpose of this paper is to examine security incident response practices of information technology (IT) security practitioners as a diagnostic work process, including the preparation phase, detection, and analysis of anomalies.},
	language = {en},
	number = {1},
	urldate = {2021-09-02},
	journal = {Information Management \& Computer Security},
	author = {Werlinger, Rodrigo and Muldner, Kasia and Hawkey, Kirstie and Beznosov, Konstantin},
	editor = {Furnell, Steven M.},
	month = mar,
	year = {2010},
	note = {Number: 1},
	pages = {26--42},
}

@article{werlinger_security_2008,
	title = {Security {Practitioners} in {Context}: {Their} {Activities} and {Interactions}},
	abstract = {This study develops the context of interactions of IT security practitioners. Preliminary qualitative analysis of 22 interviews (to date) and participatory observation has identified eight different types of activities that require interactions between security practitioners and different stakeholders. Our analysis shows that the tools used by our participants do not provide sufficient support for their complex security tasks, including the interactions with other stakeholders. We provide recommendations to improve tool support for security practitioners.},
	language = {en},
	author = {Werlinger, Rodrigo and Hawkey, Kirstie and Beznosov, Konstantin},
	year = {2008},
	pages = {6},
}

@article{vieane_task_2017,
	title = {Task {Interruptions} {Undermine} {Cyber} {Defense}},
	volume = {61},
	issn = {2169-5067, 1071-1813},
	url = {http://journals.sagepub.com/doi/10.1177/1541931213601576},
	doi = {10.1177/1541931213601576},
	abstract = {Computer network defense analysts engage a difficult, though critical, task in cyber defense. Anecdotally, these operators complain of frequent task interruptions while they are performing their duties. The goal for the current study was to investigate the effect of a commonly reported interruption, answering email, on accuracy and completion times in a simulated network analyst task. During task trials, participants were interrupted by emails between alert investigations, during alert investigations, or not at all (control). The results indicated that email interruptions increased alert completion times regardless of when they occurred, but interruptions that occurred during an alert investigation also reduced the accuracy of subsequent judgments about alert threat. Overall, the results suggest that task interruptions can potentially undermine cyber defense, and steps should be taken to better quantify and mitigate this threat.},
	language = {en},
	number = {1},
	urldate = {2021-09-01},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Vieane, Alex and Funke, Gregory and Greenlee, Eric and Mancuso, Vincent and Borghetti, Brett and Miller, Brent and Menke, Lauren and Brown, Rebecca and Foroughi, Cyrus K. and Boehm-Davis, Deborah},
	month = sep,
	year = {2017},
	note = {Number: 1},
	pages = {375--379},
}

@article{sawyer_hacking_2018,
	title = {Hacking the {Human}: {The} {Prevalence} {Paradox} in {Cybersecurity}},
	volume = {60},
	issn = {0018-7208, 1547-8181},
	shorttitle = {Hacking the {Human}},
	url = {http://journals.sagepub.com/doi/10.1177/0018720818780472},
	doi = {10.1177/0018720818780472},
	abstract = {Objective:
              This work assesses the efficacy of the “prevalence effect” as a form of cyberattack in human-automation teaming, using an email task.
            
            
              Background:
              Under the prevalence effect, rare signals are more difficult to detect, even when taking into account their proportionally low occurrence. This decline represents diminished human capability to both detect and respond. As signal probability (SP) approaches zero, accuracy exhibits logarithmic decay. Cybersecurity, a context in which the environment is entirely artificial, provides an opportunity to manufacture conditions enhancing or degrading human performance, such as prevalence effects. Email cybersecurity prevalence effects have not previously been demonstrated, nor intentionally manipulated.
            
            
              Method:
              The Email Testbed (ET) provides a simulation of a clerical email work involving messages containing sensitive personal information. Using the ET, participants were presented with 300 email interactions and received cyberattacks at rates of either 1\%, 5\%, or 20\%.
            
            
              Results:
              Results demonstrated the existence and power of prevalence effects in email cybersecurity. Attacks delivered at a rate of 1\% were significantly more likely to succeed, and the overall pattern of accuracy across declining SP exhibited logarithmic decay.
            
            
              Application:
              These findings suggest a “prevalence paradox” within human-machine teams. As automation reduces attack SP, the human operator becomes increasingly likely to fail in detecting and reporting attacks that remain. In the cyber realm, the potential to artificially inflict this state on adversaries, hacking the human operator rather than algorithmic defense, is considered. Specific and general information security design countermeasures are offered.},
	language = {en},
	number = {5},
	urldate = {2021-09-01},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Sawyer, Ben D. and Hancock, Peter A.},
	month = aug,
	year = {2018},
	note = {Number: 5},
	pages = {597--609},
}

@article{martin_signal_2018,
	title = {Signal {Detection} {Theory} ({SDT}) {Is} {Effective} for {Modeling} {User} {Behavior} {Toward} {Phishing} and {Spear}-{Phishing} {Attacks}},
	volume = {60},
	issn = {0018-7208, 1547-8181},
	url = {http://journals.sagepub.com/doi/10.1177/0018720818789818},
	doi = {10.1177/0018720818789818},
	abstract = {Objective: To examine the utility of equal-variance signal detection theory (EVSDT) for evaluating and understanding human detection of phishing and spear-phishing e-mail scams.
Background: Although the majority of cybersecurity breaches are due to erroneous responses to deceptive phishing e-mails, it is unclear how best to quantify performance in this context. In particular, it is unclear whether equal variances can safely be assumed in the SDT model, or, relatedly, whether degree of targeting, or threat level, primarily affects mean separation or evidence variability.
Method: Through an online inbox simulation, the present research found that differences in susceptibility to phishing and spear-phishing e-mails could be carefully quantified with respect to detection accuracy and response bias through the use of an EVSDT framework.
Results: The results indicated that EVSDT-based point metrics are effective for modeling and measuring phishing susceptibility in the inbox task, without the need for parameter estimation or model comparison involving unequal-variance SDT (UVSDT). Threat level modulated mean separation, with no effects on signal variances.
Conclusion: These findings support the viability of using EVSDT to initially assess and subsequently monitor training effectiveness for phishing susceptibility, thereby providing measures that are superior to more intuitive metrics, which typically confound an individual’s bias and accuracy. Effects of threat level mapped clearly onto distribution means with no effect on variances, suggesting phishing susceptibility primarily reflects temporally stable discriminative characteristics of observers. Notably, results indicated that people are particularly poor at identifying spear-phishing e-mail threats (demonstrating only 40\% accuracy).},
	language = {en},
	number = {8},
	urldate = {2021-09-01},
	journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
	author = {Martin, Jaclyn and Dubé, Chad and Coovert, Michael D.},
	month = dec,
	year = {2018},
	note = {Number: 8},
	pages = {1179--1191},
}

@inproceedings{conti_attacking_2005,
	address = {Pittsburgh, Pennsylvania},
	title = {Attacking information visualization system usability overloading and deceiving the human},
	isbn = {978-1-59593-178-8},
	url = {http://portal.acm.org/citation.cfm?doid=1073001.1073010},
	doi = {10.1145/1073001.1073010},
	abstract = {Information visualization is an effective way to easily comprehend large amounts of data. For such systems to be truly effective, the information visualization designer must be aware of the ways in which their system may be manipulated and protect their users from attack. In addition, users should be aware of potential attacks in order to minimize or negate their effect. These attacks target the information visualization system as well as the perceptual, cognitive and motor capabilities of human end users. To identify and help counter these attacks we present a framework for information visualization system security analysis, a taxonomy of visualization attacks and technology independent principles for countering malicious visualizations. These themes are illustrated with case studies and working examples from the network security visualization domain, but are widely applicable to virtually any information visualization system.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the 2005 symposium on {Usable} privacy and security  - {SOUPS} '05},
	publisher = {ACM Press},
	author = {Conti, Gregory and Ahamad, Mustaque and Stasko, John},
	year = {2005},
	pages = {89--100},
}

@article{nagy_empirical_nodate,
	title = {An empirical study on current models for reasoning about digital evidence},
	abstract = {The forensic process relies on the scientific method to scrutinize recovered evidence that either supports or negates an investigative hypothesis. Currently, analysis of digital evidence remains highly subjective to the forensic practitioner. Digital forensics is in need of a deterministic approach to obtain the most judicious conclusions from evidence. The objective of this paper is to examine current methods of digital evidence analysis. It describes the mechanisms for which these processes may be carried out, and discusses the key obstacles presented by each. Lastly, it concludes with suggestions for further improvement of the digital forensic process as a whole.},
	language = {en},
	author = {Nagy, Stefan and Palmer, Imani and Sundaramurthy, Sathya Chandran and Ou, Xinming and Campbell, Roy},
	pages = {7},
}

@incollection{li_android_2017,
	address = {Cham},
	title = {Android {Malware} {Clustering} {Through} {Malicious} {Payload} {Mining}},
	volume = {10453},
	isbn = {978-3-319-66331-9 978-3-319-66332-6},
	url = {http://link.springer.com/10.1007/978-3-319-66332-6_9},
	abstract = {Clustering has been well studied for desktop malware analysis as an eﬀective triage method. Conventional similarity-based clustering techniques, however, cannot be immediately applied to Android malware analysis due to the excessive use of third-party libraries in Android application development and the widespread use of repackaging in malware development. We design and implement an Android malware clustering system through iterative mining of malicious payload and checking whether malware samples share the same version of malicious payload. Our system utilizes a hierarchical clustering technique and an eﬃcient bit-vector format to represent Android apps. Experimental results demonstrate that our clustering approach achieves precision of 0.90 and recall of 0.75 for Android Genome malware dataset, and average precision of 0.98 and recall of 0.96 with respect to manually veriﬁed ground-truth.},
	language = {en},
	urldate = {2021-08-31},
	booktitle = {Research in {Attacks}, {Intrusions}, and {Defenses}},
	publisher = {Springer International Publishing},
	author = {Li, Yuping and Jang, Jiyong and Hu, Xin and Ou, Xinming},
	editor = {Dacier, Marc and Bailey, Michael and Polychronakis, Michalis and Antonakakis, Manos},
	year = {2017},
	doi = {10.1007/978-3-319-66332-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {192--214},
}

@article{sundaramurthy_anthropological_2014,
	title = {An {Anthropological} {Approach} to {Studying} {CSIRTs}},
	volume = {12},
	issn = {1558-4046},
	doi = {10.1109/MSP.2014.84},
	abstract = {The ethnographic method of participant observation can help researchers better understand the challenges computer security incident response teams face by illuminating underlying assumptions and tacit practices that shape how tools are actually used in different contexts.},
	number = {5},
	journal = {IEEE Security Privacy},
	author = {Sundaramurthy, Sathya Chandran and McHugh, John and Ou, Xinming Simon and Rajagopalan, S. Raj and Wesch, Michael},
	month = sep,
	year = {2014},
	note = {Number: 5
Conference Name: IEEE Security Privacy},
	keywords = {Behavioral science, Computer security, Context awareness, Cultural differences, System-on-chip, communication/networking and information technology, computer systems organization, computing milieux, management of computing and information systems, network-level security and protection, security},
	pages = {52--60},
}

@article{sundaramurthy_humans_2017,
	title = {Humans {Are} {Dynamic} - {Our} {Tools} {Should} {Be} {Too}},
	volume = {21},
	issn = {1941-0131},
	doi = {10.1109/MIC.2017.52},
	abstract = {Security Operation Centers (SOCs) are being operated by universities, government agencies, and corporations to defend their enterprise networks and identify and thwart malicious behaviors in both networks and hosts. The success of a SOC depends on combining good tools and processes with efficient and effective analysts. During four years of anthropological fieldwork methods to study SOCs, the authors discovered that successful SOC innovations must resolve multiple internal and external conflicts to be effective and efficient. This discovery, guided by activity theory (AT) as a framework for analyzing the fieldwork data, enabled them understand these realities. Their research indicates conflict resolution is a prerequisite for continuous improvement of SOCs in both human and technological aspects. Failure to do so can lead to adverse effects, such as analyst burnout and reduction in overall effectiveness.},
	number = {3},
	journal = {IEEE Internet Computing},
	author = {Sundaramurthy, Sathya Chandran and Wesch, Michael and Ou, Xinming and McHugh, John and Rajagopalan, S. Raj and Bardas, Alexandru G.},
	month = may,
	year = {2017},
	note = {Number: 3
Conference Name: IEEE Internet Computing},
	keywords = {Computer security, Creativity, Human factors, Internet/Web technologies, Malware, SOC, Technological innovation, activity theory, security and privacy, security operation center, usable security},
	pages = {40--46},
}

@article{nyre-yu_observing_2019,
	title = {Observing {Cyber} {Security} {Incident} {Response}: {Qualitative} {Themes} {From} {Field} {Research}},
	volume = {63},
	issn = {2169-5067, 1071-1813},
	shorttitle = {Observing {Cyber} {Security} {Incident} {Response}},
	url = {http://journals.sagepub.com/doi/10.1177/1071181319631016},
	doi = {10.1177/1071181319631016},
	abstract = {Cyber security increasingly focuses on the challenges faced by network defenders. Cultural and security-driven sentiments about external observation, as well as publication concerns, limit the ability of researchers to understand the context surrounding incident response. Context awareness is crucial to inform design and engineering. Furthermore, these perspectives can be heavily influenced by the targeted sector or industry of the research. Together, a lack of broad contextual understanding may be biasing approaches to improving operations, and driving faulty assumptions in cyber teams. A qualitative field study was conducted in three computer security incident response teams (CSIRTs) and included perspectives of government, academia, and private sector teams. Themes emerged and provide insights across multiple aspects of incident response, including information sharing, organization, learning, and automation. The need to focus on vertical integration of issues at different levels of the incident response system is also discussed. Future research will build upon these results, using them to inform technology advancement in CSIR settings.},
	language = {en},
	number = {1},
	urldate = {2021-08-31},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Nyre-Yu, Megan and Gutzwiller, Robert S. and Caldwell, Barrett S.},
	month = nov,
	year = {2019},
	note = {Number: 1},
	pages = {437--441},
}

@article{chandran_turning_nodate,
	title = {Turning {Contradictions} into {Innovations} or: {How} {We} {Learned} to {Stop} {Whining} and {Improve} {Security} {Operations}},
	abstract = {Eﬀorts to improve the eﬃciency of security operation centers (SOCs) have emphasized building tools for analysts or understanding the human and organizational factors involved. The importance of viewing the viability of a solution from multiple perspectives has been largely ignored. Multiple perspectives arise because of inherent conﬂicts among the objectives a SOC has to meet and diﬀerences between the goals of the parties involved. During the 3.5 years that we have used anthropological ﬁeldwork methods to study SOCs, we discovered that successful SOC innovations must resolve these conﬂicts to be eﬀective in improving operational eﬃciency. This discovery was guided by Activity Theory (AT), which provided a framework for analyzing our ﬁeldwork data. We use the version of AT proposed by Engestr¨om to model SOC operations. Template analysis, a qualitative data analysis technique, guided by AT validated the existence of contradictions in SOCs. The same technique was used to elicit from the data concrete contradictions and how they were resolved. Our analysis provide evidence of the importance of conﬂict resolution as a prerequisite for operations improvement. AT enabled us to understand why some of our innovations worked in the SOCs we studied (and why others failed). AT helps us see a potentially successful and repeatable mechanism for introducing new technologies to future SOCs. Understanding and supporting all of the spoken and unspoken requirements of SOC analysts and managers appears to be the only way to get new technologies accepted and used in SOCs.},
	language = {en},
	author = {Chandran, Sathya and McHugh, John and Ou, Xinming},
	pages = {16},
}

@article{spring_human_nodate,
	title = {Human decision-making in computer security incident response},
	language = {en},
	author = {Spring, Jonathan Michael and London, University College},
	pages = {319},
}

@article{bettenburg_what_nodate,
	title = {What {Makes} a {Good} {Bug} {Report}?},
	abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to ﬁnd out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difﬁcult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information.},
	language = {en},
	author = {Bettenburg, Nicolas and Just, Sascha and Schröter, Adrian},
	pages = {11},
}

@inproceedings{rahaman_program_2017,
	title = {Program {Analysis} of {Cryptographic} {Implementations} for {Security}},
	doi = {10.1109/SecDev.2017.23},
	abstract = {Cryptographic implementation errors in popular open source libraries (e.g., OpenSSL, GnuTLS, BotanTLS, etc.) and the misuses of cryptographic primitives (e.g., as in Juniper Network) have been the major source of vulnerabilities in the wild. These serious problems prompt the need for new compile-time security checking. Such security enforcements demand the study of various cryptographic properties and their mapping into enforceable program analysis rules. We refer to this new security approach as cryptographic program analysis (CPA). In this paper, we show how cryptographic program analysis can be performed effectively andits security applications. Specifically, we systematically investigate different threat categories on various cryptographicimplementations and their usages. Then, we derive varioussecurity rules, which are enforceable by program analysistools during code compilation. We also demonstrate the capabilities of static taint analysis to enforce most of these security rules and provide a prototype implementation. We point out promising future research and development directions in this new area of cryptographic program analysis.},
	booktitle = {2017 {IEEE} {Cybersecurity} {Development} ({SecDev})},
	author = {Rahaman, Sazzadur and Yao, Danfeng},
	month = sep,
	year = {2017},
	keywords = {Ciphers, Cryptographic Program Analysis, Cryptography, Encryption, Libraries, Program Analysis, Security, Side-channel attacks, Tools},
	pages = {61--68},
}

@article{akhawe_alice_nodate,
	title = {Alice in {Warningland}: {A} {Large}-{Scale} {Field} {Study} of {Browser} {Security} {Warning} {Effectiveness}},
	abstract = {We empirically assess whether browser security warnings are as ineffective as suggested by popular opinion and previous literature. We used Mozilla Firefox and Google Chrome’s in-browser telemetry to observe over 25 million warning impressions in situ. During our ﬁeld study, users continued through a tenth of Mozilla Firefox’s malware and phishing warnings, a quarter of Google Chrome’s malware and phishing warnings, and a third of Mozilla Firefox’s SSL warnings. This demonstrates that security warnings can be effective in practice; security experts and system architects should not dismiss the goal of communicating security information to end users. We also ﬁnd that user behavior varies across warnings. In contrast to the other warnings, users continued through 70.2\% of Google Chrome’s SSL warnings. This indicates that the user experience of a warning can have a signiﬁcant impact on user behavior. Based on our ﬁndings, we make recommendations for warning designers and researchers.},
	language = {en},
	author = {Akhawe, Devdatta and Felt, Adrienne Porter},
	pages = {17},
}

@phdthesis{cetin_increasing_2021,
	title = {Increasing the {Impact} of {Voluntary} {Action} {Against} {Cybercrime}},
	url = {http://resolver.tudelft.nl/uuid:ad5d9147-b3ef-4708-b954-142b00820499},
	language = {en},
	urldate = {2021-08-25},
	school = {Delft University of Technology},
	author = {Çetin, F.O.},
	month = aug,
	year = {2021},
	doi = {10.4233/UUID:AD5D9147-B3EF-4708-B954-142B00820499},
	doi = {10.4233/UUID:AD5D9147-B3EF-4708-B954-142B00820499},
}

@inproceedings{schechter_emperors_2007,
	address = {Berkeley, CA},
	title = {The {Emperor}'s {New} {Security} {Indicators}},
	isbn = {978-0-7695-2848-9},
	url = {http://ieeexplore.ieee.org/document/4223213/},
	doi = {10.1109/SP.2007.35},
	abstract = {We evaluate website authentication measures that are designed to protect users from man-in-the-middle, ‘phishing’, and other site forgery attacks. We asked 67 bank customers to conduct common online banking tasks. Each time they logged in, we presented increasingly alarming clues that their connection was insecure. First, we removed HTTPS indicators. Next, we removed the participant’s site-authentication image—the customer-selected image that many websites now expect their users to verify before entering their passwords. Finally, we replaced the bank’s password-entry page with a warning page. After each clue, we determined whether participants entered their passwords or withheld them.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {2007 {IEEE} {Symposium} on {Security} and {Privacy} ({SP} '07)},
	publisher = {IEEE},
	author = {Schechter, Stuart E. and Dhamija, Rachna and Ozment, Andy and Fischer, Ian},
	month = may,
	year = {2007},
	pages = {51--65},
}

@article{wash_out_nodate,
	title = {Out of the {Loop}: {How} {Automated} {Software} {Updates} {Cause} {Unintended} {Security} {Consequences}},
	abstract = {When security updates are not installed, or installed slowly, end users are at an increased risk for harm. To improve security, software designers have endeavored to remove the user from the software update loop. However, user involvement in software updates remains necessary; not all updates are wanted, and required reboots can negatively impact users. We used a multi-method approach to collect interview, survey, and computer log data from 37 Windows 7 users. We compared what the users think is happening on their computers (interview and survey data), what users want to happen on their computer (interview and survey data), and what was actually going on (log data). We found that 28 out of our 37 participants had a misunderstanding about what was happening on their computer, and that over half of the participants could not execute their intentions for computer management.},
	language = {en},
	author = {Wash, Rick and Rader, Emilee and Vaniea, Kami and Rizor, Michelle},
	pages = {16},
}

@article{sunshine_crying_nodate,
	title = {Crying {Wolf}: {An} {Empirical} {Study} of {SSL} {Warning} {Eﬀectiveness}},
	abstract = {Web users are shown an invalid certiﬁcate warning when their browser cannot validate the identity of the websites they are visiting. While these warnings often appear in benign situations, they can also signal a man-in-the-middle attack. We conducted a survey of over 400 Internet users to examine their reactions to and understanding of current SSL warnings. We then designed two new warnings using warnings science principles and lessons learned from the survey. We evaluated warnings used in three popular web browsers and our two warnings in a 100participant, between-subjects laboratory study. Our warnings performed signiﬁcantly better than existing warnings, but far too many participants exhibited dangerous behavior in all warning conditions. Our results suggest that, while warnings can be improved, a better approach may be to minimize the use of SSL warnings altogether by blocking users from making unsafe connections and eliminating warnings in benign situations.},
	language = {en},
	author = {Sunshine, Joshua and Egelman, Serge and Almuhimedi, Hazim and Atri, Neha and Cranor, Lorrie Faith},
	pages = {34},
}

@article{cheung_effectiveness_nodate,
	title = {Effectiveness of {Cybersecurity} {Competitions}},
	abstract = {There has been a heightened interest among U.S. government agencies to fund cybersecurity workforce development. These efforts include offering universities funding for student scholarships, funding for building capacity in cybersecurity education, as well as sponsoring cybersecurity competitions, games, and outreach programs. This paper examines the effectiveness of cybersecurity competitions in educating students. Our study shows that though competitions do pique students’ interest, the effectiveness of this approach in producing more high quality professionals can be limited. One reason is that the knowledge barrier to compete in these competitions is high. To be successful, students have to be proﬁcient in operating systems, application services, software engineering, system administration and networking. Many Computer Science and Information Technology students do not feel qualiﬁed, and consequently this reduces participation from a wider student audience. Our approach takes aims at lowering this barrier to entry. We employ a hands-on learning methodology where students attend lectures on background knowledge on weekdays and practice what they learn in weekend workshops. A virtual networking environment is provided for students to practice network defense in the workshops and on their own time.},
	language = {en},
	author = {Cheung, Ronald S and Cohen, Joseph Paul and Lo, Henry Z and Elia, Fabio and Carrillo-Marquez, Veronica},
	pages = {5},
}

@article{fagan_why_nodate,
	title = {Why {Do} {They} {Do} {What} {They} {Do}?},
	abstract = {Usable security researchers have long been interested in what users do to keep their devices and data safe and how that compares to recommendations. Additionally, experts have long debated and studied the psychological underpinnings and motivations for users to do what they do, especially when such behavior is seen as risky, at least to experts. This study investigates user motivations through a survey conducted on Mechanical Turk, which resulted in responses from 290 participants. We use a rational decision model to guide our design, as well as current thought on human motivation in general and in the realm of computer security. Through quantitative and qualitative analysis, we identify key gaps in perception between those who follow common security advice (i.e., update software, use a password manager, use 2FA, change passwords) and those who do not and help explain participants’ motivations behind their decisions. Additionally, we ﬁnd that social considerations are trumped by individualized rationales.},
	language = {en},
	author = {Fagan, Michael and Khan, Mohammad Maiﬁ Hasan},
	pages = {18},
}

@article{cetin_understanding_2016,
	title = {Understanding the role of sender reputation in abuse reporting and cleanup},
	volume = {2},
	issn = {2057-2085, 2057-2093},
	url = {https://academic.oup.com/cybersecurity/article-lookup/doi/10.1093/cybsec/tyw005},
	doi = {10.1093/cybsec/tyw005},
	abstract = {Motivation: Participants on the front lines of abuse reporting have a variety of options to notify intermediaries and resource owners about abuse of their systems and services. These can include emails to personal messages to blacklists to machine-generated feeds. Recipients of these reports have to voluntarily act on this information. We know remarkably little about the factors that drive higher response rates to abuse reports. One such factor is the reputation of the sender. In this article, we present the ﬁrst randomized controlled experiment into sender reputation. We used a private datafeed of Asprox-infected websites to issue notiﬁcations from three senders with different reputations: an individual, a university and an established anti-malware organization.},
	language = {en},
	number = {1},
	urldate = {2021-08-24},
	journal = {Journal of Cybersecurity},
	author = {Çetin, Orçun and Hanif Jhaveri, Mohammad and Gañán, Carlos and van Eeten, Michel and Moore, Tyler},
	month = dec,
	year = {2016},
	note = {Number: 1},
	pages = {83--98},
}

@article{almuhimedi_your_nodate,
	title = {Your {Reputation} {Precedes} {You}: {History}, {Reputation}, and the {Chrome} {Malware} {Warning}},
	abstract = {Several web browsers, including Google Chrome and Mozilla Firefox, use malware warnings to stop people from visiting infectious websites. However, users can choose to click through (i.e., ignore) these malware warnings. In Google Chrome, users click through a ﬁfth of malware warnings on average. We investigate factors that may contribute to why people ignore such warnings. First, we examine ﬁeld data to see how browsing history aﬀects click-through rates. We ﬁnd that users consistently heed warnings about websites that they have not visited before. However, users respond unpredictably to warnings about websites that they have previously visited. On some days, users ignore more than half of warnings about websites they’ve visited in the past. Next, we present results of an online, survey-based experiment that we ran to gain more insight into the eﬀects of reputation on warning adherence. Participants said that they trusted high-reputation websites more than the warnings; however, their responses suggest that a notable minority of people could be swayed by providing more information. We provide recommendations for warning designers and pose open questions about the design of malware warnings.},
	language = {en},
	author = {Almuhimedi, Hazim and Felt, Adrienne Porter and Reeder, Robert W and Consolvo, Sunny},
	pages = {16},
}

@article{urban_notice_nodate,
	title = {Notice and {Takedown} in {Everyday} {Practice}},
	language = {en},
	author = {Urban, Jennifer M and Karaganis, Joe and Schofield, Brianna L},
	pages = {182},
}

@article{ukrop_will_2020,
	title = {Will {You} {Trust} {This} {TLS} {Certificate}?: {Perceptions} of {People} {Working} in {IT} ({Extended} {Version})},
	volume = {1},
	issn = {2692-1626, 2576-5337},
	shorttitle = {Will {You} {Trust} {This} {TLS} {Certificate}?},
	url = {https://dl.acm.org/doi/10.1145/3419472},
	doi = {10.1145/3419472},
	abstract = {Flawed TLS certificates are not uncommon on the Internet. While they signal a potential issue, in most cases they have benign causes (e.g., misconfiguration or even deliberate deployment). This adds fuzziness to the decision on whether to trust a connection or not. Little is known about perceptions of flawed certificates by IT professionals, even though their decisions impact high numbers of end users. Moreover, it is unclear how much the content of error messages and documentation influences these perceptions.
            To shed light on these issues, we observed 75 attendees of an industrial IT conference investigating different certificate validation errors. We also analyzed the influence of reworded error messages and redesigned documentation. We find that people working in IT have very nuanced opinions, with trust decisions being far from binary. The self-signed and the name-constrained certificates seem to be over-trusted (the latter also being poorly understood). We show that even small changes in existing error messages can positively influence resource use, comprehension, and trust assessment. At the end of the article, we summarize lessons learned from conducting usable security studies with IT professionals.},
	language = {en},
	number = {4},
	urldate = {2021-08-24},
	journal = {Digital Threats: Research and Practice},
	author = {Ukrop, Martin and Kraus, Lydia and Matyas, Vashek},
	month = dec,
	year = {2020},
	note = {Number: 4},
	pages = {1--29},
}

@article{zeng_fixing_nodate,
	title = {Fixing {HTTPS} {Misconﬁgurations} at {Scale}: {An} {Experiment} with {Security} {Notiﬁcations}},
	abstract = {HTTPS is vital to protecting the security and privacy of users on the Internet. As the cryptographic algorithms and standards underlying HTTPS evolve to meet emerging threats, website owners are responsible for updating and maintaining their HTTPS conﬁgurations. In practice, millions of hosts have misconﬁgured and insecure conﬁgurations. In addition to presenting security and privacy risks, misconﬁgurations can harm user experience on the web, when browsers show warnings for deprecated and outdated protocols.},
	language = {en},
	author = {Zeng, Eric and Li, Frank and Stark, Emily},
	pages = {19},
}

@article{vasek_malware_nodate,
	title = {Do malware reports expedite cleanup? {An} experimental study},
	abstract = {Web-based malware is pervasive. Miscreants compromise insecure hosts or even set up dedicated servers to distribute malware to unsuspecting users. This scourge is mainly fought by the voluntary action of private actors who detect and report infections to affected site owners, hosting providers and registrars. In this paper we describe an experiment to assess whether sending reports to affected parties makes a measurable difference in cleaning up malware. Using community reports of malware submitted to StopBadware over two months in Fall 2011, we ﬁnd evidence that detailed notices are immediately effective: 32\% of malware-distributing websites are cleaned within one day of sending a notice, compared to just 13\% of sites not receiving a notice. The improved cleanup rate holds for longer periods, too – 62\% of websites receiving a detailed notice were cleaned up after 16 days, compared to 45\% of websites not receiving a notice. It turns out that including details describing the compromise is essential for the notice to work – sending reports with minimal descriptions of the malware was found to be roughly as effective as not sending reports at all. Furthermore, we present evidence that sending multiple notices from two sources is not helpful. Instead, only the ﬁrst transmitted notice makes a difference.},
	language = {en},
	author = {Vasek, Marie and Moore, Tyler},
	pages = {8},
}

@inproceedings{stock_didnt_2018,
	address = {San Diego, CA},
	title = {Didn't {You} {Hear} {Me}? - {Towards} {More} {Successful} {Web} {Vulnerability} {Notifications}},
	isbn = {978-1-891562-49-5},
	shorttitle = {Didn't {You} {Hear} {Me}?},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01B-1_Stock_paper.pdf},
	doi = {10.14722/ndss.2018.23171},
	abstract = {After treating the notiﬁcation of vulnerable parties as mere side-notes in research, the security community has recently put more focus on how to conduct vulnerability disclosure at scale. The ﬁrst works in this area have shown that while notiﬁcations are helpful to a signiﬁcant fraction of operators, the vast majority of systems remain unpatched. In this paper, we build on these previous works, aiming to understand why the effects are not more signiﬁcant. To that end, we report on a notiﬁcation experiment targeting more than 24,000 domains, which allowed us to analyze what technical and human aspects are roadblocks to a successful campaign. As part of this experiment, we explored potential alternative notiﬁcation channels beyond email, including social media and phone. In addition, we conducted an anonymous survey with the notiﬁed operators, investigating their perspectives on our notiﬁcations. We show the pitfalls of email-based communications, such as the impact of anti-spam ﬁlters, the lack of trust by recipients, and the hesitation in ﬁxing vulnerabilities despite awareness. However, our exploration of alternative communication channels did not suggest a more promising medium. Seeing these results, we pinpoint future directions in improving security notiﬁcations.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Stock, Ben and Pellegrino, Giancarlo and Li, Frank and Backes, Michael and Rossow, Christian},
	year = {2018},
}

@article{stock_hey_nodate,
	title = {Hey, {You} {Have} a {Problem}: {On} the {Feasibility} of {Large}-{Scale} {Web} {Vulnerability} {Notiﬁcation}},
	abstract = {Large-scale discovery of thousands of vulnerable Web sites has become a frequent event, thanks to recent advances in security research and the rise in maturity of Internet-wide scanning tools. The issues related to disclosing the vulnerability information to the affected parties, however, have only been treated as a side note in prior research.},
	language = {en},
	author = {Stock, Ben and Pellegrino, Giancarlo and Rossow, Christian},
	pages = {19},
}

@inproceedings{li_remedying_2016,
	address = {Montréal Québec Canada},
	title = {Remedying {Web} {Hijacking}: {Notification} {Effectiveness} and {Webmaster} {Comprehension}},
	isbn = {978-1-4503-4143-1},
	shorttitle = {Remedying {Web} {Hijacking}},
	url = {https://dl.acm.org/doi/10.1145/2872427.2883039},
	doi = {10.1145/2872427.2883039},
	abstract = {As miscreants routinely hijack thousands of vulnerable web servers weekly for cheap hosting and trafﬁc acquisition, security services have turned to notiﬁcations both to alert webmasters of ongoing incidents as well as to expedite recovery. In this work we present the ﬁrst large-scale measurement study on the effectiveness of combinations of browser, search, and direct webmaster notiﬁcations at reducing the duration a site remains compromised. Our study captures the life cycle of 760,935 hijacking incidents from July, 2014–June, 2015, as identiﬁed by Google Safe Browsing and Search Quality. We observe that direct communication with webmasters increases the likelihood of cleanup by over 50\% and reduces infection lengths by at least 62\%. Absent this open channel for communication, we ﬁnd browser interstitials—while intended to alert visitors to potentially harmful content—correlate with faster remediation. As part of our study, we also explore whether webmasters exhibit the necessary technical expertise to address hijacking incidents. Based on appeal logs where webmasters alert Google that their site is no longer compromised, we ﬁnd 80\% of operators successfully clean up symptoms on their ﬁrst appeal. However, a sizeable fraction of site owners do not address the root cause of compromise, with over 12\% of sites falling victim to a new attack within 30 days. We distill these ﬁndings into a set of recommendations for improving web security and best practices for webmasters.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Li, Frank and Ho, Grant and Kuan, Eric and Niu, Yuan and Ballard, Lucas and Thomas, Kurt and Bursztein, Elie and Paxson, Vern},
	month = apr,
	year = {2016},
	pages = {1009--1019},
}

@article{li_youve_nodate,
	title = {You’ve {Got} {Vulnerability}: {Exploring} {Effective} {Vulnerability} {Notiﬁcations}},
	abstract = {Security researchers can send vulnerability notiﬁcations to take proactive measures in securing systems at scale. However, the factors affecting a notiﬁcation’s efﬁcacy have not been deeply explored. In this paper, we report on an extensive study of notifying thousands of parties of security issues present within their networks, with an aim of illuminating which fundamental aspects of notiﬁcations have the greatest impact on efﬁcacy. The vulnerabilities used to drive our study span a range of protocols and considerations: exposure of industrial control systems; apparent ﬁrewall omissions for IPv6-based services; and exploitation of local systems in DDoS ampliﬁcation attacks. We monitored vulnerable systems for several weeks to determine their rate of remediation. By comparing with experimental controls, we analyze the impact of a number of variables: choice of party to contact (WHOIS abuse contacts versus national CERTs versus US-CERT), message verbosity, hosting an information website linked to in the message, and translating the message into the notiﬁed party’s local language. We also assess the outcome of the emailing process itself (bounces, automated replies, human replies, silence) and characterize the sentiments and perspectives expressed in both the human replies and an optional anonymous survey that accompanied our notiﬁcations.},
	language = {en},
	author = {Li, Frank and Durumeric, Zakir and Czyz, Jakub and Karami, Mohammad and Bailey, Michael and McCoy, Damon and Savage, Stefan and Paxson, Vern},
	pages = {19},
}

@inproceedings{cetin_tell_2019,
	title = {Tell {Me} {You} {Fixed} {It}: {Evaluating} {Vulnerability} {Notifications} via {Quarantine} {Networks}},
	shorttitle = {Tell {Me} {You} {Fixed} {It}},
	doi = {10.1109/EuroSP.2019.00032},
	abstract = {Mechanisms for large-scale vulnerability notifications have been confronted with disappointing remediation rates. It has proven difficult to reach the relevant party and, once reached, to incentivize them to act. We present the first empirical study of a potentially more effective mechanism: quarantining the vulnerable resource until it is remediated. We have measured the remediation rates achieved by a medium-sized ISP for 1, 688 retail customers running open DNS resolvers or Multicast DNS services. These servers can be abused in UDP-based amplification attacks. We assess the effectiveness of quarantining by comparing remediation with two other groups: one group which was notified but not quarantined and another group where no action was taken. We find very high remediation rates for the quarantined users, 87\%, even though they can self-release from the quarantine environment. Of those who received the email-only notification, 76\% remediated. Surprisingly, over half of the customers who were not notified at all also remediated, though this is tied to the fact that many observations of vulnerable servers are transient. All in all, quarantining appears more effective than other notification and remediation mechanisms, but it is also clear that it can not be deployed as a general solution for Internet-wide notifications.},
	booktitle = {2019 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS} {P})},
	author = {Çetin, Orçun and Gañán, Carlos and Altena, Lisette and Tajalizadehkhoob, Samaneh and van Eeten, Michel},
	month = jun,
	year = {2019},
	keywords = {IP networks, Internet, Multicast protocols, Network security, Quarantine networks, Transport protocols, Vulnerability notifications},
	pages = {326--339},
}

@article{cetin_let_nodate,
	title = {Let {Me} {Out}! {Evaluating} the {Effectiveness} of {Quarantining} {Compromised} {Users} in {Walled} {Gardens}},
	abstract = {In the ﬁght to clean up malware-infected machines, notiﬁcations from Internet Service Providers (ISPs) to their customers play a crucial role. Since stand-alone notiﬁcations are routinely ignored, some ISPs have invested in a potentially more effective mechanism: quarantining customers in so-called walled gardens. We present the ﬁrst empirical study on user behavior and remediation effectiveness of quarantining infected machines in broadband networks. We analyzed 1, 736 quarantining actions involving 1, 208 retail customers of a medium-sized ISP in the period of April-October 2017. The ﬁrst two times they are quarantined, users can easily release themselves from the walled garden and around two-thirds of them use this option. Notwithstanding this easy way out, we ﬁnd that 71\% of these users have actually cleaned up the infection during their ﬁrst quarantine period and, of the recidivists, 48\% are cleaned after their second quarantining. Users who do not self-release either contact customer support (30\%) or are released automatically after 30 days (3\%). They have even higher cleanup rates. Reinfection rates are quite low and most users get quarantined only once. Users that remain infected spend less time in the walled garden during subsequent quarantining events, without a major drop in cleanup rates. This suggests there are positive learning effects, rather than mere habituation to being notiﬁed and self-releasing from the walled garden. In the communications with abuse and support staff, a fraction of quarantined users ask for additional help, request a paid technician, voice frustration about being cut off, or threaten to cancel their subscriptions. All in all, walled gardens seem to be a relatively effective and usable mechanism to improve the security of end users. We reﬂect on our main ﬁndings in terms of how to advance this industry best practice for botnet mitigation by ISPs.},
	language = {en},
	author = {Çetin, Orçun and Altena, Lisette and Gañán, Carlos and van Eeten, Michel},
	pages = {14},
}

@article{cetin_make_nodate,
	title = {Make {Notiﬁcations} {Great} {Again}: {Learning} {How} to {Notify} in the {Age} of {Large}-{Scale} {Vulnerability} {Scanning}},
	abstract = {As large-scale vulnerability detection becomes more feasible, it also increases the urgency to ﬁnd effective largescale notiﬁcation mechanisms to inform the affected parties. Researchers, CERTs, security companies and other organizations with vulnerability data have a variety of options to identify, contact and communicate with the actors responsible for the affected system or service. A lot of things can – and do – go wrong. It might be impossible to identify the appropriate recipient of the notiﬁcation, the message might not be trusted by the recipient, it might be overlooked or ignored or misunderstood. Such problems multiply as the volume of notiﬁcations increases. In this paper, we undertake several large-scale notiﬁcation campaigns for a vulnerable conﬁguration of authoritative nameservers. We investigate three issues: What is the most effective way to reach the affected parties? What communication path mobilizes the strongest incentive for remediation? And ﬁnally, what is the impact of providing recipients a mechanism to actively demonstrate the vulnerability for their own system, rather than sending them the standard static notiﬁcation message. We ﬁnd that retrieving contact information at scale is highly problematic, though there are different degrees of failure for different mechanisms. For those parties who are reached, notiﬁcation signiﬁcantly increases remediation rates. Reaching out to nameserver operators directly had better results than going via their customers, the domain owners. While the latter, in principle, have a stronger incentive to care and their request for remediation would trigger the commercial incentive of the operator to keep its customers happy, this communication path turned out to have slightly worse remediation rates. Finally, we ﬁnd no evidence that vulnerability demonstrations did better than static messages. In fact, few recipients engaged with the demonstration website.},
	language = {en},
	author = {Cetin, Orcun and Ganan, Carlos and Korczynski, Maciej and van Eeten, Michel},
	pages = {23},
}

@article{pearman_why_nodate,
	title = {Why people (don’t) use password managers effectively},
	abstract = {Security experts often recommend using passwordmanagement tools that both store passwords and generate random passwords. However, research indicates that only a small fraction of users use password managers with password generators. Past studies have explored factors in the adoption of password managers using surveys and online store reviews. Here we describe a semi-structured interview study with 30 participants that allows us to provide a more comprehensive picture of the mindsets underlying adoption and effective use of password managers and password-generation features. Our participants include users who use no password-speciﬁc tools at all, those who use password managers built into browsers or operating systems, and those who use separately installed password managers. Furthermore, past ﬁeld data has indicated that users of built-in, browser-based password managers more often use weak and reused passwords than users of separate password managers that have password generation available by default. Our interviews suggest that users of built-in password managers may be driven more by convenience, while users of separately installed tools appear more driven by security. We advocate tailored designs for these two mentalities and provide actionable suggestions to induce effective password manager usage.},
	language = {en},
	author = {Pearman, Sarah and Zhang, Shikun Aerin and Bauer, Lujo and Christin, Nicolas and Cranor, Lorrie Faith},
	pages = {21},
}

@article{ray_why_nodate,
	title = {Why {Older} {Adults} ({Don}’t) {Use} {Password} {Managers}},
	abstract = {Password managers (PMs) are considered highly effective tools for increasing security, and a recent study by Pearman et al. (SOUPS’19) highlighted the motivations and barriers to adopting PMs. We expand these ﬁndings by replicating Pearman et al.’s protocol and interview instrument applied to a sample of strictly older adults ({\textgreater}60 years of age), as the prior work focused on a predominantly younger cohort. We conducted n = 26 semi-structured interviews with PM users, built-in browser/operating system PM users, and nonPM users. The average participant age was 70.4 years. Using the same codebook from Pearman et al., we showcase differences and similarities in PM adoption between the samples, including fears of a single point of failure and the importance of having control over one’s private information. Meanwhile, older adults were found to have higher mistrust of cloud storage of passwords and cross-device synchronization. We also highlight PM adoption motivators for older adults, including the power of recommendations from family members and the importance of education and outreach to improve familiarity.},
	language = {en},
	author = {Ray, Hirak and Wolf, Flynn and Kuber, Ravi and Aviv, Adam J},
	pages = {18},
}

@inproceedings{tiefenau_usability_2019,
	address = {London United Kingdom},
	title = {A {Usability} {Evaluation} of {Let}'s {Encrypt} and {Certbot}: {Usable} {Security} {Done} {Right}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {A {Usability} {Evaluation} of {Let}'s {Encrypt} and {Certbot}},
	url = {https://dl.acm.org/doi/10.1145/3319535.3363220},
	doi = {10.1145/3319535.3363220},
	abstract = {The correct configuration of HTTPS is a complex set of tasks, which many administrators have struggled with in the past. Let’s Encrypt and Electronic Frontier Foundation’s Certbot aim to improve the TLS ecosystem by offering free trusted certificates (Let’s Encrypt) and by providing user-friendly support to configure and harden TLS (Certbot). Although adoption rates have increased, to date, there has been only a little scientific evidence of the actual usability and security benefits of this semi-automated approach. Therefore, we conducted a randomized control trial to evaluate the usability of Let’s Encrypt and Certbot in comparison to the traditional certificate authority approach. We performed a within-subjects lab study with 31 participants. The study sheds light on the security and usability enhancements that Let’s Encrypt and Certbot provide. We highlight how usability improvements aimed at administrators can have a large impact on security and discuss takeaways for Certbot and other security-related tasks that experts struggle with.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Tiefenau, Christian and von Zezschwitz, Emanuel and Häring, Maximilian and Krombholz, Katharina and Smith, Matthew},
	month = nov,
	year = {2019},
	pages = {1971--1988},
}

@inproceedings{bock_geneva_2019,
	address = {London United Kingdom},
	title = {Geneva: {Evolving} {Censorship} {Evasion} {Strategies}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {Geneva},
	url = {https://dl.acm.org/doi/10.1145/3319535.3363189},
	doi = {10.1145/3319535.3363189},
	abstract = {Researchers and censoring regimes have long engaged in a cat-andmouse game, leading to increasingly sophisticated Internet-scale censorship techniques and methods to evade them. In this paper, we take a drastic departure from the previously manual evadedetect cycle by developing techniques to automate the discovery of censorship evasion strategies. We present Geneva, a novel genetic algorithm that evolves packet-manipulation-based censorship evasion strategies against nation-state level censors. Geneva composes, mutates, and evolves sophisticated strategies out of four basic packet manipulation primitives (drop, tamper headers, duplicate, and fragment). With experiments performed both in-lab and against several real censors (in China, India, and Kazakhstan), we demonstrate that Geneva is able to quickly and independently re-derive most strategies from prior work, and derive novel subspecies and altogether new species of packet manipulation strategies. Moreover, Geneva discovers successful strategies that prior work posited were not effective, and evolves extinct strategies into newly working variants. We analyze the novel strategies Geneva creates to infer previously unknown behavior in censors. Geneva is a first step towards automating censorship evasion; to this end, we have made our code and data publicly available.},
	language = {en},
	urldate = {2021-08-24},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Bock, Kevin and Hughey, George and Qiang, Xiao and Levin, Dave},
	month = nov,
	year = {2019},
	pages = {2199--2214},
}

@article{erlenhov_empirical_2020,
	title = {An {Empirical} {Study} of {Bots} in {Software} {Development} -- {Characteristics} and {Challenges} from a {Practitioner}'s {Perspective}},
	url = {http://arxiv.org/abs/2005.13969},
	doi = {10.1145/3368089.3409680},
	abstract = {Software engineering bots – automated tools that handle tedious tasks – are increasingly used by industrial and open source projects to improve developer productivity. Current research in this area is held back by a lack of consensus of what software engineering bots (DevBots) actually are, what characteristics distinguish them from other tools, and what benefits and challenges are associated with DevBot usage. In this paper we report on a mixed-method empirical study of DevBot usage in industrial practice. We report on findings from interviewing 21 and surveying a total of 111 developers. We identify three different personas among DevBot users (focusing on autonomy, chat interfaces, and “smartness”), each with different definitions of what a DevBot is, why developers use them, and what they struggle with. We conclude that future DevBot research should situate their work within our framework, to clearly identify what type of bot the work targets, and what advantages practitioners can expect. Further, we find that there currently is a lack of generalpurpose “smart” bots that go beyond simple automation tools or chat interfaces. This is problematic, as we have seen that such bots, if available, can have a transformative effect on the projects that use them.},
	language = {en},
	urldate = {2021-08-24},
	journal = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	author = {Erlenhov, Linda and Neto, Francisco Gomes de Oliveira and Leitner, Philipp},
	month = nov,
	year = {2020},
	note = {arXiv: 2005.13969},
	keywords = {Computer Science - Software Engineering},
	pages = {445--455},
}

@article{utz_informed_2019,
	title = {({Un})informed {Consent}: {Studying} {GDPR} {Consent} {Notices} in the {Field}},
	shorttitle = {({Un})informed {Consent}},
	url = {http://arxiv.org/abs/1909.02638},
	doi = {10.1145/3319535.3354212},
	abstract = {Since the adoption of the General Data Protection Regulation (GDPR) in May 2018 more than 60 \% of popular websites in Europe display cookie consent notices to their visitors. This has quickly led to users becoming fatigued with privacy notifications and contributed to the rise of both browser extensions that block these banners and demands for a solution that bundles consent across multiple websites or in the browser. In this work, we identify common properties of the graphical user interface of consent notices and conduct three experiments with more than 80,000 unique users on a German website to investigate the influence of notice position, type of choice, and content framing on consent. We find that users are more likely to interact with a notice shown in the lower (left) part of the screen. Given a binary choice, more users are willing to accept tracking compared to mechanisms that require them to allow cookie use for each category or company individually. We also show that the widespread practice of nudging has a large effect on the choices users make. Our experiments show that seemingly small implementation decisions can substantially impact whether and how people interact with consent notices. Our findings demonstrate the importance for regulation to not just require consent, but also provide clear requirements or guidance for how this consent has to be obtained in order to ensure that users can make free and informed choices.},
	language = {en},
	urldate = {2021-08-24},
	journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Utz, Christine and Degeling, Martin and Fahl, Sascha and Schaub, Florian and Holz, Thorsten},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.02638},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	pages = {973--990},
}

@article{maass_effective_nodate,
	title = {Effective {Notiﬁcation} {Campaigns} on the {Web}: {A} {Matter} of {Trust}, {Framing}, and {Support}},
	abstract = {Misconﬁgurations and outdated software are a major cause of compromised websites and data leaks. Past research has proposed and evaluated sending automated security notiﬁcations to the operators of misconﬁgured websites, but encountered issues with reachability, mistrust, and a perceived lack of importance. In this paper, we seek to understand the determinants of effective notiﬁcations. We identify a data protection misconﬁguration that affects 12.7 \% of the 1.3 million websites we scanned and opens them up to legal liability. Using a subset of 4754 websites, we conduct a multivariate randomized controlled notiﬁcation experiment, evaluating contact medium, sender, and framing of the message. We also include a link to a public web-based self-service tool that is run by us in disguise and conduct an anonymous survey of the notiﬁed website owners (N=477) to understand their perspective.},
	language = {en},
	author = {Maass, Max and Stöver, Alina and Pridöhl, Henning and Bretthauer, Sebastian and Herrmann, Dominik and Hollick, Matthias and Spiecker, Indra},
	pages = {18},
}

@article{Liu2022MLPrivacy,
	title = {When {Machine} {Learning} {Meets} {Privacy}: {A} {Survey} and {Outlook}},
	abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
	journal = {ACM Computing Surveys},
	author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
	year = {2022},
}

@book{petroski_design_1994,
	title = {Design paradigms: {Case} histories of error and judgment in engineering},
	publisher = {Cambridge University Press},
	author = {Petroski, Henry},
	year = {1994},
	keywords = {DA\_NSFGRFP, IoTFailurePaper},
}

@inproceedings{dias_brief_2018,
	address = {Vasteras},
	title = {A {Brief} {Overview} of {Existing} {Tools} for {Testing} the {Internet}-of-{Things}},
	isbn = {978-1-5386-6352-3},
	url = {https://ieeexplore.ieee.org/document/8411738/},
	doi = {10.1109/ICSTW.2018.00035},
	abstract = {Systems are error-prone. Big systems have lots of errors. The Internet-of-Things poses us one of the biggest and widespread systems, where errors directly impact people’s lives. Testing and validating is how one deals with errors; but testing and validating a planetary-scale, heterogeneous, and evergrowing ecosystem has its own challenges and idiosyncrasies. As of today, the solutions available for testing these systems are insufﬁcient and fragmentary. In this paper we provide an overview on test approaches, tools and methodologies for the Internet-of-Things, its software and its devices. Our conclusion is that we are still lagging behind on the best practices and lessons learned from the Software Engineering community in the past decades.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	publisher = {IEEE},
	author = {Dias, Joao Pedro and Couto, Flavio and Paiva, Ana C.R. and Ferreira, Hugo Sereno},
	month = apr,
	year = {2018},
	keywords = {DA\_NSFGRFP},
	pages = {104--109},
}

@techreport{paing_quertci_2022,
	title = {{QuerTCI}: {A} {Tool} {Integrating} {GitHub} {Issue} {Querying} with {Comment} {Classification}},
	shorttitle = {{QuerTCI}},
	url = {http://arxiv.org/abs/2202.08761},
	abstract = {Issue tracking systems enable users and developers to comment on problems plaguing a software system. Empirical Software Engineering (ESE) researchers study (open-source) project issues and the comments and threads within to discover -- among others -- challenges developers face when, e.g., incorporating new technologies, platforms, and programming language constructs. However, issue discussion threads accumulate over time and thus can become unwieldy, hindering any insight that researchers may gain. While existing approaches alleviate this burden by classifying issue thread comments, there is a gap between searching popular open-source software repositories (e.g., those on GitHub) for issues containing particular keywords and feeding the results into a classification model. In this paper, we demonstrate a research infrastructure tool called QuerTCI that bridges this gap by integrating the GitHub issue comment search API with the classification models found in existing approaches. Using queries, ESE researchers can retrieve GitHub issues containing particular keywords, e.g., those related to a certain programming language construct, and subsequently classify the kinds of discussions occurring in those issues. Using our tool, our hope is that ESE researchers can uncover challenges related to particular technologies using certain keywords through popular open-source repositories more seamlessly than previously possible. A tool demonstration video may be found at: https://youtu.be/fADKSxn0QUk.},
	number = {arXiv:2202.08761},
	urldate = {2022-06-11},
	institution = {arXiv},
	author = {Paing, Ye and Vélez, Tatiana Castro and Khatchadourian, Raffi},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.08761},
	note = {arXiv:2202.08761 [cs]
type: article},
	keywords = {Computer Science - Software Engineering},
}

@misc{leong_consensual_2022,
	title = {Consensual {Software}},
	copyright = {MIT},
	url = {https://github.com/consensualsoftware/consensual_software},
	abstract = {the source code for the consensual software website},
	urldate = {2022-06-11},
	publisher = {Consensual Software},
	author = {Leong, Danielle},
	month = may,
	year = {2022},
	note = {original-date: 2017-05-24T05:33:32Z},
}

@article{kumari_requirements_2010,
	title = {Requirements {Analysis} for {Privacy} in {Social} {Networks}},
	abstract = {The rise and growth of social networks can be seen as empowering the user to change website’s content by posting information using minimal technical knowledge. But, this empowerment has resulted in loads of sensitive data being let out unprotected in the public domain. To ensure user privacy, we need to understand privacy requirements relevant to social networks, per se. In this paper, we address this problem. We identify all the major stakeholders and their assets. We also look into the various aspects of user data that these stakeholders can be interested in. We then show how interests of various stakeholders can conﬂict and become threats for them. To counter these threats, we present a set of system requirements mapped to the respective privacy requirements.},
	language = {en},
	author = {Kumari, Prachi},
	year = {2010},
	pages = {13},
}

@article{ivaturi_taxonomy_2011,
	title = {A {Taxonomy} for {Social} {Engineering} attacks},
	abstract = {As the technology to secure information improves, hackers will employ less technical means to get access to unauthorized data. The use of Social Engineering as a non tech method of hacking has been increasingly used during the past few years. There are different types of social engineering methods reported but what is lacking is a unifying effort to understand these methods in the aggregate. This paper aims to classify these methods through taxonomy so that organizations can gain a better understanding of these attack methods and accordingly be vigilant against them.},
	language = {en},
	author = {Ivaturi, Koteswara and Janczewski, Lech},
	year = {2011},
	pages = {12},
}

@article{rasmussen1997risk,
	title = {Risk management in a dynamic society: a modelling problem},
	volume = {27},
	number = {2-3},
	journal = {Safety science},
	author = {Rasmussen, Jens},
	year = {1997},
	note = {Publisher: Elsevier},
	pages = {183--213},
}

@article{hudson1994tripod,
	title = {Tripod {Delta}: {Proactive} approach to enhanced safety},
	volume = {46},
	number = {01},
	journal = {Journal of petroleum technology},
	author = {Hudson, PTW and Reason, JT and Bentley, PD and Primrose, M},
	year = {1994},
	note = {Publisher: OnePetro},
	pages = {58--62},
}

@book{reason2016managing,
	title = {Managing the risks of organizational accidents},
	publisher = {Routledge},
	author = {Reason, James},
	year = {2016},
}

@book{leveson2016engineering,
	title = {Engineering a safer world: {Systems} thinking applied to safety},
	publisher = {The MIT Press},
	author = {Leveson, Nancy G},
	year = {2016},
}

@book{norman2013design,
	title = {The design of everyday things: {Revised} and expanded edition},
	publisher = {Basic books},
	author = {Norman, Don},
	year = {2013},
}

@inproceedings{ji_model-reuse_2018,
	address = {Toronto Canada},
	title = {Model-{Reuse} {Attacks} on {Deep} {Learning} {Systems}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243757},
	doi = {10.1145/3243734.3243757},
	abstract = {Many of today’s machine learning (ML) systems are built by reusing an array of, often pre-trained, primitive models, each fulfilling distinct functionality (e.g., feature extraction). The increasing use of primitive models significantly simplifies and expedites the development cycles of ML systems. Yet, because most of such models are contributed and maintained by untrusted sources, their lack of standardization or regulation entails profound security implications, about which little is known thus far.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ji, Yujie and Zhang, Xinyang and Ji, Shouling and Luo, Xiapu and Wang, Ting},
	month = oct,
	year = {2018},
	pages = {349--363},
}

@techreport{Szegedy2014IntriguingPropertiesofNN,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	number = {arXiv:1312.6199},
	institution = {arXiv},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{zhang_adversarial_2020,
	title = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}: {A} {Survey}},
	volume = {11},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Adversarial {Attacks} on {Deep}-learning {Models} in {Natural} {Language} {Processing}},
	url = {https://dl.acm.org/doi/10.1145/3374217},
	doi = {10.1145/3374217},
	abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named
              adversarial examples
              . These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
	language = {en},
	number = {3},
	urldate = {2022-06-10},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
	month = jun,
	year = {2020},
	pages = {1--41},
}

@article{sigfridsson_mixing_2008,
	title = {Mixing research methods to unveil work practices of dispersed {Open} {Source} communities: lessons learned from the {PyPy} study},
	shorttitle = {Mixing research methods to unveil work practices of dispersed {Open} {Source} communities},
	abstract = {Open Source software development projects are based on collaboration within dispersed, multifaceted, and volunteer-based communities. Studying the work practices of such a community requires adherence to a plethora of methodological approaches and employment of several different research methods. In this paper we present a study of an Open Source community called PyPy. The focus of the paper is on the evolution of the study itself and how we have utilized several different research methods - including participant observation, virtual ethnography, and electronic questionnaires - to unveil the work practices of this community. We will conclude by discussing the issues we have experienced whilst doing this and some relevant lessons learned regarding studying Open Source communities.},
	author = {Sigfridsson, Anders and Sheehan, Anne and Avram, Gabriela},
	month = jan,
	year = {2008},
}

@article{Hu2021BlockChainBasedPubliceEcosystemforAuditingSecurityofSWApplications,
	title = {Blockchain-based public ecosystem for auditing security of software applications},
	journal = {Computing},
	author = {Hu, Qinwen and Asghar, Muhammad Rizwan and Zeadally, Sherali},
	year = {2021},
}

@techreport{Taylor2020SpellBound,
	title = {{SpellBound}: {Defending} {Against} {Package} {Typosquatting}},
	url = {http://arxiv.org/abs/2003.03471},
	abstract = {Package managers for software repositories based on a single programming language are very common. Examples include npm (JavaScript), and PyPI (Python). These tools encourage code reuse, making it trivial for developers to import external packages. Unfortunately, repositories' size and the ease with which packages can be published facilitates the practice of typosquatting: the uploading of a package with name similar to that of a highly popular package, typically with the aim of capturing some of the popular package's installs. Typosquatting has serious negative implications, resulting in developers importing malicious packages, or -- as we show -- code clones which do not incorporate recent security updates. In order to tackle this problem, we present SpellBound, a tool for identifying and reporting potentially erroneous imports to developers. SpellBound implements a novel typosquatting detection technique, based on an in-depth analysis of npm and PyPI. Our technique leverages a model of lexical similarity between names, and further incorporates the notion of package popularity. This approach flags cases where unknown/scarcely used packages would be installed in place of popular ones with similar names, before installation occurs. We evaluated SpellBound on both npm and PyPI, with encouraging results: SpellBound flags typosquatting cases while generating limited warnings (0.5\% of total package installs), and low overhead (only 2.5\% of package install time). Furthermore, SpellBound allowed us to confirm known cases of typosquatting and discover one high-profile, unknown case of typosquatting that resulted in a package takedown by the npm security team.},
	number = {arXiv:2003.03471},
	institution = {arXiv},
	author = {Taylor, Matthew and Vaidya, Ruturaj K. and Davidson, Drew and De Carli, Lorenzo and Rastogi, Vaibhav},
	year = {2020},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{combe_docker_2016,
	title = {To {Docker} or {Not} to {Docker}: {A} {Security} {Perspective}},
	volume = {3},
	issn = {2325-6095},
	shorttitle = {To {Docker} or {Not} to {Docker}},
	doi = {10.1109/MCC.2016.100},
	abstract = {The need for ever-shorter development cycles, continuous delivery, and cost savings in cloud-based infrastructures led to the rise of containers, which are more flexible than virtual machines and provide near-native performance. Among all container solutions, Docker, a complete packaging and software delivery tool, currently leads the market. This article gives an overview of the container ecosystem and discusses the Docker environment's security implications through realistic use cases. The authors define an adversary model, point out several vulnerabilities affecting current Docker usage, and discuss further research directions.},
	number = {5},
	journal = {IEEE Cloud Computing},
	author = {Combe, Theo and Martin, Antony and Di Pietro, Roberto},
	month = sep,
	year = {2016},
	note = {Conference Name: IEEE Cloud Computing},
	keywords = {Cloud computing, Computer security, Containers, Cost benefit analysis, Docker, Linux, Product life cycle management, Virtual networks, cloud computing, containers, security, virtualization},
	pages = {54--62},
}

@inproceedings{nadeem_automatic_2021,
	title = {Automatic {Issue} {Classifier}: {A} {Transfer} {Learning} {Framework} for {Classifying} {Issue} {Reports}},
	shorttitle = {Automatic {Issue} {Classifier}},
	url = {http://arxiv.org/abs/2202.06149},
	doi = {10.1109/ISSREW53611.2021.00113},
	abstract = {Issue tracking systems are used in the software industry for the facilitation of maintenance activities that keep the software robust and up to date with ever-changing industry requirements. Usually, users report issues that can be categorized into different labels such as bug reports, enhancement requests, and questions related to the software. Most of the issue tracking systems make the labelling of these issue reports optional for the issue submitter, which leads to a large number of unlabeled issue reports. In this paper, we present a state-of-the-art method to classify the issue reports into their respective categories i.e. bug, enhancement, and question. This is a challenging task because of the common use of informal language in the issue reports. Existing studies use traditional natural language processing approaches adopting key-word based features, which fail to incorporate the contextual relationship between words and therefore result in a high rate of false positives and false negatives. Moreover, previous works utilize a uni-label approach to classify the issue reports however, in reality, an issue-submitter can tag one issue report with more than one label at a time. This paper presents our approach to classify the issue reports in a multi-label setting. We use an off-the-shelf neural network called RoBERTa and fine-tune it to classify the issue reports. We validate our approach on issue reports belonging to numerous industrial projects from GitHub. We were able to achieve promising F-1 scores of 81\%, 74\%, and 80\% for bug reports, enhancements, and questions, respectively. We also develop an industry tool called Automatic Issue Classifier (AIC), which automatically assigns labels to newly reported issues on GitHub repositories with high accuracy.},
	urldate = {2022-06-09},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Nadeem, Anas and Sarwar, Muhammad Usman and Malik, Muhammad Zubair},
	month = oct,
	year = {2021},
	note = {arXiv:2202.06149 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	pages = {421--426},
}

@article{heck_framework_2017,
	title = {A framework for quality assessment of just-in-time requirements: the case of open source feature requests},
	volume = {22},
	issn = {1432-010X},
	shorttitle = {A framework for quality assessment of just-in-time requirements},
	url = {https://doi.org/10.1007/s00766-016-0247-5},
	doi = {10.1007/s00766-016-0247-5},
	abstract = {Until now, quality assessment of requirements has focused on traditional up-front requirements. Contrasting these traditional requirements are just-in-time (JIT) requirements, which are by definition incomplete, not specific and might be ambiguous when initially specified, indicating a different notion of "correctness." We analyze how the assessment of JIT requirements quality should be performed based on the literature of traditional and JIT requirements. Based on that analysis, we have designed a quality framework for JIT requirements and instantiated it for feature requests in open source projects. We also indicate how the framework can be instantiated for other types of JIT requirements. We have performed an initial evaluation of our framework for feature requests with eight practitioners from the Dutch agile community, receiving overall positive feedback. Subsequently, we have used our framework to assess 550 feature requests originating from three Open Source Software systems (Netbeans, ArgoUML and Mylyn Tasks). In doing so, we obtain a view on the feature request quality for the three open source projects. The value of our framework is threefold: (1) it gives an overview of quality criteria that are applicable to feature requests (at creation time or JIT); (2) it serves as a structured basis for teams that need to assess the quality of their JIT requirements; and (3) it provides a way to get an insight into the quality of JIT requirements in existing projects.},
	language = {en},
	number = {4},
	urldate = {2022-06-09},
	journal = {Requirements Engineering},
	author = {Heck, Petra and Zaidman, Andy},
	month = nov,
	year = {2017},
	keywords = {Feature request, Just-in-time requirement, Open source, Quality assessment, Quality framework},
	pages = {453--473},
}

@inproceedings{razavian_cnn_2014,
	title = {{CNN} {Features} {Off}-the-{Shelf}: {An} {Astounding} {Baseline} for {Recognition}},
	shorttitle = {{CNN} {Features} {Off}-the-{Shelf}},
	doi = {10.1109/CVPRW.2014.131},
	abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
	month = jun,
	year = {2014},
	note = {ISSN: 2160-7516},
	keywords = {Birds, Feature extraction, Image recognition, Support vector machines, Training, Vectors, Visualization},
	pages = {512--519},
}

@article{ralph_sensemaking-coevolution-implementation_2015,
	series = {Towards general theories of software engineering},
	title = {The {Sensemaking}-{Coevolution}-{Implementation} {Theory} of software design},
	volume = {101},
	issn = {0167-6423},
	url = {https://www.sciencedirect.com/science/article/pii/S0167642314005395},
	doi = {10.1016/j.scico.2014.11.007},
	abstract = {Following calls for greater theory development in software engineering, this paper formulates a process theory of software development practice. Sensemaking-Coevolution-Implementation Theory explains how complex software systems are created by cohesive software development teams in organizations. It posits that an independent agent (the development team) creates a software system by alternating between three categories of activities: making sense of an ambiguous context, mutually refining schemas of the context and design space, and manifesting their understanding of the design space in a technological artifact. This theory development paper defines, illustrates and conceptually evaluates Sensemaking-Coevolution-Implementation Theory. It grounds the theory's concepts and relationships in existing software engineering, information systems development and interdisciplinary design literature.},
	language = {en},
	urldate = {2022-06-09},
	journal = {Science of Computer Programming},
	author = {Ralph, Paul},
	month = apr,
	year = {2015},
	keywords = {Coevolution, Design, General theory, Process theory, Theory development},
	pages = {21--41},
}

@techreport{Kurita2020WeightPoisoningPTM,
	title = {Weight {Poisoning} {Attacks} on {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2004.06660},
	abstract = {Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.},
	institution = {arXiv},
	author = {Kurita, Keita and Michel, Paul and Neubig, Graham},
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bi_first_2021,
	title = {A {First} {Look} at {Accessibility} {Issues} in {Popular} {GitHub} {Projects}},
	doi = {10.1109/ICSME52107.2021.00041},
	abstract = {Accessibility design elements allow people to access software products and services independent of their different abilities. However, accessibility is challenging to handle and whether accessibility is widely considered in software projects is unclear. In this work, we aim to understand if accessibility is a prevalent consideration in practice, what accessibility issues are discussed in GitHub projects, what potential reasons cause accessibility issues, and what solutions (e.g., tools and standards) are applied for addressing accessibility issues. In this work, we collect 11,820 accessibility issues and their threads discussed by developers in popular GitHub projects. We manually analyzed and grouped the collected accessibility issues into seven categories. The results of our study uncover that accessibility is widely discussed in general projects, and the potential reasons that cause accessibility issues are because developers are not aware of the importance of accessibility and they lack knowledge about accessibility concerns, standards, and existing tools. Our results and findings can enhance and improve developers' knowledge and awareness when they conduct accessibility-relevant design or incorporate accessibility elements into their projects.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Bi, Tingting and Xia, Xin and Lo, David and Aleti, Aldeida},
	month = sep,
	year = {2021},
	note = {ISSN: 2576-3148},
	keywords = {Accessibility Issues, Conferences, Empirical Study, Mining Repository, Software development management, Software maintenance, Standards, Tools},
	pages = {390--401},
}

@inproceedings{knauss_vissuelizer_2013,
	title = {V:{Issue}:lizer: {Exploring} requirements clarification in online communication over time},
	shorttitle = {V},
	doi = {10.1109/ICSE.2013.6606709},
	abstract = {This demo introduces V:ISSUE:LIZER, a tool for exploring online communication and analyzing clarification of requirements over time. V:Issue:lizer supports managers and developers to identify requirements with insufficient shared understanding, to analyze communication problems, and to identify developers that are knowledgeable about domain or project related issues through visualizations. Our preliminary evaluation shows that V:Issue:lizer offers managers valuable information for their decision making. (Demo video: http://youtu.be/Oy3xvzjy3BQ).},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Knauss, Eric and Damian, Daniela},
	month = may,
	year = {2013},
	note = {ISSN: 1558-1225},
	keywords = {Collaboration, Data visualization, Knowledge engineering, Social network services, Software, Trajectory, Visualization, communication of requirements, distributed requirements engineering, requirements clarification patterns},
	pages = {1327--1330},
}

@article{biggio_poisoning_nodate,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM’s test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM’s decision function due to malicious input and use this ability to construct malicious data.},
	language = {en},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	pages = {8},
}

@inproceedings{mashiyat_using_2014,
	address = {New York, NY, USA},
	series = {{RSSE} 2014},
	title = {Using developer conversations to resolve uncertainty in software development: a position paper},
	isbn = {978-1-4503-2845-6},
	shorttitle = {Using developer conversations to resolve uncertainty in software development},
	url = {https://doi.org/10.1145/2593822.2593823},
	doi = {10.1145/2593822.2593823},
	abstract = {Software development is a social process: tasks such as implementing a requirement or fixing a bug typically spark conversations between the stakeholders of a software project, where they identify points of uncertainty in the solution space and explore proposals to resolve them. Due to the fluid nature of these interactions, it is hard for project managers to maintain an overall understanding of the state of the discussion and to know when and how to intervene. We propose an approach for extracting the uncertainty information from developer conversations in order to provide managers with analytics. Using these allows us to recommend specific actions that managers can take to better facilitate the resolution of uncertainty.},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Recommendation} {Systems} for {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Mashiyat, Ahmed Shah and Famelis, Michalis and Salay, Rick and Chechik, Marsha},
	month = jun,
	year = {2014},
	keywords = {Uncertainty management, natural language processing},
	pages = {1--5},
}

@article{chen_survey_2016,
	title = {A survey on the use of topic models when mining software repositories},
	volume = {21},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-015-9402-8},
	doi = {10.1007/s10664-015-9402-8},
	abstract = {Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
	language = {en},
	number = {5},
	urldate = {2022-06-09},
	journal = {Empirical Software Engineering},
	author = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
	month = oct,
	year = {2016},
	keywords = {LDA, LSI, Survey, Topic modeling},
	pages = {1843--1919},
}

@techreport{hosseini_deceiving_2017,
	title = {Deceiving {Google}'s {Cloud} {Video} {Intelligence} {API} {Built} for {Summarizing} {Videos}},
	url = {http://arxiv.org/abs/1703.09793},
	abstract = {Despite the rapid progress of the techniques for image classification, video annotation has remained a challenging task. Automated video annotation would be a breakthrough technology, enabling users to search within the videos. Recently, Google introduced the Cloud Video Intelligence API for video analysis. As per the website, the system can be used to "separate signal from noise, by retrieving relevant information at the video, shot or per frame" level. A demonstration website has been also launched, which allows anyone to select a video for annotation. The API then detects the video labels (objects within the video) as well as shot labels (description of the video events over time). In this paper, we examine the usability of the Google's Cloud Video Intelligence API in adversarial environments. In particular, we investigate whether an adversary can subtly manipulate a video in such a way that the API will return only the adversary-desired labels. For this, we select an image, which is different from the video content, and insert it, periodically and at a very low rate, into the video. We found that if we insert one image every two seconds, the API is deceived into annotating the video as if it only contained the inserted image. Note that the modification to the video is hardly noticeable as, for instance, for a typical frame rate of 25, we insert only one image per 50 video frames. We also found that, by inserting one image per second, all the shot labels returned by the API are related to the inserted image. We perform the experiments on the sample videos provided by the API demonstration website and show that our attack is successful with different videos and images.},
	number = {arXiv:1703.09793},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Hosseini, Hossein and Xiao, Baicen and Poovendran, Radha},
	month = mar,
	year = {2017},
	note = {arXiv:1703.09793 [cs]
version: 2
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{biggio_poisoning_nodate-1,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM’s test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM’s decision function due to malicious input and use this ability to construct malicious data.},
	language = {en},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	pages = {8},
}

@inproceedings{viviani_structure_2018,
	title = {The {Structure} of {Software} {Design} {Discussions}},
	abstract = {As a software system is iteratively developed, software developers engage in many discussions, often through written forms. Some of these discussions occur on pull requests and include information about the design of the system to which the code in the pull request is being contributed. Although previous work has shown that design is discussed in forums like pull requests, little is known about the form and content of the discussion about design. In this paper, we report on an in-depth analysis of three pull requests to better understand the form and content of design information in pull request discussions to enable the development of tools to help humans access this information.},
	booktitle = {2018 {IEEE}/{ACM} 11th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	author = {Viviani, Giovanni and Janik-Jones, Calahan and Famelis, Michalis and Murphy, Gail},
	month = may,
	year = {2018},
	note = {ISSN: 2574-1837},
	keywords = {Conferences, Encoding, Rails, Security, Software design, Tools, design, empirical, natural language},
	pages = {104--107},
}

@techreport{khalajzadeh_how_2022,
	title = {How are {Diverse} {End}-user {Human}-centric {Issues} {Discussed} on {GitHub}?},
	url = {http://arxiv.org/abs/2201.05927},
	abstract = {Many software systems fail to meet the needs of the diverse end-users in society and are prone to pose problems, such as accessibility and usability issues. Some of these problems (partially) stem from the failure to consider the characteristics, limitations, and abilities of diverse end-users during software development. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues encountered by developers. In-depth knowledge of these human-centric issues is needed to design software systems that better meet their diverse end-users' needs. This paper aims to provide insights for the software development and research communities on which human-centric issues are a topic of discussion for developers on GitHub. We conducted an empirical study by extracting and manually analysing 1,691 issue comments from 12 diverse projects, ranging from small to large-scale projects, including projects designed for challenged end-users, e.g., visually impaired and dyslexic users. Our analysis shows that eight categories of human-centric issues are discussed by developers. These include Inclusiveness, Privacy \& Security, Compatibility, Location \& Language, Preference, Satisfaction, Emotional Aspects, and Accessibility. Guided by our findings, we highlight some implications and possible future paths to further understand and incorporate human-centric issues in software development to be able to design software that meets the needs of diverse end users in society.},
	number = {arXiv:2201.05927},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Khalajzadeh, Hourieh and Shahin, Mojtaba and Obie, Humphrey O. and Grundy, John},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.05927},
	note = {arXiv:2201.05927 [cs]
type: article},
	keywords = {Computer Science - Software Engineering},
}

@article{pfefferkorn_content-oblivious_2022,
	title = {Content-{Oblivious} {Trust} and {Safety} {Techniques}: {Results} from a {Survey} of {Online} {Service} {Providers}},
	volume = {1},
	copyright = {Copyright (c) 2022 Journal of Online Trust and Safety},
	issn = {2770-3142},
	shorttitle = {Content-{Oblivious} {Trust} and {Safety} {Techniques}},
	url = {https://tsjournal.org/index.php/jots/article/view/14},
	doi = {10.54501/jots.v1i2.14},
	abstract = {We present the results of a survey about the trust and safety techniques of a group of online service providers that collectively serve billions of users. We classify techniques that require the provider to be able to access the contents of users’ files and communications at will as content dependent, and content oblivious otherwise. We find that more providers use abuse-reporting features (which are content oblivious) than other abuse-detection techniques, but that participants’ abuse-reporting tools do not consistently cover the types of abuse that users may encounter. We also find that, despite strong consensus among participating providers that automated content scanning (which is content dependent) is the most useful means of detecting child sex abuse imagery, they do not consider it to be nearly as useful for other kinds of abuse. These results indicate that content-dependent techniques do not constitute a silver bullet to protect users against abuse. They also demonstrate that the impact of end-to-end encryption (which, controversially, impedes outside access to user content) on abuse detection may vary by abuse type. These findings have implications for policy debates over the regulation of online service providers’ anti-abuse obligations and their use of end-to-end encryption.},
	language = {en},
	number = {2},
	urldate = {2022-06-07},
	journal = {Journal of Online Trust and Safety},
	author = {Pfefferkorn, Riana},
	month = feb,
	year = {2022},
	note = {Number: 2},
	keywords = {trust and safety},
}

@article{bentahar_taxonomy_2010,
	title = {A taxonomy of argumentation models used for knowledge representation},
	volume = {33},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-010-9154-1},
	doi = {10.1007/s10462-010-9154-1},
	abstract = {Understanding argumentation and its role in human reasoning has been a continuous subject of investigation for scholars from the ancient Greek philosophers to current researchers in philosophy, logic and artificial intelligence. In recent years, argumentation models have been used in different areas such as knowledge representation, explanation, proof elaboration, commonsense reasoning, logic programming, legal reasoning, decision making, and negotiation. However, these models address quite specific needs and there is need for a conceptual framework that would organize and compare existing argumentation-based models and methods. Such a framework would be very useful especially for researchers and practitioners who want to select appropriate argumentation models or techniques to be incorporated in new software systems with argumentation capabilities. In this paper, we propose such a conceptual framework, based on taxonomy of the most important argumentation models, approaches and systems found in the literature. This framework highlights the similarities and differences between these argumentation models. As an illustration of the practical use of this framework, we present a case study which shows how we used this framework to select and enrich an argumentation model in a knowledge acquisition project which aimed at representing argumentative knowledge contained in texts critiquing military courses of action.},
	language = {en},
	number = {3},
	urldate = {2022-06-07},
	journal = {Artificial Intelligence Review},
	author = {Bentahar, Jamal and Moulin, Bernard and Bélanger, Micheline},
	month = mar,
	year = {2010},
	keywords = {Argumentation models, Argumentation theory, Courses of action, Knowledge representation},
	pages = {211--259},
}

@inproceedings{veselsky_establishing_2022,
	title = {Establishing {Trust} in {Vehicle}-to-{Vehicle} {Coordination}: {A} {Sensor} {Fusion} {Approach}},
	abstract = {Autonomous vehicles (AVs) use diverse sensors to understand their surroundings as they continually make safetycritical decisions. However, establishing trust with other AVs is a key prerequisite because safety-critical decisions cannot be made based on data shared from untrusted sources. Existing protocols require an infrastructure network connection and a third-party root of trust to establish a secure channel, which are not always available.},
	language = {en},
	author = {Veselsky, Jakob and West, Jack and Isaac, Ahlgren and Goel, Abhinav and Jiang, Wenxin and Lee, Kyuin and Kim, Younghyun and Davis, James C. and Thiruvathukal, George and Klingensmith, Neil},
	year = {2022},
	pages = {6},
}

@misc{xu_qiang_replication_2022,
	title = {Replication package for paper "{An} {Empirical} {Study} on the {Impact} of {Deep} {Parameters} on {Mobile} {App} {Energy} {Usage}" ({SANER} 2022)},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/5823364},
	abstract = {Improving software performance through conﬁguration parameter tuning is a common activity during software maintenance. Beyond traditional performance metrics like latency, mobile app developers are interested in reducing app energy usage. Some mobile apps have centralized locations for parameter tuning, similar to databases and operating systems, but it is common for mobile apps to have hundreds of parameters scattered around the source code. The correlation between these “deep” parameters and app energy usage is unclear. Researchers have studied the energy effects of deep parameters in speciﬁc modules, but we lack a systematic understanding of the energy impact of mobile deep parameters.},
	urldate = {2022-06-03},
	publisher = {Zenodo},
	author = {Xu, Qiang and Davis, James C. and Hu, Y. Charlie and Jindal, Abhilash},
	month = jan,
	year = {2022},
	doi = {10.5281/ZENODO.5823364},
	note = {Language: en},
	keywords = {Android, configuration, deep parameter, energy consumption, mobile app},
}

@techreport{higuera_software_1996,
	title = {Software {Risk} {Management}.},
	url = {https://apps.dtic.mil/sti/citations/ADA310913},
	abstract = {This paper presents a holistic vision of the risk-based methodologies for Software Risk Management SRM developed at the Software Engineering Institute SEI. SRM methodologies address the entire life cycle of software acquisition, development, and maintenance. This paper is driven by the premise that the ultimate efficacy of the developed methodologies and tools for software engineering is to buy smarter, manage more effectively, identify opportunities for continuous improvement, use available information and databases more efficiently, improve industry, raise the communitys playing field, and review and evaluate progress. The methodologies are based on seven management principles shared product vision, teamwork, global perspective, forward-looking view, open communication, integrated management, and continuous process.},
	language = {en},
	urldate = {2022-06-03},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
	author = {Higuera, Ronald P. and Haimes, Yacov Y.},
	month = jun,
	year = {1996},
	note = {Section: Technical Reports},
}

@inproceedings{cheng_anyone_2017,
	address = {New York, NY, USA},
	series = {{CSCW} '17},
	title = {Anyone {Can} {Become} a {Troll}: {Causes} of {Trolling} {Behavior} in {Online} {Discussions}},
	isbn = {978-1-4503-4335-0},
	shorttitle = {Anyone {Can} {Become} a {Troll}},
	url = {https://doi.org/10.1145/2998181.2998213},
	doi = {10.1145/2998181.2998213},
	abstract = {In online communities, antisocial behavior such as trolling disrupts constructive discussion. While prior work suggests that trolling behavior is confined to a vocal and antisocial minority, we demonstrate that ordinary people can engage in such behavior as well. We propose two primary trigger mechanisms: the individual's mood, and the surrounding context of a discussion (e.g., exposure to prior trolling behavior). Through an experiment simulating an online discussion, we find that both negative mood and seeing troll posts by others significantly increases the probability of a user trolling, and together double this probability. To support and extend these results, we study how these same mechanisms play out in the wild via a data-driven, longitudinal analysis of a large online news discussion community. This analysis exposes temporal mood effects, and explores long range patterns of repeated exposure to trolling. A predictive model of trolling behavior reveals that mood and discussion context together can explain trolling behavior better than an individual's history of trolling. These results combine to suggest that ordinary people can, under the right circumstances, behave like trolls.},
	urldate = {2022-06-02},
	booktitle = {Proceedings of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Justin and Bernstein, Michael and Danescu-Niculescu-Mizil, Cristian and Leskovec, Jure},
	month = feb,
	year = {2017},
	keywords = {antisocial behavior, online communities, trolling},
	pages = {1217--1230},
}

@inproceedings{stol2016grounded,
	title = {Grounded theory in software engineering research: a critical review and guidelines},
	booktitle = {Proceedings of the 38th international conference on software engineering},
	author = {Stol, Klaas-Jan and Ralph, Paul and Fitzgerald, Brian},
	year = {2016},
	pages = {120--131},
}

@inproceedings{sung2022settle,
	title = {How to settle the {ReDoS} problem: {Back} to the classical automata theory},
	booktitle = {Implementation and application of automata: 26th international conference, {CIAA} 2022, rouen, france, june 28–{July} 1, 2022, proceedings},
	author = {Sung, Sicheol and Cheon, Hyunjoon and Han, Yo-Sub},
	note = {tex.organization: Springer Nature},
	pages = {34},
}

@misc{moffat_risk_2022,
	title = {The {Risk} {Landscape} {\textbar} {Risk}-{First}},
	url = {https://riskfirst.org//risks/Risk-Landscape},
	abstract = {A way to think about the risks you face on a software project.},
	language = {en},
	urldate = {2022-05-31},
	journal = {Risk-First Software Development},
	author = {Moffat, Rob},
	year = {2022},
}

@article{raval_risk_2017,
	title = {Risk {Landscape} of {Autonomous} {Cars}},
	volume = {56},
	issn = {0736-6981},
	url = {https://doi.org/10.1080/07366981.2017.1355099},
	doi = {10.1080/07366981.2017.1355099},
	abstract = {This article presents a study of how automobile risks are changing over time as the level of automation shifts from the traditional to semi-autonomous to fully autonomous car. The study involves the concept of comparative advantage between the human driver and the driverless car to determine how the roles between the two have been changing and, in turn, how the change affects attendant risks. To develop a risk landscape across changes in levels of automation, the roles of human driver and driverless car are matched with the five levels of automation used to describe the phases of development of the driverless car. We conclude that the costs associated with fatal accidents will decrease sharply over time as the driverless cars are deployed on the road. And we offer implications for policy, regulation, and insurance based on our analysis of the changing risk landscape.},
	number = {3},
	urldate = {2022-05-31},
	journal = {EDPACS},
	author = {Raval, Vasant and Dentlinger, Michael J.},
	month = sep,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07366981.2017.1355099},
	pages = {1--18},
}

@article{oehmen_risk_2020,
	title = {Risk {Management} in {Product} {Development}: {Risk} {Identification}, {Assessment}, and {Mitigation} - {A} {Literature} {Review}},
	volume = {1},
	issn = {2633-7762},
	shorttitle = {{RISK} {MANAGEMENT} {IN} {PRODUCT} {DEVELOPMENT}},
	url = {https://www.cambridge.org/core/journals/proceedings-of-the-design-society-design-conference/article/risk-management-in-product-development-risk-identification-assessment-and-mitigation-a-literature-review/737BD10A6D55576D5A813BE37D36A22E#},
	doi = {10.1017/dsd.2020.27},
	abstract = {This paper reviews the literature on risk management practices and methods in product design and development. Based on an expert workshop by the Risk Management Processes and Methods in Design Special Interest Group within the Design Society and literature review, three key areas are discussed: risk identification, assessment, and mitigation. In each area, researchers have described practices that are used in product development organizations, proposed new methods to support risk management processes and decision-making, and generated evidence to evaluate the effectiveness of these activities.},
	language = {en},
	urldate = {2022-05-11},
	journal = {Proceedings of the Design Society: DESIGN Conference},
	author = {Oehmen, J. and Guenther, A. and Herrmann, J. W. and Schulte, J. and Willumsen, P.},
	month = may,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	keywords = {design management, literature review, product development, risk management, uncertainty},
	pages = {657--666},
}

@inproceedings{cramer_draft_2023,
	title = {[{DRAFT}] {An} {Empirical} {Study} of {Trust} \& {Safety} {Risks} in {Social} {Media} {Platforms}},
	shorttitle = {T\&{S} {Risks} in {SMPs}},
	author = {Cramer, Geoffrey},
	month = may,
	year = {2023},
}

@inproceedings{hasan_why_2014,
	address = {Doha, Qatar},
	title = {Why are {You} {Taking} this {Stance}? {Identifying} and {Classifying} {Reasons} in {Ideological} {Debates}},
	shorttitle = {Why are {You} {Taking} this {Stance}?},
	url = {https://aclanthology.org/D14-1083},
	doi = {10.3115/v1/D14-1083},
	urldate = {2022-05-27},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hasan, Kazi Saidul and Ng, Vincent},
	month = oct,
	year = {2014},
	pages = {751--762},
}

@inproceedings{boltuzic_identifying_2015,
	address = {Denver, CO},
	title = {Identifying {Prominent} {Arguments} in {Online} {Debates} {Using} {Semantic} {Textual} {Similarity}},
	url = {https://aclanthology.org/W15-0514},
	doi = {10.3115/v1/W15-0514},
	urldate = {2022-05-27},
	booktitle = {Proceedings of the 2nd {Workshop} on {Argumentation} {Mining}},
	publisher = {Association for Computational Linguistics},
	author = {Boltužić, Filip and Šnajder, Jan},
	month = jun,
	year = {2015},
	pages = {110--115},
}

@article{wang_how_2022,
	title = {How {Do} {Open} {Source} {Software} {Contributors} {Perceive} and {Address} {Usability}?: {Valued} {Factors}, {Practices}, and {Challenges}},
	volume = {39},
	issn = {1937-4194},
	shorttitle = {How {Do} {Open} {Source} {Software} {Contributors} {Perceive} and {Address} {Usability}?},
	doi = {10.1109/MS.2020.3009514},
	abstract = {Given the recent changes in the open source software (OSS) landscape, we examined OSS contributors’ current valued factors, practices, and challenges concerning usability. Our survey provides insights for OSS practitioners and tool designers to promote a user-centric mindset and improve usability practice in OSS communities.},
	number = {1},
	journal = {IEEE Software},
	author = {Wang, Wenting and Cheng, Jinghui and Guo, Jin L.C.},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Software},
	keywords = {Graphical user interfaces, Guidelines, Open source software, Usability},
	pages = {76--83},
}

@inproceedings{cheng_how_2018,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '18},
	title = {How {Do} the {Open} {Source} {Communities} {Address} {Usability} and {UX} {Issues}? {An} {Exploratory} {Study}},
	isbn = {978-1-4503-5621-3},
	shorttitle = {How {Do} the {Open} {Source} {Communities} {Address} {Usability} and {UX} {Issues}?},
	url = {https://doi.org/10.1145/3170427.3188467},
	doi = {10.1145/3170427.3188467},
	abstract = {Usability and user experience (UX) issues are often not well emphasized and addressed in open source software (OSS) development. There is an imperative need for supporting OSS communities to collaboratively identify, understand, and fix UX design issues in a distributed environment. In this paper, we provide an initial step towards this effort and report on an exploratory study that investigated how the OSS communities currently reported, discussed, negotiated, and eventually addressed usability and UX issues. We conducted in-depth qualitative analysis of selected issue tracking threads from three OSS projects hosted on GitHub. Our findings indicated that discussions about usability and UX issues in OSS communities were largely influenced by the personal opinions and experiences of the participants. Moreover, the characteristics of the community may have greatly affected the focus of such discussion.},
	urldate = {2022-05-26},
	booktitle = {Extended {Abstracts} of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Jinghui and Guo, Jin L.C.},
	month = apr,
	year = {2018},
	keywords = {issue tracking, open source community, open source software development, usability, user experience},
	pages = {1--6},
}

@inproceedings{hellman_facilitating_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {Facilitating {Asynchronous} {Participatory} {Design} of {Open} {Source} {Software}: {Bringing} {End} {Users} into the {Loop}},
	isbn = {978-1-4503-8095-9},
	shorttitle = {Facilitating {Asynchronous} {Participatory} {Design} of {Open} {Source} {Software}},
	url = {https://doi.org/10.1145/3411763.3451643},
	doi = {10.1145/3411763.3451643},
	abstract = {As open source software (OSS) becomes increasingly mature and popular, there are significant challenges with properly accounting for usability concerns for the diverse end users. Participatory design, where multiple stakeholders collaborate on iterating the design, can be an efficient way to address the usability concerns for OSS projects. However, barriers such as a code-centric mindset and insufficient tool support often prevent OSS teams from effectively including end users in participatory design methods. This paper proposes preliminary contributions to this problem through the user-centered exploration of (1) a set of design guidelines that capture the needs of OSS participatory design tools, (2) two personas that represent the characteristics of OSS designers and end users, and (3) a low-fidelity prototype tool for end user involvement in OSS projects. This work paves the road for future studies about tool design that would eventually help improve OSS usability.},
	urldate = {2022-05-26},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hellman, Jazlyn and Cheng, Jinghui and Guo, Jin L.C.},
	month = may,
	year = {2021},
	keywords = {asynchronous collaboration, open source software, participatory design, usability},
	pages = {1--7},
}

@inproceedings{gill_privacy_2011,
	address = {New York, NY, USA},
	series = {{CHI} '11},
	title = {Privacy dictionary: a linguistic taxonomy of privacy for content analysis},
	isbn = {978-1-4503-0228-9},
	shorttitle = {Privacy dictionary},
	url = {https://doi.org/10.1145/1978942.1979421},
	doi = {10.1145/1978942.1979421},
	abstract = {Privacy is frequently a key concern relating to technology and central to HCI research, yet it is notoriously difficult to study in a naturalistic way. In this paper we describe and evaluate a dictionary of privacy designed for content analysis, derived using prototype theory and informed by traditional theoretical approaches to privacy. We evaluate our dictionary categories alongside privacy-related categories from an existing content analysis tool, LIWC, using verbal discussions of privacy issues from a variety of technology and non-technology contexts. We find that our privacy dictionary is better able to distinguish between privacy and non-privacy language, and is less context-dependent than LIWC. However, the more general LIWC categories are able to describe a greater amount of variation in our data. We discuss possible improvements to the privacy dictionary and note future work.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gill, Alastair J. and Vasalou, Asimina and Papoutsi, Chrysanthi and Joinson, Adam N.},
	month = may,
	year = {2011},
	keywords = {content analysis, language, privacy, privacy dictionary},
	pages = {3227--3236},
}

@inproceedings{rastkar_summarizing_2010,
	title = {Summarizing software artifacts: a case study of bug reports},
	volume = {1},
	shorttitle = {Summarizing software artifacts},
	doi = {10.1145/1806799.1806872},
	abstract = {Many software artifacts are created, maintained and evolved as part of a software development project. As software developers work on a project, they interact with existing project artifacts, performing such activities as reading previously filed bug reports in search of duplicate reports. These activities often require a developer to peruse a substantial amount of text. In this paper, we investigate whether it is possible to summarize software artifacts automatically and effectively so that developers could consult smaller summaries instead of entire artifacts. To provide focus to our investigation, we consider the generation of summaries for bug reports. We found that existing conversation-based generators can produce better results than random generators and that a generator trained specifically on bug reports can perform statistically better than existing conversation-based generators. We demonstrate that humans also find these generated summaries reasonable indicating that summaries might be used effectively for many tasks.},
	booktitle = {2010 {ACM}/{IEEE} 32nd {International} {Conference} on {Software} {Engineering}},
	author = {Rastkar, Sarah and Murphy, Gail C. and Murray, Gabriel},
	month = may,
	year = {2010},
	note = {ISSN: 1558-1225},
	keywords = {Computer bugs, Electromagnetic compatibility, Electronic mail, Humans, Programming, Software, Training, human-centric software engineering, machine learning},
	pages = {505--514},
}

@inproceedings{ortu_mining_2018,
	address = {New York, NY, USA},
	series = {{PROMISE}'18},
	title = {Mining {Communication} {Patterns} in {Software} {Development}: {A} {GitHub} {Analysis}},
	isbn = {978-1-4503-6593-2},
	shorttitle = {Mining {Communication} {Patterns} in {Software} {Development}},
	url = {https://doi.org/10.1145/3273934.3273943},
	doi = {10.1145/3273934.3273943},
	abstract = {Background: Studies related to human factors in software engineering are providing insightful information on the emotional state of contributors and the impact this has on the code. The open source software development paradigm involves different roles, and previous studies about emotions in software development have not taken into account what different roles might play when people express their feelings. Aim: We present an analysis of issues and commits on five GitHub projects distinguishing contributors between users and developers, and between one-commit and multi-commit developers. Method: We analyzed more than 650K comments from 130K issues of 64K contributors. We calculated emotions (love, joy, anger, sadness) and politeness of the comments related to the issues of the considered projects and introduced the definition of contributor fan-in and fan-out. Results: Results show that users and developers communicate differently as well as multi-commit developers and one-commit developers do. Conclusions: We provide empirical evidence that one-commit developers are more active and more polite in posting comments. Multi-commit developers are less active in posting comments, and while commenting, they are less polite than when commented.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ortu, Marco and Hall, Tracy and Marchesi, Michele and Tonelli, Roberto and Bowes, David and Destefanis, Giuseppe},
	month = oct,
	year = {2018},
	keywords = {data analytics, human factors, software engineering},
	pages = {70--79},
}

@inproceedings{ding_entity-level_2018,
	address = {New York, NY, USA},
	series = {{SEmotion} '18},
	title = {Entity-level sentiment analysis of issue comments},
	isbn = {978-1-4503-5751-7},
	url = {https://doi.org/10.1145/3194932.3194935},
	doi = {10.1145/3194932.3194935},
	abstract = {Emotions and sentiment of software developers can largely influence the software productivity and quality. However, existing work on emotion mining and sentiment analysis is still in the early stage in software engineering in terms of accuracy, the size of datasets used and the specificity of the analysis. In this work, we are concerned with conducting entity-level sentiment analysis. We first build a manually labeled dataset containing 3,000 issue comments selected from 231,732 issue comments collected from 10 open source projects in GitHub. Then we design and develop SentiSW, an entity-level sentiment analysis tool consisting of sentiment classification and entity recognition, which can classify issue comments into {\textless}sentiment, entity{\textgreater} tuples. We evaluate the sentiment classification using ten-fold cross validation, and it achieves 68.71\% mean precision, 63.98\% mean recall and 77.19\% accuracy, which is significantly higher than existing tools. We evaluate the entity recognition by manually annotation and it achieves a 75.15\% accuracy.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Jin and Sun, Hailong and Wang, Xu and Liu, Xudong},
	month = jun,
	year = {2018},
	keywords = {entity recognition, entity-level sentiment analysis, open source software project, sentiment classification},
	pages = {7--13},
}

@inproceedings{arya_analysis_2019,
	address = {Montreal, Quebec, Canada},
	series = {{ICSE} '19},
	title = {Analysis and detection of information types of open source software issue discussions},
	url = {https://doi.org/10.1109/ICSE.2019.00058},
	doi = {10.1109/ICSE.2019.00058},
	abstract = {Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Arya, Deeksha and Wang, Wenting and Guo, Jin L. C. and Cheng, Jinghui},
	month = may,
	year = {2019},
	keywords = {collaborative software engineering, issue discussion analysis, issue tracking system},
	pages = {454--464},
}

@inproceedings{destefanis_measuring_2018,
	address = {New York, NY, USA},
	series = {{SEmotion} '18},
	title = {On measuring affects of github issues' commenters},
	isbn = {978-1-4503-5751-7},
	url = {https://doi.org/10.1145/3194932.3194936},
	doi = {10.1145/3194932.3194936},
	abstract = {In this study, we analyzed issues and comments on GitHub projects and built collaboration networks dividing contributors into two categories: users and commenters. We identified as commenters those users who only post comments without posting any issues nor committing changes in the source code. Since previous studies showed that there is a link between a positive environment (regarding affectiveness) and productivity, our goal was to investigate commenters' contribution to the project concerning affectiveness. We analyzed more than 370K comments from 100K issues of 25K contributors from 3 open source projects. We then calculated and compared the affectiveness of the issues' comments written by users and commenters in terms of sentiment, politeness, and emotions. We provide empirical evidence that commenters are less polite, less positive and in general they express a lower level of emotions in their comments than users. Our results also confirm that GitHub's contributors consist of different groups which behave differently, and this provides useful information for future studies in the field.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Destefanis, Giuseppe and Ortu, Marco and Bowes, David and Marchesi, Michele and Tonelli, Roberto},
	month = jun,
	year = {2018},
	keywords = {human factors, mining software repositories, software engineering},
	pages = {14--19},
}

@inproceedings{imtiaz_sentiment_2018,
	address = {New York, NY, USA},
	series = {{SEmotion} '18},
	title = {Sentiment and politeness analysis tools on developer discussions are unreliable, but so are people},
	isbn = {978-1-4503-5751-7},
	url = {https://doi.org/10.1145/3194932.3194938},
	doi = {10.1145/3194932.3194938},
	abstract = {Many software engineering researchers use sentiment and politeness analysis tools to study the emotional environment within collaborative software development. However, papers that use these tools rarely establish their reliability. In this paper, we evaluate popular existing tools for sentiment and politeness detection over a dataset of 589 manually rated GitHub comments that represent developer discussions. We also develop a coding scheme on how to quantify politeness for conversational texts found on collaborative platforms. We find that not only do the tools have a low agreement with human ratings on sentiment and politeness, human raters also have a low agreement among themselves.},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Imtiaz, Nasif and Middleton, Justin and Girouard, Peter and Murphy-Hill, Emerson},
	month = jun,
	year = {2018},
	keywords = {affect analysis, developer discussion, github, politeness, sentiment},
	pages = {55--61},
}

@inproceedings{ortu_empirical_2019,
	title = {Empirical {Analysis} of {Affect} of {Merged} {Issues} on {GitHub}},
	doi = {10.1109/SEmotion.2019.00017},
	abstract = {Pull-request based workflows are popular trends of modern software development platform such as GitHub. A pull-request notifies other developers that new changes are proposed, a code review process follows the pull-request that may be merged in the main branch if other developers accept the changes. Many factors influence the acceptance of pull-requests. Since open source software is based on collaboration, it is essential to discover how the affect expressed by developer discussing pull-request issues, namely how they collaborate, influences the acceptance of the pull-request proposed. In this study we analysed the relations with the affect expressed in pull-request issues'comments and whether an issue is merged in the main branch or not. We focused on pull-request issues and we found that issues with higher level anger, sadness, arousal and valence are less likely to be merged while issues with higher level of valence, joy are more likely to be merged. Positive affect indicates a good collaboration environment, and our finding shows that this healthy collaboration is likely to increase the acceptance of pull-requests.},
	booktitle = {2019 {IEEE}/{ACM} 4th {International} {Workshop} on {Emotion} {Awareness} in {Software} {Engineering} ({SEmotion})},
	author = {Ortu, Marco and Marchesi, Michele and Tonelli, Roberto},
	month = may,
	year = {2019},
	keywords = {Collaboration, Conferences, Data mining, Open source software, Productivity, Software engineering, human aspect, software engineering},
	pages = {46--48},
}

@inproceedings{cramer_draft_2023-1,
	title = {[{DRAFT}] {An} {Empirical} {Study} of {Trust} \& {Safety} {Risks} in {Social} {Media} {Platforms}},
	shorttitle = {T\&{S} {Risks} in {SMPs}},
	author = {Cramer, Geoffrey},
	month = may,
	year = {2023},
}

@misc{wikipedia_plan--check-act_2022,
	title = {Plan-do-check-act},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=PDCA&oldid=1089529583},
	abstract = {PDCA (plan–do–check–act or plan–do–check–adjust) is an iterative design and management method used in business for the control and continual improvement of processes and products. It is also known as the Deming circle/cycle/wheel, the Shewhart cycle, the control circle/cycle, or plan–do–study–act (PDSA). Another version of this PDCA cycle is OPDCA. The added "O" stands for observation or as some versions say: "Observe the current condition." This emphasis on observation and current condition has currency with the literature on lean manufacturing and the Toyota Production System. The PDCA cycle, with Ishikawa's changes, can be traced back to S. Mizuno of the Tokyo Institute of Technology in 1959.},
	language = {en},
	urldate = {2022-05-24},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = may,
	year = {2022},
	note = {Page Version ID: 1089529583},
}

@inproceedings{tsay_influence_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {Influence of social and technical factors for evaluating contribution in {GitHub}},
	isbn = {978-1-4503-2756-5},
	url = {https://doi.org/10.1145/2568225.2568315},
	doi = {10.1145/2568225.2568315},
	abstract = {Open source software is commonly portrayed as a meritocracy, where decisions are based solely on their technical merit. However, literature on open source suggests a complex social structure underlying the meritocracy. Social work environments such as GitHub make the relationships between users and between users and work artifacts transparent. This transparency enables developers to better use information such as technical value and social connections when making work decisions. We present a study on open source software contribution in GitHub that focuses on the task of evaluating pull requests, which are one of the primary methods for contributing code in GitHub. We analyzed the association of various technical and social measures with the likelihood of contribution acceptance. We found that project managers made use of information signaling both good technical contribution practices for a pull request and the strength of the social connection between the submitter and project manager when evaluating pull requests. Pull requests with many comments were much less likely to be accepted, moderated by the submitter's prior interaction in the project. Well-established projects were more conservative in accepting pull requests. These findings provide evidence that developers use both technical and social information when evaluating potential contributions to open source software projects.},
	urldate = {2022-05-24},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James},
	month = may,
	year = {2014},
	keywords = {GitHub, contribution, open source, signaling theory, social computing, social media, transparency},
	pages = {356--366},
}

@article{stromer-galley_measuring_2020,
	title = {Measuring {Deliberation}’s {Content}: {A} {Coding} {Scheme}},
	volume = {3},
	issn = {2634-0488},
	shorttitle = {Measuring {Deliberation}’s {Content}},
	url = {https://delibdemjournal.org/article/id/331/},
	doi = {10.16997/jdd.50},
	abstract = {This paper details a content analysis scheme to measure the quality of political deliberation in face-to-face and online groups. Much of deliberation research studies the outcomes of deliberation, but there has been a lack of analysis of what groups actually do when tasked with deliberating. The coding scheme was developed out of the theoretical literature on deliberation and further enhanced by the empirical literature on small groups, deliberation, online political talk, and conversation analysis. Strict standards for creating coding schemes were followed to ensure a valid and reliable coding process. Results of the coding of deliberations on the topic of public schools suggest that participants produced a fairly high level of reasoned opinion expression, but not necessarily on the topic which they were asked to deliberate. It is hoped that the code scheme can be utilized by practitioners and researchers of political and social deliberations.},
	language = {en},
	number = {1},
	urldate = {2022-05-24},
	journal = {Journal of Deliberative Democracy},
	author = {Stromer-Galley, Jennifer},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: University of Westminster Press},
}

@article{giles_microanalysis_2015,
	title = {Microanalysis {Of} {Online} {Data}: {The} methodological development of “digital {CA}”},
	volume = {7},
	issn = {2211-6958},
	shorttitle = {Microanalysis {Of} {Online} {Data}},
	url = {https://www.sciencedirect.com/science/article/pii/S2211695814000464},
	doi = {10.1016/j.dcm.2014.12.002},
	abstract = {This paper introduces the work of the MOOD (Microanalysis Of Online Data) network, an interdisciplinary association of academic researchers exploring ways of conducting close qualitative analyses of online interaction. Despite the fact that much online interaction meets the criteria for ‘conversation’, conversation analysis (CA) has only recently begun to grow and flourish as a methodology for analysing the overwhelming quantity of material that in many cases sits in archive form, visible to millions, on the Internet. We discuss the development of methods that are inherently suited for subjecting online interaction to the kind of rigorous analysis that conversation analysts have applied to talk of all kinds for several decades. We go on to explore the fundamental challenges that online data pose for CA, the value of many CA techniques for online analysis, and the possibilities of developing bespoke modes of analysis that are crafted for use with specific forms of online data (e.g. ‘tweets’ on Twitter).},
	language = {en},
	urldate = {2022-05-24},
	journal = {Discourse, Context \& Media},
	author = {Giles, David and Stommel, Wyke and Paulus, Trena and Lester, Jessica and Reed, Darren},
	month = mar,
	year = {2015},
	keywords = {Computer-mediated communication, Conversation analysis, Digital media, Online discussion, Online interaction},
	pages = {45--51},
}

@article{meredith_analysing_2017,
	title = {Analysing technological affordances of online interactions using conversation analysis},
	volume = {115},
	issn = {0378-2166},
	url = {https://www.sciencedirect.com/science/article/pii/S0378216617301558},
	doi = {10.1016/j.pragma.2017.03.001},
	abstract = {The use of conversation analysis (CA) as a method for analysing the interactional practices of online communication has been growing in recent years (Giles et al., 2015). A key challenge for analysing online communication is the varied platforms through which interaction can occur. This paper demonstrates how using CA and the concept of affordances (Hutchby, 2001) can provide a lens through which to analyse not only the interaction, but also the technological context of that interaction. A corpus of instant messaging chats, captured from Facebook chat using screen-capture software, is used as a case study to demonstrate how the concept of affordances can be used alongside CA analysis to address the role of technology in the interaction. Two key interactional practices – turn adjacency and openings – are analysed to show the insights that CA can offer for providing an in-depth analysis of online interaction. By using affordances as a lens through which CA analysis can be refracted, scholars using ‘digital CA’ can better develop an understanding of patterns of interaction across different interactional platforms.},
	language = {en},
	urldate = {2022-05-24},
	journal = {Journal of Pragmatics},
	author = {Meredith, Joanne},
	month = jul,
	year = {2017},
	keywords = {Conversation analysis, Instant messaging, Online interaction, Screen-capture, Technological affordances},
	pages = {42--55},
}

@article{lewinski_developing_2019,
	title = {Developing {Methods} {That} {Facilitate} {Coding} and {Analysis} of {Synchronous} {Conversations} via {Virtual} {Environments}},
	volume = {18},
	issn = {1609-4069},
	url = {https://doi.org/10.1177/1609406919842443},
	doi = {10.1177/1609406919842443},
	abstract = {Programs via the Internet are uniquely positioned to capture qualitative data. One reason is because the Internet facilitates the creation of a community of similar individuals who can exchange information and support related to living with a chronic illness. Synchronous conversations via the Internet can provide insight into real-time social interaction and the exchange of social support. One way to analyze interactions among individuals is by using qualitative methods such as content, conversation, or discourse analysis. This manuscript describes how we used content analysis with aspects from conversation and discourse analysis to analyze synchronous conversations via the Internet to describe what individuals talk about and how individuals talk in an Internet-mediated interaction. With the increase in Internet interventions that facilitate collection of real-time conversational data, this article provides insight into how combining qualitative methods can facilitate the coding and analysis of these complex data.},
	language = {en},
	urldate = {2022-05-24},
	journal = {International Journal of Qualitative Methods},
	author = {Lewinski, Allison A. and Anderson, Ruth A. and Vorderstrasse, Allison A. and Johnson, Constance M.},
	month = jan,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	keywords = {Internet, content analysis, qualitative analysis, qualitative research methods, synchronous conversations, virtual environments},
	pages = {1609406919842443},
}

@article{basili_software_1992,
	title = {Software {Modeling} and {Measurement}: {The} {Goal}/{Question}/{Metric} {Paradigm}},
	shorttitle = {Software {Modeling} and {Measurement}},
	url = {https://drum.lib.umd.edu/handle/1903/7538},
	language = {en\_US},
	urldate = {2022-05-24},
	author = {Basili, Victor R.},
	month = sep,
	year = {1992},
	note = {Accepted: 2008-02-13T17:12:42Z},
}

@article{carroll_incident_1995,
	title = {Incident {Reviews} in {High}-{Hazard} {Industries}: {Sense} {Making} and {Learning} {Under} {Ambiguity} and {Accountability}},
	volume = {9},
	issn = {1087-0172},
	shorttitle = {Incident {Reviews} in {High}-{Hazard} {Industries}},
	url = {http://journals.sagepub.com/doi/10.1177/108602669500900203},
	doi = {10.1177/108602669500900203},
	abstract = {Learning from practical experience is of greater importance in more complex work environments. In high-hazard industries, complexity, tight coupling, and invisibility make safe operation and learning from experience particularly difficult. There is growing recognition that further improvement is needed and that it will require more than incremental improvement and exchange of "best practices." This article describes how organization members make sense of practical experience in one high-hazard industry\&mdash;nuclear power\&mdash;and how their sense-making affects their decisions and actions. The author discusses four factors that can limit the effectiveness of the interpretive process: root cause seduction, sharp-end focus, solution-driven search, and account acceptability. He then examines the impact that myopic interpretations can have on operating performance by placing incident reviews within the organizational learning process, and he closes with suggestions for a cross-disciplinary research agenda.},
	language = {en},
	number = {2},
	urldate = {2022-05-23},
	journal = {Industrial \& Environmental Crisis Quarterly},
	author = {Carroll, John S.},
	month = jun,
	year = {1995},
	pages = {175--197},
}

@article{carroll_organizational_1998,
	title = {Organizational {Learning} {Activities} in {High}-hazard {Industries}: {The} {Logics} {Underlying} {Self}-{Analysis}},
	volume = {35},
	issn = {0022-2380, 1467-6486},
	shorttitle = {Organizational {Learning} {Activities} in {High}-hazard {Industries}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-6486.00116},
	doi = {10.1111/1467-6486.00116},
	abstract = {Organizational learning takes place through activities performed by individuals, groups, and organizations as they gather and digest information, imagine and plan new actions, and implement change. I examine the learning practices of companies in two industries ± nuclear power plants and chemical process plants ± that must manage safety as a major component of operations, and therefore must learn from precursors and near-misses rather than exclusively by trial-anderror. Speci®cally, I analyse the linked assumptions or logics underlying incident reviews, root cause analysis teams, and self-analysis programmes. These logics arise from occupational and hierarchical groups that work on dierent problems in dierent ways ± for example, anticipation and resilience, ®xing and learning, concrete and abstract. In organizations with fragmentary, myopic and disparate understandings of how the work is accomplished, there are likely to be more failures to learn from operating experience, recurrent problems, and cyclical crises. Enhanced learning requires ways to broaden and bring together disparate logics.},
	language = {en},
	number = {6},
	urldate = {2022-05-23},
	journal = {Journal of Management Studies},
	author = {Carroll, John S.},
	month = nov,
	year = {1998},
	pages = {699--717},
}

@misc{noauthor_docs_nodate,
	title = {Docs home {\textbar} {Semgrep}},
	url = {https://semgrep.dev/docs/},
	abstract = {Read the documentation and get started with Semgrep. A fast, open-source, static analysis tool for finding bugs and enforcing code standards at editor, commit, and CI time.},
	urldate = {2022-05-19},
}

@misc{noauthor_codeql_nodate,
	title = {{CodeQL}},
	url = {https://codeql.github.com/},
	urldate = {2022-05-21},
}

@inproceedings{guo_caspar_2020,
	title = {Caspar: {Extracting} and {Synthesizing} {User} {Stories} of {Problems} from {App} {Reviews}},
	shorttitle = {Caspar},
	abstract = {A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding action-problem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0\% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9\% precision and 34.2\% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Guo, Hui and Singh, Munindar P.},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	keywords = {Natural language processing, Predictive models, Software engineering, app review analysis, event extraction, event inference, natural language processing, requirements},
	pages = {628--640},
}

@misc{stoch_how_2012,
	title = {How social media can benefit small businesses},
	abstract = {Have you ever wondered what Twitter, Facebook, LinkedIn and
Pinterest can bring to your company? Alison Coleman meets the
firms that have used social media to their advantage},
	language = {en},
	author = {Stoch, David},
	month = aug,
	year = {2012},
}

@article{kashmar_access_2021,
	title = {Access {Control} in {Cybersecurity} and {Social} {Media}},
	abstract = {Social media networks and their applications (e.g. Facebook, Twitter...) are a current phenomenon with a great impact on several aspects such as, personal, commercial, political, etc. This media is vulnerable to various forms of attacks and threats due to the heterogeneity of networks, diversity of applications and platforms, and the level of users' awareness and intentions. As network technologies and their various applications evolve, the way people interact with them changes. Thus, the main concern for all social media users is to protect their data from any type of illegal access. With network and application developments, the concept of controlling access evolves in various stages. It begins with the implementation of the principles of information security (confidentiality, authentication ...), then by finding various access control (AC) models to enforce security policy in this field. For cybersecurity and social media, various methods are developed based on conventional AC models: Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role Based Access Control (RBAC), Organization Based Access Control (OrBAC), and others.
In this chapter, we highlight the various types of cybercriminal attacks in social media networks. We then introduce the challenges faced for controlling users’ access and the importance of the AC concept for cybersecurity and social media. We will also review the common AC models and the AC methods that are proposed to enhance privacy issues in social networks. Based on these methods, we will conclude our chapter by analyzing them to know their efficiency in such media and their adaptability for any future requirements.},
	journal = {Access Control in Cybersecurity and Social Media},
	author = {Kashmar, Nadine and Adda, Mehdi and Ibrahim, H. and Atieh, Mirna},
	month = feb,
	year = {2021},
}

@article{massey_where_2016,
	title = {Where {Do} {U}.{S}. {Adults} {Who} {Do} {Not} {Use} the {Internet} {Get} {Health} {Information}? {Examining} {Digital} {Health} {Information} {Disparities} {From} 2008 to 2013},
	volume = {21},
	issn = {1081-0730},
	shorttitle = {Where {Do} {U}.{S}. {Adults} {Who} {Do} {Not} {Use} the {Internet} {Get} {Health} {Information}?},
	url = {https://doi.org/10.1080/10810730.2015.1058444},
	doi = {10.1080/10810730.2015.1058444},
	abstract = {With more people turning to the Internet for health information, a few questions remain: Which populations represent the remaining few who have never used the Internet, and where do they go for health information? The purpose of this study is to describe population characteristics and sources of health information among U.S. adults who do not use the Internet. Data from 3 iterations of the Health Information National Trends Survey (n = 1,722) are used to examine trends in health information sources. Weighted predicted probabilities demonstrate changes in information source over time. Older adults, minority populations, and individuals with low educational attainment represent a growing percentage of respondents who have looked for health information but have never used the Internet, highlighting trends in digital information disparities. However, 1 in 10 respondents who have never used the Internet also indicate that the Internet was their first source of health information, presumably through surrogates. Findings highlight digital disparities in information seeking and the complex nature of online information seeking. Future research should examine how individuals conceptualize information sources, measure skills related to evaluating information and sources, and investigate the social nature of information seeking. Health care organizations and public health agencies can leverage the multifaceted nature of information seeking to better develop information resources to increase information access by vulnerable populations.},
	number = {1},
	urldate = {2022-05-20},
	journal = {Journal of Health Communication},
	author = {Massey, Philip M.},
	month = jan,
	year = {2016},
	pmid = {26166484},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10810730.2015.1058444},
	pages = {118--124},
}

@article{ishimatsu2010modeling,
	title = {Modeling and hazard analysis using {STPA}},
	author = {Ishimatsu, Takuto and Leveson, Nancy G and Thomas, John and Katahira, Masa and Miyamoto, Yuko and Nakao, Haruka},
	year = {2010},
	note = {Publisher: International Association for the Advancement of Space Safety (IAASS)},
	keywords = {IoTFailurePaper},
}

@phdthesis{song2012applying,
	title = {Applying system-theoretic accident model and processes ({STAMP}) to hazard analysis},
	author = {Song, Yao},
	year = {2012},
	keywords = {IoTFailurePaper},
}

@article{reifer_software_1979,
	title = {Software {Failure} {Modes} and {Effects} {Analysis}},
	volume = {R-28},
	issn = {1558-1721},
	doi = {10.1109/TR.1979.5220578},
	abstract = {This concept paper discusses the possible use of failure modes and effects analysis (FMEA) as a means to produce more reliable software. FMEA is a fault avoidance technique whose objective is to identify hazards in requirements that have the potential to either endanger mission success or significantly impact life-cycle costs. FMEA techniques can be profitably applied during the analysis stage to identify potential hazards in requirements and design. As hazards are identified, software defenses can be developed using fault tolerant or self-checking techniques to reduce the probability of their occurrence once the program is implemented. Critical design features can also be demonstrated a priori analytically using proof of correctness techniques prior to their implementation if warranted by cost and criticality.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Reifer, Donald J.},
	month = aug,
	year = {1979},
	keywords = {Costs, Failure analysis, Fault tolerance, Fault tolerant software, Hazards, IoTFailurePaper, Military standards, Missiles, Self checking software, Software failure modes and effects analysis, Software performance, Software reliability, Software safety, Software standards, Software testing},
	pages = {247--249},
}

@book{fairbanks_just_2010,
	title = {Just {Enough} {Software} {Architecture}: {A} {Risk}-driven {Approach}},
	isbn = {978-0-9846181-0-1},
	shorttitle = {Just {Enough} {Software} {Architecture}},
	abstract = {This is a practical guide for software developers, and different than other software architecture books. Here's why:It teaches risk-driven architecting. There is no need for meticulous designs when risks are small, nor any excuse for sloppy designs when risks threaten your success. This book describes a way to do just enough architecture. It avoids the one-size-fits-all process tar pit with advice on how to tune your design effort based on the risks you face.It democratizes architecture. This book seeks to make architecture relevant to all software developers. Developers need to understand how to use constraints as guiderails that ensure desired outcomes, and how seemingly small changes can affect a system's properties.It cultivates declarative knowledge. There is a difference between being able to hit a ball and knowing why you are able to hit it, what psychologists refer to as procedural knowledge versus declarative knowledge. This book will make you more aware of what you have been doing and provide names for the concepts.It emphasizes the engineering. This book focuses on the technical parts of software development and what developers do to ensure the system works not job titles or processes. It shows you how to build models and analyze architectures so that you can make principled design tradeoffs. It describes the techniques software designers use to reason about medium to large sized problems and points out where you can learn specialized techniques in more detail.It provides practical advice. Software design decisions influence the architecture and vice versa. The approach in this book embraces drill-down/pop-up behavior by describing models that have various levels of abstraction, from architecture to data structure design.},
	language = {en},
	publisher = {Marshall \& Brainerd},
	author = {Fairbanks, George},
	year = {2010},
	note = {Google-Books-ID: ITsWdAAzVYMC},
	keywords = {Computers / Software Development \& Engineering / General, Computers / Software Development \& Engineering / Quality Assurance \& Testing, Computers / Software Development \& Engineering / Systems Analysis \& Design, Computers / Software Development \& Engineering / Tools, IoTFailurePaper},
}

@inproceedings{lee_smart_2016,
	address = {New York, NY, USA},
	series = {{APSys} '16},
	title = {Smart and {Secure}: {Preserving} {Privacy} in {Untrusted} {Home} {Routers}},
	isbn = {978-1-4503-4265-0},
	shorttitle = {Smart and {Secure}},
	url = {https://doi.org/10.1145/2967360.2967380},
	doi = {10.1145/2967360.2967380},
	abstract = {Recently, wireless home routers increasingly become smart. While these smart routers provide rich functionalities to users, they also raise security concerns. Since a smart home router may process and store personal data for users, once compromised, these sensitive information will be exposed. Unfortunately, current operating systems on home routers are far from secure. As a consequence, users are facing a difficult tradeoff between functionality and privacy risks. This paper attacks this dilemma with a novel SEAL architecture for home routers. SEAL leverages the ARM TrustZone technology to divide a conventional router OS (i.e., Linux) in a non-secure/normal world. All sensitive user data are shielded from the normal world using encryption. Modules (called applets) that process the sensitive data are located in a secure world and confined in secure sandboxes provided by a tiny secure OS. We report the system design of SEAL and our preliminary implementation and evaluation results.},
	urldate = {2022-05-20},
	booktitle = {Proceedings of the 7th {ACM} {SIGOPS} {Asia}-{Pacific} {Workshop} on {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lee, Seung-seob and Shi, Hang and Tan, Kun and Liu, Yunxin and Lee, SuKyoung and Cui, Yong},
	month = aug,
	year = {2016},
	pages = {1--8},
}

@inproceedings{olson_border_2015,
	title = {Border control: {Sandboxing} accelerators},
	shorttitle = {Border control},
	doi = {10.1145/2830772.2830819},
	abstract = {As hardware accelerators proliferate, there is a desire to logically integrate them more tightly with CPUs through interfaces such as shared virtual memory. Although this integration has programmability and performance benefits, it may also have serious security and fault isolation implications, especially when accelerators are designed by third parties. Unchecked, accelerators could make incorrect memory accesses, causing information leaks, data corruption, or crashes not only for processes running on the accelerator, but for the rest of the system as well. Unfortunately, current security solutions are insufficient for providing memory protection from tightly integrated untrusted accelerators. We propose Border Control, a sandboxing mechanism which guarantees that the memory access permissions in the page table are respected by accelerators, regardless of design errors or malicious intent. Our hardware implementation of Border Control provides safety against improper memory accesses with a space overhead of only 0.006\% of system physical memory per accelerator. We show that when used with a current highly demanding accelerator, this initial Border Control implementation has on average a 0.15\% runtime overhead relative to the unsafe baseline.},
	booktitle = {2015 48th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Olson, Lena E. and Power, Jason and Hill, Mark D. and Wood, David A.},
	month = dec,
	year = {2015},
	note = {ISSN: 2379-3155},
	keywords = {Computer bugs, Graphics processing units, Hardware, Libraries, Safety, Security, accelerators, hardware sandboxing, memory protection},
	pages = {470--481},
}

@inproceedings{narayan_retrofitting_2020,
	title = {Retrofitting {Fine} {Grain} {Isolation} in the {Firefox} {Renderer}},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/narayan},
	language = {en},
	urldate = {2022-05-20},
	author = {Narayan, Shravan and Disselkoen, Craig and Garfinkel, Tal and Froyd, Nathan and Rahm, Eric and Lerner, Sorin and Shacham, Hovav and Stefan, Deian},
	year = {2020},
	pages = {699--716},
}

@inproceedings{abbasi_challenges_2019,
	title = {Challenges in {Designing} {Exploit} {Mitigations} for {Deeply} {Embedded} {Systems}},
	doi = {10.1109/EuroSP.2019.00013},
	abstract = {Memory corruption vulnerabilities have been around for decades and rank among the most prevalent vulnerabilities in embedded systems. Yet this constrained environment poses unique design and implementation challenges that significantly complicate the adoption of common hardening techniques. Combined with the irregular and involved nature of embedded patch management, this results in prolonged vulnerability exposure windows and vulnerabilities that are relatively easy to exploit. Considering the sensitive and critical nature of many embedded systems, this situation merits significant improvement. In this work, we present the first quantitative study of exploit mitigation adoption in 42 embedded operating systems, showing the embedded world to significantly lag behind the general-purpose world. To improve the security of deeply embedded systems, we subsequently present μArmor, an approach to address some of the key gaps identified in our quantitative analysis. μArmor raises the bar for exploitation of embedded memory corruption vulnerabilities, while being adoptable on the short term without incurring prohibitive extra performance or storage costs.},
	booktitle = {2019 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS} {P})},
	author = {Abbasi, Ali and Wetzels, Jos and Holz, Thorsten and Etalle, Sandro},
	month = jun,
	year = {2019},
	keywords = {Embedded System, Embedded systems, Exploit Mitigation, Exploiting, Hardware, Linux, Real-time systems, Security, Statistical analysis},
	pages = {31--46},
}

@techreport{yu_building_2022,
	title = {Building {Embedded} {Systems} {Like} {It}'s 1996},
	url = {http://arxiv.org/abs/2203.06834},
	abstract = {Embedded devices are ubiquitous. However, preliminary evidence shows that attack mitigations protecting our desktops/servers/phones are missing in embedded devices, posing a significant threat to embedded security. To this end, this paper presents an in-depth study on the adoption of common attack mitigations on embedded devices. Precisely, it measures the presence of standard mitigations against memory corruptions in over 10k Linux-based firmware of deployed embedded devices. The study reveals that embedded devices largely omit both user-space and kernel-level attack mitigations. The adoption rates on embedded devices are multiple times lower than their desktop counterparts. An equally important observation is that the situation is not improving over time. Without changing the current practices, the attack mitigations will remain missing, which may become a bigger threat in the upcoming IoT era. Throughout follow-up analyses, we further inferred a set of factors possibly contributing to the absence of attack mitigations. The exemplary ones include massive reuse of non-protected software, lateness in upgrading outdated kernels, and restrictions imposed by automated building tools. We envision these will turn into insights towards improving the adoption of attack mitigations on embedded devices in the future.},
	number = {arXiv:2203.06834},
	urldate = {2022-05-20},
	institution = {arXiv},
	author = {Yu, Ruotong and Del Nin, Francesca and Zhang, Yuchen and Huang, Shan and Kaliyar, Pallavi and Zakto, Sarah and Conti, Mauro and Portokalidis, Georgios and Xu, Jun},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.06834},
	note = {arXiv:2203.06834 [cs]
type: article},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{noauthor_construction_nodate,
	title = {Construction site accident analysis using text mining and natural language processing techniques {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0926580518306137?token=5A7FAE4AA4A7FAD79C6BDA08C3235E445BA29FB4C4D938F55BB1785218950E25F04A2CCFB864DD32BF7E209409C59011&originRegion=us-east-1&originCreation=20220519230633},
	language = {en},
	urldate = {2022-05-19},
	doi = {10.1016/j.autcon.2018.12.016},
}

@article{zhang_construction_2019,
	title = {Construction site accident analysis using text mining and natural language processing techniques},
	volume = {99},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580518306137},
	doi = {10.1016/j.autcon.2018.12.016},
	abstract = {Workplace safety is a major concern in many countries. Among various industries, construction sector is identified as the most hazardous work place. Construction accidents not only cause human sufferings but also result in huge financial loss. To prevent reoccurrence of similar accidents in the future and make scientific risk control plans, analysis of accidents is essential. In construction industry, fatality and catastrophe investigation summary reports are available for the past accidents. In this study, text mining and natural language process (NLP) techniques are applied to analyze the construction accident reports. To be more specific, five baseline models, support vector machine (SVM), linear regression (LR), K-nearest neighbor (KNN), decision tree (DT), Naive Bayes (NB) and an ensemble model are proposed to classify the causes of the accidents. Besides, Sequential Quadratic Programming (SQP) algorithm is utilized to optimize weight of each classifier involved in the ensemble model. Experiment results show that the optimized ensemble model outperforms rest models considered in this study in terms of average weighted F1 score. The result also shows that the proposed approach is more robust to cases of low support. Moreover, an unsupervised chunking approach is proposed to extract common objects which cause the accidents based on grammar rules identified in the reports. As harmful objects are one of the major factors leading to construction accidents, identifying such objects is extremely helpful to mitigate potential risks. Certain limitations of the proposed methods are discussed and suggestions and future improvements are provided.},
	language = {en},
	urldate = {2022-05-19},
	journal = {Automation in Construction},
	author = {Zhang, Fan and Fleyeh, Hasan and Wang, Xinru and Lu, Minghui},
	month = mar,
	year = {2019},
	keywords = {Construction site accident analysis, Machine learning, Natural language processing, Optimization, Sequential quadratic programming, Text mining},
	pages = {238--248},
}

@article{machiry_c_2022,
	title = {C to checked {C} by 3c},
	volume = {6},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3527322},
	doi = {10.1145/3527322},
	abstract = {ARAVIND MACHIRY, Purdue University, USA JOHN KASTNER, Amazon, USA MATT MCCUTCHEN, Amazon, USA AARON ELINE, Amazon, USA KYLE HEADLEY, Amazon, USA MICHAEL HICKS, Amazon, USA Owing to the continued use of C (and C++), spatial safety violations (e.g., buffer overflows) still constitute one of today’s most dangerous and prevalent security vulnerabilities. To combat these violations, Checked C extends C with bounds-enforced checked pointer types. Checked C is essentially a gradually typed spatially safe CÐchecked pointers are backwards-binary compatible with legacy pointers, and the language allows them to be added piecemeal, rather than necessarily all at once, so that safety retrofitting can be incremental. This paper presents a semi-automated process for porting a legacy C program to Checked C. The process centers on 3C, a static analysis-based annotation tool. 3C employs two novel static analysis algorithmsÐtyp3c and boun3cÐto annotate legacy pointers as checked pointers, and to infer array bounds annotations for pointers that need them. 3C performs a root cause analysis to direct a human developer to code that should be refactored; once done, 3C can be re-run to infer further annotations (and updated root causes). Experiments on 11 programs totaling 319KLoC show 3C to be effective at inferring checked pointer types, and experience with previously and newly ported code finds 3C works well when combined with human-driven refactoring. CCS Concepts: • Software and its engineering → Translator writing systems and compiler generators; Software maintenance tools; • Security and privacy → Formal security models.},
	language = {en},
	number = {OOPSLA1},
	urldate = {2022-05-19},
	journal = {Proceedings of the ACM on Programming Languages (OOPSLA)},
	author = {Machiry, Aravind and Kastner, John and McCutchen, Matt and Eline, Aaron and Headley, Kyle and Hicks, Michael},
	month = apr,
	year = {2022},
	pages = {1--29},
}

@inproceedings{elliott_checked_2018,
	address = {Cambridge, MA},
	title = {Checked {C}: {Making} {C} {Safe} by {Extension}},
	isbn = {978-1-5386-7662-2},
	shorttitle = {Checked {C}},
	url = {https://ieeexplore.ieee.org/document/8543387/},
	doi = {10.1109/SecDev.2018.00015},
	abstract = {This paper presents Checked C, an extension to C designed to support spatial safety, implemented in Clang and LLVM. Checked C’s design is distinguished by its focus on backward-compatibility, incremental conversion, developer control, and enabling highly performant code. Like past approaches to a safer C, Checked C employs a form of checked pointer whose accesses can be statically or dynamically verified. Performance evaluation on a set of standard benchmark programs shows overheads to be relatively low. More interestingly, Checked C introduces the notions of a checked region and bounds-safe interfaces.},
	language = {en},
	urldate = {2022-05-19},
	booktitle = {2018 {IEEE} {Cybersecurity} {Development} ({SecDev})},
	publisher = {IEEE},
	author = {Elliott, Archibald Samuel and Ruef, Andrew and Hicks, Michael and Tarditi, David},
	month = sep,
	year = {2018},
	pages = {53--60},
}

@book{rob_moffatt_risk-first_2019,
	title = {Risk-{First} {Software} {Development}: {Volume} 1: {The} {Menagerie}},
	isbn = {1-71749-185-5},
	shorttitle = {Risk-{First} {Software} {Development}},
	url = {https://riskfirst.org},
	author = {{Rob Moffatt}},
	year = {2019},
}

@article{oukemeni_ipam_2019,
	title = {{IPAM}: {Information} {Privacy} {Assessment} {Metric} in {Microblogging} {Online} {Social} {Networks}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{IPAM}},
	doi = {10.1109/ACCESS.2019.2932899},
	abstract = {A large amount of sensitive data is transferred, stored, processed, and analyzed daily in Online Social Networks (OSNs). Thus, an effective and efficient evaluation of the privacy level provided in such services is necessary to meet user expectations and comply with the requirement of the applicable laws and regulations. Several prior works have proposed mechanisms for evaluating and calculating privacy scores in OSNs. However, current models are system-specific and assess privacy only from the user's perspective. There is still a lack of a universal model that can quantify the level of privacy and compare between different systems. In this paper, we propose a generic framework to (i) guide the development of privacy metrics and (ii) to measure and assess the privacy level of OSNs, more specifically microblogging systems. The present study develops an algorithmic model to compute privacy scores based on the impact of privacy and security requirements, accessibility, and difficulty of information extraction. The proposed framework aims to provide users as well as system providers with a measure of how much the investigated system is protecting privacy. It allows also comparing the privacy protection level between different systems. The privacy score framework has been tested using real microblogging social networks and the results show the potential of the proposed privacy scoring framework.},
	journal = {IEEE Access},
	author = {Oukemeni, Samia and Rifà-Pous, Helena and Marquès Puig, Joan Manuel},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Data privacy, Law, Measurement, Online social networks, Privacy, Security, Social networking (online), information privacy protection, information security, metrics, microbloggings, privacy measurements},
	pages = {114817--114836},
}

@incollection{page_social_2022,
	address = {Cham},
	title = {Social {Media} and {Privacy}},
	isbn = {978-3-030-82786-1},
	url = {https://doi.org/10.1007/978-3-030-82786-1_7},
	abstract = {With the popularity of social media, researchers and designers must consider a wide variety of privacy concerns while optimizing for meaningful social interactions and connection. While much of the privacy literature has focused on information disclosures, the interpersonal dynamics associated with being on social media make it important for us to look beyond informational privacy concerns to view privacy as a form of interpersonal boundary regulation. In other words, attaining the right level of privacy on social media is a process of negotiating how much, how little, or when we desire to interact with others, as well as the types of information we choose to share with them or allow them to share about us. We propose a framework for how researchers and practitioners can think about privacy as a form of interpersonal boundary regulation on social media by introducing five boundary types (i.e., relational, network, territorial, disclosure, and interactional) social media users manage. We conclude by providing tools for assessing privacy concerns in social media, as well as noting several challenges that must be overcome to help people to engage more fully and stay on social media.},
	language = {en},
	urldate = {2022-05-17},
	booktitle = {Modern {Socio}-{Technical} {Perspectives} on {Privacy}},
	publisher = {Springer International Publishing},
	author = {Page, Xinru and Berrios, Sara and Wilkinson, Daricia and Wisniewski, Pamela J.},
	editor = {Knijnenburg, Bart P. and Page, Xinru and Wisniewski, Pamela and Lipford, Heather Richter and Proferes, Nicholas and Romano, Jennifer},
	year = {2022},
	doi = {10.1007/978-3-030-82786-1_7},
	pages = {113--147},
}

@article{tiedeman_post-mortems-methodology_1990,
	title = {Post-mortems-methodology and experiences},
	volume = {8},
	issn = {1558-0008},
	doi = {10.1109/49.46869},
	abstract = {A methodology for preparing and conducting post-mortems for software development projects is presented. Based on the continuous process improvement concept, post-mortems are systematic reviews of a product's quality and the quality of the associated processes that produce it. The intent is to learn both what worked well and what could be improved. Three types of postmortem have evolved, each focusing on particular aspects of the software development process. The planning postmortem covers requirements specification and work program definition. The design verification postmortem includes design, unit test, system verification, and first office application activities. The field post-mortem focuses on actual field experience. The post-mortem methodology has been in use for several years and has been found to be a very effective organizational learning tool and source for many software quality improvement ideas. Specific examples of what has been and can be learned are included. The evolution of the post-mortem is discussed, along with possible metrics for evaluating the effectiveness of postmortems.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Tiedeman, M.J.},
	month = feb,
	year = {1990},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	keywords = {Application software, Electrical capacitance tomography, Electronic switching systems, Feedback, IoTFailurePaper, Programming, Software quality, Spine, Switches, Switching systems, System testing},
	pages = {176--180},
}

@article{dingsoyr_postmortem_2005,
	title = {Postmortem reviews: purpose and approaches in software engineering},
	volume = {47},
	issn = {09505849},
	shorttitle = {Postmortem reviews},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584904001296},
	doi = {10.1016/j.infsof.2004.08.008},
	abstract = {Conducting postmortems is a simple and practical method for organizational learning. Yet, not many companies have implemented such practices, and in a survey, few expressed satisfaction with how postmortems were conducted. In this article, we discuss the importance of postmortem reviews as a method for knowledge sharing in software projects, and give an overview of known such processes in the field of software engineering. In particular, we present three lightweight methods for conducting postmortems found in the literature, and discuss what criteria companies should use in defining their way of conducting postmortems.},
	language = {en},
	number = {5},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Dingsøyr, Torgeir},
	month = mar,
	year = {2005},
	keywords = {IoTFailurePaper},
	pages = {293--303},
}

@incollection{diaz-herrera_falling_1994,
	address = {Berlin/Heidelberg},
	title = {Falling down is part of growing {Up}; the study of failure and the {Software} {Engineering} community},
	volume = {750},
	isbn = {978-3-540-57461-3},
	url = {http://link.springer.com/10.1007/BFb0017636},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Software {Engineering} {Education}},
	publisher = {Springer-Verlag},
	author = {Dalcher, Darren},
	editor = {Díaz-Herrera, Jorge L.},
	year = {1994},
	doi = {10.1007/BFb0017636},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {IoTFailurePaper},
	pages = {489--496},
}

@book{cusumano1998microsoft,
	title = {Microsoft secrets: how the world's most powerful software company creates technology, shapes markets, and manages people},
	publisher = {Simon and Schuster},
	author = {Cusumano, Michael A and Selby, Richard W},
	year = {1998},
	keywords = {IoTFailurePaper},
}

@book{saldana_coding_2021,
	title = {The coding manual for qualitative researchers},
	publisher = {SAGE Publications, Inc},
	author = {Saldaña, Johnny},
	year = {2021},
}

@inproceedings{tiganoaia_risks_2017,
	title = {The risks in the social networks — {An} exploratory study},
	volume = {2},
	doi = {10.1109/IDAACS.2017.8095232},
	abstract = {While social networking has gone on almost as long as societies themselves have existed, the unparalleled potential of the Web to facilitate such connections has led to an exponential and ongoing expansion of that phenomenon. In addition to social media platforms, the capacity for social interaction and collaboration is increasingly built into business applications [1]. The risks in the online space and particularly in social networks represent one of the most important and opened issue of our time. The paper is an exploratory study with the focus point related to an original research based on a questionnaire with more that 300 of respondents. Topics about the risks in the social networks such as: the security in the social networks, incidents, viruses, worms, spam, the level of security regarding the applications used on social networks, etc were analysed. The findings of this research provide a basis that is useful to understand the importance of the prevention and treatment of risks in the social networks. Regarding the management perspective of an organization, this paper highlights the necessity of the investments in companies security against cyber risks. The paper also provides useful findings in the field of cyber risks for the online community.},
	booktitle = {2017 9th {IEEE} {International} {Conference} on {Intelligent} {Data} {Acquisition} and {Advanced} {Computing} {Systems}: {Technology} and {Applications} ({IDAACS})},
	author = {Tiganoaia, Bogdan and Cernian, Alexandra and Niculescu, Andrei},
	month = sep,
	year = {2017},
	keywords = {Computer security, Education, Social network services, Terrorism, questionnaire, research, risks, social networks},
	pages = {974--977},
}

@techreport{carr_taxonomy-based_1993,
	title = {Taxonomy-{Based} {Risk} {Identification}},
	url = {https://apps.dtic.mil/sti/citations/ADA266992},
	abstract = {This report describes a method for facilitating the systematic and repeatable identification of risks associated with the development of a software-dependent project. This method, derived from published literature and previous experience in developing software, was tested in active government- funded defense and civilian software development projects for both its usefulness and for improving the method itself. Results of the field tests encouraged the claim that the described method is useful, usable, and efficient. The report concludes with some macro-level lessons learned from the field tests and brief overview of future work in establishing risk management on a firm footing in software development projects.},
	language = {en},
	urldate = {2022-05-11},
	institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SOFTWARE ENGINEERING INST},
	author = {Carr, Marvin J. and Konda, Suresh L. and Monarch, Ira and Ulrich, F. C. and Walker, Clay F.},
	month = jun,
	year = {1993},
	note = {Section: Technical Reports},
}

@article{boehm_software_1991,
	title = {Software risk management: principles and practices},
	volume = {8},
	issn = {1937-4194},
	shorttitle = {Software risk management},
	doi = {10.1109/52.62930},
	abstract = {The emerging discipline of software risk management is described. It is defined as an attempt to formalize the risk-oriented correlates of success into a readily applicable set of principles and practices. Its objectives are to identify, address, and eliminate risk items before they become either threats to successful software operation or major sources of software rework. The basic concepts are set forth, and the major steps and techniques involved in software risk management are explained. Suggestions for implementing risk management are provided.{\textless}{\textgreater}},
	number = {1},
	journal = {IEEE Software},
	author = {Boehm, B.W.},
	month = jan,
	year = {1991},
	note = {Conference Name: IEEE Software},
	keywords = {Bridges, Disaster management, Energy resolution, Frequency, Project management, Risk management, Signal resolution, Software prototyping, Tides, Virtual prototyping},
	pages = {32--41},
}

@book{reis_handbook_2000,
	title = {Handbook of {Research} {Methods} in {Social} and {Personality} {Psychology}},
	isbn = {978-0-521-55903-4},
	abstract = {This sourcebook covers conceptual and practical issues in research design, methods of research and statistical approaches in social and personality psychology. The primary purpose of the handbook is to provide readable yet comprehensive chapters on the range of methods and tools used by researchers in social and personality psychology. In addition, it should alert researchers to methodological possibilities they may not have thought of. Innovative research methods work best when they allow researchers to ask theoretically driven questions that could not have been asked previously, thereby enhancing the quality and depth of their empirical knowledge base. With the help of this text, both new and established social psychologists should learn about appropriate uses of each method and the opportunities they provide for expanding knowledge.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Reis, Harry T. and Reis, Harry T. and Judd, Charles M.},
	month = mar,
	year = {2000},
	note = {Google-Books-ID: j7aawGLbtEoC},
	keywords = {Psychology / Personality, Psychology / Social Psychology},
}

@article{kish-gephart_bad_2010,
	title = {Bad apples, bad cases, and bad barrels: {Meta}-analytic evidence about sources of unethical decisions at work.},
	volume = {95},
	issn = {1939-1854, 0021-9010},
	shorttitle = {Bad apples, bad cases, and bad barrels},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017103},
	doi = {10.1037/a0017103},
	abstract = {As corporate scandals proliferate, practitioners and researchers alike need a cumulative, quantitative understanding of the antecedents associated with unethical decisions in organizations. In this metaanalysis, the authors draw from over 30 years of research and multiple literatures to examine individual (“bad apple”), moral issue (“bad case”), and organizational environment (“bad barrel”) antecedents of unethical choice. Findings provide empirical support for several foundational theories and paint a clearer picture of relationships characterized by mixed results. Structural equation modeling revealed the complexity (multidetermined nature) of unethical choice, as well as a need for research that simultaneously examines different sets of antecedents. Moderator analyses unexpectedly uncovered better prediction of unethical behavior than of intention for several variables. This suggests a need to more strongly consider a new “ethical impulse” perspective in addition to the traditional “ethical calculus” perspective. Results serve as a data-based foundation and guide for future theoretical and empirical development in the domain of behavioral ethics.},
	language = {en},
	number = {1},
	urldate = {2022-05-10},
	journal = {Journal of Applied Psychology},
	author = {Kish-Gephart, Jennifer J. and Harrison, David A. and Treviño, Linda Klebe},
	month = jan,
	year = {2010},
	pages = {1--31},
}

@inproceedings{almudahi_social_2022,
	title = {Social {Media} {Privacy} {Issues}, {Threats}, and {Risks}},
	doi = {10.1109/WiDS-PSU54548.2022.00043},
	abstract = {General knownledge presumes that social media users are increasing. As of October 2021, more than 4.5 billion people are using social networks. Popularly utilized for communication, knowledge participation, thoughts communication, videos, building networks, images, and so on. However, there is still a lack of knowledge of the consequences associated with the use of said programs, and the growth of users increases the possible vulnerabilities and attacks. This paper proposes some suggestions for mitigation of risks and threats when people publish anything online. The authors' contribution provided in this paper is the suggested solutions for each of the listed and discussed threats and risks.},
	booktitle = {2022 {Fifth} {International} {Conference} of {Women} in {Data} {Science} at {Prince} {Sultan} {University} ({WiDS} {PSU})},
	author = {AlMudahi, Gahadh Faisal and AlSwayeh, Lama Khalid and AlAnsary, Sara Ahmed and Latif, Rabia},
	month = mar,
	year = {2022},
	keywords = {Cross-Site Scripting, Data privacy, Data science, Knowledge engineering, Media, Social networking (online), Videos, identity theft, location disclosure, malware, phishing, ransomware, risks, social media, social media threats},
	pages = {155--159},
}

@book{senge_fifth_2006,
	title = {The {Fifth} {Discipline}: {The} {Art} and {Practice} of the {Learning} {Organization}},
	isbn = {978-0-385-51725-6},
	shorttitle = {The {Fifth} {Discipline}},
	abstract = {Completely Updated and RevisedThis revised edition of Peter Senge's bestselling classic, The Fifth Discipline, is based on fifteen years of experience in putting the book's ideas into practice. As Senge makes clear, in the long run the only sustainable competitive advantage is your organization's ability to learn faster than the competition. The leadership stories in the book demonstrate the many ways that the core ideas in The Fifth Discipline, many of which seemed radical when first published in 1990, have become deeply integrated into people's ways of seeing the world and their managerial practices. In The Fifth Discipline, Senge describes how companies can rid themselves of the learning “disabilities” that threaten their productivity and success by adopting the strategies of learning organizations—ones in which new and expansive patterns of thinking are nurtured, collective aspiration is set free, and people are continually learning how to create results they truly desire. The updated and revised Currency edition of this business classic contains over one hundred pages of new material based on interviews with dozens of practitioners at companies like BP, Unilever, Intel, Ford, HP, Saudi Aramco, and organizations like Roca, Oxfam, and The World Bank. It features a new Foreword about the success Peter Senge has achieved with learning organizations since the book's inception, as well as new chapters on Impetus (getting started), Strategies, Leaders' New Work, Systems Citizens, and Frontiers for the Future. Mastering the disciplines Senge outlines in the book will:• Reignite the spark of genuine learning driven by people focused on what truly matters to them• Bridge teamwork into macro-creativity• Free you of confining assumptions and mindsets• Teach you to see the forest and the trees• End the struggle between work and personal time},
	language = {en},
	publisher = {Doubleday/Currency},
	author = {Senge, Peter M.},
	year = {2006},
	keywords = {Business \& Economics / Business Communication / General, Business \& Economics / Leadership, Business \& Economics / Management, Business \& Economics / Organizational Development},
}

@inproceedings{persia_survey_2017,
	title = {A {Survey} of {Online} {Social} {Networks}: {Challenges} and {Opportunities}},
	shorttitle = {A {Survey} of {Online} {Social} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8102989},
	doi = {10.1109/IRI.2017.74},
	abstract = {Online Social Networks (OSNs) have become fundamental parts of our online lives, and their popularity is increasing at a surprising rate every day. However, besides the revolution the OSNs have generated in social networking, they have also introduced some issues; first, since the amount of multimedia data on the Internet is growing continuously, it is extremely important for users not only to share multimedia content with each other, but also to receive the specific content they are interested in; second, OSNs have introduced new threats to their users due to their attractiveness, the ever-increasing number of users, and the massive amount of personal information they share. For such reasons, in this paper we propose a survey of online social networks, which can hopefully support both researchers and social network users. More specifically, we focus our attention on the most relevant research challenges regarding semantics and security.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Information} {Reuse} and {Integration} ({IRI})},
	author = {Persia, Fabio and D'Auria, Daniela},
	month = aug,
	year = {2017},
	keywords = {Online Social Networks, Phishing Attacks, Semantics, Sentiment Detection, Social network services, Spamming, Support vector machines, Sybil Attacks, Tools, Videos},
	pages = {614--620},
}

@article{gelles_boeing_2019,
	chapter = {Business},
	title = {Boeing 737 {Max}: {What}’s {Happened} {After} the 2 {Deadly} {Crashes}},
	issn = {0362-4331},
	shorttitle = {Boeing 737 {Max}},
	url = {https://www.nytimes.com/interactive/2019/business/boeing-737-crashes.html},
	abstract = {Boeing remains under intense scrutiny nearly one year after the first Max jet was involved in a fatal accident.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Gelles, David},
	month = mar,
	year = {2019},
	keywords = {Boeing 737 Max Groundings and Safety Concerns (2019), Boeing Company},
}

@article{zetter_inside_2016,
	title = {Inside the {Cunning}, {Unprecedented} {Hack} of {Ukraine}'s {Power} {Grid}},
	issn = {1059-1028},
	url = {https://www.wired.com/2016/03/inside-cunning-unprecedented-hack-ukraines-power-grid/},
	abstract = {The hack on Ukraine's power grid was a first-of-its-kind attack that sets an ominous precedent for the security of power grids everywhere.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Zetter, Kim},
	year = {2016},
	keywords = {critical infrastructure, hacks},
}

@article{greenberg_hackers_2015,
	title = {Hackers {Cut} a {Corvette}'s {Brakes} {Via} a {Common} {Car} {Gadget}},
	issn = {1059-1028},
	url = {https://www.wired.com/2015/08/hackers-cut-corvettes-brakes-via-common-car-gadget/},
	abstract = {The free dongles that insurance companies ask customers to plug into their dashes could expose your car to hackers.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Greenberg, Andy},
	year = {2015},
	keywords = {car hacking, mobile, uber},
}

@article{greenberg_hackers_2017,
	title = {Hackers {Remotely} {Kill} a {Jeep} on the {Highway}—{With} {Me} in {It}},
	issn = {1059-1028},
	url = {https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/},
	abstract = {I was driving 70 mph on the edge of downtown St. Louis when the exploit began to take hold.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Greenberg, Andy},
	year = {2017},
}

@article{barrett_your_2016,
	title = {Your {DVR} {Didn}'t {Take} {Down} the {Internet}},
	issn = {1059-1028},
	url = {https://www.wired.com/2016/10/internet-outage-webcam-dvr-botnet/},
	abstract = {Repeat after us. Friday's botnet was not your fault.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Barrett, Brian},
	year = {2016},
	keywords = {iot, security},
}

@article{dreyfuss_hackers_2019,
	title = {Hackers {Found} a {Freaky} {New} {Way} to {Kill} {Your} {Car}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/car-hacking-biometric-database-security-roundup/},
	abstract = {Mueller report fallout, a biometrics database, and more of the week's top security news.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Dreyfuss, Emily},
	year = {2019},
	keywords = {biometrics, cybersecurity, security roundup},
}

@inproceedings{ding_iotsafe_2021,
	title = {{IoTSafe}: {Enforcing} {Safety} and {Security} {Policy} with {Real} {IoT} {Physical} {Interaction} {Discovery}},
	shorttitle = {{IoTSafe}},
	url = {https://www.ndss-symposium.org/ndss-paper/iotsafe-enforcing-safety-and-security-policy-with-real-iot-physical-interaction-discovery/},
	language = {en-US},
	urldate = {2021-09-22},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	author = {Ding, Wenbo},
	year = {2021},
}

@article{yamauchi_anomaly_2020,
	title = {Anomaly {Detection} in {Smart} {Home} {Operation} {From} {User} {Behaviors} and {Home} {Conditions}},
	volume = {66},
	issn = {1558-4127},
	doi = {10.1109/TCE.2020.2981636},
	abstract = {As several home appliances, such as air conditioners, heaters, and refrigerators, were connecting to the Internet, they became targets of cyberattacks, which cause serious problems such as compromising safety and even harming users. We have proposed a method to detect such attacks based on user behavior. This method models user behavior as sequences of user events including operation of home IoT (Internet of Things) devices and other monitored activities. Considering users behave depending on the condition of the home such as time and temperature, our method learns event sequences for each condition. To mitigate the impact of events of other users in the home included in the monitored sequence, our method generates multiple event sequences by removing some events and learning the frequently observed sequences. For evaluation, we constructed an experimental network of home IoT devices and recorded time data for four users entering/leaving a room and operating devices. We obtained detection ratios exceeding 90\% for anomalous operations with less than 10\% of misdetections when our method observed event sequences related to the operation. In this article, we also discuss the effectiveness of our method by comparing with a method learning users' behavior by Hidden Markov Models.},
	number = {2},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Yamauchi, Masaaki and Ohsita, Yuichi and Murata, Masayuki and Ueda, Kensuke and Kato, Yoshiaki},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Consumer Electronics},
	keywords = {Anomaly detection, Heating systems, Hidden Markov models, Internet of Things, Monitoring, Smart homes, Temperature measurement, Temperature sensors, behavior pattern, consumer electronics, cybersecurity, operation by attackers, smart home},
	pages = {183--192},
}

@inproceedings{chaki_conflict_2020,
	title = {A {Conflict} {Detection} {Framework} for {IoT} {Services} in {Multi}-resident {Smart} {Homes}},
	doi = {10.1109/ICWS49710.2020.00036},
	abstract = {We propose a novel framework to detect conflicts among IoT services in a multi-resident smart home. A novel IoT conflict model is proposed considering the functional and non-functional properties of IoT services. We design a conflict ontology that formally represents different types of conflicts. A hybrid conflict detection algorithm is proposed by combining both knowledge-driven and data-driven approaches. Experimental results on real-world datasets show the efficiency of the proposed approach.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Chaki, Dipankar and Bouguettaya, Athman and Mistry, Sajib},
	month = oct,
	year = {2020},
	keywords = {Conferences, Detection algorithms, Internet of Things, IoT services, Ontologies, Smart homes, Web services, conflict detection, conflict ontology, formal conflict model, multi-resident smart home},
	pages = {224--231},
}

@inproceedings{wang_charting_2019,
	address = {New York, NY, USA},
	series = {{CCS} '19},
	title = {Charting the {Attack} {Surface} of {Trigger}-{Action} {IoT} {Platforms}},
	isbn = {978-1-4503-6747-9},
	url = {https://doi.org/10.1145/3319535.3345662},
	doi = {10.1145/3319535.3345662},
	abstract = {Internet of Things (IoT) deployments are becoming increasingly automated and vastly more complex. Facilitated by programming abstractions such as trigger-action rules, end-users can now easily create new functionalities by interconnecting their devices and other online services. However, when multiple rules are simultaneously enabled, complex system behaviors arise that are difficult to understand or diagnose. While history tells us that such conditions are ripe for exploitation, at present the security states of trigger-action IoT deployments are largely unknown. In this work, we conduct a comprehensive analysis of the interactions between trigger-action rules in order to identify their security risks. Using IFTTT as an exemplar platform, we first enumerate the space of inter-rule vulnerabilities that exist within trigger-action platforms. To aid users in the identification of these dangers, we go on to present iRuler, a system that performs Satisfiability Modulo Theories (SMT) solving and model checking to discover inter-rule vulnerabilities within IoT deployments. iRuler operates over an abstracted information flow model that represents the attack surface of an IoT deployment, but we discover in practice that such models are difficult to obtain given the closed nature of IoT platforms. To address this, we develop methods that assist in inferring trigger-action information flows based on Natural Language Processing. We develop a novel evaluative methodology for approximating plausible real-world IoT deployments based on the installation counts of 315,393 IFTTT applets, determining that 66\% of the synthetic deployments in the IFTTT ecosystem exhibit the potential for inter-rule vulnerabilities. Combined, these efforts provide the insight into the real-world dangers of IoT deployment misconfigurations.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qi and Datta, Pubali and Yang, Wei and Liu, Si and Bates, Adam and Gunter, Carl A.},
	month = nov,
	year = {2019},
	keywords = {formal methods, information flow, inter-rule vulnerability, nlp, trigger-action iot platform},
	pages = {1439--1453},
}

@inproceedings{shehata_managing_2007,
	title = {Managing {Policy} {Interactions} in {KNX}-{Based} {Smart} {Homes}},
	volume = {2},
	doi = {10.1109/COMPSAC.2007.139},
	abstract = {Smart homes have enjoyed increasing popularity in recent years. In order for them to further expand their market share, users need to be able to fully control devices. Policies are one way for users to achieve such flexible control of devices. However, user policies often tend to interact in unwanted ways leading to unexpected behavior of devices. This paper describes the design of a run-time policy interaction management module (PIMM) that serves as manager for detecting and resolving interactions between user policies in KNX-based smart homes. This module extends the traditional KNX networking system with the ability to manage policy interactions. The module operates in the run-time S-mode of the KNX network and works as part of the engineering tool software (ETS) used to configure and control the operation of the KNX network in smart homes. The proposed module serves as the first of its kind that can be implemented inside the KNX networking system to detect and resolve unwanted policies interactions.},
	booktitle = {31st {Annual} {International} {Computer} {Software} and {Applications} {Conference} ({COMPSAC} 2007)},
	author = {Shehata, Mohamed and Eberlein, Armin and Fapojuwo, Abraham O.},
	month = jul,
	year = {2007},
	note = {ISSN: 0730-3157},
	keywords = {Application software, Control systems, Drives, Engineering management, Home computing, Runtime, Smart homes, Software tools, Telecommunication control, Telecommunication services},
	pages = {367--378},
}

@inproceedings{chaki_dynamic_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dynamic {Conflict} {Resolution} of {IoT} {Services} in {Smart} {Homes}},
	isbn = {978-3-030-91431-8},
	doi = {10.1007/978-3-030-91431-8_23},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. The proposed framework employs a preference extraction model based on a temporal proximity strategy. We design a preference aggregation model using a matrix factorization-based approach (i.e., singular value decomposition). The concepts of current resident item matrix and ideal resident item matrix are introduced as key criteria to cater to the conflict resolution framework. Finally, a set of experiments on real-world datasets are conducted to show the effectiveness of the proposed approach.},
	language = {en},
	booktitle = {Service-{Oriented} {Computing}},
	publisher = {Springer International Publishing},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	editor = {Hacid, Hakim and Kao, Odej and Mecella, Massimo and Moha, Naouel and Paik, Hye-young},
	year = {2021},
	keywords = {Conflict resolution, IoT service, Multi-resident smart home, Preference aggregation, Preference extraction},
	pages = {368--384},
}

@techreport{mccall_safetap_2021,
	type = {report},
	title = {{SafeTAP}: {An} {Efficient} {Incremental} {Analyzer} for {Trigger}-{Action} {Programs}},
	shorttitle = {{SafeTAP}},
	url = {https://kilthub.cmu.edu/articles/report/SafeTAP_An_Efficient_Incremental_Analyzer_for_Trigger-Action_Programs/14792271/1},
	abstract = {Home automation rules that allow users to connect smart home devices using trigger-action programs (TAP) can interact in subtle and unexpected ways. Determining whether these rules are free of undesirable behavior is challenging; so researchers have developed tools to analyze rules and assist users. However, it is unclear whether users need such tools, and what help they need from such tools. To answer this question, we performed a user study where half of the participants were given our custom analysis tool SafeTAP and the other half were not. We found that users are not good at finding issues in their TAP rules, despite perceiving such tasks as easy. The user study also indicates that users would like to check their rules every time they make rule changes. Therefore, we designed a novel incremental symbolic model checking (SMC) algorithm, which extends the basic SMC algorithm of SafeTAP. SafeTAPΔ only performs analysis caused by the addition or removal of rules and reports only new violations that have not already been reported to the user. We evaluate the performance of SafeTAPΔ and show that incremental checking on average improves the performance by 6X when adding new rules.},
	language = {en},
	urldate = {2022-05-06},
	institution = {Carnegie Mellon University},
	author = {McCall, McKenna and Shezan, Faysal Hossain and Bichhawat, Abhishek and Cobb, Camille and Jia, Limin and Tian, Yuan and Grace, Cooper and Yang, Mitchell},
	month = jun,
	year = {2021},
	doi = {10.1184/R1/14792271.v1},
}

@inproceedings{chaki_adaptive_2021,
	title = {Adaptive {Priority}-based {Conflict} {Resolution} of {IoT} {Services}},
	doi = {10.1109/ICWS53863.2021.00091},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. An adaptive priority model is developed considering the residents' contextual factors (e.g., age, illness, impairment). The proposed priority model is designed using the concept of the analytic hierarchy process. A set of experiments on real-world datasets are conducted to show the efficiency of the proposed approach.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	month = sep,
	year = {2021},
	keywords = {Adaptation models, Adaptive priority, Analytic hierarchy process, Analytical models, Conferences, Conflict resolution, Internet of Things, IoT service, Multi-resident smart home, Smart homes, Web services},
	pages = {663--668},
}

@incollection{yuan_deresolver_2021,
	address = {New York, NY, USA},
	title = {{DeResolver}: a decentralized negotiation and conflict resolution framework for smart city services},
	isbn = {978-1-4503-8353-0},
	shorttitle = {{DeResolver}},
	url = {https://doi.org/10.1145/3450267.3450538},
	abstract = {As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8\% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1\%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1\%, compared to priority-based solution.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the {ACM}/{IEEE} 12th {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yukun and Ma, Meiyi and Han, Songyang and Zhang, Desheng and Miao, Fei and Stankovic, John and Lin, Shan},
	month = may,
	year = {2021},
	keywords = {conflicts across services, decentralized resolution, multiple services negotiation, smart services},
	pages = {98--109},
}

@inproceedings{ma_detection_2016,
	title = {Detection of {Runtime} {Conflicts} among {Services} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP.2016.7501688},
	abstract = {The populations of large cities around the world are growing rapidly. Cities are beginning to address this problem by implementing significant sensing and actuation infrastructure and building services on this infrastructure. However, as the density of sensing and actuation increases and as the complexities of services grow there is an increasing potential for conflicts across Smart City services. These conflicts can cause unsafe situations and disrupt the benefits that the services were originally intended to provide. Although some of the conflicts can be detected and avoided during designing the services, many can still occur unpredictably during runtime. This paper carefully defines and enumerates the main issues regarding the detection and resolution of runtime conflicts in smart cities. In particular, it focuses on conflicts that arise across services. This issue is becoming more and more important as Smart City designs attempt to integrate services from different domains (transportation, energy, public safety, emergency, medical, and many others). Research challenges are identified and then addressed that deal with uncertainty, dynamism, real-time, mobility and spatio-temporal availability, duration and scale of effect, efficiency, and ownership. A watchdog architecture is also described that oversees the services operating in a Smart City. This watchdog solution detects and resolves conflicts, it learns and adapts, and it provides additional inputs to decision making aspects of services. Using data from a Smart City dataset, an emulated set of services and activities using those services are created to perform a conflict analysis. A second analysis hypothesizes 41 future services across 5 domains. Both of these evaluations demonstrate the high probability of conflicts in smart cities of the future.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Ma, M. and Preum, S. Masud and Tarneberg, W. and Ahmed, M. and Ruiters, M. and Stankovic, J.},
	month = may,
	year = {2016},
	keywords = {Roads, Safety, Smart cities, Uncertainty, Vehicles},
	pages = {1--10},
}

@inproceedings{alhanahnah_scalable_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Scalable analysis of interaction threats in {IoT} systems},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397347},
	doi = {10.1145/3395363.3397347},
	abstract = {The ubiquity of Internet of Things (IoT) and our growing reliance on IoT apps are leaving us more vulnerable to safety and security threats than ever before. Many of these threats are manifested at the interaction level, where undesired or malicious coordinations between apps and physical devices can lead to intricate safety and security issues. This paper presents IoTCOM, an approach to automatically discover such hidden and unsafe interaction threats in a compositional and scalable fashion. It is backed with auto-mated program analysis and formally rigorous violation detection engines. IoTCOM relies on program analysis to automatically infer the relevant app’s behavior. Leveraging a novel strategy to trim the extracted app’s behavior prior to translating them to analyzable formal specifications,IoTCOM mitigates the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCOM’s ability to effectively detect a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown, and to significantly outperform the existing techniques in terms of scalability.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Alhanahnah, Mohannad and Stevens, Clay and Bagheri, Hamid},
	month = jul,
	year = {2020},
	keywords = {Formal Verification, Interaction Threats, IoT Safety},
	pages = {272--285},
}

@article{sun_conflict_2015,
	title = {Conflict {Detection} {Scheme} {Based} on {Formal} {Rule} {Model} for {Smart} {Building} {Systems}},
	volume = {45},
	issn = {2168-2305},
	doi = {10.1109/THMS.2014.2364613},
	abstract = {Smart building systems can provide flexible and configurational sensing and controlling operations according to users' requirements. As the number and the complexity of service rules customized by users have significantly increased, there is an increasing danger of conflict during the interaction process between users and the system. To address this issue, we propose a new rule conflict detection scheme tailored for the smart building system. First, we present a formal rule model UTEA based on User, Triggers, Environment entities, and Actuators. This model can handle not only controlled devices with discrete status but also real-valued environmental data such as temperature and humidity. In addition, this model takes multiple users with different authorities into account. Second, we define 11 rule relations and further classify conflicts into five categories. Third, we implement a rule storage system for detecting conflicts and design a conflict detection algorithm, which can detect the conflict between two rules as well as cycle conflict/multicross contradiction among multiple rules. We evaluated our scheme in a real smart building system with more than 30 000 service rules. The experiment results show that our scheme improves the performance in terms of error/missed-detection rates and running time.},
	number = {2},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Sun, Yan and Wang, Xukai and Luo, Hong and Li, Xiangyang},
	month = apr,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Human-Machine Systems},
	keywords = {Actuators, Conflict detection, Humidity, Intelligent sensors, Smart buildings, Sun, Temperature sensors, rule model, service, smart building system},
	pages = {215--227},
}

@inproceedings{ding_safety_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT{\textasciitilde}environment.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	keywords = {internet of things, physical interaction control, safety},
	pages = {832--846},
}

@article{yu_tapinspector_2021,
	title = {{TAPInspector}: {Safety} and {Liveness} {Verification} of {Concurrent} {Trigger}-{Action} {IoT} {Systems}},
	shorttitle = {{TAPInspector}},
	url = {http://arxiv.org/abs/2102.01468},
	abstract = {Trigger-action programming (TAP) is a popular end-user programming framework that can simplify the Internet of Things (IoT) automation with simple trigger-action rules. However, it also introduces new security and safety threats. A lot of advanced techniques have been proposed to address this problem. Rigorously reasoning about the security of a TAP-based IoT system requires a well-defined model and verification method both against rule semantics and physical-world states, e.g., concurrency, rule latency, and connection-based interactions, which has been missing until now. This paper presents TAPInspector, a novel system to detect vulnerabilities in concurrent TAP-based IoT systems using model checking. It automatically extracts TAP rules from IoT apps, translates them into a hybrid model with model slicing and state compression, and performs model checking with various safety and liveness properties. Our experiments corroborate that TAPInspector is effective: it identifies 533 violations with 9 new types of violations from 1108 real-world market IoT apps and is 60000 times faster than the baseline without optimization at least.},
	urldate = {2022-05-06},
	journal = {arXiv:2102.01468 [cs]},
	author = {Yu, Yinbo and Liu, Jiajia},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.01468},
	keywords = {C.2.0, Computer Science - Cryptography and Security, Computer Science - Human-Computer Interaction, Computer Science - Networking and Internet Architecture, Computer Science - Software Engineering, D.2.4, F.3.1},
}

@article{ozmen_discovering_2021,
	title = {Discovering {Physical} {Interaction} {Vulnerabilities} in {IoT} {Deployments}},
	url = {http://arxiv.org/abs/2102.01812},
	abstract = {Internet of Things (IoT) applications drive the behavior of IoT deployments according to installed sensors and actuators. It has recently been shown that IoT deployments are vulnerable to physical interactions, caused by design flaws or malicious intent, that can have severe physical consequences. Yet, extant approaches to securing IoT do not translate the app source code into its physical behavior to evaluate physical interactions. Thus, IoT consumers and markets do not possess the capability to assess the safety and security risks these interactions present. In this paper, we introduce the IoTSeer security service for IoT deployments, which uncovers undesired states caused by physical interactions. IoTSeer operates in four phases (1) translation of each actuation command and sensor event in an app source code into a hybrid I/O automaton that defines an app's physical behavior, (2) combining apps in a novel composite automaton that represents the joint physical behavior of interacting apps, (3) applying grid-based testing and falsification to validate whether an IoT deployment conforms to desired physical interaction policies, and (4) identification of the root cause of policy violations and proposing patches that guide users to prevent them. We use IoTSeer in an actual house with 13 actuators and six sensors with 37 apps and demonstrate its effectiveness and performance.},
	urldate = {2022-05-06},
	journal = {arXiv:2102.01812 [cs]},
	author = {Ozmen, Muslum Ozgur and Li, Xuansong and Chu, Andrew Chun-An and Celik, Z. Berkay and Hoxha, Bardh and Zhang, Xiangyu},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.01812},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Formal Languages and Automata Theory},
}

@incollection{stevens_comparing_2020,
	address = {New York, NY, USA},
	title = {Comparing formal models of {IoT} app coordination analysis},
	isbn = {978-1-4503-8126-0},
	url = {https://doi.org/10.1145/3416507.3423188},
	abstract = {The rising popularity of the Internet-of-Things (IoT) devices has driven their increasing adoption in various settings, such as modern homes. IoT systems integrate such physical devices with third-party apps, which can coordinate in arbitrary ways. However, malicious or undesired coordination can lead to serious vulnerabilities. This paper explores two different ways, i.e., a commonly-used state-based approach and a holistic, rule-based approach, to formally model app coordination and the safety and security thereof in the context of IoT platforms. The less common rule-base approach allows for a smaller, more scalable model. We realize both modeling approaches using bounded model checking with Alloy to automatically identify potential cases where apps exhibit coordination relationships. We evaluate the effectiveness of the modeling approaches by checking a corpus of real-world IoT apps of Samsung SmartThings and IFTTT. The experimental results demonstrate that our rule-based modeling leads to a more scalable analysis.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the 3rd {ACM} {SIGSOFT} {International} {Workshop} on {Software} {Security} from {Design} to {Deployment}},
	publisher = {Association for Computing Machinery},
	author = {Stevens, Clay and Alhanahnah, Mohannad and Yan, Qiben and Bagheri, Hamid},
	month = nov,
	year = {2020},
	keywords = {Coordination Threats, Formal Verification, IoT Safety},
	pages = {3--10},
}

@inproceedings{chi_cross-app_2020,
	title = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}: {Categorization}, {Detection} and {Handling}},
	shorttitle = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}},
	doi = {10.1109/DSN48063.2020.00056},
	abstract = {Internet of Thing platforms prosper home automation applications (apps). Prior research concerns intra-app security. Our work reveals that automation apps, even secured individually, still cause a family of threats when they interplay, termed as Cross-App Interference (CAI) threats. We systematically categorize such threats and encode them using satisfiability modulo theories (SMT). We present HomeGuard, a system for detecting and handling CAI threats in real deployments. A symbolic executor is built to extract rule semantics, and instrumentation is utilized to capture configuration during app installation. Rules and configuration are checked against SMT models, the solutions of which indicate the existence of corresponding CAI threats. We further combine app functionalities, device attributes and CAI types to label the risk level of CAI instances. In our evaluation, HomeGuard discovers 663 CAI instances from 146 SmartThings market apps, imposing minor latency upon app installation and no runtime overhead.},
	booktitle = {2020 50th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Chi, Haotian and Zeng, Qiang and Du, Xiaojiang and Yu, Jiaping},
	month = jun,
	year = {2020},
	note = {ISSN: 1530-0889},
	keywords = {Actuators, Automation, Safety, Semantics, Smart homes, Temperature sensors},
	pages = {411--423},
}

@inproceedings{jin_kang_iotbox_2021,
	title = {{IoTBox}: {Sandbox} {Mining} to {Prevent} {Interaction} {Threats} in {IoT} {Systems}},
	shorttitle = {{IoTBox}},
	doi = {10.1109/ICST49551.2021.00029},
	abstract = {Internet of Things (IoT) apps provide great convenience but exposes us to new safety threats. Unlike traditional software systems, threats may emerge from the joint behavior of multiple apps. While prior studies use handcrafted safety and security policies to detect these threats, these policies may not anticipate all usages of the devices and apps in a smart home, causing false alarms. In this study, we propose to use the technique of mining sandboxes for securing an IoT environment. After a set of behaviors are analyzed from a bundle of apps and devices, a sandbox is deployed, which enforces that previously unseen behaviors are disallowed. Hence, the execution of malicious behavior, introduced from software updates or obscured through methods to hinder program analysis, is blocked.While sandbox mining techniques have been proposed for Android apps, we show and discuss why they are insufficient for detecting malicious behavior in a more complex IoT system. We prototype IoTBox to address these limitations. IoTBox explores behavior through a formal model of a smart home. In our empirical evaluation to detect malicious code changes, we find that IoTBox achieves substantially higher precision and recall compared to existing techniques for mining sandboxes.},
	booktitle = {2021 14th {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Jin Kang, Hong and Qin Sim, Sheng and Lo, David},
	month = apr,
	year = {2021},
	note = {ISSN: 2159-4848},
	keywords = {Conferences, Malware, Prototypes, Safety, Smart homes, Software systems, Software testing, interaction threats, internet of things, mining sandboxes},
	pages = {182--193},
}

@inproceedings{ma_cityguard_2017,
	address = {New York, NY, USA},
	series = {{IoTDI} '17},
	title = {{CityGuard}: {A} {Watchdog} for {Safety}-{Aware} {Conflict} {Detection} in {Smart} {Cities}},
	isbn = {978-1-4503-4966-6},
	shorttitle = {{CityGuard}},
	url = {https://doi.org/10.1145/3054977.3054989},
	doi = {10.1145/3054977.3054989},
	abstract = {Nowadays, increasing number of smart services are being developed and deployed in cities around the world. IoT platforms have emerged to integrate smart city services and city resources, and thus improve city performance in the domains of transportation, emergency, environment, public safety, etc. Despite the increasing intelligence of smart services and the sophistication of platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, CityGuard, a safety-aware watchdog architecture is developed. To the best of our knowledge, it is the first architecture that detects and resolves conflicts among actions of different services considering both safety and performance requirements. Prior to developing CityGuard, safety and performance requirements and a spectrum of conflicts are specified. Sophisticated models are used to analyze secondary effects, and detect device and environmental conflicts. A simulation based on New York City is used for the evaluation. The results show that CityGuard (i) identifies unsafe actions and thus helps to prevent the city from safety hazards, (ii) detects and resolves two major types of conflicts, i.e., device and environmental conflicts, and (iii) improves the overall city performance.},
	urldate = {2022-05-06},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
	month = apr,
	year = {2017},
	keywords = {City Safety, City Simulation, Conflict Detection, Smart City},
	pages = {259--270},
}

@misc{international_organization_for_standardization_iso_2018,
	title = {{ISO} 26262-1:2018},
	shorttitle = {{ISO} 26262-1},
	url = {https://www.iso.org/standard/68383.html},
	abstract = {This document is intended to be applied to safety-related systems that include one or more electrical and/or electronic (E/E) systems and that are installed in series production road vehicles, excluding mopeds. This document does not address unique E/E systems in special vehicles such as E/E systems designed for drivers with disabilities.

NOTE Other dedicated application-specific safety standards exist and can complement the ISO 26262 series of standards or vice versa.

Systems and their components released for production, or systems and their components already under development prior to the publication date of this document, are exempted from the scope of this edition. This document addresses alterations to existing systems and their components released for production prior to the publication of this document by tailoring the safety lifecycle depending on the alteration. This document addresses integration of existing systems not developed according to this document and systems developed according to this document by tailoring the safety lifecycle.

This document addresses possible hazards caused by malfunctioning behaviour of safety-related E/E systems, including interaction of these systems. It does not address hazards related to electric shock, fire, smoke, heat, radiation, toxicity, flammability, reactivity, corrosion, release of energy and similar hazards, unless directly caused by malfunctioning behaviour of safety-related E/E systems.

This document describes a framework for functional safety to assist the development of safety-related E/E systems. This framework is intended to be used to integrate functional safety activities into a company-specific development framework. Some requirements have a clear technical focus to implement functional safety into a product; others address the development process and can therefore be seen as process requirements in order to demonstrate the capability of an organization with respect to functional safety.

This document defines the vocabulary of terms used in the ISO 26262 series of standards.},
	language = {en},
	urldate = {2022-05-04},
	journal = {ISO},
	author = {{International Organization for Standardization}},
	month = dec,
	year = {2018},
}

@article{wong_trust_2014,
	title = {Trust and {Privacy} {Exploitation} in {Online} {Social} {Networks}},
	volume = {16},
	issn = {1941-045X},
	doi = {10.1109/MITP.2014.79},
	abstract = {Online social networks have been typically created for convenience–so they haven't been built from the ground up with security in mind. They often have confusing privacy settings and are susceptible to various kinds of attacks that exploit users' trust and privacy. In this article, the authors discuss security pitfalls in today's social networks, briefly introducing common attack methods. They implemented a proof-of-concept Facebook app, which is actually a harmless malware that uses common attack methods to demonstrate the vulnerability of online social networks. Although today's online social networks commonly offer users a variety of security settings, users tend to trust the information obtained from online social networks regardless of the settings. This kind of user mentality can be more crucial than technical aspects in determining the level of security in online social networks.},
	number = {5},
	journal = {IT Professional},
	author = {Wong, Kaze and Wong, Angus and Yeung, Alan and Fan, Wei and Tang, Su-Kit},
	month = sep,
	year = {2014},
	note = {Conference Name: IT Professional},
	keywords = {Computer security, Facebook, Games, Information networks, Internet/Web technologies, Malware, Mobile communication, Privacy, Social network services, Trust management, Web and Internet services, information network, mobile, networking, privacy, security},
	pages = {28--33},
}

@article{tang_decision-making_2021,
	title = {Decision-{Making} {Principles} for {Better} {Software} {Design} {Decisions}},
	volume = {38},
	issn = {1937-4194},
	doi = {10.1109/MS.2021.3102358},
	abstract = {Software design is about making decisions. The quality of design decisions influences the quality of software design. This article describes nine decision-making principles to give software designers a systematic approach for decision making.},
	number = {6},
	journal = {IEEE Software},
	author = {Tang, Antony and Kazman, Rick},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Software},
	keywords = {Decision making, Software design, Software development management, Systematics},
	pages = {98--102},
}

@article{fatima_improving_2019,
	title = {Improving {Software} {Requirements} {Reasoning} by {Novices}: {A} {Story}-{Based} {Approach}},
	shorttitle = {Improving {Software} {Requirements} {Reasoning} by {Novices}},
	doi = {10.1049/iet-sen.2018.5379},
	abstract = {Context: Requirements-elicitation is one of the essential steps towards software design and construction. Business analysts and stakeholders often face challenges in gathering or conveying key software requirements. There are many methods, and tools designed by researchers and practitioners, but with the invention of new technologies, there appears to be a need to make requirements gathering and design-rationale process more efficient. Storytelling is an emerging concept and researchers are witnessing its effectiveness in education, community-building, information system, and requirement elicitation. 

Objective: Objectives of this study are to i) devise a method for requirements elicitation and improving design-rationales using story-based technique; ii) evaluate the effectiveness of the aforementioned proposed activity.

Methodology: To answer the research objectives, we have i) conducted open-ended interviews to get feedback on our proposed method; ii) case requirement from a running project to map how this method can be useful; and iii) performed empirical evaluation of the proposed card-based activity.

Result: i) Our regression model has shown that participants' perception regarding the ease of use and the fun in the game has an ultimate effect on requirements elicitation through enhancing user's desire to play the game, hence, increasing the collaborative learning outcomes of the game; ii) Our results have shown that using team-based activities helps the less-experienced designers to argue through design rationales and better elicit software requirements. Our results have reinforced the finding that using game-based solutions not only enhances communication and develops trust between stakeholders but also helps in motivating participants of requirements activity; iii) Initial results (from interview and empirical evaluation) for the proposed technique and method show positive results. Improvement in the process and activity as suggested by the participants will be accommodated in future studies.},
	journal = {IET Software},
	author = {Fatima, Rubia and Yasin Chouhan, Affan and Liu, Lin and Wang, Jianmin and Afzal, Wasif and Yasin, Atif},
	month = jul,
	year = {2019},
}

@article{lee_design_1997,
	title = {Design rationale systems: understanding the issues},
	volume = {12},
	issn = {2374-9407},
	shorttitle = {Design rationale systems},
	doi = {10.1109/64.592267},
	abstract = {Most current design rationale systems fail to consider practical concerns, such as cost-effective use and smooth integration. The author identifies seven technical and business issues and describes their implications.},
	number = {3},
	journal = {IEEE Expert},
	author = {Lee, J.},
	month = may,
	year = {1997},
	note = {Conference Name: IEEE Expert},
	keywords = {Collaboration, Collaborative tools, Costs, Decision making, Design engineering, Documentation, Problem-solving, Project management, Tail, Writing},
	pages = {78--85},
}

@article{dorst_creativity_2001,
	title = {Creativity in the design process: co-evolution of problem–solution},
	volume = {22},
	issn = {0142-694X},
	shorttitle = {Creativity in the design process},
	url = {https://www.sciencedirect.com/science/article/pii/S0142694X01000096},
	doi = {10.1016/S0142-694X(01)00009-6},
	abstract = {Empirical data on design processes were obtained from a set of protocol studies of nine experienced industrial designers, whose designs were evaluated on overall quality and on a variety of aspects including creativity. From the protocol data we identify aspects of creativity in design related to the formulation of the design problem and to the concept of originality. We also apply our observations to a model of creative design as the co-evolution of problem/solution spaces, and confirm the general validity of the model. We propose refinements to the co-evolution model, and suggest relevant new concepts of ‘default’ and ‘surprise’ problem/solution spaces.},
	language = {en},
	number = {5},
	urldate = {2022-05-04},
	journal = {Design Studies},
	author = {Dorst, Kees and Cross, Nigel},
	month = sep,
	year = {2001},
	keywords = {co-evolution, creative design, design process, product design},
	pages = {425--437},
}

@article{falessi_design_2006,
	title = {Design decision rationale: experiences and steps ahead towards systematic use},
	volume = {31},
	issn = {0163-5948},
	shorttitle = {Design decision rationale},
	url = {https://dl.acm.org/doi/10.1145/1163514.1178642},
	doi = {10.1145/1163514.1178642},
	abstract = {Design decisions crucially influence the success of every software project. While the resulting design is typically documented quite well, the situation is usually different for the underlying rationale and decision-making process. Despite being recognized as a helpful approach in general, the explicit documentation of Design Decision Rationale (DDR) is not yet largely utilized due to some inhibitors (e.g., additional documentation effort). Experience with other qualities, e.g. software reusability, evidently shows that an improvement of these qualities only pays off on a large scale and therefore has to be pursued in a strategic, pre-planned, and carefully focused way. In this paper we argue that this also has to be considered for documenting DDR. To this end the paper presents: (i) the Decision, Goal, and Alternatives (DGA) DDR framework, (ii) experience in dealing with DGA, (iii) motivators and inhibitors of using DDR, and (iv) an approach for systematic DDR use that follows value-based software engineering principles.},
	language = {en},
	number = {5},
	urldate = {2022-05-04},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Falessi, Davide and Becker, Martin and Cantone, Giovanni},
	month = sep,
	year = {2006},
	pages = {2},
}

@inproceedings{jarczyk_design_1992,
	title = {Design {Rationale} for {Software} {Engineering}: {A} {Survey}},
	shorttitle = {Design {Rationale} for {Software} {Engineering}},
	abstract = {We provide an introduction to what design rationale is and why it is important in software engineering. We look at the recent history of argumentation methods. We survey a number of the major systems developed for the support of design rationale, comparing their features and discussing their differences. We look at advantages and disadvantages of the various approaches to design rationale with special attention paid to how they can be used in the process of software engineering. We conclude with a discussion of some open issues which are important for the inclusion of},
	publisher = {Press},
	author = {Jarczyk, Alex P. J. and Löffler, Peter and Iii, Frank M. Shipman},
	year = {1992},
	pages = {577--586},
}

@inproceedings{razavian_reflective_2016,
	title = {Reflective {Approach} for {Software} {Design} {Decision} {Making}},
	doi = {10.1109/QRASA.2016.8},
	abstract = {Good software design practice is difficult to define and teach. Despite the many software design methods and processes that are available, the quality of software design relies on human factors. We notice from literature and our own experiments that some of these factors concern design reasoning and reflection. In this paper, we propose a reflective approach to software design decision making. The approach is built upon Two-Minds model and is enabled by a set of problem-generic reflective questions. We illustrate its usefulness in design sessions with an example taken from preliminary experimentation.},
	booktitle = {2016 {Qualitative} {Reasoning} about {Software} {Architectures} ({QRASA})},
	author = {Razavian, Maryam and Tang, Antony and Capilla, Rafael and Lago, Patricia},
	month = apr,
	year = {2016},
	keywords = {Cognition, Context, Decision making, Object oriented modeling, Problem-solving, Software design},
	pages = {19--26},
}

@article{tang_improving_2018,
	title = {Improving software design reasoning–{A} reminder card approach},
	volume = {144},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218301043},
	doi = {10.1016/j.jss.2018.05.019},
	abstract = {Software designers have been known to think naturalistically. This means that there may be inadequate rational thinking during software design. In the past two decades, many research works suggested that designers need to produce design rationale. However, design rationale can be produced to retrofit naturalistic decisions, which means that design decisions may still not be well reasoned. Through a controlled experiment, we studied design reasoning and design rationale by asking participants to carry out a group design. As treatment, we provided 6 out of 12 student teams with a set of reasoning reminder cards to see how they compare with teams without the reminder cards. Additionally, we performed the same experiment with 2 teams of professionals who used the reminder cards, and compared the results with 3 teams of professionals. The experimental results show that both professionals and students who were equipped with the reasoning reminder cards reasoned more with their design. Second, the more a team discusses design reasoning, the more design rationale they find.},
	language = {en},
	urldate = {2022-05-04},
	journal = {Journal of Systems and Software},
	author = {Tang, Antony and Bex, Floris and Schriek, Courtney and van der Werf, Jan Martijn E. M.},
	month = oct,
	year = {2018},
	pages = {22--40},
}

@misc{github_safety_2018,
	title = {[{Safety}] {Ability} to block everyone following a certain account},
	url = {https://github.com/mastodon/mastodon/issues/8238},
	abstract = {Use case: find the main dickheads in a toxic group and block all of their crew in one swift move. Source I searched or browsed the repo’s other issues to ensure this is not a duplicate.},
	language = {en},
	urldate = {2022-05-04},
	journal = {mastodon/mastodon},
	author = {{GitHub}},
	month = aug,
	year = {2018},
	note = {Issue \#8238},
}

@misc{international_electrotechnical_commission_iec_2010,
	title = {{IEC} 61508-1:2010 {\textbar} {IEC} {Webstore} {\textbar} functional safety, smart city},
	url = {https://webstore.iec.ch/publication/5515},
	abstract = {IEC 61508-1:2010 covers those aspects to be considered when electrical/electronic/programmable electronic (E/E/PE) systems are used to carry out safety functions. A major objective of this standard is to facilitate the development of product and application sector international standards by the technical committees responsible for the product or application sector. This will allow all the relevant factors, associated with the product or application, to be fully taken into account and thereby meet the specific needs of users of the product and the application sector. A second objective of this standard is to enable the development of E/E/PE safety-related systems where product or application sector international standards do not exist. This second edition cancels and replaces the first edition published in 1998. This edition constitutes a technical revision. It has been subject to a thorough review and incorporates many comments received at the various revision stages. It has the status of a basic safety publication according to IEC Guide 104.},
	urldate = {2022-05-03},
	journal = {IEC Webstore},
	author = {{International Electrotechnical Commission}},
	month = apr,
	year = {2010},
}

@article{corbin1990grounded,
	title = {Grounded theory research: {Procedures}, canons, and evaluative criteria},
	volume = {13},
	number = {1},
	journal = {Qualitative sociology},
	author = {Corbin, Juliet M and Strauss, Anselm},
	year = {1990},
	note = {Publisher: Springer},
	pages = {3--21},
}

@article{zannier_model_2007,
	series = {Qualitative {Software} {Engineering} {Research}},
	title = {A model of design decision making based on empirical results of interviews with software designers},
	volume = {49},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584907000122},
	doi = {10.1016/j.infsof.2007.02.010},
	abstract = {Despite the impact of design decisions on software design, we have little understanding about how design decisions are made. This hinders our ability to provide design metrics, processes and training that support inherent design work. By interviewing 25 software designers and using content analysis and explanation building as our analysis technique, we provide qualitative and quantitative results that highlight aspects of rational and naturalistic decision making in software design. Our qualitative multi-case study results in a model of design decision making to answer the question: how do software designers make design decisions? We find the structure of the design problem determines the aspects of rational and naturalistic decision making used. The more structured the design decision, the less a designer considers options.},
	language = {en},
	number = {6},
	urldate = {2022-05-03},
	journal = {Information and Software Technology},
	author = {Zannier, Carmen and Chiasson, Mike and Maurer, Frank},
	month = jun,
	year = {2007},
	keywords = {Design decision, Interviewing, Naturalistic decision making, Rational decision making},
	pages = {637--653},
}

@inproceedings{zannier_qualitative_2005,
	title = {A qualitative empirical evaluation of design decisions},
	abstract = {In this paper, we motivate examining software design decision making and provide the process by which the examination will occur. The objective is to provide qualitative results indicative of rational or naturalistic software design decision making. In a rational decision a decision maker evaluates decision alternatives and potential outcomes for each alternative using a utility function and probabilities of the outcome of each alternative. The utility function assigns a value to each possible alternative based on its outcome. The goal of rational decision making is selecting the optimal alternative. A naturalistic decision manifests itself in dynamic and continually changing conditions, embodies real-time reactions to these changes, embraces ill-defined tasks, and has a goal of selecting a satisfactory alternative. The proposed empirical qualitative study consists of inductive and deductive interviewing and deductive observations.},
	booktitle = {in: {Workshop} on {Human} \& {Social} {Factors} of {Soft}. {Eng}},
	publisher = {ACM Press},
	author = {Zannier, Carmen and Maurer, Frank},
	year = {2005},
}

@article{christiaans_accessing_2010,
	series = {Special {Issue} {Studying} {Professional} {Software} {Design}},
	title = {Accessing decision-making in software design},
	volume = {31},
	issn = {0142-694X},
	url = {https://www.sciencedirect.com/science/article/pii/S0142694X10000670},
	doi = {10.1016/j.destud.2010.09.005},
	abstract = {This paper presents an analysis of software design protocols as one of the contributions to the 2010 international workshop ‘Studying Professional Software Design’. The aim of the study described here is to analyse the design process of software designers and to compare the results with that of product designers, an area familiar to the authors. Decision-making is the main focus of this study. A descriptive model of decision-making, developed by the authors, has been used to analyse the protocols of the three software design teams. The results give insight in how software designers process their activities, on the influence of individual or team differences, and what the consequences for their outcomes are.},
	language = {en},
	number = {6},
	urldate = {2022-05-03},
	journal = {Design Studies},
	author = {Christiaans, Henri and Almendra, Rita Assoreira},
	month = nov,
	year = {2010},
	keywords = {decision-making, design behaviour, design cognition, design process},
	pages = {641--662},
}

@article{tang_what_2010,
	series = {Special {Issue} {Studying} {Professional} {Software} {Design}},
	title = {What makes software design effective?},
	volume = {31},
	issn = {0142-694X},
	url = {https://www.sciencedirect.com/science/article/pii/S0142694X10000669},
	doi = {10.1016/j.destud.2010.09.004},
	abstract = {Software design is a complex cognitive process in which decision making plays a major role, but our understanding of how decisions are made is limited, especially with regards to reasoning with design problems and formulation of design solutions. In this research, we have observed software designers at work and have analysed how they make decisions during design. We report on how factors such as design planning, design context switching, problem-solution co-evolution and the application of reasoning techniques influence software design effectiveness.},
	language = {en},
	number = {6},
	urldate = {2022-05-03},
	journal = {Design Studies},
	author = {Tang, Antony and Aleti, Aldeida and Burge, Janet and van Vliet, Hans},
	month = nov,
	year = {2010},
	keywords = {decision making, design effectiveness, design reasoning, software design},
	pages = {614--640},
}

@inproceedings{falessi_documenting_2006,
	address = {New York, NY, USA},
	series = {{ISESE} '06},
	title = {Documenting design decision rationale to improve individual and team design decision making: an experimental evaluation},
	isbn = {978-1-59593-218-1},
	shorttitle = {Documenting design decision rationale to improve individual and team design decision making},
	url = {https://doi.org/10.1145/1159733.1159755},
	doi = {10.1145/1159733.1159755},
	abstract = {Individual and team decision-making have crucial influence on the level of success of every software project. Even though several studies were already conducted, which concerned design decision rationale documentation approaches, a few of them focused on performances and evaluated them in laboratory. This paper proposes a technique to document design decision rationale, and evaluates experimentally the impact such a technique has on effectiveness and efficiency of individual/team decision-making in presence of requirement changes. The study was conducted as a controlled experiment. Fifty post-graduate Master students performed in the role of experiment subjects. Documented design decisions regarding the Ambient Intelligence paradigm constituted the experiment objects. Main results of the experiment show that, for both individual and team-based decision-making, effectiveness significantly improves, while efficiency remains unaltered, when decision-makers are allowed to use, rather not use, the proposed design rationale documentation technique.},
	urldate = {2022-05-03},
	booktitle = {Proceedings of the 2006 {ACM}/{IEEE} international symposium on {Empirical} software engineering},
	publisher = {Association for Computing Machinery},
	author = {Falessi, Davide and Cantone, Giovanni and Becker, Martin},
	month = sep,
	year = {2006},
	keywords = {design decision rationale, experimental evaluation, individual and team decision-making},
	pages = {134--143},
}

@article{boudette_tesla_2021,
	chapter = {Business},
	title = {Tesla {Says} {Autopilot} {Makes} {Its} {Cars} {Safer}. {Crash} {Victims} {Say} {It} {Kills}.},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/07/05/business/tesla-autopilot-lawsuits-safety.html},
	abstract = {A California family that lost a 15-year-old boy when a Tesla hit its pickup truck is suing the company, claiming its Autopilot system was partly responsible.},
	language = {en-US},
	urldate = {2022-05-02},
	journal = {The New York Times},
	author = {Boudette, Neal E.},
	month = jul,
	year = {2021},
	keywords = {Automobile Safety Features and Defects, Deaths (Fatalities), Driverless and Semiautonomous Vehicles, Electric and Hybrid Vehicles, Maldonado, Jovani (d 2019), Musk, Elon, National Highway Traffic Safety Administration, Suits and Litigation (Civil), Tesla Motors Inc, Traffic Accidents and Safety},
}

@article{noauthor_tesla_nodate,
	title = {Tesla {Says} {Autopilot} {Makes} {Its} {Cars} {Safer}. {Crash} {Victims} {Say} {It} {Kills}. - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/07/05/business/tesla-autopilot-lawsuits-safety.html},
	urldate = {2022-01-03},
}

@article{ozkan2020expectations,
	title = {Expectations and experiences of short-term study abroad leadership teams},
	volume = {2},
	number = {1},
	journal = {Journal of International Engineering Education},
	author = {Ozkan, Desen S and Davis, Kirsten A and Davis, James C and James, Matthew and Murzi, Homero and Knight, David B},
	year = {2020},
	pages = {1},
}

@misc{noauthor_software_2021,
	title = {Software {Bill} of {Materials} {Elements} and {Considerations}},
	url = {https://www.federalregister.gov/documents/2021/06/02/2021-11592/software-bill-of-materials-elements-and-considerations},
	abstract = {The Executive Order on Improving the Nation's Cybersecurity directs the Department of Commerce, in coordination with the National Telecommunications and Information Administration (NTIA), to publish the minimum elements for a Software Bill of Materials (SBOM). Through this Notice, following from...},
	urldate = {2022-04-29},
	journal = {Federal Register},
	month = jun,
	year = {2021},
}

@article{kayes_privacy_2017,
	title = {Privacy and security in online social networks: {A} survey},
	volume = {3-4},
	issn = {2468-6964},
	shorttitle = {Privacy and security in online social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S2468696417300332},
	doi = {10.1016/j.osnem.2017.09.001},
	abstract = {Online social networks (OSN) are a permanent presence in today’s personal and professional lives of a huge segment of the population, with direct consequences to offline activities. Built on a foundation of trust – users connect to other users with common interests or overlapping personal trajectories – online social networks and the associated applications extract an unprecedented volume of personal information. Unsurprisingly, serious privacy and security risks emerged, positioning themselves along two main types of attacks: attacks that exploit the implicit trust embedded in declared social relationships; and attacks that harvest user’s personal information for ill-intended use. This article provides an overview of the privacy and security issues that emerged so far in OSNs. We introduce a taxonomy of privacy and security attacks in OSNs, we overview existing solutions to mitigate those attacks, and outline challenges still to overcome.},
	language = {en},
	urldate = {2022-04-28},
	journal = {Online Social Networks and Media},
	author = {Kayes, Imrul and Iamnitchi, Adriana},
	month = oct,
	year = {2017},
	keywords = {Online social networks, Privacy, Security},
	pages = {1--21},
}

@misc{noauthor_details_2019,
	title = {Details of the {Cloudflare} outage on {July} 2, 2019},
	url = {http://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/},
	abstract = {Almost nine years ago, Cloudflare was a tiny company and I was a customer not an employee. Cloudflare had launched a month earlier and one day alerting told me that my little site, jgc.org, didn’t seem to have working DNS any more.},
	language = {en},
	urldate = {2022-04-28},
	journal = {The Cloudflare Blog},
	month = jul,
	year = {2019},
}

@misc{noauthor_cloudflare_nodate,
	title = {Cloudflare, {Inc}. ({NET}) {Stock} {Price}, {News}, {Quote} \& {History} - {Yahoo} {Finance}},
	url = {https://finance.yahoo.com/quote/NET/},
	urldate = {2022-04-28},
}

@misc{noauthor_dexcom_nodate,
	title = {{DexCom}, {Inc}. ({DXCM}) {Stock} {Price}, {News}, {Quote} \& {History} - {Yahoo} {Finance}},
	url = {https://finance.yahoo.com/quote/DXCM/},
	abstract = {Find the latest DexCom, Inc. (DXCM) stock quote, history, news and other vital information to help you with your stock trading and investing.},
	language = {en-US},
	urldate = {2022-04-28},
}

@misc{noauthor_consultations_nodate,
	title = {Consultations - {Center} for {Instructional} {Excellence} - {Purdue} {University}},
	url = {https://www.purdue.edu/cie/teachingresources/consultations.html},
	urldate = {2022-04-26},
}

@inproceedings{marshall2014vertically,
	title = {The vertically integrated projects ({VIP}) program: leveraging faculty research interests to transform undergraduate {STEM} education},
	booktitle = {Transforming institutions: 21st century undergraduate {STEM} education conference},
	author = {Marshall, Stephen and Coyle, Edward and Krogmeier, James V and Abler, Randal T and Johnson, Amos and Gilchrist, Brian E},
	year = {2014},
}

@book{lamport2002specifying,
	title = {Specifying systems: {The} {TLA}+ language and tools for hardware and software engineers},
	volume = {388},
	publisher = {Addison-Wesley Boston},
	author = {Lamport, Leslie},
	year = {2002},
}

@misc{holzmann2004holzmann,
	title = {Holzmann. {The} {SPIN} model {Checker}—{Primer} and reference manual},
	publisher = {Addison-Wesley, Reading},
	author = {Holzmann, Gerard J},
	year = {2004},
}

@book{abrial2010modeling,
	title = {Modeling in {Event}-{B}: system and software engineering},
	publisher = {Cambridge University Press},
	author = {Abrial, Jean-Raymond},
	year = {2010},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.acm.org/code-of-ethics},
}

@article{bail_social-media_2022,
	title = {Social-media reform is flying blind},
	volume = {603},
	copyright = {2022 Nature},
	url = {https://www.nature.com/articles/d41586-022-00805-0},
	doi = {10.1038/d41586-022-00805-0},
	abstract = {Redesigning social media to improve society requires a new platform for research.},
	language = {en},
	number = {7903},
	urldate = {2022-04-26},
	journal = {Nature},
	author = {Bail, Chris},
	month = mar,
	year = {2022},
	note = {Bandiera\_abtest: a
Cg\_type: World View
Number: 7903
Publisher: Nature Publishing Group
Subject\_term: Media, Communication, Society},
	keywords = {Communication, Media, Society},
	pages = {766--766},
}

@inproceedings{wang_privacy_2015,
	title = {Privacy threat modeling framework for online social networks},
	doi = {10.1109/CTS.2015.7210449},
	abstract = {Online social networks (OSNs) provide services for people to connect and share information. Social networking sites contain huge amount of personal information such as user profiles, user relations, and user activities. Most of the information is personal and sensitive in nature and hence disclosure of this information may cause harassment, financial loss, and even identity theft. Thus, protecting user privacy in online social networks is essential. Many threats and attacks have been found in social networks. However, there is lack of a threat model to study privacy issues in online social networks. This paper presents a privacy threat model for online social networks. The threat model includes four components, online social networking sites, third party service providers, genuine social network users, and malicious users. Threats and vulnerabilities are analyzed from six security aspects, i.e., hardware, operating systems, OSN privacy policies, user privacy settings, user relations, and user data. The paper further summarizes and analyzes the existing threats and attacks using the proposed model.},
	booktitle = {2015 {International} {Conference} on {Collaboration} {Technologies} and {Systems} ({CTS})},
	author = {Wang, Yong and Nepali, Raj Kumar},
	month = jun,
	year = {2015},
	keywords = {Data privacy, Facebook, Operating systems, Organizations, Privacy, Security, countermeasures, online social networks, privacy threat modeling, privacy threats and attacks},
	pages = {358--363},
}

@inproceedings{kumar_risk_2016,
	title = {Risk analysis of online social networks},
	doi = {10.1109/CCAA.2016.7813833},
	abstract = {Social networks are used as the communication network among the groups of people. In past few years several fields use the structure of social network and utilize it. But there are many threats and challenges associated with these networks. These threats disclose the personal information of the users and use that information for unauthorized and malicious purposes. One of the great revolutions in the field of online social networking is online health social network, which is a sub branch of ordinary social network. OHSNs are used for enhancement in the field of health care issues and improvise the better solution for those issues. In this paper we discuss security issues and privacy risk which harm the online social network user. In addition we survey the existing defense mechanism and also propose the better protection and security ideas for online social network users. Furthermore we advise future research directions.},
	booktitle = {2016 {International} {Conference} on {Computing}, {Communication} and {Automation} ({ICCCA})},
	author = {Kumar, Horesh and Jain, Shruti and Srivastava, Ritesh},
	month = apr,
	year = {2016},
	keywords = {Diseases, Facebook, Internet, Online Social Networks, Privacy, Security, Threats, Uniform resource locators},
	pages = {846--851},
}

@inproceedings{greschbach_devil_2012,
	title = {The devil is in the metadata — {New} privacy challenges in {Decentralised} {Online} {Social} {Networks}},
	doi = {10.1109/PerComW.2012.6197506},
	abstract = {Decentralised Online Social Networks (DOSN) are evolving as a promising approach to mitigate design-inherent privacy flaws of logically centralised services such as Facebook, Google+ or Twitter. A common approach to build a DOSN is to use a peer-to-peer architecture. While the absence of a single point of data aggregation strikes the most powerful attacker from the list of adversaries, the decentralisation also removes some privacy protection afforded by the central party's intermediation of all communication. As content storage, access right management, retrieval and other administrative tasks of the service become the obligation of the users, it is non-trivial to hide the metadata of objects and information flows, even when the content itself is encrypted. Such metadata is, deliberately or as a side effect, hidden by the provider in a centralised system. In this work, we aim to identify the dangers arising or made more severe from decentralisation, and show how inferences from metadata might invade users' privacy. Furthermore, we discuss general techniques to mitigate or solve the identified issues.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops}},
	author = {Greschbach, Benjamin and Kreitz, Gunnar and Buchegger, Sonja},
	month = mar,
	year = {2012},
	keywords = {Access control, Data privacy, Encryption, Peer to peer computing, Privacy, Social network services},
	pages = {333--339},
}

@article{rathore_social_2017,
	title = {Social network security: {Issues}, challenges, threats, and solutions},
	volume = {421},
	issn = {0020-0255},
	shorttitle = {Social network security},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517309106},
	doi = {10.1016/j.ins.2017.08.063},
	abstract = {Social networks are very popular in today's world. Millions of people use various forms of social networks as they allow individuals to connect with friends and family, and share private information. However, issues related to maintaining the privacy and security of a user's information can occur, especially when the user's uploaded content is multimedia, such as photos, videos, and audios. Uploaded multimedia content carries information that can be transmitted virally and almost instantaneously within a social networking site and beyond. In this paper, we present a comprehensive survey of different security and privacy threats that target every user of social networking sites. In addition, we separately focus on various threats that arise due to the sharing of multimedia content within a social networking site. We also discuss current state-of- the-art defense solutions that can protect social network users from these threats. We then present future direction and discuss some easy-to-apply response techniques to achieve the goal of a trustworthy and secure social network ecosystem.},
	language = {en},
	urldate = {2022-04-23},
	journal = {Information Sciences},
	author = {Rathore, Shailendra and Sharma, Pradip Kumar and Loia, Vincenzo and Jeong, Young-Sik and Park, Jong Hyuk},
	month = dec,
	year = {2017},
	keywords = {Multimedia data, Security and privacy, Security threats, Social network service},
	pages = {43--69},
}

@article{fire_online_2014,
	title = {Online {Social} {Networks}: {Threats} and {Solutions}},
	volume = {16},
	issn = {1553-877X},
	shorttitle = {Online {Social} {Networks}},
	doi = {10.1109/COMST.2014.2321628},
	abstract = {Many online social network (OSN) users are unaware of the numerous security risks that exist in these networks, including privacy violations, identity theft, and sexual harassment, just to name a few. According to recent studies, OSN users readily expose personal and private details about themselves, such as relationship status, date of birth, school name, email address, phone number, and even home address. This information, if put into the wrong hands, can be used to harm users both in the virtual world and in the real world. These risks become even more severe when the users are children. In this paper, we present a thorough review of the different security and privacy risks, which threaten the well-being of OSN users in general, and children in particular. In addition, we present an overview of existing solutions that can provide better protection, security, and privacy for OSN users. We also offer simple-to-implement recommendations for OSN users, which can improve their security and privacy when using these platforms. Furthermore, we suggest future research directions.},
	number = {4},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Fire, Michael and Goldschmidt, Roy and Elovici, Yuval},
	year = {2014},
	note = {Conference Name: IEEE Communications Surveys Tutorials},
	keywords = {Computer security, Facebook, Online services, Online social networks, Privacy, Social network services, Twitter, online social network security solutions, online social network security threats, security and privacy},
	pages = {2019--2036},
}

@article{wang_systematic_2019,
	title = {Systematic {Literature} {Review} on the {Spread} of {Health}-related {Misinformation} on {Social} {Media}},
	volume = {240},
	issn = {0277-9536},
	url = {https://www.sciencedirect.com/science/article/pii/S0277953619305465},
	doi = {10.1016/j.socscimed.2019.112552},
	abstract = {Contemporary commentators describe the current period as “an era of fake news” in which misinformation, generated intentionally or unintentionally, spreads rapidly. Although affecting all areas of life, it poses particular problems in the health arena, where it can delay or prevent effective care, in some cases threatening the lives of individuals. While examples of the rapid spread of misinformation date back to the earliest days of scientific medicine, the internet, by allowing instantaneous communication and powerful amplification has brought about a quantum change. In democracies where ideas compete in the marketplace for attention, accurate scientific information, which may be difficult to comprehend and even dull, is easily crowded out by sensationalized news. In order to uncover the current evidence and better understand the mechanism of misinformation spread, we report a systematic review of the nature and potential drivers of health-related misinformation. We searched PubMed, Cochrane, Web of Science, Scopus and Google databases to identify relevant methodological and empirical articles published between 2012 and 2018. A total of 57 articles were included for full-text analysis. Overall, we observe an increasing trend in published articles on health-related misinformation and the role of social media in its propagation. The most extensively studied topics involving misinformation relate to vaccination, Ebola and Zika Virus, although others, such as nutrition, cancer, fluoridation of water and smoking also featured. Studies adopted theoretical frameworks from psychology and network science, while co-citation analysis revealed potential for greater collaboration across fields. Most studies employed content analysis, social network analysis or experiments, drawing on disparate disciplinary paradigms. Future research should examine susceptibility of different sociodemographic groups to misinformation and understand the role of belief systems on the intention to spread misinformation. Further interdisciplinary research is also warranted to identify effective and tailored interventions to counter the spread of health-related misinformation online.},
	language = {en},
	urldate = {2022-04-22},
	journal = {Social Science \& Medicine},
	author = {Wang, Yuxi and McKee, Martin and Torbica, Aleksandra and Stuckler, David},
	month = nov,
	year = {2019},
	keywords = {Fake news, Health, Misinformation, Social media},
	pages = {112552},
}

@book{reddy2004investigation,
	title = {Investigation of aeronautical and engineering component failures},
	publisher = {CRC Press},
	author = {Reddy, A Venugopal},
	year = {2004},
}

@book{makhlouf2015handbook,
	title = {Handbook of materials failure analysis with case studies from the chemicals, concrete and power industries},
	publisher = {Butterworth-Heinemann},
	author = {Makhlouf, Abdel Salam Hamdy and Aliofkhazraei, Mahmood},
	year = {2015},
}

@book{kolb2014experiential,
	title = {Experiential learning: {Experience} as the source of learning and development},
	publisher = {FT press},
	author = {Kolb, David A},
	year = {2014},
}

@inproceedings{davis_experience_2022,
	title = {Experience {Paper}: {A} {First} {Offering} of {Software} {Engineering}},
	abstract = {This paper describes our first offering of a project-based software engineering course for undergraduate seniors. The course was given to 72 undergraduates, mostly seniors majoring in computer engineering. Our project taught the full engineering cycle with a narrative based on supporting the re-use of software. In two parts spanning 13 weeks, successful teams deployed a web service. We identify lessons learned and opportunities for improvement.},
	language = {en},
	booktitle = {Proceedings of {The} {First} {International} {Workshop} on {Designing} and {Running} {Project}-{Based} {Courses} in {Software} {Engineering} {Education} ({ICSE}-{DREE})},
	author = {Davis, James C and Amusuo, Paschal and Bushagour, Joseph R},
	year = {2022},
	pages = {5},
}

@inproceedings{vasudeva_varma_case_2005,
	address = {Melbourne, Victoria, Australia},
	title = {Case studies: the potential teaching instruments for software engineering education},
	isbn = {978-0-7695-2472-6},
	shorttitle = {Case studies},
	url = {https://ieeexplore.ieee.org/document/1579146/},
	doi = {10.1109/QSIC.2005.18},
	abstract = {The current approaches to the Software Engineering Education fall short to fulfill the industry demand for quality software engineering. A constant need to create and imbibe more effective learning environments is growing in order to manage this demand. This paper discusses the learning disabilities possessed by both the conventional and the non-conventional approaches for teaching Software Engineering. We propose that case studies can be used as effective teaching mediums and a case study centric learning environment can address these learning disabilities. A case study approach can help the students to gain and retain realistic exposure to concepts of Software Engineering as they are applied in the real world, and the students of today can be groomed as excellent professionals who have experienced the intricacies and complexities of the real world as well as tried their hands to manage these complexities.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {Fifth {International} {Conference} on {Quality} {Software} ({QSIC}'05)},
	publisher = {IEEE},
	author = {{Vasudeva Varma} and {Kirti Garg}},
	year = {2005},
	pages = {279--284},
}

@inproceedings{garg_study_2007,
	address = {Dublin, Ireland},
	title = {A {Study} of the {Effectiveness} of {Case} {Study} {Approach} in {Software} {Engineering} {Education}},
	isbn = {978-0-7695-2893-9},
	url = {http://ieeexplore.ieee.org/document/4271619/},
	doi = {10.1109/CSEET.2007.8},
	abstract = {Software Engineering (SE) educators have been advocating the use of non-conventional approaches for SE education since long. In this context, we conducted action-research to compare the effectiveness of a case study approach with conventional lecture based approach.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {20th {Conference} on {Software} {Engineering} {Education} \& {Training} ({CSEET}'07)},
	publisher = {IEEE},
	author = {Garg, Kirti and Varma, Vasudeva},
	month = jul,
	year = {2007},
	note = {ISSN: 1093-0175},
	pages = {309--316},
}

@inproceedings{wohlin_achieving_1999,
	address = {New Orleans, LA, USA},
	title = {Achieving industrial relevance in software engineering education},
	isbn = {978-0-7695-0131-4},
	url = {http://ieeexplore.ieee.org/document/755175/},
	doi = {10.1109/CSEE.1999.755175},
	abstract = {This paper presents a collection of experiences related to success factors in graduate and postgraduate education. The experiences are mostly concerned with how to make the education relevant from an industrial viewpoint. This is emphasized as a key issue in software engineering education and research, since the main objective is to give the students a good basis for largescale software development in an industrial environment. The presentation is divided into experiences at the graduate and postgraduate levels respectively. For each level a number of strategies to achieve industrial relevance are presented. These strategies have been successful, but it is concluded that more can be done regarding industrial collaboration in the planning and conduction of experiments and case studies. Another interesting strategy for the future is a special postgraduate programme for people employed in industry.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {Proceedings 12th {Conference} on {Software} {Engineering} {Education} and {Training} ({Cat}. {No}.{PR00131})},
	publisher = {IEEE},
	author = {Wohlin, C. and Regnell, B.},
	year = {1999},
	pages = {16--25},
}

@book{schon1987educating,
	title = {Educating the reflective practitioner: {Toward} a new design for teaching and learning in the professions.},
	publisher = {Jossey-Bass},
	author = {Schön, Donald A},
	year = {1987},
}

@article{newport_when_2020,
	title = {When technology goes awry},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3391975},
	doi = {10.1145/3391975},
	abstract = {On engineers' obligation to tame their creations.},
	number = {5},
	urldate = {2022-04-22},
	journal = {Communications of the ACM},
	author = {Newport, Cal},
	month = apr,
	year = {2020},
	pages = {49--52},
}

@article{prince2004does,
	title = {Does active learning work? {A} review of the research},
	volume = {93},
	number = {3},
	journal = {Journal of engineering education},
	author = {Prince, Michael},
	year = {2004},
	note = {Publisher: Wiley Online Library},
	pages = {223--231},
}

@article{prince2006inductive,
	title = {Inductive teaching and learning methods: {Definitions}, comparisons, and research bases},
	volume = {95},
	number = {2},
	journal = {Journal of engineering education},
	author = {Prince, Michael J and Felder, Richard M},
	year = {2006},
	note = {Publisher: Wiley Online Library},
	pages = {123--138},
}

@misc{noauthor_fall_nodate,
	title = {Fall 2021 {CoE} {Teaching} {Awards} {Announced}},
	url = {https://engineering.purdue.edu/IE/news/2022/fall-2021-coe-teaching-awards},
}

@article{mellegard2018contrasting,
	title = {Contrasting big bang with continuous integration through defect reports},
	volume = {37},
	number = {3},
	journal = {IEEE Software},
	author = {Mellegard, Niklas and Burden, Hakan and Levin, Daniel and Lind, Kenneth and Magazinius, Ana},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {14--20},
}

@article{sadagheyani_investigating_2020,
	title = {Investigating the role of social media on mental health},
	volume = {25},
	issn = {2042-8308},
	url = {https://doi.org/10.1108/MHSI-06-2020-0039},
	doi = {10.1108/MHSI-06-2020-0039},
	abstract = {Purpose Today with the internet expansion, social media has also been identified as a factor in evolutions. Social media is the title used to refer to the set of sites and tools that have been born and developed in the space created by modern media such as communication networks, the internet and mobile phones. The effects of emerging phenomena, such as social media on human health, especially mental health, are important. As the effects of social media on users mental health is unclear, and the evidence in this field is contradictory, this study aims to determine the role of social media on mental health. Design/methodology/approach The current study was a review conducted in 2020. According to keywords, an extensive search was conducted on Web of Science, PubMed, Scopus, Google Scholar, Magiran and SID databases. In total, 501 articles were obtained. The articles were screened in three stages. Finally, out of 501 evaluated articles, 50 cases were carefully assessed and included in the study. Findings The findings showed that social media has negative and positive effects on mental health. Negative effects included anxiety, depression, loneliness, poor sleep quality, poor mental health indicators, thoughts of self-harm and suicide, increased levels of psychological distress, cyber bullying, body image dissatisfaction, fear of missing out and decreased life satisfaction. Positive effects included accessing other people’s health experiences and expert health information, managing depression, emotional support and community building, expanding and strengthening offline networks and interactions, self-expression and self-identity, establish and maintain relationships. Originality/value The impact of social media on mental health can be considered as a double-edged sword. The important thing is to be able to reduce the negative effects of social media on mental health and turn it into an opportunity by implementing appropriate strategies and actions and to increase and strengthen the positive effects.},
	number = {1},
	urldate = {2022-04-20},
	journal = {Mental Health and Social Inclusion},
	author = {Sadagheyani, Hassan Ebrahimpour and Tatari, Farin},
	month = jan,
	year = {2020},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Health, Mental health, Mental well-being, Social media},
	pages = {41--51},
}

@inproceedings{chandramouli_emerging_2011,
	title = {Emerging social media threats: {Technology} and policy perspectives},
	shorttitle = {Emerging social media threats},
	abstract = {Traditional cyber threats or attacks have targeted information and communication infrastructure that usually result in economic loss. Typically, launching these attacks requires an advanced skill level. Governments around the world have a good understanding of these threats and therefore have put in place many policies to deal with them. The rapid growth of social media is giving rise to new types of threats that spill over from the cyber world to real-life. These threats profoundly alter the psychological, social and cultural dynamics of vulnerable social media users. Also, it is becoming increasingly easy even for an average user to exploit social media for malicious purposes. Organizations and governments are finding it difficult to accurately detect, identify, predict, and prevent the malicious exploitation of social media. Quantifying the socio-psychological effect of social media vulnerabilities is another major challenge. Due to these reasons there is a lack of policies to deal with this issue. In this paper, we discuss several challenges in this emerging area, from technologies to policies.},
	booktitle = {2011 {Second} {Worldwide} {Cybersecurity} {Summit} ({WCS})},
	author = {Chandramouli, R.},
	month = jun,
	year = {2011},
	keywords = {Electronic mail, Facebook, Internet, Media, Monitoring, Pragmatics, Psychology},
	pages = {1--4},
}

@article{whiting_why_2013,
	title = {Why people use social media: a uses and gratifications approach},
	volume = {16},
	issn = {1352-2752},
	shorttitle = {Why people use social media},
	url = {https://doi.org/10.1108/QMR-06-2013-0041},
	doi = {10.1108/QMR-06-2013-0041},
	abstract = {Purpose – This paper seeks to demonstrate the importance of uses and gratifications theory to social media. By applying uses and gratifications theory, this paper will explore and discuss the uses and gratifications that consumer receive from using social media. This paper seeks to provide a better and more comprehensive understanding of why consumers use social media. Design/methodology/approach – Exploratory study was conducted. 25 in‐depth interviews were conducted with individuals who use social media. Findings – This study identified ten uses and gratifications for using social media. The ten uses and gratifications are: social interaction, information seeking, pass time, entertainment, relaxation, communicatory utility, convenience utility, expression of opinion, information sharing, and surveillance/knowledge about others. Research limitations/implications – Limitations are small sample size. Research implications are that uses and gratifications theory has specific relevance to social media and should be given more prominence. Uses and gratifications theory helps explain the many and varied reasons why consumers use social media. Practical implications – This paper helps organizations to understand why consumers use social media and what gratifications they receive from social media. Originality/value – This paper makes the contribution that uses and gratifications theory has specific relevance and should be given more prominence within the area of social media. This paper also provides a rich and vivid understanding of why consumers use social media.},
	number = {4},
	urldate = {2022-04-20},
	journal = {Qualitative Market Research: An International Journal},
	author = {Whiting, Anita and Williams, David},
	month = jan,
	year = {2013},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Consumer generated media, Exploratory study, In‐depth interviews, Qualitative study, Social media, Uses and gratifications theory, Uses of social media, Web 2.0},
	pages = {362--369},
}

@book{miller_how_2016,
	title = {How the {World} {Changed} {Social} {Media}},
	isbn = {978-1-910634-49-3},
	url = {https://library.oapen.org/handle/20.500.12657/32834},
	abstract = {How the World Changed Social Media is the first book in Why We Post, a book series that investigates the findings of nine anthropologists who each spent 15 months living in communities across the world. This book offers a comparative analysis summarising the results of the research and exploring the impact of social media on politics and gender, education and commerce. What is the result of the increased emphasis on visual communication? Are we becoming more individual or more social? Why is public social media so conservative? Why does equality online fail to shift inequality offline? How did memes become the moral police of the internet? Supported by an introduction to the project’s academic framework and theoretical terms that help to account for the findings, the book argues that the only way to appreciate and understand something as intimate and ubiquitous as social media is to be immersed in the lives of the people who post. Only then can we discover how people all around the world have already transformed social media in such unexpected ways and assess the consequences.},
	language = {English},
	urldate = {2022-04-20},
	publisher = {UCL Press},
	author = {Miller, Daniel and Sinanan, Jolynna and Wang, Xinyuan and McDonald, Tom and Haynes, Nell and Costa, Elisabetta and Spyer, Juliano and Venkatraman, Shriram and Nicolescu, Razvan},
	year = {2016},
	doi = {10.14324/111.9781910634493},
	note = {Accepted: 2016-12-31 23:55:55},
	keywords = {Anthropology, China, Facebook, Field research, bic Book Industry Communication::J Society \& social sciences, bic Book Industry Communication::J Society \& social sciences::JH Sociology \& anthropology::JHM Anthropology::JHMC Social \& cultural anthropology, ethnography, memes, social media, society},
}

@book{reason_human_1990,
	title = {Human {Error}},
	isbn = {978-0-521-31419-0},
	abstract = {James Reason has produced a major theoretical integration of several previously isolated literatures in his new book Human Error. Much of the theoretical structure is new and original. Particularly important is the identification of cognitive processes common to a wide variety of error types. Modern technology has now reached a point where improved safety can only be achieved on the basis of a better understanding of human error mechanisms. In its treatment of major accidents, the book spans the disciplinary gulf between psychological theory and those concerned with maintaining the reliability of hazardous technologies. As such, it is essential reading not only for cognitive scientists and human factors specialists, but also for reliability engineers and risk managers. No existing book speaks with so much clarity to both the theorists and the practitioners of human reliability.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Reason, James},
	month = oct,
	year = {1990},
	note = {Google-Books-ID: WJL8NZc8lZ8C},
	keywords = {Psychology / Cognitive Psychology \& Cognition},
}

@article{rolland_benefit_2013,
	title = {The {Benefit} of {Social} {Media}: {Bulletin} {Board} {Focus} {Groups} as a {Tool} for {Co}-creation},
	volume = {55},
	issn = {1470-7853},
	shorttitle = {The {Benefit} of {Social} {Media}},
	url = {https://doi.org/10.2501/IJMR-2013-068},
	doi = {10.2501/IJMR-2013-068},
	abstract = {Bulletin board methodology emerged at the end of the 1990s and is becoming the most frequently used qualitative study technique. This interactive approach groups a community of participants in a private or public online forum for a duration that varies from several days to several months. Discoveries, exchanges of view, personal opinions and group reactions are all part of the power and interest of the internet in this era of social media. This article presents the principles of bulletin board development, and specifics to aid understanding of this tool within social networks and to help organisations adapt to a paradigm shift in marketing in which consumer-respondents are co-creators of meaning and knowledge.},
	language = {en},
	number = {6},
	urldate = {2022-04-18},
	journal = {International Journal of Market Research},
	author = {Rolland, Sylvie E. and Parmentier, Guy},
	month = nov,
	year = {2013},
	note = {Publisher: SAGE Publications},
	pages = {809--827},
}

@inproceedings{yue_influence_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Influence of {Content} {Layout} and {Motivation} on {Users}' {Herd} {Behavior} in {Social} {Discovery}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858497},
	doi = {10.1145/2858036.2858497},
	abstract = {Social product discovery is an emerging paradigm that enables users to seek information and inspiration from peer-contributed contents. Researchers have observed herd behaviors in social discovery, i.e., basing beliefs and decisions on what similarly situated others have done. In this paper, we explore the effects of content layout and motivation on users' herd behaviors in social discovery. We conduct an eye-tracking study with 120 participants to compare goal- and action-oriented users' behaviors on a grid versus waterfall style social discovery site. The results show that users have a higher tendency to herd on a grid-style website, more so for goal-oriented users.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yue, Yanzhen and Ma, Xiaojuan and Jiang, Zhenhui},
	month = may,
	year = {2016},
	keywords = {grid, herd, layout, motivation, social discovery, waterfall},
	pages = {5715--5719},
}

@inproceedings{ashktorab_designing_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Designing {Cyberbullying} {Mitigation} and {Prevention} {Solutions} through {Participatory} {Design} {With} {Teenagers}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858548},
	doi = {10.1145/2858036.2858548},
	abstract = {While social media platforms enable individuals to easily communicate and share experiences, they have also emerged as a tool for cyberbullying. Teenagers represent an especially vulnerable population for negative emotional responses to cyberbullying. At the same time, attempts to mitigate or prevent cyberbullying from occurring in these networked spaces have largely failed because of the complexity and nuance with which young people bully others online. To address challenges related to designing for cyberbullying intervention and mitigation, we detail findings from participatory design work with two groups of high school students in spring 2015. Over the course of five design sessions spanning five weeks, participants shared their experiences with cyberbullying and iteratively designed potential solutions. We provide an in-depth discussion of the range of cyberbullying mitigation solutions participants designed. We focus on challenges participants' identified in designing for cyberbullying support and prevention and present a set of five potential cyberbullying mitigation solutions based on the results of the design sessions.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ashktorab, Zahra and Vitak, Jessica},
	month = may,
	year = {2016},
	keywords = {adolescents, cyberbullying, participatory design, social media},
	pages = {3895--3905},
}

@inproceedings{ma_anonymity_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Anonymity, {Intimacy} and {Self}-{Disclosure} in {Social} {Media}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858414},
	doi = {10.1145/2858036.2858414},
	abstract = {Self-disclosure is rewarding and provides significant benefits for individuals, but it also involves risks, especially in social media settings. We conducted an online experiment to study the relationship between content intimacy and willingness to self-disclose in social media, and how identification (real name vs. anonymous) and audience type (social ties vs. people nearby) moderate that relationship. Content intimacy is known to regulate self-disclosure in face-to-face communication: people self-disclose less as content intimacy increases. We show that such regulation persists in online social media settings. Further, although anonymity and an audience of social ties are both known to increase self-disclosure, it is unclear whether they (1) increase self-disclosure baseline for content of all intimacy levels, or (2) weaken intimacy's regulation effect, making people more willing to disclose intimate content. We show that intimacy always regulates self-disclosure, regardless of settings. We also show that anonymity mainly increases self-disclosure baseline and (sometimes) weakens the regulation. On the other hand, an audience of social ties increases the baseline but strengthens the regulation. Finally, we demonstrate that anonymity has a more salient effect on content of negative valence.The results are critical to understanding the dynamics and opportunities of self-disclosure in social media services that vary levels of identification and types of audience.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Xiao and Hancock, Jeff and Naaman, Mor},
	month = may,
	year = {2016},
	keywords = {anonymity, intimacy, self-disclosure, social media, valence},
	pages = {3857--3869},
}

@inproceedings{de_choudhury_discovering_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Discovering {Shifts} to {Suicidal} {Ideation} from {Mental} {Health} {Content} in {Social} {Media}},
	isbn = {978-1-4503-3362-7},
	url = {https://doi.org/10.1145/2858036.2858207},
	doi = {10.1145/2858036.2858207},
	abstract = {History of mental illness is a major factor behind suicide risk and ideation. However research efforts toward characterizing and forecasting this risk is limited due to the paucity of information regarding suicide ideation, exacerbated by the stigma of mental illness. This paper fills gaps in the literature by developing a statistical methodology to infer which individuals could undergo transitions from mental health discourse to suicidal ideation. We utilize semi-anonymous support communities on Reddit as unobtrusive data sources to infer the likelihood of these shifts. We develop language and interactional measures for this purpose, as well as a propensity score matching based statistical approach. Our approach allows us to derive distinct markers of shifts to suicidal ideation. These markers can be modeled in a prediction framework to identify individuals likely to engage in suicidal ideation in the future. We discuss societal and ethical implications of this research.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {De Choudhury, Munmun and Kiciman, Emre and Dredze, Mark and Coppersmith, Glen and Kumar, Mrinal},
	month = may,
	year = {2016},
	keywords = {mental health, reddit, social media, suicidal ideation},
	pages = {2098--2110},
}

@inproceedings{such_photo_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Photo {Privacy} {Conflicts} in {Social} {Media}: {A} {Large}-scale {Empirical} {Study}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {Photo {Privacy} {Conflicts} in {Social} {Media}},
	url = {https://doi.org/10.1145/3025453.3025668},
	doi = {10.1145/3025453.3025668},
	abstract = {Items in social media such as photos may be co-owned by multiple users, i.e., the sharing decisions of the ones who upload them have the potential to harm the privacy of the others. Previous works uncovered coping strategies by co-owners to manage their privacy, but mainly focused on general practices and experiences. We establish an empirical base for the prevalence, context and severity of privacy conflicts over co-owned photos. To this aim, a parallel survey of pre-screened 496 uploaders and 537 co-owners collected occurrences and type of conflicts over co-owned photos, and any actions taken towards resolving them. We uncover nuances and complexities not known before, including co-ownership types, and divergences in the assessment of photo audiences. We also find that an all-or-nothing approach seems to dominate conflict resolution, even when parties actually interact and talk about the conflict. Finally, we derive key insights for designing systems to mitigate these divergences and facilitate consensus.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Such, Jose M. and Porter, Joel and Preibusch, Sören and Joinson, Adam},
	month = may,
	year = {2017},
	keywords = {co-ownership, conflicts, online social networks, photo sharing, privacy, social media},
	pages = {3821--3832},
}

@inproceedings{usmani_characterizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Characterizing {Social} {Insider} {Attacks} on {Facebook}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025901},
	doi = {10.1145/3025453.3025901},
	abstract = {Facebook accounts are secured against unauthorized access through passwords and device-level security. Those defenses, however, may not be sufficient to prevent social insider attacks, where attackers know their victims, and gain access to a victim's account by interacting directly with their device. To characterize these attacks, we ran two MTurk studies. In the first (n = 1,308), using the list experiment method, we estimated that 24\% of participants had perpetrated social insider attacks and that 21\% had been victims (and knew about it). In the second study (n = 45), participants wrote stories detailing personal experiences with such attacks. Using thematic analysis, we typified attacks around five motivations (fun, curiosity, jealousy, animosity, and utility), and explored dimensions associated with each type. Our combined findings indicate that social insider attacks are common, often have serious emotional consequences, and have no simple mitigation.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Usmani, Wali Ahmed and Marques, Diogo and Beschastnikh, Ivan and Beznosov, Konstantin and Guerreiro, Tiago and Carriço, Luís},
	month = may,
	year = {2017},
	keywords = {facebook, insider attack, privacy, usable security},
	pages = {3810--3820},
}

@inproceedings{devito_algorithms_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {"{Algorithms} ruin everything": \#{RIPTwitter}, {Folk} {Theories}, and {Resistance} to {Algorithmic} {Change} in {Social} {Media}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {"{Algorithms} ruin everything"},
	url = {https://doi.org/10.1145/3025453.3025659},
	doi = {10.1145/3025453.3025659},
	abstract = {As algorithmically-driven content curation has become an increasingly common feature of social media platforms, user resistance to algorithmic change has become more frequent and visible. These incidents of user backlash point to larger issues such as inaccurate understandings of how algorithmic systems work as well as mismatches between designer and user intent. Using a content analysis of 102,827 tweets from \#RIPTwitter, a recent hashtag-based backlash to rumors about introducing algorithmic curation to Twitter's timeline, this study addresses the nature of user resistance in the form of the complaints being expressed, folk theories of the algorithmic system espoused by users, and how these folk theories potentially frame user reactions. We find that resistance to algorithmic change largely revolves around expectation violation, with folk theories acting as frames for reactions such that more detailed folk theories are expressed through more specific reactions to algorithmic change.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {DeVito, Michael A. and Gergle, Darren and Birnholtz, Jeremy},
	month = may,
	year = {2017},
	keywords = {algorithm awareness, algorithmic curation, algorithms, expectation violation, folk theories, machine classification, social media, technology continuance, user resistance},
	pages = {3163--3174},
}

@inproceedings{deloatch_i_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {I {Need} {Your} {Encouragement}! {Requesting} {Supportive} {Comments} on {Social} {Media} {Reduces} {Test} {Anxiety}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025709},
	doi = {10.1145/3025453.3025709},
	abstract = {Many students underperform on exams due to experiencing high test anxiety. We report on a study comparing a novel intervention of seeking support from one's social network to the more common approaches of expressive writing and studying task-relevant materials for simulated open-ended test questions. We measured in-the-moment (state) anxiety before and after each intervention, and correctness of the solutions. We also surveyed students to learn about their perceptions of the interventions. Our results showed that social support decreased the anxiety of high test-anxious students by 21\% with the reduction in anxiety correlating with the number of messages received. Social support also allowed high test-anxious students to score at the level of low test-anxious students. Expressive writing showed a similar effect, but increased the anxiety of low test-anxious students by 61\%. Studying task materials had no effect on anxiety and high test-anxious students performed worse than low test-anxious students. Despite benefiting from social support, we found that students were uncomfortable soliciting support from their online social network. Realizing the benefits of this approach may therefore require different formulations of social support in practice.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Deloatch, Robert and Bailey, Brian P. and Kirlik, Alex and Zilles, Craig},
	month = may,
	year = {2017},
	keywords = {anxiety, expressive writing, programming, social support},
	pages = {736--747},
}

@inproceedings{flintham_falling_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Falling for {Fake} {News}: {Investigating} the {Consumption} of {News} via {Social} {Media}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Falling for {Fake} {News}},
	url = {https://doi.org/10.1145/3173574.3173950},
	doi = {10.1145/3173574.3173950},
	abstract = {In the so called 'post-truth' era, characterized by a loss of public trust in various institutions, and the rise of 'fake news' disseminated via the internet and social media, individuals may face uncertainty about the veracity of information available, whether it be satire or malicious hoax. We investigate attitudes to news delivered by social media, and subsequent verification strategies applied, or not applied, by individuals. A survey reveals that two thirds of respondents regularly consumed news via Facebook, and that one third had at some point come across fake news that they initially believed to be true. An analysis task involving news presented via Facebook reveals a diverse range of judgement forming strategies, with participants relying on personal judgements as to plausibility and scepticism around sources and journalistic style. This reflects a shift away from traditional methods of accessing the news, and highlights the difficulties in combating the spread of fake news.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Flintham, Martin and Karner, Christian and Bachour, Khaled and Creswick, Helen and Gupta, Neha and Moran, Stuart},
	month = apr,
	year = {2018},
	keywords = {facebook, fake news, post-truth, social media, trust, verification},
	pages = {1--10},
}

@inproceedings{alvarado_towards_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Towards {Algorithmic} {Experience}: {Initial} {Efforts} for {Social} {Media} {Contexts}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Towards {Algorithmic} {Experience}},
	url = {https://doi.org/10.1145/3173574.3173860},
	doi = {10.1145/3173574.3173860},
	abstract = {Algorithms influence most of our daily activities, decisions, and they guide our behaviors. It has been argued that algorithms even have a direct impact on democratic societies. Human - Computer Interaction research needs to develop analytical tools for describing the interaction with, and experience of algorithms. Based on user participatory workshops focused on scrutinizing Facebook's newsfeed, an algorithm-influenced social media, we propose the concept of Algorithmic Experience (AX) as an analytic framing for making the interaction with and experience of algorithms explicit. Connecting it to design, we articulate five functional categories of AX that are particularly important to cater for in social media: profiling transparency and management, algorithmic awareness and control, and selective algorithmic memory.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alvarado, Oscar and Waern, Annika},
	month = apr,
	year = {2018},
	keywords = {algorithmic experience, algorithms, research through design, social media, user-centered design},
	pages = {1--12},
}

@inproceedings{lottridge_lets_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Let's {Hate} {Together}: {How} {People} {Share} {News} in {Messaging}, {Social}, and {Public} {Networks}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Let's {Hate} {Together}},
	url = {https://doi.org/10.1145/3173574.3173634},
	doi = {10.1145/3173574.3173634},
	abstract = {There are currently a wide variety of ways to share news with others: from sharing in a personal message, to sharing on a social network, to publicly posting. Through a survey with over one thousand people and an artifact analysis of 262 shared articles, we examine differences in motivations and frequency of sharing news on public, social and private platforms. We find that public sharing is more focused on spreading an ideology, while private sharing in messaging is dominated by stories inspired by the recipient's interests or context. The survey revealed three main groups of news sharing practices: those who shared to all channels (public, social, private), those who didn't share at all, and those who shared to private and social. The groups differed in their attitudes toward online discussion; those that shared the most were neutral and those that didn't share had negative attitudes about discussion online. We discuss sharing practices and implications for social systems that support sharing news.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lottridge, Danielle and Bentley, Frank R.},
	month = apr,
	year = {2018},
	keywords = {diffusion, messaging, news, sharing, social media},
	pages = {1--13},
}

@inproceedings{difranzo_social_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Social {Media} {TestDrive}: {Real}-{World} {Social} {Media} {Education} for the {Next} {Generation}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {Social {Media} {TestDrive}},
	url = {https://doi.org/10.1145/3290605.3300533},
	doi = {10.1145/3290605.3300533},
	abstract = {Social media sites are where life happens for many of today's young people, so it is important to teach them to use these sites safely and effectively. Many youth receive classroom education on digital literacy topics, but have few chances to build actual skills. Social Media TestDrive, an interactive social media simulation, fills a gap in digital literacy education by combining experiential learning in a realistic and safe social media environment with educator-facilitated classroom lessons. The tool was piloted with 12 educators and over 200 students, and formative evaluation data suggest that TestDrive achieved high levels of engagement with both groups. Students reported the modules enhanced their understanding of digital citizenship issues, and educators noted that students were engaging in meaningful classroom conversations. Finally, we discuss the importance of involving multiple stakeholder groups (e.g., researchers, youth, educators, curriculum developers) in designing educational technology.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {DiFranzo, Dominic and Choi, Yoon Hyung and Purington, Amanda and Taft, Jessie G. and Whitlock, Janis and Bazarova, Natalya N.},
	month = may,
	year = {2019},
	keywords = {digital citizen, education, social media, youth development},
	pages = {1--11},
}

@inproceedings{ma_when_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {When {Do} {People} {Trust} {Their} {Social} {Groups}?},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300297},
	doi = {10.1145/3290605.3300297},
	abstract = {Trust facilitates cooperation and supports positive outcomes in social groups, including member satisfaction, information sharing, and task performance. Extensive prior research has examined individuals' general propensity to trust, as well as the factors that contribute to their trust in specific groups. Here, we build on past work to present a comprehensive framework for predicting trust in groups. By surveying 6,383 Facebook Groups users about their trust attitudes and examining aggregated behavioral and demographic data for these individuals, we show that (1) an individual's propensity to trust is associated with how they trust their groups, (2) smaller, closed, older, more exclusive, or more homogeneous groups are trusted more, and (3) a group's overall friendship-network structure and an individual's position within that structure can also predict trust. Last, we demonstrate how group trust predicts outcomes at both individual and group level such as the formation of new friendship ties.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Xiao and Cheng, Justin and Iyer, Shankar and Naaman, Mor},
	month = may,
	year = {2019},
	keywords = {communities, facebook, groups, trust},
	pages = {1--12},
}

@incollection{eschler_emergent_2020,
	address = {New York, NY, USA},
	title = {Emergent {Self}-{Regulation} {Practices} in {Technology} and {Social} {Media} {Use} of {Individuals} {Living} with {Depression}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376773},
	abstract = {Much human-computer interaction work related to depression focuses on the population level (e.g., studying social media hashtags related to depression) or evaluates prototypes for digital interventions to manage depression. However, little is known about how people living with depression perceive and manage technology use, such as time spent on social media per day. For this study, we interviewed 30 individuals living with depression to explore their technology and social media use. We find that these individuals demonstrated emergent practices related to self-regulation, such as learning to monitor and adjust technology use to improve their emotional, cognitive, and behavioral health. Our findings add a human-centered viewpoint to the relationship between living with depression and technology and social media use. We present design implications of these findings for better empowering individuals with depression to encourage their natural inclinations to self-regulate technology and social media use.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Eschler, Jordan and Burgess, Eleanor R. and Reddy, Madhu and Mohr, David C.},
	month = apr,
	year = {2020},
	keywords = {depression, qualitative, self-regulation, social media},
	pages = {1--13},
}

@incollection{trieu_private_2020,
	address = {New York, NY, USA},
	title = {Private {Responses} for {Public} {Sharing}: {Understanding} {Self}-{Presentation} and {Relational} {Maintenance} via {Stories} in {Social} {Media}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Private {Responses} for {Public} {Sharing}},
	url = {https://doi.org/10.1145/3313831.3376549},
	abstract = {With nearly two billion users, social media Stories-an ephemeral format of sharing-are increasingly popular and projected to overtake sharing via public feeds. Sharing via Stories differs from Feeds sharing by removing the visible feedback (e.g. "likes" and "comments") which has come to characterize social media. Given the salience of responses visibility to self-presentation and relational maintenance in social media literature, we conducted semi-structured interviews (N = 22) to explore how people understand these processes when using Stories. We find that users have lower expectations for responses with Stories and experience lower pressure for self-presentation. This fosters more frequent sharing and a sense of daily connectedness, which strong ties can find valuable. Finally, the act of viewing takes on new significance of signaling attention when made known to the sharer. Our findings point to the importance of effort and attention in understanding responses on social media.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Trieu, Penny and Baym, Nancy K.},
	month = apr,
	year = {2020},
	keywords = {relational maintenance, self-presentation, social media, stories},
	pages = {1--13},
}

@incollection{burke_social_2020,
	address = {New York, NY, USA},
	title = {Social {Comparison} and {Facebook}: {Feedback}, {Positivity}, and {Opportunities} for {Comparison}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Social {Comparison} and {Facebook}},
	url = {https://doi.org/10.1145/3313831.3376482},
	abstract = {People compare themselves to one another both offline and online. The specific online activities that worsen social comparison are partly understood, though much existing research relies on people recalling their own online activities post hoc and is situated in only a few countries. To better understand social comparison worldwide and the range of associated behaviors on social media, a survey of 38,000 people from 18 countries was paired with logged activity on Facebook for the prior month. People who reported more frequent social comparison spent more time on Facebook, had more friends, and saw proportionally more social content on the site. They also saw greater amounts of feedback on friends' posts and proportionally more positivity. There was no evidence that social comparison happened more with acquaintances than close friends. One in five respondents recalled recently seeing a post that made them feel worse about themselves but reported conflicting views: half wished they hadn't seen the post, while a third felt very happy for the poster. Design opportunities are discussed, including hiding feedback counts, filters for topics and people, and supporting meaningful interactions, so that when comparisons do occur, people are less affected by them.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Burke, Moira and Cheng, Justin and de Gant, Bethany},
	month = apr,
	year = {2020},
	keywords = {envy, facebook, social comparison, social media, well-being},
	pages = {1--13},
}

@inproceedings{charmaraman_prototyping_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Prototyping for {Social} {Wellbeing} with {Early} {Social} {Media} {Users}: {Belonging}, {Experimentation}, and {Self}-{Care}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Prototyping for {Social} {Wellbeing} with {Early} {Social} {Media} {Users}},
	url = {https://doi.org/10.1145/3411764.3445332},
	doi = {10.1145/3411764.3445332},
	abstract = {Many 10-14 year olds are at the early stages of using social media, habits they develop on popular platforms can have lasting effects on their socio-emotional wellbeing. We led a remote innovation workshop with 23 middle schoolers on digital wellbeing, identity exploration, and computational concepts related to social computing. This workshop was a unique opportunity to reflect on emergent habits, discuss them with peers, and imagine oneself as an ICT innovator. Resulting themes related to participants’ social wellbeing online included a) sense of belonging to communities of interest, friends, and family, b) self-care and social support strategies involving managing risks, control, and empathy, and c) experimentation while building self-confidence and bravely exploring audience reactions. Participants iteratively designed and tested a sandbox social network website, resulting in Social Sketch. Reflecting on our study, we describe the process for conceptualizing Social Sketch, and challenges in social media innovation with teenagers.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Charmaraman, Linda and Grevet Delcourt, Catherine},
	month = may,
	year = {2021},
	keywords = {ICT, adolescence, cooperative inquiry, prototyping, social media, teenagers, wellbeing, wellness},
	pages = {1--15},
}

@inproceedings{tian_deeptest_2018,
	address = {Gothenburg Sweden},
	title = {{DeepTest}: automated testing of deep-neural-network-driven autonomous cars},
	isbn = {978-1-4503-5638-1},
	shorttitle = {{DeepTest}},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180220},
	doi = {10.1145/3180155.3180220},
	abstract = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.},
	language = {en},
	urldate = {2022-04-13},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
	month = may,
	year = {2018},
	pages = {303--314},
}

@inproceedings{tang_empirical_2021,
	address = {Madrid, ES},
	title = {An {Empirical} {Study} of {Refactorings} and {Technical} {Debt} in {Machine} {Learning} {Systems}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9401990/},
	doi = {10.1109/ICSE43902.2021.00033},
	abstract = {Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today’s data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semanticspreserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.},
	language = {en},
	urldate = {2022-04-13},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Tang, Yiming and Khatchadourian, Raffi and Bagherzadeh, Mehdi and Singh, Rhia and Stewart, Ajani and Raja, Anita},
	month = may,
	year = {2021},
	pages = {238--250},
}

@misc{haidt_why_2022,
	title = {Why the {Past} 10 {Years} of {American} {Life} {Have} {Been} {Uniquely} {Stupid}},
	url = {https://www.theatlantic.com/magazine/archive/2022/05/social-media-democracy-trust-babel/629369/},
	abstract = {It’s not just a phase.},
	language = {en},
	urldate = {2022-04-12},
	journal = {The Atlantic},
	author = {Haidt, Jonathan},
	month = apr,
	year = {2022},
	note = {Section: Ideas},
}

@article{ud_din_privacy_2018,
	title = {Privacy and {Security} {Issues} in {Online} {Social} {Networks}},
	volume = {10},
	doi = {10.3390/fi10120114},
	abstract = {The advent of online social networks (OSN) has transformed a common passive reader into a content contributor. It has allowed users to share information and exchange opinions, and also express themselves in online virtual communities to interact with other users of similar interests. However, OSN have turned the social sphere of users into the commercial sphere. This should create a privacy and security issue for OSN users. OSN service providers collect the private and sensitive data of their customers that can be misused by data collectors, third parties, or by unauthorized users. In this paper, common security and privacy issues are explained along with recommendations to OSN users to protect themselves from these issues whenever they use social media.},
	journal = {Future Internet},
	author = {Ud Din, Ikram and Islam, Naveed and Rodrigues, Joel and Guizani, Mohsen},
	month = nov,
	year = {2018},
}

@article{wendell_reflective_2017,
	title = {Reflective {Decision}‐{Making} in {Elementary} {Students}' {Engineering} {Design}},
	volume = {106},
	issn = {1069-4730, 2168-9830},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jee.20173},
	doi = {10.1002/jee.20173},
	abstract = {Purpose This qualitative study sought to propose and operationalize a definition of reflective decision-making among elementary students. We investigated how urban elementary students enact reflective decision making in a formal engineering design curriculum.
Method We used naturalistic inquiry methodology and video recorded seven Engineering is Elementary design challenges in four classrooms. Students worked in small teams, and we focused on their planning and redesign phases. Maximum variation sampling, constant comparative analysis, and microethnographic accounts demonstrated the diversity of resources students utilized in their decision making.
Results In student discourse, we found evidence for six reflective decision-making elements: articulating multiple solutions, evaluating pros and cons, intentionally selecting a solution, retelling the performance of a solution, analyzing a solution according to evidence, and purposefully choosing improvements. The discourse patterns used to enact these elements both supported and interfered with students’ achievement of design goals.
Conclusions Our results suggest that during engineering design tasks, young learners working in small teams can respond productively to opportunities to engage in sophisticated discourse. However, further work is needed on tools and strategies that support reflective decision-making by all students during engineering design in elementary school.},
	language = {en},
	number = {3},
	urldate = {2022-04-06},
	journal = {Journal of Engineering Education},
	author = {Wendell, Kristen Bethke and Wright, Christopher G. and Paugh, Patricia},
	month = jul,
	year = {2017},
	pages = {356--397},
}

@inproceedings{niedermaier_correct_2020,
	title = {Correct and {Control} {Complex} {IoT} {Systems}: {Evaluation} of a {Classification} for {System} {Anomalies}},
	shorttitle = {Correct and {Control} {Complex} {IoT} {Systems}},
	doi = {10.1109/QRS51102.2020.00050},
	abstract = {In practice there are deficiencies in precise interteam communications about system anomalies to perform troubleshooting and postmortem analysis along different teams operating complex IoT systems. We evaluate the quality in use of an adaptation of IEEE Std. 1044-2009 with the objective to differentiate the handling of fault detection and fault reaction from handling of defect and its options for defect correction. We extended the scope of IEEE Std. 1044-2009 from anomalies related to software only to anomalies related to complex IoT systems. To evaluate the quality in use of our classification a study was conducted at Robert Bosch GmbH. We applied our adaptation to a postmortem analysis of an IoT solution and evaluated the quality in use by conducting interviews with three stakeholders. Our adaptation was effectively applied and interteam communications as well as iterative and inductive learning for product improvement were enhanced.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Niedermaier, Sina and Heisse, Stefan and Wagner, Stefan},
	month = dec,
	year = {2020},
	keywords = {Fault detection, IEEE Standards, Interviews, Security, Software quality, Software reliability, Stakeholders, anomaly, classification, complex system, defect, failure, fault},
	pages = {321--328},
}

@article{liblit_building_nodate,
	title = {Building a {Better} {Backtrace}: {Techniques} for {Postmortem} {Program} {Analysis}},
	abstract = {After a program has crashed, it can be diﬃcult to reconstruct why the failure occurred, or what actions led to the error. We propose a family of analysis techniques that use the evidence left behind by a failed program to build a time line of its possible actions from launch through termination. Our design can operate with zero run time instrumentation, or can ﬂexibly incorporate a wide variety of artifacts such as stack traces and event logs for increased precision. Eﬃcient demanddriven algorithms are provided, and the approach is well suited for incorporation into interactive debugging support tools.},
	language = {en},
	author = {Liblit, Ben and Aiken, Alex},
	pages = {13},
}

@article{yang_symbian_nodate,
	title = {Symbian {OS} system event log and postmortem software fault analysis},
	abstract = {This thesis introduces a postmortem software failure analysis system named MobileCrash. The system is to catch Symbian OS panics and exceptions, to collect related information and to transmit crash logs to a central database for analysis. The basics of software failure analysis and similar systems on other operation systems are presented in the thesis. After revealing the system design of the MobileCrash, the system event log as a supplementary of the MobileCrash is introduced. The thesis also introduces the core of crash analysis together with real case studies in Symbian OS. Software developers can benefit from this thesis by learning about software failure analysis on Symbian OS. Project managers can get basic information on software failure analysis from this thesis to better control projects based on Symbian OS.},
	language = {en},
	author = {Yang, Zhigang},
	pages = {62},
}

@inproceedings{jetley_using_2006,
	title = {Using {Abstraction}-driven {Slicing} for {Postmortem} {Analysis} of {Software}},
	doi = {10.1109/ICPC.2006.50},
	abstract = {Post-mortem analysis - the process of tracing software failure to source code - is an important means for maintenance engineers and regulatory reviewers for establishing the cause of an error. Historically, static slicing techniques have been used for aiding post-mortem fault analysis. However, the slices obtained in this manner can often be too large and may not give a clear understanding of the code when dealing with complex reactive systems. We propose using model abstraction in conjunction with slicing to ameliorate the problem of understanding large slices. Combining slicing with abstraction provides the analyst with an integrated cognition model, leading to a better understanding of the code, and consequently more efficient error analysis. We formalize this concept through the notion of abstraction-driven slicing, and use it to develop CAdS, an automated tool to aid postmortem error detection in C programs using abstraction-driven static slicing. We list our experiences with CAdS and illustrate how it can be used to reduce effort involved in the postmortem analysis process},
	booktitle = {14th {IEEE} {International} {Conference} on {Program} {Comprehension} ({ICPC}'06)},
	author = {Jetley, R. and Zhang, Yi and Iyer, S.P.},
	month = jun,
	year = {2006},
	note = {ISSN: 1092-8138},
	keywords = {Application software, Cognition, Computer errors, Computer science, Error analysis, Failure analysis, Food manufacturing, Forensics, Instruments, Software maintenance},
	pages = {107--116},
}

@article{ohmann_lightweight_2017,
	title = {Lightweight control-flow instrumentation and postmortem analysis in support of debugging},
	volume = {24},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/10.1007/s10515-016-0190-1},
	doi = {10.1007/s10515-016-0190-1},
	abstract = {Debugging is difﬁcult and costly. As a programmer looks for a bug, it would be helpful to see a complete trace of events leading to the point of failure. Unfortunately, full tracing is simply too slow to use after deployment, and may even be impractical during testing. We aid post-deployment debugging by giving programmers additional information about program activity shortly before failure. We use latent information in post-failure memory dumps, augmented by low-overhead, tunable runtime tracing. Our results with a realistically-tuned tracing scheme show low enough overhead (0–5 \%) to be used in production runs. We demonstrate several potential uses of this enhanced information, including a novel postmortem static slice restriction technique and a reduced view of potentially-executed code. Experimental evaluation shows our approach to be very effective. For example, our analyses shrink stacksensitive interprocedural static slices by 53–78 \% in larger applications.},
	language = {en},
	number = {4},
	urldate = {2022-04-05},
	journal = {Automated Software Engineering},
	author = {Ohmann, Peter and Liblit, Ben},
	month = dec,
	year = {2017},
	pages = {865--904},
}

@article{xu_pomp_nodate,
	title = {{POMP}: {Postmortem} {Program} {Analysis} with {Hardware}-{Enhanced} {Post}-{Crash} {Artifacts}},
	abstract = {While a core dump carries a large amount of information, it barely serves as informative debugging aids in locating software faults because it carries information that indicates only a partial chronology of how program reached a crash site. Recently, this situation has been signiﬁcantly improved. With the emergence of hardwareassisted processor tracing, software developers and security analysts can trace program execution and integrate them into a core dump. In comparison with an ordinary core dump, the new post-crash artifact provides software developers and security analysts with more clues as to a program crash. To use it for failure diagnosis, however, it still requires strenuous manual efforts.},
	language = {en},
	author = {Xu, Jun and Mu, Dongliang and Xing, Xinyu and Liu, Peng and Chen, Ping and Mao, Bing},
	pages = {17},
}

@article{manevich_pse_nodate,
	title = {{PSE}: {Explaining} {Program} {Failures} via {Postmortem} {Static} {Analysis}},
	abstract = {In this paper, we describe PSE (Postmortem Symbolic Evaluation), a static analysis algorithm that can be used by programmers to diagnose software failures. The algorithm requires minimal information about a failure, namely its kind (e.g. NULL dereference), and its location in the program’s source code. It produces a set of execution traces along which the program can be driven to the given failure.},
	language = {en},
	author = {Manevich, Roman and Sridharan, Manu and Adams, Stephen and Das, Manuvir and Yang, Zhe},
	pages = {10},
}

@inproceedings{bala_challenges_2015,
	title = {Challenges and {Outcomes} of {Enterprise} {Social} {Media} {Implementation}: {Insights} from {Cummins}, {Inc}.},
	shorttitle = {Challenges and {Outcomes} of {Enterprise} {Social} {Media} {Implementation}},
	doi = {10.1109/HICSS.2015.222},
	abstract = {Enterprise social media (ESM) are web-based platforms that improve communication and collaboration in organizations. Although the practitioner literature and industry reports have suggested the potential value of ESM for organizations, there has been limited research that focuses on the implementation process and outcomes of ESM. We conducted a mixed methods case study of a large-scale ESM implementation in a Fortune 500 manufacturing company, Cummins, Inc., and found several major challenges that Cummins faced during the implementation, such as the lack of interest and use by employees, the lack of fit with existing organizational and individual processes, inconsistent performance of the ESM platform, and unfavorable business conditions that affected organization-wide programs and projects. While employees were initially enthusiastic about the ESM, they were not using the platform as much as anticipated after the implementation. We offer a set of lessons from our in-depth case study that organizations implementing ESM should find beneficial.},
	booktitle = {2015 48th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Bala, Hillol and Massey, Anne P. and Rajanayakam, John and Hsieh, Christine J.},
	month = jan,
	year = {2015},
	note = {ISSN: 1530-1605},
	keywords = {Blogs, Collaboration, Companies, Encyclopedias, Media},
	pages = {1839--1848},
}

@misc{auxier_social_2021,
	title = {Social {Media} {Use} in 2021},
	url = {https://www.pewresearch.org/internet/2021/04/07/social-media-use-in-2021/},
	abstract = {A majority of Americans say they use YouTube and Facebook, while use of Instagram, Snapchat and TikTok is especially common among adults under 30.},
	language = {en},
	urldate = {2022-02-03},
	journal = {Pew Research Center - Internet},
	author = {Auxier, Brooke and Anderson, Monica},
	month = apr,
	year = {2021},
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {http://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2022-03-26},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {289--300},
}

@article{noauthor_facebook_2021,
	chapter = {Tech},
	title = {The {Facebook} {Files}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/the-facebook-files-11631713039},
	abstract = {Facebook knows, in acute detail, that its platforms are riddled with flaws but hasn’t fixed them. That’s a key finding of a Journal series that launched this week, based on an array of internal company documents. Read all the stories here.},
	language = {en-US},
	urldate = {2022-03-26},
	journal = {Wall Street Journal},
	month = oct,
	year = {2021},
	keywords = {FB, Facebook, GRAPHICS, Mark Zuckerberg, Media/Entertainment, Online Service Providers, SYND, Social Media Platforms/Tools, Technology, WSJ-PRO-WSJ.com, entertainment, graphics, media, online service providers, social media platforms, technology, tools},
}

@article{reason_understanding_1995,
	title = {Understanding adverse events: human factors.},
	volume = {4},
	issn = {2044-5415, 2044-5423},
	shorttitle = {Understanding adverse events},
	url = {https://qualitysafety.bmj.com/content/4/2/80},
	doi = {10.1136/qshc.4.2.80},
	abstract = {(1) Human rather than technical failures now represent the greatest threat to complex and potentially hazardous systems. This includes healthcare systems. (2) Managing the human risks will never be 100\% effective. Human fallibility can be moderated, but it cannot be eliminated. (3) Different error types have different underlying mechanisms, occur in different parts of the organisation, and require different methods of risk management. The basic distinctions are between: Slips, lapses, trips, and fumbles (execution failures) and mistakes (planning or problem solving failures). Mistakes are divided into rule based mistakes and knowledge based mistakes. Errors (information-handling problems) and violations (motivational problems) Active versus latent failures. Active failures are committed by those in direct contact with the patient, latent failures arise in organisational and managerial spheres and their adverse effects may take a long time to become evident. (4) Safety significant errors occur at all levels of the system, not just at the sharp end. Decisions made in the upper echelons of the organisation create the conditions in the workplace that subsequently promote individual errors and violations. Latent failures are present long before an accident and are hence prime candidates for principled risk management. (5) Measures that involve sanctions and exhortations (that is, moralistic measures directed to those at the sharp end) have only very limited effectiveness, especially so in the case of highly trained professionals. (6) Human factors problems are a product of a chain of causes in which the individual psychological factors (that is, momentary inattention, forgetting, etc) are the last and least manageable links. Attentional "capture" (preoccupation or distraction) is a necessary condition for the commission of slips and lapses. Yet, its occurrence is almost impossible to predict or control effectively. The same is true of the factors associated with forgetting. States of mind contributing to error are thus extremely difficult to manage; they can happen to the best of people at any time. (7) People do not act in isolation. Their behaviour is shaped by circumstances. The same is true for errors and violations. The likelihood of an unsafe act being committed is heavily influenced by the nature of the task and by the local workplace conditions. These, in turn, are the product of "upstream" organisational factors. Great gains in safety can ve achieved through relatively small modifications of equipment and workplaces. (8) Automation and increasing advanced equipment do not cure human factors problems, they merely relocate them. In contrast, training people to work effectively in teams costs little, but has achieved significant enhancements of human performance in aviation. (9) Effective risk management depends critically on a confidential and preferable anonymous incident monitoring system that records the individual, task, situational, and organisational factors associated with incidents and near misses. (10) Effective risk management means the simultaneous and targeted deployment of limited remedial resources at different levels of the system: the individual or team, the task, the situation, and the organisation as a whole.},
	language = {en},
	number = {2},
	urldate = {2022-03-26},
	journal = {BMJ Quality \& Safety},
	author = {Reason, J.},
	month = jun,
	year = {1995},
	note = {Publisher: BMJ Publishing Group Ltd
Section: Research Article},
	pages = {80--89},
}

@article{jin_bugredux_nodate,
	title = {{BugRedux}: {Reproducing} field failures for in-house debugging},
	abstract = {A recent survey conducted among developers of the Apache, Eclipse, and Mozilla projects showed that the ability to recreate ﬁeld failures is considered of fundamental importance when investigating bug reports. Unfortunately, the information typically contained in a bug report, such as memory dumps or call stacks, is usually insufﬁcient for recreating the problem. Even more advanced approaches for gathering ﬁeld data and help in-house debugging tend to collect either too little information, and be ineffective, or too much information, and be inefﬁcient. To address these issues, we present BUGREDUX, a novel general approach for in-house debugging of ﬁeld failures. BUGREDUX aims to synthesize, using execution data collected in the ﬁeld, executions that mimic the observed ﬁeld failures. We deﬁne several instances of BUGREDUX that collect different types of execution data and perform, through an empirical study, a cost-beneﬁt analysis of the approach and its variations. In the study, we apply BUGREDUX to 16 failures of 14 real-world programs. Our results are promising in that they show that it is possible to synthesize in-house executions that reproduce failures observed in the ﬁeld using a suitable set of execution data.},
	language = {en},
	author = {Jin, Wei and Orso, Alessandro},
	pages = {11},
}

@inproceedings{jin2012bugredux,
	title = {Bugredux: {Reproducing} field failures for in-house debugging},
	booktitle = {2012 34th international conference on software engineering ({ICSE})},
	author = {Jin, Wei and Orso, Alessandro},
	year = {2012},
	note = {tex.organization: IEEE},
	pages = {474--484},
}

@inproceedings{DBLP:conf/icse/BellSK13,
	title = {Chronicler: lightweight recording to reproduce field failures},
	url = {https://doi.org/10.1109/ICSE.2013.6606582},
	doi = {10.1109/ICSE.2013.6606582},
	booktitle = {35th international conference on software engineering, {ICSE} '13, san francisco, {CA}, {USA}, may 18-26, 2013},
	publisher = {IEEE Computer Society},
	author = {Bell, Jonathan and Sarda, Nikhil and Kaiser, Gail E.},
	editor = {Notkin, David and Cheng, Betty H. C. and Pohl, Klaus},
	year = {2013},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/conf/icse/BellSK13.bib
tex.timestamp: Wed, 16 Oct 2019 14:14:49 +0200},
	pages = {362--371},
}

@inproceedings{tomassi_bugswarm_2019,
	address = {Montreal, QC, Canada},
	title = {{BugSwarm}: {Mining} and {Continuously} {Growing} a {Dataset} of {Reproducible} {Failures} and {Fixes}},
	isbn = {978-1-72810-869-8},
	shorttitle = {{BugSwarm}},
	url = {https://ieeexplore.ieee.org/document/8812141/},
	doi = {10.1109/ICSE.2019.00048},
	abstract = {Fault-detection, localization, and repair methods are vital to software quality; but it is difﬁcult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and ﬁxes are vital to good experimental evaluation of approaches to software quality, but they are difﬁcult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully conﬁgurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Tomassi, David A. and Dmeiri, Naji and Wang, Yichen and Bhowmick, Antara and Liu, Yen-Chuan and Devanbu, Premkumar T. and Vasilescu, Bogdan and Rubio-Gonzalez, Cindy},
	month = may,
	year = {2019},
	pages = {339--349},
}

@inproceedings{xiao_tracking_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Tracking data structures for postmortem analysis ({NIER} track)},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985938},
	doi = {10.1145/1985793.1985938},
	abstract = {Analyzing the runtime behaviors of the data structures is important because they usually relate to the obscured program performance and understanding issues. The runtime evolution history of data structures creates the possibility of building a lightweight and non-checkpointing based solution for the backward analysis for validating and mining both the temporal and stationary properties of the data structure. We design and implement TAEDS, a framework that focuses on gathering the data evolution history of a program at the runtime and provides a virtual machine for programmers to examine the behavior of data structures back in time. We show that our approach facilitates many programming tasks such as diagnosing memory problems and improving the design of the data structures themselves.},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Xiao, Xiao and Zhou, Jinguo and Zhang, Charles},
	month = may,
	year = {2011},
	pages = {896--899},
}

@inproceedings{streit_why_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Why software quality improvement fails (and how to succeed nevertheless)},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985895},
	doi = {10.1145/1985793.1985895},
	abstract = {Quality improvement is the key to enormous cost reduction in the IT business. However, improvement projects often fail in practice. In many cases, stakeholders fearing, e.g., a loss of power or not recognizing the beneﬁts inhibit the improvement. Systematic change management and an economic perspective help to overcome these issues, but are little known and seldom applied.},
	language = {en},
	urldate = {2022-03-25},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Streit, Jonathan and Pizka, Markus},
	month = may,
	year = {2011},
	pages = {726--735},
}

@article{falessi_achieving_2014,
	title = {Achieving and {Maintaining} {CMMI} {Maturity} {Level} 5 in a {Small} {Organization}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/6728932/},
	doi = {10.1109/MS.2014.17},
	language = {en},
	number = {5},
	urldate = {2022-03-25},
	journal = {IEEE Software},
	author = {Falessi, Davide and Shaw, Michele and Mullen, Kathleen},
	month = sep,
	year = {2014},
	pages = {80--86},
}

@article{pino_software_2008,
	title = {Software process improvement in small and medium software enterprises: a systematic review},
	volume = {16},
	issn = {0963-9314, 1573-1367},
	shorttitle = {Software process improvement in small and medium software enterprises},
	url = {http://link.springer.com/10.1007/s11219-007-9038-z},
	doi = {10.1007/s11219-007-9038-z},
	abstract = {Small and medium enterprises are a very important cog in the gears of the world economy. The software industry in most countries is composed of an industrial scheme that is made up mainly of small and medium software enterprises—SMEs. To strengthen these types of organizations, efﬁcient Software Engineering practices are needed—practices which have been adapted to their size and type of business. Over the last two decades, the Software Engineering community has expressed special interest in software process improvement (SPI) in an effort to increase software product quality, as well as the productivity of software development. However, there is a widespread tendency to make a point of stressing that the success of SPI is only possible for large companies. In this article, a systematic review of published case studies on the SPI efforts carried out in SMEs is presented. Its objective is to analyse the existing approaches towards SPI which focus on SMEs and which report a case study carried out in industry. A further objective is that of discussing the signiﬁcant issues related to this area of knowledge, and to provide an up-to-date state of the art, from which innovative research activities can be thought of and planned.},
	language = {en},
	number = {2},
	urldate = {2022-03-25},
	journal = {Software Quality Journal},
	author = {Pino, Francisco J. and García, Félix and Piattini, Mario},
	month = jun,
	year = {2008},
	pages = {237--261},
}

@article{satariano_eu_2022,
	chapter = {Technology},
	title = {E.{U}. {Takes} {Aim} at {Big} {Tech}’s {Power} {With} {Landmark} {Digital} {Act}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2022/03/24/technology/eu-regulation-apple-meta-google.html},
	abstract = {The European Union was expected to finalize the Digital Markets Act, the most sweeping legislation to regulate tech since a European privacy law was passed in 2018.},
	language = {en-US},
	urldate = {2022-03-24},
	journal = {The New York Times},
	author = {Satariano, Adam},
	month = mar,
	year = {2022},
	keywords = {Alphabet Inc, Amazon.com Inc, Android (Operating System), Antitrust Laws and Competition Issues, Apple Inc, Computers and the Internet, Data-Mining and Database Marketing, E-Commerce, European Union, Facebook Inc, General Data Protection Regulation (GDPR), Google Inc, Instagram Inc, Instant Messaging, Law and Legislation, Meta Platforms Inc, Microsoft Corp, Mobile Applications, Mobile Commerce and Payments, Online Advertising, Regulation and Deregulation of Industry, Social Media},
}

@article{lehtinen_what_nodate,
	title = {What are {Problem} {Causes} of {Software} {Projects}? {Data} of {Root} {Cause} {Analysis} at {Four} {Software} {Companies}},
	abstract = {Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5\% of the causes are related to software development tools that are widely investigated in software engineering research.},
	language = {en},
	author = {Lehtinen, Timo O A and Mantyla, Mika V},
	pages = {4},
}

@inproceedings{lehtinen2011problem,
	title = {What are problem causes of software projects? {Data} of root cause analysis at four software companies},
	booktitle = {2011 international symposium on empirical software engineering and measurement},
	author = {Lehtinen, Timo OA and Mantyla, Mika V},
	year = {2011},
	note = {tex.organization: IEEE},
	pages = {388--391},
}

@inproceedings{lehtinen2012perceived,
	title = {Perceived feasibility of using root cause analysis in post project reviews: an empirical investigation'},
	booktitle = {{ESEIW}/{International} doctoral symposium on empirical software engineering ({IDoESE}’2012), lund, september, 2012},
	author = {Lehtinen, Timo OA},
	year = {2012},
	note = {tex.organization: IEEE/ESEM},
	pages = {1--8},
}

@article{myllyaho2004review,
	title = {A review of small and large post-mortem analysis methods},
	journal = {Proceedings of the ICSSEA, Paris},
	author = {Myllyaho, Mauri and Salo, Outi and Kääriäinen, Jukka and Hyysalo, Jarkko and Koskela, Juha},
	year = {2004},
	pages = {1--8},
}

@article{hoda_toward_nodate,
	title = {Toward {Learning} {Teams}},
	language = {en},
	author = {Hoda, Rashina and Babb, Jeffry and Norbjerg, Jacob},
	pages = {4},
}

@inproceedings{babb2013barriers,
	title = {Barriers to learning in agile software development projects},
	booktitle = {International conference on agile software development},
	author = {Babb, Jeffry S and Hoda, Rashina and Nørbjerg, Jacob},
	year = {2013},
	note = {tex.organization: Springer},
	pages = {1--15},
}

@article{hoda2013toward,
	title = {Toward learning teams},
	volume = {30},
	number = {4},
	journal = {IEEE software},
	author = {Hoda, Rashina and Babb, Jeffry and Nørbjerg, Jacob},
	year = {2013},
	note = {Publisher: IEEE},
	pages = {95--98},
}

@inproceedings{hazzan_reflective_nodate,
	title = {The {Reflective} {Practitioner} {Perspective} in {Software} {Engineering}},
	language = {en},
	booktitle = {{CHI} 2004 {One} {Day} {Workshop} - {Designing} for {Reflective} {Practitioners}},
	author = {Hazzan, Orit and Tomayko, Jim},
	pages = {4},
}

@article{hamill_common_2009,
	title = {Common {Trends} in {Software} {Fault} and {Failure} {Data}},
	volume = {35},
	abstract = {The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of software faults. Furthermore, these results show that contrary to the popular belief, a significant percentage of failures are linked to late life cycle activities. Another important aspect of our work is that we conduct intra- and interproject comparisons, as well as comparisons with the findings from related studies. The consistency of several main trends across software systems in this paper and several related research efforts suggests that these trends are likely to be intrinsic characteristics of software faults and failures rather than project specific.},
	language = {en},
	number = {4},
	journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
	author = {Hamill, Maggie and Goseva-Popstojanova, Katerina},
	year = {2009},
	pages = {13},
}

@article{hamill2009common,
	title = {Common trends in software fault and failure data},
	volume = {35},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Hamill, Maggie and Goseva-Popstojanova, Katerina},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {484--496},
}

@inproceedings{kloos2011risk,
	title = {Risk-based testing of safety-critical embedded systems driven by fault tree analysis},
	booktitle = {2011 {IEEE} fourth international conference on software testing, verification and validation workshops},
	author = {Kloos, Johannes and Hussain, Tanvir and Eschbach, Robert},
	year = {2011},
	note = {tex.organization: IEEE},
	pages = {26--33},
}

@misc{felderer2014taxonomy,
	title = {A taxonomy of risk-based testing},
	publisher = {Springer},
	author = {Felderer, Michael and Schieferdecker, Ina},
	year = {2014},
	note = {Number: 5
Pages: 559–568
Volume: 16},
}

@article{felderer2014multiple,
	title = {A multiple case study on risk-based testing in industry},
	volume = {16},
	number = {5},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Felderer, Michael and Ramler, Rudolf},
	year = {2014},
	note = {Publisher: Springer},
	pages = {609--625},
}

@article{freimut_industrial_2005,
	title = {An industrial case study of implementing and validating defect classification for process improvement and quality management},
	abstract = {Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements.},
	language = {en},
	author = {Freimut, B and Denger, C and Ketterer, M},
	year = {2005},
	pages = {10},
}

@inproceedings{felderer2013using,
	title = {Using defect taxonomies to improve the maturity of the system test process: results from an industrial case study},
	booktitle = {International conference on software quality},
	author = {Felderer, Michael and Beer, Armin},
	year = {2013},
	note = {tex.organization: Springer},
	pages = {125--146},
}

@inproceedings{kelly2001case,
	title = {A case study in the use of defect classification in inspections},
	booktitle = {Proceedings of the 2001 conference of the {Centre} for {Advanced} {Studies} on {Collaborative} research},
	author = {Kelly, Diane and Shepard, Terry},
	year = {2001},
	pages = {7},
}

@article{lutz2004empirical,
	title = {Empirical analysis of safety-critical anomalies during operations},
	volume = {30},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Lutz, Robyn R and Mikulski, Inés Carmen},
	year = {2004},
	note = {Publisher: IEEE},
	pages = {172--180},
}

@inproceedings{freimut2005industrial,
	title = {An industrial case study of implementing and validating defect classification for process improvement and quality management},
	booktitle = {11th {IEEE} international software metrics symposium ({METRICS}'05)},
	author = {Freimut, Bernd and Denger, Christian and Ketterer, Markus},
	year = {2005},
	note = {tex.organization: IEEE},
	pages = {10--pp},
}

@article{falessi_failure_nodate,
	title = {On {Failure} {Classification}: {The} {Impact} of "{Getting} {It} {Wrong}"},
	abstract = {Bug classification is a well-established practice which supports important activities such as enhancing verification and validation (V\&V) efficiency and effectiveness. The state of the practice is manual and hence classification errors occur. This paper investigates the sensitivity of the value of bug classification (specifically, failure type classification) to its error rate; i.e., the degree to which misclassified historic bugs decrease the V\&V effectiveness (i.e., the ability to find bugs of a failure type of interest). Results from the analysis of an industrial database of more than 3,000 bugs show that the impact of classification error rate on V\&V effectiveness significantly varies with failure type. Specifically, there are failure types for which a 5\% classification error can decrease the ability to find them by 66\%. Conversely, there are failure types for which the V\&V effectiveness is robust to very high error rates. These results show the utility of future research aimed at: 1) providing better tool support for decreasing human errors in classifying the failure type of bugs, 2) providing more robust approaches for the selection of V\&V techniques, and 3) including robustness as an important criterion when evaluating technologies.},
	language = {en},
	author = {Falessi, Davide and Kidwell, Bill and Hayes, Jane Huffman and Shull, Forrest},
	pages = {4},
}

@inproceedings{el1998repeatability,
	title = {The repeatability of code defect classifications},
	booktitle = {Proceedings ninth international symposium on software reliability engineering (cat. {No}. {98TB100257})},
	author = {El Emam, Khaled and Wieczorek, Isabella},
	year = {1998},
	note = {tex.organization: IEEE},
	pages = {322--333},
}

@inproceedings{falessi2014failure,
	title = {On failure classification: {The} impact of" getting it wrong"},
	booktitle = {Companion proceedings of the 36th international conference on software engineering},
	author = {Falessi, Davide and Kidwell, Bill and Huffman Hayes, Jane and Shull, Forrest},
	year = {2014},
	pages = {512--515},
}

@article{hazzan_reective_2002,
	title = {The reﬂective practitioner perspective in software engineering education q},
	abstract = {This paper focuses on the application of the reﬂective practitioner (RP) perspective to the profession of software engineering (SE). The RP perspective guides professional people to rethink their professional creations during and after the accomplishment of the creation process. Analysis of the ﬁeld of SE supports the adoption of the RP perspective to SE in general and to SE education in particular. The RP perspective emphasizes the studio––the basic training method in architecture schools––as the educational environment for design studies. In such studios students develop projects with a close guidance of a tutor. Analysis of the kind of tasks that architecture students are working on and a comparison of these tasks to the problems that SE students are facing, suggest that the studio may be an appropriate teaching method in SE as well. The paper presents the main ideas of the RP perspective and examines its ﬁtness to SE in general and to SE education in particular. The discussion is based on analysis of the RP perspective and of the SE profession, visits to architecture studios, and conversations with tutors in architecture studios and with computing science practitioners.},
	language = {en},
	journal = {Journal of Systems and Software (JSS)},
	author = {Hazzan, Orit},
	year = {2002},
	pages = {11},
}

@incollection{goos_reflective_2003,
	address = {Berlin, Heidelberg},
	title = {The {Reflective} {Practitioner} {Perspective} in {eXtreme} {Programming}},
	volume = {2753},
	isbn = {978-3-540-40662-4 978-3-540-45122-8},
	url = {http://link.springer.com/10.1007/978-3-540-45122-8_7},
	abstract = {This paper examines ways by which a reflective mode of thinking may improve eXtreme Programming (XP) practices. Specifically, the paper describes the reflective practitioner perspective and suggests specific ways in which such an approach may be interwoven within XP practices.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Extreme {Programming} and {Agile} {Methods} - {XP}/{Agile} {Universe} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Hazzan, Orit and Tomayko, Jim},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Maurer, Frank and Wells, Don},
	year = {2003},
	doi = {10.1007/978-3-540-45122-8_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {51--61},
}

@article{hazzan_reective_2002-1,
	title = {The reﬂective practitioner perspective in software engineering education q},
	abstract = {This paper focuses on the application of the reﬂective practitioner (RP) perspective to the profession of software engineering (SE). The RP perspective guides professional people to rethink their professional creations during and after the accomplishment of the creation process. Analysis of the ﬁeld of SE supports the adoption of the RP perspective to SE in general and to SE education in particular. The RP perspective emphasizes the studio––the basic training method in architecture schools––as the educational environment for design studies. In such studios students develop projects with a close guidance of a tutor. Analysis of the kind of tasks that architecture students are working on and a comparison of these tasks to the problems that SE students are facing, suggest that the studio may be an appropriate teaching method in SE as well. The paper presents the main ideas of the RP perspective and examines its ﬁtness to SE in general and to SE education in particular. The discussion is based on analysis of the RP perspective and of the SE profession, visits to architecture studios, and conversations with tutors in architecture studios and with computing science practitioners.},
	language = {en},
	author = {Hazzan, Orit},
	year = {2002},
	pages = {11},
}

@article{raelin2001public,
	title = {Public reflection as the basis of learning},
	volume = {32},
	number = {1},
	journal = {Management learning},
	author = {Raelin, Joseph A},
	year = {2001},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {11--30},
}

@inproceedings{moore_lessons_2000,
	address = {Limerick, Ireland},
	title = {Lessons learned from teaching reflective software engineering using the {Leap} toolkit},
	isbn = {978-1-58113-206-9},
	url = {http://portal.acm.org/citation.cfm?doid=337180.337508},
	doi = {10.1145/337180.337508},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the 22nd international conference on {Software} engineering  - {ICSE} '00},
	publisher = {ACM Press},
	author = {Moore, Carlton A.},
	year = {2000},
	pages = {672--675},
}

@article{babb_embedding_2014,
	title = {Embedding {Reflection} and {Learning} into {Agile} {Software} {Development}},
	volume = {31},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/6785921/},
	doi = {10.1109/MS.2014.54},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Babb, Jeffry and Hoda, Rashina and Norbjerg, Jacob},
	month = jul,
	year = {2014},
	pages = {51--57},
}

@article{mitchell_assessment_nodate,
	title = {An {Assessment} {Strategy} to {Determine} {Learning} {Outcomes} in a {Software} {Engineering} {Problem}-based {Learning} {Course}},
	language = {en},
	author = {Mitchell, George G and Delaney, James Declan},
	pages = {9},
}

@inproceedings{upchurch_reflective_1999,
	address = {San Juan, Puerto Rico},
	title = {Reflective essays in software engineering},
	volume = {3},
	isbn = {978-0-7803-5643-6},
	url = {http://ieeexplore.ieee.org/document/840324/},
	doi = {10.1109/FIE.1999.840324},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {{FIE}'99 {Frontiers} in {Education}. 29th {Annual} {Frontiers} in {Education} {Conference}. {Designing} the {Future} of {Science} and {Engineering} {Education}. {Conference} {Proceedings} ({IEEE} {Cat}. {No}.{99CH37011}},
	publisher = {Stripes Publishing L.L.C},
	author = {Upchurch, R.L. and Sims-Knight, J.E.},
	year = {1999},
	pages = {13A6/13--13A6/19},
}

@article{parsons_coderetreats_2014,
	title = {Coderetreats: {Reflective} {Practice} and the {Game} of {Life}},
	volume = {31},
	issn = {0740-7459},
	shorttitle = {Coderetreats},
	url = {http://ieeexplore.ieee.org/document/6756713/},
	doi = {10.1109/MS.2014.25},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Parsons, David and Mathrani, Anuradha and Susnjak, Teo and Leist, Arno},
	month = jul,
	year = {2014},
	pages = {58--64},
}

@incollection{oconnor_building_2009,
	address = {Berlin, Heidelberg},
	title = {Building an {Observatory} of {Course}-of-{Action} in {Software} {Engineering}: {Towards} a {Link} between {ISO}/{IEC} {Software} {Engineering} {Standards} and a {Reflective} {Practice}},
	volume = {42},
	isbn = {978-3-642-04132-7 978-3-642-04133-4},
	shorttitle = {Building an {Observatory} of {Course}-of-{Action} in {Software} {Engineering}},
	url = {http://link.springer.com/10.1007/978-3-642-04133-4_16},
	abstract = {As a help to compete in an evolving market, small software companies may use an observatory of their course-of-action. The course of action considers the observable aspect of the actor’s activity. Its analysis provides a description of actors’ activity and it can express recommendations concerning both the individual situations and the collective situation. The observatory is an articulated set of data collecting methods supported with semantic wikis and a dedicated application. A case study, based on the activity of a team of 6 young software engineers, depicts some aspects of the building and the filling of the course-of-action observatory. As primary results of this work, we may think that observing and analyzing software engineer’s activity help to reveal his/her theory-in-use – what governs engineers’ behavior and tends to be tacit structures – That may help engineers to establish links between “Project Processes-in-use” and a simplified Process Reference Model and contribute to reduce the fit between a project-in-action and espoused SE standards.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Software {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bru, François-Xavier and Frappin, Gaëlle and Legrand, Ludovic and Merrer, Estéban and Piteau, Sylvain and Salou, Guillaume and Saliou, Philippe and Ribaud, Vincent},
	editor = {O’Connor, Rory V. and Baddoo, Nathan and Cuadrago Gallego, Juan and Rejas Muslera, Ricardo and Smolander, Kari and Messnarz, Richard},
	year = {2009},
	doi = {10.1007/978-3-642-04133-4_16},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {185--200},
}

@inproceedings{dors_reflective_2020,
	address = {Germany},
	title = {Reflective {Practice} in {Software} {Development} {Studios}: {Findings} from an {Ethnographic} {Study}},
	isbn = {978-1-72816-807-4},
	shorttitle = {Reflective {Practice} in {Software} {Development} {Studios}},
	url = {https://ieeexplore.ieee.org/document/9206217/},
	doi = {10.1109/CSEET49119.2020.9206217},
	abstract = {Over the last two decades, software educators have adopted new approaches, techniques, and tools for practical learning. Previous research has found that studio-based learning, which has been in use by some design and architecture courses, is suitable for learning the practical aspects of software engineering. The studies available recognize the presence of reflective practice in software development studios; however, they do not show empirical evidence of its contribution to learning. The goal of this study is to understand the use of reflective practice in software development studios and its contributions to the practical learning of software engineering. The qualitative data were collected using an ethnographic method through participant observation and from written students' self-reflections, which were analyzed using Cycle Coding Method supported by Atlas.ti Qualitative Data Analysis tool. The findings suggest that reflective practice promotes the emergence of new ideas while contributing to the development of skills that are valuable to software engineering professional practice. This research presents a qualitative look at how these are developed in the context of a particular software development studio.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {2020 {IEEE} 32nd {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	publisher = {IEEE},
	author = {Dors, Tania Mara and Van Amstel, Frederick M. C. and Binder, Fabio and Reinehr, Sheila and Malucelli, Andreia},
	month = nov,
	year = {2020},
	pages = {1--10},
}

@article{ras_experience_2007,
	title = {Experience {Management} {Wikis} for {Reflective} {Practice} in {Software} {Capstone} {Projects}},
	volume = {50},
	issn = {0018-9359, 1557-9638},
	url = {https://ieeexplore.ieee.org/document/4358726/},
	doi = {10.1109/TE.2007.904580},
	abstract = {Software engineering curriculum guidelines state that students should practice methods, techniques, and tools. A capstone project is one possibility to address this aim. A capstone project helps the students to increase their problem solving competencies, improve their social skills (e.g., communication skills), and gather practical experience. A crux of such projects is that students perform “reﬂective” practice in order to learn from their experiences. The authors believe that experience gathering and reuse are effective techniques to stimulate reﬂective activities. An adapted free- and open-source Wiki-based system called software organization platform (SOP) is used to support students in managing their observations and experiences. The system can be used for experience exchange within the team and for experience reuse in forthcoming projects. The results of a case study show that standard Wiki functions improve communication and information sharing by means of explicit observation and experience documentation. A total of 183 documented observations and experiences at the end of the project provide a measure for the amount of reﬂection students have had during the capstone project. Still, the advantages of using Wikis will decrease when no technical adaptations of the Wiki to the learning objectives and to the software engineering tasks are made. Limitations of the case study, future evaluation steps, and planned developments of SOP will be provided in this paper.},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Education},
	author = {Ras, Eric and Carbon, Ralf and Decker, BjÖrn and Rech, JÖrg},
	month = nov,
	year = {2007},
	pages = {312--320},
}

@article{bull_supporting_2014,
	title = {Supporting {Reflective} {Practice} in {Software} {Engineering} {Education} through a {Studio}-{Based} {Approach}},
	volume = {31},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/6774769/},
	doi = {10.1109/MS.2014.52},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Bull, Christopher N. and Whittle, Jon},
	month = jul,
	year = {2014},
	pages = {44--50},
}

@inproceedings{prior_reflection_2016,
	address = {Canberra Australia},
	title = {Reflection is hard: teaching and learning reflective practice in a software studio},
	isbn = {978-1-4503-4042-7},
	shorttitle = {Reflection is hard},
	url = {https://dl.acm.org/doi/10.1145/2843043.2843346},
	doi = {10.1145/2843043.2843346},
	abstract = {We have observed that it is a non-trivial exercise for undergraduate students to learn how to reflect. Reflective practice is now recognised as important for software developers and has become a key part of software studios in universities, but there is limited empirical investigation into how best to teach and learn reflection. In the literature on reflection in software studios, there are many papers that claim that reflection in the studio is mandatory. However, there is inadequate guidance about teaching early stage students to reflect in that literature. The essence of the work presented in this paper is a beginning to the consideration of how the teaching of software development can best be combined with teaching reflective practice for early stage software development students.. We started on a research programme to understand how to encourage students to learn to reflect. As we were unsure about teaching reflection, and we wished to change our teaching as we progressively understood better what to do, we chose action research as the most suitable approach. Within the action research cycles we used ethnography to understand what was happening with the students when they attempted to reflect. This paper reports on the first 4 semesters of research.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the {Australasian} {Computer} {Science} {Week} {Multiconference}},
	publisher = {ACM},
	author = {Prior, Julia and Ferguson, Samuel and Leaney, John},
	month = feb,
	year = {2016},
	pages = {1--8},
}

@article{dyba_reflective_2014,
	title = {The {Reflective} {Software} {Engineer}: {Reflective} {Practice}},
	volume = {31},
	issn = {0740-7459},
	shorttitle = {The {Reflective} {Software} {Engineer}},
	url = {http://ieeexplore.ieee.org/document/6834681/},
	doi = {10.1109/MS.2014.97},
	language = {en},
	number = {4},
	urldate = {2022-03-23},
	journal = {IEEE Software},
	author = {Dyba, Tore and Maiden, Neil and Glass, Robert},
	month = jul,
	year = {2014},
	pages = {32--36},
}

@article{oconnor_foundations_2007,
	title = {Foundations in {Nursing} and {Health} {Care}: {Beginning} {Reflective} {Practice}},
	volume = {85},
	issn = {00012092},
	shorttitle = {Foundations in {Nursing} and {Health} {Care}},
	url = {http://doi.wiley.com/10.1016/S0001-2092(07)60057-X},
	doi = {10.1016/S0001-2092(07)60057-X},
	language = {en},
	number = {2},
	urldate = {2022-03-23},
	journal = {AORN Journal},
	author = {O'Connor, Ellen},
	month = feb,
	year = {2007},
	pages = {429},
}

@article{amulya_what_nodate,
	title = {What is {Reflective} {Practice}?},
	language = {en},
	author = {Amulya, Joy},
	pages = {4},
}

@article{finlay_reflecting_nodate,
	title = {Reflecting on ‘{Reflective} practice’},
	language = {en},
	author = {Finlay, Linda},
	pages = {28},
}

@article{van_manen_epistemology_1995,
	title = {On the {Epistemology} of {Reflective} {Practice}},
	volume = {1},
	issn = {1354-0602, 1470-1278},
	url = {https://www.tandfonline.com/doi/full/10.1080/1354060950010104},
	doi = {10.1080/1354060950010104},
	abstract = {Schb'n (1987) has suggested that professional education undervalues practical knowledge and grants privileged status to intellectual scientific and rational knowledge forms that may only be marginally relevant to practical acting. This is not just an issue of sociology of knowledge. The literature of teaching and teacher education has shown that professional practices of educating cannot be properly understood unless we are willing to conceive of practical knowledge and reflective practice quite differently. It is for this reason that I would like to raise some questions about the meaning and place of practical reflection in teaching and about the relation between knowledge and action in teaching, the kind of teaching that is educational or pedagogical.},
	language = {en},
	number = {1},
	urldate = {2022-03-23},
	journal = {Teachers and Teaching},
	author = {van Manen, Max},
	month = mar,
	year = {1995},
	pages = {33--50},
}

@article{mamede_structure_2004,
	title = {The structure of reflective practice in medicine},
	volume = {38},
	issn = {0308-0110, 1365-2923},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1365-2929.2004.01917.x},
	doi = {10.1111/j.1365-2929.2004.01917.x},
	abstract = {BACKGROUND The capability to reﬂect consciously upon one’s professional practice is generally considered important for the development of expertise and, hence, for education. However, to our knowledge no empirical research has been conducted to date into the nature of reﬂective practice in medicine.
PURPOSE To study the structure of reﬂective practice in medicine.
METHODS A questionnaire based on the literature was developed and administered to a group of primary care doctors. The data were subjected to conﬁrmatory factor analysis using structural equations modelling.
RESULTS A 5-factor model of reﬂective practice emerged. It consisted of the following factors: deliberate induction; deliberate deduction; testing and synthesising; openness for reﬂection, and meta-reasoning. The model ﬁtted the data sufﬁciently.
CONCLUSION A multidimensional structure of reﬂective practice in medicine was brought to light by the study. Its components in terms of reasoning processes, behaviours and attitudes were identiﬁed and measured among doctors. Once conceptualised and measured, reﬂective practice can be studied to gain a better understanding of its relation to expertise development in medicine. In addition, training students to apply reﬂective practices may become a goal in medical education.},
	language = {en},
	number = {12},
	urldate = {2022-03-23},
	journal = {Medical Education},
	author = {Mamede, Silvia and Schmidt, Henk G},
	month = dec,
	year = {2004},
	pages = {1302--1308},
}

@incollection{baumeister_reflection_2017,
	address = {Cham},
	title = {Reflection in {Agile} {Retrospectives}},
	volume = {283},
	isbn = {978-3-319-57632-9 978-3-319-57633-6},
	url = {http://link.springer.com/10.1007/978-3-319-57633-6_1},
	abstract = {A retrospective is a standard agile meeting practice designed for agile software teams to reflect and tune their process. Despite its integral importance, we know little about what aspects are focused upon during retrospectives and how reflection occurs in this practice. We conducted Case Study research involving data collected from interviews of sixteen software practitioners from four agile teams and observations of their retrospective meetings. We found that the important aspects focused on during the retrospective meeting include obstacles, feelings, previous action points, background reasons, areas of improvement, different ideas and perspectives and generating a plan. Reflection occurs when the agile teams embody these aspects within three levels of reflection: reporting and responding, relating and reasoning, and reconstructing. Critically, we show that agile teams may not achieve all levels of reflection simply by performing retrospective meetings. One of the key contributions of our work is to present a reflection framework for agile retrospective meetings that explains and embeds three levels of reflection within the five steps of a standard agile retrospective. Agile teams can use this framework to achieve better focus and higher levels of reflection in their retrospective meetings.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Agile {Processes} in {Software} {Engineering} and {Extreme} {Programming}},
	publisher = {Springer International Publishing},
	author = {Andriyani, Yanti and Hoda, Rashina and Amor, Robert},
	editor = {Baumeister, Hubert and Lichter, Horst and Riebisch, Matthias},
	year = {2017},
	doi = {10.1007/978-3-319-57633-6_1},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {3--19},
}

@inproceedings{li_redoshunter_2021,
	title = {{ReDoSHunter}: {A} {Combined} {Static} and {Dynamic} {Approach} for {Regular} {Expression} {DoS} {Detection}},
	abstract = {Regular expression Denial of Service (ReDoS) is a class of algorithmic complexity attacks using the regular expressions (regexes) that cause the typical backtracking-based matching algorithms to run super-linear time. Due to the wide adoption of regexes in computation, ReDoS poses a pervasive and serious security threat. Early detection of ReDoSvulnerable regexes in software is thus vital. Existing detection approaches mainly fall into two categories: static and dynamic analysis. However, they all suffer from either poor precision or poor recall in the detection of vulnerable regexes. The problem of accurately detecting vulnerable regexes at high precision and high recall remains unsolved. Furthermore, we observed that many ReDoS-vulnerable regex contain more than one vulnerability in reality. Another problem with existing approaches is that they are incapable of detecting multiple vulnerabilities in one regex. To address these two problems, we propose ReDoSHunter, a ReDoS-vulnerable regex detection framework that can effectively pinpoint the multiple vulnerabilities in a vulnerable regex, and generate examples of attack-triggering strings. ReDoSHunter is driven by ﬁve vulnerability patterns derived from massive vulnerable regexes. Besides pinpointing vulnerabilities, ReDoSHunter can assess the degree (i.e., exponential or polynomial) of the vulnerabilities detected. Our experiment results show that ReDoSHunter achieves 100\% precision and 100\% recall in the detection of ReDoS-vulnerable regexes in three large-scale datasets with 37,651 regexes. It signiﬁcantly outperforms seven state-of-the-art techniques. ReDoSHunter uncovered 28 new ReDoS-vulnerabilities in 26 well-maintained popular projects, resulting in 26 assigned CVEs and 2 ﬁxes.},
	language = {en},
	booktitle = {{IEEE} {Security} \& {Privacy}},
	author = {Li, Yeting and Chen, Zixuan and Cao, Jialun and Xu, Zhiwu and Peng, Qiancheng and Chen, Haiming and Chen, Liyuan and Cheung, Shing-Chi},
	year = {2021},
	pages = {19},
}

@misc{smith_social_2007,
	title = {Social {Software} {Building} {Blocks}},
	url = {https://web.archive.org/web/20171123070545/http://nform.com/ideas/social-software-building-blocks},
	urldate = {2022-02-15},
	author = {Smith, Gene},
	month = apr,
	year = {2007},
}

@article{khan_social_2014,
	title = {Social {Media} {Risks} and {Benefits}: {A} {Public} {Sector} {Perspective}},
	volume = {32},
	issn = {0894-4393},
	shorttitle = {Social {Media} {Risks} and {Benefits}},
	url = {https://doi.org/10.1177/0894439314524701},
	doi = {10.1177/0894439314524701},
	abstract = {Social media are becoming an important intermediary for interaction between governments, governments and citizens, and governmental agencies and businesses. This is due to the unique characteristics of social media: openness, participation, and sharing. However, despite rapid adoption, a growing concern and skepticism regarding the use of social media exists in the public sector. The purpose of this study is to investigate empirically the risks and benefits of social media use by public agencies. For this purpose, a research model was developed and tested in a survey of 289 government sector employees from six South Korean government research institutes. We found that both risks (i.e., social risk, time, psychological risks, and privacy concern) and benefits (i.e., social connectivity, social involvement, information attainment, and entertainment) significantly affect public sector employees’ satisfaction with and intention to use social media. However, the effect of the benefits on users’ satisfaction was stronger than the risks. The results of the study have important implications for researchers and policy makers.},
	language = {en},
	number = {5},
	urldate = {2022-02-27},
	journal = {Social Science Computer Review},
	author = {Khan, Gohar Feroz and Swar, Bobby and Lee, Sang Kon},
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	keywords = {government research institutes (GRIs), public sector, risks and benefits, social media, social network service (SNS)},
	pages = {606--627},
}

@inproceedings{bauer_post_2013,
	address = {New York, NY, USA},
	series = {{WPES} '13},
	title = {The post anachronism: the temporal dimension of facebook privacy},
	isbn = {978-1-4503-2485-4},
	shorttitle = {The post anachronism},
	url = {https://doi.org/10.1145/2517840.2517859},
	doi = {10.1145/2517840.2517859},
	abstract = {This paper reports on two studies that investigate empirically how privacy preferences about the audience and emphasis of Facebook posts change over time. In a 63-participant longitudinal study, participants gave their audience and emphasis preferences for up to ten of their Facebook posts in the week they were posted, again one week later, and again one month later. In a 234-participant retrospective study, participants expressed their preferences about posts made in the past week, as well as one year prior. We found that participants did not want content to fade away wholesale with age; the audience participants wanted to be able to access posts remained relatively constant over time. However, participants did want a handful of posts to become more private over time, as well as others to become more visible. Participants' predictions about how their preferences would change correlated poorly with their actual changes in preferences over time, casting doubt on ideas for setting an expiration date for content. Although older posts were seen as less relevant and had often been forgotten, participants found value in these posts for reminiscence. Surprisingly, we observed few concerns about privacy or self-presentation for older posts. We discuss our findings' implications for retrospective privacy mechanisms.},
	urldate = {2022-03-21},
	booktitle = {Proceedings of the 12th {ACM} workshop on {Workshop} on privacy in the electronic society},
	publisher = {Association for Computing Machinery},
	author = {Bauer, Lujo and Cranor, Lorrie Faith and Komanduri, Saranga and Mazurek, Michelle L. and Reiter, Michael K. and Sleeper, Manya and Ur, Blase},
	month = nov,
	year = {2013},
	keywords = {access control, facebook, privacy, sns, temporality, time, users},
	pages = {1--12},
}

@article{redmiles_i_2019,
	title = {“{I} {Just} {Want} to {Feel} {Safe}”: {A} {Diary} {Study} of {Safety} {Perceptions} on {Social} {Media}},
	volume = {13},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {“{I} {Just} {Want} to {Feel} {Safe}”},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/3356},
	abstract = {Social media can increase social capital, provide entertainment, and enable meaningful discourse. However, threats to safety experienced on social media platforms can inhibt users’ ability to gain these benefits. Threats to safety—whether real or perceived—detract from the pleasure people get out of their online interactions and damage the quality of online social spaces. While prior work has individually explored specific threats to safety – privacy, security, harassment – in this work we more broadly capture and characterize the full breadth of day-to-day experiences that influence users’ overall perceptions of safety on social media. We explore these perceptions through a three-week diary study (n=39). We contribute a novel, multidimensional taxonomy of how social media users define ’safety’, centered around security, privacy, and community. We conclude with a discussion of how safety perceptions can be used as a metric for social media quality, and detail the potential for enhancing safety perception through communityenhancing affordances and algorithmic transparency.},
	language = {en},
	urldate = {2022-03-21},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Redmiles, Elissa M. and Bodford, Jessica and Blackwell, Lindsay},
	month = jul,
	year = {2019},
	pages = {405--416},
}

@inproceedings{kim_rvfuzzer_2019,
	title = {\{{RVFuzzer}\}: {Finding} {Input} {Validation} {Bugs} in {Robotic} {Vehicles} through \{{Control}-{Guided}\} {Testing}},
	isbn = {978-1-939133-06-9},
	shorttitle = {\{{RVFuzzer}\}},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/kim},
	language = {en},
	urldate = {2022-03-21},
	author = {Kim, Taegyu and Kim, Chung Hwan and Rhee, Junghwan and Fei, Fan and Tu, Zhan and Walkup, Gregory and Zhang, Xiangyu and Deng, Xinyan and Xu, Dongyan},
	year = {2019},
	pages = {425--442},
}

@article{karhumaa_bluetooth_nodate,
	title = {{BLUETOOTH} {LOW} {ENERGY} {LINK} {LAYER} {INJECTION}},
	abstract = {Bluetooth Low Energy is a very widely used short-range wireless technology. During the last few years many high visibility Bluetooth related vulnerabilities have been discovered. A signiﬁcant amount of them have had an impact on implementations of the lowest protocol layers of Bluetooth in ﬁrmware running on separate embedded System on Chip dedicated for wireless communication. Bluetooth LE Link Layer implementations have not yet been under systematic fuzzing by vendors as there has been no mature way to inject fuzzed Link Layer packets over the air to the target device.},
	language = {en},
	author = {Karhumaa, Matias},
	pages = {36},
}

@inproceedings{aranda_secret_2009,
	title = {The secret life of bugs: {Going} past the errors and omissions in software repositories},
	shorttitle = {The secret life of bugs},
	doi = {10.1109/ICSE.2009.5070530},
	abstract = {Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	author = {Aranda, Jorge and Venolia, Gina},
	month = may,
	year = {2009},
	note = {ISSN: 1558-1225},
	keywords = {Automation, Computer bugs, Data mining, History, Navigation, Productivity, Programming, Software debugging, Software development management, Spatial databases},
	pages = {298--308},
}

@misc{hopkins_how_2017,
	title = {How to {Define} {Social} {Media} – {An} {Academic} {Summary}},
	url = {http://julianhopkins.com/how-to-define-social-media-an-academic-summary/},
	language = {en-GB},
	urldate = {2022-02-25},
	author = {Hopkins, Julian},
	month = oct,
	year = {2017},
}

@article{gurses_two_2013,
	title = {Two tales of privacy in online social networks},
	volume = {11},
	issn = {1558-4046},
	doi = {10.1109/MSP.2013.47},
	abstract = {Privacy is one of the friction points that emerge when communications are mediated in online social networks (OSNs). Different communities of computer science researchers have framed the OSN privacy problem as one of surveillance, institutional privacy, or social privacy. In tackling these problems, researchers have also treated them as if they were independent. In this article, the authors argue that the different privacy problems are entangled and that OSN privacy research would benefit from a more holistic approach.},
	number = {3},
	journal = {IEEE Security Privacy},
	author = {Gürses, Seda and Diaz, Claudia},
	month = may,
	year = {2013},
	note = {Conference Name: IEEE Security Privacy},
	keywords = {Computer science, Data privacy, Media, Privacy, Security, Social networking (online), Surveillance, human-computer interaction, online social networks, privacy-enhancing technologies, social privacy, surveillance},
	pages = {29--37},
}

@inproceedings{guo_mining_2012,
	title = {Mining {Privacy} {Settings} to {Find} {Optimal} {Privacy}-{Utility} {Tradeoffs} for {Social} {Network} {Services}},
	doi = {10.1109/SocialCom-PASSAT.2012.22},
	abstract = {Privacy has been a big concern for users of social network services (SNS). On recent criticism about privacy protection, most SNS now provide fine privacy controls, allowing users to set visibility levels for almost every profile item. However, this also creates a number of difficulties for users. First, SNS providers often set most items by default to the highest visibility to improve the utility of social network, which may conflict with users' intention. It is often formidable for a user to fine-tune tens of privacy settings towards the user desired settings. Second, tuning privacy settings involves an intricate tradeoff between privacy and utility. When you turn off the visibility of one item to protect your privacy, the social utility of that item is turned off as well. It is challenging for users to make a tradeoff between privacy and utility for each privacy setting. We propose a framework for users to conveniently tune the privacy settings towards the user desired privacy level and social utilities. It mines the privacy settings of a large number of users in a SNS, e.g., Facebook, to generate latent trait models for the level of privacy concern and the level of utility preference. A tradeoff algorithm is developed for helping users find the optimal privacy settings for a specified level of privacy concern and a personalized utility preference. We crawl a large number of Facebook accounts and derive the privacy settings with a novel method. These privacy setting data are used to validate and showcase the proposed approach.},
	booktitle = {2012 {International} {Conference} on {Privacy}, {Security}, {Risk} and {Trust} and 2012 {International} {Confernece} on {Social} {Computing}},
	author = {Guo, Shumin and Chen, Keke},
	month = sep,
	year = {2012},
	keywords = {Data Mining, Data models, Data privacy, Electronic mail, Facebook, Privacy, Social Network Services, Training data, Utility},
	pages = {656--665},
}

@inproceedings{alexander_privacy_2018,
	title = {Privacy {Shield} securing privacy in social networks},
	doi = {10.1109/ICCIC.2018.8782322},
	abstract = {Privacy is so important for social networks. Social networks are uncertain in terms of privacy. Now a days, privacy breach is a major problem. Data privacy will be lost when it is distributed in a social network. This is because of multi distributing the data without taking control of data. To deal with these problems, Securing privacy in Social Networks has been proposed based on privacy control over data. An efficient system named, Privacy Shield is designed to preserve privacy of data which can be managed by shared users in social networks. A third party server called Eco Server is used to do this. User privileges are administered by controlling the contents shared in social networks by the eco server. Base64 algorithm is used for data encryption.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Computational} {Intelligence} and {Computing} {Research} ({ICCIC})},
	author = {Alexander, Elizabeth and Vanathi, R. and Dhikhi, T and Sowmiya, M.},
	month = dec,
	year = {2018},
	note = {ISSN: 2473-943X},
	keywords = {Data privacy, Feeds, MD5, Organizations, Privacy, Security, Servers, Social networking (online), data privacy, post sharing, social networks, social profiling},
	pages = {1--4},
}

@inproceedings{namara_potential_2018,
	title = {The {Potential} for {User}-{Tailored} {Privacy} on {Facebook}},
	doi = {10.1109/PAC.2018.00010},
	abstract = {Research shows that Facebook users differ extensively in their use of various privacy features, and that they generally find it difficult to translate their desired privacy preferences into concrete interface actions. Our work explores the use of User-Tailored Privacy (UTP) to adapt Facebook's privacy features to the user's personal preferences. We developed adaptive versions of 19 Facebook privacy features, and for each feature we test three adaptation methods (Automation, Highlight and Suggestion) that can be used to implement the adaptive behavior. In a "think-aloud" semistructured interview study (N=18), we show participants paper prototypes of our adaptive privacy features and ask participants to judge the presented adaptive capabilities and the three adaptation methods that implement them. Our findings provide insights into the viability of User-Tailored Privacy. Specifically, we find that the optimal adaptation method depends on the users' familiarity with the privacy feature and how they use them, and their judgment of the awkwardness and irreversibility of the implemented privacy functionality. We conclude with design recommendations for the implementation of User-Tailored Privacy on Facebook and other social network platforms.},
	booktitle = {2018 {IEEE} {Symposium} on {Privacy}-{Aware} {Computing} ({PAC})},
	author = {Namara, Moses and Sloan, Henry and Jaiswal, Priyanka and Knijnenburg, Bart P.},
	month = sep,
	year = {2018},
	keywords = {Automation, Data privacy, Decision making, Facebook, Privacy, Task analysis, privacy, privacy on social media, social media, user-tailored privacy},
	pages = {31--42},
}

@inproceedings{smith_big_2012,
	title = {Big data privacy issues in public social media},
	doi = {10.1109/DEST.2012.6227909},
	abstract = {Big Data is a new label given to a diverse field of data intensive informatics in which the datasets are so large that they become hard to work with effectively. The term has been mainly used in two contexts, firstly as a technological challenge when dealing with dataintensive domains such as high energy physics, astronomy or internet search, and secondly as a sociological problem when data about us is collected and mined by companies such as Facebook, Google, mobile phone companies, retail chains and governments. In this paper we look at this second issue from a new perspective, namely how can the user gain awareness of the personally relevant part Big Data that is publicly available in the social web. The amount of user-generated media uploaded to the web is expanding rapidly and it is beyond the capabilities of any human to sift through it all to see which media impacts our privacy. Based on an analysis of social media in Flickr, Locr, Facebook and Google+, we discuss privacy implications and potential of the emerging trend of geo-tagged social media. We then present a concept with which users can stay informed about which parts of the social Big Data deluge is relevant to them.},
	booktitle = {2012 6th {IEEE} {International} {Conference} on {Digital} {Ecosystems} and {Technologies} ({DEST})},
	author = {Smith, Matthew and Szongott, Christian and Henne, Benjamin and von Voigt, Gabriele},
	month = jun,
	year = {2012},
	note = {ISSN: 2150-4946},
	keywords = {Data handling, Data storage systems, Facebook, Global Positioning System, Information management, Media, Privacy},
	pages = {1--6},
}

@inproceedings{bojanova_classifying_2021,
	title = {Classifying {Memory} {Bugs} {Using} {Bugs} {Framework} {Approach}},
	doi = {10.1109/COMPSAC51774.2021.00159},
	abstract = {In this work, we present an orthogonal classification of memory corruption bugs, allowing precise structured descriptions of related software vulnerabilities. The Common Weakness Enumeration (CWE) is a well-known and used list of software weaknesses. However, it’s exhaustive list approach is prone to gaps and overlaps in coverage. Instead, we utilize the Bugs Framework (BF) approach to define language-independent classes that cover all possible kinds of memory corruption bugs. Each class is a taxonomic category of a weakness type, defined by sets of operations, cause→consequence relations, and attributes. A BF description of a bug or a weakness is an instance of a taxonomic BF class, with one operation, one cause, one consequence, and their attributes. Any memory vulnerability then can be described as a chain of such instances and their consequence–cause transitions. We showcase that BF is a classification system that extends the CWE, providing a structured way to precisely describe real world vulnerabilities. It allows clear communication about software bugs and weaknesses and can help identify exploit mitigation techniques.},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Bojanova, Irena and Eduardo Galhardo, Carlos},
	month = jul,
	year = {2021},
	note = {ISSN: 0730-3157},
	keywords = {Bug classification, Computer bugs, Conferences, Resource management, Software, Taxonomy, Tools, bug taxonomy, memory corruption, software vulnerability, software weakness},
	pages = {1157--1164},
}

@misc{biden_remarks_2022,
	title = {Remarks of {President} {Joe} {Biden} – {State} of the {Union} {Address} {As} {Prepared} for {Delivery}},
	url = {https://www.whitehouse.gov/briefing-room/speeches-remarks/2022/03/01/remarks-of-president-joe-biden-state-of-the-union-address-as-delivered/},
	abstract = {United States Capitol Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the},
	language = {en-US},
	urldate = {2022-03-03},
	journal = {The White House},
	author = {Biden, Joe},
	month = mar,
	year = {2022},
}

@article{carr_social_2015,
	title = {Social {Media}: {Defining}, {Developing}, and {Divining}},
	volume = {23},
	issn = {1545-6870},
	shorttitle = {Social {Media}},
	url = {https://doi.org/10.1080/15456870.2015.972282},
	doi = {10.1080/15456870.2015.972282},
	abstract = {What is a social medium, and how may one moderate, isolate, and influence communicative processes within? Although scholars assume an inherent understanding of social media based on extant technology, there is no commonly accepted definition of what social media are, both functionally and theoretically, within communication studies. Given this lack of understanding, cogent theorizing regarding the uses and effects of social media has been limited. This work first draws on extant definitions of social media and subcategories (e.g., social network sites) from public relations, information technology, and management scholarship, as well as the popular press, to develop a definition of social media precise enough to embody these technologies yet robust enough to remain applicable in 2035. It then broadly explores emerging developments in the features, uses, and users of social media for which future theories will need to account. Finally, it divines and prioritizes challenges that may not yet be apparent to theorizing communication processes with and in mercurial social media. We address how social media may uniquely isolate and test communicative principles to advance our understanding of human–human and human–computer interaction. In all, this article provides a common framework to ground and facilitate future communication scholarship and beyond.},
	number = {1},
	urldate = {2022-03-08},
	journal = {Atlantic Journal of Communication},
	author = {Carr, Caleb T. and Hayes, Rebecca A.},
	month = jan,
	year = {2015},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15456870.2015.972282},
	pages = {46--65},
}

@inproceedings{yuan_simple_2014,
	title = {Simple {Testing} {Can} {Prevent} {Most} {Critical} {Failures}: {An} {Analysis} of {Production} {Failures} in {Distributed} \{{Data}-{Intensive}\} {Systems}},
	isbn = {978-1-931971-16-4},
	shorttitle = {Simple {Testing} {Can} {Prevent} {Most} {Critical} {Failures}},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/yuan},
	language = {en},
	urldate = {2022-03-07},
	author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
	year = {2014},
	pages = {249--265},
}

@inproceedings{wang_exploratory_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://doi.org/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	urldate = {2022-03-07},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	month = aug,
	year = {2021},
	keywords = {Unmanned aerial vehicles, empirical study, software bugs},
	pages = {20--31},
}

@inproceedings{makhshari_iot_2021,
	title = {{IoT} {Bugs} and {Development} {Challenges}},
	doi = {10.1109/ICSE43902.2021.00051},
	abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers' challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Makhshari, Amir and Mesbah, Ali},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	keywords = {Computer bugs, Correlation, Empirical Study, Faces, Internet of Things, Mining Software Repositories, Software Engineering, Systematics, Taxonomy, Tools},
	pages = {460--472},
}

@inproceedings{seaman_defect_2008,
	address = {New York, NY, USA},
	series = {{ESEM} '08},
	title = {Defect categorization: making use of a decade of widely varying historical data},
	isbn = {978-1-59593-971-5},
	shorttitle = {Defect categorization},
	url = {https://doi.org/10.1145/1414004.1414030},
	doi = {10.1145/1414004.1414030},
	abstract = {This paper describes our experience in aggregating a number of historical datasets containing inspection defect data using different categorization schemes. Our goal was to make use of the historical data by creating models to guide future development projects. We describe our approach to reconciling the different choices used in the historical datasets to categorize defects, and the challenges we faced. We also present a set of recommendations for others involved in classifying defects.},
	urldate = {2022-03-07},
	booktitle = {Proceedings of the {Second} {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	publisher = {Association for Computing Machinery},
	author = {Seaman, Carolyn B. and Shull, Forrest and Regardie, Myrna and Elbert, Denis and Feldmann, Raimund L. and Guo, Yuepu and Godfrey, Sally},
	month = oct,
	year = {2008},
	keywords = {defect categories, defects, historical data},
	pages = {149--157},
}

@article{hafiz_game_2016,
	title = {Game of detections: how are security vulnerabilities discovered in the wild?},
	volume = {21},
	issn = {1573-7616},
	shorttitle = {Game of detections},
	url = {https://doi.org/10.1007/s10664-015-9403-7},
	doi = {10.1007/s10664-015-9403-7},
	abstract = {There is little or no information available on what actually happens when a software vulnerability is detected. We performed an empirical study on reporters of the three most prominent security vulnerabilities: buffer overflow, SQL injection, and cross site scripting vulnerabilities. The goal was to understand the methods and tools used during the discovery and whether the community of developers exploring one security vulnerability differs—in their approach—from another community of developers exploring a different vulnerability. The reporters were featured in the SecurityFocus repository for twelve month periods for each vulnerability. We collected 127 responses. We found that the communities differ based on the security vulnerability they target; but within a specific community, reporters follow similar approaches. We also found a serious problem in the vulnerability reporting process that is common for all communities. Most reporters, especially the experienced ones, favor full-disclosure and do not collaborate with the vendors of vulnerable software. They think that the public disclosure, sometimes supported by a detailed exploit, will put pressure on vendors to fix the vulnerabilities. But, in practice, the vulnerabilities not reported to vendors are less likely to be fixed. Ours is the first study on vulnerability repositories that targets the reporters of the most common security vulnerabilities, thus concentrating on the people involved in the process; previous works have overlooked this rich information source. The results are valuable for beginners exploring how to detect and report security vulnerabilities and for tool vendors and researchers exploring how to automate and fix the process.},
	language = {en},
	number = {5},
	urldate = {2022-03-07},
	journal = {Empirical Software Engineering},
	author = {Hafiz, Munawar and Fang, Ming},
	month = oct,
	year = {2016},
	pages = {1920--1959},
}

@inproceedings{perumal_rule-based_2016,
	title = {Rule-based conflict resolution framework for {Internet} of {Things} device management in smart home environment},
	doi = {10.1109/GCCE.2016.7800444},
	abstract = {Recent developments in Internet of Things (IoT) tremendously have introduced several heterogeneous systems and devices that characterize a smart home. Generally, these heterogeneous systems are dissimilar and accomplish various services and functionalities. Due to the gradual changes of managing resources in smart home, more heterogeneous systems are being introduced from time to time depending on the consumer requirement. As such, more dependencies are created among heterogeneous systems, and this could lead towards conflict occurrences among them. Conflicts could occur in smart home when two or more events generated by heterogeneous systems need to be triggered at an instance of time. In this paper, we present a rule-based conflict resolution framework using scheduling algorithm for managing heterogeneous systems in smart home environment. Events are captured and processed by the framework which performs corresponding conflict resolution on the heterogeneous systems. The developed framework was implemented with several heterogeneous systems to validate their effectiveness in solving conflict occurrences. The framework was ascertained to be consistent in smart home environment.},
	booktitle = {2016 {IEEE} 5th {Global} {Conference} on {Consumer} {Electronics}},
	author = {Perumal, Thinagaran and Sulaiman, Md Nasir and Datta, Soumya Kanti and Ramachandran, Thinaharan and Leong, Chui Yew},
	month = oct,
	year = {2016},
	keywords = {Conflict resolution, ECA, Smart home environment},
	pages = {1--2},
}

@inproceedings{han_ghostnet_2020,
	address = {Seattle, WA, USA},
	title = {{GhostNet}: {More} {Features} {From} {Cheap} {Operations}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{GhostNet}},
	url = {https://ieeexplore.ieee.org/document/9157333/},
	doi = {10.1109/CVPR42600.2020.00165},
	abstract = {Deploying convolutional neural networks (CNNs) on embedded devices is difﬁcult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7\% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC2012 classiﬁcation dataset. Code is available at https: //github.com/huawei-noah/ghostnet.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
	month = jun,
	year = {2020},
	pages = {1577--1586},
}

@article{davidsen_longitudinal_2010,
	title = {A longitudinal study of development and maintenance},
	volume = {52},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584910000431},
	doi = {10.1016/j.infsof.2010.03.003},
	abstract = {Objective: The paper presents ﬁnding relative to the distribution of work between maintenance and development tasks, comparing to the results reported earlier to assess the stability of important metrics within the area.
Method: This paper presents the main results of a survey-investigation performed in 2008 in 67 Norwegian organizations comparing the distribution of work to results from similar investigations performed in Norway in 1993, 1998, and 2003. Some comparisons to similar investigations performed in USA before this is also provided.
Results: The amount of application portfolio upkeep (work made to keep up the functional coverage of the application system portfolio of the organization, including the development of replacement systems), is at the same level as reported in 1998 and 2003. The level of application maintenance is also on the same level as the similar investigations conducted in 2003 and 1998. There was a signiﬁcant increase in both maintenance and application portfolio upkeep from 1993 to 1998, which could partly be attributed to be the extra maintenance and replacement-oriented work necessary to deal with the ‘‘year 2000 problem”, but this seemed to be reversed in 2003 and 2008. As for the 2003 investigation, the slow ITmarket in general seemed to have inﬂuenced the results negatively seen from the point of view of application systems support efﬁciency in organization. No similar explanation can be used for the 2008 numbers.
Conclusion: Based on the last surveys it seems than a stable level of work distribution both on maintenance and application portfolio upkeep have been reached, although the underlying development technologies are still undergoing large changes. This is contrary to others claiming that the amount of maintenance is still increasing.},
	language = {en},
	number = {7},
	urldate = {2022-03-04},
	journal = {Information and Software Technology},
	author = {Davidsen, Magne Kristoffer and Krogstie, John},
	month = jul,
	year = {2010},
	pages = {707--719},
}

@article{li_muafl_2022,
	title = {\${\textbackslash}mu\${AFL}: {Non}-intrusive {Feedback}-driven {Fuzzing} for {Microcontroller} {Firmware}},
	shorttitle = {\${\textbackslash}mu\${AFL}},
	url = {http://arxiv.org/abs/2202.03013},
	doi = {10.1145/3510003.3510208},
	abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present \${\textbackslash}mu\$AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, \${\textbackslash}mu\$AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated \${\textbackslash}mu\$AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.03013 [cs]},
	author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{peng_usbfuzz_2020,
	title = {\{{USBFuzz}\}: {A} {Framework} for {Fuzzing} \{{USB}\} {Drivers} by {Device} {Emulation}},
	isbn = {978-1-939133-17-5},
	shorttitle = {\{{USBFuzz}\}},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/peng},
	language = {en},
	urldate = {2022-03-04},
	author = {Peng, Hui and Payer, Mathias},
	year = {2020},
	pages = {2559--2575},
}

@article{li_muafl_2022-1,
	title = {\${\textbackslash}mu\${AFL}: {Non}-intrusive {Feedback}-driven {Fuzzing} for {Microcontroller} {Firmware}},
	shorttitle = {\${\textbackslash}mu\${AFL}},
	url = {http://arxiv.org/abs/2202.03013},
	doi = {10.1145/3510003.3510208},
	abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present \${\textbackslash}mu\$AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, \${\textbackslash}mu\$AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated \${\textbackslash}mu\$AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.03013 [cs]},
	author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{li_muafl_2022-2,
	title = {\${\textbackslash}mu\${AFL}: {Non}-intrusive {Feedback}-driven {Fuzzing} for {Microcontroller} {Firmware}},
	shorttitle = {\${\textbackslash}mu\${AFL}},
	url = {http://arxiv.org/abs/2202.03013},
	doi = {10.1145/3510003.3510208},
	abstract = {Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present \${\textbackslash}mu\$AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, \${\textbackslash}mu\$AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated \${\textbackslash}mu\$AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.03013 [cs]},
	author = {Li, Wenqiang and Shi, Jiameng and Li, Fengjun and Lin, Jingqiang and Wang, Wei and Guan, Le},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{adams_people_2021,
	title = {People systematically overlook subtractive changes},
	volume = {592},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03380-y},
	doi = {10.1038/s41586-021-03380-y},
	abstract = {Improving objects, ideas or situations—whether a designer seeks to advance technology, a writer seeks to strengthen an argument or a manager seeks to encourage desired behaviour—requires a mental search for possible changes1–3. We investigated whether people are as likely to consider changes that subtract components from an object, idea or situation as they are to consider changes that add new components. People typically consider a limited number of promising ideas in order to manage the cognitive burden of searching through all possible ideas, but this can lead them to accept adequate solutions without considering potentially superior alternatives4–10. Here we show that people systematically default to searching for additive transformations, and consequently overlook subtractive transformations. Across eight experiments, participants were less likely to identify advantageous subtractive changes when the task did not (versus did) cue them to consider subtraction, when they had only one opportunity (versus several) to recognize the shortcomings of an additive search strategy or when they were under a higher (versus lower) cognitive load. Defaulting to searches for additive changes may be one reason that people struggle to mitigate overburdened schedules11, institutional red tape12 and damaging effects on the planet13,14.},
	language = {en},
	number = {7853},
	urldate = {2022-03-03},
	journal = {Nature},
	author = {Adams, Gabrielle S. and Converse, Benjamin A. and Hales, Andrew H. and Klotz, Leidy E.},
	month = apr,
	year = {2021},
	note = {Number: 7853
Publisher: Nature Publishing Group},
	keywords = {Decision making, Human behaviour},
	pages = {258--261},
}

@inproceedings{kalliamvakou_promises_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {The promises and perils of mining {GitHub}},
	isbn = {978-1-4503-2863-0},
	url = {https://doi.org/10.1145/2597073.2597074},
	doi = {10.1145/2597073.2597074},
	abstract = {With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40\% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.},
	urldate = {2022-03-03},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {Association for Computing Machinery},
	author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
	month = may,
	year = {2014},
	keywords = {Mining software repositories, bias, code reviews, git, github},
	pages = {92--101},
}

@article{case_emerging_2011,
	title = {Emerging {Research} {Methodologies} in {Engineering} {Education} {Research}},
	volume = {100},
	issn = {10694730},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2011.tb00008.x},
	doi = {10.1002/j.2168-9830.2011.tb00008.x},
	abstract = {BACKGROUND Methodology refers to the theoretical arguments that researchers use in order to justify their research methods and design. There is an extensive range of well established methodologies in the educational research literature of which a growing subset is beginning to be used in engineering education research.
PURPOSE A more explicit engagement with methodologies, particularly those that are only emerging in engineering education research, is important so that engineering education researchers can broaden the set of research questions they are able to address. SCOPE/METHOD Seven methodologies are outlined and for each an exemplar paper is analyzed in order to demonstrate the methodology in operation and to highlight its particular contribution. The methodologies are: Case Study, Grounded Theory, Ethnography, Action Research, Phenomenography, Discourse Analysis, and Narrative Analysis. It is noted that many of the exemplar papers use some of these methodologies in combination.
CONCLUSIONS The exemplar papers show that collectively these methodologies might allow the research community to be able to better address questions around key engineering education challenges, such as students' responses to innovative pedagogies, diversity issues in engineering, and the changing requirements for engineering graduates in the twenty-first century.},
	language = {en},
	number = {1},
	urldate = {2022-03-01},
	journal = {Journal of Engineering Education},
	author = {Case, Jennifer M. and Light, Gregory},
	month = jan,
	year = {2011},
	pages = {186--210},
}

@article{delatte_failure_2010,
	title = {Failure literacy in structural engineering},
	volume = {32},
	issn = {01410296},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014102960900399X},
	doi = {10.1016/j.engstruct.2009.12.015},
	abstract = {The history of the development of practice in many engineering disciplines is, in large part, the story of failures, both imminent and actual, and of the changes to designs, standards and procedures made as the result of timely interventions or forensic analyses. All engineers, and more particularly structural engineers, should be failure literate. Failure literacy means knowing about the critical historical failure cases that have shaped the profession: not merely the surface technical details, but the environment, the communications difficulties and the procedural issues. In the US, an intensive effort has been under way for nearly a decade to promote failure literacy in engineering education and practice. A number of educational resources have been developed to make it easier for engineering students and practicing engineers to learn from failures.},
	language = {en},
	number = {7},
	urldate = {2022-03-01},
	journal = {Engineering Structures},
	author = {Delatte, Norbert},
	month = jul,
	year = {2010},
	pages = {1952--1954},
}

@book{delatte2008beyond,
	title = {Beyond failure: {Forensic} case studies for civil engineers},
	publisher = {American Society of Civil Engineers},
	author = {Delatte Jr, Norbert J},
	year = {2008},
	note = {tex.organization: American Society of Civil Engineers},
}

@article{ubani_study_2013,
	title = {A study of failure and abandonment of public sector-driven civil engineering projects in {Nigeria}: {An} empirical review},
	volume = {4},
	issn = {2153649X},
	shorttitle = {A study of failure and abandonment of public sector-driven civil engineering projects in {Nigeria}},
	url = {http://www.scihub.org/AJSIR/PDF/2013/1/AJSIR-4-1-75-82.pdf},
	doi = {10.5251/ajsir.2013.4.1.75.82},
	abstract = {Incessant failure and abandonment of public sector projects are still posing serious concern and challenges to the society and other stakeholders in civil engineering and construction industries. The study identifies and examines the salient factors and the warning signals responsible for failure and abandonment of public sector driven civil engineering projects with a view of directing efforts towards forestalling the problems. Opinion survey was adopted with area and judgmental sampling procedures. Primary data based on the identified factors, was captured with the instrument of questionnaire from professionals in civil engineering projects operating in the South East geopolitical zone of Nigeria. The analytical tools used in the study were severity index, spearman’s rank correlation coefficient, relative agreement factors and Kendall’s coefficient of concordance (W.). The rankings of different professionals were significantly correlated. The result of percentage relative agreement factors indicates that the most salient factors causing failure and abandonment of public sector driven civil engineering projects in order of significance are frequent changes in government and political power, unreliable mode of financing and payment of completed work, and project contract sum indirectly used to compensate political big-wigs etc. Wtest further substantiates the results by indicating significant degree of concordance in opinion of experts. The study therefore concludes that politically- induced corruption, undefined and non compliance to the agreed mode of financing and payment of completed work are the bane of project success. It is therefore a matter of legislation and policy formulation which should be instituted to avert failure and abandonment of public sector-driven civil engineering projects.},
	language = {en},
	number = {1},
	urldate = {2022-03-01},
	journal = {American Journal of Scientific and Industrial Research},
	author = {Ubani, E. and Ononuju, C.},
	month = feb,
	year = {2013},
	pages = {75--82},
}

@article{johnson_forensic_2002,
	title = {Forensic software engineering: are software failures symptomatic of systemic problems?},
	volume = {40},
	issn = {09257535},
	shorttitle = {Forensic software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925753501000868},
	doi = {10.1016/S0925-7535(01)00086-8},
	abstract = {There is a growing realization that existing accident investigation techniques fail to meet the challenges created by incidents that involve software failures. Existing software development techniques cannot easily be used to provide retrospective information about the complex and systemic causes of major accidents. This paper, therefore, argues that we must develop speciﬁc techniques to support forensic software engineering. It is important that these techniques should look beyond ‘programmer error’ as a primary cause of software failure. They must enable investigators to identify the systemic problems that are created by inadequate investment, by poor management leadership and by the breakdown in communication between development teams. This argument builds on previous work by Leveson and by Reason. They have focused on the importance of a systemic approach to the development of safety-critical applications. Relatively little attention has been paid to a systemic analysis of their failure. Later sections of this paper analyze the potential problems that can arise when a systemic approach is extended from systems development to accident investigation. \# 2002 Elsevier Science Ltd. All rights reserved.},
	language = {en},
	number = {9},
	urldate = {2022-03-01},
	journal = {Safety Science},
	author = {Johnson, Chris},
	month = dec,
	year = {2002},
	pages = {835--847},
}

@inproceedings{pasquale_towards_2018,
	address = {Gothenburg Sweden},
	title = {Towards forensic-ready software systems},
	isbn = {978-1-4503-5662-6},
	url = {https://dl.acm.org/doi/10.1145/3183399.3183426},
	doi = {10.1145/3183399.3183426},
	abstract = {As software becomes more ubiquitous, and the risk of cyber-crimes increases, ensuring that software systems are forensic-ready (i.e., capable of supporting potential digital investigations) is critical. However, little or no attention has been given to how well-suited existing software engineering methodologies and practices are for the systematic development of such systems. In this paper, we consider the meaning of forensic readiness of software, define forensic readiness requirements, and highlight some of the open software engineering challenges in the face of forensic readiness. We use a real software system developed to investigate online sharing of child abuse media to illustrate the presented concepts.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	publisher = {ACM},
	author = {Pasquale, Liliana and Alrajeh, Dalal and Peersman, Claudia and Tun, Thein and Nuseibeh, Bashar and Rashid, Awais},
	month = may,
	year = {2018},
	pages = {9--12},
}

@inproceedings{grispos_are_2017,
	address = {Brighton, United Kingdom},
	title = {Are you ready? {Towards} the engineering of forensic-ready systems},
	isbn = {978-1-5090-5476-3},
	shorttitle = {Are you ready?},
	url = {http://ieeexplore.ieee.org/document/7956555/},
	doi = {10.1109/RCIS.2017.7956555},
	abstract = {As security incidents continue to impact organisations, there is a growing demand for systems to be ‘forensicready’ – to maximise the potential use of evidence whilst minimising the costs of an investigation. Researchers have supported organisational forensic readiness efforts by proposing the use of policies and processes, aligning systems with forensics objectives and training employees. However, recent work has also proposed an alternative strategy for implementing forensic readiness called forensic-by-design. This is an approach that involves integrating requirements for forensics into relevant phases of the systems development lifecycle with the aim of engineering forensic-ready systems. While this alternative forensic readiness strategy has been discussed in the literature, no previous research has examined the extent to which organisations actually use this approach for implementing forensic readiness. Hence, we investigate the extent to which organisations consider requirements for forensics during systems development. We ﬁrst assessed existing research to identify the various perspectives of implementing forensic readiness, and then undertook an online survey to investigate the consideration of requirements for forensics during systems development lifecycles. Our ﬁndings provide an initial assessment of the extent to which requirements for forensics are considered within organisations. We then use our ﬁndings, coupled with the literature, to identify a number of research challenges regarding the engineering of forensic-ready systems.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {2017 11th {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	publisher = {IEEE},
	author = {Grispos, George and Garcia-Galan, Jesus and Pasquale, Liliana and Nuseibeh, Bashar},
	month = may,
	year = {2017},
	pages = {328--333},
}

@inproceedings{jan2015transformer,
	title = {Transformer failures, causes \& impact},
	booktitle = {International conference data mining, civil and mechanical engineering},
	author = {Jan, Shayan Tariq and Afzal, Raheel and Khan, Akif Zia},
	year = {2015},
	pages = {49--52},
}

@article{barber_quality_2000,
	title = {Quality failure costs in civil engineering projects},
	volume = {17},
	issn = {0265-671X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/02656710010298544/full/html},
	doi = {10.1108/02656710010298544},
	abstract = {A methodology was developed to measure cost of quality failures in two major road projects, largely based upon a work-shadowing method. Shows how the initial data were collected and categorised into definable groups and how the costs were estimated for each of these categories. The findings suggest that, if the projects examined are typical, the cost of failures may be a significant percentage of total costs, and that conventional means of identifying them may not be reliable. Moreover, the costs will not be easy to eradicate without widespread changes in attitudes and norms of behaviour within the industry and improved managerial co-ordination of activities throughout the supply chain.},
	language = {en},
	number = {4/5},
	urldate = {2022-03-01},
	journal = {International Journal of Quality \& Reliability Management},
	author = {Barber, Patrick and Graves, Andrew and Hall, Mark and Sheath, Darryl and Tomkins, Cyril},
	month = jun,
	year = {2000},
	pages = {479--492},
}

@article{abdulrahman_capturing_1993,
	title = {Capturing the {Cost} of {Quality} {Failures} in {Civil} {Engineering}},
	volume = {10},
	issn = {0265-671X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/02656719310037956/full/html},
	doi = {10.1108/02656719310037956},
	abstract = {A description of failure events during construction illustrates the urgent need to emphasize the management of quality in civil engineering projects. During the construction of a civil engineering project, cost control techniques are used to monitor cost trends and to detect cost deviations in order to control project cost. However, this technique does not reveal the cause of any failure. The nature and collection of failure costs have been part of quality costing. Hypothetical illustrations show how failure costs can be extracted during construction using a matrix. Quality cost information can be used to supplement cost control techniques for cost control purposes and in identifying weaknesses within a system.},
	language = {en},
	number = {3},
	urldate = {2022-03-01},
	journal = {International Journal of Quality \& Reliability Management},
	author = {Abdul‐Rahman, Hamzah},
	month = mar,
	year = {1993},
}

@article{delatte_forensics_2002,
	title = {Forensics and {Case} {Studies} in {Civil} {Engineering} {Education}: {State} of the {Art}},
	volume = {16},
	issn = {0887-3828, 1943-5509},
	shorttitle = {Forensics and {Case} {Studies} in {Civil} {Engineering} {Education}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%290887-3828%282002%2916%3A3%2898%29},
	doi = {10.1061/(ASCE)0887-3828(2002)16:3(98)},
	abstract = {This paper reviews the state of the art in the use of forensic engineering and failure case studies in civil engineering education. The study of engineering failures can offer students valuable insights into associated technical, ethical, and professional issues. Lessons learned from failures have substantially affected civil engineering practice. For the student, study of these cases can help place design and analysis procedures into historical context and reinforce the necessity of lifelong learning. Three approaches for bringing forensics and failure case studies into the civil engineering curriculum are discussed in this paper. These are stand-alone forensic engineering or failure case study courses, capstone design projects, and integration of case studies into the curriculum. Some of the cases have been developed and used in courses at the United States Military Academy and the Univ. of Alabama at Birmingham, as well as at other institutions. Finally, the writers have tried to assemble many of the known sources of material, including books, technical papers, and magazine articles, videos, Web sites, prepared PowerPoint presentations, and television programs.},
	language = {en},
	number = {3},
	urldate = {2022-03-01},
	journal = {Journal of Performance of Constructed Facilities},
	author = {Delatte, Norbert J. and Rens, Kevin L.},
	month = aug,
	year = {2002},
	pages = {98--109},
}

@article{pietroforte_civil_nodate,
	title = {Civil {Engineering} {Education} through {Case} {Studies} of {Failures}},
	abstract = {Teaching through failure case studies complements and integrates the traditional civil engineering topics that are introduced with the lecture-based method. This paper describes the development of a course on failures at Worcester Polytechnic Institute with particular reference to the implementation of case study instruction and structuring of the course. The topic of failures is used to assess the decision-making process that underpins the life cycle of a facility, and the variables that affect the performance of this process. The primary objective of the case study method is to enhance learning through students' active participation in case presentation and discussion. The method is effective in blending real world situations and applications of theoretical and scientific principles.},
	language = {en},
	author = {Pietroforte, Roberto and Member, Associate},
	pages = {5},
}

@inproceedings{delatte_using_2000,
	address = {San Juan, Puerto Rico, United States},
	title = {Using {Failure} {Case} {Studies} in {Civil} {Engineering} {Education}},
	isbn = {978-0-7844-0482-9},
	url = {http://ascelibrary.org/doi/10.1061/40482%28280%2946},
	doi = {10.1061/40482(280)46},
	abstract = {The study of engineering failures can offer students valuable insights into the associated technical, ethical, and professional issues. In many cases, lessons learned from failures have substantially affected civil engineering practice. For the student, study of these cases can help place design and analysis procedures into historical context and reinforce the necessity of lifelong learning. Of course, it would be impossible to add an undergraduate course on failures and lessons learned to an already overcrowded curriculum. A more practical solution is to integrate failure case studies into existing undergraduate courses. In this paper a master plan for the use of failure case studies in an undergraduate civil engineering curriculum is discussed. First, for a selected number of courses, several relevant principles are identified. Examples include drawing correct and complete free body diagrams (statics), compression member buckling (mechanics of materials, structures), shear strength of concrete beams (reinforced concrete design), and connection detailing and behavior (structural steel design). For each of these principles, failure case studies have been identified. They are discussed in some detail within this paper, and resources and references for further development of the case studies are indicated. Some of the cases have been developed and used in courses at the United States Military Academy and the University of Alabama at Birmingham.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {Forensic {Engineering} (2000)},
	publisher = {American Society of Civil Engineers},
	author = {Delatte, Norbert J.},
	month = apr,
	year = {2000},
	pages = {430--440},
}

@article{zhang_experimental_2004,
	title = {Experimental study of steam turbine control valves},
	volume = {218},
	issn = {0954-4062, 2041-2983},
	url = {http://journals.sagepub.com/doi/10.1243/095440604323052283},
	doi = {10.1243/095440604323052283},
	abstract = {Because of the converging–diverging con guration of the valve passage, venturi valves have been widely used in large turbines to regulate inlet ow as turbine governing valves for about half a century. F rom the 1960s, a number of valve failure incidents have been reported. Improvement to current designs was strongly demanded but, owing to the complicated nature of the uid–structure interaction mechanisms, the basic mechanism causing valve failure is still far from being fully understood. Experimental investigations on a half-scale valve were performed here. The study con rmed that asymmetric unstable ow is the root cause of valve problems, such as noise, vibration and failure.},
	language = {en},
	number = {5},
	urldate = {2022-03-01},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
	author = {Zhang, D and Engeda, A and Hardin, J R and Aungier, R H},
	month = may,
	year = {2004},
	pages = {493--507},
}

@inproceedings{janzen_influence_2006,
	title = {On the {Influence} of {Test}-{Driven} {Development} on {Software} {Design}},
	doi = {10.1109/CSEET.2006.25},
	abstract = {Test-driven development (TDD) is an agile software development strategy that addresses both design and testing. This paper describes a controlled experiment that examines the effects of TDD on internal software design quality. The experiment was conducted with undergraduate students in a software engineering course. Students in three groups completed semester-long programming projects using either an iterative test-first (TDD), iterative test-last, or linear test-last approach. Results from this study indicate that TDD can be an effective software design approach improving both code-centric aspects such as object decomposition, test coverage, and external quality, and developer-centric aspects including productivity and confidence. In addition, iterative development approaches that include automated testing demonstrated benefits over a more traditional linear approach with manual tests. This study demonstrates the viability of teaching TDD with minimal effort in the context of a relatively traditional development approach. Potential dangers with TDD are identified regarding programmer motivation and discipline. Pedagogical implications and instructional techniques which may foster TDD adoption will also be referenced},
	booktitle = {19th {Conference} on {Software} {Engineering} {Education} {Training} ({CSEET}'06)},
	author = {Janzen, D.S. and Saiedian, H.},
	month = apr,
	year = {2006},
	note = {ISSN: 2377-570X},
	keywords = {Automatic testing, Iterative methods, Linear programming, Productivity, Programming profession, Software design, Software engineering, Software testing, System testing, Writing},
	pages = {141--148},
}

@inproceedings{falessi_value-based_2008,
	title = {Value-{Based} {Design} {Decision} {Rationale} {Documentation}: {Principles} and {Empirical} {Feasibility} {Study}},
	shorttitle = {Value-{Based} {Design} {Decision} {Rationale} {Documentation}},
	doi = {10.1109/WICSA.2008.8},
	abstract = {The explicit documentation of the rationale of design decisions is a practice generally encouraged, but rarely implemented in industry because of a variety of inhibitors. Methods proposed in the past for design decisions rationale documentation (DDRD) aimed to maximize benefits for the DDRD consumer by imposing on the producer of DDRD the burden to document all the potentially useful information. We propose here a compromise which consists in tailoring DDRD, based on its intended use or purpose. In our view, the adoption of a tailored DDRD, consisting only of the required set of information, would mitigate the effects of DDRD inhibitors. The aim of this paper is twofold: i) to discuss the application of value-based software engineering principles to DDRD, ii) to describe a controlled experiment to empirically analyze the feasibility of the proposed method. Results show that the level of utility related to the same category of DDRD information significantly changes depending on its purpose; such result is novel and it demonstrates the feasibility of the proposed value-based DDRD.},
	booktitle = {Seventh {Working} {IEEE}/{IFIP} {Conference} on {Software} {Architecture} ({WICSA} 2008)},
	author = {Falessi, Davide and Cantone, Giovanni and Kruchten, Philippe},
	month = feb,
	year = {2008},
	keywords = {Application software, Computer architecture, Computer industry, Costs, Design decision rationale documentation, Documentation, Inhibitors, Software architecture, Software design, Software engineering, Software testing, empirical software engineering, value based software engineering},
	pages = {189--198},
}

@article{glass_project_2002,
	title = {Project retrospectives, and why they never happen},
	volume = {19},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/1032872/},
	doi = {10.1109/MS.2002.1032872},
	language = {en},
	number = {5},
	urldate = {2022-02-28},
	journal = {IEEE Software},
	author = {Glass, R.L.},
	month = sep,
	year = {2002},
	pages = {112--111},
}

@misc{noauthor_admin_nodate,
	title = {Admin {Option} to {Disable} {Trending} {Feature} · {Issue} \#7702 · mastodon/mastodon},
	url = {https://github.com/mastodon/mastodon/issues/7702},
	abstract = {Instance admins should be able to easily disable the trending hashtag feature. [x ] I searched or browsed the repo’s other issues to ensure this is not a duplicate.},
	language = {en},
	urldate = {2022-02-27},
	journal = {GitHub},
}

@article{twenge_underestimating_2020,
	title = {Underestimating digital media harm},
	volume = {4},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-0839-4},
	doi = {10.1038/s41562-020-0839-4},
	language = {en},
	number = {4},
	urldate = {2022-02-27},
	journal = {Nature Human Behaviour},
	author = {Twenge, Jean M. and Haidt, Jonathan and Joiner, Thomas E. and Campbell, W. Keith},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Sociology},
	pages = {346--348},
}

@article{kietzmann_social_2011,
	series = {{SPECIAL} {ISSUE}: {SOCIAL} {MEDIA}},
	title = {Social media? {Get} serious! {Understanding} the functional building blocks of social media},
	volume = {54},
	issn = {0007-6813},
	shorttitle = {Social media?},
	url = {https://www.sciencedirect.com/science/article/pii/S0007681311000061},
	doi = {10.1016/j.bushor.2011.01.005},
	abstract = {Traditionally, consumers used the Internet to simply expend content: they read it, they watched it, and they used it to buy products and services. Increasingly, however, consumers are utilizing platforms—such as content sharing sites, blogs, social networking, and wikis—to create, modify, share, and discuss Internet content. This represents the social media phenomenon, which can now significantly impact a firm's reputation, sales, and even survival. Yet, many executives eschew or ignore this form of media because they don’t understand what it is, the various forms it can take, and how to engage with it and learn. In response, we present a framework that defines social media by using seven functional building blocks: identity, conversations, sharing, presence, relationships, reputation, and groups. As different social media activities are defined by the extent to which they focus on some or all of these blocks, we explain the implications that each block can have for how firms should engage with social media. To conclude, we present a number of recommendations regarding how firms should develop strategies for monitoring, understanding, and responding to different social media activities.},
	language = {en},
	number = {3},
	urldate = {2022-02-27},
	journal = {Business Horizons},
	author = {Kietzmann, Jan H. and Hermkens, Kristopher and McCarthy, Ian P. and Silvestre, Bruno S.},
	month = may,
	year = {2011},
	keywords = {Facebook, LinkedIn, Social media, Social networks, Twitter, User-generated content, Web 2.0, YouTube},
	pages = {241--251},
}

@article{ralph_empirical_2021,
	title = {Empirical {Standards} for {Software} {Engineering} {Research}},
	url = {http://arxiv.org/abs/2010.03525},
	abstract = {Empirical Standards are natural-language models of a scientific community's expectations for a specific kind of study (e.g. a questionnaire survey). The ACM SIGSOFT Paper and Peer Review Quality Initiative generated empirical standards for research methods commonly used in software engineering. These living documents, which should be continuously revised to reflect evolving consensus around research best practices, will improve research quality and make peer review more effective, reliable, transparent and fair.},
	urldate = {2022-02-27},
	journal = {arXiv:2010.03525 [cs]},
	author = {Ralph, Paul and Ali, Nauman bin and Baltes, Sebastian and Bianculli, Domenico and Diaz, Jessica and Dittrich, Yvonne and Ernst, Neil and Felderer, Michael and Feldt, Robert and Filieri, Antonio and de França, Breno Bernard Nicolau and Furia, Carlo Alberto and Gay, Greg and Gold, Nicolas and Graziotin, Daniel and He, Pinjia and Hoda, Rashina and Juristo, Natalia and Kitchenham, Barbara and Lenarduzzi, Valentina and Martínez, Jorge and Melegati, Jorge and Mendez, Daniel and Menzies, Tim and Molleri, Jefferson and Pfahl, Dietmar and Robbes, Romain and Russo, Daniel and Saarimäki, Nyyti and Sarro, Federica and Taibi, Davide and Siegmund, Janet and Spinellis, Diomidis and Staron, Miroslaw and Stol, Klaas and Storey, Margaret-Anne and Taibi, Davide and Tamburri, Damian and Torchiano, Marco and Treude, Christoph and Turhan, Burak and Wang, Xiaofeng and Vegas, Sira},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.03525},
	keywords = {Computer Science - General Literature, Computer Science - Software Engineering},
}

@article{johnson_mixed_2004,
	title = {Mixed {Methods} {Research}: {A} {Research} {Paradigm} {Whose} {Time} {Has} {Come}},
	volume = {33},
	issn = {0013-189X, 1935-102X},
	shorttitle = {Mixed {Methods} {Research}},
	url = {http://journals.sagepub.com/doi/10.3102/0013189X033007014},
	doi = {10.3102/0013189X033007014},
	abstract = {The purposes of this article are to position mixed methods research ( mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm “wars” and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research ( mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it.},
	language = {en},
	number = {7},
	urldate = {2022-02-27},
	journal = {Educational Researcher},
	author = {Johnson, R. Burke and Onwuegbuzie, Anthony J.},
	month = oct,
	year = {2004},
	pages = {14--26},
}

@book{margetts_political_2015,
	title = {Political {Turbulence}: {How} {Social} {Media} {Shape} {Collective} {Action}},
	isbn = {978-1-4008-7355-5},
	shorttitle = {Political {Turbulence}},
	abstract = {How social media is giving rise to a chaotic new form of politicsAs people spend increasing proportions of their daily lives using social media, such as Twitter and Facebook, they are being invited to support myriad political causes by sharing, liking, endorsing, or downloading. Chain reactions caused by these tiny acts of participation form a growing part of collective action today, from neighborhood campaigns to global political movements. Political Turbulence reveals that, in fact, most attempts at collective action online do not succeed, but some give rise to huge mobilizations—even revolutions.Drawing on large-scale data generated from the Internet and real-world events, this book shows how mobilizations that succeed are unpredictable, unstable, and often unsustainable. To better understand this unruly new force in the political world, the authors use experiments that test how social media influence citizens deciding whether or not to participate. They show how different personality types react to social influences and identify which types of people are willing to participate at an early stage in a mobilization when there are few supporters or signals of viability. The authors argue that pluralism is the model of democracy that is emerging in the social media age—not the ordered, organized vision of early pluralists, but a chaotic, turbulent form of politics.This book demonstrates how data science and experimentation with social data can provide a methodological toolkit for understanding, shaping, and perhaps even predicting the outcomes of this democratic turbulence.},
	language = {en},
	publisher = {Princeton University Press},
	author = {Margetts, Helen and John, Peter and Hale, Scott and Yasseri, Taha},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: tfuxCQAAQBAJ},
	keywords = {Political Science / General, Political Science / Political Ideologies / Democracy, Political Science / Political Process / Political Advocacy, Social Science / Media Studies},
}

@article{miller_empirical_1990,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
	language = {en},
	number = {12},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {Miller, Barton P. and Fredriksen, Louis and So, Bryan},
	month = dec,
	year = {1990},
	pages = {32--44},
}

@article{sen_cute_nodate,
	title = {{CUTE}: {A} {Concolic} {Unit} {Testing} {Engine} for {C}},
	abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more speciﬁcally, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an eﬃcient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
	language = {en},
	author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
	pages = {10},
}

@article{godefroid_dart_nodate,
	title = {{DART}: {Directed} {Automated} {Random} {Testing}},
	abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles – there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
	language = {en},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
	pages = {11},
}

@article{de_moura_satisfiability_2011,
	title = {Satisfiability modulo theories: introduction and applications},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Satisfiability modulo theories},
	url = {https://dl.acm.org/doi/10.1145/1995376.1995394},
	doi = {10.1145/1995376.1995394},
	abstract = {Checking the satisfiability of logical formulas, SMT solvers scale orders of magnitude beyond custom ad hoc solvers.},
	language = {en},
	number = {9},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {De Moura, Leonardo and Bjørner, Nikolaj},
	month = sep,
	year = {2011},
	pages = {69--77},
}

@inproceedings{visser2004test,
	title = {Test input generation with java {PathFinder}},
	booktitle = {Proceedings of the 2004 {ACM} {SIGSOFT} international symposium on {Software} testing and analysis},
	author = {Visser, Willem and Pǎsǎreanu, Corina S and Khurshid, Sarfraz},
	year = {2004},
	pages = {97--107},
}

@article{cadar2013symbolic,
	title = {Symbolic execution for software testing: three decades later},
	volume = {56},
	number = {2},
	journal = {Communications of the ACM},
	author = {Cadar, Cristian and Sen, Koushik},
	year = {2013},
	note = {Publisher: ACM New York, NY, USA},
	pages = {82--90},
}

@article{king_symbolic_1976,
	title = {Symbolic execution and program testing},
	volume = {19},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may he symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described, it interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	language = {en},
	number = {7},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {King, James C.},
	month = jul,
	year = {1976},
	pages = {385--394},
}

@inproceedings{cadar_symbolic_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Symbolic execution for software testing in practice: preliminary assessment},
	isbn = {978-1-4503-0445-0},
	shorttitle = {Symbolic execution for software testing in practice},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985995},
	doi = {10.1145/1985793.1985995},
	abstract = {We present results for the “Impact Project Focus Area” on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.},
	language = {en},
	urldate = {2022-02-25},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Cadar, Cristian and Godefroid, Patrice and Khurshid, Sarfraz and Păsăreanu, Corina S. and Sen, Koushik and Tillmann, Nikolai and Visser, Willem},
	month = may,
	year = {2011},
	pages = {1066--1071},
}

@article{misra_how_2016,
	title = {How {Socially} {Aware} {Are} {Social} {Media} {Privacy} {Controls}?},
	volume = {49},
	issn = {1558-0814},
	doi = {10.1109/MC.2016.83},
	abstract = {Social media sites are key mediators of online communication. Yet the privacy controls for these sites are not fully socially aware, even when privacy management is known to be fundamental to successful social relationships.},
	number = {3},
	journal = {Computer},
	author = {Misra, Gaurav and Such, Jose M.},
	month = mar,
	year = {2016},
	note = {Conference Name: Computer},
	keywords = {Access control, Context awareness, Facebook, Google, Media, Privacy, Social Computing, Social network services, privacy, privacy controls, security, social media, social media privacy controls},
	pages = {96--99},
}

@book{beck2000extreme,
	title = {Extreme programming explained: embrace change},
	publisher = {addison-wesley professional},
	author = {Beck, Kent},
	year = {2000},
}

@article{fagan1977inspecting,
	title = {Inspecting software design and code},
	volume = {23},
	number = {10},
	journal = {Datamation},
	author = {Fagan, Michael E},
	year = {1977},
	note = {Publisher: CAHNERS-DENVER PUBLISHING CO 8773 S RIDGELINE BLVD, HIGHLANDS RANCH, CO …},
	pages = {133},
}

@article{fagan1999design,
	title = {Design and code inspections to reduce errors in program development},
	volume = {38},
	number = {2.3},
	journal = {IBM Systems Journal},
	author = {Fagan, Michael E},
	year = {1999},
	note = {Publisher: IBM},
	pages = {258--287},
}

@article{knight_improved_1993,
	title = {An improved inspection technique},
	volume = {36},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/163359.163366},
	doi = {10.1145/163359.163366},
	language = {en},
	number = {11},
	urldate = {2022-02-25},
	journal = {Communications of the ACM},
	author = {Knight, John C. and Myers, E. Ann},
	month = nov,
	year = {1993},
	pages = {51--61},
}

@book{gilb1993software,
	title = {Software inspections},
	publisher = {Addison-Wesley Reading, Masachusetts},
	author = {Gilb, Tom and Graham, Dorothy},
	year = {1993},
}

@article{shen_rtkaller_2021,
	title = {Rtkaller: {State}-aware {Task} {Generation} for {RTOS} {Fuzzing}},
	volume = {20},
	issn = {1539-9087},
	shorttitle = {Rtkaller},
	url = {http://doi.org/10.1145/3477014},
	doi = {10.1145/3477014},
	abstract = {A real-time operating system (RTOS) is an operating system designed to meet certain real-time requirements. It is widely used in embedded applications, and its correctness is safety-critical. However, the validation of RTOS is challenging due to its complex real-time features and large code base. In this paper, we propose Rtkaller, a state-aware kernel fuzzer for the vulnerability detection in RTOS. First, Rtkaller implements an automatic task initialization to transform the syscall sequences into initial tasks with more real-time information. Then, a coverage-guided task mutation is designed to generate those tasks that explore more in-depth real-time related code for parallel execution. Moreover, Rtkaller realizes a task modification to correct those tasks that may hang during fuzzing. We evaluated it on recent versions of rt-Linux, which is one of the most widely used RTOS. Compared to the state-of-the-art kernel fuzzers Syzkaller and Moonshine, Rtkaller achieves the same code coverage at the speed of 1.7X and 1.6X, gains an increase of 26.1\% and 22.0\% branch coverage within 24 hours respectively. More importantly, Rtkaller has confirmed 28 previously unknown vulnerabilities that are missed by other fuzzers.},
	number = {5s},
	urldate = {2022-02-25},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Shen, Yuheng and Sun, Hao and Jiang, Yu and Shi, Heyuan and Yang, Yixiao and Chang, Wanli},
	month = sep,
	year = {2021},
	keywords = {Fuzz testing, RTOS, task generation, vulnerability detection},
	pages = {83:1--83:22},
}

@inproceedings{fioraldi_use_2021,
	title = {The {Use} of {Likely} {Invariants} as {Feedback} for {Fuzzers}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
	language = {en},
	urldate = {2022-02-25},
	author = {Fioraldi, Andrea and D'Elia, Daniele Cono and Balzarotti, Davide},
	year = {2021},
	pages = {2829--2846},
}

@article{chen_metamorphic_nodate,
	title = {Metamorphic {Testing}: {A} {New} {Approach} for {Generating} {Next} {Test} {Cases}},
	abstract = {In software testing, a set of test cases is constructed according to some prede ned selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an error-revealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software 5 . These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in 6 , the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the e ectiveness of existing test selection strategies. The yThis project was partially supported by a grant from the Australian Research Council and the Hong Kong Research Grant Council.},
	language = {en},
	author = {Chen, T Y and {Cheung} and {Yiu}},
	pages = {11},
}

@article{waldman_privacy_2016,
	title = {Privacy, {Sharing}, and {Trust}: {The} {Facebook} {Study}},
	volume = {67},
	issn = {0008-7262},
	shorttitle = {Privacy, {Sharing}, and {Trust}},
	url = {https://scholarlycommons.law.case.edu/caselrev/vol67/iss1/10},
	number = {1},
	journal = {Case Western Reserve Law Review},
	author = {Waldman, Ari},
	month = jan,
	year = {2016},
	pages = {193},
}

@book{wohlin_experimentation_2012,
	address = {Berlin, Heidelberg},
	title = {Experimentation in {Software} {Engineering}},
	isbn = {978-3-642-29043-5 978-3-642-29044-2},
	url = {http://link.springer.com/10.1007/978-3-642-29044-2},
	language = {en},
	urldate = {2022-02-22},
	publisher = {Springer Berlin Heidelberg},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	year = {2012},
	doi = {10.1007/978-3-642-29044-2},
}

@article{lange_effects_2006,
	title = {Effects of {Defects} in {UML} {Models} – {An} {Experimental} {Investigation}},
	abstract = {The Uniﬁed Modeling Language (UML) is the de facto standard for designing and architecting software systems. UML oﬀers a large number of diagram types that can be used with varying degree of rigour. As a result UML models may contain consistency defects. Previous research has shown that industrial UML models that are used as basis for implementation and maintenance contain large numbers of defects. This study investigates to what extent implementers detect defects and to what extent defects cause diﬀerent interpretations by diﬀerent readers. We performed two controlled experiments with a large group of students (111) and a group of industrial practitioners (48). The experiment’s results show that defects often remain undetected and cause misinterpretations. We present a classiﬁcation of defect types based on a ranking of detection rate and risk for misinterpretation. Additionally we observed eﬀects of using domain knowledge to compensate defects. The results are generalizable to industrial UML users and can be used for improving quality assurance techniques for UML-based development.},
	language = {en},
	author = {Lange, Christian F J and Lange, C F J and Chaudron, Michel R V and Chaudron, M R V},
	year = {2006},
	pages = {10},
}

@inproceedings{hayhurst2001practical,
	title = {A practical approach to modified condition/decision coverage},
	volume = {1},
	booktitle = {20th {DASC}. 20th digital avionics systems conference (cat. {No}. {01CH37219})},
	author = {Hayhurst, Kelly J and Veerhusen, Dan S},
	year = {2001},
	note = {tex.organization: IEEE},
	pages = {1B2--1},
}

@inproceedings{mockus_test_2009,
	address = {Lake Buena Vista, FL, USA},
	title = {Test coverage and post-verification defects: {A} multiple case study},
	isbn = {978-1-4244-4842-5},
	shorttitle = {Test coverage and post-verification defects},
	url = {http://ieeexplore.ieee.org/document/5315981/},
	doi = {10.1109/ESEM.2009.5315981},
	abstract = {Test coverage is a promising measure of test effectiveness and development organizations are interested in costeffective levels of coverage that provide sufﬁcient fault removal with contained testing effort. We have conducted a multiple-case study on two dissimilar industrial software projects to investigate if test coverage reﬂects test effectiveness and to ﬁnd the relationship between test effort and the level of test coverage. We ﬁnd that in both projects the increase in test coverage is associated with decrease in ﬁeld reported problems when adjusted for the number of prerelease changes. A qualitative investigation revealed several potential explanations, including code complexity, developer experience, the type of functionality, and remote development teams. All these factors were related to the level of coverage and quality, with coverage having an effect even after these adjustments. We also ﬁnd that the test effort increases exponentially with test coverage, but the reduction in ﬁeld problems increases linearly with test coverage. This suggests that for most projects the optimal levels of coverage are likely to be well short of 100\%.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {2009 3rd {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Mockus, Audris and Nagappan, Nachiappan and Dinh-Trong, Trung T.},
	month = oct,
	year = {2009},
	pages = {291--301},
}

@inproceedings{piwowarski_coverage_1993,
	address = {Baltimore, MD, USA},
	title = {Coverage measurement experience during function test},
	isbn = {978-0-8186-3700-1},
	url = {http://ieeexplore.ieee.org/document/346035/},
	doi = {10.1109/ICSE.1993.346035},
	abstract = {This paper discusses the issues of test coverage measurement in industry and justijies the benefits of the measurement using a framework developed by the authors. Experience with the measurement is formalized and packaged so that other researchers in industry can share and reuse it. In the paper, function test of large-scale system software is defined and analyzed. Based on the discussions of function test, a framework for analyzing the function test error removal process is developed. An experience-based error removal model and a cost model are proven to be useful tools for justifving tesf coverage measurement during function test. Data obtained from a real project is analyzed using theframework for validation.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {International {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE Comput. Soc. Press},
	author = {Piwowarski, P. and Ohba, M. and Caruso, J.},
	year = {1993},
	pages = {287--301},
}

@inproceedings{ivankovic_code_2019,
	address = {Tallinn Estonia},
	title = {Code coverage at {Google}},
	isbn = {978-1-4503-5572-8},
	url = {https://dl.acm.org/doi/10.1145/3338906.3340459},
	doi = {10.1145/3338906.3340459},
	abstract = {Code coverage is a measure of the degree to which a test suite exercises a software system. Although coverage is well established in software engineering research, deployment in industry is often inhibited by the perceived usefulness and the computational costs of analyzing coverage at scale. At Google, coverage information is computed for one billion lines of code daily, for seven programming languages. A key aspect of making coverage information actionable is to apply it at the level of changesets and code review.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ivanković, Marko and Petrović, Goran and Just, René and Fraser, Gordon},
	month = aug,
	year = {2019},
	pages = {955--963},
}

@article{rasmussen_skills_1983,
	title = {Skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models},
	volume = {SMC-13},
	issn = {0018-9472, 2168-2909},
	url = {http://ieeexplore.ieee.org/document/6313160/},
	doi = {10.1109/TSMC.1983.6313160},
	abstract = {The introduction of information technology based on digital computers for the design of man-machine interface systems has led to a requirement for consistent models of human performance in routine task environments and during unfamiliar task conditions. A discussion is pre­ sented of the requirement for different types of models for representing performance at the skill-, rule-, and knowledge-based levels, together with a review of the different ways in which information is perceived at these different levels in terms of signals, signs, and symbols. Particular attention is paid to the different possible ways of representing system properties which underlie knowledge-based performance and which can be char­ acterized at several levels of abstraction—from the representation of physical form, through functional representation, to representation in terms of intention or purpose. Furthermore, the role of qualitative and quantita­ tive models in the design and evaluation of interface systems is mentioned, and the need to consider such distinctions carefully is discussed.},
	language = {en},
	number = {3},
	urldate = {2022-02-16},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Rasmussen, Jens},
	month = may,
	year = {1983},
	pages = {257--266},
}

@article{humphrey_why_2005,
	title = {Why {Big} {Software} {Projects} {Fail}: {The} 12 {Key} {Questions}},
	language = {en},
	author = {Humphrey, Watts S},
	year = {2005},
	pages = {5},
}

@inproceedings{leesatapornwongsa_taxdc_2016,
	address = {Atlanta Georgia USA},
	title = {{TaxDC}: {A} {Taxonomy} of {Non}-{Deterministic} {Concurrency} {Bugs} in {Datacenter} {Distributed} {Systems}},
	isbn = {978-1-4503-4091-5},
	shorttitle = {{TaxDC}},
	url = {https://dl.acm.org/doi/10.1145/2872362.2872374},
	doi = {10.1145/2872362.2872374},
	abstract = {We present TaxDC, the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in distributed systems. We study 104 distributed concurrency (DC) bugs from four widely-deployed cloud-scale datacenter distributed systems, Cassandra, Hadoop MapReduce, HBase and ZooKeeper. We study DC-bug characteristics along several axes of analysis such as the triggering timing condition and input preconditions, error and failure symptoms, and ﬁx strategies, collectively stored as 2,083 classiﬁcation labels in TaxDC database. We discuss how our study can open up many new research directions in combating DC bugs.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Leesatapornwongsa, Tanakorn and Lukman, Jeffrey F. and Lu, Shan and Gunawi, Haryadi S.},
	month = mar,
	year = {2016},
	pages = {517--530},
}

@book{mizuno_management_2020,
	title = {Management for {Quality} {Improvement}: {The} 7 {New} {QC} {Tools}},
	isbn = {978-1-00-016208-0},
	shorttitle = {Management for {Quality} {Improvement}},
	abstract = {With continuous improvement (kaizen) and Total Quality Control (TQC) becoming increasingly important to world class companies, there's an urgent need to build quality into every management decision. The tools presented in this book allow you to do just that. They represent the most important advance in quality deployment and project management in recent years.Unlike the seven traditional QC tools, which measure quality problems that already exist and are used by quality circles, these seven new QC tools make it possible for managers to plan wide-ranging and detailed TQC objectives throughout the entire organization. These tools, some borrowed from other disciplines and others developed specifically for quality management, include the relations diagram, the KJ method (affinity diagram), the systematic diagram, the matrix diagram, matrix data analysis, the process decision program chart (PDPC), and the arrow diagram. Together they will help you to: Expand the scope of quality efforts company-wide. Set up and manage the systems necessary to resolve major quality problems. Anticipate potential quality problems and actually eliminate defects before they happen.Never before available in English, Management for Quality Improvement is absolutely essential reading if you are in any area of project management, quality assurance, MIS, or TQC.},
	language = {en},
	publisher = {CRC Press},
	author = {Mizuno, Sigeru},
	month = aug,
	year = {2020},
	note = {Google-Books-ID: aQ\_2DwAAQBAJ},
	keywords = {Business \& Economics / Management, Business \& Economics / Quality Control},
}

@misc{look_what_2021,
	title = {What is {Trust} \& {Safety}?},
	url = {https://sylvialook.medium.com/intro-to-trust-safety-in-tech-part-1-c142521f1806},
	abstract = {In recent years, there has been a spike in job openings for Trust \& Safety (T\&S) roles. Tech companies like TikTok, Airbnb, Twitter…},
	language = {en},
	urldate = {2022-02-02},
	journal = {Medium},
	author = {Look, Sylvia},
	month = oct,
	year = {2021},
}

@article{nakajo_case_1991,
	title = {A case history analysis of software error cause-effect relationships},
	volume = {17},
	issn = {1939-3520},
	doi = {10.1109/32.83917},
	abstract = {Approximately 700 errors in four commercial measuring-control software products were analyzed, and the cause-effect relationships of errors occurring during software development were identified. The analysis method used defined appropriate observation points along the path leading from cause to effect of a software error and gathered the corresponding data by analyzing each error using fault tree analysis. Each observation point's data were categorized, and the relationships between two adjoining points were summarized using a cross-indexing table. Four major error-occurrence mechanisms were identified; two are related to hardware and software interface specification misunderstandings, while the other two are related to system and module function misunderstandings. The effects of structured analysis and structured design methods on software errors were evaluated.{\textless}{\textgreater}},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Nakajo, T. and Kume, H.},
	month = aug,
	year = {1991},
	keywords = {Cause effect analysis, Computer aided software engineering, Computer errors, Data analysis, Error analysis, History, Humans, Marine vehicles, Programming, US Department of Transportation},
	pages = {830--838},
}

@inproceedings{thung_empirical_2012,
	title = {An {Empirical} {Study} of {Bugs} in {Machine} {Learning} {Systems}},
	doi = {10.1109/ISSRE.2012.22},
	abstract = {Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6\% of the bugs belong to the algorithm/method category, 15.6\% of the bugs belong to the non-functional category, and 13\% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.},
	booktitle = {2012 {IEEE} 23rd {International} {Symposium} on {Software} {Reliability} {Engineering}},
	author = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	month = nov,
	year = {2012},
	note = {ISSN: 2332-6549},
	keywords = {Software reliability},
	pages = {271--280},
}

@misc{wikimedia_foundation_trust_2022,
	title = {Trust and {Safety}/{Case} {Review} {Committee} - {Meta}},
	url = {https://meta.wikimedia.org/wiki/Trust_and_Safety/Case_Review_Committee},
	language = {en},
	urldate = {2022-02-07},
	journal = {Wikimedia},
	author = {{Wikimedia Foundation}},
	month = feb,
	year = {2022},
}

@misc{twitter_about_nodate,
	title = {About {Twitter} {\textbar} {Trust} and {Safety} {Council}},
	url = {https://about.twitter.com/en/our-priorities/healthy-conversations/trust-and-safety-council.html},
	abstract = {Twitter's policy and enforcement actions are informed by public interest organizations that protect individuals and their civil liberties.},
	language = {en},
	urldate = {2022-02-07},
	author = {{Twitter}},
}

@misc{macgillivray_databite_2020,
	title = {Databite {No}. 134: {Origins} of {Trust} and {Safety} with {Alexander} {Macgillivray} and {Nicole} {Wong}},
	shorttitle = {Origins of {Trust} and {Safety}},
	url = {https://datasociety.net/library/origins-of-trust-and-safety/},
	abstract = {Alexander Macgillivray and Nicole Wong discuss the role of Trust and Safety, and make suggestions as regulation, policy, and public awareness evolve.},
	language = {en-US},
	urldate = {2022-02-03},
	journal = {Data \& Society},
	author = {Macgillivray, Alexander and Wong, Nicole},
	month = jul,
	year = {2020},
}

@inproceedings{wang_comprehensive_2017,
	address = {Urbana-Champaign, IL, USA},
	series = {{ASE} 2017},
	title = {A comprehensive study on real world concurrency bugs in {Node}.js},
	isbn = {978-1-5386-2684-9},
	abstract = {Node.js becomes increasingly popular in building server-side JavaScript applications. It adopts an event-driven model, which supports asynchronous I/O and non-deterministic event processing. This asynchrony and non-determinism can introduce intricate concurrency bugs, and leads to unpredictable behaviors. An in-depth understanding of real world concurrency bugs in Node.js applications will significantly promote effective techniques in bug detection, testing and fixing for Node.js. In this paper, we present NodeCB, a comprehensive study on real world concurrency bugs in Node.js applications. Specifically, we have carefully studied 57 real bug cases from open-source Node.js applications, and have analyzed their bug characteristics, e.g., bug patterns and root causes, bug impacts, bug manifestation, and fix strategies. Through this study, we obtain several interesting findings, which may open up many new research directions in combating concurrency bugs in Node.js. For example, one finding is that two thirds of the bugs are caused by atomicity violation. However, due to lack of locks and transaction mechanism, Node.js cannot easily express and guarantee the atomic intention.},
	urldate = {2022-01-11},
	booktitle = {Proceedings of the 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Wang, Jie and Dou, Wensheng and Gao, Yu and Gao, Chushu and Qin, Feng and Yin, Kang and Wei, Jun},
	month = oct,
	year = {2017},
	keywords = {JavaScript, Node.js, concurrency bug, empirical study, event-driven},
	pages = {520--531},
}

@article{amedie_impact_2015,
	title = {The {Impact} of {Social} {Media} on {Society}},
	url = {https://scholarcommons.scu.edu/engl_176/2},
	journal = {Pop Culture Intersections},
	author = {Amedie, Jacob},
	month = sep,
	year = {2015},
}

@misc{noauthor_fig_nodate,
	title = {Fig. 2. {Reference} {Architecture} of an {Online} {Social} {Network} {Platform}},
	url = {https://www.researchgate.net/figure/Reference-Architecture-of-an-Online-Social-Network-Platform_fig1_227226547},
	abstract = {Download scientific diagram {\textbar} Reference Architecture of an Online Social Network Platform   from publication: Online Social Networks: Status and Trends {\textbar} The rapid proliferation of Online Social Network (OSN) sites has made a profound impact on the WWW, which tends to reshape its structure, design, and utility. Industry experts believe that OSNs create a potentially transformational change in consumer behavior and will bring a... {\textbar} Online Social Networks, Consumer Satisfaction and Consumer Behavior {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2022-02-16},
	journal = {ResearchGate},
}

@article{kruchten_41_1995,
	title = {The 4+1 {View} {Model} of architecture},
	volume = {12},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/469759/},
	doi = {10.1109/52.469759},
	language = {en},
	number = {6},
	urldate = {2021-08-20},
	journal = {IEEE Software},
	author = {Kruchten, P.B.},
	month = nov,
	year = {1995},
	pages = {42--50},
}

@article{clinch_learning_2018,
	title = {Learning from {Our} {Mistakes}: {Identifying} {Opportunities} for {Technology} {Intervention} against {Everyday} {Cognitive} {Failure}},
	volume = {17},
	issn = {1536-1268},
	shorttitle = {Learning from {Our} {Mistakes}},
	url = {https://ieeexplore.ieee.org/document/8383664/},
	doi = {10.1109/MPRV.2018.022511240},
	language = {en},
	number = {2},
	urldate = {2022-02-16},
	journal = {IEEE Pervasive Computing},
	author = {Clinch, Sarah and Mascolo, Cecilia},
	month = apr,
	year = {2018},
	pages = {22--33},
}

@inproceedings{hara_data-driven_2018,
	address = {Montreal QC Canada},
	title = {A {Data}-{Driven} {Analysis} of {Workers}' {Earnings} on {Amazon} {Mechanical} {Turk}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174023},
	doi = {10.1145/3173574.3174023},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hara, Kotaro and Adams, Abigail and Milland, Kristy and Savage, Saiph and Callison-Burch, Chris and Bigham, Jeffrey P.},
	month = apr,
	year = {2018},
	pages = {1--14},
}

@incollection{lee_case_2014,
	address = {Heidelberg},
	title = {A {Case} {Study} in {Defect} {Measurement} and {Root} {Cause} {Analysis} in a {Turkish} {Software} {Organization}},
	volume = {496},
	isbn = {978-3-319-00947-6 978-3-319-00948-3},
	url = {http://link.springer.com/10.1007/978-3-319-00948-3_4},
	abstract = {In software projects, ﬁnal products aim to meet customer needs and concurrently to have the least number of defects. Defect identiﬁcation and removal processes offer valuable insights regarding all stages of software development. Therefore, defects are recorded during the software development process with the intentions of not only ﬁxing them before the product is delivered to the customer, but also accumulating data that can be researched upon. That data can later be used for software process improvement. One of the techniques for analyzing defects is the root cause analysis (RCA). A case study is conducted in one of the leading, medium sized software companies of Turkey by utilizing the RCA method. The collected defect data has been analyzed with Pareto charts and the root causes for outstanding defect categories have been identiﬁed with the use of ﬁshbone diagrams and expert grading, demonstrating that these techniques can be effectively used in RCA. The main root causes of the investigated defect items have been identiﬁed as lack of knowledge and extenuation of the undertaken task, and corrective actions have been proposed to upper management. The case study is formulated in a way to provide a basis for software development organizations that aim to conduct defect analysis and obtain meaningful results. All stages of the research and the case study are explained in detail and the efforts spent are given.},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {Software {Engineering} {Research}, {Management} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Atagoren, Cagla and Chouseinoglou, Oumout},
	editor = {Lee, Roger},
	year = {2014},
	doi = {10.1007/978-3-319-00948-3_4},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {55--72},
}

@inproceedings{lehtinen_what_2011,
	address = {Banff, AB, Canada},
	title = {What are {Problem} {Causes} of {Software} {Projects}? {Data} of {Root} {Cause} {Analysis} at {Four} {Software} {Companies}},
	isbn = {978-1-4577-2203-5 978-0-7695-4604-9},
	shorttitle = {What are {Problem} {Causes} of {Software} {Projects}?},
	url = {http://ieeexplore.ieee.org/document/6092595/},
	doi = {10.1109/ESEM.2011.55},
	abstract = {Root cause analysis (RCA) is a structured investigation of a problem to detect the causes that need to be prevented. We applied ARCA, an RCA method, to target problems of four medium-sized software companies and collected 648 causes of software engineering problems. Thereafter, we applied grounded theory to the causes to study their types and related process areas. We detected 14 types of causes in 6 process areas. Our results indicate that development work and software testing are the most common process areas, whereas lack of instructions and experiences, insufficient work practices, low quality task output, task difficulty, and challenging existing product are the most common types of the causes. As the types of causes are evenly distributed between the cases, we hypothesize that the distributions could be generalizable. Finally, we found that only 2.5\% of the causes are related to software development tools that are widely investigated in software engineering research.},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {2011 {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Lehtinen, Timo O.A. and Mantyla, Mika V.},
	month = sep,
	year = {2011},
	pages = {388--391},
}

@techreport{obar_social_2015,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Social {Media} {Definition} and the {Governance} {Challenge} - {An} {Introduction} to the {Special} {Issue}},
	url = {https://papers.ssrn.com/abstract=2663153},
	abstract = {This introduction to a special issue of "Telecommunications Policy" entitled "The Governance of Social Media" begins with a definition of social media that informs all contributions in the special issue. A section describing the challenges associated with the governance of social media is presented next, followed by an overview of the various articles included in the special issue.},
	language = {en},
	number = {ID 2663153},
	urldate = {2022-02-15},
	institution = {Social Science Research Network},
	author = {Obar, Jonathan A. and Wildman, Steven S.},
	month = jul,
	year = {2015},
	doi = {10.2139/ssrn.2663153},
	keywords = {communication policy, information policy, internet governance, privacy, social media, social network, surveillance, telecommunication policy},
}

@inproceedings{pereira_social_2010,
	title = {Social software building blocks: {Revisiting} the honeycomb framework},
	shorttitle = {Social software building blocks},
	doi = {10.1109/i-Society16502.2010.6018707},
	abstract = {The possibility of developing more interactive and innovative applications allowed users interact with each other and have a primary role as producers of content — these systems are called social software. This paper examines the definition of the concept of social software with its design process and structure. We introduce the social software honeycomb, a framework built to help in understanding social software. Based on an analysis of an inclusive social network we revisit this framework discussing its elements, suggesting its expansion and how it can be used in the design and study of social software. We argue that the framework must be extended and theoretically grounded in order to address several points imposed by the “social”.},
	booktitle = {2010 {International} {Conference} on {Information} {Society}},
	author = {Pereira, Roberto and Baranauskas, M. Cecilia C. and da Silva, Sergio Roberto P.},
	month = jun,
	year = {2010},
	keywords = {Collaboration, Context, Encyclopedias, Internet, Social network services, Software, Videos},
	pages = {253--258},
}

@article{lehtinen_perceived_nodate,
	title = {Perceived {Feasibility} of {Using} {Root} {Cause} {Analysis} in {Post} {Project} {Reviews}: an {Empirical} {Investigation}},
	abstract = {Root cause analysis (RCA) is a structured investigation of the problem to identify which underlying causes need to be fixed. In software engineering (SE), the scientific work on RCA is rather scarce. Feasibility of RCA to identify SE problem causes is not widely studied, the RCA methods are not compared with one another, and there are only a few studies on the required effort to conduct RCA. Additionally, studying how the participating people experience RCA is totally set aside.},
	language = {en},
	author = {Lehtinen, Timo O A},
	pages = {8},
}

@article{lehtinen_perceived_2014,
	title = {Perceived causes of software project failures – {An} analysis of their relationships},
	volume = {56},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584914000263},
	doi = {10.1016/j.infsof.2014.01.015},
	abstract = {Objective: The aim of this study is to conduct in-depth analysis of software project failures in four software product companies in order to understand the causes of failures and their relationships. For each failure, we want to understand which causes, so called bridge causes, interconnect different process areas, and which causes were perceived as the most promising targets for process improvement.
Method: The causes of failures were detected by conducting root cause analysis. For each cause, we classified its type, process area, and interconnectedness to other causes. We quantitatively analyzed which type, process area, and interconnectedness categories (bridge, local) were common among the causes selected as the most feasible targets for process improvement activities. Finally, we qualitatively analyzed the bridge causes in order to find common denominators for the causal relationships interconnecting the process areas.
Results: For each failure, our method identified causal relationships diagrams including 130 to 185 causes each. All four cases were unique, albeit some similarities occurred. On average, 50\% of the causes were bridge causes. Lack of cooperation, weak task backlog, and lack of software testing resources were common bridge causes. Bridge causes, and causes related to tasks, people, and methods were common among the causes perceived as the most feasible targets for process improvement. The causes related to the project environment were frequent, but seldom perceived as feasible targets for process improvement.
Conclusion: Prevention of a software project failure requires a case-specific analysis and controlling causes outside the process area where the failure surfaces. This calls for collaboration between the individuals and managers responsible for different process areas.},
	language = {en},
	number = {6},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Lehtinen, Timo O.A. and Mäntylä, Mika V. and Vanhanen, Jari and Itkonen, Juha and Lassenius, Casper},
	month = jun,
	year = {2014},
	pages = {623--643},
}

@article{politowski_learning_2018,
	title = {Learning from the past: {A} process recommendation system for video game projects using postmortems experiences},
	volume = {100},
	issn = {09505849},
	shorttitle = {Learning from the past},
	url = {http://arxiv.org/abs/2009.02445},
	doi = {10.1016/j.infsof.2018.04.003},
	abstract = {Context: The video game industry is a billion dollar industry that faces problems in the way games are developed. One method to address these problems is using developer aid tools, such as Recommendation Systems. These tools assist developers by generating recommendations to help them perform their tasks.},
	language = {en},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Politowski, Cristiano and Fontoura, Lisandra M. and Petrillo, Fabio and Guéhéneuc, Yann-Gaël},
	month = aug,
	year = {2018},
	note = {arXiv: 2009.02445},
	keywords = {Computer Science - Software Engineering},
	pages = {103--118},
}

@article{lehtinen_recurring_2017,
	title = {Recurring opinions or productive improvements—what agile teams actually discuss in retrospectives},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-016-9464-2},
	doi = {10.1007/s10664-016-9464-2},
	abstract = {Team-level retrospectives are widely used in agile and lean software development, yet little is known about what is actually discussed during retrospectives or their outcomes. In this paper, we synthesise the outcomes of sprint retrospectives in a large, distributed, agile software development organisation. This longitudinal case study analyses data from 37 teamlevel retrospectives for almost 3 years. We report the outcomes of the retrospectives, their perceived importance for process improvement and relatVed action proposals. Most discussions were related to topics close to and controllable by the team. However, the discussions might suffer from participant bias, and in cases where they are not supported by hard evidence, they might not reflect reality, but rather the sometimes strong opinions of the participants. Some discussions were related to topics that could not be resolved at the team level due to their complexity. Certain topics recurred over a long period of time, either reflecting issues that can and have been solved previously, but that recur naturally as development proceeds, or reflecting waste since they cannot be resolved or improved on by the team due to a lack of controllability or their complexity. For example, the discussion on estimation accuracy did not reflect the true situation and improving the estimates was complicated. On the other hand, discussions on the high number of known bugs recurred despite effective improvements as development proceeded.},
	language = {en},
	number = {5},
	urldate = {2022-02-15},
	journal = {Empirical Software Engineering},
	author = {Lehtinen, Timo O. A. and Itkonen, Juha and Lassenius, Casper},
	month = oct,
	year = {2017},
	pages = {2409--2452},
}

@article{lehtinen_tool_2014,
	title = {A tool supporting root cause analysis for synchronous retrospectives in distributed software teams},
	volume = {56},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584914000159},
	doi = {10.1016/j.infsof.2014.01.004},
	abstract = {Objective: This paper presents a real-time cloud-based software tool (ARCA-tool) we developed to support RCA in distributed teams and its initial empirical evaluation. The feasibility of using RCA with distributed teams is also evaluated.
Method: We compared our tool with 35 existing RCA software tools. We conducted field studies of four distributed agile software teams at two international software product companies. The teams conducted RCA collaboratively in synchronous retrospective meetings by using the tool we developed. We collected the data using observations, interviews and questionnaires.
Results: Comparison revealed that none of the existing 35 tools matched all the features of our ARCA-tool. The team members found ARCA-tool to be an essential part of their distributed retrospectives. They considered the software as efficient and very easy to learn and use. Additionally, the team members perceived RCA to be a vital part of the retrospectives. In contrast to the prior retrospective practices of the teams, the introduced RCA method was evaluated as efficient and easy to use.
Conclusion: RCA is a useful practice in synchronous distributed retrospectives. However, it requires software tool support for enabling real-time view and co-creation of a cause-effect diagram. ARCA-tool supports synchronous RCA, and includes support for logging problems and causes, problem prioritization, cause-effect diagramming, and logging of process improvement proposals. It enables conducting RCA in distributed retrospectives.},
	language = {en},
	number = {4},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Lehtinen, Timo O.A. and Virtanen, Risto and Viljanen, Juha O. and Mäntylä, Mika V. and Lassenius, Casper},
	month = apr,
	year = {2014},
	pages = {408--437},
}

@article{lehtinen_diagrams_2015,
	title = {Diagrams or structural lists in software project retrospectives – {An} experimental comparison},
	volume = {103},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121215000151},
	doi = {10.1016/j.jss.2015.01.020},
	abstract = {Root cause analysis (RCA) is a recommended practice in retrospectives and cause–effect diagram (CED) is a commonly recommended technique for RCA. Our objective is to evaluate whether CED improves the outcome and perceived utility of RCA. We conducted a controlled experiment with 11 student software project teams by using a single factor paired design resulting in a total of 22 experimental units. Two visualization techniques of underlying causes were compared: CED and a structural list of causes. We used the output of RCA, questionnaires, and group interviews to compare the two techniques. In our results, CED increased the total number of detected causes. CED also increased the links between causes, thus, suggesting more structured analysis of problems. Furthermore, the participants perceived that CED improved organizing and outlining the detected causes. The implication of our results is that using CED in the RCA of retrospectives is recommended, yet, not mandatory as the groups also performed well with the structural list. In addition to increased number of detected causes, CED is visually more attractive and preferred by retrospective participants, even though it is somewhat harder to read and requires speciﬁc software tools.},
	language = {en},
	urldate = {2022-02-15},
	journal = {Journal of Systems and Software},
	author = {Lehtinen, Timo O.A. and Mäntylä, Mika V. and Itkonen, Juha and Vanhanen, Jari},
	month = may,
	year = {2015},
	pages = {17--35},
}

@article{charette_its_2017,
	title = {{IT}'s {Fatal} {Amnesia}},
	volume = {50},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/7842842/},
	doi = {10.1109/MC.2017.32},
	language = {en},
	number = {2},
	urldate = {2022-02-15},
	journal = {Computer},
	author = {Charette, Robert N.},
	month = feb,
	year = {2017},
	pages = {86--91},
}

@article{cerpa_why_2009,
	title = {Why did your project fail?},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1610252.1610286},
	doi = {10.1145/1610252.1610286},
	abstract = {Introduction
            
            
              We have been developing software since the 1960s but still have not learned enough to ensure that our software development projects are successful. Boehm
              2
              suggested that realistic schedule and budgets together with a continuing steam of requirements changes are high risk factors. The Standish Group in 1994 noted that approximately 31\% of corporate software development projects were cancelled before completion and 53\% were challenged and cost 180\% above their original estimate.
              13
              Glass discussed 16 project disasters.
              5
              He found that the failed projects he reviewed were mostly huge and that the failure factors were not just management factors but also included technical factors. Linberg in 1999 found that 20\% of software projects failed, and that 46\% experienced cost and schedule overruns or significantly reduced functionality.
              8
              Later, Glass revisited failed projects and found that poor estimation was high on his list of failure factors.
              6
            
            
              In 2007 the Standish Group reported that 35\% of software projects started in 2006 were successful compared with only 16\% in the corresponding 1994 report; however, the 2007 CHAOS report still identifies 46\% (53\% in 1994) of software projects as challenged (having cost or time overruns or not fully meeting user's requirements) and 19\% (31\% in 1994) as outright failures.
              12
              The validity of the Standish Group findings has been questioned as not consistent with cost overrun results of other surveys.
              7
              Jørgensen and Moløkken-Østvold suggested that there are serious problems with the way the Standish Group conducted their research and that the findings were biased toward reports of failure because a random sample of top IT executives was asked to share failure stories when mailed confidential surveys.
            
            
              Recently Charette
              4
              commented that "billions of dollars are wasted each year on failed software projects" and that "we have a dismal history of projects that have gone awry."
              4
              Charette suggests that from 5\%-15\% of projects will be abandoned before or shortly after delivery as hopelessly inadequate.
              4
              Other recent studies, suggest various failure rates for software development projects up to 85\%.
              7
              Stories of software failure capture public attention and in general there is a perception that software quality is not improving but getting worse.
            
            
              Developing software systems is an expensive, and often a difficult process. Although there are many guidelines for successful software development,
              9,11
              few project post-mortems are conducted, and little understanding is gained from the results of past projects. The project manager (PM) and the development team must deal with many pressures from project stakeholders (for example, upper level management, marketing, accounting, customers, and users) during the software development process. These pressures impact both the cost and the quality of the software produced. There are generally more than one or two reasons for a software project to fail, and it usually is a combination of technical, project management and business decisions. Many software project failure factors have been described in the literature.
              1-13
              However, most organizations do not see preventing failure as an urgent matter. It is not understood why this attitude persists.
              4
            
            
              Because most of the literature is based on a handful of failed project case studies, in our research we analyze 70 failed software projects to determine those practices that affected project outcome. We are interested in providing, from the perspective of software practitioners, quantitative evidence targeting those aspects of the development process that contribute to project failure. Essentially, we are interested in updating the results of prior studies and testing the validity of previously reported anecdotal evidence. To date, no one has taken a set of project data and teased it out to identify, for a whole group of such projects, the most common failure factors. We are interested in everyday projects that are not high profile enough to be reported in the literature. Our work builds on that previously reported by Boehm,
              2
              Charette,
              4
              Glass,
              5,6
              Jørgensen and Moløkken,
              7
              Linberg,
              8
              and the Standish Group,
              12,13
              among others.},
	language = {en},
	number = {12},
	urldate = {2022-02-15},
	journal = {Communications of the ACM},
	author = {Cerpa, Narciso and Verner, June M.},
	month = dec,
	year = {2009},
	pages = {130--134},
}

@article{bjornson_improving_2009,
	title = {Improving the effectiveness of root cause analysis in post mortem analysis: {A} controlled experiment},
	volume = {51},
	issn = {09505849},
	shorttitle = {Improving the effectiveness of root cause analysis in post mortem analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095058490800030X},
	doi = {10.1016/j.infsof.2008.02.003},
	abstract = {Retrospective analysis is a way to share knowledge following the completion of a project or major milestone. However, in the busy workday of a software project, there is rarely time for such reviews and there is a need for eﬀective methods that will yield good results quickly without the need for external consultants or experts. Building on an existing method for retrospective analysis and theories of group involvement, we propose improvements to the root cause analysis phase of a lightweight retrospective analysis method known as post mortem analysis (PMA). In particular, to facilitate brainstorming during the root cause analysis phase of the PMA, we propose certain processual changes to facilitate more active individual participation and the use of less rigidly structured diagrams. We conducted a controlled experiment to compare this new variation of the method with the existing one, and conclude that in our setting of small software teams with no access to an experienced facilitator, the new variation is more eﬀective when it comes to identifying possible root causes of problems and successes. The modiﬁed method also produced more speciﬁc starting points for improving the software development process.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Bjørnson, Finn Olav and Wang, Alf Inge and Arisholm, Erik},
	month = jan,
	year = {2009},
	pages = {150--161},
}

@article{bjornson_knowledge_2008,
	title = {Knowledge management in software engineering: {A} systematic review of studied concepts, findings and research methods used},
	volume = {50},
	issn = {09505849},
	shorttitle = {Knowledge management in software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584908000487},
	doi = {10.1016/j.infsof.2008.03.006},
	abstract = {Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies.},
	language = {en},
	number = {11},
	urldate = {2022-02-15},
	journal = {Information and Software Technology},
	author = {Bjørnson, Finn Olav and Dingsøyr, Torgeir},
	month = oct,
	year = {2008},
	pages = {1055--1068},
}

@article{collier1996defined,
	title = {A defined process for project post mortem review},
	volume = {13},
	number = {4},
	journal = {IEEE software},
	author = {Collier, Bonnie and DeMarco, Tom and Fearey, Peter},
	year = {1996},
	note = {Publisher: IEEE},
	pages = {65--72},
}

@article{raelin2001public,
	title = {Public reflection as the basis of learning},
	volume = {32},
	number = {1},
	journal = {Management learning},
	author = {Raelin, Joseph A},
	year = {2001},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {11--30},
}

@inproceedings{basili1993experience,
	title = {The experience factory and its relationship to other improvement paradigms},
	booktitle = {European software engineering conference},
	author = {Basili, Victor R},
	year = {1993},
	note = {tex.organization: Springer},
	pages = {68--83},
}

@article{caldiera1994experience,
	title = {The experience factory},
	volume = {1},
	journal = {Encyclopedia of Software Eng.: Vol},
	author = {Basili, Victor and Caldiera, Gianluigi and Rombach, H Dieter},
	year = {1994},
	pages = {469--476},
}

@incollection{goos_augmenting_2001,
	address = {Berlin, Heidelberg},
	title = {Augmenting {Experience} {Reports} with {Lightweight} {Postmortem} {Reviews}},
	volume = {2188},
	isbn = {978-3-540-42571-7 978-3-540-44813-6},
	url = {http://link.springer.com/10.1007/3-540-44813-6_17},
	abstract = {Many small and medium-sized companies that develop software experience the same problems repeatedly, and have few systems in place to learn from their own mistakes as well as their own successes. Here, we propose a lightweight method to collect experience from completed software projects, and compare the results of this method to more widely applied experience reports. We find that the new method captures more information about core processes related to software development in contrast to experience reports that focus more on management processes.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Product {Focused} {Software} {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dingsøyr, Torgeir and Moe, Nils Brede and Nytrø, Ø;ystein},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Bomarius, Frank and Komi-Sirviö, Seija},
	year = {2001},
	doi = {10.1007/3-540-44813-6_17},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {167--181},
}

@article{petrillo_what_2009,
	title = {What went wrong? {A} survey of problems in game development},
	volume = {7},
	issn = {1544-3574, 1544-3574},
	shorttitle = {What went wrong?},
	url = {https://dl.acm.org/doi/10.1145/1486508.1486521},
	doi = {10.1145/1486508.1486521},
	abstract = {Despite its growth and profitability, many reports about game projects show that their production is not a simple task, but one beset by common problems and still distant from having a healthy and synergetic work process. The goal of this article is to survey the problems in the development process of electronic games, which are mainly collected from game postmortems, by exploring their similarities and differences to well-known problems in traditional information systems.},
	language = {en},
	number = {1},
	urldate = {2022-02-15},
	journal = {Computers in Entertainment},
	author = {Petrillo, Fábio and Pimenta, Marcelo and Trindade, Francisco and Dietrich, Carlos},
	month = feb,
	year = {2009},
	pages = {1--22},
}

@article{birk_postmortem_2002,
	title = {Postmortem: never leave a project without it},
	volume = {19},
	issn = {0740-7459},
	shorttitle = {Postmortem},
	url = {http://ieeexplore.ieee.org/document/1003452/},
	doi = {10.1109/MS.2002.1003452},
	language = {en},
	number = {3},
	urldate = {2022-02-15},
	journal = {IEEE Software},
	author = {Birk, A. and Dingsoyr, T. and Stalhane, T.},
	month = may,
	year = {2002},
	pages = {43--45},
}

@incollection{abrahamsson_organizational_2007,
	address = {Berlin, Heidelberg},
	title = {Organizational {Learning} {Through} {Project} {Postmortem} {Reviews} – {An} {Explorative} {Case} {Study}},
	volume = {4764},
	isbn = {978-3-540-74765-9 978-3-540-75381-0},
	url = {http://link.springer.com/10.1007/978-3-540-75381-0_13},
	abstract = {A central issue in knowledge management and software process improvement is to learn from experience. In software engineering, most experience is gathered in projects, which makes project experience a prime source for learning. Many companies conduct postmortem reviews, but we have found few companies that analyze the outcome of several reviews to facilitate learning on an organizational level. This paper reports an explorative study of what we can learn from analyzing postmortem review reports of twelve projects in a medium-size software company.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Software {Process} {Improvement}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dingsøyr, Torgeir and Moe, Nils Brede and Schalken, Joost and Stålhane, Tor},
	editor = {Abrahamsson, Pekka and Baddoo, Nathan and Margaria, Tiziana and Messnarz, Richard},
	year = {2007},
	doi = {10.1007/978-3-540-75381-0_13},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {136--147},
}

@article{desouza_experiences_2005,
	title = {Experiences with conducting project postmortems: reports versus stories},
	volume = {10},
	issn = {1077-4866, 1099-1670},
	shorttitle = {Experiences with conducting project postmortems},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spip.224},
	doi = {10.1002/spip.224},
	language = {en},
	number = {2},
	urldate = {2022-02-15},
	journal = {Software Process: Improvement and Practice},
	author = {Desouza, Kevin C. and Dingsøyr, Torgeir and Awazu, Yukika},
	month = apr,
	year = {2005},
	pages = {203--215},
}

@incollection{misra_technical_2019,
	address = {Cham},
	title = {Technical and {Managerial} {Difficulties} in {Postmortem} {Analysis} in {Software} {Projects}},
	volume = {11623},
	isbn = {978-3-030-24307-4 978-3-030-24308-1},
	url = {http://link.springer.com/10.1007/978-3-030-24308-1_5},
	abstract = {Software is successfully applied in a wide variety of areas. However, software projects have suﬀered from poor reputation by repeatedly bursting deadlines, costs or failing to fully meet user requirements. Postmortem Analysis is an activity to analyze what happened in projects in search of understanding the failures occurred and the achieved successes. Despite bringing interesting data for improving future projects, Postmortem Analysis is often neglected in organizations. This article seeks to identify and analyze the technical and managerial diﬃculties that exist in its accomplishment through bibliographical research. As a result, it is possible to conclude that the main diﬃculties for realizing postmortem activities are the shortage of time, lack of management support, conﬂicts between stakeholders, diﬃculty in extracting and collecting data, lack of agreement regarding evaluation criteria, lack of standards for achievement, and lack of useful or eﬃcient historical data.},
	language = {en},
	urldate = {2022-02-15},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2019},
	publisher = {Springer International Publishing},
	author = {Vieira, Felipe J. R. and Oliveira, Manoela R. and do Nascimento, Rogério P. C. and Soares, Michel S.},
	editor = {Misra, Sanjay and Gervasi, Osvaldo and Murgante, Beniamino and Stankova, Elena and Korkhov, Vladimir and Torre, Carmelo and Rocha, Ana Maria A.C. and Taniar, David and Apduhan, Bernady O. and Tarantino, Eufemia},
	year = {2019},
	doi = {10.1007/978-3-030-24308-1_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {59--69},
}

@article{lauritsen2007empirical,
	title = {An empirical study of introducing the {Failure} {Mode} and {Effect} {Analysis} technique},
	journal = {Eur SPI’2007},
	author = {Lauritsen, Torgrim and Stålhane, Tor},
	year = {2007},
}

@misc{myhrer_student_nodate,
	title = {Student experiment using {Preliminary} {Hazard} {Analysis}},
	abstract = {Abstract. Cost-effective development of business critical system requires that we identify potential risks in the early stages of development. The safety analysis method Preliminary Hazard Analysis (PHA) is one such method used for identifying risks in the requirement phase when developing safety critical systems. In this paper we report on the result from an experiment that assessed two ways of using PHA: requirements and scenarios. The experiment showed no significant difference in the number of hazards found for the two approaches. However, scenarios turned out to get results that involved more parts of the system, while requirements got results that set more focus on fewer parts of the system.},
	author = {Myhrer, Per Trygve and Stålhane, Tor},
}

@article{dalcher_learning_2002,
	title = {Learning from {Failures}},
	volume = {7},
	issn = {1077-4866, 1099-1670},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spip.156},
	doi = {10.1002/spip.156},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {Software Process: Improvement and Practice},
	author = {Dalcher, Darren and Tully, Colin},
	month = jun,
	year = {2002},
	pages = {71--89},
}

@article{dalcher_understanding_nodate,
	title = {Understanding {Stories} of {Information} {Systems} {Failures}},
	abstract = {Information systems development failures are prevalent in many domains and countries. The aim of this paper is to explore some of the issues related to the study of such phenomena. Failure situations are not set-up in advance as the subject of studies. Analysing causes and relationship retrospectively depends on the ability to obtain rich and subjective contextual information that can be shed a light on the circumstances that precipitate failures. The paper makes the case for the use of case history and ante-narrative methods for understanding such scenarios.},
	language = {en},
	author = {Dalcher, Darren and Park, Trent and Road, Bramley},
	pages = {16},
}

@article{dalcher_learning_2003,
	title = {Learning from {Information} {Systems} failures by using narrative and ante- narrative methods},
	language = {en},
	author = {Dalcher, Darren},
	year = {2003},
	pages = {6},
}

@incollection{ruhe_rethinking_2014,
	address = {Berlin, Heidelberg},
	title = {Rethinking {Success} in {Software} {Projects}: {Looking} {Beyond} the {Failure} {Factors}},
	isbn = {978-3-642-55034-8 978-3-642-55035-5},
	shorttitle = {Rethinking {Success} in {Software} {Projects}},
	url = {http://link.springer.com/10.1007/978-3-642-55035-5_2},
	abstract = {The notions of success and failure in software projects are confusing. Failure is often considered in the context of the iron triangle as the inability to meet time, cost, and performance constraints. While there is a consensus around the prevalence of project failure, new projects seem destined to repeat past mistakes. This chapter tries to advance the discussion by offering a new perspective for reasoning about the meaning of success and the different types of software project failures.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Software {Project} {Management} in a {Changing} {World}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dalcher, Darren},
	editor = {Ruhe, Günther and Wohlin, Claes},
	year = {2014},
	doi = {10.1007/978-3-642-55035-5_2},
	pages = {27--49},
}

@article{pankratz_eliminating_nodate,
	title = {{ELIMINATING} {FAILURE} {BY} {LEARNING} {FROM} {IT} – {SYSTEMATIC} {REVIEW} {OF} {IS} {PROJECT} {FAILURE}},
	abstract = {Researchers analyzing project success and failure emphasize the prevailing challenge of successfully completing information system (IS) projects. We conduct an extensive systematic literature review of factors that contributed to failure of real-life IS projects. Our resulting overview entails 54 failure factors, which we grouped in 10 categories applying data-driven qualitative content analysis. We extend our holistic overview by linking the factors to specific project failure dimensions and integrating a stakeholder perspective to account for failure responsibility. Our analysis yields widely acknowledged failure factors like insufficient stakeholder involvement as well as less common factors like history of prior successes. Researchers gain insights into project failure factors along with responsible stakeholders and affected failure dimensions, and can use our overview to identify factors or areas of concern to guide future research. Our overview provides a pillar for IS practitioners to learn from others and to eliminate failure by avoiding past mistakes.},
	language = {en},
	author = {Pankratz, Oleg and Basten, Dirk},
	pages = {20},
}

@inproceedings{boicu_learning_2007,
	address = {Cincinnati, OH, USA},
	title = {Learning complex problem solving expertise from failures},
	isbn = {978-0-7695-3069-7},
	url = {http://ieeexplore.ieee.org/document/4457218/},
	doi = {10.1109/ICMLA.2007.42},
	abstract = {Our research addresses the issue of developing knowledge-based agents that capture and use the problem solving knowledge of subject matter experts from diverse application domains. This paper emphasizes the use of negative examples in agent learning by presenting several strategies for capturing expert’s knowledge when the agent fails to correctly solve a problem. These strategies have been implemented into the Disciple learning agent shell and used in complex application domains such as intelligence analysis, center of gravity determination, and emergency response planning.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Sixth {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA} 2007)},
	publisher = {IEEE},
	author = {Boicu, Cristina and Tecuci, Gheorghe and Boicu, Mihai},
	month = dec,
	year = {2007},
	pages = {118--123},
}

@article{blume_decentralized_2009,
	title = {Decentralized {Organizational} {Learning}: {An} {Experimental} {Investigation}},
	volume = {99},
	issn = {0002-8282},
	shorttitle = {Decentralized {Organizational} {Learning}},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.99.4.1178},
	doi = {10.1257/aer.99.4.1178},
	abstract = {We experimentally study decentralized organizational learning. Our objective is to understand how learning members of an organization cope with the confounding effects of the simultaneous learning of others. We test the predictions of a stylized, rational agent model of organizational learning that provides sharp predictions as to how learning members of an organization might cope with the simultaneous learning of others as a function of fundamental variables, e.g., firm size and the discount factor. While the problem of learning while others are learning is quite difficult, we find support for the comparative static predictions of the model's unique symmetric equilibrium. (JEL C72, D23, D83)},
	language = {en},
	number = {4},
	urldate = {2022-02-14},
	journal = {American Economic Review},
	author = {Blume, Andreas and Duffy, John and Franco, April M},
	month = aug,
	year = {2009},
	pages = {1178--1205},
}

@article{blume_decentralized_2007,
	title = {Decentralized learning from failure},
	volume = {133},
	issn = {00220531},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022053106000238},
	doi = {10.1016/j.jet.2006.01.005},
	abstract = {We study decentralized learning in organizations. Decentralization is captured through Crawford and Haller’s [Learning how to cooperate: optimal play in repeated coordination games, Econometrica 58 (1990) 571–595] attainability constraints on strategies. We analyze a repeated game with imperfectly observable actions. A ﬁxed subset of action proﬁles are successes and all others are failures. The location of successes is unknown. The game is played until either there is a success or the time horizon is reached. We partially characterize optimal attainable strategies in the inﬁnite horizon game by showing that after any ﬁxed time, agents will occasionally randomize while at the same time mixing probabilities cannot be uniformly bounded away from zero.},
	language = {en},
	number = {1},
	urldate = {2022-02-14},
	journal = {Journal of Economic Theory},
	author = {Blume, Andreas and Franco, April Mitchell},
	month = mar,
	year = {2007},
	pages = {504--523},
}

@inproceedings{lincke_comparing_2008,
	address = {New York, NY, USA},
	series = {{ISSTA} '08},
	title = {Comparing software metrics tools},
	isbn = {978-1-60558-050-0},
	url = {https://doi.org/10.1145/1390630.1390648},
	doi = {10.1145/1390630.1390648},
	abstract = {This paper shows that existing software metric tools interpret and implement the definitions of object-oriented software metrics differently. This delivers tool-dependent metrics results and has even implications on the results of analyses based on these metrics results. In short, the metrics-based assessment of a software system and measures taken to improve its design differ considerably from tool to tool. To support our case, we conducted an experiment with a number of commercial and free metrics tools. We calculated metrics values using the same set of standard metrics for three software systems of different sizes. Measurements show that, for the same software system and metrics, the metrics values are tool depended. We also defined a (simple) software quality model for "maintainability" based on the metrics selected. It defines a ranking of the classes that are most critical wrt. maintainability. Measurements show that even the ranking of classes in a software system is metrics tool dependent.},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 2008 international symposium on {Software} testing and analysis},
	publisher = {Association for Computing Machinery},
	author = {Lincke, Rüdiger and Lundberg, Jonas and Löwe, Welf},
	month = jul,
	year = {2008},
	keywords = {comparing tools, software quality metrics},
	pages = {131--142},
}

@inproceedings{bacchelli_expectations_2013,
	address = {San Francisco, CA, USA},
	title = {Expectations, outcomes, and challenges of modern code review},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606617/},
	doi = {10.1109/ICSE.2013.6606617},
	abstract = {Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more “lightweight” than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classiﬁed hundreds of review comments across diverse teams at Microsoft. Our study reveals that while ﬁnding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional beneﬁts such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we ﬁnd that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Bacchelli, Alberto and Bird, Christian},
	month = may,
	year = {2013},
	pages = {712--721},
}

@inproceedings{bohme_fuzzing_2020,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Fuzzing: on the exponential cost of vulnerability discovery},
	isbn = {978-1-4503-7043-1},
	shorttitle = {Fuzzing},
	url = {https://doi.org/10.1145/3368089.3409729},
	doi = {10.1145/3368089.3409729},
	abstract = {We present counterintuitive results for the scalability of fuzzing. Given the same non-deterministic fuzzer, finding the same bugs linearly faster requires linearly more machines. For instance, with twice the machines, we can find all known bugs in half the time. Yet, finding linearly more bugs in the same time requires exponentially more machines. For instance, for every new bug we want to find in 24 hours, we might need twice more machines. Similarly for coverage. With exponentially more machines, we can cover the same code exponentially faster, but uncovered code only linearly faster. In other words, re-discovering the same vulnerabilities is cheap but finding new vulnerabilities is expensive. This holds even under the simplifying assumption of no parallelization overhead. We derive these observations from over four CPU years worth of fuzzing campaigns involving almost three hundred open source programs, two state-of-the-art greybox fuzzers, four measures of code coverage, and two measures of vulnerability discovery. We provide a probabilistic analysis and conduct simulation experiments to explain this phenomenon.},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Böhme, Marcel and Falk, Brandon},
	month = nov,
	year = {2020},
	keywords = {efficiency, fuzzing, scalability, software testing},
	pages = {713--724},
}

@article{kim_vicarious_2007,
	title = {Vicarious {Learning} from the {Failures} and {Near}-{Failures} of {Others}: {Evidence} from the {U}.{S}. {Commercial} {Banking} {Industry}},
	volume = {50},
	issn = {0001-4273, 1948-0989},
	shorttitle = {Vicarious {Learning} from the {Failures} and {Near}-{Failures} of {Others}},
	url = {http://journals.aom.org/doi/10.5465/amj.2007.25529755},
	doi = {10.5465/amj.2007.25529755},
	language = {en},
	number = {3},
	urldate = {2022-02-14},
	journal = {Academy of Management Journal},
	author = {Kim, Ji-Yub (Jay) and Miner, Anne S.},
	month = jun,
	year = {2007},
	pages = {687--714},
}

@inproceedings{sandoval_alcocer_learning_2016,
	address = {Delft The Netherlands},
	title = {Learning from {Source} {Code} {History} to {Identify} {Performance} {Failures}},
	isbn = {978-1-4503-4080-9},
	url = {https://dl.acm.org/doi/10.1145/2851553.2851571},
	doi = {10.1145/2851553.2851571},
	abstract = {Source code changes may inadvertently introduce performance regressions. Benchmarking each software version is traditionally employed to identify performance regressions. Although e↵ective, this exhaustive approach is hard to carry out in practice. This paper contrasts source code changes against performance variations. By analyzing 1,288 software versions from 17 open source projects, we identiﬁed 10 source code changes leading to a performance variation (improvement or regression). We have produced a cost model to infer whether a software commit introduces a performance variation by analyzing the source code and sampling the execution of a few versions. By proﬁling the execution of only 17\% of the versions, our model is able to identify 83\% of the performance regressions greater than 5\% and 100\% of the regressions greater than 50\%.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 7th {ACM}/{SPEC} on {International} {Conference} on {Performance} {Engineering}},
	publisher = {ACM},
	author = {Sandoval Alcocer, Juan Pablo and Bergel, Alexandre and Valente, Marco Tulio},
	month = mar,
	year = {2016},
	pages = {37--48},
}

@article{argote_persistence_2022,
	title = {The {Persistence} and {Transfer} of {Learning} in {Industrial} {Settings}},
	language = {en},
	author = {Argote, Linda and Beckman, Sara L and Epple, Dennis},
	year = {2022},
	pages = {16},
}

@article{faraj_coordinating_2000,
	title = {Coordinating {Expertise} in {Software} {Development} {Teams}},
	volume = {46},
	issn = {0025-1909, 1526-5501},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.46.12.1554.12072},
	doi = {10.1287/mnsc.46.12.1554.12072},
	language = {en},
	number = {12},
	urldate = {2022-02-14},
	journal = {Management Science},
	author = {Faraj, Samer and Sproull, Lee},
	month = dec,
	year = {2000},
	pages = {1554--1568},
}

@inproceedings{pflugler_dual-sided_2016,
	address = {Alexandria Virginia USA},
	title = {The {Dual}-sided {Effect} of {Project} {Failure} on {IT} {Professionals}},
	isbn = {978-1-4503-4203-2},
	url = {https://dl.acm.org/doi/10.1145/2890602.2890610},
	doi = {10.1145/2890602.2890610},
	abstract = {The effects of project failure on IT professionals have not received much attention in IT research. A failed project evokes negative emotions and therefore could trigger turnover, which has negative influences from the perspective of IT human resource management. However, the failure of IT projects could also have positive influences as professionals might learn from the failed project. This paper focuses on analyzing this dual-sided effect of project failure on IT professionals. We develop hypotheses that will be tested with a large data set from an IT service provider in future research. We expect to contribute to theory by analyzing whether project failure triggers turnover and by analyzing whether IT professionals learn from failed projects and perform better in the future.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 2016 {ACM} {SIGMIS} {Conference} on {Computers} and {People} {Research}},
	publisher = {ACM},
	author = {Pflügler, Christoph and Wiesche, Manuel and Krcmar, Helmut},
	month = jun,
	year = {2016},
	pages = {33--37},
}

@article{wastell_learning_1999,
	title = {Learning {Dysfunctions} in {Information} {Systems} {Development}: {Overcoming} the {Social} {Defenses} with {Transitional} {Objects}},
	volume = {23},
	issn = {02767783},
	shorttitle = {Learning {Dysfunctions} in {Information} {Systems} {Development}},
	url = {https://www.jstor.org/stable/249490?origin=crossref},
	doi = {10.2307/249490},
	language = {en},
	number = {4},
	urldate = {2022-02-14},
	journal = {MIS Quarterly},
	author = {Wastell, David G.},
	month = dec,
	year = {1999},
	pages = {581},
}

@article{pflugler_i_nodate,
	title = {“{Do} {I} {Want} to {Have} {Losers} {In} {My} {Team}?” – {A} {Quantitative} {Study} of {Learning} from {IT} {Project} {Failure}},
	abstract = {This paper is motivated by a lack of research on the learning from failed IT projects of IT professionals. It remains unclear whether they learn from failed projects and conduct more successful projects in the future. We investigate this research gap with a large quantitative dataset from a German IT service provider. We find that IT professionals learn from failed projects and can leverage this knowledge in the future. Therefore, they should not be seen as “losers”, but as a valuable human resource. Our research contributes to the limited research of learning from failure in IT literature. We show that results that have been obtained in other domains are transferable to the IT domain. Our research is limited by the circumstance, that our dataset comes from only one IT company. This is the first paper that analyzes learning from failure of IT professionals and their performance in future projects.},
	language = {en},
	author = {Pflügler, Christoph and Jäschke, Tamara and Mälzer, Thorsten and Wiesche, Manuel and Krcmar, Helmut},
	pages = {10},
}

@article{hollan_distributed_2000,
	title = {Distributed cognition: toward a new foundation for human-computer interaction research},
	volume = {7},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Distributed cognition},
	url = {https://dl.acm.org/doi/10.1145/353485.353487},
	doi = {10.1145/353485.353487},
	abstract = {We are quickly passing through the historical moment when people work in front of a single computer, dominated by a small CRT and focused on tasks involving only local information. Networked computers are becoming ubiquitous and are playing increasingly significant roles in our lives and in the basic infrastructures of science, business, and social interaction. For human-computer interaction to advance in the new millennium we need to better understand the emerging dynamic of interaction in which the focus task is no longer confined to the desktop but reaches into a complex networked world of information and computer-mediated interactions. We think the theory of distributed cognition has a special role to play in understanding interactions between people and technologies, for its focus has always been on whole environments: what we really do in them and how we coordinate our activity in them. Distributed cognition provides a radical reorientation of how to think about designing and supporting human-computer interaction. As a theory it is specifically tailored to understanding interactions among people and technologies. In this article we propose distributed cognition as a new foundation for human-computer interaction, sketch an integrated research framework, and use selections from our earlier work to suggest how this framework can provide new opportunities in the design of digital work materials.},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Hollan, James and Hutchins, Edwin and Kirsh, David},
	month = jun,
	year = {2000},
	pages = {174--196},
}

@article{madsen_no_2018,
	title = {No {Firm} {Is} an {Island}: {The} {Role} of {Population}-{Level} {Actors} in {Organizational} {Learning} from {Failure}},
	volume = {29},
	issn = {1047-7039, 1526-5455},
	shorttitle = {No {Firm} {Is} an {Island}},
	url = {http://pubsonline.informs.org/doi/10.1287/orsc.2017.1199},
	doi = {10.1287/orsc.2017.1199},
	abstract = {When a serious failure occurs within a population of organizations, members of individual organizations in the population attempt to learn vicariously from the event so that future failures may be avoided. This organization-level vicarious learning process has been extensively studied in the organizational learning literature. However, following a serious failure in one organization, a parallel process also plays out at the population level as population-level actors draw lessons from the failure and exert inﬂuence over organizations in the population in the interest of preventing future failures. Such population-level processes may exert powerful inﬂuences on organization-level learning, but have only begun to be explored in the literature. This paper begins to ﬁll this gap by theorizing and studying the role of population-level actors in organizational learning from failure within and across organizational populations. It examines these issues in a global sample of large airlines operating between 1981 and 2011. The ﬁndings indicate that population-level forces are a major driver of improvement and learning in members of organizational populations—speciﬁcally, that the monitoring strength and activity of population-level actors inﬂuence the rates of organizational learning from failure within their populations.},
	language = {en},
	number = {4},
	urldate = {2022-02-14},
	journal = {Organization Science},
	author = {Madsen, Peter M. and Desai, Vinit},
	month = aug,
	year = {2018},
	pages = {739--753},
}

@article{clinch_learning_2018,
	title = {Learning from {Our} {Mistakes}: {Identifying} {Opportunities} for {Technology} {Intervention} against {Everyday} {Cognitive} {Failure}},
	volume = {17},
	issn = {1536-1268},
	shorttitle = {Learning from {Our} {Mistakes}},
	url = {https://ieeexplore.ieee.org/document/8383664/},
	doi = {10.1109/MPRV.2018.022511240},
	language = {en},
	number = {2},
	urldate = {2022-02-14},
	journal = {IEEE Pervasive Computing},
	author = {Clinch, Sarah and Mascolo, Cecilia},
	month = apr,
	year = {2018},
	pages = {22--33},
}

@inproceedings{borge_patterns_2012,
	address = {Seattle, Washington, USA},
	title = {Patterns of team processes and breakdowns in information analysis tasks},
	isbn = {978-1-4503-1086-4},
	url = {http://dl.acm.org/citation.cfm?doid=2145204.2145369},
	doi = {10.1145/2145204.2145369},
	abstract = {In this paper we present findings from a laboratory study of teams of three, collaborating to complete a complex information sharing, synthesis, decision-making task. We use interaction analysis, communication analysis, and task analysis methods to identify the primary activities teams engaged in as they solved a complex information dependant decision-making task. These activities serve as the foundation to present findings related to common team problems and patterns of interaction associated with team performance. We found differences between high and low performing teams related to verbal equity and how they shared and synthesized information.},
	language = {en},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the {ACM} 2012 conference on {Computer} {Supported} {Cooperative} {Work} - {CSCW} '12},
	publisher = {ACM Press},
	author = {Borge, Marcela and Ganoe, Craig H. and Shih, Shin-I and Carroll, John M.},
	year = {2012},
	pages = {1105},
}

@inproceedings{bohme_directed_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {Directed {Greybox} {Fuzzing}},
	isbn = {978-1-4503-4946-8},
	url = {http://doi.org/10.1145/3133956.3134020},
	doi = {10.1145/3133956.3134020},
	abstract = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
	urldate = {2022-02-13},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Böhme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
	month = oct,
	year = {2017},
	keywords = {coverage-based greybox fuzzing, crash reproduction, directed testing, patch testing, reachability, verifying true positives},
	pages = {2329--2344},
}

@inproceedings{liu_large-scale_2020,
	title = {A {Large}-{Scale} {Empirical} {Study} on {Vulnerability} {Distribution} within {Projects} and the {Lessons} {Learned}},
	abstract = {The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions. It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities. Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project. In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts. We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers. Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities. Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Liu, Bingchang and Meng, Guozhu and Zou, Wei and Gong, Qi and Li, Feng and Lin, Min and Sun, Dandan and Huo, Wei and Zhang, Chao},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	keywords = {Crawlers, Empirical Study, Fuzzing, Manuals, Security, Semantics, Software engineering, Static analysis, Vulnerability Distribution},
	pages = {1547--1559},
}

@article{perry_empirical_nodate,
	title = {An {Empirical} {Study} of {Software} {Interface} {Faults} — {An} {Update}},
	abstract = {In [Perry 85] we reported work in progress on the analysis of software interface faults in a large software system. In this update of that work, we complete the analysis of the interface faults for this large system and extend our results. In our original analysis, we determined that there were 15 categories of interface faults in the prima facie interface fault set of errors reports. We add one new category as a result of our completed analysis: hardware/software interface problems. We summarize our original ﬁndings for the set of errors determined by our operational deﬁnition of interface faults, present our new ﬁndings for the single-ﬁle interface faults, and then present the characterization of interface faults with respect to the total error population.},
	language = {en},
	author = {Perry, Dewayne E and Evangelist, W Michael},
	pages = {22},
}

@inproceedings{ye_fuzzer_2017,
	title = {A {Fuzzer} {Based} on a {Fine}-{Grained} {Deeper} {Strategy}},
	doi = {10.1109/ICISCE.2017.15},
	abstract = {Software nowadays suffers much danger from vulnerabilities, threatening much valuable thing, e.g. the security of private information. Fuzzing is a successful tool in bug-detecting without resorting to much prior knowledge, and has actually discovered many bugs. However, traditional fuzzing has a common drawback that it is limited in a superficial level, and nearly cannot drill deep into the program. This paper proposes DO-Fuzzer (Depth-Oriented-Fuzzer) based on an evolutionary skeleton, employing a fine-grained deep strategy to guide the fuzzer deep into the program. Experiment shows that the deep strategy can augment the fuzzer to detect program-path and consequently program-bugs.},
	booktitle = {2017 4th {International} {Conference} on {Information} {Science} and {Control} {Engineering} ({ICISCE})},
	author = {Ye, Jiaxi and Feng, Chao and Tang, Chaojing},
	month = jul,
	year = {2017},
	keywords = {Computer bugs, Deep strategy, Evolution, Fuzz, Markov processes, Security, Skeleton, Tools},
	pages = {24--28},
}

@article{kampenes_systematic_2007,
	title = {A systematic review of effect size in software engineering experiments},
	volume = {49},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584907000195},
	doi = {10.1016/j.infsof.2007.02.015},
	language = {en},
	number = {11-12},
	urldate = {2022-02-10},
	journal = {Information and Software Technology},
	author = {Kampenes, Vigdis By and Dybå, Tore and Hannay, Jo E. and Sjøberg, Dag I.K.},
	month = nov,
	year = {2007},
	pages = {1073--1086},
}

@inproceedings{yskout_security_2015,
	address = {Florence, Italy},
	title = {Do {Security} {Patterns} {Really} {Help} {Designers}?},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194582/},
	doi = {10.1109/ICSE.2015.49},
	abstract = {Security patterns are well-known solutions to security-speciﬁc problems. They are often claimed to beneﬁt designers without much security expertise. We have performed an empirical study to investigate whether the usage of security patterns by such an audience leads to a more secure design, or to an increased productivity of the designers. Our study involved 32 teams of master students enrolled in a course on software architecture, working on the design of a realisticallysized banking system. Irrespective of whether the teams were using security patterns, we have not been able to detect a difference between the two treatment groups. However, the teams prefer to work with the support of security patterns.},
	language = {en},
	urldate = {2022-02-09},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Yskout, Koen and Scandariato, Riccardo and Joosen, Wouter},
	month = may,
	year = {2015},
	pages = {292--302},
}

@article{prechelt_two_2002,
	title = {Two controlled experiments assessing the usefulness of design pattern documentation in program maintenance},
	volume = {28},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1010061/},
	doi = {10.1109/TSE.2002.1010061},
	abstract = {ÐUsing design patterns is claimed to improve programmer productivity and software quality. Such improvements may manifest both at construction time (in faster and better program design) and at maintenance time (in faster and more accurate program comprehension). This paper focuses on the maintenance context and reports on experimental tests of the following question: Does it help the maintainer if the design patterns in the program code are documented explicitly (using source code comments) compared to a well-commented program without explicit reference to design patterns? Subjects performed maintenance tasks on two programs ranging from 360 to 560 LOC including comments. Both programs contained design patterns. The controlled variable was whether the use of design patterns was documented explicitly or not. The experiments thus tested whether pattern comment lines (PCL) help during maintenance if patterns are relevant and sufficient program comments are already present. It turns out that this question is a challenge for the experimental methodology: A setup leading to relevant results is quite difficult to find. We discuss these issues in detail and suggest a general approach to such situations. The experiment was performed with Java by 74 German graduate students and then repeated with C++ by 22 American undergraduate students. A conservative analysis of the results supports the hypothesis that pattern-relevant maintenance tasks were completed faster or with fewer errors if redundant design pattern information was provided. Redundant means that the information carried in pattern comments is also available in different form in other comments. The contribution of this article is twofold: It provides the first controlled experiment results on design pattern usage and it presents a solution approach to an important class of experiment design problems for experiments regarding documentation.},
	language = {en},
	number = {6},
	urldate = {2022-02-09},
	journal = {IEEE Transactions on Software Engineering},
	author = {Prechelt, L. and Unger-Lamprecht, B. and Philippsen, M. and Tichy, W.F.},
	month = jun,
	year = {2002},
	pages = {595--606},
}

@article{prechelt_controlled_2001,
	title = {A controlled experiment in maintenance: comparing design patterns to simpler solutions},
	volume = {27},
	issn = {00985589},
	shorttitle = {A controlled experiment in maintenance},
	url = {http://ieeexplore.ieee.org/document/988711/},
	doi = {10.1109/32.988711},
	abstract = {ÐSoftware design patterns package proven solutions to recurring design problems in a form that simplifies reuse. We are seeking empirical evidence whether using design patterns is beneficial. In particular, one may prefer using a design pattern even if the actual design problem is simpler than that solved by the pattern, i.e., if not all of the functionality offered by the pattern is actually required. Our experiment investigates software maintenance scenarios that employ various design patterns and compares designs with patterns to simpler alternatives. The subjects were professional software engineers. In most of our nine maintenance tasks, we found positive effects from using a design pattern: Either its inherent additional flexibility was achieved without requiring more maintenance time or maintenance time was reduced compared to the simpler alternative. In a few cases, we found negative effects: The alternative solution was less error-prone or required less maintenance time. Although most of these effects were expected, a few were surprising: A negative effect occurs although a certain application of the Observer pattern appears to be well justified and a positive effect occurs despite superfluous flexibility (and, hence, complexity) introduced by a certain application of the Decorator pattern. Overall, we conclude that, unless there is a clear reason to prefer the simpler solution, it is probably wise to choose the flexibility provided by the design pattern because unexpected new requirements often appear. We identify several questions for future empirical research.},
	language = {en},
	number = {12},
	urldate = {2022-02-09},
	journal = {IEEE Transactions on Software Engineering},
	author = {Prechelt, L. and Unger, B. and Tichy, W.F. and Brossler, P. and Votta, L.G.},
	month = dec,
	year = {2001},
	pages = {1134--1144},
}

@article{yuan_simple_nodate,
	title = {Simple {Testing} {Can} {Prevent} {Most} {Critical} {Failures}},
	abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failure. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufﬁcient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures.},
	language = {en},
	author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U and Stumm, Michael},
	pages = {18},
}

@article{medeiros_vulnerable_2020,
	title = {Vulnerable {Code} {Detection} {Using} {Software} {Metrics} and {Machine} {Learning}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3041181},
	abstract = {Software metrics are widely-used indicators of software quality and several studies have shown that such metrics can be used to estimate the presence of vulnerabilities in the code. In this paper, we present a comprehensive experiment to study how effective software metrics can be to distinguish the vulnerable code units from the non-vulnerable ones. To this end, we use several machine learning algorithms (Random Forest, Extreme Boosting, Decision Tree, SVM Linear, and SVM Radial) to extract vulnerability-related knowledge from software metrics collected from the source code of several representative software projects developed in C/C++ (Mozilla Firefox, Linux Kernel, Apache HTTPd, Xen, and Glibc). We consider different combinations of software metrics and diverse application scenarios with different security concerns (e.g., highly critical or non-critical systems). This experiment contributes to understanding whether software metrics can effectively be used to distinguish vulnerable code units in different application scenarios, and how can machine learning algorithms help in this regard. The main observation is that using machine learning algorithms on top of software metrics helps to indicate vulnerable code units with a relatively high level of confidence for security-critical software systems (where the focus is on detecting the maximum number of vulnerabilities, even if false positives are reported), but they are not helpful for low-critical or non-critical systems due to the high number of false positives (that bring an additional development cost frequently not affordable).},
	journal = {IEEE Access},
	author = {Medeiros, Nádia and Ivaki, Naghmeh and Costa, Pedro and Vieira, Marco},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Application scenarios, Machine learning algorithms, Measurement, Security, Software, Software metrics, Support vector machines, Tools, machine learning, security vulnerabilities, software metrics, software security},
	pages = {219174--219198},
}

@inproceedings{alves_software_2016,
	title = {Software {Metrics} and {Security} {Vulnerabilities}: {Dataset} and {Exploratory} {Study}},
	shorttitle = {Software {Metrics} and {Security} {Vulnerabilities}},
	doi = {10.1109/EDCC.2016.34},
	abstract = {Code with certain characteristics is more prone to have security vulnerabilities. In fact, studies show that code not following best practices is harder to verify and maintain, and consequently is more probable to have vulnerabilities left unnoticed or inadvertently introduced. In this experience report, we study whether software metrics can reflect such characteristics, thus having some correlation with the existence of vulnerabilities. The analysis is based on 2875 security patches, used to build a dataset with metrics and vulnerabilities for all the functions, classes and files of 5750 versions of five widely used projects that are exposed to attacks: Linux Kernel, Mozilla, Xen Hypervisor, httpd and glibc. We calculated software metrics from their sources and used correlation algorithm and statistical tests on these metrics in order to identify relations between them and the existing vulnerabilities. Results show that software metrics are able to discriminate vulnerable and non vulnerable functions, but it is not possible to find strong correlations between these metrics and the number of vulnerabilities existing in the analyzed functions. Finally, the results indicate that vulnerable functions are probable to have other vulnerabilities in the future.},
	booktitle = {2016 12th {European} {Dependable} {Computing} {Conference} ({EDCC})},
	author = {Alves, Henrique and Fonseca, Baldoino and Antunes, Nuno},
	month = sep,
	year = {2016},
	keywords = {Computer bugs, Correlation, Kernel, Security, Software Metrics, Software Security, Software metrics, Vulnerabilities},
	pages = {37--44},
}

@inproceedings{medeiros_software_2017,
	title = {Software {Metrics} as {Indicators} of {Security} {Vulnerabilities}},
	doi = {10.1109/ISSRE.2017.11},
	abstract = {Detecting software security vulnerabilities and distinguishing vulnerable from non-vulnerable code is anything but simple. Most of the time, vulnerabilities remain undisclosed until they are exposed, for instance, by an attack during the software operational phase. Software metrics are widely-used indicators of software quality, but the question is whether they can be used to distinguish vulnerable software units from the non-vulnerable ones during development. In this paper, we perform an exploratory study on software metrics, their interdependency, and their relation with security vulnerabilities. We aim at understanding: i) the correlation between software architectural characteristics, represented in the form of software metrics, and the number of vulnerabilities; and ii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. To achieve these goals, we use, respectively, correlation coefficients and heuristic search techniques. Our analysis is carried out on a dataset that includes software metrics and reported security vulnerabilities, exposed by security attacks, for all functions, classes, and files of five widely used projects. Results show: i) a strong correlation between several project-level metrics and the number of vulnerabilities, ii) the possibility of using a group of metrics, at both file and function levels, to distinguish vulnerable and non-vulnerable code with a high level of accuracy.},
	booktitle = {2017 {IEEE} 28th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Medeiros, Nádia and Ivaki, Naghmeh and Costa, Pedro and Vieira, Marco},
	month = oct,
	year = {2017},
	note = {ISSN: 2332-6549},
	keywords = {Complexity theory, Correlation, Feature Selection, Security, Security Vulnerabilities, Software Metrics, Software metrics, Software systems},
	pages = {216--227},
}

@inproceedings{medeiros_software_2017-1,
	title = {Software {Metrics} as {Indicators} of {Security} {Vulnerabilities}},
	doi = {10.1109/ISSRE.2017.11},
	abstract = {Detecting software security vulnerabilities and distinguishing vulnerable from non-vulnerable code is anything but simple. Most of the time, vulnerabilities remain undisclosed until they are exposed, for instance, by an attack during the software operational phase. Software metrics are widely-used indicators of software quality, but the question is whether they can be used to distinguish vulnerable software units from the non-vulnerable ones during development. In this paper, we perform an exploratory study on software metrics, their interdependency, and their relation with security vulnerabilities. We aim at understanding: i) the correlation between software architectural characteristics, represented in the form of software metrics, and the number of vulnerabilities; and ii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. To achieve these goals, we use, respectively, correlation coefficients and heuristic search techniques. Our analysis is carried out on a dataset that includes software metrics and reported security vulnerabilities, exposed by security attacks, for all functions, classes, and files of five widely used projects. Results show: i) a strong correlation between several project-level metrics and the number of vulnerabilities, ii) the possibility of using a group of metrics, at both file and function levels, to distinguish vulnerable and non-vulnerable code with a high level of accuracy.},
	booktitle = {2017 {IEEE} 28th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Medeiros, Nádia and Ivaki, Naghmeh and Costa, Pedro and Vieira, Marco},
	month = oct,
	year = {2017},
	note = {ISSN: 2332-6549},
	keywords = {Complexity theory, Correlation, Feature Selection, Security, Security Vulnerabilities, Software Metrics, Software metrics, Software systems},
	pages = {216--227},
}

@article{laptev_optimizing_nodate,
	title = {Optimizing {Regular} {Expression} {Clustering} for {Massive} {Pattern} {Search}},
	abstract = {Optimizing the search for a large sets of patterns, due to their prevalence, is becoming critical in a variety of applications including ﬁnancial services, healthcare monitoring, RFIDbased inventory management, publish subscribe and network intrusion detection systems (NIDS). For example in NIDS, to identify particular packets in a packet stream, modern network devices need to perform deep packet inspection at high rates given a limited memory and a large number of signatures. It is often proposed to group together regular expressions (REs) denoting similar patterns into a DFA or an NFA to decrease the memory usage and increase the average throughput. In this paper, we propose a framework comprised of three novel distance metrics used to cluster REs and an execution technique that uses the information aggregated by the distance metrics to optimize the number of memory reads during execution, thereby increasing the overall throughput. Our distance metrics are unique in that each metric is specialized for a particular type of patterns. The ﬁrst two metrics optimize for compactness of REs and specialize in regular expressions that have long, contiguous similarity segments. Our third metric optimizes for redundancy and specializes in REs that have random segments of similar. We show how compression, prefetching and memory access optimizations are made possible when the notion of redundancy is leveraged. We validate our approach by implementing it in a modern NIDS Snort, which matches potentially thousands of patterns against an incoming network stream. We observe a ﬁvefold speed improvement over the unmodiﬁed version of Snort.},
	language = {en},
	author = {Laptev, Nikolay and Mousavi, Hamid and Shkapsky, Alexander and Zaniolo, Carlo},
	pages = {13},
}

@article{wang_school_2009,
	title = {School {Bullying} {Among} {Adolescents} in the {United} {States}: {Physical}, {Verbal}, {Relational}, and {Cyber}},
	volume = {45},
	issn = {1054-139X},
	url = {https://www.sciencedirect.com/science/article/pii/S1054139X09001384},
	doi = {https://doi.org/10.1016/j.jadohealth.2009.03.021},
	abstract = {Purpose Four forms of school bullying behaviors among US adolescents and their association with sociodemographic characteristics, parental support, and friends were examined. Methods Data were obtained from the Health Behavior in School-Aged Children (HBSC) 2005 Survey, a nationally representative sample of grades 6–10 (N = 7,182). The revised Olweus Bully/Victim Questionnaire was used to measure physical, verbal, and relational forms of bullying. Two items were added using the same format to measure cyber bullying. For each form, four categories were created: bully, victim, bully-victim, and not involved. Multinomial logistic regressions were applied, with sociodemographic variables, parental support, and number of friends as predictors. Results Prevalence rates of having bullied others or having been bullied at school for at least once in the last 2 months were 20.8\% physically, 53.6\% verbally, 51.4\% socially, or 13.6\% electronically. Boys were more involved in physical or verbal bullying, whereas girls were more involved in relational bullying. Boys were more likely to be cyber bullies, whereas girls were more likely to be cyber victims. African-American adolescents were involved in more bullying (physical, verbal, or cyber) but less victimization (verbal or relational). Higher parental support was associated with less involvement across all forms and classifications of bullying. Having more friends was associated with more bullying and less victimization for physical, verbal, and relational forms but was not associated with cyber bullying. Conclusions Parental support may protect adolescents from all four forms of bullying. Friends associate differentially with traditional and cyber bullying. Results indicate that cyber bullying is a distinct nature from that of traditional bullying.},
	number = {4},
	journal = {Journal of Adolescent Health},
	author = {Wang, Jing and Iannotti, Ronald J. and Nansel, Tonja R.},
	year = {2009},
	keywords = {Cyber bullying, Parental support, Peers, Relational bullying, School bullying, Sociodemographic characteristics},
	pages = {368--375},
}

@article{dalcher_stories_2004,
	title = {Stories and {Histories}: {Case} {Study} {Research} (and {Beyond}) in {Information} {Systems} {Failures}},
	abstract = {Information systems development failures are prevalent in many domains and countries. The aim of this chapter is to explore some of the issues related to the study of such phenomena. Failure situations are not set-up in advance as the subject of studies. Analysing causes and relationships retrospectively depends on the ability to obtain rich and subjective contextual information that can be utilised for shedding a light on the circumstances that precipitate failures. This chapter makes the case for the use of case history and ante-narrative methods for understanding such rich and complex scenarios.},
	language = {en},
	author = {Dalcher, Darren},
	year = {2004},
	pages = {18},
}

@inproceedings{claessen_quickcheck_2000,
	address = {New York, NY, USA},
	series = {{ICFP} '00},
	title = {{QuickCheck}: a lightweight tool for random testing of {Haskell} programs},
	isbn = {978-1-58113-202-1},
	shorttitle = {{QuickCheck}},
	url = {http://doi.org/10.1145/351240.351266},
	doi = {10.1145/351240.351266},
	abstract = {Quick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.},
	urldate = {2022-02-02},
	booktitle = {Proceedings of the fifth {ACM} {SIGPLAN} international conference on {Functional} programming},
	publisher = {Association for Computing Machinery},
	author = {Claessen, Koen and Hughes, John},
	month = sep,
	year = {2000},
	pages = {268--279},
}

@article{klees_evaluating_2018,
	title = {Evaluating {Fuzz} {Testing}},
	url = {http://arxiv.org/abs/1808.09700},
	abstract = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
	language = {en},
	urldate = {2022-02-03},
	journal = {arXiv:1808.09700 [cs]},
	author = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
	month = oct,
	year = {2018},
	note = {arXiv: 1808.09700},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{guler_antifuzz_2019,
	title = {\{{AntiFuzz}\}: {Impeding} {Fuzzing} {Audits} of {Binary} {Executables}},
	isbn = {978-1-939133-06-9},
	shorttitle = {\{{AntiFuzz}\}},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/guler},
	language = {en},
	urldate = {2022-02-03},
	author = {Güler, Emre and Aschermann, Cornelius and Abbasi, Ali and Holz, Thorsten},
	year = {2019},
	pages = {1931--1947},
}

@inproceedings{jung_fuzzification_2019,
	title = {Fuzzification: \{{Anti}-{Fuzzing}\} {Techniques}},
	isbn = {978-1-939133-06-9},
	shorttitle = {Fuzzification},
	url = {https://www.usenix.org/conference/usenixsecurity19/presentation/jung#},
	language = {en},
	urldate = {2022-02-03},
	author = {Jung, Jinho and Hu, Hong and Solodukhin, David and Pagan, Daniel and Lee, Kyu Hyung and Kim, Taesoo},
	year = {2019},
	pages = {1913--1930},
}

@book{allen_sage_2017,
	address = {2455 Teller Road, Thousand Oaks California 91320},
	title = {The {SAGE} {Encyclopedia} of {Communication} {Research} {Methods}},
	isbn = {978-1-4833-8143-5 978-1-4833-8141-1},
	url = {https://sk.sagepub.com/reference/the-sage-encyclopedia-of-communication-research-methods/},
	language = {en},
	urldate = {2022-02-03},
	publisher = {SAGE Publications, Inc},
	author = {Allen, Mike},
	year = {2017},
	doi = {10.4135/9781483381411},
}

@inproceedings{goodman_deepstate_2018,
	address = {San Diego, CA},
	title = {{DeepState}: {Symbolic} {Unit} {Testing} for {C} and {C}++},
	isbn = {978-1-891562-50-1},
	shorttitle = {{DeepState}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/07/bar2018_9_Goodman_paper.pdf},
	doi = {10.14722/bar.2018.23009},
	abstract = {Unit testing is a popular software development methodology that can help developers detect functional regressions, explore boundary conditions, and document expected behavior. However, writing comprehensive unit tests is challenging and time-consuming, and developers seldom explore the obscure (and bug-hiding) corners of software behavior without assistance. DeepState is a tool that provides a Google Test-like API to give C and C++ developers push-button access to symbolic execution engines, such as Manticore and angr, and fuzzers, such as Dr. Fuzz. Rather than learning multiple complex tools, users learn one interface for deﬁning a test harness, and can use various methods to automatically generate tests for software. In addition to providing a familiar interface to binary analysis and fuzzing for parameterized unit testing, DeepState also provides constructs that aid in the construction of API-sequence tests, where the tool chooses the functions or methods to call, allowing for even more diverse and powerful tests. By serving as a front-end to multiple tools, DeepState additionally provides a way to apply (novel) high-level strategies to test generation, and to compare effectiveness and efﬁciency of testing back-ends, including binary analysis tools.},
	language = {en},
	urldate = {2022-02-03},
	booktitle = {Proceedings 2018 {Workshop} on {Binary} {Analysis} {Research}},
	publisher = {Internet Society},
	author = {Goodman, Peter and Groce, Alex},
	year = {2018},
}

@article{gorwa_algorithmic_2020,
	title = {Algorithmic content moderation: {Technical} and political challenges in the automation of platform governance},
	volume = {7},
	issn = {2053-9517},
	shorttitle = {Algorithmic content moderation},
	url = {https://doi.org/10.1177/2053951719897945},
	doi = {10.1177/2053951719897945},
	abstract = {As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinformation. Automated hash-matching and predictive machine learning tools – what we define here as algorithmic moderation systems – are increasingly being deployed to conduct content moderation at scale by major platforms for user-generated content such as Facebook, YouTube and Twitter. This article provides an accessible technical primer on how algorithmic moderation works; examines some of the existing automated tools used by major platforms to handle copyright infringement, terrorism and toxic speech; and identifies key political and ethical issues for these systems as the reliance on them grows. Recent events suggest that algorithmic moderation has become necessary to manage growing public expectations for increased platform responsibility, safety and security on the global stage; however, as we demonstrate, these systems remain opaque, unaccountable and poorly understood. Despite the potential promise of algorithms or ‘AI’, we show that even ‘well optimized’ moderation systems could exacerbate, rather than relieve, many existing problems with content policy as enacted by platforms for three main reasons: automated moderation threatens to (a) further increase opacity, making a famously non-transparent set of practices even more difficult to understand or audit, (b) further complicate outstanding issues of fairness and justice in large-scale sociotechnical systems and (c) re-obscure the fundamentally political nature of speech decisions being executed at scale.},
	language = {en},
	number = {1},
	urldate = {2022-01-14},
	journal = {Big Data \& Society},
	author = {Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
	month = jan,
	year = {2020},
	keywords = {Platform governance, algorithms, artificial intelligence, content moderation, copyright, toxic speech},
	pages = {2053951719897945},
}

@inproceedings{long_experience_2008,
	address = {Leipzig, Germany},
	title = {Experience applying the {SPIN} model checker to an industrial telecommunications system},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368187},
	doi = {10.1145/1368088.1368187},
	abstract = {Model checking has for years been advertised as a way of ensuring the correctness of complex software systems. However, there exist surprisingly few critical studies of the application of model checking to industrial-scale software systems by people other than the model checker’s own authors. In this paper we report our experience in applying the Spin model checker to the validation of the failover protocols of a commercial telecommunications system. While we conclude that model checking is not yet ready for such applications, we ﬁnd that current research in the model checking community is working to address the diﬃculties we encountered.},
	language = {en},
	urldate = {2022-02-02},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {Long, Barry and Dingel, Juergen and Graham, T.C. Nicholas},
	year = {2008},
	pages = {693},
}

@inproceedings{chen_savior_2020,
	title = {{SAVIOR}: {Towards} {Bug}-{Driven} {Hybrid} {Testing}},
	shorttitle = {{SAVIOR}},
	doi = {10.1109/SP40000.2020.00002},
	abstract = {Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. As a result, hybrid testing is able to reach deeper into program state space than fuzz testing or concolic execution alone. Recently, hybrid testing has seen significant advancement. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43\%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths.We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4\% faster than DRILLER and 44.3\% faster than QSYM, leading to the discovery of 88 and 76 more unique bugs, respectively. According to the evaluation on 11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, Yaohui and Li, Peng and Xu, Jun and Guo, Shengjian and Zhou, Rundong and Zhang, Yulong and Wei, Tao and Lu, Long},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Benchmark testing, Computer bugs, Fuzzing, Security, Software, Tools},
	pages = {1580--1596},
}

@inproceedings{chen_savior_2020-1,
	title = {{SAVIOR}: {Towards} {Bug}-{Driven} {Hybrid} {Testing}},
	shorttitle = {{SAVIOR}},
	doi = {10.1109/SP40000.2020.00002},
	abstract = {Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. As a result, hybrid testing is able to reach deeper into program state space than fuzz testing or concolic execution alone. Recently, hybrid testing has seen significant advancement. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43\%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths.We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4\% faster than DRILLER and 44.3\% faster than QSYM, leading to the discovery of 88 and 76 more unique bugs, respectively. According to the evaluation on 11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, Yaohui and Li, Peng and Xu, Jun and Guo, Shengjian and Zhou, Rundong and Zhang, Yulong and Wei, Tao and Lu, Long},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Benchmark testing, Computer bugs, Fuzzing, Security, Software, Tools},
	pages = {1580--1596},
}

@inproceedings{poncin_process_2011,
	title = {Process {Mining} {Software} {Repositories}},
	doi = {10.1109/CSMR.2011.5},
	abstract = {Software developers’ activities are in general recorded in software repositories such as version control systems, bug trackers and mail archives. While abundant information is usually present in such repositories, successful information extraction is often challenged by the necessity to simultaneously analyze different repositories and to combine the information obtained. We propose to apply process mining techniques, originally developed for business process analysis, to address this challenge. However, in order for process mining to become applicable, different software repositories should be combined, and “related” software development events should be matched: e.g., mails sent about a file, modifications of the file and bug reports that can be traced back to it. The combination and matching of events has been implemented in FRASR (Framework for Analyzing Software Repositories), augmenting the process mining framework ProM. FRASR has been successfully applied in a series of case studies addressing such aspects of the development process as roles of different developers and the way bug reports are handled.},
	booktitle = {2011 15th {European} {Conference} on {Software} {Maintenance} and {Reengineering}},
	author = {Poncin, Wouter and Serebrenik, Alexander and Brand, Mark van den},
	month = mar,
	year = {2011},
	note = {ISSN: 1534-5351},
	keywords = {Computer bugs, Control systems, Data mining, PROM, Postal services, Process control, Software, process mining, software repositories},
	pages = {5--14},
}

@inproceedings{latoza_maintaining_2006,
	address = {Shanghai China},
	title = {Maintaining mental models: a study of developer work habits},
	isbn = {978-1-59593-375-1},
	shorttitle = {Maintaining mental models},
	url = {https://dl.acm.org/doi/10.1145/1134285.1134355},
	doi = {10.1145/1134285.1134355},
	abstract = {To understand developers’ typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.},
	language = {en},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {ACM},
	author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
	month = may,
	year = {2006},
	pages = {492--501},
}

@article{dalcher2002forensic,
	title = {Forensic software engineering and stories of failures},
	journal = {Investigation and Reporting of Incidents and Accidents (IRIA 2002)},
	author = {Dalcher, Darren},
	year = {2002},
}

@inproceedings{sjoberg_conducting_2002,
	address = {Nara, Japan},
	title = {Conducting realistic experiments in software engineering},
	isbn = {9780769517964},
	url = {http://ieeexplore.ieee.org/document/1166921/},
	doi = {10.1109/ISESE.2002.1166921},
	urldate = {2022-01-31},
	booktitle = {Proceedings {International} {Symposium} on {Empirical} {Software} {Engineering}},
	publisher = {IEEE Comput. Soc},
	author = {Sjoberg, D.I.K. and Anda, B. and Arisholm, E. and Dyba, T. and Jorgensen, M. and Karahasanovic, A. and Koren, E.F. and Vokac, M.},
	year = {2002},
	pages = {17--26},
}

@inproceedings{fonseca_describing_2017,
	title = {Describing {What} {Experimental} {Software} {Engineering} {Experts} {Do} {When} {They} {Design} {Their} {Experiments} - {A} {Qualitative} {Study}},
	doi = {10.1109/ESEM.2017.63},
	abstract = {Background: Although there has been a significant amount of research focused on designing and conducting controlled experiments, few studies report how experienced experimental software engineering researchers actually design and conduct their studies. Aims: This study aimed to offer a practical perspective from their viewpoint regarding controlled experiment planning. Method: We collected data through semi-structured interviews from 11 researchers, and we used qualitative analysis methods from the grounded theory approach to analyze them. Result: Although the complete study presents four research questions, in this paper, we answer the first one. As a result, we present a preliminary result about what these experts actually do when they design experiments. Conclusions: This work contributes to a better understanding of the practical performance of experimental software engineering.},
	booktitle = {2017 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Fonseca, Liliane Sheyla da Silva and Seaman, Carolyn Budinger and Soares, Sergio Castelo Branco},
	month = nov,
	year = {2017},
	keywords = {Data analysis, Encoding, Experiments, Guidelines, Human Subjects, Interviews, Planning, Qualitative Study, Software, Software Engineering, Software engineering},
	pages = {211--216},
}

@article{sjoeberg_survey_2005,
	title = {A survey of controlled experiments in software engineering},
	volume = {31},
	issn = {1939-3520},
	doi = {10.1109/TSE.2005.97},
	abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Sjoeberg, D.I.K. and Hannay, J.E. and Hansen, O. and Kampenes, V.B. and Karahasanovic, A. and Liborg, N.-K. and Rekdal, A.C.},
	month = sep,
	year = {2005},
	keywords = {Application software, Computer Society, Computer industry, Conference proceedings, Index Terms- Controlled experiments, Programming, Recruitment, Software engineering, Software maintenance, Software metrics, Software systems, empirical software engineering., research methodology, survey},
	pages = {733--753},
}

@article{ko_practical_2015,
	title = {A practical guide to controlled experiments of software engineering tools with human participants},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-013-9279-3},
	doi = {10.1007/s10664-013-9279-3},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {Empirical Software Engineering},
	author = {Ko, Andrew J. and LaToza, Thomas D. and Burnett, Margaret M.},
	month = feb,
	year = {2015},
	pages = {110--141},
}

@article{litzinger_engineering_2011,
	title = {Engineering {Education} and the {Development} of {Expertise}},
	volume = {100},
	issn = {10694730},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/j.2168-9830.2011.tb00006.x},
	doi = {10.1002/j.2168-9830.2011.tb00006.x},
	abstract = {BACKGROUND Although engineering education has evolved in ways that improve the readiness of graduates to meet the challenges of the twenty-first century, national and international organizations continue to call for change. Future changes in engineering education should be guided by research on expertise and the learning processes that support its development.
PURPOSE The goals of this paper are: to relate key findings from studies of the development of expertise to engineering education, to summarize instructional practices that are consistent with these findings, to provide examples of learning experiences that are consistent with these instructional practices, and finally, to identify challenges to implementing such learning experiences in engineering programs. SCOPE/METHOD The research synthesized for this article includes that on the development of expertise, students’ approaches to learning, students’ responses to instructional practices, and the role of motivation in learning. In addition, literature on the dominant teaching and learning practices in engineering education is used to frame some of the challenges to implementing alternative approaches to learning.
CONCLUSION Current understanding of expertise, and the learning processes that develop it, indicates that engineering education should encompass a set of learning experiences that allow students to construct deep conceptual knowledge, to develop the ability to apply key technical and professional skills fluently, and to engage in a number of authentic engineering projects. Engineering curricula and teaching methods are often not well aligned with these goals. Curriculum-level instructional design processes should be used to design and implement changes that will improve alignment.},
	language = {en},
	number = {1},
	urldate = {2022-01-29},
	journal = {Journal of Engineering Education},
	author = {Litzinger, Thomas and Lattuca, Lisa R. and Hadgraft, Roger and Newstetter, Wendy},
	month = jan,
	year = {2011},
	pages = {123--150},
}

@article{vessey_expertise_1985,
	title = {Expertise in debugging computer programs: {A} process analysis},
	volume = {23},
	issn = {00207373},
	shorttitle = {Expertise in debugging computer programs},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020737385800547},
	doi = {10.1016/S0020-7373(85)80054-7},
	language = {en},
	number = {5},
	urldate = {2022-01-29},
	journal = {International Journal of Man-Machine Studies},
	author = {Vessey, Iris},
	month = nov,
	year = {1985},
	pages = {459--494},
}

@book{noauthor_how_2000,
	address = {Washington, D.C.},
	title = {How {People} {Learn}: {Brain}, {Mind}, {Experience}, and {School}: {Expanded} {Edition}},
	isbn = {978-0-309-07036-2},
	shorttitle = {How {People} {Learn}},
	url = {http://www.nap.edu/catalog/9853},
	language = {en},
	urldate = {2022-01-29},
	publisher = {National Academies Press},
	month = aug,
	year = {2000},
	doi = {10.17226/9853},
	note = {Pages: 9853},
}

@article{spohrer_novice_1986,
	title = {Novice mistakes: are the folk wisdoms correct?},
	volume = {29},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Novice mistakes},
	url = {https://dl.acm.org/doi/10.1145/6138.6145},
	doi = {10.1145/6138.6145},
	abstract = {An evaluation of two folk wisdoms serves to elucidate the underlying or "deep-structure" reasons for novice errors.},
	language = {en},
	number = {7},
	urldate = {2022-01-29},
	journal = {Communications of the ACM},
	author = {Spohrer, James C. and Soloway, Elliot},
	month = jul,
	year = {1986},
	pages = {624--632},
}

@article{wheatley_learning_2010,
	title = {Learning from failure [{Manufacturing} {Safety}]},
	volume = {5},
	issn = {1750-9637},
	doi = {10.1049/et.2010.1314},
	abstract = {First, let's briefly review the facts. Since 2002, when Toyota introduced an electronic throttle control system, there have been a spate of what are termed 'unintended acceleration' incidents involving Toyota vehicles. Several investigations by the US National Highway Traffic Safety Administration duly followed, and there were a couple of small recalls. Then, in August 2009, came the harrowing deaths of California Highway Patrol officer Mark Saylor and three members of his family. In a recorded emergency call, the cautious and experienced officer can be heard saying that the accelerator was stuck, and that the brakes weren't working. As he approached an intersection, his last words were: "Hold on and pray" Finally prompted to acknowledge a problem, Toyota subsequently recalled 2.3 million vehicles, blaming an American supplier for faulty workmanship. But the supplier fought back, pointing out that they had only become a supplier in 2005 three years after reports of unintended acceleration first surfaced. In all, one American Firm of safety consultants has Identified 2,262 instances of the problem, leading to 819 crashes and 26 deaths. The world's media previously fond of extolling Toyota's Just-in-Time productions systems and quality-conscious virtues now rounded on the company, which quickly found its reputation in tatters. Public apologies soon followed from Toyota president Akio Toyoda, the grandson of the company's founder, as did still further recalls. Yet at the time of writing, no firm cause has conclusively been identified, and Toyota's reputation remains troubled.},
	number = {13},
	journal = {Engineering Technology},
	author = {Wheatley, Malcolm},
	month = sep,
	year = {2010},
	note = {Conference Name: Engineering Technology},
	pages = {56--58},
}

@incollection{shanks_implementing_2003,
	edition = {1},
	title = {Implementing {Enterprise} {Resource} {Planning} {Systems}: {The} {Role} of {Learning} from {Failure}},
	isbn = {978-0-521-81902-2 978-0-511-81507-2},
	shorttitle = {Implementing {Enterprise} {Resource} {Planning} {Systems}},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511815072A021/type/book_part},
	abstract = {ERP implementations remain problematic despite the fact that many of the issues are by now quite well known. In this paper, we take a different perspective from the critical success factors and risks approaches that are common in the information systems discipline to explain why ERP implementations fail. Speci®cally, we adapt Sitkin's theory of intelligent failure to ERP implementations resulting in a theory that we call ``learning from failure.'' We then examine from the viewpoint of this theory the details of two SAP R/3 implementations, one of which failed while the other succeeded. Although it is impossible to state, unequivocally, that the implementation that failed did so because it did not use the approach that was derived from the theory, the analysis reveals that the company that followed many of the tenets of the theory succeeded while the other did not.},
	language = {en},
	urldate = {2022-01-28},
	booktitle = {Second-{Wave} {Enterprise} {Resource} {Planning} {Systems}},
	publisher = {Cambridge University Press},
	author = {Scott, Judy E. and Vessey, Iris},
	editor = {Shanks, Graeme and Seddon, Peter B. and Willcocks, Leslie P.},
	month = sep,
	year = {2003},
	doi = {10.1017/CBO9780511815072.011},
	pages = {241--274},
}

@article{schrenker_learning_2007,
	title = {Learning from {Failure}: {The} {Teachings} of {Petroski}},
	volume = {41},
	issn = {0899-8205, 1943-5967},
	shorttitle = {Learning from {Failure}},
	url = {http://www.aami-bit.org/doi/abs/10.2345/0899-8205%282007%2941%5B395%3ALFFTTO%5D2.0.CO%3B2},
	doi = {10.2345/0899-8205(2007)41[395:LFFTTO]2.0.CO;2},
	language = {en},
	number = {5},
	urldate = {2022-01-27},
	journal = {Biomedical Instrumentation \& Technology},
	author = {Schrenker, Richard},
	month = sep,
	year = {2007},
	pages = {395--398},
}

@article{fenton_quantitative_2000,
	title = {Quantitative analysis of faults and failures in a complex software system},
	volume = {26},
	issn = {1939-3520},
	doi = {10.1109/32.879815},
	abstract = {The authors describe a number of results from a quantitative study of faults and failures in two releases of a major commercial software system. They tested a range of basic software engineering hypotheses relating to: the Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. The most important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults; those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Our results provide data-points in building up an empirical picture of the software development process.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fenton, N.E. and Ohlsson, N.},
	month = aug,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Benchmark testing, Computer industry, Density measurement, Failure analysis, Phase measurement, Programming, Software engineering, Software metrics, Software systems, Software testing},
	pages = {797--814},
}

@inproceedings{9519496,
	title = {Runtime recovery of web applications under zero-day {ReDoS} attacks},
	doi = {10.1109/SP40001.2021.00077},
	booktitle = {2021 {IEEE} symposium on security and privacy ({SP})},
	author = {Bai, Zhihao and Wang, Ke and Zhu, Hang and Cao, Yinzhi and Jin, Xin},
	year = {2021},
	pages = {1575--1588},
}

@article{buschmann_learning_2010,
	title = {Learning from {Failure}, {Part} {III}: {On} {Hammers} and {Nails}, and {Falling} in {Love} with {Technology} and {Design}},
	volume = {27},
	issn = {1937-4194},
	shorttitle = {Learning from {Failure}, {Part} {III}},
	doi = {10.1109/MS.2010.47},
	abstract = {Architects are directly affected when software architecture failure and mistakes occur. Architectural mistakes are caused by the perspectives architects have on the design and realization technologies they use for a project. Yet, taking the right perspective is actually pretty difficult as it can differ from project to project. " What's the simplest solution that could possibly work?" is the question architects must ask. Design tactics is a methodology architects can use in making and challenging their concrete design decisions. Only architectures with solutions that are easy to develop, maintain, deploy, configure, and use intuitively can result in affordable systems with high user acceptance.},
	number = {2},
	journal = {IEEE Software},
	author = {Buschmann, Frank},
	month = mar,
	year = {2010},
	note = {Conference Name: IEEE Software},
	keywords = {Concrete, Nails, Software architecture, architecture, design quality, design simplicity, design tactics, software},
	pages = {49--51},
}

@article{buschmann_learning_2010-1,
	title = {Learning from {Failure}, {Part} 2: {Featuritis}, {Performitis}, and {Other} {Diseases}},
	volume = {27},
	issn = {1937-4194},
	shorttitle = {Learning from {Failure}, {Part} 2},
	doi = {10.1109/MS.2010.14},
	abstract = {In the first part of this article, the author analyzed some common software architecture mistakes. In this article, the author discussed and explored the three mistakes that most architects know all too well. The author and his architect colleague Klaus Marquardt named these mistakes as if they were diseases: featuritis, flexibilitis, and performitis.},
	number = {1},
	journal = {IEEE Software},
	author = {Buschmann, Frank},
	month = jan,
	year = {2010},
	note = {Conference Name: IEEE Software},
	keywords = {Diseases, Software architecture, architecture quality, feature coverage, flexibility, performance, walking skeletons},
	pages = {10--11},
}

@article{buschmann_learning_2009,
	title = {Learning from {Failure}, {Part} 1: {Scoping} and {Requirements} {Woes}},
	volume = {26},
	issn = {1937-4194},
	shorttitle = {Learning from {Failure}, {Part} 1},
	doi = {10.1109/MS.2009.179},
	abstract = {The paper is an editorial on software architecture. Software projects fail for the same reasons. The mistakes that can lead software projects to trouble before concrete architecture elaboration even begins include missing, wrong, or creeping system scope; and vague, unnecessary, or extreme nonfunctional requirements. These mistakes aren't the prime responsibility of architects, but architects are directly affected if they occur because without an appropriate system scope and correspondingly appropriate requirements, they can't define sustainable architectures. A system's scope defines its responsibilities, but also its boundaries. Failing to define a precise system scope can result in architectures that support the wrong functionality, too much functionality, too many functionality variations, too few functions, or poor quality. Architects should pay special attention to nonfunctional requirements that too often include vague or unnecessarily extreme specifications. Without precision, architects must guess which nonfunctional qualities are actually needed, and if they must guess, they'll likely guess wrong. An agile, incremental approach to software development define an initial system scope and set of requirements in a reasonable time and adjust this big picture step-wise until it has enough focus, substance, and clarity. Then, architects get concrete guidance for their work and can act rather than react when designing the system's architecture. Only then do architects receive a safety network that allows them to identify and correct design flaws in their own area of responsibility.},
	number = {6},
	journal = {IEEE Software},
	author = {Buschmann, Frank},
	month = nov,
	year = {2009},
	note = {Conference Name: IEEE Software},
	keywords = {Computer architecture, Concrete, Programming, Safety, Software architecture, functionality, nonfunctional requirements, requirements engineering, software architect, software architecture, software engineering, system scope},
	pages = {68--69},
}

@article{baldoni_survey_2018,
	title = {A {Survey} of {Symbolic} {Execution} {Techniques}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3182657},
	doi = {10.1145/3182657},
	abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program’s authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the past four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience.},
	number = {3},
	urldate = {2022-01-27},
	journal = {ACM Computing Surveys},
	author = {Baldoni, Roberto and Coppa, Emilio and D’elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
	month = may,
	year = {2018},
	keywords = {Symbolic execution, concolic execution, software testing, static analysis},
	pages = {50:1--50:39},
}

@inproceedings{yun_qsym_2018,
	title = {\{{QSYM}\} : {A} {Practical} {Concolic} {Execution} {Engine} {Tailored} for {Hybrid} {Fuzzing}},
	isbn = {978-1-939133-04-5},
	shorttitle = {\{{QSYM}\}},
	url = {https://www.usenix.org/conference/usenixsecurity18/presentation/yun},
	language = {en},
	urldate = {2022-01-27},
	author = {Yun, Insu and Lee, Sangho and Xu, Meng and Jang, Yeongjin and Kim, Taesoo},
	year = {2018},
	pages = {745--761},
}

@inproceedings{stephens_driller_2016,
	address = {San Diego, CA},
	title = {Driller: {Augmenting} {Fuzzing} {Through} {Selective} {Symbolic} {Execution}},
	isbn = {978-1-891562-41-9},
	shorttitle = {Driller},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2017/09/driller-augmenting-fuzzing-through-selective-symbolic-execution.pdf},
	doi = {10.14722/ndss.2016.23368},
	abstract = {Memory corruption vulnerabilities are an everpresent risk in software, which attackers can exploit to obtain unauthorized access to conﬁdential information. As products with access to sensitive data are becoming more prevalent, the number of potentially exploitable systems is also increasing, resulting in a greater need for automated software vetting tools. DARPA recently funded a competition, with millions of dollars in prize money, to further research focusing on automated vulnerability ﬁnding and patching, showing the importance of research in this area. Current techniques for ﬁnding potential bugs include static, dynamic, and concolic analysis systems, which each having their own advantages and disadvantages. A common limitation of systems designed to create inputs which trigger vulnerabilities is that they only ﬁnd shallow bugs and struggle to exercise deeper paths in executables.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Proceedings 2016 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Stephens, Nick and Grosen, John and Salls, Christopher and Dutcher, Andrew and Wang, Ruoyu and Corbetta, Jacopo and Shoshitaishvili, Yan and Kruegel, Christopher and Vigna, Giovanni},
	year = {2016},
}

@misc{noauthor_under-constrained_nodate,
	title = {Under-{Constrained} {Symbolic} {Execution}: {Correctness} {Checking} for {Real} {Code} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/ramos},
	urldate = {2022-01-27},
}

@article{hofmann_requirements_2001,
	title = {Requirements engineering as a success factor in software projects},
	volume = {18},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/936219/},
	doi = {10.1109/MS.2001.936219},
	language = {en},
	number = {4},
	urldate = {2022-01-26},
	journal = {IEEE Software},
	author = {Hofmann, H.F. and Lehner, F.},
	month = jul,
	year = {2001},
	pages = {58--66},
}

@article{iqbal_requirements_2020,
	title = {Requirements engineering issues causing software development outsourcing failure},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0229785},
	doi = {10.1371/journal.pone.0229785},
	abstract = {Software development outsourcing is becoming more and more famous because of the advantages like cost abatement, process enhancement, and coping with the scarcity of needed resources. Studies confirm that unfortunately a large proportion of the software development outsourcing projects fails to realize anticipated benefits. Investigations into the failures of such projects divulge that in several cases software development outsourcing projects are failed because of the issues that are associated with requirements engineering process. The objective of this study is the identification and the ranking of the commonly occurring issues of the requirements engineering process in the case of software development outsourcing. For this purpose, contemporary literature has been assessed rigorously, issues faced by practitioners have been identified and three questionnaire surveys have been organized by involving experienced software development outsourcing practitioners. The Delphi technique, cut-off value method and 50\% rule have also been employed. The study explores 150 issues (129 issues from literature and 21 from industry) of requirements engineering process for software development outsourcing, groups the 150 issues into 7 identified categories and then extricates 43 customarily or commonly arising issues from the 150 issues. Founded on ‘frequency of occurrence’ the 43 customarily arising issues have been ranked with respect to respective categories (category-wise ranking) and with respect to all the categories (overall ranking). Categories of the customarily arising issues have also been ranked. The issues’ identification and ranking contribute to design proactive software project management plan for dealing with software development outsourcing failures and attaining conjectured benefits of the software development outsourcing.},
	language = {en},
	number = {4},
	urldate = {2022-01-26},
	journal = {PLOS ONE},
	author = {Iqbal, Javed and Ahmad, Rodina B. and Khan, Muzafar and {Fazal-e-Amin} and Alyahya, Sultan and Nizam Nasir, Mohd Hairul and Akhunzada, Adnan and Shoaib, Muhammad},
	editor = {Xin, Baogui},
	month = apr,
	year = {2020},
	keywords = {outsourcing},
	pages = {e0229785},
}

@article{peslak_review_2007,
	title = {A {Review} of the {Impact} of {Acm} {Code} of {Conduct} on {Information} {Technology} {Moral} {Judgment} and {Intent}},
	volume = {47},
	copyright = {Copyright International Association for Computer Information Systems Spring 2007},
	issn = {08874417},
	url = {https://www.proquest.com/docview/232575060/abstract/8B06FB3924724E2EPQ/1},
	abstract = {One of the most widely recognized code of ethics in information technology (IT) is the ACM (Association for Computing Machinery) Code of Ethics. Adopted in 1992, the code covers many of the key ethical areas that are encountered in information technology practice. But this code has been lightly studied in the literature, including its recognition and its acceptance. Likewise, its effectiveness in influencing moral intent has not previously been established. This manuscript reviews selected key statements from the ACM Code of Ethics to determine the level of agreement with these statements. The surveyed group includes IT students, faculty, and staff. In general, agreement on all issues is found, though varying in degree. Next, the study analyzes the relationship between ACM code agreement and ethical intent. The relationships in nearly all cases are positive and significant. Finally, it examines the influence this code has among participants in a hypothetical hostile work situation. Specifically, it examines moral intent when a supervisor recommends an action directly contrary to the code. The difference between actions in a hostile situation versus no supervisor influence is found to be significant in some cases. [PUBLICATION ABSTRACT]},
	language = {English},
	number = {3},
	urldate = {2022-01-26},
	journal = {The Journal of Computer Information Systems},
	author = {Peslak, Alan R.},
	year = {2007},
	note = {Num Pages: 10
Place: Stillwater, United Kingdom
Publisher: Taylor \& Francis Ltd.},
	keywords = {Agreements, Behavior, Codes, Computers, Decision making, Education, Education--Computer Applications, Ethics, Impact analysis, Influence, Information technology, Judgment, Professional ethics, Professionals, Studies},
	pages = {1--10},
}

@inproceedings{lloyd_iec_2009,
	address = {London, UK},
	title = {{IEC} 61508 and {IEC} 61511 assessments - some lessons learned},
	isbn = {978-1-84919-195-1},
	url = {https://digital-library.theiet.org/content/conferences/10.1049/cp.2009.1540},
	doi = {10.1049/cp.2009.1540},
	abstract = {In recent years we have conducted about 25 assessments using IEC 61508 or IEC 61511, working mainly to Safety Integrity Level (SIL) 2, but on some occasions to SIL 3. In this paper we present some of the lessons we have learned and offer advice to those seeking certification for components, systems or generic process capability. We cover the three main parts of the IEC 61508 standard: Functional Safety (FS) Management; Hardware; Software. More recently, our work has included software products whose assessment has entailed building complex arguments for their compliance. This has led us to use argument structuring techniques that we comment on at the end of this paper.},
	language = {en},
	urldate = {2022-01-19},
	booktitle = {4th {IET} {International} {Conference} on {Systems} {Safety} 2009. {Incorporating} the {SaRS} {Annual} {Conference}},
	publisher = {IET},
	author = {Lloyd, M.H. and Reeve, P.J.},
	year = {2009},
	pages = {2A1--2A1},
}

@article{noauthor_ecfr_nodate,
	title = {{eCFR} :: 21 {CFR} {Part} 820 -- {Quality} {System} {Regulation}},
	language = {en},
	journal = {CFR Part},
	pages = {14},
}

@inproceedings{chen_towards_2016,
	title = {Towards {Automated} {Dynamic} {Analysis} for {Linux}-based {Embedded} {Firmware}},
	doi = {10.14722/ndss.2016.23415},
	author = {Chen, Daming and Egele, Manuel and Woo, Maverick and Brumley, David},
	month = jan,
	year = {2016},
}

@article{yin_firmhunter_2021,
	title = {{FirmHunter}: {State}-{Aware} and {Introspection}-{Driven} {Grey}-{Box} {Fuzzing} towards {IoT} {Firmware}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {{FirmHunter}},
	url = {https://www.mdpi.com/2076-3417/11/19/9094},
	doi = {10.3390/app11199094},
	abstract = {IoT devices are exponentially increasing in all aspects of our lives. Via the web interfaces of IoT devices, attackers can control IoT devices by exploiting their vulnerabilities. In order to guarantee IoT security, testing these IoT devices to detect vulnerabilities is very important. In this work, we present FirmHunter, an automated state-aware and introspection-driven grey-box fuzzer towards Linux-based firmware images on the basis of emulation. It employs a message-state queue to overcome the dependency problem in test cases. Furthermore, it implements a scheduler collecting execution information from system introspection to drive fuzzing towards more interesting test cases, which speeds up vulnerability discovery. We evaluate FirmHunter by emulating and fuzzing eight firmware images including seven routers and one IP camera with a state-of-the-art IoT fuzzer FirmFuzz and a web application scanner ZAP. Our evaluation results show that (1) the message-state queue enables FirmHunter to parse the dependencies in test cases and find real-world vulnerabilities that other fuzzers cannot detect; (2) our scheduler accelerates the discovery of vulnerabilities by an average of 42\%; and (3) FirmHunter is able to find unknown vulnerabilities.},
	language = {en},
	number = {19},
	urldate = {2022-01-18},
	journal = {Applied Sciences},
	author = {Yin, Qidi and Zhou, Xu and Zhang, Hangwei},
	month = jan,
	year = {2021},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {IoT, firmware, fuzzing, vulnerability},
	pages = {9094},
}

@inproceedings{srivastava_firmfuzz_2019,
	address = {New York, NY, USA},
	series = {{IoT} {S}\&amp;{P}'19},
	title = {{FirmFuzz}: {Automated} {IoT} {Firmware} {Introspection} and {Analysis}},
	isbn = {978-1-4503-6838-4},
	shorttitle = {{FirmFuzz}},
	url = {https://doi.org/10.1145/3338507.3358616},
	doi = {10.1145/3338507.3358616},
	abstract = {While the number of IoT devices grows at an exhilarating pace their security remains stagnant. Imposing secure coding standards across all vendors is infeasible. Testing individual devices allows an analyst to evaluate their security post deployment. Any discovered vulnerabilities can then be disclosed to the vendors in order to assist them in securing their products. The search for vulnerabilities should ideally be automated for efficiency and furthermore be device-independent for scalability. We present FirmFuzz, an automated device-independent emulation and dynamic analysis framework for Linux-based firmware images. It employs a greybox-based generational fuzzing approach coupled with static analysis and system introspection to provide targeted and deterministic bug discovery within a firmware image. We evaluate FirmFuzz by emulating and dynamically analyzing 32 images (from 27 unique devices) with a network accessible from the host performing the emulation. During testing, FirmFuzz discovered seven previously undisclosed vulnerabilities across six different devices: two IP cameras and four routers. So far, 4 CVE's have been assigned.},
	urldate = {2022-01-18},
	booktitle = {Proceedings of the 2nd {International} {ACM} {Workshop} on {Security} and {Privacy} for the {Internet}-of-{Things}},
	publisher = {Association for Computing Machinery},
	author = {Srivastava, Prashast and Peng, Hui and Li, Jiahao and Okhravi, Hamed and Shrobe, Howard and Payer, Mathias},
	month = nov,
	year = {2019},
	keywords = {dynamic analysis, firmware testing, fuzzing, iot, vulnerability analysis},
	pages = {15--21},
}

@article{haque_well_2021,
	title = {Well {Begun} is {Half} {Done}: {An} {Empirical} {Study} of {Exploitability} \& {Impact} of {Base}-{Image} {Vulnerabilities}},
	shorttitle = {Well {Begun} is {Half} {Done}},
	url = {http://arxiv.org/abs/2112.12597},
	abstract = {Container technology, (e.g., Docker) is being widely adopted for deploying software infrastructures or applications in the form of container images. Security vulnerabilities in the container images are a primary concern for developing containerized software. Exploitation of the vulnerabilities could result in disastrous impact, such as loss of confidentiality, integrity, and availability of containerized software. Understanding the exploitability and impact characteristics of vulnerabilities can help in securing the configuration of containerized software. However, there is a lack of research aimed at empirically identifying and understanding the exploitability and impact of vulnerabilities in container images. We carried out an empirical study to investigate the exploitability and impact of security vulnerabilities in base-images and their prevalence in open-source containerized software. We considered base-images since container images are built from base-images that provide all the core functionalities to build and operate containerized software. We discovered and characterized the exploitability and impact of security vulnerabilities in 261 base-images, which are the origin of 4,681 actively maintained official container images in the largest container registry, i.e., Docker Hub. To characterize the prevalence of vulnerable base-images in real-world projects, we analysed 64,579 containerized software from GitHub. Our analysis of a set of \$1,983\$ unique base-image security vulnerabilities revealed 13 novel findings. These findings are expected to help developers to understand the potential security problems related to base-images and encourage them to investigate base-images from security perspective before developing their applications.},
	urldate = {2022-01-18},
	journal = {arXiv:2112.12597 [cs]},
	author = {Haque, Mubin Ul and Babar, M. Ali},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.12597},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@article{zhang_general_2020,
	title = {A {General} {Framework} to {Understand} {Vulnerabilities} in {Information} {Systems}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3006361},
	abstract = {Firms and organizations are increasingly facing security issues related to vulnerabilities in their information systems. Firms, especially small and medium-sized enterprises, usually have very limited security resources and thus have difficulty understanding vulnerabilities and fixing them accordingly. This study aims to build a general framework that can help firms understand the characteristics of vulnerabilities in information systems: for instance, what category a specific vulnerability belongs to, what potential risks it poses, and what the key clues are to addressing it. To this end, we collect data on real vulnerabilities that have emerged in firms' information systems from a popular vulnerability report platform. Features are extracted at four different levels, namely, the word, phrase, topic, and record levels. The experimental results show that the general framework helps characterize the modes and patterns of various types of vulnerabilities. This study contributes to the security literature by providing a deeper understanding of the characteristics of vulnerabilities and their related suggested solutions. Firms can apply this framework to ensure information security.},
	journal = {IEEE Access},
	author = {Zhang, Xiong and Xie, Haoran and Yang, Hao and Shao, Hongkai and Zhu, Minghao},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Analytical models, Classification, Databases, Information security, Software, Testing, information security, risk-level prediction, topic analysis, vulnerability},
	pages = {121858--121873},
}

@inproceedings{mesa_understanding_2018,
	address = {New York, NY, USA},
	series = {{SPLC} '18},
	title = {Understanding vulnerabilities in plugin-based web systems: an exploratory study of wordpress},
	isbn = {978-1-4503-6464-5},
	shorttitle = {Understanding vulnerabilities in plugin-based web systems},
	url = {http://doi.org/10.1145/3233027.3233042},
	doi = {10.1145/3233027.3233042},
	abstract = {A common software product line strategy involves plugin-based web systems that support simple and quick incorporation of custom behaviors. As a result, they have been widely adopted to create web-based applications. Indeed, the popularity of ecosystems that support plugin-based development (e.g., WordPress) is largely due to the number of customization options available as community-contributed plugins. However, plugin-related vulnerabilities tend to be recurrent, exploitable and hard to be detected and may lead to severe consequences for the customized product. Hence, there is a need to further understand such vulnerabilities to enable preventing relevant security threats. Therefore, we conducted an exploratory study to characterize vulnerabilities caused by plugins in web-based systems. To this end, we went over WordPress vulnerability bulletins cataloged by the National Vulnerability Database as well as associated patches maintained by the WordPress plugins repository. We identified the main types of vulnerabilities caused by plugins as well as their impact and the size of the patch to fix the vulnerability. Moreover, we identified the most common security-related topics discussed among WordPress developers. We observed that, while plugin-related vulnerabilities may have severe consequences and might remain unnoticed for years before being fixed, they can commonly be mitigated with small and localized changes to the source code. The characterization helps to provide an understanding on how typical plugin-based vulnerabilities manifest themselves in practice. Such information can be helpful to steer future research on plugin-based vulnerability detection and prevention.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 22nd {International} {Systems} and {Software} {Product} {Line} {Conference} - {Volume} 1},
	publisher = {Association for Computing Machinery},
	author = {Mesa, Oslien and Vieira, Reginaldo and Viana, Marx and Durelli, Vinicius H. S. and Cirilo, Elder and Kalinowski, Marcos and Lucena, Carlos},
	month = sep,
	year = {2018},
	keywords = {exploratory study, plugin-based web systems, security, software product lines},
	pages = {149--159},
}

@inproceedings{li_large-scale_2017,
	address = {New York, NY, USA},
	series = {{CCS} '17},
	title = {A {Large}-{Scale} {Empirical} {Study} of {Security} {Patches}},
	isbn = {978-1-4503-4946-8},
	url = {http://doi.org/10.1145/3133956.3134072},
	doi = {10.1145/3133956.3134072},
	abstract = {Given how the "patching treadmill" plays a central role for enabling sites to counter emergent security concerns, it behooves the security community to understand the patch development process and characteristics of the resulting fixes. Illumination of the nature of security patch development can inform us of shortcomings in existing remediation processes and provide insights for improving current practices. In this work we conduct a large-scale empirical study of security patches, investigating more than 4,000 bug fixes for over 3,000 vulnerabilities that affected a diverse set of 682 open-source software projects. For our analysis we draw upon the National Vulnerability Database, information scraped from relevant external references, affected software repositories, and their associated security fixes. Leveraging this diverse set of information, we conduct an analysis of various aspects of the patch development life cycle, including investigation into the duration of impact a vulnerability has on a code base, the timeliness of patch development, and the degree to which developers produce safe and reliable fixes. We then characterize the nature of security fixes in comparison to other non-security bug fixes, exploring the complexity of different types of patches and their impact on code bases. Among our findings we identify that: security patches have a lower footprint in code bases than non-security bug patches; a third of all security issues were introduced more than 3 years prior to remediation; attackers who monitor open-source repositories can often get a jump of weeks to months on targeting not-yet-patched systems prior to any public disclosure and patch distribution; nearly 5\% of security fixes negatively impacted the associated software; and 7\% failed to completely remedy the security hole they targeted.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Li, Frank and Paxson, Vern},
	month = oct,
	year = {2017},
	keywords = {empirical study, patch complexity, security patches, vulnerabilities},
	pages = {2201--2215},
}

@inproceedings{elia_analysis_2017,
	title = {An {Analysis} of {OpenStack} {Vulnerabilities}},
	doi = {10.1109/EDCC.2017.29},
	abstract = {Cloud management frameworks provide an effective way to deploy and manage the hardware, storage and network resources for supporting critical cloud infrastructures. OpenStack is used in the context of business critical systems and frequently deals with highly sensitive resources, where a security breach may result in severe damage, including information theft or financial losses. Despite this, there is little information on how much security is a concern during design and implementation of OpenStack components. This work analyses 5 years of security reports on OpenStack and the corresponding patches, with the goal of characterizing the most frequent vulnerabilities, how they can be exploited, and their root causes. The goal is to identify vulnerability trends, characterize frequent threats, and shed some light on the overall security of OpenStack. Special focus is placed on the framework component for virtualization management (Nova), by also analyzing the code of the available patches. Overall results show a preponderance of vulnerabilities that may be exploited to cause DoS and expose sensitive information. Also, 2/3 of the total number of vulnerabilities can be exploited by insider attacks, urging administrators to focus protection efforts on them. Finally, many bugs remain undetected for long periods when most of them are easy to avoid or detect and correct.},
	booktitle = {2017 13th {European} {Dependable} {Computing} {Conference} ({EDCC})},
	author = {Elia, Ivano Alessandro and Antunes, Nuno and Laranjeiro, Nuno and Vieira, Marco},
	month = sep,
	year = {2017},
	keywords = {Authentication, Cloud, Cloud computing, Computer bugs, Hardware, Market research, OpenStack, Security, Vulnerabilities},
	pages = {129--134},
}

@inproceedings{perez-botero_characterizing_2013,
	address = {New York, NY, USA},
	series = {Cloud {Computing} '13},
	title = {Characterizing hypervisor vulnerabilities in cloud computing servers},
	isbn = {978-1-4503-2067-2},
	url = {http://doi.org/10.1145/2484402.2484406},
	doi = {10.1145/2484402.2484406},
	abstract = {The rise of the Cloud Computing paradigm has led to security concerns, taking into account that resources are shared and mediated by a Hypervisor which may be targeted by rogue guest VMs and remote attackers. In order to better define the threats to which a cloud server's Hypervisor is exposed, we conducted a thorough analysis of the codebase of two popular open-source Hypervisors, Xen and KVM, followed by an extensive study of the vulnerability reports associated with them. Based on our findings, we propose a characterization of Hypervisor Vulnerabilities comprised of three dimensions: the trigger source (i.e. where the attacker is located), the attack vector (i.e. the Hypervisor functionality that enables the security breach), and the attack target (i.e. the runtime domain that is compromised). This can be used to understand potential paths different attacks can take, and which vulnerabilities enable them. Moreover, most common paths can be discovered to learn where the defenses should be focused, or conversely, least common paths can be used to find yet-unexplored ways attackers may use to get into the system.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2013 international workshop on {Security} in cloud computing},
	publisher = {Association for Computing Machinery},
	author = {Perez-Botero, Diego and Szefer, Jakub and Lee, Ruby B.},
	month = may,
	year = {2013},
	keywords = {attack vectors, hypervisor vulnerabilities, secure cloud computing, virtualization, vulnerability categorization},
	pages = {3--10},
}

@inproceedings{cerdeira_sok_2020,
	title = {{SoK}: {Understanding} the {Prevailing} {Security} {Vulnerabilities} in {TrustZone}-assisted {TEE} {Systems}},
	shorttitle = {{SoK}},
	doi = {10.1109/SP40000.2020.00061},
	abstract = {Hundreds of millions of mobile devices worldwide rely on Trusted Execution Environments (TEEs) built with Arm TrustZone for the protection of security-critical applications (e.g., DRM) and operating system (OS) components (e.g., Android keystore). TEEs are often assumed to be highly secure; however, over the past years, TEEs have been successfully attacked multiple times, with highly damaging impact across various platforms. Unfortunately, these attacks have been possible by the presence of security flaws in TEE systems. In this paper, we aim to understand which types of vulnerabilities and limitations affect existing TrustZone-assisted TEE systems, what are the main challenges to build them correctly, and what contributions can be borrowed from the research community to overcome them. To this end, we present a security analysis of popular TrustZone-assisted TEE systems (targeting Cortex-A processors) developed by Qualcomm, Trustonic, Huawei, Nvidia, and Linaro. By studying publicly documented exploits and vulnerabilities as well as by reverse engineering the TEE firmware, we identified several critical vulnerabilities across existing systems which makes it legitimate to raise reasonable concerns about the security of commercial TEE implementations.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Cerdeira, David and Santos, Nuno and Fonseca, Pedro and Pinto, Sandro},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Androids, Arm, Computer bugs, Hardware, Humanoid robots, Kernel, Linux, Security, Security Vulnerabilities, TEE, TrustZone},
	pages = {1416--1432},
}

@misc{inc_threatscope_2020,
	title = {{ThreatSCOPE}: {Addressing} software vulnerability in embedded systems},
	shorttitle = {{ThreatSCOPE}},
	url = {https://researchoutreach.org/articles/threatscope-addressing-software-vulnerability-embedded-systems/},
	abstract = {developed ThreatSCOPE, a system assurance tool that analyses and mitigates software vulnerabilities and cyber threats in embedded systems.},
	language = {en-GB},
	urldate = {2022-01-17},
	journal = {Research Outreach},
	author = {Inc, Contact Details Address: BlueRISC and Street, 400 Amity and Amherst, Suites 0-1-3-4},
	month = dec,
	year = {2020},
}

@inproceedings{ferrante_parallel_2012,
	address = {Berlin, Heidelberg},
	series = {{SAFECOMP}'12},
	title = {Parallel {NuSMV}: a {NuSMV} extension for the verification of complex embedded systems},
	isbn = {978-3-642-33674-4},
	shorttitle = {Parallel {NuSMV}},
	url = {https://doi.org/10.1007/978-3-642-33675-1_38},
	doi = {10.1007/978-3-642-33675-1_38},
	abstract = {In this paper we present Parallel NuSMV, a tool based on the NuSMV model checker that integrates the ManySAT parallel SAT solver. The PNuSMV is part of the FormalSpecs Verifier framework for the formal verification of Simulink/Stateflow models. The experiments we performed show that the use of a parallel SAT solver allows for an average speedup of an order of magnitude or more on industry-level size models. The main contributions of the papers are (1) the description of the PNuSMV model checker (2) the description of the verification time speedup w.r.t. the NuSMV tool for the verification of industrial-sized embedded systems and (3) the integration of the tool in the FormalSpecs Verifier framework for the verification of Simulink/Stateflow models with the application to a cruise control case study.},
	urldate = {2022-01-17},
	booktitle = {Proceedings of the 2012 international conference on {Computer} {Safety}, {Reliability}, and {Security}},
	publisher = {Springer-Verlag},
	author = {Ferrante, Orlando and Benvenuti, Luca and Mangeruca, Leonardo and Sofronis, Christos and Ferrari, Alberto},
	month = sep,
	year = {2012},
	keywords = {contract-based design, embedded systems, formal verification, model checking},
	pages = {409--416},
}

@inproceedings{andrade_test_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Test {Case} {Generation} of {Embedded} {Real}-{Time} {Systems} with {Interruptions} for {FreeRTOS}},
	isbn = {978-3-642-10452-7},
	doi = {10.1007/978-3-642-10452-7_5},
	abstract = {This paper discusses issues raised in the construction of test models and automatic generation of test cases for embedded real-time systems with interruptions that can run on the FreeRTOS operating system. The focus is on the use of symbolic transition systems (STSs) as the formalism from which test cases are generated by using the STG tool. The solution presented considers a test case execution model for real-time systems with interruptions that can be based on the integrated use of FreeRTOS components. A case study is presented to illustrate all steps from the construction of the test model to test case generation.},
	language = {en},
	booktitle = {Formal {Methods}: {Foundations} and {Applications}},
	publisher = {Springer},
	author = {Andrade, Wilkerson L. and Machado, Patrícia D. L. and Alves, Everton L. G. and Almeida, Diego R.},
	editor = {Oliveira, Marcel Vinícius Medeiros and Woodcock, Jim},
	year = {2009},
	keywords = {Alarm System, Handler Task, Interruption Behaviour, Sequence Diagram, Test Case Generation},
	pages = {54--69},
}

@misc{noauthor_create_nodate,
	title = {Create a short {URL} - {Google} {Help}},
	url = {https://support.google.com/faqs/answer/190768?hl=en},
	urldate = {2022-01-17},
}

@book{augustine2022enterprise,
	title = {Enterprise digital transformation: {Technology}, tools, and use cases},
	isbn = {978-1-00-054053-6},
	url = {https://books.google.com/books?id=2tpXEAAAQBAJ},
	publisher = {CRC Press},
	author = {Augustine, P. and Raj, P. and Munirathinam, S.},
	year = {2022},
}

@article{mullainathan_media_2002,
	title = {Media bias},
	abstract = {There are two different types of media bias. One bias, which we refer to as ideology, reflects a news outlet's desire to affect reader opinions in a particular direction. The second bias, which we refer to as spin, reflects the outlet's attempt to simply create a memorable story. We examine competition among media outlets in the presence of these biases. Whereas competition can eliminate the effect of ideological bias, it actually exaggerates the incentive to spin stories.},
	language = {en},
	author = {Mullainathan, Sendhil and Shleifer, Andrei},
	year = {2002},
	pages = {27},
}

@article{gao_em-fuzz_2020,
	title = {{EM}-{Fuzz}: {Augmented} {Firmware} {Fuzzing} via {Memory} {Checking}},
	volume = {39},
	issn = {1937-4151},
	shorttitle = {{EM}-{Fuzz}},
	doi = {10.1109/TCAD.2020.3013046},
	abstract = {Embedded systems are increasingly interconnected in the emerging application scenarios. Many of these applications are safety critical, making it a high priority to ensure that the systems are free from malicious attacks. This work aims to detect vulnerabilities, that could be exploited by adversaries to compromise functional correctness, in the embedded firmware, which is challenging especially due to the absence of source code. In particular, we propose EM-Fuzz, a firmware vulnerability detection technique that tightly integrates fuzzing with real-time memory checking. Based on the memory instrumentation, the firmware fuzzing can not only be guided by the traditional branch coverage to generate high-quality seeds to explore hard-to-reach regions but also by the recorded memory sensitive operations to continuously exercise sensitive regions which are prone to being attacked. More importantly, the instrumentation integrates real-time memory checkers to expose memory vulnerabilities, which is not well-supported by existing fuzzers without source code. The experiments on several real-world embedded firmware such as OpenSSL demonstrate that EM-Fuzz significantly improves the performance of state-of-the-art fuzzing tools, such as AFL and AFLFast, with the coverage improvements of 93.98\% and 46.89\%, respectively. Furthermore, EM-Fuzz exposes a total of 23 vulnerabilities, with an average of about 7-h per vulnerability. AFL and AFLFast together find 10 vulnerabilities, costing about 13 h and 10-h per vulnerability on average, respectively. Out of these 23 vulnerabilities, 16 are previously unknown and have been reported to the upstream product vendors, 7 of which have been assigned with unique CVE identifiers in the U.S. National Vulnerability Database.},
	number = {11},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Gao, Jian and Xu, Yiwen and Jiang, Yu and Liu, Zhe and Chang, Wanli and Jiao, Xun and Sun, Jiaguang},
	month = nov,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Design automation, Embedded firmware, Fuzzing, Instruments, Integrated circuits, Memory management, Micromechanical devices, Tools, guided fuzzing, memory checking, vulnerability},
	pages = {3420--3432},
}

@inproceedings{makhshari_iot_2021,
	title = {{IoT} {Development} {In} {The} {Wild}: {Bug} {Taxonomy} and {Developer} {Challenges}},
	shorttitle = {{IoT} {Development} {In} {The} {Wild}},
	doi = {10.1109/ICSE-Companion52605.2021.00103},
	abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Makhshari, Amir and Mesbah, Ali},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	keywords = {Computer bugs, Correlation, Empirical-Study, Faces, Internet-of-Things, Mining-Software-Repositories, Software engineering, Software-Engineering, Systematics, Taxonomy, Tools},
	pages = {225--226},
}

@inproceedings{fontana_evaluating_2021,
	title = {Evaluating the {Architectural} {Debt} of {IoT} {Projects}},
	doi = {10.1109/SERP4IoT52556.2021.00011},
	abstract = {We observed a great and increasing interest in the last few years towards the evaluation of technical debt of software projects, in particular in the direction of code and architectural debt evaluation. This kind of analysis has not yet been performed for IoT projects. Hence, in this paper we start this exploration through the analysis of four Open Source IoT projects. We focus our attention on architectural debt and we exploit a tool, called Arcan, developed for architectural smell detection and for the computation of an architectural debt index. The results show that also IoT projects are subjected to architectural debt, and in particular to the presence of Cyclic Dependency and Unstable Dependency smells. However, there is evidence that the continuous refactoring of the code helps in avoiding the increase of debt, hence also developers of IoT projects should schedule periodical clean-ups of their code.},
	booktitle = {2021 {IEEE}/{ACM} 3rd {International} {Workshop} on {Software} {Engineering} {Research} and {Practices} for the {IoT} ({SERP4IoT})},
	author = {Fontana, Francesca Arcelli and Pigazzini, Ilaria},
	month = jun,
	year = {2021},
	keywords = {Architectural Debt, Architectural Debt Index, Architectural Smells, Conferences, Indexes, IoT Projects, Schedules, Software, Software engineering, Tools},
	pages = {27--31},
}

@misc{baker_wired_2020,
	title = {Wired {Magazine}},
	url = {https://ischoolwikis.sjsu.edu/lispublications/wiki/civilian-publications/wired-magazine/},
	language = {en-US},
	urldate = {2022-01-12},
	journal = {Library \& Information Science Publications Wiki},
	author = {Baker, Bethany},
	year = {2020},
}

@misc{noauthor_credibility_2021,
	title = {Credibility of major news organizations in the {U}.{S}. 2021},
	url = {https://www.statista.com/statistics/239784/credibility-of-major-news-organizations-in-the-us/},
	abstract = {According to a survey held among adults in the United States in May 2021, ABC was considered to be the most credible news source in the country, with 58 percent of respondents believing the organization to be very or somewhat credible.},
	language = {en},
	urldate = {2022-01-12},
	journal = {Statista},
	year = {2021},
}

@book{storey_safety_1996,
	address = {USA},
	title = {Safety {Critical} {Computer} {Systems}},
	isbn = {978-0-201-42787-5},
	abstract = {From the Publisher: Increasingly, microcomputers are being used in applications where their correct operation is vital to ensure the safety of the public and the environment: from anti-lock braking systems in automobiles, to fly-by-wire aircraft, to shut-down systems at nuclear power plants. It is, therefore, vital that engineers are aware of the safety implications of the systems they develop. This book is an introduction to the field of safety-critical computer systems, and is written for any engineer who uses microcomputers within real-time embedded systems. It assumes no prior knowledge of safety, or of any specific computer hardware or programming language. This book covers all phases of the life of a safety-critical system from its conception and specification, through to its certification, installation, service and decommissioning; provides information on how to assess the safety implications of projects, and determine the measures necessary to develop systems to meet safety needs; gives a thorough grounding in the techniques available to investigate the safety aspects of computer-based systems and the methods that may be used to enhance their dependability; and uses case studies and worked examples from a wide range of industrial sectors including the nuclear, aircraft, automotive and consumer products industries. This text is intended for both engineering and computer science students, and for practising engineers within computer-related industries. The approach taken is equally suited to engineers who consider computers from a hardware, software or systems viewpoint.},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Storey, Neil R.},
	year = {1996},
	keywords = {IoTFailurePaper},
}

@article{holm_empirical_2012,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@inproceedings{clements_halucinator_2020,
	title = {\{{HALucinator}\}: {Firmware} {Re}-hosting {Through} {Abstraction} {Layer} {Emulation}},
	isbn = {978-1-939133-17-5},
	shorttitle = {\{{HALucinator}\}},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/clements},
	language = {en},
	urldate = {2022-01-12},
	author = {Clements, Abraham A. and Gustafson, Eric and Scharnowski, Tobias and Grosen, Paul and Fritz, David and Kruegel, Christopher and Vigna, Giovanni and Bagchi, Saurabh and Payer, Mathias},
	year = {2020},
	pages = {1201--1218},
}

@article{holm_empirical_2012-1,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@inproceedings{bilge_before_2012,
	address = {New York, NY, USA},
	series = {{CCS} '12},
	title = {Before we knew it: an empirical study of zero-day attacks in the real world},
	isbn = {978-1-4503-1651-4},
	shorttitle = {Before we knew it},
	url = {http://doi.org/10.1145/2382196.2382284},
	doi = {10.1145/2382196.2382284},
	abstract = {Little is known about the duration and prevalence of zero-day attacks, which exploit vulnerabilities that have not been disclosed publicly. Knowledge of new vulnerabilities gives cyber criminals a free pass to attack any target of their choosing, while remaining undetected. Unfortunately, these serious threats are difficult to analyze, because, in general, data is not available until after an attack is discovered. Moreover, zero-day attacks are rare events that are unlikely to be observed in honeypots or in lab experiments. In this paper, we describe a method for automatically identifying zero-day attacks from field-gathered data that records when benign and malicious binaries are downloaded on 11 million real hosts around the world. Searching this data set for malicious files that exploit known vulnerabilities indicates which files appeared on the Internet before the corresponding vulnerabilities were disclosed. We identify 18 vulnerabilities exploited before disclosure, of which 11 were not previously known to have been employed in zero-day attacks. We also find that a typical zero-day attack lasts 312 days on average and that, after vulnerabilities are disclosed publicly, the volume of attacks exploiting them increases by up to 5 orders of magnitude.},
	urldate = {2022-01-10},
	booktitle = {Proceedings of the 2012 {ACM} conference on {Computer} and communications security},
	publisher = {Association for Computing Machinery},
	author = {Bilge, Leyla and Dumitraş, Tudor},
	month = oct,
	year = {2012},
	keywords = {full disclosure, vulnerabilities, zero-day attacks},
	pages = {833--844},
}

@inproceedings{parnas_inspection_1994,
	title = {Inspection of {Safety}-{Critical} {Software} {Using} {Program}-{Function} {Tables}},
	abstract = {Software whose failure could cause serious damage or loss of life must be carefully inspected before it enters service. To be conﬁdent that we have considered all cases and possible event sequences, we must follow a systematic procedure based on a sound mathematical model. This paper describes our experience with such a procedure.},
	language = {en},
	booktitle = {{IFIP}},
	author = {Parnas, David Lorge},
	year = {1994},
	pages = {8},
}

@misc{noauthor_software_nodate,
	title = {Software {Engineering} {Security} {Practices} for {ML} on {IoT} - v2},
	url = {https://www.overleaf.com/project/61ae63b3af2cec826f472fdf},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-01-09},
}

@article{johnman_predicting_2018,
	title = {Predicting {FTSE} 100 returns and volatility using sentiment analysis},
	volume = {58},
	issn = {1467-629X},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/acfi.12373},
	doi = {10.1111/acfi.12373},
	abstract = {We investigate the statistical and economic effect of positive and negative sentiment on daily excess returns and volatility in the FTSE 100 index, using business news articles published by the Guardian Media Group between 01/01/2000 and 01/06/2016. The analysis indicates that while business news sentiment derived from articles aimed at retail traders does not influence excess returns in the FTSE 100 index, it does affect volatility, with negative sentiment increasing volatility and positive sentiment reducing it. Further, an ETF-based trading strategy based on these findings is found to outperform the naïve buy-and-hold approach.},
	language = {en},
	number = {S1},
	urldate = {2022-01-08},
	journal = {Accounting \& Finance},
	author = {Johnman, Mark and Vanstone, Bruce James and Gepp, Adrian},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/acfi.12373},
	keywords = {FTSE 100, IoTFailurePaper, News, Sentiment analysis, Text mining, Trading strategy},
	pages = {253--274},
}

@techreport{noauthor_baseline_2017,
	type = {Report/{Study}},
	title = {Baseline {Security} {Recommendations} for {IoT}},
	url = {https://www.enisa.europa.eu/publications/baseline-security-recommendations-for-iot},
	abstract = {The study which is titled ‘Baseline Security Recommendations for Internet of Things in the context of critical information infrastructures’, aims to set the scene for IoT security in Europe. It serves as a reference point in this field  and as a foundation for relevant forthcoming initiatives and developments.},
	language = {en},
	urldate = {2022-01-07},
	institution = {ENISA},
	year = {2017},
	keywords = {IoTFailurePaper},
}

@techreport{noauthor_internet_2021,
	title = {Internet of {Things} ({IoT}) security best practices},
	url = {https://docs.microsoft.com/en-us/azure/iot-fundamentals/iot-security-best-practices},
	abstract = {Best practices for securing your IoT data and infrastructure},
	language = {en-us},
	urldate = {2022-01-07},
	institution = {Microsoft},
	year = {2021},
	keywords = {IoTFailurePaper},
}

@techreport{jtc_1sc_27_cybersecurity_2021,
	title = {Cybersecurity - {IoT} security and privacy - {Guidelines}},
	url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/04/43/44373.html},
	language = {en},
	number = {DIS 27400},
	urldate = {2022-01-07},
	institution = {ISO/IEC},
	author = {{JTC 1/SC 27}},
	year = {2021},
	keywords = {IoTFailurePaper},
}

@book{noauthor_internet_2018,
	address = {Geneva, Switzerland},
	edition = {Edition 1.0},
	series = {International standard / {International} {Electrotechnical} {Commission}},
	title = {Internet of {Things} ({IoT}) - {Reference} guide},
	isbn = {978-2-8322-5972-6},
	language = {en},
	number = {30141},
	publisher = {IEC Central Office},
	year = {2018},
}

@book{hobbs2019embedded,
	title = {Embedded software development for safety-critical systems},
	publisher = {CRC Press},
	author = {Hobbs, Chris},
	year = {2019},
}

@article{zhou_reviewing_2021,
	title = {Reviewing {IoT} {Security} via {Logic} {Bugs} in {IoT} {Platforms} and {Systems}},
	volume = {8},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2021.3059457},
	abstract = {In recent years, Internet-of-Things (IoT) platforms and systems have been rapidly emerging. Although IoT is a new technology, new does not mean simpler (than existing networked systems). Contrarily, the complexity (of IoT platforms and systems) is actually being increased in terms of the interactions between the physical world and cyberspace. The increased complexity indeed results in new vulnerabilities. This article seeks to provide a review of the recently discovered logic bugs that are specific to IoT platforms and systems and discuss the lessons we learned from these bugs. In particular, 20 logic bugs and one weakness falling into seven categories of vulnerabilities are reviewed in this survey.},
	number = {14},
	journal = {IEEE Internet of Things Journal},
	author = {Zhou, Wei and Cao, Chen and Huo, Dongdong and Cheng, Kai and Zhang, Lan and Guan, Le and Liu, Tao and Jia, Yan and Zheng, Yaowen and Zhang, Yuqing and Sun, Limin and Wang, Yazhe and Liu, Peng},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Automation, Cloud computing, Computer bugs, Internet of Things, Internet of Things (IoT), IoTFailurePaper, Mobile applications, Monitoring, Security, logic bugs, privacy, security},
	pages = {11621--11639},
}

@article{hatton_language_2007,
	title = {Language subsetting in an industrial context: {A} comparison of {MISRA} {C} 1998 and {MISRA} {C} 2004},
	volume = {49},
	issn = {09505849},
	shorttitle = {Language subsetting in an industrial context},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584906000991},
	doi = {10.1016/j.infsof.2006.07.004},
	abstract = {The MISRA C standard [7] ﬁrst appeared in 1998 with the objective of providing a set of guidelines to restrict features in the ISO C language of known undeﬁned or otherwise dangerous behaviour. The standard was assembled by representatives of a number of companies in the automobile sector in response to the rapidly growing use of C in electronic embedded systems in automobiles. The standard attempts to build on the earlier work of [6], [3] and others. Due to various perceived deﬁciencies, notably considerable ambiguity in the rule deﬁnitions, a revision was planned and eventually appeared in 2004. This paper measures how well the two standards compare on the same population of software and also determines how well the 2004 version achieved its stated goals. Given its increasing inﬂuence, the results raise important concerns.},
	language = {en},
	number = {5},
	urldate = {2022-01-06},
	journal = {Information and Software Technology},
	author = {Hatton, Les},
	month = may,
	year = {2007},
	pages = {475--482},
}

@article{hatton_safer_2004,
	title = {Safer language subsets: an overview and a case history, {MISRA} {C}},
	volume = {46},
	issn = {09505849},
	shorttitle = {Safer language subsets},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584903002076},
	doi = {10.1016/j.infsof.2003.09.016},
	language = {en},
	number = {7},
	urldate = {2022-01-06},
	journal = {Information and Software Technology},
	author = {Hatton, Les},
	month = jun,
	year = {2004},
	pages = {465--472},
}

@techreport{basalaj_correlation_2006,
	address = {Programming Research Ltd.},
	title = {Correlation between coding standards compliance and software quality},
	abstract = {Software Quality has different meaning to different people. The ISO 9126 standard was developed to introduce clarity and establish a framework for quality to be measured. This paper aims to explore how Internal Quality characteristics of a software system (source code) can be measured effectively. Instead of relying on traditional software metrics, which are shown to be a poor predictor of underlying software quality, we advocate measuring compliance to a coding standard. We show qualitative and quantitative evidence of how adoption of a coding standard helps organizations in improving the quality of their C/C++ software.},
	urldate = {2022-01-06},
	institution = {IEE},
	author = {Basalaj, W. and {van den Beuken, Frank}},
	year = {2006},
}

@inproceedings{boogerd_assessing_2008,
	address = {Beijing, China},
	title = {Assessing the value of coding standards: {An} empirical study},
	isbn = {978-1-4244-2613-3},
	shorttitle = {Assessing the value of coding standards},
	url = {http://ieeexplore.ieee.org/document/4658076/},
	doi = {10.1109/ICSM.2008.4658076},
	abstract = {In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. Not only can compliance with a set of rules having little impact on the number of faults be considered wasted effort, but it can actually result in an increase in faults, as any modiﬁcation has a non-zero probability of introducing a fault or triggering a previously concealed one. Therefore, it is important to build a body of empirical knowledge, helping us understand which rules are worthwhile enforcing, and which ones should be ignored in the context of fault reduction. In this paper, we describe two approaches to quantify the relation between rule violations and actual faults, and present empirical data on this relation for the MISRA C 2004 standard on an industrial case study.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {2008 {IEEE} {International} {Conference} on {Software} {Maintenance}},
	publisher = {IEEE},
	author = {Boogerd, Cathal and Moonen, Leon},
	month = sep,
	year = {2008},
	pages = {277--286},
}

@inproceedings{bai_exploring_2019,
	title = {Exploring {Tools} and {Strategies} {Used} {During} {Regular} {Expression} {Composition} {Tasks}},
	doi = {10.1109/ICPC.2019.00039},
	abstract = {Regular expressions are frequently found in programming projects. Studies have found that developers can accurately determine whether a string matches a regular expression. However, we still do not know the challenges associated with composing regular expressions. We conduct an exploratory case study to reveal the tools and strategies developers use during regular expression composition. In this study, 29 students are tasked with composing regular expressions that pass unit tests illustrating the intended behavior. The tasks are in Java and the Eclipse IDE was set up with JUnit tests. Participants had one hour to work and could use any Eclipse tools, web search, or web-based tools they desired. Screen-capture software recorded all interactions with browsers and the IDE. We analyzed the videos quantitatively by transcribing logs and extracting personas. Our results show that participants were 30\% successful (28 of 94 attempts) at achieving a 100\% pass rate on the unit tests. When participants used tools frequently, as in the case of the novice tester and the knowledgeable tester personas, or when they guess at a solution prior to searching, they are more likely to pass all the unit tests. We also found that compile errors often arise when participants searched for a result and copy/pasted the regular expression from another language into their Java files. These results point to future research into making regular expression composition easier for programmers, such as integrating visualization into the IDE to reduce context switching or providing language migration support when reusing regular expressions written in another language to reduce compile errors.},
	booktitle = {2019 {IEEE}/{ACM} 27th {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Bai, Gina R. and Clee, Brian and Shrestha, Nischal and Chapman, Carl and Wright, Cimone and Stolee, Kathryn T.},
	month = may,
	year = {2019},
	note = {ISSN: 2643-7171},
	keywords = {Exploratory study, personas, problem solving strategies, regular expressions},
	pages = {197--208},
}

@book{ieeeacm_international_conference_on_automated_software_engineering_2019_2019,
	title = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}: 10-15 {November} 2019, {San} {Diego}, {California}.},
	shorttitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	url = {https://ieeexplore.ieee.org/servlet/opac?punumber=8949433},
	language = {English},
	urldate = {2022-01-06},
	author = {IEEE/ACM International Conference on Automated Software Engineering, IEEE Computer Society and {Technical Council on Software Engineering} and {ACM Sigsoft} and {ACM SIGAI} and {IEEE Computer Society} and {Institute of Electrical and Electronics Engineers}},
	year = {2019},
	note = {OCLC: 1151050482},
}

@inproceedings{davis_why_2019,
	address = {Tallinn Estonia},
	title = {Why aren’t regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions},
	isbn = {978-1-4503-5572-8},
	shorttitle = {Why aren’t regular expressions a lingua franca?},
	url = {https://dl.acm.org/doi/10.1145/3338906.3338909},
	doi = {10.1145/3338906.3338909},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Davis, James C. and Michael IV, Louis G. and Coghlan, Christy A. and Servant, Francisco and Lee, Dongyoon},
	month = aug,
	year = {2019},
	pages = {443--454},
}

@book{ko_amy_j_cooperative_2021,
	title = {Cooperative {Software} {Development}},
	url = {https://faculty.washington.edu/ajko/books/cooperative-software-development/print},
	language = {en},
	author = {{Ko, Amy J.}},
	year = {2021},
}

@inproceedings{yin_how_2011,
	address = {Szeged, Hungary},
	title = {How do fixes become bugs?},
	isbn = {978-1-4503-0443-6},
	url = {http://dl.acm.org/citation.cfm?doid=2025113.2025121},
	doi = {10.1145/2025113.2025121},
	abstract = {Software bugs aﬀect system reliability. When a bug is exposed in the ﬁeld, developers need to ﬁx them. Unfortunately, the bug-ﬁxing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors’ reputation.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th {European} conference on {Foundations} of software engineering - {SIGSOFT}/{FSE} '11},
	publisher = {ACM Press},
	author = {Yin, Zuoning and Yuan, Ding and Zhou, Yuanyuan and Pasupathy, Shankar and Bairavasundaram, Lakshmi},
	year = {2011},
	pages = {26},
}

@article{eisenstadt_my_1997,
	title = {My hairiest bug war stories},
	volume = {40},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/248448.248456},
	doi = {10.1145/248448.248456},
	language = {en},
	number = {4},
	urldate = {2022-01-06},
	journal = {Communications of the ACM},
	author = {Eisenstadt, Marc},
	month = apr,
	year = {1997},
	pages = {30--37},
}

@inproceedings{ko_debugging_2008,
	address = {Leipzig, Germany},
	title = {Debugging reinvented: asking and answering why and why not questions about program behavior},
	isbn = {978-1-60558-079-1},
	shorttitle = {Debugging reinvented},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368130},
	doi = {10.1145/1368088.1368130},
	abstract = {When software developers want to understand the reason for a program’s behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn’t questions derived from the program’s code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {Ko, Andrew J. and Myers, Brad A.},
	year = {2008},
	pages = {301},
}

@article{zimmermann_what_2010,
	title = {What {Makes} a {Good} {Bug} {Report}?},
	volume = {36},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/5487527/},
	doi = {10.1109/TSE.2010.63},
	abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.},
	language = {en},
	number = {5},
	urldate = {2022-01-06},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zimmermann, Thomas and Premraj, Rahul and Bettenburg, Nicolas and Just, Sascha and Schroter, Adrian and Weiss, Cathrin},
	month = sep,
	year = {2010},
	pages = {618--643},
}

@inproceedings{beller_dichotomy_2018,
	address = {Gothenburg Sweden},
	title = {On the dichotomy of debugging behavior among programmers},
	isbn = {978-1-4503-5638-1},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180175},
	doi = {10.1145/3180155.3180175},
	abstract = {Debugging is an inevitable activity in most software projects, often difﬁcult and more time-consuming than expected, giving it the nickname the “dirty little secret of computer science.” Surprisingly, we have little knowledge on how software engineers debug software problems in the real world, whether they use dedicated debugging tools, and how knowledgeable they are about debugging. This study aims to shed light on these aspects by following a mixed-methods research approach. We conduct an online survey capturing how 176 developers reﬂect on debugging. We augment this subjective survey data with objective observations on how 458 developers use the debugger included in their integrated development environments (IDEs) by instrumenting the popular ECLIPSE and INTELLIJ IDEs with the purpose-built plugin WATCHDOG 2.0. To clarify the insights and discrepancies observed in the previous steps, we followed up by conducting interviews with debugging experts and regular debugging users. Our results indicate that IDE-provided debuggers are not used as often as expected, as “printf debugging” remains a feasible choice for many programmers. Furthermore, both knowledge and use of advanced debugging features are low. These results call to strengthen hands-on debugging experience in computer science curricula and have already reﬁned the implementation of modern IDE debuggers.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Beller, Moritz and Spruit, Niels and Spinellis, Diomidis and Zaidman, Andy},
	month = may,
	year = {2018},
	pages = {572--583},
}

@article{van1997software,
	title = {Software release management},
	volume = {22},
	number = {6},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Van Der Hoek, Andre and Hall, Richard S and Heimbigner, Dennis and Wolf, Alexander L},
	year = {1997},
	note = {Publisher: ACM New York, NY, USA},
	pages = {159--175},
}

@article{chen_continuous_2015,
	title = {Continuous {Delivery}: {Huge} {Benefits}, but {Challenges} {Too}},
	volume = {32},
	issn = {0740-7459},
	shorttitle = {Continuous {Delivery}},
	url = {http://ieeexplore.ieee.org/document/7006384/},
	doi = {10.1109/MS.2015.27},
	language = {en},
	number = {2},
	urldate = {2022-01-06},
	journal = {IEEE Software},
	author = {Chen, Lianping},
	month = mar,
	year = {2015},
	pages = {50--54},
}

@inproceedings{hilton_usage_2016,
	address = {Singapore Singapore},
	title = {Usage, costs, and benefits of continuous integration in open-source projects},
	isbn = {978-1-4503-3845-5},
	url = {https://dl.acm.org/doi/10.1145/2970276.2970358},
	doi = {10.1145/2970276.2970358},
	abstract = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and beneﬁts associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 opensource projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and beneﬁts of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as ﬁnding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
	month = aug,
	year = {2016},
	pages = {426--437},
}

@article{glerum_debugging_nodate,
	title = {Debugging in the (very) large: ten years of implementation and experience},
	abstract = {Windows Error Reporting (WER) is a distributed system that automates the processing of error reports coming from an installed base of a billion machines. WER has collected billions of error reports in ten years of operation. It collects error data automatically and classifies errors into buckets, which are used to prioritize developer effort and report fixes to users. WER uses a progressive approach to data collection, which minimizes overhead for most reports yet allows developers to collect detailed information when needed. WER takes advantage of its scale to use error statistics as a tool in debugging; this allows developers to isolate bugs that could not be found at smaller scale. WER has been designed for large scale: one pair of database servers can record all the errors that occur on all Windows computers worldwide.},
	language = {en},
	author = {Glerum, Kirk and Kinshumann, Kinshuman and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
	pages = {14},
}

@inproceedings{brindescu_how_2014,
	address = {Hyderabad India},
	title = {How do centralized and distributed version control systems impact software changes?},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568322},
	doi = {10.1145/2568225.2568322},
	abstract = {Distributed Version Control Systems (DVCS) have seen an increase in popularity relative to traditional Centralized Version Control Systems (CVCS). Yet we know little on whether developers are beneﬁtting from the extra power of DVCS. Without such knowledge, researchers, developers, tool builders, and team managers are in the danger of making wrong assumptions.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Brindescu, Caius and Codoban, Mihai and Shmarkatiuk, Sergii and Dig, Danny},
	month = may,
	year = {2014},
	pages = {322--333},
}

@inproceedings{van_lamsweerde_requirements_2008,
	address = {Atlanta, Georgia},
	title = {Requirements engineering: from craft to discipline},
	isbn = {978-1-59593-995-1},
	shorttitle = {Requirements engineering},
	url = {http://portal.acm.org/citation.cfm?doid=1453101.1453133},
	doi = {10.1145/1453101.1453133},
	abstract = {Getting the right software requirements under the right environment assumptions is a critical precondition for developing the right software. This task is intrinsically difficult. We need to produce a complete, adequate, consistent, and well-structured set of measurable requirements and assumptions from incomplete, imprecise, and sparse material originating from multiple, often conflicting sources. The system we need to consider comprises software and environment components including people and devices.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of software engineering - {SIGSOFT} '08/{FSE}-16},
	publisher = {ACM Press},
	author = {van Lamsweerde, Axel},
	year = {2008},
	pages = {238},
}

@article{mader_developers_2015,
	title = {Do developers benefit from requirements traceability when evolving and maintaining a software system?},
	volume = {20},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-014-9314-z},
	doi = {10.1007/s10664-014-9314-z},
	abstract = {Software traceability is a required component of many software development processes. Advocates of requirements traceability cite advantages like easier program comprehension and support for software maintenance (i.e., software change). However, despite its growing popularity, there exists no published evaluation about the usefulness of requirements traceability. It is important, if not crucial, to investigate whether the use of requirements traceability can significantly support development tasks to eventually justify its costs. We thus conducted a controlled experiment with 71 subjects re-performing real maintenance tasks on two third-party development projects: half of the tasks with and the other half without traceability. Subjects sketched their task solutions on paper to focus on the their ability to solving the problems rather than their programming skills. Our findings show that subjects with traceability performed on average 24 \% faster on a given task and created on average 50 \% more correct solutions—suggesting that traceability not only saves effort but can profoundly improve software maintenance quality.},
	language = {en},
	number = {2},
	urldate = {2022-01-06},
	journal = {Empirical Software Engineering},
	author = {Mäder, Patrick and Egyed, Alexander},
	month = apr,
	year = {2015},
	pages = {413--441},
}

@inproceedings{kim_field_2012,
	address = {Cary, North Carolina},
	title = {A field study of refactoring challenges and benefits},
	isbn = {978-1-4503-1614-9},
	url = {http://dl.acm.org/citation.cfm?doid=2393596.2393655},
	doi = {10.1145/2393596.2393655},
	abstract = {It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring beneﬁts or investigate developers’ perception towards these beneﬁts. This paper presents a ﬁeld study of refactoring beneﬁts and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey ﬁnds that the refactoring deﬁnition in practice is not conﬁned to a rigorous deﬁnition of semanticspreserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized eﬀort on refactoring Windows. The quantitative analysis of Windows 7 version history ﬁnds that the binary modules refactored by this team experienced signiﬁcant reduction in the number of inter-module dependencies and post-release defects, indicating a visible beneﬁt of refactoring.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering} - {FSE} '12},
	publisher = {ACM Press},
	author = {Kim, Miryung and Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2012},
	pages = {1},
}

@inproceedings{mohanani_requirements_2014,
	address = {Hyderabad India},
	title = {Requirements fixation},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568235},
	doi = {10.1145/2568225.2568235},
	abstract = {There is a broad consensus that understanding system desiderata (requirements) and design creativity are both important for software engineering success. However, little research has addressed the relationship between design creativity and the way requirements are framed or presented. This paper therefore aims to investigate the possibility that the way desiderata are framed or presented can affect design creativity. Forty two participants took part in a randomized control trial where one group received desiderata framed as “requirements” while the other received desiderata framed as “ideas”. Participants produced design concepts which were judged for originality. Participants who received requirements framing produced significantly less original designs than participants who received ideas framing (MannWhitney U=116.5, p=0.004). We conclude that framing desiderata as “requirements” may cause requirements fixation where designers’ preoccupation with satisfying explicit requirements inhibits their creativity.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Mohanani, Rahul and Ralph, Paul and Shreeve, Ben},
	month = may,
	year = {2014},
	pages = {895--906},
}

@inproceedings{kim_field_2012-1,
	address = {Cary, North Carolina},
	title = {A field study of refactoring challenges and benefits},
	isbn = {978-1-4503-1614-9},
	url = {http://dl.acm.org/citation.cfm?doid=2393596.2393655},
	doi = {10.1145/2393596.2393655},
	abstract = {It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring beneﬁts or investigate developers’ perception towards these beneﬁts. This paper presents a ﬁeld study of refactoring beneﬁts and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey ﬁnds that the refactoring deﬁnition in practice is not conﬁned to a rigorous deﬁnition of semanticspreserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized eﬀort on refactoring Windows. The quantitative analysis of Windows 7 version history ﬁnds that the binary modules refactored by this team experienced signiﬁcant reduction in the number of inter-module dependencies and post-release defects, indicating a visible beneﬁt of refactoring.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering} - {FSE} '12},
	publisher = {ACM Press},
	author = {Kim, Miryung and Zimmermann, Thomas and Nagappan, Nachiappan},
	year = {2012},
	pages = {1},
}

@inproceedings{khadka_how_2014,
	address = {Hyderabad India},
	title = {How do professionals perceive legacy systems and software modernization?},
	isbn = {978-1-4503-2756-5},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568318},
	doi = {10.1145/2568225.2568318},
	abstract = {Existing research in legacy system modernization has traditionally focused on technical challenges, and takes the standpoint that legacy systems are obsolete, yet crucial for an organization’s operation. Nonetheless, it remains unclear whether practitioners in the industry also share this perception. This paper describes the outcome of an exploratory study in which 26 industrial practitioners were interviewed on what makes a software system a legacy system, what the main drivers are that lead to the modernization of such systems, and what challenges are faced during the modernization process. The ﬁndings of the interviews have been validated by means of a survey with 198 respondents. The results show that practitioners value their legacy systems highly, the challenges they face are not just technical, but also include business and organizational aspects.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Khadka, Ravi and Batlajery, Belfrit V. and Saeidi, Amir M. and Jansen, Slinger and Hage, Jurriaan},
	month = may,
	year = {2014},
	pages = {36--47},
}

@inproceedings{ernst_measure_2015,
	address = {Bergamo Italy},
	title = {Measure it? {Manage} it? {Ignore} it? software practitioners and technical debt},
	isbn = {978-1-4503-3675-8},
	shorttitle = {Measure it?},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786848},
	doi = {10.1145/2786805.2786848},
	abstract = {The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ernst, Neil A. and Bellomo, Stephany and Ozkaya, Ipek and Nord, Robert L. and Gorton, Ian},
	month = aug,
	year = {2015},
	pages = {50--60},
}

@inproceedings{walker_crosscutting_2012,
	address = {Cary, North Carolina},
	title = {Do crosscutting concerns cause modularity problems?},
	isbn = {978-1-4503-1614-9},
	url = {http://dl.acm.org/citation.cfm?doid=2393596.2393654},
	doi = {10.1145/2393596.2393654},
	abstract = {It has been claimed that crosscutting concerns are pervasive and problematic, leading to diﬃculties in program comprehension, evolution, and long-term design degradation. To consider whether this theory bears out, we examine the patch history of the Mozilla project over a period of a decade to consider whether crosscutting concerns exist therein and whether we can see evidence of problems arising from them. Mozilla is an interesting case, due to its longevity; size; polylingual nature; and use of a patch review process, which maintains strong connections between issue reports and the patches that are intended to address each. We perform several statistical analyses of the over 200,000 patches submitted to address over 90,000 issues reported in this time period. We ﬁnd that 90\% of patches show little or no evidence of scattering, that the scattering of a patch tends to decrease slightly upon review on average, and that the system shows at worst a slow increase of average scattering over time.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering} - {FSE} '12},
	publisher = {ACM Press},
	author = {Walker, Robert J. and Rawal, Shreya and Sillito, Jonathan},
	year = {2012},
	pages = {1},
}

@inproceedings{murphy-hill_how_2009,
	address = {Vancouver, BC, Canada},
	title = {How we refactor, and how we know it},
	isbn = {978-1-4244-3453-4},
	url = {http://ieeexplore.ieee.org/document/5070529/},
	doi = {10.1109/ICSE.2009.5070529},
	abstract = {Much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects. Researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based. To help put refactoring research on a sound scientiﬁc basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool-assisted refactorings, 2500 developer hours, and 3400 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. For example, we ﬁnd that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers. In contrast, we were able to conﬁrm the assumption that programmers do frequently intersperse refactoring with other program changes. By conﬁrming assumptions and replicating studies made by other researchers, we can have greater conﬁdence that those researchers’ conclusions are generalizable.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Murphy-Hill, Emerson and Parnin, Chris and Black, Andrew P.},
	year = {2009},
	pages = {287--297},
}

@inproceedings{petre_uml_2013,
	address = {San Francisco, CA, USA},
	title = {{UML} in practice},
	isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
	url = {http://ieeexplore.ieee.org/document/6606618/},
	doi = {10.1109/ICSE.2013.6606618},
	abstract = {UML has been described by some as “the lingua franca of software engineering”. Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry – if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Petre, Marian},
	month = may,
	year = {2013},
	pages = {722--731},
}

@misc{malte_ubl_design_2020,
	title = {Design {Docs} at {Google}},
	url = {https://www.industrialempathy.com/posts/design-docs-at-google/},
	language = {en},
	urldate = {2022-01-06},
	author = {{Malte Ubl}},
	year = {2020},
}

@inproceedings{beck_industrial_1996,
	address = {Berlin, Germany},
	title = {Industrial experience with design patterns},
	isbn = {978-0-8186-7247-7},
	url = {http://ieeexplore.ieee.org/document/493406/},
	doi = {10.1109/ICSE.1996.493406},
	abstract = {A design pattern is a particular prose form of recording design information such that designs which have worked well in the past can be applied again in similar situations in the future. The availability of a collection of design patterns can help both the experienced and the novice designer recognize situations in which design reuse could or should occur.},
	language = {en},
	urldate = {2022-01-06},
	booktitle = {Proceedings of {IEEE} 18th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Beck, K. and Crocker, R. and Meszaros, G. and Coplien, J.O. and Dominick, L. and Paulisch, F. and Vlissides, J.},
	year = {1996},
	pages = {103--114},
}

@article{holm_empirical_2012,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@article{holm_empirical_2012-1,
	title = {Empirical {Analysis} of {System}-{Level} {Vulnerability} {Metrics} through {Actual} {Attacks}},
	volume = {9},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2012.66},
	abstract = {The Common Vulnerability Scoring System (CVSS) is a widely used and well-established standard for classifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database (NVD) are scored according to this method. As computer systems typically have multiple vulnerabilities, it is often desirable to aggregate the score of individual vulnerabilities to a system level. Several such metrics have been proposed, but their quality has not been studied. This paper presents a statistical analysis of how 18 security estimation metrics based on CVSS data correlate with the time-to-compromise of 34 successful attacks. The empirical data originates from an international cyber defense exercise involving over 100 participants and were collected by studying network traffic logs, attacker logs, observer logs, and network vulnerabilities. The results suggest that security modeling with CVSS data alone does not accurately portray the time-to-compromise of a system. However, results also show that metrics employing more CVSS data are more correlated with time-to-compromise. As a consequence, models that only use the weakest link (most severe vulnerability) to compose a metric are less promising than those that consider all vulnerabilities.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Holm, Hannes and Ekstedt, Mathias and Andersson, Dennis},
	month = nov,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Authorization, Computational modeling, Computer crime, Mathematical model, Network security, Network-level security and protection, Risk management, Telecommunication network management, network management, phreaking), risk management, unauthorized access (hacking},
	pages = {825--837},
}

@inproceedings{papp_embedded_2015,
	title = {Embedded systems security: {Threats}, vulnerabilities, and attack taxonomy},
	shorttitle = {Embedded systems security},
	doi = {10.1109/PST.2015.7232966},
	abstract = {Embedded systems are the driving force for technological development in many domains such as automotive, healthcare, and industrial control in the emerging post-PC era. As more and more computational and networked devices are integrated into all aspects of our lives in a pervasive and “invisible” way, security becomes critical for the dependability of all smart or intelligent systems built upon these embedded systems. In this paper, we conduct a systematic review of the existing threats and vulnerabilities in embedded systems based on public available data. Moreover, based on the information, we derive an attack taxonomy for embedded systems. We envision that the findings in this paper provide a valuable insight of the threat landscape facing embedded systems. The knowledge can be used for a better understanding and the identification of security risks in system analysis and design.},
	booktitle = {2015 13th {Annual} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Papp, Dorottya and Ma, Zhendong and Buttyan, Levente},
	month = jul,
	year = {2015},
	keywords = {Authentication, Cryptography, Embedded systems, Protocols, Taxonomy},
	pages = {145--152},
}

@inproceedings{papp_embedded_2015-1,
	title = {Embedded systems security: {Threats}, vulnerabilities, and attack taxonomy},
	shorttitle = {Embedded systems security},
	doi = {10.1109/PST.2015.7232966},
	abstract = {Embedded systems are the driving force for technological development in many domains such as automotive, healthcare, and industrial control in the emerging post-PC era. As more and more computational and networked devices are integrated into all aspects of our lives in a pervasive and “invisible” way, security becomes critical for the dependability of all smart or intelligent systems built upon these embedded systems. In this paper, we conduct a systematic review of the existing threats and vulnerabilities in embedded systems based on public available data. Moreover, based on the information, we derive an attack taxonomy for embedded systems. We envision that the findings in this paper provide a valuable insight of the threat landscape facing embedded systems. The knowledge can be used for a better understanding and the identification of security risks in system analysis and design.},
	booktitle = {2015 13th {Annual} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Papp, Dorottya and Ma, Zhendong and Buttyan, Levente},
	month = jul,
	year = {2015},
	keywords = {Authentication, Cryptography, Embedded systems, Protocols, Taxonomy},
	pages = {145--152},
}

@article{barrett_water_nodate,
	title = {Water {Supply} {Hacks} {Are} a {Serious} {Threat}—and {Only} {Getting} {Worse}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/threat-to-water-supply-is-real-and-only-getting-worse/},
	abstract = {An ex-employee allegedly tampered with a Kansas water system. It was too easy, and it's happening too often.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Barrett, Brian},
	note = {Section: tags},
	keywords = {critical infrastructure, cybersecurity, hacks, vulnerabilities},
}

@article{newman_decades-old_nodate,
	title = {Decades-{Old} {Code} {Is} {Putting} {Millions} of {Critical} {Devices} at {Risk}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/urgent-11-ipnet-vulnerable-devices/},
	abstract = {Nearly two decades ago, a company called Interpeak created a network protocol that became an industry standard. It also had severe bugs that are only now coming to light.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Newman, Lily Hay},
	note = {Section: tags},
	keywords = {bugs, critical infrastructure, iot, vulnerabilities},
}

@article{stewart_teslas_2018,
	title = {Tesla's {Self}-{Driving} {Autopilot} {Involved} in {Another} {Deadly} {Crash}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/tesla-autopilot-self-driving-crash-california/},
	abstract = {The automaker says its semi-autonomous system was engaged when a Model X SUV hit a freeway barrier last week in California, killing the driver.},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {Wired},
	author = {Stewart, Jack},
	year = {2018},
	note = {Section: tags},
	keywords = {elon musk, self-driving cars, tesla},
}

@misc{noauthor_hackers_2015,
	title = {Hackers {Cut} a {Corvette}'s {Brakes} {Via} a {Common} {Car} {Gadget} {\textbar} {WIRED}},
	url = {http://archive.fo/z1qCi},
	abstract = {The free dongles that insurance companies ask customers to plug into their dashes could expose your car to hackers.},
	urldate = {2022-01-05},
	journal = {archive.fo},
	month = aug,
	year = {2015},
}

@article{boudette_us_2021,
	chapter = {Business},
	title = {U.{S}. {Will} {Investigate} {Tesla}’s {Autopilot} {System} {Over} {Crashes} {With} {Emergency} {Vehicles}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/08/16/business/tesla-autopilot-nhtsa.html},
	abstract = {It will be the broadest look yet at Tesla’s assisted-driving technology. The National Highway Traffic Safety Administration has the authority to force a recall or require new safety features.},
	language = {en-US},
	urldate = {2022-01-04},
	journal = {The New York Times},
	author = {Boudette, Neal E. and Chokshi, Niraj},
	month = aug,
	year = {2021},
	keywords = {Automobile Safety Features and Defects, Deaths (Fatalities), Driver Distraction and Fatigue, Driverless and Semiautonomous Vehicles, Electric and Hybrid Vehicles, Musk, Elon, National Highway Traffic Safety Administration, Tesla Motors Inc, Traffic Accidents and Safety},
}

@inproceedings{li_have_2006,
	address = {New York, NY, USA},
	series = {{ASID} '06},
	title = {Have things changed now? an empirical study of bug characteristics in modern open source software},
	isbn = {978-1-59593-576-2},
	shorttitle = {Have things changed now?},
	url = {https://doi.org/10.1145/1181309.1181314},
	doi = {10.1145/1181309.1181314},
	abstract = {Software errors are a major cause for system failures. To effectively design tools and support for detecting and recovering from software failures requires a deep understanding of bug characteristics. Recently, software and its development process have significantly changed in many ways, including more help from bug detection tools, shift towards multi-threading architecture, the open-source development paradigm and increasing concerns about security and user-friendly interface. Therefore, results from previous studies may not be applicable to present software. Furthermore, many new aspects such as security, concurrency and open-source-related characteristics have not well studied. Additionally, previous studies were based on a small number of bugs, which may lead to non-representative results.To investigate the impacts of the new factors on software errors, we analyze bug characteristics by first sampling hundreds of real world bugs in two large, representative open-source projects. To validate the representativeness of our results, we use natural language text classification techniques and automatically analyze around 29, 000 bugs from the Bugzilla databases of the software.Our study has discovered several new interesting characteristics: (1) memory-related bugs have decreased because quite a few effective detection tools became available recently; (2) surprisingly, some simple memory-related bugs such as NULL pointer dereferences that should have been detected by existing tools in development are still a major component, which indicates that the tools have not been used with their full capacity; (3) semantic bugs are the dominant root causes, as they are application specific and difficult to fix, which suggests that more efforts should be put into detecting and fixing them; (4) security bugs are increasing, and the majority of them cause severe impacts.},
	urldate = {2022-01-04},
	booktitle = {Proceedings of the 1st workshop on {Architectural} and system support for improving software dependability},
	publisher = {Association for Computing Machinery},
	author = {Li, Zhenmin and Tan, Lin and Wang, Xuanhui and Lu, Shan and Zhou, Yuanyuan and Zhai, Chengxiang},
	month = oct,
	year = {2006},
	keywords = {bug characteristics, bug detection, empirical study, open source, security},
	pages = {25--33},
}

@inproceedings{jimenez_empirical_2016,
	title = {An {Empirical} {Analysis} of {Vulnerabilities} in {OpenSSL} and the {Linux} {Kernel}},
	doi = {10.1109/APSEC.2016.025},
	abstract = {Vulnerabilities are one of the main concerns faced by practitioners when working with security critical applications. Unfortunately, developers and security teams, even experienced ones, fail to identify many of them with severe consequences. Vulnerabilities are hard to discover since they appear in various forms, caused by many different issues and their identification requires an attacker's mindset. In this paper, we aim at increasing the understanding of vulnerabilities by investigating their characteristics on two major open-source software systems, i.e., the Linux kernel and OpenSSL. In particular, we seek to analyse and build a profile for vulnerable code, which can ultimately help researchers in building automated approaches like vulnerability prediction models. Thus, we examine the location, criticality and category of vulnerable code along with its relation with software metrics. To do so, we collect more than 2,200 vulnerable files accounting for 863 vulnerabilities and compute more than 35 software metrics. Our results indicate that while 9 Common Weakness Enumeration (CWE) types of vulnerabilities are prevalent, only 3 of them are critical in OpenSSL and 2 of them in the Linux kernel. They also indicate that different types of vulnerabilities have different characteristics, i.e., metric profiles, and that vulnerabilities of the same type have different profiles in the two projects we examined. We also found that the file structure of the projects can provide useful information related to the vulnerabilities. Overall, our results demonstrate the need for making project specific approaches that focus on specific types of vulnerabilities.},
	booktitle = {2016 23rd {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Jimenez, Matthieu and Papadakis, Mike and Traon, Yves Le},
	month = dec,
	year = {2016},
	note = {ISSN: 1530-1362},
	keywords = {Common Vulnerability Exposures, Kernel, Linux, Predictive models, Security, Software Metrics, Software Security, Software metrics, Vulnerabilities},
	pages = {105--112},
}

@article{tan_bug_2014,
	title = {Bug characteristics in open source software},
	volume = {19},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-013-9258-8},
	doi = {10.1007/s10664-013-9258-8},
	abstract = {To design effective tools for detecting and recovering from software failures requires a deep understanding of software bug characteristics. We study software bug characteristics by sampling 2,060 real world bugs in three large, representative open-source projects—the Linux kernel, Mozilla, and Apache. We manually study these bugs in three dimensions—root causes, impacts, and components. We further study the correlation between categories in different dimensions, and the trend of different types of bugs. The findings include: (1) semantic bugs are the dominant root cause. As software evolves, semantic bugs increase, while memory-related bugs decrease, calling for more research effort to address semantic bugs; (2) the Linux kernel operating system (OS) has more concurrency bugs than its non-OS counterparts, suggesting more effort into detecting concurrency bugs in operating system code; and (3) reported security bugs are increasing, and the majority of them are caused by semantic bugs, suggesting more support to help developers diagnose and fix security bugs, especially semantic security bugs. In addition, to reduce the manual effort in building bug benchmarks for evaluating bug detection and diagnosis tools, we use machine learning techniques to classify 109,014 bugs automatically.},
	language = {en},
	number = {6},
	urldate = {2022-01-04},
	journal = {Empirical Software Engineering},
	author = {Tan, Lin and Liu, Chen and Li, Zhenmin and Wang, Xuanhui and Zhou, Yuanyuan and Zhai, Chengxiang},
	month = dec,
	year = {2014},
	pages = {1665--1705},
}

@inproceedings{chen_data-driven_2003,
	title = {A {Data}-{Driven} {Finite} {State} {Machine} {Model} for {Analyzing} {Security} {Vulnerabilities}.},
	booktitle = {{DSN}},
	publisher = {Citeseer},
	author = {Chen, Shuo and Kalbarczyk, Zbigniew and Xu, Jun and Iyer, Ravishankar K.},
	year = {2003},
	pages = {605--614},
}

@inproceedings{turns_integrating_2014,
	title = {Integrating {Reflection} into {Engineering} {Education}},
	url = {http://peer.asee.org/20668},
	doi = {10.18260/1-2--20668},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {2014 {ASEE} {Annual} {Conference} \& {Exposition} {Proceedings}},
	author = {Turns, Jennifer and Sattler, Brook and Yasuhara, Ken and Borgford-Parnell, Jim and Atman, Cynthia},
	year = {2014},
}

@article{chen_security_2006,
	title = {Security {Vulnerabilities}: {From} {Analysis} to {Detection} and {Masking} {Techniques}},
	volume = {94},
	issn = {1558-2256},
	shorttitle = {Security {Vulnerabilities}},
	doi = {10.1109/JPROC.2005.862473},
	abstract = {This paper presents a study that uses extensive analysis of real security vulnerabilities to drive the development of: 1) runtime techniques for detection/masking of security attacks and 2) formal source code analysis methods to enable identification and removal of potential security vulnerabilities. A finite-state machine (FSM) approach is employed to decompose programs into multiple elementary activities, making it possible to extract simple predicates to be ensured for security. The FSM analysis pinpoints common characteristics among a broad range of security vulnerabilities: predictable memory layout, unprotected control data, and pointer taintedness. We propose memory layout randomization and control data randomization to mask the vulnerabilities at runtime. We also propose a static analysis approach to detect potential security vulnerabilities using the notion of pointer taintedness.},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Chen, S. and Xu, J. and Kalbarczyk, Z. and Iyer, K.},
	month = feb,
	year = {2006},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Buffer overflow, Computer science, Computer security, Data analysis, Data mining, Data security, Databases, Gain measurement, Protection, Runtime, randomization, security attack, vulnerability},
	pages = {407--418},
}

@article{oconnor_weekend_2019,
	chapter = {Well},
	title = {In {Weekend} {Outage}, {Diabetes} {Monitors} {Fail} to {Send} {Crucial} {Alerts}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2019/12/02/well/live/Dexcom-G6-diabetes-monitor-outage.html},
	abstract = {Parents who use the Dexcom G6 depend on alarms on their phones if their children’s blood sugar levels are dangerous. They say the outage put them at risk.},
	language = {en-US},
	urldate = {2022-01-04},
	journal = {The New York Times},
	author = {O’Connor, Anahad},
	month = dec,
	year = {2019},
	keywords = {Children and Childhood, Computer Network Outages, DexCom Inc, Diabetes, Mobile Applications, Parenting, Smartphones, Wearable Computing},
}

@article{saad2018pendekatan,
	title = {Named entity recognition approach for malay crime news retrieval},
	volume = {18},
	doi = {https://doi.org/10.17576/gema-2018-1804-14},
	number = {4},
	journal = {GEMA Online® Journal of Language Studies},
	author = {Saad, Saidah and Mansor, Mohamed Kamil},
	year = {2018},
	keywords = {IoTFailurePaper},
}

@article{foufi_mining_2019,
	title = {Mining of {Textual} {Health} {Information} from {Reddit}: {Analysis} of {Chronic} {Diseases} {With} {Extracted} {Entities} and {Their} {Relations}},
	volume = {21},
	issn = {1438-8871},
	shorttitle = {Mining of {Textual} {Health} {Information} from {Reddit}},
	url = {http://www.jmir.org/2019/6/e12876/},
	doi = {10.2196/12876},
	abstract = {Background: Social media platforms constitute a rich data source for natural language processing tasks such as named entity recognition, relation extraction, and sentiment analysis. In particular, social media platforms about health provide a different insight into patient’s experiences with diseases and treatment than those found in the scientific literature.
Objective: This paper aimed to report a study of entities related to chronic diseases and their relation in user-generated text posts. The major focus of our research is the study of biomedical entities found in health social media platforms and their relations and the way people suffering from chronic diseases express themselves.
Methods: We collected a corpus of 17,624 text posts from disease-specific subreddits of the social news and discussion website Reddit. For entity and relation extraction from this corpus, we employed the PKDE4J tool developed by Song et al (2015). PKDE4J is a text mining system that integrates dictionary-based entity extraction and rule-based relation extraction in a highly flexible and extensible framework.
Results: Using PKDE4J, we extracted 2 types of entities and relations: biomedical entities and relations and subject-predicate-object entity relations. In total, 82,138 entities and 30,341 relation pairs were extracted from the Reddit dataset. The most highly mentioned entities were those related to oncological disease (2884 occurrences of cancer) and asthma (2180 occurrences). The relation pair anatomy-disease was the most frequent (5550 occurrences), the highest frequent entities in this pair being cancer and lymph. The manual validation of the extracted entities showed a very good performance of the system at the entity extraction task (3682/5151, 71.48\% extracted entities were correctly labeled).
Conclusions: This study showed that people are eager to share their personal experience with chronic diseases on social media platforms despite possible privacy and security issues. The results reported in this paper are promising and demonstrate the need for more in-depth studies on the way patients with chronic diseases express themselves on social media platforms.},
	language = {en},
	number = {6},
	urldate = {2022-01-04},
	journal = {Journal of Medical Internet Research},
	author = {Foufi, Vasiliki and Timakum, Tatsawan and Gaudet-Blavignac, Christophe and Lovis, Christian and Song, Min},
	month = jun,
	year = {2019},
	keywords = {IoTFailurePaper},
	pages = {e12876},
}

@article{drury_survey_2019,
	title = {A survey of the applications of text mining for agriculture},
	volume = {163},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169919302960},
	doi = {10.1016/j.compag.2019.104864},
	abstract = {Agricultural researchers, in common with other domains, have recently began to have access to large collections of agricultural texts such as scientific papers and news stories. These texts can be analysed with text mining techniques to resolve agricultural problems or extract knowledge. Despite the potential of these techniques, text mining is a relatively underused technique in the agricultural domain. Therefore, this survey is intended to provide a current state of the art survey of the application of text mining techniques to agricultural problems.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Computers and Electronics in Agriculture},
	author = {Drury, Brett and Roche, Mathieu},
	month = aug,
	year = {2019},
	keywords = {Agriculture, Information extraction, Information retrieval, IoTFailurePaper, Sentiment analysis, Survey, Text mining},
	pages = {104864},
}

@misc{noauthor_as_nodate,
	title = {As {U}.{S}. {Investigates} {Fatal} {Tesla} {Crash}, {Company} {Defends} {Autopilot} {System} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2016/07/13/business/tesla-autopilot-fatal-crash-investigation.html},
	urldate = {2022-01-03},
}

@misc{noauthor_inside_nodate,
	title = {Inside a {Fatal} {Tesla} {Autopilot} {Accident}: ‘{It} {Happened} {So} {Fast}’ - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/08/17/business/tesla-autopilot-accident.html},
	urldate = {2022-01-03},
}

@misc{noauthor_boeing_nodate,
	title = {Boeing {Starliner} {Flight}’s {Flaws} {Show} ‘{Fundamental} {Problem},’ {NASA} {Says} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2020/02/07/science/boeing-starliner-nasa.html},
	urldate = {2022-01-03},
}

@article{bilton_nest_2016,
	chapter = {Style},
	title = {Nest {Thermostat} {Glitch} {Leaves} {Users} in the {Cold}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2016/01/14/fashion/nest-thermostat-glitch-battery-dies-software-freeze.html},
	abstract = {As more “smart” devices enter our lives, small software bugs can cause big headaches.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Bilton, Nick},
	month = jan,
	year = {2016},
	keywords = {Bilton, Nick, Defective Products, Home Automation and Smart Homes, Nest Labs Inc, Thermostats},
}

@misc{noauthor_mt_nodate,
	title = {The {M}.{T}.{A}. {Is} {Breached} by {Hackers} as {Cyberattacks} {Surge} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/06/02/nyregion/mta-cyber-attack.html},
	urldate = {2022-01-03},
}

@article{sanger_pipeline_2021,
	chapter = {U.S.},
	title = {Pipeline {Attack} {Yields} {Urgent} {Lessons} {About} {U}.{S}. {Cybersecurity}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/05/14/us/politics/pipeline-hack.html},
	abstract = {The hack underscored how vulnerable government and industry are to even basic assaults on computer networks.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Sanger, David E. and Perlroth, Nicole},
	month = may,
	year = {2021},
	keywords = {Biden, Joseph R Jr, Colonial Pipeline Co, Cyberwarfare and Defense, DarkSide (Hacking Group), Gordon, Susan M, Pipelines, United States Cyber Command, United States Politics and Government},
}

@article{perlroth_cyberattacks_2018,
	chapter = {U.S.},
	title = {Cyberattacks {Put} {Russian} {Fingers} on the {Switch} at {Power} {Plants}, {U}.{S}. {Says}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2018/03/15/us/politics/russia-cyberattacks.html},
	abstract = {In the last year, Russian hackers have gone from infiltrating business networks of energy, water and nuclear plants to worming their way into control rooms.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Perlroth, Nicole and Sanger, David E.},
	month = mar,
	year = {2018},
	keywords = {Cyberwarfare and Defense, Electric Light and Power, Nuclear Energy, Russia, Russian Interference in 2016 US Elections and Ties to Trump Associates, United States, United States International Relations, United States Politics and Government, Water},
}

@article{robles_dangerous_2021,
	chapter = {U.S.},
	title = {‘{Dangerous} {Stuff}’: {Hackers} {Tried} to {Poison} {Water} {Supply} of {Florida} {Town}},
	issn = {0362-4331},
	shorttitle = {‘{Dangerous} {Stuff}’},
	url = {https://www.nytimes.com/2021/02/08/us/oldsmar-florida-water-supply-hack.html},
	abstract = {For years, cybersecurity experts have warned of attacks on small municipal systems. In Oldsmar, Fla., the levels of lye were changed and could have sickened residents.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Robles, Frances and Perlroth, Nicole},
	month = feb,
	year = {2021},
	keywords = {Cyberwarfare and Defense, Florida, Gualtieri, Bob, Tampa (Fla), Water},
}

@article{bowles_thermostats_2018,
	chapter = {Technology},
	title = {Thermostats, {Locks} and {Lights}: {Digital} {Tools} of {Domestic} {Abuse}},
	issn = {0362-4331},
	shorttitle = {Thermostats, {Locks} and {Lights}},
	url = {https://www.nytimes.com/2018/06/23/technology/smart-home-devices-domestic-abuse.html},
	abstract = {Internet-connected home devices that are marketed as the newest conveniences are also being used to harass, monitor and control.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Bowles, Nellie},
	month = jun,
	year = {2018},
	keywords = {Domestic Violence, Home Appliances, Home Automation and Smart Homes, Mobile Applications, Women and Girls},
}

@misc{noauthor_tesla_nodate,
	title = {Tesla {Autopilot} {Faces} {U}.{S}. {Inquiry} {After} {Series} of {Crashes} - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/08/16/business/tesla-autopilot-nhtsa.html},
	urldate = {2022-01-03},
}

@article{caron_more_2020,
	chapter = {Parenting},
	title = {‘{More} {Anxiety} {Than} {Relief}’: {Baby} {Monitors} {That} {Track} {Vital} {Signs} {Are} {Raising} {Questions}},
	issn = {0362-4331},
	shorttitle = {‘{More} {Anxiety} {Than} {Relief}’},
	url = {https://www.nytimes.com/2020/04/17/parenting/owlet-baby-monitor.html},
	abstract = {After a popular app stopped receiving medical data, some families wondered how reliable monitoring is.},
	language = {en-US},
	urldate = {2022-01-03},
	journal = {The New York Times},
	author = {Caron, Christina},
	month = apr,
	year = {2020},
	keywords = {Babies and Infants, Children and Childhood, Company Reports, Cribs (Baby Beds), Infant Mortality, Mobile Applications, Parenting, Sleep, Sudden Infant Death Syndrome},
}

@article{sharkey_identification_2016,
	title = {Identification and {Classification} of {Restoration} {Interdependencies} in the {Wake} of {Hurricane} {Sandy}},
	volume = {22},
	issn = {1076-0342, 1943-555X},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29IS.1943-555X.0000262},
	doi = {10.1061/(ASCE)IS.1943-555X.0000262},
	abstract = {This paper introduces the new concept of restoration interdependencies that exist among infrastructures during their restoration efforts after an extreme event. Restoration interdependencies occur whenever a restoration task in one infrastructure is impacted by a restoration task, or lack thereof, in another infrastructure. This work identifies examples of observed restoration interdependencies during the restoration efforts after Hurricane Sandy as reported by major newspapers in the affected areas. A classification scheme for the observed restoration interdependencies is provided that includes five distinct classes: traditional precedence, effectiveness precedence, options precedence, time-sensitive options, and competition for resources. This work provides an overview of these different classes by providing the frequency they were observed, the infrastructures involved with the restoration interdependency, and a discussion of their potential impact on interdependent infrastructure restoration. The analysis is important because it provides a new understanding of how the restoration efforts of infrastructures are linked across systems and motivates the need for potential information-sharing in interdependent infrastructure restoration. DOI: 10.1061/(ASCE)IS.1943-555X.0000262. © 2015 American Society of Civil Engineers.},
	language = {en},
	number = {1},
	urldate = {2022-01-03},
	journal = {Journal of Infrastructure Systems},
	author = {Sharkey, Thomas C. and Nurre, Sarah G. and Nguyen, Huy and Chow, Joe H. and Mitchell, John E. and Wallace, William A.},
	month = mar,
	year = {2016},
	keywords = {IoTFailurePaper},
	pages = {04015007},
}

@article{zhou_delineating_2020,
	title = {Delineating {Infrastructure} {Failure} {Interdependencies} and {Associated} {Stakeholders} through {News} {Mining}: {The} {Case} of {Hong} {Kong}’s {Water} {Pipe} {Bursts}},
	volume = {36},
	issn = {0742-597X, 1943-5479},
	shorttitle = {Delineating {Infrastructure} {Failure} {Interdependencies} and {Associated} {Stakeholders} through {News} {Mining}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29ME.1943-5479.0000821},
	doi = {10.1061/(ASCE)ME.1943-5479.0000821},
	abstract = {The failure of one infrastructure system could trigger cascading impacts on other interdependent infrastructures. In order to improve the management of diverse infrastructure systems, decision makers should be mindful of infrastructure failure interdependencies (IFIs) and associated stakeholders when the failure of a particular infrastructure occurs. Currently, approaches to identify IFIs and associated stakeholders rely heavily on expert knowledge or limited historical records. To complement the shortage of empirical evidence, a synthetic approach that exploits media news is proposed to delineate the patterns of IFIs and stakeholders associated with the initial infrastructure failure. The integrated approach collects and cleanses the corpus from news articles, prepares the domain knowledge components, recognizes the affected infrastructure and stakeholder entities, verifies the information captured, applies association rule learning to discover IFI chains, and adopts a network analysis to depict associated stakeholders. Incidents of bursting water pipes in Hong Kong are used as a case study to demonstrate the proposed approach with 2,828 news articles being collected and analyzed. Altogether, 18 one-order or second-order IFI rules are identified. Besides, 25 associated stakeholders are delineated from the news, and they are divided into three tiers according to their degree centralities. The findings provide insightful information to policymakers for helping to respond to the cascading effects among infrastructures and coordinate a wide spectrum of stakeholders who might be embroiled. DOI: 10.1061/(ASCE)ME.1943-5479.0000821. © 2020 American Society of Civil Engineers.},
	language = {en},
	number = {5},
	urldate = {2022-01-02},
	journal = {Journal of Management in Engineering},
	author = {Zhou, Shenghua and Ng, S. Thomas and Yang, Yifan and Xu, J. Frank},
	month = sep,
	year = {2020},
	keywords = {IoTFailurePaper},
	pages = {04020060},
}

@inproceedings{jimenez_empirical_2016-1,
	title = {An {Empirical} {Analysis} of {Vulnerabilities} in {OpenSSL} and the {Linux} {Kernel}},
	doi = {10.1109/APSEC.2016.025},
	abstract = {Vulnerabilities are one of the main concerns faced by practitioners when working with security critical applications. Unfortunately, developers and security teams, even experienced ones, fail to identify many of them with severe consequences. Vulnerabilities are hard to discover since they appear in various forms, caused by many different issues and their identification requires an attacker's mindset. In this paper, we aim at increasing the understanding of vulnerabilities by investigating their characteristics on two major open-source software systems, i.e., the Linux kernel and OpenSSL. In particular, we seek to analyse and build a profile for vulnerable code, which can ultimately help researchers in building automated approaches like vulnerability prediction models. Thus, we examine the location, criticality and category of vulnerable code along with its relation with software metrics. To do so, we collect more than 2,200 vulnerable files accounting for 863 vulnerabilities and compute more than 35 software metrics. Our results indicate that while 9 Common Weakness Enumeration (CWE) types of vulnerabilities are prevalent, only 3 of them are critical in OpenSSL and 2 of them in the Linux kernel. They also indicate that different types of vulnerabilities have different characteristics, i.e., metric profiles, and that vulnerabilities of the same type have different profiles in the two projects we examined. We also found that the file structure of the projects can provide useful information related to the vulnerabilities. Overall, our results demonstrate the need for making project specific approaches that focus on specific types of vulnerabilities.},
	booktitle = {2016 23rd {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Jimenez, Matthieu and Papadakis, Mike and Traon, Yves Le},
	month = dec,
	year = {2016},
	note = {ISSN: 1530-1362},
	keywords = {Common Vulnerability Exposures, Kernel, Linux, Predictive models, Security, Software Metrics, Software Security, Software metrics, Vulnerabilities},
	pages = {105--112},
}

@article{aslam_use_nodate,
	title = {Use of {A} {Taxonomy} of {Security} {Faults}},
	language = {en},
	author = {Aslam, Taimur and Krsul, Ivan and Spafford, Eugene H},
	pages = {12},
}

@article{landwehr_taxonomy_1994,
	title = {A taxonomy of computer program security flaws},
	volume = {26},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/185403.185412},
	doi = {10.1145/185403.185412},
	abstract = {An organized record of actual flaws can be useful to computer system designers, programmers, analysts, administrators, and users. This survey provides a taxonomy for computer program security flaws, with an Appendix that documents 50 actual security flaws. These flaws have all been described previously in the open literature, but in widely separated places. For those new to the field of computer security, they provide a good introduction to the characteristics of security flaws and how they can arise. Because these flaws were not randomly selected from a valid statistical sample of such flaws, we make no strong claims concerning the likely distribution of actual security flaws within the taxonomy. However, this method of organizing security flaw data can help those who have custody of more representative samples to organize them and to focus their efforts to remove and, eventually, to prevent the introduction of security flaws.},
	number = {3},
	urldate = {2021-12-31},
	journal = {ACM Computing Surveys},
	author = {Landwehr, Carl E. and Bull, Alan R. and McDermott, John P. and Choi, William S.},
	month = sep,
	year = {1994},
	keywords = {error/defect classification, security flaw, taxonomy},
	pages = {211--254},
}

@article{aslam_use_nodate-1,
	title = {Use of {A} {Taxonomy} of {Security} {Faults}},
	language = {en},
	author = {Aslam, Taimur and Krsul, Ivan and Spafford, Eugene H},
	pages = {12},
}

@book{leveson1995safeware,
	address = {New York, NY, USA},
	title = {Safeware: {System} safety and computers},
	isbn = {0-201-11972-2},
	publisher = {ACM},
	author = {Leveson, Nancy G.},
	year = {1995},
	keywords = {IoTFailurePaper},
}

@incollection{gervasi_pathology_2021,
	address = {Cham},
	title = {The {Pathology} of {Failures} in {IoT} {Systems}},
	volume = {12957},
	isbn = {978-3-030-87012-6 978-3-030-87013-3},
	url = {https://link.springer.com/10.1007/978-3-030-87013-3_33},
	abstract = {The presence of faults is inevitable in the Internet of Things (IoT) systems. Dependability in these systems is challenging due to the increasing level of dynamicity, heterogeneity, and complexity. IoT connects anything, anytime, and everywhere, introducing a complex relationship of interdependence, generating an increase in the susceptibility of the propagation of failures. The purpose of this study is to propose a pathology of failure in IoT Systems, exploring and characterizing faults, errors, failures, and their eﬀects. This study investigates and classiﬁes the source of faults, deﬁnes a taxonomy of the types of faults prone to happen, and deﬁnes the failure propagation model. As a result, the pathology establishes a common reference for fault, errors, and failures to be used by researchers and practitioners to improve tools for fault detection, fault diagnosis, fault tolerance, and fault handling in IoT Systems. This paper also proposes a failure propagation model for IoT systems that identify diﬀerent combinations, paths, and fault-failure propagation eﬀects.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2021},
	publisher = {Springer International Publishing},
	author = {Melo, Mário and Aquino, Gibeon},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blečić, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria},
	year = {2021},
	doi = {10.1007/978-3-030-87013-3_33},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {IoTFailurePaper},
	pages = {437--452},
}

@inproceedings{makhshari_iot_2021,
	address = {Madrid, ES},
	title = {{IoT} {Bugs} and {Development} {Challenges}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9402092/},
	doi = {10.1109/ICSE43902.2021.00051},
	abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners’ point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers’ challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Makhshari, Amir and Mesbah, Ali},
	year = {2021},
	keywords = {IoTFailurePaper},
	pages = {460--472},
}

@inproceedings{luo_internet_2020,
	title = {An {Internet} of {Things} ({loT}) {Perspective} of {Understanding} the {Boeing} 737 {MAX} {Crash}},
	doi = {10.1109/PHM-Shanghai49105.2020.9280967},
	abstract = {Recently, two serious airplane crashes involving Ethiopian Airlines and Lion Airlines have greatly impacted the aviation industry, and sparked worldwide discussions as well as investigations from government agencies to original equipment manufacturers. This paper first briefly describes the two airplane crashes and presents a summary report. Then the possible causes of the crashes are analyzed from the design of the stall protection system and other aspects, including airworthiness certification, aircraft management and support. Especially, the crashes are analyzed from the Internet of Things (IoT) perspective and system reliability recommendations are presented. Lastly, discussions concerning the balance and integration between IoT systems and human beings are presented based on the multiple investigation outcomes.},
	booktitle = {2020 {Global} {Reliability} and {Prognostics} and {Health} {Management} ({PHM}-{Shanghai})},
	author = {Luo, Pan and Li, Meiyan and Li, Zhaojun Steven},
	month = oct,
	year = {2020},
	keywords = {Accidents, Airplanes, Boeing 737 MAX, Government, Industries, Internet of Things, IoT, IoTFailurePaper, Prognostics and health management, Reliability, crash, reliability},
	pages = {1--8},
}

@article{bagchi_new_2020,
	title = {New {Frontiers} in {IoT}: {Networking}, {Systems}, {Reliability}, and {Security} {Challenges}},
	volume = {7},
	issn = {2327-4662, 2372-2541},
	shorttitle = {New {Frontiers} in {IoT}},
	url = {https://ieeexplore.ieee.org/document/9136673/},
	doi = {10.1109/JIOT.2020.3007690},
	abstract = {The ﬁeld of IoT has blossomed and is positively inﬂuencing many application domains. In this article, we bring out the unique challenges this ﬁeld poses to research in computer systems and networking. The unique challenges arise from the unique characteristics of IoT systems such as the diversity of application domains where they are used and the increasingly demanding protocols they are being called upon to run (such as video and LIDAR processing) on constrained resources (on-node and network). We show how these open challenges can beneﬁt from foundations laid in other areas, such as ﬁfth-generation network cellular protocols, machine learning model reduction, and device–edge–cloud ofﬂoading. We then discuss the unique challenges for reliability, security, and privacy posed by IoT systems due to their salient characteristics which include heterogeneity of devices and protocols, dependence on the physical environment, and the close coupling with humans. We again show how open research challenges beneﬁt from the reliability, security, and privacy advancements in other areas. We conclude by providing a vision for a desirable end state for IoT systems.},
	language = {en},
	number = {12},
	urldate = {2021-09-21},
	journal = {IEEE Internet of Things Journal},
	author = {Bagchi, Saurabh and Abdelzaher, Tarek F. and Govindan, Ramesh and Shenoy, Prashant and Atrey, Akanksha and Ghosh, Pradipta and Xu, Ran},
	year = {2020},
	keywords = {IoTFailurePaper},
	pages = {11330--11346},
}

@article{eschelbeck_laws_2005,
	title = {The {Laws} of {Vulnerabilities}: {Which} security vulnerabilities really matter?},
	volume = {10},
	issn = {13634127},
	shorttitle = {The {Laws} of {Vulnerabilities}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1363412705000646},
	doi = {10.1016/j.istr.2005.09.005},
	abstract = {New security vulnerabilities are discovered on a daily basis. With each new announcement, the same questions arise. How signiﬁcant is this vulnerability? How prevalent? How easy is it to exploit? Due to a lack of global vulnerability data, answers are hard to ﬁnd and risk rating is even more difﬁcult. The Laws of Vulnerabilities are the conclusions of analyzing statistical vulnerability information over a three-year period. Those vulnerabilities have been identiﬁed in the real world across hundreds of thousands of systems and networks. These data are not identiﬁable to individual users or systems. However, it provides signiﬁcant statistical data for research and analysis, which enabled us to deﬁne and publish the Laws of Vulnerabilities (http://www.qualys.com/research/rnd/vulnlaws/).},
	language = {en},
	number = {4},
	urldate = {2021-12-28},
	journal = {Information Security Technical Report},
	author = {Eschelbeck, Gerhard},
	month = jan,
	year = {2005},
	pages = {213--219},
}

@inproceedings{cai_understanding_2019,
	address = {New York, NY, USA},
	series = {{APSys} '19},
	title = {Understanding {Security} {Vulnerabilities} in {File} {Systems}},
	isbn = {978-1-4503-6893-3},
	url = {https://doi.org/10.1145/3343737.3343753},
	doi = {10.1145/3343737.3343753},
	abstract = {File systems have been developed for decades with the security-critical foundation provided by operating systems. However, they are still vulnerable to malware attacks and software defects. In this paper, we undertake the first attempt to systematically understand the security vulnerabilities in various file systems. We conduct an empirical study of 157 real cases reported in Common Vulnerabilities and Exposures (CVE). We characterize the file system vulnerabilities in different dimensions that include the common vulnerabilities leveraged by adversaries to initiate their attacks, their exploitation procedures, root causes, consequences, and mitigation approaches. We believe the insights derived from this study have broad implications related to the further enhancement of the security aspect of file systems, and the associated vulnerability detection tools.},
	urldate = {2021-12-27},
	booktitle = {Proceedings of the 10th {ACM} {SIGOPS} {Asia}-{Pacific} {Workshop} on {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Miao and Huang, Hao and Huang, Jian},
	month = aug,
	year = {2019},
	pages = {8--15},
}

@article{noghabi_emerging_2020,
	title = {The {Emerging} {Landscape} of {Edge} {Computing}},
	volume = {23},
	issn = {2375-0529, 2375-0537},
	url = {https://dl.acm.org/doi/10.1145/3400713.3400717},
	doi = {10.1145/3400713.3400717},
	abstract = {On examining current edge computing deployments (edge-site deployments) we find significantly different characteristics than the original vision of edge computing (cyber foraging). In current deployments, edge clusters are deployed by a single entity (not multi-tenant) with enterprise applications (rather than consumer applications) driving the deployments. Table 2 expands on these points.},
	language = {en},
	number = {4},
	urldate = {2021-12-20},
	journal = {GetMobile: Mobile Computing and Communications},
	author = {Noghabi, Shadi A. and Cox, Landon and Agarwal, Sharad and Ananthanarayanan, Ganesh},
	month = may,
	year = {2020},
	pages = {11--20},
}

@article{parker_build_2020,
	title = {Build {It}, {Break} {It}, {Fix} {It}: {Contesting} {Secure} {Development}},
	volume = {23},
	issn = {2471-2566, 2471-2574},
	shorttitle = {Build {It}, {Break} {It}, {Fix} {It}},
	url = {https://dl.acm.org/doi/10.1145/3383773},
	doi = {10.1145/3383773},
	abstract = {Typical security contests focus on breaking or mitigating the impact of buggy systems. We present the Build-it, Break-it, Fix-it (BIBIFI) contest, which aims to assess the ability to securely build software, not just break it. In BIBIFI, teams build specified software with the goal of maximizing correctness, performance, and security. The latter is tested when teams attempt to break other teams’ submissions. Winners are chosen from among the best builders and the best breakers. BIBIFI was designed to be open-ended—teams can use any language, tool, process, and so on, that they like. As such, contest outcomes shed light on factors that correlate with successfully building secure software and breaking insecure software. We ran three contests involving a total of 156 teams and three different programming problems. Quantitative analysis from these contests found that the most efficient build-it submissions used C/C++, but submissions coded in a statically type safe language were 11× less likely to have a security flaw than C/C++ submissions. Break-it teams that were also successful build-it teams were significantly better at finding security bugs.},
	language = {en},
	number = {2},
	urldate = {2021-12-15},
	journal = {ACM Transactions on Privacy and Security},
	author = {Parker, James and Hicks, Michael and Ruef, Andrew and Mazurek, Michelle L. and Levin, Dave and Votipka, Daniel and Mardziel, Piotr and Fulton, Kelsey R.},
	year = {2020},
	keywords = {gamification},
	pages = {1--36},
}

@inproceedings{hornbrook_intercultural_2021,
	title = {An {Intercultural} {Engineering} {Module} for {Software} {Engineers}},
	abstract = {The world is continuing to intertwine more and more because of technology, and, as a result, it is becoming more important to develop software products that are culturally inclusive and acceptable. It is important to educate future software engineers about intercultural engineering, both in terms of requirements and in terms of engineering collaboration. In this poster we discuss a learning module for a software engineering course on Intercultural Engineering. We describe three case studies for use in assignments or in-class discussion.},
	language = {en},
	booktitle = {24th {Annual} {Colloquium} on {International} {Engineering} {Education} ({ACIEE})},
	author = {Hornbrook, Nicole and Davis, James C},
	year = {2021},
}

@techreport{jones2006economics,
	title = {The economics of software maintenance in the twenty first century},
	author = {Jones, Capers},
	year = {2006},
}

@article{reason2006revisiting,
	title = {Revisiting the {Swiss} cheese model of accidents},
	volume = {27},
	number = {4},
	journal = {Journal of Clinical Engineering},
	author = {Reason, J and Hollnagel, E and Paries, J},
	year = {2006},
	pages = {110--115},
}

@article{larouzee_good_2020,
	title = {Good and bad reasons: {The} {Swiss} cheese model and its critics},
	volume = {126},
	issn = {09257535},
	shorttitle = {Good and bad reasons},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925753520300576},
	doi = {10.1016/j.ssci.2020.104660},
	abstract = {This article provides a historical and critical account of James Reason’s contribution to safety research with a focus on the Swiss cheese model (SCM), its developments and its critics. This article shows that the SCM is a product of specific historical circumstances, has been developed over a ten years period following several steps, and has benefited of the direct influence of John Wreathall. Reason took part in intense intellectual debates and publications in the 1980s during which many ideas circulated among researchers, featuring authors as influent as Donald Norman, Jens Rasmussen, Charles Perrow or Barry Turner. The 1980s and 1990s were highly productive from a safety research point of view (e.g. human error, incubation models, high reliability organisation, safety culture) and Reason has considerably influenced it with a rich production of models, based on both research and industrial projects. Historical perspectives offer interesting insights because they can question research, the conditions of its production, its relevance and, sometimes, its success, as for the SCM. But, because of this success, critics have vividly argued about some of the SCM limitations, including its simplistic vision of accidents and its degree of generality. Against these positions, the article develops a ‘critique of the criticism’, and the article concludes that the SCM remains a relevant model because of its systemic foundations and its sustained use in high-risk industries; despite of course, the need to keep imagining alternatives based on the mix of collective empirical, practical and graphical research which was in the SCM background.},
	language = {en},
	urldate = {2021-12-15},
	journal = {Safety Science},
	author = {Larouzee, Justin and Le Coze, Jean-Christophe},
	month = jun,
	year = {2020},
	pages = {104660},
}

@article{jones2009motivating,
	title = {Motivating students to engage in learning: the {MUSIC} model of academic motivation.},
	volume = {21},
	number = {2},
	journal = {International Journal of Teaching and Learning in Higher Education},
	author = {Jones, Brett D},
	year = {2009},
	note = {Publisher: ERIC},
	pages = {272--285},
}

@misc{loukides_thinking_2021,
	title = {Thinking {About} {Glue} – {O}’{Reilly}},
	url = {https://www.oreilly.com/radar/thinking-about-glue/},
	urldate = {2021-12-15},
	author = {Loukides, Mike},
	year = {2021},
}

@inproceedings{baltes_sketches_2014,
	address = {Hong Kong China},
	title = {Sketches and diagrams in practice},
	isbn = {978-1-4503-3056-5},
	url = {https://dl.acm.org/doi/10.1145/2635868.2635891},
	doi = {10.1145/2635868.2635891},
	abstract = {Sketches and diagrams play an important role in the daily work of software developers. In this paper, we investigate the use of sketches and diagrams in software engineering practice. To this end, we used both quantitative and qualitative methods. We present the results of an exploratory study in three companies and an online survey with 394 participants. Our participants included software developers, software architects, project managers, consultants, as well as researchers. They worked in diﬀerent countries and on projects from a wide range of application areas. Most questions in the survey were related to the last sketch or diagram that the participants had created. Contrary to our expectations and previous work, the majority of sketches and diagrams contained at least some UML elements. However, most of them were informal. The most common purposes for creating sketches and diagrams were designing, explaining, and understanding, but analyzing requirements was also named often. More than half of the sketches and diagrams were created on analog media like paper or whiteboards and have been revised after creation. Most of them were used for more than a week and were archived. We found that the majority of participants related their sketches to methods, classes, or packages, but not to source code artifacts with a lower level of abstraction.},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Baltes, Sebastian and Diehl, Stephan},
	year = {2014},
	pages = {530--541},
}

@inproceedings{shaffer_impact_2021,
	address = {Virtual Event USA},
	title = {The {Impact} of {Programming} {Project} {Milestones} on {Procrastination}, {Project} {Outcomes}, and {Course} {Outcomes}: {A} {Quasi}-{Experimental} {Study} in a {Third}-{Year} {Data} {Structures} {Course}},
	isbn = {978-1-4503-8062-1},
	shorttitle = {The {Impact} of {Programming} {Project} {Milestones} on {Procrastination}, {Project} {Outcomes}, and {Course} {Outcomes}},
	url = {https://dl.acm.org/doi/10.1145/3408877.3432356},
	doi = {10.1145/3408877.3432356},
	abstract = {When faced with a large and complex project for the first time, students face numerous self-regulatory challenges that they may be ill-equipped to overcome. These challenges can result in degraded project outcomes, as commonly observed in programming-intensive mid-level CS courses. We have previously found that success in these situations is associated with a disciplined personal software process. Procrastination is a prominent failure of self-regulation that can occur for a number of reasons, e.g., low expectancy of success, low perceived value of the task at hand, or decision-paralysis regarding how to begin when faced with a large task. It is pervasive, but may be addressed through targeted interventions. We draw on theory related to goal theory and problem-solving in engineering education to evaluate the value of explicit project milestones at curbing procrastination and its negative impacts on relatively long-running software projects. We conduct a quasi-experiment in which we study differences in project and course outcomes between students in a treatment (with milestones) and control group (without milestones). We found that students in the treatment group were more likely to finish their projects on time, produced projects with higher correctness, and finished the course with generally better outcomes. Within the treatment group, we found that students who completed more milestones saw better outcomes than those who completed fewer milestones. We found no differences in withdrawal or failure rates between the treatment and control groups. An end-of-term survey indicated that student perceptions of the milestones were overwhelmingly positive.},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 52nd {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Shaffer, Clifford A. and Kazerouni, Ayaan M.},
	year = {2021},
	pages = {907--913},
}

@inproceedings{shamshiri_automatically_2015,
	title = {Do {Automatically} {Generated} {Unit} {Tests} {Find} {Real} {Faults}? {An} {Empirical} {Study} of {Effectiveness} and {Challenges} ({T})},
	shorttitle = {Do {Automatically} {Generated} {Unit} {Tests} {Find} {Real} {Faults}?},
	doi = {10.1109/ASE.2015.86},
	abstract = {Rather than tediously writing unit tests manually, tools can be used to generate them automatically - sometimes even resulting in higher code coverage than manual testing. But how good are these tests at actually finding faults? To answer this question, we applied three state-of-the-art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. Although the automatically generated test suites detected 55.7\% of the faults overall, only 19.9\% of all the individual test suites detected a fault. By studying the effectiveness and problems of the individual tools and the tests they generate, we derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. These insights include 1) improving the obtained code coverage so that faulty statements are executed in the first instance, 2) improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, and 3) improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time.},
	booktitle = {2015 30th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Shamshiri, Sina and Just, René and Rojas, José Miguel and Fraser, Gordon and McMinn, Phil and Arcuri, Andrea},
	month = nov,
	year = {2015},
	keywords = {Generators, Java, Manuals, Software, Testing, Writing, automated test generation, empirical study, regression testing, test effectiveness, unit testing},
	pages = {201--211},
}

@inproceedings{chen_learning-guided_2019,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	doi = {10.1109/ASE.2019.00093},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds-a water purification plant and a water distribution system-finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chen, Yuqi and Poskitt, Christopher M. and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Actuators, Benchmark testing, Fuzzing, Machine learning, Monitoring, Predictive models, benchmark generation, cyber-physical systems, fuzzing, machine learning, metaheuristic optimisation, testing},
	pages = {962--973},
}

@article{hellendoorn_are_2019,
	title = {Are {My} {Invariants} {Valid}? {A} {Learning} {Approach}},
	shorttitle = {Are {My} {Invariants} {Valid}?},
	url = {http://arxiv.org/abs/1903.06089},
	abstract = {Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.},
	urldate = {2021-12-12},
	journal = {arXiv:1903.06089 [cs]},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Polozov, Oleksandr and Marron, Mark},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06089},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{almaghairbe_machine_2020,
	address = {New York, NY, USA},
	series = {{ICEMIS}'20},
	title = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}: {Empirical} {Evaluation} {Study}},
	isbn = {978-1-4503-7736-2},
	shorttitle = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}},
	url = {https://doi.org/10.1145/3410352.3410747},
	doi = {10.1145/3410352.3410747},
	abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
	urldate = {2021-12-12},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Engineering} \& {MIS} 2020},
	publisher = {Association for Computing Machinery},
	author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
	month = sep,
	year = {2020},
	keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
	pages = {1--12},
}

@article{wang_detecting_2017,
	title = {Detecting {Bugs} of {Concurrent} {Programs} {With} {Program} {Invariants}},
	volume = {66},
	issn = {1558-1721},
	doi = {10.1109/TR.2017.2681107},
	abstract = {Concurrency bug detection is a time-consuming activity in the debugging process for concurrent programs. Existing techniques mainly focus on detecting data race bugs with pattern analysis; however, the number of interleaving patterns could be huge, only the most suspicious write-read pattern is given, and an oracle is needed, which is not available in the operational phase. This paper proposes a program-invariant-based technique to detect a class of concurrent program bugs. By unit testing of the components of a concurrent program, we obtain a set of program invariants, which can be used as an oracle to obtain “bad” invariants when the program is online. By using the function call graph of the components and applying a reduction technique to the invariants, we find the candidates of suspicious functions and rank them. From the interactions among components, we analyze the causes to the concurrency bugs. Experimental results show that our proposed technique is effective in concurrency bug detection.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Rong and Ding, Zuohua and Gui, Ning and Liu, Yang},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Reliability},
	keywords = {Bug detection, Computer bugs, Concurrent computing, Message systems, Pattern analysis, Programming, System recovery, Testing, concurrent program, function call graph, program invariants, suspicious rate},
	pages = {425--439},
}

@inproceedings{almaghairbe_empirical_2019,
	title = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}: {Machine} {Learning} {Versus} {Dynamic} {Analysis}},
	shorttitle = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}},
	doi = {10.1109/ISSREW.2019.00099},
	abstract = {Software testing is an established method to ensure software quality and reliability, but it is an expensive process. In recent years, the automation of test case generation has received significant attention as a way to reduce costs. However, the oracle problem (a mechanism for determine the (in) correctness of an executed test case) is still major problem which has been largely ignored. Recent work has shown that building a test oracle using the principles of anomaly detection techniques (mainly semisupervised/ unsupervised learning models based on dynamic execution data consisting of an amalgamation of input/output pairs and execution traces) is able to demonstrate a reasonable level of success in automatically detect passing and failing execution [1], [2]. In this paper, we present a comparison study between our machine-learning based approaches and an existing techniques from the specification mining domain (the data invariant detector Daikon [3]). The two approaches are evaluated on a range of midsized systems and compared in terms of their fault detection ability. The results show that in most cases semi-supervised learning techniques perform far better as an automated test classifier than Daikon. However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Almaghairbe, Rafig and Roper, Marc},
	month = oct,
	year = {2019},
	keywords = {Automated Test Oracles, Empirical Study, Machine Learning Techniques, Software Testing},
	pages = {378--385},
}

@article{bodei_measuring_2019,
	series = {Selected papers of {ICTCS} 2016 ({The} {Italian} {Conference} on {Theoretical} {Computer} {Science} ({ICTCS})},
	title = {Measuring security in {IoT} communications},
	volume = {764},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397518307205},
	doi = {10.1016/j.tcs.2018.12.002},
	abstract = {More smart objects and more applications on the Internet of Things (IoT) mean more security challenges. In IoT security is crucial but difficult to obtain. On the one hand the usual trade-off between highly secure and usable systems is more impelling than ever; on the other hand security is considered a feature that has a cost often unaffordable. Therefore, IoT designers not only need tools to assess possible risks and to study countermeasures, but also methodologies to estimate their costs. Here, we present a methodology, based on the process calculus IoT-LySa, to infer quantitative measures on evolution of systems. The derived quantitative evaluation is exploited to establish the cost of the possible security countermeasures, in terms of time and energy.},
	language = {en},
	urldate = {2021-12-09},
	journal = {Theoretical Computer Science},
	author = {Bodei, Chiara and Chessa, Stefano and Galletta, Letterio},
	month = apr,
	year = {2019},
	keywords = {Cost evaluation, Internet of Things, Security},
	pages = {100--124},
}

@inproceedings{toussaint_machine_2020,
	title = {Machine {Learning} {Systems} in the {IoT}: {Trustworthiness} {Trade}-offs for {Edge} {Intelligence}},
	shorttitle = {Machine {Learning} {Systems} in the {IoT}},
	doi = {10.1109/CogMI50398.2020.00030},
	abstract = {Machine learning systems (MLSys) are emerging in the Internet of Things (IoT) to provision edge intelligence, which is paving our way towards the vision of ubiquitous intelligence. However, despite the maturity of machine learning systems and the IoT, we are facing severe challenges when integrating MLSys and IoT in practical context. For instance, many machine learning systems have been developed for large-scale production (e.g., cloud environments), but IoT introduces additional demands due to heterogeneous and resource-constrained devices and decentralized operation environment. To shed light on this convergence of MLSys and IoT, this paper analyzes the tradeoffs by covering the latest developments (up to 2020) on scaling and distributing ML across cloud, edge, and IoT devices. We position machine learning systems as a component of the IoT, and edge intelligence as a socio-technical system. On the challenges of designing trustworthy edge intelligence, we advocate a holistic design approach that takes multi-stakeholder concerns, design requirements and trade-offs into consideration, and highlight the future research opportunities in edge intelligence.},
	booktitle = {2020 {IEEE} {Second} {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Toussaint, Wiebke and Ding, Aaron Yi},
	month = oct,
	year = {2020},
	keywords = {Adaptation models, Computational modeling, Data models, Internet of Things, Machine learning, Predictive models, Training, edge intelligence, machine learning systems, smart services, trade-offs, trustworthiness},
	pages = {177--184},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} {Systems} in the {IoT}: {Trustworthiness} {Trade}-offs for {Edge} {Intelligence}},
	shorttitle = {Machine {Learning} {Systems} in the {IoT}},
	url = {https://ieeexplore-ieee-org.ezproxy.lib.purdue.edu/document/9319287/},
	abstract = {Machine learning systems (MLSys) are emerging in the Internet of Things (IoT) to provision edge intelligence, which is paving our way towards the vision of ubiquitous intelligence. However, despite the maturity of machine learning systems and the IoT, we are facing severe challenges when integrating MLSys and IoT in practical context. For instance, many machine learning systems have been developed for large-scale production (e.g., cloud environments), but IoT introduces additional demands due to heterogeneous and resource-constrained devices and decentralized operation environment. To shed light on this convergence of MLSys and IoT, this paper analyzes the tradeoffs by covering the latest developments (up to 2020) on scaling and distributing ML across cloud, edge, and IoT devices. We position machine learning systems as a component of the IoT, and edge intelligence as a socio-technical system. On the challenges of designing trustworthy edge intelligence, we advocate a holistic design approach that takes multi-stakeholder concerns, design requirements and trade-offs into consideration, and highlight the future research opportunities in edge intelligence.},
	language = {en-US},
	urldate = {2021-12-09},
}

@book{heldal_functional_1992,
	address = {London},
	series = {Workshops in {Computing}},
	title = {Functional {Programming}, {Glasgow} 1991: {Proceedings} of the 1991 {Glasgow} {Workshop} on {Functional} {Programming}, {Portree}, {Isle} of {Skye}, 12–14 {August} 1991},
	isbn = {978-3-540-19760-7 978-1-4471-3196-0},
	shorttitle = {Functional {Programming}, {Glasgow} 1991},
	url = {http://link.springer.com/10.1007/978-1-4471-3196-0},
	language = {en},
	urldate = {2021-12-09},
	publisher = {Springer London},
	editor = {Heldal, Rogardt and Holst, Carsten Kehler and Wadler, Philip and van Rijsbergen, C. J.},
	year = {1992},
	doi = {10.1007/978-1-4471-3196-0},
}

@inproceedings{nagappan_diversity_2013,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2013},
	title = {Diversity in software engineering research},
	isbn = {978-1-4503-2237-9},
	url = {http://doi.org/10.1145/2491411.2491415},
	doi = {10.1145/2491411.2491415},
	abstract = {One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nagappan, Meiyappan and Zimmermann, Thomas and Bird, Christian},
	month = aug,
	year = {2013},
	keywords = {Coverage, Diversity, Representativeness, Sampling},
	pages = {466--476},
}

@article{sutcliffe2018activity,
	title = {Activity in social media and intimacy in social relationships},
	volume = {85},
	journal = {Computers in human behavior},
	author = {Sutcliffe, Alistair G and Binder, Jens F and Dunbar, Robin IM},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {227--235},
}

@article{spaiser2017communication,
	title = {Communication power struggles on social media: {A} case study of the 2011–12 {Russian} protests},
	volume = {14},
	number = {2},
	journal = {Journal of Information Technology \& Politics},
	author = {Spaiser, Viktoria and Chadefaux, Thomas and Donnay, Karsten and Russmann, Fabian and Helbing, Dirk},
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	pages = {132--153},
}

@article{pineiro2017influence,
	title = {Influence of social media over the stock market},
	volume = {34},
	number = {1},
	journal = {Psychology \& Marketing},
	author = {Piñeiro-Chousa, Juan and Vizcaíno-González, Marcos and Pérez-Pico, Ada María},
	year = {2017},
	note = {Publisher: Wiley Online Library},
	pages = {101--108},
}

@inproceedings{arcuri_automated_2014,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Automated unit test generation for classes with environment dependencies},
	isbn = {978-1-4503-3013-8},
	url = {https://doi.org/10.1145/2642937.2642986},
	doi = {10.1145/2642937.2642986},
	abstract = {Automated test generation for object-oriented software typically consists of producing sequences of calls aiming at high code coverage. In practice, the success of this process may be inhibited when classes interact with their environment, such as the file system, network, user-interactions, etc. This leads to two major problems: First, code that depends on the environment can sometimes not be fully covered simply by generating sequences of calls to a class under test, for example when execution of a branch depends on the contents of a file. Second, even if code that is environment-dependent can be covered, the resulting tests may be unstable, i.e., they would pass when first generated, but then may fail when executed in a different environment. For example, tests on classes that make use of the system time may have failing assertions if the tests are executed at a different time than when they were generated. In this paper, we apply bytecode instrumentation to automatically separate code from its environmental dependencies, and extend the EvoSuite Java test generation tool such that it can explicitly set the state of the environment as part of the sequences of calls it generates. Using a prototype implementation, which handles a wide range of environmental interactions such as the file system, console inputs and many non-deterministic functions of the Java virtual machine (JVM), we performed experiments on 100 Java projects randomly selected from SourceForge (the SF100 corpus). The results show significantly improved code coverage - in some cases even in the order of +80\%/+90\%. Furthermore, our techniques reduce the number of unstable tests by more than 50\%.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} international conference on {Automated} software engineering},
	publisher = {Association for Computing Machinery},
	author = {Arcuri, Andrea and Fraser, Gordon and Galeotti, Juan Pablo},
	month = sep,
	year = {2014},
	keywords = {automated test generation, environment, unit testing},
	pages = {79--90},
}

@inproceedings{arcuri_automated_2014-1,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Automated unit test generation for classes with environment dependencies},
	isbn = {978-1-4503-3013-8},
	url = {https://doi.org/10.1145/2642937.2642986},
	doi = {10.1145/2642937.2642986},
	abstract = {Automated test generation for object-oriented software typically consists of producing sequences of calls aiming at high code coverage. In practice, the success of this process may be inhibited when classes interact with their environment, such as the file system, network, user-interactions, etc. This leads to two major problems: First, code that depends on the environment can sometimes not be fully covered simply by generating sequences of calls to a class under test, for example when execution of a branch depends on the contents of a file. Second, even if code that is environment-dependent can be covered, the resulting tests may be unstable, i.e., they would pass when first generated, but then may fail when executed in a different environment. For example, tests on classes that make use of the system time may have failing assertions if the tests are executed at a different time than when they were generated. In this paper, we apply bytecode instrumentation to automatically separate code from its environmental dependencies, and extend the EvoSuite Java test generation tool such that it can explicitly set the state of the environment as part of the sequences of calls it generates. Using a prototype implementation, which handles a wide range of environmental interactions such as the file system, console inputs and many non-deterministic functions of the Java virtual machine (JVM), we performed experiments on 100 Java projects randomly selected from SourceForge (the SF100 corpus). The results show significantly improved code coverage - in some cases even in the order of +80\%/+90\%. Furthermore, our techniques reduce the number of unstable tests by more than 50\%.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} international conference on {Automated} software engineering},
	publisher = {Association for Computing Machinery},
	author = {Arcuri, Andrea and Fraser, Gordon and Galeotti, Juan Pablo},
	month = sep,
	year = {2014},
	keywords = {automated test generation, environment, unit testing},
	pages = {79--90},
}

@inproceedings{galler_automatically_2010,
	address = {New York, NY, USA},
	series = {{AST} '10},
	title = {Automatically extracting mock object behavior from {Design} by {Contract}\&\#x2122; specification for test data generation},
	isbn = {978-1-60558-970-1},
	url = {https://doi.org/10.1145/1808266.1808273},
	doi = {10.1145/1808266.1808273},
	abstract = {Test data generation is an important task in the process of automated unit test generation. Random and heuristic approaches are well known for test input data generation. Unfortunately, in the presence of complex pre-conditions especially in the case of non-primitive data types those approaches often fail. A promising technique for generating an object that exactly satisfies a given pre-condition is mocking, i.e., replacing the concrete implementation with an implementation only considering the necessary behavior for a specific test case. In this paper we follow this technique and present an approach for automatically deriving the behavior of mock objects from given Design by Contract™ specifications. The generated mock objects behave according to the Design by Contract™ specification of the original class. Furthermore, we make sure that the observed behavior of the mock object satisfies the pre-condition of the method under test. We evaluate the approach using the Java implementations of 20 common Design Patterns and a stack based calculator. Our approach clearly outperforms pure random data generation in terms of line coverage.},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 5th {Workshop} on {Automation} of {Software} {Test}},
	publisher = {Association for Computing Machinery},
	author = {Galler, Stefan J. and Maller, Andreas and Wotawa, Franz},
	month = may,
	year = {2010},
	keywords = {Design-by-Contract, automated unit testing, mock object, test data generation},
	pages = {43--50},
}

@inproceedings{marri_empirical_2009,
	title = {An empirical study of testing file-system-dependent software with mock objects},
	doi = {10.1109/IWAST.2009.5069054},
	abstract = {Unit testing is a technique of testing a single unit of a program in isolation. The testability of the unit under test can be reduced when the unit interacts with its environment. The construction of high-covering unit tests and their execution require appropriate interactions with the environment such as a file system or database. To help set up the required environment, developers can use mock objects to simulate the behavior of the environment. In this paper, we present an empirical study to analyze the use of mock objects to test file-system-dependent software. We use a mock object of the FileSystem API provided with the Pex automatic testing tool in our study. We share our insights gained on the benefits of using mock objects in unit testing and discuss the faced challenges.},
	booktitle = {2009 {ICSE} {Workshop} on {Automation} of {Software} {Test}},
	author = {Marri, Madhuri R. and Xie, Tao and Tillmann, Nikolai and de Halleux, Jonathan and Schulte, Wolfram},
	month = may,
	year = {2009},
	keywords = {Automatic testing, Computer science, Databases, File systems, Logic, Software testing, System testing},
	pages = {149--153},
}

@inproceedings{gafford_synthesis-based_2020,
	address = {New York, NY, USA},
	series = {{ASE} '20},
	title = {Synthesis-based resolution of feature interactions in cyber-physical systems},
	isbn = {978-1-4503-6768-4},
	url = {https://doi.org/10.1145/3324884.3416630},
	doi = {10.1145/3324884.3416630},
	abstract = {The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.},
	urldate = {2021-12-04},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Gafford, Benjamin and Dürschmid, Tobias and Moreno, Gabriel A. and Kang, Eunsuk},
	month = dec,
	year = {2020},
	pages = {1090--1102},
}

@article{spiekermann_inside_2019,
	title = {Inside the {Organization}: {Why} {Privacy} and {Security} {Engineering} {Is} a {Challenge} for {Engineers}},
	volume = {107},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Inside the {Organization}},
	url = {https://ieeexplore.ieee.org/document/8466102/},
	doi = {10.1109/JPROC.2018.2866769},
	language = {en},
	number = {3},
	urldate = {2021-12-01},
	journal = {Proceedings of the IEEE},
	author = {Spiekermann, Sarah and Korunovska, Jana and Langheinrich, Marc},
	month = mar,
	year = {2019},
	pages = {600--615},
}

@inproceedings{benbenisty_privacy_2021,
	title = {Privacy as first-class requirements in software development: {A} socio-technical approach},
	abstract = {Privacy requirements have become increasingly important as information about us is continuously accumulated and digitally stored. However, despite the many proposed methodologies and tools to address these requirements, privacy engineering is often underperformed in most domains of the software industry. Two of the major reasons underlying this under-performance are (1) the low expertise and understanding of privacy by the two main actors in requirements engineering: users and analysts, and (2) the fact that software developers often do not perceive privacy requirements as a priority for their companies, thus neglecting to meet these requirements even when they do have the required knowledge, skills, and supporting tools to do so. To address these two problems, we propose to integrate knowledge from software engineering and organizational psychology in an iterative, customizable, socio-technical environment. Such environment has the potential to support the design of systems by providing technical tools for eliciting, modeling, and designing privacy aspects, thus addressing the knowledge gap of both data subjects and analysts, and social mechanisms for achieving a supportive and sustainable organizational privacy climate within a company, thus reorienting the organizational attention and engagement toward addressing privacy requirements.},
	language = {en},
	booktitle = {Automated {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results}},
	author = {Benbenisty, Yizhaq and Hadar, Irit and Luria, Gil and Spoletini, Paola},
	year = {2021},
	pages = {5},
}

@misc{noauthor_gdpr_nodate,
	title = {{GDPR} {Enforcement} {Tracker} - list of {GDPR} fines},
	url = {https://www.enforcementtracker.com},
	abstract = {List and overview of fines and penalties under the EU General Data Protection Regulation (GDPR, DSGVO)},
	urldate = {2021-12-01},
}

@misc{noauthor_top_nodate,
	title = {Top 10 {Open} {Source} {Social} {Network} {Development} {Platforms}},
	url = {https://topdigital.agency/ top-10-open-source-social-network-development-platforms/},
	urldate = {2021-01-01},
}

@misc{noauthor_top_nodate-1,
	title = {The {Top} 1,539 {Social} {Network} {Open} {Source} {Projects} on {Github}},
	url = {https://awesomeopensource.com/projects/social-network},
}

@misc{noauthor_top_nodate-2,
	title = {The {Top} 1,539 {Social} {Network} {Open} {Source} {Projects} on {Github}},
	url = {https://awesomeopensource.com/projects/social-network},
	urldate = {2021-12-01},
}

@misc{noauthor_9_nodate,
	title = {9 {Decentralized}, {Open} {Source} {Alternative} {Social} {Media} {Platforms}},
	url = {https://itsfoss.com/mainstream-social-media-alternaives/},
	abstract = {Tired of Big Tech prying on your data and invading your privacy? Here are some open source, decentralized alternate social platforms.},
	language = {en\_US},
	urldate = {2021-12-01},
	journal = {https://itsfoss.com/},
	note = {Section: List},
}

@misc{noauthor_most_nodate,
	title = {Most used social media 2021},
	url = {https://www.statista.com/statistics/272014/global-social-networks-ranked-by-number-of-users/},
	abstract = {Facebook, YouTube, and WhatsApp are the most popular social networks worldwide, each with more than two billion active users.},
	language = {en},
	urldate = {2021-12-01},
	journal = {Statista},
}

@article{zulli_rethinking_2020,
	title = {Rethinking the “social” in “social media”: {Insights} into topology, abstraction, and scale on the {Mastodon} social network},
	volume = {22},
	issn = {1461-4448, 1461-7315},
	shorttitle = {Rethinking the “social” in “social media”},
	url = {http://journals.sagepub.com/doi/10.1177/1461444820912533},
	doi = {10.1177/1461444820912533},
	abstract = {Online interactions are often understood through the corporate social media (CSM) model where social interactions are determined through layers of abstraction and centralization that eliminate users from decision-making processes. This study demonstrates how alternative social media (ASM)—namely Mastodon—restructure the relationship between the technical structure of social media and the social interactions that follow, offering a particular type of sociality distinct from CSM. Drawing from a variety of qualitative data, this analysis finds that (1) the decentralized structure of Mastodon enables community autonomy, (2) Mastodon’s open-source protocol allows the internal and technical development of the site to become a social enterprise in and of itself, and (3) Mastodon’s horizontal structure shifts the site’s scaling focus from sheer number of users to quality engagement and niche communities. To this end, Mastodon helps us rethink “the social” in social media in terms of topology, abstraction, and scale.},
	language = {en},
	number = {7},
	urldate = {2021-12-01},
	journal = {New Media \& Society},
	author = {Zulli, Diana and Liu, Miao and Gehl, Robert},
	month = jul,
	year = {2020},
	pages = {1188--1205},
}

@incollection{burgess_alternative_2018,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Alternative {Social} {Media}: {From} {Critique} to {Code}},
	isbn = {978-1-4129-6229-2 978-1-4739-8406-6},
	shorttitle = {Alternative {Social} {Media}},
	url = {https://sk.sagepub.com/reference/the-sage-handbook-of-social-media/i2724.xml},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {The {SAGE} {Handbook} of {Social} {Media}},
	publisher = {SAGE Publications Ltd},
	author = {Gehl, Robert W.},
	collaborator = {Burgess, Jean and Marwick, Alice and Poell, Thomas},
	year = {2018},
	doi = {10.4135/9781473984066.n19},
	pages = {330--350},
}

@inproceedings{towne_your_2013,
	address = {San Antonio, Texas, USA},
	title = {Your process is showing: controversy management and perceived quality in wikipedia},
	isbn = {978-1-4503-1331-5},
	shorttitle = {Your process is showing},
	url = {http://dl.acm.org/citation.cfm?doid=2441776.2441896},
	doi = {10.1145/2441776.2441896},
	abstract = {Large-scale collaboration systems often separate their content from the deliberation around how that content was produced. Surfacing this deliberation may engender trust in the content generation process if the deliberation process appears fair, well-reasoned, and thorough. Alternatively, it could encourage doubts about content quality, especially if the process appears messy or biased. In this paper we report the results of an experiment where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration, especially – but not only – if the discussion revealed conflict. The effect size depends on the type of editors’ interactions. Finally, this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall.},
	language = {en},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 2013 conference on {Computer} supported cooperative work - {CSCW} '13},
	publisher = {ACM Press},
	author = {Towne, W. Ben and Kittur, Aniket and Kinnaird, Peter and Herbsleb, James},
	year = {2013},
	pages = {1059},
}

@inproceedings{muller-birn_work--rule_2013,
	address = {Munich, Germany},
	title = {Work-to-rule: the emergence of algorithmic governance in {Wikipedia}},
	isbn = {978-1-4503-2104-4},
	shorttitle = {Work-to-rule},
	url = {http://dl.acm.org/citation.cfm?doid=2482991.2482999},
	doi = {10.1145/2482991.2482999},
	abstract = {Research has shown the importance of a functioning governance system for the success of peer production communities. It particularly highlights the role of human coordination and communication within the governance regime. In this article, we extend this line of research by differentiating two categories of governance mechanisms. The ﬁrst category is based primarily on communication, in which social norms emerge that are often formalized by written rules and guidelines. The second category refers to the technical infrastructure that enables users to access artifacts, and that allows the community to communicate and coordinate their collective actions to create those artifacts. We collected qualitative and quantitative data from Wikipedia in order to show how a community’s consensus gradually converts social mechanisms into algorithmic mechanisms. In detail, we analyze algorithmic governance mechanisms in two embedded cases: the software extension “ﬂagged revisions” and the bot “xqbot”. Our insights point towards a growing relevance of algorithmic governance in the realm of governing large-scale peer production communities. This extends previous research, in which algorithmic governance is almost absent. Further research is needed to unfold, understand, and also modify existing interdependencies between social and algorithmic governance mechanisms.},
	language = {en},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Communities} and {Technologies} - {C}\&{T} '13},
	publisher = {ACM Press},
	author = {Müller-Birn, Claudia and Dobusch, Leonhard and Herbsleb, James D.},
	year = {2013},
	pages = {80--89},
}

@inproceedings{tsay_social_2013,
	address = {San Francisco, CA, USA},
	title = {Social media in transparent work environments},
	isbn = {978-1-4673-6290-0},
	url = {http://ieeexplore.ieee.org/document/6614733/},
	doi = {10.1109/CHASE.2013.6614733},
	abstract = {Social media is being integrated into work environments making them more transparent. When the work environment is transparent, it has the potential to allow projects to transmit information about work artifacts and events quickly through a large network. Using signaling theory, we propose a theory that users interpret this information and then make workrelated decisions about attention and effort allocation in a principled manner. In the open source context of voluntary participation, broadcast activity information act as signals that allow developers to make highly informed choices about where to expend their attention and effort and with whom to collaborate. We propose four potential signals from literature and interviews with developers in our research setting and discuss the implications for social media in software development environments.},
	language = {en},
	urldate = {2021-11-30},
	booktitle = {2013 6th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	publisher = {IEEE},
	author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James D.},
	month = may,
	year = {2013},
	pages = {65--72},
}

@article{shaikh_learning-based_2021,
	title = {A {Learning}-{Based} {Fault} {Localization} {Approach} {Using} {Subset} of {Likely} and {Dynamic} {Invariants}},
	volume = {31},
	doi = {10.32604/iasc.2022.021163},
	abstract = {Fault localization is one of the main tasks of software debugging. Developers spend a lot of time, cost, and effort to locate the faults correctly manually. For reducing this effort, many automatic fault localization techniques have been proposed, which inputs test suites and outputs a sorted list of faulty entities of the program. For further enhancement in this area, we developed a system called SILearning, which is based on invariant analysis. It learns from some existing fixed bugs to locate faulty methods in the program. It combines machine-learned ranking, program invariant differences, and spectrum-based fault localization (SBFL). Using the execution of test cases and code coverage analysis, it obtains each method's invariant differences and the suspiciousness value based on the program spectral location and uses them as features for ranking the faulty methods. The experimental analysis of SILearning has been performed on the dataset of real fault which is extracted from the database Defects4J. The tools used in this research are Daikon and cobertura for detection of the invariants and code coverage, respectively. The results show that SILearning performs better when combined features are utilized and can successfully locate the faulty methods on average for 76.1, 90.4, 108.2, 123, and 143.5 at the top positions of 1, 2, 3, 4, and 5.},
	journal = {Intelligent Automation and Soft Computing},
	author = {Shaikh, Asadullah and Rizwan, Syed and Alghamdi, Abdullah and Islam, Noman and Elmagzoub, M and Syed, Darakhshan},
	month = oct,
	year = {2021},
}

@article{hellendoorn_are_2019,
	title = {Are {My} {Invariants} {Valid}? {A} {Learning} {Approach}},
	shorttitle = {Are {My} {Invariants} {Valid}?},
	url = {http://arxiv.org/abs/1903.06089},
	abstract = {Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.},
	urldate = {2021-11-30},
	journal = {arXiv:1903.06089 [cs]},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Polozov, Oleksandr and Marron, Mark},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06089},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{almaghairbe_machine_2020,
	address = {New York, NY, USA},
	series = {{ICEMIS}'20},
	title = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}: {Empirical} {Evaluation} {Study}},
	isbn = {978-1-4503-7736-2},
	shorttitle = {Machine {Learning} {Techniques} for {Automated} {Software} {Fault} {Detection} via {Dynamic} {Execution} {Data}},
	url = {https://doi.org/10.1145/3410352.3410747},
	doi = {10.1145/3410352.3410747},
	abstract = {The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1], [2], [10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing techniques from the specification mining domain (the data invariant detector Daikon [5]). The two approaches are evaluated on a range of mid-sized systems and compared in terms of their fault detection ability and false positive rate. The empirical study also discuss the major limitations and the most important properties related to the application of machine learning techniques as test oracles in practice. The study also gives a road map for further research direction in order to tackle some of discussed limitations such as accuracy and scalability. The results show that in most cases semi-supervised learning techniques performed far better as an automated test classifier than Daikon (especially in the case that input/output pairs were augmented with their execution traces). However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases particularly when input/output pairs were used together with execution traces.},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Engineering} \& {MIS} 2020},
	publisher = {Association for Computing Machinery},
	author = {Almaghairbe, Rafig and Roper, Marc and Almabruk, Tahani},
	month = sep,
	year = {2020},
	keywords = {Automated Testing Oracles, Empirical Study, Machine Learning Techniques, Specification Mining},
	pages = {1--12},
}

@article{wang_detecting_2017,
	title = {Detecting {Bugs} of {Concurrent} {Programs} {With} {Program} {Invariants}},
	volume = {66},
	issn = {1558-1721},
	doi = {10.1109/TR.2017.2681107},
	abstract = {Concurrency bug detection is a time-consuming activity in the debugging process for concurrent programs. Existing techniques mainly focus on detecting data race bugs with pattern analysis; however, the number of interleaving patterns could be huge, only the most suspicious write-read pattern is given, and an oracle is needed, which is not available in the operational phase. This paper proposes a program-invariant-based technique to detect a class of concurrent program bugs. By unit testing of the components of a concurrent program, we obtain a set of program invariants, which can be used as an oracle to obtain “bad” invariants when the program is online. By using the function call graph of the components and applying a reduction technique to the invariants, we find the candidates of suspicious functions and rank them. From the interactions among components, we analyze the causes to the concurrency bugs. Experimental results show that our proposed technique is effective in concurrency bug detection.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Rong and Ding, Zuohua and Gui, Ning and Liu, Yang},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Reliability},
	keywords = {Bug detection, Computer bugs, Concurrent computing, Message systems, Pattern analysis, Programming, System recovery, Testing, concurrent program, function call graph, program invariants, suspicious rate},
	pages = {425--439},
}

@inproceedings{almaghairbe_empirical_2019,
	title = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}: {Machine} {Learning} {Versus} {Dynamic} {Analysis}},
	shorttitle = {An {Empirical} {Comparison} of {Two} {Different} {Strategies} to {Automated} {Fault} {Detection}},
	doi = {10.1109/ISSREW.2019.00099},
	abstract = {Software testing is an established method to ensure software quality and reliability, but it is an expensive process. In recent years, the automation of test case generation has received significant attention as a way to reduce costs. However, the oracle problem (a mechanism for determine the (in) correctness of an executed test case) is still major problem which has been largely ignored. Recent work has shown that building a test oracle using the principles of anomaly detection techniques (mainly semisupervised/ unsupervised learning models based on dynamic execution data consisting of an amalgamation of input/output pairs and execution traces) is able to demonstrate a reasonable level of success in automatically detect passing and failing execution [1], [2]. In this paper, we present a comparison study between our machine-learning based approaches and an existing techniques from the specification mining domain (the data invariant detector Daikon [3]). The two approaches are evaluated on a range of midsized systems and compared in terms of their fault detection ability. The results show that in most cases semi-supervised learning techniques perform far better as an automated test classifier than Daikon. However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Almaghairbe, Rafig and Roper, Marc},
	month = oct,
	year = {2019},
	keywords = {Automated Test Oracles, Empirical Study, Machine Learning Techniques, Software Testing},
	pages = {378--385},
}

@inproceedings{zhang_firmware_2020,
	address = {New York, NY, USA},
	series = {Internetware'20},
	title = {Firmware {Fuzzing}: {The} {State} of the {Art}},
	isbn = {978-1-4503-8819-1},
	shorttitle = {Firmware {Fuzzing}},
	url = {https://doi.org/10.1145/3457913.3457934},
	doi = {10.1145/3457913.3457934},
	abstract = {Background: Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited. Due to the limited resources of IoT devices, it is impractical to deploy sophisticated run-time protection techniques. Under an insecure network environment, when a firmware is exploited, it may lead to denial of service, information disclosure, elevation of privilege, or even life-threatening. Therefore, firmware vulnerability detection by fuzzing has become the key to ensure the security of IoT devices, and has also become a hot topic in academic and industrial research. With the rapid growth of the existing IoT devices, the size and complexity of firmware, the variety of firmware types, and the firmware defects, existing IoT firmware fuzzing methods face challenges. Objective: This paper summarizes the typical types of IoT firmware fuzzing methods, analyzes the contribution of these works, and summarizes the shortcomings of existing fuzzing methods. Method: We design several research questions, extract keywords from the research questions, then use the keywords to search for related literature. Result: We divide the existing firmware fuzzing work into real-device-based fuzzing and simulation-based fuzzing according to the firmware execution environment, and simulation-based fuzzing is the mainstream in the future; we found that the main types of vulnerabilities targeted by existing fuzzing methods are memory corruption vulnerabilities; firmware fuzzing faces more difficulties than ordinary software fuzzing. Conclusion: Through the analysis of the advantages and disadvantages of different methods, this review provides guidance for further improving the performance of fuzzing techniques, and proposes several recommendations from the findings of this review.},
	urldate = {2021-11-30},
	booktitle = {12th {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chi and Wang, Yu and Wang, Linzhang},
	month = nov,
	year = {2020},
	keywords = {Internet of Things, firmware, fuzzing, literature review},
	pages = {110--115},
}

@inproceedings{aliabadi_artinali_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {{ARTINALI}: dynamic invariant detection for cyber-physical system security},
	isbn = {978-1-4503-5105-8},
	shorttitle = {{ARTINALI}},
	url = {https://doi.org/10.1145/3106237.3106282},
	doi = {10.1145/3106237.3106282},
	abstract = {Cyber-Physical Systems (CPSes) are being widely deployed in security critical scenarios such as smart homes and medical devices. Unfortunately, the connectedness of these systems and their relative lack of security measures makes them ripe targets for attacks. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing CPSs. Unfortunately, deriving invariants for capturing the specifications of CPS systems is a tedious and error-prone process. Therefore, it is important to dynamically monitor the CPS system to learn its common behaviors and formulate invariants for detecting security attacks. Existing techniques for invariant mining only incorporate data and events, but not time. However, time is central to most CPS systems, and hence incorporating time in addition to data and events, is essential for achieving low false positives and false negatives. This paper proposes ARTINALI, which mines dynamic system properties by incorporating time as a first-class property of the system. We build ARTINALI-based Intrusion Detection Systems (IDSes) for two CPSes, namely smart meters and smart medical devices, and measure their efficacy. We find that the ARTINALI-based IDSes significantly reduce the ratio of false positives and false negatives by 16 to 48\% (average 30.75\%) and 89 to 95\% (average 93.4\%) respectively over other dynamic invariant detection tools.},
	urldate = {2021-11-30},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Aliabadi, Maryam Raiyat and Kamath, Amita Ajith and Gascon-Samson, Julien and Pattabiraman, Karthik},
	month = aug,
	year = {2017},
	keywords = {CPS, Cyber Physical System, Multi-dimensional model, Security, Software Engineering},
	pages = {349--361},
}

@inproceedings{chen_learning-guided_2019,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	doi = {10.1109/ASE.2019.00093},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds-a water purification plant and a water distribution system-finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Chen, Yuqi and Poskitt, Christopher M. and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Actuators, Benchmark testing, Fuzzing, Machine learning, Monitoring, Predictive models, benchmark generation, cyber-physical systems, fuzzing, machine learning, metaheuristic optimisation, testing},
	pages = {962--973},
}

@inproceedings{viticchie_remotely_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Remotely {Assessing} {Integrity} of {Software} {Applications} by {Monitoring} {Invariants}: {Present} {Limitations} and {Future} {Directions}},
	isbn = {978-3-319-76687-4},
	shorttitle = {Remotely {Assessing} {Integrity} of {Software} {Applications} by {Monitoring} {Invariants}},
	doi = {10.1007/978-3-319-76687-4_5},
	abstract = {Invariants monitoring is a software attestation technique that aims at proving the integrity of a running application by checking likely invariants, which are predicates built on variables’ values. Being very promising in literature, we developed a software protection that remotely checks invariants. However, we faced a series of issues and limitations. This paper, after presenting an extensive background on invariants and their use, reports, analyses, and categorizes the identified limitations. Our work suggests that, even if it is still promising, further studies are needed to decree if invariants monitoring could be practically used as a remote protection of software applications.},
	language = {en},
	booktitle = {Risks and {Security} of {Internet} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Viticchié, Alessio and Basile, Cataldo and Lioy, Antonio},
	editor = {Cuppens, Nora and Cuppens, Frédéric and Lanet, Jean-Louis and Legay, Axel and Garcia-Alfaro, Joaquin},
	year = {2018},
	pages = {66--82},
}

@inproceedings{zhang_firmware_2020-1,
	address = {New York, NY, USA},
	series = {Internetware'20},
	title = {Firmware {Fuzzing}: {The} {State} of the {Art}},
	isbn = {978-1-4503-8819-1},
	shorttitle = {Firmware {Fuzzing}},
	url = {https://doi.org/10.1145/3457913.3457934},
	doi = {10.1145/3457913.3457934},
	abstract = {Background: Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited. Due to the limited resources of IoT devices, it is impractical to deploy sophisticated run-time protection techniques. Under an insecure network environment, when a firmware is exploited, it may lead to denial of service, information disclosure, elevation of privilege, or even life-threatening. Therefore, firmware vulnerability detection by fuzzing has become the key to ensure the security of IoT devices, and has also become a hot topic in academic and industrial research. With the rapid growth of the existing IoT devices, the size and complexity of firmware, the variety of firmware types, and the firmware defects, existing IoT firmware fuzzing methods face challenges. Objective: This paper summarizes the typical types of IoT firmware fuzzing methods, analyzes the contribution of these works, and summarizes the shortcomings of existing fuzzing methods. Method: We design several research questions, extract keywords from the research questions, then use the keywords to search for related literature. Result: We divide the existing firmware fuzzing work into real-device-based fuzzing and simulation-based fuzzing according to the firmware execution environment, and simulation-based fuzzing is the mainstream in the future; we found that the main types of vulnerabilities targeted by existing fuzzing methods are memory corruption vulnerabilities; firmware fuzzing faces more difficulties than ordinary software fuzzing. Conclusion: Through the analysis of the advantages and disadvantages of different methods, this review provides guidance for further improving the performance of fuzzing techniques, and proposes several recommendations from the findings of this review.},
	urldate = {2021-11-30},
	booktitle = {12th {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chi and Wang, Yu and Wang, Linzhang},
	month = nov,
	year = {2020},
	keywords = {Internet of Things, firmware, fuzzing, literature review},
	pages = {110--115},
}

@inproceedings{alimi_analysis_2014,
	title = {Analysis of embedded applications by evolutionary fuzzing},
	doi = {10.1109/HPCSim.2014.6903734},
	abstract = {In this paper, we propose to use fuzzing techniques to discover vulnerabilities in programs hosted into smart cards used for telecommunications or banking purposes (SIM cards, credit cards, secure element into NFC mobile devices...). Those programs - called applets - usually host sensitive applications and manipulate sensitive data. A flaw by design or by implementation in one of those applet could have disastrous consequences. The proposed approach uses a genetic algorithm to optimize the vulnerabilities search. We illustrate the benefit of the proposed method on a MasterCard M/Chip applet through experimental results.},
	booktitle = {2014 {International} {Conference} on {High} {Performance} {Computing} {Simulation} ({HPCS})},
	author = {Alimi, V. and Vernois, S. and Rosenberger, C.},
	month = jul,
	year = {2014},
	keywords = {Credit cards, Genetic algorithms, Libraries, Protocols, Smart cards, Sociology, Statistics},
	pages = {551--557},
}

@article{eceiza_fuzzing_2021,
	title = {Fuzzing the {Internet} of {Things}: {A} {Review} on the {Techniques} and {Challenges} for {Efficient} {Vulnerability} {Discovery} in {Embedded} {Systems}},
	volume = {8},
	issn = {2327-4662},
	shorttitle = {Fuzzing the {Internet} of {Things}},
	doi = {10.1109/JIOT.2021.3056179},
	abstract = {With a growing number of embedded devices that create, transform, and send data autonomously at its core, the Internet of Things (IoT) is a reality in different sectors, such as manufacturing, healthcare, or transportation. With this expansion, the IoT is becoming more present in critical environments, where security is paramount. Infamous attacks, such as Mirai, have shown the insecurity of the devices that power the IoT, as well as the potential of such large-scale attacks. Therefore, it is important to secure these embedded systems that form the backbone of the IoT. However, the particular nature of these devices and their resource constraints mean that the most cost-effective manner of securing these devices is to secure them before they are deployed, by minimizing the number of vulnerabilities they ship. To this end, fuzzing has proved itself as a valuable technique for automated vulnerability finding, where specially crafted inputs are fed to programs in order to trigger vulnerabilities and crash the system. In this survey, we link the world of embedded IoT devices and fuzzing. For this end, we list the particularities of the embedded world as far as security is concerned, we perform a literature review on fuzzing techniques and proposals, studying their applicability to embedded IoT devices and, finally, we present future research directions by pointing out the gaps identified in the review.},
	number = {13},
	journal = {IEEE Internet of Things Journal},
	author = {Eceiza, Maialen and Flores, Jose Luis and Iturbe, Mikel},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Embedded system, Embedded systems, Fuzzing, Hardware, Internet of Things, Security, Software, Task analysis, fuzzing, internet of Things (IoT), software testing, vulnerabilities},
	pages = {10390--10411},
}

@inproceedings{fioraldi_use_2021,
	title = {The {Use} of {Likely} {Invariants} as {Feedback} for {Fuzzers}},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/fioraldi},
	language = {en},
	urldate = {2021-11-29},
	author = {Fioraldi, Andrea and D'Elia, Daniele Cono and Balzarotti, Davide},
	year = {2021},
	pages = {2829--2846},
}

@inproceedings{katz_detecting_2020,
	title = {Detecting {Execution} {Anomalies} {As} an {Oracle} for {Autonomy} {Software} {Robustness}},
	doi = {10.1109/ICRA40945.2020.9197060},
	abstract = {We propose a method for detecting execution anomalies in robotics and autonomy software. The algorithm uses system monitoring techniques to obtain profiles of executions. It uses a clustering algorithm to create clusters of those executions, representing nominal execution. A distance metric determines whether additional execution profiles belong to the existing clusters or should be considered anomalies. The method is suitable for identifying faults in robotics and autonomy systems. We evaluate the technique in simulation on two robotics systems, one of which is a real-world industrial system. We find that our technique works well to detect possibly unsafe behavior in autonomous systems.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Katz, Deborah S. and Hutchison, Casidhe and Zizyte, Milda and Goues, Claire Le},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Clustering algorithms, Instruments, Safety, Service robots, Testing, Tools},
	pages = {9366--9373},
}

@misc{isoiec_jtc_1sc_41_isoiec_2018,
	title = {{ISO}/{IEC} 30141:2018 {Internet} of {Things} ({IoT}) — {Reference} {Architecture}},
	url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/06/56/65695.html},
	language = {en},
	urldate = {2021-11-29},
	journal = {ISO},
	author = {{ISO/IEC JTC 1/SC 41}},
	month = aug,
	year = {2018},
}

@article{rymaszewska_iot_2017,
	series = {Service {Implementation} in {Manufacturing} {Firms}: {Strategy}, {Economics} and {Practice}},
	title = {{IoT} powered servitization of manufacturing – an exploratory case study},
	volume = {192},
	issn = {0925-5273},
	url = {https://www.sciencedirect.com/science/article/pii/S0925527317300531},
	doi = {10.1016/j.ijpe.2017.02.016},
	abstract = {More than ever companies are challenged to rethink their offerings while simultaneously being provided with a unique opportunity for creating or recreating their product-service systems. This paper seeks to address how servitisation can utilise the third wave of Internet development, referred to as the Internet of Things (IoT), which may unlock the potential for innovative product-service systems on an unprecedented scale. By providing an analysis of this technological breakthrough and the literature on servitisation, these concepts are combined to address the question of how organisations offering product-service systems can reap the benefits that the IoT. An analysis of three successful IoT implementation cases in manufacturing companies, representing different industry sectors such as metal processing, power generation and distribution, is provided. The results of the empirical research presented in the paper provide an insight into different ways of creating value in servitisation. The paper also proposes a framework that is aimed at proving a better understanding of how companies can create value, and add it to their servitisation processes with, the data obtained by the IoT based solutions. From the value chain perspective, IoT aided servitisation enables organisations to extend their value chains in order better serve their customers which, in turn, might result in increased profitability. The article proposes further research avenues, and offers valuable insight for practitioners.},
	language = {en},
	urldate = {2021-11-28},
	journal = {International Journal of Production Economics},
	author = {Rymaszewska, Anna and Helo, Petri and Gunasekaran, Angappa},
	month = oct,
	year = {2017},
	keywords = {Creation, Internet of Things, IoT, Manufacturing, Servitization, Value},
	pages = {92--105},
}

@inproceedings{salva_using_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Model} {Learning} for the {Generation} of {Mock} {Components}},
	isbn = {978-3-030-64881-7},
	doi = {10.1007/978-3-030-64881-7_1},
	abstract = {Mocking objects is a common technique that substitutes parts of a program to simplify the test case development, to increase test coverage or to speed up performance. Today, mocks are almost exclusively used with object oriented programs. But mocks could offer the same benefits with communicating systems to make them more reliable. This paper proposes a model-based approach to help developers generate mocks for this kind of system, i.e. systems made up of components interacting with each other by data networks and whose communications can be monitored. The approach combines model learning to infer models from event logs, quality metric measurements to help chose the components that may be replaced by mocks, and mock generation and execution algorithms to reduce the mock development time. The approach has been implemented as a tool chain with which we performed experimentations to evaluate its benefits in terms of usability and efficiency.},
	language = {en},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Salva, Sébastien and Blot, Elliott},
	editor = {Casola, Valentina and De Benedictis, Alessandra and Rak, Massimiliano},
	year = {2020},
	keywords = {Communicating systems, Mock, Model learning, Quality metrics},
	pages = {3--19},
}

@inproceedings{marri_empirical_2009-1,
	title = {An empirical study of testing file-system-dependent software with mock objects},
	doi = {10.1109/IWAST.2009.5069054},
	abstract = {Unit testing is a technique of testing a single unit of a program in isolation. The testability of the unit under test can be reduced when the unit interacts with its environment. The construction of high-covering unit tests and their execution require appropriate interactions with the environment such as a file system or database. To help set up the required environment, developers can use mock objects to simulate the behavior of the environment. In this paper, we present an empirical study to analyze the use of mock objects to test file-system-dependent software. We use a mock object of the FileSystem API provided with the Pex automatic testing tool in our study. We share our insights gained on the benefits of using mock objects in unit testing and discuss the faced challenges.},
	booktitle = {2009 {ICSE} {Workshop} on {Automation} of {Software} {Test}},
	author = {Marri, Madhuri R. and Xie, Tao and Tillmann, Nikolai and de Halleux, Jonathan and Schulte, Wolfram},
	month = may,
	year = {2009},
	keywords = {Automatic testing, Computer science, Databases, File systems, Logic, Software testing, System testing},
	pages = {149--153},
}

@inproceedings{galler_automatically_2010-1,
	address = {New York, NY, USA},
	series = {{AST} '10},
	title = {Automatically extracting mock object behavior from {Design} by {Contract}\&\#x2122; specification for test data generation},
	isbn = {978-1-60558-970-1},
	url = {https://doi.org/10.1145/1808266.1808273},
	doi = {10.1145/1808266.1808273},
	abstract = {Test data generation is an important task in the process of automated unit test generation. Random and heuristic approaches are well known for test input data generation. Unfortunately, in the presence of complex pre-conditions especially in the case of non-primitive data types those approaches often fail. A promising technique for generating an object that exactly satisfies a given pre-condition is mocking, i.e., replacing the concrete implementation with an implementation only considering the necessary behavior for a specific test case. In this paper we follow this technique and present an approach for automatically deriving the behavior of mock objects from given Design by Contract™ specifications. The generated mock objects behave according to the Design by Contract™ specification of the original class. Furthermore, we make sure that the observed behavior of the mock object satisfies the pre-condition of the method under test. We evaluate the approach using the Java implementations of 20 common Design Patterns and a stack based calculator. Our approach clearly outperforms pure random data generation in terms of line coverage.},
	urldate = {2021-11-26},
	booktitle = {Proceedings of the 5th {Workshop} on {Automation} of {Software} {Test}},
	publisher = {Association for Computing Machinery},
	author = {Galler, Stefan J. and Maller, Andreas and Wotawa, Franz},
	month = may,
	year = {2010},
	keywords = {Design-by-Contract, automated unit testing, mock object, test data generation},
	pages = {43--50},
}

@book{runeson_case_2012,
	edition = {1st},
	title = {Case {Study} {Research} in {Software} {Engineering}: {Guidelines} and {Examples}},
	isbn = {978-1-118-10435-4},
	shorttitle = {Case {Study} {Research} in {Software} {Engineering}},
	abstract = {Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.},
	publisher = {Wiley Publishing},
	author = {Runeson, Per and Host, Martin and Rainer, Austen and Regnell, Bjorn},
	year = {2012},
}

@inproceedings{salva_using_2020-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Model} {Learning} for the {Generation} of {Mock} {Components}},
	isbn = {978-3-030-64881-7},
	doi = {10.1007/978-3-030-64881-7_1},
	abstract = {Mocking objects is a common technique that substitutes parts of a program to simplify the test case development, to increase test coverage or to speed up performance. Today, mocks are almost exclusively used with object oriented programs. But mocks could offer the same benefits with communicating systems to make them more reliable. This paper proposes a model-based approach to help developers generate mocks for this kind of system, i.e. systems made up of components interacting with each other by data networks and whose communications can be monitored. The approach combines model learning to infer models from event logs, quality metric measurements to help chose the components that may be replaced by mocks, and mock generation and execution algorithms to reduce the mock development time. The approach has been implemented as a tool chain with which we performed experimentations to evaluate its benefits in terms of usability and efficiency.},
	language = {en},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Salva, Sébastien and Blot, Elliott},
	editor = {Casola, Valentina and De Benedictis, Alessandra and Rak, Massimiliano},
	year = {2020},
	keywords = {Communicating systems, Mock, Model learning, Quality metrics},
	pages = {3--19},
}

@inproceedings{tillmann_mock-object_2006,
	title = {Mock-object generation with behavior},
	doi = {10.1109/ASE.2006.51},
	abstract = {Unit testing is a popular way to guide software development and testing. Each unit test should target a single feature, but in practice it is difficult to test features in isolation. Mock objects are a well-known technique to substitute parts of a program which are irrelevant for a particular unit test. Today mock objects are usually written manually supported by tools that generate method stubs or distill behavior from existing programs. We have developed a prototype tool based on symbolic execution of .NET code that generates mock objects including their behavior by analyzing all uses of the mock object in a given unit test. It is not required that an actual implementation of the mocked behavior exists. We are working towards an integration of our tool into Visual Studio Team System},
	booktitle = {21st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE}'06)},
	author = {Tillmann, Nikolai and Schulte, Wolfram},
	month = sep,
	year = {2006},
	note = {ISSN: 1938-4300},
	keywords = {Concrete, Debugging, Instruments, Programming, Prototypes, Software engineering, Software testing},
	pages = {365--368},
}

@article{ernst_daikon_2007,
	series = {Special issue on {Experimental} {Software} and {Toolkits}},
	title = {The {Daikon} system for dynamic detection of likely invariants},
	volume = {69},
	issn = {0167-6423},
	url = {https://www.sciencedirect.com/science/article/pii/S016764230700161X},
	doi = {10.1016/j.scico.2007.01.015},
	abstract = {Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often used in assert statements, documentation, and formal specifications. Examples include being constant (x=a), non-zero (x≠0), being in a range (a≤x≤b), linear relationships (y=ax+b), ordering (x≤y), functions from a library (x=fn(y)), containment (x∈y), sortedness (xissorted), and many more. Users can extend Daikon to check for additional invariants. Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data. Daikon can detect invariants in C, C++, Java, and Perl programs, and in record-structured data sources; it is easy to extend Daikon to other applications. Invariants can be useful in program understanding and a host of other applications. Daikon’s output has been used for generating test cases, predicting incompatibilities in component integration, automating theorem proving, repairing inconsistent data structures, and checking the validity of data streams, among other tasks. Daikon is freely available in source and binary form, along with extensive documentation, at http://pag.csail.mit.edu/daikon/.},
	language = {en},
	number = {1},
	urldate = {2021-11-23},
	journal = {Science of Computer Programming},
	author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and McCamant, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
	month = dec,
	year = {2007},
	keywords = {Daikon, Dynamic analysis, Dynamic invariant detection, Inductive logic programming, Inference, Invariant, Likely invariant, Program understanding, Specification, Specification mining},
	pages = {35--45},
}

@book{marwick2013status,
	title = {Status update},
	publisher = {Yale University Press},
	author = {Marwick, Alice E},
	year = {2013},
}

@article{winner_artifacts_1980,
	title = {Do {Artifacts} {Have} {Politics}?},
	language = {en},
	journal = {Daedalus},
	author = {Winner, Langdon},
	year = {1980},
	pages = {17},
}

@article{jonassen2014engineers,
	title = {Engineers as problem solvers},
	journal = {Cambridge handbook of engineering education research},
	author = {Jonassen, David H},
	year = {2014},
	note = {Publisher: Cambridge University Press New York},
	pages = {103--118},
}

@article{downey2014normative,
	title = {The normative contents of engineering formation: {Engineering} studies},
	journal = {The Cambridge Handbook of Engineering Education Research},
	author = {Downey, GL},
	year = {2014},
	note = {Publisher: Cambridge University Press},
	pages = {693--711},
}

@misc{noauthor_case_nodate,
	title = {Case {Studies} - {Trust} \& {Safety} {Foundation} {Project}},
	url = {https://www.tsf.foundation/case-studies},
	urldate = {2021-11-22},
}

@inproceedings{winkler2021partial,
	title = {A partial replication of “{DeepBugs}: {A} learning approach to name-based bug detection”},
	booktitle = {Proceedings of the 29th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE}'21 artifact track)},
	author = {Winkler, Jordan Matthew and Agarwal, Abhimanyu and Tung, Caleb and Ugalde, Dario Rios and Jung, Young Jin and Davis, James C.},
	year = {2021},
}

@inproceedings{fu2019edgewise,
	title = {Edgewise: a better stream processing engine for the edge},
	booktitle = {2019 {USENIX} annual technical conference ({USENIX} {ATC})},
	author = {Fu, Xinwei and Ghaffar, Talha and Davis, James C and Lee, Dongyoon},
	year = {2019},
	pages = {929--946},
}

@inproceedings{davis2017case,
	title = {The case of the poisoned event handler: {Weaknesses} in the {Node}.js event-driven architecture},
	booktitle = {Proceedings of the 10th european workshop on systems security ({EuroSec})},
	author = {Davis, James and Kildow, Gregor and Lee, Dongyoon},
	year = {2017},
	pages = {1--6},
}

@article{kazerouni2021fast,
	title = {Fast and accurate incremental feedback for students’ software tests using selective mutation analysis},
	volume = {175},
	journal = {Journal of Systems and Software (JSS)},
	author = {Kazerouni, Ayaan M and Davis, James C and Basak, Arinjoy and Shaffer, Clifford A and Servant, Francisco and Edwards, Stephen H},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {110905},
}

@inproceedings{rupprecht2020improving,
	title = {Improving reproducibility of data science pipelines through transparent provenance capture},
	volume = {13},
	booktitle = {Proceedings of the {VLDB} endowment ({VLDB})},
	author = {Rupprecht, Lukas and Davis, James C and Arnold, Constantine and Gur, Yaniv and Bhagwat, Deepavali},
	year = {2020},
	note = {Number: 12
tex.organization: VLDB Endowment},
	pages = {3354--3368},
}

@inproceedings{rupprecht2019ursprung,
	title = {Ursprung: {Provenance} for large-scale analytics environments},
	booktitle = {Proceedings of the 2019 international conference on management of data ({SIGMOD}-{Demo})},
	author = {Rupprecht, Lukas and Davis, James C and Arnold, Constantine and Lubbock, Alexander and Tyson, Darren and Bhagwat, Deepavali},
	year = {2019},
	pages = {1989--1992},
}

@inproceedings{wittern2019empirical,
	title = {An empirical study of {GraphQL} schemas},
	booktitle = {International conference on service-oriented computing ({ICSOC})},
	author = {Wittern, Erik and Cha, Alan and Davis, James C and Baudart, Guillaume and Mandel, Louis},
	year = {2019},
	note = {tex.organization: Springer, Cham},
	pages = {3--19},
}

@inproceedings{davis2019rethinking,
	title = {Rethinking regex engines to address {ReDoS}},
	booktitle = {Proceedings of the 2019 27th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE} student research competition)},
	author = {Davis, James C},
	year = {2019},
	pages = {1256--1258},
}

@inproceedings{davis2019testing,
	title = {Testing regex generalizability and its implications: {A} large-scale many-language measurement study},
	booktitle = {2019 34th {IEEE}/{ACM} international conference on automated software engineering ({ASE})},
	author = {Davis, James C and Moyer, Daniel and Kazerouni, Ayaan M and Lee, Dongyoon},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {427--439},
}

@inproceedings{davis2019aren,
	title = {Why aren’t regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions},
	booktitle = {Proceedings of the 2019 27th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering ({ESEC}/{FSE})},
	author = {Davis, James C and Michael IV, Louis G and Coghlan, Christy A and Servant, Francisco and Lee, Dongyoon},
	year = {2019},
	pages = {443--454},
}

@misc{davis2020file,
	title = {File metadata verification in a distributed file system},
	publisher = {Google Patents},
	author = {Davis, James C and Davis, Willard A},
	year = {2020},
}

@misc{davis2021injection,
	title = {Injection of simulated hardware failure(s) in a file system for establishing file system tolerance-to-storage-failure(s)},
	author = {Davis, James C and Davis, Willard A},
	year = {2021},
}

@inproceedings{davis2017node,
	title = {Node.fz: {Fuzzing} the server-side event-driven architecture},
	booktitle = {Proceedings of the twelfth european conference on computer systems ({EuroSys})},
	author = {Davis, James and Thekumparampil, Arun and Lee, Dongyoon},
	year = {2017},
	pages = {145--160},
}

@inproceedings{cha2020principled,
	title = {A principled approach to {GraphQL} query cost analysis},
	booktitle = {Proceedings of the 28th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C and Mandel, Louis and Laredo, Jim A},
	year = {2020},
	pages = {257--268},
}

@misc{davis2018detection,
	title = {Detection of file corruption in a distributed file system},
	publisher = {Google Patents},
	author = {Davis, James C and Davis, Willard A and Knop, Felipe},
	year = {2018},
}

@phdthesis{davis2020impact,
	title = {On the impact and defeat of regular expression denial of service},
	school = {Virginia Tech},
	author = {Davis, James Collins},
	year = {2020},
}

@inproceedings{davis2021using,
	title = {Using selective memoization to defeat regular expression denial of service ({ReDoS})},
	booktitle = {2021 {IEEE} symposium on security and privacy ({SP}), los alamitos, {CA}, {USA}},
	author = {Davis, James C and Servant, Francisco and Lee, Dongyoon},
	year = {2021},
	pages = {543--559},
}

@article{davisplausible,
	title = {Plausible plausible deniability with an epistemological gap},
	author = {Davis, James C},
}

@misc{davis2021performing,
	title = {Performing hierarchical provenance collection},
	publisher = {Google Patents},
	author = {Davis, James Collins and Rupprecht, Lukas and Bhagwat, Deepavali and Arnold, Constantine and Sawdon, Wayne},
	year = {2021},
}

@article{herbold2020large,
	title = {Large-scale manual validation of bug fixing commits: {A} fine-grained analysis of tangling},
	journal = {arXiv preprint arXiv:2011.06244},
	author = {Herbold, Steffen and Trautsch, Alexander and Ledel, Benjamin and Aghamohammadi, Alireza and Ghaleb, Taher Ahmed and Chahal, Kuljit Kaur and Bossenmaier, Tim and Nagaria, Bhaveet and Makedonski, Philip and Ahmadabadi, Matin Nili and {others}},
	year = {2020},
}

@inproceedings{davis2018sense,
	title = {A sense of time for javascript and {Node}.js: {First}-class timeouts as a cure for event handler poisoning},
	booktitle = {27th {USENIX} security symposium ({USENIX} security)},
	author = {Davis, James C and Williamson, Eric R and Lee, Dongyoon},
	year = {2018},
	pages = {343--359},
}

@misc{davis2018testing,
	title = {Testing of lock managers in computing environments},
	publisher = {Google Patents},
	author = {Davis, Willard A and Davis, James C},
	year = {2018},
}

@misc{noauthor_yolo_2021,
	title = {{YOLO} {Object} {Detectors}, {You} {Only} {Look} {Once}},
	url = {https://github.com/tensorflow/models/tree/master/official/vision/beta/projects/yolo},
	abstract = {Models and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-11-22},
	journal = {GitHub},
	year = {2021},
}

@inproceedings{goel2022efficient,
	title = {Efficient computer vision on edge devices with pipeline-parallel hierarchical neural networks},
	booktitle = {Asia and south pacific design automation conference ({ASP}-{DAC})},
	author = {Goel, Abhinav and Tung, Caleb and Hu, Xiao and Thiruvathukal, George and Davis, James C and Lu, Yung-Hsiang},
	year = {2022},
}

@misc{noauthor_add_2017,
	title = {Add moderation tool to edit content warning on a post · {Issue} \#1307 · mastodon/mastodon},
	url = {https://github.com/mastodon/mastodon/issues/1307},
	abstract = {Rather than asking users to delete posts, we should really add a mechanism for being able to edit the content warning of a post in the moderation tools, both through reports and also with a clickab...},
	language = {en},
	urldate = {2021-11-21},
	journal = {GitHub},
	year = {2017},
}

@misc{noauthor_disallow_2018,
	title = {Disallow access to private posts older than the follow relationship · {Issue} \#8283 · mastodon/mastodon},
	url = {https://github.com/mastodon/mastodon/issues/8283},
	abstract = {It appears people have different expectations as to whether posts older than a follow relationship should be accessible to new followers. I\&\#39;m honestly not sure what the best behavior should be....},
	language = {en},
	urldate = {2021-11-21},
	journal = {GitHub},
	year = {2018},
}

@misc{noauthor_mastodon_nodate,
	title = {Mastodon {Homepage}},
	url = {https://joinmastodon.org},
	urldate = {2021-11-21},
}

@misc{tyler_cave_what_2019,
	title = {What is {Mastodon} and why is everyone talking about it?},
	url = {https://www.androidauthority.com/what-is-mastodon-1052151/},
	urldate = {2021-11-21},
	author = {{Tyler Cave}},
	year = {2019},
}

@misc{valens_ana_mastodon_2021,
	title = {Mastodon is crumbling—and many blame its creator},
	url = {https://www.dailydot.com/debug/mastodon-fediverse-eugen-rochko/},
	urldate = {2021-11-21},
	author = {{Valens, Ana}},
	year = {2021},
}

@misc{noauthor_mastodon_2019,
	title = {Mastodon is crumbling—and many blame its creator},
	url = {https://www.dailydot.com/debug/mastodon-fediverse-eugen-rochko/},
	abstract = {It was hailed as a progressive alternative to Twitter. But marginalized, queer users are being alienated by well-off tech bros.},
	language = {en-US},
	urldate = {2021-11-21},
	journal = {The Daily Dot},
	month = jan,
	year = {2019},
}

@article{manes_art_2021,
	title = {The {Art}, {Science}, and {Engineering} of {Fuzzing}: {A} {Survey}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {The {Art}, {Science}, and {Engineering} of {Fuzzing}},
	doi = {10.1109/TSE.2019.2946563},
	abstract = {Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
	number = {11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Computer bugs, Fuzzing, Security, Software security, Terminology, automated software testing, fuzz testing, fuzzing},
	pages = {2312--2331},
}

@article{sharp_ethnographic_2004,
	title = {An {Ethnographic} {Study} of {XP} {Practice}},
	volume = {9},
	issn = {1382-3256},
	url = {http://link.springer.com/10.1023/B:EMSE.0000039884.79385.54},
	doi = {10.1023/B:EMSE.0000039884.79385.54},
	abstract = {Agile methods are a response to more rigorous and traditional approaches to software development which are perceived to have failed both customers and software development practitioners. eXtreme Programming (XP) is an example agile method and we report on an ethnographic study of XP practice carried out in a small company developing web-based intelligent advertisements. We identify ﬁve characterizing themes within XP practice and summarize these ﬁndings in terms of XP culture.},
	language = {en},
	number = {4},
	urldate = {2021-11-18},
	journal = {Empirical Software Engineering},
	author = {Sharp, Helen and Robinson, Hugh},
	month = dec,
	year = {2004},
	pages = {353--375},
}

@inproceedings{mclaughlin_regulator_2022,
	title = {Regulator: {Dynamic} {Analysis} to {Detect} {ReDoS}},
	abstract = {Regular expressions (regexps) are a convenient way for programmers to express complex string searching logic. Several popular programming languages expose an interface to a regexp matching subsystem, either by language-level primitives or through standard libraries. The implementations behind these matching systems vary greatly in their capabilities and running-time characteristics. In particular, backtracking matchers may exhibit worst-case running-time that is either linear, polynomial, or exponential in the length of the string being searched. Such super-linear worst-case regexps expose applications to Regular Expression Denial-of-Service (ReDoS) when inputs can be controlled by an adversarial attacker. In this work, we investigate the impact of ReDoS in backtracking engines, a popular type of engine used by most programming languages. We evaluate several existing tools against a dataset of broadly collected regexps, and ﬁnd that despite extensive theoretical work in this ﬁeld, none are able to achieve both high precision and high recall. To address this gap in existing work, we develop REGULATOR, a novel dynamic, fuzzer-based analysis system for identifying regexps vulnerable to ReDoS. We implement this system by directly instrumenting a popular backtracking regexp engine, which increases the scope of supported regexp syntax and features over prior work. Finally, we evaluate this system against three common regexp datasets, and demonstrate a seven-fold increase in true positives discovered when comparing against existing tools.},
	language = {en},
	booktitle = {{USENIX} {Security}},
	author = {McLaughlin, Robert and Pagani, Fabio and Spahn, Noah and Kruegel, Christopher and Vigna, Giovanni},
	year = {2022},
	pages = {17},
}

@article{saltzer_protection_1975,
	title = {The protection of information in computer systems},
	volume = {63},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1451869/},
	doi = {10.1109/PROC.1975.9939},
	language = {en},
	number = {9},
	urldate = {2021-11-14},
	journal = {Proceedings of the IEEE},
	author = {Saltzer, J.H. and Schroeder, M.D.},
	year = {1975},
	pages = {1278--1308},
}

@incollection{us_department_of_defense_department_1985,
	address = {London},
	title = {Department of {Defense} {Trusted} {Computer} {System} {Evaluation} {Criteria}},
	isbn = {978-0-333-53947-7 978-1-349-12020-8},
	url = {http://link.springer.com/10.1007/978-1-349-12020-8_1},
	language = {en},
	urldate = {2021-11-14},
	booktitle = {The ‘{Orange} {Book}’ {Series}},
	publisher = {Palgrave Macmillan UK},
	author = {{US Department of Defense}},
	editor = {{US Department of Defense}},
	year = {1985},
	doi = {10.1007/978-1-349-12020-8_1},
	pages = {1--129},
}

@inproceedings{rigby_convergent_2013,
	address = {Saint Petersburg, Russia},
	title = {Convergent contemporary software peer review practices},
	isbn = {978-1-4503-2237-9},
	url = {http://dl.acm.org/citation.cfm?doid=2491411.2491444},
	doi = {10.1145/2491411.2491444},
	abstract = {Software peer review is practiced on a diverse set of software projects that have drastically diﬀerent settings, cultures, incentive systems, and time pressures. In an eﬀort to characterize and understand these diﬀerences we examine two Google-led projects, Android and Chromium OS, three Microsoft projects, Bing, Oﬃce, and MS SQL, and projects internal to AMD. We contrast our ﬁndings with data taken from traditional software inspection conducted on a Lucent project and from open source software peer review on six projects, including Apache, Linux, and KDE. Our measures of interest include the review interval, the number of developers involved in review, and proxy measures for the number of defects found during review. We ﬁnd that despite diﬀerences among projects, many of the characteristics of the review process have independently converged to similar values which we think indicate general principles of code review practice. We also introduce a measure of the degree to which knowledge is shared during review. This is an aspect of review practice that has traditionally only had experiential support. Our knowledge sharing measure shows that conducting peer review increases the number of distinct ﬁles a developer knows about by 66\% to 150\% depending on the project. This paper is one of the ﬁrst studies of contemporary review in software ﬁrms and the most diverse study of peer review to date.},
	language = {en},
	urldate = {2021-11-09},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering} - {ESEC}/{FSE} 2013},
	publisher = {ACM Press},
	author = {Rigby, Peter C. and Bird, Christian},
	year = {2013},
	pages = {202},
}

@inproceedings{czerwonka_code_2015,
	address = {Florence, Italy},
	title = {Code {Reviews} {Do} {Not} {Find} {Bugs}. {How} the {Current} {Code} {Review} {Best} {Practice} {Slows} {Us} {Down}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7202946/},
	doi = {10.1109/ICSE.2015.131},
	abstract = {Because of its many uses and benefits, code reviews are a standard part of the modern software engineering workflow. Since they require involvement of people, code reviewing is often the longest part of the code integration activities. Using experience gained at Microsoft and with support of data, we posit (1) that code reviews often do not find functionality issues that should block a code submission; (2) that effective code reviews should be performed by people with specific set of skills; and (3) that the social aspect of code reviews cannot be ignored. We find that we need to be more sophisticated with our guidelines for the code review workflow. We show how our findings from code reviewing practice influence our code review tools at Microsoft. Finally, we assert that, due to its costs, code reviewing practice is a topic deserving to be better understood, systematized and applied to software engineering workflow with more precision than the best practice currently prescribes.},
	language = {en},
	urldate = {2021-11-09},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Czerwonka, Jacek and Greiler, Michaela and Tilford, Jack},
	month = may,
	year = {2015},
	pages = {27--28},
}

@article{alhazmi2021m,
	title = {I’m all ears! {Listening} to software developers on putting {GDPR} principles into software development practice},
	journal = {Personal and Ubiquitous Computing},
	author = {Alhazmi, Abdulrahman and Arachchilage, Nalin Asanka Gamagedara},
	year = {2021},
	note = {Publisher: Springer},
	pages = {1--14},
}

@article{alhazmi_im_2021,
	title = {I’m all ears! {Listening} to software developers on putting {GDPR} principles into software development practice},
	volume = {25},
	issn = {1617-4909, 1617-4917},
	url = {https://link.springer.com/10.1007/s00779-021-01544-1},
	doi = {10.1007/s00779-021-01544-1},
	abstract = {Previous research has been carried out to identify the impediments that prevent developers from incorporating privacy protocols into software applications. No research has been carried out to find out why developers are not able to develop systems that preserve privacy while specifically considering the General Data Protection Regulation principles (GDPR principles). Consequently, this paper aims to examine the issues, which prevent developers from creating applications, which consider and include GDPR principles into their software systems. From our research findings, we identified the lack of familiarity with GDPR principles by developers as one of the obstacles that prevent GDPR onboarding. Those who were familiar with the principles did not have the requisite knowledge about the principles including their techniques. Developers focused on functional than on privacy requirements. Unavailability of resourceful online tools and lack of support from institutions and clients were also identified as issues inimical to the onboarding of GDPR principles.},
	language = {en},
	number = {5},
	urldate = {2021-11-09},
	journal = {Personal and Ubiquitous Computing},
	author = {Alhazmi, Abdulrahman and Arachchilage, Nalin Asanka Gamagedara},
	month = oct,
	year = {2021},
	pages = {879--892},
}

@inproceedings{ma_detection_2016,
	title = {Detection of {Runtime} {Conflicts} among {Services} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP.2016.7501688},
	abstract = {The populations of large cities around the world are growing rapidly. Cities are beginning to address this problem by implementing significant sensing and actuation infrastructure and building services on this infrastructure. However, as the density of sensing and actuation increases and as the complexities of services grow there is an increasing potential for conflicts across Smart City services. These conflicts can cause unsafe situations and disrupt the benefits that the services were originally intended to provide. Although some of the conflicts can be detected and avoided during designing the services, many can still occur unpredictably during runtime. This paper carefully defines and enumerates the main issues regarding the detection and resolution of runtime conflicts in smart cities. In particular, it focuses on conflicts that arise across services. This issue is becoming more and more important as Smart City designs attempt to integrate services from different domains (transportation, energy, public safety, emergency, medical, and many others). Research challenges are identified and then addressed that deal with uncertainty, dynamism, real-time, mobility and spatio-temporal availability, duration and scale of effect, efficiency, and ownership. A watchdog architecture is also described that oversees the services operating in a Smart City. This watchdog solution detects and resolves conflicts, it learns and adapts, and it provides additional inputs to decision making aspects of services. Using data from a Smart City dataset, an emulated set of services and activities using those services are created to perform a conflict analysis. A second analysis hypothesizes 41 future services across 5 domains. Both of these evaluations demonstrate the high probability of conflicts in smart cities of the future.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Ma, M. and Preum, S. Masud and Tarneberg, W. and Ahmed, M. and Ruiters, M. and Stankovic, J.},
	month = may,
	year = {2016},
	keywords = {Roads, Safety, Smart cities, Uncertainty, Vehicles},
	pages = {1--10},
}

@inproceedings{yuan_deresolver_2021,
	address = {New York, NY, USA},
	series = {{ICCPS} '21},
	title = {{DeResolver}: a decentralized negotiation and conflict resolution framework for smart city services},
	isbn = {978-1-4503-8353-0},
	shorttitle = {{DeResolver}},
	url = {https://doi.org/10.1145/3450267.3450538},
	doi = {10.1145/3450267.3450538},
	abstract = {As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8\% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1\%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1\%, compared to priority-based solution.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the {ACM}/{IEEE} 12th {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yukun and Ma, Meiyi and Han, Songyang and Zhang, Desheng and Miao, Fei and Stankovic, John and Lin, Shan},
	month = may,
	year = {2021},
	keywords = {conflicts across services, decentralized resolution, multiple services negotiation, smart services},
	pages = {98--109},
}

@inproceedings{ma_cityguard_2017,
	address = {New York, NY, USA},
	series = {{IoTDI} '17},
	title = {{CityGuard}: {A} {Watchdog} for {Safety}-{Aware} {Conflict} {Detection} in {Smart} {Cities}},
	isbn = {978-1-4503-4966-6},
	shorttitle = {{CityGuard}},
	url = {https://doi.org/10.1145/3054977.3054989},
	doi = {10.1145/3054977.3054989},
	abstract = {Nowadays, increasing number of smart services are being developed and deployed in cities around the world. IoT platforms have emerged to integrate smart city services and city resources, and thus improve city performance in the domains of transportation, emergency, environment, public safety, etc. Despite the increasing intelligence of smart services and the sophistication of platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, CityGuard, a safety-aware watchdog architecture is developed. To the best of our knowledge, it is the first architecture that detects and resolves conflicts among actions of different services considering both safety and performance requirements. Prior to developing CityGuard, safety and performance requirements and a spectrum of conflicts are specified. Sophisticated models are used to analyze secondary effects, and detect device and environmental conflicts. A simulation based on New York City is used for the evaluation. The results show that CityGuard (i) identifies unsafe actions and thus helps to prevent the city from safety hazards, (ii) detects and resolves two major types of conflicts, i.e., device and environmental conflicts, and (iii) improves the overall city performance.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
	month = apr,
	year = {2017},
	keywords = {City Safety, City Simulation, Conflict Detection, Smart City},
	pages = {259--270},
}

@techreport{mccall_safetap_2021,
	type = {report},
	title = {{SafeTAP}: {An} {Efficient} {Incremental} {Analyzer} for {Trigger}-{Action} {Programs}},
	shorttitle = {{SafeTAP}},
	url = {https://kilthub.cmu.edu/articles/report/SafeTAP_An_Efficient_Incremental_Analyzer_for_Trigger-Action_Programs/14792271/1},
	abstract = {Home automation rules that allow users to connect smart home devices using trigger-action programs (TAP) can interact in subtle and unexpected ways. Determining whether these rules are free of undesirable behavior is challenging; so researchers have developed tools to analyze rules and assist users. However, it is unclear whether users need such tools, and what help they need from such tools. To answer this question, we performed a user study where half of the participants were given our custom analysis tool SafeTAP and the other half were not. We found that users are not good at finding issues in their TAP rules, despite perceiving such tasks as easy. The user study also indicates that users would like to check their rules every time they make rule changes. Therefore, we designed a novel incremental symbolic model checking (SMC) algorithm, which extends the basic SMC algorithm of SafeTAP. SafeTAPΔ only performs analysis caused by the addition or removal of rules and reports only new violations that have not already been reported to the user. We evaluate the performance of SafeTAPΔ and show that incremental checking on average improves the performance by 6X when adding new rules.},
	language = {en},
	urldate = {2021-11-05},
	institution = {Carnegie Mellon University},
	author = {McCall, McKenna and Shezan, Faysal Hossain and Bichhawat, Abhishek and Cobb, Camille and Jia, Limin and Tian, Yuan and Grace, Cooper and Yang, Mitchell},
	month = jun,
	year = {2021},
	doi = {10.1184/R1/14792271.v1},
}

@inproceedings{alhanahnah_scalable_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {Scalable analysis of interaction threats in {IoT} systems},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397347},
	doi = {10.1145/3395363.3397347},
	abstract = {The ubiquity of Internet of Things (IoT) and our growing reliance on IoT apps are leaving us more vulnerable to safety and security threats than ever before. Many of these threats are manifested at the interaction level, where undesired or malicious coordinations between apps and physical devices can lead to intricate safety and security issues. This paper presents IoTCOM, an approach to automatically discover such hidden and unsafe interaction threats in a compositional and scalable fashion. It is backed with auto-mated program analysis and formally rigorous violation detection engines. IoTCOM relies on program analysis to automatically infer the relevant app’s behavior. Leveraging a novel strategy to trim the extracted app’s behavior prior to translating them to analyzable formal specifications,IoTCOM mitigates the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCOM’s ability to effectively detect a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown, and to significantly outperform the existing techniques in terms of scalability.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Alhanahnah, Mohannad and Stevens, Clay and Bagheri, Hamid},
	month = jul,
	year = {2020},
	keywords = {Formal Verification, Interaction Threats, IoT Safety},
	pages = {272--285},
}

@inproceedings{chi_cross-app_2020,
	title = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}: {Categorization}, {Detection} and {Handling}},
	shorttitle = {Cross-{App} {Interference} {Threats} in {Smart} {Homes}},
	doi = {10.1109/DSN48063.2020.00056},
	abstract = {Internet of Thing platforms prosper home automation applications (apps). Prior research concerns intra-app security. Our work reveals that automation apps, even secured individually, still cause a family of threats when they interplay, termed as Cross-App Interference (CAI) threats. We systematically categorize such threats and encode them using satisfiability modulo theories (SMT). We present HomeGuard, a system for detecting and handling CAI threats in real deployments. A symbolic executor is built to extract rule semantics, and instrumentation is utilized to capture configuration during app installation. Rules and configuration are checked against SMT models, the solutions of which indicate the existence of corresponding CAI threats. We further combine app functionalities, device attributes and CAI types to label the risk level of CAI instances. In our evaluation, HomeGuard discovers 663 CAI instances from 146 SmartThings market apps, imposing minor latency upon app installation and no runtime overhead.},
	booktitle = {2020 50th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Chi, Haotian and Zeng, Qiang and Du, Xiaojiang and Yu, Jiaping},
	month = jun,
	year = {2020},
	note = {ISSN: 1530-0889},
	keywords = {Actuators, Automation, Safety, Semantics, Smart homes, Temperature sensors, n/a},
	pages = {411--423},
}

@article{gold_ethics_2022,
	title = {Ethics in the mining of software repositories},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-10057-7},
	doi = {10.1007/s10664-021-10057-7},
	abstract = {Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ and users’ interactions with the repositories and with each other. The ethics issues raised by such research therefore need to be considered before beginning. This paper presents a discussion of ethics issues that can arise in MSR research, using the mining challenges from the years 2006 to 2021 as a case study to identify the kinds of data used. On the basis of contemporary research ethics frameworks we discuss ethics challenges that may be encountered in creating and using repositories and associated datasets. We also report some results from a small community survey of approaches to ethics in MSR research. In addition, we present four case studies illustrating typical ethics issues one encounters in projects and how ethics considerations can shape projects before they commence. Based on our experience, we present some guidelines and practices that can help in considering potential ethics issues and reducing risks.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Empirical Software Engineering},
	author = {Gold, Nicolas E. and Krinke, Jens},
	month = jan,
	year = {2022},
	pages = {17},
}

@article{kommeren_philips_2007,
	title = {Philips experiences in global distributed software development},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-007-9047-3},
	doi = {10.1007/s10664-007-9047-3},
	abstract = {Global software development is increasingly common. Main expected benefits are improvements in time-to-market efficiency and access to greater—and less costly—resources. A number of problems are still to be solved before the full potential of global development can be obtained. This paper describes the experience of over 10 years of global distributed development at Philips, derived from about 200 projects. We discuss the experience and lessons learnt from multi-site development. Main lessons learned are that explicit agreements and ways of working should be defined for the following areas needing the most attention; team coordination and communication, requirements and architectures, integration, and configuration management. In addition, we discuss the experience gained from subcontracting software development to suppliers. Main lesson learned from subcontracting software development is the need for explicit attention and ways of working with respect to selection of suppliers, specification of the work to be subcontracted and establishment and content of the contract.},
	language = {en},
	number = {6},
	urldate = {2021-11-02},
	journal = {Empirical Software Engineering},
	author = {Kommeren, Rob and Parviainen, Päivi},
	month = oct,
	year = {2007},
	pages = {647--660},
}

@inproceedings{leesatapornwongsa_taxdc_2016,
	address = {Atlanta Georgia USA},
	title = {{TaxDC}: {A} {Taxonomy} of {Non}-{Deterministic} {Concurrency} {Bugs} in {Datacenter} {Distributed} {Systems}},
	isbn = {978-1-4503-4091-5},
	shorttitle = {{TaxDC}},
	url = {https://dl.acm.org/doi/10.1145/2872362.2872374},
	doi = {10.1145/2872362.2872374},
	abstract = {We present TaxDC, the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in distributed systems. We study 104 distributed concurrency (DC) bugs from four widely-deployed cloud-scale datacenter distributed systems, Cassandra, Hadoop MapReduce, HBase and ZooKeeper. We study DC-bug characteristics along several axes of analysis such as the triggering timing condition and input preconditions, error and failure symptoms, and ﬁx strategies, collectively stored as 2,083 classiﬁcation labels in TaxDC database. We discuss how our study can open up many new research directions in combating DC bugs.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Leesatapornwongsa, Tanakorn and Lukman, Jeffrey F. and Lu, Shan and Gunawi, Haryadi S.},
	month = mar,
	year = {2016},
	pages = {517--530},
}

@article{sun_conflict_2015,
	title = {Conflict {Detection} {Scheme} {Based} on {Formal} {Rule} {Model} for {Smart} {Building} {Systems}},
	volume = {45},
	issn = {2168-2305},
	doi = {10.1109/THMS.2014.2364613},
	abstract = {Smart building systems can provide flexible and configurational sensing and controlling operations according to users' requirements. As the number and the complexity of service rules customized by users have significantly increased, there is an increasing danger of conflict during the interaction process between users and the system. To address this issue, we propose a new rule conflict detection scheme tailored for the smart building system. First, we present a formal rule model UTEA based on User, Triggers, Environment entities, and Actuators. This model can handle not only controlled devices with discrete status but also real-valued environmental data such as temperature and humidity. In addition, this model takes multiple users with different authorities into account. Second, we define 11 rule relations and further classify conflicts into five categories. Third, we implement a rule storage system for detecting conflicts and design a conflict detection algorithm, which can detect the conflict between two rules as well as cycle conflict/multicross contradiction among multiple rules. We evaluated our scheme in a real smart building system with more than 30 000 service rules. The experiment results show that our scheme improves the performance in terms of error/missed-detection rates and running time.},
	number = {2},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Sun, Yan and Wang, Xukai and Luo, Hong and Li, Xiangyang},
	month = apr,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Human-Machine Systems},
	keywords = {Actuators, Conflict detection, Humidity, Intelligent sensors, Smart buildings, Sun, Temperature sensors, rule model, service, smart building system},
	pages = {215--227},
}

@inproceedings{ma_cityguard_2017-1,
	title = {{CityGuard}: {A} {Watchdog} for {Safety}-{Aware} {Conflict} {Detection} in {Smart} {Cities}},
	shorttitle = {{CityGuard}},
	abstract = {Nowadays, increasing number of smart services are being developed and deployed in cities around the world. IoT platforms have emerged to integrate smart city services and city resources, and thus improve city performance in the domains of transportation, emergency, environment, public safety, etc. Despite the increasing intelligence of smart services and the sophistication of platforms, the safety issues in smart cities are not addressed adequately, especially the safety issues arising from the integration of smart services. Therefore, CityGuard, a safety-aware watchdog architecture is developed. To the best of our knowledge, it is the first architecture that detects and resolves conflicts among actions of different services considering both safety and performance requirements. Prior to developing CityGuard, safety and performance requirements and a spectrum of conflicts are specified. Sophisticated models are used to analyze secondary effects, and detect device and environmental conflicts. A simulation based on New York City is used for the evaluation. The results show that CityGuard (i) identifies unsafe actions and thus helps to prevent the city from safety hazards, (ii) detects and resolves two major types of conflicts, i.e., device and environmental conflicts, and (iii) improves the overall city performance.},
	booktitle = {2017 {IEEE}/{ACM} {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation} ({IoTDI})},
	author = {Ma, Meiyi and Preum, Sarah Masud and Stankovic, John A.},
	month = apr,
	year = {2017},
	keywords = {City Safety, City Simulation, Conflict Detection, Context, Emergency services, Roads, Smart City, Smart cities},
	pages = {259--270},
}

@article{nosco_industrial_nodate,
	title = {The {Industrial} {Age} of {Hacking}},
	abstract = {There is a cognitive bias in the hacker community to select a piece of software and invest signiﬁcant human resources into ﬁnding bugs in that software without any prior indication of success. We label this strategy depth-ﬁrst search and propose an alternative: breadth-ﬁrst search. In breadthﬁrst search, humans perform minimal work to enable automated analysis on a range of targets before committing additional time and effort to research any particular one. We present a repeatable human study that leverages teams of varying skill while using automation to the greatest extent possible. Our goal is a process that is effective at ﬁnding bugs; has a clear plan for the growth, coaching, and efﬁcient use of team members; and supports measurable, incremental progress. We derive an assembly-line process that improves on what was once intricate, manual work. Our work provides evidence that the breadth-ﬁrst approach increases the effectiveness of teams.},
	language = {en},
	author = {Nosco, Tim and Ziegler, Jared and Clark, Zechariah},
	pages = {19},
}

@inproceedings{ma_detection_2016-1,
	title = {Detection of {Runtime} {Conflicts} among {Services} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP.2016.7501688},
	abstract = {The populations of large cities around the world are growing rapidly. Cities are beginning to address this problem by implementing significant sensing and actuation infrastructure and building services on this infrastructure. However, as the density of sensing and actuation increases and as the complexities of services grow there is an increasing potential for conflicts across Smart City services. These conflicts can cause unsafe situations and disrupt the benefits that the services were originally intended to provide. Although some of the conflicts can be detected and avoided during designing the services, many can still occur unpredictably during runtime. This paper carefully defines and enumerates the main issues regarding the detection and resolution of runtime conflicts in smart cities. In particular, it focuses on conflicts that arise across services. This issue is becoming more and more important as Smart City designs attempt to integrate services from different domains (transportation, energy, public safety, emergency, medical, and many others). Research challenges are identified and then addressed that deal with uncertainty, dynamism, real-time, mobility and spatio-temporal availability, duration and scale of effect, efficiency, and ownership. A watchdog architecture is also described that oversees the services operating in a Smart City. This watchdog solution detects and resolves conflicts, it learns and adapts, and it provides additional inputs to decision making aspects of services. Using data from a Smart City dataset, an emulated set of services and activities using those services are created to perform a conflict analysis. A second analysis hypothesizes 41 future services across 5 domains. Both of these evaluations demonstrate the high probability of conflicts in smart cities of the future.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Ma, M. and Preum, S. Masud and Tarneberg, W. and Ahmed, M. and Ruiters, M. and Stankovic, J.},
	month = may,
	year = {2016},
	keywords = {Roads, Safety, Smart cities, Uncertainty, Vehicles},
	pages = {1--10},
}

@article{kozlov_assessing_2008,
	title = {Assessing maintainability change over multiple software releases},
	volume = {20},
	issn = {1532060X, 15320618},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/smr.361},
	doi = {10.1002/smr.361},
	abstract = {The focus of the paper is to reveal the relationships between software maintainability and other internal software quality attributes. The source code characteristics of ﬁve Java-based open-source software products are analyzed using the software measurement tool SoftCalc. The relationships between maintainability and internal quality attributes are identiﬁed based on the Pearson product moment correlation analysis. Our results show negative correlations between maintainability and some well-known internal software quality attributes, as well as the ones between maintainability and complexity metrics. Particularly, according to our results, the Number of Data Variables Declared and the Decisional Complexity McClure Metric have the strongest correlations with maintainability. The results of our study, that is to say, knowledge about the relationships between internal software quality attributes and maintainability, can be used as a basis for improvement of software maintainability at earlier stages of the software development process. Copyright © 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2021-10-26},
	journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	author = {Kozlov, Denis and Koskinen, Jussi and Sakkinen, Markku and Markkula, Jouni},
	month = jan,
	year = {2008},
	pages = {31--58},
}

@inproceedings{ahmed_empirical_2015,
	address = {Beijing, China},
	title = {An {Empirical} {Study} of {Design} {Degradation}: {How} {Software} {Projects} {Get} {Worse} over {Time}},
	isbn = {978-1-4673-7899-4},
	shorttitle = {An {Empirical} {Study} of {Design} {Degradation}},
	url = {http://ieeexplore.ieee.org/document/7321186/},
	doi = {10.1109/ESEM.2015.7321186},
	abstract = {Method: We conducted an empirical study on the presence and evolution of code smells, used as an indicator of design degradation in 220 open source projects.
Results: The best approach to maintain the quality of a project is to spend time reducing both software defects (bugs) and design issues (refactoring). We found that design issues are frequently ignored in favor of fixing defects. We also found that design issues have a higher chance of being fixed in the early stages of a project, and that efforts to correct these stall as projects mature and the code base grows, leading to a build-up of problems.
Conclusions: From studying a large set of open source projects, our research suggests that while core contributors tend to fix design issues more often than non-core contributors, there is no difference once the relative quantity of commits is accounted for. We also show that design issues tend to build up over time.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {2015 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	publisher = {IEEE},
	author = {Ahmed, Iftekhar and Mannan, Umme Ayda and Gopinath, Rahul and Jensen, Carlos},
	month = oct,
	year = {2015},
	pages = {1--10},
}

@inproceedings{codabux_managing_2013,
	address = {San Francisco, CA, USA},
	title = {Managing technical debt: {An} industrial case study},
	isbn = {978-1-4673-6443-0},
	shorttitle = {Managing technical debt},
	url = {https://ieeexplore.ieee.org/document/6608672},
	doi = {10.1109/MTD.2013.6608672},
	abstract = {Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management’s high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20\% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {2013 4th {International} {Workshop} on {Managing} {Technical} {Debt} ({MTD})},
	publisher = {IEEE},
	author = {Codabux, Zadia and Williams, Byron},
	month = may,
	year = {2013},
	pages = {8--15},
}

@inproceedings{besker_how_2019,
	address = {Kallithea-Chalkidiki, Greece},
	title = {How {Regulations} of {Safety}-{Critical} {Software} {Affect} {Technical} {Debt}},
	isbn = {978-1-72813-421-5},
	url = {https://ieeexplore.ieee.org/document/8906517/},
	doi = {10.1109/SEAA.2019.00020},
	abstract = {In recent years in the software industry, the use of safety-critical software is increasing at a rapid rate. However, little is known about the relationship between safety-critical regulations and the management of technical debt. The research is based on interviews with 19 practitioners working in different safety-critical domains implementing software according to different safety regulation standards. The results are three-fold. First, the result shows that performing technical debt refactoring tasks in safety-critical software requires several additional activities and costs, compared to non-safety-critical software. This study has also identified several negative effects due to the impact of these regulatory requirements. Second, the results show that the safety-critical regulations strengthen the implementation of both source code and architecture and thereby initially limit the introduction of technical debt. However, at the same time, the regulations also force the software companies to perform later suboptimal work-around solutions that are counterproductive in achieving a high-quality software since the regulations constrain the possibility of performing optimal TD refactoring activities. Third, the result shows that technical debt refactoring decisions are heavily weighed on the costs associated with the application’s recertification process and that these decisions seldom include the benefits of the refactoring activities in a structured way.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {2019 45th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Besker, Terese and Martini, Antonio and Bosch, Jan},
	month = aug,
	year = {2019},
	pages = {74--81},
}

@article{paliotta_paying_nodate,
	title = {Paying off technical debt in safety-critical automotive software},
	language = {en},
	author = {Paliotta, John},
	pages = {2},
}

@article{cunningham1992wycash,
	title = {The {WyCash} portfolio management system},
	volume = {4},
	number = {2},
	journal = {ACM SIGPLAN OOPS Messenger},
	author = {Cunningham, Ward},
	year = {1992},
	note = {Publisher: ACM New York, NY, USA},
	pages = {29--30},
}

@misc{mcconnell2008managing,
	title = {Managing technical debt},
	author = {McConnell, Steve},
	year = {2008},
}

@inproceedings{ernst_measure_2015,
	address = {Bergamo Italy},
	title = {Measure it? {Manage} it? {Ignore} it? software practitioners and technical debt},
	isbn = {978-1-4503-3675-8},
	shorttitle = {Measure it?},
	url = {https://dl.acm.org/doi/10.1145/2786805.2786848},
	doi = {10.1145/2786805.2786848},
	abstract = {The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore, while respondents believe the metaphor is itself important for communication, existing tools are not currently helpful in managing the details. We use our results to motivate a technical debt timeline to focus management and tooling approaches.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of the 2015 10th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ernst, Neil A. and Bellomo, Stephany and Ozkaya, Ipek and Nord, Robert L. and Gorton, Ian},
	month = aug,
	year = {2015},
	pages = {50--60},
}

@article{lim_balancing_2012,
	title = {A {Balancing} {Act}: {What} {Software} {Practitioners} {Have} to {Say} about {Technical} {Debt}},
	volume = {29},
	issn = {0740-7459},
	shorttitle = {A {Balancing} {Act}},
	url = {http://ieeexplore.ieee.org/document/6280547/},
	doi = {10.1109/MS.2012.130},
	language = {en},
	number = {6},
	urldate = {2021-10-26},
	journal = {IEEE Software},
	author = {Lim, Erin and Taksande, Nitin and Seaman, Carolyn},
	month = nov,
	year = {2012},
	pages = {22--27},
}

@article{hofstede1984hofstede,
	title = {Hofstede's culture dimensions: {An} independent validation using {Rokeach}'s value survey},
	volume = {15},
	number = {4},
	journal = {Journal of cross-cultural psychology},
	author = {Hofstede, Geert and Bond, Michael H},
	year = {1984},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {417--433},
}

@article{kruchten_technical_2012,
	title = {Technical {Debt}: {From} {Metaphor} to {Theory} and {Practice}},
	volume = {29},
	issn = {0740-7459},
	shorttitle = {Technical {Debt}},
	url = {http://ieeexplore.ieee.org/document/6336722/},
	doi = {10.1109/MS.2012.167},
	language = {en},
	number = {6},
	urldate = {2021-10-25},
	journal = {IEEE Software},
	author = {Kruchten, Philippe and Nord, Robert L. and Ozkaya, Ipek},
	month = nov,
	year = {2012},
	pages = {18--21},
}

@misc{tulsa_replication_2018,
	title = {Replication {Data} for: {Cybersecurity} {Research} {Datasets}: {Taxonomy} and {Empirical} {Analysis}},
	shorttitle = {Replication {Data} for},
	url = {https://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/4EPUIA},
	doi = {10.7910/DVN/4EPUIA},
	abstract = {We inspect 965 cybersecurity research papers published between 2012 and 2016 in order to understand better how datasets are used, produced and shared. We construct a taxonomy of the types of data created and shared, informed and validated by the examined papers. We then analyze the gathered data on datasets. Three quarters of existing datasets used as input to research are publicly available, but less than one ﬁfth of datasets created by researchers are publicly shared. Using a series of linear regressions, we demonstrate that those researchers who do make public the datasets they create are rewarded with more citations to the associated papers. Hence, we conclude that an under-appreciated incentive exists for researchers to share their created datasets with the broader research community.},
	language = {en},
	urldate = {2021-10-25},
	publisher = {Harvard Dataverse},
	author = {Tulsa), Tyler (The University Of, Moore},
	year = {2018},
}

@article{leesatapornwongsa_transactuations_2020,
	title = {Transactuations: {Where} {Transactions} {Meet} the {Physical} {World}},
	volume = {36},
	issn = {0734-2071, 1557-7333},
	shorttitle = {Transactuations},
	url = {https://dl.acm.org/doi/10.1145/3380907},
	doi = {10.1145/3380907},
	abstract = {A large class of IoT applications read sensors, execute application logic, and actuate actuators. However, the lack of highlevel programming abstractions compromises correctness especially in presence of failures and unwanted interleaving between applications. A key problem arises when operations on IoT devices or the application itself fails, which leads to inconsistencies between the physical state and application state, breaking application semantics and causing undesired consequences. Transactions are a well-established abstraction for correctness, but assume properties that are absent in an IoT context. In this paper, we study one such environment, smart home, and establish inconsistencies manifesting out of failures. We propose an abstraction called transactuation that empowers developers to build reliable applications. Our runtime, Relacs, implements the abstraction atop a real smarthome platform. We evaluate programmability, performance, and effectiveness of transactuations to demonstrate its potential as a powerful abstraction and execution model.},
	language = {en},
	number = {4},
	urldate = {2021-10-22},
	journal = {ACM Transactions on Computer Systems},
	author = {Leesatapornwongsa, Tanakorn and Sengupta, Aritra and Ardekani, Masoud Saeida and Petri, Gustavo and Stuardo, Cesar A.},
	month = jun,
	year = {2020},
	pages = {1--31},
}

@article{celik_sensitive_nodate,
	title = {Sensitive {Information} {Tracking} in {Commodity} {IoT}},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has had profound effects on society–smart homes, personal monitoring devices, enhanced manufacturing and other IoT applications have changed the way we live, play, and work. Yet extant IoT platforms provide few means of evaluating the use (and potential avenues for misuse) of sensitive information. Thus, consumers and organizations have little information to assess the security and privacy risks these devices present. In this paper, we present SAINT, a static taint analysis tool for IoT applications. SAINT operates in three phases; (a) translation of platform-speciﬁc IoT source code into an intermediate representation (IR), (b) identifying sensitive sources and sinks, and (c) performing static analysis to identify sensitive data ﬂows. We evaluate SAINT on 230 SmartThings market apps and ﬁnd 138 (60\%) include sensitive data ﬂows. In addition, we demonstrate SAINT on IOTBENCH, a novel open-source test suite containing 19 apps with 27 unique data leaks. Through this effort, we introduce a rigorously grounded framework for evaluating the use of sensitive information in IoT apps—and therein provide developers, markets, and consumers a means of identifying potential threats to security and privacy.},
	language = {en},
	author = {Celik, Z Berkay and Babun, Leonardo and Sikder, Amit K and Aksu, Hidayet and Tan, Gang and McDaniel, Patrick and Uluagac, A Selcuk},
	pages = {19},
}

@inproceedings{li_what_2015,
	address = {Florence, Italy},
	title = {What {Makes} a {Great} {Software} {Engineer}?},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194618/},
	doi = {10.1109/ICSE.2015.335},
	abstract = {Good software engineers are essential to the creation of good software. However, most of what we know about softwareengineering expertise are vague stereotypes, such as ‘excellent communicators’ and ‘great teammates’. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Li, Paul Luo and Ko, Andrew J. and Zhu, Jiamin},
	month = may,
	year = {2015},
	pages = {700--710},
}

@inproceedings{baltes_towards_2018,
	address = {Lake Buena Vista FL USA},
	title = {Towards a theory of software development expertise},
	isbn = {978-1-4503-5573-5},
	url = {https://dl.acm.org/doi/10.1145/3236024.3236061},
	doi = {10.1145/3236024.3236061},
	abstract = {Software development includes diverse tasks such as implementing new features, analyzing requirements, and fixing bugs. Being an expert in those tasks requires a certain set of skills, knowledge, and experience. Several studies investigated individual aspects of software development expertise, but what is missing is a comprehensive theory. We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance. Our theory currently focuses on programming, but already provides valuable insights for researchers, developers, and employers. The theory describes important properties of software development expertise and which factors foster or hinder its formation, including how developers’ performance may decline over time. Moreover, our quantitative results show that developers’ expertise self-assessments are context-dependent and that experience is not necessarily related to expertise.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Baltes, Sebastian and Diehl, Stephan},
	month = oct,
	year = {2018},
	keywords = {Exemplar},
	pages = {187--200},
}

@article{li_what_2020,
	title = {What distinguishes great software engineers?},
	volume = {25},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-019-09773-y},
	doi = {10.1007/s10664-019-09773-y},
	abstract = {Great software engineers are essential to the creation of great software. However, today, we lack an understanding of what distinguishes great engineers from ordinary ones. We address this knowledge gap by conducting one of the largest mixed-method studies of experienced engineers to date. We surveyed 1,926 expert engineers, including senior engineers, architects, and technical fellows, asking them to judge the importance of a comprehensive set of 54 attributes of great engineers. We then conducted 77 email interviews to interpret our findings and to understand the influence of contextual factors on the ratings. After synthesizing the findings, we believe that the top five distinguishing characteristics of great engineers are writing good code, adjusting behaviors to account for future value and costs, practicing informed decision-making, avoiding making others’ jobs harder, and learning continuously. We relate the findings to prior work, and discuss implications for researchers, practitioners, and educators.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Empirical Software Engineering},
	author = {Li, Paul Luo and Ko, Amy J. and Begel, Andrew},
	month = jan,
	year = {2020},
	pages = {322--352},
}

@article{capretz_personality_2003,
	title = {Personality types in software engineering},
	volume = {58},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581902001374},
	doi = {10.1016/S1071-5819(02)00137-4},
	abstract = {Software engineering is forecast to be among the fastest growing employment ﬁeld in the next decades. The purpose of this investigation is two-fold: Firstly, empirical studies on the personality types of software professionals are reviewed. Secondly, this work provides an upto-date personality proﬁle of software engineers according to the Myers–Briggs Type Indicator.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {International Journal of Human-Computer Studies},
	author = {Capretz, Luiz Fernando},
	month = feb,
	year = {2003},
	pages = {207--214},
}

@article{chaki_adaptive_2021,
	title = {Adaptive {Priority}-based {Conflict} {Resolution} of {IoT} {Services}},
	url = {http://arxiv.org/abs/2107.08348},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. An adaptive priority model is developed considering the residents' contextual factors (e.g., age, illness, impairment). The proposed priority model is designed using the concept of the analytic hierarchy process. A set of experiments on real-world datasets are conducted to show the efficiency of the proposed approach.},
	urldate = {2021-10-21},
	journal = {arXiv:2107.08348 [cs]},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.08348},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{chaki_dynamic_2021,
	title = {Dynamic {Conflict} {Resolution} of {IoT} {Services} in {Smart} {Homes}},
	url = {http://arxiv.org/abs/2110.07083},
	abstract = {We propose a novel conflict resolution framework for IoT services in multi-resident smart homes. The proposed framework employs a preference extraction model based on a temporal proximity strategy. We design a preference aggregation model using a matrix factorization-based approach (i.e., singular value decomposition). The concepts of current resident item matrix and ideal resident item matrix are introduced as key criteria to cater to the conflict resolution framework. Finally, a set of experiments on real-world datasets are conducted to show the effectiveness of the proposed approach.},
	urldate = {2021-10-21},
	journal = {arXiv:2110.07083 [cs]},
	author = {Chaki, Dipankar and Bouguettaya, Athman},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.07083},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{ananthanarayanan_keeping_2019,
	address = {Dresden Germany},
	title = {Keeping {Master} {Green} at {Scale}},
	isbn = {978-1-4503-6281-8},
	url = {https://dl.acm.org/doi/10.1145/3302424.3303970},
	doi = {10.1145/3302424.3303970},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the {Fourteenth} {EuroSys} {Conference} 2019},
	publisher = {ACM},
	author = {Ananthanarayanan, Sundaram and Ardekani, Masoud Saeida and Haenikel, Denis and Varadarajan, Balaji and Soriano, Simon and Patel, Dhaval and Adl-Tabatabai, Ali-Reza},
	month = mar,
	year = {2019},
	pages = {1--15},
}

@inproceedings{sadowski_modern_2018,
	address = {Gothenburg Sweden},
	title = {Modern code review: a case study at google},
	isbn = {978-1-4503-5659-6},
	shorttitle = {Modern code review},
	url = {https://dl.acm.org/doi/10.1145/3183519.3183525},
	doi = {10.1145/3183519.3183525},
	abstract = {Employing lightweight, tool-based code review of code changes (aka modern code review) has become the norm for a wide variety of open-source and industrial systems. In this paper, we make an exploratory investigation of modern code review at Google. Google introduced code review early on and evolved it over the years; our study sheds light on why Google introduced this practice and analyzes its current status, after the process has been reﬁned through decades of code changes and millions of code reviews. By means of 12 interviews, a survey with 44 respondents, and the analysis of review logs for 9 million reviewed changes, we investigate motivations behind code review at Google, current practices, and developers’ satisfaction and challenges.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Sadowski, Caitlin and Söderberg, Emma and Church, Luke and Sipko, Michal and Bacchelli, Alberto},
	month = may,
	year = {2018},
	pages = {181--190},
}

@article{potvin_why_2016,
	title = {Why {Google} stores billions of lines of code in a single repository},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2854146},
	doi = {10.1145/2854146},
	abstract = {Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world.},
	language = {en},
	number = {7},
	urldate = {2021-10-21},
	journal = {Communications of the ACM},
	author = {Potvin, Rachel and Levenberg, Josh},
	month = jun,
	year = {2016},
	pages = {78--87},
}

@article{capretz_making_2010,
	title = {Making {Sense} of {Software} {Development} and {Personality} {Types}},
	volume = {12},
	issn = {1520-9202},
	url = {http://ieeexplore.ieee.org/document/5403172/},
	doi = {10.1109/MITP.2010.33},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {IT Professional},
	author = {Capretz, L.F. and Ahmed, F.},
	month = jan,
	year = {2010},
	keywords = {Exemplar},
	pages = {6--13},
}

@article{peck_where_1980,
	title = {"{Where} has all the judgment gone?" {The} fifth {Laurits} {Bjerrum} memorial lecture},
	volume = {17},
	issn = {0008-3674, 1208-6010},
	shorttitle = {"{Where} has all the judgment gone?},
	url = {http://www.nrcresearchpress.com/doi/10.1139/t80-065},
	doi = {10.1139/t80-065},
	abstract = {This lecture aims at significantly reducing the probability of failure of earth dams by consciously fostering the application of engineering judgment in design and application.},
	language = {en},
	number = {4},
	urldate = {2021-10-20},
	journal = {Canadian Geotechnical Journal},
	author = {Peck, Ralph B.},
	month = nov,
	year = {1980},
	pages = {584--590},
}

@article{petroski_failure_1993,
	title = {Failure as {Source} of {Engineering} {Judgment}: {Case} of {John} {Roebling}},
	volume = {7},
	issn = {0887-3828, 1943-5509},
	shorttitle = {Failure as {Source} of {Engineering} {Judgment}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%290887-3828%281993%297%3A1%2846%29},
	doi = {10.1061/(ASCE)0887-3828(1993)7:1(46)},
	abstract = {The proper use of the concepts and realities of failure is essential for successful design practice, which involves proper engineering judgment. Among the most valuable sources of good design judgment are case studies of how great engineers designed against failure. John Roebling is among the engineers whose works provide excellent models of good judgment and the explicit use of the knowledge of failures in designing successfulstructures. Roebling's use of failure concepts and case studies to avoid failure in his own designs provides a paradigm for good engineering practice generally. Although the analytical state of the art has certainly advanced since Roebling's time, the basic ideas of good engineering practice are no different now than they were in the 19th century. It therefore follows that a study of the methods of model engineers like Roebling can help develop judgment in modern engineers and thereby reduce the occurrence of failures in modern designs.},
	language = {en},
	number = {1},
	urldate = {2021-10-20},
	journal = {Journal of Performance of Constructed Facilities},
	author = {Petroski, Henry},
	month = feb,
	year = {1993},
	pages = {46--58},
}

@article{holt_designers_1997,
	title = {The designer's judgement},
	volume = {18},
	issn = {0142694X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0142694X96000130},
	doi = {10.1016/S0142-694X(96)00013-0},
	abstract = {Judgement is ubiquitous in engineering work but the demand for the exercise of good judgement is particularly evident in engineering design. Every advance, or change of direction, in the design process is the result of the designer's judgement. But the notion o f judgement is somewhat elusive. What is it? How can we know whether a judgement is good or bad? Can judgement be taught and if so when and how? This paper attempts to answer at least in part these questions. It draws on the work of Sir Geoffrey Vickers, seeking to adapt and relate his ideas on judgement to engineering design. It constructs aframework for design in which judgements are made in three knowledge domains; the formative, the commercial and the instrumental. It then considers the efficacy of contemporary academe and industry in developing the capacity for judgement in the neophyte engineer. It concludes with a broad strategy for teaching design in engineering courses. 0 19917Published by Elsevier Science Ltd.},
	language = {en},
	number = {1},
	urldate = {2021-10-20},
	journal = {Design Studies},
	author = {Holt, J.E.},
	month = jan,
	year = {1997},
	pages = {113--123},
}

@article{davis_plea_2012,
	title = {A {Plea} for {Judgment}},
	volume = {18},
	issn = {1353-3452, 1471-5546},
	url = {http://link.springer.com/10.1007/s11948-011-9254-6},
	doi = {10.1007/s11948-011-9254-6},
	abstract = {Judgment is central to engineering, medicine, the sciences and many other practical activities. For example, one who otherwise knows what engineers know but lacks ‘‘engineering judgment’’ may be an expert of sorts, a handy resource much like a reference book or database, but cannot be a competent engineer. Though often overlooked or at least passed over in silence, the central place of judgment in engineering, the sciences, and the like should be obvious once pointed out. It is important here because it helps to explain where ethics ﬁts into these disciplines. There is no good engineering, no good science, and so on without good judgment and no good judgment in these disciplines without ethics. Doing even a minimally decent job of teaching one of these disciplines necessarily includes teaching its ethics; teaching the ethics is teaching the discipline (or at least a large part of it).},
	language = {en},
	number = {4},
	urldate = {2021-10-20},
	journal = {Science and Engineering Ethics},
	author = {Davis, Michael},
	month = dec,
	year = {2012},
	pages = {789--808},
}

@techreport{abet2020abet,
	title = {{ABET} criteria for accrediting engineering programs},
	author = {{ABET Engineering Accreditation Commission}},
	year = {2020},
}

@article{weedon_role_2019,
	title = {The {Role} of {Rhetoric} in {Engineering} {Judgment}},
	volume = {62},
	issn = {0361-1434, 1558-1500},
	url = {https://ieeexplore.ieee.org/document/8667081/},
	doi = {10.1109/TPC.2019.2900824},
	abstract = {Introduction: ABET has approved changes to the EAC’s Criterion 3 that will take effect for the 2019-2020 accreditation cycle. Among many changes and rearrangements is the introduction of the term “engineering judgment” as one of the competencies that students must develop to prepare for professional engineering. Literature review: However, engineering judgment is not deﬁned in the criterion, and although it is a ubiquitous concept in the philosophy of engineering and engineering education, little empirical investigation has been undertaken into the practice of engineering judgment. And there is even less conceptual or empirical investigation into communication’s role in the practice of engineering judgment. Research questions: 1. What does engineering judgment look like in practice? 2. How does the sociotechnical situation affect engineering judgment? 3. What role does rhetoric have, not only in communicating judgments, but in forming them as well? 4. How can teachers and practitioners in engineering and technical communication use these ﬁndings to facilitate better judgment in the classroom and at work? Methods: Using videotape and ﬁeldnotes, the author examines the two sequences of decision-making from a student engineering design project. An ethnomethodologically inspired framework is used to exhibit the phenomenal details of “doing” engineering judgment. Discussion/conclusion: Data reveal that engineering judgment may be fruitfully understood by educators as not just a cognitive and individual ability to apply technical knowledge, but instead a capacity of participants to rhetorically establish common cause to interrogate and reﬂect on the relations between technical data and situations.},
	language = {en},
	number = {2},
	urldate = {2021-10-20},
	journal = {IEEE Transactions on Professional Communication},
	author = {Weedon, Scott},
	month = jun,
	year = {2019},
	pages = {165--177},
}

@article{reitz2001engineering,
	title = {What is engineering judgment?},
	volume = {73},
	number = {9},
	journal = {Machine Design},
	author = {Reitz, V and Graham, R and Wescott, T},
	year = {2001},
	note = {Publisher: Penton Publishing},
	pages = {174--174},
}

@article{herkert_managements_1991,
	title = {Management's hat trick: {Misuse} of "engineering judgment" in the challenger incident},
	volume = {10},
	issn = {0167-4544, 1573-0697},
	shorttitle = {Management's hat trick},
	url = {http://link.springer.com/10.1007/BF00382881},
	doi = {10.1007/BF00382881},
	language = {en},
	number = {8},
	urldate = {2021-10-20},
	journal = {Journal of Business Ethics},
	author = {Herkert, Joseph R.},
	month = aug,
	year = {1991},
	pages = {617--620},
}

@incollection{kirkman2005impact,
	title = {The impact of cultural value diversity on multicultural team performance},
	booktitle = {Managing multinational teams: {Global} perspectives},
	publisher = {Emerald Group Publishing Limited},
	author = {Kirkman, Bradley L and Shapiro, Debra L},
	year = {2005},
}

@phdthesis{spring_human_2019,
	title = {Human decision-making in computer security incident response},
	language = {en},
	school = {University College London},
	author = {Spring, Jonathan Michael},
	year = {2019},
}

@inproceedings{garcia_comprehensive_2020,
	title = {A {Comprehensive} {Study} of {Autonomous} {Vehicle} {Bugs}},
	abstract = {Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the open-source community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classification of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major findings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Garcia, Joshua and Feng, Yang and Shen, Junjie and Almanee, Sumaya and Xia, Yuan and Chen, Qi Alfred},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	keywords = {Autonomous vehicles, Companies, Computer bugs, Maintenance engineering, Open source software, Software engineering, Software systems, autonomous vehicles, bugs, defects, empirical software engineering},
	pages = {385--396},
}

@article{maznevski_cultural_2002,
	title = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}: {The} {Cultural} {Orientations} {Framework}},
	volume = {2},
	issn = {1470-5958, 1741-2838},
	shorttitle = {Cultural {Dimensions} at the {Individual} {Level} of {Analysis}},
	url = {http://journals.sagepub.com/doi/10.1177/147059580223001},
	doi = {10.1177/147059580223001},
	abstract = {This article describes a theoretically-grounded framework of cultural dimensions conceptualized and operationalized at the individual level of analysis, based on the work of anthropologists Kluckhohn and Strodtbeck. We present empirical data gathered from five countries – Canada, Mexico, the Netherlands, Taiwan, and the United States – to assess the validity of the framework. We then use the results to explore how the cultural orientations framework can add insight and new perspectives to critical questions in cross cultural management research.},
	language = {en},
	number = {3},
	urldate = {2021-10-14},
	journal = {International Journal of Cross Cultural Management},
	author = {Maznevski, Martha L. and Gomez, Carolina B. and DiStefano, Joseph J. and Noorderhaven, Niels G. and Wu, Pei-Chuan},
	month = dec,
	year = {2002},
	pages = {275--295},
}

@article{goldberg_alternative_nodate,
	title = {An {Alternative} "{Description} of {Personality}": {The} {Big}-{Five} {Factor} {Structure}},
	language = {en},
	author = {Goldberg, Lewis R},
	pages = {14},
}

@article{lee_internet_2015,
	title = {The {Internet} of {Things} ({IoT}): {Applications}, investments, and challenges for enterprises},
	volume = {58},
	issn = {00076813},
	shorttitle = {The {Internet} of {Things} ({IoT})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0007681315000373},
	doi = {10.1016/j.bushor.2015.03.008},
	abstract = {The Internet of Things (IoT), also called the Internet of Everything or the Industrial Internet, is a new technology paradigm envisioned as a global network of machines and devices capable of interacting with each other. The IoT is recognized as one of the most important areas of future technology and is gaining vast attention from a wide range of industries. This article presents ﬁve IoT technologies that are essential in the deployment of successful IoT-based products and services and discusses three IoT categories for enterprise applications used to enhance customer value. In addition, it examines the net present value method and the real option approach widely used in the justiﬁcation of technology projects and illustrates how the real option approach can be applied for IoT investment. Finally, this article discusses ﬁve technical and managerial challenges.},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {Business Horizons},
	author = {Lee, In and Lee, Kyoochun},
	year = {2015},
}

@techreport{stouffer_guide_2015,
	title = {Guide to {Industrial} {Control} {Systems} ({ICS}) {Security}},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-82r2.pdf},
	abstract = {This document provides guidance on how to secure Industrial Control Systems (ICS), including Supervisory Control and Data Acquisition (SCADA) systems, Distributed Control Systems (DCS), and other control system configurations such as Programmable Logic Controllers (PLC), while addressing their unique performance, reliability, and safety requirements. The document provides an overview of ICS and typical system topologies, identifies typical threats and vulnerabilities to these systems, and provides recommended security countermeasures to mitigate the associated risks.},
	language = {en},
	number = {NIST SP 800-82r2},
	urldate = {2021-09-29},
	institution = {National Institute of Standards and Technology},
	author = {Stouffer, Keith and Pillitteri, Victoria and Lightman, Suzanne and Abrams, Marshall and Hahn, Adam},
	year = {2015},
	doi = {10.6028/NIST.SP.800-82r2},
}

@article{freedman_why_2010,
	title = {Why {Scientific} {Studies} {Are} {So} {Often} {Wrong}: {The} {Streetlight} {Effect}},
	url = {https://www.discovermagazine.com/the-sciences/why-scientific-studies-are-so-often-wrong-the-streetlight-effect},
	urldate = {2021-10-04},
	journal = {Discover Magazine},
	author = {Freedman, David},
	year = {2010},
}

@article{long_oct_2009,
	title = {Oct. 26, 1992: {Software} {Glitch} {Cripples} {Ambulance} {Service}},
	issn = {1059-1028},
	url = {https://www.wired.com/2009/10/1026london-ambulance-computer-meltdown/},
	abstract = {Computerized systems can help, or hurt. In the case of the London Ambulance Service, it definitely hurt.},
	language = {en-US},
	urldate = {2021-10-05},
	journal = {Wired},
	author = {Long, Tony},
	year = {2009},
	note = {Section: tags},
	keywords = {20th century, britain, computers, health, london, tech gone bad},
}

@inproceedings{wang_exploratory_2021,
	address = {Athens Greece},
	title = {An exploratory study of autopilot software bugs in unmanned aerial vehicles},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468559},
	doi = {10.1145/3468264.3468559},
	abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly important and widely used in modern society. Software bugs in these systems can cause severe issues, such as system crashes, hangs, and undefined behaviors. Some bugs can also be exploited by hackers to launch security attacks, resulting in catastrophic consequences. Therefore, techniques that can help detect and fix software bugs in UAVs are highly desirable. However, although there are many existing studies on bugs in various types of software, the characteristics of UAV software bugs have never been systematically studied. This impedes the development of tools for assuring the dependability of UAVs. To bridge this gap, we conducted the first large-scale empirical study on two well-known open-source autopilot software platforms for UAVs, namely PX4 and Ardupilot, to characterize bugs in UAVs. Through analyzing 569 bugs from these two projects, we observed eight types of UAV-specific bugs (i.e., limit, math, inconsistency, priority, parameter, hardware support, correction, and initialization) and learned their root causes. Based on the bug taxonomy, we summarized common bug patterns and repairing strategies. We further identified five challenges associated with detecting and fixing such UAV-specific bugs. Our study can help researchers and practitioners to better understand the threats to the dependability of UAV systems and facilitate the future development of UAV bug diagnosis tools.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {European {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering} ({ESEC}/{FSE})},
	publisher = {ACM},
	author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei},
	year = {2021},
}

@inproceedings{celik2018soteria,
	title = {Soteria: {Automated} iot safety and security analysis},
	booktitle = {{USENIX} {Annual} {Technical} {Conference} ({ATC}\vphantom{\{}\}},
	author = {Celik, Z Berkay and McDaniel, Patrick and Tan, Gang},
	year = {2018},
}

@inproceedings{glomsrud_structured_2019,
	title = {A {Structured} {STPA} {Safety} and {Security} {Co}-analysis {Framework} for {Autonomous} {Ships}},
	isbn = {978-981-11-2724-3},
	url = {http://rpsonline.com.sg/proceedings/9789811127243/html/0105.xml},
	doi = {10.3850/978-981-11-2724-3_0105-cd},
	abstract = {Research and development of autonomous ships is progressing rapidly and involves all relevant stakeholders’ contributions. DNV GL as a classiﬁcation society is obligated to develop new maritime rules and guidelines aiming to ensure safe, secure and efﬁcient future maritime operations. We need to understand new challenges and assess new risks arising from the development and operations of such ships. Autonomous ships are complex safety-critical cyber-physical systems (CPS) of which safety and security are two crucial properties. It is imperative to identity inter-dependencies between safety and security for thoroughly assessing and managing potential risks. Therefore, we need a systematic framework with concrete execution steps to perform a combined safety and security co-analysis. The System Theoretic Process Analysis (STPA) is primarily developed for safety analysis of systems which can be depicted using well deﬁned control system designs. However, the STPA needs to be improved to properly analyze autonomous ships which typically integrate emerging technologies and deﬁne novel functionalities. This paper explores the feasibility of applying an STPA safety and security co-analysis on novel CPS. We identify a gap existing between the ﬁrst two steps in basic STPA, i.e., it is not straightforward to model the control structure(s) (in Step 2) from the system engineering foundation established in Step 1, with limited prior knowledge. Our ﬁrst contribution is to bridge this gap by extending the analysis of Step 1. More speciﬁcally, functional requirements derived from safety (or security) constraints are used to facilitate the modeling of control structures. The STPA safety and security co-analysis has been studied to assess the risks of CPS collectively. However, to the best of our knowledge, not too much work focusing on how to tightly incorporate conventional security analysis techniques into the STPA analysis process has been performed. Our second contribution is to improve the current STPA-Sec by appropriately integrating widely adopted security analysis methods into the analysis process. A comprehensive list of vulnerabilities and threats can be identiﬁed through the improved analysis process and speciﬁc security requirements are derived accordingly.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {European {Safety} and {Reliability} {Conference} ({ESREL})},
	author = {Glomsrud, Jon Arne},
	year = {2019},
}

@article{laplante_software_2017,
	title = {Software {Engineering} of {Safety}-{Critical} {Systems}: {Themes} {From} {Practitioners}},
	volume = {66},
	issn = {0018-9529, 1558-1721},
	shorttitle = {Software {Engineering} of {Safety}-{Critical} {Systems}},
	url = {http://ieeexplore.ieee.org/document/8006260/},
	doi = {10.1109/TR.2017.2731953},
	abstract = {This study addresses two important questions related to engineering of safety-critical software and software-intensive systems. The ﬁrst question is: which software and softwareintensive systems should be considered safety critical? The second question is: what processes, design practices, and tools have practitioners been using for building these systems? We answer these questions through an analysis of unstructured interviews with experienced engineers who self-describe as working on safety-critical systems. Then, a thematic analysis of these responses was conducted. The results of this study are intended to provide guidance to those building safety-critical systems and have implications on state engineering licensure boards, in the determination of legal liability, and in risk assessment for policymakers, corporate governors, and insurance executives.},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {IEEE Transactions on Reliability},
	author = {Laplante, Phillip A. and DeFranco, Joanna F.},
	year = {2017},
}

@article{munaiahCuratingGitHubEngineered2017,
	title = {Curating {GitHub} for engineered software projects},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-017-9512-6},
	doi = {10.1007/s10664-017-9512-6},
	language = {en},
	number = {6},
	urldate = {2021-04-01},
	journal = {Empirical Software Engineering (EMSE)},
	author = {Munaiah, Nuthan and Kroh, Steven and Cabrey, Craig and Nagappan, Meiyappan},
	year = {2017},
}

@article{leveson_role_2004,
	title = {Role of {Software} in {Spacecraft} {Accidents}},
	volume = {41},
	issn = {0022-4650, 1533-6794},
	url = {https://arc.aiaa.org/doi/10.2514/1.11950},
	doi = {10.2514/1.11950},
	language = {en},
	number = {4},
	urldate = {2021-10-06},
	journal = {Journal of Spacecraft and Rockets},
	author = {Leveson, Nancy G.},
	year = {2004},
}

@inproceedings{gladisch_experience_2019,
	title = {Experience {Paper}: {Search}-{Based} {Testing} in {Automated} {Driving} {Control} {Applications}},
	shorttitle = {Experience {Paper}},
	doi = {10.1109/ASE.2019.00013},
	abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
	booktitle = {{IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Measurement, Monitoring, Optimization, Software, Test pattern generators, Tools, automated driving, automated test generation, experience paper, search-based testing},
}

@article{fenton_strategy_1998,
	title = {A strategy for improving safety related software engineering standards},
	volume = {24},
	issn = {00985589},
	url = {http://ieeexplore.ieee.org/document/730547/},
	doi = {10.1109/32.730547},
	abstract = {There are many standards which are relevant for building safety or mission critical software systems. An effective standard is one that should help developers, assessors, and users of such systems. For developers the standard should help them build the system cost-effectively, and it should be clear what is required in order to conform to the standard. For assessors it should be possible to determine, objectively, compliance to the standard. Users and society at large should have some assurance that a system developed to the standard has quantified risks and benefits. Unfortunately, the existing standards do not adequately fulfill any of these varied requirements. We explain why standards are the way they are and then provide a strategy for improving them. Our approach is to evaluate standards on a number of key criteria that enable us to interpret the standard, identify its scope, and check the ease with which it can be applied and checked. We also need to demonstrate that the use of a standard is likely either to deliver reliable and safe systems at an acceptable cost or help predict reliability and safety accurately. Throughout the paper we examine, by example, a specific standard for safety critical systems (namely IEC 1508) and show how it can be improved by applying our strategy.},
	language = {en},
	number = {11},
	urldate = {2021-10-11},
	journal = {IEEE Transactions on Software Engineering (TSE)},
	author = {Fenton, N.E. and Neil, M.},
	year = {1998},
	pages = {1002--1013},
}

@inproceedings{emami-naeini_privacy_2017,
	title = {Privacy {Expectations} and {Preferences} in an {IoT} {World}},
	abstract = {With the rapid deployment of Internet of Things (IoT) technologies and the variety of ways in which IoT-connected sensors collect and use personal data, there is a need for transparency, control, and new tools to ensure that individual privacy requirements are met. To develop these tools, it is important to better understand how people feel about the privacy implications of IoT and the situations in which they prefer to be notiﬁed about data collection. We report on a 1,007-participant vignette study focusing on privacy expectations and preferences as they pertain to a set of 380 IoT data collection and use scenarios. Participants were presented with 14 scenarios that varied across eight categorical factors, including the type of data collected (e.g. location, biometrics, temperature), how the data is used (e.g., whether it is shared, and for what purpose), and other attributes such as the data retention period. Our ﬁndings show that privacy preferences are diverse and context dependent; participants were more comfortable with data being collected in public settings rather than in private places, and are more likely to consent to data being collected for uses they ﬁnd beneﬁcial. They are less comfortable with the collection of biometrics (e.g. ﬁngerprints) than environmental data (e.g. room temperature, physical presence). We also ﬁnd that participants are more likely to want to be notiﬁed about data practices that they are uncomfortable with. Finally, our study suggests that after observing individual decisions in just three data-collection scenarios, it is possible to predict their preferences for the remaining scenarios, with our model achieving an average accuracy of up to 86\%.},
	language = {en},
	booktitle = {Symposium on {Usable} {Privacy} and {Security} ({SOUPS})},
	author = {Emami-Naeini, Pardis and Bhagavatula, Sruti and Habib, Hana and Degeling, Martin and Bauer, Lujo and Cranor, Lorrie Faith and Sadeh, Norman},
	year = {2017},
}

@techreport{faganFoundationalCybersecurityActivities2020,
	address = {Gaithersburg, MD},
	title = {Foundational cybersecurity activities for {IoT} device manufacturers},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8259.pdf},
	abstract = {Internet of Things (IoT) devices often lack device cybersecurity capabilities their customers—organizations and individuals—can use to help mitigate their cybersecurity risks. Manufacturers can help their customers by improving how securable the IoT devices they make are by providing necessary cybersecurity functionality and by providing customers with the cybersecurity-related information they need. This publication describes recommended activities related to cybersecurity that manufacturers should consider performing before their IoT devices are sold to customers. These foundational cybersecurity activities can help manufacturers lessen the cybersecurity-related efforts needed by customers, which in turn can reduce the prevalence and severity of IoT device compromises and the attacks performed using compromised devices.},
	language = {en},
	number = {NIST IR 8259},
	urldate = {2021-03-29},
	institution = {National Institute of Standards and Technology},
	author = {Fagan, Michael and Megas, Katerina N and Scarfone, Karen and Smith, Matthew},
	year = {2020},
	doi = {10.6028/NIST.IR.8259},
	pages = {NIST IR 8259},
}

@inproceedings{ding_gfxdoctor_2017,
	title = {{GfxDoctor}: {A} {Holistic} {Graphics} {Energy} {Profiler} for {Mobile} {Devices}},
	isbn = {978-1-4503-4938-3},
	shorttitle = {{GfxDoctor}},
	url = {https://dl.acm.org/doi/10.1145/3064176.3064206},
	doi = {10.1145/3064176.3064206},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Twelfth {European} {Conference} on {Computer} {Systems} ({EuroSys})},
	publisher = {ACM},
	author = {Ding, Ning and Hu, Y. Charlie},
	year = {2017},
}

@inproceedings{charpenay2016introducing,
	title = {Introducing thing descriptions and interactions: {An} ontology for the web of things.},
	booktitle = {Joint {Proceedings} of the 3rd {Stream} {Reasoning} ({SR} 2016) and the 1st {Semantic} {Web} {Technologies} for the {Internet} of {Things} ({SWIT}) workshops at {ISWC}},
	author = {Charpenay, Victor and Käbisch, Sebastian and Kosch, Harald},
	year = {2016},
}

@inproceedings{bagheri_synthesis_2020,
	address = {Seoul South Korea},
	title = {Synthesis of assurance cases for software certification},
	isbn = {978-1-4503-7126-1},
	url = {https://dl.acm.org/doi/10.1145/3377816.3381728},
	doi = {10.1145/3377816.3381728},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Software} {Engineering}: {New} {Ideas} and {Emerging} {Results} ({ICSE}-{NIER})},
	publisher = {ACM},
	author = {Bagheri, Hamid and Kang, Eunsuk and Mansoor, Niloofar},
	year = {2020},
}

@inproceedings{alhanahnah_scalable_2020,
	title = {Scalable analysis of interaction threats in {IoT} systems},
	isbn = {978-1-4503-8008-9},
	url = {https://dl.acm.org/doi/10.1145/3395363.3397347},
	doi = {10.1145/3395363.3397347},
	abstract = {The ubiquity of Internet of Things (IoT) and our growing reliance on IoT apps are leaving us more vulnerable to safety and security threats than ever before. Many of these threats are manifested at the interaction level, where undesired or malicious coordinations between apps and physical devices can lead to intricate safety and security issues. This paper presents IotCom, an approach to automatically discover such hidden and unsafe interaction threats in a compositional and scalable fashion. It is backed with automated program analysis and formally rigorous violation detection engines. IotCom relies on program analysis to automatically infer the relevant app’s behavior. Leveraging a novel strategy to trim the extracted app’s behavior prior to translating them to analyzable formal specifications, IotCom mitigates the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IotCom’s ability to effectively detect a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown, and to significantly outperform the existing techniques in terms of scalability.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {{ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis} ({ISSTA})},
	publisher = {ACM},
	author = {Alhanahnah, Mohannad and Stevens, Clay and Bagheri, Hamid},
	year = {2020},
}

@inproceedings{bird2009promises,
	title = {The promises and perils of mining git},
	booktitle = {{IEEE} {International} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Bird, Christian and Rigby, Peter C and Barr, Earl T and Hamilton, David J and German, Daniel M and Devanbu, Prem},
	year = {2009},
}

@article{carreras_guzman_integrated_2021,
	title = {An integrated safety and security analysis for cyber-physical harm scenarios},
	volume = {144},
	issn = {0925-7535},
	url = {https://www.sciencedirect.com/science/article/pii/S0925753521003015},
	doi = {10.1016/j.ssci.2021.105458},
	abstract = {Increasing digitalization and autonomous solutions in physical systems promise to enhance their performance, cost-efficiency and reliability. However, the integration of novel information technologies with safety-related systems also brings new vulnerabilities and risks that challenge the traditional field of safety analysis. Particularly, cyber security threats are becoming key factors in complex accident scenarios in cyber-physical systems (CPSs), where unintentional errors and design flaws overlap with cyber security vulnerabilities that could lead to harm to humans and assets. This overlap between safety and security analysis is still a loosely defined domain without established theories and methods, leading to complications during the risk analysis of CPSs. In this paper, we first describe how the domain of safety science increasingly overlaps with security analysis. Subsequently, based on this overlapping, we illustrate and complement an integrated method for the identification of harm scenarios in CPSs. This method, coined Uncontrolled Flows of Information and Energy (UFoI-E), offers a distinct theoretical foundation rooted in accident causation models and a framework to design diagrammatic representations of CPSs during the analysis. After summarizing these features of the UFoI-E method, we present our original contribution to the method, which is a new practical toolkit for risk identification composed of an ontology of harm scenarios and a database of checklists built from lessons learned analysis and expert knowledge. Finally, we demonstrate an application of the method in an illustrative case and show representative fields for future work.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Safety Science},
	author = {Carreras Guzman, Nelson H. and Kozine, Igor and Lundteigen, Mary Ann},
	year = {2021},
	keywords = {Autonomous Systems, Bowtie Method, Cyber-Physical Harm Analysis for Safety and Security (CyPHASS), Cyber-Physical Systems (CPSs)},
}

@phdthesis{islam_towards_2020,
	type = {{PhD} {Thesis}},
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	language = {en},
	school = {Iowa State University},
	author = {Islam, Johirul},
	year = {2020},
}

@article{laplante_licensing_2014,
	title = {Licensing professional software engineers: seize the opportunity},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Licensing professional software engineers},
	url = {https://dl.acm.org/doi/10.1145/2618111},
	doi = {10.1145/2618111},
	abstract = {Professional organizations should be in the forefront of the ongoing discussion about licensing professional software engineers.},
	language = {en},
	number = {7},
	urldate = {2021-10-13},
	journal = {Communications of the ACM},
	author = {Laplante, Phillip A.},
	month = jul,
	year = {2014},
	pages = {38--40},
}

@article{house2013presidential,
	title = {Presidential policy directive/{PPD} 21–{Critical} infrastructure security and resilience},
	journal = {Washington, DC},
	author = {House, White},
	year = {2013},
}

@misc{dahmen-lhuissier_etsi_nodate,
	title = {{ETSI} - {Consumer} {IoT} security},
	url = {https://www.etsi.org/technologies/consumer-iot-security},
	abstract = {Consumer IoT security},
	language = {en-gb},
	urldate = {2021-10-12},
	journal = {ETSI},
	author = {Dahmen-Lhuissier, Sabine},
}

@misc{noauthor_isoiec_nodate,
	title = {{ISO}/{IEC} 30147:2021 {\textbar} {IEC} {Webstore}},
	url = {https://webstore.iec.ch/publication/62644},
	urldate = {2021-10-12},
}

@misc{noauthor_iec_nodate,
	title = {{IEC} 61508-1:2010 {\textbar} {IEC} {Webstore} {\textbar} functional safety, smart city},
	url = {https://webstore.iec.ch/publication/5515#additionalinfo},
	urldate = {2021-10-12},
}

@misc{noauthor_general_nodate,
	title = {General {Data} {Protection} {Regulation} ({GDPR}) – {Official} {Legal} {Text}},
	url = {https://gdpr-info.eu/},
	abstract = {General Data Protection Regulation (EU GDPR) – The official PDF of the Regulation (EU) 2016/679, its recitals \& key issues as a neatly arranged website.},
	language = {en-US},
	urldate = {2021-10-12},
	journal = {General Data Protection Regulation (GDPR)},
}

@misc{noauthor_family_2021,
	type = {Guides},
	title = {Family {Educational} {Rights} and {Privacy} {Act} ({FERPA})},
	url = {https://www2.ed.gov/policy/gen/guid/fpco/ferpa/index.html},
	abstract = {Family Educational Rights and Privacy Act (FERPA) Home Page.},
	language = {en},
	urldate = {2021-10-12},
	month = aug,
	year = {2021},
	note = {Publisher: US Department of Education (ED)},
}

@misc{rights_ocr_health_2021,
	type = {Text},
	title = {Health {Information} {Privacy}},
	url = {https://www.hhs.gov/hipaa/index.html},
	abstract = {Health Information Privacy},
	language = {en},
	urldate = {2021-10-12},
	journal = {HHS.gov},
	author = {Rights (OCR), Office for Civil},
	month = jun,
	year = {2021},
	note = {Last Modified: 2021-08-16T16:08:51-0400},
}

@book{hobbs2019embedded,
	title = {Embedded software development for safety-critical systems},
	publisher = {CRC Press},
	author = {Hobbs, Chris},
	year = {2019},
}

@book{sommerville2015software,
	title = {Software engineering},
	volume = {137035152},
	publisher = {Pearson Education},
	author = {Sommerville, Ian},
	year = {2015},
}

@inproceedings{atzori_blockchain-based_2017,
	title = {Blockchain-{Based} {Architectures} for the {Internet} of {Things}: {A} {Survey}},
	shorttitle = {Blockchain-{Based} {Architectures} for the {Internet} of {Things}},
	doi = {10.2139/SSRN.2846810},
	abstract = {This research explores the main features of three blockchain-based platforms for the Internet of Things, as recently emerged in academia as well as in industry. If properly engineered, the blockchain technology offers a disruptive solution to the problem of security and privacy in the Internet of Things environment, providing a new computational layer where data can be safely processed and analyzed, remaining private. The blockchain can also enable micro-payment functionality between digitally-enhanced devices, through ultra-light cryptocurrencies and smart contracts. The implementation of such features is expected to ensure a more efficient allocation of resources at global level, however it may also lead to undesirable consequences – such as a hyper-tokenization of society and a potentially dystopian concentration of power on big global platforms. Therefore, overall benefits and drawbacks of the blockchain deployment must necessarily take account of specific contexts of use, finding a balance between need for innovation, economic development and social sustainability.},
	author = {Atzori, M.},
	year = {2017},
}

@inproceedings{teslya_blockchain-based_2017,
	title = {Blockchain-based platform architecture for industrial {IoT}},
	doi = {10.23919/FRUCT.2017.8250199},
	abstract = {The development of robotics, the Internet of Things concept, big data processing techniques, automation, and distributed digital ledgers leads to the fourth industrial revolution. One of the main issues of new industry is interaction between the "smart factory" components both internally and with other factories based on the Internet of Things. This interaction should provide trust between the participants of the Internet of Things; control over the distribution of resources (such as maintenance time, energy, etc.) and finished products. The paper describes one of the possible ways of integrating Internet of Things and blockchain technologies to solve these issues. For this purpose, an architecture has been developed that combines Smart-M3 information sharing platform and blockchain platform. One of the main features of the proposed architecture is the use of smart contracts for processing and storing information related to the interaction between smart space components.},
	booktitle = {2017 21st {Conference} of {Open} {Innovations} {Association} ({FRUCT})},
	author = {Teslya, Nikolay and Ryabchikov, Igor},
	month = nov,
	year = {2017},
	keywords = {Contracts, Fault tolerance, Fault tolerant systems, Internet of Things, Production facilities, Service robots},
	pages = {321--329},
}

@article{reyna_blockchain_2018,
	title = {On blockchain and its integration with {IoT}. {Challenges} and opportunities},
	volume = {88},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329205},
	doi = {10.1016/j.future.2018.05.046},
	abstract = {In the Internet of Things (IoT) vision, conventional devices become smart and autonomous. This vision is turning into a reality thanks to advances in technology, but there are still challenges to address, particularly in the security domain e.g., data reliability. Taking into account the predicted evolution of the IoT in the coming years, it is necessary to provide confidence in this huge incoming information source. Blockchain has emerged as a key technology that will transform the way in which we share information. Building trust in distributed environments without the need for authorities is a technological advance that has the potential to change many industries, the IoT among them. Disruptive technologies such as big data and cloud computing have been leveraged by IoT to overcome its limitations since its conception, and we think blockchain will be one of the next ones. This paper focuses on this relationship, investigates challenges in blockchain IoT applications, and surveys the most relevant work in order to analyze how blockchain could potentially improve the IoT.},
	language = {en},
	urldate = {2021-10-11},
	journal = {Future Generation Computer Systems},
	author = {Reyna, Ana and Martín, Cristian and Chen, Jaime and Soler, Enrique and Díaz, Manuel},
	month = nov,
	year = {2018},
	keywords = {Blockchain, Internet of Things, Smart contract, Trust},
	pages = {173--190},
}

@article{astarita_review_2020,
	title = {A {Review} of {Blockchain}-{Based} {Systems} in {Transportation}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2078-2489/11/1/21},
	doi = {10.3390/info11010021},
	abstract = {This paper presents a literature review about the application of blockchain-based systems in transportation. The main aim was to identify, through the implementation of a multi-step methodology: current research-trends, main gaps in the literature, and possible future challenges. First, a bibliometric analysis was carried out to obtain a broad overview of the topic of interest. Subsequently, the most influential contributions were analysed in depth, with reference to the following two areas: supply chain and logistics; road traffic management and smart cities. The most important result is that the blockchain technology is still in an early stage, but appears extremely promising, given its possible applications within multiple fields, such as food track and trace, regulatory compliance, smart vehicles\&rsquo; security, and supply-demand matching. Much effort is still necessary for reaching the maturation stage because several models have been theorized in recent years, but very few have been implemented within real contexts. Moreover, the link blockchain-sustainability was explored, showing that this technology could be the trigger for limiting food waste, reducing exhaust gas emissions, favouring correct urban development, and, in general, improving quality of life.},
	language = {en},
	number = {1},
	urldate = {2021-10-11},
	journal = {Information},
	author = {Astarita, Vittorio and Giofrè, Vincenzo Pasquale and Mirabelli, Giovanni and Solina, Vittorio},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {blockchain, literature review, logistics, supply chain, transportation},
	pages = {21},
}

@inproceedings{shafagh_towards_2017,
	address = {New York, NY, USA},
	series = {{CCSW} '17},
	title = {Towards {Blockchain}-based {Auditable} {Storage} and {Sharing} of {IoT} {Data}},
	isbn = {978-1-4503-5204-8},
	url = {https://doi.org/10.1145/3140649.3140656},
	doi = {10.1145/3140649.3140656},
	abstract = {Today the cloud plays a central role in storing, processing, and distributing data. Despite contributing to the rapid development of IoT applications, the current IoT cloud-centric architecture has led into a myriad of isolated data silos that hinders the full potential of holistic data-driven analytics within the IoT. In this paper, we present a blockchain-based design for the IoT that brings a distributed access control and data management. We depart from the current trust model that delegates access control of our data to a centralized trusted authority and instead empower the users with data ownership. Our design is tailored for IoT data streams and enables secure data sharing. We enable a secure and resilient access control management, by utilizing the blockchain as an auditable and distributed access control layer to the storage layer. We facilitate the storage of time-series IoT data at the edge of the network via a locality-aware decentralized storage system that is managed with the blockchain technology. Our system is agnostic of the physical storage nodes and supports as well utilization of cloud storage resources as storage nodes.},
	urldate = {2021-10-10},
	booktitle = {Proceedings of the 2017 on {Cloud} {Computing} {Security} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Shafagh, Hossein and Burkhalter, Lukas and Hithnawi, Anwar and Duquennoy, Simon},
	month = nov,
	year = {2017},
	keywords = {access control, blockchain, cloud, edge, iot, security, time-series},
	pages = {45--50},
}

@article{sun_blockchain-based_2016,
	title = {Blockchain-based sharing services: {What} blockchain technology can contribute to smart cities},
	volume = {2},
	issn = {2199-4730},
	shorttitle = {Blockchain-based sharing services},
	url = {https://doi.org/10.1186/s40854-016-0040-y},
	doi = {10.1186/s40854-016-0040-y},
	abstract = {The notion of smart city has grown popular over the past few years. It embraces several dimensions depending on the meaning of the word “smart” and benefits from innovative applications of new kinds of information and communications technology to support communal sharing.},
	number = {1},
	urldate = {2021-10-11},
	journal = {Financial Innovation},
	author = {Sun, Jianjun and Yan, Jiaqi and Zhang, Kem Z. K.},
	month = dec,
	year = {2016},
	keywords = {Blockchain, Internet of Things (IoT), Sharing economy, Smart city, Smart contract},
	pages = {26},
}

@inproceedings{samaniego_blockchain_2016,
	title = {Blockchain as a {Service} for {IoT}},
	doi = {10.1109/iThings-GreenCom-CPSCom-SmartData.2016.102},
	abstract = {A blockchain is a distributed and decentralized ledger that contains connected blocks of transactions. Unlike other ledger approaches, blockchain guarantees tamper proof storage of approved transactions. Due to its distributed and decentralized organization, blockchain is beeing used within IoT e.g. to manage device configuration, store sensor data and enable micro-payments. This paper presents the idea of using blockchain as a service for IoT and evaluates the performance of a cloud and edge hosted blockchain implementation.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Internet} of {Things} ({iThings}) and {IEEE} {Green} {Computing} and {Communications} ({GreenCom}) and {IEEE} {Cyber}, {Physical} and {Social} {Computing} ({CPSCom}) and {IEEE} {Smart} {Data} ({SmartData})},
	author = {Samaniego, Mayra and Jamsrandorj, Uurtsaikh and Deters, Ralph},
	month = dec,
	year = {2016},
	keywords = {Bandwidth, Blockchain as a service, Cloud computing, Computer science, Delays, Distributed databases, Internet of Things, IoT, Online banking, edge},
	pages = {433--436},
}

@incollection{lieberman_end-user_2006,
	address = {Dordrecht},
	series = {Human-{Computer} {Interaction} {Series}},
	title = {End-{User} {Development}: {An} {Emerging} {Paradigm}},
	isbn = {978-1-4020-5386-3},
	shorttitle = {End-{User} {Development}},
	url = {https://doi.org/10.1007/1-4020-5386-X_1},
	abstract = {We think that over the next few years, the goal of interactive systems and services will evolve from just making systems easy to use (even though that goal has not yet been completely achieved) to making systems that are easy to develop by end users. By now, most people have become familiar with the basic functionality and interfaces of computers, but they are not able to manage any programming language. Therefore, they cannot develop new applications or modify current ones according to their needs.In order to address such challenges it is necessary a new paradigm, based on a multidisciplinary approach involving several types of expertise, such as software engineering, human-computer interaction, CSCW, which are now rather fragmented and with little interaction. The resulting methods and tools can provide results useful across many application domains, such as ERP, multi-device services (accessible through both mobile and stationary devices), and professional applications.Key words. tailorability, end user programming, flexibility, usability},
	language = {en},
	urldate = {2021-10-11},
	booktitle = {End {User} {Development}},
	publisher = {Springer Netherlands},
	author = {Lieberman, Henry and Paternò, Fabio and Klann, Markus and Wulf, Volker},
	editor = {Lieberman, Henry and Paternò, Fabio and Wulf, Volker},
	year = {2006},
	doi = {10.1007/1-4020-5386-X_1},
	keywords = {Agile Software Development, Computer Support Cooperative Work, Human Centric Computing, Software Cost Estimation, Software Professional},
	pages = {1--8},
}

@article{ko_state_2011,
	title = {The state of the art in end-user software engineering},
	volume = {43},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/1922649.1922658},
	doi = {10.1145/1922649.1922658},
	abstract = {Most programs today are written not by professional software developers, but by people with expertise in other domains working towards goals for which they need computational support. For example, a teacher might write a grading spreadsheet to save time grading, or an interaction designer might use an interface builder to test some user interface design ideas. Although these end-user programmers may not have the same goals as professional developers, they do face many of the same software engineering challenges, including understanding their requirements, as well as making decisions about design, reuse, integration, testing, and debugging. This article summarizes and classifies research on these activities, defining the area of End-User Software Engineering (EUSE) and related terminology. The article then discusses empirical research about end-user software engineering activities and the technologies designed to support them. The article also addresses several crosscutting issues in the design of EUSE tools, including the roles of risk, reward, and domain complexity, and self-efficacy in the design of EUSE tools and the potential of educating users about software engineering principles.},
	number = {3},
	urldate = {2021-10-11},
	journal = {ACM Computing Surveys},
	author = {Ko, Amy J. and Abraham, Robin and Beckwith, Laura and Blackwell, Alan and Burnett, Margaret and Erwig, Martin and Scaffidi, Chris and Lawrance, Joseph and Lieberman, Henry and Myers, Brad and Rosson, Mary Beth and Rothermel, Gregg and Shaw, Mary and Wiedenbeck, Susan},
	month = apr,
	year = {2011},
	keywords = {End-user software engineering, end-user development, end-user programming, human-computer interaction, visual programming},
	pages = {21:1--21:44},
}

@misc{IETF2014COAP,
	title = {{RFC} 7252: {The} constrained application protocol ({CoAP})},
	url = {https://datatracker.ietf.org/doc/html/rfc7252},
	author = {{Shelby} and {Z.} and Hartke, K. and Bormann, C.},
	month = jun,
	year = {2014},
}

@book{white2011making,
	title = {Making embedded systems: {Design} patterns for great software},
	publisher = {O'Reilly Media, Inc.},
	author = {White, Elecia},
	year = {2011},
}

@inproceedings{tomlein_chariot_2017,
	address = {New York, NY, USA},
	series = {{IoT} '17},
	title = {{CharIoT}: an end-user programming environment for the {IoT}},
	isbn = {978-1-4503-5318-2},
	shorttitle = {{CharIoT}},
	url = {https://doi.org/10.1145/3131542.3140261},
	doi = {10.1145/3131542.3140261},
	abstract = {Despite the breadth of related work, enabling end-users of varying technical ability to leverage sensor data to control their Internet of Things (IoT)-enabled installations remains a challenge. This work proposes a unified interface that provides three building blocks to support the end-user configuration of IoT environments: capturing higher-level events in the installation through virtual sensors, construction of automation rules with a visual overview of the current configuration and support for sharing configuration between end-users using a recommendation mechanism.},
	urldate = {2021-10-09},
	booktitle = {Proceedings of the {Seventh} {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Tomlein, Matúš and Boovaraghavan, Sudershan and Agarwal, Yuvraj and Dey, Anind K.},
	month = oct,
	year = {2017},
	keywords = {end-user programming, visual programming},
	pages = {1--2},
}

@article{lyytinen2002ubiquitous,
	title = {Ubiquitous computing},
	volume = {45},
	number = {12},
	journal = {Communications of the ACM},
	author = {Lyytinen, Kalle and Yoo, Youngjin},
	year = {2002},
	note = {Publisher: Citeseer},
	pages = {63--96},
}

@book{mitchell1996city,
	title = {City of bits: space, place, and the infobahn},
	publisher = {MIT press},
	author = {Mitchell, William John},
	year = {1996},
}

@book{Brooks1978MythicalManMonth,
	address = {USA},
	edition = {1st},
	title = {The mythical man-month: {Essays} on software engineering},
	isbn = {0-201-00650-2},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Brooks, Frederick P.},
	year = {1978},
}

@inproceedings{zhang_iot_2014,
	title = {{IoT} {Security}: {Ongoing} {Challenges} and {Research} {Opportunities}},
	shorttitle = {{IoT} {Security}},
	doi = {10.1109/SOCA.2014.58},
	abstract = {The Internet of Things (IoT) opens opportunities for wearable devices, home appliances, and software to share and communicate information on the Internet. Given that the shared data contains a large amount of private information, preserving information security on the shared data is an important issue that cannot be neglected. In this paper, we begin with general information security background of IoT and continue on with information security related challenges that IoT will encountered. Finally, we will also point out research directions that could be the future work for the solutions to the security challenges that IoT encounters.},
	booktitle = {2014 {IEEE} 7th {International} {Conference} on {Service}-{Oriented} {Computing} and {Applications}},
	author = {Zhang, Zhi-Kai and Cho, Michael Cheng Yi and Wang, Chia-Wei and Hsu, Chia-Wei and Chen, Chong-Kuan and Shieh, Shiuhpyng},
	month = nov,
	year = {2014},
	note = {ISSN: 2163-2871},
	keywords = {Androids, Cryptography, Humanoid robots, Internet of Things, Malware, Software, authenticity, identification, information security, malware, naming},
	pages = {230--234},
}

@inproceedings{berger_using_2018,
	address = {New York, NY, USA},
	series = {{SEsCPS} '18},
	title = {On using blockchains for safety-critical systems},
	isbn = {978-1-4503-5728-9},
	url = {https://doi.org/10.1145/3196478.3196480},
	doi = {10.1145/3196478.3196480},
	abstract = {Today's industries in various domains are becoming more and more driven by software as innovator. They range from web applications powering our increasingly digitalized daily lives to deeply embedded systems driving complex and safety-critical cyber-physical systems (CPS) as in, for example, self-driving vehicles. Companies need to continuously rejuvenate their product portfolio for adopting new ideas to remain competitive. A recent idea that is permeating from its original application domain of financial use cases are blockchains, where researchers and companies try to apply key ideas behind them to other domains.},
	urldate = {2021-10-08},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Software} {Engineering} for {Smart} {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Berger, Christian and Penzenstadler, Birgit and Drögehorn, Olaf},
	month = may,
	year = {2018},
	pages = {30--36},
}

@article{kaku_using_2016,
	series = {The 11th {International} {Conference} on {Future} {Networks} and {Communications} ({FNC} 2016) / {The} 13th {International} {Conference} on {Mobile} {Systems} and {Pervasive} {Computing} ({MobiSPC} 2016) / {Affiliated} {Workshops}},
	title = {Using {Provenance} and {CoAP} to track {Requests}/{Responses} in {IoT}},
	volume = {94},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S187705091631763X},
	doi = {10.1016/j.procs.2016.08.023},
	abstract = {Until recently, not much attention has been drawn to the need to provide documentary evidence for ensuring reliability, transparency and, most importantly, tracing the source of requests/responses in the Internet of Things. The knowledge of provenance is considered as a key component in establishing the above-mentioned issues. Most research, to a large extent, focus on requesting data, which is based on user inference and decision making, by utilising provenance information. However, little or nothing has been done regarding requests and responses and, most importantly, from the machine perspective. Consequently, this paper proposes a light-weight prototype system for tracing the source of requests/responses using provenance information over CoAP in the Internet of Things. We also provide performance evaluation of the prototypic system using metrics such as response time (ms) and throughput (KB/s). Finally, findings from our experiment are presented and discussed.},
	language = {en},
	urldate = {2021-10-08},
	journal = {Procedia Computer Science},
	author = {Kaku, Emmanuel and Lomotey, Richard . K. and Deters, Ralph},
	month = jan,
	year = {2016},
	keywords = {CoAP, IoT, Meta-data, Provenance, REST, URI},
	pages = {144--151},
}

@misc{noauthor_opentelemetry_nodate,
	title = {{OpenTelemetry} {\textbar} {Core} {Concepts}},
	url = {https://opentelemetry.lightstep.com/core-concepts/context-propagation/},
	abstract = {OpenTelemetry makes robust, portable telemetry a built-in feature of cloud-native software, providing a single set of APIs, libraries, agents, and collector services to capture distributed traces and metrics from your application.},
	language = {en},
	urldate = {2021-10-08},
}

@article{incki_novel_2018,
	title = {A {Novel} {Runtime} {Verification} {Solution} for {IoT} {Systems}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2813887},
	abstract = {Internet of Things (IoT) systems promise a seamless connected world with machines integrating their services without human intervention. It's highly probable that the entities participating in such autonomous machine to machine interactions are to be provided by different manufactures. Thus, integrating such heterogeneous devices from many providers complicates design and verification of IoT systems at an unprecedented scale. In this paper, we propose a novel runtime verification approach for IoT systems. The contributions of our proposed solution include: exploiting the interactions in message sequence charts (MSC) to specify message exchanges of constrained application protocol-based IoT systems in terms of events, a novel event calculus for formally describing IoT system constraints specified by means of MSCs, and an event processing algebra that uses complex-event processing techniques for detecting failures in the system by monitoring the runtime event occurrences with respect to the system constraints defined by event calculus. We further demonstrate the viability of proposed solution with case studies.},
	journal = {IEEE Access},
	author = {Incki, Koray and Ari, Ismail},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Calculus, Internet of Things, Mediation, Monitoring, Protocols, Runtime, Servers, Testing, complex-event processing, event calculus, message sequence charts, runtime verification},
	pages = {13501--13512},
}

@inproceedings{asadollah_runtime_2018,
	title = {A {Runtime} {Verification} {Tool} for {Detecting} {Concurrency} {Bugs} in {FreeRTOS} {Embedded} {Software}},
	doi = {10.1109/ISPDC2018.2018.00032},
	abstract = {This article presents a runtime verification tool for embedded software executing under the open source real-time operating system FreeRTOS. The tool detects and diagnoses concurrency bugs such as deadlock, starvation, and suspension based-locking. The tool finds concurrency bugs at runtime without debugging and tracing the source code. The tool uses the Tracealyzer tool for logging relevant events. Analysing the logs, our tool can detect the concurrency bugs by applying algorithms for diagnosing each concurrency bug type individually. In this paper, we present the implementation of the tool, as well as its functional architecture, together with illustration of its use. The tool can be used during program testing to gain interesting information about embedded software executions. We present initial results of running the tool on some classical bug examples running on an AVR 32-bit board SAM4S.},
	booktitle = {2018 17th {International} {Symposium} on {Parallel} and {Distributed} {Computing} ({ISPDC})},
	author = {Asadollah, Sara Abbaspour and Sundmark, Daniel and Eldh, Sigrid and Hansson, Hans},
	month = jun,
	year = {2018},
	keywords = {Bug Detector, Computer bugs, Concurrency Bugs, Concurrent computing, Embedded Software, Embedded software, FreeRTOS, Monitoring, Runtime, Runtime Verification Tool, Task analysis, Tools},
	pages = {172--179},
}

@inproceedings{incki_runtime_2017,
	title = {Runtime verification of {IoT} systems using {Complex} {Event} {Processing}},
	doi = {10.1109/ICNSC.2017.8000163},
	abstract = {Internet of Things (IoT) is a new computing paradigm that is proliferated by wide adoption of application level protocols such as MQTT and CoAP, each of which defines different styles of sequential interaction of events. Even though there is a considerable effort in the literature for verification of such complex and distributed systems, a practical solution for IoT systems that supports runtime system verification is still missing. In this paper, we present a runtime monitoring approach for IoT systems that exploits event relations expressed in terms of sequential interaction messaging model of Constrained Application Protocol (CoAP). We propose the use of Complex-Event Processing (CEP) to detect failures at runtime by exploiting complex event patterns defined via predetermined event algebra. We further present a simple case scenario to demonstrate the applicability of the approach on Wireless Token Ring Protocol execution.},
	booktitle = {2017 {IEEE} 14th {International} {Conference} on {Networking}, {Sensing} and {Control} ({ICNSC})},
	author = {İnçki, Koray and Arı, İsmail and Sözer, Hasan},
	month = may,
	year = {2017},
	keywords = {CoAP, Correlation, Internet of things, Tools, complex-event processing, event algebra, runtime monitoring, verification},
	pages = {625--630},
}

@techreport{qiu_fault_2003,
	title = {Fault {Detection}, {Isolation}, and {Diagnosis} in {Multihop} {Wireless} {Networks}},
	abstract = {Network management in multihop wireless networks is key to efficient and reliable network operation. In this paper, we focus on a part of the general network management problem, namely fault detection, isolation, and diagnosis. We propose a system that employs online trace-driven simulation as a diagnostic tool for detecting faults and performing root cause analysis. We apply our system to diagnose faults such as packet dropping, link congestion, MAC misbehavior, and external noise, and show that it yields reasonably accurate results. In addition, we show that our system can be used to evaluate alternative network and node configurations to improve performance of on-going long-lived flows. Moreover, our technique is general enough to be applied in other wireless and wireline network management system.},
	author = {Qiu, Lili and Bahl, Paramvir and Rao, Ananth and Zhou, Lidong},
	year = {2003},
}

@article{ayers_traceback_nodate,
	title = {{TraceBack}: {First} {Fault} {Diagnosis} by {Reconstruction} of {Distributed} {Control} {Flow}},
	abstract = {Faults that occur in production systems are the most important faults to fix, but most production systems lack the debugging facilities present in development environments. TraceBack provides debugging information for production systems by providing execution history data about program problems (such as crashes, hangs, and exceptions). TraceBack supports features commonly found in production environments such as multiple threads, dynamically loaded modules, multiple source languages (e.g., Java applications running with JNI modules written in C++), and distributed execution across multiple computers. TraceBack supports first fault diagnosis—discovering what went wrong the first time a fault is encountered. The user can see how the program reached the fault state without having to re-run the computation; in effect enabling a limited form of a debugger in production code.},
	language = {en},
	author = {Ayers, Andrew and Schooler, Richard and Metcalf, Chris and Agarwal, Anant and Rhee, Junghwan and Witchel, Emmett},
	pages = {12},
}

@article{conger_hundreds_2021,
	chapter = {Technology},
	title = {Hundreds of {Google} {Employees} {Unionize}, {Culminating} {Years} of {Activism}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2021/01/04/technology/google-employees-union.html},
	abstract = {The creation of the union, a rarity in Silicon Valley, follows years of increasing outspokenness by Google workers. Executives have struggled to handle the change.},
	language = {en-US},
	urldate = {2021-10-07},
	journal = {The New York Times},
	author = {Conger, Kate},
	month = jan,
	year = {2021},
	keywords = {Alphabet Inc, Alphabet Workers Union, Artificial Intelligence, Collective Bargaining, Communications Workers of America, Computers and the Internet, Corporate Social Responsibility, Engineering and Engineers, Google Inc, Labor and Jobs, Organized Labor, Pichai, Sundar},
}

@article{kruchten_licensing_2008,
	title = {Licensing software engineers?},
	volume = {25},
	number = {6},
	journal = {IEEE Software},
	author = {Kruchten, Philippe},
	year = {2008},
	note = {Publisher: IEEE},
	pages = {35--37},
}

@incollection{bartocci_introduction_2018,
	address = {Cham},
	title = {Introduction to {Runtime} {Verification}},
	volume = {10457},
	isbn = {978-3-319-75631-8 978-3-319-75632-5},
	url = {http://link.springer.com/10.1007/978-3-319-75632-5_1},
	abstract = {The aim of this chapter is to act as a primer for those wanting to learn about Runtime Veriﬁcation (RV). We start by providing an overview of the main speciﬁcation languages used for RV. We then introduce the standard terminology necessary to describe the monitoring problem, covering the pragmatic issues of monitoring and instrumentation, and discussing extensively the monitorability problem.},
	language = {en},
	urldate = {2021-10-07},
	booktitle = {Lectures on {Runtime} {Verification}},
	publisher = {Springer International Publishing},
	author = {Bartocci, Ezio and Falcone, Yliès and Francalanza, Adrian and Reger, Giles},
	editor = {Bartocci, Ezio and Falcone, Yliès},
	year = {2018},
	doi = {10.1007/978-3-319-75632-5_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--33},
}

@inproceedings{reiss_iot_2019,
	title = {{IoT} {End} {User} {Programming} {Models}},
	doi = {10.1109/SERP4IoT.2019.00008},
	abstract = {The advent of smart devices and sensors (the Internet of Things or IoT) will create increasing demands for the automation of devices based on sensor, time, and other inputs. This is essentially a programming task with all the problems and difficulties that programming entails, for example, modularity, feature interaction, debugging, and understanding. Moreover, much of the programming for smart devices is going to be done not by professional programmers but by end users, often end users without any programming experience or computational literacy. Our research is aimed at exploring the programming space and the associated issues using a case study of a smart sign that can be controlled using a variety of sensors. We have developed a general system for programming smart devices and, in this paper, explore a variety of different user interfaces for programming this system for our smart sign.},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} {Research} {Practices} for the {Internet} of {Things} ({SERP4IoT})},
	author = {Reiss, Steven P.},
	month = may,
	year = {2019},
	keywords = {Debugging, Internet of Things, Internet of thingws, Programming profession, Sensors, Smart devices, Software, debugging, end-user programming, program understanding},
	pages = {1--8},
}

@inproceedings{heimdahl_safety_2007,
	address = {Minneapolis, MN, USA},
	title = {Safety and {Software} {Intensive} {Systems}: {Challenges} {Old} and {New}},
	isbn = {978-0-7695-2829-8},
	shorttitle = {Safety and {Software} {Intensive} {Systems}},
	url = {http://ieeexplore.ieee.org/document/4221617/},
	doi = {10.1109/FOSE.2007.18},
	abstract = {There is an increased use of software in safety-critical systems; a trend that is likely to continue in the future. Although traditional system safety techniques are applicable to software intensive systems, there are new challenges emerging. In this report we will address four issues we believe will pose challenges in the future.},
	language = {en},
	urldate = {2021-10-06},
	booktitle = {Future of {Software} {Engineering} ({FOSE} '07)},
	publisher = {IEEE},
	author = {Heimdahl, Mats P.E.},
	month = may,
	year = {2007},
	pages = {137--152},
}

@article{leveson1993investigation,
	title = {An investigation of the {Therac}-25 accidents},
	volume = {26},
	number = {7},
	journal = {Computer},
	author = {Leveson, Nancy G and Turner, Clark S},
	year = {1993},
	note = {Publisher: IEEE},
	pages = {18--41},
}

@incollection{romanovsky_devils_2019,
	address = {Cham},
	title = {Devil’s in the {Detail}: {Through}-{Life} {Safety} and {Security} {Co}-assurance {Using} {SSAF}},
	volume = {11698},
	isbn = {978-3-030-26600-4 978-3-030-26601-1},
	shorttitle = {Devil’s in the {Detail}},
	url = {http://link.springer.com/10.1007/978-3-030-26601-1_21},
	abstract = {Regulatory bodies, industry and academia present a plethora of approaches for risk analysis and engineering for safety and security. However, few standards and approaches discuss the management of both safety and security risks. Fewer yet provide detail on how the two attributes interact within a given system. In this paper, the SafetySecurity Assurance Framework (SSAF) is presented as a candidate solution to many of the extant challenges of attribute co-assurance. It is a holistic approach, based on the concept of independent co-assurance, that considers both the technical risk impact and the socio-technical impact on assurance. The Framework’s Technical Risk Model (TRM) is applied and evaluated against a case study of an insulin pump. It is argued that SSAF TRM is not only a plausible and practical approach, but also more eﬀective for co-assurance than many existing approaches alone.},
	language = {en},
	urldate = {2021-10-05},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Johnson, Nikita and Kelly, Tim},
	editor = {Romanovsky, Alexander and Troubitsyna, Elena and Bitsch, Friedemann},
	year = {2019},
	doi = {10.1007/978-3-030-26601-1_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {299--314},
}

@inproceedings{asadollah_runtime_2018,
	title = {A {Runtime} {Verification} {Tool} for {Detecting} {Concurrency} {Bugs} in {FreeRTOS} {Embedded} {Software}},
	doi = {10.1109/ISPDC2018.2018.00032},
	abstract = {This article presents a runtime verification tool for embedded software executing under the open source real-time operating system FreeRTOS. The tool detects and diagnoses concurrency bugs such as deadlock, starvation, and suspension based-locking. The tool finds concurrency bugs at runtime without debugging and tracing the source code. The tool uses the Tracealyzer tool for logging relevant events. Analysing the logs, our tool can detect the concurrency bugs by applying algorithms for diagnosing each concurrency bug type individually. In this paper, we present the implementation of the tool, as well as its functional architecture, together with illustration of its use. The tool can be used during program testing to gain interesting information about embedded software executions. We present initial results of running the tool on some classical bug examples running on an AVR 32-bit board SAM4S.},
	booktitle = {2018 17th {International} {Symposium} on {Parallel} and {Distributed} {Computing} ({ISPDC})},
	author = {Asadollah, Sara Abbaspour and Sundmark, Daniel and Eldh, Sigrid and Hansson, Hans},
	month = jun,
	year = {2018},
	keywords = {Bug Detector, Computer bugs, Concurrency Bugs, Concurrent computing, Embedded Software, Embedded software, FreeRTOS, Monitoring, Runtime, Runtime Verification Tool, Task analysis, Tools},
	pages = {172--179},
}

@inproceedings{kraft_trace_2010,
	title = {Trace {Recording} for {Embedded} {Systems}: {Lessons} {Learned} from {Five} {Industrial} {Projects}},
	isbn = {978-3-642-16611-2},
	shorttitle = {Trace {Recording} for {Embedded} {Systems}},
	doi = {10.1007/978-3-642-16612-9_24},
	abstract = {This paper presents experiences from five industry collaboration projects performed between 2004 – 2009 where solutions for
embedded systems trace recording have been developed and evaluated; in four cases for specific industrial systems and in the
last case as a generic solution for a commercial real-time operating system, in collaboration with the RTOS company. The experiences
includes technical solutions regarding efficient instrumentation and logging, technology transfer issues and evaluation results
regarding CPU and RAM overhead. A brief overview of the Tracealyzer tool is also presented, a result of the first project (2004) which still is used by ABB Robotics and now in commercialization.},
	author = {Kraft, Johan and Wall, Anders and Kienle, Holger},
	month = nov,
	year = {2010},
	pages = {315--329},
}

@inproceedings{emt_department________________________bonn-rhein-sieg_university_of_applied_sciences_sankt-augustin________________________germany_use_2020,
	title = {{THE} {USE} {OF} {PERCEPIO} {TRACEALYZER} {FOR} {THE} {DEVELOPMENT} {OF} {FREERTOS}-{BASED} {APPLICATIONS}},
	url = {https://mcfpga.nure.ua/conf/2020-mcfpga/10-35598-mcfpga-2020-008},
	doi = {10.35598/mcfpga.2020.008},
	abstract = {This paper discusses some problems of development and testing of FreeRTOS-based application. The use of Tracealyzer software tool is proposed to make this process more convenient. The benefits of such usage have been shown on typical issues that can be met in development and debug phase.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {{MC}\&{FPGA}-2020},
	author = {{EMT Department
                        Bonn-Rhein-Sieg University of Applied Sciences Sankt-Augustin,
                        Germany} and Khomenko, Maksym and Velihorskyi, Oleksandr and {Biomedical
                        radioelectronic apparatus and system department Chernihiv National
                        University of Technology Chernihiv, Ukraine}},
	year = {2020},
	pages = {26--29},
}

@inproceedings{yang_medley_2019,
	address = {Davis CA USA},
	title = {Medley: {A} {Novel} {Distributed} {Failure} {Detector} for {IoT} {Networks}},
	isbn = {978-1-4503-7009-7},
	shorttitle = {Medley},
	url = {https://dl.acm.org/doi/10.1145/3361525.3361556},
	doi = {10.1145/3361525.3361556},
	abstract = {Efficient and correct operation of an IoT network requires the presence of a failure detector and membership protocol amongst the IoT nodes. This paper presents a new failure detector for IoT settings where nodes are connected via a wireless ad-hoc network. This failure detector, which we name Medley, is fully decentralized, allows IoT nodes to maintain a local membership list of other alive nodes, detects failures quickly (and updates the membership list), and incurs low communication overhead in the underlying ad-hoc network. In order to minimize detection time and communication, we adapt a failure detector originally proposed for datacenters (SWIM), for the IoT environment. In Medley each node picks a medley of ping targets in a randomized and skewed manner, preferring nearer nodes. Via analysis and NS-3 simulation we show the right mix of pinging probabilities that simultaneously optimize detection time and communication traffic. We have also implemented Medley for Raspberry Pis, and present deployment results.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 20th {International} {Middleware} {Conference}},
	publisher = {ACM},
	author = {Yang, Rui and Zhu, Shichu and Li, Yifei and Gupta, Indranil},
	month = dec,
	year = {2019},
	pages = {319--331},
}

@misc{kingatua_iot_2019,
	title = {{IoT} {System} {Tests} :: {Checking} for {Failure}},
	shorttitle = {{IoT} {System} {Tests}},
	url = {https://medium.com/supplyframe-hardware/iot-system-tests-checking-for-failure-c146d2ebb8ef},
	abstract = {Internet of Things sensors use a diverse range of technologies to collect information, communicate with each other, and transmit data to…},
	language = {en},
	urldate = {2021-10-04},
	journal = {Supplyframe},
	author = {Kingatua, Amos},
	month = may,
	year = {2019},
}

@inproceedings{huang_software_2014,
	title = {Software {Failure} {Detection} {Using} {Pattern}'s {Position} {Distribution}},
	doi = {10.1109/ICMTMA.2014.145},
	abstract = {Pattern-based software failure detection is an important topic of research in recent years. In this method, a set of patterns from program execution traces are extracted, and represented as features, while their occurrence frequencies are treated as the corresponding feature values. But this conventional method has its limitation due to neglect the pattern's position information, which is important for the classification of program traces. Patterns occurs in the different positions of the trace are likely to represent different meanings. In this paper, we present a novel approach for using pattern's position distribution as features to detect software failure. In this method, we divide sequence into several sections and then compute pattern's distribution in each section, the distribution of all patterns are then used as features to train a classifier. This method outperforms conventional frequency based method due to effectively identify software failures occur through mis-used software patterns. The comparative experiments in both artificial and real datasets show the effectiveness of this method.},
	booktitle = {2014 {Sixth} {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation}},
	author = {Huang, He and Chen, Ziniu and Chen, Peng and Sun, Yan and Xie, Qianlong and Ma, Shuai and Chen, Hongjun},
	month = jan,
	year = {2014},
	note = {ISSN: 2157-1481},
	keywords = {Automation, Classification, Feature, Mechatronics, Pattern, Position Distribution, Software Failure Detection},
	pages = {593--597},
}

@article{hangal_tracking_nodate,
	title = {Tracking {Down} {Software} {Bugs} {Using} {Automatic} {Anomaly} {Detection}},
	abstract = {This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program’s results.},
	language = {en},
	author = {Hangal, Sudheendra and Lam, Monica S},
	pages = {11},
}

@misc{noauthor_toward_nodate,
	title = {Toward automatic detection of software failures {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/707619},
	urldate = {2021-10-04},
}

@inproceedings{zhang_empirical_2018,
	address = {Amsterdam Netherlands},
	title = {An empirical study on {TensorFlow} program bugs},
	isbn = {978-1-4503-5699-2},
	url = {https://dl.acm.org/doi/10.1145/3213846.3213866},
	doi = {10.1145/3213846.3213866},
	abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research e orts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To ll this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These ndings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 27th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
	month = jul,
	year = {2018},
	pages = {129--140},
}

@article{islam_towards_nodate,
	title = {Towards understanding the challenges faced by machine learning software developers and enabling automated solutions},
	language = {en},
	author = {Islam, Johirul},
	pages = {162},
}

@inproceedings{vahabzadeh_empirical_2015,
	address = {Bremen, Germany},
	title = {An empirical study of bugs in test code},
	isbn = {978-1-4673-7532-0},
	url = {http://ieeexplore.ieee.org/document/7332456/},
	doi = {10.1109/ICSM.2015.7332456},
	abstract = {Testing aims at detecting (regression) bugs in production code. However, testing code is just as likely to contain bugs as the code it tests. Buggy test cases can silently miss bugs in the production code or loudly ring false alarms when the production code is correct. We present the ﬁrst empirical study of bugs in test code to characterize their prevalence and root cause categories. We mine the bug repositories and version control systems of 211 Apache Software Foundation (ASF) projects and ﬁnd 5,556 test-related bug reports. We (1) compare properties of test bugs with production bugs, such as active time and ﬁxing effort needed, and (2) qualitatively study 443 randomly sampled test bug reports in detail and categorize them based on their impact and root causes. Our results show that (1) around half of all the projects had bugs in their test code; (2) the majority of test bugs are false alarms, i.e., test fails while the production code is correct, while a minority of these bugs result in silent horrors, i.e., test passes while the production code is incorrect; (3) incorrect and missing assertions are the dominant root cause of silent horror bugs; (4) semantic (25\%), ﬂaky (21\%), environmentrelated (18\%) bugs are the dominant root cause categories of false alarms; (5) the majority of false alarm bugs happen in the exercise portion of the tests, and (6) developers contribute more actively to ﬁxing test bugs and test bugs are ﬁxed sooner compared to production bugs. In addition, we evaluate whether existing bug detection tools can detect bugs in test code.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Vahabzadeh, Arash and Fard, Amin Milani and Mesbah, Ali},
	month = sep,
	year = {2015},
	pages = {101--110},
}

@inproceedings{islam_repairing_2020,
	address = {Seoul South Korea},
	title = {Repairing deep neural networks: fix patterns and challenges},
	isbn = {978-1-4503-7121-6},
	shorttitle = {Repairing deep neural networks},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380378},
	doi = {10.1145/3377811.3380378},
	abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
	month = jun,
	year = {2020},
	pages = {1135--1146},
}

@inproceedings{islam_comprehensive_2019,
	address = {Tallinn Estonia},
	title = {A comprehensive study on deep learning bug characteristics},
	isbn = {978-1-4503-5572-8},
	url = {https://dl.acm.org/doi/10.1145/3338906.3338955},
	doi = {10.1145/3338906.3338955},
	abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48\% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43\% of the times. We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
	month = aug,
	year = {2019},
	pages = {510--520},
}

@article{humbatova_taxonomy_2019,
	title = {Taxonomy of {Real} {Faults} in {Deep} {Learning} {Systems}},
	url = {http://arxiv.org/abs/1910.11015},
	abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Over ow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our nal taxonomy was validated with a survey involving an additional set of 21 developers, con rming that almost all fault categories (13/15) were experienced by at least 50\% of the survey participants.},
	language = {en},
	urldate = {2021-10-04},
	journal = {arXiv:1910.11015 [cs]},
	author = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.11015},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{saha_empirical_2014,
	address = {Antwerp, Belgium},
	title = {An empirical study of long lived bugs},
	isbn = {978-1-4799-3752-3},
	url = {http://ieeexplore.ieee.org/document/6747164/},
	doi = {10.1109/CSMR-WCRE.2014.6747164},
	abstract = {Bug ﬁxing is a crucial part of software development and maintenance. A large number of bugs often indicate poor software quality since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user’s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug ﬁxing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs. In this paper, we analyzed long lived bugs from ﬁve different perspectives: their proportion, severity, assignment, reasons, as well as the nature of ﬁxes. Our study on four open-source projects shows that there are a considerable number of long lived bugs in each system and over 90\% of them adversely affect the user’s experience. The reasons of these long lived bugs are diverse including long assignment time, not understanding their importance in advance etc. However, many bug-ﬁxes were delayed without any speciﬁc reasons. Our analysis of bug ﬁxing changes further shows that many long lived bugs can be ﬁxed quickly through careful prioritization. We believe our results will help both developers and researchers to better understand factors behind delays, improve the overall bug ﬁxing process, and investigate analytical approaches for prioritizing bugs based on bug severity as well as expected bug ﬁxing effort. Index Terms—Bug tracking system, bug triaging, bug survival time I. INTRODUCTION Software development and maintenance is a complex process. Although developers and testers try their best to make their software error free, in practice software ships with bugs. The number of bugs in software is a signiﬁcant indicator of software quality since bugs can adversely affect users experience directly. Therefore, developers are generally very active in ﬁnding and removing bugs.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
	publisher = {IEEE},
	author = {Saha, Ripon K. and Khurshid, Sarfraz and Perry, Dewayne E.},
	month = feb,
	year = {2014},
	pages = {144--153},
}

@inproceedings{wan_bug_2017,
	address = {Buenos Aires, Argentina},
	title = {Bug {Characteristics} in {Blockchain} {Systems}: {A} {Large}-{Scale} {Empirical} {Study}},
	isbn = {978-1-5386-1544-7},
	shorttitle = {Bug {Characteristics} in {Blockchain} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7962390/},
	doi = {10.1109/MSR.2017.59},
	abstract = {Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug ﬁxing time. The ﬁndings include: (1) semantic bugs are the dominant runtime bug category; (2) frequency distributions of bug types show similar trends across different projects and programming languages; (3) security bugs take the longest median time to be ﬁxed; (4) 35.71\% performance bugs are ﬁxed in more than one year; performance bugs take the longest average time to be ﬁxed.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Wan, Zhiyuan and Lo, David and Xia, Xin and Cai, Liang},
	month = may,
	year = {2017},
	pages = {413--424},
}

@inproceedings{zhang_empirical_2020,
	address = {Seoul South Korea},
	title = {An empirical study on program failures of deep learning jobs},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380362},
	doi = {10.1145/3377811.3380362},
	abstract = {Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao},
	month = jun,
	year = {2020},
	pages = {1159--1170},
}

@inproceedings{thung_empirical_2012,
	address = {Dallas, TX, USA},
	title = {An {Empirical} {Study} of {Bugs} in {Machine} {Learning} {Systems}},
	isbn = {978-1-4673-4638-2 978-0-7695-4888-3},
	url = {http://ieeexplore.ieee.org/document/6405375/},
	doi = {10.1109/ISSRE.2012.22},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2012 {IEEE} 23rd {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE},
	author = {Thung, Ferdian and Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	month = nov,
	year = {2012},
	pages = {271--280},
}

@inproceedings{eghbali_no_2020,
	address = {Virtual Event Australia},
	title = {No strings attached: an empirical study of string-related software bugs},
	isbn = {978-1-4503-6768-4},
	shorttitle = {No strings attached},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416576},
	doi = {10.1145/3324884.3416576},
	abstract = {Strings play many roles in programming because they often contain complex and semantically rich information. For example, programmers use strings to filter inputs via regular expression matching, to express the names of program elements accessed through some form of reflection, to embed code written in another formal language, and to assemble textual output produced by a program. The omnipresence of strings leads to a wide range of mistakes that developers may make, yet little is currently known about these mistakes. The lack of knowledge about string-related bugs leads to developers repeating the same mistakes again and again, and to poor support for finding and fixing such bugs. This paper presents the first empirical study of the root causes, consequences, and other properties of string-related bugs. We systematically study 204 string-related bugs in a diverse set of projects written in JavaScript, a language where strings play a particularly important role. Our findings include (i) that many string-related mistakes are caused by a recurring set of root cause patterns, such as incorrect string literals and regular expressions, (ii) that string-related bugs have a diverse set of consequences, including incorrect output or silent omission of expected behavior, (iii) that fixing string-related bugs often requires changing just a single line, with many of the required repair ingredients available in the surrounding code, (iv) that stringrelated bugs occur across all parts of applications, including the core components, and (v) that almost none of these bugs are detected by existing static analyzers. Our findings not only show the importance and prevalence of string-related bugs, but they help developers to avoid common mistakes and tool builders to tackle the challenge of finding and fixing string-related bugs.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Eghbali, Aryaz and Pradel, Michael},
	month = dec,
	year = {2020},
	pages = {956--967},
}

@article{miller_relevance_2020,
	title = {The {Relevance} of {Classic} {Fuzz} {Testing}: {Have} {We} {Solved} {This} {One}?},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {The {Relevance} of {Classic} {Fuzz} {Testing}},
	url = {http://arxiv.org/abs/2008.06537},
	doi = {10.1109/TSE.2020.3047766},
	abstract = {As fuzz testing has passed its 30th anniversary, and in the face of the incredible progress in fuzz testing techniques and tools, the question arises if the classic, basic fuzz technique is still useful and applicable? In that tradition, we have updated the basic fuzz tools and testing scripts and applied them to a large collection of Unix utilities on Linux, FreeBSD, and MacOS. As before, our failure criteria was whether the program crashed or hung. We found that 9 crash or hang out of 74 utilities on Linux, 15 out of 78 utilities on FreeBSD, and 12 out of 76 utilities on MacOS. A total of 24 different utilities failed across the three platforms. We note that these failure rates are somewhat higher than our in previous 1995, 2000, and 2006 studies of the reliability of command line utilities. In the basic fuzz tradition, we debugged each failed utility and categorized the causes the failures. Classic categories of failures, such as pointer and array errors and not checking return codes, were still broadly present in the current results. In addition, we found a couple of new categories of failures appearing. We present examples of these failures to illustrate the programming practices that allowed them to happen.},
	language = {en},
	urldate = {2021-10-04},
	journal = {IEEE Transactions on Software Engineering},
	author = {Miller, Barton P. and Zhang, Mengxiao and Heymann, Elisa R.},
	year = {2020},
	note = {arXiv: 2008.06537},
	keywords = {Computer Science - Software Engineering},
	pages = {1--1},
}

@inproceedings{menghi_trace-checking_2021,
	address = {Madrid, ES},
	title = {Trace-{Checking} {CPS} {Properties}: {Bridging} the {Cyber}-{Physical} {Gap}},
	isbn = {978-1-66540-296-5},
	shorttitle = {Trace-{Checking} {CPS} {Properties}},
	url = {https://ieeexplore.ieee.org/document/9402030/},
	doi = {10.1109/ICSE43902.2021.00082},
	abstract = {Cyber-physical systems combine software and physical components. Speciﬁcation-driven trace-checking tools for CPS usually provide users with a speciﬁcation language to express the requirements of interest, and an automatic procedure to check whether these requirements hold on the execution traces of a CPS. Although there exist several speciﬁcation languages for CPS, they are often not sufﬁciently expressive to allow the speciﬁcation of complex CPS properties related to the software and the physical components and their interactions.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Menghi, Claudio and Vigano, Enrico and Bianculli, Domenico and Briand, Lionel C.},
	month = may,
	year = {2021},
	pages = {847--859},
}

@inproceedings{dorri_blockchain_2017,
	title = {Blockchain for {IoT} security and privacy: {The} case study of a smart home},
	shorttitle = {Blockchain for {IoT} security and privacy},
	doi = {10.1109/PERCOMW.2017.7917634},
	abstract = {Internet of Things (IoT) security and privacy remain a major challenge, mainly due to the massive scale and distributed nature of IoT networks. Blockchain-based approaches provide decentralized security and privacy, yet they involve significant energy, delay, and computational overhead that is not suitable for most resource-constrained IoT devices. In our previous work, we presented a lightweight instantiation of a BC particularly geared for use in IoT by eliminating the Proof of Work (POW) and the concept of coins. Our approach was exemplified in a smart home setting and consists of three main tiers namely: cloud storage, overlay, and smart home. In this paper we delve deeper and outline the various core components and functions of the smart home tier. Each smart home is equipped with an always online, high resource device, known as “miner” that is responsible for handling all communication within and external to the home. The miner also preserves a private and secure BC, used for controlling and auditing communications. We show that our proposed BC-based smart home framework is secure by thoroughly analysing its security with respect to the fundamental security goals of confidentiality, integrity, and availability. Finally, we present simulation results to highlight that the overheads (in terms of traffic, processing time and energy consumption) introduced by our approach are insignificant relative to its security and privacy gains.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} ({PerCom} {Workshops})},
	author = {Dorri, Ali and Kanhere, Salil S. and Jurdak, Raja and Gauravaram, Praveen},
	month = mar,
	year = {2017},
	keywords = {Cloud computing, Conferences, Internet of Things, Online banking, Privacy, Security, Smart homes},
	pages = {618--623},
}

@article{satija_blockene_nodate,
	title = {Blockene: {A} {High}-throughput {Blockchain} {Over} {Mobile} {Devices}},
	abstract = {We introduce Blockene, a blockchain that reduces resource usage at member nodes by orders of magnitude, requiring only a smartphone to participate in block validation and consensus. Despite being lightweight, Blockene provides a high throughput of transactions and scales to a large number of participants. Blockene consumes negligible battery and data in smartphones, enabling millions of users to participate in the blockchain without incentives, to secure transactions with their collective honesty. Blockene achieves these properties with a novel split-trust design based on delegating storage and gossip to untrusted nodes.},
	language = {en},
	author = {Satija, Sambhav and Mehra, Apurv and Singanamalla, Sudheesh and Grover, Karan and Sivathanu, Muthian and Chandran, Nishanth and Gupta, Divya and Lokam, Satya},
	pages = {17},
}

@article{hangal_tracking_nodate-1,
	title = {Tracking {Down} {Software} {Bugs} {Using} {Automatic} {Anomaly} {Detection}},
	abstract = {This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program’s results.},
	language = {en},
	author = {Hangal, Sudheendra and Lam, Monica S},
	pages = {11},
}

@misc{noauthor_osmotic_nodate,
	title = {Osmotic {Flow}: {Osmotic} {Computing} + {IoT} {Workflow} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7912282},
	urldate = {2021-10-03},
}

@article{pflanzner_taxonomy_2018,
	title = {A {Taxonomy} and {Survey} of {IoT} {Cloud} {Applications}},
	volume = {3},
	doi = {10.4108/eai.6-4-2018.154391},
	abstract = {Internet of Things (IoT) systems are realized by dynamic global network infrastructure with self-configuring capabilities, in which things can interact and communicate in the environment through the Internet by exchanging sensor data, and react autonomously to events generally without direct human intervention. Such systems can be utilized in many application areas, thus they may have very dierent properties. There is a growing number of cloud providers oering IoT-specific services, since cloud computing has the potential to satisfy IoT needs such as standardizing the custom data structures of the devices, processing and visualization tasks. IoT application developers do not only have to decide which cloud provider to use, but they also have to choose which combination of protocols and data structures best fits their application. As a result, it is necessary to know what properties these systems have and to learn to what extent cloud providers support IoT capabilities. In this paper, we address these issues and investigate 23 IoT cloud use cases and perform a detailed classification of them in a survey, and introduce a taxonomy of IoT application properties based on this survey. We also compare current cloud providers supporting IoT capabilities and gather requirements for IoT device simulation to support further research on IoT application development.},
	journal = {EAI Endorsed Transactions on Internet of Things},
	author = {Pflanzner, Tamas and Kertész, Attila},
	month = apr,
	year = {2018},
	pages = {154391},
}

@article{hamdan_edge-computing_2020,
	title = {Edge-{Computing} {Architectures} for {Internet} of {Things} {Applications}: {A} {Survey}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Edge-{Computing} {Architectures} for {Internet} of {Things} {Applications}},
	url = {https://www.mdpi.com/1424-8220/20/22/6441},
	doi = {10.3390/s20226441},
	abstract = {The rapid growth of the Internet of Things (IoT) applications and their interference with our daily life tasks have led to a large number of IoT devices and enormous sizes of IoT-generated data. The resources of IoT devices are limited; therefore, the processing and storing IoT data in these devices are inefficient. Traditional cloud-computing resources are used to partially handle some of the IoT resource-limitation issues; however, using the resources in cloud centers leads to other issues, such as latency in time-critical IoT applications. Therefore, edge-cloud-computing technology has recently evolved. This technology allows for data processing and storage at the edge of the network. This paper studies, in-depth, edge-computing architectures for IoT (ECAs-IoT), and then classifies them according to different factors such as data placement, orchestration services, security, and big data. Besides, the paper studies each architecture in depth and compares them according to various features. Additionally, ECAs-IoT is mapped according to two existing IoT layered models, which helps in identifying the capabilities, features, and gaps of every architecture. Moreover, the paper presents the most important limitations of existing ECAs-IoT and recommends solutions to them. Furthermore, this survey details the IoT applications in the edge-computing domain. Lastly, the paper recommends four different scenarios for using ECAs-IoT by IoT applications.},
	language = {en},
	number = {22},
	urldate = {2021-10-03},
	journal = {Sensors},
	author = {Hamdan, Salam and Ayyash, Moussa and Almajali, Sufyan},
	month = jan,
	year = {2020},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things, cloud computing, edge computing},
	pages = {6441},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	doi = {10.1109/JIOT.2016.2579198},
	abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Bandwidth, Cloud computing, Data privacy, Edge computing, Internet of Things (IoT), Internet of things, Mobile handsets, Smart homes, Time factors, smart home and city},
	pages = {637--646},
}

@inproceedings{villari_towards_2018,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Towards {Osmotic} {Computing}: {Looking} at {Basic} {Principles} and {Technologies}},
	isbn = {978-3-319-61566-0},
	shorttitle = {Towards {Osmotic} {Computing}},
	doi = {10.1007/978-3-319-61566-0_86},
	abstract = {Osmotic Computing is becoming the new paradigm in the area of Computing. This paper shows how it can represents the glue of recent topics including Cloud, Edge and Fog Computing, and Internet of Things (IoT). Osmotic Computing introduces elements allowing to treat computation, networking, storage, data transfer and management among Cloud and IoT devices in Edge computing layers in a more harmonized fashion. In particular, we discuss how it can enable an abstraction of services that could bring into a new Software Defined of Everything era.},
	language = {en},
	booktitle = {Complex, {Intelligent}, and {Software} {Intensive} {Systems}},
	publisher = {Springer International Publishing},
	author = {Villari, Massimo and Celesti, Antonio and Fazio, Maria},
	editor = {Barolli, Leonard and Terzo, Olivier},
	year = {2018},
	keywords = {Edge Computing, Future Internet Services, Interrupt Latency, Microservices (MS), Software Defined},
	pages = {906--915},
}

@article{villari_osmotic_2016,
	title = {Osmotic {Computing}: {A} {New} {Paradigm} for {Edge}/{Cloud} {Integration}},
	volume = {3},
	issn = {2325-6095},
	shorttitle = {Osmotic {Computing}},
	doi = {10.1109/MCC.2016.124},
	abstract = {Osmotic computing is a new paradigm to support the efficient execution of Internet of Things (IoT) services and applications at the network edge. This paradigm is founded on the need for a holistic distributed system abstraction enabling the deployment of lightweight microservices on resource-constrained IoT platforms at the network edge, coupled with more complex microservices running on large-scale datacenters. This paradigm is driven by the significant increase in resource capacity/capability at the network edge, along with support for data transfer protocols that enable such resources to interact more seamlessly with datacenter-based services. This installment of "Blue Skies" discusses osmotic computing features, challenges, and future directions.},
	number = {6},
	journal = {IEEE Cloud Computing},
	author = {Villari, Massimo and Fazio, Maria and Dustdar, Schahram and Rana, Omer and Ranjan, Rajiv},
	month = nov,
	year = {2016},
	note = {Conference Name: IEEE Cloud Computing},
	keywords = {Adaptation models, Cloud computing, Computational modeling, Containers, Edge computing, Internet of Things, Quality of service, cloud computing, edge cloud integration, edge computing},
	pages = {76--83},
}

@inproceedings{buzachis_towards_2018,
	title = {Towards {Osmotic} {Computing}: {Analyzing} {Overlay} {Network} {Solutions} to {Optimize} the {Deployment} of {Container}-{Based} {Microservices} in {Fog}, {Edge} and {IoT} {Environments}},
	shorttitle = {Towards {Osmotic} {Computing}},
	doi = {10.1109/CFEC.2018.8358729},
	abstract = {In recent years, the rapid growth of new Cloud technologies acted as an enabling factor for the adoption of microservices based architecture that leverages container virtualization in order to build modular and robust systems. As the number of containers running on hosts increases, it becomes essential to have tools to manage them in a simple, straightforward manner and with a high level of abstraction. Osmotic Computing is an emerging research field that studies the migration, deployment and optimization of microservices from the Cloud to Fog, Edge, and Internet of Things (IoT) environments. However, in order to achieve Osmotic Computing environments, connectivity issues have to be addressed. This paper investigates these connectivity issues leveraging different network overlays. In particular, we analyze the performance of four network overlays that are OVN, Calico, Weave, and Flannel. Our results give a concrete overview in terms of overhead and performances for each proposed overlay solution, helping us to understand which the best overlay solution is. Specifically, we deployed CoAP and FTP microservices which helped us to carry out these benchmarks and collect the results in terms of transfer times.},
	booktitle = {2018 {IEEE} 2nd {International} {Conference} on {Fog} and {Edge} {Computing} ({ICFEC})},
	author = {Buzachis, Alina and Galletta, Antonino and Carnevale, Lorenzo and Celesti, Antonio and Fazio, Maria and Villari, Massimo},
	month = may,
	year = {2018},
	keywords = {Cloud computing, Computer architecture, Containers, Ecosystems, Edge computing, Internet of Things},
	pages = {1--10},
}

@article{fernandez_enabling_2019,
	title = {Enabling the {Orchestration} of {IoT} {Slices} through {Edge} and {Cloud} {Microservice} {Platforms}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/19/13/2980},
	doi = {10.3390/s19132980},
	abstract = {This article addresses one of the main challenges related to the practical deployment of Internet of Things (IoT) solutions: the coordinated operation of entities at different infrastructures to support the automated orchestration of end-to-end Internet of Things services. This idea is referred to as \&ldquo;Internet of Things slicing\&rdquo; and is based on the network slicing concept already defined for the Fifth Generation (5G) of mobile networks. In this context, we present the architectural design of a slice orchestrator addressing the aforementioned challenge, based on well-known standard technologies and protocols. The proposed solution is able to integrate existing technologies, like cloud computing, with other more recent technologies like edge computing and network slicing. In addition, a functional prototype of the proposed orchestrator has been implemented, using open-source software and microservice platforms. As a first step to prove the practical feasibility of our solution, the implementation of the orchestrator considers cloud and edge domains. The validation results obtained from the prototype prove the feasibility of the solution from a functional perspective, verifying its capacity to deploy Internet of Things related functions even on resource constrained platforms. This approach enables new application models where these Internet of Things related functions can be onboarded on small unmanned aerial vehicles, offering a flexible and cost-effective solution to deploy these functions at the network edge. In addition, this proposal can also be used on commercial cloud platforms, like the Google Compute Engine, showing that it can take advantage of the benefits of edge and cloud computing respectively.},
	language = {en},
	number = {13},
	urldate = {2021-10-03},
	journal = {Sensors},
	author = {Fernandez, Juan-Manuel and Vidal, Ivan and Valera, Francisco},
	month = jan,
	year = {2019},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things (IoT), IoT slice, Small Unmanned Aerial Vehicle (SUAV), edge computing, microservice, network slicing, orchestration},
	pages = {2980},
}

@inproceedings{kaur_osmotic_2020,
	title = {Osmotic {Computing} and {Related} {Challenges}: {A} {Survey}},
	shorttitle = {Osmotic {Computing} and {Related} {Challenges}},
	doi = {10.1109/PDGC50313.2020.9315757},
	abstract = {Internet of Things (IoT) is associated with a worldwide network of interconnecting devices which are further connected to the internet, thus considerably increasing the number, range and type of devices. These devices provide anywhere and anytime connection to anyone. The IoT devices produce a large volume of data which necessitates data management. Osmotic Computing (OC), a new paradigm is driven by an increase in the resource capability or capacity at the network edge. The process of OC represents how to migrate services across the data centre to the network edge. Thus, it implies the dynamic management of macroservices and microservices across edge and cloud data centres. The goal of this work is to identify key findings of OC.},
	booktitle = {2020 {Sixth} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Kaur, Akashdeep and Kumar, Rajesh and Saxena, Sharad},
	month = nov,
	year = {2020},
	note = {ISSN: 2573-3079},
	keywords = {Cloud computing, Computer architecture, Data centers, Edge computing, Internet of Things, IoT, MicroELements, Microservices, Osmosis, Osmotic Computing, Service Migration, Solvents},
	pages = {378--383},
}

@inproceedings{kaur_osmotic_2020-1,
	title = {Osmotic {Computing} and {Related} {Challenges}: {A} {Survey}},
	shorttitle = {Osmotic {Computing} and {Related} {Challenges}},
	doi = {10.1109/PDGC50313.2020.9315757},
	abstract = {Internet of Things (IoT) is associated with a worldwide network of interconnecting devices which are further connected to the internet, thus considerably increasing the number, range and type of devices. These devices provide anywhere and anytime connection to anyone. The IoT devices produce a large volume of data which necessitates data management. Osmotic Computing (OC), a new paradigm is driven by an increase in the resource capability or capacity at the network edge. The process of OC represents how to migrate services across the data centre to the network edge. Thus, it implies the dynamic management of macroservices and microservices across edge and cloud data centres. The goal of this work is to identify key findings of OC.},
	booktitle = {2020 {Sixth} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Kaur, Akashdeep and Kumar, Rajesh and Saxena, Sharad},
	month = nov,
	year = {2020},
	note = {ISSN: 2573-3079},
	keywords = {Cloud computing, Computer architecture, Data centers, Edge computing, Internet of Things, IoT, MicroELements, Microservices, Osmosis, Osmotic Computing, Service Migration, Solvents},
	pages = {378--383},
}

@inproceedings{buzachis_towards_2019,
	title = {Towards {Osmotic} {Computing}: a {Blue}-{Green} {Strategy} for the {Fast} {Re}-{Deployment} of {Microservices}},
	shorttitle = {Towards {Osmotic} {Computing}},
	doi = {10.1109/ISCC47284.2019.8969621},
	abstract = {The rapid development of Cloud, Edge, Fog Computing and Internet of Things (IoT) technologies has played a key role in the Industry 4.0 evolution. In this context, the Osmotic Computing paradigm, theorized in 2016 as integration between a centralized Cloud layer and Edge and/or IoT layers, has further emphasized the Industry 4.0 objectives including productivity and Quality of Services (QoS). This emerging paradigm proposes a new elastic management model of microservices, where deployment and migration strategies are strongly related to the underlaying infrastructure requirements (i.e., load balancing, reliability, availability, and so on) and applications (i.e., anomalies detection, awareness of the context, proximity, QoS, and so on). Specifically, knowing that an Osmotic application must have a failover behavior (highly horizontally/vertically scalable, 24 hours 24 available, fault-tolerant and secure), this paper highlights the Osmotic ecosystem platform focusing on the implementation of a blue-green mechanism for the fast re-deployment of microservices, exploiting emerging technologies, such as Docker, Kubernetes, Agento and MongoDB. Experiments shows the time required to arrange, deploy and destroy microservices.},
	booktitle = {2019 {IEEE} {Symposium} on {Computers} and {Communications} ({ISCC})},
	author = {Buzachis, Alina and Galletta, Antonino and Celesti, Antonio and Carnevale, Lorenzo and Villari, Massimo},
	month = jun,
	year = {2019},
	note = {ISSN: 2642-7389},
	keywords = {Cloud Computing, Cloud computing, Computer architecture, Containers, Edge Computing, Internet of Things, IoT, Measurement, Microservice., Monitoring, Orchestration, Osmotic Computing, Quality of service},
	pages = {1--6},
}

@inproceedings{carnevale_cloud_2018,
	title = {From the {Cloud} to {Edge} and {IoT}: a {Smart} {Orchestration} {Architecture} for {Enabling} {Osmotic} {Computing}},
	shorttitle = {From the {Cloud} to {Edge} and {IoT}},
	doi = {10.1109/WAINA.2018.00122},
	abstract = {The latest technological and conceptual developments have destroyed the centralized Cloud Computing model, moving Cloud services in emerging ICT infrastructures such as Edge, Fog and Internet of Things (IoT) that are closer to end users. Specifically, current Cloud computing programming models and resource orchestration techniques are challenged by the recent evolution of the IoT phenomenon because smart devices are becoming more and more pervasive, powerful and inexpensive. Therefore, services need to be place near such devices. In this regard, the Osmotic Computing aims to provide a new computing paradigm based on the deployment and migration strategies related to the infrastructures and applications requirements across Cloud, Edge, Fog an IoT layers. In this scientific paper, we investigate the Smart Orchestration of a new software abstraction called MicroELement (MEL), that encapsulates resources, services and data necessary to run IoT applications. Several use cases are presented for describing the Artificial Intelligence processes that enables the MELs deployment.},
	booktitle = {2018 32nd {International} {Conference} on {Advanced} {Information} {Networking} and {Applications} {Workshops} ({WAINA})},
	author = {Carnevale, Lorenzo and Celesti, Antonio and Galletta, Antonino and Dustdar, Schahram and Villari, Massimo},
	month = may,
	year = {2018},
	keywords = {Artificial Intelligence, Cloud Computing, Cloud computing, Computational modeling, Computer architecture, Containers, Edge Computing, Elasticity, Internet of Things, IoT, Orchestration, Osmotic Computing, Quality of service},
	pages = {419--424},
}

@inproceedings{zhang_10_2020,
	title = {The 10 {Research} {Topics} in the {Internet} of {Things}},
	doi = {10.1109/CIC50333.2020.00015},
	abstract = {Since the term first coined in 1999 by Kevin Ashton, the Internet of Things (IoT) has gained significant momentum as a technology to connect physical objects to the Internet and to facilitate machine-to-human and machine-to-machine communications. Over the past two decades, IoT has been an active area of research and development endeavors by many technical and commercial communities. Yet, IoT technology is still not mature and many issues need to be addressed. In this paper, we identify 10 key research topics and discuss the research problems and opportunities within these topics.},
	booktitle = {2020 {IEEE} 6th {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Tran, Dai Hoang and Zaib, Munazza and Hamad, Salma Abdalla and Aljubairy, Abdulwahab and Alhazmi, Ahoud Abdulrahmn F. and Sagar, Subhash and Ma, Congbo},
	month = dec,
	year = {2020},
	keywords = {Conversational IoT, Energy Harvesting, Energy harvesting, Indexes, Internet of Things, IoT Service Discovery, Linked data, Recommendation, Search, Search problems, Semantics, Sensors, Summarization},
	pages = {34--43},
}

@article{schallehn_beyond_nodate,
	title = {Beyond {Proofs} of {Concept}: {Scaling} the {Industrial} {IoT}},
	language = {en},
	author = {Schallehn, Michael and Schorling, Christopher and Bowen, Peter and Straehle, Oliver},
	pages = {12},
}

@article{chandran_turning_nodate,
	title = {Turning {Contradictions} into {Innovations} or: {How} {We} {Learned} to {Stop} {Whining} and {Improve} {Security} {Operations}},
	abstract = {Eﬀorts to improve the eﬃciency of security operation centers (SOCs) have emphasized building tools for analysts or understanding the human and organizational factors involved. The importance of viewing the viability of a solution from multiple perspectives has been largely ignored. Multiple perspectives arise because of inherent conﬂicts among the objectives a SOC has to meet and diﬀerences between the goals of the parties involved. During the 3.5 years that we have used anthropological ﬁeldwork methods to study SOCs, we discovered that successful SOC innovations must resolve these conﬂicts to be eﬀective in improving operational eﬃciency. This discovery was guided by Activity Theory (AT), which provided a framework for analyzing our ﬁeldwork data. We use the version of AT proposed by Engestr¨om to model SOC operations. Template analysis, a qualitative data analysis technique, guided by AT validated the existence of contradictions in SOCs. The same technique was used to elicit from the data concrete contradictions and how they were resolved. Our analysis provide evidence of the importance of conﬂict resolution as a prerequisite for operations improvement. AT enabled us to understand why some of our innovations worked in the SOCs we studied (and why others failed). AT helps us see a potentially successful and repeatable mechanism for introducing new technologies to future SOCs. Understanding and supporting all of the spoken and unspoken requirements of SOC analysts and managers appears to be the only way to get new technologies accepted and used in SOCs.},
	language = {en},
	author = {Chandran, Sathya and McHugh, John and Ou, Xinming},
	pages = {16},
}

@book{storey_safety_1996,
	address = {USA},
	title = {Safety critical computer systems},
	isbn = {0-201-42787-7},
	abstract = {From the Publisher:Increasingly, microcomputers are being used in applications where their correct operation is vital to ensure the safety of the public and the environment: from anti-lock braking systems in automobiles, to fly-by-wire aircraft, to shut-down systems at nuclear power plants. It is, therefore, vital that engineers are aware of the safety implications of the systems they develop. This book is an introduction to the field of safety-critical computer systems, and is written for any engineer who uses microcomputers within real-time embedded systems. It assumes no prior knowledge of safety, or of any specific computer hardware or programming language. This book covers all phases of the life of a safety-critical system from its conception and specification, through to its certification, installation, service and decommissioning; provides information on how to assess the safety implications of projects, and determine the measures necessary to develop systems to meet safety needs; gives a thorough grounding in the techniques available to investigate the safety aspects of computer-based systems and the methods that may be used to enhance their dependability; and uses case studies and worked examples from a wide range of industrial sectors including the nuclear, aircraft, automotive and consumer products industries. This text is intended for both engineering and computer science students, and for practising engineers within computer-related industries. The approach taken is equally suited to engineers who consider computers from a hardware, software or systems viewpoint.},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	author = {Storey, Neil R.},
	year = {1996},
	note = {storey1996safety},
}

@inproceedings{yagemann_feasibility_2020,
	address = {Austin USA},
	title = {On the {Feasibility} of {Automating} {Stock} {Market} {Manipulation}},
	isbn = {978-1-4503-8858-0},
	url = {https://dl.acm.org/doi/10.1145/3427228.3427241},
	doi = {10.1145/3427228.3427241},
	abstract = {This work presents the first findings on the feasibility of using botnets to automate stock market manipulation. Our analysis incorporates data gathered from SEC case files, security surveys of online brokerages, and dark web marketplace data. We address several technical challenges, including how to adapt existing techniques for automation, the cost of hijacking brokerage accounts, avoiding detection, and more. We consolidate our findings into a working proof-of-concept, man-in-the-browser malware, Bot2Stock, capable of controlling victim email and brokerage accounts to commit fraud. We evaluate our bots and protocol using agent-based market simulations, where we find that a 1.5\% ratio of bots to benign traders yields a 2.8\% return on investment (ROI) per attack. Given the short duration of each attack ({\textless} 1 minute), achieving this ratio is trivial, requiring only 4 bots to target stocks like IBM. 1,000 bots, cumulatively gathered over 1 year, can turn \$100,000 into \$1,022,000, placing Bot2Stock on par with existing botnet scams.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Annual {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Yagemann, Carter and Chung, Simon P. and Uzun, Erkam and Ragam, Sai and Saltaformaggio, Brendan and Lee, Wenke},
	month = dec,
	year = {2020},
	pages = {277--290},
}

@inproceedings{karami_awakening_2021,
	address = {Virtual},
	title = {Awakening the {Web}'s {Sleeper} {Agents}: {Misusing} {Service} {Workers} for {Privacy} {Leakage}},
	isbn = {978-1-891562-66-2},
	shorttitle = {Awakening the {Web}'s {Sleeper} {Agents}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/ndss2021_1C-2_23104_paper.pdf},
	doi = {10.14722/ndss.2021.23104},
	abstract = {Service workers are a powerful technology supported by all major modern browsers that can improve users’ browsing experience by offering capabilities similar to those of native applications. While they are gaining signiﬁcant traction in the developer community, they have not received much scrutiny from security researchers. In this paper, we explore the capabilities and inner workings of service workers and conduct the ﬁrst comprehensive large-scale study of their API use in the wild. Subsequently, we show how attackers can exploit the strategic placement of service workers for history-snifﬁng in most major browsers, including Chrome and Firefox. We demonstrate two novel history-snifﬁng attacks that exploit the lack of appropriate isolation in these browsers, including a nondestructive cache-based version. Next, we present a series of use cases that illustrate how our techniques enable privacy-invasive attacks that can infer sensitive application-level information, such as a user’s social graph. We have disclosed our techniques to all vulnerable vendors, prompting the Chromium team to explore a redesign of their site isolation mechanisms for defending against our attacks. We also propose a countermeasure that can be incorporated by websites to protect their users, and develop a tool that streamlines its deployment, thus facilitating adoption at a large scale. Overall, our work presents a cautionary tale on the severe risks of browsers deploying new features without an in-depth evaluation of their security and privacy implications.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Proceedings 2021 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Karami, Soroush and Ilia, Panagiotis and Polakis, Jason},
	year = {2021},
}

@inproceedings{lopez-morales_honeyplc_2020,
	address = {Virtual Event USA},
	title = {{HoneyPLC}: {A} {Next}-{Generation} {Honeypot} for {Industrial} {Control} {Systems}},
	isbn = {978-1-4503-7089-9},
	shorttitle = {{HoneyPLC}},
	url = {https://dl.acm.org/doi/10.1145/3372297.3423356},
	doi = {10.1145/3372297.3423356},
	abstract = {Industrial Control Systems (ICS) provide management and control capabilities for mission-critical utilities such as the nuclear, power, water, and transportation grids. Within ICS, Programmable Logic Controllers (PLCs) play a key role as they serve as a convenient bridge between the cyber and the physical worlds, e.g., controlling centrifuge machines in nuclear power plants. The critical roles that ICS and PLCs play have made them the target of sophisticated cyberattacks that are designed to disrupt their operation, which creates both social unrest and financial losses. In this context, honeypots have been shown to be highly valuable tools for collecting real data, e.g., malware payload, to better understand the many different methods and strategies that attackers use.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {López-Morales, Efrén and Rubio-Medrano, Carlos and Doupé, Adam and Shoshitaishvili, Yan and Wang, Ruoyu and Bao, Tiffany and Ahn, Gail-Joon},
	month = oct,
	year = {2020},
	pages = {279--291},
}

@inproceedings{giese_amazon_2021,
	address = {Abu Dhabi United Arab Emirates},
	title = {Amazon echo dot or the reverberating secrets of {IoT} devices},
	isbn = {978-1-4503-8349-3},
	url = {https://dl.acm.org/doi/10.1145/3448300.3467820},
	doi = {10.1145/3448300.3467820},
	abstract = {Smart speakers, such as the Amazon Echo Dot, are very popular and routinely trusted with private and sensitive information. Yet, little is known about their security and potential attack vectors. We develop and synthesize a set of IoT forensics techniques, apply them to reverse engineer the hardware and software of the Amazon Echo Dot, and demonstrate its lacking protections of private user data. An adversary with physical access to such devices (e.g., purchasing a used one) can retrieve sensitive information such as Wi-Fi credentials, the physical location of (previous) owners, and cyber-physical devices (e.g., cameras, door locks). We show that such information, including all previous passwords and tokens, remains on the flash memory, even after a factory reset. This is due to the wear-leveling algorithms of the flash memory and lack of encryption. We identify and discuss the design flaws in the storage of sensitive information and the process of de-provisioning used devices. We demonstrate the practical feasibility of such attacks on 86 used devices purchased on eBay and flea markets. Finally, we propose secure design alternatives and mitigation techniques.},
	language = {en},
	urldate = {2021-09-30},
	booktitle = {Proceedings of the 14th {ACM} {Conference} on {Security} and {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {ACM},
	author = {Giese, Dennis and Noubir, Guevara},
	month = jun,
	year = {2021},
	pages = {13--24},
}

@techreport{noauthor_why_2020,
	title = {Why {IoT} {Projects} {Fail}},
	institution = {Beecham Research},
	year = {2020},
}

@inproceedings{pan_homecloud_2016,
	title = {{HomeCloud}: {An} edge cloud framework and testbed for new application delivery},
	shorttitle = {{HomeCloud}},
	doi = {10.1109/ICT.2016.7500391},
	abstract = {Conventional centralized cloud computing is a success for benefits such as on-demand, elasticity, and high colocation of data and computation. However, the paradigm shift towards “Internet of things” (IoT) will pose some unavoidable challenges: (1) massive data volume impossible for centralized datacenters to handle; (2) high latency between edge “things” and centralized datacenters; (3) monopoly, inhibition of innovations, and non-portable applications due to the proprietary application delivery in centralized cloud. The emergence of edge cloud gives hope to address these challenges. In this paper, we propose a new framework called “HomeCloud” focusing on an open and efficient new application delivery in edge cloud integrating two complementary technologies: Network Function Virtualization (NFV) and Software-Defined Networking (SDN). We also present a preliminary proof-of-concept testbed demonstrating the whole process of delivering a simple multi-party chatting application in the edge cloud. In the future, the HomeCloud framework can be further extended to support other use cases that demand portability, cost-efficiency, scalability, flexibility, and manageability. To the best of our knowledge, this framework is the first effort aiming at facilitating new application delivery in such a new edge cloud context.},
	booktitle = {2016 23rd {International} {Conference} on {Telecommunications} ({ICT})},
	author = {Pan, Jianli and Ma, Lin and Ravindran, Ravishankar and TalebiFard, Peyman},
	month = may,
	year = {2016},
	keywords = {Cloud Computing, Cloud computing, Edge Cloud, HomeCloud, Internet of Things, Internet of things, Monitoring, Monopoly, Network Function Virtualization (NFV), Servers, Software-Defined Networking (SDN)},
	pages = {1--6},
}

@article{nagappan_realizing_2008,
	title = {Realizing quality improvement through test driven development: results and experiences of four industrial teams},
	volume = {13},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Realizing quality improvement through test driven development},
	url = {http://link.springer.com/10.1007/s10664-008-9062-z},
	doi = {10.1007/s10664-008-9062-z},
	abstract = {Test-driven development (TDD) is a software development practice that has been used sporadically for decades. With this practice, a software engineer cycles minute-by-minute between writing failing unit tests and writing implementation code to pass those tests. Testdriven development has recently re-emerged as a critical enabling practice of agile software development methodologies. However, little empirical evidence supports or refutes the utility of this practice in an industrial context. Case studies were conducted with three development teams at Microsoft and one at IBM that have adopted TDD. The results of the case studies indicate that the pre-release defect density of the four products decreased between 40\% and 90\% relative to similar projects that did not use the TDD practice. Subjectively, the teams experienced a 15–35\% increase in initial development time after adopting TDD.},
	language = {en},
	number = {3},
	urldate = {2021-09-29},
	journal = {Empirical Software Engineering},
	author = {Nagappan, Nachiappan and Maximilien, E. Michael and Bhat, Thirumalesh and Williams, Laurie},
	month = jun,
	year = {2008},
	pages = {289--302},
}

@article{pan_future_2018,
	title = {Future {Edge} {Cloud} and {Edge} {Computing} for {Internet} of {Things} {Applications}},
	volume = {5},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2017.2767608},
	abstract = {The Internet is evolving rapidly toward the future Internet of Things (IoT) which will potentially connect billions or even trillions of edge devices which could generate huge amount of data at a very high speed and some of the applications may require very low latency. The traditional cloud infrastructure will run into a series of difficulties due to centralized computation, storage, and networking in a small number of datacenters, and due to the relative long distance between the edge devices and the remote datacenters. To tackle this challenge, edge cloud and edge computing seem to be a promising possibility which provides resources closer to the resource-poor edge IoT devices and potentially can nurture a new IoT innovation ecosystem. Such prospect is enabled by a series of emerging technologies, including network function virtualization and software defined networking. In this survey paper, we investigate the key rationale, the state-of-the-art efforts, the key enabling technologies and research topics, and typical IoT applications benefiting from edge cloud. We aim to draw an overall picture of both ongoing research efforts and future possible research directions through comprehensive discussions.},
	number = {1},
	journal = {IEEE Internet of Things Journal},
	author = {Pan, Jianli and McElhannon, James},
	month = feb,
	year = {2018},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Cloud computing, Computational modeling, Edge cloud, Edge computing, HomeCloud, Internet of Things, Internet of Things (IoT), Network function virtualization, Software defined networking, edge computing, network function virtualization (NFV), software defined networking (SDN), survey},
	pages = {439--449},
}

@techreport{griffor_framework_2017,
	address = {Gaithersburg, MD},
	title = {Framework for cyber-physical systems: volume 1, overview},
	shorttitle = {Framework for cyber-physical systems},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-201.pdf},
	language = {en},
	number = {NIST SP 1500-201},
	urldate = {2021-09-29},
	institution = {National Institute of Standards and Technology},
	author = {Griffor, Edward R and Greer, Chris and Wollman, David A and Burns, Martin J},
	month = jun,
	year = {2017},
	doi = {10.6028/NIST.SP.1500-201},
	pages = {NIST SP 1500--201},
}

@article{leveson_software_1986,
	title = {Software safety: why, what, and how},
	volume = {18},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Software safety},
	url = {https://dl.acm.org/doi/10.1145/7474.7528},
	doi = {10.1145/7474.7528},
	abstract = {Software safety issues become important when computers are used to control real-time, safety-critical processes. This survey attempts to explain why there is a problem, what the problem is, and what is known about how to solve it. Since this is a relatively new software research area, emphasis is placed on delineating the outstanding issues and research topics.},
	language = {en},
	number = {2},
	urldate = {2021-09-29},
	journal = {ACM Computing Surveys},
	author = {Leveson, Nancy G.},
	month = jun,
	year = {1986},
	pages = {125--163},
}

@article{neumann_computer-related_1986,
	title = {Some {Computer}-{Related} {Disasters} and {Other} {Egregious} {Horrors}},
	volume = {1},
	issn = {0885-8985},
	url = {http://ieeexplore.ieee.org/document/5004967/},
	doi = {10.1109/MAES.1986.5004967},
	language = {en},
	number = {10},
	urldate = {2021-09-29},
	journal = {IEEE Aerospace and Electronic Systems Magazine},
	author = {Neumann, Peter G.},
	month = oct,
	year = {1986},
	pages = {18--19},
}

@incollection{grilo_computer_2018,
	address = {Cham},
	series = {Materials {Forming}, {Machining} and {Tribology}},
	title = {Computer {Vision} in {Industrial} {Automation} and {Mobile} {Robots}},
	isbn = {978-3-319-78488-5},
	url = {https://doi.org/10.1007/978-3-319-78488-5_8},
	abstract = {Computer vision is presently a very relevant and important tool in both industrial manufacturing and mobile robots. As human vision is the most relevant sense to feed the brain with environmental information for decision making, computer vision is nowadays becoming the main artificial sensor in the domains of industrial quality assurance and trajectory control of mobile robots.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Introduction to {Mechanical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Grilo, Frederico and Figueiredo, Joao},
	editor = {Davim, J. Paulo},
	year = {2018},
	doi = {10.1007/978-3-319-78488-5_8},
	keywords = {Computer vision, Industrial automation, Mobile robots},
	pages = {241--266},
}

@techreport{uddin_security_2021,
	title = {Security and {Machine} {Learning} {Adoption} in {IoT}: {A} {Preliminary} {Study} of {IoT} {Developer} {Discussions}},
	shorttitle = {Security and {Machine} {Learning} {Adoption} in {IoT}},
	url = {http://arxiv.org/abs/2104.00634},
	abstract = {Internet of Things (IoT) is deﬁned as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, it is crucial to ensure IoT devices follow strict security requirements. At the same time, the prevalence of IoT devices offers developers a chance to design and develop Machine Learning (ML)-based intelligent software systems using their IoT devices. However, given the diversity of IoT devices, IoT developers may ﬁnd it challenging to introduce appropriate security and ML techniques into their devices. Traditionally, we learn about the IoT ecosystem/problems by conducting surveys of IoT developers/practitioners. Another way to learn is by analyzing IoT developer discussions in popular online developer forums like Stack Overﬂow (SO). However, we are aware of no such studies that focused on IoT developers’ security and ML-related discussions in SO. This paper offers the results of preliminary study of IoT developer discussions in SO. First, we collect around 53K IoT posts (questions + accepted answers) from SO. Second, we tokenize each post into sentences. Third, we automatically identify sentences containing security and MLrelated discussions. We ﬁnd around 12\% of sentences contain security discussions, while around 0.12\% sentences contain MLrelated discussions. There is no overlap between security and ML-related discussions, i.e., IoT developers discussing security requirements did not discuss ML requirements and vice versa. We ﬁnd that IoT developers discussing security issues frequently inquired about how the shared data can be stored, shared, and transferred securely across IoT devices and users. We also ﬁnd that IoT developers are interested to adopt deep neural network-based ML models into their IoT devices, but they ﬁnd it challenging to accommodate those into their resource-constrained IoT devices. Our ﬁndings offer implications for IoT vendors and researchers to develop and design novel techniques for improved security and ML adoption into IoT devices.},
	language = {en},
	urldate = {2021-09-29},
	author = {Uddin, Gias},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.00634},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@inproceedings{carreras_guzman_combined_2019,
	title = {Combined {Safety} and {Security} {Risk} {Analysis} using the {Ufoi}-{E} {Method}: {A} {Case} {Study} of an {Autonomous} {Surface} {Vessel}},
	isbn = {978-981-11-2724-3},
	shorttitle = {Combined {Safety} and {Security} {Risk} {Analysis} using the {Ufoi}-{E} {Method}},
	url = {http://rpsonline.com.sg/proceedings/9789811127243/html/0208.xml},
	doi = {10.3850/978-981-11-2724-3_0208-cd},
	abstract = {Many standards consider safety and security risk analysis as separate fields, specifying the system specific safety or security issues and methods to analyze them. Having these separated fields of safety and security standards complicates the risk analysis of cyber-physical systems (CPSs), where safety and security issues coexist within the integrated layers of the system. Even though several integrated safety and security analysis methods exist in the literature, they are not tailored to assess the complex and tight interactions among the CPS layers and the system’s surrounding environments. Therefore, this paper describes a method to conduct a combined safety and security risk analysis in CPSs for safety verification. Namely, we propose the Uncontrolled Flows of Information and Energy (UFoI-E) method, introducing novel diagrammatic representations to consider the dependencies within a CPS and its surrounding environments. As a case study, this paper describes a risk analysis of the collision avoidance function of an autonomous surface vessel, proving the convenience of examining the safety of autonomous vessels as safe and secure CPSs. The results of this paper may be input to new revisions and initiatives on new standards combining safety and security analysis.},
	language = {en},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 29th {European} {Safety} and {Reliability} {Conference} ({ESREL})},
	publisher = {Research Publishing Services},
	author = {Carreras Guzman, Nelson H. and Kufoalor, D. Kwame Minde and Kozine, Igor and Lundteigen, Mary Ann},
	year = {2019},
	keywords = {Exemplar, formal methods},
	pages = {4099--4106},
}

@article{sisinni_industrial_2018,
	title = {Industrial {Internet} of {Things}: {Challenges}, {Opportunities}, and {Directions}},
	volume = {14},
	issn = {1551-3203, 1941-0050},
	shorttitle = {Industrial {Internet} of {Things}},
	url = {https://ieeexplore.ieee.org/document/8401919/},
	doi = {10.1109/TII.2018.2852491},
	abstract = {Internet of Things (IoT) is an emerging domain that promises ubiquitous connection to the Internet, turning common objects into connected devices. The IoT paradigm is changing the way people interact with things around them. It paves the way for creating pervasively connected infrastructures to support innovative services and promises better ﬂexibility and efﬁciency. Such advantages are attractive not only for consumer applications, but also for the industrial domain. Over the last few years, we have been witnessing the IoT paradigm making its way into the industry marketplace with purposely designed solutions. In this paper, we clarify the concepts of IoT, Industrial IoT, and Industry 4.0. We highlight the opportunities brought in by this paradigm shift as well as the challenges for its realization. In particular, we focus on the challenges associated with the need of energy efﬁciency, real-time performance, coexistence, interoperability, and security and privacy. We also provide a systematic overview of the state-of-the-art research efforts and potential research directions to solve Industrial IoT challenges.},
	language = {en},
	number = {11},
	urldate = {2021-09-29},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Sisinni, Emiliano and Saifullah, Abusayeed and Han, Song and Jennehag, Ulf and Gidlund, Mikael},
	month = nov,
	year = {2018},
	pages = {4724--4734},
}

@article{gebremichael_security_2020,
	title = {Security and {Privacy} in the {Industrial} {Internet} of {Things}: {Current} {Standards} and {Future} {Challenges}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Security and {Privacy} in the {Industrial} {Internet} of {Things}},
	doi = {10.1109/ACCESS.2020.3016937},
	abstract = {The Internet of Things (IoT) is rapidly becoming an integral component of the industrial market in areas such as automation and analytics, giving rise to what is termed as the Industrial IoT (IIoT). The IIoT promises innovative business models in various industrial domains by providing ubiquitous connectivity, efficient data analytics tools, and better decision support systems for a better market competitiveness. However, IIoT deployments are vulnerable to a variety of security threats at various levels of the connectivity and communications infrastructure. The complex nature of the IIoT infrastructure means that availability, confidentiality and integrity are difficult to guarantee, leading to a potential distrust in the network operations and concerns of loss of critical infrastructure, compromised safety of network end-users and privacy breaches on sensitive information. This work attempts to look at the requirements currently specified for a secure IIoT ecosystem in industry standards, such as Industrial Internet Consortium (IIC) and OpenFog Consortium, and to what extent current IIoT connectivity protocols and platforms hold up to the standards with regard to security and privacy. The paper also discusses possible future research directions to enhance the security, privacy and safety of the IIoT.},
	journal = {IEEE Access},
	author = {Gebremichael, Teklay and Ledwaba, Lehlogonolo P. I. and Eldefrawy, Mohamed H. and Hancke, Gerhard P. and Pereira, Nuno and Gidlund, Mikael and Akerberg, Johan},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Cryptography, IIoT, Industrial Internet of Things, Internet of Things, Peer-to-peer computing, Privacy, Protocols, Standards, industrial networks, security and privacy},
	pages = {152351--152366},
}

@article{gupta_scalability_nodate,
	title = {Scalability in {Internet} of {Things}: {Features}, {Techniques} and {Research} {Challenges}},
	abstract = {Internet of things (IOT) is one of the most rapidly emerging fields in today’s scenario. IOT is basically a network of objects that are connected to each other through the internet and are also capable of transferring and modifying data with the help of embedded sensors. IOT is becoming the next phase of internet which has the ability to collect, analyse and spread data that can be turned into information and knowledge. With the growing idea of IOT we face a major challenge of “scalability in IOT”. Scalability is the ability of a device to adapt to the changes in the environment and meet the changing needs in the future. It is essential feature of any system which has the capability to handle the growing amount of work. It is a desirable attribute of a system or a network whose lack can cause a poor system performance and the necessity of reengineering of the whole system. In this paper we present the definition, features to be considered for scalability, techniques to achieve scalability, types of scalability and research and challenges.},
	language = {en},
	author = {Gupta, Anisha and Christie, Rivana and Manjula, R},
	pages = {12},
}

@article{antikainen_improving_2021,
	title = {Improving {Service} {Scalability} in {IoT} {Platform} {Business}},
	abstract = {The presented productized service models are one step that the case companies can take to improve their service scalability, but the models are not a solution to all scalability problems. However, similar models could be used in other companies that provide their service offerings through an IoT platform to improve their service scalability as well.},
	language = {en},
	author = {Antikainen, Eino},
	year = {2021},
	pages = {82},
}

@article{hill_what_1990,
	title = {What is scalability?},
	volume = {18},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/121973.121975},
	doi = {10.1145/121973.121975},
	abstract = {Scalability is a frequently-claimed attribute of multiprocessor systems. While the basic notion is intuitive, scalability has no generally-accepted definition. For this reason, current use of the term adds more to marketing potential than technical insight.In this paper, I first examine formal definitions of scalability, but I fail to lind a useful, rigorous definition of it. I then question whether scalability is useful and conclude by challenging the technical community to either (1) rigorously define scalability or (2) stop using it to describe systems.},
	number = {4},
	urldate = {2021-09-29},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Hill, Mark D.},
	month = dec,
	year = {1990},
	keywords = {multiprocessor, parallel random access machine (PRAM), scalability and speedup},
	pages = {18--21},
}

@article{lee_tackling_2021,
	title = {Tackling {IoT} {Scalability} with {5G} {NFV}-{Enabled} {Network} {Slicing}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=110420},
	doi = {10.4236/ait.2021.113009},
	abstract = {With emerging large volume and diverse heterogeneity of Internet of Things (IoT) applications, the one-size-fits-all design of the current 4G networks is no longer adequate to serve various types of IoT applications. Consequently, the concepts of network slicing enabled by Network Function Virtualization (NFV) have been proposed in the upcoming 5G networks. 5G network slicing allows IoT applications of different QoS requirements to be served by different virtual networks. Moreover, these network slices are equipped with scalability that allows them to grow or shrink their instances of Virtual Network Functions (VNFs) when needed. However, all current research only focuses on scalability on a single network slice, which is the scalability at the VNF level only. Such a design will eventually reach the capacity limit of a single slice under stressful incoming traffic, and cause the breakdown of an IoT system. Therefore, we propose a new IoT scalability architecture in this research to provide scalability at the NS level and design a testbed to implement the proposed architecture in order to verify its effectiveness. For evaluation, three systems are compared for their throughput, response time, and CPU utilization under three different types of IoT traffic, including the single slice scaling system, the multiple slices scaling system and the hybrid scaling system where both single slicing and multiple slicing can be simultaneously applied. Due to the balanced tradeoff between slice scalability and resource availability, the hybrid scaling system turns out to perform the best in terms of throughput and response time with medium CPU utilization.},
	language = {en},
	number = {3},
	urldate = {2021-09-29},
	journal = {Advances in Internet of Things},
	author = {Lee, Pei-Hsuan and Lin, Fuchun Joseph},
	month = jul,
	year = {2021},
	note = {Number: 3
Publisher: Scientific Research Publishing},
	pages = {123--139},
}

@techreport{moro_visconti_digital_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Digital {Scalability} and {Growth} {Options} of {Intangible} {Assets}},
	url = {https://papers.ssrn.com/abstract=3533865},
	abstract = {Scalability indicates the ability of a process, network, or system to handle a growing amount of work. Scalability fosters economic marginality, especially in intangible-driven businesses where variable costs are typically negligible. Massive volumes may offset low margins, producing economic gains. Digitalization is defined as the concept of “going paperless”, the technical process of transforming analog information or physical products into digital form. Digital scalability operates in a web context, where networked agents interact to generate co-created value. Economic and financial margins that represent a primary parameter for valuation are boosted by cost savings and scalable increases of expected revenues. Digitalized intangibles synergistically interact through networked platforms that reshape traditional supply chains.},
	language = {en},
	number = {ID 3533865},
	urldate = {2021-09-29},
	institution = {Social Science Research Network},
	author = {Moro Visconti, Roberto},
	month = feb,
	year = {2019},
	doi = {10.2139/ssrn.3533865},
	keywords = {Blitzscaling, Break-even Point, CAPEX, Digital Supply Chain, EBIT, EBITDA, Externalities, Fixed Costs, Geolocalization, Learning Curves, Liquidity, Networks, OPEX, On-demand Economy, Outsourcing, Real Options, Tracking, Variable Costs, eCommerce, eProcurement},
}

@article{romero-gazquez_in4wood_2021,
	title = {{IN4WOOD}: {A} {Successful} {European} {Training} {Action} of {Industry} 4.0 for {Academia} and {Business}},
	issn = {1557-9638},
	shorttitle = {{IN4WOOD}},
	doi = {10.1109/TE.2021.3111696},
	abstract = {The Industry 4.0 (I4.0) aims to develop a framework where the new technologies interoperate with each other and with employees, creating a smart and efficient environment. Although there are many public and private initiatives focused on boosting the deployment of I4.0 in all sectors worldwide, the adoption is slower than expected. One of the main reasons is the lack of training in those technologies involved in I4.0, the so-called key-enabling technologies (KET). In this article, the current status of I4.0 adoption from the industry, employees, and training point of view is analyzed. The lack of I4.0 competences in the curricula of vocational education training (VET) and higher education (HE) is also highlighted. Finally, the European innovative training action IN4WOOD is presented as a successful open and free training tool developed to offer students, employees, and managers an easy way to learn, use, and deploy KET of I4.0. Although the main target users of the training action are those in the furniture and woodworking sector, it has been designed to be useful also for users in other business sectors. The training tool is composed of more than 300 video learning pills, practical use cases, gamification, and evaluation test for measuring the level of knowledge acquired. The training tool has been tested in a pilot launched in four European countries. The results from the pilot prove that the IN4WOOD training helps to fill the skill gaps identified in the current VET/HE students and improves the competitiveness of employees, managers, and enterprises.},
	journal = {IEEE Transactions on Education},
	author = {Romero-Gázquez, Jose Luis and Cañavate-Cruzado, Gregorio and Bueno-Delgado, María-Victoria},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Education},
	keywords = {Business, Companies, Europe, Higher education (HE), IN4WOOD, Industries, Production facilities, Tools, Training, industry 4.0 (I4.0), key-enabling technologies (KET), vocational education training (VET).},
	pages = {1--10},
}

@article{verma_auto-scaling_2021,
	title = {Auto-scaling techniques for {IoT}-based cloud applications: a review},
	volume = {24},
	issn = {1573-7543},
	shorttitle = {Auto-scaling techniques for {IoT}-based cloud applications},
	url = {https://doi.org/10.1007/s10586-021-03265-9},
	doi = {10.1007/s10586-021-03265-9},
	abstract = {Cloud and IoT applications have inquiring effects that can strongly influence today’s ever-growing internet life along with necessity to resolve numerous challenges for each application such as scalability, security, privacy, and reliability. During the deployment of IoT-based Cloud applications, the demand for Cloud tenants is dynamic that makes challenging to maintain scalability of the system. Developing an effective scaling technique is not merely a big concern, but how to achieve autonomic scaling results using future load prediction and migration policies is also a crucial phase. Also, to evaluate such auto-scaling strategy, certain Quality of Service (QoS) metrics must be recognized, explored and leveraged to enhance the performance of the system. Therefore, in this paper, a survey of existing auto-scaling, load prediction and VM migration techniques for IoT-based Cloud applications has been carried out along with the evaluation of various QoS parameters. Further, the future trends have also been discussed for performing auto-scaling in a Cloud environment.},
	language = {en},
	number = {3},
	urldate = {2021-09-29},
	journal = {Cluster Computing},
	author = {Verma, Shveta and Bala, Anju},
	month = sep,
	year = {2021},
	pages = {2425--2459},
}

@article{lou_investigation_2021,
	title = {An investigation into the state-of-the-practice autonomous driving testing},
	url = {http://arxiv.org/abs/2106.12233},
	abstract = {Autonomous driving shows great potential to reform modern transportation and its safety is attracting much attention from public. Autonomous driving systems generally include deep neural networks (DNNs) for gaining better performance (e.g., accuracy on object detection and trajectory prediction). However, compared with traditional software systems, this new paradigm (i.e., program + DNNs) makes software testing more difficult. Recently, software engineering community spent significant effort in developing new testing methods for autonomous driving systems. However, it is not clear that what extent those testing methods have addressed the needs of industrial practitioners of autonomous driving. To fill this gap, in this paper, we present the first comprehensive study to identify the current practices and needs of testing autonomous driving systems in industry. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. Through thematic analysis of interview and questionnaire data, we identified five urgent needs of testing autonomous driving systems from industry. We further analyzed the limitations of existing testing methods to address those needs and proposed several future directions for software testing researchers.},
	urldate = {2021-09-29},
	journal = {arXiv:2106.12233 [cs]},
	author = {Lou, Guannan and Deng, Yao and Zheng, Xi and Zhang, Tianyi and Zhang, Mengshi},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.12233},
	keywords = {Computer Science - Software Engineering},
}

@incollection{rathee_introduction_2020,
	address = {Singapore},
	series = {Studies in {Big} {Data}},
	title = {Introduction to {Blockchain} and {IoT}},
	isbn = {9789811387753},
	url = {https://doi.org/10.1007/978-981-13-8775-3_1},
	abstract = {The blockchain is emerging rapidly as a current area of research these days. The blockchain is a technology used to run bitcoin. It is distributed database maintaining a list of record growing continuously called blocks in order to ensure the security of those blocks from revision and tampering. Every block is connected to other blocks by maintaining the hash of the previous block in the chain. This chapter discusses the technical aspects of blockchain and IoT. The IoT is merely not a concept these days. It is the necessity of time in everyday life. The “smartphone” is the most familiar application of IoT in the day-to-day life. The application of IoT is not limited to smart homes. It is ranging from industrial and commercial sectors to agriculture, public safety, and the health sector. The IoT can also be considered as “Internet of Everything (IoE)” because of a wide range of real-life applications of IoT.},
	language = {en},
	urldate = {2021-09-28},
	booktitle = {Advanced {Applications} of {Blockchain} {Technology}},
	publisher = {Springer},
	author = {Rathee, Priyanka},
	editor = {Kim, Shiho and Deka, Ganesh Chandra},
	year = {2020},
	doi = {10.1007/978-981-13-8775-3_1},
	keywords = {Bitcoin, Blockchain, IoT},
	pages = {1--14},
}

@article{uviase_iot_2018,
	title = {{IoT} {Architectural} {Framework}: {Connection} and {Integration} {Framework} for {IoT} {Systems}},
	volume = {264},
	issn = {2075-2180},
	shorttitle = {{IoT} {Architectural} {Framework}},
	url = {http://arxiv.org/abs/1803.04780},
	doi = {10.4204/EPTCS.264.1},
	abstract = {The proliferation of the Internet of Things (IoT) has since seen a growing interest in architectural design and adaptive frameworks to promote the connection between heterogeneous IoT devices and IoT systems. The most widely favoured software architecture in IoT is the Service Oriented Architecture (SOA), which aims to provide a loosely coupled systems to leverage the use and reuse of IoT services at the middle-ware layer, to minimise system integration problems. However, despite the flexibility offered by SOA, the challenges of integrating, scaling and ensuring resilience in IoT systems persist. One of the key causes of poor integration in IoT systems is the lack of an intelligent, connection-aware framework to support interaction in IoT systems. This paper reviews existing architectural frameworks for integrating IoT devices and identifies the key areas that require further research improvements. The paper concludes by proposing a possible solution based on microservice. The proposed IoT integration framework benefits from an intelligent API layer that employs an external service assembler, service auditor, service monitor and service router component to coordinate service publishing, subscription, decoupling and service combination within the architecture.},
	urldate = {2021-09-28},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Uviase, Onoriode and Kotonya, Gerald},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.04780},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering, Internet of Things},
	pages = {1--17},
}

@article{badiger_violet_2018,
	title = {{VIoLET}: {A} {Large}-scale {Virtual} {Environment} for {Internet} of {Things}},
	volume = {11014},
	shorttitle = {{VIoLET}},
	url = {http://arxiv.org/abs/1806.06032},
	doi = {10.1007/978-3-319-96983-1_22},
	abstract = {IoT deployments have been growing manifold, encompassing sensors, networks, edge, fog and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose VIoLET, a virtual environment for deﬁning and launching large-scale IoT deployments within cloud VMs. It oﬀers a declarative model to specify container-based compute resources that match the performance of the native edge, fog and cloud devices using Docker. These can be interconnected by complex topologies on which private/public networks, and bandwidth and latency rules are enforced. Users can conﬁgure synthetic sensors for data generation on these devices as well. We validate VIoLET for deployments with {\textgreater} 400 devices and {\textgreater} 1500 device-cores, and show that the virtual IoT environment closely matches the expected compute and network performance at modest costs. This ﬁlls an important gap between IoT simulators and real deployments.},
	language = {en},
	urldate = {2021-09-28},
	journal = {arXiv:1806.06032 [cs]},
	author = {Badiger, Shreyas and Baheti, Shrey and Simmhan, Yogesh},
	year = {2018},
	note = {arXiv: 1806.06032},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {309--324},
}

@inproceedings{wang_iot_2015,
	title = {An {IoT} {Application} for {Fault} {Diagnosis} and {Prediction}},
	doi = {10.1109/DSDIS.2015.97},
	abstract = {Internet of Things (IoT) has become an important topic in both industry and academia for the recent years as it offers great potentials in numerous real world applications. This paper considers the problem of fault diagnosis and prediction from IoT data collected in the process industry. We propose a solution by making use of IoT enabling technologies offered by SAP. The proposed solution first discovers the causal relationship of the physical devices by analyzing only the device sensor data without the knowledge of the physical manufacturing system. While faults of certain devices can be detected by monitoring the healthy index of these devices in real-time, possible faults of other devices can be predicted based on the causal relationship discovered in the previous step. Such prediction capability enables new breeds of predictive maintenance applications where appropriate actions can be recommended to operators of the manufacturing system in a timely manner. The viability of the proposed solution is confirmed by a real world application of IoT conducted with an industry partner.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Data} {Science} and {Data} {Intensive} {Systems}},
	author = {Wang, Chen and Vo, Hoang Tam and Ni, Peng},
	month = dec,
	year = {2015},
	keywords = {Algorithm design and analysis, Databases, Fault diagnosis, Industries, IoT, Monitoring, Prediction algorithms, Predictive maintenance, Predictive models, Real-time systems},
	pages = {726--731},
}

@article{zhang_blockchain-based_2021,
	title = {Blockchain-{Based} {Federated} {Learning} for {Device} {Failure} {Detection} in {Industrial} {IoT}},
	volume = {8},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2020.3032544},
	abstract = {Device failure detection is one of most essential problems in Industrial Internet of Things (IIoT). However, in conventional IIoT device failure detection, client devices need to upload raw data to the central server for model training, which might lead to disclosure of sensitive business data. Therefore, in this article, to ensure client data privacy, we propose a blockchain-based federated learning approach for device failure detection in IIoT. First, we present a platform architecture of blockchain-based federated learning systems for failure detection in IIoT, which enables verifiable integrity of client data. In the architecture, each client periodically creates a Merkle tree in which each leaf node represents a client data record, and stores the tree root on a blockchain. Furthermore, to address the data heterogeneity issue in IIoT failure detection, we propose a novel centroid distance weighted federated averaging (CDW\_FedAvg) algorithm taking into account the distance between positive class and negative class of each client data set. In addition, to motivate clients to participate in federated learning, a smart contact-based incentive mechanism is designed depending on the size and the centroid distance of client data used in local model training. A prototype of the proposed architecture is implemented with our industry partner, and evaluated in terms of feasibility, accuracy, and performance. The results show that the approach is feasible, and has satisfactory accuracy and performance.},
	number = {7},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Weishan and Lu, Qinghua and Yu, Qiuyu and Li, Zhaotong and Liu, Yue and Lo, Sin Kit and Chen, Shiping and Xu, Xiwei and Zhu, Liming},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {AI, Blockchain, Collaborative work, Computational modeling, Data models, IoT, Servers, Training, blockchain, edge computing, failure detection, federated learning, machine learning},
	pages = {5926--5937},
}

@article{banerjee_hardware-assisted_2020,
	title = {A {Hardware}-assisted {Heartbeat} {Mechanism} for {Fault} {Identification} in {Large}-scale {IoT} {Systems}},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2020.3009212},
	abstract = {With increased inter-connectivity among disparate devices, such as Internet-of-Things (IoT) devices, including those deployed in a nation's critical infrastructure, there is a need to ensure that any failure in the deployed devices can be detected. The capability to automatically detect device failures is particularly crucial in a large-scale, complex IoT system, since it can be very time-consuming and challenging to investigate a large number of geographically-dispersed devices that are also of different makes and types. In this paper, we present a faulty-device identification technique that is designed to achieve lightweight processor-level architectural support. Specifically, a hardware-based monitoring agent is incorporated within a processor and connected to a separate monitoring program when an examination is required. By analyzing information collected by the agent, the monitoring program determines whether the device being monitored is functioning. Findings from our detailed evaluation demonstrate that the proposed approach can detect around 90\% of the failures with minimal hardware overhead of approximately 5k gates. This area overhead is reasonable and amounts to 7.69\% of the ARM Cortex-M4 a lightweight IoT-class processor that has a total area (excluding optional caches and scratch-pad memory) of 65k gates.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Banerjee, Mandrita and Borges, Carlo and Choo, Kim-Kwang Raymond and Lee, Junghee and Nicopoulos, Chrysostomos},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Biomedical monitoring, Circuit faults, Computer security, Control-flow integrity, Heart beat, Internet-of-Things (IoT), Logic gates, Monitoring, Self testing, Task analysis},
	pages = {1--1},
}

@inproceedings{lohokare_iot_2017,
	title = {An {IoT} ecosystem for the implementation of scalable wireless home automation systems at smart city level},
	doi = {10.1109/TENCON.2017.8228095},
	abstract = {Today home automation is quite common in developed nations. Remote access of appliances and devices in a home is provided via the internet. The challenge lies in making the system smart and deployable in a scalable manner across an entire city in a developing nation. This paper proposes a framework for making home automation achievable across a smart city in a scalable and data efficient manner. We have proposed an end to end home automation suite with modular implementation using open source components. The backend implemented supports multiple IoT protocols and the system is horizontally scalable. The `Rules engine' implemented analyzes the data real time thus triggering various actions pre-decided by users. The system involves energy monitoring for enabling judicial energy consumption in a home. Unlike smart-meters, which give data about total energy consumption per house, the proposed solution is an innovative way to get fairly accurate analysis of per device consumption. A single system can be deployed and can handle data coming from homes for an entire city. The implemented system includes web-dashboard and android application for monitoring the consumption. The scalable architecture used ensures that there will be no problem handling data traffic as number of users increase. Experiments on the prototype system showed low latency for real time analytics even with large simultaneous data streams. The unique features of this system are its modular \& dynamic nature, scalability and real time analytics for quick decision making.},
	booktitle = {{TENCON} 2017 - 2017 {IEEE} {Region} 10 {Conference}},
	author = {Lohokare, Jay and Dani, Reshul and Rajurkar, Ajit and Apte, Ameya},
	month = nov,
	year = {2017},
	note = {ISSN: 2159-3450},
	keywords = {Cloud computing, Home automation, Internet of Things, IoT framework, IoT protocols, Protocols, Real-time systems, Scalability, Servers, Sparks, apache kafka, cloud computing, emqttd, home automation, mqtt, real time analytics, rules engine, smart home, spark streaming, wireless networks},
	pages = {1503--1508},
}

@inproceedings{theodoridis_developing_2013,
	title = {Developing an {IoT} {Smart} {City} framework},
	doi = {10.1109/IISA.2013.6623710},
	abstract = {In this paper, we discuss key findings, technological challenges and socioeconomic opportunities in Smart City era. Most of the conclusions were gathered during SmartSantander project, an EU project that is developing a city-scale testbed for IoT and Future Internet experimentation, providing an integrated framework for implementing Smart City services.},
	booktitle = {{IISA} 2013},
	author = {Theodoridis, Evangelos and Mylonas, Georgios and Chatzigiannakis, Ioannis},
	month = jul,
	year = {2013},
	keywords = {Cities and towns, Future Internet, Internet, IoT, Monitoring, Semantics, Sensors, Servers, Smart City, Smart phones, Socioeconomic Impact},
	pages = {1--6},
}

@inproceedings{velasquez_fast-data_2018,
	title = {Fast-data architecture proposal to alert people in emergency},
	doi = {10.1109/CCWC.2018.8301721},
	abstract = {This paper states a brief overview of technologies related to Smart Cities and Big Data ecosystems in order to develop and present an architecture proposal for deploying services using the paradigm of Fast Data. The main goal of this architecture, is to present a set of tools and how it could be integrated for providing fast data services focus on Resilient Smart Cities. Finally, proposals for analysis the response times, delays, incidents, and future works are presented.},
	booktitle = {2018 {IEEE} 8th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Velásquez, Washington and Munoz-Arcentales, Andres and Salvachúa, Joaquin},
	month = jan,
	year = {2018},
	keywords = {Architecture, Big Data, Emergency, Emergency services, Fast Data, IoT, Proposals, Real-time systems, Smart City, Smart cities, Sparks, Tools},
	pages = {165--168},
}

@inproceedings{kyriazis_sustainable_2013,
	title = {Sustainable smart city {IoT} applications: {Heat} and electricity management amp; {Eco}-conscious cruise control for public transportation},
	shorttitle = {Sustainable smart city {IoT} applications},
	doi = {10.1109/WoWMoM.2013.6583500},
	abstract = {In a world of multi-stakeholder information and assets provision on top of millions of real-time interacting and communicating things, systems based on Internet of Things (IoT) technologies aim at exploiting these assets in a resilient and sustainable way allowing them to reach their full potential. In this paper we present two innovative smart city IoT applications: the first one refers to heat and energy management, and aims at utilizing different resources (such as heat and electricity meters) in order to optimize use of energy in commercial and residential areas. The second application refers to cruise control for public transportation, and aims at utilizing different resources (such as environmental and traffic sensors) in order to provide driving recommendations that aim at eco efficiency. We also highlight the IoT challenges as well as potential enabling technologies that will allow for the realization of the proposed applications.},
	booktitle = {2013 {IEEE} 14th {International} {Symposium} on "{A} {World} of {Wireless}, {Mobile} and {Multimedia} {Networks}" ({WoWMoM})},
	author = {Kyriazis, Dimosthenis and Varvarigou, Theodora and White, Daniel and Rossi, Andrea and Cooper, Joshua},
	month = jun,
	year = {2013},
	keywords = {Cities and towns, Electricity, Internet, Real-time systems, Resistance heating, Sensors, Transportation},
	pages = {1--5},
}

@inproceedings{mezghani_autonomic_2020,
	title = {Autonomic {Coordination} of {IoT} {Device} {Management} {Platforms}},
	doi = {10.1109/WETICE49692.2020.00015},
	abstract = {With the adoption of the Internet of Things (IoT), devices become integrated in our daily lives. These devices are more and more interdependent, either by guaranteeing connectivity, or by providing data for complex services. In an ever-evolving IoT context, continuously managing these connected objects is crucial, to handle vulnerabilities, ensure their well-functioning, and add new services. These features fall under the scope of Device Management (DM). The distribution and scalability of DM platforms become all the more exacerbated as device profiles and deployment use cases get more heterogeneous. Thus, multiple actors are involved in IoT services, with their respective, isolated, DM platforms. However, DM operations, such as reboots and firmware updates, have a direct impact on the state of devices, which requires awareness across DM platforms. In this paper, we address conflicts linked to connectivity dependencies, which may arise during the simultaneous execution of DM operations on interdependent devices. Hence, we propose a coordinator that collects DM operation intentions from isolated DM platforms and detects the relevant dependencies, so as to plan and orchestrate the execution of these operations. An evaluation of this middleware is presented, with perspectives for industrial applications.},
	booktitle = {2020 {IEEE} 29th {International} {Conference} on {Enabling} {Technologies}: {Infrastructure} for {Collaborative} {Enterprises} ({WETICE})},
	author = {Mezghani, Emna and Berlemont, Samuel and Douet, Marc},
	month = sep,
	year = {2020},
	note = {ISSN: 2641-8169},
	keywords = {Autonomic Computing, Complexity theory, Coordination, Device Management, Internet of Things, Middleware, Planning, Scalability, Task analysis, Topology},
	pages = {30--35},
}

@article{douglass_agile_nodate,
	title = {Agile analysis practices for safety-critical software development},
	abstract = {Because of their discipline and efﬁciency, agile development practices should be applied to the development of safety-critical software. Bruce Douglass, author of the IBM Rational Harmony for Embedded RealTime Development process, explains the key analysis practices for the development of safety-critical systems and how they can be realized in an agile way.},
	language = {en},
	author = {Douglass, Bruce},
	pages = {13},
}

@article{sun_edgeiot_2016,
	title = {{EdgeIoT}: {Mobile} {Edge} {Computing} for the {Internet} of {Things}},
	volume = {54},
	issn = {1558-1896},
	shorttitle = {{EdgeIoT}},
	doi = {10.1109/MCOM.2016.1600492CM},
	abstract = {In order to overcome the scalability problem of the traditional Internet of Things architecture (i.e., data streams generated from distributed IoT devices are transmitted to the remote cloud via the Internet for further analysis), this article proposes a novel approach to mobile edge computing for the IoT architecture, edgeIoT, to handle the data streams at the mobile edge. Specifically, each BS is connected to a fog node, which provides computing resources locally. On the top of the fog nodes, the SDN-based cellular core is designed to facilitate packet forwarding among fog nodes. Meanwhile, we propose a hierarchical fog computing architecture in each fog node to provide flexible IoT services while maintaining user privacy: each user's IoT devices are associated with a proxy VM (located in a fog node), which collects, classifies, and analyzes the devices' raw data streams, converts them into metadata, and transmits the metadata to the corresponding application VMs (which are owned by IoT service providers). Each application VM receives the corresponding metadata from different proxy VMs and provides its service to users. In addition, a novel proxy VM migration scheme is proposed to minimize the traffic in the SDNbased core.},
	number = {12},
	journal = {IEEE Communications Magazine},
	author = {Sun, Xiang and Ansari, Nirwan},
	month = dec,
	year = {2016},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {Edge computing, Internet of Things, Mobile computing},
	pages = {22--29},
}

@article{pan_edgechain_2019,
	title = {{EdgeChain}: {An} {Edge}-{IoT} {Framework} and {Prototype} {Based} on {Blockchain} and {Smart} {Contracts}},
	volume = {6},
	issn = {2327-4662},
	shorttitle = {{EdgeChain}},
	doi = {10.1109/JIOT.2018.2878154},
	abstract = {The emerging Internet of Things (IoT) is facing significant scalability and security challenges. On one hand, IoT devices are “weak” and need external assistance. Edge computing provides a promising direction addressing the deficiency of centralized cloud computing in scaling massive number of devices. On the other hand, IoT devices are also relatively “vulnerable” facing malicious hackers due to resource constraints. The emerging blockchain and smart contracts technologies bring a series of new security features for IoT and edge computing. In this paper, to address the challenges, we design and prototype an edge-IoT framework named “EdgeChain” based on blockchain and smart contracts. The core idea is to integrate a permissioned blockchain and the internal currency or “coin” system to link the edge cloud resource pool with each IoT device' account and resource usage, and hence behavior of the IoT devices. EdgeChain uses a credit-based resource management system to control how much resource IoT devices can obtain from edge servers, based on predefined rules on priority, application types, and past behaviors. Smart contracts are used to enforce the rules and policies to regulate the IoT device behavior in a nondeniable and automated manner. All the IoT activities and transactions are recorded into blockchain for secure data logging and auditing. We implement an EdgeChain prototype and conduct extensive experiments to evaluate the ideas. The results show that while gaining the security benefits of blockchain and smart contracts, the cost of integrating them into EdgeChain is within a reasonable and acceptable range.},
	number = {3},
	journal = {IEEE Internet of Things Journal},
	author = {Pan, Jianli and Wang, Jianyu and Hester, Austin and Alqerm, Ismail and Liu, Yuanni and Zhao, Ying},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Blockchain, Cloud computing, Edge computing, EdgeChain, Internet of Things, Internet of Things (IoT), Servers, edge computing, fog computing, scalability, security, smart contracts},
	pages = {4719--4732},
}

@article{cui_blockchain_2019,
	title = {Blockchain in {IoT}: {Current} {Trends}, {Challenges}, and {Future} {Roadmap}},
	volume = {3},
	issn = {2509-3436},
	shorttitle = {Blockchain in {IoT}},
	url = {https://doi.org/10.1007/s41635-019-00079-5},
	doi = {10.1007/s41635-019-00079-5},
	abstract = {The Internet of Things (IoT) is one of the most promising technologies in the era of information technology. IoT enables ubiquitous data collections and network communications to bring significant and indispensable convenience and intelligence both to daily life and industrial operations. However, IoT is still confronting a number of challenges and manifesting a series of issues that need to be addressed urgently. Counterfeit hardware, software faults, security issues during communication, system management difficulties, and data privacy issues are significant issues for current IoT infrastructure. Meanwhile, blockchain, as an emerging information technology, has attracted huge public interest and has shown significant promise because of its decentralization, transparency, and security. The features of blockchain seem to be an ideal match for IoT, and by applying blockchain to an IoT environment, some of the aforementioned weaknesses can be addressed. This paper’s purpose is to introduce the use of blockchain in IoT applications. We present various challenges facing an IoT system and summarize the benefits of adopting blockchain into IoT infrastructure. We primarily focus on illustrating the blockchain applications in IoT with refined capabilities and enhanced security. To shed light on blockchain in IoT research, we also discuss limitations and future directions.},
	language = {en},
	number = {4},
	urldate = {2021-09-23},
	journal = {Journal of Hardware and Systems Security},
	author = {Cui, Pinchen and Guin, Ujjwal and Skjellum, Anthony and Umphress, David},
	month = dec,
	year = {2019},
	pages = {338--364},
}

@incollection{miraz_blockchain_2020,
	address = {Singapore},
	series = {Studies in {Big} {Data}},
	title = {Blockchain of {Things} ({BCoT}): {The} {Fusion} of {Blockchain} and {IoT} {Technologies}},
	isbn = {9789811387753},
	shorttitle = {Blockchain of {Things} ({BCoT})},
	url = {https://doi.org/10.1007/978-981-13-8775-3_7},
	abstract = {Blockchain, as well as Internet of Things (IoT), is considered as two major disruptive emerging technologies. However, both of them suffer from innate technological limitations to some extent. IoT requires strengthening its security features while Blockchain inherently possesses them due to its extensive use of cryptographic mechanisms and Blockchain––in an inverted manner–– needs contributions from the distributed nodes for its P2P (Peer-to-Peer) consensus model while IoT rudimentarily embodies them within its architecture. This chapter, therefore, acutely dissects the viability, along with prospective challenges, of incorporating Blockchain with IoT technologies––inducing the notion of Blockchain of Things (BCoT)––as well as the benefits such consolidation can offer.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Advanced {Applications} of {Blockchain} {Technology}},
	publisher = {Springer},
	author = {Miraz, Mahdi H.},
	editor = {Kim, Shiho and Deka, Ganesh Chandra},
	year = {2020},
	doi = {10.1007/978-981-13-8775-3_7},
	keywords = {Blockchain, Blockchain of Things (BCoT), Internet of things (IoT), Security, Wireless Sensor Network (WSN)},
	pages = {141--159},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917634&casa_token=qR5HPcERvs0AAAAA:_9hENFOlx7g69XskpxQ9FW5EtBxl8eEyOotnwXf1obtiSk_GxdPwiI9PA4LZhPxt1wO5vdtuVw&tag=1},
	urldate = {2021-09-23},
}

@misc{noauthor_ieee_nodate-1,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917634&casa_token=qR5HPcERvs0AAAAA:_9hENFOlx7g69XskpxQ9FW5EtBxl8eEyOotnwXf1obtiSk_GxdPwiI9PA4LZhPxt1wO5vdtuVw&tag=1},
	urldate = {2021-09-23},
}

@inproceedings{dorri_towards_2017,
	title = {Towards an {Optimized} {BlockChain} for {IoT}},
	abstract = {There has been increasing interest in adopting BlockChain (BC), that underpins the crypto-currency Bitcoin, in Internet of Things (IoT) for security and privacy. However, BCs are computationally expensive and involve high bandwidth overhead and delays, which are not suitable for most IoT devices. This paper proposes a lightweight BC-based architecture for IoT that virtually eliminates the overheads of classic BC, while maintaining most of its security and privacy benefits. IoT devices benefit from a private immutable ledger, that acts similar to BC but is managed centrally, to optimize energy consumption. High resource devices create an overlay network to implement a publicly accessible distributed BC that ensures end-to-end security and privacy. The proposed architecture uses distributed trust to reduce the block validation processing time. We explore our approach in a smart home setting as a representative case study for broader IoT applications. Qualitative evaluation of the architecture under common threat models highlights its effectiveness in providing security and privacy for IoT applications. Simulations demonstrate that our method decreases packet and processing overhead significantly compared to the BC implementation used in Bitcoin.},
	booktitle = {2017 {IEEE}/{ACM} {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation} ({IoTDI})},
	author = {Dorri, Ali and Kanhere, Salil S. and Jurdak, Raja},
	month = apr,
	year = {2017},
	keywords = {BlockChain, Computer architecture, Data privacy, Internet of Things, Online banking, Peer-to-peer computing, Privacy, Security, Smart homes},
	pages = {173--178},
}

@article{panarello_blockchain_2018,
	title = {Blockchain and {IoT} {Integration}: {A} {Systematic} {Survey}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Blockchain and {IoT} {Integration}},
	url = {https://www.mdpi.com/1424-8220/18/8/2575},
	doi = {10.3390/s18082575},
	abstract = {The Internet of Things (IoT) refers to the interconnection of smart devices to collect data and make intelligent decisions. However, a lack of intrinsic security measures makes IoT vulnerable to privacy and security threats. With its \&ldquo;security by design,\&rdquo; Blockchain (BC) can help in addressing major security requirements in IoT. BC capabilities like immutability, transparency, auditability, data encryption and operational resilience can help solve most architectural shortcomings of IoT. This article presents a comprehensive survey on BC and IoT integration. The objective of this paper is to analyze the current research trends on the usage of BC-related approaches and technologies in an IoT context. This paper presents the following novelties, with respect to related work: (i) it covers different application domains, organizing the available literature according to this categorization, (ii) it introduces two usage patterns, i.e., device manipulation and data management (open marketplace solution), and (iii) it reports on the development level of some of the presented solutions. We also analyze the main challenges faced by the research community in the smooth integration of BC and IoT, and point out the main open issues and future research directions. Last but not least, we also present a survey about novel uses of BC in the machine economy.},
	language = {en},
	number = {8},
	urldate = {2021-09-23},
	journal = {Sensors},
	author = {Panarello, Alfonso and Tapas, Nachiket and Merlino, Giovanni and Longo, Francesco and Puliafito, Antonio},
	month = aug,
	year = {2018},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Internet of Things, IoT, blockchain, machine economy, survey},
	pages = {2575},
}

@article{novo_blockchain_2018,
	title = {Blockchain {Meets} {IoT}: {An} {Architecture} for {Scalable} {Access} {Management} in {IoT}},
	volume = {5},
	issn = {2327-4662},
	shorttitle = {Blockchain {Meets} {IoT}},
	doi = {10.1109/JIOT.2018.2812239},
	abstract = {The Internet of Things (IoT) is stepping out of its infancy into full maturity and establishing itself as a part of the future Internet. One of the technical challenges of having billions of devices deployed worldwide is the ability to manage them. Although access management technologies exist in IoT, they are based on centralized models which introduce a new variety of technical limitations to manage them globally. In this paper, we propose a new architecture for arbitrating roles and permissions in IoT. The new architecture is a fully distributed access control system for IoT based on blockchain technology. The architecture is backed by a proof of concept implementation and evaluated in realistic IoT scenarios. The results show that the blockchain technology could be used as access management technology in specific scalable IoT scenarios.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {Novo, Oscar},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Access control, Bitcoin, Contracts, Internet of Things, Internet of Things (IoT), Peer-to-peer computing, blockchain, smart contracts},
	pages = {1184--1195},
}

@incollection{bondavalli_security_2014,
	address = {Cham},
	title = {Security {Application} of {Failure} {Mode} and {Effect} {Analysis} ({FMEA})},
	volume = {8666},
	isbn = {978-3-319-10505-5 978-3-319-10506-2},
	url = {http://link.springer.com/10.1007/978-3-319-10506-2_21},
	abstract = {Increasingly complex systems lead to an interweaving of security, safety, availability and reliability concerns. Most dependability analysis techniques do not include security aspects. In order to include security, a holistic risk model for systems is needed. In our novel approach, the basic failure cause, failure mode and failure effect model known from FMEA is used as a template for a vulnerability cause-effect chain, and an FMEA analysis technique extended with security is presented. This represents a unified model for safety and security cause-effect analysis. As an example the technique is then applied to a distributed industrial measurement system.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Schmittner, Christoph and Gruber, Thomas and Puschner, Peter and Schoitsch, Erwin},
	editor = {Bondavalli, Andrea and Di Giandomenico, Felicita},
	year = {2014},
	doi = {10.1007/978-3-319-10506-2_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {310--325},
}

@article{noauthor_procedures_nodate,
	title = {Procedures for {Performing} a {Failure} {Mode}, {Effects} and {Criticality} {Analysis}},
	language = {en},
	pages = {80},
}

@incollection{van_der_aalst_combined_2012,
	address = {Berlin, Heidelberg},
	title = {A {Combined} {Process} for {Elicitation} and {Analysis} of {Safety} and {Security} {Requirements}},
	volume = {113},
	isbn = {978-3-642-31071-3 978-3-642-31072-0},
	url = {http://link.springer.com/10.1007/978-3-642-31072-0_24},
	abstract = {The aim of safety and security assessments are very similar since they both consider harm during system development. However, they apply different means for it and are performed in separated processes. As security and safety areas are merging in new systems that are critical, and more openly interconnected, there is a need to relate the different processes during the development. A combined assessment process could save resources compared to separated safety and security assessments, as well as support the understanding of mutual constraints and the resolution of conflicts between the two areas. We suggest a combined method covering the harm identification and analysis part of the assessment process using UML-based models. The process is applied on a case from the Air Traffic Management domain. Experts’ opinions about the results have also been collected for feedback.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Enterprise, {Business}-{Process} and {Information} {Systems} {Modeling}},
	publisher = {Springer Berlin Heidelberg},
	author = {Raspotnig, Christian and Karpati, Peter and Katta, Vikash},
	editor = {van der Aalst, Wil and Mylopoulos, John and Rosemann, Michael and Shaw, Michael J. and Szyperski, Clemens and Bider, Ilia and Halpin, Terry and Krogstie, John and Nurcan, Selmin and Proper, Erik and Schmidt, Rainer and Soffer, Pnina and Wrycza, Stanisław},
	year = {2012},
	doi = {10.1007/978-3-642-31072-0_24},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {347--361},
}

@inproceedings{wilson_trust_2017,
	address = {Niagara Falls New York USA},
	title = {Trust but {Verify}: {Auditing} the {Secure} {Internet} of {Things}},
	isbn = {978-1-4503-4928-4},
	shorttitle = {Trust but {Verify}},
	url = {https://dl.acm.org/doi/10.1145/3081333.3081342},
	doi = {10.1145/3081333.3081342},
	abstract = {Internet-of-Things devices often collect and transmit sensitive information like camera footage, health monitoring data, or whether someone is home. These devices protect data in transit with end-to-end encryption, typically using TLS connections between devices and associated cloud services.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 15th {Annual} {International} {Conference} on {Mobile} {Systems}, {Applications}, and {Services}},
	publisher = {ACM},
	author = {Wilson, Judson and Wahby, Riad S. and Corrigan-Gibbs, Henry and Boneh, Dan and Levis, Philip and Winstein, Keith},
	month = jun,
	year = {2017},
	pages = {464--474},
}

@inproceedings{wang_fear_2018,
	address = {San Diego, CA},
	title = {Fear and {Logging} in the {Internet} of {Things}},
	isbn = {978-1-891562-49-5},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-2_Wang_paper.pdf},
	doi = {10.14722/ndss.2018.23282},
	abstract = {As the Internet of Things (IoT) continues to proliferate, diagnosing incorrect behavior within increasinglyautomated homes becomes considerably more difﬁcult. Devices and apps may be chained together in long sequences of triggeraction rules to the point that from an observable symptom (e.g., an unlocked door) it may be impossible to identify the distantly removed root cause (e.g., a malicious app). This is because, at present, IoT audit logs are siloed on individual devices, and hence cannot be used to reconstruct the causal relationships of complex workﬂows. In this work, we present ProvThings, a platform-centric approach to centralized auditing in the Internet of Things. ProvThings performs efﬁcient automated instrumentation of IoT apps and device APIs in order to generate data provenance that provides a holistic explanation of system activities, including malicious behaviors. We prototype ProvThings for the Samsung SmartThings platform, and benchmark the efﬁcacy of our approach against a corpus of 26 IoT attacks. Through the introduction of a selective code instrumentation optimization, we demonstrate in evaluation that ProvThings imposes just 5\% overhead on physical IoT devices while enabling real time querying of system behaviors, and further consider how ProvThings can be leveraged to meet the needs of a variety of stakeholders in the IoT ecosystem.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Wang, Qi and Hassan, Wajih Ul and Bates, Adam and Gunter, Carl},
	year = {2018},
}

@inproceedings{nguyen_iotsan_2018,
	address = {New York, NY, USA},
	series = {{CoNEXT} '18},
	title = {{IotSan}: fortifying the safety of {IoT} systems},
	isbn = {978-1-4503-6080-7},
	shorttitle = {{IotSan}},
	url = {https://doi.org/10.1145/3281411.3281440},
	doi = {10.1145/3281411.3281440},
	abstract = {Today's IoT systems include event-driven smart applications (apps) that interact with sensors and actuators. A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states. Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. In this paper, we design IotSan, a novel practical system that uses model checking as a building block to reveal "interaction-level" flaws by identifying events that can lead the system to unsafe states. In building IotSan, we design novel techniques tailored to IoT systems, to alleviate the state explosion associated with model checking. IotSan also automatically translates IoT apps into a format amenable to model checking. Finally, to understand the root cause of a detected vulnerability, we design an attribution mechanism to identify problematic and potentially malicious apps. We evaluate IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities. We also evaluate IotSan with malicious SmartThings apps from a previous effort. IotSan detects the potential safety violations and also effectively attributes these apps as malicious.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 14th {International} {Conference} on emerging {Networking} {EXperiments} and {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Dang Tu and Song, Chengyu and Qian, Zhiyun and Krishnamurthy, Srikanth V. and Colbert, Edward J. M. and McDaniel, Patrick},
	month = dec,
	year = {2018},
	pages = {191--203},
}

@inproceedings{ding_safety_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT{\textasciitilde}environment.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	keywords = {internet of things, physical interaction control, safety},
	pages = {832--846},
}

@inproceedings{trimananda_understanding_2020,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Understanding and automatically detecting conflicting interactions between smart home {IoT} applications},
	isbn = {978-1-4503-7043-1},
	url = {https://doi.org/10.1145/3368089.3409682},
	doi = {10.1145/3368089.3409682},
	abstract = {Smart home devices provide the convenience of remotely control-ling and automating home appliances. The most advanced smart home environments allow developers to write apps to make smart home devices work together to accomplish tasks, e.g., home security and energy conservation. A smart home app typically implements narrow functionality and thus to fully implement desired functionality homeowners may need to install multiple apps. These different apps can conflict with each other and these conflicts can result in undesired actions such as locking the door during a fire. In this paper, we study conflicts between apps on Samsung SmartThings, the most popular platform for developing and deploying smart home IoT devices. By collecting and studying 198 official and 69 third-party apps, we found significant app conflicts in 3 categories: (1) close to 60\% of app pairs that access the same device, (2) more than 90\% of app pairs with physical interactions, and (3) around 11\% of app pairs that access the same global variable. Our results suggest that the problem of conflicts between smart home apps is serious and can create potential safety risks. We then developed a conflict detection tool that uses model checking to automatically detect up to 96\% of the conflicts.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Trimananda, Rahmadi and Aqajari, Seyed Amir Hossein and Chuang, Jason and Demsky, Brian and Xu, Guoqing Harry and Lu, Shan},
	month = nov,
	year = {2020},
	keywords = {concurrency, model checking, program analysis, smart home apps},
	pages = {1215--1227},
}

@inproceedings{gu_iotgaze_2020,
	title = {{IoTGaze}: {IoT} {Security} {Enforcement} via {Wireless} {Context} {Analysis}},
	shorttitle = {{IoTGaze}},
	doi = {10.1109/INFOCOM41043.2020.9155459},
	abstract = {Internet of Things (IoT) has become the most promising technology for service automation, monitoring, and interconnection, etc. However, the security and privacy issues caused by IoT arouse concerns. Recent research focuses on addressing security issues by looking inside platform and apps. In this work, we creatively change the angle to consider security problems from a wireless context perspective. We propose a novel framework called IoTGaze, which can discover potential anomalies and vulnerabilities in the IoT system via wireless traffic analysis. By sniffing the encrypted wireless traffic, IoTGaze can automatically identify the sequential interaction of events between apps and devices. We discover the temporal event dependencies and generate the Wireless Context for the IoT system. Meanwhile, we extract the IoT Context, which reflects user's expectation, from IoT apps' descriptions and user interfaces. If the wireless context does not match the expected IoT context, IoTGaze reports an anomaly. Furthermore, IoTGaze can discover the vulnerabilities caused by the inter-app interaction via hidden channels, such as temperature and illuminance. We provide a proof-of-concept implementation and evaluation of our framework on the Samsung SmartThings platform. The evaluation shows that IoTGaze can effectively discover anomalies and vulnerabilities, thereby greatly enhancing the security of IoT systems.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Gu, Tianbo and Fang, Zheng and Abhishek, Allaukik and Fu, Hao and Hu, Pengfei and Mohapatra, Prasant},
	month = jul,
	year = {2020},
	note = {ISSN: 2641-9874},
	keywords = {Anomaly Detection, Communication system security, Feature extraction, Internet of Things, IoT Security, Natural Language Processing, Privacy, Security, Surveillance, Wireless Context, Wireless communication, Wireless sensor networks},
	pages = {884--893},
}

@article{sulaman_comparison_2019,
	title = {Comparison of the {FMEA} and {STPA} safety analysis methods–a case study},
	volume = {27},
	issn = {1573-1367},
	url = {https://doi.org/10.1007/s11219-017-9396-0},
	doi = {10.1007/s11219-017-9396-0},
	abstract = {As our society becomes more and more dependent on IT systems, failures of these systems can harm more and more people and organizations. Diligently performing risk and hazard analysis helps to minimize the potential harm of IT system failures on the society and increases the probability of their undisturbed operation. Risk and hazard analysis is an important activity for the development and operation of critical software intensive systems, but the increased complexity and size puts additional requirements on the effectiveness of risk and hazard analysis methods. This paper presents a qualitative comparison of two hazard analysis methods, failure mode and effect analysis (FMEA) and system theoretic process analysis (STPA), using case study research methodology. Both methods have been applied on the same forward collision avoidance system to compare the effectiveness of the methods and to investigate what are the main differences between them. Furthermore, this study also evaluates the analysis process of both methods by using a qualitative criteria derived from the technology acceptance model (TAM). The results of the FMEA analysis were compared to the results of the STPA analysis, which were presented in a previous study. Both analyses were conducted on the same forward collision avoidance system. The comparison shows that FMEA and STPA deliver similar analysis results.},
	language = {en},
	number = {1},
	urldate = {2021-09-21},
	journal = {Software Quality Journal},
	author = {Sulaman, Sardar Muhammad and Beer, Armin and Felderer, Michael and Höst, Martin},
	month = mar,
	year = {2019},
	pages = {349--387},
}

@inproceedings{schmittner_case_2015,
	address = {New York, NY, USA},
	series = {{CPSS} '15},
	title = {A {Case} {Study} of {FMVEA} and {CHASSIS} as {Safety} and {Security} {Co}-{Analysis} {Method} for {Automotive} {Cyber}-physical {Systems}},
	isbn = {978-1-4503-3448-8},
	url = {http://doi.org/10.1145/2732198.2732204},
	doi = {10.1145/2732198.2732204},
	abstract = {The increasing integration of computational components and physical systems creates cyber-physical system, which provide new capabilities and possibilities for humans to control and interact with physical machines. However, the correlation of events in cyberspace and physical world also poses new safety and security challenges. This calls for holistic approaches to safety and security analysis for the identification of safety failures and security threats and a better understanding of their interplay. This paper presents the application of two promising methods, i.e. Failure Mode, Vulnerabilities and Effects Analysis (FMVEA) and Combined Harm Assessment of Safety and Security for Information Systems (CHASSIS), to a case study of safety and security co-analysis of cyber-physical systems in the automotive domain. We present the comparison, discuss their applicabilities, and identify future research needs.},
	urldate = {2021-09-21},
	booktitle = {Proceedings of the 1st {ACM} {Workshop} on {Cyber}-{Physical} {System} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Schmittner, Christoph and Ma, Zhendong and Schoitsch, Erwin and Gruber, Thomas},
	month = apr,
	year = {2015},
	keywords = {automotive, cyber-physical system, safety and security co-analysis, systems engineering},
	pages = {69--80},
}

@article{kriaa_comparing_2013,
	title = {Comparing {Two} {Approaches} to {Safety} and {Security} {Modelling}: {BDMP} {Technique} and {CHASSIS} {Method}},
	abstract = {In the context of risk analysis for critical systems, it is recognized an emerging need for combining safety and security aspects. Safety critical systems have traditionally been isolated, but with the development of digital industrial control systems they become interconnected and thereby more exposed to cyber security threats. In this paper we focus on combining safety and security aspects during the development of such systems by comparing two of few existing approaches that allow modelling of both aspects. The two methods – Combined Harm Assessment of Safety and Security for Information Systems (CHASSIS) and Boolean logic Driven Markov Processes (BDMP) - are based on extensions of two different approaches, UML and fault/attack trees respectively, and provide a risk assessment framework for critical systems, whose failure or malfunction can have a large negative impact on humans, the environment or economic assets. A system transporting a polluting substance has been modelled with both approaches as a basis for the comparison. It mainly emphasizes the complementarities of both approaches and their possible combination. The results suggest that such a combination of the two approaches is viable and promising.},
	language = {en},
	author = {Kriaa, Siwar and Raspotnig, Christian and Bouissou, Marc and Piètre-Cambacedes, Ludovic and Halgand, Yoran and Katta, Vikash},
	year = {2013},
	pages = {18},
}

@misc{kriaa_comparing_2013-1,
	title = {Comparing {Two} {Approaches} to {Safety} and {Security} {Modelling} : {BDMP} {Technique} and {CHASSIS} {Method}},
	shorttitle = {Comparing {Two} {Approaches} to {Safety} and {Security} {Modelling}},
	url = {https://www.semanticscholar.org/paper/Comparing-Two-Approaches-to-Safety-and-Security-%3A-Kriaa-Raspotnig/d356c60b7a901c5575b7d8bc96349e432c7c5314},
	abstract = {In the context of risk analysis for critical systems, it is recognized an emerging need for combining safety and security aspects. Safety critical systems have traditionally been isolated, but with the development of digital industrial control systems they become interconnected and thereby more exposed to cyber security threats. In this paper we focus on combining safety and security aspects during the development of such systems by comparing two of few existing approaches that allow modelling of both aspects. The two methods – Combined Harm Assessment of Safety and Security for Information Systems (CHASSIS) and Boolean logic Driven Markov Processes (BDMP) are based on extensions of two different approaches, UML and fault/attack trees respectively, and provide a risk assessment framework for critical systems, whose failure or malfunction can have a large negative impact on humans, the environment or economic assets. A system transporting a polluting substance has been modelled with both approaches as a basis for the comparison. It mainly emphasizes the complementarities of both approaches and their possible combination. The results suggest that such a combination of the two approaches is viable and promising.},
	language = {en},
	urldate = {2021-09-21},
	author = {Kriaa, Siwar and Raspotnig, Christian and Bouissou, M. and Piètre-Cambacédès, L. and Peter and Karpati and Halgand, Yoran and Katta, Vikash},
	year = {2013},
}

@inproceedings{young_systems_2013,
	address = {New Orleans Louisiana USA},
	title = {Systems thinking for safety and security},
	isbn = {978-1-4503-2015-3},
	url = {https://dl.acm.org/doi/10.1145/2523649.2530277},
	doi = {10.1145/2523649.2530277},
	abstract = {The fundamental challenge facing security professionals is preventing losses, be they operational, financial or mission losses. As a result, one could argue that security professionals share this challenge with safety professionals. Despite their shared challenge, there is little evidence that recent advances that enable one community to better prevent losses have been shared with the other for possible implementation. Limitations in current safety approaches have led researchers and practitioners to develop new models and techniques. These techniques could potentially benefit the field of security. This paper describes a new systems thinking approach to safety that may be suitable for meeting the challenge of securing complex systems against cyber disruptions. SystemsTheoretic Process Analysis for Security (STPA-Sec) augments traditional security approaches by introducing a top-down analysis process designed to help a multidisciplinary team consisting of security, operations, and domain experts identify and constrain the system from entering vulnerable states that lead to losses. This new framework shifts the focus of the security analysis away from threats as the proximate cause of losses and focuses instead on the broader system structure that allowed the system to enter a vulnerable system state that the threat exploits to produce the disruption leading to the loss.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Proceedings of the 29th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Young, William and Leveson, Nancy},
	month = dec,
	year = {2013},
	pages = {1--8},
}

@article{kavallieratos_cybersecurity_2020,
	title = {Cybersecurity and {Safety} {Co}-{Engineering} of {Cyberphysical} {Systems}—{A} {Comprehensive} {Survey}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1999-5903/12/4/65},
	doi = {10.3390/fi12040065},
	abstract = {Safeguarding both safety and cybersecurity is paramount to the smooth and trustworthy operation of contemporary cyber physical systems, many of which support critical functions and services. As safety and security have been known to be interdependent, they need to be jointly considered in such systems. As a result, various approaches have been proposed to address safety and cybersecurity co-engineering in cyber physical systems. This paper provides a comprehensive survey of safety and cybersecurity co-engineering methods, and discusses relevant open issues and research challenges. Despite the extent of the existing literature, several aspects of the subject still remain to be fully addressed.},
	language = {en},
	number = {4},
	urldate = {2021-09-20},
	journal = {Future Internet},
	author = {Kavallieratos, Georgios and Katsikas, Sokratis and Gkioulos, Vasileios},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {co-engineering, cyber physical systems, cybersecurity, safety},
	pages = {65},
}

@inproceedings{sabaliauskaite_integrating_2017,
	title = {Integrating {Six}-{Step} {Model} with {Information} {Flow} {Diagrams} for {Comprehensive} {Analysis} of {Cyber}-{Physical} {System} {Safety} and {Security}},
	doi = {10.1109/HASE.2017.25},
	abstract = {An approach for integrating Six-Step Model (SSM) with Information Flow Diagrams (IFDs) is proposed. SSM is a model for Cyber-Physical System (CPS) safety and security analysis, which incorporates six hierarchies of CPS, namely, functions, structure, failures, safety countermeasures, cyber-attacks, and security countermeasures. Relationship matrices are used in SSM to identify inter-relationships between these hierarchies and determine the effect of failures and cyber-attacks on CPSs. Although SSM is a useful tool for CPS safety and security modeling, it lacks guidance for identifying failures and attacks, and selecting adequate set of safety and security countermeasures. To address this issue, an approach for integrating SSM with IFDs is proposed and explained using the water treatment system example.},
	booktitle = {2017 {IEEE} 18th {International} {Symposium} on {High} {Assurance} {Systems} {Engineering} ({HASE})},
	author = {Sabaliauskaite, Giedre and Adepu, Sridhar},
	month = jan,
	year = {2017},
	note = {ISSN: 1530-2059},
	keywords = {3-Step Model, Analytical models, Communication channels, Computational modeling, GTST-MLD, ISA-99, Information Flow Diagram, Safety, Security, Sensors, Six-Step Model, cyber-attacks, cyber-physical system, failures, safety, security},
	pages = {41--48},
}

@article{carreras_guzman_comparative_2021,
	title = {A {Comparative} {Study} of {STPA}-{Extension} and the {UFoI}-{E} {Method} for {Safety} and {Security} {Co}-analysis},
	volume = {211},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832021001745},
	doi = {10.1016/j.ress.2021.107633},
	abstract = {Emerging challenges in cyber-physical systems (CPSs) have been encouraging the development of safety and security co-analysis methods. These methods aim at mitigating the new risks associated with the convergence of safety-related systemic flaws and security-related cyber-attacks that have led to major losses in CPSs. Although several studies have reviewed existing safety and security co-analysis methods, only a few empirical studies have attempted to compare their strengths and limitations to guide risk analysis in practice. This paper bridges the gap between two novel safety and security co-analysis methods and their practical implementations. Namely, this paper compares a novel extension of the System-Theoretic Process Analysis (STPA-Extension) and the Uncontrolled Flows of Information and Energy (UFoI-E) method through a common case study. In our case study, the CPS under analysis is a conceptual autonomous ship. We conducted our comparative study as two independent teams to guarantee that the implementation of one method did not influence the other method. Furthermore, we developed a comparative framework that evaluates the relative completeness and the effort required in each analysis. Finally, we propose a tailored combination of these methods, exploiting their unique strengths to achieve more complete and cost-effective risk analysis results.},
	language = {en},
	urldate = {2021-09-20},
	journal = {Reliability Engineering \& System Safety},
	author = {Carreras Guzman, Nelson H. and Zhang, Jin and Xie, Jing and Glomsrud, Jon Arne},
	month = jul,
	year = {2021},
	keywords = {Safety and security, comparative study, cyber-physical systems (CPSs), autonomous ship, risk identification},
	pages = {107633},
}

@inproceedings{kaneko_stamp_2020,
	title = {{STAMP} {S} amp;{S}: {Safety} amp; {Security} {Scenario} for {Specification} and {Standard} in the society of {AI}/{IoT}},
	shorttitle = {{STAMP} {S} amp;{S}},
	doi = {10.1109/QRS-C51114.2020.00037},
	abstract = {Systems, including AI/IoT, have complex relationships. It is necessary to analyze risks from various perspectives to build a system that can be used safely and securely throughout society, including people and organizations. In order to model a complex system hierarchically, we propose to model the control structure diagram of STAMP into five layers according to the life cycle of software and system requirements. Based on the STAMP S \& S five-layer model, the necessity of performing safety and security analysis in each layer and performing scenario analysis to generate specifications and standards ensuring safety and security is presented.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Kaneko, Tomoko and Yoshioka, Nobukazu and Sasaki, Ryoichi},
	month = dec,
	year = {2020},
	keywords = {Analytical models, Artificial intelligence, Complex systems, STAMP, Safety, Scenario, Security, Specification, Standard, Standards, Task analysis},
	pages = {168--175},
}

@book{internationale_elektrotechnische_kommission_industrial-process_2019,
	address = {Geneva},
	edition = {Edition 1.0, 2019-05},
	series = {{IEC} technical report},
	title = {Industrial-process measurement, control and automation: framework for functional safety and security},
	isbn = {978-2-8322-6925-1},
	shorttitle = {Industrial-process measurement, control and automation},
	language = {en},
	number = {63069},
	publisher = {IEC Central Office},
	editor = {Internationale Elektrotechnische Kommission},
	year = {2019},
}

@inproceedings{wang2019looking,
	title = {Looking from the mirror: {Evaluating} iot device security through mobile companion apps},
	booktitle = {28th \{{USENIX}\} security symposium (\{{USENIX}\} security 19)},
	author = {Wang, Xueqiang and Sun, Yuqiong and Nanda, Susanta and Wang, XiaoFeng},
	year = {2019},
	pages = {1151--1167},
}

@inproceedings{chen_iotfuzzer_2018,
	address = {San Diego, CA},
	title = {{IoTFuzzer}: {Discovering} {Memory} {Corruptions} in {IoT} {Through} {App}-based {Fuzzing}},
	isbn = {978-1-891562-49-5},
	shorttitle = {{IoTFuzzer}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-1_Chen_paper.pdf},
	doi = {10.14722/ndss.2018.23159},
	abstract = {With more IoT devices entering the consumer market, it becomes imperative to detect their security vulnerabilities before an attacker does. Existing binary analysis based approaches only work on ﬁrmware, which is less accessible except for those equipped with special tools for extracting the code from the device. To address this challenge in IoT security analysis, we present in this paper a novel automatic fuzzing framework, called IOTFUZZER, which aims at ﬁnding memory corruption vulnerabilities in IoT devices without access to their ﬁrmware images. The key idea is based upon the observation that most IoT devices are controlled through their ofﬁcial mobile apps, and such an app often contains rich information about the protocol it uses to communicate with its device. Therefore, by identifying and reusing program-speciﬁc logic (e.g., encryption) to mutate the test case (particularly message ﬁelds), we are able to effectively probe IoT targets without relying on any knowledge about its protocol speciﬁcations. In our research, we implemented IOTFUZZER and evaluated 17 real-world IoT devices running on different protocols, and our approach successfully identiﬁed 15 memory corruption vulnerabilities (including 8 previously unknown ones).},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Chen, Jiongyi and Diao, Wenrui and Zhao, Qingchuan and Zuo, Chaoshun and Lin, Zhiqiang and Wang, XiaoFeng and Lau, Wing Cheong and Sun, Menghan and Yang, Ronghai and Zhang, Kehuan},
	year = {2018},
}

@incollection{abrahamsson_use_2015,
	address = {Cham},
	title = {On the {Use} of {Safety} {Certification} {Practices} in {Autonomous} {Field} {Robot} {Software} {Development}: {A} {Systematic} {Mapping} {Study}},
	volume = {9459},
	isbn = {978-3-319-26843-9 978-3-319-26844-6},
	shorttitle = {On the {Use} of {Safety} {Certification} {Practices} in {Autonomous} {Field} {Robot} {Software} {Development}},
	url = {http://link.springer.com/10.1007/978-3-319-26844-6_25},
	abstract = {Robotics has recently seen an increasing development, and the areas addressed within robotics has extended into domains we consider safety-critical, fostering the development of standards that facilitate the development of safe robots. Safety standards describe concepts to maintain desired reactions or performance in malfunctioning systems, and inﬂuence industry regarding software development and project management. However, academia seemingly did not reach the same degree of utilisation of standards. This paper presents the ﬁndings from a systematic mapping study in which we study the state-of-the-art in developing software for safety-critical software for autonomous ﬁeld robots. The purpose of the study is to identify practices used for the development of autonomous ﬁeld robots and how these practices relate to available safety standards. Our ﬁndings from reviewing 49 papers show that standards, if at all, are barely used. The majority of the papers propose various solutions to achieve safety, and about half of the papers refer to non-standardised approaches that mainly address the methodical rather than the development level. The present study thus shows an emerging ﬁeld still on the quest for suitable approaches to develop safety-critical software, awaiting appropriate standards for this support.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Ingibergsson, Johann Thor Mogensen and Schultz, Ulrik Pagh and Kuhrmann, Marco},
	editor = {Abrahamsson, Pekka and Corral, Luis and Oivo, Markku and Russo, Barbara},
	year = {2015},
	doi = {10.1007/978-3-319-26844-6_25},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {335--352},
}

@article{ebeid_survey_2018,
	title = {A survey of {Open}-{Source} {UAV} flight controllers and flight simulators},
	volume = {61},
	issn = {01419331},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933118300930},
	doi = {10.1016/j.micpro.2018.05.002},
	abstract = {The current disruptive innovation in civilian drone (UAV) applications has led to an increased need for research and development in UAV technology. The key challenges currently being addressed are related to UAV platform properties such as functionality, reliability, fault tolerance, and endurance, which are all tightly linked to the UAV ﬂight controller hardware and software. The lack of standardization of ﬂight controller architectures and the use of proprietary closed-source ﬂight controllers on many UAV platforms, however, complicates this work: solutions developed for one ﬂight controller may be diﬃcult to port to another without substantial extra development and testing. Using open-source ﬂight controllers mitigates some of these challenges and enables other researchers to validate and build upon existing research.},
	language = {en},
	urldate = {2021-09-20},
	journal = {Microprocessors and Microsystems},
	author = {Ebeid, Emad and Skriver, Martin and Terkildsen, Kristian Husum and Jensen, Kjeld and Schultz, Ulrik Pagh},
	month = sep,
	year = {2018},
	keywords = {UAVs},
	pages = {11--20},
}

@article{albonico_mining_2021,
	title = {Mining {Energy}-{Related} {Practices} in {Robotics} {Software}},
	url = {http://arxiv.org/abs/2103.13762},
	abstract = {Robots are becoming more and more commonplace in many industry settings. This successful adoption can be partly attributed to (1) their increasingly affordable cost and (2) the possibility of developing intelligent, software-driven robots. Unfortunately, robotics software consumes signiﬁcant amounts of energy. Moreover, robots are often battery-driven, meaning that even a small energy improvement can help reduce its energy footprint and increase its autonomy and user experience.},
	language = {en},
	urldate = {2021-09-20},
	journal = {arXiv:2103.13762 [cs]},
	author = {Albonico, Michel and Malavolta, Ivano and Pinto, Gustavo and Guzman, Emitza and Chinnappan, Katerina and Lago, Patricia},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.13762},
	keywords = {Computer Science - Robotics, Computer Science - Software Engineering},
}

@inproceedings{fischer-nielsen_forgotten_2020,
	address = {Seoul South Korea},
	title = {The forgotten case of the dependency bugs: on the example of the robot operating system},
	isbn = {978-1-4503-7123-0},
	shorttitle = {The forgotten case of the dependency bugs},
	url = {https://dl.acm.org/doi/10.1145/3377813.3381364},
	doi = {10.1145/3377813.3381364},
	abstract = {A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Fischer-Nielsen, Anders and Fu, Zhoulai and Su, Ting and Wąsowski, Andrzej},
	month = jun,
	year = {2020},
	pages = {21--30},
}

@inproceedings{lee_cyber_2008,
	address = {Orlando, FL, USA},
	title = {Cyber {Physical} {Systems}: {Design} {Challenges}},
	isbn = {978-0-7695-3132-8},
	shorttitle = {Cyber {Physical} {Systems}},
	url = {http://ieeexplore.ieee.org/document/4519604/},
	doi = {10.1109/ISORC.2008.25},
	abstract = {Cyber-Physical Systems (CPS) are integrations of computation and physical processes. Embedded computers and networks monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa. The economic and societal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in generalpurpose computing. Moreover, physical components are qualitatively different from object-oriented software components. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether today’s computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufﬁcient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today’s abstractions. To realize the full potential of CPS, we will have to rebuild computing and networking abstractions. These abstractions will have to embrace physical dynamics and computation in a uniﬁed way.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {2008 11th {IEEE} {International} {Symposium} on {Object} and {Component}-{Oriented} {Real}-{Time} {Distributed} {Computing} ({ISORC})},
	publisher = {IEEE},
	author = {Lee, Edward A.},
	month = may,
	year = {2008},
	pages = {363--369},
}

@inproceedings{reddy_cloud-based_2014,
	address = {Maui, HI, USA},
	title = {Cloud-{Based} {Cyber} {Physical} {Systems}: {Design} {Challenges} and {Security} {Needs}},
	isbn = {978-1-4799-7394-1},
	shorttitle = {Cloud-{Based} {Cyber} {Physical} {Systems}},
	url = {http://ieeexplore.ieee.org/document/7051787/},
	doi = {10.1109/MSN.2014.50},
	abstract = {The cyber-physical systems are the combination of computational elements and physical entities that can interact with humans through many modalities. The security includes the malicious attempts by adversary that disrupts or fails the functions of physical systems and affects infrastructure, businesses, and routine human life. The research in cyberphysical systems is in its initial stage. Therefore, first we discussed the status of security in cloud cyber-physical systems. Second, we introduced the challenges ahead to the design and development of the future engineering systems with new security capabilities. Third, we presented the security requirements in Hadoop distributed file systems. Since trustbased packet transfer in sensor networks is one of the important security issue infrastructure security, we presented a trust-based approach using Sporas formula and presented the simulations to trust of a successive node before transferring the packets. Finally, the paper presents the future research on cyber-physical systems in the cloud environment.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {2014 10th {International} {Conference} on {Mobile} {Ad}-hoc and {Sensor} {Networks}},
	publisher = {IEEE},
	author = {Reddy, Yenumula B.},
	month = dec,
	year = {2014},
	pages = {315--322},
}

@inproceedings{fitzgerald_cyber-physical_2015,
	address = {Florence, Italy},
	title = {Cyber-{Physical} {Systems} {Design}: {Formal} {Foundations}, {Methods} and {Integrated} {Tool} {Chains}},
	isbn = {978-1-4673-7043-1},
	shorttitle = {Cyber-{Physical} {Systems} {Design}},
	url = {http://ieeexplore.ieee.org/document/7166696/},
	doi = {10.1109/FormaliSE.2015.14},
	abstract = {The engineering of dependable cyber-physical systems (CPSs) is inherently collaborative, demanding cooperation between diverse disciplines. A goal of current research is the development of integrated tool chains for model-based CPS design that support co-modelling, analysis, co-simulation, testing and implementation. We discuss the role of formal methods in addressing three key aspects of this goal: providing reasoning support for semantically heterogeneous models, managing the complexity and scale of design space exploration, and supporting traceability and provenance in the CPS design set. We brieﬂy outline an approach to the development of such a tool chain based on existing tools and discuss ongoing challenges and open research questions in this area.},
	language = {en},
	urldate = {2021-09-20},
	booktitle = {2015 {IEEE}/{ACM} 3rd {FME} {Workshop} on {Formal} {Methods} in {Software} {Engineering}},
	publisher = {IEEE},
	author = {Fitzgerald, John and Gamble, Carl and Larsen, Peter Gorm and Pierce, Kenneth and Woodcock, Jim},
	month = may,
	year = {2015},
	pages = {40--46},
}

@inproceedings{ding_safety_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT{\textasciitilde}environment.},
	urldate = {2021-09-17},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	keywords = {internet of things, physical interaction control, safety},
	pages = {832--846},
}

@article{subramanian_quantitative_2016,
	title = {Quantitative {Assessment} of {Safety} and {Security} of {System} {Architectures} for {Cyberphysical} {Systems} {Using} the {NFR} {Approach}},
	volume = {10},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2013.2294628},
	abstract = {Cyberphysical systems (CPSs) are an integral part of modern societies since most critical infrastructures are controlled by these systems. CPSs incorporate computer-based and network-based technologies for the monitoring and control of physical processes. Two critically important properties of CPSs are safety and security. It is widely accepted that properties such as safety and security should be considered at the system design phase itself, particularly at the architectural level wherein such properties are embedded in the final system. However, safety and security are interrelated, and there seems to be a lack of techniques that consider both of them together. The nonfunctional requirement (NFR) approach is a technique that allows the simultaneous evaluation of both safety and security at the architectural level. In this paper, we apply the NFR approach to quantitatively evaluate the safety and security properties of an example CPS, i.e., an oil pipeline control system. We conclude that the NFR approach provides practical results that can be used by designers and developers to create safe and secure CPSs.},
	number = {2},
	journal = {IEEE Systems Journal},
	author = {Subramanian, Nary and Zalewski, Janusz},
	month = jun,
	year = {2016},
	note = {Conference Name: IEEE Systems Journal},
	keywords = {Communication system security, Cyberphysical systems (CPSs), Monitoring, Pipelines, Safety, Security, Wireless communication, Wireless sensor networks, nonfunctional requirement (NFR) approach, safety, security, system architecture assessment},
	pages = {397--409},
}

@article{wang_hyperscan_nodate,
	title = {Hyperscan: {A} {Fast} {Multi}-pattern {Regex} {Matcher} for {Modern} {CPUs}},
	abstract = {Regular expression matching serves as a key functionality of modern network security applications. Unfortunately, it often becomes the performance bottleneck as it involves compute-intensive scan of every byte of packet payload. With trends towards increasing network bandwidth and a large ruleset of complex patterns, the performance requirement gets ever more demanding. In this paper, we present Hyperscan, a high performance regular expression matcher for commodity server machines. Hyperscan employs two core techniques for efficient pattern matching. First, it exploits graph decomposition that translates regular expression matching into a series of string and finite automata matching. Unlike existing solutions, string matching becomes a part of regular expression matching, eliminating duplicate operations. Decomposed regular expression components also increase the chance of fast DFA matching as they tend to be smaller than the original pattern. Second, Hyperscan accelerates both string and finite automata matching using SIMD operations, which brings substantial throughput improvement. Our evaluation shows that Hyperscan improves the performance of Snort by a factor of 8.7 for a real traffic trace.},
	language = {en},
	author = {Wang, Xiang and Hong, Yang and Chang, Harry and Langdale, Geoff and Hu, Jiayu},
	pages = {19},
}

@article{biro_safe_2021,
	title = {Safe and secure cyber-physical systems},
	volume = {33},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2340},
	doi = {10.1002/smr.2340},
	abstract = {Cyber-Physical Systems (CPSs) differ from traditional Information Technology (IT) systems in such a way that they interact with the physical environment, i.e., they can monitor and manipulate real objects and processes. For this special issue, the authors of the best papers of IWCFS 2019 were invited to submit extended versions of their workshop papers. Additionally, we received eight submissions from around the globe as a result of an open call. After thorough and stringent reviews, we selected six articles that provide relevant contributions to the field of safety and security for CPSs.},
	language = {en},
	number = {9},
	urldate = {2021-09-16},
	journal = {Journal of Software: Evolution and Process},
	author = {Biró, Miklós and Mashkoor, Atif and Sametinger, Johannes},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2340},
	keywords = {cyber-physical system, safety, security},
	pages = {e2340},
}

@article{ponsard_goal-driven_2021,
	title = {A goal-driven approach for the joint deployment of safety and security standards for operators of essential services},
	volume = {33},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2338},
	doi = {10.1002/smr.2338},
	abstract = {Designing safety-critical software in domains ensuring essential services like transportation, energy, or health requires high assurance techniques and compliance with domain specific standards. As a result of the global interconnectivity and the evolution toward cyber-physical systems, the increasing exposure to cyber threats calls for the adoption of cyber security standards and frameworks. Although safety and security have different cultures, both fields share similar concepts and tools and are worth being investigated together. This paper provides the background to understand emerging co-engineering approaches. It advocates for the use of a model-based approach to provide a sound risk-oriented process and to capture rationales interconnecting top-level standards/directives to concrete safety/security measures. We show the benefits of adopting goal-oriented analysis that can be transposed later to domain-specific frameworks. Both qualitative and quantitative reasoning aspects are analyzed and discussed, especially to support trade-off analysis. Our work is driven by a representative case study in drinking water utility in the scope of the NIS regulation for operator of essential services.},
	language = {en},
	number = {9},
	urldate = {2021-09-16},
	journal = {Journal of Software: Evolution and Process},
	author = {Ponsard, Christophe and Grandclaudon, Jeremy and Massonet, Philippe},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2338},
	keywords = {NIS directive, co-engineering, cyber security, risk management, safety analysis, standards},
	pages = {e2338},
}

@article{lyu_safety_2019,
	title = {Safety and security risk assessment in cyber-physical systems},
	volume = {4},
	issn = {2398-3396},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-cps.2018.5068},
	doi = {10.1049/iet-cps.2018.5068},
	abstract = {The term cyber physical systems (CPS) refers to a new generation of systems with integrated computational and physical capabilities through computation, communication, and control. In the past decades, related techniques for CPS have been well studied and developed, and are widely applied in the fields such as industrial automation, smart transportation, aerospace, environment monitoring, and smart grids. However, with the expansion of CPS complexity and the enhancement of the system openness, most of CPS become not only safety-critical but also security-critical since deeply involving both physical objects and computer networks. In the last decade, it is no longer rare to see safety incidents and security attacks happening in industries. Safety and security issues are increasingly converging on CPS, leading to new situations in which these two closely interdependent issues should now be considered together, rather than separately or in sequence. This paper reviews the existing approaches of risk assessment and management from the perspective of safety, security, and their integration. The comparisons of these approaches are summarised with their pros and cons before the technical gaps between the demand and the current situation of safety and security issues in CPS are identified.},
	language = {en},
	number = {3},
	urldate = {2021-09-16},
	journal = {IET Cyber-Physical Systems: Theory \& Applications},
	author = {Lyu, Xiaorong and Ding, Yulong and Yang, Shuang-Hua},
	year = {2019},
	note = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-cps.2018.5068},
	keywords = {CPS complexity, computer networks, cyber-physical systems, integrated computational capabilities, physical capabilities, physical objects, risk management, safety incidents, safety-critical system, security attacks, security of data, security risk assessment, security-critical system, system openness},
	pages = {221--232},
}

@incollection{skavhaug_goal-oriented_2016,
	address = {Cham},
	title = {Goal-{Oriented} {Co}-{Engineering} of {Security} and {Safety} {Requirements} in {Cyber}-{Physical} {Systems}},
	volume = {9923},
	isbn = {978-3-319-45479-5 978-3-319-45480-1},
	url = {http://link.springer.com/10.1007/978-3-319-45480-1_27},
	abstract = {Many safety critical systems are integrating more and more software based systems and are becoming increasingly connected. Such Cyber-Physical Systems require high assurance both on safety and security but also on how such properties aﬀect each other. This covers not only design time aspects but also the run-time: as cyber-security threats evolve constantly, it is necessary to consider how to perform updates of the software without breaking any safety properties. This paper proposes a method to co-engineer them based on sound techniques taken from goal-oriented requirements engineering. The approach is illustrated on a case study from the automotive domain. The case study illustrates the challenges to safety and security co-engineering created by the trend of growing connectivity and the evolution towards more autonomous vehicles in the transportation domain.},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Computer {Safety}, {Reliability}, and {Security}},
	publisher = {Springer International Publishing},
	author = {Ponsard, Christophe and Dallons, Gautier and Massonet, Philippe},
	editor = {Skavhaug, Amund and Guiochet, Jérémie and Schoitsch, Erwin and Bitsch, Friedemann},
	year = {2016},
	doi = {10.1007/978-3-319-45480-1_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {334--345},
}

@article{vasisht_farmbeats_nodate,
	title = {{FarmBeats}: {An} {IoT} {Platform} for {Data}-{Driven} {Agriculture}},
	abstract = {Data-driven techniques help boost agricultural productivity by increasing yields, reducing losses and cutting down input costs. However, these techniques have seen sparse adoption owing to high costs of manual data collection and limited connectivity solutions. In this paper, we present FarmBeats, an end-to-end IoT platform for agriculture that enables seamless data collection from various sensors, cameras and drones. FarmBeats’s system design that explicitly accounts for weather-related power and Internet outages has enabled six month long deployments in two US farms.},
	language = {en},
	author = {Vasisht, Deepak and Kapetanovic, Zerina and Won, Jong-ho and Jin, Xinxin and Chandra, Ranveer and Kapoor, Ashish and Sinha, Sudipta N and Sudarshan, Madhusudhan and Stratman, Sean},
	pages = {17},
}

@article{celik_soteria_nodate,
	title = {{SOTERIA}: {Automated} {IoT} {Safety} and {Security} {Analysis}},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital systems have changed the way we live, play and work. Yet existing IoT platforms cannot evaluate whether an IoT app or environment is safe, secure, and operates correctly. In this paper, we present SOTERIA, a static analysis system for validating whether an IoT app or IoT environment (collection of apps working in concert) adheres to identiﬁed safety, security, and functional properties. SOTERIA operates in three phases; (a) translation of platform-speciﬁc IoT source code into an intermediate representation (IR), (b) extracting a state model from the IR, (c) applying model checking to verify desired properties. We evaluate SOTERIA on 65 SmartThings market apps through 35 properties and ﬁnd nine (14\%) individual apps violate ten (29\%) properties. Further, our study of combined app environments uncovered eleven property violations not exhibited in the isolated apps. Lastly, we demonstrate SOTERIA on MALIOT, a novel open-source test suite containing 17 apps with 20 unique violations.},
	language = {en},
	author = {Celik, Z Berkay and McDaniel, Patrick and Tan, Gang},
	pages = {13},
}

@inproceedings{celik_iotguard_2019,
	address = {San Diego, CA},
	title = {{IoTGuard}: {Dynamic} {Enforcement} of {Security} and {Safety} {Policy} in {Commodity} {IoT}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{IoTGuard}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_07A-1_Celik_paper.pdf},
	doi = {10.14722/ndss.2019.23326},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has changed the way we live, play, and work. To date, the traditional approach to securing IoT has treated devices individually. However, in practice, it has been recently shown that the interactions among devices are often the real cause of safety and security violations. In this paper, we present IOTGUARD, a dynamic, policy-based enforcement system for IoT, which protects users from unsafe and insecure device states by monitoring the behavior of IoT and triggeraction platform apps. IOTGUARD operates in three phases: (a) implementation of a code instrumentor that adds extra logic to an app’s source code to collect app’s information at runtime, (b) storing the apps’ information in a dynamic model that represents the runtime execution behavior of apps, and (c) identifying IoT safety and security policies, and enforcing relevant policies on the dynamic model of individual apps or sets of interacting apps. We demonstrate IOTGUARD on 20 ﬂawed apps and ﬁnd that IOTGUARD correctly enforces 12 of the 12 policy violations. In addition, we evaluate IOTGUARD on 35 SmartThings IoT and 30 IFTTT trigger-action platform market apps executed in a simulated smart home. IOTGUARD enforces 11 unique policies and blocks 16 states in six (17.1\%) SmartThings and ﬁve (16.6\%) IFTTT apps. IOTGUARD imposes only 17.3\% runtime overhead on an app and 19.8\% for ﬁve interacting apps. Through this effort, we introduce a rigorously grounded system for enforcing correct operation of IoT devices through systematically identiﬁed IoT policies, demonstrating the effectiveness and value of monitoring IoT apps with tools such as IOTGUARD.},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Celik, Z. Berkay and Tan, Gang and McDaniel, Patrick},
	year = {2019},
}

@inproceedings{sokolowski_automating_2021,
	address = {Athens Greece},
	title = {Automating serverless deployments for {DevOps} organizations},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468575},
	doi = {10.1145/3468264.3468575},
	abstract = {DevOps unifies software development and operations in cross-functional teams to improve software delivery and operations (SDO) performance. Ideally, cross-functional DevOps teams independently deploy their services, but the correct operation of a service often demands other services, requiring coordination to ensure the correct deployment order. This issue is currently solved either with a central deployment or manual out-of-band communication across teams, e.g., via phone, chat, or email. Unfortunately, both contradict the independence of teams, hindering SDO performanceÐthe reason why DevOps is adopted in the first place. In this work, we conduct a study on 73 IT professionals, showing that, in practice, they resort to manual coordination for correct deployments even if they expect better SDO performance with fully automated approaches. To address this issue, we propose µs ([mju:z] łmusež ), a novel IaC system automating deployment coordination in a fully decentralized fashion, still retaining compatibility with the DevOps practiceÐin contrast to today’s solutions. We implement µs, demonstrate that it effectively enables automated coordination, introduces negligible definition overhead, has no performance overhead, and is broadly applicable, as shown by the migration of 64 third-party IaC projects.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Sokolowski, Daniel and Weisenburger, Pascal and Salvaneschi, Guido},
	month = aug,
	year = {2021},
	pages = {57--69},
}

@article{melzner_case_2013,
	title = {A case study on automated safety compliance checking to assist fall protection design and planning in building information models},
	volume = {31},
	issn = {0144-6193, 1466-433X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01446193.2013.780662},
	doi = {10.1080/01446193.2013.780662},
	abstract = {Worldwide occupational safety statistics show that the construction industry in many countries experiences one of the highest accident rates of all industry sectors. Falls remain a major concern as they contribute to very serious injuries or even fatalities on construction projects around the world. Since the standards and rules for protective safety equipment vary by country, the growing numbers of internationally operating companies are in need of tools that allow ubiquitous understanding and planning of safety regardless of the country where they operate. The problem is examined using a customizable automatic safety rule-checking platform for building information models. The applied rule-based checking algorithms are designed to be add-ons to existing building information modelling (BIM) software and can check models for safety hazards early in the design and planning process. Once hazards have been identiﬁed preventative safety equipment can be designed, estimated, and included in the construction schedule before construction starts. A case study implements the safety rule-checking platform on a high-rise building project. Fall protection regulations from both the USA and Germany are applied to the developed rule-checking platform. Visualization of the safety information further explains the differences in the results once country-speciﬁc safety-regulative standards are applied on the same building information model. The case study also indicates that the role of BIM in safety design and planning can effectively assist the traditional safety decision-making process for fall protection equipment.},
	language = {en},
	number = {6},
	urldate = {2021-09-15},
	journal = {Construction Management and Economics},
	author = {Melzner, Jürgen and Zhang, Sijie and Teizer, Jochen and Bargstädt, Hans-Joachim},
	month = jun,
	year = {2013},
	pages = {661--674},
}

@article{fang_framework_2016,
	title = {A framework for real-time pro-active safety assistance for mobile crane lifting operations},
	volume = {72},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580516301807},
	doi = {10.1016/j.autcon.2016.08.025},
	abstract = {Despite many safety considerations addressed in lift pre-planning, the ability to provide real-time safety assistance to crane operators and to mitigate human errors during the lifting operation is missing. This research developed a framework for real-time pro-active safety assistance for mobile crane lifting operations. First, crane poses are reconstructed in real-time based on the critical motions of crane parts captured by a sensor system. Second, as-is lift site conditions are automatically modeled and updated based on point cloud data. Lastly, the risk of colliding the crane parts and lifted load into nearby obstructions is pro-actively analyzed and warnings are provided to the operator through a graphical user interface. A prototype system was developed based on the framework and deployed on a mobile crane. Field test results indicate that the system can accurately reconstruct crane motion in real-time and provide pro-active warnings that allow the operator to make timely decisions to mitigate the risk.},
	language = {en},
	urldate = {2021-09-15},
	journal = {Automation in Construction},
	author = {Fang, Yihai and Cho, Yong K. and Chen, Jingdao},
	month = dec,
	year = {2016},
	pages = {367--379},
}

@inproceedings{qu_experimental_2020,
	title = {An {Experimental} {Study} on {Microservices} based {Edge} {Computing} {Platforms}},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163068},
	abstract = {The rapid technological advances in the Internet of Things (IoT) allows the blueprint of Smart Cities to become feasible by integrating heterogeneous cloud/fog/edge computing paradigms to collaboratively provide variant smart services in our cities and communities. Thanks to attractive features like fine granularity and loose coupling, the microservices architecture has been proposed to provide scalable and extensible services in large scale distributed IoT systems. Recent studies have evaluated and analyzed the performance interference between microservices based on scenarios on the cloud computing environment. However, they are not holistic for IoT applications given the restriction of the edge device like computation consumption and network capacity. This paper investigates multiple microservice deployment policies on edge computing platform. The microservices are developed as docker containers, and comprehensive experimental results demonstrate the performance and interference of microservices running on benchmark scenarios.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	author = {Qu, Qian and Xu, Ronghua and Nikouei, Seyed Yahya and Chen, Yu},
	month = jul,
	year = {2020},
	keywords = {Benchmark testing, Cloud computing, Computer architecture, Container, Containers, Edge Computing, Edge computing, Internet of Things (IoT), Microservices Architecture, Performance evaluation, Service-oriented architecture},
	pages = {836--841},
}

@inproceedings{oconnor_exploring_2016,
	title = {Exploring the {Impact} of {Situational} {Context} — {A} {Case} {Study} of a {Software} {Development} {Process} for a {Microservices} {Architecture}},
	doi = {10.1109/ICSSP.2016.009},
	abstract = {Over the decades, a variety of software development processes have been proposed, each with their own advantages and disadvantages. It is however widely accepted that there is no single process that is perfectly suited to all settings, thus a software process should be molded to the needs of its situational context. In previous work, we have consolidated a substantial body of related research into an initial reference framework of the situational factors affecting the software development process. Practitioners can consult this framework in order to profile their context, a step necessary for effective software process decision making. In this paper, we report on the findings from a case study involving process discovery in a small but successful and growing software development firm. In this organization, which has a focus on continuous software evolution and delivery, we also applied the situational factors reference framework, finding that context is a complex and key informant for software process decisions. Studies of this type highlight the role of situational context in software process definition and evolution, and they raise awareness not just of the importance of situational context, but also of the complexity surrounding software process contexts, a complexity which may not be fully appreciated in all software development settings.},
	booktitle = {2016 {IEEE}/{ACM} {International} {Conference} on {Software} and {System} {Processes} ({ICSSP})},
	author = {O’Connor, Rory and Elger, Peter and Clarke, Paul M.},
	month = may,
	year = {2016},
	keywords = {Agile, Companies, Computer architecture, Containers, Context, Lean, Personnel, Process Selection, Software, Software Development Context, Software Development Process},
	pages = {6--10},
}

@inproceedings{santana_microservices_2018,
	title = {Microservices: {A} {Mapping} {Study} for {Internet} of {Things} {Solutions}},
	shorttitle = {Microservices},
	doi = {10.1109/NCA.2018.8548331},
	abstract = {The Internet of Things (IoT) involves the connectivity of a number of different physical and virtual devices, allowing the development of new services and applications. These intelligent objects, along with their tasks, constitute domain-specific applications (vertical markets), while ubiquitous and analytical services form domain-independent services (horizontal markets). The development of these applications and services in these markets brings challenges such as deployment, scalability, integration, interoperability, mobility and performance. Recent researches indicates that Microservices have been adopted in several domains such as IoT. This paper provides an overview of the current state of the art (studies selected by May 2018) regarding the adoption of Microservices in the development of IoT applications by means of the mapping study methodology. In this context, we present the eighteen selected studies, the existing contributions and the future research perspectives.},
	booktitle = {2018 {IEEE} 17th {International} {Symposium} on {Network} {Computing} and {Applications} ({NCA})},
	author = {Santana, Cleber and Alencar, Brenno and Prazeres, Cássio},
	month = nov,
	year = {2018},
	keywords = {Architecture, Cloud Computing, Cloud computing, Computer architecture, Fog Computing, Internet of Things, Microservices, Reactive Microservices, Scalability, Service-oriented architecture, Systematics, systematic mapping study},
	pages = {1--4},
}

@inproceedings{krivic_microservices_2017,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Microservices as {Agents} in {IoT} {Systems}},
	isbn = {978-3-319-59394-4},
	doi = {10.1007/978-3-319-59394-4_3},
	abstract = {Developing robust monolith systems has achieved its limitations, since the implementation of changes in today’s large, complex, and fast evolving systems would be too slow and inefficient. As a response to these problems, microservice architecture emerged, and quickly became a widely used solution. Such modular architecture is appropriate for distributed environment of Internet of Things (IoT) solutions. In this paper we present a solution for service management on Machine-to-Machine (M2M) devices within IoT system by using collaborative microservices. Collaboration of distributed modules highly reminds of multi-agent systems where autonomous agents also cooperate to provide services to the end-user. Because of these similarities we consider microservices as modern agents that could improve systems in distributed environments, such as IoT.},
	language = {en},
	booktitle = {Agent and {Multi}-{Agent} {Systems}: {Technology} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Krivic, Petar and Skocir, Pavle and Kusek, Mario and Jezic, Gordan},
	editor = {Jezic, Gordan and Kusek, Mario and Chen-Burger, Yun-Heh Jessica and Howlett, Robert J. and Jain, Lakhmi C.},
	year = {2017},
	keywords = {Agents, IoT, M2M, Microservices, Service management},
	pages = {22--31},
}

@inproceedings{abdollahi_vayghan_deploying_2018,
	title = {Deploying {Microservice} {Based} {Applications} with {Kubernetes}: {Experiments} and {Lessons} {Learned}},
	shorttitle = {Deploying {Microservice} {Based} {Applications} with {Kubernetes}},
	doi = {10.1109/CLOUD.2018.00148},
	abstract = {Microservices represent a new architectural style where small and loosely coupled modules can be developed and deployed independently to compose an application. This architectural style brings various benefits such as maintainability and flexibility in scaling and aims at decreasing downtime in case of failure or upgrade. One of the enablers is Kubernetes, an open source platform that provides mechanisms for deploying, maintaining, and scaling containerized applications across a cluster of hosts. Moreover, Kubernetes enables healing through failure recovery actions to improve the availability of applications. As our ultimate goal is to devise architectures to enable high availability (HA) with Kubernetes for microservice based applications, in this paper we examine the availability achievable through Kubernetes under its default configuration. We have conducted a set of experiments which show that the service outage can be significantly higher than expected.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Abdollahi Vayghan, Leila and Saied, Mohamed Aymen and Toeroe, Maria and Khendek, Ferhat},
	month = jul,
	year = {2018},
	note = {ISSN: 2159-6190},
	keywords = {Cloud computing, Computer crashes, Containers, IP networks, Maintenance engineering, Measurement, Microservices, Containers, Orchestration, Docker, Kubernetes, Failure, Availability, Streaming media},
	pages = {970--973},
}

@article{yaguache_enabling_nodate,
	title = {Enabling {Edge} {Computing} {Using} {Container} {Orchestration} and {Software} {Deﬁned} {Networking}},
	abstract = {With software-deﬁned wide-area networks (SD-WAN) being increasingly adopted, and Kubernetes becoming the de-facto container orchestration tool, the opportunities for deploying edge-computing applications running over a SD-WAN scenario are vast. In this context, a service discovery function will help developing a dynamic infrastructure where clients are able to seek and ﬁnd particular services. Service discovery also enables a self-healing network capable of detecting the unavailable services. Most of the research in the service discovery ﬁeld focuses in the discovery of cloud-based services over software-deﬁned networks (SDN). A lack of research in containerized service discovery over SD-WAN is evident. In this thesis, an in-house service discovery solution that works alongside a container orchestrator for allowing an improved traﬃc handling and better user experience through containerized service discovery and service requests redirection is developed. First, a proof-ofconcept SD-WAN topology was implemented alongside a Kubernetes cluster and the in-house service discovery solution. Next, the implementation’s performance is tested based on the time required for discovering whether a service has been created, updated or removed. Finally, improvements in node distance computation, local breakout support and the usage of data plane programmability are discussed.},
	language = {en},
	author = {Yaguache, Felipe Andres Rodriguez},
	pages = {63},
}

@article{kurbatov_design_nodate,
	title = {Design and implementation of secure communication between microservices},
	language = {en},
	author = {Kurbatov, Aleksandr},
	pages = {50},
}

@article{truong_devops_2020,
	title = {{DevOps} {Contract} for {Assuring} {Execution} of {IoT} {Microservices} in the {Edge}},
	volume = {9},
	issn = {2542-6605},
	url = {https://www.sciencedirect.com/science/article/pii/S2542660519301726},
	doi = {10.1016/j.iot.2019.100150},
	abstract = {The increasing availability of edge and IoT infrastructure-as-a-service allows us to develop lightweight IoT components and deploy them into edge/IoT infrastructures, enabling edge analytics and controls. This paper introduces the development of service contracts for IoT microservices from DevOps perspectives. We analyze stakeholders and present our methods to support stakeholders to program IoT service contracts. We address the diversity of service contracts by using common languages for IoT data and programming. We integrate the development and operation lifecycle of IoT contracts with IoT software components and with supporting DevOps services. To illustrate our approach, we use a real-world Base Transceiver Station maintenance application with Raspberry Pi, Java, JavaScript, JSON and other microservices.},
	language = {en},
	urldate = {2021-09-15},
	journal = {Internet of Things},
	author = {Truong, Hong-Linh and Klein, Peter},
	month = mar,
	year = {2020},
	keywords = {Edge computing, Execution management, IoT, Service contract},
	pages = {100150},
}

@article{alam_orchestration_2018,
	title = {Orchestration of {Microservices} for {IoT} {Using} {Docker} and {Edge} {Computing}},
	volume = {56},
	issn = {1558-1896},
	doi = {10.1109/MCOM.2018.1701233},
	abstract = {The world of connected devices has led to the rise of the Internet of Things paradigm, where applications rely on multiple devices, gathering and sharing data across highly heterogeneous networks. The variety of possible mechanisms, protocols, and hardware has become a hindrance in the development of architectures capable of addressing the most common IoT use cases, while abstracting services from the underlying communication subsystem. Moreover, the world is moving toward new strict requirements in terms of timeliness and low latency in combination with ultra-high availability and reliability. Thus, future IoT architectures will also have to support the requirements of these cyber-physical applications. In this regard, edge computing has been presented as one of the most promising solutions, relying on the cooperation of nodes by moving services directly to end devices and caching information locally. Therefore, in this article, we propose a modular and scalable architecture based on lightweight virtualization. The provided modularity, combined with the orchestration supplied by Docker, simplifies management and enables distributed deployments, creating a highly dynamic system. Moreover, characteristics such as fault tolerance and system availability are achieved by distributing the application logic across different layers, where failures of devices and micro-services can be masked by this natively redundant architecture, with minimal impact on the overall system performance. Experimental results have validated the implementation of the proposed architecture for on-demand services deployment across different architecture layers.},
	number = {9},
	journal = {IEEE Communications Magazine},
	author = {Alam, Muhammad and Rufino, Joao and Ferreira, Joaquim and Ahmed, Syed Hassan and Shah, Nadir and Chen, Yuanfang},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {Cloud computing, Computer architecture, Edge computing, Fault tolerance, Logic gates, Topology, Virtualization},
	pages = {118--123},
}

@article{martinez_uav_2020,
	title = {{UAV} {Integration} in {Current} {Construction} {Safety} {Planning} and {Monitoring} {Processes}: {Case} {Study} of a {High}-{Rise} {Building} {Construction} {Project} in {Chile}},
	volume = {36},
	issn = {0742-597X, 1943-5479},
	shorttitle = {{UAV} {Integration} in {Current} {Construction} {Safety} {Planning} and {Monitoring} {Processes}},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29ME.1943-5479.0000761},
	doi = {10.1061/(ASCE)ME.1943-5479.0000761},
	abstract = {The majority of construction fatalities and accidents in Chile occur in high-rise building construction projects that also suffer from an insufficient number of safety managers on-site. Unmanned aerial vehicles (UAVs) and their generated aerial visual contents have the potential to help the limited number of safety managers in such projects to quickly and properly inspect the inaccessible, hard-to-reach, or unsafe locations on the site and to enhance their safety assessment of those projects. In this study, a case study approach was adopted to investigate how UAV technology and their generated aerial visual contents might affect the current approach of conducting safety planning and monitoring in a high-rise building construction site in Chile with a limited number of safety managers. The case study involved three steps: (1) understanding the current safety planning and monitoring process in a high-rise construction project, (2) investigating how UAVrelated tasks and generated visual contents could be integrated into the current process, and (3) assessment of how such UAV integration might affect the current safety planning and monitoring process. The outcome of the case study provided a detailed overview of the new steps required to integrate UAVs in the current safety planning and monitoring process and concluded that adoption of UAV technology had enhanced identification and assessment of hazards in the high-rise project. Hazards associated with unsafe acts and conditions at height (e.g., missing guardrails or safety nets around unprotected edges or openings and loose or unsecured material at height) were the most common types of hazards identified using the UAV in this case study. Safety managers in the project also rated aerial videos captured by the UAV as the most useful type of data for their safety planning and monitoring tasks. The UAV also significantly reduced the amount of time required for safety managers to conduct their site visit walkthroughs on the project site. DOI: 10.1061/(ASCE)ME.1943-5479.0000761. © 2020 American Society of Civil Engineers.},
	language = {en},
	number = {3},
	urldate = {2021-09-15},
	journal = {Journal of Management in Engineering},
	author = {Martinez, Jhonattan G. and Gheisari, Masoud and Alarcón, Luis F.},
	month = may,
	year = {2020},
	pages = {05020005},
}

@book{creswell2017research,
	title = {Research design: {Qualitative}, quantitative, and mixed methods approaches},
	publisher = {Sage publications},
	author = {Creswell, John W and Creswell, J David},
	year = {2017},
}

@techreport{FTCBCP2018internet,
	title = {The internet of things and consumer product hazards},
	institution = {Federal Trade Commission’s Bureau of Consumer Protection},
	author = {BCP Staff},
	year = {2018},
}

@article{sundmaeker2010vision,
	title = {Vision and challenges for realising the {Internet} of {Things}},
	volume = {3},
	number = {3},
	journal = {Cluster of European research projects on the internet of things, European Commision},
	author = {Sundmaeker, Harald and Guillemin, Patrick and Friess, Peter and Woelfflé, Sylvie and {others}},
	year = {2010},
	note = {Publisher: Citeseer},
	pages = {34--36},
}

@article{shakhatreh_unmanned_2019,
	title = {Unmanned {Aerial} {Vehicles} ({UAVs}): {A} {Survey} on {Civil} {Applications} and {Key} {Research} {Challenges}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Unmanned {Aerial} {Vehicles} ({UAVs})},
	doi = {10.1109/ACCESS.2019.2909530},
	abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than \$45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
	journal = {IEEE Access},
	author = {Shakhatreh, Hazim and Sawalmeh, Ahmad H. and Al-Fuqaha, Ala and Dou, Zuochao and Almaita, Eyad and Khalil, Issa and Othman, Noor Shamsiah and Khreishah, Abdallah and Guizani, Mohsen},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Civil infrastructure inspection, Communication system security, Market research, Security, Surveillance, UAVs, Unmanned aerial vehicles, Wireless communication, Wireless sensor networks, delivery of goods, precision agriculture, real-time monitoring, remote sensing, search and rescue, security and surveillance, wireless coverage},
	pages = {48572--48634},
}

@article{baker_model-driven_nodate,
	title = {Model-{Driven} {Engineering} in a {Large} {Industrial} {Context} — {Motorola} {Case} {Study}},
	abstract = {In an ongoing eﬀort to reduce development costs in spite of increasing system complexity, Motorola has been a long-time adopter of Model-Driven Engineering (MDE) practices. The foundation of this approach is the creation of rigorous models throughout the development process, thereby enabling the introduction of automation. In this paper we present our experiences within Motorola in deploying a top-down approach to MDE for more than 15 years. We describe some of the key competencies that have been developed and the impact of MDE within the organization. Next we present some of the main issues encountered during MDE deployment, together with some possible resolutions.},
	language = {en},
	author = {Baker, Paul and Loh, Shiou and Weil, Frank},
	pages = {16},
}

@article{whittle_state_2014,
	title = {The {State} of {Practice} in {Model}-{Driven} {Engineering}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/6507223/},
	doi = {10.1109/MS.2013.65},
	language = {en},
	number = {3},
	urldate = {2021-09-14},
	journal = {IEEE Software},
	author = {Whittle, Jon and Hutchinson, John and Rouncefield, Mark},
	month = may,
	year = {2014},
	pages = {79--85},
}

@article{hutchinson_model-driven_nodate,
	title = {Model-driven engineering practices in industry},
	abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
	language = {en},
	author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
	pages = {10},
}

@inproceedings{hutchinson_model-driven_2011,
	address = {Waikiki, Honolulu HI USA},
	title = {Model-driven engineering practices in industry},
	isbn = {978-1-4503-0445-0},
	url = {https://dl.acm.org/doi/10.1145/1985793.1985882},
	doi = {10.1145/1985793.1985882},
	abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
	language = {en},
	urldate = {2021-09-14},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
	month = may,
	year = {2011},
	pages = {633--642},
}

@article{mashkoor_evaluating_2018,
	title = {Evaluating the suitability of state-based formal methods for industrial deployment},
	volume = {48},
	issn = {1097-024X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2634},
	doi = {10.1002/spe.2634},
	abstract = {After a number of success stories in safety-critical domains, we are starting to witness applications of formal methods in contemporary systems and software engineering. However, one thing that is still missing is the evaluation criteria that help software practitioners choose the right formal method for the problem at hand. In this paper, we present the criteria for evaluating and comparing different formal methods. The criteria were chosen through a literature review, discussions with experts from academia and practitioners from industry, and decade-long personal experience with the application of formal methods in industrial and academic projects. The criteria were then evaluated on several model-oriented state-based formal methods. Our research shows that besides technical grounds (eg, modeling capabilities and supported development phases), formal methods should also be evaluated from social and industrial perspectives. We also found out that it is not possible to generate a matrix that renders the selection of the right formal method an automatic process. However, we can generate several pointers, which make this selection process a lot less cumbersome.},
	language = {en},
	number = {12},
	urldate = {2021-09-14},
	journal = {Software: Practice and Experience},
	author = {Mashkoor, Atif and Kossak, Felix and Egyed, Alexander},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2634},
	keywords = {evaluation criteria, formal methods},
	pages = {2350--2379},
}

@article{newcombe_how_2015,
	title = {How {Amazon} web services uses formal methods},
	volume = {58},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2699417},
	doi = {10.1145/2699417},
	abstract = {Engineers use TLA+ to prevent serious but subtle bugs from reaching production.},
	language = {en},
	number = {4},
	urldate = {2021-09-14},
	journal = {Communications of the ACM},
	author = {Newcombe, Chris and Rath, Tim and Zhang, Fan and Munteanu, Bogdan and Brooker, Marc and Deardeuff, Michael},
	month = mar,
	year = {2015},
	pages = {66--73},
}

@inproceedings{ponsard_assessment_2018,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Assessment of {Emerging} {Standards} for {Safety} and {Security} {Co}-{Design} on a {Railway} {Case} {Study}},
	isbn = {978-3-030-02852-7},
	doi = {10.1007/978-3-030-02852-7_12},
	abstract = {Design for safety-critical software intended for domains like transportation or medical systems is known to be difficult but is required to give a sufficient level of assurance that the system will not harm or kill people. To add to the difficulty, systems have now become highly connected and are turning into cyber-physical systems. This results in the need to address intentional cyber security threats on top of risks related to unintentional software defects. Different approaches are being defined to co-engineer both software security and safety in a consistent way. This paper aims at providing a deeper understanding of those approaches and the evolution of related standards by analysing them using a sound goal-oriented framework that can model both kind of properties and also reason on them in a risk-oriented way. In the process interesting co-design patterns are also identified and discussed. The approach is driven by a real world open specification from the railways.},
	language = {en},
	booktitle = {New {Trends} in {Model} and {Data} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Ponsard, Christophe and Grandclaudon, Jeremy and Massonet, Philippe and Touzani, Mounir},
	editor = {Abdelwahed, El Hassan and Bellatreche, Ladjel and Benslimane, Djamal and Golfarelli, Matteo and Jean, Stéphane and Mery, Dominique and Nakamatsu, Kazumi and Ordonez, Carlos},
	year = {2018},
	keywords = {Co-design, Cyber security, Goals, Rolls Royce-Fall21, Safety, Standards, Threats},
	pages = {130--145},
}

@article{suo_merging_2018,
	title = {Merging safety and cybersecurity analysis in product design},
	volume = {12},
	issn = {1751-9578},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-its.2018.5323},
	doi = {10.1049/iet-its.2018.5323},
	abstract = {When developing cyber-physical systems such as automated vehicles, safety and cybersecurity analyses are often conducted separately. However, unlike in the IT world, safety hazards and cybersecurity threats converge in cyber-physical systems; a malicious party can exploit cyber-threats to create extremely hazardous situations, whether in autonomous vehicles or nuclear plants. The authors propose a framework for integrated system-level analyses for functional safety and cyber security. They present a generic model named Threat Identification and Refinement for Cyber-Physical Systems (TIRCPS) extending Microsoft's six classes of threat modelling including Spoofing, Tampering, Repudiation, Information Disclosure, Denial-of-Service and Elevation Privilege. TIRCPS introduces three benefits of developing complex systems: first, it allows the refinement of abstract threats into specific ones as physical design information becomes available. Second, the approach provides support for constructing attack trees with traceability from high-level goals and hazardous events (HEs) to threats. Third, TIRCPS formalises the definition of threats such that intelligent tools can be built to automatically detect most of a system's vulnerable components requiring protection. They present a case study on an automated-driving system to illustrate the proposed approach. The analysis results of a hierarchical attack tree with cyber-threats traceable to high-level HEs are used to design mitigation solutions.},
	language = {en},
	number = {9},
	urldate = {2021-09-13},
	journal = {IET Intelligent Transport Systems},
	author = {Suo, Dajiang and Siegel, Joshua E. and Sarma, Sanjay E.},
	year = {2018},
	note = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-its.2018.5323},
	keywords = {Microsoft, Rolls Royce-Fall21, TIRCPS, abstract threats refinement, automated vehicles, automated-driving system, autonomous vehicles, cyber-physical systems, cyber-threats, cybersecurity analysis, denial-of-service, elevation privilege, extremely hazardous situations, functional safety analysis, hazardous events, hazards, hierarchical attack tree, information disclosure, intelligent tools, malicious party, nuclear plants, physical design information, product design, repudiation, security of data, spoofing, system-level analysis, tampering, threat identification-and-refinement-for-cyber-physical systems, threat modelling},
	pages = {1103--1109},
}

@techreport{faganIoTDeviceCybersecurity2020,
	address = {Gaithersburg, MD},
	title = {{IoT} device cybersecurity capability core baseline},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8259A.pdf},
	abstract = {Device cybersecurity capabilities are cybersecurity features or functions that computing devices provide through their own technical means (i.e., device hardware and software). This publication defines an Internet of Things (IoT) device cybersecurity capability core baseline, which is a set of device capabilities generally needed to support common cybersecurity controls that protect an organization’s devices as well as device data, systems, and ecosystems. The purpose of this publication is to provide organizations a starting point to use in identifying the device cybersecurity capabilities for new IoT devices they will manufacture, integrate, or acquire. This publication can be used in conjunction with NISTIR 8259, Foundational Cybersecurity Activities for IoT Device Manufacturers.},
	language = {en},
	number = {NIST IR 8259A},
	urldate = {2021-03-29},
	institution = {National Institute of Standards and Technology},
	author = {Fagan, Michael and Megas, Katerina N and Scarfone, Karen and Smith, Matthew},
	month = may,
	year = {2020},
	doi = {10.6028/NIST.IR.8259a},
	pages = {NIST IR 8259A},
}

@techreport{boecklConsiderationsManagingInternet2019,
	address = {Gaithersburg, MD},
	title = {Considerations for managing {Internet} of {Things} ({IoT}) cybersecurity and privacy risks},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8228.pdf},
	abstract = {The Internet of Things (IoT) is a rapidly evolving and expanding collection of diverse technologies that interact with the physical world. Many organizations are not necessarily aware of the large number of IoT devices they are already using and how IoT devices may affect cybersecurity and privacy risks differently than conventional information technology (IT) devices do. The purpose of this publication is to help federal agencies and other organizations better understand and manage the cybersecurity and privacy risks associated with their individual IoT devices throughout the devices’ lifecycles. This publication is the introductory document providing the foundation for a planned series of publications on more specific aspects of this topic.},
	language = {en},
	number = {NIST IR 8228},
	urldate = {2021-03-29},
	institution = {National Institute of Standards and Technology},
	author = {Boeckl, Katie and Fagan, Michael and Fisher, William and Lefkovitz, Naomi and Megas, Katerina N and Nadeau, Ellen and O'Rourke, Danna Gabel and Piccarreta, Ben and Scarfone, Karen},
	month = jun,
	year = {2019},
	doi = {10.6028/NIST.IR.8228},
	pages = {NIST IR 8228},
}

@book{paulitsch_non-functional_2008,
	title = {Non-functional {Avionics} {Requirements}},
	volume = {17},
	abstract = {Embedded systems in aerospace become more and more integrated in order to reduce weight, volume/size, and power of hardware for more fuel-effi ciency. Such integration tendencies change architectural approaches of system ar chi tec tures, which subsequently change non-functional requirements for plat forms. This paper provides some insight into state-of-the-practice of non-func tional requirements for developing ultra-critical embedded systems in the aero space industry, including recent changes and trends. In particular, formal requi re ment capture and formal analysis of non-functional requirements of avionic systems - including hard-real time, fault-tolerance, reliability, and per for mance - are exemplified by means of recent developments in SAL and HiLiTE.},
	author = {Paulitsch, Michael and Ruess, Harald and Sorea, Maria},
	month = oct,
	year = {2008},
	doi = {10.1007/978-3-540-88479-8_26},
	note = {Journal Abbreviation: Communications in Computer and Information Science
Pages: 384
Publication Title: Communications in Computer and Information Science},
}

@incollection{rayadurgam_improving_2016,
	address = {Cham},
	title = {Improving an {Industrial} {Test} {Generation} {Tool} {Using} {SMT} {Solver}},
	volume = {9690},
	isbn = {978-3-319-40647-3 978-3-319-40648-0},
	url = {http://link.springer.com/10.1007/978-3-319-40648-0_8},
	abstract = {We present an SMT solving based test generation approach for MATLAB Simulink designs, implemented in the HiLiTE tool developed by Honeywell for veriﬁcation of avionic systems. The test requirements for a Simulink model are represented by a set of behavioral equivalence classes for each block in the model, in terms of its input(s) and output. A unique feature of our approach is that the equivalence class deﬁnitions, as well as the upstream subgraph of a block under test, are translated as constraints into SMT expressions. An SMT solver is called at the back-end of HiLiTE to ﬁnd a satisﬁable solution that is further augmented into an end-to-end test case at the model level.},
	language = {en},
	urldate = {2021-09-13},
	booktitle = {{NASA} {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Ren, Hao and Bhatt, Devesh and Hvozdovic, Jan},
	editor = {Rayadurgam, Sanjai and Tkachuk, Oksana},
	year = {2016},
	doi = {10.1007/978-3-319-40648-0_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {100--106},
}

@article{bhatt_effective_nodate,
	title = {Effective {Verification} of {Flight} {Critical} {Software} {Systems}: {Issues} and {Approaches}},
	language = {en},
	author = {Bhatt, Devesh and Schloegel, Kirk},
	pages = {4},
}

@misc{barnat_aerospace_nodate,
	title = {Aerospace {Advanced} {Technology} {Europe}},
	abstract = {Abstract. Embedded systems have become an inevitable part of control systems in many industrial domains including avionics. The nature of this domain traditionally requires the highest possible degree of system availability and integrity. While embedded systems have become extremely complex and they have been continuously replacing legacy mechanical components, the amount of defects of hardware and software has to be kept to absolute minimum to avoid casualties and material damages. Despite the above-mentioned facts, significant improvements are still required in the validation and verification processes accompanying embedded systems development. In this paper we report on integration of a parallel, explicit-state LTL model checker (DIVINE) and a tool for requirements-based verification of aerospace system components (HiLiTE, a tool implemented and used by Honeywell). HiLiTE and the proposed partial toolchain use MATLAB Simulink/Stateflow as the primary design language. The work has been conducted within the Artemis project industrial Framework for Embedded Systems Tools (iFEST). 1},
	author = {Barnat, J. and Beran, J. and Brim, L. and Kratochvíla, T. and Ročkai, P.},
}

@incollection{newell_mechanisms_1981,
	address = {Hillsdale, NJ},
	title = {Mechanisms of {Skill} {Acquisition} and the {Law} of {Practice}},
	booktitle = {Cognitive {Skills} and {Their} {Acquisition}},
	publisher = {Lawrence Erlbaum Associates, Inc.},
	author = {Newell, A. and Rosenbloom, P. S.},
	editor = {Anderson, J. R.},
	year = {1981},
	note = {Section: 1},
	pages = {1--51},
}

@article{samuel_studies_1959,
	title = {Some {Studies} in {Machine} {Learning} {Using} the {Game} of {Checkers}},
	volume = {3},
	number = {3},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A. L.},
	year = {1959},
	pages = {211--229},
}

@misc{author_suppressed_2021,
	title = {Suppressed for {Anonymity}},
	author = {Author, N. N.},
	year = {2021},
}

@book{michalski_machine_1983,
	address = {Palo Alto, CA},
	title = {Machine {Learning}: {An} {Artificial} {Intelligence} {Approach}, {Vol}. {I}},
	publisher = {Tioga},
	editor = {Michalski, R. S. and Carbonell, J. G. and Mitchell, T. M.},
	year = {1983},
}

@phdthesis{kearns_computational_1989,
	type = {{PhD} {Thesis}},
	title = {Computational {Complexity} of {Machine} {Learning}},
	school = {Department of Computer Science, Harvard University},
	author = {Kearns, M. J.},
	year = {1989},
}

@inproceedings{langley_crafting_2000,
	address = {Stanford, CA},
	title = {Crafting {Papers} on {Machine} {Learning}},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Machine} {Learning} ({ICML} 2000)},
	publisher = {Morgan Kaufmann},
	author = {Langley, P.},
	editor = {Langley, Pat},
	year = {2000},
	pages = {1207--1216},
}

@book{duda_pattern_2000,
	edition = {2nd},
	title = {Pattern {Classification}},
	publisher = {John Wiley and Sons},
	author = {Duda, R. O. and Hart, P. E. and Stork, D. G.},
	year = {2000},
}

@techreport{mitchell_need_1980,
	address = {New Brunswick, MA},
	title = {The {Need} for {Biases} in {Learning} {Generalizations}},
	institution = {Computer Science Department, Rutgers University},
	author = {Mitchell, T. M.},
	year = {1980},
}

@article{noauthor_exploiting_2022,
	title = {Exploiting {Input} {Sanitization} for {Regex} {Denial} of {Service}},
	language = {en},
	year = {2022},
	pages = {12},
}

@article{leitner_mixed-method_2019,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice},
	volume = {149},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218302735},
	doi = {10.1016/j.jss.2018.12.013},
	abstract = {Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.},
	language = {en},
	urldate = {2021-09-11},
	journal = {Journal of Systems and Software},
	author = {Leitner, Philipp and Wittern, Erik and Spillner, Josef and Hummer, Waldemar},
	month = mar,
	year = {2019},
	keywords = {Cloud computing, Empirical research, Function-as-a-Service, Serverless},
	pages = {340--359},
}

@inproceedings{datta_valve_2020,
	address = {Taipei Taiwan},
	title = {Valve: {Securing} {Function} {Workflows} on {Serverless} {Computing} {Platforms}},
	isbn = {978-1-4503-7023-3},
	shorttitle = {Valve},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380173},
	doi = {10.1145/3366423.3380173},
	abstract = {Serverless Computing has quickly emerged as a dominant cloud computing paradigm, allowing developers to rapidly prototype eventdriven applications using a composition of small functions that each perform a single logical task. However, many such application workflows are based in part on publicly-available functions developed by third-parties, creating the potential for functions to behave in unexpected, or even malicious, ways. At present, developers are not in total control of where and how their data is flowing, creating significant security and privacy risks in growth markets that have embraced serverless (e.g., IoT).},
	language = {en},
	urldate = {2021-09-11},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Datta, Pubali and Kumar, Prabuddha and Morris, Tristan and Grace, Michael and Rahmati, Amir and Bates, Adam},
	month = apr,
	year = {2020},
	pages = {939--950},
}

@book{esposito_pervasive_2019,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pervasive {Systems}, {Algorithms} and {Networks}: 16th {International} {Symposium}, {I}-{SPAN} 2019, {Naples}, {Italy}, {September} 16-20, 2019, {Proceedings}},
	volume = {1080},
	isbn = {978-3-030-30142-2 978-3-030-30143-9},
	shorttitle = {Pervasive {Systems}, {Algorithms} and {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-30143-9},
	language = {en},
	urldate = {2021-09-09},
	publisher = {Springer International Publishing},
	editor = {Esposito, Christian and Hong, Jiman and Choo, Kim-Kwang Raymond},
	year = {2019},
	doi = {10.1007/978-3-030-30143-9},
}

@book{esposito_pervasive_2019-1,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pervasive {Systems}, {Algorithms} and {Networks}: 16th {International} {Symposium}, {I}-{SPAN} 2019, {Naples}, {Italy}, {September} 16-20, 2019, {Proceedings}},
	volume = {1080},
	isbn = {978-3-030-30142-2 978-3-030-30143-9},
	shorttitle = {Pervasive {Systems}, {Algorithms} and {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-30143-9},
	language = {en},
	urldate = {2021-09-09},
	publisher = {Springer International Publishing},
	editor = {Esposito, Christian and Hong, Jiman and Choo, Kim-Kwang Raymond},
	year = {2019},
	doi = {10.1007/978-3-030-30143-9},
}

@incollection{fathoni_performance_2019,
	title = {Performance {Comparison} of {Lightweight} {Kubernetes} in {Edge} {Devices}},
	isbn = {978-3-030-30142-2},
	abstract = {Traditional cloud computing has the challenge to serve many clients with many services. Spreading the services to across of edge server will reduce the load of traditional cloud computing. Kubernetes is one of the platforms used for cloud management. Kubernetes helps to deploy, and scaling the application. Nowadays, a lot of communities build a lightweight Kubernetes than suitable for edge device such as Raspberry Pi. This paper Investigate the performance of Kubernetes lightweight that installed in the Raspberry Pi.},
	author = {Fathoni, Halim and Yang, Chao-Tung and Chang, Chih-Hung and Huang, Chin-Yin},
	month = nov,
	year = {2019},
	doi = {10.1007/978-3-030-30143-9_25},
	pages = {304--309},
}

@book{esposito_pervasive_2019-2,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pervasive {Systems}, {Algorithms} and {Networks}: 16th {International} {Symposium}, {I}-{SPAN} 2019, {Naples}, {Italy}, {September} 16-20, 2019, {Proceedings}},
	volume = {1080},
	isbn = {978-3-030-30142-2 978-3-030-30143-9},
	shorttitle = {Pervasive {Systems}, {Algorithms} and {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-30143-9},
	language = {en},
	urldate = {2021-09-09},
	publisher = {Springer International Publishing},
	editor = {Esposito, Christian and Hong, Jiman and Choo, Kim-Kwang Raymond},
	year = {2019},
	doi = {10.1007/978-3-030-30143-9},
}

@article{alam_orchestration_2018-1,
	title = {Orchestration of {Microservices} for {IoT} {Using} {Docker} and {Edge} {Computing}},
	volume = {56},
	issn = {1558-1896},
	doi = {10.1109/MCOM.2018.1701233},
	abstract = {The world of connected devices has led to the rise of the Internet of Things paradigm, where applications rely on multiple devices, gathering and sharing data across highly heterogeneous networks. The variety of possible mechanisms, protocols, and hardware has become a hindrance in the development of architectures capable of addressing the most common IoT use cases, while abstracting services from the underlying communication subsystem. Moreover, the world is moving toward new strict requirements in terms of timeliness and low latency in combination with ultra-high availability and reliability. Thus, future IoT architectures will also have to support the requirements of these cyber-physical applications. In this regard, edge computing has been presented as one of the most promising solutions, relying on the cooperation of nodes by moving services directly to end devices and caching information locally. Therefore, in this article, we propose a modular and scalable architecture based on lightweight virtualization. The provided modularity, combined with the orchestration supplied by Docker, simplifies management and enables distributed deployments, creating a highly dynamic system. Moreover, characteristics such as fault tolerance and system availability are achieved by distributing the application logic across different layers, where failures of devices and micro-services can be masked by this natively redundant architecture, with minimal impact on the overall system performance. Experimental results have validated the implementation of the proposed architecture for on-demand services deployment across different architecture layers.},
	number = {9},
	journal = {IEEE Communications Magazine},
	author = {Alam, Muhammad and Rufino, Joao and Ferreira, Joaquim and Ahmed, Syed Hassan and Shah, Nadir and Chen, Yuanfang},
	month = sep,
	year = {2018},
	note = {Conference Name: IEEE Communications Magazine},
	keywords = {Cloud computing, Computer architecture, Edge computing, Fault tolerance, Logic gates, Topology, Virtualization},
	pages = {118--123},
}

@inproceedings{bandeira_we_2019,
	title = {We {Need} to {Talk} {About} {Microservices}: an {Analysis} from the {Discussions} on {StackOverflow}},
	shorttitle = {We {Need} to {Talk} {About} {Microservices}},
	doi = {10.1109/MSR.2019.00051},
	abstract = {Microservices are a new and rapidly growing architectural model aimed at developing highly scalable software solutions based on independently deployable and evolvable components. Due to its novelty, microservice-related discussions are increasing in Q\&A websites, such as StackOverflow (SO). In order to understand what is being discussed by the microservice community, this work has applied mining techniques and topic modelling to a manually-curated dataset of 1,043 microservice-related posts from StackOverflow. As a result, we found that 13.68\% of microservice technical posts on SO discuss a single technology: Netflix Eureka. Moreover, buzzwords in the microservice ecosystem, e.g., blue/green deployment, were not identified as relevant subjects of discussion on SO. Finally, we show how a high discussion rate on SO may not reflect the popularity of a certain subject within the microservice community.},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Bandeira, Alan and Medeiros, Carlos Alberto and Paixao, Matheus and Maia, Paulo Henrique},
	month = may,
	year = {2019},
	note = {ISSN: 2574-3864},
	keywords = {Authentication, Biological system modeling, Cloud computing, Computer architecture, Fault tolerance, Manuals, Microservices, StackOverflow, Topic Modelling, Tools},
	pages = {255--259},
}

@inproceedings{qu_experimental_2020-1,
	title = {An {Experimental} {Study} on {Microservices} based {Edge} {Computing} {Platforms}},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163068},
	abstract = {The rapid technological advances in the Internet of Things (IoT) allows the blueprint of Smart Cities to become feasible by integrating heterogeneous cloud/fog/edge computing paradigms to collaboratively provide variant smart services in our cities and communities. Thanks to attractive features like fine granularity and loose coupling, the microservices architecture has been proposed to provide scalable and extensible services in large scale distributed IoT systems. Recent studies have evaluated and analyzed the performance interference between microservices based on scenarios on the cloud computing environment. However, they are not holistic for IoT applications given the restriction of the edge device like computation consumption and network capacity. This paper investigates multiple microservice deployment policies on edge computing platform. The microservices are developed as docker containers, and comprehensive experimental results demonstrate the performance and interference of microservices running on benchmark scenarios.},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	author = {Qu, Qian and Xu, Ronghua and Nikouei, Seyed Yahya and Chen, Yu},
	month = jul,
	year = {2020},
	keywords = {Benchmark testing, Cloud computing, Computer architecture, Container, Containers, Edge Computing, Edge computing, Internet of Things (IoT), Microservices Architecture, Performance evaluation, Service-oriented architecture},
	pages = {836--841},
}

@article{li_understanding_2021,
	title = {Understanding and addressing quality attributes of microservices architecture: {A} {Systematic} literature review},
	volume = {131},
	issn = {0950-5849},
	shorttitle = {Understanding and addressing quality attributes of microservices architecture},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584920301993},
	doi = {10.1016/j.infsof.2020.106449},
	abstract = {Context: As a rapidly adopted architectural style in software engineering, Microservices Architecture (MSA) advocates implementing small-scale and independently distributed services, rather than binding all functions into one monolith. Although many initiatives have contributed to the quality improvement of microservices-based systems, there is still a lack of a systematic understanding of the Quality Attributes (QAs) associated with MSA. Objective: This study aims to investigate the evidence-based state-of-the-art of QAs of microservices-based systems. Method: We carried out a Systematic Literature Review (SLR) to identify and synthesize the relevant studies that report evidence related to QAs of MSA. Results: Based on the data extracted from the 72 selected primary studies, we portray an overview of the six identified QAs most concerned in MSA, scalability, performance, availability, monitorability, security, and testability. We identify 19 tactics that architecturally address the critical QAs in MSA, including two tactics for scalability, four for performance, four for availability, four for monitorability, three for security, and two for testability. Conclusion: This SLR concludes that for MSA-based systems: 1) Although scalability is the commonly acknowledged benefit of MSA, it is still an indispensable concern among the identified QAs, especially when trading-off with other QAs, e.g., performance. Apart from the six identified QAs in this study, other QAs for MSA like maintainability need more attention for effective improvement and evaluation in the future. 3) Practitioners need to carefully make the decision of migrating to MSA based on the return on investment, since this architectural style additionally cause some pains in practice.},
	language = {en},
	urldate = {2021-09-09},
	journal = {Information and Software Technology},
	author = {Li, Shanshan and Zhang, He and Jia, Zijia and Zhong, Chenxing and Zhang, Cheng and Shan, Zhihao and Shen, Jinfeng and Babar, Muhammad Ali},
	month = mar,
	year = {2021},
	keywords = {Microservices, Monolith, Quality attributes, Systematic literature review},
	pages = {106449},
}

@article{mendonca_developing_2021,
	title = {Developing {Self}-{Adaptive} {Microservice} {Systems}: {Challenges} and {Directions}},
	volume = {38},
	issn = {1937-4194},
	shorttitle = {Developing {Self}-{Adaptive} {Microservice} {Systems}},
	doi = {10.1109/MS.2019.2955937},
	abstract = {A self-adaptive system can dynamically monitor and adapt its behavior to preserve and enhance its quality attributes under uncertain operating conditions. This article identifies key challenges for the development of microservice applications as self-adaptive systems, using a cloud-based intelligent video-surveillance application as a motivating example. It also suggests potential new directions for addressing most of the identified challenges by leveraging existing microservice practices and technologies.},
	number = {2},
	journal = {IEEE Software},
	author = {Mendonca, Nabor C. and Jamshidi, Pooyan and Garlan, David and Pahl, Claus},
	month = mar,
	year = {2021},
	note = {Conference Name: IEEE Software},
	keywords = {Computer architecture, Containers, DevOps, Face recognition, Pipelines, Self-adaptive systems, Streaming media, Video surveillance, continuous delivery, microservices},
	pages = {70--79},
}

@article{zhou_fault_2021,
	title = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}: {Industrial} {Survey}, {Benchmark} {System}, and {Empirical} {Study}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}},
	doi = {10.1109/TSE.2018.2887384},
	abstract = {The complexity and dynamism of microservice systems pose unique challenges to a variety of software engineering tasks such as fault analysis and debugging. In spite of the prevalence and importance of microservices in industry, there is limited research on the fault analysis and debugging of microservice systems. To fill this gap, we conduct an industrial survey to learn typical faults of microservice systems, current practice of debugging, and the challenges faced by developers in practice. We then develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 industrial fault cases on it. Based on the benchmark system and the replicated fault cases, we conduct an empirical study to investigate the effectiveness of existing industrial debugging practices and whether they can be further improved by introducing the state-of-the-art tracing and visualization techniques for distributed systems. The results show that the current industrial practices of microservice debugging can be improved by employing proper tracing and visualization techniques and strategies. Our findings also suggest that there is a strong need for more intelligent trace analysis and visualization, e.g., by combining trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Li, Wenhai and Ding, Dan},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Benchmark testing, Companies, Computer architecture, Debugging, Industries, Microservices, Runtime, Visualization, debugging, fault localization, tracing, visualization},
	pages = {243--260},
}

@article{zhou_fault_2021-1,
	title = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}: {Industrial} {Survey}, {Benchmark} {System}, and {Empirical} {Study}},
	volume = {47},
	issn = {1939-3520},
	shorttitle = {Fault {Analysis} and {Debugging} of {Microservice} {Systems}},
	doi = {10.1109/TSE.2018.2887384},
	abstract = {The complexity and dynamism of microservice systems pose unique challenges to a variety of software engineering tasks such as fault analysis and debugging. In spite of the prevalence and importance of microservices in industry, there is limited research on the fault analysis and debugging of microservice systems. To fill this gap, we conduct an industrial survey to learn typical faults of microservice systems, current practice of debugging, and the challenges faced by developers in practice. We then develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 industrial fault cases on it. Based on the benchmark system and the replicated fault cases, we conduct an empirical study to investigate the effectiveness of existing industrial debugging practices and whether they can be further improved by introducing the state-of-the-art tracing and visualization techniques for distributed systems. The results show that the current industrial practices of microservice debugging can be improved by employing proper tracing and visualization techniques and strategies. Our findings also suggest that there is a strong need for more intelligent trace analysis and visualization, e.g., by combining trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Li, Wenhai and Ding, Dan},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Benchmark testing, Companies, Computer architecture, Debugging, Industries, Microservices, Runtime, Visualization, debugging, fault localization, tracing, visualization},
	pages = {243--260},
}

@article{janusz_zalewski_trends_2019,
	title = {Trends and challenges in the aviation systems safety and cybersecurity},
	volume = {23},
	issn = {14286394},
	url = {https://doi.org/10.17466/tq2019/23.2/a},
	doi = {10.17466/tq2019/23.2/a},
	abstract = {Aviation systems are an essential component of every nation’s critical infrastructure. Considering millions of passengers flying per year and busy airports, the safe and secure flight and traffic operation is of primary importance to the proper functioning of the society. This paper discusses fundamental problems of providing critical systems safety and cybersecurity in the aviation infrastructure including both airborne and ground systems such as avionics, navigation, air traffic control and management, as well as unmanned systems. It reviews the major challenges and current trends in providing viable solutions. Both industrial practices and research approaches are mentioned, including established methodologies and standards, as well as new developments in certification.},
	language = {en},
	number = {2},
	urldate = {2021-09-08},
	journal = {TASK Quarterly},
	author = {{Janusz Zalewski} and {Andrew Kornecki}},
	year = {2019},
	pages = {159--175},
}

@inproceedings{valdes_aviation_2017,
	address = {Rome, Italy},
	title = {{AVIATION} 4.0: {MORE} {SAFETY} {THROUGH} {AUTOMATION} {AND} {DIGITIZATION}},
	shorttitle = {{AVIATION} 4.0},
	url = {http://library.witpress.com/viewpaper.asp?pcode=SAFE17-021-1},
	doi = {10.2495/SAFE170211},
	abstract = {The whole world is talking about Industry 4.0 or the fourth industrial revolution. That is the current trend of higher levels of automation, digitalization and data exchange in manufacturing technologies. It includes cyber-physical systems, the Internet of Things and cloud computing among others technological assets. With more than 5000 sensors, which generate up to 10 GB of data per second, new modern aircraft engines are a clearly exponent of what digitalization and the internet of aircraft things could furnish, as part of the upcoming Industry 4.0 revolution, in the aviation industry. Called Aviation 4.0, this new era has the potential to help to improve all key performance areas of air transport. Particularly, in an industry where safety levels are so high, and the margins for improvement are extremely tight, this upcoming era might imply a paradigm shift opportunity in safety improvement. In an attempt to define Aviation 4.0 this paper discusses the stages of aviation development from basic VFR flight rules at the Aviation 1.0, up to Aviation 4.0 stage where cyber-physical systems will be designed to assist humans physically tireless, unkind or hazardous work and to take decisions and to complete tasks autonomously. The paper illustrates current real cases of application of Aviation 4.0 concept to increase aviation safety and introduces future possible applications while outlines how they might significantly increase safety levels in aviation.},
	language = {en},
	urldate = {2021-09-08},
	author = {Valdes, Rosa Arnaldo and Comendador, Victor Fernando Gómez},
	month = jun,
	year = {2017},
	pages = {225--236},
}

@inproceedings{pate_amelia_2018,
	title = {{AMELIA}: {An} application of the {Internet} of {Things} for aviation safety},
	shorttitle = {{AMELIA}},
	doi = {10.1109/CCNC.2018.8319163},
	abstract = {This paper presents AMELIA: Aircraft Monitoring and Electronically Linked Instantaneous Analytics as an application of the Internet of Things (IoT) for aviation safety - a safety-critical use-case - from an edge computing perspective. AMELIA is a multi-layered edge computing system that automatically detects aircraft emergencies, and only transmits relevant data and information to enable quicker and more efficient response to emergencies. We describe a prototype of AMELIA to illustrate, explore, and motivate the potentials of the IoT for aviation safety, and lay a foundation for the design of diverse high-impact edge computing systems on the IoT.},
	booktitle = {2018 15th {IEEE} {Annual} {Consumer} {Communications} {Networking} {Conference} ({CCNC})},
	author = {Pate, Jeremiah and Adegbija, Tosiron},
	month = jan,
	year = {2018},
	note = {ISSN: 2331-9860},
	keywords = {Aerospace electronics, Aircraft, Edge computing, Internet of Things, IoT prototype, Monitoring, Prototypes, Safety, Sensors, automatic control, aviation safety, edge computing},
	pages = {1--6},
}

@article{hamzehloui_study_2019,
	title = {A {Study} on the {Most} {Prominent} {Areas} of {Research} in {Microservices}},
	volume = {9},
	doi = {10.18178/ijmlc.2019.9.2.793},
	abstract = {Microservices have recently gained a lot of attention in the software industry. Their modularity and smaller size offer flexibility advantageous to both development and operational teams. However, the bigger picture is still lacking despite numerous researches on microservices. There are few aspects of microservices that have never been discussed in depth despite being acknowledged repeatedly. The current research is the continuation of our previous paper, "A systematic mapping on microservices". In the named paper we have identified the focus areas of microservices' researches. Along with our previous findings we have spotted several crucial key points that require further discussions. These includes: definition of microservices, their sizes and boundaries. We have also explored the relationship of microservices with SOA and DDD. These are the two terms that are frequently associated with microservices. Finally, we have discussed DevOps, cloud and virtualization as three of the most essential factors in microservices ecosystem. We attempted to clarify the role of each of these factors. Based on our findings, there is still no standardized definition for microservices to-date. In absence of clear guidelines, SOA and DDD concepts are widely being used to develop microservices. DevOps practices together with the cloud environment are playing an important role in facilitating the implementation of microservices. We have also identified containerization as an effective method to overcome the hardware limitation besides speeding up the delivery process. © 2019, International Association of Computer Science and Information Technology.},
	journal = {International Journal of Machine Learning and Computing},
	author = {Hamzehloui, Mohammad and Sahibuddin, Shamsul and Ashabi, Ardavan},
	month = apr,
	year = {2019},
	pages = {242--247},
}

@article{bowen_safety-critical_1993,
	title = {Safety-{Critical} {Systems}, {Formal} {Methods} and {Standards}},
	volume = {8},
	abstract = {Standards concerned with the development of safety-critical systems, and the software in such systems in particular, abound today as the software crisis increasingly affects the world of embedded computer-based systems. The use of formal methods is often advocated as a way of increasing confidence in such systems. This paper examines the industrial use of these techniques, the recommendations concerning formal methods in a number of current and draft standards, and comments on the applicability and problems of using formal methods for the development of safety-critical systems of an industrial scale. Some possible future directions are suggested.},
	journal = {Software Engineering Journal},
	author = {Bowen, Jonathan P. and Stavridou, Victoria},
	year = {1993},
	pages = {189--209},
}

@inproceedings{campeanu_mapping_2018,
	title = {A mapping study on microservice architectures of {Internet} of {Things} and cloud computing solutions},
	doi = {10.1109/MECO.2018.8406008},
	abstract = {Internet of Things is a fairly new paradigm adopted by the industry, which offers the connectivity, via wireless systems, of all the devices that surround us. One of the challenges of IoT relates to the required resources to store and compute the huge amount of data resulted from devices' connections. Cloud computing is a solution to the IoT challenges; it provides on-demand resources in an easy-to-access manner. Another trend in the enterprise world is the usage of microservice architectures. Being a newly developed paradigm, and although its principles are defined, it is difficult to have a vision of the existing microservice-based research solutions. This paper, through the mapping study methodology, provides an overview of the current state-of-the-art and -practice regarding the usage of microservice architectures by IoT and cloud computing solutions. More specifically, we synthesize the data from 364 selected studies and describe the research types, number of publications and their main venues.},
	booktitle = {2018 7th {Mediterranean} {Conference} on {Embedded} {Computing} ({MECO})},
	author = {Campeanu, Gabriel},
	month = jun,
	year = {2018},
	keywords = {Cloud computing, Computer architecture, Embedded computing, Internet of Things, IoT, Libraries, Market research, SLR, Systematics, cloud computing, microservice, systematic mapping study},
	pages = {1--4},
}

@misc{garavel_experiment_1993,
	title = {An {Experiment} with the {LOTOS} {Formal} {Description} {Technique} on the {Flight} {Warning} {Computer} of {Airbus} 330/340 {Aircrafts}},
	abstract = {This paper presents the main results of a two-year study concerning the introduction of  formal methods in the life cycle of avionics software. This study was done in the framework  of the EUREKA European project AIMS (Aerospace Intelligent Management and development  environment for embedded Systems).  The ISO language Lotos was used to describe a significant subset of the Flight Warning  Computer of Airbus 330/340 aircrafts, which is a typical representative of Embedded Computer  Systems. Six Lotos descriptions were developed, (using both the abstract data types and the  process algebra features of Lotos) which are probably among the largest algebraic specifications  written today. The Caesar/Ald' ebaran toolset for Lotos was used to support the description  and analysis process. The Lotos descriptions were automatically translated into executable  prototypes, and then validated by means of simulation and testing. The paper presents the  techniques used and the results obtained. It e...},
	author = {Garavel, Hubert and Hautbois, René-Pierre},
	year = {1993},
}

@misc{noauthor_-178c_2021,
	title = {{DO}-{178C}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=DO-178C&oldid=1032602731},
	abstract = {DO-178C, Software Considerations in Airborne Systems and Equipment Certification is the primary document by which the certification authorities such as FAA, EASA and Transport Canada approve all commercial software-based aerospace systems. The document is published by RTCA, Incorporated, in a joint effort with EUROCAE, and replaces DO-178B. The new document is called DO-178C/ED-12C and was completed in November 2011 and approved by the RTCA in December 2011.  It became available for sale and use in January 2012.The FAA approved AC 20-115C  on 19 Jul 2013, making DO-178C a recognized "acceptable means, but not the only means, for showing compliance with the applicable airworthiness regulations for the software aspects of airborne systems and equipment certification."},
	language = {en},
	urldate = {2021-09-08},
	journal = {Wikipedia},
	month = jul,
	year = {2021},
	note = {Page Version ID: 1032602731},
}

@misc{noauthor_-178b_2021,
	title = {{DO}-{178B}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=DO-178B&oldid=1042198834},
	abstract = {DO-178B, Software Considerations in Airborne Systems and Equipment Certification is a guideline dealing with the safety of safety-critical software used in certain airborne systems. It was jointly developed by the safety-critical working group RTCA SC-167 of the Radio Technical Commission for Aeronautics (RTCA) and WG-12 of the European Organisation for Civil Aviation Equipment (EUROCAE). RTCA  published the document as RTCA/DO-178B, while EUROCAE published the document as ED-12B. Although technically a guideline, it was a de facto standard for developing avionics software systems until it was replaced in 2012 by DO-178C. 
The Federal Aviation Administration (FAA) applies DO-178B as the document it uses for guidance to determine if the software will perform reliably in an airborne environment, when specified by the Technical Standard Order (TSO) for which certification is sought. In the United States, the introduction of TSOs into the airworthiness certification process, and by extension DO-178B, is explicitly established in  Title 14: Aeronautics and Space of the Code of Federal Regulations (CFR), also known as the Federal Aviation Regulations, Part 21, Subpart O.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Wikipedia},
	month = sep,
	year = {2021},
	note = {Page Version ID: 1042198834},
}

@article{macher_bridging_2014,
	title = {Bridging {Automotive} {Systems}, {Safety} and {Software} {Engineering} with a {Seamless} {Toolchain}},
	abstract = {Multi-core technologies strongly support functional integration, e.g. integration of different applications on the same control unit. However, these applications require different safety concepts with different levels of criticality; and providing consistency of the safety concept during the entire product lifecycle is a tedious task. The aim of this paper is to enhance a modeldriven systems and safety engineering framework for multi-core systems, enabling the seamless description of the system, from requirements at the system level down to software component implementation.},
	language = {en},
	author = {Macher, Georg and Armengaud, Eric and Kreiner, Christian},
	year = {2014},
	pages = {9},
}

@article{lutz_software_2000,
	title = {Software {Engineering} for {Safety}: {A} {Roadmap}},
	abstract = {This report describes the current state of software engineering for safety and proposes some directions for needed work that appears to be achievable in the near future.},
	language = {en},
	author = {Lutz, Robyn},
	year = {2000},
	pages = {11},
}

@incollection{yasuura_security_2017,
	address = {Cham},
	title = {Security and {Privacy} in {IoT} {Era}},
	isbn = {978-3-319-55344-3 978-3-319-55345-0},
	url = {http://link.springer.com/10.1007/978-3-319-55345-0_14},
	abstract = {Trends in miniaturization have resulted in an explosion of small, low power devices with network connectivity. Welcome to the era of Internet of Things (IoT), wearable devices, and automated home and industrial systems. These devices are loaded with sensors, collect information from their surroundings, process it, and relay it to remote locations for further analysis. Pervasive and seeminly harmless, this new breed of devices raise security and privacy concerns. In this chapter, we evaluate the security of these devices from an industry point of view, concentrating on the design ﬂow, and catalogue the types of vulnerabilities we have found. We also present an in-depth evaluation of the Google Nest Thermostat, the Nike+ Fuelband SE Fitness Tracker, the Haier SmartCare home automation system, and the Itron Centron CL200 electric meter. We study and present an analysis of the effects of these compromised devices in an every day setting. We then ﬁnish by discussing design ﬂow enhancements, with security mechanisms that can be efﬁciently added into a device in a comparative way.},
	language = {en},
	urldate = {2021-09-07},
	booktitle = {Smart {Sensors} at the {IoT} {Frontier}},
	publisher = {Springer International Publishing},
	author = {Arias, Orlando and Ly, Kelvin and Jin, Yier},
	editor = {Yasuura, Hiroto and Kyung, Chong-Min and Liu, Yongpan and Lin, Youn-Long},
	year = {2017},
	doi = {10.1007/978-3-319-55345-0_14},
	pages = {351--378},
}

@article{gan_leveraging_2019,
	title = {Leveraging {Deep} {Learning} to {Improve} the {Performance} {Predictability} of {Cloud} {Microservices}},
	url = {http://arxiv.org/abs/1905.00968},
	abstract = {Performance unpredictability is a major roadblock towards cloud adoption, and has performance, cost, and revenue ramifications. Predictable performance is even more critical as cloud services transition from monolithic designs to microservices. Detecting QoS violations after they occur in systems with microservices results in long recovery times, as hotspots propagate and amplify across dependent services. We present Seer, an online cloud performance debugging system that leverages deep learning and the massive amount of tracing data cloud systems collect to learn spatial and temporal patterns that translate to QoS violations. Seer combines lightweight distributed RPC-level tracing, with detailed low-level hardware monitoring to signal an upcoming QoS violation, and diagnose the source of unpredictable performance. Once an imminent QoS violation is detected, Seer notifies the cluster manager to take action to avoid performance degradation altogether. We evaluate Seer both in local clusters, and in large-scale deployments of end-to-end applications built with microservices with hundreds of users. We show that Seer correctly anticipates QoS violations 91\% of the time, and avoids the QoS violation to begin with in 84\% of cases. Finally, we show that Seer can identify application-level design bugs, and provide insights on how to better architect microservices to achieve predictable performance.},
	urldate = {2021-09-07},
	journal = {arXiv:1905.00968 [cs]},
	author = {Gan, Yu and Zhang, Yanqi and Hu, Kelvin and Cheng, Dailun and He, Yuan and Pancholi, Meghna and Delimitrou, Christina},
	month = may,
	year = {2019},
	note = {arXiv: 1905.00968
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{noauthor_open-source_2019,
	title = {An {Open}-{Source} {Benchmark} {Suite} for {Cloud} and {IoT} {Microservices}},
	url = {https://deepai.org/publication/an-open-source-benchmark-suite-for-cloud-and-iot-microservices},
	abstract = {05/27/19 - Cloud services have recently started undergoing a major shift from monolithic
applications, to graphs of hundreds of loosely-coupl...},
	urldate = {2021-09-07},
	journal = {DeepAI},
	month = may,
	year = {2019},
}

@inproceedings{gan_open-source_2019,
	address = {Providence RI USA},
	title = {An {Open}-{Source} {Benchmark} {Suite} for {Microservices} and {Their} {Hardware}-{Software} {Implications} for {Cloud} \& {Edge} {Systems}},
	isbn = {978-1-4503-6240-5},
	url = {https://dl.acm.org/doi/10.1145/3297858.3304013},
	doi = {10.1145/3297858.3304013},
	language = {en},
	urldate = {2021-09-07},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
	month = apr,
	year = {2019},
	pages = {3--18},
}

@inproceedings{butler_virtual_2004,
	address = {New Orleans, LA, USA},
	title = {Virtual fences for controlling cows},
	isbn = {978-0-7803-8232-9},
	url = {http://ieeexplore.ieee.org/document/1302415/},
	doi = {10.1109/ROBOT.2004.1302415},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}, 2004. {Proceedings}. {ICRA} '04. 2004},
	publisher = {IEEE},
	author = {Butler, Z. and Corke, P. and Peterson, R. and Rus, D.},
	year = {2004},
	pages = {4429--4436 Vol.5},
}

@incollection{ammari_decade_2014,
	address = {Berlin, Heidelberg},
	title = {A {Decade} of {Wireless} {Sensing} {Applications}: {Survey} and {Taxonomy}},
	isbn = {978-3-642-40008-7 978-3-642-40009-4},
	shorttitle = {A {Decade} of {Wireless} {Sensing} {Applications}},
	url = {http://link.springer.com/10.1007/978-3-642-40009-4_2},
	abstract = {The popularity of low-power wireless sensors increased signiﬁcantly in the last decade, triggering a golden era for wireless sensor network research and development. During the early years of the twenty-ﬁrst century, wireless sensor network applications have evolved from small demonstrations with a lifetime of only a few hours to complete systems made up of hundreds of tiny wireless nodes deployed in a wide variety of settings, ranging from harsh and remote environments to residential buildings and clinical units. This survey gives an overview of the most relevant applications of wireless sensor network applications deployed during the last ten years, and classiﬁes them using a novel taxonomy that aims to help identifying relevant programming constructs and run-time services. With more than 60 applications reviewed, ranging from military and civilian surveillance to tracking systems, from environmental and structural monitoring to home and building automation, from agriculture and industrial settings to health care, this survey will serve as a reference to guide researchers and system designers.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {The {Art} of {Wireless} {Sensor} {Networks}},
	publisher = {Springer Berlin Heidelberg},
	author = {Oppermann, Felix Jonathan and Boano, Carlo Alberto and Römer, Kay},
	editor = {Ammari, Habib M.},
	year = {2014},
	doi = {10.1007/978-3-642-40009-4_2},
	note = {Series Title: Signals and Communication Technology},
	pages = {11--50},
}

@article{ning_cyberentity_2013,
	title = {Cyberentity {Security} in the {Internet} of {Things}},
	volume = {46},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/6475947/},
	doi = {10.1109/MC.2013.74},
	language = {en},
	number = {4},
	urldate = {2021-09-06},
	journal = {Computer},
	author = {Ning, Huansheng and Liu, Hong and Yang, Laurence T.},
	month = apr,
	year = {2013},
	pages = {46--53},
}

@inproceedings{zhou_cloudthings_2013,
	address = {Whistler, BC, Canada},
	title = {{CloudThings}: {A} common architecture for integrating the {Internet} of {Things} with {Cloud} {Computing}},
	isbn = {978-1-4673-6085-2 978-1-4673-6084-5 978-1-4673-6083-8},
	shorttitle = {{CloudThings}},
	url = {http://ieeexplore.ieee.org/document/6581037/},
	doi = {10.1109/CSCWD.2013.6581037},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 2013 {IEEE} 17th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	publisher = {IEEE},
	author = {Zhou, Jiehan and Leppanen, Teemu and Harjula, Erkki and Ylianttila, Mika and Ojala, Timo and Yu, Chen and Jin, Hai},
	month = jun,
	year = {2013},
	pages = {651--657},
}

@inproceedings{buckl_services_2009,
	address = {Bradford, United Kingdom},
	title = {Services to the {Field}: {An} {Approach} for {Resource} {Constrained} {Sensor}/{Actor} {Networks}},
	isbn = {978-1-4244-3999-7},
	shorttitle = {Services to the {Field}},
	url = {http://ieeexplore.ieee.org/document/5136693/},
	doi = {10.1109/WAINA.2009.20},
	abstract = {More and more devices become network enabled and are integrated within one large, distributed system. The serviceoriented paradigm is the main concept to implement this approach and to cope with the heterogeneity of the underlying network. However, resource constraints imposed by the underlying hardware, such as 8-Bit micro controllers, require efﬁcient protocols and often prohibit the use of technologies known from the Web service domain, the major implementation of the service-oriented paradigm. Nevertheless, a quick and seamless information ﬂow between embedded devices and Web services is an important requirement for many application scenarios, e.g., real-time aware production management or the Internet of Things. Within this paper, we present an approach that allows to proﬁt from the beneﬁts of traditional SOA implementations, such as Web service interfaces and an IP compatible addressing scheme, and on the other hand can be implemented on resource constraint devices. The main idea is to use a data-centric processing paradigm at the device level and a gateway that mediates between the Web service and the embedded device world.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {2009 {International} {Conference} on {Advanced} {Information} {Networking} and {Applications} {Workshops}},
	publisher = {IEEE},
	author = {Buckl, Christian and Sommer, Stephan and Scholz, Andreas and Knoll, Alois and Kemper, Alfons and Heuer, Jörg and Schmitt, Anton},
	month = may,
	year = {2009},
	pages = {476--481},
}

@article{gershenfeld2004internet,
	title = {The internet of things},
	volume = {291},
	number = {4},
	journal = {Scientific American},
	author = {Gershenfeld, Neil and Krikorian, Raffi and Cohen, Danny},
	year = {2004},
	note = {Publisher: JSTOR},
	pages = {76--81},
}

@article{akyildiz_wireless_2002,
	title = {Wireless sensor networks: a survey},
	abstract = {This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors inﬂuencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed. Ó 2002 Published by Elsevier Science B.V.},
	language = {en},
	journal = {Computer Networks},
	author = {Akyildiz, I F and Su, W and Sankarasubramaniam, Y and Cayirci, E},
	year = {2002},
	pages = {30},
}

@incollection{floerkemeier_sensor_2008,
	address = {Berlin, Heidelberg},
	title = {Sensor {Applications} in the {Supply} {Chain}: {The} {Example} of {Quality}-{Based} {Issuing} of {Perishables}},
	volume = {4952},
	isbn = {978-3-540-78730-3 978-3-540-78731-0},
	shorttitle = {Sensor {Applications} in the {Supply} {Chain}},
	url = {http://link.springer.com/10.1007/978-3-540-78731-0_9},
	abstract = {Miniaturization and price decline are increasingly allowing for the use of RFID tags and sensors in inter-organizational supply chain applications. This contribution aims at investigating the potential of sensor-based issuing policies on product quality in the perishables supply chain. We develop a simple simulation model that allows us to study the quality of perishable goods at a retailer under different issuing policies at the distributor. Our results show that policies that rely on automatically collected expiry dates and product quality bear the potential to improve the quality of items in stores with regard to mean quality and standard deviation.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {The {Internet} of {Things}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dada, Ali and Thiesse, Frédéric},
	editor = {Floerkemeier, Christian and Langheinrich, Marc and Fleisch, Elgar and Mattern, Friedemann and Sarma, Sanjay E.},
	year = {2008},
	doi = {10.1007/978-3-540-78731-0_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {140--154},
}

@article{babun_real-time_2019,
	title = {Real-time {Analysis} of {Privacy}-(un)aware {IoT} {Applications}},
	url = {http://arxiv.org/abs/1911.10461},
	abstract = {Users trust IoT apps to control and automate their smart devices. These apps necessarily have access to sensitive data to implement their functionality. However, users lack visibility into how their sensitive data is used (or leaked), and they often blindly trust the app developers. In this paper, we present IOTWATCH, a novel dynamic analysis tool that uncovers the privacy risks of IoT apps in real-time. We designed and built IOTWATCH based on an IoT privacy survey that considers the privacy needs of IoT users. IOTWATCH provides users with a simple interface to specify their privacy preferences with an IoT app. Then, in runtime, it analyzes both the data that is sent out of the IoT app and its recipients using Natural Language Processing (NLP) techniques. Moreover, IOTWATCH informs the users with its ﬁndings to make them aware of the privacy risks with the IoT app. We implemented IOTWATCH on real IoT applications. Speciﬁcally, we analyzed 540 IoT apps to train the NLP model and evaluate its effectiveness. IOTWATCH successfully classiﬁes IoT app data sent to external parties to correct privacy labels with an average accuracy of 94.25\%, and ﬂags IoT apps that leak privacy data to unauthorized parties. Finally, IOTWATCH yields minimal overhead to an IoT app’s execution, on average 105 ms additional latency.},
	language = {en},
	urldate = {2021-09-06},
	journal = {arXiv:1911.10461 [cs]},
	author = {Babun, Leonardo and Celik, Z. Berkay and McDaniel, Patrick and Uluagac, A. Selcuk},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.10461},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{rondon_lightningstrike_2021,
	address = {Abu Dhabi United Arab Emirates},
	title = {{LightningStrike}: (in)secure practices of {E}-{IoT} systems in the wild},
	isbn = {978-1-4503-8349-3},
	shorttitle = {{LightningStrike}},
	url = {https://dl.acm.org/doi/10.1145/3448300.3467830},
	doi = {10.1145/3448300.3467830},
	abstract = {The widespread adoption of specialty smart ecosystems has changed the everyday lives of users. As a part of smart ecosystems, Enterprise Internet of Things (E-IoT) allows users to integrate and control more complex installations in comparison to off-the-shelf IoT systems. With E-IoT, users have a complete control of audio, video, scheduled events, lightning fixtures, shades, door access, and relays via available user interfaces. As such, these systems see widespread use in government or smart private offices, schools, smart buildings, professional conference rooms, hotels, smart homes, yachts, and similar professional settings. However, even with their widespread use, the security of many E-IoT systems has not been researched in the literature. Further, many E-IoT systems utilize proprietary communication protocols that rely mostly on security through obscurity, which has perhaps led many users to mistakenly assume that these systems are secure. To address this open research problem and determine if E-IoT systems are vulnerable, we focus on one of the core E-IoT components, E-IoT communication buses. Communication buses are used by E-IoT proprietary protocols to connect multiple E-IoT devices (e.g., keypads and touchscreens) and trigger pre-configured events upon user actions. In this study, we introduce LightningStrike, the implementation of four proof-ofconcept attacks that demonstrate several weaknesses in E-IoT proprietary communication protocols through communication buses. With LightningStrike, we show that it is feasible for an attacker to compromise E-IoT systems using E-IoT communication buses. We demonstrate that popular E-IoT proprietary communication protocols are susceptible to Denial-of-Service, eavesdropping, impersonation, and replay attacks. As E-IoT systems control physical access, safety components, and emergency equipment, an attacker with a low level of knowledge and effort can easily exploit E-IoT vulnerabilities to impact the security and safety of users, smart systems, and smart buildings worldwide.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 14th {ACM} {Conference} on {Security} and {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {ACM},
	author = {Rondon, Luis Puche and Babun, Leonardo and Aris, Ahmet and Akkaya, Kemal and Uluagac, A. Selcuk},
	month = jun,
	year = {2021},
	pages = {106--116},
}

@inproceedings{rondon_poisonivy_2020,
	address = {Virtual Event Japan},
	title = {{PoisonIvy}: ({In})secure {Practices} of {Enterprise} {IoT} {Systems} in {Smart} {Buildings}},
	isbn = {978-1-4503-8061-4},
	shorttitle = {{PoisonIvy}},
	url = {https://dl.acm.org/doi/10.1145/3408308.3427606},
	doi = {10.1145/3408308.3427606},
	abstract = {The rise of IoT devices has led to the proliferation of smart buildings, offices, and homes worldwide. Although commodity IoT devices are employed by ordinary end-users, complex environments such as smart buildings, government, or private smart offices, conference rooms, or hospitality require customized and highly reliable solutions. Those systems called Enterprise Internet of Things (EIoT) connect such environments to the Internet and are professionally managed solutions usually offered by dedicated vendors (e.g., Control4, Crestron, Lutron, etc.). As EIoT systems require specialized training, software, and equipment to deploy, many of these systems are closed-source and proprietary in nature. This has led to very little research investigating the security of EIoT systems and their components. In effect, EIoT systems in smart settings such as smart buildings present an unprecedented and unexplored threat vector for an attacker. In this work, we explore EIoT system vulnerabilities and insecure development practices. Specifically, focus on the usage of drivers as an attack mechanism, and introduce PoisonIvy, a number of novel attacks that demonstrate how it is possible for an attacker to easily attack and command EIoT system controllers using malicious drivers. Specifically, we show how drivers used to integrate third-party services and devices to EIoT systems can be trivially misused in a systematic fashion. To demonstrate the capabilities of attackers, we implement and evaluate PoisonIvy using a testbed of real EIoT devices in a smart building setting. We show that an attacker can easily perform DoS attacks, gain remote control, and maliciously abuse system resources (e.g., bitcoin mining) of EIoT systems. Further, we discuss the (in)securities in drivers and possible countermeasures. To the best of our knowledge, this is the first work to analyze the (in)securities of EIoT deployment practices and demonstrate the associated vulnerabilities in this ecosystem. With this work, we raise awareness on the (in)secure development practices used for EIoT systems, the consequences of which can largely impact the security, privacy, safety, reliability, and performance of millions, if not billions, of EIoT systems worldwide.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 7th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {ACM},
	author = {Rondon, Luis Puche and Babun, Leonardo and Aris, Ahmet and Akkaya, Kemal and Uluagac, A. Selcuk},
	month = nov,
	year = {2020},
	pages = {130--139},
}

@inproceedings{sikder_aegis_2019,
	address = {San Juan Puerto Rico USA},
	title = {Aegis: a context-aware security framework for smart home systems},
	isbn = {978-1-4503-7628-0},
	shorttitle = {Aegis},
	url = {https://dl.acm.org/doi/10.1145/3359789.3359840},
	doi = {10.1145/3359789.3359840},
	abstract = {Our everyday lives are expanding fast with the introduction of new Smart Home Systems (SHSs). Today, a myriad of SHS devices and applications are widely available to users and have already started to re-define our modern lives. Smart home users utilize the apps to control and automate such devices. Users can develop their own apps or easily download and install them from vendor-specific app markets. App-based SHSs offer many tangible benefits to our lives, but also unfold diverse security risks. Several attacks have already been reported for SHSs. However, current security solutions consider smart home devices and apps individually to detect malicious actions rather than the context of the SHS as a whole. The existing mechanisms cannot capture user activities and sensor-device-user interactions in a holistic fashion. To address these issues, in this paper, we introduce Aegis, a novel context-aware security framework to detect malicious behavior in a SHS. Specifically, Aegis observes the states of the connected smart home entities (sensors and devices) for different user activities and usage patterns in a SHS and builds a contextual model to differentiate between malicious and benign behavior. We evaluated the efficacy and performance of Aegis in multiple smart home settings (i.e., single bedroom, double bedroom, duplex) with real-life users performing day-to-day activities and real SHS devices. We also measured the performance of Aegis against five different malicious behaviors. Our detailed evaluation shows that Aegis can detect malicious behavior in SHS with high accuracy (over 95\%) and secure the SHS regardless of the smart home layout, device configuration, installed apps, and enforced user policies. Finally, Aegis achieves minimum overhead in detecting malicious behavior in SHS, ensuring easy deployability in real-life smart environments.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Proceedings of the 35th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Sikder, Amit Kumar and Babun, Leonardo and Aksu, Hidayet and Uluagac, A. Selcuk},
	month = dec,
	year = {2019},
	pages = {28--41},
}

@article{celik_verifying_2019,
	title = {Verifying {Internet} of {Things} {Safety} and {Security} in {Physical} {Spaces}},
	volume = {17},
	issn = {1540-7993, 1558-4046},
	url = {https://ieeexplore.ieee.org/document/8733994/},
	doi = {10.1109/MSEC.2019.2911511},
	language = {en},
	number = {5},
	urldate = {2021-09-06},
	journal = {IEEE Security \& Privacy},
	author = {Celik, Z. Berkay and McDaniel, Patrick and Tan, Gang and Babun, Leonardo and Uluagac, A. Selcuk},
	month = sep,
	year = {2019},
	pages = {30--37},
}

@inproceedings{datta_valve_2020,
	address = {Taipei Taiwan},
	title = {Valve: {Securing} {Function} {Workflows} on {Serverless} {Computing} {Platforms}},
	isbn = {978-1-4503-7023-3},
	shorttitle = {Valve},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380173},
	doi = {10.1145/3366423.3380173},
	abstract = {Serverless Computing has quickly emerged as a dominant cloud computing paradigm, allowing developers to rapidly prototype eventdriven applications using a composition of small functions that each perform a single logical task. However, many such application workflows are based in part on publicly-available functions developed by third-parties, creating the potential for functions to behave in unexpected, or even malicious, ways. At present, developers are not in total control of where and how their data is flowing, creating significant security and privacy risks in growth markets that have embraced serverless (e.g., IoT).},
	language = {en},
	urldate = {2021-09-02},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Datta, Pubali and Kumar, Prabuddha and Morris, Tristan and Grace, Michael and Rahmati, Amir and Bates, Adam},
	month = apr,
	year = {2020},
	pages = {939--950},
}

@misc{noauthor_mixed-method_nodate,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0164121218302735?token=296A8F69C4D6DDED6E7076AFB02E6863649FA17AEDAE2F3163F9885605C7C7426783763C70C137E7EE5B59380E4BE1D5&originRegion=us-east-1&originCreation=20210902140209},
	language = {en},
	urldate = {2021-09-02},
	doi = {10.1016/j.jss.2018.12.013},
}

@article{thibaud_internet_2018,
	title = {Internet of {Things} ({IoT}) in high-risk {Environment}, {Health} and {Safety} ({EHS}) industries: {A} comprehensive review},
	volume = {108},
	issn = {0167-9236},
	shorttitle = {Internet of {Things} ({IoT}) in high-risk {Environment}, {Health} and {Safety} ({EHS}) industries},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923618300344},
	doi = {10.1016/j.dss.2018.02.005},
	abstract = {The rise of ubiquitous systems is sustained by the development and progressive adoption of the Internet of Things (IoT) devices and their enabling technologies. IoT has been shown to have significant potential in high-risk Environment, Health, and Safety (EHS) industries. In these industries, human lives are at stake and IoT-based applications are primed to offer safe, reliable, and efficient solutions due to their ability to operate at a fine granular level and provide rich low-level information. We review existing published research on IoT-based applications in high-risk EHS industries with specific emphasis on healthcare industry, food supply chain (FSC), mining and energy industries (oil \& gas and nuclear), intelligent transportation (e.g., connected vehicles), and building \& infrastructure management for emergency response operations until 2016. We also highlight IoT-related challenges and proposed solutions in high risk EHS industries. We then conclude by presenting research challenges and expected trends for IoT in these industries.},
	language = {en},
	urldate = {2021-09-02},
	journal = {Decision Support Systems},
	author = {Thibaud, Montbel and Chi, Huihui and Zhou, Wei and Piramuthu, Selwyn},
	month = apr,
	year = {2018},
	keywords = {Connected vehicles, Environment Health and Safety (EHS), Food supply chain, Healthcare, Internet of Things (IoT), Smart city},
	pages = {79--95},
}

@article{conway1968committees,
	title = {How do committees invent},
	volume = {14},
	number = {4},
	journal = {Datamation},
	author = {Conway, Melvin E},
	year = {1968},
	pages = {28--31},
}

@article{sreekanti_fault-tolerance_2020,
	title = {A {Fault}-{Tolerance} {Shim} for {Serverless} {Computing}},
	url = {http://arxiv.org/abs/2003.06007},
	abstract = {Serverless computing has grown in popularity in recent years, with an increasing number of applications being built on Functions-as-a-Service (FaaS) platforms. By default, FaaS platforms support retry-based fault tolerance, but this is insufficient for programs that modify shared state, as they can unwittingly persist partial sets of updates in case of failures. To address this challenge, we would like atomic visibility of the updates made by a FaaS application. In this paper, we present AFT, an atomic fault tolerance shim for serverless applications. AFT interposes between a commodity FaaS platform and storage engine and ensures atomic visibility of updates by enforcing the read atomic isolation guarantee. AFT supports new protocols to guarantee read atomic isolation in the serverless setting. We demonstrate that aft introduces minimal overhead relative to existing storage engines and scales smoothly to thousands of requests per second, while preventing a significant number of consistency anomalies.},
	urldate = {2021-09-02},
	journal = {arXiv:2003.06007 [cs]},
	author = {Sreekanti, Vikram and Wu, Chenggang and Chhatrapati, Saurav and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Faleiro, Jose M.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06007},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{marquez_actual_2018,
	title = {Actual {Use} of {Architectural} {Patterns} in {Microservices}-{Based} {Open} {Source} {Projects}},
	doi = {10.1109/APSEC.2018.00017},
	abstract = {Microservice-based systems instantiate an architectural style that conceives of systems as sets of modular, customer-centric, independent, and scalable services. These systems express a similar essential structural organization and seems appropriate to design them using architectural patterns because these combine an understanding of the system domain and good practices. Code repository platforms provide the developer community with ideas and examples about microservice systems, but since they are in early adoption, there is still no clear notion of which actual microservice systems incarnate architectural patterns (if any), reducing the use of frameworks and the achievement of quality attributes. This paper extends a previous study on architectural patterns for microservices in academic and industry sources. We explored which architectural patterns for microservices are used in actual microservice-based open source systems, by subjecting thirty well-known open source projects to a comprehensive multi-criteria code and design review. We found that (1) open source projects use only a few architectural patterns broadly; (2) most projects use the same few frameworks; (3) there are very few microservice architectural patterns as such; and (4) what most projects use (what was previously called) are SOA patterns. This study shows that microservice systems builders do use architectural patterns, but only a few of them. It remains to be determined whether additional patterns would be productively used to build microservice systems, or the few ones currently used are the only ones actually necessary.},
	booktitle = {2018 25th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Márquez, Gastón and Astudillo, Hernán},
	month = dec,
	year = {2018},
	note = {ISSN: 2640-0715},
	keywords = {Architectural patterns, Companies, Computer architecture, Correlation, Framework, Industries, Market research, Microservices, Open source projects, Quality attributes, Scalability, Service-oriented architecture},
	pages = {31--40},
}

@inproceedings{zhang_microservice_2019,
	title = {Microservice {Architecture} in {Reality}: {An} {Industrial} {Inquiry}},
	shorttitle = {Microservice {Architecture} in {Reality}},
	doi = {10.1109/ICSA.2019.00014},
	abstract = {Background: Seeking an appropriate architecture for a software design is always a challenge in recent decades. Although microservices as a lightweight architecture style is claimed that can improve the current practices with several characteristics, many practices are based upon the different circumstances and reflect the variant effects. An empirical inquiry brings us a systematic insight into the industrial practices on microservices. Objective: This study is to investigate the gap between the ideal visions and real industrial practices on microservices and what benefits we can gain from the industrial experiences. Method: We carried out a series of industrial interviews with thirteen different types of companies. The collected data were then codified according to the defined qualitative methods. Results: We characterized the gaps between the typical characteristics accepted in the community and the industrial practices of microservices. Furthermore, the compromise between benefits and sufferings of microservices around these nine dimensions were also investigated. Conclusion: We confirmed the benefits of the microservices that can be obtained from practice as well as their possible pains that need to be addressed with extra expense from experiences. Besides, some outlined pains, e.g., organizational transformation, decomposition, distributed monitoring, and bug localization, may inspire researchers to conduct the further research.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Software} {Architecture} ({ICSA})},
	author = {Zhang, He and Li, Shanshan and Jia, Zijia and Zhong, Chenxing and Zhang, Cheng},
	month = mar,
	year = {2019},
	keywords = {Companies, Computer architecture, Industries, Instruments, Interviews, Pain, Software, empirical study, interview, microservices, pains},
	pages = {51--60},
}

@inproceedings{haselbock_expert_2018,
	title = {An {Expert} {Interview} {Study} on {Areas} of {Microservice} {Design}},
	doi = {10.1109/SOCA.2018.00028},
	abstract = {Microservices are single-responsibility units that are implemented in various technologies by independent, cross-cutting teams. A shift to a microservice architecture therefore touches many different areas, including system design, organizational structures, and runtime infrastructure. To investigate the importance of different areas of microservice design, we interviewed 10 microservice domain experts to find out which design areas are relevant for microservices, how important they are, and why they are important. This paper presents the resulting microservice design areas, assessments of their importance, and rationales for the provided assessments.},
	booktitle = {2018 {IEEE} 11th {Conference} on {Service}-{Oriented} {Computing} and {Applications} ({SOCA})},
	author = {Haselböck, Stefan and Weinreich, Rainer and Buchgeher, Georg},
	month = nov,
	year = {2018},
	note = {ISSN: 2163-2871},
	keywords = {Cloud computing, Companies, Computer architecture, Encoding, Interviews, Monitoring, Software engineering, expert interview study, microservice design areas, microservices},
	pages = {137--144},
}

@inproceedings{liu_microservices_2020,
	title = {Microservices: architecture, container, and challenges},
	shorttitle = {Microservices},
	doi = {10.1109/QRS-C51114.2020.00107},
	abstract = {Microservices are emerging as a new computing paradigm which is a suitable complementation of cloud computing. Microservices will decompose traditional monolithic applications into a set of fine-grained services, which can be independently developed, tested, and deployed. However, there are many challenges of microservices. This paper provides a comprehensive overview of microservices. More specifically, firstly, we systematically compare traditional monolithic architecture, service-oriented architecture (SOA), and microservices architecture. Secondly, we give an overview of the container technology. Finally, we outline the technical challenges of microservices, such as performance, debugging and data consistency.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Liu, Guozhi and Huang, Bi and Liang, Zhihong and Qin, Minmin and Zhou, Hua and Li, Zhang},
	month = dec,
	year = {2020},
	keywords = {Computer architecture, Containers, Debugging, Security, Service-oriented architecture, Software quality, Software reliability, container, debugging, microservices, monolithic architecture, performance, service-oriented architecture},
	pages = {629--635},
}

@article{cerny_contextual_2018,
	title = {Contextual understanding of microservice architecture: current and future directions},
	volume = {17},
	issn = {1559-6915},
	shorttitle = {Contextual understanding of microservice architecture},
	url = {https://doi.org/10.1145/3183628.3183631},
	doi = {10.1145/3183628.3183631},
	abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
	number = {4},
	urldate = {2021-09-01},
	journal = {ACM SIGAPP Applied Computing Review},
	author = {Cerny, Tomas and Donahoo, Michael J. and Trnka, Michal},
	month = jan,
	year = {2018},
	keywords = {SOA, architectures, microservices, self-contained systems, survey, systematic mapping study},
	pages = {29--45},
}

@inproceedings{cerny_disambiguation_2017,
	address = {Krakow Poland},
	title = {Disambiguation and {Comparison} of {SOA}, {Microservices} and {Self}-{Contained} {Systems}},
	isbn = {978-1-4503-5027-3},
	url = {https://dl.acm.org/doi/10.1145/3129676.3129682},
	doi = {10.1145/3129676.3129682},
	abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key diﬀerences between these two approaches and their features, we can design a more eﬀective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the diﬀerences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {ACM},
	author = {Cerny, Tomas and Donahoo, Michael J. and Pechanec, Jiri},
	month = sep,
	year = {2017},
	pages = {228--235},
}

@inproceedings{cerny_disambiguation_2017-1,
	address = {Krakow Poland},
	title = {Disambiguation and {Comparison} of {SOA}, {Microservices} and {Self}-{Contained} {Systems}},
	isbn = {978-1-4503-5027-3},
	url = {https://dl.acm.org/doi/10.1145/3129676.3129682},
	doi = {10.1145/3129676.3129682},
	abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key diﬀerences between these two approaches and their features, we can design a more eﬀective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the diﬀerences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {ACM},
	author = {Cerny, Tomas and Donahoo, Michael J. and Pechanec, Jiri},
	month = sep,
	year = {2017},
	pages = {228--235},
}

@inproceedings{munaf_microservices_2019,
	title = {Microservices {Architecture}: {Challenges} and {Proposed} {Conceptual} {Design}},
	shorttitle = {Microservices {Architecture}},
	doi = {10.1109/COMTECH.2019.8737831},
	abstract = {Microservices Architecture has evolved in the recent past and has gained significant popularity offering various benefits as compared to existing architectures addressing various serious concerns of the recent era in Software Engineering. This paper will first briefly introduce the microservices architecture and its evolution being still in inception phase. After enlisting its offered benefits, its envisaged implementation challenges will be addressed including various options available for coping up with those challenges by using empirical and conceptual method. Based on the challenges and available options, a conceptual design of microservices architecture including major components and their role will be proposed. This paper addresses the new comers for understanding of microservices architecture, researchers and the practitioners for verification of evolved conceptual design and future prospects.},
	booktitle = {2019 {International} {Conference} on {Communication} {Technologies} ({ComTech})},
	author = {Munaf, Raja Mubashir and Ahmed, Jawwad and Khakwani, Faraz and Rana, Tauseef},
	month = mar,
	year = {2019},
	keywords = {Computer architecture, Databases, Fine grained SOA, Microservices, Microservices Architecture, SOA, Scalability, Service Oriented Architecture, Service-oriented architecture, Software Architecture, Software systems, Task analysis},
	pages = {82--87},
}

@inproceedings{liu_revealer_2021,
	address = {San Francisco, CA, USA},
	title = {Revealer: {Detecting} and {Exploiting} {Regular} {Expression} {Denial}-of-{Service} {Vulnerabilities}},
	isbn = {978-1-72818-934-5},
	shorttitle = {Revealer},
	url = {https://ieeexplore.ieee.org/document/9519406/},
	doi = {10.1109/SP40001.2021.00062},
	abstract = {Regular expression Denial-of-Service (ReDoS) is a class of algorithmic complexity attacks. Attackers can craft particular strings to trigger the worst-case super-linear matching time of some vulnerable regular expressions (regex) with extended features that are commonly supported by popular programming languages. ReDoS attacks can severely degrade the performance of web applications, which extensively employ regexes in their server-side logic. Nevertheless, the characteristics of vulnerable regexes with extended features remain understudied, making it difficult to mitigate or even detect such vulnerabilities.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Liu, Yinxi and Zhang, Mingxue and Meng, Wei},
	month = may,
	year = {2021},
	pages = {1468--1484},
}

@inproceedings{alshuqayran_systematic_2016,
	title = {A {Systematic} {Mapping} {Study} in {Microservice} {Architecture}},
	doi = {10.1109/SOCA.2016.15},
	abstract = {The accelerating progress of network speed, reliability and security creates an increasing demand to move software and services from being stored and processed locally on users' machines to being managed by third parties that are accessible through the network. This has created the need to develop new software development methods and software architectural styles that meet these new demands. One such example in software architectural design is the recent emergence of the microservices architecture to address the maintenance and scalability demands of online service providers. As microservice architecture is a new research area, the need for a systematic mapping study is crucial in order to summarise the progress so far and identify the gaps and requirements for future studies. In this paper we present a systematic mapping study of microservices architectures and their implementation. Our study focuses on identifying architectural challenges, the architectural diagrams/views and quality attributes related to microsevice systems.},
	booktitle = {2016 {IEEE} 9th {International} {Conference} on {Service}-{Oriented} {Computing} and {Applications} ({SOCA})},
	author = {Alshuqayran, Nuha and Ali, Nour and Evans, Roger},
	month = nov,
	year = {2016},
	keywords = {Business, Computer architecture, Conferences, Security, Service-oriented architecture, Systematics},
	pages = {44--51},
}

@inproceedings{pereira-vale_security_2019,
	title = {Security {Mechanisms} {Used} in {Microservices}-{Based} {Systems}: {A} {Systematic} {Mapping}},
	shorttitle = {Security {Mechanisms} {Used} in {Microservices}-{Based} {Systems}},
	doi = {10.1109/CLEI47609.2019.235060},
	abstract = {Microservices is an architectural style that conceives systems as a modular, costumer, independent and scalable suite of services; it offers several advantages but its growing popularity has given rise to security challenges. Building secure systems is greatly helped by deploying existing security mechanisms, but current literature does not guide developers about which mechanisms are actually used by developers of microservices-based systems. This article describes the design and results of a systematic mapping study to identify the security mechanisms used in microservices-based systems described in the literature. The study yielded 321 articles, of which 26 are primary studies. Key findings are that (i) the studies mention 18 security mechanisms; (ii) the most mentioned security mechanisms are authentication, authorization and credentials; and (iii) almost 2/3 of security mechanisms focus on stopping or mitigating attacks, but none on recovering from them. Additionally, it emerges that experiments and case studies are the most used empirical strategies in microservices security research. The clear identification of most-used security solutions will facilitate the reuse of existing architectural knowledge to address security problems in microservices-based systems.},
	booktitle = {2019 {XLV} {Latin} {American} {Computing} {Conference} ({CLEI})},
	author = {Pereira-Vale, Anelis and Márquez, Gastón and Astudillo, Hernán and Fernandez, Eduardo B.},
	month = sep,
	year = {2019},
	keywords = {Authentication, Cloud computing, Computer architecture, Conferences, Software, Systematics, microservices-based systems, secure software development, security mechanisms, systematic mapping},
	pages = {01--10},
}

@article{de_toledo_identifying_2021,
	title = {Identifying architectural technical debt, principal, and interest in microservices: {A} multiple-case study},
	volume = {177},
	issn = {0164-1212},
	shorttitle = {Identifying architectural technical debt, principal, and interest in microservices},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121221000650},
	doi = {10.1016/j.jss.2021.110968},
	abstract = {Background:
Using a microservices architecture is a popular strategy for software organizations to deliver value to their customers fast and continuously. However, scientific knowledge on how to manage architectural debt in microservices is scarce.
Objectives:
In the context of microservices applications, this paper aims to identify architectural technical debts (ATDs), their costs, and their most common solutions.
Method:
We conducted an exploratory multiple case study by conducting 25 interviews with practitioners working with microservices in seven large companies.
Results:
We found 16 ATD issues, their negative impact (interest), and common solutions to repay each debt together with the related costs (principal). Two examples of critical ATD issues found were the use of shared databases that, if not properly planned, leads to potential breaks on services every time the database schema changes and bad API designs, which leads to coupling among teams. We identified ATDs occurring in different domains and stages of development and created a map of the relationships among those debts.
Conclusion:
The findings may guide organizations in developing microservices systems that better manage and avoid architectural debts.},
	language = {en},
	urldate = {2021-09-01},
	journal = {Journal of Systems and Software},
	author = {de Toledo, Saulo S. and Martini, Antonio and Sjøberg, Dag I. K.},
	month = jul,
	year = {2021},
	keywords = {Cost of software, Cross-company study, Qualitative analysis, Software maintainability, Software quality},
	pages = {110968},
}

@inproceedings{soares_de_toledo_architectural_2019,
	title = {Architectural {Technical} {Debt} in {Microservices}: {A} {Case} {Study} in a {Large} {Company}},
	shorttitle = {Architectural {Technical} {Debt} in {Microservices}},
	doi = {10.1109/TechDebt.2019.00026},
	abstract = {Introduction: Software companies aim to achieve continuous delivery to constantly provide value to their customers. A popular strategy is to use microservices architecture. However, such an architecture is also subject to debt, which hinders the continuous delivery process and thus negatively affects the software released to the customers. Objectives: The aim of this study is to identify issues, solutions and risks related to Architecture Technical Debt in microservices. Method: We conducted an exploratory case study of a real life project with about 1000 services in a large, international company. Through qualitative analysis of documents and interviews, we investigated Architecture Technical Debt in the communication layer of a system with microservices architecture. Results: Our main contributions are a list of Architecture Technical Debt issues specific for the communication layer in a system with microservices architecture, as well as their associated negative impact (interest), a solution to repay the debt and the its cost (principal). Among the found Architecture Technical Debt issues were the existence of business logic in the communication layer and a high amount of point-to-point connections between services. The studied solution consists of the implementation of different canonical models specific to different domains, the removal of business logic from the communication layer, and migration from services to use the communication layer correctly. We also contributed with a list of possible risks that can affect the payment of the debt, as lack of funding and inadequate prioritization. Conclusion: We found issues, solutions and possible risks that are specific for microservices architectures not yet encountered in the current literature. Our results may be useful for practitioners that want to avoid or repay Technical Debt in their microservices architecture.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Technical} {Debt} ({TechDebt})},
	author = {Soares de Toledo, Saulo and Martini, Antonio and Przybyszewska, Agata and Sjøberg, Dag I.K.},
	month = may,
	year = {2019},
	keywords = {Companies, Computer architecture, Data models, Informatics, Software, Technical Debt, Architecture, Microservices, Case Study},
	pages = {78--87},
}

@inproceedings{wen_empirical_2021,
	address = {Athens Greece},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, ∗Jinfeng Wen and Zhenpeng Chen made equal contributions to this work.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	keywords = {Empirical study, Serverless computing},
	pages = {416--428},
}

@inproceedings{wen_empirical_2021-1,
	address = {Athens Greece},
	title = {An empirical study on challenges of application development in serverless computing},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468558},
	doi = {10.1145/3468264.3468558},
	abstract = {Serverless computing is an emerging paradigm for cloud computing, gaining traction in a wide range of applications such as video processing and machine learning. This new paradigm allows developers to focus on the development of the logic of serverless computing based applications (abbreviated as serverless-based applications) in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, it also introduces new challenges on the design, implementation, and deployment of serverless-based applications, and current serverless computing platforms are far away from satisfactory. However, to the best of our knowledge, these challenges have not been well studied. To fill this knowledge gap, this paper presents the first comprehensive study on understanding the challenges in developing serverless-based applications from the developers’ perspective. We mine and analyze 22,731 relevant questions from Stack Overflow (a popular Q\&A website for developers), and show the increasing popularity trend and the high difficulty level of serverless computing for developers. Through manual inspection of 619 sampled questions, we construct a taxonomy of challenges that developers encounter, and report a series of findings and actionable implications. Stakeholders including application developers, researchers, ∗Jinfeng Wen and Zhenpeng Chen made equal contributions to this work.},
	language = {en},
	urldate = {2021-09-01},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Wen, Jinfeng and Chen, Zhenpeng and Liu, Yi and Lou, Yiling and Ma, Yun and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
	month = aug,
	year = {2021},
	pages = {416--428},
}

@misc{bidlingmaier_additional_2021,
	title = {An additional non-backtracking {RegExp} engine},
	url = {https://v8.dev/blog/non-backtracking-regexp},
	author = {Bidlingmaier, Martin},
	year = {2021},
}

@article{zalewski_iot_2019,
	title = {{IoT} {Safety}: {State} of the {Art}},
	volume = {21},
	issn = {1941-045X},
	shorttitle = {{IoT} {Safety}},
	doi = {10.1109/MITP.2018.2883858},
	abstract = {Reports on safety issues relating to the Internet of Things (IoT). Users, developers, managers, and other stakeholders are concerned that the heterogeneity and complexity of this technology, essentially composed of systems of systems, may open the door to security breaches on an unprecedented scale. There is, however, another important system property, which is not very often brought up as an imminent concern, but is equally important. This is device safety, the violation of which may cause severe harm to the environment in which the device operates.},
	number = {1},
	journal = {IT Professional},
	author = {Zalewski, Janusz},
	month = jan,
	year = {2019},
	note = {Conference Name: IT Professional},
	keywords = {Computer security, Internet of Things, Safety, Security, Smart devices},
	pages = {16--20},
}

@inproceedings{van2021memoized,
	title = {Memoized regular expressions},
	booktitle = {International conference on implementation and application of automata},
	author = {van der Merwe, Brink and Mouton, Jacobie and van Litsenborgh, Steyn and Berglund, Martin},
	year = {2021},
	note = {tex.organization: Springer},
	pages = {39--52},
}

@article{ralph_illusion_2013,
	title = {The illusion of requirements in software development},
	volume = {18},
	issn = {0947-3602, 1432-010X},
	url = {http://link.springer.com/10.1007/s00766-012-0161-4},
	doi = {10.1007/s00766-012-0161-4},
	abstract = {This viewpoint explores the possibility that many software development projects may have no useful requirements. Speciﬁcally, for problems (e.g., knowledge worker burnout) with two completely different solutions (e.g., better tool support or hire more employees), an analyst may state a goal (e.g., decrease work hours) but more speciﬁc desiderata are contingent on the chosen solution. Furthermore, without fully exploring the design space, the designer cannot be sure whether there exists another approach, which would achieve the goal without any commonality with known approaches. In these situations of sparse requirements, analysts may misrepresent design decisions as requirements, creating an illusion of requirements in software development.},
	language = {en},
	number = {3},
	urldate = {2021-08-31},
	journal = {Requirements Engineering},
	author = {Ralph, Paul},
	month = sep,
	year = {2013},
	pages = {293--296},
}

@inproceedings{ralph_is_2015,
	address = {Florence, Italy},
	title = {Is {Requirements} {Engineering} {Inherently} {Counterproductive}?},
	isbn = {978-1-4673-7100-1},
	url = {http://ieeexplore.ieee.org/document/7184708/},
	doi = {10.1109/TwinPeaks.2015.12},
	abstract = {This paper explores the possibility that requirements engineering is, in principle, detrimental to software project success. Requirements engineering is conceptually divided into two distinct processes: sensemaking (learning about the project context) and problem structuring (specifying problems, goals, requirements, constraints, etc.). An interdisciplinary literature review revealed substantial evidence that while sensemaking improves design performance, problem structuring reduces design performance. Future research should therefore investigate decoupling the sensemaking aspects of requirements engineering from the problem structuring aspects.},
	language = {en},
	urldate = {2021-08-31},
	booktitle = {2015 {IEEE}/{ACM} 5th {International} {Workshop} on the {Twin} {Peaks} of {Requirements} and {Architecture}},
	publisher = {IEEE},
	author = {Ralph, Paul and Mohanani, Rahul},
	month = may,
	year = {2015},
	pages = {20--23},
}

@article{madni_leveraging_2019,
	title = {Leveraging {Digital} {Twin} {Technology} in {Model}-{Based} {Systems} {Engineering}},
	volume = {7},
	issn = {2079-8954},
	url = {http://www.mdpi.com/2079-8954/7/1/7},
	doi = {10.3390/systems7010007},
	abstract = {Digital twin, a concept introduced in 2002, is becoming increasingly relevant to systems engineering and, more speciﬁcally, to model-based system engineering (MBSE). A digital twin, like a virtual prototype, is a dynamic digital representation of a physical system. However, unlike a virtual prototype, a digital twin is a virtual instance of a physical system (twin) that is continually updated with the latter’s performance, maintenance, and health status data throughout the physical system’s life cycle. This paper presents an overall vision and rationale for incorporating digital twin technology into MBSE. The paper discusses the beneﬁts of integrating digital twins with system simulation and Internet of Things (IoT) in support of MBSE and provides speciﬁc examples of the use and beneﬁts of digital twin technology in different industries. It concludes with a recommendation to make digital twin technology an integral part of MBSE methodology and experimentation testbeds.},
	language = {en},
	number = {1},
	urldate = {2021-08-30},
	journal = {Systems},
	author = {Madni, Azad and Madni, Carla and Lucero, Scott},
	month = jan,
	year = {2019},
	pages = {7},
}

@article{lamport_state_1978,
	title = {State the problem before describing the solution},
	volume = {3},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1010734.1010737},
	doi = {10.1145/1010734.1010737},
	language = {en},
	number = {1},
	urldate = {2021-08-30},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Lamport, Leslie},
	month = jan,
	year = {1978},
	pages = {26--26},
}

@article{davison2004principles,
	title = {Principles of canonical action research},
	volume = {14},
	number = {1},
	journal = {Information systems journal},
	author = {Davison, Robert and Martinsons, Maris G and Kock, Ned},
	year = {2004},
	note = {Publisher: Wiley Online Library},
	pages = {65--86},
}

@article{lutters2007revealing,
	title = {Revealing actual documentation usage in software maintenance through war stories},
	volume = {49},
	number = {6},
	journal = {Information and Software Technology},
	author = {Lutters, Wayne G and Seaman, Carolyn B},
	year = {2007},
	note = {Publisher: Elsevier},
	pages = {576--587},
}

@article{rekdal_academic_2014,
	title = {Academic urban legends},
	volume = {44},
	issn = {0306-3127, 1460-3659},
	url = {http://journals.sagepub.com/doi/10.1177/0306312714535679},
	doi = {10.1177/0306312714535679},
	abstract = {Many of the messages presented in respectable scientific publications are, in fact, based on various forms of rumors. Some of these rumors appear so frequently, and in such complex, colorful, and entertaining ways that we can think of them as academic urban legends. The explanation for this phenomenon is usually that authors have lazily, sloppily, or fraudulently employed sources, and peer reviewers and editors have not discovered these weaknesses in the manuscripts during evaluation. To illustrate this phenomenon, I draw upon a remarkable case in which a decimal point error appears to have misled millions into believing that spinach is a good nutritional source of iron. Through this example, I demonstrate how an academic urban legend can be conceived and born, and can continue to grow and reproduce within academia and beyond.},
	language = {en},
	number = {4},
	urldate = {2021-07-20},
	journal = {Social Studies of Science},
	author = {Rekdal, Ole Bjørn},
	month = aug,
	year = {2014},
	keywords = {Plagiarism, Training PhD students},
	pages = {638--654},
}

@inproceedings{tondel2020using,
	title = {Using situational and narrative analysis for investigating the messiness of software security},
	booktitle = {Proceedings of the 14th {ACM}/{IEEE} international symposium on empirical software engineering and measurement ({ESEM})},
	author = {Tøndel, Inger Anne and Cruzes, Daniela Soares and Jaatun, Martin Gilje},
	year = {2020},
	pages = {1--6},
}

@article{baltes_sampling_2021,
	title = {Sampling in {Software} {Engineering} {Research}: {A} {Critical} {Review} and {Guidelines}},
	shorttitle = {Sampling in {Software} {Engineering} {Research}},
	url = {http://arxiv.org/abs/2002.07764},
	abstract = {Representative sampling appears rare in empirical software engineering research. Not all studies need representative samples, but a general lack of representative sampling undermines a scientiﬁc ﬁeld. This article therefore reports a critical review of the state of sampling in recent, high-quality software engineering research. The key ﬁndings are: (1) random sampling is rare; (2) sophisticated sampling strategies are very rare; (3) sampling, representativeness and randomness often appear misunderstood. These ﬁndings suggest that software engineering research has a generalizability crisis. To address these problems, this paper synthesizes existing knowledge of sampling into a succinct primer and proposes extensive guidelines for improving the conduct, presentation and evaluation of sampling in software engineering research. It is further recommended that while researchers should strive for more representative samples, disparaging non-probability sampling is generally capricious and particularly misguided for predominately qualitative research.},
	language = {en},
	urldate = {2021-07-30},
	journal = {arXiv:2002.07764 [cs]},
	author = {Baltes, Sebastian and Ralph, Paul},
	month = jul,
	year = {2021},
	note = {arXiv: 2002.07764},
	keywords = {Computer Science - Software Engineering},
}

@article{kitchenhamPrinciplesSurveyResearch,
	title = {Principles of {Survey} {Research} {Part} 2: {Designing} a {Survey}},
	volume = {27},
	language = {en},
	number = {1},
	journal = {ACM SIGSOFT},
	author = {Kitchenham, Barbara A and Pfleeger, Shad Lawrence},
	pages = {3},
}

@article{pfleegerPrinciplesSurveyResearch,
	title = {Principles of {Survey} {Research} {Part} 1: {Turning} {Lemons} into {Lemonade}},
	volume = {26},
	language = {en},
	number = {6},
	journal = {ACM SIGSOFT},
	author = {Pfleeger, Shad Lawrence and Kitchenham, Barbara A},
	pages = {3},
}

@incollection{easterbrookSelectingEmpiricalMethods2008,
	address = {London},
	title = {Selecting {Empirical} {Methods} for {Software} {Engineering} {Research}},
	isbn = {978-1-84800-043-8 978-1-84800-044-5},
	url = {http://link.springer.com/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	language = {en},
	urldate = {2021-04-06},
	booktitle = {Guide to {Advanced} {Empirical} {Software} {Engineering}},
	publisher = {Springer London},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	year = {2008},
	doi = {10.1007/978-1-84800-044-5_11},
	pages = {285--311},
}

@article{dittrichEthicsSocialHoneypots,
	title = {The ethics of social honeypots},
	language = {en},
	journal = {Research Ethics},
	author = {Dittrich, David},
	pages = {19},
}

@article{kwasnik_role_nodate,
	title = {The {Role} of {Classification} in {Knowledge} {Representation} and {Discovery}'},
	abstract = {THELINK BETWEEN CLASSIFICATION AND KNOWLEDGE is explored. Classification schemes have properties that enable the representation of entities and relationships in structures that reflect knowledge of the domain being classified. The strengths and limitations of four classificatory approaches are described in terms of their ability to reflect, discover, and create new knowledge. These approaches are hierarchies, trees, paradigms, and faceted analysis. Examples are provided of the way in which knowledge and the classification process affect each other.},
	language = {en},
	author = {Kwasnik, Barbarah},
	pages = {26},
}

@article{usman_taxonomies_2017,
	title = {Taxonomies in software engineering: {A} {Systematic} mapping study and a revised taxonomy development method},
	volume = {85},
	issn = {09505849},
	shorttitle = {Taxonomies in software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584917300472},
	doi = {10.1016/j.infsof.2017.01.006},
	abstract = {Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies.
Method: A systematic mapping study was conducted, based on 270 primary studies.
Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55\%), design (19.55\%), requirements (15.50\%) and maintenance (11.81\%). Illustration (45.76\%) is the most frequently used approach for taxonomy validation. Hierarchy (53.14\%) and faceted analysis (39.48\%) are the most frequently used classiﬁcation structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53\%) these procedures are not described in suﬃcient detail. The majority of the taxonomies (97\%) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way.
Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classiﬁcation structures, procedures and descriptive bases are usually not well described and motivated.},
	language = {en},
	urldate = {2021-07-14},
	journal = {Information and Software Technology},
	author = {Usman, Muhammad and Britto, Ricardo and Börstler, Jürgen and Mendes, Emilia},
	month = may,
	year = {2017},
	pages = {43--59},
}

@article{DBLP:journals/corr/abs-2010-03525,
	title = {{ACM} {SIGSOFT} empirical standards},
	volume = {abs/2010.03525},
	url = {https://arxiv.org/abs/2010.03525},
	journal = {CoRR},
	author = {Ralph, Paul and Baltes, Sebastian and Bianculli, Domenico and Dittrich, Yvonne and Felderer, Michael and Feldt, Robert and Filieri, Antonio and Furia, Carlo Alberto and Graziotin, Daniel and He, Pinjia and Hoda, Rashina and Juristo, Natalia and Kitchenham, Barbara A. and Robbes, Romain and Méndez, Daniel and Molleri, Jefferson and Spinellis, Diomidis and Staron, Miroslaw and Stol, Klaas-Jan and Tamburri, Damian A. and Torchiano, Marco and Treude, Christoph and Turhan, Burak and Vegas, Sira},
	year = {2020},
	note = {arXiv: 2010.03525
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/abs-2010-03525.bib
tex.timestamp: Mon, 18 Jan 2021 08:56:31 +0100},
}

@article{wuFeasibilityStealthilyIntroducing,
	title = {On the {Feasibility} of {Stealthily} {Introducing} {Vulnerabilities} in {Open}-{Source} {Software} via {Hypocrite} {Commits}},
	abstract = {Open source software (OSS) has thrived since the forming of Open Source Initiative in 1998. A prominent example is the Linux kernel, which has been used by numerous major software vendors and empowering billions of devices. The higher availability and lower costs of OSS boost its adoption, while its openness and flexibility enable quicker innovation. More importantly, the OSS development approach is believed to produce more reliable and higher-quality software since it typically has thousands of independent programmers testing and fixing bugs of the software collaboratively.},
	language = {en},
	author = {Wu, Qiushi and Lu, Kangjie},
	pages = {16},
}

@article{gatesExpandingParticipationUndergraduate1999,
	title = {Expanding {Participation} in {Undergraduate} {Research} {Using} the {Affinity} {Group} {Model}*},
	volume = {88},
	issn = {10694730},
	url = {http://doi.wiley.com/10.1002/j.2168-9830.1999.tb00467.x},
	doi = {10.1002/j.2168-9830.1999.tb00467.x},
	abstract = {The benefits of working in a research group are clear: students develop domain expertise, gain an understanding and appreciation of the research process and its practice, and acquire team, communication, problem-solving, and higher-level thinking skills. Students with this experience are better equipped to make informed judgements about technical matters and to communicate and work in teams to solve complex problems. Clearly, this type of research experience must be made available to a broader population. This paper discusses how the Systems and Software Engineering Affinity Research Group model provides a socialization mechanism and infrastructure that supports the development and management of large research groups that engage undergraduate and graduate students, who have a wide range of skill levels and experiences, in research and projects. This non-hierarchical model, which is based on the cooperative paradigm, integrates students into small research groups and an encompassing large research group, and uses structured activities to develop their research, technical, communication, and group skills.},
	language = {en},
	number = {4},
	urldate = {2021-05-04},
	journal = {Journal of Engineering Education},
	author = {Gates, Ann Q. and Teller, Patricia J. and Bernat, Andrew and Delgado, Nelly and Della-Piana, Connie Kubo},
	month = oct,
	year = {1999},
	pages = {409--414},
}

@phdthesis{emami2020informing,
	title = {Informing privacy and security decision making in an {IoT} world},
	school = {Carnegie Mellon University},
	author = {Emami-Naeini, Pardis},
	year = {2020},
}

@article{leveson_software_1991,
	title = {Software safety in embedded computer systems},
	volume = {34},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/102792.102799},
	doi = {10.1145/102792.102799},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {Communications of the ACM},
	author = {Leveson, Nancy G.},
	month = feb,
	year = {1991},
	pages = {34--46},
}

@article{nguyen_cyber-physical_2018,
	title = {Cyber-{Physical} {Specification} {Mismatches}},
	volume = {2},
	issn = {2378-962X, 2378-9638},
	url = {https://dl.acm.org/doi/10.1145/3170500},
	doi = {10.1145/3170500},
	abstract = {Embedded systems use increasingly complex software and are evolving into cyber-physical systems (CPS) with sophisticated interaction and coupling between physical and computational processes. Many CPS operate in safety-critical environments and have stringent certification, reliability, and correctness requirements. These systems undergo changes throughout their lifetimes, where either the software or physical hardware is updated in subsequent design iterations. One source of failure in safety-critical CPS is when there are unstated assumptions in either the physical or cyber parts of the system, and new components do not match those assumptions. In this work, we present an automated method toward identifying unstated assumptions in CPS. Dynamic specifications in the form of candidate invariants of both the software and physical components are identified using dynamic analysis (executing and/or simulating the system implementation or model thereof). A prototype tool called Hynger (for HYbrid iNvariant GEneratoR) was developed that instruments Simulink/Stateflow (SLSF) model diagrams to generate traces in the input format compatible with the Daikon invariant inference tool, which has been extensively applied to software systems. Hynger, in conjunction with Daikon, is able to detect candidate invariants of several CPS case studies. We use the running example of a DC-to-DC power converter and demonstrate that Hynger can detect a specification mismatch where a tolerance assumed by the software is violated due to a plant change. Another case study of an automotive control system is also introduced to illustrate the power of Hynger and Daikon in automatically identifying cyber-physical specification mismatches.},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {ACM Transactions on Cyber-Physical Systems},
	author = {Nguyen, Luan V. and Hoque, Khaza Anuarul and Bak, Stanley and Drager, Steven and Johnson, Taylor T.},
	month = sep,
	year = {2018},
	pages = {1--26},
}

@inproceedings{celik_iotguard_2019,
	address = {San Diego, CA},
	title = {{IoTGuard}: {Dynamic} {Enforcement} of {Security} and {Safety} {Policy} in {Commodity} {IoT}},
	isbn = {978-1-891562-55-6},
	shorttitle = {{IoTGuard}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_07A-1_Celik_paper.pdf},
	doi = {10.14722/ndss.2019.23326},
	abstract = {Broadly deﬁned as the Internet of Things (IoT), the growth of commodity devices that integrate physical processes with digital connectivity has changed the way we live, play, and work. To date, the traditional approach to securing IoT has treated devices individually. However, in practice, it has been recently shown that the interactions among devices are often the real cause of safety and security violations. In this paper, we present IOTGUARD, a dynamic, policy-based enforcement system for IoT, which protects users from unsafe and insecure device states by monitoring the behavior of IoT and triggeraction platform apps. IOTGUARD operates in three phases: (a) implementation of a code instrumentor that adds extra logic to an app’s source code to collect app’s information at runtime, (b) storing the apps’ information in a dynamic model that represents the runtime execution behavior of apps, and (c) identifying IoT safety and security policies, and enforcing relevant policies on the dynamic model of individual apps or sets of interacting apps. We demonstrate IOTGUARD on 20 ﬂawed apps and ﬁnd that IOTGUARD correctly enforces 12 of the 12 policy violations. In addition, we evaluate IOTGUARD on 35 SmartThings IoT and 30 IFTTT trigger-action platform market apps executed in a simulated smart home. IOTGUARD enforces 11 unique policies and blocks 16 states in six (17.1\%) SmartThings and ﬁve (16.6\%) IFTTT apps. IOTGUARD imposes only 17.3\% runtime overhead on an app and 19.8\% for ﬁve interacting apps. Through this effort, we introduce a rigorously grounded system for enforcing correct operation of IoT devices through systematically identiﬁed IoT policies, demonstrating the effectiveness and value of monitoring IoT apps with tools such as IOTGUARD.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Celik, Z. Berkay and Tan, Gang and McDaniel, Patrick},
	year = {2019},
}

@article{wolf_safety_2018,
	title = {Safety and {Security} in {Cyber}-{Physical} {Systems} and {Internet}-of-{Things} {Systems}},
	volume = {106},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/8232537/},
	doi = {10.1109/JPROC.2017.2781198},
	abstract = {Safety and security have traditionally been distinct problems in engineering and computer science. The introduction of computing elements to create cyber-physical systems (CPSs) has opened up a vast new range of potential problems that do not always show up on the radar of traditional engineers. Security, in contrast, is traditionally viewed as a data or communications security problem to be handled by computer scientists and/ or computer engineers. Advances in CPSs and the Internetof-Things (IoT) requires us to take a unified view of safety and security. This paper defines a safety/security threat model for CPSs and IoT systems and surveys emerging techniques which improve the safety and security of CPSs and IoT systems.},
	language = {en},
	number = {1},
	urldate = {2021-08-26},
	journal = {Proceedings of the IEEE},
	author = {Wolf, Marilyn and Serpanos, Dimitrios},
	month = jan,
	year = {2018},
	pages = {9--20},
}

@inproceedings{lee_development_2017,
	address = {Sapporo, Japan},
	title = {Development of an {IoT}-based bridge safety monitoring system},
	isbn = {978-1-5090-4897-7},
	url = {http://ieeexplore.ieee.org/document/7988352/},
	doi = {10.1109/ICASI.2017.7988352},
	abstract = {In this study, an IoT-based bridge safety monitoring system is developed using the ZigBee technology. This system is composed of: (1) monitoring devices installed in the bridge environment; (2) communication devices connecting the bridge monitoring devices and the cloud-based server; (3) a dynamic database that stores bridge condition data; and (4) a cloud-based server that calculates and analyzes data transmitted from the monitoring devices. This system can monitor and analyze in real time the conditions of a bridge and its environment, including the waters levels nearby, pipelines, air and other safety conditions. The detected data and images are transmitted to the server and database for users to have realtime monitoring of the bridge conditions via mobile telecommunication devices.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2017 {International} {Conference} on {Applied} {System} {Innovation} ({ICASI})},
	publisher = {IEEE},
	author = {Lee, Jin-Lian and Tyan, Yaw-Yauan and Wen, Ming-Hui and Wu, Yun-Wu},
	month = may,
	year = {2017},
	pages = {84--86},
}

@inproceedings{singh_systematic_1999,
	address = {Takamatsu, Japan},
	title = {A systematic approach to software safety},
	isbn = {978-0-7695-0509-1},
	url = {http://ieeexplore.ieee.org/document/809632/},
	doi = {10.1109/APSEC.1999.809632},
	abstract = {This paper briefly presents a modified software quality framework that could be employed to address critical software specifications, primarily of safety, within the context of total quality and with differing interests of stakeholders in mind. The modified framework is derived from the McCall software quality-factors framework to accommodate safety.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings {Sixth} {Asia} {Pacific} {Software} {Engineering} {Conference} ({ASPEC}'99) ({Cat}. {No}.{PR00509})},
	publisher = {IEEE Comput. Soc},
	author = {Singh, R.},
	year = {1999},
	pages = {420--423},
}

@inproceedings{conmy_assuring_2014,
	address = {Miami Beach, FL, USA},
	title = {Assuring {Safety} for {Component} {Based} {Software} {Engineering}},
	isbn = {978-1-4799-3466-9 978-1-4799-3465-2},
	url = {http://ieeexplore.ieee.org/document/6754596/},
	doi = {10.1109/HASE.2014.25},
	abstract = {Developing Safety-Critical Systems (SCS) is an expensive activity largely due to the cost of testing both components and the systems produced by integrating them. In more mainstream system design, Model-Based Development (MBD) and ComponentBased Software Engineering (CBSE) are seen as complementary activities that can reduce these costs, however their use is not yet well supported in the safety critical domain, as safety is an emergent property. The contributions of this paper are to describe some of the challenges of using these approaches in SCS, and then argue how through appropriate safety argument patterns the challenges can be addressed.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2014 {IEEE} 15th {International} {Symposium} on {High}-{Assurance} {Systems} {Engineering}},
	publisher = {IEEE},
	author = {Conmy, Philippa and Bate, Iain},
	month = jan,
	year = {2014},
	pages = {121--128},
}

@article{rowland1995professional,
	title = {Professional competence in safety-related software engineering},
	volume = {10},
	number = {2},
	journal = {Software Engineering Journal},
	author = {Rowland, JJ and Rowland, D},
	year = {1995},
	note = {Publisher: IET},
	pages = {43--48},
}

@article{noauthor_internet_nodate,
	title = {Internet of {Things} ({IoT}) in high-risk {Environment}, {Health} and {Safety} ({EHS}) industries: {A} comprehensive review {\textbar} {Elsevier} {Enhanced} {Reader}},
	language = {en},
	journal = {Internet of Things},
	pages = {18},
}

@article{taivalsaari_roadmap_2017,
	title = {A {Roadmap} to the {Programmable} {World}: {Software} {Challenges} in the {IoT} {Era}},
	volume = {34},
	issn = {0740-7459, 1937-4194},
	shorttitle = {A {Roadmap} to the {Programmable} {World}},
	url = {https://ieeexplore.ieee.org/document/7819416/},
	doi = {10.1109/MS.2017.26},
	language = {en},
	number = {1},
	urldate = {2021-07-14},
	journal = {IEEE Software},
	author = {Taivalsaari, Antero and Mikkonen, Tommi},
	month = jan,
	year = {2017},
	pages = {72--80},
}

@article{aly_is_2019,
	title = {Is {Fragmentation} a {Threat} to the {Success} of the {Internet} of {Things}?},
	volume = {6},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/8424819/},
	doi = {10.1109/JIOT.2018.2863180},
	language = {en},
	number = {1},
	urldate = {2021-07-30},
	journal = {IEEE Internet of Things Journal},
	author = {Aly, Mohab and Khomh, Foutse and Gueheneuc, Yann-Gael and Washizaki, Hironori and Yacout, Soumaya},
	month = feb,
	year = {2019},
	pages = {472--487},
}

@inproceedings{heimdahl_safety_2007,
	address = {Minneapolis, MN, USA},
	title = {Safety and {Software} {Intensive} {Systems}: {Challenges} {Old} and {New}},
	isbn = {978-0-7695-2829-8},
	shorttitle = {Safety and {Software} {Intensive} {Systems}},
	url = {http://ieeexplore.ieee.org/document/4221617/},
	doi = {10.1109/FOSE.2007.18},
	abstract = {There is an increased use of software in safety-critical systems; a trend that is likely to continue in the future. Although traditional system safety techniques are applicable to software intensive systems, there are new challenges emerging. In this report we will address four issues we believe will pose challenges in the future.},
	language = {en},
	urldate = {2021-08-30},
	booktitle = {Future of {Software} {Engineering} ({FOSE} '07)},
	publisher = {IEEE},
	author = {Heimdahl, Mats P.E.},
	month = may,
	year = {2007},
	pages = {137--152},
}

@inproceedings{loupos2019cognition,
	title = {Cognition enabled {IoT} platform for industrial {IoT} safety, security and {Privacy}—{The} {CHARIOT} project},
	booktitle = {2019 {IEEE} 24th international workshop on computer aided modeling and design of communication links and networks ({CAMAD})},
	author = {Loupos, Konstantinos and Caglayan, Bora and Papageorgiou, Alexandros and Starynkevitch, Basile and Vedrine, Franck and Skoufis, Christos and Christofi, Stelios and Karakostas, Bill and Mygiakis, Antonis and Theofilis, George and {others}},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {1--4},
}

@article{kanan2018iot,
	title = {An {IoT}-based autonomous system for workers' safety in construction sites with real-time alarming, monitoring, and positioning strategies},
	volume = {88},
	journal = {Automation in Construction},
	author = {Kanan, Riad and Elhassan, Obaidallah and Bensalem, Rofaida},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {73--86},
}

@article{witjaksono_iot_2018,
	title = {{IOT} for {Agriculture}: {Food} {Quality} and {Safety}},
	volume = {343},
	issn = {1757-8981, 1757-899X},
	shorttitle = {{IOT} for {Agriculture}},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/343/1/012023},
	doi = {10.1088/1757-899X/343/1/012023},
	abstract = {Food is the main energy source for the living beings; as such food quality and safety have been in the highest demand throughout the human history. Internet of things (IOT) is a technology with a vision to connect anything at anytime and anywhere. Utilizing IOT in the food supply chain (FSC) is believed to enhance the quality of life by tracing and tracking the food conditions and live-sharing the obtained data with the consumers or the FSC supervisors. Currently, full application of IOT in the FSC is still in the developing stage and there is a big gap for improvements. The purpose of this paper is to explore the possibility of applying IOT for agriculture to trace and track food quality and safety. Mobile application for food freshness investigation was successfully developed and the results showed that consumer mobile camera could be used to test the freshness of food. By applying the IOT technology this information could be shared with all the consumers and also the supervisors.},
	language = {en},
	urldate = {2021-08-26},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Witjaksono, Gunawan and Saeed Rabih, Almur Abdelkreem and Yahya, Noorhana bt and Alva, Sagir},
	month = mar,
	year = {2018},
	pages = {012023},
}

@inproceedings{vishal_iot-driven_2017,
	address = {Mysuru},
	title = {{IoT}-driven road safety system},
	isbn = {978-1-5386-1205-7 978-1-5386-2361-9},
	url = {http://ieeexplore.ieee.org/document/8284624/},
	doi = {10.1109/ICEECCOT.2017.8284624},
	abstract = {Roads are integral part of human civilization. They are the nervous system of any country; hence these are being laid on hill sides and narrow ridges which is a major hazard to human life. As roads play a crucial role in our daily routine these can be modelled in a smart manner to serve us with enhanced capabilities. The architecture of IoT is comprised of an ability to make things more coherent and effective. This paper synchronizes the concept of IoT with roads to make them smart. The paper talks about using the IoT technologies, with the onset of smart cities, to reduce the risk of run off road collisions. As every vehicle is IoT enabled and connected to the internet, we have an effective technique to guide emergency service vehicles through the road within least time. This IoT system is a combination of simple cost-effective antenna technology and internet platforms which works with complete automation. These abilities will make the system to serve us with better accuracy and delicacy.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2017 {International} {Conference} on {Electrical}, {Electronics}, {Communication}, {Computer}, and {Optimization} {Techniques} ({ICEECCOT})},
	publisher = {IEEE},
	author = {Vishal, Dasari and Afaque, H. Saliq and Bhardawaj, Harsh and Ramesh, T. K.},
	month = dec,
	year = {2017},
	pages = {1--5},
}

@article{chatterjee_safety_2020,
	title = {The safety of {IoT}-enabled system in smart cities of {India}: do ethics matter?},
	volume = {36},
	issn = {2514-9369, 2514-9369},
	shorttitle = {The safety of {IoT}-enabled system in smart cities of {India}},
	url = {https://www.emerald.com/insight/content/doi/10.1108/IJOES-05-2019-0085/full/html},
	doi = {10.1108/IJOES-05-2019-0085},
	abstract = {Purpose – The purpose of this study is to analyze the impact of ethics and technology towards safety of internet of things (IoT)-enabled system in smart cities of India (SCI). Design/methodology/approach – The determinants that would impact on securing IoT-enabled system in SCI have been identiﬁed by the studies of literature. Some hypothesis has been formulated. A conceptual model has been developed. Hypotheses and conceptual models have been tested by a statistical approach through survey works considering the feedbacks of 331 usable respondents. The results have been discussed followed by explaining the implications of this study. A comprehensive conclusion has been provided at the end.},
	language = {en},
	number = {4},
	urldate = {2021-08-26},
	journal = {International Journal of Ethics and Systems},
	author = {Chatterjee, Sheshadri},
	month = sep,
	year = {2020},
	pages = {601--618},
}

@inproceedings{wu_design_2019,
	address = {Limerick, Ireland},
	title = {Design and {Implementation} of a {Wearable} {Sensor} {Network} {System} for {IoT}-{Connected} {Safety} and {Health} {Applications}},
	isbn = {978-1-5386-4980-0},
	url = {https://ieeexplore.ieee.org/document/8767280/},
	doi = {10.1109/WF-IoT.2019.8767280},
	abstract = {This paper presents a wearable sensor network system for Internet of Things (IoT) connected safety and health applications. Safety and health of workers are important for industrial workplace; therefore, an IoT network system which can monitor both environmental and physiological can greatly improve the safety in the workplace. The proposed network system incorporates multiple wearable sensors to monitor environmental and physiological parameters. The wearable sensors on different subjects can communicate with each other and transmit the data to a gateway via a LoRa network which forms a heterogeneous IoT platform with Bluetooth-based medical signal sensing network. Once harmful environments are detected and, the sensor node will provide an effective notiﬁcation and warning mechanism for the users. A smart IoT gateway is implemented to provide data processing, local web server and cloud connection. After the gateway receives the data from wearable sensors, it will forward the data to an IoT cloud for further data storage, processing and visualization.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {2019 {IEEE} 5th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	publisher = {IEEE},
	author = {Wu, Fan and Wu, Taiyang and Yuce, Mehmet Rasit},
	month = apr,
	year = {2019},
	pages = {87--90},
}

@article{chung_iot-based_2020,
	title = {{IoT}-based application for construction site safety monitoring},
	issn = {1562-3599, 2331-2327},
	url = {https://www.tandfonline.com/doi/full/10.1080/15623599.2020.1847405},
	doi = {10.1080/15623599.2020.1847405},
	abstract = {Hong Kong construction safety has witnessed substantial improvement in the last three decades, however, accidents still occur frequently as more than 4,000 accidents are reported in the year 2017. Against this background, this research, firstly, aims to investigate the effectiveness of safety training for construction personnel in Hong Kong. A questionnaire is designed to explore the efficacy and weaknesses of mandatory basic safety training. The results indicate the inadequate knowledge of the concept of personal protective equipment as the main weakness of the workers. Secondly, to overcome the training weakness, an Internet-of-Things (IoT) based innovative safety model is designed to provide real-time monitoring of construction site personnel and environment. The proposed model not only identifies realtime personnel safety problems, i.e., near misses, to reduce the accident rates but also stores the digital data to improve future training and system itself. The proposed model in this research provides a costeffective solution for optimal construction safety to the stakeholders. A cost comparison analysis suggests that the IoT system can provide 1) 78\% cost-savings with respect to the traditional manual system and 2) 65\% cost-savings with respect to the traditional sensor system.},
	language = {en},
	urldate = {2021-08-26},
	journal = {International Journal of Construction Management},
	author = {Chung, William Wong Shiu and Tariq, Salman and Mohandes, Saeed Reza and Zayed, Tarek},
	month = nov,
	year = {2020},
	pages = {1--17},
}

@inproceedings{ding_safety_2018,
	address = {Toronto Canada},
	title = {On the {Safety} of {IoT} {Device} {Physical} {Interaction} {Control}},
	isbn = {978-1-4503-5693-0},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243865},
	doi = {10.1145/3243734.3243865},
	abstract = {Emerging Internet of Things (IoT) platforms provide increased functionality to enable human interaction with the physical world in an autonomous manner. The physical interaction features of IoT platforms allow IoT devices to make an impact on the physical environment. However, such features also bring new safety challenges, where attackers can leverage stealthy physical interactions to launch attacks against IoT systems. In this paper, we propose a framework called IoTMon that discovers any possible physical interactions and generates all potential interaction chains across applications in the IoT environment. IoTMon also includes an assessment of the safety risk of each discovered inter-app interaction chain based on its physical influence. To demonstrate the feasibility of our approach, we provide a proof-of-concept implementation of IoTMon and present a comprehensive system evaluation on the Samsung SmartThings platform. We study 185 official SmartThings applications and find they can form 162 hidden inter-app interaction chains through physical surroundings. In particular, our experiment reveals that 37 interaction chains are highly risky and could be potentially exploited to impact the safety of the IoT environment.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ding, Wenbo and Hu, Hongxin},
	month = oct,
	year = {2018},
	pages = {832--846},
}

@inproceedings{nguyen_iotsan_2018,
	address = {Heraklion Greece},
	title = {{IotSan}: fortifying the safety of {IoT} systems},
	isbn = {978-1-4503-6080-7},
	shorttitle = {{IotSan}},
	url = {https://dl.acm.org/doi/10.1145/3281411.3281440},
	doi = {10.1145/3281411.3281440},
	abstract = {Today’s IoT systems include event-driven smart applications (apps) that interact with sensors and actuators. A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states. Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. In this paper, we design IotSan, a novel practical system that uses model checking as a building block to reveal “interaction-level” flaws by identifying events that can lead the system to unsafe states. In building IotSan, we design novel techniques tailored to IoT systems, to alleviate the state explosion associated with model checking. IotSan also automatically translates IoT apps into a format amenable to model checking. Finally, to understand the root cause of a detected vulnerability, we design an attribution mechanism to identify problematic and potentially malicious apps. We evaluate IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities. We also evaluate IotSan with malicious SmartThings apps from a previous effort. IotSan detects the potential safety violations and also effectively attributes these apps as malicious.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 14th {International} {Conference} on emerging {Networking} {EXperiments} and {Technologies}},
	publisher = {ACM},
	author = {Nguyen, Dang Tu and Song, Chengyu and Qian, Zhiyun and Krishnamurthy, Srikanth V. and Colbert, Edward J. M. and McDaniel, Patrick},
	month = dec,
	year = {2018},
	pages = {191--203},
}

@article{medikonda_framework_2009,
	title = {A framework for software safety in safety-critical systems},
	volume = {34},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1507195.1507207},
	doi = {10.1145/1507195.1507207},
	abstract = {Software for safety-critical systems must deal with the hazards identified by safety analysis in order to make the system safe, riskfree, and fail-safe. Because human lives may be lost and tremendous economic costs may result if the software fails, the development of high-integrity software adopts practices that impose greater rigor on the software development processes. Software safety is a composite of many factors. Existing software quality models like McCall’s and Boehm’s and ISO 9126 are inadequate in addressing the software safety issues of real time safety-critical embedded systems. At present there does not exist any standard framework that comprehensively addresses the factors, criteria and metrics (FCM) approach of the quality models in respect of software safety. The safety of a software component must be considered within the context of both the overall system of which it is a component and the environment in which this system operates. It is not useful to investigate the safety of a software component in isolation. This paper proposes a new framework for software safety based on the McCall’s software quality model that specifically identifies the criteria corresponding to software safety in safety critical applications. The criteria in the proposed software safety framework pertains to system hazard analysis, completeness of requirements, identification of softwarerelated safety-critical requirements, safety-constraints based design, run-time issues management, and software safety-critical testing. This framework is then applied to a prototype safetycritical system viz. a software–based Railroad Crossing Control System (RCCS) to validate its utility.},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Medikonda, Ben Swarup and Panchumarthy, Seetha Ramaiah},
	month = feb,
	year = {2009},
	pages = {1--9},
}

@article{penzenstadler_safety_2014,
	title = {Safety, {Security}, {Now} {Sustainability}: {The} {Nonfunctional} {Requirement} for the 21st {Century}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Safety, {Security}, {Now} {Sustainability}},
	url = {https://ieeexplore.ieee.org/document/6728940/},
	doi = {10.1109/MS.2014.22},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {IEEE Software},
	author = {Penzenstadler, Birgit and Raturi, Ankita and Richardson, Debra and Tomlinson, Bill},
	month = may,
	year = {2014},
	pages = {40--47},
}

@article{bhansali_software_2005,
	title = {Software safety: current status and future direction},
	volume = {30},
	issn = {0163-5948},
	shorttitle = {Software safety},
	url = {https://dl.acm.org/doi/10.1145/1039174.1039193},
	doi = {10.1145/1039174.1039193},
	abstract = {This paper describes the current status of software safety in terms of research and existing standards. It highlights the differences between various standards set up by government agencies to accomplish the same safety objectives. For example, European standards tend to place more emphasis on static analysis whereas American standards prefer dynamic testing to verify the software. An optimal verification approach is still a debatable issue in the software safety community. As for future direction, the author believes that the key to making safer and cheaper software is to have better requirements validation that ensure that the requirements are correct and complete before the design and coding phases begin.},
	language = {en},
	number = {1},
	urldate = {2021-08-26},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Bhansali, P. V.},
	month = jan,
	year = {2005},
	pages = {3},
}

@inproceedings{beltrame_engineering_2018,
	address = {Gothenburg Sweden},
	title = {Engineering safety in swarm robotics},
	isbn = {978-1-4503-5760-9},
	url = {https://dl.acm.org/doi/10.1145/3196558.3196565},
	doi = {10.1145/3196558.3196565},
	abstract = {Robotics, artificial intelligence, and the Internet-of-Things are driving current research and development for the technology sector. Robotic and multi-robot systems are becoming pervasive and more and more lives rely on their proper functioning in transportation, medical systems, personal robotics, and manufacturing. Assuring the security and safety of these systems is of primary importance to guarantee the real-world applicability of current research, and we argue that it should be an integral part of system design. Current software standards for safety and security for critical systems (e.g. industrial and aerospace) are not directly applicable to the large distributed systems that are envisioned for the near future. In this paper, we propose to address safety and security of swarm robotics systems at the programming language level. We propose to extend the Buzz multi-robot scripting language with constructs and code analysis that allow the verification of safety and security during development. We believe that detecting and correcting issues with what are inherently emergent systems—i.e. where collective behavior might not be immediately apparent from a single robot’s code—during development would allow for a more effective advancement of swarm robotics.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Robotics} {Software} {Engineering}},
	publisher = {ACM},
	author = {Beltrame, Giovanni and Merlo, Ettore and Panerati, Jacopo and Pinciroli, Carlo},
	month = may,
	year = {2018},
	pages = {36--39},
}

@article{macher_filling_2015,
	title = {Filling the gap between automotive systems, safety, and software engineering},
	volume = {132},
	issn = {0932-383X, 1613-7620},
	url = {http://link.springer.com/10.1007/s00502-015-0301-x},
	doi = {10.1007/s00502-015-0301-x},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {e \& i Elektrotechnik und Informationstechnik},
	author = {Macher, Georg and Stolz, Michael and Armengaud, Eric and Kreiner, Christian},
	month = jun,
	year = {2015},
	pages = {142--148},
}

@misc{firesmith_engineering_2007,
	title = {Engineering {Safety}- and {Security}-{Related} {Requirements} for {Software}- {Intensive} {Systems}},
	author = {Firesmith, Donald},
	year = {2007},
}

@inproceedings{bak_sandboxing_2011,
	address = {Chicago, IL, USA},
	title = {Sandboxing {Controllers} for {Cyber}-{Physical} {Systems}},
	isbn = {978-1-61284-640-8},
	url = {http://ieeexplore.ieee.org/document/5945416/},
	doi = {10.1109/ICCPS.2011.25},
	abstract = {Cyber-physical systems bridge the gap between cyber components, typically written in software, and the physical world. Software written with traditional development practices, however, likely contains bugs or unintended interactions among components, which can result in uncontrolled and possibly disastrous physical-world interactions. Complete veriﬁcation of cyber-physical systems, however, is often impractical due to outsourced development of software, cost, software created without formal models, or excessively large or complex models where the veriﬁcation process becomes intractable.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2011 {IEEE}/{ACM} {Second} {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {IEEE},
	author = {Bak, Stanley and Manamcheri, Karthik and Mitra, Sayan and Caccamo, Marco},
	month = apr,
	year = {2011},
	pages = {3--12},
}

@inproceedings{johnson_safety_2010,
	address = {Manchester, UK},
	title = {Safety arguments for next generation, location aware computing},
	isbn = {978-1-84919-303-0},
	url = {https://digital-library.theiet.org/content/conferences/10.1049/cp.2010.0813},
	doi = {10.1049/cp.2010.0813},
	abstract = {A range of common software components are gradually being integrated into the infrastructures that support safety-critical systems. These include network management tools, operating systems-especially Linux, Voice Over IP (VOIP) communications technologies, Satellite Based Augmentation Systems for navigation/timing data etc. The increasing use of these common components creates concerns that bugs might affect multiple systems across many different safety-related industries. It also raises significant security concerns. Malware has been detected in power distribution, healthcare, military and transportation infrastructures. Most previous attacks do not seem to have deliberately targeted critical applications. However, there is no room for complacency in the face of increasing vulnerability to cyber attacks on safetyrelated systems. This paper illustrates the threat to Air Traffic Management infrastructures and goes on to present a roadmap to increase our resilience to future CyberSafety attacks. Some components of this proposal are familiar concepts from Security Management Systems (SecMS); including a focus on incident reporting and the need for improved risk assessment tools. Other components of the roadmap focus on structural and organizational problems that have limited the effectiveness of existing SecMS; in particular there is a need to raise awareness amongst regulators and senior management who often lack the technical and engineering background to understand the nature of the threats to safety-critical software.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {5th {IET} {International} {Conference} on {System} {Safety} 2010},
	publisher = {IET},
	author = {Johnson, C.W. and Holloway, C.M.},
	year = {2010},
	pages = {1C1--1C1},
}

@article{firesmith_engineering_2004,
	title = {Engineering {Safety} {Requirements}, {Safety} {Constraints}, and {Safety}-{Critical} {Requirements}.},
	volume = {3},
	issn = {1660-1769},
	url = {http://www.jot.fm/contents/issue_2004_03/column3.html},
	doi = {10.5381/jot.2004.3.3.c3},
	abstract = {As software-intensive systems become more pervasive, more and more safety-critical systems are being developed. In this column, I will use the concept of a quality model to define safety as a quality factor. Thus, safety (like security and survivability) is a kind of defensibility, which is a kind of dependability, which is a kind of quality. Next, I discuss the structure of quality requirements and show how safety requirements can be engineered based on safety’s numerous quality subfactors. Then, I define and discuss safety constraints (i.e., mandated safeguards) and safety-critical requirements (i.e., functional, data, and interface requirements that can cause accidents if not implemented correctly). Finally, I pose a set of questions regarding the engineering of these three kinds of safety-related requirements for future research and experience to answer.},
	language = {en},
	number = {3},
	urldate = {2021-08-26},
	journal = {The Journal of Object Technology},
	author = {Firesmith, Donald},
	year = {2004},
	pages = {27},
}

@techreport{firesmith_common_2003,
	address = {Fort Belvoir, VA},
	title = {Common {Concepts} {Underlying} {Safety} {Security} and {Survivability} {Engineering}:},
	shorttitle = {Common {Concepts} {Underlying} {Safety} {Security} and {Survivability} {Engineering}},
	url = {http://www.dtic.mil/docs/citations/ADA421683},
	language = {en},
	urldate = {2021-08-26},
	institution = {Defense Technical Information Center},
	author = {Firesmith, Donald G.},
	month = dec,
	year = {2003},
	doi = {10.21236/ADA421683},
}

@inproceedings{firesmith_engineering_2005,
	address = {St. Louis, MO, USA},
	title = {Engineering safety-related requirements for software-intensive systems},
	url = {http://ieeexplore.ieee.org/document/1553680/},
	doi = {10.1109/ICSE.2005.1553680},
	abstract = {Many software-intensive systems have significant safety ramifications and need to have their associated safety-related requirements properly engineered. However, there is little effective interaction and collaboration between the requirements and safety teams on most projects. This tutorial is intended to improve such collaboration by providing clear definitions of the different kinds of safety-related requirements, examples of such requirements, and a generic process for producing them.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings. 27th {International} {Conference} on {Software} {Engineering}, 2005. {ICSE} 2005.},
	publisher = {IEEe},
	author = {Firesmith, D.G.},
	year = {2005},
	pages = {720--721},
}

@article{bak_safety_2015,
	title = {Safety and {Progress} for {Distributed} {Cyber}-{Physical} {Systems} with {Unreliable} {Communication}},
	volume = {14},
	issn = {1539-9087, 1558-3465},
	url = {https://dl.acm.org/doi/10.1145/2739046},
	doi = {10.1145/2739046},
	abstract = {Cyber-physical systems (CPSs) may interact and manipulate objects in the physical world, and therefore formal guarantees about their behavior are strongly desired. Static-time proofs of safety invariants, however, may be intractable for systems with distributed physical-world interactions. This is further complicated when realistic communication models are considered, for which there may not be bounds on message delays, or even when considering that messages will eventually reach their destination.
            In this work, we address the challenge of proving safety and progress in distributed CPSs communicating over an unreliable communication layer. We show that for this type of communication model, system safety is closely related to the results of a hybrid system’s reachability computation, which can be computed at runtime. However, since computing reachability at runtime may be computationally intensive, we provide an approach that moves significant parts of the computation to design time. This approach is demonstrated with a case study of a simulation of multiple vehicles moving within a shared environment.},
	language = {en},
	number = {4},
	urldate = {2021-07-14},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Bak, Stanley and Huang, Zhenqi and Abad, Fardin Abdi Taghi and Caccamo, Marco},
	month = dec,
	year = {2015},
	pages = {1--22},
}

@inproceedings{liang_understanding_2016,
	address = {Shanghai, China},
	title = {Understanding and detecting performance and security bugs in {IOT} {OSes}},
	isbn = {978-1-5090-2239-7},
	url = {http://ieeexplore.ieee.org/document/7515933/},
	doi = {10.1109/SNPD.2016.7515933},
	abstract = {Operating system (OS) plays an important role in the efficiency and security of service in Internet of Things (IOT). Considering the limited storage resources and power utilization mechanism in IOT OS, we explore the performance bugs and security bugs of three open source IOT OS (Contiki, TinyOS and RIOT OS), including the features of bugs cause, bugs mitigation, bugs detection rules and bugs fixing in them. We present Rulede, a tool built on LLVM compiler framework to find bugs. Experimental results show that Rulede can effectively detect performance bugs and security bugs in target IOT OSes.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2016 17th {IEEE}/{ACIS} {International} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking} and {Parallel}/{Distributed} {Computing} ({SNPD})},
	publisher = {IEEE},
	author = {Liang, Hongliang and Zhao, Qian and Wang, Yuying and Liu, Haifeng},
	month = may,
	year = {2016},
	pages = {413--418},
}

@phdthesis{bak2013verifiable,
	title = {Verifiable {COTS}-based cyber-physical systems},
	school = {University of Illinois at Urbana-Champaign},
	author = {Bak, Stanley Zbigniew},
	year = {2013},
}

@inproceedings{bak_verifying_2016,
	address = {Pittsburgh Pennsylvania},
	title = {Verifying cyber-physical systems by combining software model checking with hybrid systems reachability},
	isbn = {978-1-4503-4485-2},
	url = {https://dl.acm.org/doi/10.1145/2968478.2968490},
	doi = {10.1145/2968478.2968490},
	abstract = {Cyber-physical systems (CPS) span the communication, computation and control domains. Creating a single, complete, and detailed model of a CPS is not only diﬃcult, but, in terms of veriﬁcation, probably not useful; current veriﬁcation algorithms are likely intractable for such allencompassing models. However, speciﬁc CPS domains have specialized formal reasoning methods that can successfully analyze certain aspects of the integrated system. To prove overall system correctness, however, care must be taken to ensure the interfaces of the proofs are consistent and leave no gaps, which can be diﬃcult since they may use diﬀerent model types and describe diﬀerent aspects of the CPS.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Embedded} {Software}},
	publisher = {ACM},
	author = {Bak, Stanley and Chaki, Sagar},
	month = oct,
	year = {2016},
	pages = {1--10},
}

@inproceedings{looga_mammoth_2012,
	address = {Hangzhou, China},
	title = {{MAMMOTH}: {A} massive-scale emulation platform for {Internet} of {Things}},
	isbn = {978-1-4673-1857-0 978-1-4673-1855-6},
	shorttitle = {{MAMMOTH}},
	url = {http://ieeexplore.ieee.org/document/6664581/},
	doi = {10.1109/CCIS.2012.6664581},
	abstract = {Internet of Things (IoT) is increasingly used in a plethora of fields to enable radically new ways for various purposes, ranging from monitoring the environment to enhancing the wellbeing of human life. With the ever-increasing size of such networks, it is fundamental to understand the issues that come with scaling on different networking layers. A cost-efficient approach to examine large-scale networks is to use simulators or emulators to test the infrastructure and its ability to support the desired applications. In this paper, we investigate and compare the currently available simulation/emulation software. We found out that the current solutions are mostly appropriate for small- and medium-scale emulation, however they are not suitable for large-scale testing that reaches millions of node running concurrently. We then propose a large-scale IoT emulator called MAMMotH and present a brief overview of its design. Finally we discuss some of the current issues and future directions, e.g. radio link simulation.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2012 {IEEE} 2nd {International} {Conference} on {Cloud} {Computing} and {Intelligence} {Systems}},
	publisher = {IEEE},
	author = {Looga, Vilen and Ou, Zhonghong and Deng, Yang and Yla-Jaaski, Antti},
	month = oct,
	year = {2012},
	pages = {1235--1239},
}

@inproceedings{voas_testing_2018,
	address = {Bamberg},
	title = {Testing {IoT} {Systems}},
	isbn = {978-1-5386-5207-7},
	url = {https://ieeexplore.ieee.org/document/8359148/},
	doi = {10.1109/SOSE.2018.00015},
	abstract = {This article presents challenges and solutions to testing systems based on the underlying products and services commonly referred to as the Internet of ‘things’ (IoT).},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2018 {IEEE} {Symposium} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	publisher = {IEEE},
	author = {Voas, Jeff and Kuhn, Rick and Laplante, Phil},
	month = mar,
	year = {2018},
	pages = {48--52},
}

@article{gutierrez-madronal_iotteg_2018,
	title = {{IoT}–{TEG}: {Test} event generator system},
	volume = {137},
	issn = {01641212},
	shorttitle = {{IoT}–{TEG}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121217301280},
	doi = {10.1016/j.jss.2017.06.037},
	language = {en},
	urldate = {2021-07-14},
	journal = {Journal of Systems and Software},
	author = {Gutiérrez-Madroñal, L. and Medina-Bulo, I. and Domínguez-Jiménez, J.J.},
	month = mar,
	year = {2018},
	pages = {784--803},
}

@inproceedings{johnson_cyber-physical_2015,
	address = {Seattle Washington},
	title = {Cyber-physical specification mismatch identification with dynamic analysis},
	isbn = {978-1-4503-3455-6},
	url = {https://dl.acm.org/doi/10.1145/2735960.2735979},
	doi = {10.1145/2735960.2735979},
	abstract = {Embedded systems use increasingly complex software and are evolving into cyber-physical systems (CPS) with sophisticated interaction and coupling between physical and computational processes. Many CPS operate in safety-critical environments and have stringent certiﬁcation, reliability, and correctness requirements. These systems undergo changes throughout their lifetimes, where either the software or physical hardware is updated in subsequent design iterations. One source of failure in safety-critical CPS is when there are unstated assumptions in either the physical or cyber parts of the system, and new components do not match those assumptions. In this work, we present an automated method towards identifying unstated assumptions in CPS. Dynamic speciﬁcations in the form of candidate invariants of both the software and physical components are identiﬁed using dynamic analysis (executing and/or simulating the system implementation or model thereof). A prototype tool called Hynger (for HYbrid iNvariant GEneratoR) was developed that instruments Simulink/Stateﬂow (SLSF) model diagrams to generate traces in the input format compatible with the Daikon invariant inference tool, which has been extensively applied to software systems. Hynger, in conjunction with Daikon, is able to detect candidate invariants of several CPS case studies. We use the running example of a DC-to-DC power converter, and demonstrate that Hynger can detect a speciﬁcation mismatch where a tolerance assumed by the software is violated due to a plant change.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the {ACM}/{IEEE} {Sixth} {International} {Conference} on {Cyber}-{Physical} {Systems}},
	publisher = {ACM},
	author = {Johnson, Taylor T. and Bak, Stanley and Drager, Steven},
	month = apr,
	year = {2015},
	pages = {208--217},
}

@incollection{margaria_model-based_2016,
	address = {Cham},
	title = {Model-{Based} {Testing} as a {Service} for {IoT} {Platforms}},
	volume = {9953},
	isbn = {978-3-319-47168-6 978-3-319-47169-3},
	url = {http://link.springer.com/10.1007/978-3-319-47169-3_55},
	abstract = {The Internet of Things (IoT) has increased its footprint becoming globally a ’must have’ for today’s most innovative companies. Applications extend to multitude of domains, such as smart cities, healthcare, logistics, manufacturing, etc. Gartner Group estimates an increase up to 21 billion connected things by 2020. To manage ’things’ heterogeneity and data streams over large scale and secured deployments, IoT and data platforms are becoming a central part of the IoT. To respond to this fast growing demand we see more and more platforms being developed, requiring systematic testing. Combining Model-Based Testing (MBT) technique and a service-oriented solution, we present Model-Based Testing As A Service (MBTAAS) for testing data and IoT platforms. In this paper, we present a ﬁrst step towards MBTAAS for data and IoT Platforms, with experimentation on FIWARE, one of the EU most emerging IoT enabled platforms.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}: {Discussion}, {Dissemination}, {Applications}},
	publisher = {Springer International Publishing},
	author = {Ahmad, Abbas and Bouquet, Fabrice and Fourneret, Elizabeta and Le Gall, Franck and Legeard, Bruno},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	year = {2016},
	doi = {10.1007/978-3-319-47169-3_55},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {727--742},
}

@article{mccormickSmartDevicePush2021,
	title = {Smart {Device} {Push} {Brings} {IT}, {R}\&{D} {Teams} {Together}},
	abstract = {Information Technology (IT) staffers at consumer packaged goods companies increasingly are working with research and development (R\&D) teams to develop more connected devices. To develop Colgate's new smart toothbrush, the R\&D team worked on the brush head and sensors, while the IT team developed the underlying application, which uses machine learning to analyze data collected by the toothbrush to make recommendations for brushing better. Colgate's Mike Crowe said, "It is really about maximizing the use of the combined skills." The IT and R\&D teams at Proctor \& Gamble (P\&G) also have collaborated on a smart toothbrush, a personalized skin-care analyzer, and a facial-hair style assistant. McKinsey’s Ed Roth said teaming IT and R\&D is not without its challenges. “It’s not a natural pairing,” Roth said, because each unit is staffed by people from a different work culture.},
	journal = {The Wall Street Journal},
	author = {McCormick, John},
	month = mar,
	year = {2021},
}

@article{atzori_internet_2010,
	title = {The {Internet} of {Things}: {A} survey},
	volume = {54},
	issn = {13891286},
	shorttitle = {The {Internet} of {Things}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128610001568},
	doi = {10.1016/j.comnet.2010.05.010},
	abstract = {This paper addresses the Internet of Things. Main enabling factor of this promising paradigm is the integration of several technologies and communications solutions. Identiﬁcation and tracking technologies, wired and wireless sensor and actuator networks, enhanced communication protocols (shared with the Next Generation Internet), and distributed intelligence for smart objects are just the most relevant. As one can easily imagine, any serious contribution to the advance of the Internet of Things must necessarily be the result of synergetic activities conducted in different ﬁelds of knowledge, such as telecommunications, informatics, electronics and social science. In such a complex scenario, this survey is directed to those who want to approach this complex discipline and contribute to its development. Different visions of this Internet of Things paradigm are reported and enabling technologies reviewed. What emerges is that still major issues shall be faced by the research community. The most relevant among them are addressed in details.},
	language = {en},
	number = {15},
	urldate = {2021-07-14},
	journal = {Computer Networks},
	author = {Atzori, Luigi and Iera, Antonio and Morabito, Giacomo},
	month = oct,
	year = {2010},
	pages = {2787--2805},
}

@article{roman_features_2013,
	title = {On the features and challenges of security and privacy in distributed internet of things},
	volume = {57},
	issn = {13891286},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128613000054},
	doi = {10.1016/j.comnet.2012.12.018},
	abstract = {In the Internet of Things, services can be provisioned using centralized architectures, where central entities acquire, process, and provide information. Alternatively, distributed architectures, where entities at the edge of the network exchange information and collaborate with each other in a dynamic way, can also be used. In order to understand the applicability and viability of this distributed approach, it is necessary to know its advantages and disadvantages – not only in terms of features but also in terms of security and privacy challenges. The purpose of this paper is to show that the distributed approach has various challenges that need to be solved, but also various interesting properties and strengths.},
	language = {en},
	number = {10},
	urldate = {2021-07-14},
	journal = {Computer Networks},
	author = {Roman, Rodrigo and Zhou, Jianying and Lopez, Javier},
	month = jul,
	year = {2013},
	pages = {2266--2279},
}

@article{colakovic_internet_2018,
	title = {Internet of {Things} ({IoT}): {A} review of enabling technologies, challenges, and open research issues},
	volume = {144},
	issn = {13891286},
	shorttitle = {Internet of {Things} ({IoT})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128618305243},
	doi = {10.1016/j.comnet.2018.07.017},
	language = {en},
	urldate = {2021-07-14},
	journal = {Computer Networks},
	author = {Čolaković, Alem and Hadžialić, Mesud},
	month = oct,
	year = {2018},
	pages = {17--39},
}

@article{swamy_empirical_2020,
	title = {An {Empirical} {Study} on {System} {Level} {Aspects} of {Internet} of {Things} ({IoT})},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9218916/},
	doi = {10.1109/ACCESS.2020.3029847},
	abstract = {Internet of Things (IoT) is an integration of the Sensor, Embedded, Computing, and Communication technologies. The purpose of the IoT is to provide seamless services to anything, anytime at any place. IoT technologies play a crucial role everywhere, which brings the fourth revolution of disruptive technologies after the internet and Information and Communication Technology (ICT). The Research \& Development community has predicted that the impact of IoT will be more than the internet and ICT on society, which improves the well-being of society and industries. Addressing the predominant system-level design aspects like energy efﬁciency, robustness, scalability, interoperability, and security issues result in the use of a potential IoT system. This paper presents the current state of art of the functional pillars of IoT and its emerging applications to motivate academicians and researches to develop real-time, energyefﬁcient, scalable, reliable, and secure IoT applications. This paper summarizes the architecture of IoT, with the contemporary status of IoT architectures. Highlights of the IoT system-level issues to develop more advanced real-time IoT applications have been discussed. Millions of devices exchange information using different communication standards, and interoperability between them is a signiﬁcant issue. This paper provides the current status of the communication standards and application layer protocols used in IoT with the detailed analysis. The computing paradigms like Cloud, Cloudlet, Fog, and Edge computing facilitate IoT with various services like data ofﬂoading, resource and device management, etc. In this paper, an exhaustive analysis of Edge Computing in IoT with different edge computing architectures and existing status are deliberated. The widespread adoption of IoT in society has resulted in privacy and security issues. This paper emphasizes on analyzing the security challenges, privacy and security threats, conventional mitigation techniques, and further scope for IoT security. The features like fewer memory footprints, scheduling, real-time task execution, fewer interrupt, and thread switching latency of Real-Time Operating Systems (RTOS) enables the development of time critical IoT applications. Also, this review offers the analysis of the RTOS’s suitable for IoT with the current status and networking stack. Finally, open research issues in IoT system development are discussed.},
	language = {en},
	urldate = {2021-07-30},
	journal = {IEEE Access},
	author = {Swamy, S. Narasimha and Kota, Solomon Raju},
	year = {2020},
	pages = {188082--188134},
}

@article{zou_smart_2019,
	title = {Smart {Contract} {Development}: {Challenges} and {Opportunities}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {Smart {Contract} {Development}},
	url = {https://ieeexplore.ieee.org/document/8847638/},
	doi = {10.1109/TSE.2019.2942301},
	abstract = {Smart contract, a term which was originally coined to refer to the automation of legal contracts in general, has recently seen much interest due to the advent of blockchain technology. Recently, the term is popularly used to refer to low-level code scripts running on a blockchain platform. Our study focuses exclusively on this subset of smart contracts. Such smart contracts have increasingly been gaining ground, ﬁnding numerous important applications (e.g., crowdfunding) in the real world. Despite the increasing popularity, smart contract development still remains somewhat a mystery to many developers largely due to its special design and applications. Are there any differences between smart contract development and traditional software development? What kind of challenges are faced by developers during smart contract development? Questions like these are important but have not been explored by researchers yet. In this paper, we performed an exploratory study to understand the current state and potential challenges developers are facing in developing smart contracts on blockchains, with a focus on Ethereum (the most popular public blockchain platform for smart contracts). Toward this end, we conducted this study in two phases. In the ﬁrst phase, we conducted semi-structured interviews with 20 developers from GitHub and industry professionals who are working on smart contracts. In the second phase, we performed a survey on 232 practitioners to validate the ﬁndings from the interviews. Our interview and survey results revealed several major challenges developers are facing during smart contract development: (1) there is no effective way to guarantee the security of smart contract code; (2) existing tools for development are still very basic; (3) the programming languages and the virtual machines still have a number of limitations; (4) performance problems are hard to handle under resource constrained running environment; and (5) online resources (including advanced/updated documents and community support) are still limited. Our study suggests several directions that researchers and practitioners can work on to help improve developers’ experience on developing high-quality smart contracts.},
	language = {en},
	urldate = {2021-07-14},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zou, Weiqin and Lo, David and Kochhar, Pavneet Singh and Le, Xuan-Bach D. and Xia, Xin and Feng, Yang and Chen, Zhenyu and Xu, Baowen},
	year = {2019},
	pages = {1--1},
}

@article{risteska_stojkoska_review_2017,
	title = {A review of {Internet} of {Things} for smart home: {Challenges} and solutions},
	volume = {140},
	issn = {09596526},
	shorttitle = {A review of {Internet} of {Things} for smart home},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095965261631589X},
	doi = {10.1016/j.jclepro.2016.10.006},
	abstract = {Although Internet of Things (IoT) brings signiﬁcant advantages over traditional communication technologies for smart grid and smart home applications, these implementations are still very rare. Relying on a comprehensive literature review, this paper aims to contribute towards narrowing the gap between the existing state-of-the-art smart home applications and the prospect of their integration into an IoT enabled environment. We propose a holistic framework which incorporates different components from IoT architectures/frameworks proposed in the literature, in order to efﬁciently integrate smart home objects in a cloud-centric IoT based solution. We identify a smart home management model for the proposed framework and the main tasks that should be performed at each level. We additionally discuss practical design challenges with emphasis on data processing, as well as smart home communication protocols and their interoperability. We believe that the holistic framework ascertained in this paper can be used as a solid base for the future developers of Internet of Things based smart home solutions.},
	language = {en},
	urldate = {2021-07-14},
	journal = {Journal of Cleaner Production},
	author = {Risteska Stojkoska, Biljana L. and Trivodaliev, Kire V.},
	month = jan,
	year = {2017},
	keywords = {home automation},
	pages = {1454--1464},
}

@article{javed_internet_2018,
	title = {Internet of {Things} ({IoT}) {Operating} {Systems} {Support}, {Networking} {Technologies}, {Applications}, and {Challenges}: {A} {Comparative} {Review}},
	volume = {20},
	issn = {1553-877X, 2373-745X},
	shorttitle = {Internet of {Things} ({IoT}) {Operating} {Systems} {Support}, {Networking} {Technologies}, {Applications}, and {Challenges}},
	url = {https://ieeexplore.ieee.org/document/8320780/},
	doi = {10.1109/COMST.2018.2817685},
	abstract = {The Internet of Things (IoT) has become a reality. As the IoT is now becoming a far more common ﬁeld, the demand for IoT technologies to manage the communication of devices with the rest of the world has increased. The IoT is connecting various individual devices called things and wireless sensor networks is also playing an important role. A thing can be deﬁned as an embedded device based on a micro controller that can transmit and receive information. These devices are extremely low in power, memory, and resources. Therefore, the research community has recognized the importance of IoT device operating systems (OSs). An adequate OS with a kernel, networking, real-time capability, and more can make these devices ﬂexible. This review provides a detailed comparison of the OSs designed for IoT devices on the basis of their architecture, scheduling methods, networking technologies, programming models, power and memory management methods, together with other features required for IoT applications. In addition, various applications, challenges, and case studies in the ﬁeld of IoT research is discussed.},
	language = {en},
	number = {3},
	urldate = {2021-07-14},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Javed, Farhana and Afzal, Muhamamd Khalil and Sharif, Muhammad and Kim, Byung-Seo},
	year = {2018},
	pages = {2062--2100},
}

@article{chen_internet--things_2018,
	title = {Internet-of-{Things} {Security} and {Vulnerabilities}: {Taxonomy}, {Challenges}, and {Practice}},
	volume = {2},
	issn = {2509-3428, 2509-3436},
	shorttitle = {Internet-of-{Things} {Security} and {Vulnerabilities}},
	url = {http://link.springer.com/10.1007/s41635-017-0029-7},
	doi = {10.1007/s41635-017-0029-7},
	abstract = {Recent years have seen rapid development and deployment of Internet-of-Things (IoT) applications in a diversity of application domains. This has resulted in creation of new applications (e.g., vehicle networking, smart grid, and wearables) as well as advancement, consolidation, and transformation of various traditional domains (e.g., medical and automotive). One upshot of this scale and diversity of applications is the emergence of new and critical threats to security and privacy: it is getting increasingly easier for an adversary to break into an application, make it unusable, or steal sensitive information and data. This paper provides a summary of IoT security attacks and develops a taxonomy and classification based on the application domain and underlying system architecture. We also discuss some key characteristics of IoT that make it difficult to develop robust security architectures for IoT applications.},
	language = {en},
	number = {2},
	urldate = {2021-07-14},
	journal = {Journal of Hardware and Systems Security},
	author = {Chen, Kejun and Zhang, Shuai and Li, Zhikun and Zhang, Yi and Deng, Qingxu and Ray, Sandip and Jin, Yier},
	month = jun,
	year = {2018},
	pages = {97--110},
}

@article{bornholt_programming_nodate,
	title = {Programming the {Internet} of {Uncertain} {\textless}{T}{\textgreater}hings},
	abstract = {The transformation from desktops and servers to devices and cloud services—the Internet of things (IoT)—is well underway. A key problem facing IoT applications is their increasing reliance on estimated data from diverse sources, such as sensors, machine learning, and human computing. Current programming abstractions treat these estimates as if they were precise, creating buggy applications. Existing approaches to mitigate noise in estimates either add naive ad-hoc ﬁlters or construct sophisticated statistical models. They are either too fragile or too complex for ordinary developers to use. IoT developers need abstractions that help them (1) reason about estimates, because they are noisy and inherently inaccurate; (2) trade accuracy for energy efﬁciency on battery limited devices; and (3) compose data from disparate sources on devices and in the cloud.},
	language = {en},
	author = {Bornholt, James and Meng, Na and Mytkowicz, Todd and McKinley, Kathryn S},
	pages = {7},
}

@article{morin_model-based_2017,
	title = {Model-{Based} {Software} {Engineering} to {Tame} the {IoT} {Jungle}},
	volume = {34},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/7819419/},
	doi = {10.1109/MS.2017.11},
	language = {en},
	number = {1},
	urldate = {2021-07-14},
	journal = {IEEE Software},
	author = {Morin, Brice and Harrand, Nicolas and Fleurey, Franck},
	month = jan,
	year = {2017},
	pages = {30--36},
}

@phdthesis{xiangInterpretedFormalismSystem2016,
	title = {Interpreted {Formalism}: {Towards} {System} {Assurance} and the {Real}-{World} {Semantics} of {Software}},
	copyright = {Attribution 4.0 International (CC BY)},
	shorttitle = {Interpreted {Formalism}},
	url = {https://libraetd.lib.virginia.edu/public_view/g732d899x},
	abstract = {Software systems, especially cyber-physical systems, sense and influence real-world entities under the control of software logic in order to realize desired real-world behaviors. Such software systems are based upon three essential components: (1) a computing platform, (2) a set of physical entities with which the computing platform interacts, and (3) the relationship between the first two components. These three components seem familiar, and the third component seems trivial. In fact, the third component, the relationship, is crucial, because it defines how logical values read and produced by the computing platform will be affected by and will affect the various physical entities.
Formally, the relationship between real-world entities and a computer system’s logic is the interpretation of the logic. Software logic is necessarily formal, but, in practice, interpretations are usually documented informally and incompletely, and programmers treat elements in software logic as if they were the real-world entities themselves. As a result, faults are introduced into systems due to unrecognized discrepancies, and executions end up violating constraints inherited from the real world. The results are software and system failures and adverse downstream consequences.
This dissertation argues that, to mitigate such risks, software engineers should produce not just traditional software, but a new engineering structure, the interpreted formalism. The structure combines software logic with an explicitly documented interpretation. Among other things, an interpretation documents differences that arise inevitably between real-world values and corresponding logic values. An interpreted formalism provides centralized documentation of a system’s software and its intended relationship to the real world in an analyzable form, facilitating fault detection.
An implementation of the interpretation, real-world type, is introduced. For a specific software system, an interpretation is composed of a set of real-world types, and an interpreted formalism is implemented as a real-world type system combined with a software system.
The pragmatics of the interpreted formalism concept are illustrated by conducting case studies on open-source software systems of different sizes. The interpreted formalism is evaluated from several viewpoints: (a) overall feasibility, (b) error detection capability, (c) effort level required, and (d) scalability. The results of the case studies suggest that (1) the interpreted formalism concept can be used on modern software systems of different sizes, (2) the technology is capable of detecting real errors that violate real-world constraints, and (3) the effort required from engineers for developing and using the interpreted formalism can be reduced greatly by an automated synthesis framework developed as part of this research.},
	language = {en},
	urldate = {2021-05-24},
	school = {University of Virginia},
	author = {Xiang, Jian},
	collaborator = {Knight, John},
	month = dec,
	year = {2016},
	doi = {10.18130/V3DS4G},
	keywords = {Software Assurance, Software Dependability, Static Analysis, Type System},
}

@inproceedings{chauhan2016development,
	title = {A development framework for programming cyber-physical systems},
	booktitle = {2016 {IEEE}/{ACM} 2nd international workshop on software engineering for smart cyber-physical systems ({SEsCPS})},
	author = {Chauhan, Saurabh and Patel, Pankesh and Delicato, Flávia C and Chaudhary, Sanjay},
	year = {2016},
	note = {tex.organization: IEEE},
	pages = {47--53},
}

@inproceedings{corno_easing_2018,
	address = {Gothenburg Sweden},
	title = {Easing {IoT} development for novice programmers through code recipes},
	isbn = {978-1-4503-5660-2},
	url = {https://dl.acm.org/doi/10.1145/3183377.3183385},
	doi = {10.1145/3183377.3183385},
	abstract = {The co-existence of various kinds of devices, protocols, architectures, and programming languages make Internet of Things (IoT) systems complex to develop, even for experienced programmers. Perforce, Software Engineering challenges are even more difficult to address by novice programmers. Previous research focused on identifying the most challenging issues that novice programmers experience when developing IoT systems. The results suggested that the integration of heterogeneous software components is one of the most painful issues, mainly due to the lack of documentation understandable by inexperienced developers, from both conceptual and technical perspectives. In fact, novice programmers devote a significant effort looking for documentation and code samples willing to understand them conceptually, or in the worst case, at least to make them work. Driven by the research question: “How do the lessons learned by IoT novice programmers can be captured, so they become an asset for other novice developers?”, in this paper, we introduce Code Recipes. They consist of summarized and welldefined documentation modules, independent from programming languages or run-time environments, by which non-expert programmers can smoothly become familiar with source code, written by other developers that faced similar issues. Through a use case, we show how Code Recipes are a feasible mechanism to support novice IoT programmers in building their IoT systems.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} {Education} and {Training}},
	publisher = {ACM},
	author = {Corno, Fulvio and De Russis, Luigi and Sáenz, Juan Pablo},
	month = may,
	year = {2018},
	pages = {13--16},
}

@inproceedings{corno_towards_2019,
	address = {Glasgow Scotland Uk},
	title = {Towards {Computational} {Notebooks} for {IoT} {Development}},
	isbn = {978-1-4503-5971-9},
	url = {https://dl.acm.org/doi/10.1145/3290607.3312963},
	doi = {10.1145/3290607.3312963},
	abstract = {Internet of Things systems are complex to develop. They are required to exhibit various features and run across several environments. Software developers have to deal with this heterogeneity both when configuring the development and execution environments and when writing the code. Meanwhile, computational notebooks have been gaining prominence due to their capability to consolidate text, executable code, and visualizations in a single document. Although they are mainly used in the field of data science, the characteristics of such notebooks could make them suitable to support the development of IoT systems as well. This work proposes an IoT-tailored literate computing approach in the form of a computational notebook. We present a use case of a typical IoT system involving several interconnected components and describe the implementation of a computational notebook as a tool to support its development. Finally, we point out the opportunities and limitations of this approach.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Corno, Fulvio and De Russis, Luigi and Sáenz, Juan Pablo},
	month = may,
	year = {2019},
	pages = {1--6},
}

@article{alamalhodaei_security_2021,
	title = {Security flaws found in popular {EV} chargers},
	url = {https://techcrunch.com/2021/08/03/security-flaws-found-in-popular-ev-chargers/amp/?guccounter=1},
	journal = {TechCrunch},
	author = {Alamalhodaei, Aria},
	month = aug,
	year = {2021},
}

@inproceedings{emami-naeiniExploringHowPrivacy2019,
	address = {Glasgow Scotland Uk},
	title = {Exploring {How} {Privacy} and {Security} {Factor} into {IoT} {Device} {Purchase} {Behavior}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300764},
	doi = {10.1145/3290605.3300764},
	abstract = {Despite growing concerns about security and privacy of Internet of Things (IoT) devices, consumers generally do not have access to security and privacy information when purchasing these devices. We interviewed 24 participants about IoT devices they purchased. While most had not considered privacy and security prior to purchase, they reported becoming concerned later due to media reports, opinions shared by friends, or observing unexpected device behavior. Those who sought privacy and security information before purchase, reported that it was difficult or impossible to find. We asked interviewees to rank factors they would consider when purchasing IoT devices; after features and price, privacy and security were ranked among the most important. Finally, we showed interviewees our prototype privacy and security label. Almost all found it to be accessible and useful, encouraging them to incorporate privacy and security in their IoT purchase decisions.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Emami-Naeini, Pardis and Dixon, Henry and Agarwal, Yuvraj and Cranor, Lorrie Faith},
	month = may,
	year = {2019},
	pages = {1--12},
}

@inproceedings{emami-naeiniAskExpertsWhat2020,
	address = {San Francisco, CA, USA},
	title = {Ask the {Experts}: {What} {Should} {Be} on an {IoT} {Privacy} and {Security} {Label}?},
	isbn = {978-1-72813-497-0},
	shorttitle = {Ask the {Experts}},
	url = {https://ieeexplore.ieee.org/document/9152770/},
	doi = {10.1109/SP40000.2020.00043},
	abstract = {Information about the privacy and security of Internet of Things (IoT) devices is not readily available to consumers who want to consider it before making purchase decisions. While legislators have proposed adding succinct, consumer accessible, labels, they do not provide guidance on the content of these labels. In this paper, we report on the results of a series of interviews and surveys with privacy and security experts, as well as consumers, where we explore and test the design space of the content to include on an IoT privacy and security label. We conduct an expert elicitation study by following a three-round Delphi process with 22 privacy and security experts to identify the factors that experts believed are important for consumers when comparing the privacy and security of IoT devices to inform their purchase decisions. Based on how critical experts believed each factor is in conveying risk to consumers, we distributed these factors across two layers—a primary layer to display on the product package itself or prominently on a website, and a secondary layer available online through a web link or a QR code. We report on the experts’ rationale and arguments used to support their choice of factors. Moreover, to study how consumers would perceive the privacy and security information speciﬁed by experts, we conducted a series of semi-structured interviews with 15 participants, who had purchased at least one IoT device (smart home device or wearable). Based on the results of our expert elicitation and consumer studies, we propose a prototype privacy and security label to help consumers make more informed IoTrelated purchase decisions. Index Terms—Internet of Things (IoT), Privacy and Security, Label, Expert Elicitation, Delphi.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Emami-Naeini, Pardis and Agarwal, Yuvraj and Faith Cranor, Lorrie and Hibshi, Hanan},
	month = may,
	year = {2020},
	pages = {447--464},
}

@techreport{joint_task_force_transformation_initiative_guide_2012,
	address = {Gaithersburg, MD},
	title = {Guide for conducting risk assessments},
	url = {https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf},
	language = {en},
	number = {NIST SP 800-30r1},
	urldate = {2021-06-08},
	institution = {National Institute of Standards and Technology},
	author = {{Joint Task Force Transformation Initiative}},
	year = {2012},
	doi = {10.6028/NIST.SP.800-30r1},
	note = {Edition: 0},
	pages = {NIST SP 800--30r1},
}

@article{fan_formal_nodate,
	title = {Formal methods for safe autonomy: data-driven verification, synthesis, and applications},
	language = {en},
	author = {Fan, Chuchu},
	pages = {182},
}

@article{uddin_empirical_nodate,
	title = {An {Empirical} {Study} of {IoT} {Topics} in {IoT} {Developer} {Discussions} on {Stack} {Overﬂow}},
	language = {en},
	author = {Uddin, Gias and Sabir, Fatima and Alam, Omar and Khomh, Foutse},
	pages = {47},
}

@article{ahmad_empirical_2018,
	title = {An {Empirical} {Study} of {Investigating} {Mobile} {Applications} {Development} {Challenges}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8326707/},
	doi = {10.1109/ACCESS.2018.2818724},
	abstract = {Context: mobile application development is rapidly evolving with substantial economic and scientiﬁc interest. One of the primary reasons for mobile application development failure is the increasing number of mobile platforms; some organizations endorse mobile application development before understanding the associated development challenges of each target platform. Objective: the objective of this paper is to identify the challenges of native, web, and hybrid mobile applications, which can undermine the successful development of such applications. Method: we adopted a two-phase research approach: at ﬁrst, the challenges were identiﬁed via a systematic literature review (SLR); and then, the identiﬁed challenges were validated through conducting interviews with practitioners. Results: through both research approaches, we identiﬁed nine challenges vital to the success of mobile application development and four additional challenges from interviews not reported in the literature. A comparison of the challenges (native, web, and hybrid) identiﬁed in SLR indicates that there are slightly more differences than similarities between the challenges. On the other hand, the challenges (native, web, and hybrid) identiﬁed in interviews indicates that there are more similarities than differences between the challenges. Our results show a weak negative correlation between the ranks obtained from the SLR and the interviews ([rs(9) = −.034], p = 0.932). The results obtained from our t-test (i.e., t = 0.868, p = 0.402 {\textgreater} 0.05) depicts that there is no signiﬁcant difference between the ﬁndings of SLR and interviews. Conclusions: mobile application development organizations should try to address the identiﬁed challenges when developing mobile applications (native, web, or hybrid) to increase the probability of mobile application success.},
	language = {en},
	urldate = {2021-07-14},
	journal = {IEEE Access},
	author = {Ahmad, Arshad and Li, Kan and Feng, Chong and Asim, Syed Mohammad and Yousif, Abdallah and Ge, Shi},
	year = {2018},
	pages = {17711--17728},
}

@book{iotify_testing_2019,
	title = {Testing the {Internet} of {Things}},
	publisher = {IoTIfy},
	author = {IoTify},
	year = {2019},
}

@techreport{voas_networks_2016,
	address = {Gaithersburg, MD},
	title = {Networks of 'things':},
	shorttitle = {Networks of 'things'},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-183.pdf},
	abstract = {System primitives allow formalisms, reasoning, simulations, and reliability and security risktradeoffs to be formulated and argued. In this work, five core primitives belonging to most distributed systems are presented. These primitives apply well to systems with large amounts of data, scalability concerns, heterogeneity concerns, temporal concerns, and elements of unknown pedigree with possible nefarious intent. These primitives are the basic building blocks for a Network of ‘Things’ (NoT), including the Internet of Things (IoT). This document offers an underlying and foundational understanding of IoT based on the realization that IoT involves sensing, computing, communication, and actuation. The material presented here is generic to all distributed systems that employ IoT technologies (i.e., ‘things’ and networks). The expected audience is computer scientists, IT managers, networking specialists, and networking and cloud computing software engineers. To our knowledge, the ideas and the manner in which IoT is presented here is unique.},
	language = {en},
	number = {NIST SP 800-183},
	urldate = {2021-07-14},
	institution = {National Institute of Standards and Technology},
	author = {Voas, Jeffrey M},
	year = {2016},
	doi = {10.6028/NIST.SP.800-183},
	note = {Edition: 0},
	pages = {NIST SP 800--183},
}

@article{corno_how_2020,
	title = {How is {Open} {Source} {Software} {Development} {Different} in {Popular} {IoT} {Projects}?},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8986632/},
	doi = {10.1109/ACCESS.2020.2972364},
	abstract = {From the software point of view, the development of IoT applications differs from other kinds of applications due to the speciﬁc features that the former exhibit. In this paper, we investigate how developers contribute to IoT applications in the Open Source Software (OSS) context, to gain a deeper understanding of how their work differs from that of non-IoT applications. To that end, we conducted a quantitative analysis of a broad set of the 60 most popular publicly available IoT and non-IoT projects on GitHub. By comparing how developers contribute to these projects, our analysis provides insight into the purpose and characteristics of the code, the behavior of the contributors, and the maturity of the IoT software development ecosystem. Results reveal signiﬁcant differences between IoT and non-IoT application development, in terms of how applications are realized, in the diversity of developers’ specializations, and in how code is reused. This work provides evidence about some Open Source IoT software development peculiarities to be considered by future research efforts aimed at better satisfying software engineering needs in the IoT scenario.},
	language = {en},
	urldate = {2021-08-19},
	journal = {IEEE Access},
	author = {Corno, Fulvio and De Russis, Luigi and Saenz, Juan Pablo},
	year = {2020},
	pages = {28337--28348},
}

@inproceedings{hnat_hitchhikers_2011,
	address = {Seattle, Washington},
	title = {The hitchhiker's guide to successful residential sensing deployments},
	isbn = {978-1-4503-0718-5},
	url = {http://dl.acm.org/citation.cfm?doid=2070942.2070966},
	doi = {10.1145/2070942.2070966},
	abstract = {Homes are rich with information about people’s energy consumption, medical health, and personal or family functions. In this paper, we present our experiences deploying large-scale residential sensing systems in over 20 homes. Deploying small-scale systems in homes can be deceptively easy, but in our deployments we encountered a phase transition in which deployment effort increases dramatically as residential deployments scale up in terms of 1) the number of nodes, 2) the length of time, and 3) the number of houses. In this paper, we distill our experiences down to a set of guidelines and design principles to help future deployments avoid the potential pitfalls of large-scale sensing in homes.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 9th {ACM} {Conference} on {Embedded} {Networked} {Sensor} {Systems} - {SenSys} '11},
	publisher = {ACM Press},
	author = {Hnat, Timothy W. and Srinivasan, Vijay and Lu, Jiakang and Sookoor, Tamim I. and Dawson, Raymond and Stankovic, John and Whitehouse, Kamin},
	year = {2011},
	keywords = {home automation},
	pages = {232},
}

@inproceedings{liangUnderstandingBoundingFunctions2021,
	address = {Madrid, Spain},
	title = {Understanding {Bounding} {Functions} in {Safety}-{Critical} {UAV} {Software}},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9401993/},
	doi = {10.1109/ICSE43902.2021.00119},
	abstract = {Unmanned Aerial Vehicles (UAVs) are an emerging computation platform known for their safety-critical need. In this paper, we conduct an empirical study on a widely used open-source UAV software framework, Paparazzi, with the goal of understanding the safety-critical concerns of UAV software from a bottom-up developer-in-the-field perspective. We set our focus on the use of Bounding Functions (BFs), the runtime checks injected by Paparazzi developers on the range of variables. Through an in-depth analysis on BFs in the Paparazzi autopilot software, we found a large number of them (109 instances) are used to bound safety-critical variables essential to the cyberphysical nature of the UAV, such as its thrust, its speed, and its sensor values. The novel contributions of this study are two fold. First, we take a static approach to classify all BF instances, presenting a novel datatype-based 5-category taxonomy with finegrained insight on the role of BFs in ensuring the safety of UAV systems. Second, we dynamically evaluate the impact of the BF uses through a differential approach, establishing the UAV behavioral difference with and without BFs. The two-pronged static and dynamic approach together illuminates a rarely studied design space of safety-critical UAV software systems.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Liang, Xiaozhou and Burns, John Henry and Sanchez, Joseph and Dantu, Karthik and Ziarek, Lukasz and Liu, Yu David},
	month = may,
	year = {2021},
	pages = {1311--1322},
}

@incollection{lee_embedded_2002,
	title = {Embedded {Software}},
	volume = {56},
	isbn = {978-0-12-012156-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065245802800043},
	abstract = {The science of computation has systematically abstracted away the physical world. Embedded software systems, however, engage the physical world. Time, concurrency, liveness, robustness, continuums, reactivity, and resource management must be remarried to computation. Prevailing abstractions of computational systems leave out these "nonfunctional" aspects. This chapter explains why embedded software is not just software on small computers, and why it therefore needs fundamentally new views of computation. It suggests component architectures based on a principle called "actor-oriented design," where actors interact according to a model of computation, and describes some models of computation that are suitable for embedded software. It then suggests that actors can define interfaces that declare dynamic aspects that are essential to embedded software, such as temporal properties. These interfaces can be structured in a "system-level-type system" that supports the sort of design-time- and run-time-type checking that conventional software benefits from.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {Advances in {Computers}},
	publisher = {Elsevier},
	author = {Lee, Edward A.},
	year = {2002},
	doi = {10.1016/S0065-2458(02)80004-3},
	pages = {55--95},
}

@article{schneier_iot_2017,
	title = {{IoT} {Security}: {What}’s {Plan} {B}?},
	volume = {15},
	issn = {1540-7993},
	shorttitle = {{IoT} {Security}},
	url = {http://ieeexplore.ieee.org/document/8055681/},
	doi = {10.1109/MSP.2017.3681066},
	language = {en},
	number = {5},
	urldate = {2021-08-26},
	journal = {IEEE Security \& Privacy},
	author = {Schneier, Bruce},
	year = {2017},
	pages = {96--96},
}

@article{corno_challenges_2019,
	title = {On the challenges novice programmers experience in developing {IoT} systems: {A} {Survey}},
	volume = {157},
	issn = {01641212},
	shorttitle = {On the challenges novice programmers experience in developing {IoT} systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121219301566},
	doi = {10.1016/j.jss.2019.07.101},
	language = {en},
	urldate = {2021-07-14},
	journal = {Journal of Systems and Software},
	author = {Corno, Fulvio and De Russis, Luigi and Sáenz, Juan Pablo},
	month = nov,
	year = {2019},
	pages = {110389},
}

@article{chen_application_2017,
	title = {Application of {Fault} {Tree} {Analysis} and {Fuzzy} {Neural} {Networks} to {Fault} {Diagnosis} in the {Internet} of {Things} ({IoT}) for {Aquaculture}},
	volume = {17},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/17/1/153},
	doi = {10.3390/s17010153},
	abstract = {In the Internet of Things (IoT) equipment used for aquaculture is often deployed in outdoor ponds located in remote areas. Faults occur frequently in these tough environments and the staff generally lack professional knowledge and pay a low degree of attention in these areas. Once faults happen, expert personnel must carry out maintenance outdoors. Therefore, this study presents an intelligent method for fault diagnosis based on fault tree analysis and a fuzzy neural network. In the proposed method, ﬁrst, the fault tree presents a logic structure of fault symptoms and faults. Second, rules extracted from the fault trees avoid duplicate and redundancy. Third, the fuzzy neural network is applied to train the relationship mapping between fault symptoms and faults. In the aquaculture IoT, one fault can cause various fault symptoms, and one symptom can be caused by a variety of faults. Four fault relationships are obtained. Results show that one symptom-to-one fault, two symptoms-to-two faults, and two symptoms-to-one fault relationships can be rapidly diagnosed with high precision, while one symptom-to-two faults patterns perform not so well, but are still worth researching. This model implements diagnosis for most kinds of faults in the aquaculture IoT.},
	language = {en},
	number = {12},
	urldate = {2021-07-14},
	journal = {Sensors},
	author = {Chen, Yingyi and Zhen, Zhumi and Yu, Huihui and Xu, Jing},
	month = jan,
	year = {2017},
	pages = {153},
}

@inproceedings{zhou_empirical_2015,
	address = {Florence, Italy},
	title = {An {Empirical} {Study} on {Quality} {Issues} of {Production} {Big} {Data} {Platform}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7202945/},
	doi = {10.1109/ICSE.2015.130},
	abstract = {Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. To date, there is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-deﬁned escalation process (i.e., incident management process), which helps customers report service quality issues on 24/7 basis. This paper investigates the common symptom, causes and mitigation of service quality issues in Big Data platform. We conduct a comprehensive empirical study on 210 real service quality issues of ProductA. Our major ﬁndings include (1) 21.0\% of escalations are caused by hardware faults; (2) 36.2\% are caused by system side defects; (3) 37.2\% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our study results provide valuable guidance on improving existing development and maintenance practice of production Big Data platform, and motivate tool support. Index Terms- empirical study, Big Data computing, quality issues, escalations, fault tolerance.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Zhou, Hucheng and Lou, Jian-Guang and Zhang, Hongyu and Lin, Haibo and Lin, Haoxiang and Qin, Tingting},
	month = may,
	year = {2015},
	pages = {17--26},
}

@inproceedings{goyal_mind_2016,
	address = {Pisa Italy},
	title = {Mind the tracker you wear: a security analysis of wearable health trackers},
	isbn = {978-1-4503-3739-7},
	shorttitle = {Mind the tracker you wear},
	url = {https://dl.acm.org/doi/10.1145/2851613.2851685},
	doi = {10.1145/2851613.2851685},
	abstract = {Wearable tracking devices have gained widespread usage and popularity because of the valuable services they oﬀer, monitoring human’s health parameters and, in general, assisting persons to take a better care of themselves. Nevertheless, the security risks associated with such devices can represent a concern among consumers, because of the sensitive information these devices deal with, like sleeping patterns, eating habits, heart rate and so on. In this paper, we analyse the key security and privacy features of two entry level health trackers from leading vendors (Jawbone and Fitbit), exploring possible attack vectors and vulnerabilities at several system levels. The results of the analysis show how these devices are vulnerable to several attacks (perpetrated with consumer-level devices equipped with just bluetooth and Wi-Fi) that can compromise users’ data privacy and security, and eventually call the tracker vendors to raise the stakes against such attacks.},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Goyal, Rohit and Dragoni, Nicola and Spognardi, Angelo},
	month = apr,
	year = {2016},
	keywords = {consumer electronics},
	pages = {131--136},
}

@inproceedings{ronen_iot_2017,
	address = {San Jose, CA, USA},
	title = {{IoT} {Goes} {Nuclear}: {Creating} a {ZigBee} {Chain} {Reaction}},
	isbn = {978-1-5090-5533-3},
	shorttitle = {{IoT} {Goes} {Nuclear}},
	url = {http://ieeexplore.ieee.org/document/7958578/},
	doi = {10.1109/SP.2017.14},
	abstract = {Within the next few years, billions of IoT devices will densely populate our cities. In this paper we describe a new type of threat in which adjacent IoT devices will infect each other with a worm that will rapidly spread over large areas, provided that the density of compatible IoT devices exceeds a certain critical mass. In particular, we developed and veriﬁed such an infection using the popular Philips Hue smart lamps as a platform. The worm spreads by jumping directly from one lamp to its neighbors, using only their built-in ZigBee wireless connectivity and their physical proximity. The attack can start by plugging in a single infected bulb anywhere in the city, and then catastrophically spread everywhere within minutes. It enables the attacker to turn all the city lights on or off, to permanently brick them, or to exploit them in a massive DDOS attack. To demonstrate the risks involved, we use results from percolation theory to estimate the critical mass of installed devices for a typical city such as Paris whose area is about 105 square kilometers: The chain reaction will ﬁzzle if there are fewer than about 15,000 randomly located smart lamps in the whole city, but will spread everywhere when the number exceeds this critical mass (which had almost certainly been surpassed already).},
	language = {en},
	urldate = {2021-07-14},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Ronen, Eyal and Shamir, Adi and Weingarten, Achi-Or and OFlynn, Colin},
	month = may,
	year = {2017},
	pages = {195--212},
}

@article{suler_online_nodate,
	title = {The {Online} {Disinhibition} {Effect}},
	abstract = {While online, some people self-disclose or act out more frequently or intensely than they would in person. This article explores six factors that interact with each other in creating this online disinhibition effect: dissociative anonymity, invisibility, asynchronicity, solipsistic introjection, dissociative imagination, and minimization of authority. Personality variables also will influence the extent of this disinhibition. Rather than thinking of disinhibition as the revealing of an underlying “true self,” we can conceptualize it as a shift to a constellation within self-structure, involving clusters of affect and cognition that differ from the in-person constellation.},
	language = {en},
	author = {Suler, John},
	pages = {83},
}

@misc{yliluoma_perl-compatible_nodate,
	title = {Perl-compatible regular expression optimizer},
	url = {https://bisqwit.iki.fi/source/regexopt.html},
	author = {Yliluoma, Joel},
}

@incollection{ilieNFAReductions2004,
	address = {Berlin, Heidelberg},
	title = {On {NFA} {Reductions}},
	volume = {3113},
	isbn = {978-3-540-22393-1 978-3-540-27812-2},
	url = {http://link.springer.com/10.1007/978-3-540-27812-2_11},
	abstract = {We give faster algorithms for two methods of reducing the number of states in nondeterministic ﬁnite automata. The ﬁrst uses equivalences and the second uses preorders. We develop restricted reduction algorithms that operate on position automata while preserving some of its properties. We show empirically that these reductions are effective in largely reducing the memory requirements of regular expression search algorithms, and compare the eﬀectiveness of diﬀerent reductions.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {Theory {Is} {Forever}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ilie, Lucian and Navarro, Gonzalo and Yu, Sheng},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Karhumäki, Juhani and Maurer, Hermann and Păun, Gheorghe and Rozenberg, Grzegorz},
	year = {2004},
	doi = {10.1007/978-3-540-27812-2_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {112--124},
}

@article{champarnaudNFAReductionAlgorithms2004,
	title = {{NFA} reduction algorithms by means of regular inequalities},
	volume = {327},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397504004803},
	doi = {10.1016/j.tcs.2004.02.048},
	abstract = {We present different techniques for reducing the number of states and transitions in nondeterministic automata. These techniques are based on the two preorders over the set of states, related to the inclusion of left and right languages. Since their exact computation is NP-hard, we focus on polynomial approximations which enable a reduction of the NFA all the same. Our main algorithm relies on a ﬁrst approximation, which can be easily implemented by means of matrix products with an O(mn3) time complexity, and optimized to an O(mn) time complexity, where m is the number of transitions and n is the number of states. This ﬁrst algorithm appears to be more efﬁcient than the known techniques based on equivalence relations as described by Lucian Ilie and Sheng Yu. Afterwards, we brieﬂy describe some more accurate approximations and the exact (but exponential) calculation of these preorders by means of determinization.},
	language = {en},
	number = {3},
	urldate = {2021-05-07},
	journal = {Theoretical Computer Science},
	author = {Champarnaud, J.-M. and Coulon, F.},
	month = nov,
	year = {2004},
	pages = {241--253},
}

@article{schmerge_regis_nodate,
	title = {{ReGiS}: {Regular} {Expression} {Simpliﬁcation} via {Rewrite}-{Guided} {Synthesis}},
	abstract = {Expression simpliﬁcation is an important task necessary in a variety of domains, e.g., compilers, digital logic design, etc. Syntax-guided synthesis (SyGuS) with a cost function can be used for this purpose, but ordered enumeration through a large space of candidate expressions can be expensive. Equality saturation is an alternative approach which allows efﬁcient construction and maintenance of expression equivalence classes generated by rewrite rules, but the procedure may not reach saturation, meaning global minimality cannot be conﬁrmed. We present a new approach called rewrite-guided synthesis (ReGiS), in which a unique interplay between SyGuS and equality saturation-based rewriting helps to overcome these problems, resulting in an efﬁcient, scalable framework for expression simpliﬁcation. We demonstrate the ﬂexibility and practicality of our approach by applying ReGiS to regular expression denial of service (ReDoS) attack prevention. Many real-world regular expression matching engines are vulnerable to these complexity-based attacks, and while much research has focused on detecting vulnerable regular expressions, we allow developers to go further, by automatically transforming their regular expressions to remove vulnerabilities.},
	language = {en},
	author = {Schmerge, Jordan and Claver, Miles and Garner, Jackson and Vossen, Jake and McClurg, Jedidiah},
	pages = {13},
}

@article{gramlichMinimizingNfaRegular2007,
	title = {Minimizing nfa's and regular expressions},
	volume = {73},
	issn = {00220000},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022000006001735},
	doi = {10.1016/j.jcss.2006.11.002},
	abstract = {We show inapproximability results concerning minimization of nondeterministic ﬁnite automata (nfa’s) as well as of regular expressions relative to given nfa’s, regular expressions or deterministic ﬁnite automata (dfa’s). We show that it is impossible to efﬁciently minimize a given nfa or regular expression with n states, transitions, respectively symbols within the factor o(n), unless P = PSPACE. For the unary case, we show that for any δ {\textgreater} 0 it is impossible to efﬁciently construct an approximately minimal nfa or regular expression within the factor n1−δ, unless P = NP.},
	language = {en},
	number = {6},
	urldate = {2021-05-07},
	journal = {Journal of Computer and System Sciences},
	author = {Gramlich, Gregor and Schnitger, Georg},
	month = sep,
	year = {2007},
	pages = {908--923},
}

@article{arnoldNoteMinimalNondeterministic,
	title = {A note about minimal non-deterministic automata},
	abstract = {With every rational language we associate a canonical non-deterministic automaton, that subsumes all possible “minimal” automata recognizing this language.},
	language = {en},
	author = {Arnold, A and Dicky, A and Nivat, M},
	pages = {5},
}

@book{carrez1970minimalization,
	title = {On the minimalization of non-deterministic automaton},
	publisher = {Laboratoire de calcul de l'Université des Sciences et techniques de Lille},
	author = {Carrez, Christian},
	year = {1970},
}

@inproceedings{goldEthicalMiningCase2020,
	address = {Seoul Republic of Korea},
	title = {Ethical {Mining}: {A} {Case} {Study} on {MSR} {Mining} {Challenges}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {Ethical {Mining}},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387462},
	doi = {10.1145/3379597.3387462},
	abstract = {Research in Mining Software Repositories (MSR) is research involving human subjects, as the repositories usually contain data about developers’ interactions with the repositories. Therefore, any research in the area needs to consider the ethics implications of the intended activity before starting. This paper presents a discussion of the ethics implications of MSR research, using the mining challenges from the years 2010 to 2019 as a case study to identify the kinds of data used. It highlights problems that one may encounter in creating such datasets, and discusses ethics challenges that may be encountered when using existing datasets, based on a contemporary research ethics framework. We suggest that the MSR community should increase awareness of ethics issues by openly discussing ethics considerations in published articles.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Gold, Nicolas E. and Krinke, Jens},
	month = jun,
	year = {2020},
	pages = {265--276},
}

@misc{TracyChouLaunches,
	title = {Tracy {Chou} launches {Block} {Party} to combat online harassment and abuse {\textbar} {TechCrunch}},
	url = {https://techcrunch.com/2021/01/15/block-party-launch/},
	urldate = {2021-04-07},
}

@article{fitzgerald2003legal,
	title = {Legal issues relating to free and open source software},
	journal = {Legal Issues Relationg to Free and Open Source Software},
	author = {Fitzgerald, Brian and Bassett, Graham},
	year = {2003},
	note = {Publisher: Queensland University of Technology},
	pages = {11--36},
}

@inproceedings{novielli2014towards,
	title = {Towards discovering the role of emotions in stack overflow},
	booktitle = {Proceedings of the 6th international workshop on social software engineering},
	author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo},
	year = {2014},
	pages = {33--36},
}

@article{blei2003latent,
	title = {Latent dirichlet allocation},
	volume = {3},
	journal = {the Journal of machine Learning research},
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	year = {2003},
	note = {Publisher: JMLR. org},
	pages = {993--1022},
}

@inproceedings{masmoudi2017event,
	title = {From {Event} to {Evidence}: {An} {Approach} for multi-tenant cloud services’ accountability},
	booktitle = {2017 {IEEE} 31st international conference on advanced information networking and applications ({AINA})},
	author = {Masmoudi, Fatma and Sellami, Mohamed and Loulou, Monia and Kacem, Ahmed Hadj},
	year = {2017},
	note = {tex.organization: IEEE},
	pages = {1082--1089},
}

@article{lessig2000code,
	title = {Code is law},
	volume = {1},
	number = {2000},
	journal = {Harvard magazine},
	author = {Lessig, Lawrence},
	year = {2000},
}

@article{moghaddam2006coding,
	title = {Coding issues in grounded theory},
	volume = {16},
	number = {1},
	journal = {Issues in educational research},
	author = {Moghaddam, Alireza},
	year = {2006},
	pages = {52--66},
}

@inproceedings{masmoudi2018optimal,
	title = {Optimal evidence collection for accountability in the cloud},
	booktitle = {2018 {IEEE} 15th international conference on e-{Business} engineering ({ICEBE})},
	author = {Masmoudi, Fatma and Sellami, Mohamed and Loulou, Monia and Kacem, Ahmed Hadj},
	year = {2018},
	note = {tex.organization: IEEE},
	pages = {78--85},
}

@article{liGDPRComplianceContext2020,
	title = {{GDPR} {Compliance} in the {Context} of {Continuous} {Integration}},
	url = {http://arxiv.org/abs/2002.06830},
	abstract = {The enactment of the General Data Protection Regulation (GDPR) in 2018 forced any organization that collects and/or processes EU-based personal data to comply with stringent privacy regulations. Software organizations have struggled to achieve GDPR compliance both before and after the GDPR deadline. While some studies have relied on surveys or interviews to ﬁnd general implications of the GDPR, there is a lack of in-depth studies that investigate compliance practices and compliance challenges of software organizations. In particular, there is no information on small and medium enterprises (SMEs), which represent the majority of organizations in the EU, nor on organizations that practice continuous integration. Using design science methodology, we conducted an in-depth study over the span of 20 months regarding GDPR compliance practices and challenges in collaboration with a small, startup organization. We ﬁrst identiﬁed our collaborator’s business problems and then iteratively developed two artifacts to address those problems: a set of operationalized GDPR principles, and an automated GDPR tool that tests those GDPR-derived privacy requirements. This design science approach resulted in four implications for research and for practice. For example, our research reveals that GDPR regulations can be partially operationalized and tested through automated means, which improves compliance practices, but more research is needed to create more efﬁcient and effective means to disseminate and manage GDPR knowledge among software developers.},
	language = {en},
	urldate = {2021-03-26},
	journal = {arXiv:2002.06830 [cs]},
	author = {Li, Ze Shi and Werner, Colin and Ernst, Neil and Damian, Daniela},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06830},
	keywords = {Computer Science - Software Engineering},
}

@article{liPrivacyStreamsEnablingTransparency2017,
	title = {{PrivacyStreams}: {Enabling} {Transparency} in {Personal} {Data} {Processing} for {Mobile} {Apps}},
	volume = {1},
	issn = {2474-9567},
	shorttitle = {{PrivacyStreams}},
	url = {https://dl.acm.org/doi/10.1145/3130941},
	doi = {10.1145/3130941},
	language = {en},
	number = {3},
	urldate = {2021-03-26},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Li, Yuanchun and Chen, Fanglin and Li, Toby Jia-Jun and Guo, Yao and Huang, Gang and Fredrikson, Matthew and Agarwal, Yuvraj and Hong, Jason I.},
	month = sep,
	year = {2017},
	pages = {1--26},
}

@article{hadarPrivacyDesignersSoftware2018a,
	title = {Privacy by designers: software developers’ privacy mindset},
	volume = {23},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Privacy by designers},
	url = {http://link.springer.com/10.1007/s10664-017-9517-1},
	doi = {10.1007/s10664-017-9517-1},
	abstract = {Privacy by design (PbD) is a policy measure that guides software developers to apply inherent solutions to achieve better privacy protection. For PbD to be a viable option, it is important to understand developers’ perceptions, interpretation and practices as to informational privacy (or data protection). To this end, we conducted in-depth interviews with 27 developers from different domains, who practice software design. Grounded analysis of the data revealed an interplay between several different forces affecting the way in which developers handle privacy concerns. Borrowing the schema of Social Cognitive Theory (SCT), we classified and analyzed the cognitive, organizational and behavioral factors that play a role in developers’ privacy decision making. Our findings indicate that developers use the vocabulary of data security to approach privacy challenges, and that this vocabulary limits their perceptions of privacy mainly to third-party threats coming from outside of the organization; that organizational privacy climate is a powerful means for organizations to guide developers toward particular practices of privacy; and that software architectural patterns frame privacy solutions that are used throughout the development process, possibly explaining developers’ preference of policy-based solutions to architectural solutions. Further, we show, through the use of the SCT schema for framing the findings of this study, how a theoretical model of the factors that influence developers’ privacy practices can be conceptualized and used as a guide for future research toward effective implementation of PbD.},
	language = {en},
	number = {1},
	urldate = {2021-03-26},
	journal = {Empirical Software Engineering},
	author = {Hadar, Irit and Hasson, Tomer and Ayalon, Oshrat and Toch, Eran and Birnhack, Michael and Sherman, Sofia and Balissa, Arod},
	month = feb,
	year = {2018},
	pages = {259--289},
}

@article{spiekermannEngineeringPrivacy2009,
	title = {Engineering {Privacy}},
	volume = {35},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/4657365/},
	doi = {10.1109/TSE.2008.88},
	abstract = {In this paper, we integrate insights from diverse islands of research on electronic privacy to offer a holistic view of privacy engineering and a systematic structure for the discipline’s topics. First, we discuss privacy requirements grounded in both historic and contemporary perspectives on privacy. We use a three-layer model of user privacy concerns to relate them to system operations (data transfer, storage, and processing) and examine their effects on user behavior. In the second part of this paper, we develop guidelines for building privacy-friendly systems. We distinguish two approaches: “privacy-by-policy” and “privacy-by-architecture.” The privacy-bypolicy approach focuses on the implementation of the notice and choice principles of fair information practices, while the privacy-byarchitecture approach minimizes the collection of identifiable personal data and emphasizes anonymization and client-side data storage and processing. We discuss both approaches with a view to their technical overlaps and boundaries as well as to economic feasibility. This paper aims to introduce engineers and computer scientists to the privacy research domain and provide concrete guidance on how to design privacy-friendly systems.},
	language = {en},
	number = {1},
	urldate = {2021-03-25},
	journal = {IEEE Transactions on Software Engineering},
	author = {Spiekermann, S. and Cranor, L.F.},
	month = jan,
	year = {2009},
	pages = {67--82},
}

@inproceedings{piccioniEmpiricalStudyAPI2013,
	address = {Baltimore, Maryland},
	title = {An {Empirical} {Study} of {API} {Usability}},
	isbn = {978-0-7695-5056-5},
	url = {http://ieeexplore.ieee.org/document/6681333/},
	doi = {10.1109/ESEM.2013.14},
	abstract = {Modern software development extensively involves reusing library components accessed through their Application Programming Interfaces (APIs). Usability is therefore a fundamental goal of API design, but rigorous empirical studies of API usability are still relatively uncommon. In this paper, we present the design of an API usability study which combines interview questions based on the cognitive dimensions framework, with systematic observations of programmer behavior while solving programming tasks based on “tokens”. We also discuss the implementation of the study to assess the usability of a persistence library API (offering functionalities such as storing objects into relational databases). The study involved 25 programmers (including students, researchers, and professionals), and provided additional evidence to some critical features evidenced by related studies, such as the difﬁculty of ﬁnding good names for API features and of discovering relations between API types. It also discovered new issues relevant to API design, such as the impact of ﬂexibility, and conﬁrmed the crucial importance of accurate documentation for usability.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {2013 {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Piccioni, Marco and Furia, Carlo A. and Meyer, Bertrand},
	month = oct,
	year = {2013},
	pages = {5--14},
}

@article{schellerAutomatedMeasurementAPI2015,
	title = {Automated measurement of {API} usability: {The} {API} {Concepts} {Framework}},
	volume = {61},
	issn = {09505849},
	shorttitle = {Automated measurement of {API} usability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584915000178},
	doi = {10.1016/j.infsof.2015.01.009},
	abstract = {Objective: To make API usability measurement easier, an automated and objective measurement method would be needed. This article proposes such a method. Since it would be impossible to ﬁnd and integrate all possible factors that inﬂuence API usability in one step, the main goal is to prove the feasibility of the introduced approach, and to deﬁne an extensible framework so that additional factors can easily be deﬁned and added later.
Method: A literature review is conducted to ﬁnd potential factors inﬂuencing API usability. From these factors, a selected few are investigated more closely with usability studies. The statistically evaluated results from these studies are used to deﬁne speciﬁc elements of the introduced framework. Further, the inﬂuence of the user as a critical factor for the framework’s feasibility is evaluated.
Results: The API Concepts Framework is deﬁned, with an extensible structure based on concepts that represent the user’s actions, measurable properties that deﬁne what inﬂuences the usability of these concepts, and learning effects that represent the inﬂuence of the user’s experience. A comparison of values calculated by the framework with user studies shows promising results.
Conclusion: It is concluded that the introduced approach is feasible and provides useful results for evaluating API usability. The extensible framework easily allows to add new concepts and measurable properties in the future.},
	language = {en},
	urldate = {2021-03-24},
	journal = {Information and Software Technology},
	author = {Scheller, Thomas and Kühn, Eva},
	month = may,
	year = {2015},
	pages = {145--162},
}

@article{merigouxCatalaMovingFuture,
	title = {Catala: {Moving} {Towards} the {Future} of {Legal} {Expert} {Systems}},
	abstract = {Software called legal expert systems are used around the world by private and public organizations to compute taxes. A bug in such programs can lead to tax miscalculations and heavy legal and democratic consequences. Yet, increasing evidence suggests that some legal expert systems may not meet satisfying criteria to be in compliance with the law. Moreover, they are difﬁcult to adapt to the continuous ﬂow of new legislation just by using traditional software development process. To prevent further software decay and reconcile these systems with the growing demand for algorithmic transparency, we argue that there is a need for a new development process for legal expert systems. As such, we present a solution built by lawyers and computer scientists : a new programming language coupled with a pair programming development process.},
	language = {en},
	author = {Merigoux, Denis and Huttner, Liane},
	pages = {11},
}

@inproceedings{matthewsWhenTrustedBlack2020,
	address = {New York NY USA},
	title = {When {Trusted} {Black} {Boxes} {Don}'t {Agree}: {Incentivizing} {Iterative} {Improvement} and {Accountability} in {Critical} {Software} {Systems}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {When {Trusted} {Black} {Boxes} {Don}'t {Agree}},
	url = {https://dl.acm.org/doi/10.1145/3375627.3375807},
	doi = {10.1145/3375627.3375807},
	abstract = {Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Matthews, Jeanna Neefe and Northup, Graham and Grasso, Isabella and Lorenz, Stephen and Babaeianjelodar, Marzieh and Bashaw, Hunter and Mondal, Sumona and Matthews, Abigail and Njie, Mariama and Goldthwaite, Jessica},
	month = feb,
	year = {2020},
	pages = {102--108},
}

@incollection{irvineShortPaperIntegrating2020,
	address = {Cham},
	title = {Short {Paper}: {Integrating} the {Data} {Protection} {Impact} {Assessment} into the {Software} {Development} {Lifecycle}},
	volume = {12484},
	isbn = {978-3-030-66171-7 978-3-030-66172-4},
	shorttitle = {Short {Paper}},
	url = {http://link.springer.com/10.1007/978-3-030-66172-4_13},
	abstract = {Recent years have seen many privacy violations that have cost both the users of software systems and the businesses that run them in a variety of ways. One potential cause of these violations may be the ad hoc nature of the implementation of privacy measures within software systems, which may stem from the poor representation of privacy within many Software Development LifeCycle (SDLC) processes. We propose to give privacy a higher priority within the SDLC through the creation of a confederated Privacy-Aware SDLC (PASDLC) which incorporates the Data Protection Impact Assessment (DPIA) lifecycle. The PASDLC brings stakeholders of the software system closer together through the implementation of multiple interception points, whilst prompting the stakeholders to consider privacy within the software system. We consider many challenges to the creation of the PASDLC, including potential communication issues from confederating the processes of a SDLC and the eﬀective measurement of privacy as an attribute of a software system.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Data {Privacy} {Management}, {Cryptocurrencies} and {Blockchain} {Technology}},
	publisher = {Springer International Publishing},
	author = {Irvine, Christopher and Balasubramaniam, Dharini and Henderson, Tristan},
	editor = {Garcia-Alfaro, Joaquin and Navarro-Arribas, Guillermo and Herrera-Joancomarti, Jordi},
	year = {2020},
	doi = {10.1007/978-3-030-66172-4_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {219--228},
}

@inproceedings{shethUsThemStudy2014,
	address = {Hyderabad India},
	title = {Us and them: a study of privacy requirements across north america, asia, and europe},
	isbn = {978-1-4503-2756-5},
	shorttitle = {Us and them},
	url = {https://dl.acm.org/doi/10.1145/2568225.2568244},
	doi = {10.1145/2568225.2568244},
	abstract = {Data privacy when using online systems like Facebook and Amazon has become an increasingly popular topic in the last few years. However, only a little is known about how users and developers perceive privacy and which concrete measures would mitigate their privacy concerns. To investigate privacy requirements, we conducted an online survey with closed and open questions and collected 408 valid responses. Our results show that users often reduce privacy to security, with data sharing and data breaches being their biggest concerns. Users are more concerned about the content of their documents and their personal data such as location than about their interaction data. Unlike users, developers clearly prefer technical measures like data anonymization and think that privacy laws and policies are less eﬀective. We also observed interesting diﬀerences between people from diﬀerent geographies. For example, people from Europe are more concerned about data breaches than people from North America. People from Asia/Paciﬁc and Europe believe that content and metadata are more critical for privacy than people from North America. Our results contribute to developing a user-driven privacy framework that is based on empirical evidence in addition to the legal, technical, and commercial perspectives.},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Sheth, Swapneel and Kaiser, Gail and Maalej, Walid},
	month = may,
	year = {2014},
	pages = {859--870},
}

@article{merigouxCatalaProgrammingLanguage2021,
	title = {Catala: {A} {Programming} {Language} for the {Law}},
	shorttitle = {Catala},
	url = {http://arxiv.org/abs/2103.03198},
	abstract = {Law at large underpins modern society, codifying and governing many aspects of citizens' daily lives. Oftentimes, law is subject to interpretation, debate and challenges throughout various courts and jurisdictions. But in some other areas, law leaves little room for interpretation, and essentially aims to rigorously describe a computation, a decision procedure or, simply said, an algorithm. Unfortunately, prose remains a woefully inadequate tool for the job. The lack of formalism leaves room for ambiguities; the structure of legal statutes, with many paragraphs and sub-sections spread across multiple pages, makes it hard to compute the intended outcome of the algorithm underlying a given text; and, as with any other piece of poorly-specified critical software, the use of informal language leaves corner cases unaddressed. We introduce Catala, a new programming language that we specifically designed to allow a straightforward and systematic translation of statutory law into an executable implementation. Catala aims to bring together lawyers and programmers through a shared medium, which together they can understand, edit and evolve, bridging a gap that often results in dramatically incorrect implementations of the law. We have implemented a compiler for Catala, and have proven the correctness of its core compilation steps using the F* proof assistant. We evaluate Catala on several legal texts that are algorithms in disguise, notably section 121 of the US federal income tax and the byzantine French family benefits; in doing so, we uncover a bug in the official implementation. We observe as a consequence of the formalization process that using Catala enables rich interactions between lawyers and programmers, leading to a greater understanding of the original legislative intent, while producing a correct-by-construction executable specification reusable by the greater software ecosystem.},
	language = {en},
	urldate = {2021-03-24},
	journal = {arXiv:2103.03198 [cs]},
	author = {Merigoux, Denis and Chataing, Nicolas and Protzenko, Jonathan},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03198},
	keywords = {Computer Science - Programming Languages},
}

@article{senarathWillTheyUse2019,
	title = {Will {They} {Use} {It} or {Not}? {Investigating} {Software} {Developers}’ {Intention} to {Follow} {Privacy} {Engineering} {Methodologies}},
	volume = {22},
	issn = {2471-2566, 2471-2574},
	shorttitle = {Will {They} {Use} {It} or {Not}?},
	url = {https://dl.acm.org/doi/10.1145/3364224},
	doi = {10.1145/3364224},
	abstract = {With the increasing concerns over privacy in software systems, there is a growing enthusiasm to develop methods to support the development of privacy aware software systems. Inadequate privacy in software system designs could result in users losing their sensitive data, such as health information and financial information, which may cause financial and reputation loss. Privacy Engineering Methodologies (PEMs) are introduced into the software development processes with the goal of guiding software developers to embed privacy into the systems they design. However, for PEMs to be successful it is imperative that software developers have a positive intention to use PEMs. Otherwise, developers may attempt to bypass the privacy methodologies or use them partially and hence develop software systems that may not protect user privacy appropriately. To investigate the factors that affect software developers’ behavioural intention to follow PEMs, in this article, we conducted a study with 149 software developers. Findings of the study show that the usefulness of the PEM to the developers’ existing work to be the strongest determinant that affects software developers’ intention to follow PEMs. Moreover, the compatibility of the PEM with their way of work and how the PEM demonstrates its results when used were also found to be significant. These findings provide important insights in understanding the behaviour of software developers and how they perceive PEMs. The findings could be used to assist organisations and researchers to deploy PEMs and design PEMs that are positively accepted by software developers.},
	language = {en},
	number = {4},
	urldate = {2021-03-24},
	journal = {ACM Transactions on Privacy and Security},
	author = {Senarath, Awanthika and Grobler, Marthie and Arachchilage, Nalin Asanka Gamagedara},
	month = dec,
	year = {2019},
	pages = {1--30},
}

@article{shapiroPrivacyDesignMoving2010,
	title = {Privacy by design: moving from art to practice},
	volume = {53},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Privacy by design},
	url = {https://dl.acm.org/doi/10.1145/1743546.1743559},
	doi = {10.1145/1743546.1743559},
	abstract = {Designing privacy into systems at the beginning of the development process necessitates the effective translation of privacy principles, models, and mechanisms into system requirements.},
	language = {en},
	number = {6},
	urldate = {2021-03-24},
	journal = {Communications of the ACM},
	author = {Shapiro, Stuart S.},
	month = jun,
	year = {2010},
	pages = {27--29},
}

@article{gursesEngineeringPrivacyDesign,
	title = {Engineering {Privacy} by {Design}},
	abstract = {The design and implementation of privacy requirements in systems is a diﬃcult problem and requires the translation of complex social, legal and ethical concerns into systems requirements. The concept of “privacy by design” has been proposed to serve as a guideline on how to address these concerns.},
	language = {en},
	author = {Gurses, Seda and Troncoso, Carmela and Diaz, Claudia},
	pages = {25},
}

@article{diazDiazClaudiaCOSIC,
	title = {Diaz, {Claudia} ‡ {COSIC}/{iMinds}, {Dept}. of {Electrical} {Engineering}, {KU} {Leuven}},
	language = {en},
	author = {Diaz, Claudia},
	pages = {21},
}

@inproceedings{kneuperTranslatingDataProtection2020,
	address = {Valletta, Malta},
	title = {Translating {Data} {Protection} into {Software} {Requirements}:},
	isbn = {978-989-758-399-5},
	shorttitle = {Translating {Data} {Protection} into {Software} {Requirements}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0008873902570264},
	doi = {10.5220/0008873902570264},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Information} {Systems} {Security} and {Privacy}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Kneuper, Ralf},
	year = {2020},
	pages = {257--264},
}

@article{haeberlenPeerReviewPracticalAccountability,
	title = {{PeerReview}: {Practical} {Accountability} for {Distributed} {Systems}},
	abstract = {We describe PeerReview, a system that provides accountability in distributed systems. PeerReview ensures that Byzantine faults whose eﬀects are observed by a correct node are eventually detected and irrefutably linked to a faulty node. At the same time, PeerReview ensures that a correct node can always defend itself against false accusations. These guarantees are particularly important for systems that span multiple administrative domains, which may not trust each other.},
	language = {en},
	author = {Haeberlen, Andreas and Kouznetsov, Petr and Druschel, Peter},
	pages = {14},
}

@article{smithInformationPrivacyMeasuring1996,
	title = {Information {Privacy}: {Measuring} {Individuals}' {Concerns} about {Organizational} {Practices}},
	volume = {20},
	issn = {02767783},
	shorttitle = {Information {Privacy}},
	url = {https://www.jstor.org/stable/249477?origin=crossref},
	doi = {10.2307/249477},
	language = {en},
	number = {2},
	urldate = {2021-03-24},
	journal = {MIS Quarterly},
	author = {Smith, H. Jeff and Milberg, Sandra J. and Burke, Sandra J.},
	month = jun,
	year = {1996},
	pages = {167},
}

@article{spiekermannChallengesPrivacyDesign2012,
	title = {The challenges of privacy by design},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2209249.2209263},
	doi = {10.1145/2209249.2209263},
	abstract = {Heralded by regulators, Privacy by Design holds the promise to solve the digital world's privacy problems. But there are immense challenges, including management commitment and step-by-step methods to integrate privacy into systems.},
	language = {en},
	number = {7},
	urldate = {2021-03-24},
	journal = {Communications of the ACM},
	author = {Spiekermann, Sarah},
	month = jul,
	year = {2012},
	pages = {38--40},
}

@inproceedings{veseliEngineeringPrivacyDesign2019,
	address = {Limassol Cyprus},
	title = {Engineering privacy by design: lessons from the design and implementation of an identity wallet platform},
	isbn = {978-1-4503-5933-7},
	shorttitle = {Engineering privacy by design},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297429},
	doi = {10.1145/3297280.3297429},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Veseli, Fatbardh and Olvera, Jetzabel Serna and Pulls, Tobias and Rannenberg, Kai},
	month = apr,
	year = {2019},
	pages = {1475--1483},
}

@inproceedings{katellSituatedInterventionsAlgorithmic2020,
	address = {Barcelona Spain},
	title = {Toward situated interventions for algorithmic equity: lessons from the field},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Toward situated interventions for algorithmic equity},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372874},
	doi = {10.1145/3351095.3372874},
	abstract = {Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is “scalable” beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.},
	language = {en},
	urldate = {2021-03-23},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Katell, Michael and Young, Meg and Dailey, Dharma and Herman, Bernease and Guetler, Vivian and Tam, Aaron and Bintz, Corinne and Raz, Daniella and Krafft, P. M.},
	month = jan,
	year = {2020},
	pages = {45--55},
}

@inproceedings{coleskyCriticalAnalysisPrivacy2016,
	address = {San Jose, CA},
	title = {A {Critical} {Analysis} of {Privacy} {Design} {Strategies}},
	isbn = {978-1-5090-3690-5},
	url = {http://ieeexplore.ieee.org/document/7527750/},
	doi = {10.1109/SPW.2016.23},
	abstract = {The upcoming General Data Protection Regulation is quickly becoming of great concern to organizations which process personal data of European citizens. It is however nontrivial to translate these legal requirements into privacy friendly designs. One recently proposed approach to make ‘privacy by design’ more practical is privacy design strategies. This paper improves the strategy definitions and suggests an additional level of abstraction between strategies and privacy patterns: ‘tactics’. We have identified a collection of such tactics based on an extensive literature review, in particular a catalogue of surveyed privacy patterns. We explore the relationships between the concepts we introduce and similar concepts used in software engineering. This paper helps bridge the gap between data protection requirements set out in law, and system development practice.},
	language = {en},
	urldate = {2021-03-23},
	booktitle = {2016 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	publisher = {IEEE},
	author = {Colesky, Michael and Hoepman, Jaap-Henk and Hillen, Christiaan},
	month = may,
	year = {2016},
	pages = {33--40},
}

@inproceedings{mailewadissanayakaReviewMongoDBSingularity2017,
	address = {Austin Texas USA},
	title = {A {Review} of {MongoDB} and {Singularity} {Container} {Security} in regards to {HIPAA} {Regulations}},
	isbn = {978-1-4503-5195-9},
	url = {https://dl.acm.org/doi/10.1145/3147234.3148133},
	doi = {10.1145/3147234.3148133},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Companion {Proceedings} of the10th {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {ACM},
	author = {Mailewa Dissanayaka, Akalanka and Shetty, Roshan Ramprasad and Kothari, Samip and Mengel, Susan and Gittner, Lisa and Vadapalli, Ravi},
	month = dec,
	year = {2017},
	pages = {91--97},
}

@inproceedings{khaitzinPrivacyEnforcementLarge2018,
	address = {Haifa Israel},
	title = {Privacy {Enforcement} at a {Large} {Scale} for {GDPR} {Compliance}},
	isbn = {978-1-4503-5849-1},
	url = {https://dl.acm.org/doi/10.1145/3211890.3211913},
	doi = {10.1145/3211890.3211913},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 11th {ACM} {International} {Systems} and {Storage} {Conference}},
	publisher = {ACM},
	author = {Khaitzin, Ety and Shlomo, Roee and Anderson, Maya},
	month = jun,
	year = {2018},
	pages = {124--124},
}

@inproceedings{murmannUsableTransparencyEnhancing2018,
	address = {Barcelona Spain},
	title = {Usable transparency for enhancing privacy in mobile health apps},
	isbn = {978-1-4503-5941-2},
	url = {https://dl.acm.org/doi/10.1145/3236112.3236184},
	doi = {10.1145/3236112.3236184},
	abstract = {We report on our research on usable transparency in the context of mobile health (mhealth) tracking. Usable transparency refers to the usability of transparency-enhancing tools (TETs), which seek to aid users of online data services in improving their privacy. Focusing on ﬁtness tracking scenarios, our research addresses the conceptual and technical demands of such tools in terms of usability.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services} {Adjunct}},
	publisher = {ACM},
	author = {Murmann, Patrick},
	month = sep,
	year = {2018},
	pages = {440--442},
}

@inproceedings{koscinaEnablingTrustHealthcare2019,
	address = {Thessaloniki, Greece},
	title = {Enabling trust in healthcare data exchange with a federated blockchain-based architecture},
	isbn = {978-1-4503-6988-6},
	url = {http://dl.acm.org/citation.cfm?doid=3358695.3360897},
	doi = {10.1145/3358695.3360897},
	abstract = {We propose a new healthcare data exchange platform for research centers, hospitals and healthcare institutions. Our model is based on a federated blockchain network that interconnect the healthcare institutions and orchestrate the data life cycle from the data publication to the data consumption. The blockchain is responsabible to keep the traceability of the whole process and we use a specially designed smart contract to control the data sharing process. Moreover, we provide the means to enforce GDPR and thus achieve a GDPR compliant model.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} on   - {WI} '19 {Companion}},
	publisher = {ACM Press},
	author = {Koscina, Mirko and Manset, David and Negri, Claudia and Perez, Octavio},
	year = {2019},
	pages = {231--237},
}

@inproceedings{povseItAllFun2018,
	address = {Ljubljana Slovenia},
	title = {It's all fun and games, and some legalese: data protection implications for increasing cyber-skills of employees through games},
	isbn = {978-1-4503-6515-4},
	shorttitle = {It's all fun and games, and some legalese},
	url = {https://dl.acm.org/doi/10.1145/3277570.3277580},
	doi = {10.1145/3277570.3277580},
	abstract = {In order to combat cyber-attacks, an organisation can decide to train its employees. Improving cyber-skills of employees through educational games means their personal data will be processed and therefore it falls under the scope of the General Data Protection Regulation (GDPR). The goal of this paper is to address challenges that organisations are likely to face in practice, such as invalidity of employees’ consent and over-intrusive monitoring. It argues that in order to approach training lawfully, organisations should (1) choose their external trainer with due diligence, (2) carry out a data protection impact assessment, and under certain circumstances (3) appoint a data protection officer.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Central} {European} {Cybersecurity} {Conference} 2018},
	publisher = {ACM},
	author = {Povse, Danaja Fabcic},
	month = nov,
	year = {2018},
	pages = {1--5},
}

@inproceedings{sanchez-rolaCanOptOut2019,
	address = {Auckland New Zealand},
	title = {Can {I} {Opt} {Out} {Yet}?: {GDPR} and the {Global} {Illusion} of {Cookie} {Control}},
	isbn = {978-1-4503-6752-3},
	shorttitle = {Can {I} {Opt} {Out} {Yet}?},
	url = {https://dl.acm.org/doi/10.1145/3321705.3329806},
	doi = {10.1145/3321705.3329806},
	abstract = {The European Union’s (EU) General Data Protection Regulation (GDPR), in effect since May 2018, enforces strict limitations on handling users’ personal data, hence impacting their activity tracking on the Web. In this study, we perform an evaluation of the tracking performed in 2,000 high-traffic websites, hosted both inside and outside of the EU.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2019 {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Sanchez-Rola, Iskander and Dell'Amico, Matteo and Kotzias, Platon and Balzarotti, Davide and Bilge, Leyla and Vervier, Pierre-Antoine and Santos, Igor},
	month = jul,
	year = {2019},
	pages = {340--351},
}

@article{kotsiosAnalysisConsequencesGeneral2019,
	title = {An {Analysis} of the {Consequences} of the {General} {Data} {Protection} {Regulation} on {Social} {Network} {Research}},
	volume = {2},
	issn = {2469-7818, 2469-7826},
	url = {https://dl.acm.org/doi/10.1145/3365524},
	doi = {10.1145/3365524},
	abstract = {This article examines the principles outlined in the General Data Protection Regulation in the context of social network data. We provide both a practical guide to General Data Protection Regulation--compliant social network data processing, covering aspects such as data collection, consent, anonymization, and data analysis, and a broader discussion of the problems emerging when the general principles on which the regulation is based are instantiated for this research area.},
	language = {en},
	number = {3},
	urldate = {2021-03-19},
	journal = {ACM Transactions on Social Computing},
	author = {Kotsios, Andreas and Magnani, Matteo and Vega, Davide and Rossi, Luca and Shklovski, Irina},
	month = dec,
	year = {2019},
	pages = {1--22},
}

@inproceedings{zarourSoftwareSecuritySpecifications2020,
	address = {Trondheim Norway},
	title = {Software {Security} {Specifications} and {Design}: {How} {Software} {Engineers} and {Practitioners} {Are} {Mixing} {Things} up},
	isbn = {978-1-4503-7731-7},
	shorttitle = {Software {Security} {Specifications} and {Design}},
	url = {https://dl.acm.org/doi/10.1145/3383219.3383284},
	doi = {10.1145/3383219.3383284},
	abstract = {Huge numbers of worldwide-deployed software suffer from poor quality and possess vulnerabilities with serious impact. Meanwhile, people are using such software to save and manage their valuable information including their monetary data. This has increased the hackers’ appetite to attack software. Henceforth, researchers and practitioners are convinced that software security is not an added value or a gold-plating need. Consequently, security requirements specification and implementation become vital during the software development process. Unfortunately, researchers and practitioners are doing so in a rush. This has made them mix concepts and practices up in a way that can terribly make the problem of delivering software overdue more chronic which will result in a security and technical debt. This research represents a corrective study that sheds light on what has been achieved in analyzing and designing secure software and what are the problems committed and how to handle them.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {ACM},
	author = {Zarour, Mohammad and Alenezi, Mamdouh and Alsarayrah, Khalid},
	month = apr,
	year = {2020},
	pages = {451--456},
}

@article{shastriUnderstandingBenchmarkingImpact2020,
	title = {Understanding and benchmarking the impact of {GDPR} on database systems},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3384345.3384354},
	doi = {10.14778/3384345.3384354},
	abstract = {The General Data Protection Regulation (GDPR) provides new rights and protections to European people concerning their personal data. We analyze GDPR from a systems perspective, translating its legal articles into a set of capabilities and characteristics that compliant systems must support. Our analysis reveals the phenomenon of metadata explosion, wherein large quantities of metadata needs to be stored along with the personal data to satisfy the GDPR requirements. Our analysis also helps us identify new workloads that must be supported under GDPR. We design and implement an open-source benchmark called GDPRbench that consists of workloads and metrics needed to understand and assess personal-data processing database systems. To gauge the readiness of modern database systems for GDPR, we follow best practices and developer recommendations to modify Redis, PostgreSQL, and a commercial database system to be GDPR compliant. Our experiments demonstrate that the resulting GDPR-compliant systems achieve poor performance on GPDR workloads, and that performance scales poorly as the volume of personal data increases. We discuss the real-world implications of these .ndings, and identify research challenges towards making GDPR-compliance efficient in production environments. We release all of our so.ware artifacts and datasets at h.p://www:gdprbench:org},
	language = {en},
	number = {7},
	urldate = {2021-03-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Shastri, Supreeth and Banakar, Vinay and Wasserman, Melissa and Kumar, Arun and Chidambaram, Vijay},
	month = mar,
	year = {2020},
	pages = {1064--1077},
}

@inproceedings{krogerHowAppVendors2020,
	address = {Virtual Event Ireland},
	title = {How do app vendors respond to subject access requests? {A} longitudinal privacy study on {iOS} and {Android} {Apps}},
	isbn = {978-1-4503-8833-7},
	shorttitle = {How do app vendors respond to subject access requests?},
	url = {https://dl.acm.org/doi/10.1145/3407023.3407057},
	doi = {10.1145/3407023.3407057},
	abstract = {EU data protection laws grant consumers the right to access the personal data that companies hold about them. In a first-of-itskind longitudinal study, we examine how service providers have complied with subject access requests over four years. In three iterations between 2015 and 2019, we sent subject access requests to vendors of 225 mobile apps popular in Germany. Throughout the iterations, 19 to 26 \% of the vendors were unreachable or did not reply at all. Our subject access requests were fulfilled in 15 to 53 \% of the cases, with an unexpected decline between the GDPR enforcement date and the end of our study. The remaining responses exhibit a long list of shortcomings, including severe violations of information security and data protection principles. Some responses even contained deceptive and misleading statements (7 to 13 \%). Further, 9 \% of the apps were discontinued and 27 \% of the user accounts vanished during our study, mostly without proper notification about the consequences for our personal data. While we observe improvements for selected aspects over time, the results indicate that subject access request handling will be unsatisfactory as long as vendors accept such requests via email and process them manually.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {ACM},
	author = {Kröger, Jacob Leon and Lindemann, Jens and Herrmann, Dominik},
	month = aug,
	year = {2020},
	pages = {1--10},
}

@inproceedings{soeCircumventionDesignDark2020,
	address = {Tallinn Estonia},
	title = {Circumvention by design - dark patterns in cookie consent for online news outlets},
	isbn = {978-1-4503-7579-5},
	url = {https://dl.acm.org/doi/10.1145/3419249.3420132},
	doi = {10.1145/3419249.3420132},
	abstract = {To ensure that users of online services understand what data are collected and how they are used in algorithmic decision-making, the European Union’s General Data Protection Regulation (GDPR) specifies informed consent as a minimal requirement. For online news outlets consent is commonly elicited through interface design elements in the form of a pop-up. We have manually analyzed 300 data collection consent notices from news outlets that are built to ensure compliance with GDPR. The analysis uncovered a variety of strategies or dark patterns that circumvent the intent of GDPR by design. We further study the presence and variety of these dark patterns in these “cookie consents” and use our observations to specify the concept of dark pattern in the context of consent elicitation.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 11th {Nordic} {Conference} on {Human}-{Computer} {Interaction}: {Shaping} {Experiences}, {Shaping} {Society}},
	publisher = {ACM},
	author = {Soe, Than Htut and Nordberg, Oda Elise and Guribye, Frode and Slavkovik, Marija},
	month = oct,
	year = {2020},
	pages = {1--12},
}

@inproceedings{vanbinsbergenEFLINTDomainspecificLanguage2020,
	address = {Virtual USA},
	title = {{eFLINT}: a domain-specific language for executable norm specifications},
	isbn = {978-1-4503-8174-1},
	shorttitle = {{eFLINT}},
	url = {https://dl.acm.org/doi/10.1145/3425898.3426958},
	doi = {10.1145/3425898.3426958},
	abstract = {Software systems that share potentially sensitive data are subjected to laws, regulations, policies and/or contracts. The monitoring, control and enforcement processes applied to these systems are currently to a large extent manual, which we rather automate by embedding the processes as dedicated and adaptable software services in order to improve efficiency and effectiveness. This approach requires such regulatory services to be closely aligned with a formal description of the relevant norms. This paper presents eflint, a domain-specific language developed for formalizing norms. The theoretical foundations of the language are found in transition systems and in Hohfeld’s framework of legal fundamental conceptions. The language can be used to formalize norms from a large variety of sources. The resulting specifications are executable and support several forms of reasoning such as automatic case assessment, manual exploration and simulation. Moreover, the specifications can be used to develop regulatory services for several types of monitoring, control and enforcement. The language is evaluated through a case study formalizing articles 6(1)(a) and 16 of the General Data Protection Regulation (GDPR). A prototype implementation of eflint is discussed and is available online.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {van Binsbergen, L. Thomas and Liu, Lu-Chi and van Doesburg, Robert and van Engers, Tom},
	month = nov,
	year = {2020},
	pages = {124--136},
}

@article{huynhAddressingRegulatoryRequirements2021,
	title = {Addressing {Regulatory} {Requirements} on {Explanations} for {Automated} {Decisions} with {Provenance}—{A} {Case} {Study}},
	volume = {2},
	issn = {2691-199X, 2639-0175},
	url = {https://dl.acm.org/doi/10.1145/3436897},
	doi = {10.1145/3436897},
	abstract = {AI-based automated decisions are increasingly used as part of new services being deployed to the general public. This approach to building services presents significant potential benefits, such as the reduced speed of execution, increased accuracy, lower cost, and ability to adapt to a wide variety of situations. However, equally significant concerns have been raised and are now well documented such as concerns about privacy, fairness, bias, and ethics. On the consumer side, more often than not, the users of those services are provided with no or inadequate explanations for decisions that may impact their lives. In this article, we report the experience of developing a socio-technical approach to constructing explanations for such decisions from their audit trails, or provenance, in an automated manner. The work has been carried out in collaboration with the UK Information Commissioner’s Office. In particular, we have implemented an automated Loan Decision scenario, instrumented its decision pipeline to record provenance, categorized relevant explanations according to their audience and their regulatory purposes, built an explanation-generation prototype, and deployed the whole system in an online demonstrator.},
	language = {en},
	number = {2},
	urldate = {2021-03-19},
	journal = {Digital Government: Research and Practice},
	author = {Huynh, Trung Dong and Tsakalakis, Niko and Helal, Ayah and Stalla-Bourdillon, Sophie and Moreau, Luc},
	month = mar,
	year = {2021},
	pages = {1--14},
}

@inproceedings{zimmeckStandardizingImplementingNot2020,
	address = {Virtual Event USA},
	title = {Standardizing and {Implementing} {Do} {Not} {Sell}},
	isbn = {978-1-4503-8086-7},
	url = {https://dl.acm.org/doi/10.1145/3411497.3420224},
	doi = {10.1145/3411497.3420224},
	abstract = {The California Consumer Privacy Act gives consumers the right to request that businesses do not sell their personal information. “Selling” is defined broadly and covers, among others, making personal information available to ad networks on websites via third party cookies. We began standardizing and implementing Do Not Sell technologies with the goal of integrating Do Not Sell directly into browser settings. Based on OptMeowt, our proof of concept Do Not Sell browser extension, we conduct experiments on the design, implementation, and current state of Do Not Sell. OptMeowt automatically places Do Not Sell cookies on visited sites and sends Do Not Sell headers per our draft standard. We believe that standardizing Do Not Sell provides an important building block for evolving the web towards increased privacy protections.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 19th {Workshop} on {Privacy} in the {Electronic} {Society}},
	publisher = {ACM},
	author = {Zimmeck, Sebastian and Alicki, Kuba},
	month = nov,
	year = {2020},
	pages = {15--20},
}

@inproceedings{kasem-madaniPrivacyPreservingWarningManagement2020,
	address = {Rennes France},
	title = {Privacy-{Preserving} {Warning} {Management} for an {Identity} {Leakage} {Warning} {Network}},
	isbn = {978-1-4503-7599-3},
	url = {https://dl.acm.org/doi/10.1145/3424954.3424955},
	doi = {10.1145/3424954.3424955},
	abstract = {Identity leakage is the public disclosure of user accounts that were stolen from an online service provider, e.g. email adresses and passwords. Identity leakage is an emerging threat to the security of user accounts because the number of online identities grows notably faster than the amount of used email adresses and passwords.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {European} {Interdisciplinary} {Cybersecurity} {Conference}},
	publisher = {ACM},
	author = {Kasem-Madani, Saffija and Malderle, Timo and Boes, Felix and Meier, Michael},
	month = nov,
	year = {2020},
	pages = {1--6},
}

@inproceedings{marquezExploringSecurityIssues2019,
	address = {Montreal, QC, Canada},
	title = {Exploring {Security} {Issues} in {Telehealth} {Systems}},
	isbn = {978-1-72812-251-9},
	url = {https://ieeexplore.ieee.org/document/8823899/},
	doi = {10.1109/SEH.2019.00019},
	abstract = {Telehealth systems (TS’s) provide remote healthbased services to improve the quality of service of patient treatment. Most healthcare professionals have access to standard telecommunications technology (such as Wireless Body Area Network (WBAN), biosensors, remote medical robots, and others) to offer remote care of elderly and physically less able patients as well as remote surgeries, treatments, and diagnoses. In order to ensure the functionality of TS’s, several systemic properties must be satisﬁed, including security. Although there are studies that discuss different security approaches in TS’s, it is difﬁcult to have a clear view of existing security issues and solutions for these systems. In this article, a systematic mapping study was performed to detect, organize and characterize security issues in TS’s. We identiﬁed 41 studies which were classiﬁed according to their research strategy, target problem, security issue addressed, and proposals. Results reveal that (i) 4 security issues were identiﬁed; (ii) 3 strategies were distinguished to handle security issues; (iii) patient and wireless medical data are the most affected medical supplies. Security in TS’s reveals diverse challenges that concern Software Engineering. Areas such as requirements, software architecture, and security patterns play an important role in order to handle security issues.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} for {Healthcare} ({SEH})},
	publisher = {IEEE},
	author = {Marquez, Gaston and Astudillo, Hernan and Taramasco, Carla},
	month = may,
	year = {2019},
	pages = {65--72},
}

@article{ModelingInformationSeeking1996,
	title = {Modeling the {Information} {Seeking} of {Professionals}: {A} {General} {Model} {Derived} from {Research} on {Engineers}, {Health} {Care} {Professionals}, and {Lawyers}},
	volume = {66},
	issn = {0024-2519, 1549-652X},
	shorttitle = {Modeling the {Information} {Seeking} of {Professionals}},
	url = {https://www.journals.uchicago.edu/doi/10.1086/602864},
	doi = {10.1086/602864},
	language = {en},
	number = {2},
	urldate = {2021-03-31},
	journal = {The Library Quarterly},
	month = apr,
	year = {1996},
	pages = {161--193},
}

@article{friedmanLawLawyersLegal,
	title = {Law, {Lawyers}, and {Legal} {Practice} in {Silicon} {Valley}: {A} {Preliminary} {Report}},
	language = {en},
	author = {Friedman, Lawrence M and Gordon, Robert W and Pirie, Sophie and Whatley, Edwin},
	pages = {15},
}

@article{duboisHowLawyersEngineer2020,
	title = {How do {Lawyers} {Engineer} and {Develop} {LegalTech} {Projects}? {A} {Story} of {Opportunities}, {Platforms}, {Creative} {Rationalities}, and {Strategies}},
	volume = {2},
	issn = {2652-4074},
	shorttitle = {How do {Lawyers} {Engineer} and {Develop} {LegalTech} {Projects}?},
	url = {https://lthj.qut.edu.au/article/view/1558},
	doi = {10.5204/lthj.v3i1.1558},
	abstract = {Over the last 15 years, the working context of lawyers has undergone many changes. Evolving in an increasingly competitive, deregulated, and globalized market, they are subject to higher tax pressure while being exposed to unbridled technological innovation. Indeed, a growing number of entrepreneurs are using digital solutions to provide online legal services that are supposed to be faster and cheaper. If many of them are nonlawyer legal entrepreneurs, many lawyers are also engineering innovative projects and launching their own start-up companies, known as “LegalTech” or “LawTech.” However, few studies—or none to our limited knowledge—provide an empirically grounded analysis of such projects, leaving some questions unanswered. Who are these entrepreneurial lawyers? How and why do they engineer and develop LegalTech projects? How do they challenge the legal profession? To answer these questions, this article draws on a qualitative study of three contrasted start-ups Belgian lawyers have recently developed. The research methodology combines gray and scientific literature reviews, webdocument (hereafter “manifestos”) analysis, and semi-directive interviews led with the start-up’s founders (n = 5), the Bar Association’s representatives (n = 3), and some members of the main Belgian LegalTech network (n = 4).},
	language = {en},
	number = {2},
	urldate = {2021-03-31},
	journal = {Law, Technology and Humans},
	author = {Dubois, Christophe},
	month = dec,
	year = {2020},
}

@article{bobkowskaEfficientCollaborationLawyers,
	title = {On efficient collaboration between lawyers and software engineers when transforming legal regulations to law-related requirements},
	abstract = {In order to develop information systems which comply with law, cooperation between lawyers and software engineers is necessary. The problem is how to transform legal regulations to law-related systems requirements efficiently. In this paper, we present both lawyer's perspective and software engineer's perspective on this problem and then we attempt to capture a common information space. The lawyer's perspective delivers a method for identifying and analyzing relevant laws and legal regulations. The software engineer's perspective discovers the specifics of dealing with law-related requirements with the use of requirements engineering as a frame of reference. The common information space includes a process of transforming legal regulations to legal requirements and a description of knowledge which must be shared in order to facilitate efficient collaboration.},
	language = {en},
	author = {Bobkowska, Anna and Kowalska, Magdalena},
	pages = {5},
}

@article{ajzen2011theory,
	title = {The theory of planned behaviour: {Reactions} and reflections},
	author = {Ajzen, Icek},
	year = {2011},
	note = {Publisher: Taylor \& Francis},
}

@article{johnson2004mixed,
	title = {Mixed methods research: {A} research paradigm whose time has come},
	volume = {33},
	number = {7},
	journal = {Educational researcher},
	author = {Johnson, R Burke and Onwuegbuzie, Anthony J},
	year = {2004},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {14--26},
}

@article{jonassenEverydayProblemSolving2006,
	title = {Everyday {Problem} {Solving} in {Engineering}:: {Lessons} tor {Engineering} {Educators}},
	language = {en},
	author = {Jonassen, David},
	year = {2006},
	pages = {14},
}

@article{jonassenDesignTheoryProblem2000,
	title = {Toward a design theory of problem solving},
	volume = {48},
	issn = {1042-1629, 1556-6501},
	url = {http://link.springer.com/10.1007/BF02300500},
	doi = {10.1007/BF02300500},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Educational Technology Research and Development},
	author = {Jonassen, David H.},
	month = dec,
	year = {2000},
	pages = {63--85},
}

@article{richards1983conversational,
	title = {Conversational analysis},
	journal = {Language and communication},
	author = {Richards, Jack C and Schmidt, Richard W},
	year = {1983},
	note = {Publisher: Longman New York},
	pages = {117--154},
}

@article{trevelyanPublishedResearchEngineering2007,
	title = {Published {Research} on {Engineering} {Work}},
	volume = {133},
	issn = {1052-3928, 1943-5541},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%291052-3928%282007%29133%3A4%28300%29},
	doi = {10.1061/(ASCE)1052-3928(2007)133:4(300)},
	abstract = {Engineering literature mostly focuses on the “objects” and techniques that interest engineers: there are only a few accounts of the people that do engineering work and what that work actually involves. The relevant literature contributes a combination of personal opinion, anecdotal reports, and empirical evidence from quantitative and a few qualitative surveys. Most of the empirical research provides only indirect and limited evidence on engineering work as it is practiced because the survey questions seem to be based on preconceived notions of engineering work. Almost all the papers in the literature that refer to engineering practice aim to provide evidence for changing engineering education or evidence to support a particular set of engineering competencies. From these narrow objectives we can learn a little about engineering work. More empirical evidence is needed before we can work towards a comprehensive understanding, let alone improvement of engineering working methods and practices.},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {Journal of Professional Issues in Engineering Education and Practice},
	author = {Trevelyan, James and Tilli, Sabbia},
	month = oct,
	year = {2007},
	pages = {300--307},
}

@article{trevelyanTechnicalCoordinationEngineering2007,
	title = {Technical {Coordination} in {Engineering} {Practice}},
	abstract = {An empirical ethnographic survey of engineers using interviews andfieldohservations in Australia provides evidence that coordinating technical work of other people by gaining their willing cooperation is a major aspect of engineering practice. Technical coordination in the context of this study means working with and influencing other people so they conscientiously perform necessary work to a mutually agreed schedule. While coordination seems to be non-technical, analysis provides evidence supporting the critical importance of technical expertise. Coordination usually involves one-on-one relationships with superiors, clients, peers, subordinates, and outsiders. Coordinating the work of other people seems to be importantfromthe start of an engineering career. Engineering education only provides limited informal coordination skill development and current accreditation criteria may not reflect this aspect of engineering. This paper suggests ways in which students can leam coordination, and describes some ofthe author's experiences in applying this research.},
	language = {en},
	author = {Trevelyan, James},
	year = {2007},
	pages = {15},
}

@article{johriSociomaterialBricolageCreation2011,
	title = {Sociomaterial bricolage: {The} creation of location-spanning work practices by global software developers},
	volume = {53},
	issn = {09505849},
	shorttitle = {Sociomaterial bricolage},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584911000437},
	doi = {10.1016/j.infsof.2011.01.014},
	abstract = {Objective: This study draws on practice-based studies of work to examine successful work practices of global software developers. The primary aim of this study was to understand how workers develop practices that allow them to function effectively across geographically dispersed locations.
Method: An ethnographically-informed ﬁeld study was conducted with data collection at two international locations of a ﬁrm. Interview, observation and archival data were collected. A total of 42 interviews and 3 weeks of observations were conducted.
Results: Teams spread across different locations around the world developed work practices through sociomaterial bricolage. Two facets of technology use were necessary for the creation of these practices: multiplicity of media and relational personalization at dyadic and team levels. New practices were triggered by the need to achieve a work-life balance, which was disturbed by global development. Reﬂecting on my role as a researcher, I underscore the importance of understanding researchers’ own frames of reference and using research practices that mirror informants’ work practices.
Conclusion: Software developers on global teams face unique challenges which necessitate a shift in their work practices. Successful teams are able to create practices that span locations while still being tied to location based practices. Inventive use of material and social resources is central to the creation of these practices.},
	language = {en},
	number = {9},
	urldate = {2021-03-26},
	journal = {Information and Software Technology},
	author = {Johri, Aditya},
	month = sep,
	year = {2011},
	pages = {955--968},
}

@inproceedings{jesiekWorkProgressNovel2020,
	address = {Virtual On line},
	title = {Work in {Progress}: {Novel} {Ethnographic} {Approaches} for {Investigating} {Engineering} {Practice}},
	shorttitle = {Work in {Progress}},
	url = {http://peer.asee.org/35672},
	doi = {10.18260/1-2--35672},
	language = {en},
	urldate = {2021-03-26},
	booktitle = {2020 {ASEE} {Virtual} {Annual} {Conference} {Content} {Access} {Proceedings}},
	publisher = {ASEE Conferences},
	author = {Jesiek, Brent and Johri, Aditya and Brozina, Cory and Korte, Russell},
	month = jun,
	year = {2020},
	pages = {35672},
}

@article{jesiekTypologySociotechnicalEngineering,
	title = {Toward a typology of the sociotechnical in engineering practice},
	abstract = {There remains a surprising lack of empirical research on the day-today work experiences of engineers and other technical professionals. Nonetheless, a growing body of scholarship in engineering studies and allied fields has revealed that engineering practice can be viewed as a kind of sociotechnical performance, with many job tasks requiring technical expertise as well as extensive social interactions and negotiations. The goal of this paper is to offer preliminary insights toward developing a typology of the sociotechnical in engineering practice. Analyzing interviews with early career engineers, we present our findings organized around three themes: 1) learning in engineering practice, 2) engineers as sociotechnical gatekeepers, and 3) sociotechnical interactions with boundary objects. These findings have implications for engineering education by demonstrating the need for learning activities that mirror the engineering workplace.},
	language = {en},
	author = {Jesiek, Brent K and Buswell, Natascha T and Mazzurco, Andrea and Zephirin, Tasha},
	pages = {10},
}

@article{almeidaEngineeringCommunicationIndustry2020,
	title = {Engineering communication in industry and cross-generational challenges: an exploratory study},
	issn = {0304-3797, 1469-5898},
	shorttitle = {Engineering communication in industry and cross-generational challenges},
	url = {https://www.tandfonline.com/doi/full/10.1080/03043797.2020.1737646},
	doi = {10.1080/03043797.2020.1737646},
	abstract = {Engineering communication is a highly desirable competence in industry. However, to become eﬃcient communicators, engineers must consider cross-generational styles in the process. In this exploratory case study, engineering leaders from four industrial segments (High-Tech, Automotive, Aerospace, and Manufacturing) who have had a history of communicating throughout diﬀerent contexts and projects were studied. Interviews and memos were primary data sources used in this study. Key ﬁndings included the need for younger generations of engineers to communicate thoughtfully and with humility. Also, older engineers expressed the need to adjust their communication styles with younger engineers by adapting their communication to include varying modalities (e.g., in person, in writing, with technology). The ﬁndings from this exploratory study can help engineering educators consider how to teach communication skills to students while increasing their awareness for cross-generational interactions upon graduation.},
	language = {en},
	urldate = {2021-03-26},
	journal = {European Journal of Engineering Education},
	author = {Almeida, Lilian Maria de Souza and Becker, Kurt Henry and Villanueva, Idalis},
	month = mar,
	year = {2020},
	pages = {1--13},
}

@article{bielefeldtWorkingEngineersSatisfaction2019,
	title = {Working engineers’ satisfaction with helping people and society through their jobs},
	volume = {44},
	issn = {0304-3797, 1469-5898},
	url = {https://www.tandfonline.com/doi/full/10.1080/03043797.2018.1476468},
	doi = {10.1080/03043797.2018.1476468},
	abstract = {This research explored the extent that working engineers were satisfied with their ability to help or serve people and/or society through their jobs. Over 450 engineering graduates responded to an online survey, including alumni recently transitioning to the workforce from 16 U.S. institutions and professional volunteers with Engineers Without BordersU.S.A. Only 18\% of the respondents currently working in engineering jobs had some level of dissatisfaction with helping others through their job; this differed by job sector and discipline but not gender or between recent alumni and service-active engineers. Forty per cent cited dissatisfaction with service aspects of their work as a contributing factor for leaving an engineering job. A few seemed to have left engineering careers due to dissatisfaction with their ability to help others. The results point to the importance of aligning personal goals for helping people/ society with engineering careers; employers facilitating these connections may reap benefits in employee retention.},
	language = {en},
	number = {6},
	urldate = {2021-03-26},
	journal = {European Journal of Engineering Education},
	author = {Bielefeldt, Angela R. and Canney, Nathan E.},
	month = nov,
	year = {2019},
	pages = {939--953},
}

@article{cranorInformingCaliforniaPrivacy2021,
	title = {Informing {California} privacy regulations with evidence from research},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3447253},
	doi = {10.1145/3447253},
	abstract = {Designing and testing 'Do Not Sell My Personal Information' icons.},
	language = {en},
	number = {3},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Cranor, Lorrie Faith},
	month = mar,
	year = {2021},
	pages = {29--32},
}

@inproceedings{jesiekClosingPracticeGap2017,
	address = {Indianapolis, IN},
	title = {Closing the practice gap: {Studying} boundary spanning in engineering practice to inform educational practice},
	isbn = {978-1-5090-5920-1},
	shorttitle = {Closing the practice gap},
	url = {http://ieeexplore.ieee.org/document/8190503/},
	doi = {10.1109/FIE.2017.8190503},
	abstract = {How to adequately prepare engineering graduates for careers in industry has long been a concern for academics, policymakers, and employers. However, the realities of engineering practice remain somewhat mysterious to many students and instructors, often leaving engineering graduates underprepared for the workplace, and employers dissatisfied with new employees. In this work, we seek to better understand the job expectations and realities of working life as experienced by early career engineers. In this paper, we more specifically focus on three engineering practice scenarios drawn from three interviews to demonstrate the complex, sociotechnical nature of engineering work in large corporations. Our analysis of workplace boundaries, boundary spanning activities, and personal attributes revealed an inextricably linked set of concepts that we present in a new conceptual framework. We conclude with a discussion of implications for both how we understand engineering practice and approach the teaching of engineering. It is expected that this paper can help students, faculty, and industry affiliates reflect on the realities of engineering work.},
	language = {en},
	urldate = {2021-03-26},
	booktitle = {2017 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	publisher = {IEEE},
	author = {Jesiek, Brent K. and Trellinger, Natascha and Nittala, Swetha},
	month = oct,
	year = {2017},
	pages = {1--9},
}

@inproceedings{jesiekBecomingBoundarySpanning2016,
	address = {New Orleans, Louisiana},
	title = {Becoming {Boundary} {Spanning} {Engineers}: {Research} {Methods} and {Preliminary} {Findings}},
	shorttitle = {Becoming {Boundary} {Spanning} {Engineers}},
	url = {http://peer.asee.org/26370},
	doi = {10.18260/p.26370},
	abstract = {A growing body of evidence suggests that practicing engineers are increasingly expected to act as boundary spanners who can participate in and manage diverse local and global teams, translate competing stakeholder demands into effective design solutions, and leverage expert knowledge from multiple fields and specialties. The larger project represented by this paper responds to this reality by proposing boundary spanning as a core meta-attribute for engineering students and early career professionals. This paper more specifically offers a detailed description of the study design for a major phase of this research project that involves conducting in-depth, semi-structured interviews about boundary spanning experiences with more than two dozen early career engineers in the manufacturing, construction, and electronics industries. To keep the scope of the present account more manageable, this paper provides a preliminary window onto our findings. We utilize a single case approach with a focus on three themes emerging from our analysis of two interviews with one research subject. Namely, we discuss transitions from school to internships to full-time job, the social aspects of engineering practice, and the emotional and psychological dimensions of professional work. One leading objective for this paper is to explore the utility of investigating the realities of engineering work through the lens of an overarching meta-attribute such as boundary spanning. We also propose that our findings provide valuable glimpses of engineering practice that might benefit students who are studying or considering studying engineering. This paper may additionally appeal to educators and researchers who are interested in qualitative methods and/or empirical studies of professional practice.},
	language = {en},
	urldate = {2021-03-26},
	booktitle = {2016 {ASEE} {Annual} {Conference} \& {Exposition} {Proceedings}},
	publisher = {ASEE Conferences},
	author = {Jesiek, Brent and Trellinger, Natascha and Mazzurco, Andrea},
	month = jun,
	year = {2016},
	pages = {26370},
}

@article{baytiyehIdentifyingChallengingFactors2012,
	title = {Identifying the challenging factors in the transition from colleges of engineering to employment},
	volume = {37},
	issn = {0304-3797, 1469-5898},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03043797.2011.644761},
	doi = {10.1080/03043797.2011.644761},
	language = {en},
	number = {1},
	urldate = {2021-03-26},
	journal = {European Journal of Engineering Education},
	author = {Baytiyeh, Hoda and Naja, Mohamad},
	month = mar,
	year = {2012},
	pages = {3--14},
}

@inproceedings{aydinWhenGDPRMeets2020,
	address = {Merkez Turkey},
	title = {When {GDPR} {Meets} {CRAs} ({Credit} {Reference} {Agencies}): {Looking} through the {Lens} of {Twitter}},
	isbn = {978-1-4503-8751-4},
	shorttitle = {When {GDPR} {Meets} {CRAs} ({Credit} {Reference} {Agencies})},
	url = {https://dl.acm.org/doi/10.1145/3433174.3433586},
	doi = {10.1145/3433174.3433586},
	abstract = {Collecting information about consumers and businesses from various sources, Credit reference agencies (CRAs) help many organizations such as financial institutions to assess creditworthiness of applicants and customers of their services. CRAs’ business model depends on processing a high volume of personal data including highly sensitive ones, which must be processed within the relevant legal frameworks in different countries they operate their business, e.g., the European Union’s new GDPR (General Data Protection Regulation). This paper reports a data-driven analysis of CRA- and GDPR-related discussions on Twitter. Our analysis covers the three largest multi-national CRAs: Equifax, Experian and TransUnion and we also looked at the UK’s data protection authority, ICO, and two UK-based privacy-advocating NGOs, Privacy International and Open Rights Group (ORG). We have analyzed public tweets of their official Twitter accounts and other public tweets talking about them. Our analysis revealed a very surprising lack of awareness of CRA- and GDPR-related data privacy issues within the general public and an astonishing lack of active communications of CRAs to the general public on relevant GDPR-related privacy issues: out of 39,549 collected tweets we identified only 153 relevant tweets (0.387\%). This small number of tweets are dominated by mentions of security issues (\%73.2), especially data breaches affecting CRAs, not data subject rights or privacy issues directly. Other tweets are mainly about complaints regarding inaccurate data in credit files and questions about how to exercise right to rectification, just two of many data subject rights defined in the GDPR.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {13th {International} {Conference} on {Security} of {Information} and {Networks}},
	publisher = {ACM},
	author = {Aydin, Kubra and Saglam, Rahime Belen and Li, Shujun and Bulbul, Abdullah},
	month = nov,
	year = {2020},
	pages = {1--8},
}

@article{baratiGDPRComplianceVerification2020,
	title = {{GDPR} {Compliance} {Verification} in {Internet} of {Things}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9127459/},
	doi = {10.1109/ACCESS.2020.3005509},
	abstract = {Data privacy in Internet of Things (IoT) applications remains a major concern of regulation bodies. The introduction of the European General Data Protection Regulation (GDPR) enables users to control how their data is accessed and processed, requiring consent from users before any data manipulation is carried out on their (personal) data by smart devices or cloud-hosted services. Blockchains provide the beneﬁts of a distributed and immutable ledger recording digital transactions across a global network of peer nodes. Blockchain support for tracking of operations carried out by an IoT-based system provides greater conﬁdence to a user that the IoT device is not infringing user privacy (as the Blockchain can be audited to verify which operation was carried out, by which actor). A formal model (following the privacy-bydesign approach) is proposed for supporting GDPR compliance checking for smart devices. The privacy requirements of such applications are related to GDPR obligations of device (and software systems) operators (such as user consent, data protection, right to forget etc). Three smart contracts are proposed as a practical solution to support automated veriﬁcation of operations carried out by devices on user data, in accordance with GDPR rules. We evaluate the performance and scalability costs of our approach using a Blockchain test network.},
	language = {en},
	urldate = {2021-03-24},
	journal = {IEEE Access},
	author = {Barati, Masoud and Rana, Omer and Petri, Ioan and Theodorakopoulos, George},
	year = {2020},
	pages = {119697--119709},
}

@article{shastriGDPRAntipatterns2021,
	title = {{GDPR} anti-patterns},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3378061},
	doi = {10.1145/3378061},
	abstract = {How design and operation of modern cloud-scale systems conflict with GDPR.},
	language = {en},
	number = {2},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Shastri, Supreeth and Wasserman, Melissa and Chidambaram, Vijay},
	month = jan,
	year = {2021},
	pages = {59--65},
}

@article{pappachanSieveMiddlewareApproach2020,
	title = {Sieve: a middleware approach to scalable access control for database management systems},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {Sieve},
	url = {https://dl.acm.org/doi/10.14778/3407790.3407835},
	doi = {10.14778/3407790.3407835},
	abstract = {Current approaches for enforcing Fine Grained Access Control (FGAC) in DBMS do not scale to scenarios when the number of access control policies are in the order of thousands. This paper identiﬁes such a use case in the context of emerging smart spaces wherein systems may be required by legislation, such as Europe’s GDPR and California’s CCPA, to empower users to specify who may have access to their data and for what purposes. We present Sieve, a layered approach of implementing FGAC in existing DBMSs, that exploits a variety of their features (e.g., UDFs, index usage hints, query explain) to scale to a large number of policies. Given a query, Sieve exploits its context to ﬁlter the policies that need to be checked. It also generates guarded expressions that save on evaluation cost by grouping policies and exploit database indices to cut on read cost. Our experimental results demonstrate that existing DBMSs can utilize Sieve to signiﬁcantly reduce query-time policy evaluation cost. Using Sieve DBMSs can support real-time access control in applications such as emerging smart environments.},
	language = {en},
	number = {12},
	urldate = {2021-03-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Pappachan, Primal and Yus, Roberto and Mehrotra, Sharad and Freytag, Johann-Christoph},
	month = aug,
	year = {2020},
	pages = {2424--2437},
}

@inproceedings{jungPrivacyOracleSystem2008,
	address = {Alexandria, Virginia, USA},
	title = {Privacy oracle: a system for finding application leaks with black box differential testing},
	isbn = {978-1-59593-810-7},
	shorttitle = {Privacy oracle},
	url = {http://portal.acm.org/citation.cfm?doid=1455770.1455806},
	doi = {10.1145/1455770.1455806},
	abstract = {We describe the design and implementation of Privacy Oracle, a system that reports on application leaks of user information via the network trafﬁc that they send. Privacy Oracle treats each application as a black box, without access to either its internal structure or communication protocols. This means that it can be used over a broad range of applications and information leaks (i.e., not only Web trafﬁc content or credit card numbers). To accomplish this, we develop a differential testing technique in which perturbations in the application inputs are mapped to perturbations in the application outputs to discover likely leaks; we leverage alignment algorithms from computational biology to ﬁnd high quality mappings between different byte-sequences efﬁciently. Privacy Oracle includes this technique and a virtual machine-based testing system. To evaluate it, we tested 26 popular applications, including system and ﬁle utilities, media players, and IM clients. We found that Privacy Oracle discovered many small and previously undisclosed information leaks. In several cases, these are leaks of directly identifying information that are regularly sent in the clear (without endto-end encryption) and which could make users vulnerable to tracking by third parties or providers.},
	language = {en},
	urldate = {2021-04-04},
	booktitle = {Proceedings of the 15th {ACM} conference on {Computer} and communications security - {CCS} '08},
	publisher = {ACM Press},
	author = {Jung, Jaeyeon and Sheth, Anmol and Greenstein, Ben and Wetherall, David and Maganis, Gabriel and Kohno, Tadayoshi},
	year = {2008},
	pages = {279},
}

@misc{directorTop10Open2019,
	title = {Top 10 {Open} {Source} {Social} {Network} {Development} {Platforms} - {TDA}},
	url = {https://topdigital.agency/top-10-open-source-social-network-development-platforms/},
	abstract = {World Web Technology, a social network website development company, shares its top 10 open-source development platforms.},
	language = {en-US},
	urldate = {2021-04-06},
	journal = {Top Digital Agency},
	author = {Director, Sanjay Ghinaiya},
	month = sep,
	year = {2019},
}

@inproceedings{lodgeIoTAppDevelopment2018,
	address = {Singapore Singapore},
	title = {{IoT} {App} {Development}: {Supporting} {Data} {Protection} by {Design} and {Default}},
	isbn = {978-1-4503-5966-5},
	shorttitle = {{IoT} {App} {Development}},
	url = {https://dl.acm.org/doi/10.1145/3267305.3274151},
	doi = {10.1145/3267305.3274151},
	abstract = {In the domestic IoT domain, data is often collected by physical sensors and actuators embedded in the household and used to provide contextually relevant services to end users. Given that this data is often personal, the EU’s General Data Protection Regulation can implicate IoT app developers, requiring them to adhere to "data protection by design and default" to ensure safeguards that protect a data subject’s rights. Yet the simple-to-use task-oriented development environments that are commonly used to build domestic IoT apps provide little support for developers to engage with data protection measures. In this paper we present an overview of an IoT development environment that has been designed to help developers engage with data protection at app design time. We describe a data tracking feature, which makes all personal ﬂows in an app explicit at development time and which provides the foundation for an additonal set of data protection measures, including personal data disclosure risk assessments, transparency of processing and runtime inspection.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2018 {ACM} {International} {Joint} {Conference} and 2018 {International} {Symposium} on {Pervasive} and {Ubiquitous} {Computing} and {Wearable} {Computers}},
	publisher = {ACM},
	author = {Lodge, Tom and Crabtree, Andy and Brown, Anthony},
	month = oct,
	year = {2018},
	pages = {901--910},
}

@inproceedings{ghayyurDesigningPrivacyPreserving2020,
	address = {Virtual Event Japan},
	title = {Designing privacy preserving data sharing middleware for internet of things},
	isbn = {978-1-4503-8136-9},
	url = {https://dl.acm.org/doi/10.1145/3419016.3431484},
	doi = {10.1145/3419016.3431484},
	abstract = {The rise of low-cost Internet of Things (IoT) sensing and communication capabilities has given rise to a range of new smart services that rely on heterogeneous data from devices embedded in our everyday lives. The provision of such IoT services relies on environmental or user data from other data controllers (e.g. network provider, water agency, building management). Recent privacy regulations such as the European General Data Protection Requirement (GDPR) and California Consumer Privacy Act (CCPA) have made it mandatory for data controllers to perform enhanced processing of the shared data with appropriate privacy-preserving mechanisms before release to service providers. To facilitate this, we propose PEIoT, a system for orchestrating privacy-enhanced data flows that (a) provides users (data subjects) with capabilities to opt-in/opt-out in the data that is shared with the service providers and (b) enable data controllers to invoke a range of Privacy Enhancing Technologies (PETs) such as anonymization, randomization, and perturbation to transform data streams into their privacy preserving counterparts. PE-IoT is based on a new model for privacy compliant data sharing and we describe the design and architecture of the PE-IoT system based on this model.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Third} {Workshop} on {Data}: {Acquisition} {To} {Analysis}},
	publisher = {ACM},
	author = {Ghayyur, Sameera and Pappachan, Primal and Wang, Guoxi and Mehrotra, Sharad and Venkatasubramanian, Nalini},
	month = nov,
	year = {2020},
	pages = {1--6},
}

@inproceedings{arztFlowDroidPreciseContext2014,
	address = {Edinburgh United Kingdom},
	title = {{FlowDroid}: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for {Android} apps},
	isbn = {978-1-4503-2784-8},
	shorttitle = {{FlowDroid}},
	url = {https://dl.acm.org/doi/10.1145/2594291.2594299},
	doi = {10.1145/2594291.2594299},
	abstract = {Today’s smartphones are a ubiquitous source of private and conﬁdential data. At the same time, smartphone users are plagued by carelessly programmed apps that leak important data by accident, and by malicious apps that exploit their given privileges to copy such data intentionally. While existing static taint-analysis approaches have the potential of detecting such data leaks ahead of time, all approaches for Android use a number of coarse-grain approximations that can yield high numbers of missed leaks and false alarms. In this work we thus present FLOWDROID, a novel and highly precise static taint analysis for Android applications. A precise model of Android’s lifecycle allows the analysis to properly handle callbacks invoked by the Android framework, while context, ﬂow, ﬁeld and object-sensitivity allows the analysis to reduce the number of false alarms. Novel on-demand algorithms help FLOWDROID maintain high efﬁciency and precision at the same time.},
	language = {en},
	urldate = {2021-04-04},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Arzt, Steven and Rasthofer, Siegfried and Fritz, Christian and Bodden, Eric and Bartel, Alexandre and Klein, Jacques and Le Traon, Yves and Octeau, Damien and McDaniel, Patrick},
	month = jun,
	year = {2014},
	pages = {259--269},
}

@inproceedings{floreaGlobalViewHard2019,
	address = {Montreal, QC, Canada},
	title = {A {Global} {View} on the {Hard} {Skills} and {Testing} {Tools} in {Software} {Testing}},
	isbn = {978-1-5386-9196-0},
	url = {https://ieeexplore.ieee.org/document/8807575/},
	doi = {10.1109/ICGSE.2019.00035},
	abstract = {Developing software with high quality is challenging in distributed software development. The purpose of the current study is to investigate the testing skills and tools required in the ever-changing world of global software engineering, according to industrial needs. We analysed 500 job ads from 33 countries. The results show that a quarter of the testers and a fifth of developers are asked to work in distributed projects. The testers are asked to be highly skilled in a variety of test activities and tools, while the testing-skills demand for developers is low and somewhat vague. The profile of testers has a strong technical component in addition to the managerial one. Our findings show that employers need most that testers are competent in automated testing. Furthermore, the industry does not cover all aspects of testing with the demand for testers and developers. Surprisingly, neither role is asked to test the implementation of the general data protection requirements. Our study bridges the industrial needs and the practitioners’ skill development process. Therefore, software testers can use our study as a reference point to enhance their skills. Employers should use our results to check their testing-skill coverage within the development teams. Tertiary education providers are encouraged to use our findings, to update the curriculum in the software development area.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {2019 {ACM}/{IEEE} 14th {International} {Conference} on {Global} {Software} {Engineering} ({ICGSE})},
	publisher = {IEEE},
	author = {Florea, Raluca and Stray, Viktoria},
	month = may,
	year = {2019},
	pages = {143--151},
}

@misc{prakashDecentralizedOpenSource,
	title = {9 {Decentralized}, {Open} {Source} {Alternative} {Social} {Media} {Platforms}},
	url = {https://itsfoss.com/mainstream-social-media-alternaives/},
	abstract = {Tired of Big Tech prying on your data and invading your privacy? Here are some open source, decentralized alternate social platforms.},
	language = {en\_US},
	urldate = {2021-04-06},
	journal = {https://itsfoss.com/},
	author = {Prakash, Abhishek},
}

@article{kalloniatis2008addressing,
	title = {Addressing privacy requirements in system design: the {PriS} method},
	volume = {13},
	number = {3},
	journal = {Requirements Engineering},
	author = {Kalloniatis, Christos and Kavakli, Evangelia and Gritzalis, Stefanos},
	year = {2008},
	note = {Publisher: Springer},
	pages = {241--255},
}

@article{deng2011privacy,
	title = {A privacy threat analysis framework: supporting the elicitation and fulfillment of privacy requirements},
	volume = {16},
	number = {1},
	journal = {Requirements Engineering},
	author = {Deng, Mina and Wuyts, Kim and Scandariato, Riccardo and Preneel, Bart and Joosen, Wouter},
	year = {2011},
	note = {Publisher: Springer},
	pages = {3--32},
}

@inproceedings{pereraIntegratingHumanValues2019,
	address = {Jeju Island, Korea (South)},
	title = {Towards {Integrating} {Human} {Values} into {Software}: {Mapping} {Principles} and {Rights} of {GDPR} to {Values}},
	isbn = {978-1-72813-912-8},
	shorttitle = {Towards {Integrating} {Human} {Values} into {Software}},
	url = {https://ieeexplore.ieee.org/document/8920447/},
	doi = {10.1109/RE.2019.00053},
	abstract = {Software has become an integral part of human life. This gives rise to the need of developing software that respects human values such as transparency, fairness and privacy. Software that compromises on human values (e.g. privacy) can affect people’s reputation and impinges on their ability to function in society with the usual freedom and autonomy. Integrating human values into software is, however, a challenging task due to its imprecise and subjective nature. Enforcing regulations is one way to make software development considerate of the desired standards and values. The European Union’s General Data Protection Regulation (GDPR) on software is one such effort to protect EU citizens’ data and personal information. GDPR prescribes data protection principles and data subject rights mainly to protect user privacy. Looking beyond privacy, we studied GDPR to identify the extent to which it covers human values. We mapped GDPR’s data protection principles and data subject rights to a widely accepted human values structure adopted from social sciences. Our results show that GDPR addresses not only privacy but also several other human values including power, security and universalism. Moreover, fairness and transparency stand out as the most value-conscious principles prescribed in GDPR.},
	language = {en},
	urldate = {2021-03-30},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Perera, Harsha and Hussain, Waqar and Mougouei, Davoud and Shams, Rifat Ara and Nurwidyantoro, Arif and Whittle, Jon},
	month = sep,
	year = {2019},
	pages = {404--409},
}

@inproceedings{chopraSocialMachinesSocial2016,
	address = {Montréal Québec Canada},
	title = {From {Social} {Machines} to {Social} {Protocols}: {Software} {Engineering} {Foundations} for {Sociotechnical} {Systems}},
	isbn = {978-1-4503-4143-1},
	shorttitle = {From {Social} {Machines} to {Social} {Protocols}},
	url = {https://dl.acm.org/doi/10.1145/2872427.2883018},
	doi = {10.1145/2872427.2883018},
	abstract = {The overarching vision of social machines is to facilitate social processes by having computers provide administrative support. We conceive of a social machine as a sociotechnical system (STS): a software-supported system in which autonomous principals such as humans and organizations interact to exchange information and services. Existing approaches for social machines emphasize the technical aspects and inadequately support the meanings of social processes, leaving them informally realized in human interactions. We posit that a fundamental rethinking is needed to incorporate accountability, essential for addressing the openness of the Web and the autonomy of its principals.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Chopra, Amit K. and Singh, Munindar P.},
	month = apr,
	year = {2016},
	pages = {903--914},
}

@inproceedings{hutchinsonAccountabilityMachineLearning2021,
	address = {Virtual Event Canada},
	title = {Towards {Accountability} for {Machine} {Learning} {Datasets}: {Practices} from {Software} {Engineering} and {Infrastructure}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Towards {Accountability} for {Machine} {Learning} {Datasets}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445918},
	doi = {10.1145/3442188.3445918},
	abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decisionmaking, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
	month = mar,
	year = {2021},
	pages = {560--575},
}

@misc{Top155Social,
	title = {The {Top} 155 {Social} {Network} {Open} {Source} {Projects}},
	url = {https://awesomeopensource.com/projects/social-network},
	urldate = {2021-04-06},
}

@inproceedings{hjerppeGeneralDataProtection2019,
	address = {Jeju Island, Korea (South)},
	title = {The {General} {Data} {Protection} {Regulation}: {Requirements}, {Architectures}, and {Constraints}},
	isbn = {978-1-72813-912-8},
	shorttitle = {The {General} {Data} {Protection} {Regulation}},
	url = {https://ieeexplore.ieee.org/document/8920529/},
	doi = {10.1109/RE.2019.00036},
	abstract = {The General Data Protection Regulation (GDPR) in the European Union is the most famous recently enacted privacy regulation. Despite of the regulation’s legal, political, and technological ramiﬁcations, relatively little research has been carried out for better understanding the GDPR’s practical implications for requirements engineering and software architectures. Building on a grounded theory approach with close ties to the Finnish software industry, this paper contributes to the sealing of this gap in previous research. Three questions are asked and answered in the context of software development organizations. First, the paper elaborates nine practical constraints under which many small and medium-sized enterprises (SMEs) often operate when implementing solutions that address the new regulatory demands. Second, the paper elicits nine regulatory requirements from the GDPR for software architectures. Third, the paper presents an implementation for a software architecture that complies both with the requirements elicited and the constraints elaborated.},
	language = {en},
	urldate = {2021-03-31},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Hjerppe, Kalle and Ruohonen, Jukka and Leppanen, Ville},
	month = sep,
	year = {2019},
	pages = {265--275},
}

@article{blixDataProtectionDesign,
	title = {Data {Protection} by {Design} in {Systems} {Development}},
	abstract = {Data protection by design is a principle to systems development meaning that the protection of personal data is built into the systems design from the start. For many jurisdictions, this principle is becoming a legal requirement. Using a research approach based on design science, a framework is constructed helping systems developers achieve privacy by design in a systematic manner. The framework articulate how the business requirements can be captured, assessed, and implemented in the systems development. Examples of how the data protection principles can be concretely implemented is also presented.},
	language = {en},
	author = {Blix, Fredrik and Elshekeil, Salah Addin and Laoyookhong, Saran},
	pages = {6},
}

@inproceedings{boehm1976quantitative,
	title = {Quantitative evaluation of software quality},
	booktitle = {Proceedings of the 2nd international conference on {Software} engineering},
	author = {Boehm, Barry W and Brown, John R and Lipow, Mlity},
	year = {1976},
	pages = {592--605},
}

@incollection{alshammariPrincipledApproachEngineering2017,
	address = {Cham},
	title = {Towards a {Principled} {Approach} for {Engineering} {Privacy} by {Design}},
	volume = {10518},
	isbn = {978-3-319-67279-3 978-3-319-67280-9},
	url = {http://link.springer.com/10.1007/978-3-319-67280-9_9},
	abstract = {Privacy by Design has emerged as a proactive approach for embedding privacy into the early stages of the design of information and communication technologies, but it is no ‘silver bullet’. Challenges involved in engineering Privacy by Design include a lack of holistic and systematic methodologies that address the complexity and variability of privacy issues and support the translation of its principles into engineering activities. A consequence is that its principles are given at a high level of abstraction without accompanying tools and guidelines to address these challenges. We analyse three privacy requirements engineering methods from which we derive a set of criteria that aid in identifying data-processing activities that may lead to privacy violations and harms and also aid in specifying appropriate design decisions. We also present principles for engineering Privacy by Design that can be developed upon these criteria. Based on these, we outline some preliminary thoughts on the form of a principled framework that addresses the plurality and contextuality of privacy issues and supports the translation of the principles of Privacy by Design into engineering activities.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Privacy {Technologies} and {Policy}},
	publisher = {Springer International Publishing},
	author = {Alshammari, Majed and Simpson, Andrew},
	editor = {Schweighofer, Erich and Leitold, Herbert and Mitrakas, Andreas and Rannenberg, Kai},
	year = {2017},
	doi = {10.1007/978-3-319-67280-9_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {161--177},
}

@inproceedings{hafizCollectionPrivacyDesign2006,
	address = {Portland, Oregon},
	title = {A collection of privacy design patterns},
	isbn = {978-1-60558-372-3},
	url = {http://portal.acm.org/citation.cfm?doid=1415472.1415481},
	doi = {10.1145/1415472.1415481},
	abstract = {The growth in computing power has enabled the storage and analysis of large volumes of data. Monitoring the Internet access proﬁles of millions of users has become feasible and also economically lucrative. The interesting thing here is that it is not only the crooks who are interested in privacy intrusion, but government agencies also have vested interest in proﬁling the population mass. This paper describes 4 design patterns that can aide the decision making process for the designers of privacy protecting systems. These design patterns are applicable to the design of anonymity systems for various types of online communication, online data sharing, location monitoring, voting and electronic cash management.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2006 conference on {Pattern} languages of programs - {PLoP} '06},
	publisher = {ACM Press},
	author = {Hafiz, Munawar},
	year = {2006},
	pages = {1},
}

@article{gerberHowStopEngineers2009,
	title = {How to {Stop} {Engineers} from {Becoming} “{Bush} {Lawyers}”: {The} {Art} of {Teaching} {Law} to {Engineering} and {Construction} {Students}},
	volume = {1},
	issn = {1943-4162, 1943-4170},
	shorttitle = {How to {Stop} {Engineers} from {Becoming} “{Bush} {Lawyers}”},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%291943-4162%282009%291%3A4%28179%29},
	doi = {10.1061/(ASCE)1943-4162(2009)1:4(179)},
	abstract = {Law forms a core part of most engineering and construction programs. The way that law subjects are taught varies dramatically, and too often focuses on trying to teach students complex aspects of the law, such as contract, tort, and trade practices. This paper suggests that the aim of including law subjects in construction and engineering degrees needs to be clearly understood as this determines the content of the law subject. It is argued that the reason for including a law subject should be not to teach students the law, but rather to train them to recognize when legal issues arise in their work, and how to respond to such issues. With this aim in mind, a model curriculum is proposed and insight given into how to most effectively implement such a course.},
	language = {en},
	number = {4},
	urldate = {2021-03-31},
	journal = {Journal of Legal Affairs and Dispute Resolution in Engineering and Construction},
	author = {Gerber, Paula},
	month = nov,
	year = {2009},
	pages = {179--188},
}

@inproceedings{ayalonHowDevelopersMake2017,
	address = {Portland Oregon USA},
	title = {How {Developers} {Make} {Design} {Decisions} about {Users}' {Privacy}: {The} {Place} of {Professional} {Communities} and {Organizational} {Climate}},
	isbn = {978-1-4503-4688-7},
	shorttitle = {How {Developers} {Make} {Design} {Decisions} about {Users}' {Privacy}},
	url = {https://dl.acm.org/doi/10.1145/3022198.3026326},
	doi = {10.1145/3022198.3026326},
	abstract = {New technologies continuously challenge people's information privacy, while privacy protection practices, such as Privacy-by-Design, did not become widespread engineering practices. The difficulty of designing privacy-preserving information systems highlights the need for studying developers’ privacy decision-making processes, as the developers’ communities are well coordinated and they are the ones who can promote or hinder privacy in the systems they design. We conducted an online survey (n=101), to understand the effectors on developers’ professional privacy attitudes and practices. We investigate whether developer’s privacy decision-making is shaped by several factors, including organizational (organizational privacy climate, business and legal contexts), professional (privacy knowledge) and personal (personal perceived privacy).},
	language = {en},
	urldate = {2021-03-25},
	booktitle = {Companion of the 2017 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {ACM},
	author = {Ayalon, Oshrat and Toch, Eran and Hadar, Irit and Birnhack, Michael},
	month = feb,
	year = {2017},
	pages = {135--138},
}

@inproceedings{ahmadianPrivacyenhancedSystemDesign2019,
	address = {Limassol Cyprus},
	title = {Privacy-enhanced system design modeling based on privacy features},
	isbn = {978-1-4503-5933-7},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297431},
	doi = {10.1145/3297280.3297431},
	abstract = {To ensure that their stakeholders’ privacy concerns are addressed systematically from the early development phases, organizations can perform a privacy enhancement of the system design. Such a privacy enhancement needs to account for three crucial types of input: First, risks to the rights of natural persons. Second, potential interrelations and dependencies among the privacy controls. Third, potential trade-offs regarding the costs of the controls. Despite numerous existing privacy enhancing technologies and catalogs of privacy controls, there has been no systematic methodology to support privacy enhancement based on these types of input.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Ahmadian, Amir Shayan and Strüber, Daniel and Jürjens, Jan},
	month = apr,
	year = {2019},
	pages = {1492--1499},
}

@article{rivaSoKEngineeringPrivacyaware2020,
	title = {{SoK}: {Engineering} privacy-aware high-tech systems},
	abstract = {The processing of personal data is becoming a key business factor, especially for high-tech system industries such as automotive and healthcare service providers. To protect such data, the European Union (EU) has introduced the General Data Protection Regulation (GDPR), with the aim to standardize and strengthen data protection policies across EU countries. The GDPR defines stringent requirements on the collection and processing of personal data and imposes severe fines and penalties on data controllers and processors for non-compliance. Although the GDPR is enforce since 2018, many public and private organizations are still struggling to fully comply with the regulation. A main reason for this is the lack of usable methodologies that can support developers in designing of GDPRcomplaint high-tech systems. This paper examines the growing literature on methodologies for the design of privacy-aware systems, and identifies the main challenges to be addressed in order to facilitate developers in the design of such systems. In particular, we investigate to what extent existing methodologies (i) cover GDPR and privacy-by-design principles, (ii) address different levels of system design concerns, and (iii) have demonstrated their suitability for the purpose. Our literature study shows that the domain landscape appears to be heterogeneous and disconnected, as existing methodologies often focus only on subsets of the GDPR principles and/or on specific angles of system design. Based on our findings, we provide recommendations on the definition of comprehensive methodologies tailored to designing GDPR-compliant high-tech systems.},
	language = {en},
	author = {Riva, Giovanni Maria and Vasenev, Alexandr and Zannone, Nicola},
	year = {2020},
	pages = {10},
}

@inproceedings{chen2019reliable,
	title = {How reliable is the crowdsourced knowledge of security implementation?},
	booktitle = {2019 {IEEE}/{ACM} 41st international conference on software engineering ({ICSE})},
	author = {Chen, Mengsu and Fischer, Felix and Meng, Na and Wang, Xiaoyin and Grossklags, Jens},
	year = {2019},
	note = {tex.organization: IEEE},
	pages = {536--547},
}

@article{balebakoImprovingAppPrivacy2014,
	title = {Improving {App} {Privacy}: {Nudging} {App} {Developers} to {Protect} {User} {Privacy}},
	volume = {12},
	issn = {1540-7993},
	shorttitle = {Improving {App} {Privacy}},
	url = {http://ieeexplore.ieee.org/document/6876252/},
	doi = {10.1109/MSP.2014.70},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {IEEE Security \& Privacy},
	author = {Balebako, Rebecca and Cranor, Lorrie},
	month = jul,
	year = {2014},
	pages = {55--58},
}

@article{szekely201310,
	title = {10 what do {IT} professionals think about surveillance?},
	volume = {16},
	journal = {Internet and surveillance: The challenges of Web 2.0 and social media},
	author = {Szekely, Ivan},
	year = {2013},
	note = {Publisher: Routledge},
	pages = {198},
}

@inproceedings{tahaeiUnderstandingPrivacyRelatedQuestions2020,
	address = {Honolulu HI USA},
	title = {Understanding {Privacy}-{Related} {Questions} on {Stack} {Overflow}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376768},
	doi = {10.1145/3313831.3376768},
	abstract = {We analyse Stack Overﬂow (SO) to understand challenges and confusions developers face while dealing with privacy-related topics. We apply topic modelling techniques to 1,733 privacyrelated questions to identify topics and then qualitatively analyse a random sample of 315 privacy-related questions. Identiﬁed topics include privacy policies, privacy concerns, access control, and version changes. Results show that developers do ask SO for support on privacy-related issues. We also ﬁnd that platforms such as Apple and Google are deﬁning privacy requirements for developers by specifying what “sensitive” information is and what types of information developers need to communicate to users (e.g. privacy policies). We also examine the accepted answers in our sample and ﬁnd that 28\% of them link to ofﬁcial documentation and more than half are answered by SO users without references to any external resources.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Tahaei, Mohammad and Vaniea, Kami and Saphra, Naomi},
	month = apr,
	year = {2020},
	pages = {1--14},
}

@inproceedings{senarathWhyDevelopersCannot2018,
	address = {Christchurch New Zealand},
	title = {Why developers cannot embed privacy into software systems?: {An} empirical investigation},
	isbn = {978-1-4503-6403-4},
	shorttitle = {Why developers cannot embed privacy into software systems?},
	url = {https://dl.acm.org/doi/10.1145/3210459.3210484},
	doi = {10.1145/3210459.3210484},
	abstract = {Pervasive use of software applications continue to challenge user privacy when users interact with software systems. Even though privacy practices such as Privacy by Design (PbD), have clear instructions for software developers to embed privacy into software designs, those practices are yet to become a common practice among software developers. The difficulty of developing privacy preserving software systems highlights the importance of investigating software developers and the problems they face when they are asked to embed privacy into application designs. Software developers are the community who can put practices such as PbD into action. Therefore identifying the problems they face when embedding privacy into software applications and providing solutions to those problems are important to enable the development of privacy preserving software systems. This study investigates 36 software developers in a software design task with instructions to embed privacy in order to identify the problems they face. We derive recommendation guidelines to address the problems to enable the development of privacy preserving software systems.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2018},
	publisher = {ACM},
	author = {Senarath, Awanthika and Arachchilage, Nalin A. G.},
	month = jun,
	year = {2018},
	pages = {211--216},
}

@article{bednarEngineeringPrivacyDesign2019,
	title = {Engineering {Privacy} by {Design}: {Are} engineers ready to live up to the challenge?},
	volume = {35},
	issn = {0197-2243, 1087-6537},
	shorttitle = {Engineering {Privacy} by {Design}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01972243.2019.1583296},
	doi = {10.1080/01972243.2019.1583296},
	abstract = {Organizations struggle to comply with legal requirements as well as customers’ calls for better data protection. On the implementation level, incorporation of privacy protections in products and services depends on the commitment of the engineers who design them. We interviewed six senior engineers, who work for globally leading IT corporations and research institutions, to investigate their motivation and ability to comply with privacy regulations. Our findings point to a lack of perceived responsibility, control, autonomy, and frustrations with interactions with the legal world. While we increasingly call on engineers to go beyond functional requirements and be responsive to human values in our increasingly technological society, we may be facing the dilemma of asking engineers to live up to a challenge they are currently not ready to embrace.},
	language = {en},
	number = {3},
	urldate = {2021-03-24},
	journal = {The Information Society},
	author = {Bednar, Kathrin and Spiekermann, Sarah and Langheinrich, Marc},
	month = may,
	year = {2019},
	pages = {122--142},
}

@inproceedings{urbanMeasuringImpactGDPR2020,
	address = {Taipei Taiwan},
	title = {Measuring the {Impact} of the {GDPR} on {Data} {Sharing} in {Ad} {Networks}},
	isbn = {978-1-4503-6750-9},
	url = {https://dl.acm.org/doi/10.1145/3320269.3372194},
	doi = {10.1145/3320269.3372194},
	abstract = {The European General Data Protection Regulation (GDPR), which went into effect in May 2018, brought new rules for the processing of personal data that affect many business models, including online advertising. The regulation’s definition of personal data applies to every company that collects data from European Internet users. This includes tracking services that, until then, argued that they were collecting anonymous information and data protection requirements would not apply to their businesses.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 15th {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Urban, Tobias and Tatang, Dennis and Degeling, Martin and Holz, Thorsten and Pohlmann, Norbert},
	month = oct,
	year = {2020},
	pages = {222--235},
}

@inproceedings{van2019data,
	title = {Data, data, everywhere: quantifying software developers’ privacy attitudes},
	booktitle = {Int. {Workshop} on socio-technical aspects in security},
	author = {van der Linden, Dirk and Hadar, Irit and Edwards, Matthew and Rashid, Awais},
	year = {2019},
}

@article{tahaeiPrivacyChampionsSoftware2021,
	title = {Privacy {Champions} in {Software} {Teams}: {Understanding} {Their} {Motivations}, {Strategies}, and {Challenges}},
	abstract = {Software development teams are responsible for making and implementing software design decisions that directly impact end-user privacy, a challenging task to do well. Privacy Champions—people who strongly care about advocating privacy—play a useful role in supporting privacy-respecting development cultures. To understand their motivations, challenges, and strategies for protecting end-user privacy, we conducted 12 interviews with Privacy Champions in software development teams. We find that common barriers to implementing privacy in software design include: negative privacy culture, internal prioritisation tensions, limited tool support, unclear evaluation metrics, and technical complexity. To promote privacy, Privacy Champions regularly use informal discussions, management support, communication among stakeholders, and documentation and guidelines. They perceive code reviews and practical training as more instructive than general privacy awareness and on-boarding training. Our study is a first step towards understanding how Privacy Champions work to improve their organisation’s privacy approaches and improve the privacy of enduser products.},
	language = {en},
	author = {Tahaei, Mohammad and Frik, Alisa and Vaniea, Kami},
	year = {2021},
	pages = {16},
}

@inproceedings{razaghpanahAppsTrackersPrivacy2018,
	address = {San Diego, CA},
	title = {Apps, {Trackers}, {Privacy}, and {Regulators}: {A} {Global} {Study} of the {Mobile} {Tracking} {Ecosystem}},
	isbn = {978-1-891562-49-5},
	shorttitle = {Apps, {Trackers}, {Privacy}, and {Regulators}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-3_Razaghpanah_paper.pdf},
	doi = {10.14722/ndss.2018.23353},
	abstract = {Third-party services form an integral part of the mobile ecosystem: they ease application development and enable features such as analytics, social network integration, and app monetization through ads. However, aided by the general opacity of mobile systems, such services are also largely invisible to users. This has negative consequences for user privacy as third-party services can potentially track users without their consent, even across multiple applications. Using real-world mobile trafﬁc data gathered by the Lumen Privacy Monitor (Lumen), a privacyenhancing app with the ability to analyze network trafﬁc on mobile devices in user space, we present insights into the mobile advertising and tracking ecosystem and its stakeholders. In this study, we develop automated methods to detect third-party advertising and tracking services at the trafﬁc level. Using this technique we identify 2,121 such services, of which 233 were previously unknown to other popular advertising and tracking blacklists. We then uncover the business relationships between the providers of these services and characterize them by their prevalence in the mobile and Web ecosystem. Our analysis of the privacy policies of the largest advertising and tracking service providers shows that sharing harvested data with subsidiaries and third-party afﬁliates is the norm. Finally, we seek to identify the services likely to be most impacted by privacy regulations such as the European General Data Protection Regulation (GDPR) and ePrivacy directives.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Razaghpanah, Abbas and Nithyanand, Rishab and Vallina-Rodriguez, Narseo and Sundaresan, Srikanth and Allman, Mark and Kreibich, Christian and Gill, Phillipa},
	year = {2018},
}

@inproceedings{krebsTellMeWhat2019,
	address = {Glasgow Scotland Uk},
	title = {Tell {Me} {What} {You} {Know}: {GDPR} {Implications} on {Designing} {Transparency} and {Accountability} for {News} {Recommender} {Systems}},
	isbn = {978-1-4503-5971-9},
	shorttitle = {Tell {Me} {What} {You} {Know}},
	url = {https://dl.acm.org/doi/10.1145/3290607.3312808},
	doi = {10.1145/3290607.3312808},
	abstract = {The GDPR has a significant impact on the way users interact with technologies, especially the everyday platforms used to personalize news and related forms of information. This paper presents the initial results from a study whose primary objective is to empirically test those platforms’ level of compliance with the so-called ‘right to explanation’. Four research topics considered as gaps in existing legal and HCI scholarship originated from the project’s initial phase, namely (1) GDPR compliance through user-centered design; (2) the inclusion of values in the system; (3) design considerations regarding interaction strategies, algorithmic experience, transparency, and explanations; and (4) technical challenges. The second phase is currently ongoing and allows us to make some observations regarding the registration process and the privacy policies of three categories of news actors: first-party content providers, news aggregators and social media platforms.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Krebs, Luciana Monteiro and Alvarado Rodriguez, Oscar Luis and Dewitte, Pierre and Ausloos, Jef and Geerts, David and Naudts, Laurens and Verbert, Katrien},
	month = may,
	year = {2019},
	pages = {1--6},
}

@article{liHowDevelopersTalk2021,
	title = {How {Developers} {Talk} {About} {Personal} {Data} and {What} {It} {Means} for {User} {Privacy}: {A} {Case} {Study} of a {Developer} {Forum} on {Reddit}},
	volume = {4},
	issn = {2573-0142},
	shorttitle = {How {Developers} {Talk} {About} {Personal} {Data} and {What} {It} {Means} for {User} {Privacy}},
	url = {https://dl.acm.org/doi/10.1145/3432919},
	doi = {10.1145/3432919},
	abstract = {While online developer forums are major resources of knowledge for application developers, their roles in promoting better privacy practices remain underexplored. In this paper, we conducted a qualitative analysis of a sample of 207 threads (4772 unique posts) mentioning different forms of personal data from the /r/androiddev forum on Reddit. We started with bottom-up open coding on the sampled posts to develop a typology of discussions about personal data use and conducted follow-up analyses to understand what types of posts elicited in-depth discussions on privacy issues or mentioned risky data practices. Our results show that Android developers rarely discussed privacy concerns when talking about a specific app design or implementation problem, but often had active discussions around privacy when stimulated by certain external events representing new privacy-enhancing restrictions from the Android operating system, app store policies, or privacy laws. Developers often felt these restrictions could cause considerable cost yet fail to generate any compelling benefit for themselves. Given these results, we present a set of suggestions for Android OS and the app store to design more effective methods to enhance privacy, and for developer forums(e.g., /r/androiddev) to encourage more in-depth privacy discussions and nudge developers to think more about privacy.},
	language = {en},
	number = {CSCW3},
	urldate = {2021-03-19},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Li, Tianshi and Louie, Elizabeth and Dabbish, Laura and Hong, Jason I.},
	month = jan,
	year = {2021},
	pages = {1--28},
}

@article{stewartEmpiricalExaminationConcern2002,
	title = {An {Empirical} {Examination} of the {Concern} for {Information} {Privacy} {Instrument}},
	volume = {13},
	issn = {1047-7047, 1526-5536},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/isre.13.1.36.97},
	doi = {10.1287/isre.13.1.36.97},
	language = {en},
	number = {1},
	urldate = {2021-03-24},
	journal = {Information Systems Research},
	author = {Stewart, Kathy A. and Segars, Albert H.},
	month = mar,
	year = {2002},
	pages = {36--49},
}

@inproceedings{utzInformedConsentStudying2019,
	address = {London United Kingdom},
	title = {({Un})informed {Consent}: {Studying} {GDPR} {Consent} {Notices} in the {Field}},
	isbn = {978-1-4503-6747-9},
	shorttitle = {({Un})informed {Consent}},
	url = {https://dl.acm.org/doi/10.1145/3319535.3354212},
	doi = {10.1145/3319535.3354212},
	abstract = {Since the adoption of the General Data Protection Regulation (GDPR) in May 2018 more than 60 \% of popular websites in Europe display cookie consent notices to their visitors. This has quickly led to users becoming fatigued with privacy notifications and contributed to the rise of both browser extensions that block these banners and demands for a solution that bundles consent across multiple websites or in the browser. In this work, we identify common properties of the graphical user interface of consent notices and conduct three experiments with more than 80,000 unique users on a German website to investigate the influence of notice position, type of choice, and content framing on consent. We find that users are more likely to interact with a notice shown in the lower (left) part of the screen. Given a binary choice, more users are willing to accept tracking compared to mechanisms that require them to allow cookie use for each category or company individually. We also show that the widespread practice of nudging has a large effect on the choices users make. Our experiments show that seemingly small implementation decisions can substantially impact whether and how people interact with consent notices. Our findings demonstrate the importance for regulation to not just require consent, but also provide clear requirements or guidance for how this consent has to be obtained in order to ensure that users can make free and informed choices.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Utz, Christine and Degeling, Martin and Fahl, Sascha and Schaub, Florian and Holz, Thorsten},
	month = nov,
	year = {2019},
	pages = {973--990},
}

@article{zaeemEffectGDPRPrivacy2021,
	title = {The {Effect} of the {GDPR} on {Privacy} {Policies}: {Recent} {Progress} and {Future} {Promise}},
	volume = {12},
	issn = {2158-656X, 2158-6578},
	shorttitle = {The {Effect} of the {GDPR} on {Privacy} {Policies}},
	url = {https://dl.acm.org/doi/10.1145/3389685},
	doi = {10.1145/3389685},
	abstract = {The General Data Protection Regulation (GDPR) is considered by some to be the most important change in data privacy regulation in 20 years. Effective May 2018, the European Union GDPR privacy law applies to any organization that collects and processes the personal information of EU citizens within or outside the EU. In this work, we seek to quantify the progress the GDPR has made in improving privacy policies around the globe. We leverage our data mining tool, PrivacyCheck, to automatically compare three corpora (totaling 550) of privacy policies, pre- and post-GDPR. In addition, to evaluate the current level of compliance with the GDPR around the globe, we manually studied the policies within two corpora (450 policies). We find that the GDPR has made progress in protecting user data, but more progress is necessary—particularly in the area of giving users the right to edit and delete their information—to entirely fulfill the GDPR’s promise. We also observe that the GDPR encourages sharing user data with law enforcement, and as a result, many policies have facilitated such sharing after the GDPR. Finally, we see that when there is non-compliance with the GDPR, it is often in the form of failing to explicitly indicate compliance, which in turn speaks to an organization’s lack of transparency and disclosure regarding their processing and protection of personal information. If Personally Identifiable Information (PII) is the “currency of the Internet,” these findings mark continued alarm regarding an individual’s agency to protect and secure their PII assets.},
	language = {en},
	number = {1},
	urldate = {2021-03-19},
	journal = {ACM Transactions on Management Information Systems},
	author = {Zaeem, Razieh Nokhbeh and Barber, K. Suzanne},
	month = mar,
	year = {2021},
	pages = {1--20},
}

@article{strobel2013empathy,
	title = {Empathy and care within engineering: qualitative perspectives from engineering faculty and practicing engineers},
	volume = {5},
	number = {2},
	journal = {Engineering Studies},
	author = {Strobel, Johannes and Hess, Justin and Pan, Rui and Wachter Morris, Carrie A},
	year = {2013},
	note = {Publisher: Taylor \& Francis},
	pages = {137--159},
}

@inproceedings{levy2018importance,
	title = {The importance of empathy for analyzing privacy requirements},
	booktitle = {2018 {IEEE} 5th international workshop on evolving security \& privacy requirements engineering ({ESPRE})},
	author = {Levy, Meira and Hadar, Irit},
	year = {2018},
	note = {tex.organization: IEEE},
	pages = {9--13},
}

@incollection{ajzen1985intentions,
	title = {From intentions to actions: {A} theory of planned behavior},
	booktitle = {Action control},
	publisher = {Springer},
	author = {Ajzen, Icek},
	year = {1985},
	pages = {11--39},
}

@inproceedings{tesfayPrivacyGuideImplementationEU2018,
	address = {Tempe AZ USA},
	title = {{PrivacyGuide}: {Towards} an {Implementation} of the {EU} {GDPR} on {Internet} {Privacy} {Policy} {Evaluation}},
	isbn = {978-1-4503-5634-3},
	shorttitle = {{PrivacyGuide}},
	url = {https://dl.acm.org/doi/10.1145/3180445.3180447},
	doi = {10.1145/3180445.3180447},
	abstract = {Nowadays Internet services have dramatically changed the way people interact with each other and many of our daily activities are supported by those services. Statistical indicators show that more than half of the world’s population uses the Internet generating about 2.5 quintillion bytes of data on daily basis. While such a huge amount of data is useful in a number of fields, such as in medical and transportation systems, it also poses unprecedented threats for user’s privacy. This is aggravated by the excessive data collection and user profiling activities of service providers. Yet, regulation require service providers to inform users about their data collection and processing practices. The de facto way of informing users about these practices is through the use of privacy policies. Unfortunately, privacy policies suffer from bad readability and other complexities which make them unusable for the intended purpose. To address this issue, we introduce PrivacyGuide, a privacy policy summarization tool inspired by the European Union (EU) General Data Protection Regulation (GDPR) and based on machine learning and natural language processing techniques. Our results show that PrivacyGuide is able to classify privacy policy content into eleven privacy aspects with a weighted average accuracy of 74\% and further shed light on the associated risk level with an accuracy of 90\%.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the {Fourth} {ACM} {International} {Workshop} on {Security} and {Privacy} {Analytics}},
	publisher = {ACM},
	author = {Tesfay, Welderufael B. and Hofmann, Peter and Nakamura, Toru and Kiyomoto, Shinsaku and Serna, Jetzabel},
	month = mar,
	year = {2018},
	pages = {15--21},
}

@inproceedings{nokhbehzaeemPrivacyCheckV2Tool2020,
	address = {Virtual Event Ireland},
	title = {{PrivacyCheck} v2: {A} {Tool} that {Recaps} {Privacy} {Policies} for {You}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {{PrivacyCheck} v2},
	url = {https://dl.acm.org/doi/10.1145/3340531.3417469},
	doi = {10.1145/3340531.3417469},
	abstract = {Despite the efforts to regulate privacy policies to protect user privacy, these policies remain lengthy and hard to comprehend. Powered by machine learning, our publicly available browser extension, PrivacyCheck v2, automatically summarizes any privacy policy by answering 20 questions based upon User Control and the General Data Protection Regulation. Furthermore, PrivacyCheck v2 incorporates a competitor analysis tool that highlights the top competitors with the best privacy policies in the same market sector. PrivacyCheck v2 enhances the users’ understanding of privacy policies and empowers them to make informed decisions when it comes to selecting services with better privacy policies.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Nokhbeh Zaeem, Razieh and Anya, Safa and Issa, Alex and Nimergood, Jake and Rogers, Isabelle and Shah, Vinay and Srivastava, Ayush and Barber, K. Suzanne},
	month = oct,
	year = {2020},
	pages = {3441--3444},
}

@incollection{voigtPracticalImplementationRequirements2017,
	title = {Practical implementation of the requirements under the {GDPR}},
	booktitle = {The {EU} {General} {Data} {Protection} {Regulation} ({GDPR})},
	publisher = {Springer},
	author = {Voigt, Paul and von dem Bussche, Axel},
	year = {2017},
	pages = {245--249},
}

@inproceedings{sirurAreWeThere2018,
	address = {Toronto Canada},
	title = {Are {We} {There} {Yet}?: {Understanding} the {Challenges} {Faced} in {Complying} with the {General} {Data} {Protection} {Regulation} ({GDPR})},
	isbn = {978-1-4503-5988-7},
	shorttitle = {Are {We} {There} {Yet}?},
	url = {https://dl.acm.org/doi/10.1145/3267357.3267368},
	doi = {10.1145/3267357.3267368},
	abstract = {The EU General Data Protection Regulation (GDPR), enforced from 25th May 2018, aims to reform how organisations view and control the personal data of private EU citizens. The scope of GDPR is somewhat unprecedented: it regulates every aspect of personal data handling, includes hefty potential penalties for non-compliance, and can prosecute any company in the world that processes EU citizens’ data. In this paper, we look behind the scenes to investigate the real challenges faced by organisations in engaging with the GDPR. This considers issues in working with the regulation, the implementation process, and how compliance is verified. Our research approach relies on literature but, more importantly, draws on detailed interviews with several organisations. Key findings include the fact that large organisations generally found GDPR compliance to be reasonable and doable. The same was found for small-to-medium organisations (SMEs/SMBs) that were highly security-oriented. SMEs with less focus on data protection struggled to make what they felt was a satisfactory attempt at compliance. The main issues faced in their compliance attempts emerged from: the sheer breadth of the regulation; questions around how to enact the qualitative recommendations of the regulation; and the need to map out the entirety of their complex data networks.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Multimedia} {Privacy} and {Security}},
	publisher = {ACM},
	author = {Sirur, Sean and Nurse, Jason R.C. and Webb, Helena},
	month = jan,
	year = {2018},
	pages = {88--95},
}

@inproceedings{bartoliniGDPRBusinessProcesses2019,
	address = {Las Palmas de Gran Canaria, Spain},
	title = {{GDPR} and business processes: an effective solution},
	isbn = {978-1-4503-6085-2},
	shorttitle = {{GDPR} and business processes},
	url = {http://dl.acm.org/citation.cfm?doid=3309772.3309779},
	doi = {10.1145/3309772.3309779},
	abstract = {In the European Union, the recent update to data protection laws by virtue of the General Data Protection Regulation (GDPR) significantly changed the landscape of the processing of personal data. Consequently, adequate solutions to ensure that the controller and processor properly understand and meet the data protection requirements are needed. In enterprise reality it is quite common to use Business Process (BP) models to manage the different business activities. Hence the idea of integrating privacy concepts into BP models so as to leverage them to the role of GDPR recommenders. To this end, suggestions and recommendations about data management pursuant to GDPR provisions have been added to specific tasks of the BP, to improve both the process management and personnel learning and training. Feasibility of the proposed idea, implemented into an Eclipse plugin, has been provided through a realistic example.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Applications} of {Intelligent} {Systems}  - {APPIS} '19},
	publisher = {ACM Press},
	author = {Bartolini, Cesare and Calabró, Antonello and Marchetti, Eda},
	year = {2019},
	pages = {1--5},
}

@inproceedings{capodieciBusinessProcessAwareness2019,
	address = {Cairo Egypt},
	title = {Business process awareness to support {GDPR} compliance},
	isbn = {978-1-4503-6292-4},
	url = {https://dl.acm.org/doi/10.1145/3361570.3361573},
	doi = {10.1145/3361570.3361573},
	abstract = {This paper proposes a model-driven approach based on business process awareness to support compliance with the European General Data Protection Regulation (GDPR, EU 2016/679). GDPR concerns the processing of personal data and the free movement of such data, and its main purpose is to safeguard the human dignity and fundamental rights of the data subject. To achieve this goal, it is necessary to identify the motivation for data management, to define who has access to data, and to determine with high precision how, when and how many times the organisation should store and manage these data. GDPR requires the self-assessment of digital risk on the basis of an impact assessment analysis. The adoption of GDPR by an organisation raises the main question of how to audit the organisation’s adherence. Starting from BPMN, which can allow businesses to better understand their internal business procedures, we propose an approach that helps to identify the most important key point(s) for GDPR compliance. To analyse the potential applicability of our thesis, we describe a vacation request scenario to which we apply the proposed approach.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Information} {Systems} and {Technologies}},
	publisher = {ACM},
	author = {Capodieci, Antonio and Mainetti, Luca},
	month = mar,
	year = {2019},
	pages = {1--6},
}

@inproceedings{manginiEmpiricalStudyImpact2020,
	address = {Virtual Event Ireland},
	title = {An empirical study on the impact of {GDPR} and right to be forgotten - organisations and users perspective},
	isbn = {978-1-4503-8833-7},
	url = {https://dl.acm.org/doi/10.1145/3407023.3407080},
	doi = {10.1145/3407023.3407080},
	abstract = {The General Data Protection Regulation (GDPR) is a prescriptive legislation in the European Union (EU) for privacy and data protection that applies to every organisation within the EU and any organisation outside the EU if they offer goods or services to EU citizens. The enforcement of GDPR created a big challenge for organisations which were required to create new professional figures, system, policies, procedures and standards, budget for new investments, and to set up a project plan or catalogue specific to the GDPR. This paper focuses on the GDPR ‘right to be forgotten’ and the specific implementation challenges it poses. The research study used two surveys to collect data from both organisations and users. The results show that while organisations are struggling with GDPR and right to be forgotten, there are also positive aspects about its implementation that translate into improved data privacy. The findings related to the users show that they are in general happy with the legislation.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {ACM},
	author = {Mangini, Vincenzo and Tal, Irina and Moldovan, Arghir-Nicolae},
	month = aug,
	year = {2020},
	pages = {1--9},
}

@article{chungCONANCOunterNArratives2019,
	title = {{CONAN} -- {COunter} {NArratives} through {Nichesourcing}: a {Multilingual} {Dataset} of {Responses} to {Fight} {Online} {Hate} {Speech}},
	shorttitle = {{CONAN} -- {COunter} {NArratives} through {Nichesourcing}},
	url = {http://arxiv.org/abs/1910.03270},
	doi = {10.18653/v1/P19-1271},
	abstract = {Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the ﬁrst large-scale, multilingual, expert-based dataset of hate speech/counternarrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data.},
	language = {en},
	urldate = {2021-04-02},
	journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Chung, Y. L. and Kuzmenko, E. and Tekiroglu, S. S. and Guerini, M.},
	year = {2019},
	note = {arXiv: 1910.03270},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	pages = {2819--2829},
}

@article{vidgenDirectionsAbusiveLanguage2020,
	title = {Directions in {Abusive} {Language} {Training} {Data}: {Garbage} {In}, {Garbage} {Out}},
	volume = {15},
	issn = {1932-6203},
	shorttitle = {Directions in {Abusive} {Language} {Training} {Data}},
	url = {http://arxiv.org/abs/2004.01670},
	doi = {10.1371/journal.pone.0243300},
	abstract = {Data-driven analysis and detection of abusive online content covers many different tasks, phenomena, contexts, and methodologies. This paper systematically reviews abusive language dataset creation and content in conjunction with an open website for cataloguing abusive language data. This collection of knowledge leads to a synthesis providing evidence-based recommendations for practitioners working with this complex and highly diverse data.},
	language = {en},
	number = {12},
	urldate = {2021-04-02},
	journal = {PLOS ONE},
	author = {Vidgen, Bertie and Derczynski, Leon},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.01670},
	keywords = {Computer Science - Computation and Language},
	pages = {e0243300},
}

@inproceedings{rezvanQualityTypeawareAnnotated2018,
	address = {Amsterdam Netherlands},
	title = {A {Quality} {Type}-aware {Annotated} {Corpus} and {Lexicon} for {Harassment} {Research}},
	isbn = {978-1-4503-5563-6},
	url = {https://dl.acm.org/doi/10.1145/3201064.3201103},
	doi = {10.1145/3201064.3201103},
	abstract = {A quality annotated corpus is essential to research. Despite the recent focus of the Web science community on cyberbullying research, the community lacks standard benchmarks. This paper provides both a quality annotated corpus and an offensive words lexicon capturing different types of harassment content: (i) sexual, (ii) racial, (iii) appearance-related, (iv) intellectual, and (v) political1. We first crawled data from Twitter using this content-tailored offensive lexicon. As mere presence of an offensive word is not a reliable indicator of harassment, human judges annotated tweets for the presence of harassment. Our corpus consists of 25,000 annotated tweets for the five types of harassment content and is available on the Git repository2.},
	language = {en},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Web} {Science}},
	publisher = {ACM},
	author = {Rezvan, Mohammadreza and Shekarpour, Saeedeh and Balasuriya, Lakshika and Thirunarayan, Krishnaprasad and Shalin, Valerie L. and Sheth, Amit},
	month = may,
	year = {2018},
	pages = {33--36},
}

@article{18956GoogleLLC2021,
	title = {18-956 {Google} {LLC} v. {Oracle} {America}, {Inc}. (04/05/2021)},
	language = {en},
	year = {2021},
	pages = {62},
}

@article{jurgensJustComprehensiveStrategy2019,
	title = {A {Just} and {Comprehensive} {Strategy} for {Using} {NLP} to {Address} {Online} {Abuse}},
	url = {http://arxiv.org/abs/1906.01738},
	abstract = {Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow deﬁnition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.},
	language = {en},
	urldate = {2021-04-02},
	journal = {arXiv:1906.01738 [cs]},
	author = {Jurgens, David and Chandrasekharan, Eshwar and Hemphill, Libby},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01738},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@article{polettoResourcesBenchmarkCorpora2020,
	title = {Resources and benchmark corpora for hate speech detection: a systematic review},
	issn = {1574-020X, 1574-0218},
	shorttitle = {Resources and benchmark corpora for hate speech detection},
	url = {http://link.springer.com/10.1007/s10579-020-09502-8},
	doi = {10.1007/s10579-020-09502-8},
	abstract = {Hate Speech in social media is a complex phenomenon, whose detection has recently gained signiﬁcant traction in the Natural Language Processing community, as attested by several recent review works. Annotated corpora and benchmarks are key resources, considering the vast number of supervised approaches that have been proposed. Lexica play an important role as well for the development of hate speech detection systems. In this review, we systematically analyze the resources made available by the community at large, including their development methodology, topical focus, language coverage, and other factors. The results of our analysis highlight a heterogeneous, growing landscape, marked by several issues and venues for improvement.},
	language = {en},
	urldate = {2021-04-02},
	journal = {Language Resources and Evaluation},
	author = {Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
	month = sep,
	year = {2020},
}

@article{dadvarCyberbullyingDetectionSocial,
	title = {Cyberbullying {Detection} in {Social} {Networks} {Using} {Deep} {Learning} {Based} {Models}; {A} {Reproducibility} {Study}},
	abstract = {Cyberbullying is a disturbing online misbehaviour with troubling consequences. It appears in different forms, and in most of the social networks, it is in textual format. Automatic detection of such incidents requires intelligent systems. Most of the existing studies have approached this problem with conventional machine learning models and the majority of the developed models in these studies are adaptable to a single social network at a time. In recent studies, deep learning based models have found their way in the detection of cyberbullying incidents, claiming that they can overcome the limitations of the conventional models, and improve the detection performance. In this paper, we investigate the findings of a recent literature in this regard. We successfully reproduced the findings of this literature and validated their findings using the same datasets, namely Wikipedia, Twitter, and Formspring, used by the authors. Then we expanded our work by applying the developed methods on a new YouTube dataset ({\textasciitilde}54k posts by {\textasciitilde}4k users) and investigated the performance of the models in new social media platforms. We also transferred and evaluated the performance of the models trained on one platform to another platform. Our findings show that the deep learning based models outperform the machine learning models previously applied to the same YouTube dataset. We believe that the deep learning based models can also benefit from integrating other sources of information and looking into the impact of profile information of the users in social networks.},
	language = {en},
	author = {Dadvar, Maral and Eckert, Kai},
	pages = {13},
}

@article{emmeryCurrentLimitationsCyberbullying2019,
	title = {Current {Limitations} in {Cyberbullying} {Detection}: on {Evaluation} {Criteria}, {Reproducibility}, and {Data} {Scarcity}},
	shorttitle = {Current {Limitations} in {Cyberbullying} {Detection}},
	url = {http://arxiv.org/abs/1910.11922},
	abstract = {The detection of online cyberbullying has seen an increase in societal importance, popularity in research, and available open data. Nevertheless, while computational power and affordability of resources continue to increase, the access restrictions on high-quality data limit the applicability of state-of-the-art techniques. Consequently, much of the recent research uses small, heterogeneous datasets, without a thorough evaluation of applicability. In this paper, we further illustrate these issues, as we (i) evaluate many publicly available resources for this task and demonstrate difﬁculties with data collection. These predominantly yield small datasets that fail to capture the required complex social dynamics and impede direct comparison of progress. We (ii) conduct an extensive set of experiments that indicate a general lack of cross-domain generalization of classiﬁers trained on these sources, and openly provide this framework to replicate and extend our evaluation criteria. Finally, we (iii) present an effective crowdsourcing method: simulating real-life bullying scenarios in a lab setting generates plausible data that can be effectively used to enrich real data. This largely circumvents the restrictions on data that can be collected, and increases classiﬁer performance. We believe these contributions can aid in improving the empirical practices of future research in the ﬁeld.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:1910.11922 [cs]},
	author = {Emmery, Chris and Verhoeven, Ben and De Pauw, Guy and Jacobs, Gilles and Van Hee, Cynthia and Lefever, Els and Desmet, Bart and Hoste, Véronique and Daelemans, Walter},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.11922},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@article{qianBenchmarkDatasetLearning2019,
	title = {A {Benchmark} {Dataset} for {Learning} to {Intervene} in {Online} {Hate} {Speech}},
	url = {http://arxiv.org/abs/1909.04251},
	abstract = {Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets1 collected from Gab2 and Reddit3. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk4 Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:1909.04251 [cs]},
	author = {Qian, Jing and Bethke, Anna and Liu, Yinyin and Belding, Elizabeth and Wang, William Yang},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{fehnunsvagEffectsUserFeatures2018,
	address = {Brussels, Belgium},
	title = {The {Effects} of {User} {Features} on {Twitter} {Hate} {Speech} {Detection}},
	url = {http://aclweb.org/anthology/W18-5110},
	doi = {10.18653/v1/W18-5110},
	abstract = {The paper investigates the potential effects user features have on hate speech classiﬁcation. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classiﬁer based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classiﬁcation performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the 2nd {Workshop} on {Abusive} {Language} {Online} ({ALW2})},
	publisher = {Association for Computational Linguistics},
	author = {Fehn Unsvåg, Elise and Gambäck, Björn},
	year = {2018},
	pages = {75--85},
}

@inproceedings{kennedyTechnologySolutionsCombat2017,
	address = {Vancouver, BC, Canada},
	title = {Technology {Solutions} to {Combat} {Online} {Harassment}},
	url = {http://aclweb.org/anthology/W17-3011},
	doi = {10.18653/v1/W17-3011},
	abstract = {This work is part of a new initiative to use machine learning to identify online harassment in social media and comment streams. Online harassment goes underreported due to the reliance on humans to identify and report harassment, reporting that is further slowed by requirements to ﬁll out forms providing context. In addition, the time for moderators to respond and apply human judgment can take days, but response times in terms of minutes are needed in the online context. Though some of the major social media companies have been doing proprietary work in automating the detection of harassment, there are few tools available for use by the public. In addition, the amount of labeled online harassment data and availability of cross platform online harassment datasets is limited. We present the methodology used to create a harassment dataset and classiﬁer and the dataset used to help the system learn what harassment looks like.},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the {First} {Workshop} on {Abusive} {Language} {Online}},
	publisher = {Association for Computational Linguistics},
	author = {Kennedy, George and McCollough, Andrew and Dixon, Edward and Bastidas, Alexei and Ryan, John and Loo, Chris and Sahay, Saurav},
	year = {2017},
	pages = {73--77},
}

@inproceedings{golbeckLargeLabeledCorpus2017,
	address = {Troy New York USA},
	title = {A {Large} {Labeled} {Corpus} for {Online} {Harassment} {Research}},
	isbn = {978-1-4503-4896-6},
	url = {https://dl.acm.org/doi/10.1145/3091478.3091509},
	doi = {10.1145/3091478.3091509},
	abstract = {A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, handcoded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15\% positive harassment examples and 85\% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Proceedings of the 2017 {ACM} on {Web} {Science} {Conference}},
	publisher = {ACM},
	author = {Golbeck, Jennifer and Ashktorab, Zahra and Banjo, Rashad O. and Berlinger, Alexandra and Bhagwan, Siddharth and Buntain, Cody and Cheakalos, Paul and Geller, Alicia A. and Gergory, Quint and Gnanasekaran, Rajesh Kumar and Gunasekaran, Raja Rajan and Hoffman, Kelly M. and Hottle, Jenny and Jienjitlert, Vichita and Khare, Shivika and Lau, Ryan and Martindale, Marianna J. and Naik, Shalmali and Nixon, Heather L. and Ramachandran, Piyush and Rogers, Kristine M. and Rogers, Lisa and Sarin, Meghna Sardana and Shahane, Gaurav and Thanki, Jayanee and Vengataraman, Priyanka and Wan, Zijian and Wu, Derek Michael},
	month = jun,
	year = {2017},
	pages = {229--233},
}

@article{leeComparativeStudiesDetecting2018,
	title = {Comparative {Studies} of {Detecting} {Abusive} {Language} on {Twitter}},
	url = {http://arxiv.org/abs/1808.10245},
	abstract = {The context-dependent nature of online aggression makes annotating large collections of data extremely difﬁcult. Previously studied datasets in abusive language detection have been insufﬁcient in size to efﬁciently train deep learning models. Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential. In this paper, we conduct the ﬁrst comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements. Experimental results show that bidirectional GRU networks trained on word-level features, with Latent Topic Clustering modules, is the most accurate model scoring 0.805 F1.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:1808.10245 [cs]},
	author = {Lee, Younghun and Yoon, Seunghyun and Jung, Kyomin},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10245},
	keywords = {Computer Science - Computation and Language},
}

@article{kielaHatefulMemesChallenge2020,
	title = {The {Hateful} {Memes} {Challenge}: {Detecting} {Hate} {Speech} in {Multimodal} {Memes}},
	shorttitle = {The {Hateful} {Memes} {Challenge}},
	url = {http://arxiv.org/abs/2005.04790},
	abstract = {This work proposes a new challenge set for multimodal classiﬁcation, focusing on detecting hate speech in multimodal memes. It is constructed such that unimodal models struggle and only multimodal models can succeed: difﬁcult examples (“benign confounders”) are added to the dataset to make it hard to rely on unimodal signals. The task requires subtle reasoning, yet is straightforward to evaluate as a binary classiﬁcation problem. We provide baseline performance numbers for unimodal models, as well as for multimodal models with various degrees of sophistication. We ﬁnd that state-of-the-art methods perform poorly compared to humans (64.73\% vs. 84.7\% accuracy), illustrating the difﬁculty of the task and highlighting the challenge that this important problem poses to the community.},
	language = {en},
	urldate = {2021-04-01},
	journal = {arXiv:2005.04790 [cs]},
	author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.04790},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{benghabrit2015abstract,
	title = {Abstract accountability language: {Translation}, compliance and application},
	booktitle = {2015 asia-pacific software engineering conference ({APSEC})},
	author = {Benghabrit, Walid and Grall, Hervé and Royer, Jean-Claude and Sellami, Mohamed},
	year = {2015},
	note = {tex.organization: IEEE},
	pages = {214--221},
}

@inproceedings{baratiDevelopingGDPRCompliant2019,
	address = {Auckland New Zealand},
	title = {Developing {GDPR} {Compliant} {User} {Data} {Policies} for {Internet} of {Things}},
	isbn = {978-1-4503-6894-0},
	url = {https://dl.acm.org/doi/10.1145/3344341.3368812},
	doi = {10.1145/3344341.3368812},
	abstract = {With recent adoption of Internet of Things (IoT) technologies and their use in industry, user data privacy concerns remain a major preoccupation of regulation bodies. The European General Data Protection Regulation (GDPR) enables users to control their data and be informed about any devices involved in collecting and processing this data. The overall objective is to enable individuals to have full rights and control over their data assets and to be able to transfer their data without any unmitigated risk. Blockchains provide the benefits of a distributed ledger that can securely manage digital transactions – where the centralisation of data is eliminated. Blockchains have recently entered as an enabling technology into the IoT market, and used in a variety of different application areas. Blockchains enable the implementation of a more trusted system capable of processing operations between IoT services and sources of data. In smart buildings, for example, Blockchains support the formation of smart contracts as a means to give transactional capabilities to IoT devices, allowing users to keep data ownership and privacy using an immutable dataset. We describe how Blockchain technology can be used to develop an audit trail of data generated in IoT devices, enabling GDPR rules to be verified on such a trail. We describe how to translate a set of such rules into smart contracts to protect personal data in a transparent and automatic way.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 12th {IEEE}/{ACM} {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {ACM},
	author = {Barati, Masoud and Petri, Ioan and Rana, Omer F.},
	month = dec,
	year = {2019},
	pages = {133--141},
}

@article{bastidasHarassmentDetectionBenchmark,
	title = {Harassment detection: a benchmark on the \#{HackHarassment} dataset},
	language = {en},
	author = {Bastidas, Alexei and Dixon, Edward and Loo, Chris and Ryan, John},
	pages = {3},
}

@article{rivest1978method,
	title = {A method for obtaining digital signatures and public-key cryptosystems},
	volume = {21},
	number = {2},
	journal = {Communications of the ACM},
	author = {Rivest, Ronald L and Shamir, Adi and Adleman, Leonard},
	year = {1978},
	note = {Publisher: ACM New York, NY, USA},
	pages = {120--126},
}

@inproceedings{dwork2008differential,
	title = {Differential privacy: {A} survey of results},
	booktitle = {International conference on theory and applications of models of computation},
	author = {Dwork, Cynthia},
	year = {2008},
	note = {tex.organization: Springer},
	pages = {1--19},
}

@inproceedings{fanEmpiricalEvaluationGDPR2020,
	address = {Coimbra, Portugal},
	title = {An {Empirical} {Evaluation} of {GDPR} {Compliance} {Violations} in {Android} {mHealth} {Apps}},
	isbn = {978-1-72819-870-5},
	url = {https://ieeexplore.ieee.org/document/9251060/},
	doi = {10.1109/ISSRE5003.2020.00032},
	abstract = {The purpose of the General Data Protection Regulation (GDPR) is to provide improved privacy protection. If an app controls personal data from users, it needs to be compliant with GDPR. However, GDPR lists general rules rather than exact step-by-step guidelines about how to develop an app that fulﬁlls the requirements. Therefore, there may exist GDPR compliance violations in existing apps, which would pose severe privacy threats to app users. In this paper, we take mobile health applications (mHealth apps) as a peephole to examine the status quo of GDPR compliance in Android apps. We ﬁrst propose an automated system, named HPDROID, to bridge the semantic gap between the general rules of GDPR and the app implementations by identifying the data practices declared in the app privacy policy and the data relevant behaviors in the app code. Then, based on HPDROID, we detect three kinds of GDPR compliance violations, including the incompleteness of privacy policy, the inconsistency of data collections, and the insecurity of data transmission. We perform an empirical evaluation of 796 mHealth apps. The results reveal that 189 (23.7\%) of them do not provide complete privacy policies. Moreover, 59 apps collect sensitive data through different measures, but 46 (77.9\%) of them contain at least one inconsistent collection behavior. Even worse, among the 59 apps, only 8 apps try to ensure the transmission security of collected data. However, all of them contain at least one encryption or SSL misuse. Our work exposes severe privacy issues to raise awareness of privacy protection for app users and developers.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {2020 {IEEE} 31st {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Fan, Ming and Yu, Le and Chen, Sen and Zhou, Hao and Luo, Xiapu and Li, Shuyue and Liu, Yang and Liu, Jun and Liu, Ting},
	month = oct,
	year = {2020},
	pages = {253--264},
}

@inproceedings{macgahanProvableEnforcementHIPAAcompliant2017,
	address = {Indianapolis Indiana USA},
	title = {Provable enforcement of {HIPAA}-compliant release of medical records using the history aware programming language},
	isbn = {978-1-4503-4702-0},
	url = {https://dl.acm.org/doi/10.1145/3078861.3084176},
	doi = {10.1145/3078861.3084176},
	abstract = {Dependence on reliable information systems to safeguard personally identi able information implies a need for privacy policies which guide the release and management of such information, whose mismanaged disclosure can be damaging to both the subject and the organization that releases it. Enforcing such policies requires a ention to detail and care, and thus any aid that a compiler can render may be of value. We present a demonstration of compiler enforcement of privacy policy by implementation of the History Aware Programming Language (HAPL) framework. is framework allows expression of arbitrary HAPL code for actors in an actor system to be used to back a web application. is code is then checked for compliance with privacy policies described in assume-guarantee form before being assembled into a functioning application. e framework is demonstrated by implementing ve use cases based on scenarios described in the Health Insurance Portability and Accountability Act (HIPAA), and the performance of the framework is tested.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 22nd {ACM} on {Symposium} on {Access} {Control} {Models} and {Technologies}},
	publisher = {ACM},
	author = {MacGahan, Thomas and Johnson, Claiborne and Rodriguez, Armando and von Ronne, Jeffery and Niu, Jianwei},
	month = jun,
	year = {2017},
	pages = {191--198},
}

@inproceedings{benthallSituatedInformationFlow2019,
	address = {Nashville, Tennessee},
	title = {Situated information flow theory},
	isbn = {978-1-4503-7147-6},
	url = {http://dl.acm.org/citation.cfm?doid=3314058.3314066},
	doi = {10.1145/3314058.3314066},
	abstract = {A key component of recent privacy rules is restriction on the flows of personal information or data based on information categories. This tendency conflicts with the fact that data’s meaning is not stable but depends on how it was formed and with what other information it is combined. These properties of information challenge naive intuitions that information ‘flows’ like a fluid, such as water or oil. Rather, we build on on Dretske, Pearl, and Nissenbaum to develop situated information flow theory (SIFT): a view of information flows as causal flows with nomic associations due to a larger context of causal relations. The semantics of situated information flow are precise within the statistical framework of Bayesian networks. We argue this understanding of information flow has three policy implications. (1) Restrictions on data transfers are more precise and enforceable than restrictions on information flow. (2) Information ‘categories’ or meanings must be defined relative to a particular class of observers and take into account their reasonable background information. (3) The semantics of data are ambiguous when there is uncertainty about causal structure, and this structure is learned from data aggregation. Hence, the information asymmetry between data aggregators and individual data subjects are one reason why data processors are ‘opaque’ and difficult to regulate.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 6th {Annual} {Symposium} on {Hot} {Topics} in the {Science} of {Security} - {HotSoS} '19},
	publisher = {ACM Press},
	author = {Benthall, Sebastian},
	year = {2019},
	pages = {1--10},
}

@article{akanfeDesignInclusiveFinancial2021,
	title = {Design of an {Inclusive} {Financial} {Privacy} {Index} ({INF}-{PIE}): {A} {Financial} {Privacy} and {Digital} {Financial} {Inclusion} {Perspective}},
	volume = {12},
	issn = {2158-656X, 2158-6578},
	shorttitle = {Design of an {Inclusive} {Financial} {Privacy} {Index} ({INF}-{PIE})},
	url = {https://dl.acm.org/doi/10.1145/3403949},
	doi = {10.1145/3403949},
	abstract = {Financial privacy is an important part of an individual's privacy, but efforts to enhance financial privacy have often not been given enough prominence by some countries when advancing financial inclusion. This impedes under-served communities from utilizing financial services. This article adopts a design science approach to create an INclusive Financial Privacy IndEx (INF-PIE) from the two perspectives of financial privacy and digital financial inclusion to help ensure financial services for a wide range of populations. This article first examines the privacy policies of Mobile Wallet and Remittance (MWR) apps (a digital financial solution), uses an analytics approach for extracting semi-structured information components; and based on text categorization and topic modeling, creates privacy policy compliance scores. In particular, it analyses the privacy policies using natural language processing techniques such as Term Frequency-Inverse Document Frequency (tf-idf) and Latent Dirichlet Allocation (LDA). This article then develops a digital financial inclusion score through a multivariate analysis of indexes extracted from the global findex dataset using Principal Component Analysis (PCA). Finally, the INF-PIE framework is established to analyze various countries and assess their financial privacy and digital financial inclusion practices. This framework can show how countries’ relative data privacy compliance and digital financial inclusion practices underscore their inclusive financial privacy.},
	language = {en},
	number = {1},
	urldate = {2021-03-19},
	journal = {ACM Transactions on Management Information Systems},
	author = {Akanfe, Oluwafemi and Valecha, Rohit and Rao, H. Raghav},
	month = mar,
	year = {2021},
	pages = {1--21},
}

@article{murrillModesConstitutionalInterpretation,
	title = {Modes of {Constitutional} {Interpretation}},
	abstract = {When exercising its power to review the constitutionality of governmental action, the Supreme Court has relied on certain “methods” or “modes” of interpretation—that is, ways of figuring out a particular meaning of a provision within the Constitution. This report broadly describes the most common modes of constitutional interpretation; discusses examples of Supreme Court decisions that demonstrate the application of these methods; and provides a general overview of the various arguments in support of, and in opposition to, the use of such methods of constitutional interpretation.},
	language = {en},
	author = {Murrill, Brandon J},
	pages = {28},
}

@inproceedings{deyoungExperiencesLogicalSpecification2010,
	address = {Chicago, Illinois, USA},
	title = {Experiences in the logical specification of the {HIPAA} and {GLBA} privacy laws},
	isbn = {978-1-4503-0096-4},
	url = {http://portal.acm.org/citation.cfm?doid=1866919.1866930},
	doi = {10.1145/1866919.1866930},
	abstract = {Despite the wide array of frameworks proposed for the formal speciﬁcation and analysis of privacy laws, there has been comparatively little work on expressing large fragments of actual privacy laws in these frameworks. We attempt to bridge this gap by giving complete logical formalizations of the transmission-related portions of the Health Insurance Portability and Accountability Act (HIPAA) and the Gramm-Leach-Bliley Act (GLBA). To this end, we develop the PrivacyLFP logic, whose features include support for disclosure purposes, real-time constructs, and self-reference via ﬁxed points. To illustrate these features and demonstrate PrivacyLFP’s utility, we present formalizations of a collection of clauses from these laws. Due to their size, our full formalizations of HIPAA and GLBA appear in a companion technical report. We discuss ambiguities in the laws that our formalizations revealed and sketch preliminary ideas for computer-assisted enforcement of such privacy policies.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 9th annual {ACM} workshop on {Privacy} in the electronic society - {WPES} '10},
	publisher = {ACM Press},
	author = {DeYoung, Henry and Garg, Deepak and Jia, Limin and Kaynar, Dilsun and Datta, Anupam},
	year = {2010},
	pages = {73},
}

@inproceedings{bennettGLOBALPrivacyProtection2018,
	address = {Singapore Singapore},
	title = {{GLOBAL} {Privacy} {Protection}: {Adequate} {Laws}, {Accountable} {Organizations} and/or {Data} {Localization}?},
	isbn = {978-1-4503-5966-5},
	shorttitle = {{GLOBAL} {Privacy} {Protection}},
	url = {https://dl.acm.org/doi/10.1145/3267305.3274149},
	doi = {10.1145/3267305.3274149},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2018 {ACM} {International} {Joint} {Conference} and 2018 {International} {Symposium} on {Pervasive} and {Ubiquitous} {Computing} and {Wearable} {Computers}},
	publisher = {ACM},
	author = {Bennett, Colin and Oduro-Marfo, Smith},
	month = oct,
	year = {2018},
	pages = {880--890},
}

@inproceedings{kangAlgorithmicAccountabilityPublic2020,
	address = {Barcelona Spain},
	title = {Algorithmic accountability in public administration: the {GDPR} paradox},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Algorithmic accountability in public administration},
	url = {https://dl.acm.org/doi/10.1145/3351095.3373153},
	doi = {10.1145/3351095.3373153},
	abstract = {The EU General Data Protection Regulation (“GDPR”) is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Kang, Sunny Seon},
	month = jan,
	year = {2020},
	pages = {32--32},
}

@inproceedings{biegaOperationalizingLegalPrinciple2020,
	address = {Virtual Event China},
	title = {Operationalizing the {Legal} {Principle} of {Data} {Minimization} for {Personalization}},
	isbn = {978-1-4503-8016-4},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401034},
	doi = {10.1145/3397271.3401034},
	abstract = {Article 5(1)(c) of the European Union’s General Data Protection Regulation (GDPR) requires that "personal data shall be [...] adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed (‘data minimisation’)". To date, the legal and computational definitions of ‘purpose limitation’ and ‘data minimization’ remain largely unclear. In particular, the interpretation of these principles is an open issue for information access systems that optimize for user experience through personalization and do not strictly require personal data collection for the delivery of basic service. In this paper, we identify a lack of a homogeneous interpretation of the data minimization principle and explore two operational definitions applicable in the context of personalization. The focus of our empirical study in the domain of recommender systems is on providing foundational insights about the (i) feasibility of different data minimization definitions, (ii) robustness of different recommendation algorithms to minimization, and (iii) performance of different minimization strategies. We find that the performance decrease incurred by data minimization might not be substantial, but that it might disparately impact different users—a finding which has implications for the viability of different formal minimization definitions. Overall, our analysis uncovers the complexities of the data minimization problem in the context of personalization and maps the remaining computational and regulatory challenges.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Biega, Asia J. and Potash, Peter and Daumé, Hal and Diaz, Fernando and Finck, Michèle},
	month = jul,
	year = {2020},
	pages = {399--408},
}

@article{kaminskiRecentRenaissancePrivacy2020,
	title = {A recent renaissance in privacy law},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3411049},
	doi = {10.1145/3411049},
	abstract = {Considering the recent increased attention to privacy law issues amid the typically slow pace of legal change.},
	language = {en},
	number = {9},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Kaminski, Margot},
	month = aug,
	year = {2020},
	pages = {24--27},
}

@inproceedings{anishEnhancedAccountabilityComplying2019,
	address = {Montreal, QC, Canada},
	title = {Towards {Enhanced} {Accountability} in {Complying} with {Healthcare} {Regulations}},
	isbn = {978-1-72812-251-9},
	url = {https://ieeexplore.ieee.org/document/8823886/},
	doi = {10.1109/SEH.2019.00012},
	abstract = {The healthcare ecosystem is highly complex. It is composed of multiple stakeholders with intersecting interests. The healthcare regulations such as Health Insurance Portability and Accountability Act, much like the systems they protect, are complex and often difficult to interpret. Regulations contain obligations that must be fulfilled by healthcare systems that form the backbone of modern healthcare. In this paper, we present a model for extracting and deconstructing obligations. The Obligation Model allows for capturing the essence of obligations in terms of their attributes such as Responsible entity, Trigger, Action, Deadline and Reference. We augment the extracted obligations with auxiliary information present in regulation documents and provide an ownership based rendering of the deconstructed obligation in an HTML format. This helps in establishing an explicit ownership of obligations and contributes towards enhancing accountability of stakeholders towards fulfilling the obligations. The rendering will be useful for building compliant healthcare systems by making the legal text more comprehensible for system designers.},
	language = {en},
	urldate = {2021-03-19},
	booktitle = {2019 {IEEE}/{ACM} 1st {International} {Workshop} on {Software} {Engineering} for {Healthcare} ({SEH})},
	publisher = {IEEE},
	author = {Anish, Preethu Rose and Joshi, Vivek and Sainani, Abhishek and Ghaisas, Smita},
	month = may,
	year = {2019},
	pages = {25--28},
}

@article{greengardWeighingImpactGDPR2018,
	title = {Weighing the impact of {GDPR}},
	volume = {61},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3276744},
	doi = {10.1145/3276744},
	abstract = {The EU data regulation will affect computer, Internet, and technology usage within and outside the EU; how it will play out remains to be seen.},
	language = {en},
	number = {11},
	urldate = {2021-03-19},
	journal = {Communications of the ACM},
	author = {Greengard, Samuel},
	month = oct,
	year = {2018},
	pages = {16--18},
}

@article{SigmaProblemSearch2021,
	title = {The 2 {Sigma} {Problem}: {The} {Search} for {Methods} of {Group} {Instruction} as {Effective} as {One}-to-{One} {Tutoring}},
	language = {en},
	year = {2021},
	pages = {14},
}

@article{lockwood2006someone,
	title = {“{Someone} like me can be successful”: {Do} college students need same-gender role models?},
	volume = {30},
	number = {1},
	journal = {Psychology of women quarterly},
	author = {Lockwood, Penelope},
	year = {2006},
	note = {Publisher: SAGE Publications Sage CA: Los Angeles, CA},
	pages = {36--46},
}

@article{dimitrov_intercultural_nodate,
	title = {Intercultural {Teaching} {Competence} in the {Disciplines}: {Teaching} {Strategies} for {Intercultural} {Learning}},
	abstract = {As universities continue to internationalize their curricula and recruit a growing number of international students, instructors facilitate learning in increasingly diverse classrooms. This chapter explores the application of Intercultural Teaching Competence (ITC) by faculty members across the disciplines at a large Canadian research university. Based on focus group interviews with instructors in eighteen disciplines, it provides varied and concrete examples of how instructors mobilize intercultural teaching competence to navigate diverse classrooms, promote perspective-taking and global learning goals among students, practice culturally relevant teaching, and validate different ways of knowing and communicating among students through assessment practices. Placing disciplines at the centre of the discussion in this way elucidates the extent to which ITC may be adapted to fit the contours of the academic field and allows readers to explore best practices for facilitating the development of intercultural competence among students in their disciplines. Finally, the implications of disciplinary differences in ITC are discussed for faculty development and curriculum support.},
	language = {en},
	author = {Dimitrov, Nanda and Haque, Aisha},
	pages = {23},
}

@article{gondra_intercultural_2018,
	title = {Intercultural {Knowledge} {Development} {During} {Short}-{Term} {Study} {Abroad} in the {Basque} {Country}: {A} {Cultural} and {Linguistic} {Minority} {Context}},
	volume = {30},
	issn = {2380-8144, 1085-4568},
	shorttitle = {Intercultural {Knowledge} {Development} {During} {Short}-{Term} {Study} {Abroad} in the {Basque} {Country}},
	url = {http://frontiersjournal.org/index.php/Frontiers/article/view/427},
	doi = {10.36366/frontiers.v30i3.427},
	abstract = {This investigation examined intercultural knowledge development in a short-term study abroad program in a cultural and linguistic minority context (Basque Country, Spain). A pre- and postprogram quantitative and qualitative design was used with 26 participants. The quantitative, surveybased results demonstrated an increase in intercultural knowledge over the study’s five-week duration. Qualitative analysis of interview data indicated that students’ intercultural knowledge aligned with Lussier’s (2007) description of knowledge about “small c” culture and included knowledge of social groups—a distinct finding from prior research. Students’ knowledge changed over the period abroad, indicating adjustment to City Life and Time and Schedule norms. Additionally, knowledge growth was greatest with respect to subthemes strongly linked to the minority context (e.g. Basque ethnicity, culture, language). This study offers detailed information about intercultural knowledge development during short-term study abroad experiences and demonstrates that minority context programs encourage development of intercultural knowledge about “small c” culture and social groups.},
	language = {en},
	number = {3},
	urldate = {2021-08-16},
	journal = {Frontiers: The Interdisciplinary Journal of Study Abroad},
	author = {Gondra, Ager and Czerwionka, Lori},
	month = nov,
	year = {2018},
	pages = {119--146},
}

@article{lehto_intercultural_2014,
	title = {Intercultural {Interactions} {Outside} the {Classroom}: {Narratives} on a {US} {Campus}},
	volume = {55},
	issn = {1543-3382},
	shorttitle = {Intercultural {Interactions} {Outside} the {Classroom}},
	url = {http://muse.jhu.edu/content/crossref/journals/journal_of_college_student_development/v055/55.8.lehto.html},
	doi = {10.1353/csd.2014.0083},
	language = {en},
	number = {8},
	urldate = {2021-08-16},
	journal = {Journal of College Student Development},
	author = {Lehto, Xinran Y. and Cai, Liping A. and Fu, Xiaoxiao and Chen, Yi},
	year = {2014},
	pages = {837--853},
}

@article{young-jones_autonomy-supportive_2021,
	title = {Autonomy-supportive language in the syllabus: supporting students from the first day},
	volume = {26},
	issn = {1356-2517, 1470-1294},
	shorttitle = {Autonomy-supportive language in the syllabus},
	url = {https://www.tandfonline.com/doi/full/10.1080/13562517.2019.1661375},
	doi = {10.1080/13562517.2019.1661375},
	abstract = {An autonomy supportive classroom enhances the learning climate and improves academic motivation. Alternatively, a controlling classroom environment constricts the learning climate and hinders academic motivation. The current study evaluated whether autonomy supportive or controlling language presented in a class syllabus inﬂuenced students’ perceptions of a college course. Students were randomly assigned to read a syllabus written with either autonomy supportive or controlling language. After reading, participants rated their perceptions of the learning climate, intrinsic motivation, satisfaction of their basic psychological needs, and intentions to take the class. Analyses revealed that students who viewed the autonomy-supportive syllabus had a more overall positive perception of the course compared to students who read the controlling syllabus. The ﬁndings suggest language within a course syllabus can inﬂuence student’s early perceptions of and intentions toward taking a course, but not their reported level of intrinsic motivation. Implications for instructors and future directions are discussed.},
	language = {en},
	number = {4},
	urldate = {2021-08-10},
	journal = {Teaching in Higher Education},
	author = {Young-Jones, Adena and Levesque, Chantal and Fursa, Sophie and McCain, Jason},
	month = may,
	year = {2021},
	pages = {541--556},
}

@article{cushner_developing_2015,
	title = {Developing intercultural competence through overseas student teaching: checking our assumptions},
	volume = {26},
	issn = {1467-5986, 1469-8439},
	shorttitle = {Developing intercultural competence through overseas student teaching},
	url = {http://www.tandfonline.com/doi/full/10.1080/14675986.2015.1040326},
	doi = {10.1080/14675986.2015.1040326},
	language = {en},
	number = {3},
	urldate = {2021-08-16},
	journal = {Intercultural Education},
	author = {Cushner, Kenneth and Chang, Shu-Ching},
	month = may,
	year = {2015},
	pages = {165--178},
}

@inproceedings{raduDatasetNonFunctionalBugs2019,
	address = {Montreal, QC, Canada},
	title = {A {Dataset} of {Non}-{Functional} {Bugs}},
	isbn = {978-1-72813-412-3},
	url = {https://ieeexplore.ieee.org/document/8816810/},
	doi = {10.1109/MSR.2019.00066},
	abstract = {While several researchers have published bug data sets in the past, there has been less focus on bugs related to nonfunctional requirements. Non-functional requirements describe the quality attributes of a program. In this work, we introduce NFBugs, a data set of 133 non-functional bug ﬁxes collected from 65 open-source projects written in Java and Python. NFBugs can be used to support code recommender systems focusing on nonfunctional properties.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {2019 {IEEE}/{ACM} 16th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Radu, Aida and Nadi, Sarah},
	month = may,
	year = {2019},
	pages = {399--403},
}

@article{brooks_no_1986,
	title = {No {Silver} {Bullet} – {Essence} and {Accident} in {Software} {Engineering}},
	language = {en},
	author = {Brooks, Frederick},
	year = {1986},
	pages = {16},
}

@article{penzenstadlerSafetySecurityNow2014,
	title = {Safety, {Security}, {Now} {Sustainability}: {The} {Nonfunctional} {Requirement} for the 21st {Century}},
	volume = {31},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Safety, {Security}, {Now} {Sustainability}},
	url = {https://ieeexplore.ieee.org/document/6728940/},
	doi = {10.1109/MS.2014.22},
	language = {en},
	number = {3},
	urldate = {2021-03-24},
	journal = {IEEE Software},
	author = {Penzenstadler, Birgit and Raturi, Ankita and Richardson, Debra and Tomlinson, Bill},
	month = may,
	year = {2014},
	pages = {40--47},
}

@inproceedings{veras_errors_2010,
	address = {San Jose, CA, USA},
	title = {Errors on {Space} {Software} {Requirements}: {A} {Field} {Study} and {Application} {Scenarios}},
	isbn = {978-1-4244-9056-1},
	shorttitle = {Errors on {Space} {Software} {Requirements}},
	url = {http://ieeexplore.ieee.org/document/5635117/},
	doi = {10.1109/ISSRE.2010.30},
	abstract = {This paper presents a field study on real errors found in space software requirements documents. The goal is to understand and characterize the most frequent types of requirement problems in this critical application domain. To classify the software requirement errors analyzed we initially used a well-known existing taxonomy that was later extended in order to allow a more thorough analysis. The results of the study show a high rate of requirement errors (9.5 errors per each 100 requirements), which is surprising if we consider that the focus of the work is critical embedded software. Besides the characterization of the most frequent types of errors, the paper also proposes a set of operators that define how to inject realistic errors in requirement documents. This may be used in several scenarios, including: evaluating and training reviewers, estimating the number of requirement errors in real specifications, defining checklists for quick requirement verification, and defining benchmarks for requirements specifications.},
	language = {en},
	urldate = {2021-06-28},
	booktitle = {2010 {IEEE} 21st {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE},
	author = {Veras, Paulo C. and Villani, Emilia and Ambrosio, Ana Maria and Silva, Nuno and Vieira, Marco and Madeira, Henrique},
	month = nov,
	year = {2010},
	pages = {61--70},
}

@inproceedings{lutz_analyzing_1992,
	address = {San Diego, CA, USA},
	title = {Analyzing software requirements errors in safety-critical, embedded systems},
	isbn = {978-0-8186-3120-7},
	url = {http://ieeexplore.ieee.org/document/324825/},
	doi = {10.1109/ISRE.1993.324825},
	abstract = {This paper analyzes the root causes of safety-related software errors in safety-critical, embedded systems. The results show that software errors identified as potentially hazardous t o the s y s t e m tend t o be produced by different error mechanisms than non-safetyrelated software errors. Safety-related software errors are shown to arise most commonly from (1) discrepancies between the documented requirements specifications and the requirements needed for correct functioning of the s y s t e m and (2) misunderstandings of the soflware's interface with the rest of ihe system. The paper uses these results t o identify methods by which requirements errors can be prevented. The goal is t o reduce safety-related software errors and io enhance the safety of complex, embedded systems.},
	language = {en},
	urldate = {2021-06-28},
	booktitle = {[1993] {Proceedings} of the {IEEE} {International} {Symposium} on {Requirements} {Engineering}},
	publisher = {IEEE Comput. Soc. Press},
	author = {Lutz, R.R.},
	year = {1992},
	pages = {126--133},
}

@article{biAccessibilitySoftwarePractice2021,
	title = {Accessibility in {Software} {Practice}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {Accessibility in {Software} {Practice}},
	url = {http://arxiv.org/abs/2103.08778},
	abstract = {Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this paper, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners' viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct v.s. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We also propose some remedies to resolve the gaps and to highlight key future research directions.},
	language = {en},
	urldate = {2021-03-29},
	journal = {arXiv:2103.08778 [cs]},
	author = {Bi, Tingting and Xia, Xin and Lo, David and Grundy, John and Zimmermann, Thomas and Ford, Denae},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.08778},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{bird_does_2009,
	address = {Vancouver, BC, Canada},
	title = {Does distributed development affect software quality? {An} empirical case study of {Windows} {Vista}},
	isbn = {978-1-4244-3453-4},
	shorttitle = {Does distributed development affect software quality?},
	url = {http://ieeexplore.ieee.org/document/5070550/},
	doi = {10.1109/ICSE.2009.5070550},
	abstract = {It is widely believed that distributed software development is riskier and more challenging than collocated development. Prior literature on distributed development in software engineering and other ﬁelds discuss various challenges, including cultural barriers, expertise transfer difﬁculties, and communication and coordination overhead. We evaluate this conventional belief by examining the overall development of Windows Vista and comparing the postrelease failures of components that were developed in a distributed fashion with those that were developed by collocated teams. We found a negligible difference in failures. This difference becomes even less signiﬁcant when controlling for the number of developers working on a binary. We also examine component characteristics such as code churn, complexity, dependency information, and test code coverage and ﬁnd very little difference between distributed and collocated components to investigate if less complex components are more distributed. Further, we examine the software process and phenomena that occurred during the Vista development cycle and present ways in which the development process utilized may be insensitive to geography by mitigating the difﬁculties introduced in prior work in this area.},
	language = {en},
	urldate = {2021-06-21},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Bird, Christian and Nagappan, Nachiappan and Devanbu, Premkumar and Gall, Harald and Murphy, Brendan},
	year = {2009},
	pages = {518--528},
}

@inproceedings{mcnamara2018does,
	title = {Does {ACM}’s code of ethics change ethical decision making in software development?},
	booktitle = {Proceedings of the 2018 26th {ACM} joint meeting on european software engineering conference and symposium on the foundations of software engineering},
	author = {McNamara, Andrew and Smith, Justin and Murphy-Hill, Emerson},
	year = {2018},
	pages = {729--733},
}

@inproceedings{parkSystematicTestingReactive2015,
	address = {Florence, Italy},
	title = {Systematic {Testing} of {Reactive} {Software} with {Non}-{Deterministic} {Events}: {A} {Case} {Study} on {LG} {Electric} {Oven}},
	isbn = {978-1-4799-1934-5},
	shorttitle = {Systematic {Testing} of {Reactive} {Software} with {Non}-{Deterministic} {Events}},
	url = {http://ieeexplore.ieee.org/document/7202947/},
	doi = {10.1109/ICSE.2015.132},
	abstract = {Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.},
	language = {en},
	urldate = {2021-04-05},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Park, Yongbae and Hong, Shin and Kim, Moonzoo and Lee, Dongju and Cho, Junhee},
	month = may,
	year = {2015},
	keywords = {IoT},
	pages = {29--38},
}

@article{chapman_what_2017,
	title = {What can agile methods bring to high-integrity software development?},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3133233},
	doi = {10.1145/3133233},
	abstract = {Considering the issues and opportunities raised by Agile practices in the development of high-integrity software.},
	language = {en},
	number = {10},
	urldate = {2021-06-25},
	journal = {Communications of the ACM},
	author = {Chapman, Roderick and White, Neil and Woodcock, Jim},
	month = sep,
	year = {2017},
	pages = {38--41},
}

@inproceedings{muenchWhatYouCorrupt2018,
	address = {San Diego, CA},
	title = {What {You} {Corrupt} {Is} {Not} {What} {You} {Crash}: {Challenges} in {Fuzzing} {Embedded} {Devices}},
	isbn = {978-1-891562-49-5},
	shorttitle = {What {You} {Corrupt} {Is} {Not} {What} {You} {Crash}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-4_Muench_paper.pdf},
	doi = {10.14722/ndss.2018.23166},
	abstract = {As networked embedded systems are becoming more ubiquitous, their security is becoming critical to our daily life. While manual or automated large scale analysis of those systems regularly uncover new vulnerabilities, the way those systems are analyzed follows often the same approaches used on desktop systems. More speciﬁcally, traditional testing approaches relies on observable crashes of a program, and binary instrumentation techniques are used to improve the detection of those faulty states.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Muench, Marius and Stijohann, Jan and Kargl, Frank and Francillon, Aurelien and Balzarotti, Davide},
	year = {2018},
	keywords = {IoT},
}

@article{eismann_serverless_2021,
	title = {Serverless {Applications}: {Why}, {When}, and {How}?},
	volume = {38},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Serverless {Applications}},
	url = {https://ieeexplore.ieee.org/document/9190031/},
	doi = {10.1109/MS.2020.3023302},
	language = {en},
	number = {1},
	urldate = {2021-06-02},
	journal = {IEEE Software},
	author = {Eismann, Simon and Scheuner, Joel and van Eyk, Erwin and Schwinger, Maximilian and Grohmann, Johannes and Herbst, Nikolas and Abad, Cristina L. and Iosup, Alexandru},
	month = jan,
	year = {2021},
	pages = {32--39},
}

@article{ferrari_formal_2021,
	title = {Formal {Methods} in {Railways}: a {Systematic} {Mapping} {Study}},
	shorttitle = {Formal {Methods} in {Railways}},
	url = {http://arxiv.org/abs/2107.05413},
	abstract = {Formal methods are mathematically-based techniques for the rigorous development of software-intensive systems. The railway signaling domain is a field in which formal methods have traditionally been applied, with several success stories. This article reports on a mapping study that surveys the landscape of research on applications of formal methods to the development of railway systems. Our main results are as follows: (i) we identify a total of 328 primary studies relevant to our scope published between 1989 and 2020, of which 44\% published during the last 5 years and 24\% involving industry; (ii) the majority of studies are evaluated through Examples (41\%) and Experience Reports (38\%), while full-fledged Case Studies are limited (1.5\%); (iii) Model checking is the most commonly adopted technique (47\%), followed by simulation (27\%) and theorem proving (19.5\%); (iv) the dominant languages are UML (18\%) and B (15\%), while frequently used tools are ProB (9\%), NuSMV (8\%) and UPPAAL (7\%); however, a diverse landscape of languages and tools is employed; (v) the majority of systems are interlocking products (40\%), followed by models of high-level control logic (27\%); (vi) most of the studies focus on the Architecture (66\%) and Detailed Design (45\%) development phases. Based on these findings, we highlight current research gaps and expected actions. In particular, the need to focus on more empirically sound research methods, such as Case Studies and Controlled Experiments, and to lower the degree of abstraction, by applying formal methods and tools to development phases that are closer to software development. Our study contributes with an empirically based perspective on the future of research and practice in formal methods applications for railways.},
	language = {en},
	urldate = {2021-07-15},
	journal = {arXiv:2107.05413 [cs]},
	author = {Ferrari, Alessio and ter Beek, Maurice H.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.05413},
	keywords = {Computer Science - Software Engineering, Exemplar},
}

@inproceedings{koziolekOpenPnPPlugandProduceArchitecture2019,
	address = {Montreal, QC, Canada},
	title = {{OpenPnP}: {A} {Plug}-and-{Produce} {Architecture} for the {Industrial} {Internet} of {Things}},
	isbn = {978-1-72811-760-7},
	shorttitle = {{OpenPnP}},
	url = {https://ieeexplore.ieee.org/document/8804425/},
	doi = {10.1109/ICSE-SEIP.2019.00022},
	abstract = {Industrial control systems are complex, softwareintensive systems that manage mission-critical production processes. Commissioning such systems requires installing, conﬁguring, and integrating thousands of sensors, actuators, and controllers and is still a largely manual and costly process. Therefore, practitioners and researchers have been working on “plug and produce” approaches that automate commissioning for more than 15 years, but have often focused on network discovery and proprietary technologies. We introduce the vendor-neutral OpenPnP reference architecture, which can largely automate the conﬁguration and integration tasks for commissioning. Using an example implementation, we demonstrate that OpenPnP can reduce the conﬁguration and integration effort up to 90 percent and scales up to tens of thousands of communicated signals per second for large Industrial Internet-of-Things (IIoT) systems. OpenPnP can serve as a template for practitioners implementing IIoT applications throughout the automation industry and streamline commissioning processes in many thousands of control system installations.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Koziolek, Heiko and Burger, Andreas and Platenius-Mohr, Marie and Ruckert, Julius and Stomberg, Gosta},
	month = may,
	year = {2019},
	pages = {131--140},
}

@article{harris2008good,
	title = {The good engineer: {Giving} virtue its due in engineering ethics},
	volume = {14},
	number = {2},
	journal = {Science and Engineering Ethics},
	author = {Harris, Charles E},
	year = {2008},
	note = {Publisher: Springer},
	pages = {153--164},
}

@phdthesis{brown1998authenticity,
	title = {Authenticity in learning, teaching and assessment: {Perspectives} from a course in professional ethics for software engineers},
	school = {University of Sheffield, Division of Education},
	author = {Brown, Guy Jason},
	year = {1998},
}

@article{gotterbarn1997software,
	title = {Software engineering code of ethics},
	volume = {40},
	number = {11},
	journal = {Communications of the ACM},
	author = {Gotterbarn, Don and Miller, Keith and Rogerson, Simon},
	year = {1997},
	note = {Publisher: ACM New York, NY, USA},
	pages = {110--118},
}

@article{wanHowDoesMachine2020,
	title = {How does {Machine} {Learning} {Change} {Software} {Development} {Practices}?},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/8812912/},
	doi = {10.1109/TSE.2019.2937083},
	abstract = {Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit signiﬁcant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers signiﬁcant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our ﬁndings, we highlight future research directions and provide recommendations for practitioners.},
	language = {en},
	urldate = {2021-05-11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wan, Zhiyuan and Xia, Xin and Lo, David and Murphy, Gail C.},
	year = {2020},
	pages = {1--1},
}

@inproceedings{li_evolving_2021,
	address = {Madrid, Spain},
	title = {Evolving {Software} to be {ML}-{Driven} {Utilizing} {Real}-{World} {A}/{B} {Testing}: {Experiences}, {Insights}, {Challenges}},
	isbn = {978-1-66543-869-8},
	shorttitle = {Evolving {Software} to be {ML}-{Driven} {Utilizing} {Real}-{World} {A}/{B} {Testing}},
	url = {https://ieeexplore.ieee.org/document/9402122/},
	doi = {10.1109/ICSE-SEIP52600.2021.00026},
	abstract = {ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be ML-driven. In this paper, we contribute practical knowledge about evolving software to be ML-driven, utilizing real-world A/B testing. We draw on experiences evolving two software features from the Windows operating system to be ML-driven, with more than ten realworld A/B tests on millions of PCs over more than two years. We discuss practical reasons for using A/B testing to engineer ML-driven software, insights for success, as well as on-going realworld challenges. This knowledge may help practitioners, as well as help direct future research and innovations.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Li, Paul Luo and Chai, Xiaoyu and Campbell, Frederick and Liao, Jilong and Abburu, Neeraja and Kang, Minsuk and Niculescu, Irina and Brake, Greg and Patil, Siddharth and Dooley, James and Paddock, Brandon},
	month = may,
	year = {2021},
	pages = {170--179},
}

@article{liskovPROGRAMMINGABSTRACTDATA,
	title = {{PROGRAMMING} {WITH} {ABSTRACT} {DATA} {TYPES}},
	abstract = {The motivation behind the work in very-high-level languages is to ease the programming task by providing the programmer with a language containing primitives or abstractions suitable to his problem area. The programmer is then able to spend his effort in the right place; he concentrates on solving his problem, and the resulting program will be more reliable as a result. Clearly, this is a worthwhile goal.},
	language = {en},
	author = {Liskov, Barbara and Zilles, Stephen},
	pages = {10},
}

@article{gulwaniProgramSynthesis2017,
	title = {Program {Synthesis}},
	volume = {4},
	issn = {2325-1107, 2325-1131},
	url = {http://www.nowpublishers.com/article/Details/PGL-010},
	doi = {10.1561/2500000010},
	language = {en},
	number = {1-2},
	urldate = {2021-04-26},
	journal = {Foundations and Trends® in Programming Languages},
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	year = {2017},
	pages = {1--119},
}

@inproceedings{zhangEndtoEndAutomaticCloud2019,
	address = {Amsterdam Netherlands},
	title = {An {End}-to-{End} {Automatic} {Cloud} {Database} {Tuning} {System} {Using} {Deep} {Reinforcement} {Learning}},
	isbn = {978-1-4503-5643-5},
	url = {https://dl.acm.org/doi/10.1145/3299869.3300085},
	doi = {10.1145/3299869.3300085},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
	month = jun,
	year = {2019},
	pages = {415--432},
}

@inproceedings{molinaTrainingBinaryClassifiers2019,
	address = {Montreal, QC, Canada},
	title = {Training {Binary} {Classifiers} as {Data} {Structure} {Invariants}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8811951/},
	doi = {10.1109/ICSE.2019.00084},
	abstract = {We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artiﬁcial neural network, more precisely a binary classiﬁer, and training it to identify valid and invalid instances of a data structure. The obtained classiﬁer can then be used in place of the data structure’s invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object ﬁelds that “break” the collected valid values, i.e., that assign values to object ﬁelds that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classiﬁers that achieve signiﬁcantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug ﬁnding.},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Molina, Facundo and Degiovanni, Renzo and Ponzio, Pablo and Regis, German and Aguirre, Nazareno and Frias, Marcelo},
	month = may,
	year = {2019},
	pages = {759--770},
}

@article{fraser_no_2008,
	title = {No {Silver} {Bullet}: {Software} {Engineering} {Reloaded}},
	volume = {25},
	issn = {0740-7459},
	shorttitle = {No {Silver} {Bullet}},
	url = {http://ieeexplore.ieee.org/document/4420077/},
	doi = {10.1109/MS.2008.14},
	language = {en},
	number = {1},
	urldate = {2021-07-07},
	journal = {IEEE Software},
	author = {Fraser, Steven and Mancl, Dennis},
	month = jan,
	year = {2008},
	pages = {91--94},
}

@article{chapman_correctness_nodate,
	title = {Correctness by {Construction}: {A} {Manifesto} for {High} {Integrity} {Software}},
	abstract = {High integrity software systems are often so large that conventional development processes cannot get anywhere near achieving tolerable defect rates. This paper presents Correctness by Construction (CbyC)—an approach that has delivered very low defect rate software costeffectively. We describe the main principles of CbyC and the results achieved to date. We also touch on some of the barriers that we have encountered in trying to field CbyC within our own and other organisations.},
	language = {en},
	author = {Chapman, Roderick},
	pages = {4},
}

@article{luqi_formal_1997,
	title = {Formal methods: promises and problems},
	volume = {14},
	issn = {07407459},
	shorttitle = {Formal methods},
	url = {http://ieeexplore.ieee.org/document/566430/},
	doi = {10.1109/52.566430},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {IEEE Software},
	author = {{Luqi} and Goguen, J.A.},
	month = feb,
	year = {1997},
	pages = {73--85},
}

@inproceedings{allamanisMiningIdiomsSource2014,
	address = {Hong Kong, China},
	title = {Mining idioms from source code},
	isbn = {978-1-4503-3056-5},
	url = {http://dl.acm.org/citation.cfm?doid=2635868.2635901},
	doi = {10.1145/2635868.2635901},
	language = {en},
	urldate = {2021-04-18},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering} - {FSE} 2014},
	publisher = {ACM Press},
	author = {Allamanis, Miltiadis and Sutton, Charles},
	year = {2014},
	pages = {472--483},
}

@inproceedings{olney2016part,
	title = {Part of speech tagging {Java} method names},
	booktitle = {2016 {IEEE} international conference on software maintenance and evolution ({ICSME})},
	author = {Olney, Wyatt and Hill, Emily and Thurber, Chris and Lemma, Bezalem},
	year = {2016},
	note = {tex.organization: IEEE},
	pages = {483--487},
}

@article{parnas_really_2010,
	title = {Really rethinking 'formal methods'},
	volume = {43},
	number = {1},
	journal = {Computer},
	author = {Parnas, David Lorge},
	year = {2010},
	note = {Publisher: Institute of Electrical and Electronics Engineers, Inc., 3 Park Avenue, 17 …},
	pages = {28--34},
}

@article{abrial2009faultless,
	title = {Faultless systems: {Yes} we can!},
	volume = {42},
	number = {9},
	journal = {Computer},
	author = {Abrial, Jean-Raymond},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {30--36},
}

@incollection{sommerville_formal_nodate,
	title = {Formal {Specification}},
	url = {https://iansommerville.com/software-engineering-book/downloads/},
	urldate = {2021-06-25},
	booktitle = {Software {Engineering}},
	author = {Sommerville, Ian},
}

@article{white_formal_2017,
	title = {Formal verification: will the seedling ever flower?},
	volume = {375},
	issn = {1364-503X, 1471-2962},
	shorttitle = {Formal verification},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0402},
	doi = {10.1098/rsta.2015.0402},
	abstract = {In one sense, formal specification and verification have been highly successful: techniques have been developed in pioneering academic research, transferred to software companies through training and partnerships, and successfully deployed in systems with national significance. Altran UK has been in the vanguard of this movement. This paper summarizes some of our key deployments of formal techniques over the past 20 years, including both security- and safety-critical systems. The impact of formal techniques, however, remains within an industrial niche, and while government and suppliers across industry search for solutions to the problems of poor-quality software, the wider software industry remains resistant to adoption of this proven solution. We conclude by reflecting on some of the challenges we face as a community in ensuring that formal techniques achieve their true potential impact on society.
            This article is part of the themed issue ‘Verified trustworthy software systems’.},
	language = {en},
	number = {2104},
	urldate = {2021-06-25},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {White, Neil and Matthews, Stuart and Chapman, Roderick},
	month = oct,
	year = {2017},
	pages = {20150402},
}

@article{woodcock_formal_nodate,
	title = {Formal methods: {Practice} and experience},
	volume = {41},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Woodcock, Jim and Larsen, Peter Gorm and Bicarregui, Juan and Fitzgerald, John},
	pages = {36},
}

@article{wing_specifiers_1990,
	title = {A specifier's introduction to formal methods},
	volume = {23},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/58215/},
	doi = {10.1109/2.58215},
	language = {en},
	number = {9},
	urldate = {2021-06-28},
	journal = {Computer},
	author = {Wing, J.M.},
	month = sep,
	year = {1990},
	pages = {8--22},
}

@article{avizienis_basic_2004,
	title = {Basic concepts and taxonomy of dependable and secure computing},
	volume = {1},
	abstract = {This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
	language = {en},
	number = {1},
	journal = {IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING},
	author = {Avizienis, A},
	year = {2004},
	pages = {23},
}

@article{jhala_software_nodate,
	title = {Software model checking},
	volume = {41},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	pages = {54},
}

@article{holzmann_mars_2014,
	title = {Mars code},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2560217.2560218},
	doi = {10.1145/2560217.2560218},
	abstract = {Redundant software (and hardware) ensured Curiosity reached its destination and functioned as its designers intended.},
	language = {en},
	number = {2},
	urldate = {2021-06-25},
	journal = {Communications of the ACM},
	author = {Holzmann, Gerard J.},
	month = feb,
	year = {2014},
	pages = {64--73},
}

@inproceedings{fonseca_empirical_2017,
	address = {Belgrade Serbia},
	title = {An {Empirical} {Study} on the {Correctness} of {Formally} {Verified} {Distributed} {Systems}},
	isbn = {978-1-4503-4938-3},
	url = {https://dl.acm.org/doi/10.1145/3064176.3064183},
	doi = {10.1145/3064176.3064183},
	abstract = {Recent advances in formal veriﬁcation techniques enabled the implementation of distributed systems with machinechecked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation of the results and veriﬁcation practices.},
	language = {en},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Fonseca, Pedro and Zhang, Kaiyuan and Wang, Xi and Krishnamurthy, Arvind},
	month = apr,
	year = {2017},
	pages = {328--343},
}

@article{hoare_axiomatic_1969,
	title = {An axiomatic basis for computer programming},
	volume = {12},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/363235.363259},
	doi = {10.1145/363235.363259},
	abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics.},
	language = {en},
	number = {10},
	urldate = {2021-07-12},
	journal = {Communications of the ACM},
	author = {Hoare, C. A. R.},
	month = oct,
	year = {1969},
	pages = {576--580},
}

@article{chlipala2017formal,
	title = {Formal reasoning about programs},
	journal = {url: http://adam. chlipala. net/frap},
	author = {Chlipala, Adam},
	year = {2017},
}

@book{fowler2010domain,
	title = {Domain-specific languages},
	publisher = {Pearson Education},
	author = {Fowler, Martin},
	year = {2010},
}

@article{burrows_chubby_nodate,
	title = {The {Chubby} lock service for loosely-coupled distributed systems},
	abstract = {We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed ﬁle system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modiﬁed to accommodate the differences.},
	language = {en},
	author = {Burrows, Mike},
	pages = {16},
}

@article{swartz_airport_nodate,
	title = {Airport 95: {Automated} {Baggage} {System}?},
	volume = {21},
	abstract = {The Denver International Airport automated baggage system was a major news story spanning the years 1994-95. Reconstruction of the events of the project management of this system serves as an example of project summary reporting, which is stipulated in every project management methodology, but which is seldom or never done. The author provides sufficient detail to enable simulation of the design approach alternatives. If other projects are reported in the same format, it will be possible to compare projects on a design phase and/or event-by-event basis. The author recommends establishment and maintenance of a knowledge base of specific causes for failed software development projects. Background On 28 February, 1995 the opening of the Denver International Airport represented the first new major airport since Dallas/Fort Worth in 1975. The DIA automated baggage system was a major news story of 1994/1995. In the Open Channel column of the February, 1995 issue of the IEEE Computer magazine, I advocated further investigation into the problems associated with this project.},
	language = {en},
	number = {2},
	journal = {ACM SIGSOFT},
	author = {Swartz, A John},
	pages = {5},
}

@inproceedings{tian2015comparative,
	title = {A comparative study on the effectiveness of part-of-speech tagging techniques on bug reports},
	booktitle = {2015 {IEEE} 22nd international conference on software analysis, evolution, and reengineering ({SANER})},
	author = {Tian, Yuan and Lo, David},
	year = {2015},
	note = {tex.organization: IEEE},
	pages = {570--574},
}

@inproceedings{chitchyanSustainabilityDesignRequirements2016,
	address = {Austin Texas},
	title = {Sustainability design in requirements engineering: state of practice},
	isbn = {978-1-4503-4205-6},
	shorttitle = {Sustainability design in requirements engineering},
	url = {https://dl.acm.org/doi/10.1145/2889160.2889217},
	doi = {10.1145/2889160.2889217},
	abstract = {Sustainability is now a major concern in society, but there is little understanding of how it is perceived by software engineering professionals and how sustainability design can become an embedded part of software engineering process. This paper presents the results of a qualitative study exploring requirements engineering practitioners’ perceptions and attitudes towards sustainability. It identiﬁes obstacles and mitigation strategies regarding the application of sustainability design principles in daily work life. The results of this study reveal several factors that can prevent sustainability design from becoming a ﬁrst class citizen in software engineering: software practitioners tend to have a narrow understanding of the concept of sustainability; organizations show limited awareness of its potential opportunities and beneﬁts; and the norms in the discipline are not conducive to sustainable outcomes. These ﬁndings suggest the need for focused eﬀorts in sustainability education, but also a need to rethink professional norms and practices.},
	language = {en},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering} {Companion}},
	publisher = {ACM},
	author = {Chitchyan, Ruzanna and Becker, Christoph and Betz, Stefanie and Duboc, Leticia and Penzenstadler, Birgit and Seyff, Norbert and Venters, Colin C.},
	month = may,
	year = {2016},
	pages = {533--542},
}

@inproceedings{laignerMonolithicBigData2020,
	address = {Portoroz, Slovenia},
	title = {From a {Monolithic} {Big} {Data} {System} to a {Microservices} {Event}-{Driven} {Architecture}},
	isbn = {978-1-72819-532-2},
	url = {https://ieeexplore.ieee.org/document/9226286/},
	doi = {10.1109/SEAA51224.2020.00045},
	abstract = {Context] Data-intensive systems, a.k.a. big data systems (BDS), are software systems that handle a large volume of data in the presence of performance quality attributes, such as scalability and availability. Before the advent of big data management systems (e.g. Cassandra) and frameworks (e.g. Spark), organizations had to cope with large data volumes with customtailored solutions. In particular, a decade ago, Tecgraf/PUCRio developed a system to monitor truck ﬂeet in real-time and proactively detect events from the positioning data received. Over the years, the system evolved into a complex and large obsolescent code base involving a costly maintenance process. [Goal] We report our experience on replacing a legacy BDS with a microservice-based event-driven system. [Method] We applied action research, investigating the reasons that motivate the adoption of a microservice-based event-driven architecture, intervening to deﬁne the new architecture, and documenting the challenges and lessons learned. [Results] We perceived that the resulting architecture enabled easier maintenance and faultisolation. However, the myriad of technologies and the complex data ﬂow were perceived as drawbacks. Based on the challenges faced, we highlight opportunities to improve the design of big data reactive systems. [Conclusions] We believe that our experience provides helpful takeaways for practitioners modernizing systems with data-intensive requirements.},
	language = {en},
	urldate = {2021-05-14},
	booktitle = {2020 46th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Laigner, Rodrigo and Kalinowski, Marcos and Diniz, Pedro and Barros, Leonardo and Cassino, Carlos and Lemos, Melissa and Arruda, Darlan and Lifschitz, Sergio and Zhou, Yongluan},
	month = aug,
	year = {2020},
	pages = {213--220},
}

@inproceedings{offutt_fault_2001,
	address = {Hong Kong, China},
	title = {A fault model for subtype inheritance and polymorphism},
	isbn = {978-0-7695-1306-5},
	url = {http://ieeexplore.ieee.org/document/989461/},
	doi = {10.1109/ISSRE.2001.989461},
	abstract = {Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about 00 software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a f u l l understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fuult types. This paper presents a model f o r the appearance and realization of 00 faults and defines and discusses specijic categories of inheritance and polymorphic faults. The model and categories can be used to support empirical investigations of object-oriented testing techniques, to inspire further research into objectoriented testing and analysis, and to help improve design and development of object-oriented software.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {Proceedings 12th {International} {Symposium} on {Software} {Reliability} {Engineering}},
	publisher = {IEEE Comput. Soc},
	author = {Offutt, J. and Alexander, R. and Wu, Y. and Xiao, Q. and Hutchinson, C.},
	year = {2001},
	pages = {84--93},
}

@article{lagoFramingSustainabilityProperty2015,
	title = {Framing sustainability as a property of software quality},
	volume = {58},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2714560},
	doi = {10.1145/2714560},
	abstract = {This framework addresses the environmental dimension of software performance, as applied here by a paper mill and a car-sharing service.},
	language = {en},
	number = {10},
	urldate = {2021-03-24},
	journal = {Communications of the ACM},
	author = {Lago, Patricia and Koçak, Sedef Akinli and Crnkovic, Ivica and Penzenstadler, Birgit},
	month = sep,
	year = {2015},
	pages = {70--78},
}

@inproceedings{leveson1992high,
	title = {High-pressure steam engines and computer software},
	booktitle = {Proceedings of the 14th international conference on {Software} engineering},
	author = {Leveson, Nancy G},
	year = {1992},
	pages = {2--14},
}

@article{david_l_parnas_evaluation_1990,
	title = {Evaluation of safety-critical software},
	volume = {33},
	language = {en},
	number = {6},
	journal = {Communications of the ACM},
	author = {{David L Parnas} and van Schouwen, A. John and Kwan, Shu Po},
	year = {1990},
	pages = {13},
}

@article{hatton_n-version_1997,
	title = {N-version design versus one good version},
	volume = {14},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/636672/},
	doi = {10.1109/52.636672},
	language = {en},
	number = {6},
	urldate = {2021-06-25},
	journal = {IEEE Software},
	author = {Hatton, L.},
	month = dec,
	year = {1997},
	pages = {71--76},
}

@article{randell_turing_2000,
	title = {Turing {Memorial} {Lecture} {Facing} {Up} to {Faults}},
	volume = {43},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/43.2.95},
	doi = {10.1093/comjnl/43.2.95},
	language = {en},
	number = {2},
	urldate = {2021-06-25},
	journal = {The Computer Journal},
	author = {Randell, B.},
	month = feb,
	year = {2000},
	pages = {95--106},
}

@article{knight_experimental_1986,
	title = {An experimental evaluation of the assumption of independence in multiversion programming},
	volume = {SE-12},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/6312924/},
	doi = {10.1109/TSE.1986.6312924},
	language = {en},
	number = {1},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Knight, John C. and Leveson, Nancy G.},
	month = jan,
	year = {1986},
	pages = {96--109},
}

@article{knight_consistent_nodate,
	title = {The {Consistent} {Comparison} {Problem} in {N}-{Version} {Software}},
	abstract = {We have identified a difficulty in the implementation of N-version programming. The problem, which we call the Consistent Comparison Problem, arises for applications in which decisions are based on the results of comparisons of finite-precision numbers. We show that when versions make comparisons involving the results of finite-precision calculations, it is impossible to guarantee the consistency of their results. It is therefore possible that correct versions may arrive at completely different outputs for an application that does not apparently have multiple correct solutions. If this problem is not dealt with explicitly, an N-version system may be unable to reach a consensus even when none of its component versions fails.},
	language = {en},
	author = {Knight, John C and Nancy, A N D},
	pages = {5},
}

@article{chaum_security_1985,
	title = {Security without identification: transaction systems to make big brother obsolete},
	volume = {28},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Security without identification},
	url = {https://dl.acm.org/doi/10.1145/4372.4373},
	doi = {10.1145/4372.4373},
	abstract = {The large-scale automated transaction systems of the near future can be designed to protect the privacy and maintain the security of both individuals and organizations.},
	language = {en},
	number = {10},
	urldate = {2021-06-04},
	journal = {Communications of the ACM},
	author = {Chaum, David},
	month = oct,
	year = {1985},
	pages = {1030--1044},
}

@book{sanger2019perfect,
	title = {The perfect weapon: {War}, sabotage, and fear in the cyber age},
	publisher = {Broadway Books},
	author = {Sanger, David E},
	year = {2019},
}

@inproceedings{vanderlindenSchrodingerSecurityOpening2020,
	address = {Seoul South Korea},
	title = {Schrödinger's security: opening the box on app developers' security rationale},
	isbn = {978-1-4503-7121-6},
	shorttitle = {Schrödinger's security},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380394},
	doi = {10.1145/3377811.3380394},
	abstract = {Research has established the wide variety of security failures in mobile apps, their consequences, and how app developers introduce or exacerbate them. What is not well known is why developers do so—what is the rationale underpinning the decisions they make which eventually strengthen or weaken app security? This is all the more complicated in modern app development’s increasingly diverse demographic: growing numbers of independent, solo, or small team developers who do not have the organizational structures and support that larger software development houses enjoy.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {van der Linden, Dirk and Anthonysamy, Pauline and Nuseibeh, Bashar and Tun, Thein Than and Petre, Marian and Levine, Mark and Towse, John and Rashid, Awais},
	month = jun,
	year = {2020},
	pages = {149--160},
}

@article{gasibaSecureCodingEducation2021,
	title = {Is {Secure} {Coding} {Education} in the {Industry} {Needed}? {An} {Investigation} {Through} a {Large} {Scale} {Survey}},
	shorttitle = {Is {Secure} {Coding} {Education} in the {Industry} {Needed}?},
	url = {http://arxiv.org/abs/2102.05343},
	abstract = {The Department of Homeland Security in the United States estimates that 90\% of software vulnerabilities can be traced back to defects in design and software coding. The ﬁnancial impact of these vulnerabilities has been shown to exceed 380 million USD in industrial control systems alone. Since software developers write software, they also introduce these vulnerabilities into the source code. However, secure coding guidelines exist to prevent software developers from writing vulnerable code. This study focuses on the human factor, the software developer, and secure coding, in particular secure coding guidelines. We want to understand the software developers’ awareness and compliance to secure coding guidelines and why, if at all, they aren’t compliant or aware. We base our results on a large-scale survey on secure coding guidelines, with more than 190 industrial software developers. Our work’s main contribution motivates the need to educate industrial software developers on secure coding guidelines, and it gives a list of ﬁfteen actionable items to be used by practitioners in the industry. We also make our raw data openly available for further research.},
	language = {en},
	urldate = {2021-05-20},
	journal = {arXiv:2102.05343 [cs]},
	author = {Gasiba, Tiago Espinha and Lechner, Ulrike and Pinto-Albuquerque, Maria and Mendez, Daniel},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05343},
	keywords = {Computer Science - Software Engineering},
}

@article{landwehrTaxonomyComputerProgram1994,
	title = {A taxonomy of computer program security flaws},
	volume = {26},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/185403.185412},
	doi = {10.1145/185403.185412},
	abstract = {An organized record of actual flaws can be useful to computer system designers, programmers, analysts, administrators, and users. This survey provides a taxonomy for computer program security flaws, with an Appendix that documents 50 actual security flaws. These flaws have all been described previously in the open literature, but in widely separated places. For those new to the field of computer security, they provide a good introduction to the characteristics of security flaws and how they can arise. Because these flaws were not randomly selected from a valid statistical sample of such flaws, we make no strong claims concerning the likely distribution of actual security flaws within the taxonomy. However, this method of organizing security flaw data can help those who have custody of more representative samples to organize them and to focus their efforts to remove and, eventually, to prevent the introduction of security flaws.},
	language = {en},
	number = {3},
	urldate = {2021-04-30},
	journal = {ACM Computing Surveys},
	author = {Landwehr, Carl E. and Bull, Alan R. and McDermott, John P. and Choi, William S.},
	month = sep,
	year = {1994},
	pages = {211--254},
}

@incollection{howard2005measuring,
	title = {Measuring relative attack surfaces},
	booktitle = {Computer security in the 21st century},
	publisher = {Springer},
	author = {Howard, Michael and Pincus, Jon and Wing, Jeannette M},
	year = {2005},
	pages = {109--137},
}

@article{shin_evaluating_2011,
	title = {Evaluating {Complexity}, {Code} {Churn}, and {Developer} {Activity} {Metrics} as {Indicators} of {Software} {Vulnerabilities}},
	volume = {37},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/5560680/},
	doi = {10.1109/TSE.2010.81},
	abstract = {Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A.},
	month = nov,
	year = {2011},
	pages = {772--787},
}

@inproceedings{assalThinkSecureBeginning2019,
	address = {Glasgow Scotland Uk},
	title = {Think secure from the beginning: {A} {Survey} with {Software} {Developers}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {\textit{'{Think} secure from the beginning'}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300519},
	doi = {10.1145/3290605.3300519},
	abstract = {Vulnerabilities persist despite existing software security initiatives and best practices. This paper focuses on the human factors of software security, including human behaviour and motivation. We conducted an online survey to explore the interplay between developers and software security processes, e.g., we looked into how developers influence and are influenced by these processes. Our data included responses from 123 software developers currently employed in North America who work on various types of software applications. Whereas developers are often held responsible for security vulnerabilities, our analysis shows that the real issues frequently stem from a lack of organizational or process support to handle security throughout development tasks. Our participants are self-motivated towards software security, and the majority did not dismiss it but identified obstacles to achieving secure code. Our work highlights the need to look beyond the individual, and take a holistic approach to investigate organizational issues influencing software security.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Assal, Hala and Chiasson, Sonia},
	month = may,
	year = {2019},
	pages = {1--13},
}

@inproceedings{thomasSecurityApplicationDevelopment2018,
	address = {Montreal QC Canada},
	title = {Security {During} {Application} {Development}: an {Application} {Security} {Expert} {Perspective}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Security {During} {Application} {Development}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173836},
	doi = {10.1145/3173574.3173836},
	abstract = {Many of the security problems that people face today, such as security breaches and data theft, are caused by security vulnerabilities in application source code. Thus, there is a need to understand and improve the experiences of those who can prevent such vulnerabilities in the ﬁrst place - software developers as well as application security experts. Several studies have examined developers’ perceptions and behaviors regarding security vulnerabilities, demonstrating the challenges they face in performing secure programming and utilizing tools for vulnerability detection. We expand upon this work by focusing on those primarily responsible for application security - security auditors. In an interview study of 32 application security experts, we examine their views on application security processes, their workﬂows, and their interactions with developers in order to further inform the design of tools and processes to improve application security.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Thomas, Tyler W. and Tabassum, Madiha and Chu, Bill and Lipford, Heather},
	month = apr,
	year = {2018},
	pages = {1--12},
}

@inproceedings{lopezHopefullyWeAre2019,
	address = {Montreal, QC, Canada},
	title = {"{Hopefully} {We} {Are} {Mostly} {Secure}": {Views} on {Secure} {Code} in {Professional} {Practice}},
	isbn = {978-1-72812-239-7},
	shorttitle = {"{Hopefully} {We} {Are} {Mostly} {Secure}"},
	url = {https://ieeexplore.ieee.org/document/8816991/},
	doi = {10.1109/CHASE.2019.00023},
	abstract = {Security of software systems is of general concern, yet breaches caused by common vulnerabilities still occur. Software developers are routinely called upon to ”do more” to address this situation. However there has been little focus on the developers’ point of view, and understanding how security features in their day-to-day activities. This paper reports preliminary ﬁndings of semi-structured interviews taken during an ethnographic study of professional software developers in one organization who are not security experts. The overall study aims to understand how security features in day-to-day practice, while analysis of the interview data asks whether developers are responsible for security. The study reveals that awareness around security matters is raised through several paths including processes, standards, practices and company training and that a focus on security is driven by contextual factors. Security is taken care of with policies and through safeguards, and is handled differently depending on whether a team is developing new features, and hence ”looking forward”, or working with existing code and hence ”looking back”. Developers take and share responsibility for security in the code, but suggest that their responsibility has limits, and relies on collective practice.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {2019 {IEEE}/{ACM} 12th {International} {Workshop} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering} ({CHASE})},
	publisher = {IEEE},
	author = {Lopez, Tamara and Sharp, Helen and Tun, Thein and Bandara, Arosha and Levine, Mark and Nuseibeh, Bashar},
	month = may,
	year = {2019},
	pages = {61--68},
}

@article{tiefenauSecurityAvailabilityMultiple,
	title = {Security, {Availability}, and {Multiple} {Information} {Sources}:  {Exploring} {Update} {Behavior} of {System} {Administrators}},
	abstract = {Experts agree that keeping systems up to date is a powerful security measure. Previous work found that users sometimes explicitly refrain from performing timely updates, e.g., due to bad experiences which has a negative impact on end-user security. Another important user group has been investigated less extensively: system administrators, who are responsible for keeping complex and heterogeneous system landscapes available and secure.},
	language = {en},
	author = {Tiefenau, Christian and Häring, Maximilian and Krombholz, Katharina},
	pages = {21},
}

@article{dalelaMixedmethodStudySecurity2021,
	title = {A {Mixed}-method {Study} on {Security} and {Privacy} {Practices} in {Danish} {Companies}},
	url = {http://arxiv.org/abs/2104.04030},
	abstract = {Increased levels of digitalization in society expose companies to new security threats, requiring them to establish adequate security and privacy measures. Additionally, the presence of exogenous forces like new regulations, e.g., GDPR and the global COVID-19 pandemic, pose new challenges for companies that should preserve an adequate level of security while having to adapt to change. In this paper, we investigate such challenges through a two-phase study in companies located in Denmark—a country characterized by a high level of digitalization and trust—focusing on software development and techrelated companies. Our results show a number of issues, most notably i) a misalignment between software developers and management when it comes to the implementation of security and privacy measures, ii) difﬁculties in adapting company practices in light of implementing GDPR compliance, and iii) different views on the need to adapt security measures to cope with the COVID-19 pandemic.},
	language = {en},
	urldate = {2021-05-20},
	journal = {arXiv:2104.04030 [cs]},
	author = {Dalela, Asmita and Giallorenzo, Saverio and Kulyk, Oksana and Mauro, Jacopo and Paja, Elda},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.04030},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{pan_easy_2019,
	address = {Montreal, QC, Canada},
	title = {Easy {Modelling} and {Verification} of {Unpredictable} and {Preemptive} {Interrupt}-{Driven} {Systems}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812085/},
	doi = {10.1109/ICSE.2019.00037},
	abstract = {The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difﬁcult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts’ essential features visually and concisely. We also propose an automatabased semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both realworld and existing literature were conducted, and the results demonstrate our approach’s usability and effectiveness.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Pan, Minxue and Chen, Shouyu and Pei, Yu and Zhang, Tian and Li, Xuandong},
	month = may,
	year = {2019},
	pages = {212--222},
}

@article{choiNTFUZZEnablingTypeAware,
	title = {{NTFUZZ}: {Enabling} {Type}-{Aware} {Kernel} {Fuzzing} on {Windows} with {Static} {Binary} {Analysis}},
	abstract = {Although it is common practice for kernel fuzzers to leverage type information of system calls, current Windows kernel fuzzers do not follow the practice as most system calls are private and largely undocumented. In this paper, we present a practical static binary analyzer that automatically infers system call types on Windows at scale. We incorporate our analyzer to NTFUZZ, a type-aware Windows kernel fuzzing framework. To our knowledge, this is the ﬁrst practical fuzzing system that utilizes scalable binary analysis on a COTS OS. With NTFUZZ, we found 11 previously unknown kernel bugs, and earned \$25,000 through the bug bounty program offered by Microsoft. All these results conﬁrm the practicality of our system as a kernel fuzzer.},
	language = {en},
	author = {Choi, Jaeseung and Kim, Kangsu and Lee, Daejin and Cha, Sang Kil},
	pages = {17},
}

@article{rediniDIANEIdentifyingFuzzing,
	title = {{DIANE}: {Identifying} {Fuzzing} {Triggers} in {Apps} to {Generate} {Under}-constrained {Inputs} for {IoT} {Devices}},
	abstract = {Internet of Things (IoT) devices have rooted themselves in the everyday life of billions of people. Thus, researchers have applied automated bug finding techniques to improve their overall security. However, due to the difficulties in extracting and emulating custom firmware, black-box fuzzing is often the only viable analysis option. Unfortunately, this solution mostly produces invalid inputs, which are quickly discarded by the targeted IoT device and do not penetrate its code. Another proposed approach is to leverage the companion app (i.e., the mobile app typically used to control an IoT device) to generate well-structured fuzzing inputs. Unfortunately, the existing solutions produce fuzzing inputs that are constrained by app-side validation code, thus significantly limiting the range of discovered vulnerabilities.},
	language = {en},
	author = {Redini, Nilo and Continella, Andrea and Das, Dipanjan and Pasquale, Giulio De and Spahn, Noah and Machiry, Aravind and Bianchi, Antonio and Kruegel, Christopher and Vigna, Giovanni},
	keywords = {IoT},
	pages = {17},
}

@article{tsipenyukSevenPerniciousKingdoms2005,
	title = {Seven {Pernicious} {Kingdoms}: {A} {Taxonomy} of {Software} {Security} {Errors}},
	volume = {3},
	issn = {1540-7993},
	shorttitle = {Seven {Pernicious} {Kingdoms}},
	url = {http://ieeexplore.ieee.org/document/1556543/},
	doi = {10.1109/MSP.2005.159},
	abstract = {We want to help developers and security practitioners understand common types of coding errors that lead to vulnerabilities. By organizing these errors into a simple taxonomy, we can teach developers to recognize categories of problems that lead to vulnerabilities and identify existing errors as they build software.},
	language = {en},
	number = {6},
	urldate = {2021-04-30},
	journal = {IEEE Security and Privacy Magazine},
	author = {Tsipenyuk, K. and Chess, B. and McGraw, G.},
	month = nov,
	year = {2005},
	pages = {81--84},
}

@inproceedings{acarDevelopersNeedSupport2017,
	address = {Cambridge, MA, USA},
	title = {Developers {Need} {Support}, {Too}: {A} {Survey} of {Security} {Advice} for {Software} {Developers}},
	isbn = {978-1-5386-3467-7},
	shorttitle = {Developers {Need} {Support}, {Too}},
	url = {http://ieeexplore.ieee.org/document/8077802/},
	doi = {10.1109/SecDev.2017.17},
	abstract = {Increasingly developers are becoming aware of the importance of software security, as frequent high-proﬁle security incidents emphasize the need for secure code. Faced with this new problem, most developers will use their normal approach: web search. But are the resulting web resources useful and effective at promoting security in practice? Recent research has identiﬁed security problems arising from Q\&A resources that help with speciﬁc secure-programming problems, but the web also contains many general resources that discuss security and secure programming more broadly, and to our knowledge few if any of these have been empirically evaluated. The continuing prevalence of security bugs suggests that this guidance ecosystem is not currently working well enough: either effective guidance is not available, or it is not reaching the developers who need it. This paper takes a ﬁrst step toward understanding and improving this guidance ecosystem by identifying and analyzing 19 general advice resources. The results identify important gaps in the current ecosystem and provide a basis for future work evaluating existing resources and developing new ones to ﬁll these gaps.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {2017 {IEEE} {Cybersecurity} {Development} ({SecDev})},
	publisher = {IEEE},
	author = {Acar, Yasemin and Stransky, Christian and Wermke, Dominik and Weir, Charles and Mazurek, Michelle L. and Fahl, Sascha},
	month = sep,
	year = {2017},
	pages = {22--26},
}

@inproceedings{chaPrincipledApproachGraphQL2020,
	address = {Virtual Event USA},
	title = {A principled approach to {GraphQL} query cost analysis},
	isbn = {978-1-4503-7043-1},
	url = {https://dl.acm.org/doi/10.1145/3368089.3409670},
	doi = {10.1145/3368089.3409670},
	abstract = {The landscape of web APIs is evolving to meet new client requirements and to facilitate how providers fulfill them. A recent web API model is GraphQL, which is both a query language and a runtime. Using GraphQL, client queries express the data they want to retrieve or mutate, and servers respond with exactly those data or changes. GraphQL’s expressiveness is risky for service providers because clients can succinctly request stupendous amounts of data, and responding to overly complex queries can be costly or disrupt service availability. Recent empirical work has shown that many service providers are at risk. Using traditional API management methods is not sufficient, and practitioners lack principled means of estimating and measuring the cost of the GraphQL queries they receive.},
	language = {en},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Cha, Alan and Wittern, Erik and Baudart, Guillaume and Davis, James C. and Mandel, Louis and Laredo, Jim A.},
	month = nov,
	year = {2020},
	pages = {257--268},
}

@article{yangInterviewStudyHow,
	title = {An interview study of how developers use execution logs in embedded software engineering},
	abstract = {Execution logs capture the run-time behavior of software systems. To assist developers in their maintenance tasks, many studies have proposed tools to analyze execution information from logs. However, it is as yet unknown how industry developers analyze logs in embedded software engineering.},
	language = {en},
	author = {Yang, Nan and Schiffelers, Ramon and Lukkien, Johan and Cuijpers, Pieter and Serebrenik, Alexander},
	keywords = {IoT},
	pages = {10},
}

@article{chenLearningGuidedNetworkFuzzing,
	title = {Learning-{Guided} {Network} {Fuzzing} for {Testing} {Cyber}-{Physical} {System} {Defences}},
	abstract = {The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically ﬁnding ‘test suites’ of CPS network attacks, without requiring any knowledge of the system’s control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efﬁcacy of smart fuzzing by implementing it for two real-world CPS testbeds—a water puriﬁcation plant and a water distribution system—ﬁnding attacks that drive them into 27 different unsafe states involving water ﬂow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, ﬁnding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.},
	language = {en},
	author = {Chen, Yuqi and Poskitt, Christopher M and Sun, Jun and Adepu, Sridhar and Zhang, Fan},
	keywords = {IoT},
	pages = {12},
}

@techreport{alspaugh1992software,
	title = {Software requirements for the a-7e aircraft.},
	institution = {NAVAL RESEARCH LAB WASHINGTON DC},
	author = {Alspaugh, Thomas A and Faulk, Stuart R and Britton, Kathryn H and Parker, R Alan and Parnas, David L},
	year = {1992},
}

@article{zave_reference_2000,
	title = {A reference model for requirements and specifications},
	volume = {17},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/896248/},
	doi = {10.1109/52.896248},
	language = {en},
	number = {3},
	urldate = {2021-06-23},
	journal = {IEEE Software},
	author = {Zave, P. and Jackson, M. and Gunter, E.L. and Gunter, C.A.},
	month = jun,
	year = {2000},
	pages = {37--43},
}

@inproceedings{osterweil1997software,
	title = {Software processes are software too, revisited: an invited talk on the most influential paper of {ICSE} 9},
	booktitle = {Proceedings of the 19th international conference on {Software} engineering},
	author = {Osterweil, Leon J},
	year = {1997},
	pages = {540--548},
}

@article{mckeeman_differential_1998,
	title = {Differential {Testing} for {Software}},
	volume = {10},
	language = {en},
	number = {1},
	author = {McKeeman, William M},
	year = {1998},
	pages = {8},
}

@inproceedings{pirelliRequirementsElicitationService2019,
	address = {Jeju Island, Korea (South)},
	title = {Requirements {Elicitation} with a {Service} {Canvas} for {Packaged} {Enterprise} {Systems}},
	isbn = {978-1-72813-912-8},
	url = {https://ieeexplore.ieee.org/document/8920569/},
	doi = {10.1109/RE.2019.00043},
	abstract = {We present a technique for eliciting requirements based on the use of a service canvas and the results of its application in the early phase of a customer relationship management integration project. The project was a collaboration between a research group and two industry partners. We describe (1) our service canvas, (2) how we designed a set of workshops to elicit the requirements, (3) the support tools used for running the workshops, and (4) the resulting canvas, listing the customer relationship management requirements, that was the basis for the project proposal. We explain how, as participant observers, we conducted the project and how we collected and analyzed the data. We describe what worked well and the lessons we learned. We outline some practical problems that remain unsolved.},
	language = {en},
	urldate = {2021-03-22},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	publisher = {IEEE},
	author = {Pirelli, Blagovesta and Etzlinger, Lucien and Derrier, David and Regev, Gil and Wegmann, Alain},
	month = sep,
	year = {2019},
	pages = {340--350},
}

@inproceedings{appleton_streamed_1998,
	title = {Streamed {Lines}: {Branching} {Patterns} for {Parallel} {Software} {Development}},
	abstract = {Most software version control systems provide mechanisms for branching into multiple lines of development and merging source code from one development line into another. However, the techniques, policies and guidelines for using these mechanisms are often misapplied or not fully understood. This is unfortunate, since the use or misuse of branching and merging can make or break a parallel software development project. Streamed Lines is a pattern language for organizing related lines of development into appropriately diverging and converging streams of source code changes.},
	language = {en},
	author = {Appleton, Brad and Berczuk, Stephen P and Cabrera, Ralph and Orenstein, Robert},
	year = {1998},
	pages = {67},
}

@incollection{osterweilSoftwareProcessesAre2011,
	address = {Berlin, Heidelberg},
	title = {Software {Processes} are {Software} {Too}},
	isbn = {978-3-642-19822-9 978-3-642-19823-6},
	url = {http://link.springer.com/10.1007/978-3-642-19823-6_17},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {Engineering of {Software}},
	publisher = {Springer Berlin Heidelberg},
	author = {Osterweil, Leon},
	editor = {Tarr, Peri L. and Wolf, Alexander L.},
	year = {2011},
	doi = {10.1007/978-3-642-19823-6_17},
	pages = {323--344},
}

@inproceedings{tsantalis2009identification,
	title = {Identification of extract method refactoring opportunities},
	booktitle = {2009 13th european conference on software maintenance and reengineering},
	author = {Tsantalis, Nikolaos and Chatzigeorgiou, Alexander},
	year = {2009},
	note = {tex.organization: IEEE},
	pages = {119--128},
}

@article{tsantalis2011identification,
	title = {Identification of extract method refactoring opportunities for the decomposition of methods},
	volume = {84},
	number = {10},
	journal = {Journal of Systems and Software},
	author = {Tsantalis, Nikolaos and Chatzigeorgiou, Alexander},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {1757--1782},
}

@incollection{garlanINTRODUCTIONSOFTWAREARCHITECTURE1993,
	title = {An {INTRODUCTION} {TO} {SOFTWARE} {ARCHITECTURE}},
	volume = {2},
	isbn = {978-981-02-1594-1 978-981-279-803-9},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812798039_0001},
	abstract = {This work was funded in part by the Department of Defense Advanced Research Project Agency under grant MDA972-92-J-1002, by National Science Foundation Grants CCR-9109469 and CCR-9112880, and by a grant from Siemens Corporate Research. It was also funded in part by the Carnegie Mellon University School of Computer Science and Software Engineering Institute (which is sponsored by the U.S. Department of Defense). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government, the Department of Defense, the National Science Foundation, Siemens Corporation, or Carnegie Mellon University.},
	language = {en},
	urldate = {2021-04-21},
	booktitle = {Series on {Software} {Engineering} and {Knowledge} {Engineering}},
	publisher = {WORLD SCIENTIFIC},
	author = {Garlan, David and Shaw, Mary},
	collaborator = {Ambriola, Vincenzo and Tortora, Genoveffa},
	month = dec,
	year = {1993},
	doi = {10.1142/9789812798039_0001},
	pages = {1--39},
}

@article{martinDesignPrinciplesDesign2000,
	title = {Design {Principles} and {Design} {Patterns}},
	language = {en},
	author = {Martin, Robert C},
	year = {2000},
	pages = {34},
}

@article{adamsPeopleSystematicallyOverlook,
	title = {People systematically overlook subtractive changes},
	language = {en},
	author = {Adams, Gabrielle S},
	pages = {17},
}

@article{kruchten_41_1995,
	title = {The 4+1 {View} {Model} of architecture},
	volume = {12},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/469759/},
	doi = {10.1109/52.469759},
	language = {en},
	number = {6},
	urldate = {2021-08-20},
	journal = {IEEE Software},
	author = {Kruchten, P.B.},
	month = nov,
	year = {1995},
	pages = {42--50},
}

@article{shneiderman_experimental_1977,
	title = {Experimental investigations of the utility of detailed flowcharts in programming},
	volume = {20},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/359605.359610},
	doi = {10.1145/359605.359610},
	abstract = {This paper describes previous research on flowcharts and a series of controlled experiments to test the utility of detailed flowcharts as an aid to program composition, comprehension, debugging, and modification. No statistically significant difference between flowchart and nonflowchart groups has been shown, thereby calling into question the utility of detailed flowcharting. A program of further research is suggested.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben and Mayer, Richard and McKay, Don and Heller, Peter},
	month = jun,
	year = {1977},
	pages = {373--381},
}

@article{lampsonHintsComputerSystemDesign,
	title = {Hints for {ComputerSystemDesign}},
	abstract = {Experience with the design and implementation of a number of computer systems, and study of many other systems, has led to some general hints for system design which are described here. They are illustrated by a number of examples, ranging from hardware such as the Alto and the Dorado to applications programs such as Bravo and Star.},
	language = {en},
	journal = {SOSP},
	author = {Lampson, Butler W},
	pages = {16},
}

@book{fowler2018refactoring,
	title = {Refactoring: improving the design of existing code},
	publisher = {Addison-Wesley Professional},
	author = {Fowler, Martin},
	year = {2018},
}

@inproceedings{tufanoWhenWhyYour2015,
	address = {Florence, Italy},
	title = {When and {Why} {Your} {Code} {Starts} to {Smell} {Bad}},
	isbn = {978-1-4799-1934-5},
	url = {http://ieeexplore.ieee.org/document/7194592/},
	doi = {10.1109/ICSE.2015.59},
	abstract = {In past and recent years, the issues related to managing technical debt received signiﬁcant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To ﬁll this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smellintroducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identiﬁed as smellintroducing). Our ﬁndings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Di Penta, Massimiliano and De Lucia, Andrea and Poshyvanyk, Denys},
	month = may,
	year = {2015},
	pages = {403--414},
}

@article{krueger1992software,
	title = {Software reuse},
	volume = {24},
	number = {2},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Krueger, Charles W},
	year = {1992},
	note = {Publisher: ACM New York, NY, USA},
	pages = {131--183},
}

@article{frakes_software_2005,
	title = {Software reuse research: status and future},
	volume = {31},
	issn = {0098-5589},
	shorttitle = {Software reuse research},
	url = {http://ieeexplore.ieee.org/document/1492369/},
	doi = {10.1109/TSE.2005.85},
	abstract = {This paper briefly summarizes software reuse research, discusses major research contributions and unsolved problems, provides pointers to key publications, and introduces four papers selected from The Eighth International Conference on Software Reuse (ICSR8).},
	language = {en},
	number = {7},
	urldate = {2021-07-13},
	journal = {IEEE Transactions on Software Engineering},
	author = {Frakes, W.B. and {Kyo Kang}},
	month = jul,
	year = {2005},
	pages = {529--536},
}

@article{morisio_success_2002,
	title = {Success and failure factors in software reuse},
	volume = {28},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/995420/},
	doi = {10.1109/TSE.2002.995420},
	abstract = {ÐThis paper aims at identifying some of the key factors in adopting or running a company-wide software reuse program. Key factors are derived from empirical evidence of reuse practices, as emerged from a survey of projects for the introduction of reuse in European companies: 24 such projects performed from 1994 to 1997 were analyzed using structured interviews. The projects were undertaken in both large and small companies, working in a variety of business domains, and using both object-oriented and procedural development approaches. Most of them produce software with high commonality between applications, and have at least reasonably mature processes. Despite that apparent potential for success, around one-third of the projects failed. Three main causes of failure were not introducing reuse-specific processes, not modifying nonreuse processes, and not considering human factors. The root cause was a lack of commitment by top management, or nonawareness of the importance of those factors, often coupled with the belief that using the object-oriented approach or setting up a repository seamlessly is all that is necessary to achieve success in reuse. Conversely, successes were achieved when, given a potential for reuse because of commonality among applications, management committed to introducing reuse processes, modifying nonreuse processes, and addressing human factors. While addressing those three issues turned out to be essential, the lower-level details of how to address them varied greatly: for instance, companies produced large-grained or small-grained reusable assets, did or did not perform domain analysis, did or did not use dedicated reuse groups, used specific tools for the repository or no tools. As far as these choices are concerned, the key point seems to be the sustainability of the approach and its suitability to the context of the company.},
	language = {en},
	number = {4},
	urldate = {2021-07-13},
	journal = {IEEE Transactions on Software Engineering},
	author = {Morisio, M. and Ezran, M. and Tully, C.},
	month = apr,
	year = {2002},
	pages = {340--357},
}

@incollection{varnell-sarjeant_comparing_2015,
	title = {Comparing {Reuse} {Strategies} in {Different} {Development} {Environments}},
	volume = {97},
	isbn = {978-0-12-802133-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065245814000035},
	language = {en},
	urldate = {2021-06-29},
	booktitle = {Advances in {Computers}},
	publisher = {Elsevier},
	author = {Varnell-Sarjeant, Julia and Amschler Andrews, Anneliese},
	year = {2015},
	doi = {10.1016/bs.adcom.2014.10.002},
	pages = {1--47},
}

@article{parnas_modular_1985,
	title = {The {Modular} {Structure} of {Complex} {Systems}},
	volume = {SE-11},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1702002/},
	doi = {10.1109/TSE.1985.232209},
	language = {en},
	number = {3},
	urldate = {2021-07-12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Parnas, D.L. and Clements, P.C. and Weiss, D.M.},
	month = mar,
	year = {1985},
	pages = {259--266},
}

@inproceedings{hayhurst_considering_2003,
	address = {Indianapolis, IN, USA},
	title = {Considering object oriented technology in aviation applications},
	isbn = {978-0-7803-7844-5},
	url = {http://ieeexplore.ieee.org/document/5731073/},
	doi = {10.1109/DASC.2003.1245823},
	abstract = {Few developers of commercial aviation software products are using object-oriented technology (OOT), despite its popularity in some other industries. Safety concerns about using OOT in critical applications, uncertainty about how to comply with regulatory requirements, and basic conservatism within the aviation community have been factors behind this caution.},
	language = {en},
	urldate = {2021-07-13},
	booktitle = {22nd {Digital} {Avionics} {Systems} {Conference} {Proceedings} ({Cat} {No} {03CH37449}) {DASC}-03},
	publisher = {IEEE},
	author = {{Hayhurst} and {Holloway}},
	year = {2003},
	pages = {3.B.1--3.1},
}

@article{weisertPseudoObjectorientedProgramming2002,
	title = {Pseudo object-oriented programming considered harmful},
	volume = {37},
	issn = {0362-1340, 1558-1160},
	url = {https://dl.acm.org/doi/10.1145/510857.510865},
	doi = {10.1145/510857.510865},
	language = {en},
	number = {4},
	urldate = {2021-04-30},
	journal = {ACM SIGPLAN Notices},
	author = {Weisert, Conrad},
	month = apr,
	year = {2002},
	pages = {31--31},
}

@article{mohagheghi_quality_2007,
	title = {Quality, productivity and economic benefits of software reuse: a review of industrial studies},
	volume = {12},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Quality, productivity and economic benefits of software reuse},
	url = {http://link.springer.com/10.1007/s10664-007-9040-x},
	doi = {10.1007/s10664-007-9040-x},
	abstract = {Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges.},
	language = {en},
	number = {5},
	urldate = {2021-07-13},
	journal = {Empirical Software Engineering},
	author = {Mohagheghi, Parastoo and Conradi, Reidar},
	month = sep,
	year = {2007},
	pages = {471--516},
}

@inproceedings{leveson2004making,
	title = {Making embedded software reuse practical and safe},
	booktitle = {Proceedings of the 12th acm sigsoft twelfth international symposium on foundations of software engineering},
	author = {Leveson, Nancy G and Weiss, Kathryn Anne},
	year = {2004},
	pages = {171--178},
}

@article{parnasCriteriaBeUsed1972,
	title = {On the criteria to be used in decomposing systems into modules},
	volume = {15},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/361598.361623},
	doi = {10.1145/361598.361623},
	abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a "modularization" is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
	language = {en},
	number = {12},
	urldate = {2021-04-19},
	journal = {Communications of the ACM},
	author = {Parnas, D. L.},
	month = dec,
	year = {1972},
	pages = {1053--1058},
}

@article{xingPseudoObjectorientedProgramming2003,
	title = {On pseudo object-oriented programming considered harmful},
	volume = {46},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/944217.944246},
	doi = {10.1145/944217.944246},
	abstract = {Reconsidering and disentangling some fundamental relationships and issues.},
	language = {en},
	number = {10},
	urldate = {2021-04-30},
	journal = {Communications of the ACM},
	author = {Xing, Cong-cong and Belkhouche, Boumediene},
	month = oct,
	year = {2003},
	pages = {115--117},
}

@article{leitner_mixed-method_2019,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice},
	volume = {149},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121218302735},
	doi = {10.1016/j.jss.2018.12.013},
	language = {en},
	urldate = {2021-06-02},
	journal = {Journal of Systems and Software},
	author = {Leitner, Philipp and Wittern, Erik and Spillner, Josef and Hummer, Waldemar},
	month = mar,
	year = {2019},
	pages = {340--359},
}

@inproceedings{vanDeursen2017TeachingSWArch,
	title = {A collaborative approach to teaching software architecture},
	doi = {10.1145/3017680.3017737},
	booktitle = {Proceedings of 48th {ACM} technical symposium on computer science education ({SIGCSE})},
	author = {van Deursen, Arie and Aniche, Maurício and Aué, Joop and Slag, Rogier and de Jong, Michael and Nederlof, Alex and Bouwers, Eric},
	year = {2017},
}

@inproceedings{paul_why_2021,
	address = {Madrid, Spain},
	title = {Why {Security} {Defects} {Go} {Unnoticed} during {Code} {Reviews}? {A} {Case}-{Control} {Study} of the {Chromium} {OS} {Project}},
	isbn = {978-1-66540-296-5},
	shorttitle = {Why {Security} {Defects} {Go} {Unnoticed} during {Code} {Reviews}?},
	url = {https://ieeexplore.ieee.org/document/9402130/},
	doi = {10.1109/ICSE43902.2021.00124},
	abstract = {Peer code review has been found to be effective in identifying security vulnerabilities. However, despite practicing mandatory code reviews, many Open Source Software (OSS) projects still encounter a large number of post-release security vulnerabilities, as some security defects escape those. Therefore, a project manager may wonder if there was any weakness or inconsistency during a code review that missed a security vulnerability. Answers to this question may help a manager pinpointing areas of concern and taking measures to improve the effectiveness of his/her project’s code reviews in identifying security defects. Therefore, this study aims to identify the factors that differentiate code reviews that successfully identified security defects from those that missed such defects.},
	language = {en},
	urldate = {2021-05-27},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Paul, Rajshakhar and Turzo, Asif Kamal and Bosu, Amiangshu},
	month = may,
	year = {2021},
	pages = {1373--1385},
}

@article{noauthor_challenges_nodate,
	title = {Challenges and success factors for large-scale agile transformations: {A} systematic literature review {\textbar} {Elsevier} {Enhanced} {Reader}},
	language = {en},
	pages = {23},
}

@article{mccracken_life_1982,
	title = {Life cycle concept considered harmful},
	volume = {7},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1005937.1005943},
	doi = {10.1145/1005937.1005943},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {McCracken, Daniel D. and Jackson, Michael A.},
	month = apr,
	year = {1982},
	pages = {29--32},
}

@article{boehm_spiral_1988,
	title = {A spiral model of software development and enhancement},
	volume = {21},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/59/},
	doi = {10.1109/2.59},
	language = {en},
	number = {5},
	urldate = {2021-08-26},
	journal = {Computer},
	author = {Boehm, B. W.},
	month = may,
	year = {1988},
	pages = {61--72},
}

@incollection{douglass2013agile,
	title = {Agile development for embedded systems},
	booktitle = {Software engineering for embedded systems},
	publisher = {Elsevier},
	author = {Douglass, Bruce},
	year = {2013},
	pages = {731--766},
}

@book{kenschwaberScrumGuide2020,
	title = {The {Scrum} {Guide}},
	url = {https://scrumguides.org},
	author = {Ken Schwaber, Jeff Sutherland},
	year = {2020},
}

@article{herbsleb_empirical_2003,
	title = {An empirical study of speed and communication in globally distributed software development},
	volume = {29},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1205177/},
	doi = {10.1109/TSE.2003.1205177},
	abstract = {Global software development is rapidly becoming the norm for technology companies. Previous qualitative research suggests that distributed development may increase development cycle time for individual work items (modification requests). We use both data from the source code change management system and survey data to model the extent of delay in a distributed software development organization and explore several possible mechanisms for this delay. One key finding is that distributed work items appear to take about two and one-half times as long to complete as similar items where all the work is colocated. The data strongly suggest a mechanism for the delay, i.e., that distributed work items involve more people than comparable same-site work items, and the number of people involved is strongly related to the calendar time to complete a work item. We replicate the analysis of change data in a different organization with a different product and different sites and confirm our main findings. We also report survey results showing differences between same-site and distributed social networks, testing several hypotheses about characteristics of distributed social networks that may be related to delay. We discuss implications of our findings for practices and collaboration technology that have the potential for dramatically speeding distributed software development.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Herbsleb, J.D. and Mockus, A.},
	month = jun,
	year = {2003},
	pages = {481--494},
}

@inproceedings{herbsleb_global_2005,
	address = {St. Louis, MO, USA},
	title = {Global software development at siemens: experience from nine projects},
	shorttitle = {Global software development at siemens},
	url = {http://ieeexplore.ieee.org/document/1553598/},
	doi = {10.1109/ICSE.2005.1553598},
	abstract = {We report on the experiences of Siemens Corporation in nine globally-distributed software development projects. These projects represent a range of collaboration models, from co-development to outsourcing of components to outsourcing the software for an entire project. We report experience and lessons in issues of project management, division of labor, ongoing coordination of technical work, and communication. We include lessons learned, and conclude the paper with suggestions about important open research issues in this area.},
	language = {en},
	urldate = {2021-06-21},
	booktitle = {Proceedings. 27th {International} {Conference} on {Software} {Engineering}, 2005. {ICSE} 2005.},
	publisher = {IEEe},
	author = {Herbsleb, J.D. and Paulish, D.J. and Bass, M.},
	year = {2005},
	pages = {524--533},
}

@inproceedings{holmstrom_global_2006,
	address = {Florianopolis, Brazil},
	title = {Global {Software} {Development} {Challenges}: {A} {Case} {Study} on {Temporal}, {Geographical} and {Socio}-{Cultural} {Distance}},
	isbn = {978-0-7695-2663-8},
	shorttitle = {Global {Software} {Development} {Challenges}},
	url = {http://ieeexplore.ieee.org/document/4031736/},
	doi = {10.1109/ICGSE.2006.261210},
	abstract = {Global software development (GSD) is a phenomenon that is receiving considerable interest from companies all over the world. In GSD, stakeholders from different national and organizational cultures are involved in developing software and the many benefits include access to a large labour pool, cost advantage and round-the-clock development. However, GSD is technologically and organizationally complex and presents a variety of challenges to be managed by the software development team. In particular, temporal, geographical and socio-cultural distances impose problems not experienced in traditional systems development. In this paper, we present findings from a case study in which we explore the particular challenges associated with managing GSD. Our study also reveals some of the solutions that are used to deal with these challenges. We do so by empirical investigation at three US based GSD companies operating in Ireland. Based on qualitative interviews we present challenges related to temporal, geographical and socio-cultural distance.},
	language = {en},
	urldate = {2021-06-21},
	booktitle = {2006 {IEEE} {International} {Conference} on {Global} {Software} {Engineering} ({ICGSE}'06)},
	publisher = {IEEE},
	author = {Holmstrom, Helena and Conchuir, Eoin and Agerfalk, Par and Fitzgerald, Brian},
	month = oct,
	year = {2006},
	pages = {3--11},
}

@article{russoAgileSuccessModel,
	title = {The {Agile} {Success} {Model}: {A} {Mixed} {Methods} {Study} of a {Large}-{Scale} {Agile} {Transformation}},
	volume = {37},
	abstract = {Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This paper presents an in-depth field study of a large-scale Agile transformation in a missioncritical environment, where stakeholders’ commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command \& Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the paper with data-driven recommendations concerning the management of Agile projects. CCS Concepts: • Social and professional topics → Project and people management; Software management; Sustainability; • Software and its engineering → Agile software development.},
	language = {en},
	number = {4},
	author = {Russo, Daniel},
	pages = {45},
}

@inproceedings{mitchellComparisonSoftwareCost2009,
	address = {Lake Buena Vista, FL, USA},
	title = {A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development: {A} systematic review},
	isbn = {978-1-4244-4842-5},
	shorttitle = {A comparison of software cost, duration, and quality for waterfall vs. iterative and incremental development},
	url = {http://ieeexplore.ieee.org/document/5314228/},
	doi = {10.1109/ESEM.2009.5314228},
	abstract = {The objective of this study is to present a body of evidence that will assist software project managers to make informed choices about software development approaches for their projects. In particular, two broadly defined competing approaches, the traditional “waterfall” approach and iterative and incremental development (IID), are compared with regards to development cost and duration, and resulting product quality. The method used for this comparison is a systematic literature review. The small set of studies we located did not demonstrate any identifiable cost, duration, or quality trends, although there was some evidence suggesting the superiority of IID (in particular XP). The results of this review indicate that further empirical studies, both quantitative and qualitative, on this topic need to be undertaken. In order to effectively compare study results, the research community needs to reach a consensus on a set of comparable parameters that best assess cost, duration, and quality.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {2009 3rd {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE},
	author = {Mitchell, Susan M. and Seaman, Carolyn B.},
	month = oct,
	year = {2009},
	pages = {511--515},
}

@article{beckEmbracingChangeExtreme1999,
	title = {Embracing change with extreme programming},
	volume = {32},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/796139/},
	doi = {10.1109/2.796139},
	language = {en},
	number = {10},
	urldate = {2021-04-04},
	journal = {Computer},
	author = {Beck, K.},
	month = oct,
	year = {1999},
	pages = {70--77},
}

@article{battin_leveraging_2001,
	title = {Leveraging resources in global software development},
	volume = {18},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/914750/},
	doi = {10.1109/52.914750},
	language = {en},
	number = {2},
	urldate = {2021-06-21},
	journal = {IEEE Software},
	author = {Battin, R.D. and Crocker, R. and Kreidler, J. and Subramanian, K.},
	month = apr,
	year = {2001},
	pages = {70--77},
}

@article{herbsleb_architectures_1999,
	title = {Architectures, coordination, and distance: {Conway}'s law and beyond},
	volume = {16},
	issn = {07407459},
	shorttitle = {Architectures, coordination, and distance},
	url = {http://ieeexplore.ieee.org/document/795103/},
	doi = {10.1109/52.795103},
	language = {en},
	number = {5},
	urldate = {2021-06-21},
	journal = {IEEE Software},
	author = {Herbsleb, J.D. and Grinter, R.E.},
	month = oct,
	year = {1999},
	pages = {63--70},
}

@article{herbsleb_formulation_nodate,
	title = {Formulation and {Preliminary} {Test} of an {Empirical} {Theory} of {Coordination} in {Software} {Engineering}},
	abstract = {Motivated by evidence that coordination and dependencies among engineering decisions in a software project are key to better understanding and better methods of software creation, we set out to create empirically testable theory to characterize and make predictions about coordination of engineering decisions. We demonstrate that our theory is capable of expressing some of the main ideas about coordination in software engineering, such as Conway’s law and the effects of information hiding in modular design. We then used software project data to create measures and test two hypotheses derived from our theory. Our results provide preliminary support for our formulations.},
	language = {en},
	author = {Herbsleb, James D and Mockus, Audris},
	pages = {10},
}

@inproceedings{elovitz1979experiment,
	title = {An experiment in software engineering: {The} architecture research facility as a case study},
	booktitle = {Proceedings of the 4th international conference on {Software} engineering},
	author = {Elovitz, Honey S},
	year = {1979},
	pages = {145--152},
}

@inproceedings{begel2008pair,
	title = {Pair programming: what's in it for me?},
	booktitle = {Proceedings of the {Second} {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	author = {Begel, Andrew and Nagappan, Nachiappan},
	year = {2008},
	pages = {120--128},
}

@inproceedings{ford2016paradise,
	title = {Paradise unplugged: {Identifying} barriers for female participation on stack overflow},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} international symposium on foundations of software engineering},
	author = {Ford, Denae and Smith, Justin and Guo, Philip J and Parnin, Chris},
	year = {2016},
	pages = {846--857},
}

@inproceedings{mao2013pricing,
	title = {Pricing crowdsourcing-based software development tasks},
	booktitle = {2013 35th international conference on software engineering ({ICSE})},
	author = {Mao, Ke and Yang, Ye and Li, Mingshu and Harman, Mark},
	year = {2013},
	note = {tex.organization: IEEE},
	pages = {1205--1208},
}

@inproceedings{hulkko2005multiple,
	title = {A multiple case study on the impact of pair programming on product quality},
	booktitle = {Proceedings of the 27th international conference on {Software} engineering},
	author = {Hulkko, Hanna and Abrahamsson, Pekka},
	year = {2005},
	pages = {495--504},
}

@article{menzies_software_2013,
	title = {Software {Analytics}: {So} {What}?},
	volume = {30},
	issn = {0740-7459},
	shorttitle = {Software {Analytics}},
	url = {http://ieeexplore.ieee.org/document/6547619/},
	doi = {10.1109/MS.2013.86},
	language = {en},
	number = {4},
	urldate = {2021-06-21},
	journal = {IEEE Software},
	author = {Menzies, Tim and Zimmermann, Thomas},
	month = jul,
	year = {2013},
	pages = {31--37},
}

@article{boehm_spiral_1988,
	title = {A spiral model of software development and enhancement},
	volume = {21},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/59/},
	doi = {10.1109/2.59},
	language = {en},
	number = {5},
	urldate = {2021-08-20},
	journal = {Computer},
	author = {Boehm, B. W.},
	month = may,
	year = {1988},
	pages = {61--72},
}

@inproceedings{royce1970managing,
	title = {Managing the development of large software systems: concepts and techniques},
	booktitle = {Proceedings of {IEEE} {WESCON}},
	author = {Royce, Winston W},
	year = {1970},
	pages = {328--338},
}

@book{yourdon2004death,
	title = {Death march},
	publisher = {Prentice Hall Professional},
	author = {Yourdon, Edward},
	year = {2004},
}

@inproceedings{boehm_view_2006,
	address = {Shanghai China},
	title = {A view of 20th and 21st century software engineering},
	isbn = {978-1-59593-375-1},
	url = {https://dl.acm.org/doi/10.1145/1134285.1134288},
	doi = {10.1145/1134285.1134288},
	abstract = {George Santayana's statement, "Those who cannot remember the past are condemned to repeat it," is only half true. The past also includes successful histories. If you haven't been made aware of them, you're often condemned not to repeat their successes.},
	language = {en},
	urldate = {2021-07-15},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {ACM},
	author = {Boehm, Barry},
	month = may,
	year = {2006},
	pages = {12--29},
}

@inproceedings{perry_empirical_2000,
	title = {Empirical {Studies} of {Software} {Engineering}: {A} {Roadmap}},
	abstract = {In this article we summarize the strengths and weaknesses of empirical research in software engineering. We argue that in order to improve the current situation we must create better studies and draw more credible interpretations from them. We finally present a roadmap for this improvement, which includes a general structure for software empirical studies and concrete steps for achieving these goals: designing better studies, collecting data more effectively, and involving others in our empirical enterprises.},
	language = {en},
	booktitle = {Proceedings of the conference on {The} future of {Software} engineering},
	author = {Perry, Dewayne and Porter, Adam and Votta, Lawrence},
	year = {2000},
	pages = {11},
}

@book{demarco2013peopleware,
	title = {Peopleware: productive projects and teams},
	publisher = {Addison-Wesley},
	author = {DeMarco, Tom and Lister, Tim},
	year = {2013},
}

@misc{harman_how_1998,
	title = {How long is this going to take?},
	url = {http://www0.cs.ucl.ac.uk/staff/M.Harman/exe11.html},
	author = {Harman, Mark},
	year = {1998},
}

@article{boochHistorySoftwareEngineering2018,
	title = {The {History} of {Software} {Engineering}},
	volume = {35},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/8474489/},
	doi = {10.1109/MS.2018.3571234},
	language = {en},
	number = {5},
	urldate = {2021-05-16},
	journal = {IEEE Software},
	author = {Booch, Grady},
	month = sep,
	year = {2018},
	pages = {108--114},
}

@article{knight_should_2002,
	title = {Should software engineers be licensed?},
	volume = {45},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/581571.581601},
	doi = {10.1145/581571.581601},
	language = {en},
	number = {11},
	urldate = {2021-06-29},
	journal = {Communications of the ACM},
	author = {Knight, John C. and Leveson, Nancy G.},
	month = nov,
	year = {2002},
	pages = {87--90},
}

@article{white_acms_2002,
	title = {{ACM}'s position on the licensing of software engineers},
	volume = {45},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/581571.581602},
	doi = {10.1145/581571.581602},
	language = {en},
	number = {11},
	urldate = {2021-06-29},
	journal = {Communications of the ACM},
	author = {White, John and Simons, Barbara},
	month = nov,
	year = {2002},
	pages = {91},
}

@inproceedings{gotterbarn1997professionalization,
	title = {The professionalization of software engineering and its significance for ethics education},
	volume = {1},
	booktitle = {Proceedings frontiers in education 1997 27th annual conference. {Teaching} and learning in an era of change},
	author = {Gotterbarn, Donald},
	year = {1997},
	note = {tex.organization: IEEE},
	pages = {484--489},
}

@article{mok_review_2010,
	title = {A {Review} of the {Professionalization} of the {Software} {Industry}: {Has} it {Made} {Software} {Engineering} a {Real} {Profession}?},
	volume = {16},
	abstract = {Every industry strives to be called a “profession”, and software engineering is no exception. This paper attempts to define “profession” from three different perspectives and provides a chronological narration of the professionalization efforts of major IT bodies such as the IEEE Computer Society, Association of Computing Machinery and British Computer Society to promote software engineering from “occupation” to “profession”. The outcome of this professionalization process is then examined against the three vastly different definitions of “profession” to qualitatively gauge the success of the professionalization process.},
	language = {en},
	number = {1},
	journal = {International Journal of Information Technology},
	author = {Mok, Heng Ngee},
	year = {2010},
	pages = {16},
}

@article{landwehr_software_2017,
	title = {Software {Systems} {Engineering} programmes a capability approach},
	volume = {125},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121216302576},
	doi = {10.1016/j.jss.2016.12.016},
	language = {en},
	urldate = {2021-07-12},
	journal = {Journal of Systems and Software},
	author = {Landwehr, Carl and Ludewig, Jochen and Meersman, Robert and Parnas, David Lorge and Shoval, Peretz and Wand, Yair and Weiss, David and Weyuker, Elaine},
	month = mar,
	year = {2017},
	pages = {354--364},
}

@article{parnas_software_1999,
	title = {Software engineering programs are not computer science programs},
	volume = {16},
	issn = {07407459},
	url = {http://ieeexplore.ieee.org/document/805469/},
	doi = {10.1109/52.805469},
	language = {en},
	number = {6},
	urldate = {2021-06-29},
	journal = {IEEE Software},
	author = {Parnas, D.L.},
	month = dec,
	year = {1999},
	pages = {19--30},
}

@article{mcconnell1999software,
	title = {Software engineering principles},
	volume = {16},
	number = {2},
	journal = {IEEE software},
	author = {McConnell, Steve},
	year = {1999},
	note = {Publisher: IEEE Computer Society},
	pages = {6},
}

@article{feitelsonHowDevelopersChoose2020,
	title = {How {Developers} {Choose} {Names}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9018121/},
	doi = {10.1109/TSE.2020.2976920},
	abstract = {The names of variables and functions serve as implicit documentation and are instrumental for program comprehension. But choosing good meaningful names is hard. We perform a sequence of experiments in which a total of 334 subjects are required to choose names in given programming scenarios. The ﬁrst experiment shows that the probability that two developers would select the same name is low: in the 47 instances in our experiments the median probability was only 6.9\%. At the same time, given that a speciﬁc name is chosen, it is usually understood by the majority of developers. Analysis of the names given in the experiment suggests a model where naming is a (not necessarily cognizant or serial) three-step process: (1) selecting the concepts to include in the name, (2) choosing the words to represent each concept, and (3) constructing a name using these words. A followup experiment, using the same experimental setup, then checked whether using this model explicitly can improve the quality of names. The results were that names selected by subjects using the model were judged by two independent judges to be superior to names chosen in the original experiment by a ratio of two-to-one. Using the model appears to encourage the use of more concepts and longer names.},
	language = {en},
	urldate = {2021-05-25},
	journal = {IEEE Transactions on Software Engineering},
	author = {Feitelson, Dror and Mizrahi, Ayelet and Noy, Nofar and Ben Shabat, Aviad and Eliyahu, Or and Sheffer, Roy},
	year = {2020},
	pages = {1--1},
}

@article{allamanisSurveyMachineLearning2018,
	title = {A {Survey} of {Machine} {Learning} for {Big} {Code} and {Naturalness}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3212695},
	doi = {10.1145/3212695},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
	language = {en},
	number = {4},
	urldate = {2021-04-18},
	journal = {ACM Computing Surveys},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	month = sep,
	year = {2018},
	pages = {1--37},
}

@article{institute1991ansi,
	title = {{ANSI}/{IEEE} standard glossary of software engineering terminology},
	author = {Electrical, Institute of and Engineers, Electronics},
	year = {1991},
}

@article{abran2004software,
	title = {Software engineering body of knowledge},
	journal = {IEEE Computer Society, Angela Burgess},
	author = {Abran, Alain and Moore, James W and Bourque, Pierre and Dupuis, Robert and Tripp, L},
	year = {2004},
}

@inproceedings{wainakhIdBenchEvaluatingSemantic2021,
	address = {Madrid, Spain},
	title = {{IdBench}: {Evaluating} {Semantic} {Representations} of {Identifier} {Names} in {Source} {Code}},
	isbn = {978-1-66540-296-5},
	shorttitle = {{IdBench}},
	url = {https://ieeexplore.ieee.org/document/9401986/},
	doi = {10.1109/ICSE43902.2021.00059},
	abstract = {Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of namebased analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., le n and s i z e , are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Wainakh, Yaza and Rauf, Moiz and Pradel, Michael},
	month = may,
	year = {2021},
	pages = {562--573},
}

@article{ramseyLiterateProgrammingSimplified1994,
	title = {Literate programming simplified},
	volume = {11},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/311070/},
	doi = {10.1109/52.311070},
	language = {en},
	number = {5},
	urldate = {2021-04-26},
	journal = {IEEE Software},
	author = {Ramsey, N.},
	month = sep,
	year = {1994},
	pages = {97--105},
}
